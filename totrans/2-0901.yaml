- en: Fine-Tuning Large Language Models (LLMs)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91](https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91](https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91)
- en: A conceptual overview with example Python code
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¸¦æœ‰ç¤ºä¾‹Pythonä»£ç çš„æ¦‚å¿µæ¦‚è¿°
- en: '[](https://shawhin.medium.com/?source=post_page-----23473d763b91--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----23473d763b91--------------------------------)[](https://towardsdatascience.com/?source=post_page-----23473d763b91--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----23473d763b91--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----23473d763b91--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shawhin.medium.com/?source=post_page-----23473d763b91--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----23473d763b91--------------------------------)[](https://towardsdatascience.com/?source=post_page-----23473d763b91--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----23473d763b91--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----23473d763b91--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----23473d763b91--------------------------------)
    Â·14 min readÂ·Sep 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----23473d763b91--------------------------------)
    Â·14åˆ†é’Ÿé˜…è¯»Â·2023å¹´9æœˆ11æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: This is the 5th article in a [series on using Large Language Models](https://medium.com/towards-data-science/a-practical-introduction-to-llms-65194dda1148)
    (LLMs) in practice. In this post, we will discuss how to fine-tune (FT) a pre-trained
    LLM. We start by introducing key FT concepts and techniques, then finish with
    a concrete example of how to fine-tune a model (locally) using Python and Hugging
    Faceâ€™s software ecosystem.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å…³äº[ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹](https://medium.com/towards-data-science/a-practical-introduction-to-llms-65194dda1148)ï¼ˆLLMsï¼‰å®è·µçš„ç¬¬5ç¯‡æ–‡ç« ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•å¯¹é¢„è®­ç»ƒçš„LLMè¿›è¡Œå¾®è°ƒï¼ˆFTï¼‰ã€‚æˆ‘ä»¬å°†é¦–å…ˆä»‹ç»å…³é”®çš„FTæ¦‚å¿µå’ŒæŠ€æœ¯ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªå…·ä½“çš„ç¤ºä¾‹ï¼Œæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨Pythonå’ŒHugging
    Faceçš„è½¯ä»¶ç”Ÿæ€ç³»ç»Ÿåœ¨æœ¬åœ°å¾®è°ƒæ¨¡å‹ã€‚
- en: '![](../Images/d74b7565b127d69e37ddf51e16125896.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d74b7565b127d69e37ddf51e16125896.png)'
- en: Tuning a language model. Image by author.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒæ•´è¯­è¨€æ¨¡å‹ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: In the [previous article](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)
    of this series, we saw how we could build practical LLM-powered applications by
    integrating prompt engineering into our Python code. For the vast majority of
    LLM use cases, this is the initial approach I recommend because it requires significantly
    less resources and technical expertise than other methods while still providing
    much of the upside.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[æœ¬ç³»åˆ—çš„ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°å¦‚ä½•é€šè¿‡å°†æç¤ºå·¥ç¨‹æ•´åˆåˆ°Pythonä»£ç ä¸­æ¥æ„å»ºå®ç”¨çš„LLMé©±åŠ¨åº”ç”¨ç¨‹åºã€‚å¯¹äºç»å¤§å¤šæ•°LLMä½¿ç”¨æ¡ˆä¾‹ï¼Œè¿™æ˜¯æˆ‘æ¨èçš„åˆæ­¥æ–¹æ³•ï¼Œå› ä¸ºå®ƒæ¯”å…¶ä»–æ–¹æ³•éœ€è¦çš„èµ„æºå’ŒæŠ€æœ¯ä¸“é•¿å°‘å¾—å¤šï¼ŒåŒæ—¶ä»ç„¶èƒ½æä¾›è®¸å¤šå¥½å¤„ã€‚
- en: However, there are situations where prompting an existing LLM out-of-the-box
    doesnâ€™t cut it, and a more sophisticated solution is required. This is where model
    fine-tuning can help.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå­˜åœ¨ä¸€äº›æƒ…å†µï¼Œå…¶ä¸­ç›´æ¥æç¤ºç°æœ‰çš„LLMå¹¶ä¸å¤Ÿæœ‰æ•ˆï¼Œéœ€è¦æ›´å¤æ‚çš„è§£å†³æ–¹æ¡ˆã€‚è¿™å°±æ˜¯æ¨¡å‹å¾®è°ƒå¯ä»¥å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚
- en: Supplemental Video.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: é™„åŠ è§†é¢‘ã€‚
- en: '**What is Fine-tuning?**'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ä»€ä¹ˆæ˜¯å¾®è°ƒï¼Ÿ**'
- en: '**Fine-tuning** is taking a pre-trained model and **training at least one internal
    model parameter** (i.e. weights). In the context of LLMs, what this typically
    accomplishes is transforming a general-purpose base model (e.g. GPT-3) into a
    specialized model for a particular use case (e.g. ChatGPT) [1].'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¾®è°ƒ**æ˜¯å¯¹ä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œ**è‡³å°‘ä¸€ä¸ªå†…éƒ¨æ¨¡å‹å‚æ•°çš„è®­ç»ƒ**ï¼ˆå³æƒé‡ï¼‰ã€‚åœ¨LLMsçš„ä¸Šä¸‹æ–‡ä¸­ï¼Œè¿™é€šå¸¸æ˜¯å°†ä¸€ä¸ªé€šç”¨çš„åŸºç¡€æ¨¡å‹ï¼ˆä¾‹å¦‚GPT-3ï¼‰è½¬å˜ä¸ºä¸€ä¸ªé’ˆå¯¹ç‰¹å®šç”¨ä¾‹çš„ä¸“ä¸šæ¨¡å‹ï¼ˆä¾‹å¦‚ChatGPTï¼‰[1]ã€‚'
- en: The **key upside** of this approach is that models can achieve better performance
    while requiring (far) fewer manually labeled examples compared to models that
    solely rely on supervised training.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•çš„**ä¸»è¦ä¼˜åŠ¿**åœ¨äºï¼Œä¸ä»…ä¾èµ–ç›‘ç£è®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œæ¨¡å‹å¯ä»¥åœ¨éœ€è¦ï¼ˆè¿œï¼‰å°‘å¾—å¤šçš„äººå·¥æ ‡è®°ç¤ºä¾‹çš„æƒ…å†µä¸‹å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚
- en: While strictly self-supervised base models can exhibit impressive performance
    on a wide variety of tasks with the help of prompt engineering [2], they are still
    word predictors and may generate completions that are not entirely helpful or
    accurate. For example, letâ€™s compare the completions of davinci (base GPT-3 model)
    and text-davinci-003 (a fine-tuned model).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶ä¸¥æ ¼çš„è‡ªç›‘ç£åŸºç¡€æ¨¡å‹åœ¨å€ŸåŠ©æç¤ºå·¥ç¨‹[2]çš„å¸®åŠ©ä¸‹å¯ä»¥åœ¨å„ç§ä»»åŠ¡ä¸Šå±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ï¼Œä½†å®ƒä»¬ä»ç„¶æ˜¯è¯é¢„æµ‹å™¨ï¼Œå¯èƒ½ç”Ÿæˆçš„å®Œæˆç»“æœå¹¶ä¸å®Œå…¨æœ‰ç”¨æˆ–å‡†ç¡®ã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬æ¯”è¾ƒdavinciï¼ˆåŸºç¡€GPT-3æ¨¡å‹ï¼‰å’Œtext-davinci-003ï¼ˆä¸€ä¸ªç»è¿‡å¾®è°ƒçš„æ¨¡å‹ï¼‰çš„å®Œæˆç»“æœã€‚
- en: '![](../Images/b3618c95fe5ccf99162d28d4b1bb1344.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3618c95fe5ccf99162d28d4b1bb1344.png)'
- en: Completion comparison of davinci (base GPT-3 model) and text-davinci-003 (a
    fine-tuned model). Image by author.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: davinciï¼ˆåŸºç¡€GPT-3æ¨¡å‹ï¼‰å’Œtext-davinci-003ï¼ˆä¸€ä¸ªç»è¿‡å¾®è°ƒçš„æ¨¡å‹ï¼‰çš„å®Œæˆç»“æœæ¯”è¾ƒã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Notice the base model is simply trying to complete the text by listing a set
    of questions like a Google search or homework assignment, while the **fine-tuned
    model gives a more helpful response**. The flavor of fine-tuning used for text-davinci-003
    is **alignment tuning,** which aims to make the LLMâ€™s responses more helpful,
    honest, and harmless, but more on that later [3,4].
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼ŒåŸºç¡€æ¨¡å‹ä»…ä»…æ˜¯é€šè¿‡åˆ—å‡ºä¸€ç³»åˆ—é—®é¢˜æ¥å®Œæˆæ–‡æœ¬ï¼Œå°±åƒè°·æ­Œæœç´¢æˆ–ä½œä¸šä»»åŠ¡ä¸€æ ·ï¼Œè€Œ**å¾®è°ƒæ¨¡å‹æä¾›äº†æ›´æœ‰å¸®åŠ©çš„å“åº”**ã€‚text-davinci-003æ‰€ä½¿ç”¨çš„å¾®è°ƒæ–¹æ³•æ˜¯**å¯¹é½å¾®è°ƒ**ï¼Œå…¶ç›®çš„æ˜¯ä½¿å¤§å‹è¯­è¨€æ¨¡å‹çš„å“åº”æ›´æœ‰å¸®åŠ©ã€æ›´è¯šå®å’Œæ— å®³ï¼Œä½†ç¨åä¼šè¯¦ç»†ä»‹ç»[3,4]ã€‚
- en: '**Why Fine-tune**'
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ä¸ºä»€ä¹ˆè¦å¾®è°ƒ**'
- en: Fine-tuning not only improves the performance of a base model, but **a smaller
    (fine-tuned) model can often outperform larger (more expensive) models** on the
    set of tasks on which it was trained [4]. This was demonstrated by OpenAI with
    their first generation â€œInstructGPTâ€ models, where the 1.3B parameter InstructGPT
    model completions were preferred over the 175B parameter GPT-3 base model despite
    being 100x smaller [4].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å¾®è°ƒä¸ä»…èƒ½æé«˜åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼Œè€Œä¸”**è¾ƒå°çš„ï¼ˆå¾®è°ƒçš„ï¼‰æ¨¡å‹åœ¨è®­ç»ƒçš„ä»»åŠ¡é›†ä¸Šå¾€å¾€èƒ½è¶…è¶Šæ›´å¤§çš„ï¼ˆæ›´æ˜‚è´µçš„ï¼‰æ¨¡å‹**[4]ã€‚OpenAIé€šè¿‡å…¶ç¬¬ä¸€ä»£â€œInstructGPTâ€æ¨¡å‹è¯æ˜äº†è¿™ä¸€ç‚¹ï¼Œå…¶ä¸­1.3Bå‚æ•°çš„InstructGPTæ¨¡å‹åœ¨å®Œæˆç»“æœä¸Šä¼˜äº175Bå‚æ•°çš„GPT-3åŸºç¡€æ¨¡å‹ï¼Œå°½ç®¡å®ƒå°äº†100å€[4]ã€‚
- en: Although most of the LLMs we may interact with these days are not strictly self-supervised
    models like GPT-3, there are still drawbacks to prompting an existing fine-tuned
    model for a specific use case.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æˆ‘ä»¬ä»Šå¤©å¯èƒ½ä¸ä¹‹äº’åŠ¨çš„å¤§å¤šæ•°å¤§å‹è¯­è¨€æ¨¡å‹å¹¶éåƒGPT-3é‚£æ ·ä¸¥æ ¼çš„è‡ªç›‘ç£æ¨¡å‹ï¼Œä½†å¯¹ç°æœ‰çš„å¾®è°ƒæ¨¡å‹è¿›è¡Œç‰¹å®šç”¨ä¾‹çš„æç¤ºä»ç„¶å­˜åœ¨ç¼ºé™·ã€‚
- en: A big one is LLMs have a finite context window. Thus, the model may perform
    sub-optimally on tasks that require a large knowledge base or domain-specific
    information [1]. Fine-tuned models can avoid this issue by â€œlearningâ€ this information
    during the fine-tuning process. This also precludes the need to jam-pack prompts
    with additional context and thus can result in lower inference costs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé‡è¦çš„é—®é¢˜æ˜¯ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹æœ‰ä¸€ä¸ªæœ‰é™çš„ä¸Šä¸‹æ–‡çª—å£ã€‚å› æ­¤ï¼Œæ¨¡å‹å¯èƒ½åœ¨éœ€è¦å¤§é‡çŸ¥è¯†åŸºç¡€æˆ–ç‰¹å®šé¢†åŸŸä¿¡æ¯çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³[1]ã€‚å¾®è°ƒæ¨¡å‹å¯ä»¥é€šè¿‡åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­â€œå­¦ä¹ â€è¿™äº›ä¿¡æ¯æ¥é¿å…è¿™ä¸ªé—®é¢˜ã€‚è¿™ä¹Ÿé¿å…äº†åœ¨æç¤ºä¸­å¡å…¥é¢å¤–çš„ä¸Šä¸‹æ–‡ï¼Œä»è€Œå¯ä»¥é™ä½æ¨ç†æˆæœ¬ã€‚
- en: '**3 Ways to Fine-tune**'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**å¾®è°ƒçš„ä¸‰ç§æ–¹æ³•**'
- en: 'There are **3 generic ways one can fine-tune** a model: self-supervised, supervised,
    and reinforcement learning. These are not mutually exclusive in that any combination
    of these three approaches can be used in succession to fine-tune a single model.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰**ä¸‰ç§é€šç”¨çš„æ¨¡å‹å¾®è°ƒæ–¹æ³•**ï¼šè‡ªç›‘ç£ã€ç›‘ç£å’Œå¼ºåŒ–å­¦ä¹ ã€‚è¿™äº›æ–¹æ³•å¹¶éç›¸äº’æ’æ–¥ï¼Œå¯ä»¥æŒ‰é¡ºåºç»„åˆä½¿ç”¨è¿™ä¸‰ç§æ–¹æ³•æ¥å¾®è°ƒä¸€ä¸ªæ¨¡å‹ã€‚
- en: '**Self-supervised Learning**'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**è‡ªç›‘ç£å­¦ä¹ **'
- en: '**Self-supervised learning** consists of **training a model based on the inherent
    structure of the training data**. In the context of LLMs, what this typically
    looks like is given a sequence of words (or tokens, to be more precise), predict
    the next word (token).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**è‡ªç›‘ç£å­¦ä¹ **åŒ…æ‹¬**åŸºäºè®­ç»ƒæ•°æ®çš„å›ºæœ‰ç»“æ„æ¥è®­ç»ƒæ¨¡å‹**ã€‚åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒŒæ™¯ä¸‹ï¼Œè¿™é€šå¸¸è¡¨ç°ä¸ºç»™å®šä¸€ä¸ªè¯ï¼ˆæˆ–æ›´å‡†ç¡®åœ°è¯´æ˜¯ä»¤ç‰Œï¼‰çš„åºåˆ—ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼ˆä»¤ç‰Œï¼‰ã€‚'
- en: While this is how many pre-trained language models are developed these days,
    it can also be used for model fine-tuning. A potential use case of this is developing
    a model that can mimic a personâ€™s writing style given a set of example texts.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¿™æ˜¯è®¸å¤šç°ä»Šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å¼€å‘æ–¹å¼ï¼Œä½†å®ƒä¹Ÿå¯ä»¥ç”¨äºæ¨¡å‹å¾®è°ƒã€‚ä¸€ä¸ªæ½œåœ¨çš„åº”ç”¨åœºæ™¯æ˜¯å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿæ¨¡æ‹ŸæŸäººå†™ä½œé£æ ¼çš„æ¨¡å‹ï¼Œç»™å®šä¸€ç»„ç¤ºä¾‹æ–‡æœ¬ã€‚
- en: '**Supervised Learning**'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç›‘ç£å­¦ä¹ **'
- en: The next, and perhaps most popular, way to fine-tune a model is via **supervised
    learning**. This involves **training a model on input-output pairs** for a particular
    task. An example is **instruction tuning,** which aims to improve model performance
    in answering questions or responding to user prompts [1,3].
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€ç§ï¼Œä¹Ÿè®¸æ˜¯æœ€æµè¡Œçš„ï¼Œå¾®è°ƒæ¨¡å‹çš„æ–¹æ³•æ˜¯é€šè¿‡**ç›‘ç£å­¦ä¹ **ã€‚è¿™æ¶‰åŠåˆ°**å¯¹ç‰¹å®šä»»åŠ¡çš„è¾“å…¥-è¾“å‡ºå¯¹è¿›è¡Œæ¨¡å‹è®­ç»ƒ**ã€‚ä¸€ä¸ªä¾‹å­æ˜¯**æŒ‡ä»¤è°ƒæ•´**ï¼Œå…¶ç›®çš„æ˜¯æå‡æ¨¡å‹åœ¨å›ç­”é—®é¢˜æˆ–å›åº”ç”¨æˆ·æç¤ºæ—¶çš„è¡¨ç°[1,3]ã€‚
- en: 'The **key step** in supervised learning is **curating a training dataset**.
    A simple way to do this is to create question-answer pairs and integrate them
    into a prompt template [1,3]. For example, the question-answer pair: *Who was
    the 35th President of the United States? â€” John F. Kennedy* could be pasted into
    the below prompt template. More example prompt templates are available in section
    A.2.1 of ref [4].'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç›‘ç£å­¦ä¹ ä¸­çš„**å…³é”®æ­¥éª¤**æ˜¯**ç­–åˆ’è®­ç»ƒæ•°æ®é›†**ã€‚ä¸€ç§ç®€å•çš„æ–¹æ³•æ˜¯åˆ›å»ºé—®ç­”å¯¹å¹¶å°†å…¶æ•´åˆåˆ°æç¤ºæ¨¡æ¿ä¸­[1,3]ã€‚ä¾‹å¦‚ï¼Œé—®ç­”å¯¹ï¼š*è°æ˜¯ç¾å›½ç¬¬35ä»»æ€»ç»Ÿï¼Ÿâ€”â€”
    çº¦ç¿°Â·FÂ·è‚¯å°¼è¿ª* å¯ä»¥ç²˜è´´åˆ°ä¸‹é¢çš„æç¤ºæ¨¡æ¿ä¸­ã€‚æ›´å¤šç¤ºä¾‹æç¤ºæ¨¡æ¿å¯ä»¥åœ¨å‚è€ƒæ–‡çŒ®[4]çš„A.2.1èŠ‚ä¸­æ‰¾åˆ°ã€‚
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using a prompt template is important because base models like GPT-3 are essentially
    â€œdocument completersâ€. Meaning, given some text, the model generates more text
    that (statistically) makes sense in that context. This goes back to the [previous
    blog](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)
    of this series and the idea of â€œtrickingâ€ a language model into solving your problem
    via prompt engineering.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æç¤ºæ¨¡æ¿å¾ˆé‡è¦ï¼Œå› ä¸ºåƒGPT-3è¿™æ ·çš„åŸºç¡€æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯â€œæ–‡æ¡£è¡¥å…¨å™¨â€ã€‚è¿™æ„å‘³ç€ï¼Œç»™å®šä¸€äº›æ–‡æœ¬ï¼Œæ¨¡å‹ä¼šç”Ÿæˆåœ¨è¯¥ä¸Šä¸‹æ–‡ä¸­ï¼ˆç»Ÿè®¡ä¸Šï¼‰æœ‰æ„ä¹‰çš„æ›´å¤šæ–‡æœ¬ã€‚è¿™å›åˆ°æœ¬ç³»åˆ—çš„[ä¸Šä¸€ç¯‡åšå®¢](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)å’Œé€šè¿‡æç¤ºå·¥ç¨‹â€œæ¬ºéª—â€è¯­è¨€æ¨¡å‹æ¥è§£å†³é—®é¢˜çš„æƒ³æ³•ã€‚
- en: '[](/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f?source=post_page-----23473d763b91--------------------------------)
    [## Prompt Engineering â€” How to trick AI into solving your problems'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f?source=post_page-----23473d763b91--------------------------------)
    [## æç¤ºå·¥ç¨‹ â€” å¦‚ä½•â€œæ¬ºéª—â€AIæ¥è§£å†³ä½ çš„é—®é¢˜'
- en: 7 prompting tricks, Langchain, and Python example code
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7ä¸ªæç¤ºæŠ€å·§ã€Langchainå’ŒPythonç¤ºä¾‹ä»£ç 
- en: towardsdatascience.com](/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f?source=post_page-----23473d763b91--------------------------------)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f?source=post_page-----23473d763b91--------------------------------)
- en: '**Reinforcement Learning**'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**å¼ºåŒ–å­¦ä¹ **'
- en: Finally, one can use **reinforcement learning (RL)** to fine-tune models. RL
    **uses a reward model to guide the training of the base model**. This can take
    many different forms, but the basic idea is to train the reward model to score
    language model completions such that they reflect the preferences of human labelers
    [3,4]. The reward model can then be combined with a reinforcement learning algorithm
    (e.g. Proximal Policy Optimization (PPO)) to fine-tune the pre-trained model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå¯ä»¥ä½¿ç”¨**å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰**æ¥å¾®è°ƒæ¨¡å‹ã€‚RL**ä½¿ç”¨å¥–åŠ±æ¨¡å‹æ¥æŒ‡å¯¼åŸºç¡€æ¨¡å‹çš„è®­ç»ƒ**ã€‚è¿™å¯ä»¥é‡‡å–å¤šç§å½¢å¼ï¼Œä½†åŸºæœ¬æ€æƒ³æ˜¯è®­ç»ƒå¥–åŠ±æ¨¡å‹ä»¥å¯¹è¯­è¨€æ¨¡å‹çš„å®Œæˆæƒ…å†µè¿›è¡Œè¯„åˆ†ï¼Œä½¿å…¶åæ˜ äººç±»æ ‡æ³¨è€…çš„åå¥½[3,4]ã€‚ç„¶åï¼Œå¥–åŠ±æ¨¡å‹å¯ä»¥ä¸å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆä¾‹å¦‚ï¼Œè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ï¼‰ç»“åˆä½¿ç”¨ï¼Œä»¥å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚
- en: An example of how RL can be used for model fine-tuning is demonstrated by OpenAIâ€™s
    InstructGPT models, which were developed through **3 key steps** [4].
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAIçš„InstructGPTæ¨¡å‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œè¿™äº›æ¨¡å‹æ˜¯é€šè¿‡**3ä¸ªå…³é”®æ­¥éª¤**å¼€å‘çš„[4]ã€‚
- en: 'Generate high-quality prompt-response pairs and fine-tune a pre-trained model
    using supervised learning. (~13k training prompts) *Note: One can (alternatively)
    skip to step 2 with the pre-trained model [3].*'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç”Ÿæˆé«˜è´¨é‡çš„æç¤º-å“åº”å¯¹ï¼Œå¹¶ä½¿ç”¨ç›‘ç£å­¦ä¹ å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚ï¼ˆ~13kè®­ç»ƒæç¤ºï¼‰ *æ³¨æ„ï¼šå¯ä»¥ï¼ˆå¯é€‰åœ°ï¼‰è·³åˆ°æ­¥éª¤2ï¼Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹[3]ã€‚*
- en: Use the fine-tuned model to generate completions and have human-labelers rank
    responses based on their preferences. Use these preferences to train the reward
    model. (~33k training prompts)
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆå®Œæˆï¼Œå¹¶è®©äººå·¥æ ‡æ³¨è€…æ ¹æ®å…¶åå¥½å¯¹å“åº”è¿›è¡Œæ’åã€‚ä½¿ç”¨è¿™äº›åå¥½æ¥è®­ç»ƒå¥–åŠ±æ¨¡å‹ã€‚ï¼ˆ~33kè®­ç»ƒæç¤ºï¼‰
- en: Use the reward model and an RL algorithm (e.g. PPO) to fine-tune the model further.
    (~31k training prompts)
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¥–åŠ±æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆä¾‹å¦‚PPOï¼‰è¿›ä¸€æ­¥å¾®è°ƒæ¨¡å‹ã€‚ï¼ˆ~31kè®­ç»ƒæç¤ºï¼‰
- en: While the strategy above does generally result in LLM completions that are significantly
    more preferable to the base model, it can also come at a cost of lower performance
    in a subset of tasks. This drop in performance is also known as an **alignment
    tax** [3,4].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ä¸Šè¿°ç­–ç•¥é€šå¸¸ä¼šå¯¼è‡´LLMå®Œæˆåº¦æ˜¾è‘—ä¼˜äºåŸºç¡€æ¨¡å‹ï¼Œä½†è¿™ä¹Ÿå¯èƒ½å¯¼è‡´åœ¨æŸäº›ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸‹é™ã€‚è¿™ç§æ€§èƒ½ä¸‹é™ä¹Ÿè¢«ç§°ä¸º**å¯¹é½æˆæœ¬**[3,4]ã€‚
- en: '**Supervised Fine-tuning Steps (High-level)**'
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç›‘ç£å¾®è°ƒæ­¥éª¤ï¼ˆé«˜çº§ï¼‰**'
- en: As we saw above, there are many ways in which one can fine-tune an existing
    language model. However, for the remainder of this article, we will focus on fine-tuning
    via supervised learning. Below is a high-level procedure for supervised model
    fine-tuning [1].
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸Šæ‰€è¿°ï¼Œå¯ä»¥é€šè¿‡è®¸å¤šæ–¹æ³•å¾®è°ƒç°æœ‰è¯­è¨€æ¨¡å‹ã€‚ç„¶è€Œï¼Œæœ¬æ–‡å‰©ä½™éƒ¨åˆ†å°†ä¸“æ³¨äºé€šè¿‡ç›‘ç£å­¦ä¹ è¿›è¡Œå¾®è°ƒã€‚ä»¥ä¸‹æ˜¯ç›‘ç£æ¨¡å‹å¾®è°ƒçš„é«˜çº§ç¨‹åº[1]ã€‚
- en: '**Choose fine-tuning task** (e.g. summarization, question answering, text classification)'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é€‰æ‹©å¾®è°ƒä»»åŠ¡**ï¼ˆä¾‹å¦‚æ€»ç»“ã€é—®ç­”ã€æ–‡æœ¬åˆ†ç±»ï¼‰'
- en: '**Prepare training dataset** i.e. create (100â€“10k) input-output pairs and preprocess
    data (i.e. tokenize, truncate, and pad text).'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å‡†å¤‡è®­ç»ƒæ•°æ®é›†**å³åˆ›å»ºï¼ˆ100â€“10kï¼‰è¾“å…¥è¾“å‡ºå¯¹å¹¶é¢„å¤„ç†æ•°æ®ï¼ˆå³æ ‡è®°åŒ–ã€æˆªæ–­å’Œå¡«å……æ–‡æœ¬ï¼‰ã€‚'
- en: '**Choose a base model** (experiment with different models and choose one that
    performs best on the desired task).'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é€‰æ‹©åŸºç¡€æ¨¡å‹**ï¼ˆå°è¯•ä¸åŒæ¨¡å‹å¹¶é€‰æ‹©åœ¨æ‰€éœ€ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³çš„ä¸€ä¸ªï¼‰ã€‚'
- en: '**Fine-tune model via supervised learning**'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é€šè¿‡ç›‘ç£å­¦ä¹ å¾®è°ƒæ¨¡å‹**'
- en: '**Evaluate model performance**'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è¯„ä¼°æ¨¡å‹æ€§èƒ½**'
- en: While each of these steps could be an article of their own, I want to focus
    on **step 4** and discuss how we can go about training the fine-tuned model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™äº›æ­¥éª¤ä¸­çš„æ¯ä¸€ä¸ªéƒ½å¯ä»¥æˆä¸ºä¸€ç¯‡æ–‡ç« ï¼Œä½†æˆ‘æƒ³ä¸“æ³¨äº**ç¬¬ 4 æ­¥**å¹¶è®¨è®ºå¦‚ä½•è®­ç»ƒå¾®è°ƒåçš„æ¨¡å‹ã€‚
- en: '**3 Options for Parameter Training**'
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**å‚æ•°è®­ç»ƒçš„ 3 ç§é€‰é¡¹**'
- en: When it comes to fine-tuning a model with ~100M-100B parameters, one needs to
    be thoughtful of computational costs. Toward this end, an important question is
    â€” *which parameters do we (re)train?*
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¾®è°ƒæ‹¥æœ‰~100M-100Bå‚æ•°çš„æ¨¡å‹æ—¶ï¼Œéœ€è¦è€ƒè™‘è®¡ç®—æˆæœ¬ã€‚åœ¨è¿™æ–¹é¢ï¼Œä¸€ä¸ªé‡è¦çš„é—®é¢˜æ˜¯â€”â€”*æˆ‘ä»¬ï¼ˆé‡æ–°ï¼‰è®­ç»ƒå“ªäº›å‚æ•°ï¼Ÿ*
- en: With the mountain of parameters at play, we have countless choices for which
    ones we train. Here, I will focus on **three generic options** of which to choose.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤„ç†å¤§é‡å‚æ•°æ—¶ï¼Œæˆ‘ä»¬æœ‰æ— æ•°çš„é€‰æ‹©æ¥å†³å®šè®­ç»ƒå“ªäº›å‚æ•°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘å°†ä¸“æ³¨äº**ä¸‰ç§é€šç”¨é€‰é¡¹**ã€‚
- en: '**Option 1: Retrain all parameters**'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**é€‰é¡¹ 1ï¼šé‡æ–°è®­ç»ƒæ‰€æœ‰å‚æ•°**'
- en: The first option is to **train all internal model parameters** (called **full
    parameter tuning**) [3]. While this option is simple (conceptually), it is the
    most computationally expensive. Additionally, a known issue with full parameter
    tuning is the phenomenon of catastrophic forgetting. This is where the model â€œforgetsâ€
    useful information it â€œlearnedâ€ in its initial training [3].
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªé€‰é¡¹æ˜¯**è®­ç»ƒæ‰€æœ‰å†…éƒ¨æ¨¡å‹å‚æ•°**ï¼ˆç§°ä¸º**å®Œå…¨å‚æ•°è°ƒä¼˜**ï¼‰[3]ã€‚è™½ç„¶è¿™ä¸ªé€‰é¡¹åœ¨æ¦‚å¿µä¸Šå¾ˆç®€å•ï¼Œä½†å®ƒæ˜¯æœ€è€—è´¹è®¡ç®—èµ„æºçš„ã€‚æ­¤å¤–ï¼Œå®Œå…¨å‚æ•°è°ƒä¼˜çš„ä¸€ä¸ªå·²çŸ¥é—®é¢˜æ˜¯ç¾éš¾æ€§é—å¿˜ç°è±¡ã€‚è¿™æ˜¯æ¨¡å‹â€œé—å¿˜â€äº†åœ¨åˆå§‹è®­ç»ƒä¸­â€œå­¦åˆ°â€çš„æœ‰ç”¨ä¿¡æ¯[3]ã€‚
- en: One way we can mitigate the downsides of Option 1 is to freeze a large portion
    of the model parameters, which brings us to Option 2.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡å†»ç»“æ¨¡å‹å‚æ•°çš„å¤§éƒ¨åˆ†æ¥ç¼“è§£é€‰é¡¹ 1 çš„ç¼ºç‚¹ï¼Œè¿™å°±å¼•å‡ºäº†é€‰é¡¹ 2ã€‚
- en: '**Option 2: Transfer Learning**'
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**é€‰é¡¹ 2ï¼šè¿ç§»å­¦ä¹ **'
- en: 'The big idea with **transfer learning (TL)** is to preserve the useful representations/features
    the model has learned from past training when applying the model to a new task.
    This generally consists of **dropping â€œthe headâ€ of a neural network (NN) and
    replacing it with a new one** (e.g. adding new layers with randomized weights).
    *Note: The head of an NN includes its final layers, which translate the modelâ€™s
    internal representations to output values.*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¿ç§»å­¦ä¹ ï¼ˆTLï¼‰**çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä¿ç•™æ¨¡å‹ä»è¿‡å»è®­ç»ƒä¸­å­¦åˆ°çš„æœ‰ç”¨è¡¨ç¤º/ç‰¹å¾ï¼Œå¹¶å°†æ¨¡å‹åº”ç”¨äºæ–°ä»»åŠ¡æ—¶ä½¿ç”¨ã€‚è¿™é€šå¸¸åŒ…æ‹¬**â€œå»æ‰ç¥ç»ç½‘ç»œï¼ˆNNï¼‰çš„å¤´éƒ¨å¹¶ç”¨æ–°çš„å¤´éƒ¨æ›¿æ¢å®ƒâ€**ï¼ˆä¾‹å¦‚ï¼Œæ·»åŠ å…·æœ‰éšæœºæƒé‡çš„æ–°å±‚ï¼‰ã€‚*æ³¨æ„ï¼šç¥ç»ç½‘ç»œçš„å¤´éƒ¨åŒ…æ‹¬å…¶æœ€ç»ˆå±‚ï¼Œè¿™äº›å±‚å°†æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºè½¬æ¢ä¸ºè¾“å‡ºå€¼ã€‚*'
- en: While leaving the majority of parameters untouched mitigates the huge computational
    cost of training an LLM, TL may not necessarily resolve the problem of catastrophic
    forgetting. To better handle both of these issues, we can turn to a different
    set of approaches.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶ä¿æŒå¤§éƒ¨åˆ†å‚æ•°ä¸å˜å¯ä»¥å‡è½»è®­ç»ƒLLMçš„å·¨å¤§è®¡ç®—æˆæœ¬ï¼Œä½†TLå¯èƒ½æ— æ³•è§£å†³ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚ä¸ºäº†æ›´å¥½åœ°å¤„ç†è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥è½¬å‘ä¸€ç»„ä¸åŒçš„æ–¹æ³•ã€‚
- en: '**Option 3: Parameter Efficient Fine-tuning (PEFT)**'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**é€‰é¡¹ 3ï¼šå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰**'
- en: '**PEFT** involves **augmenting a base model with a relatively small number
    of trainable parameters**. The key result of this is a fine-tuning methodology
    that demonstrates comparable performance to full parameter tuning at a tiny fraction
    of the computational and storage cost [5].'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**PEFT** æ¶‰åŠ **ç”¨ç›¸å¯¹å°‘é‡çš„å¯è®­ç»ƒå‚æ•°å¢å¼ºåŸºç¡€æ¨¡å‹**ã€‚å…¶å…³é”®ç»“æœæ˜¯ä¸€ç§å¾®è°ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æä½çš„è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ä¸‹å±•ç¤ºå‡ºä¸å®Œå…¨å‚æ•°è°ƒæ•´ç›¸å½“çš„æ€§èƒ½
    [5]ã€‚'
- en: PEFT encapsulates a family of techniques, one of which is the popular **LoRA
    (Low-Rank Adaptation)** method [6]. The basic idea behind LoRA is to pick a subset
    of layers in an existing model and modify their weights according to the following
    equation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: PEFT å°è£…äº†ä¸€ç³»åˆ—æŠ€æœ¯ï¼Œå…¶ä¸­ä¹‹ä¸€æ˜¯æµè¡Œçš„ **LoRA (ä½ç§©é€‚é…)** æ–¹æ³• [6]ã€‚LoRA çš„åŸºæœ¬æ€æƒ³æ˜¯é€‰æ‹©ç°æœ‰æ¨¡å‹ä¸­çš„ä¸€éƒ¨åˆ†å±‚ï¼Œå¹¶æ ¹æ®ä»¥ä¸‹æ–¹ç¨‹å¼ä¿®æ”¹å®ƒä»¬çš„æƒé‡ã€‚
- en: '![](../Images/446dc4369edc5bc4655f4986a426a513.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/446dc4369edc5bc4655f4986a426a513.png)'
- en: Equation showing how weight matrices are modified for fine-tuning using LoRA
    [6]. Image by author.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼æ˜¾ç¤ºäº†å¦‚ä½•ä½¿ç”¨ LoRA ä¿®æ”¹æƒé‡çŸ©é˜µä»¥è¿›è¡Œå¾®è°ƒ [6]ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Where *h()* = a hidden layer that will be tuned, *x* = the input to *h()*, *Wâ‚€*
    = the original weight matrix for the *h*, and *Î”W* = a matrix of trainable parameters
    injected into *h*. *Î”W* is decomposed according to *Î”W*=*BA*, where *Î”W* is a
    d by k matrix, *B* is d by r, and *A* is r by k*.* r is the assumed â€œintrinsic
    rankâ€ of *Î”W* (which can be as small as 1 or 2) [6].
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *h()* = ä¸€ä¸ªå°†è¢«è°ƒæ•´çš„éšè—å±‚ï¼Œ*x* = è¾“å…¥åˆ° *h()* çš„æ•°æ®ï¼Œ*Wâ‚€* = *h* çš„åŸå§‹æƒé‡çŸ©é˜µï¼Œè€Œ *Î”W* = æ³¨å…¥åˆ° *h*
    çš„å¯è®­ç»ƒå‚æ•°çŸ©é˜µã€‚*Î”W* æ ¹æ® *Î”W*=*BA* è¢«åˆ†è§£ï¼Œå…¶ä¸­ *Î”W* æ˜¯ä¸€ä¸ª d ä¹˜ k çš„çŸ©é˜µï¼Œ*B* æ˜¯ d ä¹˜ r çš„çŸ©é˜µï¼Œè€Œ *A* æ˜¯ r
    ä¹˜ k çš„çŸ©é˜µã€‚r æ˜¯ *Î”W* çš„å‡å®šâ€œå†…åœ¨ç§©â€ï¼ˆå¯ä»¥å°åˆ° 1 æˆ– 2ï¼‰[6]ã€‚
- en: Sorry for all the math, but the **key point is the (d * k) weights in *Wâ‚€* are
    frozen and, thus, not included in optimization**. Instead, the ((d * r) + (r *
    k)) weights making up matrices *B* and *A* are the only ones that are trained.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ä¸èµ·æœ‰è¿™ä¹ˆå¤šæ•°å­¦å…¬å¼ï¼Œä½†**å…³é”®ç‚¹æ˜¯ *Wâ‚€* ä¸­çš„ (d * k) æƒé‡æ˜¯å†»ç»“çš„ï¼Œå› æ­¤ä¸åŒ…æ‹¬åœ¨ä¼˜åŒ–ä¸­**ã€‚ç›¸åï¼Œæ„æˆçŸ©é˜µ *B* å’Œ *A* çš„ ((d
    * r) + (r * k)) æƒé‡æ˜¯å”¯ä¸€ç»è¿‡è®­ç»ƒçš„ã€‚
- en: Plugging in some made-up numbers for d=100, k=100, and r=2 to get a sense of
    the efficiency gains, the **number of trainable parameters drops from 10,000 to
    400** in that layer. In practice, the authors of the LoRA paper cited a **10,000x
    reduction in parameter checkpoint size** using LoRA fine-tune GPT-3 compared to
    full parameter tuning [6].
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†ä¸€äº›è™šæ„çš„æ•°å­—ä»£å…¥ d=100, k=100 å’Œ r=2 æ¥æ„Ÿå—æ•ˆç‡æå‡ï¼Œ**å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ä» 10,000 é™åˆ° 400**ã€‚å®é™…ä¸Šï¼ŒLoRA
    è®ºæ–‡çš„ä½œè€…æåˆ°ï¼Œä½¿ç”¨ LoRA å¾®è°ƒ GPT-3 ç›¸æ¯”å®Œå…¨å‚æ•°è°ƒæ•´ï¼Œ**å‚æ•°æ£€æŸ¥ç‚¹å¤§å°å‡å°‘äº† 10,000 å€** [6]ã€‚
- en: To make this more concrete, letâ€™s see how we can use LoRA to fine-tune a language
    model efficiently enough to run on a personal computer.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿è¿™æ›´åŠ å…·ä½“ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ LoRA é«˜æ•ˆåœ°å¾®è°ƒè¯­è¨€æ¨¡å‹ï¼Œä»¥ä¾¿åœ¨ä¸ªäººè®¡ç®—æœºä¸Šè¿è¡Œã€‚
- en: '**Example Code: Fine-tuning an LLM using LoRA**'
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç¤ºä¾‹ä»£ç ï¼šä½¿ç”¨ LoRA å¾®è°ƒ LLM**'
- en: In this example, we will use the Hugging Face ecosystem to fine-tune a language
    model to classify text as â€˜positiveâ€™ or â€˜negativeâ€™. Here, we fine-tune [*distilbert-base-uncased*](https://huggingface.co/distilbert-base-uncased),
    a ~70M parameter model based on [BERT](https://arxiv.org/pdf/1810.04805.pdf).
    Since this base model was trained to do language modeling and not classification,
    we employ **transfer learning** to replace the base model head with a classification
    head. Additionally, we use **LoRA** to fine-tune the model efficiently enough
    that it can run on my Mac Mini (M1 chip with 16GB memory) in a reasonable amount
    of time (~20 min).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Hugging Face ç”Ÿæ€ç³»ç»Ÿæ¥å¾®è°ƒè¯­è¨€æ¨¡å‹ï¼Œä»¥å°†æ–‡æœ¬åˆ†ç±»ä¸ºâ€œæ­£é¢â€æˆ–â€œè´Ÿé¢â€ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¾®è°ƒ [*distilbert-base-uncased*](https://huggingface.co/distilbert-base-uncased)ï¼Œä¸€ä¸ªåŸºäº
    [BERT](https://arxiv.org/pdf/1810.04805.pdf) çš„çº¦ 70M å‚æ•°æ¨¡å‹ã€‚ç”±äºè¿™ä¸ªåŸºç¡€æ¨¡å‹æ˜¯ä¸ºäº†è¯­è¨€å»ºæ¨¡è€Œä¸æ˜¯åˆ†ç±»è®­ç»ƒçš„ï¼Œæˆ‘ä»¬é‡‡ç”¨
    **è¿ç§»å­¦ä¹ ** å°†åŸºç¡€æ¨¡å‹å¤´éƒ¨æ›¿æ¢ä¸ºåˆ†ç±»å¤´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨ **LoRA** é«˜æ•ˆåœ°å¾®è°ƒæ¨¡å‹ï¼Œä½¿å…¶å¯ä»¥åœ¨æˆ‘çš„ Mac Miniï¼ˆM1 èŠ¯ç‰‡ï¼Œ16GB å†…å­˜ï¼‰ä¸Šåœ¨åˆç†æ—¶é—´å†…ï¼ˆçº¦
    20 åˆ†é’Ÿï¼‰è¿è¡Œã€‚
- en: The code, along with the conda environment files, are available on the [GitHub
    repository](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/fine-tuning).
    The [final model](https://huggingface.co/shawhin/distilbert-base-uncased-lora-text-classification)
    and [dataset](https://huggingface.co/datasets/shawhin/imdb-truncated) [7] are
    available on Hugging Face.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç ä»¥åŠ conda ç¯å¢ƒæ–‡ä»¶å¯ä»¥åœ¨ [GitHub ä»“åº“](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/fine-tuning)
    ä¸­æ‰¾åˆ°ã€‚æœ€ç»ˆæ¨¡å‹å’Œ [æ•°æ®é›†](https://huggingface.co/datasets/shawhin/imdb-truncated) [7] å¯åœ¨
    Hugging Face ä¸Šè·å¾—ã€‚
- en: '[](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/fine-tuning?source=post_page-----23473d763b91--------------------------------)
    [## YouTube-Blog/LLMs/fine-tuning at main Â· ShawhinT/YouTube-Blog'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/fine-tuning?source=post_page-----23473d763b91--------------------------------)
    [## YouTube-Blog/LLMs/fine-tuning at main Â· ShawhinT/YouTube-Blog'
- en: Codes to complement YouTube videos and blog posts on Medium. - YouTube-Blog/LLMs/fine-tuning
    at main Â·â€¦
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä»£ç ä»¥è¡¥å…… YouTube è§†é¢‘å’Œ Medium ä¸Šçš„åšå®¢æ–‡ç« ã€‚ - YouTube-Blog/LLMs/fine-tuning at main Â·â€¦
- en: github.com](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/fine-tuning?source=post_page-----23473d763b91--------------------------------)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/fine-tuning?source=post_page-----23473d763b91--------------------------------)
- en: Imports
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¼å…¥
- en: We start by importing helpful libraries and modules. [Datasets](https://huggingface.co/docs/datasets/index),
    [transformers](https://huggingface.co/docs/transformers/index), [peft](https://huggingface.co/docs/peft/index),
    and [evaluate](https://huggingface.co/docs/evaluate/index) are all libraries from
    [Hugging Face](https://huggingface.co/) (HF).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»å¯¼å…¥æœ‰ç”¨çš„åº“å’Œæ¨¡å—å¼€å§‹ã€‚[Datasets](https://huggingface.co/docs/datasets/index)ã€[transformers](https://huggingface.co/docs/transformers/index)ã€[peft](https://huggingface.co/docs/peft/index)
    å’Œ [evaluate](https://huggingface.co/docs/evaluate/index) éƒ½æ˜¯æ¥è‡ª[Hugging Face](https://huggingface.co/)
    (HF) çš„åº“ã€‚
- en: '[PRE1]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Base model
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŸºç¡€æ¨¡å‹
- en: Next, we load in our base model. The base model here is a relatively small one,
    but there are several other (larger) ones that we could have used (e.g. roberta-base,
    llama2, gpt2). A full list is available [here](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSequenceClassification).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åŠ è½½åŸºç¡€æ¨¡å‹ã€‚è¿™é‡Œçš„åŸºç¡€æ¨¡å‹ç›¸å¯¹è¾ƒå°ï¼Œä½†æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨å…¶ä»–å‡ ä¸ªï¼ˆæ›´å¤§çš„ï¼‰æ¨¡å‹ï¼ˆä¾‹å¦‚ roberta-baseã€llama2ã€gpt2ï¼‰ã€‚å®Œæ•´åˆ—è¡¨è¯·å‚è§[è¿™é‡Œ](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSequenceClassification)ã€‚
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Load data
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½æ•°æ®
- en: We can then load our [training and validation data](https://huggingface.co/datasets/shawhin/imdb-truncated)
    from HFâ€™s datasets library. This is a dataset of 2000 movie reviews (1000 for
    training and 1000 for validation) with binary labels indicating whether the review
    is positive (or not).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä» HF çš„æ•°æ®é›†åº“ä¸­åŠ è½½æˆ‘ä»¬çš„[è®­ç»ƒå’ŒéªŒè¯æ•°æ®](https://huggingface.co/datasets/shawhin/imdb-truncated)ã€‚è¿™æ˜¯ä¸€ä¸ªåŒ…å«2000æ¡ç”µå½±è¯„è®ºï¼ˆ1000æ¡ç”¨äºè®­ç»ƒï¼Œ1000æ¡ç”¨äºéªŒè¯ï¼‰çš„æ•°æ®é›†ï¼Œå…¶ä¸­çš„äºŒå…ƒæ ‡ç­¾è¡¨ç¤ºè¯„è®ºæ˜¯ç§¯æçš„ï¼ˆè¿˜æ˜¯æ¶ˆæçš„ï¼‰ã€‚
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Preprocess data
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†æ•°æ®
- en: Next, we need to preprocess our data so that it can be used for training. This
    consists of using a tokenizer to convert the text into an integer representation
    understood by the base model.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦é¢„å¤„ç†æ•°æ®ï¼Œä»¥ä¾¿ç”¨äºè®­ç»ƒã€‚è¿™åŒ…æ‹¬ä½¿ç”¨åˆ†è¯å™¨å°†æ–‡æœ¬è½¬æ¢ä¸ºåŸºç¡€æ¨¡å‹å¯ä»¥ç†è§£çš„æ•´æ•°è¡¨ç¤ºã€‚
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To apply the tokenizer to the dataset, we use the .*map()* method. This takes
    in a custom function that specifies how the text should be preprocessed. In this
    case, that function is called *tokenize_function()*. In addition to translating
    text to integers, this function truncates integer sequences such that they are
    no longer than 512 numbers to conform to the base modelâ€™s max input length.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å°†åˆ†è¯å™¨åº”ç”¨äºæ•°æ®é›†ï¼Œæˆ‘ä»¬ä½¿ç”¨.*map()*æ–¹æ³•ã€‚è¿™éœ€è¦ä¸€ä¸ªè‡ªå®šä¹‰å‡½æ•°æ¥æŒ‡å®šæ–‡æœ¬åº”å¦‚ä½•é¢„å¤„ç†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯¥å‡½æ•°ç§°ä¸º*tokenize_function()*ã€‚é™¤äº†å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•´æ•°ä¹‹å¤–ï¼Œæ­¤å‡½æ•°è¿˜ä¼šæˆªæ–­æ•´æ•°åºåˆ—ï¼Œä½¿å…¶ä¸è¶…è¿‡512ä¸ªæ•°å­—ï¼Œä»¥ç¬¦åˆåŸºç¡€æ¨¡å‹çš„æœ€å¤§è¾“å…¥é•¿åº¦ã€‚
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: At this point, we can also create a data collator, which will dynamically pad
    examples in each batch during training such that they all have the same length.
    This is computationally more efficient than padding all examples to be equal in
    length across the entire dataset.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ—¶ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥åˆ›å»ºä¸€ä¸ªæ•°æ®æ•´ç†å™¨ï¼Œå®ƒå°†åœ¨è®­ç»ƒæœŸé—´åŠ¨æ€å¡«å……æ¯ä¸ªæ‰¹æ¬¡ä¸­çš„ç¤ºä¾‹ï¼Œä»¥ä½¿å®ƒä»¬éƒ½å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚è¿™æ¯”å°†æ‰€æœ‰ç¤ºä¾‹å¡«å……åˆ°æ•´ä¸ªæ•°æ®é›†ä¸­çš„ç›¸åŒé•¿åº¦æ›´å…·è®¡ç®—æ•ˆç‡ã€‚
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Evaluation metrics
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°æŒ‡æ ‡
- en: We can define how we want to evaluate our fine-tuned model via a custom function.
    Here, we define the *compute_metrics()* function to compute the modelâ€™s accuracy.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡è‡ªå®šä¹‰å‡½æ•°å®šä¹‰å¦‚ä½•è¯„ä¼°æˆ‘ä»¬å¾®è°ƒåçš„æ¨¡å‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å®šä¹‰äº†*compute_metrics()*å‡½æ•°æ¥è®¡ç®—æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Untrained model performance
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœªè®­ç»ƒæ¨¡å‹çš„æ€§èƒ½
- en: Before training our model, we can evaluate how the base model with a randomly
    initialized classification head performs on some example inputs.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬å¯ä»¥è¯„ä¼°åŸºç¡€æ¨¡å‹åœ¨éšæœºåˆå§‹åŒ–åˆ†ç±»å¤´ä¸Šçš„ä¸€äº›ç¤ºä¾‹è¾“å…¥çš„è¡¨ç°ã€‚
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As expected, the model performance is equivalent to random guessing. Letâ€™s see
    how we can improve this with fine-tuning.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œæ¨¡å‹çš„æ€§èƒ½ç­‰åŒäºéšæœºçŒœæµ‹ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•é€šè¿‡å¾®è°ƒæ¥æ”¹å–„è¿™ä¸€ç‚¹ã€‚
- en: Fine-tuning with LoRA
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ LoRA å¾®è°ƒ
- en: To use LoRA for fine-tuning, we first need a config file. This sets all the
    parameters for the LoRA algorithm. See comments in the code block for more details.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨ LoRA è¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ä¸€ä¸ªé…ç½®æ–‡ä»¶ã€‚è¿™è®¾ç½®äº† LoRA ç®—æ³•çš„æ‰€æœ‰å‚æ•°ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ä»£ç å—ä¸­çš„æ³¨é‡Šã€‚
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We can then create a new version of our model that can be trained via PEFT.
    Notice that the scale of trainable parameters was reduced by about 100x.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹ç‰ˆæœ¬ï¼Œè¯¥æ¨¡å‹å¯ä»¥é€šè¿‡PEFTè¿›è¡Œè®­ç»ƒã€‚æ³¨æ„åˆ°å¯è®­ç»ƒå‚æ•°çš„è§„æ¨¡å‡å°‘äº†å¤§çº¦100å€ã€‚
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Next, we define hyperparameters for model training.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰æ¨¡å‹è®­ç»ƒçš„è¶…å‚æ•°ã€‚
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Finally, we create a trainer() object and fine-tune the model!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªtrainer()å¯¹è±¡å¹¶å¾®è°ƒæ¨¡å‹ï¼
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The above code will generate the following table of metrics during training.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°ä»£ç å°†åœ¨è®­ç»ƒæœŸé—´ç”Ÿæˆä»¥ä¸‹æŒ‡æ ‡è¡¨ã€‚
- en: '![](../Images/75c8df11198f7db7e4d932cd45924488.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75c8df11198f7db7e4d932cd45924488.png)'
- en: Model training metrics. Image by author.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹è®­ç»ƒæŒ‡æ ‡ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Trained model performance
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¨¡å‹æ€§èƒ½
- en: To see how the model performance has improved, letâ€™s apply it to the same 5
    examples from before.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æŸ¥çœ‹æ¨¡å‹æ€§èƒ½çš„æå‡ï¼Œæˆ‘ä»¬æ¥å°†å…¶åº”ç”¨äºä¹‹å‰çš„ç›¸åŒ5ä¸ªç¤ºä¾‹ã€‚
- en: '[PRE13]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The fine-tuned model improved significantly from its prior random guessing,
    correctly classifying all but one of the examples in the above code. This aligns
    with the ~90% accuracy metric we saw during training.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å¾®è°ƒåçš„æ¨¡å‹ç›¸è¾ƒäºä¹‹å‰çš„éšæœºçŒœæµ‹æœ‰äº†æ˜¾è‘—æ”¹å–„ï¼Œæ­£ç¡®åˆ†ç±»äº†ä¸Šè¿°ä»£ç ä¸­çš„æ‰€æœ‰ç¤ºä¾‹ï¼Œé™¤äº†ä¸€ä¸ªã€‚è¿™ä¸æˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çœ‹åˆ°çš„çº¦90%å‡†ç¡®ç‡æŒ‡æ ‡ç›¸ç¬¦ã€‚
- en: 'Links: [Code Repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/fine-tuning)
    | [Model](https://huggingface.co/shawhin/distilbert-base-uncased-lora-text-classification)
    | [Dataset](https://huggingface.co/datasets/shawhin/imdb-truncated)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 'é“¾æ¥: [ä»£ç åº“](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/fine-tuning)
    | [æ¨¡å‹](https://huggingface.co/shawhin/distilbert-base-uncased-lora-text-classification)
    | [æ•°æ®é›†](https://huggingface.co/datasets/shawhin/imdb-truncated)'
- en: Conclusions
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: While fine-tuning an existing model requires more computational resources and
    technical expertise than using one out-of-the-box, (smaller) fine-tuned models
    can outperform (larger) pre-trained base models for a particular use case, even
    when employing clever prompt engineering strategies. Furthermore, with all the
    open-source LLM resources available, itâ€™s never been easier to fine-tune a model
    for a custom application.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å¾®è°ƒç°æœ‰æ¨¡å‹æ¯”ä½¿ç”¨å¼€ç®±å³ç”¨çš„æ¨¡å‹éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºå’ŒæŠ€æœ¯ä¸“é•¿ï¼Œï¼ˆè¾ƒå°çš„ï¼‰å¾®è°ƒæ¨¡å‹å¯ä»¥åœ¨ç‰¹å®šç”¨ä¾‹ä¸­è¶…è¿‡ï¼ˆè¾ƒå¤§çš„ï¼‰é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œå³ä½¿åœ¨ä½¿ç”¨å·§å¦™çš„æç¤ºå·¥ç¨‹ç­–ç•¥æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ­¤å¤–ï¼Œå€ŸåŠ©æ‰€æœ‰å¯ç”¨çš„å¼€æºLLMèµ„æºï¼Œå¾®è°ƒæ¨¡å‹ä»¥æ»¡è¶³è‡ªå®šä¹‰åº”ç”¨ä»æœªå¦‚æ­¤ç®€å•ã€‚
- en: The next article of this series will go one step beyond model fine-tuning and
    discuss how to train a language model from scratch.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç³»åˆ—çš„ä¸‹ä¸€ç¯‡æ–‡ç« å°†è¶…è¶Šæ¨¡å‹å¾®è°ƒï¼Œè®¨è®ºå¦‚ä½•ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªè¯­è¨€æ¨¡å‹ã€‚
- en: 'ğŸ‘‰ **More on LLMs**: [Introduction](/a-practical-introduction-to-llms-65194dda1148)
    | [OpenAI API](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)
    | [Hugging Face Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    | [Prompt Engineering](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)
    | [Build an LLM](/how-to-build-an-llm-from-scratch-8c477768f1f9) | [QLoRA](/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)
    | [RAG](https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac)
    | [Text Embeddings](/text-embeddings-classification-and-semantic-search-8291746220be)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ‘‰ **æ›´å¤šå…³äºLLMs**: [ä»‹ç»](/a-practical-introduction-to-llms-65194dda1148) | [OpenAI
    API](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)
    | [Hugging Face Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    | [æç¤ºå·¥ç¨‹](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)
    | [æ„å»ºLLM](/how-to-build-an-llm-from-scratch-8c477768f1f9) | [QLoRA](/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)
    | [RAG](https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac)
    | [æ–‡æœ¬åµŒå…¥](/text-embeddings-classification-and-semantic-search-8291746220be)'
- en: '![Shaw Talebi](../Images/02eefb458c6eeff7cd29d40c212e3b22.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![Shaw Talebi](../Images/02eefb458c6eeff7cd29d40c212e3b22.png)'
- en: '[Shaw Talebi](https://shawhin.medium.com/?source=post_page-----23473d763b91--------------------------------)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[Shaw Talebi](https://shawhin.medium.com/?source=post_page-----23473d763b91--------------------------------)'
- en: Large Language Models (LLMs)
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰
- en: '[View list](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c?source=post_page-----23473d763b91--------------------------------)13
    stories![](../Images/82e865594c68f5307e75665842d197bb.png)![](../Images/b9436354721f807e0390b5e301be2119.png)![](../Images/59c8db581de77a908457dec8981f3c37.png)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹åˆ—è¡¨](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c?source=post_page-----23473d763b91--------------------------------)13
    ä¸ªæ•…äº‹![](../Images/82e865594c68f5307e75665842d197bb.png)![](../Images/b9436354721f807e0390b5e301be2119.png)![](../Images/59c8db581de77a908457dec8981f3c37.png)'
- en: Resources
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: èµ„æº
- en: '**Connect**: [My website](https://shawhintalebi.com/) | [Book a call](https://calendly.com/shawhintalebi)
    | [Ask me anything](https://shawhintalebi.com/contact/)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**è”ç³»**: [æˆ‘çš„ç½‘ç«™](https://shawhintalebi.com/) | [é¢„çº¦é€šè¯](https://calendly.com/shawhintalebi)
    | [é—®æˆ‘ä»»ä½•é—®é¢˜](https://shawhintalebi.com/contact/)'
- en: '**Socials**: [YouTube ğŸ¥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)
    | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¤¾äº¤åª’ä½“**: [YouTube ğŸ¥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)
    | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)'
- en: '**Support**: [Buy me a coffee](https://www.buymeacoffee.com/shawhint) â˜•ï¸'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ”¯æŒ**: [è¯·æˆ‘å–å’–å•¡](https://www.buymeacoffee.com/shawhint) â˜•ï¸'
- en: '[](https://shawhin.medium.com/subscribe?source=post_page-----23473d763b91--------------------------------)
    [## Get FREE access to every new story I write'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shawhin.medium.com/subscribe?source=post_page-----23473d763b91--------------------------------)
    [## å…è´¹è·å–æˆ‘å†™çš„æ¯ä¸ªæ–°æ•…äº‹'
- en: Get FREE access to every new story I write P.S. I do not share your email with
    anyone By signing up, you will create aâ€¦
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å…è´¹è·å–æˆ‘å†™çš„æ¯ä¸ªæ–°æ•…äº‹ P.S. æˆ‘ä¸ä¼šå°†æ‚¨çš„é‚®ä»¶åˆ†äº«ç»™ä»»ä½•äºº æ³¨å†Œå³åˆ›å»ºä¸€ä¸ªâ€¦
- en: shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----23473d763b91--------------------------------)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----23473d763b91--------------------------------)
- en: '[1] Deeplearning.ai Finetuning Large Langauge Models Short Course: [https://www.deeplearning.ai/short-courses/finetuning-large-language-models/](https://www.deeplearning.ai/short-courses/finetuning-large-language-models/)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Deeplearning.ai å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒçŸ­è¯¾ç¨‹: [https://www.deeplearning.ai/short-courses/finetuning-large-language-models/](https://www.deeplearning.ai/short-courses/finetuning-large-language-models/)'
- en: '[2] [arXiv:2005.14165](https://arxiv.org/abs/2005.14165) **[cs.CL] (**GPT-3
    Paper)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [arXiv:2005.14165](https://arxiv.org/abs/2005.14165) **[cs.CL] (**GPT-3è®ºæ–‡)'
- en: '[3] [arXiv:2303.18223](https://arxiv.org/abs/2303.18223) **[cs.CL] (**Survey
    of LLMs)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [arXiv:2303.18223](https://arxiv.org/abs/2303.18223) **[cs.CL] (**LLMsç»¼è¿°)'
- en: '[4] [arXiv:2203.02155](https://arxiv.org/abs/2203.02155) **[cs.CL] (**InstructGPT
    paper)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [arXiv:2203.02155](https://arxiv.org/abs/2203.02155) **[cs.CL] (**InstructGPTè®ºæ–‡)'
- en: '[5] ğŸ¤— PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource
    Hardware: [https://huggingface.co/blog/peft](https://huggingface.co/blog/peft)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] ğŸ¤— PEFT: åœ¨ä½èµ„æºç¡¬ä»¶ä¸Šå¯¹äº¿çº§è§„æ¨¡æ¨¡å‹è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ: [https://huggingface.co/blog/peft](https://huggingface.co/blog/peft)'
- en: '[6] [arXiv:2106.09685](https://arxiv.org/abs/2106.09685) **[cs.CL]** (LoRA
    paper)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [arXiv:2106.09685](https://arxiv.org/abs/2106.09685) **[cs.CL]** (LoRAè®ºæ–‡)'
- en: '[7] Original dataset source â€” Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
    Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011\. [Learning Word Vectors
    for Sentiment Analysis](https://aclanthology.org/P11-1015). In *Proceedings of
    the 49th Annual Meeting of the Association for Computational Linguistics: Human
    Language Technologies*, pages 142â€“150, Portland, Oregon, USA. Association for
    Computational Linguistics.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] åŸå§‹æ•°æ®é›†æ¥æº â€” Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew
    Y. Ng, å’Œ Christopher Potts. 2011\. [å­¦ä¹ è¯å‘é‡è¿›è¡Œæƒ…æ„Ÿåˆ†æ](https://aclanthology.org/P11-1015)ã€‚åœ¨*ç¬¬49å±Šè®¡ç®—è¯­è¨€å­¦åä¼šå¹´ä¼šï¼šäººç±»è¯­è¨€æŠ€æœ¯ä¼šè®®è®ºæ–‡é›†*ï¼Œç¬¬142â€“150é¡µï¼Œç¾å›½ä¿„å‹’å†ˆå·æ³¢ç‰¹å…°å¸‚ã€‚è®¡ç®—è¯­è¨€å­¦åä¼šã€‚'
