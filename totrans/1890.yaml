- en: 'Machines That Learn Like Us: Solving the Generalization-Memorization Dilemma'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/solving-machine-learnings-generalization-memorization-dilemma-3-promising-paradigms-ab9c236add3e](https://towardsdatascience.com/solving-machine-learnings-generalization-memorization-dilemma-3-promising-paradigms-ab9c236add3e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 3 paradigms to solve one of the most important problems in Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samuel.flender?source=post_page-----ab9c236add3e--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----ab9c236add3e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ab9c236add3e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ab9c236add3e--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----ab9c236add3e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ab9c236add3e--------------------------------)
    ·7 min read·Apr 5, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d07a01c4830a1e5c49f4bbf135d147d5.png)'
  prefs: []
  type: TYPE_IMG
- en: (Midjourney)
  prefs: []
  type: TYPE_NORMAL
- en: The “holy grail” of Machine Learning is the ability to build systems that can
    both memorize known patterns in the training data as well as generalize to unknown
    patterns in the wild.
  prefs: []
  type: TYPE_NORMAL
- en: It’s the holy grail because this is how we humans learn as well. You can recognize
    your grandma in an old photo, but you could also recognize a Xoloitzcuintli as
    a dog even though you’ve never actually seen one before. Without memorization
    we’d have to constantly re-learn everything from scratch, and without generalization
    we wouldn’t be able to adapt to an ever-changing world. To survive, we need both.
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional statistical learning theory tells us that this is impossible: models
    can either generalize well or memorize well, but not both. It’s the well-known
    bias-variance trade-off, one of the first things we learn in standard ML curricula.'
  prefs: []
  type: TYPE_NORMAL
- en: How then can we build such universal learning systems? Is the holy grail within
    reach?
  prefs: []
  type: TYPE_NORMAL
- en: In this post, let’s dive into 3 paradigms from the literature,
  prefs: []
  type: TYPE_NORMAL
- en: Generalize first, memorize later
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generalize and memorize at the same time
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generalize with machines, memorize with humans
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: 1 — Generalize first, memorize later
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[BERT](/what-exactly-happens-when-we-fine-tune-bert-f5dc32885d76) revolutionized
    Machine Leaning with its introduction of the pre-training/fine-tuning paradigm:
    after pre-training in an unsupervised way on a massive amount of text data, the
    model can be rapidly fine-tuned on a specific downstream task with relatively
    fewer labels.'
  prefs: []
  type: TYPE_NORMAL
- en: Surprisingly, this pre-training/fine-tuning approach turns out to solve the
    generalization/memorization problem. BERT can generalize as well as memorize,
    show Michael Tänzer and collaborators from the Imperial College London in a 2022
    [paper](https://aclanthology.org/2022.acl-long.521/).
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, the authors show that during fine-tuning, BERT learns in 3 distinct
    phases:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fitting (epoch 1): the model learns simple, generic pattens that explain as
    much of the training data as possible. During this phase, both training and validation
    performance increases.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Setting (epochs 2–5): there are no more simple patterns left to learn. Both
    training and validation performance saturate, forming a plateau in the learning
    curve.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Memorization (epochs 6+): the model starts to memorize specific examples in
    the training set, including noise, which improves training performance but degrades
    validation performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'How did they figure this out? By starting with a noise-free training set (CoNLL03,
    a named-entity-recognition benchmark dataset), and then gradually introducing
    more and more artificial label noise. Comparing the learning curves with different
    amounts of noise clearly reveals the 3 distinct phases: more noise results in
    a steeper drop during phase 3.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4f7edec8029dad338c529bd40303703.png)'
  prefs: []
  type: TYPE_IMG
- en: Fine-tuning BERT exhibits 3 distinct learning phases. Figure taken from Tänzer
    et al, “Memorisation versus Generalisation in Pre-trained Language Models” ([link](https://aclanthology.org/2022.acl-long.521/))
  prefs: []
  type: TYPE_NORMAL
- en: 'Tänzer et al also show that memorization in BERT requires repetition: BERT
    memorizes a specific training example only once it has seen that example a certain
    number of times. This can be deduced from the learning curve for the artificially
    introduced noise: it’s a step function, which improves with each epoch. In other
    words, during phase 3 BERT can eventually memorize the entire training set, if
    we just let it train for a sufficient number of epochs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT, in conclusion, appears to generalize first and memorize later, as evidenced
    by the observation of its 3 distinct learning phases during fine-tuning. In fact,
    it can also be shown that this behavior is a direct consequence of pre-training:
    Tänzer et al show that a randomly initialized BERT model does not share the same
    3 learning phases. This leads to the conclusion that the pre-training/fine-tuning
    paradigm may be a possible solution to the generalization/memorization dilemma.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 — Generalize and memorize at the same time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s leave the world of natural language processing and enter the world of
    [recommender systems](https://medium.com/towards-data-science/biases-in-recommender-systems-top-challenges-and-recent-breakthroughs-edcda59d30bf).
  prefs: []
  type: TYPE_NORMAL
- en: In modern recommender systems, the ability to memorize and generalize at the
    same time is critical. YouTube, for example, wants to show you videos that are
    similar to the ones you’ve watched in the past (memorization), but also new ones
    that are little bit different and you didn’t even knew you’d like (generalization).
    Without memorization you’d get frustrated, and without generalization you’d get
    bored.
  prefs: []
  type: TYPE_NORMAL
- en: The best recommender systems today need to do both. But how?
  prefs: []
  type: TYPE_NORMAL
- en: In a 2016 [paper](https://arxiv.org/abs/1606.07792), Heng-Tze Cheng and collaborators
    from Google propose what they call “Wide and Deep Learning” to address this problem.
    The key idea is to build a single neural network that has both a deep component
    (a deep neural net with embedding inputs) for generalization as well as a wide
    component (a linear model with a large number of sparse inputs) for memorization.
    The authors demonstrate the effectiveness of this approach on recommendations
    within the Google Play store, which recommends apps to users.
  prefs: []
  type: TYPE_NORMAL
- en: The inputs to the deep component are dense features as well as embeddings of
    categorical features such as user language, user gender, impressed app, installed
    apps, and so on. These embeddings are initialized randomly, and then tuned during
    model training along with the other parameters in the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: The inputs to the wide component of the network are granular cross-features
    such as
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: the value of which is 1 if the user has Netflix installed and the impressed
    app is Hulu.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s easy to see why the wide component enables a form of memorization: if
    99% of users that install Netflix also end up installing Hulu, the wide component
    will be able to learn this piece of information, while it may get lost in the
    deep component. Having both the wide and deep components really is the key to
    peak performance, argue the Cheng et al.'
  prefs: []
  type: TYPE_NORMAL
- en: And in fact, the experimental results confirm the authors’ hypothesis. The wide
    & deep model outperformed both a wide-only model (by 2.9%) and a deep-only model
    (by 1%) in terms of online acquisition gain in the Google Play store. These experimental
    results indicate that “Wide & Deep” is another promising paradigm to solve the
    generalization/memorization dilemma.
  prefs: []
  type: TYPE_NORMAL
- en: 3 — Generalize with machines, memorize with humans
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Both Tänzer and Cheng proposed approaches to solve the generalization/memorization
    dilemma with machines alone. However, machines have a hard time memorizing singular
    examples: Tänzer et al find that BERT requires at least 25 instances of a class
    to learn to predict it at all and 100 examples to predict it “with some accuracy”.'
  prefs: []
  type: TYPE_NORMAL
- en: Taking a step back, we don’t have to let machines do all of the work. Instead
    of fighting our machines’ failure to memorize, why not embrace it? Why not build
    a hybrid system that combines ML with human expertise?
  prefs: []
  type: TYPE_NORMAL
- en: That’s precisely the idea behind Chimera, Walmart’s production system for large-scale
    e-commerce item classification, presented in a 2014 [paper](https://pages.cs.wisc.edu/~anhai/papers/chimera-vldb14.pdf)
    by Chong Sun and collaborators from Walmart Labs. The premise behind Chimera is
    that Machine Learning alone is not the enough to handle item classification at
    scale due to the existence of a large number of edge cases with little training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, Walmart may agree to carry a limited number of new products from
    a new vendor on a trial basis. An ML system may not be able to accurately classify
    these products because there’s not enough training data. However, human analysts
    can write rules to cover these cases precisely. These rule-based decisions can
    then later be used in the model training, so that after some time the model can
    catch up to the new patterns.
  prefs: []
  type: TYPE_NORMAL
- en: The authors conclude;
  prefs: []
  type: TYPE_NORMAL
- en: We use both machine learning and hand-crafted rules extensively. Rules in our
    system are not “nice to have”. They are absolutely essential to achieving the
    desired performance, and they give domain analysts a fast and effective way to
    provide feedback into the system. As far as we know, we are the first to describe
    an industrial-strength system where both [Machine] learning and rules co-exist
    as first-class citizens.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Alas, this co-existence may also be the key to solving the generalization/memorization
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Coda: systems that learn like us'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s recap. Building systems that can both memorize known patterns and generalize
    to unknown ones is the holy grail of Machine Learning. As of today, no one has
    yet cracked this problem completely, but we’ve seen a few promising directions:'
  prefs: []
  type: TYPE_NORMAL
- en: BERT has been shown to generalize first and memorize later during fine-tuning,
    a capability that’s possible due to being pre-trained beforehand.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wide & Deep neural networks have been designed to both generalize (using the
    deep component) and memorize (using the wide component) at the same time, outperforming
    both wide-only and deep-only networks in Google Play store recommendations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Walmart’s hybrid production system Chimera leverages human experts to write
    rules for edge cases that their ML models fail to memorize. By adding these rule-based
    decisions back into the training data, over time the ML models can catch up, but
    ultimately ML and rules co-exist as first-class citizens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And this is really just a small glimpse of what’s out there. Any industrial
    ML team fundamentally needs to tackle some version of the memorization/generalization
    dilemma. If you’re working in ML, chances are you will eventually encounter it
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, solving this problem will not just enable us to build vastly superior
    ML systems. It will enable us build systems that [learn more like us](https://medium.com/towards-data-science/the-origin-of-intelligent-behavior-3d3f2f659dc2).
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samuel.flender/subscribe?source=post_page-----ab9c236add3e--------------------------------)
    [## Don''t want to rely on Medium''s algorithms? Sign up.'
  prefs: []
  type: TYPE_NORMAL
- en: Don't want to rely on Medium's algorithms? Sign up. Make sure you won't miss
    my next article by signing up to my Email…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@samuel.flender/subscribe?source=post_page-----ab9c236add3e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
