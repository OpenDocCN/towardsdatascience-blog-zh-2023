- en: 'Biases in Recommender Systems: Top Challenges and Recent Breakthroughs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/biases-in-recommender-systems-top-challenges-and-recent-breakthroughs-edcda59d30bf](https://towardsdatascience.com/biases-in-recommender-systems-top-challenges-and-recent-breakthroughs-edcda59d30bf)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Behind the ongoing quest for building unbiased models from biased data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samuel.flender?source=post_page-----edcda59d30bf--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----edcda59d30bf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----edcda59d30bf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----edcda59d30bf--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----edcda59d30bf--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----edcda59d30bf--------------------------------)
    ·7 min read·Feb 23, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9efdc9be3f175bee78b1c8774b8b4d96.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by the author with Midjourney
  prefs: []
  type: TYPE_NORMAL
- en: '[Recommender systems](/learning-to-rank-a-primer-40d2ff9960af) have become
    ubiquitous in our daily lives, from online shopping to social media to entertainment
    platforms. These systems use complex algorithms to analyze historic user engagement
    data and make recommendations based on their inferred preferences and behaviors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While these systems can be incredibly useful in helping users discover new
    content or products, they are not without their flaws: recommender systems are
    plagued by various forms of bias that can lead to poor recommendations and therefore
    poor user experience. One of today’s main research threads around recommender
    systems is therefore how to de-bias them.'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we’ll dive into 5 of the most prevalent biases in recommender
    systems, and learn about some of the recent research from Google, YouTube, Netflix,
    Kuaishou, and others.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: 1 — Clickbait bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Wherever there’s an entertainment platform, there’s [clickbait](https://medium.com/mind-cafe/im-boycotting-these-forms-of-youtube-clickbait-8148b0d6363b):
    sensational or misleading headlines or video thumbnails designed to grab a user’s
    attention and entice them to click, without providing any real value. *“You won’t
    believe what happened next!”*'
  prefs: []
  type: TYPE_NORMAL
- en: If we train a ranking model using clicks as positives, naturally that model
    will be biased in favor of clickbait. This is bad, because such a model would
    promote even more clickbait to users, and therefore amplify the damage it does.
  prefs: []
  type: TYPE_NORMAL
- en: One solution for de-biasing ranking models from clickbait, proposed by [Covington
    et al (2016)](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf)
    in the context of YouTube video recommendations, is **weighted logistic regression**,
    where the weights are the watch time for positive training examples (impressions
    with clicks), and unity for the negative training example (impressions without
    clicks).
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, it can be shown that such a weighted logistic regression model
    learns odds that are approximately the expected watch time for a video. At serving
    time, videos are ranked by their predicted odds, resulting in videos with long
    expected watch times to be placed high on top of the recommendations, and clickbait
    (with the lowest expected watch times) at the bottom of it.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, Covington et al don’t share all of their experimental results,
    but they do say that weighted logistic regression performs “much better” than
    predicting clicks directly.
  prefs: []
  type: TYPE_NORMAL
- en: 2 — Duration bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Weighted logistic regression work well for solving the clickbait problem, but
    it introduces a new problem: duration bias. Simply put, longer videos always have
    a tendency to be watched for a longer time, not necessarily because they’re more
    relevant, but simply because they’re longer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Think about a video catalog that contains 10-second short-form videos along
    with 2-hour long-form videos. A watch time of 10 seconds means something completely
    different in the two cases: it’s a strong positive signal in the former, and a
    weak positive (perhaps even a negative) signal in the latter. Yet, the Covington
    approach would not be able to distinguish between these two cases, and would bias
    the model in favor of long-form videos (which generate longer watch times simply
    because they’re longer).'
  prefs: []
  type: TYPE_NORMAL
- en: A solution to duration bias, proposed by [Zhan et al (2022)](https://dl.acm.org/doi/abs/10.1145/3534678.3539092)
    from KuaiShou, is **quantile-based watch-time prediction**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key idea is to bucket all videos into duration quantiles, and then bucket
    all watch times within a duration bucket into quantiles as well. For example,
    with 10 quantiles, such an assignment could look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: By translating all time intervals into quantiles, the model understands that
    10s is “high” in the latter example, but “low” in the former, so the author’s
    hypothesis. At training time, we’re providing the model with the video quantile,
    and task it with predicting the watch quantile. At inference time, we’re simply
    ranking all videos by their predicted watch time, which will now be de-confounded
    from the video duration itself.
  prefs: []
  type: TYPE_NORMAL
- en: And indeed, this approach appears to work. Using A/B testing, the authors report
  prefs: []
  type: TYPE_NORMAL
- en: 0.5% improvements in total watch time compared weighted logistic regression
    (the idea from Covington et al), and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0.75% improvements in total watch time compared to predicting watch time directly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results show that removing duration bias can be a powerful approach on platforms
    that serve both long-form and short-form videos. Perhaps counter-intuitively,
    removing bias in favor of long videos in fact improves overall user user watch
    times.
  prefs: []
  type: TYPE_NORMAL
- en: 3 — Position bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Position bias means that the highest-ranked items are the ones which create
    the most engagement not because they’re actually the best content for the user,
    but instead simply because they’re ranked highest, and users start to blindly
    trust the ranking they’re being shown. The model predictions become a self-fulfilling
    prophecy, but this is not what we really want. We want to predict what users want,
    and not make them want what we predict.
  prefs: []
  type: TYPE_NORMAL
- en: Position bias can be mitigated by techniques such as rank randomization, intervention
    harvesting, or using the ranks themselves as features, which I covered in my other
    post [here](/machine-learning-does-not-only-predict-the-future-it-actively-creates-it-1615895c80a9).
  prefs: []
  type: TYPE_NORMAL
- en: Particularly problematic is that position bias will always make our models look
    better on paper than they actually are. Our models may be slowly degrading in
    quality, but we wouldn’t know what is happening until it’s too late (and users
    have churned away). It is therefore important, when working with recommender systems,
    to monitor multiple quality metrics about the system, including metrics that quantify
    user retention and the diversity of recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: 4 — Popularity bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Popularity bias refers to the tendency of the model to give higher rankings
    to items that are more popular overall (due to the fact that they’ve been rated
    by more users), rather than being based on their actual quality or relevance for
    a particular user. This can lead to a distorted ranking, where less popular or
    niche items that could be a better fit for the user’s preferences are not given
    adequate consideration.
  prefs: []
  type: TYPE_NORMAL
- en: '[Yi et al (2019)](https://research.google/pubs/pub48840/) from Google propose
    a simple but effective algorithmic tweak to de-bias a video recommendation model
    from popularity bias. During model training, they replace the logits in their
    logistic regression layer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: logit(u,v) is the logit function (i.e., the log-odds) for user u engaging with
    video v, and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: log(P(v)) is the log-frequency of video v.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Of course, the right hand side is equivalent to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In other words, they simply normalize the predicted odds for a user/video pair
    by the video probability. Extremely high odds from popular videos count as much
    as moderately high odds from not-so-popular videos. And that’s the entire magic.
  prefs: []
  type: TYPE_NORMAL
- en: 'And indeed, the magic appears to work: in online A/B tests, the authors find
    a 0.37% improvement in overall user engagements with the de-biased ranking model.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 — Single-interest bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose you watch mostly drama movies, but sometimes you like to watch a comedy,
    and from time to time a documentary. You have multiple interests, yet a ranking
    model trained to maximize your watch time may over-emphasize drama movies because
    that’s what you’re most likely to engage with. This is **single-interest bias**,
    the failure of a model to understand that users inherently have multiple interests
    and preferences.
  prefs: []
  type: TYPE_NORMAL
- en: In order to remove single-interest bias, a ranking model needs to be calibrated.
    Calibration simply means that, if you watch drama movies 80% of the time, then
    the model’s top 100 recommendations should in fact include around 80 drama movies
    (and not 100).
  prefs: []
  type: TYPE_NORMAL
- en: Netflix’s [Harald Steck](https://dl.acm.org/doi/10.1145/3240323.3240372) (2018)
    demonstrates the benefits of model calibration with a simple post-processing technique
    called Platt scaling. He presents experimental results that demonstrate the effectiveness
    of the method in improving the calibration of Netflix recommendations, which he
    quantifies with KL divergence scores. The resulting movie recommendations are
    more diverse — in fact, as diverse as the actual user preferences — and result
    in improved overall watch times.
  prefs: []
  type: TYPE_NORMAL
- en: Final thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recap for memory:'
  prefs: []
  type: TYPE_NORMAL
- en: clickbait bias means that the model is biased in favor of clickbait content
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: duration bias means that the model is biased in favor of long videos (and against
    short videos)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: position bias means that the model is biased in favor of its own predictions
    instead of what users really want
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: popularity bias means that the model is biased in favor of popular content instead
    of the unique interests of a particular user
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: single-interest bias means that the model fails to learn multiple user interests
    at the same time
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The list of biases is long — we’ve only scratched the surface here — and it
    is also constantly evolving. In some cases, solving for one bias may even introduce
    a new bias, as we’ve seen with clickbait and duration bias.
  prefs: []
  type: TYPE_NORMAL
- en: 'Coming up with innovative ways to quantify and alleviate these biases therefore
    remains one of the most important tasks for today’s ranking engineers. It’s not
    enough to simply assume that ranking models are neutral or objective: they’ll
    always reflect the biases that exist in the data they are trained on.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samuel.flender/subscribe?source=post_page-----edcda59d30bf--------------------------------)
    [## Don''t want to rely on Medium''s algorithms? Sign up.'
  prefs: []
  type: TYPE_NORMAL
- en: Don't want to rely on Medium's algorithms? Sign up. Make sure you won't miss
    my next article by signing up to my Email…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@samuel.flender/subscribe?source=post_page-----edcda59d30bf--------------------------------)
  prefs: []
  type: TYPE_NORMAL
