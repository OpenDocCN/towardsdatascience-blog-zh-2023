- en: Integrate Distributed Ray Serve Deployment with Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/integrate-distributed-ray-serve-deployment-with-kafka-181403f4e194](https://towardsdatascience.com/integrate-distributed-ray-serve-deployment-with-kafka-181403f4e194)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn how to simply combine Ray Serve Deployment with asynchronous Kafka Consumer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@slanjr?source=post_page-----181403f4e194--------------------------------)[![Rostyslav
    Neskorozhenyi](../Images/7d8616b6a3c60bcc29fde5d0a72f13c9.png)](https://medium.com/@slanjr?source=post_page-----181403f4e194--------------------------------)[](https://towardsdatascience.com/?source=post_page-----181403f4e194--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----181403f4e194--------------------------------)
    [Rostyslav Neskorozhenyi](https://medium.com/@slanjr?source=post_page-----181403f4e194--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----181403f4e194--------------------------------)
    ·5 min read·Jul 16, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/86f8c4b46bf6189e1dc7d0edbd8945ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Image is generated by [Midjourney](https://www.midjourney.com/)
  prefs: []
  type: TYPE_NORMAL
- en: '[Ray](https://github.com/ray-project/ray) is a modern open source framework
    that allows you to create distributed applications in Python with ease. You can
    create simple training pipelines, do hyperparameter tuning, data processing and
    model serving.'
  prefs: []
  type: TYPE_NORMAL
- en: Ray allows you to create online inference APIs with [Ray Serve](https://docs.ray.io/en/latest/serve/index.html).
    You can easily combine several ML models and custom business logic in one application.
    Ray Serve automatically creates an HTTP interface for your deployments, taking
    care of fault tolerance and replication.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/93c068fcae1fe686691387d9f713af16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Ray ecosystem. Source: [https://docs.ray.io/en/latest/ray-air/getting-started.html](https://docs.ray.io/en/latest/ray-air/getting-started.html)
    ([APACHE 2.0](https://github.com/ray-project/ray/blob/releases/2.6.1/LICENSE)
    licence )'
  prefs: []
  type: TYPE_NORMAL
- en: But there is one thing that **Ray Serve** misses for now. Many modern distributed
    applications communicate through [Kafka](https://kafka.apache.org/), but there
    is no out-of-the-box way to connect Ray Serve service to Kafka topic.
  prefs: []
  type: TYPE_NORMAL
- en: But don’t panic. It will not take too much effort to teach Ray Serve to communicate
    with Kafka. So, let’s begin.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all we will need to prepare our local environment. We will use a docker-compose
    file with Kafka and [Kafdrop UI](https://github.com/obsidiandynamics/kafdrop)
    docker containers to start and explore our local Kafka instance (so we assume
    that you have [Docker](https://docs.docker.com/engine/install/ubuntu/) and [Docker
    Compose](https://docs.docker.com/compose/install/linux/#install-using-the-repository)
    installed). Also we will need to install some Python requirements to get the work
    done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Ray](https://github.com/ray-project/ray) itself'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[aiokafka](https://github.com/aio-libs/aiokafka)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All requirements can be downloaded by this [**link**](https://github.com/slanj/ray-serve-kafka/blob/main/requirements.txt).
  prefs: []
  type: TYPE_NORMAL
- en: Now we will create a [*ray-consumer.py*](https://github.com/slanj/ray-serve-kafka/blob/main/ray-consumer.py)
    file with [**Ray Deployment**](https://docs.ray.io/en/latest/serve/key-concepts.html#deployment)
    that will be served with Ray Serve. I will not go into details about Ray Serve
    concepts, as you can read about that in the [documentation](https://docs.ray.io/en/latest/serve/getting_started.html).
    Basically it takes the usual Python class and converts it to a asynchronous service
    [Ray Deployment](https://docs.ray.io/en/latest/serve/key-concepts.html#deployment)
    with **@serve.deployment** decorator**:**
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore this code a little. Here we can see a *serve.deployment* decorator
    with several parameters. These parameters will be used by Ray Serve to run our
    deployment. In our case Ray Serve will run one deployment replica on a local Ray
    cluster (cluster also may exist on [*AWS or Kubernetes*](https://docs.ray.io/en/latest/cluster/getting-started.html)*)*.
    You can start several deployment replicas with **num_replicas** parameter, Ray
    Serve will split traffic between these replicas.
  prefs: []
  type: TYPE_NORMAL
- en: Also we specify the amount of resources which each replica will consume with
    **num_cpus** and **num_gpus** parameters. Such parameters are useful for example
    when you have a heavy deployment and it needs a specific number of CPU cores to
    run. In our case we will not need much, so we will use no GPU, and only 0.1 CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Each time when deployment starts it will get the running event loop (read about
    asyncio event loops [here](https://superfastpython.com/asyncio-event-loop/)),
    initialize [AIOKafkaConsumer](https://aiokafka.readthedocs.io/en/stable/consumer.html),
    connect to Kafka(with *topics*, *bootstrap_servers* and *group_id* parameters)
    and start *consume* task for that consumer in the current event loop. Also it
    will set deployment health status to True.
  prefs: []
  type: TYPE_NORMAL
- en: By default, the Serve controller periodically [health-checks](https://docs.ray.io/en/latest/serve/production-guide/fault-tolerance.html)
    each Serve deployment replica and restarts it on failure. We defined a custom
    health-check, by adding a *check_health* method to our *RayConsumer* deployment
    class. Healt-check periods are defined by **health_check_period_s** and **health_check_timeout_s**
    parameters in *serve.deployment* decorator*.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a5171bdd11207330b93d84f57385aea.png)'
  prefs: []
  type: TYPE_IMG
- en: Ray Serve Architecture. Source [https://docs.ray.io/en/latest/serve/architecture.html](https://docs.ray.io/en/latest/serve/architecture.html)
    ([APACHE 2.0](https://github.com/ray-project/ray/blob/releases/2.6.1/LICENSE)
    license)
  prefs: []
  type: TYPE_NORMAL
- en: Our consumer will continue to get messages from Kafka and print a toy ML model
    prediction to the terminal till Ray Cluster is alive. In our example we just use
    a toy model to simulate ML prediction, but we can include any logic or ML model
    inference. We can also use [Ray object store](https://docs.ray.io/en/latest/ray-core/objects.html)
    to store ML models and [speed up load time](https://medium.com/ibm-data-ai/how-to-load-pytorch-models-340-times-faster-with-ray-8be751a6944c).
  prefs: []
  type: TYPE_NORMAL
- en: In a case when a consumer fails due to some random error — our deployment will
    set health status to False, **check_health** method will return an error and Ray
    Serve Controller will automatically restart deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'It goes without saying that before you start consuming messages from Kafka,
    we need to start Kafka and create a topic in which our messages will be. Go to
    the folder where you have placed [docker-compose.yaml](https://github.com/slanj/ray-serve-kafka/blob/main/docker-compose.yaml)
    and run the following command in terminal to start Kafka and Kafdrop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now we can go to [http://localhost:9000](http://localhost:9000) and create a
    new Kafka topic. In our case we name it “ray-topic”
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b550a643616984f7ce5ea368ace98ad9.png)'
  prefs: []
  type: TYPE_IMG
- en: Kafdrop UI. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Everything is ready, so we can start our deployment with the command in terminal
    (you should run this command in a folder that contains [*ray-consumer.py*](https://github.com/slanj/ray-serve-kafka/blob/main/ray-consumer.py)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This command will set up a local Ray Cluster and start Ray Serve Deployment
    on it. We can check your deployments status in Ray Dashboard, which is usually
    accessible by this address [http://localhost:8265](http://localhost:8265) .
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56921ff96966851db92df4604602fa98.png)'
  prefs: []
  type: TYPE_IMG
- en: Ray Dashboard. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can send some messages to Kafka, so our deployment can consume them.
    We can do this with a simple script [aio-producer.py](https://github.com/slanj/ray-serve-kafka/blob/main/aio-producer.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'This script will use [AIOKafkaProducer](https://aiokafka.readthedocs.io/en/stable/producer.html)
    to send a particular message to a particular topic, for example like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If everything is ok, our Ray Consumer Deployment will get all those messages
    from Kafka and just print them to the terminal.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e248114d092042f9417bdf2944ae87ef.png)'
  prefs: []
  type: TYPE_IMG
- en: Ray Deployment output. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Basically, that’s all. Now you have a working fault tolerant Ray Serve Deployment
    that consumes messages from Kafka. Of course it is basic, but you can easily add
    custom business logic, [compose it with other deployments](https://docs.ray.io/en/latest/serve/model_composition.html)
    or even combine it with [Ray Workflows](https://docs.ray.io/en/latest/workflows/index.html).
    You can add additional logic into *check_health* method so your deployment will
    periodically check additional services if needed.
  prefs: []
  type: TYPE_NORMAL
- en: You can add replicas to your deployment with serve.deployment parameter **num_replicas**
    or even [set up autoscaling](https://docs.ray.io/en/latest/serve/scaling-and-resource-allocation.html)
    with **min_replicas** and **max_replicas** parameters, so Ray will automatically
    add replicas to your deployment depending on traffic and load.
  prefs: []
  type: TYPE_NORMAL
- en: Ray provides you all the tools to build scalable and fault tolerant Python applications,
    and now knowing how to combine it with Kafka you are able to build some new really
    powerful distributed apps.
  prefs: []
  type: TYPE_NORMAL
- en: All the code mentioned in this article can be found by [this link](https://github.com/slanj/ray-serve-kafka/).
  prefs: []
  type: TYPE_NORMAL
