- en: Integrate Distributed Ray Serve Deployment with Kafka
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将分布式 Ray Serve 部署与 Kafka 集成
- en: 原文：[https://towardsdatascience.com/integrate-distributed-ray-serve-deployment-with-kafka-181403f4e194](https://towardsdatascience.com/integrate-distributed-ray-serve-deployment-with-kafka-181403f4e194)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/integrate-distributed-ray-serve-deployment-with-kafka-181403f4e194](https://towardsdatascience.com/integrate-distributed-ray-serve-deployment-with-kafka-181403f4e194)
- en: Learn how to simply combine Ray Serve Deployment with asynchronous Kafka Consumer
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何简单地将 Ray Serve 部署与异步 Kafka 消费者结合起来
- en: '[](https://medium.com/@slanjr?source=post_page-----181403f4e194--------------------------------)[![Rostyslav
    Neskorozhenyi](../Images/7d8616b6a3c60bcc29fde5d0a72f13c9.png)](https://medium.com/@slanjr?source=post_page-----181403f4e194--------------------------------)[](https://towardsdatascience.com/?source=post_page-----181403f4e194--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----181403f4e194--------------------------------)
    [Rostyslav Neskorozhenyi](https://medium.com/@slanjr?source=post_page-----181403f4e194--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@slanjr?source=post_page-----181403f4e194--------------------------------)[![Rostyslav
    Neskorozhenyi](../Images/7d8616b6a3c60bcc29fde5d0a72f13c9.png)](https://medium.com/@slanjr?source=post_page-----181403f4e194--------------------------------)[](https://towardsdatascience.com/?source=post_page-----181403f4e194--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----181403f4e194--------------------------------)
    [Rostyslav Neskorozhenyi](https://medium.com/@slanjr?source=post_page-----181403f4e194--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----181403f4e194--------------------------------)
    ·5 min read·Jul 16, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----181403f4e194--------------------------------)
    ·阅读时间 5 分钟·2023年7月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/86f8c4b46bf6189e1dc7d0edbd8945ac.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/86f8c4b46bf6189e1dc7d0edbd8945ac.png)'
- en: Image is generated by [Midjourney](https://www.midjourney.com/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Midjourney](https://www.midjourney.com/) 生成
- en: '[Ray](https://github.com/ray-project/ray) is a modern open source framework
    that allows you to create distributed applications in Python with ease. You can
    create simple training pipelines, do hyperparameter tuning, data processing and
    model serving.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ray](https://github.com/ray-project/ray) 是一个现代开源框架，使你能够轻松创建 Python 分布式应用程序。你可以创建简单的训练管道，进行超参数调优，数据处理和模型服务。'
- en: Ray allows you to create online inference APIs with [Ray Serve](https://docs.ray.io/en/latest/serve/index.html).
    You can easily combine several ML models and custom business logic in one application.
    Ray Serve automatically creates an HTTP interface for your deployments, taking
    care of fault tolerance and replication.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 允许你使用 [Ray Serve](https://docs.ray.io/en/latest/serve/index.html) 创建在线推断
    API。你可以在一个应用程序中轻松地将多个 ML 模型和自定义业务逻辑结合起来。Ray Serve 自动为你的部署创建 HTTP 接口，负责容错和复制。
- en: '![](../Images/93c068fcae1fe686691387d9f713af16.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93c068fcae1fe686691387d9f713af16.png)'
- en: 'Ray ecosystem. Source: [https://docs.ray.io/en/latest/ray-air/getting-started.html](https://docs.ray.io/en/latest/ray-air/getting-started.html)
    ([APACHE 2.0](https://github.com/ray-project/ray/blob/releases/2.6.1/LICENSE)
    licence )'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 'Ray 生态系统。来源: [https://docs.ray.io/en/latest/ray-air/getting-started.html](https://docs.ray.io/en/latest/ray-air/getting-started.html)
    ([APACHE 2.0](https://github.com/ray-project/ray/blob/releases/2.6.1/LICENSE)
    许可证)'
- en: But there is one thing that **Ray Serve** misses for now. Many modern distributed
    applications communicate through [Kafka](https://kafka.apache.org/), but there
    is no out-of-the-box way to connect Ray Serve service to Kafka topic.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但目前**Ray Serve**还有一个缺陷。许多现代分布式应用程序通过 [Kafka](https://kafka.apache.org/) 进行通信，但目前没有现成的方法将
    Ray Serve 服务连接到 Kafka 主题。
- en: But don’t panic. It will not take too much effort to teach Ray Serve to communicate
    with Kafka. So, let’s begin.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但不用担心。教会 Ray Serve 与 Kafka 进行通信不会花费太多精力。那么，让我们开始吧。
- en: 'First of all we will need to prepare our local environment. We will use a docker-compose
    file with Kafka and [Kafdrop UI](https://github.com/obsidiandynamics/kafdrop)
    docker containers to start and explore our local Kafka instance (so we assume
    that you have [Docker](https://docs.docker.com/engine/install/ubuntu/) and [Docker
    Compose](https://docs.docker.com/compose/install/linux/#install-using-the-repository)
    installed). Also we will need to install some Python requirements to get the work
    done:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要准备我们的本地环境。我们将使用带有 Kafka 和 [Kafdrop UI](https://github.com/obsidiandynamics/kafdrop)
    的 docker-compose 文件来启动和探索我们的本地 Kafka 实例（因此我们假设你已经安装了 [Docker](https://docs.docker.com/engine/install/ubuntu/)
    和 [Docker Compose](https://docs.docker.com/compose/install/linux/#install-using-the-repository)）。此外，我们还需要安装一些
    Python 依赖来完成工作：
- en: '[Ray](https://github.com/ray-project/ray) itself'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ray](https://github.com/ray-project/ray) 本身'
- en: '[aiokafka](https://github.com/aio-libs/aiokafka)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[aiokafka](https://github.com/aio-libs/aiokafka)'
- en: All requirements can be downloaded by this [**link**](https://github.com/slanj/ray-serve-kafka/blob/main/requirements.txt).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所有需求可以通过这个[**链接**](https://github.com/slanj/ray-serve-kafka/blob/main/requirements.txt)下载。
- en: Now we will create a [*ray-consumer.py*](https://github.com/slanj/ray-serve-kafka/blob/main/ray-consumer.py)
    file with [**Ray Deployment**](https://docs.ray.io/en/latest/serve/key-concepts.html#deployment)
    that will be served with Ray Serve. I will not go into details about Ray Serve
    concepts, as you can read about that in the [documentation](https://docs.ray.io/en/latest/serve/getting_started.html).
    Basically it takes the usual Python class and converts it to a asynchronous service
    [Ray Deployment](https://docs.ray.io/en/latest/serve/key-concepts.html#deployment)
    with **@serve.deployment** decorator**:**
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将创建一个[*ray-consumer.py*](https://github.com/slanj/ray-serve-kafka/blob/main/ray-consumer.py)文件，并使用[**Ray
    部署**](https://docs.ray.io/en/latest/serve/key-concepts.html#deployment)进行服务。我不会详细介绍
    Ray Serve 的概念，你可以在[文档](https://docs.ray.io/en/latest/serve/getting_started.html)中阅读。基本上，它将普通的
    Python 类转换为异步服务[Ray 部署](https://docs.ray.io/en/latest/serve/key-concepts.html#deployment)，并使用**@serve.deployment**装饰器**:**
- en: Let’s explore this code a little. Here we can see a *serve.deployment* decorator
    with several parameters. These parameters will be used by Ray Serve to run our
    deployment. In our case Ray Serve will run one deployment replica on a local Ray
    cluster (cluster also may exist on [*AWS or Kubernetes*](https://docs.ray.io/en/latest/cluster/getting-started.html)*)*.
    You can start several deployment replicas with **num_replicas** parameter, Ray
    Serve will split traffic between these replicas.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微深入了解一下这段代码。在这里，我们可以看到一个带有多个参数的*serve.deployment*装饰器。这些参数将被 Ray Serve 用来运行我们的部署。在我们的案例中，Ray
    Serve 将在本地 Ray 集群上运行一个部署副本（集群也可以存在于[*AWS 或 Kubernetes*](https://docs.ray.io/en/latest/cluster/getting-started.html)）。你可以使用**num_replicas**参数启动多个部署副本，Ray
    Serve 将在这些副本之间分配流量。
- en: Also we specify the amount of resources which each replica will consume with
    **num_cpus** and **num_gpus** parameters. Such parameters are useful for example
    when you have a heavy deployment and it needs a specific number of CPU cores to
    run. In our case we will not need much, so we will use no GPU, and only 0.1 CPU.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还通过**num_cpus**和**num_gpus**参数指定了每个副本将消耗的资源量。例如，当你有一个重型部署且需要特定数量的 CPU 核心来运行时，这些参数非常有用。在我们的案例中，我们不需要太多资源，所以我们将不使用
    GPU，仅使用 0.1 CPU。
- en: Each time when deployment starts it will get the running event loop (read about
    asyncio event loops [here](https://superfastpython.com/asyncio-event-loop/)),
    initialize [AIOKafkaConsumer](https://aiokafka.readthedocs.io/en/stable/consumer.html),
    connect to Kafka(with *topics*, *bootstrap_servers* and *group_id* parameters)
    and start *consume* task for that consumer in the current event loop. Also it
    will set deployment health status to True.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 每次部署启动时，它将获取正在运行的事件循环（有关 asyncio 事件循环的信息可以[这里](https://superfastpython.com/asyncio-event-loop/)找到），初始化[AIOKafkaConsumer](https://aiokafka.readthedocs.io/en/stable/consumer.html)，连接到
    Kafka（使用*topics*、*bootstrap_servers*和*group_id*参数），并在当前事件循环中启动*consume*任务。此外，它将把部署健康状态设置为
    True。
- en: By default, the Serve controller periodically [health-checks](https://docs.ray.io/en/latest/serve/production-guide/fault-tolerance.html)
    each Serve deployment replica and restarts it on failure. We defined a custom
    health-check, by adding a *check_health* method to our *RayConsumer* deployment
    class. Healt-check periods are defined by **health_check_period_s** and **health_check_timeout_s**
    parameters in *serve.deployment* decorator*.*
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Serve 控制器会定期对每个 Serve 部署副本进行[健康检查](https://docs.ray.io/en/latest/serve/production-guide/fault-tolerance.html)，并在失败时重启它。我们通过在*RayConsumer*部署类中添加一个*check_health*方法来定义了一个自定义健康检查。健康检查周期由*serve.deployment*装饰器中的**health_check_period_s**和**health_check_timeout_s**参数定义。
- en: '![](../Images/6a5171bdd11207330b93d84f57385aea.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a5171bdd11207330b93d84f57385aea.png)'
- en: Ray Serve Architecture. Source [https://docs.ray.io/en/latest/serve/architecture.html](https://docs.ray.io/en/latest/serve/architecture.html)
    ([APACHE 2.0](https://github.com/ray-project/ray/blob/releases/2.6.1/LICENSE)
    license)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Serve 架构。来源 [https://docs.ray.io/en/latest/serve/architecture.html](https://docs.ray.io/en/latest/serve/architecture.html)
    ([APACHE 2.0](https://github.com/ray-project/ray/blob/releases/2.6.1/LICENSE)
    许可证)
- en: Our consumer will continue to get messages from Kafka and print a toy ML model
    prediction to the terminal till Ray Cluster is alive. In our example we just use
    a toy model to simulate ML prediction, but we can include any logic or ML model
    inference. We can also use [Ray object store](https://docs.ray.io/en/latest/ray-core/objects.html)
    to store ML models and [speed up load time](https://medium.com/ibm-data-ai/how-to-load-pytorch-models-340-times-faster-with-ray-8be751a6944c).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的消费者将继续从 Kafka 获取消息，并将玩具 ML 模型的预测打印到终端，直到 Ray 集群仍然运行。在我们的示例中，我们仅使用玩具模型来模拟
    ML 预测，但我们可以包括任何逻辑或 ML 模型推断。我们还可以使用 [Ray 对象存储](https://docs.ray.io/en/latest/ray-core/objects.html)
    来存储 ML 模型和 [加快加载时间](https://medium.com/ibm-data-ai/how-to-load-pytorch-models-340-times-faster-with-ray-8be751a6944c)。
- en: In a case when a consumer fails due to some random error — our deployment will
    set health status to False, **check_health** method will return an error and Ray
    Serve Controller will automatically restart deployment.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果消费者由于某些随机错误而失败——我们的部署将把健康状态设置为 False，**check_health** 方法将返回一个错误，Ray Serve
    控制器将自动重新启动部署。
- en: 'It goes without saying that before you start consuming messages from Kafka,
    we need to start Kafka and create a topic in which our messages will be. Go to
    the folder where you have placed [docker-compose.yaml](https://github.com/slanj/ray-serve-kafka/blob/main/docker-compose.yaml)
    and run the following command in terminal to start Kafka and Kafdrop:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 不用说，在你开始从 Kafka 消费消息之前，我们需要启动 Kafka 并创建一个存放我们消息的主题。前往你放置 [docker-compose.yaml](https://github.com/slanj/ray-serve-kafka/blob/main/docker-compose.yaml)
    的文件夹，并在终端中运行以下命令以启动 Kafka 和 Kafdrop：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now we can go to [http://localhost:9000](http://localhost:9000) and create a
    new Kafka topic. In our case we name it “ray-topic”
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以访问 [http://localhost:9000](http://localhost:9000) 并创建一个新的 Kafka 主题。在我们的例子中，我们将其命名为“ray-topic”
- en: '![](../Images/b550a643616984f7ce5ea368ace98ad9.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b550a643616984f7ce5ea368ace98ad9.png)'
- en: Kafdrop UI. Image by Author
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Kafdrop 用户界面。图片来自作者
- en: 'Everything is ready, so we can start our deployment with the command in terminal
    (you should run this command in a folder that contains [*ray-consumer.py*](https://github.com/slanj/ray-serve-kafka/blob/main/ray-consumer.py)):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一切准备就绪，我们可以使用终端中的命令启动我们的部署（你应该在包含 [*ray-consumer.py*](https://github.com/slanj/ray-serve-kafka/blob/main/ray-consumer.py)
    的文件夹中运行此命令）：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This command will set up a local Ray Cluster and start Ray Serve Deployment
    on it. We can check your deployments status in Ray Dashboard, which is usually
    accessible by this address [http://localhost:8265](http://localhost:8265) .
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令将设置一个本地 Ray 集群并在其上启动 Ray Serve 部署。我们可以在 Ray Dashboard 中检查你的部署状态，通常可以通过这个地址
    [http://localhost:8265](http://localhost:8265) 访问。
- en: '![](../Images/56921ff96966851db92df4604602fa98.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56921ff96966851db92df4604602fa98.png)'
- en: Ray Dashboard. Image by Author
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Dashboard。图片来自作者
- en: 'Now we can send some messages to Kafka, so our deployment can consume them.
    We can do this with a simple script [aio-producer.py](https://github.com/slanj/ray-serve-kafka/blob/main/aio-producer.py):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以向 Kafka 发送一些消息，以便我们的部署可以消费它们。我们可以使用一个简单的脚本 [aio-producer.py](https://github.com/slanj/ray-serve-kafka/blob/main/aio-producer.py)
    来实现：
- en: 'This script will use [AIOKafkaProducer](https://aiokafka.readthedocs.io/en/stable/producer.html)
    to send a particular message to a particular topic, for example like this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本将使用 [AIOKafkaProducer](https://aiokafka.readthedocs.io/en/stable/producer.html)
    将特定消息发送到特定主题，例如这样：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If everything is ok, our Ray Consumer Deployment will get all those messages
    from Kafka and just print them to the terminal.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切正常，我们的 Ray 消费者部署将从 Kafka 获取所有这些消息，并将其打印到终端。
- en: '![](../Images/e248114d092042f9417bdf2944ae87ef.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e248114d092042f9417bdf2944ae87ef.png)'
- en: Ray Deployment output. Image by Author
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 部署输出。图片来自作者
- en: Basically, that’s all. Now you have a working fault tolerant Ray Serve Deployment
    that consumes messages from Kafka. Of course it is basic, but you can easily add
    custom business logic, [compose it with other deployments](https://docs.ray.io/en/latest/serve/model_composition.html)
    or even combine it with [Ray Workflows](https://docs.ray.io/en/latest/workflows/index.html).
    You can add additional logic into *check_health* method so your deployment will
    periodically check additional services if needed.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上就是这样。现在你有一个工作中的容错 Ray Serve 部署，它从 Kafka 消费消息。当然，这只是基础，你可以轻松地添加自定义业务逻辑，[将它与其他部署组合](https://docs.ray.io/en/latest/serve/model_composition.html)
    或者甚至与 [Ray Workflows](https://docs.ray.io/en/latest/workflows/index.html) 结合。你可以在
    *check_health* 方法中添加额外的逻辑，以便你的部署在需要时定期检查额外的服务。
- en: You can add replicas to your deployment with serve.deployment parameter **num_replicas**
    or even [set up autoscaling](https://docs.ray.io/en/latest/serve/scaling-and-resource-allocation.html)
    with **min_replicas** and **max_replicas** parameters, so Ray will automatically
    add replicas to your deployment depending on traffic and load.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过 `serve.deployment` 参数**num_replicas**将副本添加到你的部署中，甚至可以使用**min_replicas**和**max_replicas**参数来[设置自动扩展](https://docs.ray.io/en/latest/serve/scaling-and-resource-allocation.html)，这样Ray将根据流量和负载自动添加副本到你的部署中。
- en: Ray provides you all the tools to build scalable and fault tolerant Python applications,
    and now knowing how to combine it with Kafka you are able to build some new really
    powerful distributed apps.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Ray提供了所有构建可扩展和容错的Python应用程序所需的工具，现在了解如何将其与Kafka结合，你就能够构建一些真正强大的分布式应用程序。
- en: All the code mentioned in this article can be found by [this link](https://github.com/slanj/ray-serve-kafka/).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中提到的所有代码可以通过[这个链接](https://github.com/slanj/ray-serve-kafka/)找到。
