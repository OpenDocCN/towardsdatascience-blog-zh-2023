- en: Text Data Pre-processing for Time-Series Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/text-data-pre-processing-for-time-series-models-162c0d01f5c5](https://towardsdatascience.com/text-data-pre-processing-for-time-series-models-162c0d01f5c5)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Have you ever thought about how sentiment from text data can be used as a regressor
    in time-series models?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://petrkorab.medium.com/?source=post_page-----162c0d01f5c5--------------------------------)[![Petr
    Korab](../Images/9f3afb4b8985584981220e30f18e3b69.png)](https://petrkorab.medium.com/?source=post_page-----162c0d01f5c5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----162c0d01f5c5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----162c0d01f5c5--------------------------------)
    [Petr Korab](https://petrkorab.medium.com/?source=post_page-----162c0d01f5c5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----162c0d01f5c5--------------------------------)
    ·6 min read·Feb 9, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c572f7ea6033e1d39e2accc7b85121c6.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by Kaleidico on Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text data offer qualitative information that can be quantified, aggregated,
    and used as a variable in time-series models. Simple methods of text data representation,
    such as [one-hot encoding](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/)
    of categorical variables and word [n-grams](https://www.analyticsvidhya.com/blog/2021/09/what-are-n-grams-and-how-to-implement-them-in-python/),
    have been used since NLP’s early beginnings. Over time, more complex methods,
    including the [Bag-of-words](https://machinelearningmastery.com/gentle-introduction-bag-words-model/)
    model, found their way to represent text data for machine learning algorithms.
    Based on the distributional hypothesis formulated by Harris [1] and Firth [2],
    modern models such as Word-to-Vec [3] and [4], GloVe [5], and ELMo [6] use vector
    representation of words in their neural network architectures. Since computers
    process text as vectors, it can be used as a variable in time-series econometric
    models.
  prefs: []
  type: TYPE_NORMAL
- en: In this way, we can use qualitative information from the text and use it to
    extend the possibilities of quantitative time-series models.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In this article, you’ll learn more about:'
  prefs: []
  type: TYPE_NORMAL
- en: How to use qualitative information from text for quantitative modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to clean and represent text data for time-series models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to work efficiently with **1 million rows of text data**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: End-to-end coding example in Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our recent conference paper, we developed a structural plan for text-data
    pre-processing that might be used for areas such as: (1) predicting exchange rates
    with the sentiment from social networks, (2) predicting agricultural prices using
    public news data, (3) demand prediction in various areas.'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Structural plan of** text data representation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s start with a plan. In the beginning, there is **qualitative raw text**
    data collected over time. In the end, we have empirical estimates with time-varying
    numerical vectors (= **quantitative data**). This graph says more about how we
    will proceed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf08c69096f31e5002ac150c93450ef3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1\. Structural plan of text data representation. Source: Poměnková et
    al., submitted to [MAREW 2023](https://www.marew.cz/).'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Empirical example in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s illustrate the coding on the [**News Category Dataset**](https://www.kaggle.com/datasets/rmisra/news-category-dataset)
    compiled by Rishabh Misra [8], [9] and released under the [Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/)
    license. The data contains news headlines published between 2012 and 2022 on huffpost.com.
    It was multiplicated to reach a 1-million-row dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The primary aim is to construct a time series in monthly frequency from news
    headlines reflecting public sentiment.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The dataset contains 1 million headlines. Because of its size, I used [Polars](https://pypi.org/project/polars/)
    library, which makes dataframe operations much faster. Compared to the mainstream
    Pandas, it handles large data files highly efficiently. On top of that, the code
    was run in Google Colab with GPU hardware accelerator.
  prefs: []
  type: TYPE_NORMAL
- en: 'The python code is [here](https://github.com/PetrKorab/Text-Data-Pre-processing-for-Time-series-Models),
    and the data looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25d4f8392aa4cf56a94555b83f266278.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. News Category Dataset
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Text data pre-processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The purpose of text data pre-processing is to remove all redundant information
    that might bias the analysis or lead to an incorrect interpretation of the results.
    We’ll remove **punctuation**, **numbers**, **extra spaces**, English **stopwords**
    (most common words with low or zero information value), and **lowercase** the
    text.
  prefs: []
  type: TYPE_NORMAL
- en: Probably the simplest and most efficient way of cleaning text data in Python
    is with [cleantext](https://pypi.org/project/cleantext/) library.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'First, define a cleaning function to perform the cleaning operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we clean the 1 mil. dataset with Polars:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The clean dataset contains text with maximum informational value for further
    steps. Any unnecessary strings and digits reduce the accuracy of the final empirical
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Text data representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data representation** involves methods used to represent data in a computer.
    Since computers work with numbers, we select an appropriate model to vectorize
    the text dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: In our project, we are constructing a time series of sentiment. For this use
    case, the pre-trained sentiment classifier **VADER *(Valence Aware Dictionary
    and Sentiment Reasoner)***is a good choice. Read my [previous article](https://medium.com/towards-data-science/the-most-favorable-pre-trained-sentiment-classifiers-in-python-9107c06442c6)
    to learn more about this classifier, along with some other alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'The classification with [vaderSentiment](https://pypi.org/project/vaderSentiment/)
    library looks in the code as follows. First, create the function for classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, apply the function for the time-series dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is what the result looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba7ed9741a751c82c61badb4b7430180.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Sentiment evaluation
  prefs: []
  type: TYPE_NORMAL
- en: The *headline* column includes sentiment on the scale [-1:1] reflecting prevalent
    emotional content in the headlines for each row.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Time-series representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next step in time-series text data representation involves extending the
    data matrix with a time dimension. It can be achieved by (a) aggregating data
    along a time axis and (b) selecting a method implementing time-series text data
    representation. In the case of our data, we’ll do the former and aggregate sentiment
    from each row by monthly frequency.
  prefs: []
  type: TYPE_NORMAL
- en: 'This code makes the average aggregation of sentiment and prepares monthly time
    series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 2.4\. Quantitative modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final step is to use the time series for modeling. To show an example, in
    our recent conference paper, we similarly extracted the sentiment from headlines
    of research articles published in the top 5 economic journals. Then, we use rolling
    time-varying correlations of a 5-year window and looked at how sentiment relates
    to GDP and other global economic indicators (see figure 2).
  prefs: []
  type: TYPE_NORMAL
- en: We hypothesized that sentiment correlates with the macroeconomic environment
    during periods of sharp recessions and inflation shocks. The results support,
    except for one specific journal, these considerations for the Oil Shocks of the
    1970s, which led to a steep recession accompanied by a massive inflation spike.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/58d0b0e9bbd828241d5a69f71b6583a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4\. Rolling correlations of sentiment and GDP. Source: Poměnková et
    al., submitted to [MAREW 2023](https://www.marew.cz/).'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article, we have constructed monthly time series of sentiment from
    1 million rows of text data. The key points are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Qualitative information can extend the capacities of quantitative time-series
    models**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Polars library makes large text-data pre-processing feasible even in Python
    language**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud services such as Google Colab make the processing of extensive text
    datasets even faster.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The complete code in this tutorial is on my [GitHub](https://github.com/PetrKorab/Text-Data-Pre-processing-for-Time-series-Models).
    The recommended reading is [*The Most Favorable Pre-trained Sentiment Classifiers
    in Python*](https://medium.com/towards-data-science/the-most-favorable-pre-trained-sentiment-classifiers-in-python-9107c06442c6)*.*
  prefs: []
  type: TYPE_NORMAL
- en: '*Did you like the article? You can invite me* [*for coffee*](https://www.buymeacoffee.com/petrkorab)
    *and support my writing. You can also subscribe to my* [*email list*](https://medium.com/subscribe/@petrkorab)
    *to get notified about my new articles. Thanks!*'
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Z. Harris. 1954\. Distributional structure. *Word*, vol. 10, no. 23, pp.
    146–162.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] J. R. Firth. 1957\. A synopsis of linguistic theory 1930–1955\. In Studies
    in Linguistic Analysis, pp. 1–32\. Oxford: Philological Society. Reprinted in
    F.R. Palmer (ed.), Selected Papers of J.R. Firth 1952–1959, London: Longman 1968.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Mikolov, T., Chen, K., Corrado, G. S., Dean, J. 2013b. Efficient estimation
    of word representations in vector space. Computation and Language: International
    Conference on Learning Representations.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado and J. Dean. 2013\. Distributed
    representations of words and phrases and their compositionality. *Advances in
    Neural Information Processing Systems*, vol. 26 (NIPS 2013).'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee and L.
    Zettlemoyer, L. 2018\. Proceedings of the 2018 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    vol. 1.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] J. Pennington, R. Socher and C. D. Manning. 2014\. GloVe: Global Vectors
    for Word Representation. Proceedings of the 2014 Conference on Empirical Methods
    in Natural Language Processing (EMNLP).'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Poměnková, J., Koráb, P., Štrba, D. Text Data Pre-processing for Time-series
    Modelling. Submitted to [MAREW 2023](https://www.marew.cz/).'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Misra, Rishabh. “News Category Dataset.” arXiv preprint arXiv:2209.11429
    (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Misra, Rishabh and Jigyasa Grover. “Sculpting Data for ML: The first act
    of Machine Learning.” ISBN 9798585463570 (2021).'
  prefs: []
  type: TYPE_NORMAL
