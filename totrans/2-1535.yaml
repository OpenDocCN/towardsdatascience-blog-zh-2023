- en: Monocular Depth Estimation to Predict Surface Reliefs of Mars
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单目深度估计预测火星表面高程
- en: 原文：[https://towardsdatascience.com/monocular-depth-estimation-to-predict-surface-reliefs-of-mars-1b50aed3361a](https://towardsdatascience.com/monocular-depth-estimation-to-predict-surface-reliefs-of-mars-1b50aed3361a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/monocular-depth-estimation-to-predict-surface-reliefs-of-mars-1b50aed3361a](https://towardsdatascience.com/monocular-depth-estimation-to-predict-surface-reliefs-of-mars-1b50aed3361a)
- en: A different application of monocular depth estimation models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单目深度估计模型的另一种应用
- en: '[](https://mattiagatti.medium.com/?source=post_page-----1b50aed3361a--------------------------------)[![Mattia
    Gatti](../Images/9d5aeb356ff01ed9e4ead66c18994595.png)](https://mattiagatti.medium.com/?source=post_page-----1b50aed3361a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1b50aed3361a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1b50aed3361a--------------------------------)
    [Mattia Gatti](https://mattiagatti.medium.com/?source=post_page-----1b50aed3361a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mattiagatti.medium.com/?source=post_page-----1b50aed3361a--------------------------------)[![Mattia
    Gatti](../Images/9d5aeb356ff01ed9e4ead66c18994595.png)](https://mattiagatti.medium.com/?source=post_page-----1b50aed3361a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1b50aed3361a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1b50aed3361a--------------------------------)
    [Mattia Gatti](https://mattiagatti.medium.com/?source=post_page-----1b50aed3361a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1b50aed3361a--------------------------------)
    ·5 min read·Sep 4, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----1b50aed3361a--------------------------------)
    ·阅读时间5分钟·2023年9月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d2e0288d571818e893578b3268a36a16.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2e0288d571818e893578b3268a36a16.png)'
- en: Photo by [NASA](https://unsplash.com/@nasa?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 摄影：由[NASA](https://unsplash.com/@nasa?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Several approaches to estimating surface elevation from a single image have
    been discussed in the literature. In a previous [article](/generate-a-3d-mesh-from-an-image-with-python-12210c73e5cc),
    I discussed how it is possible to predict the depth of a single 2D image using
    a monocular estimation model. However, when the input to the model is an image
    of a particular surface, the prediction represents a Digital Elevation Model (DEM).
    In my first research paper I showed how a DEM of the surface of Mars can be obtained
    from 2D greyscale images using deep learning approaches. To better understand
    the idea I’m going to propose, I suggest you first to try the demo of the project
    [here](https://huggingface.co/spaces/mattiagatti/mars_dtm_estimation).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中讨论了多种从单张图像估算表面高程的方法。在之前的[文章](/generate-a-3d-mesh-from-an-image-with-python-12210c73e5cc)中，我讨论了如何使用单目估计模型预测单张2D图像的深度。然而，当模型的输入是特定表面的图像时，预测结果代表的是数字高程模型（DEM）。在我的第一篇研究论文中，我展示了如何利用深度学习方法从2D灰度图像中获得火星表面的DEM。为了更好地理解我将要提出的想法，我建议你首先尝试项目的演示版，[这里](https://huggingface.co/spaces/mattiagatti/mars_dtm_estimation)。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: 'As discussed in more detail in another [story](/the-ultimate-beginners-guide-to-geospatial-raster-data-feb7673f6db0),
    the DEM of a surface is a grid of elevation values where each cell stores the
    elevation of a particular point on the surface:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如在另一个[故事](/the-ultimate-beginners-guide-to-geospatial-raster-data-feb7673f6db0)中更详细地讨论过，表面的数字高程模型（DEM）是一个高程值的网格，每个单元格存储表面上特定点的高程：
- en: '![](../Images/41059f8b3822f57ceff36a4448ebd9af.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41059f8b3822f57ceff36a4448ebd9af.png)'
- en: Graphic visualization of a DEM. [NSIDC](https://commons.wikimedia.org/wiki/File:Digital_elevation_model_(DEM)_of_the_Mt._Everest_region_-_50090548573.png),
    [CC BY 2.0](https://creativecommons.org/licenses/by/2.0), via Wikimedia Commons
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: DEM的图形可视化。[NSIDC](https://commons.wikimedia.org/wiki/File:Digital_elevation_model_(DEM)_of_the_Mt._Everest_region_-_50090548573.png)，[CC
    BY 2.0](https://creativecommons.org/licenses/by/2.0)，通过Wikimedia Commons
- en: DEMs are usually represented graphically using color maps. In the image above,
    the highest points are red and the lowest points are purple.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: DEM通常通过颜色地图图形化表示。在上图中，最高点是红色的，最低点是紫色的。
- en: 'On the other hand, monocular depth estimation models are used to estimate the
    distance of each pixel of an image from the camera (even the camera of a satellite)
    that took the image:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，单目深度估计模型用于估算图像中每个像素与拍摄该图像的相机（甚至是卫星相机）之间的距离：
- en: '![](../Images/67951727978d275cd717678c504bdc73.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67951727978d275cd717678c504bdc73.png)'
- en: Depth prediction of a bedroom. Input image from [NYU-Depth V2](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一个卧室的深度预测。输入图像来自[NYU-Depth V2](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html)。
- en: The idea is that a satellite image of a surface can be fed into a monocular
    depth estimation model. In this way, it’s possible to predict the DEM of that
    surface, because each point of the output represents a distance (depth), and elevations
    of the surface can be derived by using depths (more on this later).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是，将表面的卫星图像输入到单目深度估计模型中。这样可以预测该表面的DEM，因为输出的每个点代表一个距离（深度），而表面的高程可以通过使用深度推导出来（稍后会详细讲述）。
- en: The method discussed in this article can be used for other surfaces as well,
    and not just for Mars.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本文讨论的方法也可以用于其他表面，而不仅仅是火星。
- en: UAHiRISE
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UAHiRISE
- en: The High-Resolution Imaging Science Experiment (HiRISE) is a camera on board
    the Mars Reconnaissance Orbiter (a spacecraft that provides support for missions
    to Mars). HiRISE took a large number of grayscale images of the Martian surface,
    and each image is associated with a DTM. [UAHiRISE](https://www.uahirise.org)
    is the University of Arizona website where all these resources are available.
    These files are geographical rasters, if you want more information about working
    with this file format, see my other [article](https://medium.com/towards-data-science/the-ultimate-beginners-guide-to-geospatial-raster-data-feb7673f6db0).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 高分辨率成像科学实验（HiRISE）是火星勘测轨道器上的一台相机（该航天器为火星任务提供支持）。HiRISE拍摄了大量火星表面的灰度图像，每张图像都与一个DTM相关联。[UAHiRISE](https://www.uahirise.org)是亚利桑那大学的网站，所有这些资源都可以在这里找到。这些文件是地理栅格数据，如果您想了解更多关于这种文件格式的信息，请参见我另一篇[文章](https://medium.com/towards-data-science/the-ultimate-beginners-guide-to-geospatial-raster-data-feb7673f6db0)。
- en: '![](../Images/9b9083bc1cbda720699421a1ca887107.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b9083bc1cbda720699421a1ca887107.png)'
- en: A satellite imagery of the Mars surface with the associated DTM. Image from
    [UAHiRISE](https://www.uahirise.org/dtm/ESP_019147_2395).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 火星表面的卫星图像及其相关DTM。图片来自[UAHiRISE](https://www.uahirise.org/dtm/ESP_019147_2395)。
- en: Dataset
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: A web scrapper was used to download all the greyscale images with their associated
    DTMs (code below). However, these files are too large in resolution to be used
    as input to a neural network. Thus, they had to be split into smaller patches
    (I discussed this procedure in another [article](https://levelup.gitconnected.com/how-to-split-an-image-into-patches-with-python-e1cf42cf4f77)).
    The final dataset consisted of 150,000 patches.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用了一个网页抓取工具来下载所有的灰度图像及其相关的DTM（代码见下）。然而，这些文件的分辨率过大，无法作为神经网络的输入。因此，必须将它们拆分为更小的补丁（我在另一篇[文章](https://levelup.gitconnected.com/how-to-split-an-image-into-patches-with-python-e1cf42cf4f77)中讨论了这一过程）。最终的数据集由150,000个补丁组成。
- en: To train the model, a set of tiles of the Martian surface was used with the
    corresponding depth. However, each tile of the original DTM had to be scaled.
    Absolute elevations were converted to relative depths. This means that if a ground
    truth had values in the range [-3500, -2500] they were scaled to [0, 1000] where
    0 is the closest point and 1000 is the farthest. This had to be done because a
    monocular depth estimation model can’t directly predict absolute elevation values.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练模型，使用了一组火星表面的图块及其相应的深度。然而，原始DTM的每个图块都必须进行缩放。绝对高程被转换为相对深度。这意味着如果地面真值的范围是[-3500,
    -2500]，它们会被缩放到[0, 1000]，其中0是最近的点，1000是最远的点。这是因为单目深度估计模型不能直接预测绝对高程值。
- en: 'An example of a training sample is represented by the image below:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图像展示了一个训练样本的示例：
- en: '![](../Images/ae30c7588927001fe379be77d69fb830.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae30c7588927001fe379be77d69fb830.png)'
- en: One of the training samples. Dataset made of images and DTMs from [UAHiRISE](https://www.uahirise.org).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一个训练样本。数据集由[UAHiRISE](https://www.uahirise.org)的图像和DTM组成。
- en: The reddish colours are the closest points and the bluish colours are the furthest.
    However, the distance of a point is inversely proportional to its elevation, i.e.
    reddish colours have a higher elevation than blueish colours. After predicting
    depths, all points are converted to elevations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 红色表示最接近的点，蓝色表示最远的点。然而，点的距离与其高程成反比，即红色点的高程高于蓝色点。预测深度后，所有点都被转换为高程。
- en: The dataset is about 1TB in size and it’s available on [Kaggle](https://www.kaggle.com/datasets/mattiagatti/uahirise-mars-dtm-estimation).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集大小约为1TB，可在[Kaggle](https://www.kaggle.com/datasets/mattiagatti/uahirise-mars-dtm-estimation)上获取。
- en: Model
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型
- en: 'After an analysis of all the available architectures for monocular depth estimation,
    the GLPN¹ model was chosen:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析了所有可用的单目深度估计架构后，选择了GLPN¹模型：
- en: '![](../Images/8171e7e02724d0340c88fd3eaf17df17.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8171e7e02724d0340c88fd3eaf17df17.png)'
- en: GLPN architecture. Image from the [official paper](https://arxiv.org/abs/2201.07436).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: GLPN架构。图像来自[官方论文](https://arxiv.org/abs/2201.07436)。
- en: The architecture uses a hierarchical transformer encoder to obtain global information
    at different resolutions from the H × W × 3 input (RGB image). Then a decoder
    restore the bottleneck feature into the size of H × W × 1 (the output depth map).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构使用层次化的变换器编码器从H × W × 3输入（RGB图像）中获得不同分辨率的全局信息。然后解码器将瓶颈特征恢复到H × W × 1的大小（输出深度图）。
- en: The model was almost ready for use in the experiment, except for the input layer.
    Most of the images released by UAHiRISE are greyscale. Therefore, the input layer
    was modified to take 1-channel (greyscale) images instead of 3-channel (RGB) images.
    The ImageNet pre-trained for the encoder was still used, but without loading the
    weights for the input layer.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 模型几乎准备好用于实验，唯一缺少的是输入层。UAHiRISE发布的大多数图像是灰度图像。因此，输入层被修改为接受1通道（灰度）图像而非3通道（RGB）图像。虽然仍然使用了ImageNet为编码器预训练的模型，但没有加载输入层的权重。
- en: Final result
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最终结果
- en: 'After training the model, it’s ready to predict DTM. The following is an example
    of prediction made by the final model:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型后，它已准备好预测DTM。以下是最终模型做出的预测示例：
- en: '![](../Images/8a8e591cc3540ab9de6de87b0c206b86.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a8e591cc3540ab9de6de87b0c206b86.png)'
- en: Input grayscale image. Tile from a [UAHiRISE](https://www.uahirise.org) image.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 输入灰度图像。来自[UAHiRISE](https://www.uahirise.org)的图块。
- en: '![](../Images/48109a6b9ad24d590f4d88fcfcd43bb3.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48109a6b9ad24d590f4d88fcfcd43bb3.png)'
- en: Predicted DTM with relative elevations.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的DTM与相对高程。
- en: '![](../Images/c9ac0273db5772f1f02d7c22ad42cfe8.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9ac0273db5772f1f02d7c22ad42cfe8.png)'
- en: 3D model of the DTM. Image made by using [this generator](https://imagetostl.com/convert/file/stl/to/gif).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: DTM的3D模型。图像使用[此生成器](https://imagetostl.com/convert/file/stl/to/gif)制作。
- en: '![](../Images/a4e04f815be0b7c47f5dc0bdb89c389c.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4e04f815be0b7c47f5dc0bdb89c389c.png)'
- en: 3D model of the DTM. Image made by using [this generator](https://imagetostl.com/convert/file/stl/to/gif).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: DTM的3D模型。图像使用[此生成器](https://imagetostl.com/convert/file/stl/to/gif)制作。
- en: After calculating the metrics for all the test samples, the results showed a
    mean absolute error of *10.3 meters*, which can be a good value or not depending
    on the scenario. However, the main obstacle to getting good results is that all
    the images used were greyscale and thus had less information than RGB or multispectral
    images. You can find all the metrics in the README of my repository.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算所有测试样本的指标后，结果显示平均绝对误差为*10.3米*，这取决于具体情况可能是一个好的值。然而，获得良好结果的主要障碍是所有使用的图像都是灰度图像，因此信息量比RGB或多光谱图像少。你可以在我仓库的README中找到所有指标。
- en: Concluding remarks
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: While I was working on this research, someone suggested predicting heights directly
    rather than predicting depths. However, experiments showed better results by predicting
    depths and then converting them to heights.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在进行这项研究时，有人建议直接预测高度而不是预测深度。然而，实验表明，通过预测深度然后转换为高度的结果更好。
- en: A demo of the project is available [here](https://huggingface.co/spaces/mattiagatti/mars_dtm_estimation)
    where you can predict a DTM and also obtain a 3D model. All the code is available
    in my [repository](https://gitlab.com/mattiagatti/hirise-monocular-depth-estimation.git)
    along with all the resources to preparing the dataset and training the model.
    You can also use my pre-trained model directly.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 项目的演示可以在[这里](https://huggingface.co/spaces/mattiagatti/mars_dtm_estimation)查看，你可以预测一个DTM并获得一个3D模型。所有代码都可以在我的[仓库](https://gitlab.com/mattiagatti/hirise-monocular-depth-estimation.git)中找到，同时还包括准备数据集和训练模型的所有资源。你也可以直接使用我预训练的模型。
- en: The idea of this project can be applied to other scenarios where you want to
    predict a DTM from satellite images.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目的想法可以应用于其他需要从卫星图像预测DTM的场景。
- en: Thanks for reading, I hope you have found this useful.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读，希望你觉得这有用。
- en: '*All images, unless otherwise noted, are by the Author.*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，否则所有图像均由作者提供。*'
- en: '[1] Doyeon K., Woonghyun K., Pyungwhan A., Donggyu J., Sehwan C., Junmo K.,
    [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://arxiv.org/abs/2201.07436)
    (2022)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Doyeon K., Woonghyun K., Pyungwhan A., Donggyu J., Sehwan C., Junmo K.,
    [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://arxiv.org/abs/2201.07436)
    (2022)'
