- en: Transformer Models For Custom Text Classification Through Fine-Tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/transformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1](https://towardsdatascience.com/transformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A tutorial on how to build a spam classifier (or any other classifier) by fine-tuning
    the DistilBERT model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://skanda-vivek.medium.com/?source=post_page-----3b065cc08da1--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----3b065cc08da1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3b065cc08da1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3b065cc08da1--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----3b065cc08da1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3b065cc08da1--------------------------------)
    ·4 min read·Jan 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b7152fa9d2fbe16e6c1e4d3080b00560.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Fine-Tuned SMS Spam Classifier Model](https://huggingface.co/skandavivek2/spam-classifier)
    Output | Skanda Vivek'
  prefs: []
  type: TYPE_NORMAL
- en: 'The [DistiBERT mode](https://arxiv.org/abs/1910.01108)l was released by the
    folks at Hugging Face, as a cheaper, faster alternative to large transformer models
    like BERT. It was [originally introduced in a blog post.](https://medium.com/huggingface/distilbert-8cf3380435b5)
    The way this model works — is by using a teacher-student training approach, where
    the “student” model is a smaller version of the teacher model. Then, instead of
    training the student on the ultimate target outputs (basically one-hot encodings
    of the label class), the model is trained on the softmax outputs of the original
    “teacher model”. This is a brilliantly simple idea, and the authors show that:'
  prefs: []
  type: TYPE_NORMAL
- en: “it is possible to reduce the size of a BERT model by 40%, while retaining 97%
    of its language understanding capabilities and being 60% faster.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Loading and Preprocessing the Data For Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, I use the [SMS spam collection dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset)
    in the UCI Machine Learning Repository and build a classifier that detects SPAM
    vs HAM (not SPAM). The data contains 5,574 rows of SMS texts that are labeled
    as SPAM or HAM.
  prefs: []
  type: TYPE_NORMAL
- en: First, I make train and validation files from the original csv and use the load_dataset
    function from the Hugging Face datasets library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to load in the DistilBERT tokenizer to preprocess the text
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prior to training, you need to map IDs to labels. After this, you need to specify
    the training hyperparameters, call trainer.train() to begin fine-tuning, and push
    the trained model to the Hugging Face hub using trainer.push_to_hub().
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: That’s it! As you can see from the Hugging Face hub, the model accuracy is pretty
    good (0.9885)!
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://huggingface.co/skandavivek2/spam-classifier?source=post_page-----3b065cc08da1--------------------------------)
    [## skandavivek2/spam-classifier · Hugging Face'
  prefs: []
  type: TYPE_NORMAL
- en: Edit model card This model is a fine-tuned version of distilbert-base-uncased
    on an unknown dataset.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: huggingface.co](https://huggingface.co/skandavivek2/spam-classifier?source=post_page-----3b065cc08da1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Model Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Inference is also relatively straightforward. You can see the output through
    running python scripts as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/78dd23f6739a2b0de887816b84ce25b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample Fine-Tuned SMS Spam Classifier Model Output | Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: 'Or run on the Hugging Face hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7793d9e19f53046bd874784b47b3e1fa.png)![](../Images/b1becd4d222f41d89b5c4f00e68aece3.png)'
  prefs: []
  type: TYPE_IMG
- en: Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: And that’s it! Hugging Face makes it very easy and accessible to adapt state
    of the art transformer models to custom language tasks as long as you have the
    data!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the GitHub link to the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/skandavivek/fine-tune-transformer-classifier?source=post_page-----3b065cc08da1--------------------------------)
    [## GitHub - skandavivek/fine-tune-transformer-classifier'
  prefs: []
  type: TYPE_NORMAL
- en: github.com](https://github.com/skandavivek/fine-tune-transformer-classifier?source=post_page-----3b065cc08da1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: If you liked this blog, check out my other blog on [fine-tuning Transformers
    for Question Answering!](https://medium.com/towards-data-science/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80)
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset*](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml].
    Irvine, CA: University of California, School of Information and Computer Science.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Almeida, T.A., GÃ³mez Hidalgo, J.M., Yamakami, A. Contributions to the Study
    of SMS Spam Filtering: New Collection and Results. Proceedings of the 2011 ACM
    Symposium on Document Engineering (DOCENG’11), Mountain View, CA, USA, 2011.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*https://huggingface.co/docs/transformers/training*](https://huggingface.co/docs/transformers/training)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*If you are not yet a Medium member and want to support writers like me, feel
    free to sign-up through my referral link:* [*https://skanda-vivek.medium.com/membership*](https://skanda-vivek.medium.com/membership)'
  prefs: []
  type: TYPE_NORMAL
- en: '*For weekly data-based perspectives* [*subscribe here*](https://skandavivek.substack.com/)*!*'
  prefs: []
  type: TYPE_NORMAL
