- en: Unsupervised Learning Series —Exploring Self-Organizing Maps
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习系列——探索自组织映射
- en: 原文：[https://towardsdatascience.com/unsupervised-learning-series-exploring-self-organizing-maps-fe2efde9f7a1](https://towardsdatascience.com/unsupervised-learning-series-exploring-self-organizing-maps-fe2efde9f7a1)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/unsupervised-learning-series-exploring-self-organizing-maps-fe2efde9f7a1](https://towardsdatascience.com/unsupervised-learning-series-exploring-self-organizing-maps-fe2efde9f7a1)
- en: Learn how Self-Organizing Maps work and why they are a useful unsupervised learning
    algorithm
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解自组织映射的工作原理以及它们为什么是有用的无监督学习算法
- en: '[](https://ivopbernardo.medium.com/?source=post_page-----fe2efde9f7a1--------------------------------)[![Ivo
    Bernardo](../Images/39887b6f3e63a67c0545e87962ad5df0.png)](https://ivopbernardo.medium.com/?source=post_page-----fe2efde9f7a1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fe2efde9f7a1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fe2efde9f7a1--------------------------------)
    [Ivo Bernardo](https://ivopbernardo.medium.com/?source=post_page-----fe2efde9f7a1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ivopbernardo.medium.com/?source=post_page-----fe2efde9f7a1--------------------------------)[![Ivo
    Bernardo](../Images/39887b6f3e63a67c0545e87962ad5df0.png)](https://ivopbernardo.medium.com/?source=post_page-----fe2efde9f7a1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fe2efde9f7a1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fe2efde9f7a1--------------------------------)
    [Ivo Bernardo](https://ivopbernardo.medium.com/?source=post_page-----fe2efde9f7a1--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fe2efde9f7a1--------------------------------)
    ·16 min read·Aug 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fe2efde9f7a1--------------------------------)
    ·阅读时间16分钟·2023年8月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5f541e88bf23d4f4e488dda83aa7536d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f541e88bf23d4f4e488dda83aa7536d.png)'
- en: Image by [teckhonc](https://unsplash.com/pt-br/@teckhonc) @Unsplash.com
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [teckhonc](https://unsplash.com/pt-br/@teckhonc) @Unsplash.com
- en: '**Self-Organizing Maps (SOMs) are a type of unsupervised neural network utilized
    for clustering** and **visualization of high-dimensional data**. SOMs are trained
    using a competitive learning algorithm, in which nodes (also known as neurons)
    in the network compete for the right to represent input data.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**自组织映射（SOMs）是一种用于聚类的无监督神经网络类型** 和 **高维数据的可视化**。SOMs通过一种竞争学习算法进行训练，在这种算法中，网络中的节点（也称为神经元）竞争表示输入数据的权利。'
- en: The SOM architecture consists of a 2D grid of nodes, where each node is associated
    with a weight vector that represents the means of the centroids in the SOM solution.
    The nodes are organized in such a way that nodes are organized around similar
    data points, producing a layer that represents the underlying data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: SOM架构由一个2D网格节点组成，其中每个节点都与一个权重向量相关，该向量表示SOM解决方案中的质心均值。节点以这样的方式组织，使得节点围绕相似的数据点组织，从而产生一个表示潜在数据的层。
- en: 'SOMs are commonly used for a wide array of tasks such as:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: SOMs通常用于各种任务，如：
- en: data visualization
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化
- en: anomaly detection
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常检测
- en: feature extraction
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取
- en: clustering
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类
- en: '**We can also visualize SOMs as the most simple neural network version for
    unsupervised learning!**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们还可以将SOMs视为最简单的无监督学习神经网络版本！**'
- en: 'While they seem confusing at first, Self-Organizing Maps (or Kohonen Maps,
    named after their [inventor](https://en.wikipedia.org/wiki/Teuvo_Kohonen)) are
    one interesting type of algorithm that is able to map the underlying structure
    from the data. They can be described as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然最初看起来有些困惑，但自组织映射（或称Kohonen映射，以其[发明者](https://en.wikipedia.org/wiki/Teuvo_Kohonen)命名）是一种有趣的算法类型，能够从数据中映射出潜在结构。它们可以描述如下：
- en: a one-layer unsupervised neural network, without backpropagation.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种单层无监督神经网络，没有反向传播。
- en: a restricted *k-means* solution, where nodes have the ability to influence the
    movement of other nodes (in the context of k-means, the nodes are known as centroids).
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种受限的 *k-means* 解决方案，其中节点有能力影响其他节点的移动（在k-means的上下文中，节点被称为质心）。
- en: In this blog post, we’ll do a couple of experiments on the SOM model. Later,
    we’ll apply a Self-Organizing Map to a real use case, where we will be able to
    see the main features and shortcomings of the algorithm.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我们将对SOM模型进行几个实验。之后，我们将把自组织映射应用于一个实际案例，在那里我们将能够看到算法的主要特性和缺陷。
- en: Understanding how SOMs Learn
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解SOM如何学习
- en: To understand how SOMs learn, let’s start by plotting a toy dataset in 2 dimensions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解SOMs如何学习，我们从在2维中绘制一个玩具数据集开始。
- en: 'We’ll create a `numpy`array with the following dataset and plot it afterwards:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个`numpy`数组，并随后绘制出来：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/ecbedb622d04b6f6b3854fe98eb3e165.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ecbedb622d04b6f6b3854fe98eb3e165.png)'
- en: Toy Dataset Plotting — Image by Author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 玩具数据集绘制 — 作者提供的图像
- en: These 8 data points are arranged using *X* and *Y* axis, representing arbitrary
    variables.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这 8 个数据点使用 *X* 和 *Y* 轴进行排列，代表任意变量。
- en: How do we start to train a Self-Organizing Map on this dataset? **The first
    important feature of SOMs is that it is an algorithm that relies on several hyper-parameters.**
    It’s very important to understand how they work and how they impact the learning
    process.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何开始在这个数据集上训练自组织映射？**SOM 的第一个重要特征是它是一个依赖于多个超参数的算法。** 了解这些超参数的工作原理及其对学习过程的影响非常重要。
- en: The first set of hyper-parameters defines the size of the SOM, commonly called
    *Lattice.* This value is similar to *k-means’s* number of centroids and to a neural
    networks layer *number of neurons*. It provides the number of elements that we
    are going to use to reduce our data points.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 第一组超参数定义了 SOM 的大小，通常称为 *网格*。这个值类似于 *k-means* 的质心数量和神经网络层的 *神经元数量*。它提供了我们将用于减少数据点的元素数量。
- en: For this example, we are going to build a simple 2 by 2 *lattice*. This means
    that we will have 4 centroids (neurons) that we will use to represent our full
    dataset.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将构建一个简单的 2 by 2 *网格*。这意味着我们将有 4 个质心（神经元）来表示我们的完整数据集。
- en: 'Learning 1: The size of the lattice is one of SOM’s hyper-parameters'
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '学习 1: 网格的大小是 SOM 的一个超参数'
- en: 'Let’s plot them:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制它们：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/71d89512276db5bb873e70f465d48cb2.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71d89512276db5bb873e70f465d48cb2.png)'
- en: 2 by 2 Lattice — Image by Author
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 2 by 2 网格 — 作者提供的图像
- en: 'In orange, we can see our *Lattice* being represented as centroids in our data.
    We can also represent these data points in “*Hidden Layer*” format, as if we are
    speaking of a Neural Network, where we can imagine that each neuron is represented
    by a value (weight) from the *X* and *Y* features:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在橙色中，我们可以看到我们的 *网格* 作为数据中的质心进行表示。我们还可以将这些数据点以“*隐藏层*”格式表示，仿佛我们在谈论一个神经网络，在这个网络中，每个神经元由
    *X* 和 *Y* 特征的值（权重）表示：
- en: '![](../Images/5e9502fafcd8577e9f44c62d5b72183a.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e9502fafcd8577e9f44c62d5b72183a.png)'
- en: 2 by 2 Lattice in Network Format — Image by Author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 2 by 2 网格的网络格式 — 作者提供的图像
- en: Black lines represent the *weight* (value) of the *x*-axis and the red dashed
    line represent the *weight* (value) of the *y*-axis.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 黑线表示 *x* 轴的 *权重*（值），红色虚线表示 *y* 轴的 *权重*（值）。
- en: 'Learning 2: We can speak about the SOM’s units as nodes (neural network terminology)
    or centroids (clustering terminology)'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '学习 2: 我们可以将 SOM 的单元称为节点（神经网络术语）或质心（聚类术语）'
- en: 'Step 1 of SOM Training: picking a random point and calculate the euclidean
    distance to each *Neuron/Centroid:*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: SOM 训练的第 1 步：选择一个随机点，并计算到每个 *神经元/质心* 的欧几里得距离
- en: '![](../Images/da7cb684cfa18cc5091184a4c1623ed7.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da7cb684cfa18cc5091184a4c1623ed7.png)'
- en: First Step of SOM Training — Image by Author
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: SOM 训练的第一步 — 作者提供的图像
- en: This data point will choose the neuron with the shortest distance as the **Best
    Matching Unit (BMU).**
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据点将选择与 **最佳匹配单元 (BMU)** 距离最短的神经元。
- en: 'Learning 3: Best Matching Unit is the unit that our data point chooses as the
    most similar to itself'
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '学习 3: 最佳匹配单元是我们的数据点选择为最相似的单元'
- en: '**This unit is the neuron that is going to be dragged towards our data point.**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**这个单元是将被拖向我们的数据点的神经元。**'
- en: But.. is it the only node that is going to move in this training iteration?
    No! All other centroids are also going to move, **given how far they are from
    the BMU**. This is a significantly different from *K-Means*, where centroids don’t
    have the ability do drag other centroids in every iteration of the algorithm.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 但是.. 是否只有这个节点会在此次训练迭代中移动？不！所有其他质心也会移动，**根据它们与 BMU 的距离**。这与 *K-Means* 有显著不同，在
    *K-Means* 中，质心没有能力在算法的每次迭代中拖动其他质心。
- en: The **BMU** and other nodeswill be dragged towards the data point. This movement
    will happen using a proximity function that will work as the “strength” that we
    will apply when pushing the nodes of our SOM.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**BMU** 和其他节点将被拖向数据点。这一移动将使用一个邻近函数，该函数将作为我们在推动 SOM 节点时应用的“力量”。'
- en: 'One of the most usual ways to define this “proximity function” is to create
    a *distance based gaussian approach*. We start by calculating the distance between
    the nodes and the **BMU**:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 定义这种“邻近函数”的一种最常见方式是创建 *基于距离的高斯方法*。我们首先计算节点与 **BMU** 之间的距离：
- en: '![](../Images/1bb1ae6226ca238a2e82bc08b570a7d5.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bb1ae6226ca238a2e82bc08b570a7d5.png)'
- en: Distance between nodes and BMU— Image by Author
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 节点与 BMU 之间的距离 — 作者图片
- en: In this case, we’re using the euclidean distance, where *distance = sqrt((x2-x1)²
    + (y2-y1)²).*
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用的是欧氏距离，其中 *distance = sqrt((x2-x1)² + (y2-y1)²)*。
- en: Now, we need to input this distance inside a gaussian kernel to consider the
    influence that the BMU will have on the other nodes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要将这个距离输入到高斯核中，以考虑 BMU 对其他节点的影响。
- en: 'This Gaussian Approximation will be based on the following function:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个高斯近似将基于以下函数：
- en: '![](../Images/487e561a2f61b665f59d65ff98346bbc.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/487e561a2f61b665f59d65ff98346bbc.png)'
- en: Gaussian Kernel example — Image by Author
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯核示例 — 作者图片
- en: The numerator is the euclidean distance between neurons and the denominator
    multiplies the standard-deviation (*σ*) by 2 . Keep in mind that this standard
    deviation is another hyperparameter of the algorithm!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 分子是神经元之间的欧氏距离，分母将标准差 (*σ*) 乘以 2。请记住，这个标准差是算法的另一个超参数！
- en: Let’s see this visually. If we consider a smaller standard deviation, a gaussian
    kernel similar to this one will “draw” the following circle around the **BMU:**
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从视觉上来看这个问题。如果我们考虑较小的标准差，一个类似于这个的高斯核将“绘制”围绕**BMU**的以下圆圈：
- en: '![](../Images/859fd0787990a77950388457c54ef923.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/859fd0787990a77950388457c54ef923.png)'
- en: Gaussian Kernel around BMU — Image by Author
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: BMU 周围的高斯核 — 作者图片
- en: The opacity of the shade on the image above is a proxy for how strong the nodes
    will be pulled towards the data point.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 上面图像的阴影不透明度是节点将被拉向数据点的力量的代理。
- en: 'If we choose a higher standard deviation, the influence of our BMU is larger
    and more nodes will be affected:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择更高的标准差，BMU 的影响会更大，更多的节点会受到影响：
- en: '![](../Images/bda239e4813e6c9b69d2be9cb5bd3f36.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bda239e4813e6c9b69d2be9cb5bd3f36.png)'
- en: Gaussian Kernel around BMU, larger standard deviation — Image by Author
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: BMU 周围的高斯核，大的标准差 — 作者图片
- en: 'Learning 4: The Standard Deviation is an Hyper-Parameter that we can configure
    on the SOM training'
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 学习4：标准差是我们可以在 SOM 训练中配置的超参数
- en: 'I’ll call the result of the gaussian formula *howclose —* for example, the
    *howclose* between our BMU node (remember that it was chosen by the data point)
    and the other nodes is the following (using *σ=1)*:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我会将高斯公式的结果称为*howclose* — 例如，*howclose* 在我们 BMU 节点（记住它是由数据点选择的）和其他节点之间的值如下（使用
    *σ=1*）：
- en: '![](../Images/0c2d3b0822b8d34c054d7d5530e3a795.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c2d3b0822b8d34c054d7d5530e3a795.png)'
- en: howclose of BMU vs. each data point — Image by Author
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: BMU 与每个数据点的 *howclose* — 作者图片
- en: We now have the **strength** that we will apply on each node when pulling it
    towards the data point. Regarding neighborhing nodes, the data point where we
    will apply most strength will be the node on the bottom-right corner. On the reverse,
    we will barely move the nodes in the top-right and top-left.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了**我们将在每个节点上施加的力量**，以将其拉向数据点。对于邻近节点，我们将施加最多力量的数据点是位于右下角的节点。相反，我们几乎不会移动位于右上角和左上角的节点。
- en: 'We know the strength that we will use to update our SOM but we still need the
    direction where we will move each node. That can be achieved with the update formula:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道了更新 SOM 时将使用的力量，但仍需要知道我们将如何移动每个节点。这可以通过更新公式来实现：
- en: '![](../Images/0c07053c88898b27da1c8ed7677a9fce.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c07053c88898b27da1c8ed7677a9fce.png)'
- en: Node Value Update Formula — Image by Author
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 节点值更新公式 — 作者图片
- en: 'Where:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*wi,k* is the current coordinates of variables *i* of the node *k*.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*wi,k* 是节点 *k* 的变量 *i* 的当前坐标。'
- en: '*howclose* is the strength that we’ve calculated before.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*howclose* 是我们之前计算的力量。'
- en: '*learningrate* is another hyper-parameter of the SOM algorithm that is similar
    to other neural networks training process.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*learningrate* 是 SOM 算法的另一个超参数，类似于其他神经网络训练过程。'
- en: '*(xi-w i,k)* is the **direction to where we will push** the node. For example,
    in our example, all nodes will be dragged down as the data point sits below every
    node (on the y-variable).'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*(xi-w i,k)* 是**我们将推动节点的方向**。例如，在我们的示例中，所有节点都会被向下拖动，因为数据点位于每个节点下方（在 y 变量上）。'
- en: '**The result of the calculation above will give us the new coordinates of the
    existing nodes in the SOM!** Another important detail: we don’t have a calculation
    for the weight to apply to the BMU (only to neighborhing nodes). **A common way
    to go around this is to apply maximum strength when dragging the BMU (output of
    gaussian function = 1).**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**上述计算的结果将为我们提供SOM中现有节点的新坐标！** 另一个重要细节：我们没有对BMU应用权重的计算（仅对邻近节点）。**一种常见的方法是应用最大强度来拖动BMU（高斯函数的输出=1）。**'
- en: 'What are the new coordinates for our nodes if we apply this logic? The calculations
    are available in the table below:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们应用此逻辑，节点的新坐标是什么？计算结果见下表：
- en: '![](../Images/c1124e28f70687ae02ca570b02bf175c.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1124e28f70687ae02ca570b02bf175c.png)'
- en: Node Value Update Summary— Image by Author
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 节点值更新摘要 — 图片由作者提供
- en: 'Let’s see the example for the bottom-right node:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看右下角节点的示例：
- en: On the *x-axis* (first row of the table above), **the new coordinate of this
    point will be 2.53*,* instead of 3**. This is the result of applying the strength
    times the learning rate and updating the node towards the data point on the x
    variable.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*x轴*（上表的第一行），**此点的新坐标将是2.53**，而不是3**。这是应用强度乘以学习率并将节点更新到x变量的数据点的结果。
- en: On the *y-axis* (fourth row of the table above), the new coordinate of this
    point will be 3.377*,* instead of 4\. This is the result of applying the strength
    times the learning rate and updating the node towards the data point on the y
    variable.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*y轴*（上表的第四行），此点的新坐标将是3.377**，而不是4**。这是应用强度乘以学习率并将节点更新到y变量的数据点的结果。
- en: Additionally, notice that the movement on the nodes on the top-right and left
    are negligible. Why? Because the *howclose* applies a low strength when pushing
    these nodes towards the data points.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，注意到右上角和左侧的节点移动微乎其微。为什么？因为*howclose*在将这些节点推向数据点时施加了较低的强度。
- en: 'How will our BMU move? Naturally, it will be dragged almost to the top of our
    data point:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的BMU将如何移动？自然，它将被拖到数据点的顶部：
- en: '![](../Images/010e24eab3d39069dee9a85acc015885.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/010e24eab3d39069dee9a85acc015885.png)'
- en: Updating BMU— Image by Author
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 更新BMU — 图片由作者提供
- en: 'Based on the coordinates we’ve obtain on the *Updated Value* table above*,*
    let’s view our “new” SOM:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们在*更新值*表中获得的坐标，让我们查看我们“新的”SOM：
- en: '![](../Images/c344db6460463b91ff8bc881c10591ae.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c344db6460463b91ff8bc881c10591ae.png)'
- en: New Nodes in the SOM — Image by Author
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: SOM中的新节点 — 图片由作者提供
- en: 'The purple nodes are the updated “SOM” nodes. Our self organizing map is trying
    to mimic the underlying data points by adjusting itself based on the random point
    we chose. Let’s see what happens in the 3 dimensional view:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 紫色节点是更新后的“SOM”节点。我们的自组织映射图试图通过根据我们选择的随机点进行调整来模拟潜在的数据点。让我们看看三维视图中发生了什么：
- en: '![](../Images/f7e59b5cc04fd102d16be966bd50d19d.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7e59b5cc04fd102d16be966bd50d19d.png)'
- en: New Nodes in the SOM — 3D View — Image by Author
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: SOM中的新节点 — 3D视图 — 图片由作者提供
- en: Notice that our SOM moved a bit when we compare it with the original random
    network and the new black and dashed lines represent the new weights of the nodes.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当我们将其与原始随机网络进行比较时，我们的SOM稍微移动了一下，新的黑色和虚线表示节点的新权重。
- en: '![](../Images/5e9502fafcd8577e9f44c62d5b72183a.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e9502fafcd8577e9f44c62d5b72183a.png)'
- en: Original Random SOM — 3D View — Image by Author
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 原始随机SOM — 3D视图 — 图片由作者提供
- en: 'Our SOM will do this process for all the data points available in the dataset
    (adjust itself for each pass). Remembering each step:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的SOM将对数据集中所有可用的数据点执行此过程（每次迭代时调整自身）。记住每一步：
- en: '*Calculate the BMU* (best matching unit) using a distance metric.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*计算BMU*（最佳匹配单元）使用距离度量。'
- en: '*Calculate the kernel around the BMU* to know how to pull other nodes in the
    SOM toward the data point.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*计算BMU周围的核*以了解如何将其他节点拉向数据点。'
- en: '*Pull the nodes using a combination of strength, learning rate and direction.*'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用强度、学习率和方向的组合来拉动节点。*'
- en: Move to the next point and repeat.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移动到下一个点并重复。
- en: Just like other neural networks model, the number of epochs in a SOM is another
    hyperparameter we need to define. An epoch consists of a single pass through the
    entire dataset.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 就像其他神经网络模型一样，SOM中的纪元数是我们需要定义的另一个超参数。一个纪元包括对整个数据集的单次遍历。
- en: 'Learning 5: The SOM will continue to adjust itself until it reaches the number
    of epochs defined by the user.'
  id: totrans-103
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 学习5：SOM将继续调整自身，直到达到用户定义的纪元数。
- en: 'In Python, we can use the *minisom* package to train a Self-Organizing-Map:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，我们可以使用*minisom*包来训练自组织映射：
- en: '[PRE2]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The parameters of the `Minisom` function are the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`Minisom`函数的参数如下：'
- en: 'The first three numbers represent the dimensions of the SOM: number of nodes
    in the *x-axis*, number of nodes in *y-axis,* and number of variables.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前三个数字表示SOM的维度：*x轴*上的节点数量，*y轴*上的节点数量，以及变量的数量。
- en: Sigma contains the starting standard deviation.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigma包含起始标准差。
- en: Then we have the starting learning rate of the algorithm (in this case, set
    to 1).
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们有算法的起始学习率（在这种情况下，设置为1）。
- en: '`neighborhood_function` defines the `gaussian` kernel for our neighborhood.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neighborhood_function`定义了我们的邻域的`gaussian`核。'
- en: '`random_seed` ensures reproducibility of our SOM.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_seed`确保我们的SOM结果可复现。'
- en: 'After fitting and training this SOM (for 100 epochs, as specified in the `som.train`
    function) where will the nodes end? Let’s see:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在对这个自组织映射（SOM）进行拟合和训练（如`som.train`函数中指定的100轮）后，节点会最终在哪里呢？让我们来看一下：
- en: '[PRE3]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/5f6ead61493920e3b99ad7fc76263829.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f6ead61493920e3b99ad7fc76263829.png)'
- en: Trained SOM — Image by Author
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后的SOM——作者图片
- en: Notice that our SOM tried to mimic the underlying data points as best as it
    could!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们的SOM尽力模仿了底层数据点的特征！
- en: 'Although you may think that this is very similar to *k-means,* let’s see the
    result from fitting a 4 centroid k-means solution on this dataset:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管你可能认为这与*k-means*非常相似，但让我们看看在这个数据集上拟合4个质心的k-means解决方案的结果：
- en: '![](../Images/79b6c08ff8c7bebaa24f2af3ed8e4d6c.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79b6c08ff8c7bebaa24f2af3ed8e4d6c.png)'
- en: Notice that the node in the top right corner was drawn toward the outlier in
    this k-means solution. **The fact that the nodes are tied to each other in the
    SOM using the neighboring function, is one of the main features of the algoritm,
    preventing nodes from being drawn away by outliers.**
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个k-means解决方案中，右上角的节点被拉向了离群值。**节点通过邻域函数连接在SOM中的这一事实，是算法的主要特征之一，防止节点被离群值拉离。**
- en: Ok, cool! We’ve had a nice introduction to SOMs, let’s now see an application
    of SOM with real data!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，很棒！我们已经对SOM有了很好的介绍，现在让我们看看SOM在实际数据中的应用！
- en: Applying SOM to real data
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将SOM应用于实际数据
- en: 'For this part of the blog post, I’ll use a dataset with information about S&P
    500 companies:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这部分博客文章中，我将使用一个包含S&P 500公司信息的数据集：
- en: '[PRE4]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here are the top 5 rows of the dataframe:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是数据框的前5行：
- en: '![](../Images/cb532983b9d05b4ece3797aeaf822c86.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb532983b9d05b4ece3797aeaf822c86.png)'
- en: financial_valuation_data top 5 rows — Image by Author
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: financial_valuation_data前5行——作者图片
- en: 'In this dataset, we can see the following columns:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中，我们可以看到以下列：
- en: '*Symbol*: Ticker of the company.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*符号*：公司的股票代码。'
- en: '*Name*: Name of the company.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*名称*：公司的名称。'
- en: '*Sector*: Sector of the company.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*行业*：公司的行业。'
- en: '*Price*: Current stock price of the company (as of 31st Dec 2014).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*价格*：公司当前的股价（截至2014年12月31日）。'
- en: '*Price/Earnings*: Ratio between price of the stock and earning-per-share.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*价格/收益*：股价与每股收益的比率。'
- en: '*Earnings/Share*: Amount of earnings per each share for the company.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*每股收益*：公司每股的收益金额。'
- en: '*52 Week Low*: Minimum stock price in the last year.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*52周低点*：过去一年中的最低股价。'
- en: '*52 Week High*: Maximum stock price in the last year.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*52周高点*：过去一年中的最高股价。'
- en: '*Market Cap*: Total value of the company, equals to the price of each share
    times the number of shares.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*市值*：公司的总价值，等于每股价格乘以股份数量。'
- en: '*EBITDA*: Earnings before interest, depreciation and amortization.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*EBITDA*：息税折旧摊销前利润。'
- en: '*Price/Sales*: Price divided by the revenue per share.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*价格/销售额*：价格除以每股收入。'
- en: '*Price/Book*: Financial valuation metric that divides the Price by the Book
    value of the company (*assets-liabilities*).'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*价格/账面价值*：将价格除以公司的账面价值（*资产-负债*）的财务估值指标。'
- en: 'Additionally, I’ll also create a variable that will emulate the volatility
    of the stock price (although this is not the theoretical formula of stock price
    volatility, it’s a simple proxy):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我还会创建一个变量来模拟股价的波动性（尽管这不是股价波动性的理论公式，但它是一个简单的代理）：
- en: '[PRE5]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Cool! So let’s start by subsetting a few columns to make our SOM simpler:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！所以我们从子集化几个列开始，使我们的SOM更简单：
- en: '[PRE6]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Can you determine how many dimensions our SOM will have based on the subset
    above?**'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**你能根据上述子集确定我们的SOM将有多少维度吗？**'
- en: '**6, the number of variables we’ve just extracted!** Our SOM from the toy example
    only had two dimensions (for visualization purposes). This new SOM will be harder
    to visualize, but luckily there are a few plots that will give us some insights
    on the trained model.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**6，我们刚提取的变量数量！** 我们的玩具示例中的SOM只有两个维度（用于可视化）。这个新的SOM将更难以可视化，但幸运的是，还有一些图可以为我们提供训练模型的洞察。'
- en: 'Another step that we didn’t go through in the toy example is the process of
    standardization. Remember that Self-Organizing Maps start by calculating distances
    between the random data point and each node? Because of that (and as we normally
    use euclidean distance), we need to standardize our data! Let’s use `sklearn''s
    StandardScaler` to achieve that:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在玩具示例中没有涉及的另一个步骤是标准化过程。记得自组织映射开始时计算随机数据点与每个节点之间的距离吗？因此（而且我们通常使用欧几里得距离），我们需要对数据进行标准化！我们将使用`sklearn's
    StandardScaler`来实现：
- en: '[PRE7]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: I’ll also impute some `nans` to 0\. As we have few `nas` in our dataset, this
    won’t hurt much.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我还会将一些`nans`填补为0。由于我们数据集中只有少量`nas`，这不会造成太大影响。
- en: 'With our dataset ready, we can try to train our `SOM` using the `MiniSom` package.
    Let’s define the following hyperparameters:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集准备好后，我们可以尝试使用`MiniSom`包训练我们的`SOM`。让我们定义以下超参数：
- en: A 15 by 15 lattice (225 nodes) — this is a large network!
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个15乘15的网格（225个节点）——这是一个大型网络！
- en: 6 features (dimensions of the SOM).
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 6个特征（SOM的维度）。
- en: A sigma of 0.5.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: σ为0.5。
- en: Learning Rate of 1.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率为1。
- en: Gaussian approximation function.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯近似函数。
- en: '[PRE8]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: And you may be thinking.. what’s the number of epochs that I should use on my
    SOM?
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想...我应该在SOM中使用多少次迭代？
- en: 'A cool way to visualize that is to train our SOM in batches and calculate the
    `quantization_error` , a metric that checks the average distance between each
    data point and it’s BMU:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一种很酷的可视化方式是分批训练SOM并计算`quantization_error`，这是一个检查每个数据点与其BMU之间平均距离的度量：
- en: '[PRE9]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This will tell us how good our SOM is at replicating the underlying data points.
    Ideally, quantization error would be zero **(almost impossible, as the goal of
    SOM is to reduce the dimensionality of our dataset and there must be some error
    involved).**
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这将告诉我们我们的SOM在复制底层数据点方面的效果如何。理想情况下，量化误差应该为零**（几乎不可能，因为SOM的目标是减少数据集的维度，并且一定会涉及一些误差）**。
- en: '![](../Images/86db4780b1cb4b3a5d48ca995627d2f7.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/86db4780b1cb4b3a5d48ca995627d2f7.png)'
- en: SOM Training Process — Image by Author
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: SOM训练过程 — 作者插图
- en: Interestingly, after ~500 epochs, our quantization error is pretty stable. This
    plot gives us an overview on how many epochs we may target for the training process.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，在约500次迭代后，我们的量化误差相当稳定。这个图给了我们一个关于训练过程中可能需要的迭代次数的概述。
- en: 'To be on the safe side, we’ll train our SOM for 1000 epochs, using the same
    parameters as before:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安全起见，我们将训练SOM 1000次，使用与之前相同的参数：
- en: '[PRE10]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We’ve just trained our SOM on this data! With that taken care of, we’ll see
    how our data points relate to the map we’ve created.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚在这些数据上训练了我们的SOM！处理完这些后，我们将查看我们的数据点如何与我们创建的地图相关。
- en: 'The first row is represented by the following array:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行由以下数组表示：
- en: '[PRE11]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/3f2dc67ab4bf3f94698c4ba6a0b3b3f2.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f2dc67ab4bf3f94698c4ba6a0b3b3f2.png)'
- en: First Row Array — Image by Author
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行数组 — 作者插图
- en: 'With a trained model, we can call the `som.winner()` function to retrieve the
    `BMU` (nearest SOM node):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 有了训练好的模型，我们可以调用`som.winner()`函数来检索`BMU`（最近的SOM节点）：
- en: '[PRE12]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output of the code above is *(2, 11)* — this means that the node that is
    closest to this data point (at the end of the SOM training) is the one that sits
    on coordinates 2,11 (remember that Python indexes start on 0):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出是*(2, 11)* — 这意味着在SOM训练结束时，距离此数据点最近的节点坐标是2,11（记住Python的索引从0开始）：
- en: '![](../Images/01607422134344cedcec28c634fd1322.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01607422134344cedcec28c634fd1322.png)'
- en: BMU of our first Data Point — Image by Author
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们第一个数据点的BMU — 作者插图
- en: 'Each node in the SOM above is characterized by an array of 6 values (the dimensions
    or number of variables). Continuing this process, let’s see what is the BMU for
    the second data point:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 上述SOM中的每个节点由一个包含6个值（维度或变量数量）的数组描述。继续这个过程，让我们看看第二个数据点的BMU是什么：
- en: '[PRE13]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/e44464378736c348ab5a5b5da1490736.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e44464378736c348ab5a5b5da1490736.png)'
- en: BMU of our Second Data Point — Image by Author
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们第二个数据点的BMU — 作者插图
- en: 'We can generalize this behavior for all data points in our dataframe:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这种行为推广到数据框中的所有数据点：
- en: '[PRE14]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'A natural follow-up question is: “can we see which nodes have more data points
    near them?” *(this is similar to k-means most representative clusters)*'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 自然的后续问题是：“我们能否看到哪些节点附近有更多的数据点？” *(这类似于 k-means 中最具代表性的簇)*
- en: '[PRE15]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/9ecbee38e17f6609bc2bc03e2e529974.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ecbee38e17f6609bc2bc03e2e529974.png)'
- en: Top 10 Nodes according to Data Point Representation — Image by Author
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据点表示的前 10 个节点 — 作者提供的图像
- en: 'Another common plot we can do with trained SOMs is the distance map. This plot
    gives us the normalized average distance between each node and it’s neighbours:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们可以用训练好的 SOM 做的常见图是距离图。这个图显示了每个节点与其邻居之间的标准化平均距离：
- en: '![](../Images/4ecc6dd863c16b908e56b37b460eab2d.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ecc6dd863c16b908e56b37b460eab2d.png)'
- en: SOM Distance Map (U-Matrix) — Image by Author
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: SOM 距离图（U-Matrix）— 作者提供的图像
- en: 'In this distance map, we understand that there are 7 or 8 nodes that are relatively
    distant (red ones) from the rest of the SOM. If those nodes have data points attached
    to them, there may be something special about them — for example:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个距离图中，我们可以理解到有 7 或 8 个节点（红色的）与其他 SOM 节点相对较远。如果这些节点上附有数据点，可能有些特殊之处——例如：
- en: '![](../Images/eed6a270ec44aacb8aa1e6c162c85d26.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eed6a270ec44aacb8aa1e6c162c85d26.png)'
- en: Distant Node in the SOM — Image by Author
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: SOM 中的远离节点 — 作者提供的图像
- en: '[PRE16]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Filtering by the companies that have this node as the BMU, we obtain the following
    data:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 通过筛选以此节点作为 BMU 的公司，我们获得了以下数据：
- en: '![](../Images/53779eb8f898ec5faab39b0f4a8ca6e0.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53779eb8f898ec5faab39b0f4a8ca6e0.png)'
- en: Companies attached to BMU node — Image by Author
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 附属于 BMU 节点的公司 — 作者提供的图像
- en: The next question is— why are these companies tied together in this node? **Probably,
    because one of the 6 dimensions that we were analyzing have a very peculiar value
    on this node.**
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个问题是——为什么这些公司在这个节点上聚集在一起？**可能是因为我们分析的 6 个维度中的一个在这个节点上有一个非常特殊的值。**
- en: 'We can check this by looking at the value of the weight of the variables for
    each node:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看每个节点的变量权重值来检查这一点：
- en: '![](../Images/fc35342d6df59069337356ad79970de5.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc35342d6df59069337356ad79970de5.png)'
- en: Weight for Each variable by Node — Image by Author
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点的变量权重 — 作者提供的图像
- en: '**This is another useful plot to visualize trained SOMs as it shows the influence
    of variables in the SOM.**'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**这是另一种可视化训练好的 SOM 的有用图表，因为它显示了变量在 SOM 中的影响。**'
- en: '**For L Brands and Phillip Morris, it seems that they have a very values of
    Price to Book Ratio (> 1000) — this can be seen in the influence of Price/Book
    in the node**. This may be something that requires further investigation, particularly
    because the average book/value of our dataset is 14.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**对于 L Brands 和 Philip Morris，它们的市净率值似乎非常高（> 1000）——这可以在节点的价格/账面价值影响中看到**。这可能需要进一步的调查，特别是因为我们数据集的平均账面价值是
    14。'
- en: 'Let’s investigate more about our weights in the SOM by looking at other variables,
    for example checking the node areas with unusual price to earnings ratio:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过查看其他变量来进一步探讨 SOM 中的权重，例如检查具有异常市盈率的节点区域：
- en: '![](../Images/0f4ccfdb2cb69f3d24f90ff9624c82fc.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f4ccfdb2cb69f3d24f90ff9624c82fc.png)'
- en: PE Ratio Weights on the SOM Node
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: PE 比率在 SOM 节点上的权重
- en: 'Starting with the high values of the nodes in the red region:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 从红色区域中的高值节点开始：
- en: '[PRE17]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](../Images/32660e173b86563248bb985c0b020591.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32660e173b86563248bb985c0b020591.png)'
- en: Companies with High P/E Ratios — Image by Author
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 市盈率高的公司 — 作者提供的图像
- en: 'On the other hand, let’s see the blue shaded area, where we expect to have
    companies with small P/E ratios:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，让我们看看蓝色阴影区域，我们预计在这里会有市盈率较低的公司：
- en: '[PRE18]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/e43a2b935f8fe525cfdedeced4c7365e.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e43a2b935f8fe525cfdedeced4c7365e.png)'
- en: Companies with Low P/E Ratios — Image by Author
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 市盈率低的公司 — 作者提供的图像
- en: We only have one company associated with these nodes, with a very low P/E ratio!
    One interesting thing is that the node with this data point is that it is also
    flagged as distant from other nodes in the `distance_map` .
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只有一家公司与这些节点相关联，并且市盈率非常低！有趣的是，具有此数据点的节点在 `distance_map` 中也被标记为远离其他节点。
- en: Having a trained SOM helps us understand the underlying structure of the data,
    particularly on how data points cluster around these centroids and what variables
    relate to them. Another common thing do with SOMs is to try to understand how
    different categories cluster around nodes. For example, are we able to capture
    any industry trend on the trained SOM?
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有训练好的SOM可以帮助我们理解数据的潜在结构，特别是数据点如何围绕这些质心进行聚集，以及哪些变量与这些质心相关。另一个常见的做法是尝试理解不同类别如何围绕节点进行聚集。例如，我们能否在训练好的SOM上捕捉到任何行业趋势？
- en: 'We can plot the industry of every data point inside each node to assess that:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在每个节点内部绘制每个数据点的行业，以评估以下内容：
- en: '![](../Images/420a1f3bb2217b654e68314691ab67ee.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/420a1f3bb2217b654e68314691ab67ee.png)'
- en: Data Points by Industry in the SOM — Image by Author
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: SOM中按行业分类的数据点 — 作者提供的图片
- en: Each square represents a SOM node and inside the square we plot each data point
    attached to that node. With the variables that we have chosen for the network,
    can we identify a trend on any industry?
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 每个方块代表一个SOM节点，在方块内部，我们绘制了附加到该节点的每个数据点。根据我们为网络选择的变量，我们能否识别出任何行业趋势？
- en: '*(Spoiler: there’s actually two industries where our SOM may be able to identify
    and cluster industries together very well).*'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*（剧透：实际上有两个行业，我们的SOM可能能够很好地识别并将行业聚集在一起）。*'
- en: 'Let’s subset “*Real Estate*” and “*Utilities*” companies on the plot above:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在上面的图中子集“*房地产*”和“*公用事业*”公司：
- en: '![](../Images/16c1ef2433889c5e4bd9958329cc1cbe.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16c1ef2433889c5e4bd9958329cc1cbe.png)'
- en: Real Estate and Utilities Companies in the SOM — Image by Author
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: SOM中的房地产和公用事业公司 — 作者提供的图片
- en: 'Look how nicely our data points cluster together in few nodes! If we compare
    this with the “I*nformation Technology*” industry:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 看看我们的数据点在少数节点中如何紧密聚集！如果我们将其与“*信息技术*”行业进行比较：
- en: '![](../Images/58ef6f11dbc72a98ebe005d670ad1325.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58ef6f11dbc72a98ebe005d670ad1325.png)'
- en: Information Technology Companies in the SOM — Image by Author
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: SOM中的信息技术公司 — 作者提供的图片
- en: If we wanted to do some supervised model on top of this data (predicting industry),
    this SOM would give us some good insights on why we would have trouble predicting
    the “Information Technology” class with the variables that we are using.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想在这些数据上进行一些监督模型（预测行业），这个SOM会为我们提供一些关于为什么我们会在使用的变量下预测“信息技术”类别时遇到困难的有益见解。
- en: For more examples, check the awesome [JustGlowing](https://github.com/JustGlowing)
    (creator of the minisom package) [example](https://github.com/JustGlowing/minisom/tree/master/examples)
    repo.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多示例，请查看很棒的[JustGlowing](https://github.com/JustGlowing)（minisom包的创作者）[示例](https://github.com/JustGlowing/minisom/tree/master/examples)库。
- en: Conclusion
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Thank you for taking the time to read this post!
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您花时间阅读这篇帖子！
- en: SOMs are very interesting overlooked algorithms able to detail the structure
    of our dataset. They may help in a variety of tasks, such as detecting outliers
    (in large SOMs) or building clustering algorithms that are robust to them (smaller
    SOMs). SOMs have historically been used in diverse problems such as [Image Compression](https://ieeexplore.ieee.org/abstract/document/668891?casa_token=Skb2_ne1VHEAAAAA%3AwSrP8MPLH9xMcSVYVuCzJh3-RwmUKiYj0GK1evhBKcgo_dRqiMbgubabTSUE1bz310-FP-y3bg),
    [Detection Systems](https://ieeexplore.ieee.org/abstract/document/8370435?casa_token=f_sMzz7FmjsAAAAA%3AXQvBv1DA39RMjl5jAIio1YhKU0eTI8ry0544AfpFB3CbRECrstASiXbWofxN2pyXEakXOW7TLw)
    and in problems related to [GeoSciences](https://d1wqtxts1xzle7.cloudfront.net/40037324/00463515e8c54d6911000000.pdf20151115-68247-iz3ok7-libre.pdf?1447636982=&response-content-disposition=inline%3B+filename%3DThe_self_organizing_map_the_Geo_SOM_and.pdf&Expires=1691321052&Signature=FI53U9nEQTeZtjWlnZH4ACWHUtK66MCmnjHKHr846t9BY3HbZJE%7ElC%7Ehwoj6UB1Y6iGL2RZhwK8IXjnAYcRhf%7Eh28iEc2r1heLxNT8XCZoM9eNGq0QOQTuzFfkh2YcBSwe7oZVnPVCXhwTEK6-QS9sYdB5tK7moddNxJegksJmvhGx%7EUsrCq4IPEevn80LjDJYX1PadWEAng9W4FDUslrPikl5PeKvaXndrg0ItGp8MbP-6jpwJ7svwWTkJMLvDGn1fSQtVJn%7EJjPBEFL4fZHLA40kURkejx0BtK4dZHfLeB7UUkY2MPBjKcdgzMjF2Xw1xbY3LqpZEIehmohMl8wQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: SOMs 是非常有趣的被忽视的算法，能够详细描述我们数据集的结构。它们可以帮助完成多种任务，例如检测异常值（在大型 SOMs 中）或构建对异常值具有鲁棒性的聚类算法（较小的
    SOMs）。SOMs 在历史上已被用于解决各种问题，如 [图像压缩](https://ieeexplore.ieee.org/abstract/document/668891?casa_token=Skb2_ne1VHEAAAAA%3AwSrP8MPLH9xMcSVYVuCzJh3-RwmUKiYj0GK1evhBKcgo_dRqiMbgubabTSUE1bz310-FP-y3bg)、[检测系统](https://ieeexplore.ieee.org/abstract/document/8370435?casa_token=f_sMzz7FmjsAAAAA%3AXQvBv1DA39RMjl5jAIio1YhKU0eTI8ry0544AfpFB3CbRECrstASiXbWofxN2pyXEakXOW7TLw)
    和与 [地球科学](https://d1wqtxts1xzle7.cloudfront.net/40037324/00463515e8c54d6911000000.pdf20151115-68247-iz3ok7-libre.pdf?1447636982=&response-content-disposition=inline%3B+filename%3DThe_self_organizing_map_the_Geo_SOM_and.pdf&Expires=1691321052&Signature=FI53U9nEQTeZtjWlnZH4ACWHUtK66MCmnjHKHr846t9BY3HbZJE%7ElC%7Ehwoj6UB1Y6iGL2RZhwK8IXjnAYcRhf%7Eh28iEc2r1heLxNT8XCZoM9eNGq0QOQTuzFfkh2YcBSwe7oZVnPVCXhwTEK6-QS9sYdB5tK7moddNxJegksJmvhGx%7EUsrCq4IPEevn80LjDJYX1PadWEAng9W4FDUslrPikl5PeKvaXndrg0ItGp8MbP-6jpwJ7svwWTkJMLvDGn1fSQtVJn%7EJjPBEFL4fZHLA40kURkejx0BtK4dZHfLeB7UUkY2MPBjKcdgzMjF2Xw1xbY3LqpZEIehmohMl8wQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA)
    相关的问题。
- en: When compared with [*K-Means*](/unsupervised-learning-method-series-exploring-k-means-clustering-d129fff3ab6a?sk=0618fadb196a71ce4b9f378f9ee83107)
    and [*Hierarchical Clustering*](/unsupervised-learning-series-exploring-hierarchical-clustering-15d992467aa8?sk=87714b0d7aa252330dd7a8d3b1c620f4),
    Self-Organizing Maps tend to be more robust to outliers, particularly for the
    same number of clusters (don’t confuse this with the fact that large SOMs are
    able to identify edge cases in the data).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 与 [*K-Means*](/unsupervised-learning-method-series-exploring-k-means-clustering-d129fff3ab6a?sk=0618fadb196a71ce4b9f378f9ee83107)
    和 [*层次聚类*](/unsupervised-learning-series-exploring-hierarchical-clustering-15d992467aa8?sk=87714b0d7aa252330dd7a8d3b1c620f4)
    相比，自组织映射在对异常值的鲁棒性方面通常更强，特别是在相同数量的聚类时（不要混淆这一点与大型 SOMs 能识别数据中的边界情况的事实）。
- en: 'In a nutshell, you can use them for a variety of data science tasks, such as:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，你可以将它们用于各种数据科学任务，例如：
- en: Pre-processing steps (identifying outliers using Large SOMs, understanding high
    correlation between features).
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理步骤（使用大型 SOMs 识别异常值，理解特征之间的高相关性）。
- en: Understand your features ability to predict categories.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解你的特征预测类别的能力。
- en: Build clustering algorithms.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建聚类算法。
- en: 'If you enjoyed this post, make sure to read my other posts on the Unsupervised
    Learning series:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章，确保阅读我在无监督学习系列中的其他文章：
- en: '[K-Means](/unsupervised-learning-method-series-exploring-k-means-clustering-d129fff3ab6a?sk=0618fadb196a71ce4b9f378f9ee83107)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[K-Means](/unsupervised-learning-method-series-exploring-k-means-clustering-d129fff3ab6a?sk=0618fadb196a71ce4b9f378f9ee83107)'
- en: '[Hierarchical Clustering](/unsupervised-learning-series-exploring-hierarchical-clustering-15d992467aa8?sk=87714b0d7aa252330dd7a8d3b1c620f4)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[层次聚类](/unsupervised-learning-series-exploring-hierarchical-clustering-15d992467aa8?sk=87714b0d7aa252330dd7a8d3b1c620f4)'
- en: '*The dataset we’ve used in this post is under Open Data Commons License and
    is available* [*here*](https://github.com/datasets/s-and-p-500-companies-financials/tree/main)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们在这篇文章中使用的数据集采用了开放数据共享许可，并且可以在* [*这里*](https://github.com/datasets/s-and-p-500-companies-financials/tree/main)
    获取。'
