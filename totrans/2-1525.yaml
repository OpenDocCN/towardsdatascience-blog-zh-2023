- en: MLOps with Optuna
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/mlops-with-optuna-b7c52d931b4b](https://towardsdatascience.com/mlops-with-optuna-b7c52d931b4b)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Don’t waste your time, use Optuna
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://zjwarnes.medium.com/?source=post_page-----b7c52d931b4b--------------------------------)[![Zachary
    Warnes](../Images/c05c5245aa640e016f730ef7c258a424.png)](https://zjwarnes.medium.com/?source=post_page-----b7c52d931b4b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b7c52d931b4b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b7c52d931b4b--------------------------------)
    [Zachary Warnes](https://zjwarnes.medium.com/?source=post_page-----b7c52d931b4b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b7c52d931b4b--------------------------------)
    ·7 min read·Mar 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52d8b8b6c3b6e3792658e18a40051b48.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated with DALLE-2, hyper-parameter optimization (photo by author)
  prefs: []
  type: TYPE_NORMAL
- en: For anyone familiar with the arduous process of hyperparameter tuning, Optuna
    is a lifesaver.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to tune a range of models using different hyperparameter optimization
    techniques is nothing short of amazing.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re still tuning your models through grid search you need to change your
    approach — You’re losing performance
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This article contains ready-to-use code you can implement right away. Fast forward
    to the end of the article if you want to experiment yourself. Don’t forget to
    load the functions throughout the article. This post will discuss hyperparameter
    tuning at a high level before diving into Optuna.
  prefs: []
  type: TYPE_NORMAL
- en: It will outline how you can create new studies with custom parameter grids and
    metrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It will showcase how to save and load studies and gather the best trials and
    models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And finally, it will show how to fork studies to continue searches with updated
    search spaces.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter Tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When learning machine learning you will be confronted with hyperparameter tuning
    which dictates the structure of the model you optimize. The most common method
    is grid search, where permutations of parameters are used to train and test models.
  prefs: []
  type: TYPE_NORMAL
- en: Grid search is wildly inefficient. Both in terms of wasting time and exploring
    less of your hyperparameter space.
  prefs: []
  type: TYPE_NORMAL
- en: The result is a worse-performing model.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple ways to improve over brute force grid searches. I’ve outlined
    exactly why different search methods, including random, outperform grid searches.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/hyperparameter-tuning-always-tune-your-models-7db7aeaf47e9?source=post_page-----b7c52d931b4b--------------------------------)
    [## Hyperparameter Tuning — Always Tune your Models'
  prefs: []
  type: TYPE_NORMAL
- en: Don’t leave free performance gains on the table.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/hyperparameter-tuning-always-tune-your-models-7db7aeaf47e9?source=post_page-----b7c52d931b4b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Essentially don’t use grid search. It takes too long to analyze your hyperparameter
    search space.
  prefs: []
  type: TYPE_NORMAL
- en: What matters more is how to manage the different models you create with more
    effective Bayesian techniques like *‘Tree-structured Parzen Estimators’.*
  prefs: []
  type: TYPE_NORMAL
- en: But you’ll quickly find that you’re rapidly creating and saving more and more
    models. Thus, you’ll find yourself needing to track, store, and monitor the different
    models you’ve optimized.
  prefs: []
  type: TYPE_NORMAL
- en: Optuna
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Optuna is a hyperparameter optimization framework for tuning models. It lets
    you understand how hyperparameters affect your model and improves your model performance.
  prefs: []
  type: TYPE_NORMAL
- en: I previously wrote about how you can use this library to quickly optimize models
    with incredibly large hyperparameter spaces with some ease.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/hyper-parameter-optimization-with-optuna-4920d5732edf?source=post_page-----b7c52d931b4b--------------------------------)
    [## Hyper-Parameter Optimization with Optuna'
  prefs: []
  type: TYPE_NORMAL
- en: How to generate the optimal version of your model.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/hyper-parameter-optimization-with-optuna-4920d5732edf?source=post_page-----b7c52d931b4b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many samplers available to tune your models. It still contains the
    standard grid search and random search models. But, in addition, you can also
    choose :'
  prefs: []
  type: TYPE_NORMAL
- en: Tree-structured Parzen Estimator (used in this article)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Quasi-Monte Carlo Sampler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Intersection Search Space Sampler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And a half dozen more options, all of which more systematically search through
    your hyperparameter space.
  prefs: []
  type: TYPE_NORMAL
- en: Each optimization within Optuna takes the form of a study. These studies track
    many different components of the hyperparameter optimization process. They let
    you view performance at different steps, view the effects of certain hyperparameters,
    or select models from the best trails.
  prefs: []
  type: TYPE_NORMAL
- en: Study Templates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One issue with studies is the fixed parameter grids. This is a limitation of
    the optimization function required for the study. In Optuna tutorials you’ll see
    this function must follow a standard format to track trials.
  prefs: []
  type: TYPE_NORMAL
- en: The optimization function does not allow a user to pass in different models
    on the fly. It also does not allow different parameter grids to the optimization
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '**But**, there is a workaround which allows variable parameter grids and variable
    models. In comes lambda functions.'
  prefs: []
  type: TYPE_NORMAL
- en: By defining our optimization functions as lambdas, we can pass in multiple values.
    This lambda then calls an underlying function. The result is a much more robust
    and flexible study setup.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic Model Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Below I’ve defined a few functions that act as wrappers for the underlying Optuna
    functions. These wrappers all you to quickly pass in different models and parameter
    grids without having to define whole new optimization functions each time.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you can simply create different parameter grids for different experiments
    in python dictionaries and pass in whatever models you may like.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic Optuna Study Initialization (Code by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can also see that the models will be uniquely saved whenever a model
    name isn’t specifically passed in.
  prefs: []
  type: TYPE_NORMAL
- en: Since you can quickly update your search spaces and models, the number of studies
    rapidly increases. Therefore, loading, renaming, forking and otherwise tracking
    models become more problematic.
  prefs: []
  type: TYPE_NORMAL
- en: Load model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Optuna studies are stored in a *db* file which can be loaded using their *load_study*
    function. This function also provides the opportunity to change the sampler used
    in the underlying optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Load Optuna Studies from Local Database (Code by Author)
  prefs: []
  type: TYPE_NORMAL
- en: '**Analyze Studies**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After generating a slew of models from your hyperparameter samples, what’s next
    is to analyze your results.
  prefs: []
  type: TYPE_NORMAL
- en: Below I’ve defined some more functions to help.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes the best model isn’t always the top performer. For various reasons,
    your target metrics and goal problem may add additional layers of complexity that
    require handling.
  prefs: []
  type: TYPE_NORMAL
- en: With the function below you can grab the top *n* models to review.
  prefs: []
  type: TYPE_NORMAL
- en: Get Top Trials from an Optuna Study (Code by Author)
  prefs: []
  type: TYPE_NORMAL
- en: After running studies, and determining your best models you’ll need to set a
    model into production.
  prefs: []
  type: TYPE_NORMAL
- en: To accomplish this you can identify your ideal study and retrieve the parameters
    of the best underlying model with the following function.
  prefs: []
  type: TYPE_NORMAL
- en: Get Best Parameters from Optuna Study (Code by Author)
  prefs: []
  type: TYPE_NORMAL
- en: These best trail parameters are a dictionary of parameters. These can then be
    loaded into the model you’re using with the double star ‘**’ python operator.
  prefs: []
  type: TYPE_NORMAL
- en: Fork Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes your model is making progress but you haven’t quite run the model
    long enough. This is an easy fix for model optimization. But, this is a more difficult
    problem for hyperparameter optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, with limited adjustment, you can load old studies and continue
    your search. Moreover, upon the analysis, you may find that your best models are
    within a certain range of hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: With the fork function, you can split off your studies and explore different
    hyper-parameter grids.
  prefs: []
  type: TYPE_NORMAL
- en: Think your learning rate isn’t quite low enough? Adjust the parameter grid and
    keep running. The underlying objective model for the hyperparameter search continues
    to optimize when continuing the study.
  prefs: []
  type: TYPE_NORMAL
- en: Fork Existing Optuna Study (Code by Author)
  prefs: []
  type: TYPE_NORMAL
- en: '**Rename Study**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just to iron out some other utilities which might come in handy, I’ve also created
    a study rename function. With so many studies, the best models may get lost in
    the mix.
  prefs: []
  type: TYPE_NORMAL
- en: Using the parse studies function defined above along with the rename study function
    below you can easily run a bulk search across different models and parameter grids.
  prefs: []
  type: TYPE_NORMAL
- en: Then once you’ve found a great model you can quickly rename these and their
    underlying data storage to keep track of your progress.
  prefs: []
  type: TYPE_NORMAL
- en: Rename Existing Optuna Study and Local Database (Code by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To showcase this dynamic optimization I’ve set up a brief experiment which you
    can re-run and change with your datasets. This code is intended to be used directly
    by modifying the dataset. After which you can adjust the model and it’s own respective
    hyperparameters. The aim here was to use this code to support my own MLOps pipelines
    to build, optimize, and track many different models.
  prefs: []
  type: TYPE_NORMAL
- en: Data used
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dataset used is the open-source toy diabetes dataset from scikit-learn.
    This dataset comes standard with your installation. This is a regression dataset
    with numerical variables and a numerical target. Perfect to showcase how to set
    up these Optuna studies.
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’ll need to specify a name for the study and the number of trails that you
    want to optimize. For my brief example, I’m showcasing a light gradient boosting
    model. These models have a lot of hyperparameters making them difficult to tune.
  prefs: []
  type: TYPE_NORMAL
- en: Take the time to review the parameter distributions used as they highlight the
    different ways you can search over a hyperparameter space. There are various distributions
    which you can use to fine tune how new candidate values are sampled.
  prefs: []
  type: TYPE_NORMAL
- en: Run Dynamic Optuna Study (Code by Author)
  prefs: []
  type: TYPE_NORMAL
- en: After completing your hyper-parameter search with the study you can view your
    results. Besides the functions I’ve defined, you may also find the *trails_dataframe()*
    function to be useful. This function simply returns all the study details as a
    dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to load the best parameters from either the best trail to use.
    Or, one of the top trials and use these in a model.
  prefs: []
  type: TYPE_NORMAL
- en: This is a straightforward process, with the *set_params()* function of your
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Display Results of Dynamic Optuna Study (Code by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Continued Studies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Optuna is set up to create a parameter grid for a study and optimize over a
    series of trials.
  prefs: []
  type: TYPE_NORMAL
- en: Something you may have also done is reload your studies and continue to optimize
    them. But, if you’ve already exhausted your search space, your results may not
    improve much.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, with my dynamic study setup, you can load an existing trial, fork the trial,
    change your parameter grid, and continue your hyper-optimization search.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, you can **only update numerical parameter distributions** in
    your hyper-parameter optimization. This issue appears to be a current limitation
    of Optuna.
  prefs: []
  type: TYPE_NORMAL
- en: But you can adjust your hyper-parameter setting to a range completely outside
    the start range. Thus, you can pick up right where you left off or explore a new
    distribution entirely.
  prefs: []
  type: TYPE_NORMAL
- en: Fork and Continue Optimization of Optuna Study (Code by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Optuna is a powerful tool. Top data scientists flock to it, and for good reason.
  prefs: []
  type: TYPE_NORMAL
- en: It saves time, and it produces better models.
  prefs: []
  type: TYPE_NORMAL
- en: My goal for this article was to create some of the utility functions to support
    better MLOps for data science teams. It is often easy to lose track of different
    models and different setups used to create and train each model.
  prefs: []
  type: TYPE_NORMAL
- en: But by offloading the tracking of models to existing tools you can save your
    team valuable time and effort.
  prefs: []
  type: TYPE_NORMAL
- en: '*If you’re interested in reading articles about novel data science tools and
    understanding machine learning algorithms, consider following me on Medium. I
    always include code in my articles that you can apply to your work!*'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you’re interested in my writing and want to support me directly, please
    subscribe through the following link. This link ensures that I will receive a
    portion of your membership fees.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://zjwarnes.medium.com/membership?source=post_page-----b7c52d931b4b--------------------------------)
    [## Join Medium with my referral link - Zachary Warnes'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Zachary Warnes (and thousands of other writers on Medium).
    Your membership fee directly supports…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: zjwarnes.medium.com](https://zjwarnes.medium.com/membership?source=post_page-----b7c52d931b4b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
