- en: Improving Performance and Explainability of Zero-Shot CLIP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/improving-performance-and-explainability-of-zero-shot-clip-33e579d3f4bb](https://towardsdatascience.com/improving-performance-and-explainability-of-zero-shot-clip-33e579d3f4bb)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Part 2 — Visual classification via description from LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@alexml0123?source=post_page-----33e579d3f4bb--------------------------------)[![Alexey
    Kravets](../Images/3b31f9b3c73c6c7ca709f845e6f70023.png)](https://medium.com/@alexml0123?source=post_page-----33e579d3f4bb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----33e579d3f4bb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----33e579d3f4bb--------------------------------)
    [Alexey Kravets](https://medium.com/@alexml0123?source=post_page-----33e579d3f4bb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----33e579d3f4bb--------------------------------)
    ·6 min read·Nov 25, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: This is the second part of a series on enhancing Zero-Shot CLIP performance.
    In the first part, I provided a detailed explanation of how the CLIP model operates
    and described a straightforward method to improve its performance. This involved
    extending standard prompts like *“A picture of {class}”* with customized prompts
    generated by a large language model (LLM). If you haven’t already, you can find
    part 1 [here](https://medium.com/towards-data-science/simple-way-of-improving-zero-shot-clip-performance-4eae474cb447).
    In this article we will present a relatively similar method to improve zero-shot
    CLIP performance which is additionally highly explainable.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CLIP model is an impressive zero-shot predictor, enabling predictions on
    tasks it hasn’t explicitly been trained for. Despite its inherent capabilities,
    there exist several strategies to notably improve its performance. In the first
    article we have seen one of these strategies, however, while achieving enhanced
    performance is valuable, there are instances where we might be willing to make
    trade-offs to prioritize better explainability. In this second article of our
    series we will explore a method that not only enhances the performance of the
    zero-shot CLIP model but also ensures that its predictions are easily understandable
    and interpretable.
  prefs: []
  type: TYPE_NORMAL
- en: Explainability in Deep Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Various explainability techniques are available for deep learning models today.
    In a [previous article](https://medium.com/towards-data-science/integrated-gradients-from-scratch-b46311e4ab4),
    I delved into Integrated Gradients, a method that tells how each feature of an
    input influences the output of a machine learning model, especially deep neural
    networks. Another popular approach for model interpretation relies on Shap values,
    where we assign the contribution of each feature to the model’s output based on
    concepts from cooperative game theory. While these methods are versatile and can
    be applied to any deep learning model, they can be somewhat challenging to implement
    and interpret. CLIP, which has been trained to map image and text features into
    the same embedding space, provides an alternative explainability method based
    on text. This approach is more user-friendly and offers easy interpretability,
    providing a different perspective on model explanation.
  prefs: []
  type: TYPE_NORMAL
- en: Quick refresh of the problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As a quick refresh from the first part of this series, the problem we are tackling
    here is to predict the class of the image displayed below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5bd2eb765643b658df5c6cd4e04b9d8d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Tree Frog image from [FreeImages](https://images.freeimages.com/images/large-previews/342/green-tree-frog2-1616738.jpg)
    (license: [https://www.freeimages.com/license](https://www.freeimages.com/license))'
  prefs: []
  type: TYPE_NORMAL
- en: A standard method of using a simple prompt *“Picture of a {class}”* gives the
    wrong answer predicting *“tailed frog”* with 0.68 probability score*:*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let’s now see how we can improve it.
  prefs: []
  type: TYPE_NORMAL
- en: Visual classification via Description from LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To improve the predictive accuracy of zero-shot CLIP, we are going to implement
    a similar idea to what we discussed in the first article. However, this time,
    rather than providing generic prompts for a class like *“tree frog”* such as *“The
    identifying characteristics of a tree frog vary depending on the species, but
    some common features include large adhesive toes, protruding eyes, and bright
    colors”* we will separate it into specific **descriptive features**. For example,
    considering the *“tree frog”* and *“tailed frog”* classes descriptive features
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tree frog*:'
  prefs: []
  type: TYPE_NORMAL
- en: “Protruding eyes”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Large mouth”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Without a tail”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Bright green color”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tailed frog*:'
  prefs: []
  type: TYPE_NORMAL
- en: “Tiny eyes”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Small mouth”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Dark color”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Has long tail”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*These features can be again generated using a LLM with a prompt like:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: What are useful features for distinguishing a {class} in a photo?'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: There are several useful visual features to tell there is a {class} in a
    photo:'
  prefs: []
  type: TYPE_NORMAL
- en: -*
  prefs: []
  type: TYPE_NORMAL
- en: The *“-”* is important as it will force the model to generate a list of features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, similarly to what we have done in the first article, to classify the
    image of a frog we take the average vector embedding of these textual features
    descriptions that represent each class in the multi-modal space and evaluate which
    average vector is the closest to the test image we want to classify. In code,
    we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'First of all, we observe that our prediction is now accurate and the model
    correctly identifies the class as *“tree frog”*.Although we achieved the right
    classification result also with the method in part 1 of this series, there is
    a notable distinction in this method — it offers high explainability. Rather than
    simply taking the average of the features’ descriptions we can examine the non-standardized
    scores *S(feature)* for each feature description. This allows us to understand
    why the model predicted a particular class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: S(“protruding eyes”) = 25.5400 > S(“tiny eyes”) = 24.0911;
  prefs: []
  type: TYPE_NORMAL
- en: S(“large mouth”) = 22.6840 > S(“small mouth”) = 22.3996;
  prefs: []
  type: TYPE_NORMAL
- en: S(“without a tail”) ~ S(“has no tail”) are similar probably because the tail
    is
  prefs: []
  type: TYPE_NORMAL
- en: not visible in the picture;
  prefs: []
  type: TYPE_NORMAL
- en: S(“bright green colour”) = 25.9017 > S(“dark colour”)= 21.0066;
  prefs: []
  type: TYPE_NORMAL
- en: The scores for features belonging to the *“tree frog”* class are higher than
    those for the features describing the *“tailed frog”* class. Analysing these feature
    scores helps us understand **why** the model predicted a certain class. In this
    example, very high scores were given to features like “protruding eyes,” “bright
    green colour,” and “large mouth,” providing a clear explanation for the predicted
    class. This level of explainability was not available in the method described
    in the first part because the generated prompts were quite generic and contained
    sentences that included different concepts. Changing prompts to simple feature
    descriptions gives us the best of both worlds — high accuracy and great explainability.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the second part of our series we have seen how to improve the standard prompt
    *“Picture of a {class}”* boosting performance. This solution is not only scalable,
    as LLMs can generate descriptive features for any number of classes and datasets,
    but it is also highly explainable. In the upcoming articles, we will explore few-shot
    learning methods that leverage few-shot image examples for each class to achieve
    higher accuracy than zero-shot methods.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] [CLIP (huggingface.co)](https://huggingface.co/docs/transformers/model_doc/clip)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [https://openreview.net/pdf?id=jlAjNL8z5cs](https://openreview.net/pdf?id=jlAjNL8z5cs)'
  prefs: []
  type: TYPE_NORMAL
