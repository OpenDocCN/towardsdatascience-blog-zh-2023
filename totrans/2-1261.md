# 如何使用Chat-GPT和Python在Neo4j中基于你自己的文章构建知识图谱

> 原文：[https://towardsdatascience.com/how-to-use-chat-gpt-and-python-to-build-a-knowledge-graph-in-neo4j-based-on-your-own-articles-c622bc4e2eaa](https://towardsdatascience.com/how-to-use-chat-gpt-and-python-to-build-a-knowledge-graph-in-neo4j-based-on-your-own-articles-c622bc4e2eaa)

## 一个包含120多篇关于数学和数据科学的文章结构化知识的图形

[](https://kaspermuller.medium.com/?source=post_page-----c622bc4e2eaa--------------------------------)[![Kasper Müller](../Images/336cad595544ba68e3376a038d44df62.png)](https://kaspermuller.medium.com/?source=post_page-----c622bc4e2eaa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c622bc4e2eaa--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c622bc4e2eaa--------------------------------) [Kasper Müller](https://kaspermuller.medium.com/?source=post_page-----c622bc4e2eaa--------------------------------)

·发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c622bc4e2eaa--------------------------------) ·8分钟阅读·2023年8月26日

--

![](../Images/c43ea8409a8a04894916f41afe7075dc.png)

作者截图

在这篇文章中，我将展示如何利用图形技术和一些编程来结构化和探索你自己的文章内容。

使用NLP技术来结构化非结构化数据的想法并不新鲜，然而，最新的大型语言模型（LLMs）的进展激发了无数机会。Chat-GPT的普及技术使得业余爱好者对LLMs和生成模型产生了很多关注。

实际上，生成式AI已经成为许多公司议程中的一部分！

在这篇文章中，我们将通过编程语言Python使用OpenAI的开发者API来处理技术。我们将处理来自Medium的数据（哈哈，元数据？），并构建一个知识图谱。听起来可能有些复杂，但实际上开始时非常简单。

# 开始

首先，攻击计划如下。

1.  使API正常工作并通过Python访问它。

1.  使用示例文本进行提示工程，以确保GPT-4模型理解你的需求。

1.  从Medium下载你的文章（如果你愿意，也可以使用其他文本），并对数据进行预处理。

1.  提取和收集Chat-GPT的输出。

1.  对Chat-GPT的输出进行后处理

1.  编写代码使用Cypher查询语言进一步将数据结构化为图形。

1.  玩弄你的新伙伴并探索你的文章。

话不多说，让我们通过快速设置基础技术来开始吧。

# 设置

我们需要在本地计算机上安装编程语言Python和图形数据库Neo4j。

首先，你需要确保你在 OpenAI 有一个付费账户，以便可以使用 GPT-4。其次，你应该确保你已[注册](https://platform.openai.com/account/billing/overview) API 使用。一旦准备好，你需要生成一个[API 密钥](https://platform.openai.com/account/api-keys)。然后你需要 *pip install openai*。

在连接 ChatGPT 之前，让我们去浏览器中尝试找到询问此任务的正确方式。这被称为提示工程，正确处理非常重要。通过使用我的一篇随机文章作为示例，尝试不同的询问方式，我发现正确的方法是提供一个详细且有指导性的预设，然后再给出实际文本。

我最终得到了以下的预设：

![](../Images/3cd99f403e3ce9c9fa0eb1603461a774.png)

作者截图

作为示例，我给它了一段关于 Gamma 函数的文章摘录，这篇文章是我很久以前写的：

![](../Images/6140a54366f5d59cf2039d4ff217bca2.png)

作者截图

它得出的结果是：

![](../Images/890fb9f93a7201446ed9faf280133be8.png)

作者截图

尽管它显然并没有真正理解任务，但它做得还不错，特别是在格式上。然而，有时它会创建重复项，并且注意到它有时会虚构一些实体和关系，尽管我们要求它不要这样做。恼人的不听话的机器！我们稍后会处理这个问题。

为了将来使用，我们将把这个预设存储在一个名为 *prompt_input.py* 的 Python 文件中。

现在基础设置已经就绪，让我们测试一下它是否真的有效。

如果代码仅供你自己使用，并且只在本地机器上，你可以将 API 密钥硬编码到 Python 文件中，否则，你可以将其设置为环境变量或放在一个你不会推送的配置文件中！

让我们测试一下这个设置是否有效。我们创建一个名为 *connect.py* 的文件，包含从 Python 连接 ChatGPT 的基本代码。

我们验证这个是否有效！

# 数据

我需要从我的 Medium 账户中提取文章。在写作时，我已经发布了 123 篇文章，但从 Medium 下载功能返回了 259 个文件！这是因为它也将评论和草稿分类为帖子。我们只想要已发布的文章，但这还不是唯一的问题。这些文件是 HTML 文件！如果你想在浏览器中阅读当然没问题，但如果你想处理纯文本就不太适用了。

> 好吧，中等水平，你做得不错，但这不能阻止一个装备了编程语言和各种窍门的数据科学家！

我们还注意到下载文件的文件名相当混乱。一个标准名称的例子是“*2020–12–11_The-Most-Beautiful-Equation-in-the-World-5ab6e49c363.html*”

让我们将这些文件存储在一个名为 *raw* 的文件夹中。

我们编写一个名为 *extract_text_from_html.py* 的小模块，功能是从这些文件中提取文本：

在我们能够实际从 ChatGPT 获取结果之前，我们需要能够将文本拆分成批次。原因是 GPT-4 有一个令牌限制。幸运的是，这很简单。在名为*preprocess.py*的文件中，我们编写：

现在我们准备实际从 ChatGPT 获取一些数据。

我们编写一个名为*process_articels.py*的文件，在其中**循环**处理文章，从令人恐惧的文件名中检索**标题**，从 HTML 文件中提取实际的**文本**，将每批文本通过**ChatGPT**处理，从文件中**收集**结果，并将模型的输出**保存**到一个名为*data*的文件夹中。我们还将实际的文本保存在一个名为**cleaned**的文件夹中，以便以后使用。

呼！！那真是一大堆工作。但实际上，代码很简单，因为我们已经在其他文件中完成了一些工作。

上述代码可能需要一段时间来执行，因为 GPT-4 模型相对较慢，与其他表现较差的模型相比。我们确保使用缓存设置，这样如果程序崩溃，我们不会从头开始，而是从中断的地方继续。

现在（经过几个小时的痛苦之后），我们有了一个结构化的 GPT-4 结果数据集。完美。现在我们“只”需要从中构建图谱。

# 构建知识图谱

我们将预处理和图谱创建过程合并为一个单独的函数。这通常不是很明智（关注点分离），但由于我们在预处理阶段需要查看“关系和实体”层级，我们可以在双手沾满污垢时创建图谱中的节点和关系。

我们创建一个小型 API，包含一个驱动程序，以便我们可以与图形进行交互。

我们需要**循环**处理结果，确保实体**不**太长，**清理**结果，并定义 gpt 模型输出中的**节点**和**关系**，我们**不**希望用相同的查询多次调用图形，我们希望实体与原始**文章**相关联，然后我们需要确保 Chat-GPT 提出的每个实体和关系实际**存在**于文本中，以免我们构建一个**机器梦**的图谱！

> 上述要求的最后一点是提高我们信任图形的概率，即使它不是万无一失的，考虑到这一点。

没问题！我们写下以下内容：

当然，有很多方法可以为知识图谱创建模式。不是总能容易看出什么应该是节点，什么应该是关系，但由于我们不希望关系之间有关系，所以我们选择了上述方法。

此外，我为这篇文章选择了简约的方法。通常，我们会用更多的属性来丰富节点和关系。

现在我们只需要一个主要入口点。

就这样。现在我们有了一个包含 Medium 上我文章信息的知识图谱。事实上，我们有大约 2000 个节点和 4500 个关系。

# 探索图谱

那么我们可以用这个做什么？我们应该问它什么？

让我们尝试找出不同的人出现在多少篇文章中。我们有如下结果：

![](../Images/3f012b4855bea74d20b5a06362b50949.png)

作者截图

不出所料，欧拉排在榜首，但我惊讶于拉马努jan和牛顿出现在我的4篇文章中。当然，我可以找到这些文章的标题，但让我们继续。好吧，这很有趣，但你不需要一个图表就能搞清楚这一点。

我们再试试别的。让我们看看有多少篇文章同时提到瑞曼和欧拉。

![](../Images/3f0ef614507164a3ad4b347c3cda3e10.png)

作者截图

让我们看看我的文章提到了多少欧拉的发现。

![](../Images/504694e28a228ec117657e1761a49456.png)

作者截图

嗯，没有欧拉线？我得写一篇关于这个的文章。

让我们找出有多少篇文章与文章*群论*共享了一些数学关键词。

![](../Images/e817643569064a6b1e3bc802326559dd.png)

作者截图

结果显示在这里，与上图中非橙色节点连接的27篇其他文章。尽管这只是一个玩具示例，但可以想象它同样可以显示业务相关文档如何通过一些在某些学科（如GDPR或审计）中重要的敏感关键词相互关联。

# 收获

显然，这项工作应该被视为我们所谓的“概念验证”。我们不能真正地利用我的文章，但如果这些文本来自一个公司，包含关于他们的客户和员工的信息，从电子邮件到Word文档、PDF等等，这可以用来绘制客户之间的关系以及哪些员工密切合作。

这将使我们能够360度了解数据如何在整个组织中流动，谁是特定信息流的最重要人物，谁是了解特定主题或文档的合适人选，我们部门联系的客户曾被另一个部门联系过，等等。

极其宝贵的信息。当然，我们不能用ChatGPT来做这个，因为我们不知道我们发送的数据会发生什么。因此，询问它关于敏感或业务关键的信息不是一个好主意。我们需要做的是下载另一个仅存在于我们笔记本电脑上的LLM（大型语言模型）。一个本地LLM。我们甚至可以在自己的数据上进行微调。目前许多公司已经在进行这种操作，用于构建聊天机器人、助手等。

但如果你问我，用它来构建一个关于你非结构化数据的知识图谱是更高层次的工作，我认为我已经展示了这完全是可行的！

> 如果你的公司想知道我们如何利用你的数据为你的业务编织可能性网络，请联系[我](https://www.linkedin.com/in/kasper-m%C3%BCller-96ba95169/)或我的同事[Kenneth Nielsen](https://www.linkedin.com/in/kenneth-h-m-nielsen-phd/)。

感谢你的阅读。

*如果你喜欢在 Medium 上阅读像这样的一些文章，你可以* [*获取会员资格*](https://kaspermuller.medium.com/membership) *以获得全部访问权限。要加入社区，只需点击* [*这里*](https://kaspermuller.medium.com/membership)*。*
