# 推动双塔模型的极限

> 原文：[https://towardsdatascience.com/pushing-the-limits-of-the-two-tower-model-a577090e5140](https://towardsdatascience.com/pushing-the-limits-of-the-two-tower-model-a577090e5140)

## 双塔模型架构背后的假设在哪些地方失效——以及如何超越这些限制

[](https://medium.com/@samuel.flender?source=post_page-----a577090e5140--------------------------------)[![Samuel Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----a577090e5140--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a577090e5140--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a577090e5140--------------------------------) [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----a577090e5140--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a577090e5140--------------------------------) ·阅读时长8分钟·2023年12月10日

--

![](../Images/0368a7208d8a12f6c14723d1d837cc2c.png)

（图像由作者使用生成式 AI 创建）

[双塔模型](https://medium.com/towards-data-science/the-rise-of-two-tower-models-in-recommender-systems-be6217494831) 是现代推荐系统中最常见的架构设计选择之一——关键思想是一个塔学习相关性，第二个浅层塔学习观察性偏差，如位置偏差。

在这篇文章中，我们将仔细审视双塔模型背后的两个假设，特别是：

+   **因式分解假设**，即假设我们可以简单地将两个塔计算的概率相乘（或将它们的对数相加），以及

+   **位置独立性假设**，即假设决定位置偏差的唯一变量是项目本身的位置，而不是影响它的上下文。

我们将看到这两个假设在哪些地方失效，以及如何通过新算法如 MixEM 模型、点积模型和 XPA 超越这些限制。

让我们从一个非常简短的提醒开始。

[](/the-rise-of-two-tower-models-in-recommender-systems-be6217494831?source=post_page-----a577090e5140--------------------------------) [## 双塔模型在推荐系统中的兴起

### 深入探讨用于去偏排名模型的最新技术

towardsdatascience.com](/the-rise-of-two-tower-models-in-recommender-systems-be6217494831?source=post_page-----a577090e5140--------------------------------)

## 双塔模型：迄今为止的故事

推荐系统中排名模型的主要学习目标是相关性：我们希望模型在给定上下文的情况下预测最佳的内容。这里，上下文简单地指我们从用户的先前互动或搜索历史中学到的一切，具体取决于应用场景。

然而，排名模型通常表现出某些观察偏差，即用户根据展示方式对印象的参与程度的倾向。最突出的观察偏差是位置偏差——用户更倾向于与最先展示的项目进行互动。

双塔模型的关键思想是并行训练两个“塔”，即神经网络，一个主塔用于学习相关性，另一个浅层塔用于学习数据中的各种观察偏差。然后可以将两个塔的对数值相加以计算最终预测，如YouTube的“[Watch Next](https://daiwk.github.io/assets/youtube-multitask.pdf)”论文中所述：

![](../Images/584941fc2372fb657c41691a8f0ce54e.png)

([赵等 2019](https://daiwk.github.io/assets/youtube-multitask.pdf))

基础假设是，通过为偏差设置专门的塔，主塔可以专注于主要的学习目标，即相关性。实际上，经验上，双塔模型已经被证明在建模方面带来了显著的改进：

+   [华为的 PAL](https://github.com/tangxyw/RecSysPapers/blob/main/Debias/%5B2019%5D%5BHuawei%5D%5BPAL%5D%20a%20position-bias%20aware%20learning%20framework%20for%20CTR%20prediction%20in%20live%20recommender%20systems.pdf) 在华为应用商店的点击率提高了 25%。

+   [YouTube 的双塔加性模型](https://daiwk.github.io/assets/youtube-multitask.pdf)将互动率提高了 0.24%，并且

+   [AirBnb 的双塔模型](https://arxiv.org/pdf/2002.05515.pdf)将预订率提高了 0.7%。

简而言之，双塔模型通过将学习目标分解为相关性和偏差来工作，并已被证明在整个行业的排名模型中带来了显著的改进。

## 分解假设

双塔模型依赖于分解假设，即我们可以将点击预测分解为

[PRE0]

即，在用户观察到该项目的情况下的点击概率（第一个因素）与在给定位置（以及其他观察特征）的情况下观察到该项目的概率的乘积。YouTube将其重新表述为对数值的和，而不是概率的乘积，这在大致上是等效的：

[PRE1]

然而，很容易看出这个分解假设在哪里破裂。例如，考虑一个训练数据包含两种不同类型用户的场景，类型 1 和类型 2：

+   类型 1 用户总是点击他们被展示的第一个项目。他们急于获得即时奖励。

+   类型2用户总是滚动浏览他们被展示的项目，直到找到他们确切需要的内容。他们耐心且挑剔。

现在，我们的分解假设破裂了，因为`p(seen|position)`在数据中不是常量，而是取决于用户群体！当前模型无法建模由具有不同点击行为和偏见的用户群体混合生成的数据集。

因此，我们应该做些其他的事情，而不是分解。那是什么呢？最近的Google[论文](https://dl.acm.org/doi/pdf/10.1145/3477495.3531837)《重新审视双塔模型以进行无偏排序学习》中提出了两个解决方案：

+   点积模型，以及

+   Mix-EM模型。

让我们更详细地了解它们的工作原理。

**点积模型**的核心思想是让两个塔模型不输出概率或logits，而是输出嵌入，然后通过点积将两个嵌入组合起来：

[PRE2]

之所以有效，是因为嵌入表示比单一的logit更具表达能力，它们允许我们在嵌入空间的不同维度中编码不同用户的偏见程度。

在**Mix-EM**模型中，我们不是使用单一的双塔模型，而是使用多个（作者考虑了2个），然后应用期望最大化（EM）算法，其中

+   在E步中，我们将训练样本分配给一对双塔模型中的一个，

+   在M步中，我们最小化这一对双塔模型的总体损失。

这种方法背后的假设是，不同的双塔模型最终将学会处理不同的用户群体（例如我们上面介绍的类型1和类型2用户）。

作者在Google Chrome Web商店的推荐中测试了这两种算法，并发现

+   使用点积模型，NDGC@5提升了+0.47%。

+   使用MixEM，NDGC@5提升了+1.1%。

相较于标准的双塔加性模型，这个结果证实了分解假设确实不适用于这个问题，并量化了我们由于这种失败而损失的性能。点积模型和MixEM模型的效果更好，因为它们是更具表达力的方式来建模两个塔的组合，MixEM比点积模型更好。

## 独立假设

标准双塔模型背后的另一个假设是，在位置p展示的项目与邻近位置p-1、p+1等位置展示的项目是独立的。

然而，这并非现实世界的情况，最近的Google论文[跨位置注意力以去偏见点击](https://hongleizhuang.github.io/files/WWW2021.pdf)的作者认为。点击概率不仅依赖于显示的项目，还依赖于附近显示的内容，尤其是在项目具有水平和垂直位置的情况下，例如在Google Chrome商店中。

为了超越这一限制，作者提出了XPA，即跨位置注意力。其工作原理如下：

+   偏差塔的输入不仅仅是当前项目的位置，还包括所有邻近项目的位置，这些位置被表示为一个单一的嵌入向量。

+   相关性塔的输入不仅仅是当前项目的特征向量，还包括所有邻近项目的特征向量，这些特征向量被表示为一个单一的嵌入向量。

那么关键问题是如何用单一嵌入向量表示所有邻近项目？这通过交叉注意力层来实现，该层首先使用缩放的 softmax 计算注意力权重：

![](../Images/12ea1d976f0a4025bc93cd4edde957a8.png)

然后计算邻近项目的嵌入（用波浪号表示）作为加权和，其中权重是注意力权重：

![](../Images/92da8ed1503bf874f5e3e8e5978a7f79.png)

这里有一个架构图显示其工作原理——注意这两个塔不仅仅接收单一项目作为输入，而是一次处理多个项目，因为它查看了每个项目的所有邻居：

![](../Images/ea5c86c665536017afafc1eb2cb2dc02.png)

([Zhuang et al 2021](https://hongleizhuang.github.io/files/WWW2021.pdf))

最终预测是四个项的组合，其中包括两个相关性塔的输出（分别针对项目本身及其邻居）和两个偏差塔的输出（同样分别针对项目本身及其邻居），经过一个 sigmoid 函数进行缩放。

让我们看看结果。在来自 Chrome Web Store 的生产数据中，作者发现

+   XPA 相较于基础的天真模型（完全不使用位置）提高了 +11.8% 的点击率。

+   XPA 相较于标准双塔模型提高了 +6.2% 的点击率。

6.2% 是一个巨大的提升——通过这一点，作者证明了不仅仅利用项目的位置，还利用显示在其附近的项目的有效性。

观察交叉注意力矩阵 a_jk 作为位置 j 和 k 的函数也是很有趣的。（“位置”在这里指的是 Chrome Web Store UI 中的位置，从左上到右下进行编号。）

![](../Images/0fcb6ddf9c8e726d3ebcc37248702d09.png)

([Zhuang et al 2021](https://hongleizhuang.github.io/files/WWW2021.pdf))

矩阵中存在的非对角线元素证明了需要交叉注意力来建模这些数据——一个普通的双塔模型只能建模对角线元素，而不能处理这些非对角线元素。非对角线注意力权重的存在是因为如果用户点击了 Chrome Store 中某一行的一个项目，他们相对更可能点击同一行中的另一个项目。

## 收获

让我们回顾一下。双塔模型通过允许将学习目标分解为相关性 + 偏差，来改进推荐系统背后的排名模型。

因子分解假设是指我们可以简单地将两个塔的logits相加（或将它们的概率相乘）。这个假设在我们考虑到数据中存在不同用户群体时就会破裂，每个群体展示了不同类型和程度的观察偏差。谷歌的MixEM和点积模型已被证明能突破这一限制。

独立假设是指观察偏差仅依赖于项目的位置，而不依赖于邻近项目。这个假设在现实应用中会破裂，因为用户参与确实依赖于项目的邻域，而不仅仅是它的位置。XPA利用来自邻近位置的交叉位置注意力，是突破这一限制的算法。

总结一下，标准的双塔模型假设用户都是相同的，位置也是相同的——MixEM（以及点积模型）突破了第一个限制，而XPA突破了第二个限制。

*想要以深入了解最新机器学习技术和突破来打动同行或在下一个机器学习面试中表现出色吗？* [*订阅我的新闻通讯*](https://mlfrontiers.substack.com)*！*
