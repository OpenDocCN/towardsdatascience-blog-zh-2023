# ChatGPT 的工作原理：聊天机器人背后的模型

> 原文：[https://towardsdatascience.com/how-chatgpt-works-the-models-behind-the-bot-1ce5fca96286](https://towardsdatascience.com/how-chatgpt-works-the-models-behind-the-bot-1ce5fca96286)

## 对你听说过的聊天机器人背后的直觉和方法的简要介绍。

[](https://medium.com/@molly.ruby?source=post_page-----1ce5fca96286--------------------------------)[![Molly Ruby](../Images/2a493bd01057722138857a90035347cd.png)](https://medium.com/@molly.ruby?source=post_page-----1ce5fca96286--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ce5fca96286--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ce5fca96286--------------------------------) [Molly Ruby](https://medium.com/@molly.ruby?source=post_page-----1ce5fca96286--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ce5fca96286--------------------------------) ·阅读时间 9 分钟·2023年1月30日

--

![](../Images/0e4bc81c17a6711eda750f7e522f100e.png)

这篇关于驱动 ChatGPT 的机器学习模型的温和介绍，将从大语言模型的介绍开始，深入探讨使 GPT-3 能够训练的革命性自注意力机制，然后深入挖掘使 ChatGPT 异常出色的强化学习技术。

# 大语言模型

ChatGPT 是一种自然语言处理模型的扩展，称为大语言模型（LLMs）。LLMs 消化大量的文本数据，并推断文本中的单词之间的关系。随着计算能力的进步，这些模型在过去几年里得到了发展。LLMs 随着输入数据集和参数空间的增大而提高其能力。

语言模型的最基本训练涉及在单词序列中预测一个单词。最常见的是下一词预测和掩码语言建模。

![](../Images/190916d7e4a8d73ec41e379347e46df9.png)

作者生成的下一词预测和掩码语言建模的任意示例。

在这种基本的序列技术中，通常通过长短期记忆（LSTM）模型实现，模型根据周围的上下文填入最有统计概率的单词。这种序列建模结构有两个主要的限制。

1.  模型无法对一些周围的单词赋予比其他单词更多的权重。在上述示例中，虽然‘reading’通常会与‘hates’相关联，但在数据库中‘Jacob’可能是如此热衷于阅读，以至于模型应给予‘Jacob’比‘reading’更多的权重，从而选择‘love’而不是‘hates’。

1.  输入数据是单独且顺序处理的，而不是作为整个语料库处理。这意味着，当LSTM被训练时，上下文窗口是固定的，仅在序列中的几个步骤之外扩展。这限制了单词之间的关系的复杂性和可以推导出的意义。

针对这个问题，2017年Google Brain团队引入了变换器。与LSTM不同，变换器可以同时处理所有输入数据。通过使用自注意力机制，模型可以根据语言序列的任何位置对输入数据的不同部分给予不同的权重。这一特性显著提升了将意义注入LLM的能力，并使得处理显著更大的数据集成为可能。

# GPT和自注意力

生成预训练变换器（GPT）模型最早由openAI在2018年推出，命名为GPT-1。之后，模型在2019年演变为GPT-2，2020年演变为GPT-3，最近在2022年推出了InstructGPT和ChatGPT。在将人类反馈整合到系统之前，GPT模型演变的最大进展是由计算效率的成就驱动的，这使得GPT-3能够在比GPT-2上训练更多的数据，从而拥有更广泛的知识基础和执行更广泛任务的能力。

![](../Images/27bb3cefc523b0279028e7691d6f0747.png)

GPT-2（左）和GPT-3（右）的比较。由作者生成。

所有GPT模型大体上遵循在“Attention is All You Need”（Vaswani et al., 2017）中建立的变换器架构，该架构包括一个编码器来处理输入序列和一个解码器来生成输出序列。在原始变换器中，编码器和解码器都有一个多头自注意力机制，使得模型能够对序列的不同部分赋予不同的权重，从而推断意义和上下文。作为对原始变换器的演变，GPT模型利用了Radford等人（2018）建立的仅解码器的变换器，采用了掩蔽自注意力头。该架构通过Radford等人（2019）和Brown等人（2020）的工作进一步微调。采用仅解码器的框架是因为GPT的主要目标是生成连贯且上下文相关的文本。自回归解码由解码器处理，使得模型能够保持上下文，并逐个标记生成序列。

驱动GPT的自注意力机制通过将标记（文本片段，可以是一个单词、句子或其他文本组）转换为向量，来表示标记在输入序列中的重要性。

1.  为输入序列中的每个标记创建查询、键和值向量。

1.  通过计算步骤一中的查询向量与每个其他标记的键向量之间的点积，计算它们之间的相似性。

1.  通过将步骤2的输出输入到[softmax函数](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer)中生成标准化权重。

1.  生成最终向量，通过将步骤3中生成的权重与每个令牌的值向量相乘，来表示令牌在序列中的重要性。

GPT使用的‘多头’注意机制是自注意力的进化。模型并非仅执行步骤1-4一次，而是并行地多次迭代此机制，每次生成查询、键和值向量的新线性投影。通过这种方式扩展自注意力，模型能够理解输入数据中的子含义和更复杂的关系。

![](../Images/203d683689d56d1fe795bcdf1c72a21b.png)

作者生成的ChatGPT截图。

尽管GPT-3在自然语言处理方面引入了显著的进展，但其在与用户意图对齐的能力上仍有限。例如，GPT-3可能会生成

+   **缺乏帮助性**，意味着它们不遵循用户的明确指示。

+   **包含幻觉**，即反映不存在或不正确的事实。

+   **缺乏可解释性**，使得人们难以理解模型如何得出特定的决定或预测。

+   **包含有害或偏见内容**，即有害或冒犯性内容，并传播虚假信息。

在ChatGPT中引入了创新的训练方法，以应对标准LLM的一些固有问题。

# ChatGPT

ChatGPT是InstructGPT的衍生产品，它引入了一种新方法，将人类反馈纳入训练过程，以更好地使模型输出符合用户意图。有关强化学习从人类反馈（RLHF）的详细描述，请参阅[openAI的2022](https://arxiv.org/pdf/2203.02155.pdf)论文**训练语言模型以遵循人类反馈的指示**，下面进行了简化。

## 步骤1：监督微调（SFT）模型

第一个开发涉及对GPT-3模型进行微调，通过聘请40名承包商创建一个监督训练数据集，其中输入具有已知输出供模型学习。输入或提示收集自实际用户输入到Open API中。标签员随后为每个提示编写了适当的回应，从而为每个输入创建了已知输出。然后使用这个新的监督数据集对GPT-3模型进行微调，创建了GPT-3.5，也称为SFT模型。

为了最大化提示数据集的多样性，每个用户ID最多只能有200个提示，任何共享长前缀的提示都被删除。最后，所有包含个人可识别信息（PII）的提示都被删除。

在汇总来自OpenAI API的提示后，标签员还被要求创建样本提示，以填补仅有少量真实样本数据的类别。感兴趣的类别包括

+   **简单提示：** 任何任意请求。

+   **少样本提示：** 包含多个查询/响应对的指令。

+   **基于用户的提示：** 对应于OpenAI API请求的特定用例。

在生成响应时，要求标注者尽力推断用户的指令是什么。论文描述了提示请求信息的三种主要方式。

1.  **直接：** “告诉我关于……”

1.  **少样本：** 给出这两个故事的例子，写另一个关于相同主题的故事。

1.  **续写：** 给出一个故事的开头，完成它。

从OpenAI API和标注者手工编写的提示汇编得到了13,000个输入/输出样本，用于监督模型。

![](../Images/46ce281745760324f5e4d28caef00644.png)

图片（左侧）摘自**《训练语言模型以遵循人类反馈的指令》** *OpenAI等，2022* [https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)。额外的上下文由作者在右侧红色部分添加。

## 第2步：奖励模型

在第1步中训练好SFT模型后，模型生成了更符合用户提示的响应。下一步的改进形式是训练奖励模型，其中模型输入是一系列提示和响应，而输出是一个缩放值，称为奖励。奖励模型是为了利用强化学习，在这种学习方式中，模型学会生成输出以最大化其奖励（参见第3步）。

为了训练奖励模型，标注者会看到4到9个SFT模型的输出，这些输出都是针对一个输入提示的。他们需要将这些输出从最好到最差进行排名，生成输出排名的组合如下。

![](../Images/a1673228cc37425149a900d709638d35.png)

响应排名组合的示例。由作者生成。

将每个组合作为单独的数据点纳入模型导致了过拟合（无法超越已见数据进行推断）。为了解决这个问题，模型是通过将每组排名作为一个单独的批次数据点来构建的。

![](../Images/0bf445ed52fc76ca26038ac769aa92d6.png)

图片（左侧）摘自**《训练语言模型以遵循人类反馈的指令》** *OpenAI等，2022* [https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)。额外的上下文由作者在右侧红色部分添加。

## 第3步：强化学习模型

在最终阶段，模型会接收到一个随机提示并返回一个响应。这个响应是使用模型在第2步中学到的“策略”生成的。策略代表了机器为了实现目标而学会使用的策略；在这个案例中，目标是最大化奖励。基于在第2步中开发的奖励模型，会为提示和响应对确定一个缩放奖励值。然后，奖励反馈到模型中，以进化策略。

2017 年，Schulman *et al.* 介绍了 [Proximal Policy Optimization (PPO)](/proximal-policy-optimization-ppo-explained-abed1952457b)，这一方法用于更新每次生成回应时的模型策略。PPO 结合了来自 SFT 模型的每个令牌 Kullback–Leibler (KL) 惩罚。KL 散度测量两个分布函数的相似性，并惩罚极端距离。在这种情况下，使用 KL 惩罚减少了回应与在步骤 1 中训练的 SFT 模型输出之间的距离，以避免过度优化奖励模型，并过于偏离人类意图数据集。

![](../Images/63cb058ba1ca66a58f3c7f1a95e40004.png)

从 **Training language models to follow instructions with human feedback** *OpenAI et al., 2022* [https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf) 插入的图片（左）。作者添加的额外背景信息（右）以红色标注。

过程的第 2 和第 3 步可以重复迭代，尽管在实践中尚未广泛实施。

![](../Images/10114a9a3d2694f61c6a87766c260d5e.png)

由作者生成的 ChatGPT 截图。

## 模型评估

模型评估通过在训练期间留出一个模型未见过的测试集来进行。在测试集上进行一系列评估，以确定模型是否比其前身 GPT-3 更好地对齐。

**有用性**：模型推断和遵循用户指令的能力。标注者 85 ± 3% 的时间更喜欢 InstructGPT 的输出而不是 GPT-3。

**真实度**：模型出现幻觉的倾向。PPO 模型在使用 [TruthfulQA](https://arxiv.org/abs/2109.07958) 数据集进行评估时，显示出真实度和信息量的轻微增加。

**无害性**：模型避免不适当、贬低和诋毁内容的能力。无害性通过使用 RealToxicityPrompts 数据集进行了测试。测试在三种条件下进行。

1.  指示提供尊重的回应：结果显著减少了有害回应。

1.  指示提供回应，没有任何尊重设置：毒性没有显著变化。

1.  指示提供有害回应：回应实际上比 GPT-3 模型显著更有害。

要了解更多关于创建 ChatGPT 和 InstructGPT 的方法，请阅读由 OpenAI 发布的原始论文 **Training language models to follow instructions with human feedback**，*2022* [https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)。

![](../Images/3b6e18b5276e5cb70c20909110019b56.png)

由作者生成的 ChatGPT 截图。

祝学习愉快！

# 来源

1.  [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/)

1.  [https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)

1.  [https://medium.com/r/?url=https%3A%2F%2Fdeepai.org%2Fmachine-learning-glossary-and-terms%2Fsoftmax-layer](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer)

1.  [https://www.assemblyai.com/blog/how-chatgpt-actually-works/](https://www.assemblyai.com/blog/how-chatgpt-actually-works/)

1.  [https://medium.com/r/?url=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b](/proximal-policy-optimization-ppo-explained-abed1952457b)
