- en: How to Evaluate Representations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-evaluate-representations-886ce5ef7a66](https://towardsdatascience.com/how-to-evaluate-representations-886ce5ef7a66)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From unsupervised to supervised metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mina.ghashami?source=post_page-----886ce5ef7a66--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----886ce5ef7a66--------------------------------)[](https://towardsdatascience.com/?source=post_page-----886ce5ef7a66--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----886ce5ef7a66--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----886ce5ef7a66--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----886ce5ef7a66--------------------------------)
    ·8 min read·Sep 14, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/92a89b397bf306df6fcef24bd78eb2a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'credit: Image from unsplash.com'
  prefs: []
  type: TYPE_NORMAL
- en: '*Embeddings*, also known as *representations*, are dense vector representations
    of entities such as words, documents, products, and more. They are designed to
    capture semantic meanings and highlight similarities among entities. A good set
    of representations should not only efficiently encode the essential features of
    entities but also exhibit properties like compactness, meaningfulness, and robustness
    across various tasks. In this article, we look into various evaluation metrics
    to assess the quality of representations. Let’s get started.'
  prefs: []
  type: TYPE_NORMAL
- en: An Evaluation Framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Any evaluation framework consists of three main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A baseline method**: this serves as a benchmark against which new approaches
    or models are compared. It provides a reference point for evaluating the performance
    of the proposed methods.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**A set of evaluation metrics**: evaluation metrics are quantitative measures
    used to evaluate the performance of the models. These metrics can be supervised
    or unsupervised, and define how the success of the outputs is assessed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**An evaluation dataset**: the evaluation dataset is a collection of labeled/annotated
    or unlabelled data used to assess the performance of the models. This dataset
    should be representative of the real-world scenarios that the models are expected
    to handle. It needs to cover a diverse range of examples to ensure a comprehensive
    evaluation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on if evaluation metrics require ground truth labels, we can split them
    into *un-supervised metrics*, and *supervised metrics*. It is often more advantageous
    to employ un-supervised metrics, as they do not require labels, and the collection
    of labels is very expensive in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Below, we will look into state-of-the-art metrics. For each metric, pick a baseline
    method to compare your evaluations against. The baseline can be as simple as `*random
    embedding generator*`!
  prefs: []
  type: TYPE_NORMAL
- en: Supervised Evaluation Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Supervised metrics require a labelled evaluation dataset. A common strategy
    is to choose a predictor such as a classifier or regressor. Then train the predictor
    on a limited set of labeled data from a specific task. Next, using a supervised
    metric measure the predictor’s performance on a held-out dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth it to mention two valuable points here:'
  prefs: []
  type: TYPE_NORMAL
- en: 1️⃣ The `*validation accuracy`*, a commonly used metric, is shown to be sensitive
    to the size of the dataset used for training the probe[3]!! It is shown that *validation
    accuracy* is sensitive to the size of the training dataset; they may choose a
    different representations for the same task when given dataset of different sizes
    [3]! Ideally, this assessment must be independent of the dataset size and *be
    only dependent on the data distribution* [3].
  prefs: []
  type: TYPE_NORMAL
- en: 'The figure below shows this phenomena: two representations, one in red (representation
    A), and one in blue (representation B) are shown. The x-axis denotes the size
    of the training data used to train the predictor (referred to as probe). The y-axis
    shows the validation loss of the trained probe. Note the probe is trained on representations
    from each method. As we see, as training data size increases on the x-axis, the
    validation loss of the probe decreases on the y-axis. However, at some point,
    the two loss curves cross each other!! Therefore in small data sizes, representation
    A has lesser loss, while in larger data sizes the representation B has lesser
    loss. The paper in[3] names this curve the *loss-data curve* as it measures the
    loss of the predictor vs the training data size used to train the probe.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9a278072f2c134d332d32c902bb2bc3b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: loss-data curve — image from [3] modified by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 2️⃣ *Linear predictors* (linear classifiers/regressors) are widely criticized
    as a method to evaluate representations[4]; it is shown that models that perform
    strongly on linear classification tasks can perform weakly on more complex tasks
    [4].
  prefs: []
  type: TYPE_NORMAL
- en: There are many supervised metrics proposed in literature; **Mutual Information
    (MI), F1 score, BLEU, Precision, Recall, Minimum Descriptor Length (MDL)** to
    name a few but the SOTA metrics [3,9] in supervised evaluation realm are **Surplus
    Description Length (SDL)** and **ϵ−sample complexity.**
  prefs: []
  type: TYPE_NORMAL
- en: '**Surplus Description Length (SDL)** and **ϵ−sample complexity** are inspired
    by the idea that “*the best representation is the one which allows for the most
    efficient learning of a predictor to solve the task”* [3,9]*.* As [3] mentions
    *“this position is motivated by practical concerns; the more labels that are needed
    to solve a task in the deployment phase, the more expensive to use and the less
    widely applicable a representation will be”.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Intuition**: To give an intuition, the **SDL** and **ϵ−sample complexity**
    metrics do not measure a quantity on a fixed data size, instead they estimate
    the loss-data curve and measure quantities with respect to the curve[3]. To that
    end, for both metrics the user specifies a tolerance ϵ so that a population loss
    of less than ϵ qualifies as solving the task. An ϵ-loss predictor is then any
    predictor which achieves loss lesser than ϵ [3]. Then the measures compute the
    cost of learning a predictor which achieves that loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s dive into the details of each metric:'
  prefs: []
  type: TYPE_NORMAL
- en: Surplus Description Length (SDL)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in [3], this metric corresponds to computing the area between the
    loss-data curve and a baseline set by y = ϵ. In other words, it measures the cost
    of re-creating an ϵ-loss predictor using the representation [3]. Mathematically,
    it is defined as following
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3936abce369eaef49193ac7562622fcd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: SDL definition — Image from [3]'
  prefs: []
  type: TYPE_NORMAL
- en: Here, A is an algorithm, it can be any predicting algorithm such as classification
    or regression, and L(A, i) refers to the loss of this predictor resulting from
    training algorithm A on the first i data points [3]. Naively computing this metric
    require unbounded data so in practice it is always estimated [3]. For an implementation
    of this metric refer to the Github repository at [8].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23b716387cce185742bcab901a09a3db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: SDL metric corresponds to computing the area between the loss-data
    curve and a baseline — image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: ϵ−sample complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned in [3], this metric measures the complexity of learning an ϵ-loss
    predictor by the number of samples it takes to find it. It is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7f79555f51a31f88249fc2a37e52031.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: epsilon-sample complexity definition — Image from [3]'
  prefs: []
  type: TYPE_NORMAL
- en: This measures allows the comparison of two representation by first picking a
    target function to learn (e.g., an epsilon loss 2 layer MLP classifier) then measuring
    which representations enables learning that function with less data [3].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96e6347cb6ca74db82618336981fc74e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: epsilon-sample complexity corresponds to number of data points which
    achieve epsilon loss — image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unsupervised Evaluation Metrics:**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These metrics are every scientist’s favorite as they do not require labelled
    data. Some examples of common unsupervised metrics are the followings:'
  prefs: []
  type: TYPE_NORMAL
- en: '1) Cluster Learnability (CL):'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This metric[1] assesses the learnability of representations. A higher Cluster
    Learnability (CL) score indicates that the learned representation is better at
    separating different clusters in the input data space. CL is measured as the learning
    speed of a K-Nearest Neighbor (KNN) classifier trained to predict labels obtained
    by clustering the representations with K-means.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c2f954130c618a34ea4faec71a58af0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'It consists of three main steps and works as following:'
  prefs: []
  type: TYPE_NORMAL
- en: Pick hyper-parameters *k=number of clusters* and *k′ = number of neighbors*.
    Let x_i​ denote the representations of i−th data point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run k-means on the dataset to obtain k clusters; assign cluster id to each datapoint
    x_i​ as its label. Denote it as y^_i$​.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run KNN in a prequential manner on each datapoint to obtain a predicted label
    by majority rule. If a datapoint is {x_i​, y_​i}​ take all data points numbered
    lesser than i i.e {x_j​ | j<i } and run KNN on them to get majority vote of their
    class labels. This will be y~_i​.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then the Cluster Learnability (CL) metric would be
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37c521a7bbf556cffbccfd27f821af29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Cluster-Learnability metric — image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'The intuition for this metric is as following: The “prequential” approach is
    a way of evaluating the performance of a machine learning model, such as KNN,
    in an online learning setting. In this context, “prequential” combines the words
    “predictive” and “sequential.” In the prequential manner of KNN computation, the
    model is evaluated and updated sequentially, one instance at a time, using a stream
    of data. Since the data is unlabeled, this method uses clustering in step 2 to
    synthetically generate labels (i.e. cluster ids) and use them to compute the speed
    of cluster learnability in an online (prequential) manner.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code computes this metric :'
  prefs: []
  type: TYPE_NORMAL
- en: 2) *Silhouette Coefficient*
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to CL metric, silhouette coefficient metric measures clusterability
    of entities that inherently belong to one cluster. This metric measures the quality
    of clusters through their cohesion and separation. In other words, it measures
    how well each sample in a cluster is separated from samples in other clusters,
    and provides an indication of how compact and well-separated the clusters are.
    The Silhouette Coefficient ranges from -1 to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: A value close to 1 indicates that samples are well-clustered and far from samples
    in neighboring clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A value close to 0 suggests overlapping clusters or samples that are on or very
    close to the decision boundary between clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A value close to -1 indicates that samples are likely assigned to the wrong
    clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code computes this metric :'
  prefs: []
  type: TYPE_NORMAL
- en: 3) Cluster Purity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This metric evaluates the quality of clusters obtained from representations.
    It measures the extent to which clusters contain data points from a single class.
    The steps to compute cluster purity are:'
  prefs: []
  type: TYPE_NORMAL
- en: For each cluster, identify the majority class in that cluster. Count the number
    of data points of the majority class in each cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sum these counts for all clusters. Divide the sum by the total number of data
    points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The value ranges from 0 to 1\. Higher values indicate better clusters, with
    1 being perfect clustering where each cluster contains solely points from one
    class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code computes this metric :'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It’s imperative to assess quality of representations before feeding them into
    any machine learning model. To evaluate their quality we must have a framework
    consisting of three components: a baseline method, evaluation metrics, and an
    evaluation datasets. Metric divide into supervised and unsupervised, based on
    if they need a labelled dataset or not. Often unsupervised metrics are advantageous
    as they don’t require labelled dataset and collecting/curating label is difficult
    and time-consuming. The unsupervised metrics include cluster purity, cluster-ability
    and cluster-learnability. On the other hand, the state of the art supervised metrics
    are surplus descriptor length (SDL) and ϵ-sample complexity. Regardless of the
    choice of the metric, it’s important to stay consistent and use one single metric
    to compare two set of representations against each other.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have any questions or suggestions, feel free to reach out to me:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Email: mina.ghashami@gmail.com'
  prefs: []
  type: TYPE_NORMAL
- en: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Expressiveness and Learnability: A Unifying View for Evaluating Self-Supervised
    Learning](https://arxiv.org/pdf/2206.01251.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Self-Supervised Representation Learning: Introduction, Advances and Challenges](https://arxiv.org/pdf/2110.09327.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Evaluating representations by the complexity of learning low-loss predictors](https://arxiv.org/abs/2009.07368)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Probing the State of the Art: A Critical Look at Visual Representation Evaluation](https://arxiv.org/pdf/1912.00215.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Representation learning with contrastive predictive coding](https://arxiv.org/abs/1807.03748)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Learning representations by maximizing mutual information across views](https://arxiv.org/abs/1906.00910)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[On mutual information maximization for representation learning](https://arxiv.org/abs/1907.13625)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://github.com/willwhitney/reprieve](https://github.com/willwhitney/reprieve)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
