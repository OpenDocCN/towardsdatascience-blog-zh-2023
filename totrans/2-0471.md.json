["```py\nclass TicTacToe:\n    def __init__(self):\n        # Initialise an empty board\n        self.board = ['-' for _ in range(9)]\n        self.current_player = 'X'  # X will start\n\n    def make_move(self, position):\n        \"\"\"Make a move on the board.\"\"\"\n        if self.board[position] == '-':\n            self.board[position] = self.current_player\n            self.switch_player()\n            return True\n        else: return False  # illegal move\n\n    def switch_player(self):\n        \"\"\"Switch the current player.\"\"\"\n        self.current_player = 'O' if self.current_player == 'X' else 'X'\n\n    def check_winner(self):\n        \"\"\"Check if there is a winner.\"\"\"\n        # Rows, columns, diagonals\n        winning_positions = [\n            [0, 1, 2], [3, 4, 5], [6, 7, 8],  # Rows\n            [0, 3, 6], [1, 4, 7], [2, 5, 8],  # Columns\n            [0, 4, 8], [2, 4, 6]  # Diagonals\n        ]\n\n        for positions in winning_positions:\n            values = [self.board[pos] for pos in positions]\n            if values[0] == values[1] == values[2] and values[0] != '-':\n                return values[0]\n\n        return None  # No winner yet\n\n    def is_draw(self):\n        \"\"\"Check if the game is a draw.\"\"\"\n        return all(cell != '-' for cell in self.board)\n\n    def get_board_string(self):\n        \"\"\"Get the current board state as a string.\"\"\"\n        return ''.join(self.board)\n\n    def get_legal_moves(self):\n        \"\"\"Get the positions of all legal moves.\"\"\"\n        return [i for i, cell in enumerate(self.board) if cell == '-']\n\n    def pretty_print_board(self):\n        \"\"\"Pretty-print the board.\"\"\"\n        for i in range(0, 9, 3):\n            print(f\"{self.board[i]} | {self.board[i+1]} | {self.board[i+2]}\")\n            if i < 6:\n                print(\"- \"*5)\n\n# Test the pretty_print_board method\ntic_tac_toe = TicTacToe()\nprint(\"Initial board:\")\ntic_tac_toe.pretty_print_board()\n\n# Make some moves\ntic_tac_toe.make_move(0)\ntic_tac_toe.make_move(4)\ntic_tac_toe.make_move(8)\nprint(\"\\nBoard after some moves:\")\ntic_tac_toe.pretty_print_board()\n\nInitial board:\n- | - | -\n- - - - - \n- | - | -\n- - - - - \n- | - | -\n\nBoard after some moves:\nX | - | -\n- - - - - \n- | O | -\n- - - - - \n- | - | X\n```", "```py\nfrom copy import deepcopy\nfrom itertools import product\nimport numpy as np\nimport torch\n\n# Define character to integer mapping\nchars = sorted(list(set('XO-')))\nvocab_size = len(chars)\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {0: '-', 1: 'X', 2: 'O'}\nencode = lambda s: [stoi[c] for c in s]\ndecode = lambda l: ''.join([itos[i] for i in l])\n\ninput_sequences = []\noutput_sequences = []\n\n# Function to simulate all possible games recursively\ndef simulate_all_games(game, x_moves, o_moves):\n    global input_sequences, output_sequences\n\n    # Check if the game has reached a terminal state\n    winner = game.check_winner()\n    if winner == 'X' or game.is_draw():\n        # Add the sequence of board states and moves leading to this win for 'X' or draw\n        board = ['-' for _ in range(9)]\n        for i, x_move in enumerate(x_moves):\n            input_sequences.append(encode(''.join(board)))\n            output_sequences.append(x_move)\n            board[x_move] = 'X'\n            if i < len(o_moves):\n                board[o_moves[i]] = 'O'\n        return\n    elif winner == 'O':\n        return  # We don't add these to our training data\n\n    # Otherwise, continue simulating the game\n    legal_moves = game.get_legal_moves()\n    for move in legal_moves:\n        # Create a copy of the game to simulate the move\n        new_game = deepcopy(game)\n        was_legal = new_game.make_move(move)\n\n        # If the move was legal, continue simulating\n        if was_legal:\n            if new_game.current_player == 'X':\n                simulate_all_games(new_game, x_moves + [move], o_moves)\n            else:\n                simulate_all_games(new_game, x_moves, o_moves + [move])\n\n# Create an initial empty game\ninitial_game = TicTacToe()\n\n# Simulate all possible games starting with 'X'\nsimulate_all_games(initial_game, [], [])\n\n# Convert to PyTorch tensors\ninput_tensor = torch.tensor(input_sequences, dtype=torch.long)\noutput_tensor = torch.tensor(output_sequences, dtype=torch.long)\n\n# Show some sample input-output pairs\nprint(input_tensor[:10], output_tensor[:10])\n\nprint(\"Number of input-output pairs:\", len(input_sequences))\n\ntensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 2, 0, 0, 0, 0, 0, 0, 0],\n        [1, 2, 1, 2, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 2, 0, 0, 0, 0, 0, 0, 0],\n        [1, 2, 1, 2, 0, 0, 0, 0, 0],\n        [1, 2, 1, 2, 1, 2, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 2, 0, 0, 0, 0, 0, 0, 0],\n        [1, 2, 1, 2, 0, 0, 0, 0, 0]]) \ntensor([1, 3, 5, 1, 3, 5, 6, 1, 3, 5])\n\nNumber of input-output pairs: 658224\n```", "```py\ndef pretty_print_board(board: str):\n    \"\"\"Pretty-print the board.\"\"\"\n    for i in range(0, 9, 3):\n        print(f\"{board[i]} | {board[i+1]} | {board[i+2]}\")\n        if i < 6:\n            print(\"- \"*5)\n\nrand_idx = torch.randint(len(input_tensor), (1,))[0]\nrandom_game = input_tensor[rand_idx].tolist()\nprint(\"Current game state:\")\ndecoded_game = decode(random_game)\npretty_print_board(decoded_game)\nprint( )\n\nmove = output_tensor[rand_idx].item()\ndecoded_game = decoded_game[:move] + 'X' + decoded_game[move+1:]\nprint(\"New game state:\")\npretty_print_board(decoded_game)\n\nCurrent game state:\nX | - | -\n- - - - - \n- | X | O\n- - - - - \n- | O | -\n\nNew game state:\nX | - | X\n- - - - - \n- | X | O\n- - - - - \n- | O | -\n```", "```py\ninput_sequences = []\noutput_sequences = []\n\ndef simulate_all_games(game, x_moves, o_moves):\n    global input_sequences, output_sequences\n\n    # Check if the game has reached a terminal state\n    winner = game.check_winner()\n    if winner == 'X' or game.is_draw():\n        # Add the sequence of board states and moves leading to this win for 'X' or draw\n        board = ['-' for _ in range(9)]\n        for i, x_move in enumerate(x_moves):\n            input_sequences.append(encode(''.join(board)))\n            output_sequences.append(x_move)\n            board[x_move] = 'X'\n            if i < len(o_moves):\n                board[o_moves[i]] = 'O'\n        return\n    elif winner == 'O':\n        return  # We don't add these to our training data\n\n    # Before simulating further moves, check if a winning move is available\n    legal_moves = game.get_legal_moves()\n    for move in legal_moves:\n        test_game = deepcopy(game)\n        test_game.make_move(move)\n        if test_game.check_winner() == game.current_player:\n            # This move is a winning move, so we make it and end further simulation\n            if test_game.current_player == 'X':\n                simulate_all_games(test_game, x_moves + [move], o_moves)\n            else:\n                simulate_all_games(test_game, x_moves, o_moves + [move])\n            return  # End further exploration for this branch\n\n    # If no immediate winning move is found, continue simulating the game\n    for move in legal_moves:\n        # Create a copy of the game to simulate the move\n        new_game = deepcopy(game)\n        was_legal = new_game.make_move(move)\n\n        # If the move was legal, continue simulating\n        if was_legal:\n            if new_game.current_player == 'X':\n                simulate_all_games(new_game, x_moves + [move], o_moves)\n            else:\n                simulate_all_games(new_game, x_moves, o_moves + [move])\n\n# Create an initial empty game\ninitial_game = TicTacToe()\n\n# Simulate all possible games starting with 'X'\nsimulate_all_games(initial_game, [], [])\n\n# Convert to PyTorch tensors\ninput_tensor = torch.tensor(input_sequences, dtype=torch.long)\noutput_tensor = torch.tensor(output_sequences, dtype=torch.long)\n```", "```py\nprint(\"Number of input-output pairs:\", len(input_sequences))\n\nNumber of input-output pairs: 147104\n```", "```py\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n```", "```py\nwei = q @ k.transpose(-2,-1) # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n```", "```py\ntril = torch.tril(torch.ones((T,T)))\nwei = wei.masked_fill(tril==0, float(\"-inf\"))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x) # (B,T,16)\nout = wei @ v\nout.shape\n```", "```py\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Hyperparameters\nbatch_size = 128  # How many independent sequences will we process in parallel?\nblock_size = 9  # The size of the tic-tac-toe board\nmax_iters = 10000\neval_interval = 500\nlearning_rate = 1e-3\ndevice = 'mps' if torch.backends.mps.is_available() else 'cpu'\neval_iters = 100\nn_embd = 32  # Reduced the embedding size\nn_head = 2  # Reduced the number of heads\nn_layer = 2  # Reduced the number of layers\ndropout = 0.1\n\nprint(f'Training on {device}')\n\n# Initialize random seed\ntorch.manual_seed(1337)\n\n# Split into training and validation sets\nn = int(0.90 * len(input_tensor))  # 90% for training\ntrain_input = input_tensor[:n]\ntrain_output = output_tensor[:n]\nval_input = input_tensor[n:]\nval_output = output_tensor[n:]\n\n# Updated data loading function\ndef get_batch(split):\n    input_data = train_input if split == 'train' else val_input\n    output_data = train_output if split == 'train' else val_output\n    # Choose index locs for batch_size sequences\n    ix = torch.randint(len(input_data) - block_size + 1, (batch_size,))\n    # Get the input and output sequences\n    x = input_data[ix]\n    y = output_data[ix]\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x) # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,hs)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass Transformer(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, 9)\n\n        # better init, not covered in the original GPT video, but important, will cover in followup video\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n        x = tok_emb + pos_emb  # (B,T,C)\n        x = self.blocks(x)  # (B,T,C)\n        x = self.ln_f(x)  # (B,T,C)\n        logits = self.lm_head(x)  # (B,T,vocab_size)\n        # Take the logits corresponding to the last time step T\n        logits = logits[:, -1, :]  # Now logits is (B, 9)\n\n        if targets is None:\n            loss = None\n        else:\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n```", "```py\nxb, yb = get_batch('train')\nprint(xb.shape, yb.shape)\nm = Transformer().to(device)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(f\"Loss: {loss.item():.3f}\")\n\ntorch.Size([128, 9]) torch.Size([128])\ntorch.Size([128, 9])\nLoss: 2.203\n```", "```py\nmodel = Transformer()\nmodel = model.to(device)\n\n# Print the number of parameters in the model\nprint(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\ntrain_loss_history = []\nval_loss_history = []\n\n# Training loop\nfor iter in tqdm(range(max_iters)):\n    # Evaluate the loss on train and val sets occasionally\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n        val_loss_history.append(losses['val'])\n\n    # Sample a batch of data\n    xb, yb = get_batch('train')\n\n    # Evaluate the loss\n    logits, loss = model(xb, yb)\n    train_loss_history.append(loss.item())\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n0.025961 M parameters\n  0%|          | 5/10000 [00:00<24:42,  6.74it/s]  \nstep 0: train loss 2.2033, val loss 2.2106\n  5%|▌         | 504/10000 [00:14<12:01, 13.15it/s]\nstep 500: train loss 1.9162, val loss 2.0215\n 10%|█         | 1008/10000 [00:27<08:27, 17.73it/s]\nstep 1000: train loss 1.7846, val loss 1.8570\n 15%|█▌        | 1505/10000 [00:40<10:34, 13.39it/s]\nstep 1500: train loss 1.7370, val loss 1.7648\n 20%|██        | 2007/10000 [00:53<07:35, 17.55it/s]\nstep 2000: train loss 1.7188, val loss 1.7770\n 25%|██▌       | 2506/10000 [01:05<07:11, 17.36it/s]\nstep 2500: train loss 1.6957, val loss 1.7456\n 30%|███       | 3006/10000 [01:18<06:35, 17.69it/s]\nstep 3000: train loss 1.6965, val loss 1.7448\n 35%|███▌      | 3506/10000 [01:31<06:12, 17.41it/s]\nstep 3500: train loss 1.6961, val loss 1.7809\n 40%|████      | 4005/10000 [01:43<07:41, 12.98it/s]\nstep 4000: train loss 1.6819, val loss 1.7256\n 45%|████▌     | 4506/10000 [01:56<05:18, 17.24it/s]\nstep 4500: train loss 1.6892, val loss 1.7066\n 50%|█████     | 5005/10000 [02:09<05:14, 15.88it/s]\nstep 5000: train loss 1.6846, val loss 1.7141\n 55%|█████▌    | 5508/10000 [02:23<04:37, 16.19it/s]\nstep 5500: train loss 1.6835, val loss 1.6998\n 60%|██████    | 6004/10000 [02:36<05:19, 12.51it/s]\nstep 6000: train loss 1.6828, val loss 1.7095\n 65%|██████▌   | 6506/10000 [02:49<03:23, 17.13it/s]\nstep 6500: train loss 1.6722, val loss 1.7151\n 70%|███████   | 7008/10000 [03:02<03:05, 16.17it/s]\nstep 7000: train loss 1.6656, val loss 1.7158\n 75%|███████▌  | 7505/10000 [03:15<02:30, 16.54it/s]\nstep 7500: train loss 1.6672, val loss 1.7078\n 80%|████████  | 8007/10000 [03:28<02:01, 16.38it/s]\nstep 8000: train loss 1.6808, val loss 1.7120\n 85%|████████▌ | 8505/10000 [03:41<01:47, 13.94it/s]\nstep 8500: train loss 1.6733, val loss 1.7144\n 90%|█████████ | 9007/10000 [03:54<00:56, 17.54it/s]\nstep 9000: train loss 1.6714, val loss 1.7031\n 95%|█████████▌| 9506/10000 [04:07<00:28, 17.39it/s]\nstep 9500: train loss 1.6707, val loss 1.7073\n100%|██████████| 10000/10000 [04:20<00:00, 38.43it/s]\nstep 9999: train loss 1.6664, val loss 1.7506\n```", "```py\nimport matplotlib.pyplot as plt\n\ndef plot_transformer_loss(loss_history, val_loss_history):\n\n    # Two horizontal figures side-by-side\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n    # First plot = training loss\n    ax1.plot(loss_history, lw=0.5)\n    # Plot moving average of loss\n    window_size = 100\n    ax1.plot(np.convolve(loss_history, np.ones(window_size) / window_size, mode='valid'), label='Moving average')\n    ax1.set_xlabel('Iteration')\n    ax1.set_ylabel('Cross-entropy Loss')\n    ax1.set_title('Training Loss')\n    ax1.legend()\n\n    # Second plot = validation loss\n    # Set marker style to be circles at each data point\n    indices = np.arange(0, len(val_loss_history) * eval_interval, eval_interval)\n    ax2.plot(indices, val_loss_history, marker='o')\n    ax2.set_title('Validation Loss')\n    ax2.set_xlabel('Iteration')\n\n    plt.show()\n\nplot_transformer_loss(train_loss_history, val_loss_history)\n```", "```py\nimport random\nimport torch\nfrom IPython.display import clear_output\n\ndef play_game(model, stoi, itos, device):\n    game = TicTacToe()\n\n    # Randomly decide who goes first\n    game.current_player = random.choice(['X', 'O'])\n\n    while game.check_winner() is None and not game.is_draw():\n        #clear_output(wait=True)\n\n        print(f\"{game.current_player}'s turn.\")\n        game.pretty_print_board()\n\n        current_board_str = game.get_board_string()\n\n        if game.current_player == 'X':\n            print(\"Model's turn...\")\n            current_board_encoded = torch.tensor([stoi[c] for c in current_board_str], dtype=torch.long).unsqueeze(0).to(device)\n            logits, _ = model(current_board_encoded)\n\n            # Move logits to cpu\n            logits = logits.cpu()\n\n            # Create a mask for legal moves and zero out logits for illegal moves\n            legal_moves = game.get_legal_moves()\n            mask = torch.zeros(9)\n            mask[legal_moves] = 1\n            masked_logits = logits * mask\n\n            # Get the model's move\n            predicted_move = masked_logits.argmax(dim=-1).item()\n\n            # Make the model's move\n            game.make_move(predicted_move)\n\n        else:\n            print(\"Your turn!\")\n            legal_moves = game.get_legal_moves()\n            print(\"Legal moves:\", legal_moves)\n\n            user_move = int(input(\"Enter your move: \"))\n            if user_move in legal_moves:\n                game.make_move(user_move)\n            else:\n                print(\"Illegal move. Try again.\")\n                continue\n\n        winner = game.check_winner()\n        if winner is not None:\n            #clear_output(wait=True)\n            print(f\"{winner} wins!\")\n            game.pretty_print_board()\n            break\n        elif game.is_draw():\n            #clear_output(wait=True)\n            print(\"It's a draw!\")\n            game.pretty_print_board()\n            break\n\nO's turn.\n- | - | -\n- - - - - \n- | - | -\n- - - - - \n- | - | -\nYour turn!\nLegal moves: [0, 1, 2, 3, 4, 5, 6, 7, 8]\nX's turn.\n- | - | -\n- - - - - \n- | - | -\n- - - - - \n- | - | O\nModel's turn...\nO's turn.\n- | - | X\n- - - - - \n- | - | -\n- - - - - \n- | - | O\nYour turn!\nLegal moves: [0, 1, 3, 4, 5, 6, 7]\nX's turn.\nO | - | X\n- - - - - \n- | - | -\n- - - - - \n- | - | O\nModel's turn...\nO's turn.\nO | - | X\n- - - - - \nX | - | -\n- - - - - \n- | - | O\nYour turn!\nLegal moves: [1, 4, 5, 6, 7]\nO wins!\nO | - | X\n- - - - - \nX | O | -\n- - - - - \n- | - | O\n```", "```py\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndevice = 'cpu'\n\nclass TicTacToeNN(nn.Module):\n    def __init__(self):\n        super(TicTacToeNN, self).__init__()\n        self.fc1 = nn.Linear(9, 16)  # Input layer to hidden layer 1\n        self.fc2 = nn.Linear(16, 32)  # Hidden layer 1 to hidden layer 2\n        self.fc3 = nn.Linear(32, 16)  # Hidden layer 2 to hidden layer \n        self.fc4 = nn.Linear(16, 9)  # Hidden layer 3 to output layer\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = self.fc4(x)\n        return x\n\ndef get_batch(split):\n    input_data = train_input if split == 'train' else val_input\n    output_data = train_output if split == 'train' else val_output\n    # Choose index locs for batch_size sequences\n    ix = torch.randint(len(input_data) - block_size + 1, (batch_size,))\n    # Get the input and output sequences\n    x = input_data[ix].float()\n    y = output_data[ix]\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n# Create an initial empty game\ninitial_game = TicTacToe()\n\n# Simulate all possible games starting with 'X'\nsimulate_all_games(initial_game, [], [])\n\n# Convert to PyTorch tensors\ninput_tensor = torch.tensor(input_sequences, dtype=torch.long)\noutput_tensor = torch.tensor(output_sequences, dtype=torch.long)\n\nnn_model = TicTacToeNN()\nnn_model.to(device)\n\n# Print the number of parameters in the model\nprint(sum(p.numel() for p in nn_model.parameters()), 'parameters')\n\n# Create a PyTorch optimizer\noptimizer = torch.optim.AdamW(nn_model.parameters(), lr=learning_rate, weight_decay=1e-4)\ntrain_loss_history = []\nval_loss_history = []\n\n# Training loop\nmax_iters = 1000000\nfor iter in tqdm(range(max_iters)):\n    # Evaluate the loss on train and val sets occasionally\n\n    # Sample a batch of data\n    xb, yb = get_batch('train')\n    # Evaluate the loss\n    logits = nn_model(xb)\n    # Calculate cross-entropy loss\n    loss = F.cross_entropy(logits, yb)\n    train_loss_history.append(loss.item())\n    # Get the validation loss\n    xb, yb = get_batch('val')\n    logits = nn_model(xb)\n    val_loss = F.cross_entropy(logits, yb)\n    val_loss_history.append(val_loss.item())\n    # Backpropagate and update the weights\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n1385 parameters\n100%|██████████| 1000000/1000000 [08:08<00:00, 2048.42it/s]\n```", "```py\nfrom copy import deepcopy\n\n# Helper function to find if there's a winning move or a move that blocks the opponent from winning\ndef find_winning_or_blocking_move(board, player):\n    winning_positions = [\n        [0, 1, 2], [3, 4, 5], [6, 7, 8],  # Rows\n        [0, 3, 6], [1, 4, 7], [2, 5, 8],  # Columns\n        [0, 4, 8], [2, 4, 6]  # Diagonals\n    ]\n    for positions in winning_positions:\n        values = [board[pos] for pos in positions]\n        if values.count(player) == 2 and values.count('-') == 1:\n            return positions[values.index('-')]\n    return None\n\n# Helper function for checking for fork opportunities\ndef find_fork_move(board, player):\n    fork_move = None\n    for i in range(9):\n        if board[i] == '-':\n            temp_board = board[:]\n            temp_board[i] = player\n            winning_moves = 0\n            for j in range(9):\n                if temp_board[j] == '-':\n                    temp_board_2 = temp_board[:]\n                    temp_board_2[j] = player\n                    if find_winning_or_blocking_move(temp_board_2, player) is not None:\n                        winning_moves += 1\n            if winning_moves >= 2:\n                fork_move = i\n                break\n    return fork_move\n\n# Helper function to find the optimal move according to a defined strategy\ndef optimal_strategy(board, player):\n    opponent = 'O' if player == 'X' else 'X'\n\n    # 1\\. Win: If you have two in a row, play the third to get three in a row.\n    win_move = find_winning_or_blocking_move(board, player)\n    if win_move is not None:\n        return win_move\n\n    # 2\\. Block: If the opponent has two in a row, play the third to block them.\n    block_move = find_winning_or_blocking_move(board, opponent)\n    if block_move is not None:\n        return block_move\n\n    # 3\\. Fork: Create an opportunity where you can win in two ways.\n    fork_move = find_fork_move(board, player)\n    if fork_move is not None:\n        return fork_move\n\n    # 4\\. Block Opponent's Fork\n    opponent_fork_move = find_fork_move(board, opponent)\n    if opponent_fork_move is not None:\n        return opponent_fork_move\n\n    # 5\\. Center: Play the center.\n    if board[4] == '-':\n        return 4\n\n    # 6\\. Opposite Corner: If the opponent is in the corner, play the opposite corner.\n    corners = [(0, 8), (2, 6), (8, 0), (6, 2)]\n    for corner1, corner2 in corners:\n        if board[corner1] == opponent and board[corner2] == '-':\n            return corner2\n\n    # 7\\. Empty Corner: Play an empty corner.\n    for corner in [0, 2, 6, 8]:\n        if board[corner] == '-':\n            return corner\n\n    # 8\\. Empty Side: Play an empty side.\n    for side in [1, 3, 5, 7]:\n        if board[side] == '-':\n            return side\n\n# Function to simulate all games according to the optimal strategy\ndef simulate_all_games_optimal_v2(game, x_starts=True):\n    global input_sequences, output_sequences\n\n    # Check for terminal state\n    winner = game.check_winner()\n    if winner or game.is_draw():\n        return\n\n    # If it's X's turn, apply the optimal strategy and save the board state and move\n    if game.current_player == 'X':\n        move = optimal_strategy(game.board, 'X')\n        if move is None:\n            move = game.get_legal_moves()[0]  # fallback\n        input_sequences.append(encode(''.join(game.board)))\n        output_sequences.append(move)\n        new_game = deepcopy(game)\n        new_game.make_move(move)\n        simulate_all_games_optimal_v2(new_game, x_starts)\n    else:\n        # If it's O's turn, explore all possible legal moves\n        for move in game.get_legal_moves():\n            new_game = deepcopy(game)\n            new_game.make_move(move)\n            simulate_all_games_optimal_v2(new_game, x_starts)\n\n# Character to integer mapping\nchars = sorted(list(set('XO-')))\nvocab_size = len(chars)\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {0: '-', 1: 'X', 2: 'O'}\nencode = lambda s: [stoi[c] for c in s]\ndecode = lambda l: ''.join([itos[i] for i in l])\n\n# Reset and re-simulate\ninput_sequences = []\noutput_sequences = []\n\n# 'X' starts\ninitial_game = TicTacToe()\nsimulate_all_games_optimal_v2(initial_game, True)\n\n# 'O' starts\ninitial_game = TicTacToe()\ninitial_game.current_player = 'O'\nsimulate_all_games_optimal_v2(initial_game, False)\n\n# Convert to Pytorch tensors\ninput_tensor = torch.tensor(input_sequences, dtype=torch.long)\noutput_tensor = torch.tensor(output_sequences, dtype=torch.long)\n\nprint(\"Number of input-output pairs:\", len(input_sequences))\n\nNumber of input-output pairs: 1017\n```", "```py\n# Hyperparameters\nbatch_size = 128  # How many independent sequences will we process in parallel?\nblock_size = 9  # The size of the tic-tac-toe board\nmax_iters = 10000\neval_interval = 500\nlearning_rate = 1e-3\ndevice = 'mps' if torch.backends.mps.is_available() else 'cpu'\neval_iters = 100\nn_embd = 32  # Reduced the embedding size\nn_head = 2  # Reduced the number of heads\nn_layer = 2  # Reduced the number of layers\ndropout = 0.1\n\nprint(f'Training on {device}')\n\n# Initialize random seed\ntorch.manual_seed(1337)\n\n# Split into training and validation sets\nn = int(0.90 * len(input_tensor))  # 90% for training\ntrain_input = input_tensor[:n]\ntrain_output = output_tensor[:n]\nval_input = input_tensor[n:]\nval_output = output_tensor[n:]\n\n# Updated data loading function\ndef get_batch(split):\n    input_data = train_input if split == 'train' else val_input\n    output_data = train_output if split == 'train' else val_output\n    # Choose index locs for batch_size sequences\n    ix = torch.randint(len(input_data) - block_size + 1, (batch_size,))\n    # Get the input and output sequences\n    x = input_data[ix]\n    y = output_data[ix]\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n# Initialize the model\nmodel = Transformer()\nmodel = model.to(device)\n\nmax_iters = 5000\n\n# Print the number of parameters in the model\nprint(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n\n# Create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\ntrain_loss_history = []\nval_loss_history = []\n\n# Training loop\nfor iter in tqdm(range(max_iters)):\n    # Evaluate the loss on train and val sets occasionally\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n        val_loss_history.append(losses['val'])\n\n    # Sample a batch of data\n    xb, yb = get_batch('train')\n\n    # Evaluate the loss\n    logits, loss = model(xb, yb)\n    train_loss_history.append(loss.item())\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nTraining on mps\n0.025961 M parameters\n  0%|          | 6/5000 [00:00<10:15,  8.11it/s]  \nstep 0: train loss 2.2078, val loss 2.2166\n 10%|█         | 505/5000 [00:13<05:23, 13.89it/s]\nstep 500: train loss 0.3063, val loss 0.6145\n 20%|██        | 1005/5000 [00:26<04:40, 14.24it/s]\nstep 1000: train loss 0.0741, val loss 0.2259\n 30%|███       | 1505/5000 [00:38<04:08, 14.05it/s]\nstep 1500: train loss 0.0368, val loss 0.1799\n 40%|████      | 2005/5000 [00:51<03:36, 13.83it/s]\nstep 2000: train loss 0.0134, val loss 0.1589\n 50%|█████     | 2504/5000 [01:04<02:57, 14.06it/s]\nstep 2500: train loss 0.0081, val loss 0.0884\n 60%|██████    | 3008/5000 [01:17<01:56, 17.06it/s]\nstep 3000: train loss 0.0041, val loss 0.0521\n 70%|███████   | 3505/5000 [01:29<01:46, 14.09it/s]\nstep 3500: train loss 0.0028, val loss 0.0855\n 80%|████████  | 4005/5000 [01:42<01:10, 14.06it/s]\nstep 4000: train loss 0.0036, val loss 0.1125\n 90%|█████████ | 4506/5000 [01:56<00:29, 16.68it/s]\nstep 4500: train loss 0.0014, val loss 0.0892\n100%|██████████| 5000/5000 [02:08<00:00, 38.79it/s]\nstep 4999: train loss 0.0026, val loss 0.0721\n```", "```py\nO's turn.\n- | - | -\n- - - - - \n- | - | -\n- - - - - \n- | - | -\nYour turn!\nLegal moves: [0, 1, 2, 3, 4, 5, 6, 7, 8]\nX's turn.\n- | O | -\n- - - - - \n- | - | -\n- - - - - \n- | - | -\nModel's turn...\nO's turn.\nX | O | -\n- - - - - \n- | - | -\n- - - - - \n- | - | -\nYour turn!\nLegal moves: [2, 3, 4, 5, 6, 7, 8]\nX's turn.\nX | O | -\n- - - - - \n- | - | -\n- - - - - \n- | O | -\nModel's turn...\nO's turn.\nX | O | -\n- - - - - \n- | X | -\n- - - - - \n- | O | -\nYour turn!\nLegal moves: [2, 3, 5, 6, 8]\nX's turn.\nX | O | -\n- - - - - \n- | X | -\n- - - - - \n- | O | O\nModel's turn...\nO's turn.\nX | O | -\n- - - - - \n- | X | -\n- - - - - \nX | O | O\nYour turn!\nLegal moves: [2, 3, 5]\nX's turn.\nX | O | O\n- - - - - \n- | X | -\n- - - - - \nX | O | O\nModel's turn...\nX wins!\nX | O | O\n- - - - - \nX | X | -\n- - - - - \nX | O | O\n```"]