["```py\nfrom abc import ABC, abstractmethod\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\n\nclass LLMBot(ABC):\n    \"\"\"Class definition for a single LLM bot\"\"\"\n\n    def __init__(self, endpoint_type, temperature):\n        \"\"\"Initialize the large language model.\n\n        Args:\n        --------------\n        endpoint_type: \"chat\" or \"completion\".\n        temperature: temperature of the LLM.\n        \"\"\"        \n        # Instantiate llm\n        # Reminder: need to set up openAI API key \n        # (e.g., via environment variable OPENAI_API_KEY)\n        if endpoint_type == 'chat':\n            self.llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", \n                                temperature=temperature)\n\n        elif endpoint_type == 'completion':\n            self.llm = OpenAI(model_name=\"text-davinci-003\", \n                            temperature=temperature)\n\n        else:\n            raise KeyError(\"Currently unsupported endpoint type!\")\n\n    @abstractmethod\n    def instruct(self):\n        \"\"\"Determine the context of LLM bot behavior. \n        \"\"\"\n        pass\n\n    @abstractmethod\n    def step(self):\n        \"\"\"Response produced by the LLM bot. \n        \"\"\"\n        pass\n```", "```py\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import (\n    ChatPromptTemplate, \n    MessagesPlaceholder, \n    SystemMessagePromptTemplate, \n    HumanMessagePromptTemplate\n)\n\nclass ScenarioGenerator(LLMBot):\n    \"\"\"Class definition for the scenario generator bot.\"\"\"\n\n    def __init__(self, temperature=1.0):       \n        \"\"\"Setup scenario generator bot.\n\n        Args:\n        --------------\n        temperature: temperature of the LLM.\n        \"\"\"   \n\n        # Instantiate llm\n        super().__init__('chat', temperature)\n\n        # Instantiate memory\n        self.memory = ConversationBufferMemory(return_messages=True)\n\n    def instruct(self, industry, business_size, problem_type, details):\n        \"\"\"Determine the context of scenario generator. \n\n        Args:\n        --------------\n        industry: interested industry, e.g., healthcare, finance, etc.\n        business_size: large, medium, small\n        problem_type: type of machine learning problem, e.g., classification, regression, etc.\n        details: specific details added to the description.\n        \"\"\"        \n\n        self.industry = industry\n        self.business_size = business_size\n        self.problem_type = problem_type\n        self.details = ScenarioGenerator.industry_specifics[industry]\n\n        prompt = ChatPromptTemplate.from_messages([\n            MessagesPlaceholder(variable_name=\"history\"),\n            HumanMessagePromptTemplate.from_template(\"\"\"{input}\"\"\")\n        ])\n\n        # Construct a conversation chain\n        self.scen_generator = ConversationChain(memory=self.memory, prompt=prompt, \n                                                llm=self.llm)\n\n    def step(self):\n        \"\"\"Interact with the LLM bot. \n\n        Outputs:\n        --------------\n        scenario: the generated scenario description.\n        \"\"\"       \n\n        # 1st stage (draft)\n        print(\"Generating scenario description: drafting stage...\")\n        prompt_1st = self._get_1st_stage_prompt()\n        self.interm_scenario = self.scen_generator.predict(input=prompt_1st)\n\n        # 2nd stage (review and enrich)\n        print(\"Generating scenario description: refining stage...\")\n        prompt_2nd = self._get_2nd_stage_prompt()\n        self.scenario = self.scen_generator.predict(input=prompt_2nd)\n        print(\"Scenario description generated!\")\n\n        return self.scenario\n```", "```py\ndef _get_1st_stage_prompt(self):\n\n    # Note that the prompt is generated and fine-tuned by ChatGPT (GPT-4)\n    prompt = f\"\"\"For a {self.industry} company of {self.business_size} size \n    focusing on {self.problem_type} problems, generate a concrete data science \n    project scenario that a data scientist might encounter in real life. \n    Please provide concrete and specific details relevant to the selected \n    industry and problem type.\n\n    For the generated scenario, please provide:\n    1\\. A specific and realistic description of a problem faced by the company.\n    2\\. The desired outcome that the company is hoping to achieve by solving \n    the problem.\n    3\\. A list of the top 3 most relevant data sources that might be available \n    for solving the problem.\n\n    Output format:\n    Problem description: [content of problem description]\n    Desired outcome: [content of desired outcome]\n    Available data: [content of available data]\n    \"\"\"\n\n    return prompt\n```", "```py\nclass ScenarioGenerator(LLMBot):\n\n    # Note that the descriptions of the industry specifics\n    # are generated and optimized by ChatGPT (GPT-4)\n    industry_specifics = {\n        'healthcare': \"\"\"types of patients treated (e.g., age, medical conditions), \n        common treatments and procedures, challenges faced in patient care, \n        medical equipment used.\"\"\",\n\n        'finance': \"\"\"types of financial products and services offered, \n        ...\"\"\",\n\n        ...\n    }\n```", "```py\ndef _get_2nd_stage_prompt(self):\n\n    # Note that the prompt is generated and fine-tuned by ChatGPT (GPT-4)\n    prompt = f\"\"\"Based on the previously generated scenario, please enrich \n    the problem description by providing more specific details \n    (such as {self.details}) about the problem.\n\n    Output format:\n    Enriched problem description: [content of enriched problem description]\n    Desired outcome: [content of desired outcome]\n    Available data: [content of available data]\n    \"\"\"\n\n    return prompt\n```", "```py\n# User selections\nindustry = \"manufacturing\"\nbusiness_size = \"medium\"\nproblem_type = \"anomaly detection\"\ndetails = \"Types of products manufactured, machines used in the production process, \\\ncommon issues faced by the company, tools and technologies used for quality control.\"\n\n# Scenario generation\ngenerator = ScenarioGenerator()\ngenerator.instruct(industry, business_size, problem_type, details)\nscenario = generator.step()\n```", "```py\nclass ClientBot(LLMBot):\n    \"\"\"Class definition for the client bot.\"\"\"\n\n    def __init__(self, temperature=0.8):       \n        \"\"\"Setup scenario generator bot.\n\n        Args:\n        --------------\n        temperature: temperature of the LLM.\n        \"\"\"   \n\n        # Instantiate llm\n        super().__init__('chat', temperature)\n\n        # Instantiate memory\n        self.memory = ConversationBufferMemory(return_messages=True)\n\n    def instruct(self, industry, business_size, scenario):\n        \"\"\"Determine the context of client chatbot. \n        \"\"\"\n\n        self.industry = industry\n        self.business_size = business_size\n        self.scenario = scenario\n\n        # Define prompt template\n        prompt = ChatPromptTemplate.from_messages([\n            SystemMessagePromptTemplate.from_template(self._specify_system_message()),\n            MessagesPlaceholder(variable_name=\"history\"),\n            HumanMessagePromptTemplate.from_template(\"\"\"{input}\"\"\")\n        ])\n\n        # Create conversation chain\n        self.conversation = ConversationChain(memory=self.memory, prompt=prompt, \n                                              llm=self.llm, verbose=False)\n\n    def step(self, prompt):\n        \"\"\"Client chatbot speaks. \n\n        Args:\n        --------\n        prompt: data scientist's response.\n        \"\"\"\n        response = self.conversation.predict(input=prompt)\n\n        return response\n```", "```py\ndef _specify_system_message(self):\n    \"\"\"Specify the behavior of the client chatbot.\n    \"\"\"      \n\n    # Prompt\n    # The prompt is generated and fine-tuned by ChatGPT (GPT-4)\n    prompt = f\"\"\"You are role-playing a representative from a {self.industry} \n    company of {self.business_size} size and you are meeting with a \n    data scientist (which is played by another bot), to discuss how to \n    leverage machine learning to address a problem your company is facing. \n\n    The problem description, desired outcome, and available data are:\n    {self.scenario}.\n\n    Your ultimate goal is to work with the data scientist to define a clear \n    problem and agree on a suitable data science solution or approach.\n\n    Guidelines to keep in mind:\n    - **Get Straight to the Point**: Start the conversation by directly \n      addressing the problem at hand. There is no need for pleasantries or \n      introductions.\n    - **Engage in Conversation**: Respond to the data scientist's questions \n      and prompts. Do not provide all the information at once or provide \n      the entire conversation yourself.\n    - **Clarify and Confirm**: Always make sure to clarify and confirm the \n      problem, desired outcome, and any proposed solutions with the data \n      scientist. \n    - **Stay in Role**: Your role as a client is to represent your company's \n      needs and work with the data scientist to define a clear problem and \n      agree on a suitable data science solution or approach. \n      Do not try to propose solutions.\n    - **Provide Information as Needed**: Provide information about the problem,\n      available data, constraints, and requirements as it becomes relevant in \n      the conversation. If the data scientist asks a question and the \n      information was not provided in the problem description, it is okay to \n      improvise and create details that seem reasonable.\n    - **Collaborate**: Collaborate with the data scientist to clearly define \n      the problem and to consider any proposed solutions or approaches.\n    \"\"\"\n\n    return prompt\n```", "```py\ndef instruct(self, industry, business_size, problem_type):\n    \"\"\"Determine the context of data scientist chatbot. \n    \"\"\"\n\n    self.industry = industry\n    self.business_size = business_size\n    self.problem_type = problem_type\n\n    # Define prompt template\n    ...\n\n    # Create conversation chain\n    ...\n```", "```py\ndef _specify_system_message(self):\n    \"\"\"Specify the behavior of the data scientist chatbot.\n    \"\"\"      \n\n    # Prompt\n    # The prompt is generated and fine-tuned by ChatGPT (GPT-4)\n    prompt = f\"\"\"You are role-playing a data scientist meeting with a \n    representative (which is played by another chatbot) \n    from a {self.industry} company of {self.business_size} size. \n    They are currently concerned with a {self.problem_type} problem.\n\n    Your ultimate goal is to understand the problem in depth and agree on a \n    suitable data science solution or approach by engaging in a conversation \n    with the client representative. \n\n    Guidelines to keep in mind:\n    - **Engage in Conversation**: You are only the data scientist. \n    Do not provide the entire conversation yourself.\n    - **Understand the Problem**: Make sure to ask questions to get a clear \n    and detailed understanding of the problem, the desired outcome, \n    available data, constraints, and requirements.\n    - **Propose Solutions**: Based on the information provided by the client, \n    suggest possible data science approaches or solutions to address the \n    problem.\n    - **Consider Constraints**: Be mindful of any constraints that the client \n    may have, such as budget, timeline, or data limitations, and tailor your \n    proposed solutions accordingly.\n    - **Collaborate**: Collaborate with the client to refine the problem \n    definition, proposed solutions, and ultimately agree on a suitable data \n    science approach.\n    \"\"\"\n\n    return prompt\n```", "```py\n# Create two chatbots\nclient = ClientBot()\ndata_scientist = DataScientistBot()\n\n# Specify instructions\nclient.instruct(industry, business_size, scenario)\ndata_scientist.instruct(industry, business_size, problem_type)\n\n# Book-keeping\nquestion_list = []\nanswer_list = []\n\n# Start conversation\nfor i in range(6):\n    if i == 0:\n        question = client.step('Start the conversation')\n    else:\n        question = client.step(answer)\n    question_list.append(question)\n    print(\"👨‍💼 Client: \" + question)\n\n    answer = data_scientist.step(question)\n    answer_list.append(answer)\n\n    print(\"👩‍💻 Data Scientist: \" + answer)\n    print(\"\\n\\n\")\n```", "```py\nclass SummarizerBot(LLMBot):\n\n    def __init__(self, temperature=0.8):       \n        \"\"\"Setup summarizer bot.\n\n        Args:\n        --------------\n        temperature: temperature of the LLM.\n        \"\"\"   \n\n        # Instantiate llm\n        super().__init__('completion', temperature)\n```", "```py\ndef instruct(self):\n    \"\"\"Determine the context of summarizer. \n    \"\"\"        \n\n    # Note: The prompt is generated and optimized by ChatGPT (GPT-4)\n    template = \"\"\"Please concisely summarize the following segment of a \n    conversation between a client and a data scientist discussing a \n    potential data science project:\n\n    {conversation}\n    \"\"\"\n\n    self.prompt = PromptTemplate(\n        template=template,\n        input_variables=[\"conversation\"],\n    )\n```", "```py\ndef step(self, q_list, a_list):\n    \"\"\"Summarize the conversation script. \n\n    Args:\n    ---------\n    q_list: list of responses from the client bot\n    a_list: list of responses from the data scientist bot\n    \"\"\"     \n\n    # Loop over individual rounds\n    conversation_summary = []\n    for i, (q, a) in enumerate(zip(q_list, a_list)):\n        print(f\"Processing {i+1}/{len(q_list)}th conversation round.\")\n\n        # Compile one round of conversation\n        conversation_round = ''\n        conversation_round += 'Client: ' + q + '\\n\\n'\n        conversation_round += 'Data scientist: ' + a\n\n        response = self.llm.predict(self.prompt.format(conversation=conversation_round))\n        conversation_summary.append(response)\n\n    return conversation_summary\n```", "```py\nclass AssessorBot(LLMBot):\n\n    def __init__(self, temperature=0.8):       \n        \"\"\"Setup assessor bot.\n\n        Args:\n        --------------\n        temperature: temperature of the LLM.\n        \"\"\"   \n\n        # Instantiate llm\n        super().__init__('completion', temperature)\n```", "```py\ndef instruct(self, industry, business_size, problem_type):\n    \"\"\"Determine the context of assessor. \n    \"\"\"        \n\n    self.industry = industry\n    self.business_size = business_size\n    self.problem_type = problem_type\n\n    # Note: The prompt is generated and optimized by ChatGPT (GPT-4)\n    template = \"\"\"You are a senior data scientist who has been asked to \n    review a conversation between a data scientist and a client from a \n    {industry} company of {business_size} size, focusing on a {problem_type} \n    problem. The client and data scientist are discussing how to define and \n    scope a data science project to address the problem.\n\n    Please provide an assessment of the conversation, focusing on the strategy \n    adopted by the data scientist to define and scope the problem, \n    any potential room for improvement, and any other key points you think \n    are important. Please organize your response with nicely formatted \n    bulletpoints.\n\n    Here is the conversation: \n    {conversation}\n    \"\"\"\n\n    self.prompt = PromptTemplate(\n        template=template,\n        input_variables=[\"industry\", \"business_size\", \n                        \"problem_type\", \"conversation\"],\n    )\n```", "```py\ndef step(self, conversation_summary):\n    \"\"\"Assess the conversation script. \n\n    Args:\n    ---------\n    conversation_summary: condensed version of the conversation script.\n    \"\"\"     \n\n    analysis = self.llm.predict(self.prompt.format(industry=industry,\n                                                    business_size=business_size,\n                                                    problem_type=problem_type,\n                                                    conversation=' '.join(conversation_summary)))\n\n    return analysis\n```"]