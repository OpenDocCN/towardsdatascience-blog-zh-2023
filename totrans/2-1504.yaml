- en: Mean Average Precision at K (MAP@K) clearly explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/mean-average-precision-at-k-map-k-clearly-explained-538d8e032d2](https://towardsdatascience.com/mean-average-precision-at-k-map-k-clearly-explained-538d8e032d2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: One of the most popular evaluation metrics for recommender or ranking problems
    step by step explained
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://konstantin-rink.medium.com/?source=post_page-----538d8e032d2--------------------------------)[![Konstantin
    Rink](../Images/41bfc069d7382a0fd56f081eea7eb2d9.png)](https://konstantin-rink.medium.com/?source=post_page-----538d8e032d2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----538d8e032d2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----538d8e032d2--------------------------------)
    [Konstantin Rink](https://konstantin-rink.medium.com/?source=post_page-----538d8e032d2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----538d8e032d2--------------------------------)
    ·7 min read·Jan 18, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8af88de2aaea97d6425f70111af801f.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Joshua Burdick](https://unsplash.com/de/@gadgetpastor) on [Unsplash](https://unsplash.com/de/Fotos/0xpFNUjzdbw).
  prefs: []
  type: TYPE_NORMAL
- en: Mean Average Precision at K (MAP@K) is one of the most commonly used evaluation
    metrics for recommender systems and other **ranking** related classification tasks.
    Since this metric is a composition of different error metrics or layers, it may
    not be that easy to understand at first glance.
  prefs: []
  type: TYPE_NORMAL
- en: This article explains MAP@K and its components **step by step**. At the end
    of this article you will also find **code snippets** how to calculate the metric.
    But before diving into each part of the metric, let’s talk about the **WHY** first.
  prefs: []
  type: TYPE_NORMAL
- en: WHY use MAP@K?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MAP@K is an error metric that can be used when the **sequence** or **ranking**
    of your recommended items **plays an important role or is the objective of your
    task.** By using it, you get answers to the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Are my generated or predicted recommendations **relevant**?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are the **most relevant** recommendations on the **first ranks**?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making the following steps easier to understand
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you know the WHY, let’s talk about the **HOW**. The following chapters
    explain step by step in an “onion” style, from the inside (starting with Precision
    P) to the outside (MAP@K), MAP@K’s structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the steps and their composition easier to understand, we work with
    the following example: We want to evaluate our recommender system, which recommends
    six items to potential customers when visiting a product detail page (figure 1).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1113537826f88df96afb13512147e77.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Recommendation example (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: Precision (P)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might have already heard of precision in books or articles when you learned
    about error metrics for classification models. Precision can be seen as a measure
    of **quality**. High precision means that our model **returns more relevant than
    irrelevant results** or recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Precision can be defined as the fraction of **relevant items** in **all recommended
    items (relevant + irrelevant items)**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7b3504f0d3ba8a5edbdd8cbab670174.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Precision formula (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: The example below (figure 3) shows 6 recommended items. Out of these 6 recommendations,
    2 are relevant.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ce034f648acc4c58d583f677a138221d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Precision example (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: By putting these values in our formula (figure 1), we get a precision of **0.33**
    ( `**2** relevant items / (**2** relevant **+** **4** irrelevant items)` ).
  prefs: []
  type: TYPE_NORMAL
- en: Precision@K (P@K)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The precision metric (figure 2) itself does not consider the rank or order in
    which the relevant items appear. Time to include the ranks to our precision formula.
    Precision**@K** can be defined as the fraction of relevant items in the top **K**
    recommended items (figure 4).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/570a1c4029c23a173f0d0eb0acf13340.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Precision@K formula (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: The following figure (figure 5) shows our example from above (figure 3) in a
    **ranked** scenario.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c93503da99784d70a3e9af0a9d6a000.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. Precision@K example (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: The **Precision@K** column shows for each rank (1 to 6) the Precision@K value.
    The **K** stands for the **number of ranks** (1, 2, …, 6) we consider.
  prefs: []
  type: TYPE_NORMAL
- en: Precision@1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Assuming we would consider **only the first rank** (**K=1**), we would then
    have **0 relevant items** divided by **1** (total items), which leads to **0**
    for Precision@1.
  prefs: []
  type: TYPE_NORMAL
- en: Precision@3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s assume we consider the **first three ranks** (K=3) we then would have
    under the top 3 recommendations 1 **relevant item** and 2 **irrelevant ones**.
    If we place these numbers in our formula (figure 3), we would get **0.33** (`**1**
    relevant item / (**1** relevant + **2** irrelevant items)` ).
  prefs: []
  type: TYPE_NORMAL
- en: Precision@5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Last but not least, let’s consider the **first five ranks** (K=5). We then would
    have under the first 5 recommendations 2 **relevant** and 3 **irrelevant** ones.
    If we do the math again, we would get a value of **0.4**
  prefs: []
  type: TYPE_NORMAL
- en: (`**2** relevant items / (**2** relevant + **3** irrelevant items)`).
  prefs: []
  type: TYPE_NORMAL
- en: Average Precision@K (AP@K)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen, Precision and Precision@K are pretty straightforward. The next
    step has a bit more complexity.
  prefs: []
  type: TYPE_NORMAL
- en: The Average Precision@K or AP@K is the sum of precision@K where the item at
    the kₜₕ rank is **relevant** (`rel(k)`) divided by the **total number of relevant
    items** (r) in the top K recommendations (figure 6).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d3f306435bb84bdd4d3d919b2ad31e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. AP@K formula (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: Confused? Let’s have a look at the following example for AP@6 (figure 7).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/58c566a74d9de0add78d514c4bb97753.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7\. AP@K example 1 (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: The total number of relevant items (r) is in this example **2** (on the ranks
    2 and 4). Therefore, we can place the 2 in the fraction `1/r`.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the first rank 1\. The Precision@1 is 0 and the item is not relevant
    (grey). Therefore, we multiply it’s Precision@1 value with 0 which leads to `0
    * 0`.
  prefs: []
  type: TYPE_NORMAL
- en: On the 2nd rank however, we have a Precision@2 value of 0.5 and a relevant item
    on the 2nd rank. This leads to `0.5 * 1` .
  prefs: []
  type: TYPE_NORMAL
- en: On the 3rd rank we have again an irrelevant item and a Precision@3 of 0.33\.
    This results in `0.33 * 0`.
  prefs: []
  type: TYPE_NORMAL
- en: We would proceed here by going over each rank. If the rank k contains a relevant
    item, we would multiply its Precision@k by 1\. In case it is irrelevant, we multiply
    it with 0, which means it has no effect in increasing our summation.
  prefs: []
  type: TYPE_NORMAL
- en: The **end result** for this example would be a **AP@6** of **0.5.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on to the last step, do you remember when we said at the beginning
    that MAP@K can answer the question:'
  prefs: []
  type: TYPE_NORMAL
- en: Are the **most relevant** recommendations on the **first ranks**?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A great characteristic of this metric is that it penalizes relevant items in
    the lower ranks. To give you a better understanding, let’s look at the following
    example (figure 8).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af3ce9d044b5b30d17b8e300181872f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8\. Example for relevant items on different ranks. Best case (left),
    worst case (right). The higher the better (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the initial example (figure 7), the number of relevant items has
    not changed. But what has changed is the rank in which they are placed. AP@K (and
    so then MAP@K) penalizes your recommendations or model if the relevant items are
    placed on lower ranks.
  prefs: []
  type: TYPE_NORMAL
- en: Mean Average Precision@K (MAP@K)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous steps and examples were all based on evaluating **one single query**
    or one single list of recommendations one visitor gets when browsing the product
    detail page of product X. But we have more than one visitor…
  prefs: []
  type: TYPE_NORMAL
- en: Mean Average Precision@K or MAP@K considers that. It averages the AP@K for recommendations
    shown to ***M* users.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/51ef07924cf2fce8e85841273309f8cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9\. MAP@K formula (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: '**Please note**: For the sake of simplicity I chose in this example “users”.
    However, depending on your case, M could be also e.g., search queries.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To get a better idea of how MAP@K is calculated, the following example can be
    seen below (figure 10).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94dc3618c0c8ef7fa03167c883baa0bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10\. Example of MAP@K (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: Based on 3 different users (or queries), the MAP@6 is **0.59**.
  prefs: []
  type: TYPE_NORMAL
- en: Coding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we are familiar with the theory, let’s do some coding. We will work
    in the following examples with two lists `actuals` and `predicted`, that contain
    product_ids.
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to check if our actual values appear in our predicted list and,
    if yes, on which rank/place they appear. That’s why the order of the items does
    not matter in our `actuals` but in our `predicted` list.
  prefs: []
  type: TYPE_NORMAL
- en: AP@K
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following lists reflect the example we used earlier (figure 11).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ef0672a28c17ae46725ad5b7ce570428.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11\. Recap example from the start (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If we compare the actuals vs. the predicted, then we can see that `p_b` shows
    up on the **2nd** rank and `p_d` on the **4th** one.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If we place our two lists in our function
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: then we get **0.5** like in our manual example (figure 7).
  prefs: []
  type: TYPE_NORMAL
- en: MAP@K
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since MAP@K averages over multiple queries, we adjust our lists to the following
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Our `actuals` stay the same but our `predicted` list contains now several lists
    (one for each query). The `predicted` lists correspond to the ones from figure
    10.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8da4585bdf5304dae0bc165eb1599a95.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12\. Recap example from figure 10 (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: 'The code below shows the calculation of MAP@K:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If we place our lists in this function
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: then we get **0.59**.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When evaluating recommender systems or ranking models, MAP@K is a great choice.
    It not only provides insights if your recommendations are relevant but also considers
    the rank of your correct predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Due to its ranking considerations, it is a bit more challenging to understand
    than other standard error metrics like the F1 score. But I hope that this article
    gave you a comprehensive explanation of how this error metric is calculated and
    implemented.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to measure not only **if your recommendations are relevant** but
    also **how relevant they are**, feel free to check out Normalized Discounted Cumulative
    Gain (NDCG).
  prefs: []
  type: TYPE_NORMAL
- en: Sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Introduction to Information Retrieval — Evaluation**, [https://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-1-per.pdf](https://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-1-per.pdf)'
  prefs: []
  type: TYPE_NORMAL
