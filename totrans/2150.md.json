["```py\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler\n\nclass DataLoader:\n\n  def __init__(self, batch_size, seq_len, pred_len):\n    self.batch_size = batch_size\n    self.seq_len = seq_len\n    self.pred_len = pred_len\n    self.target_slice = slice(0, None)\n\n    self._read_data()\n```", "```py\ndef _read_data(self):\n\n    filepath = ('data/ETTh1_original.csv')\n\n    df_raw = pd.read_csv(filepath)\n    df = df_raw.set_index('date')\n\n    # split train/valid/test\n    n = len(df)\n    train_end = int(n * 0.7)\n    val_end = n - int(n * 0.2)\n    test_end = n\n\n    train_df = df[:train_end]\n    val_df = df[train_end - self.seq_len : val_end]\n    test_df = df[val_end - self.seq_len : test_end]\n\n    # standardize by training set\n    self.scaler = StandardScaler()\n    self.scaler.fit(train_df.values)\n\n    def scale_df(df, scaler):\n      data = scaler.transform(df.values)\n      return pd.DataFrame(data, index=df.index, columns=df.columns)\n\n    self.train_df = scale_df(train_df, self.scaler)\n    self.val_df = scale_df(val_df, self.scaler)\n    self.test_df = scale_df(test_df, self.scaler)\n    self.n_feature = self.train_df.shape[-1]\n```", "```py\ndef _split_window(self, data):\n    inputs = data[:, : self.seq_len, :]\n    labels = data[:, self.seq_len :, self.target_slice]\n\n    inputs.set_shape([None, self.seq_len, None])\n    labels.set_shape([None, self.pred_len, None])\n    return inputs, labels\n\n  def _make_dataset(self, data, shuffle=True):\n    data = np.array(data, dtype=np.float32)\n    ds = tf.keras.utils.timeseries_dataset_from_array(\n        data=data,\n        targets=None,\n        sequence_length=(self.seq_len + self.pred_len),\n        sequence_stride=1,\n        shuffle=shuffle,\n        batch_size=self.batch_size,\n    )\n    ds = ds.map(self._split_window)\n    return ds\n```", "```py\n def inverse_transform(self, data):\n    return self.scaler.inverse_transform(data)\n\n  def get_train(self, shuffle=True):\n    return self._make_dataset(self.train_df, shuffle=shuffle)\n\n  def get_val(self):\n    return self._make_dataset(self.val_df, shuffle=False)\n\n  def get_test(self):\n    return self._make_dataset(self.test_df, shuffle=False)\n```", "```py\nclass DataLoader:\n\n  def __init__(self, batch_size, seq_len, pred_len):\n    self.batch_size = batch_size\n    self.seq_len = seq_len\n    self.pred_len = pred_len\n    self.target_slice = slice(0, None)\n\n    self._read_data()\n\n  def _read_data(self):\n\n    filepath = ('data/ETTh1_original.csv')\n\n    df_raw = pd.read_csv(filepath)\n    df = df_raw.set_index('date')\n\n    # split train/valid/test\n    n = len(df)\n    train_end = int(n * 0.7)\n    val_end = n - int(n * 0.2)\n    test_end = n\n\n    train_df = df[:train_end]\n    val_df = df[train_end - self.seq_len : val_end]\n    test_df = df[val_end - self.seq_len : test_end]\n\n    # standardize by training set\n    self.scaler = StandardScaler()\n    self.scaler.fit(train_df.values)\n\n    def scale_df(df, scaler):\n      data = scaler.transform(df.values)\n      return pd.DataFrame(data, index=df.index, columns=df.columns)\n\n    self.train_df = scale_df(train_df, self.scaler)\n    self.val_df = scale_df(val_df, self.scaler)\n    self.test_df = scale_df(test_df, self.scaler)\n    self.n_feature = self.train_df.shape[-1]\n\n  def _split_window(self, data):\n    inputs = data[:, : self.seq_len, :]\n    labels = data[:, self.seq_len :, self.target_slice]\n\n    inputs.set_shape([None, self.seq_len, None])\n    labels.set_shape([None, self.pred_len, None])\n    return inputs, labels\n\n  def _make_dataset(self, data, shuffle=True):\n    data = np.array(data, dtype=np.float32)\n    ds = tf.keras.utils.timeseries_dataset_from_array(\n        data=data,\n        targets=None,\n        sequence_length=(self.seq_len + self.pred_len),\n        sequence_stride=1,\n        shuffle=shuffle,\n        batch_size=self.batch_size,\n    )\n    ds = ds.map(self._split_window)\n    return ds\n\n  def inverse_transform(self, data):\n    return self.scaler.inverse_transform(data)\n\n  def get_train(self, shuffle=True):\n    return self._make_dataset(self.train_df, shuffle=shuffle)\n\n  def get_val(self):\n    return self._make_dataset(self.val_df, shuffle=False)\n\n  def get_test(self):\n    return self._make_dataset(self.test_df, shuffle=False)\n```", "```py\ndata_loader = DataLoader(batch_size=32, seq_len=512, pred_len=96)\n\ntrain_data = data_loader.get_train()\nval_data = data_loader.get_val()\ntest_data = data_loader.get_test()\n```", "```py\nfrom tensorflow.keras import layers\n\ndef res_block(inputs, norm_type, activation, dropout, ff_dim):\n\n  norm = layers.BatchNormalization\n\n  # Time mixing\n  x = norm(axis=[-2, -1])(inputs)\n  x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Channel, Input Length]\n  x = layers.Dense(x.shape[-1], activation='relu')(x)\n  x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Input Length, Channel]\n  x = layers.Dropout(dropout)(x)\n  res = x + inputs\n```", "```py\n # Feature mixing\n  x = norm(axis=[-2, -1])(res)\n  x = layers.Dense(ff_dim, activation='relu')(x)  # [Batch, Input Length, FF_Dim]\n  x = layers.Dropout(0.7)(x)\n  x = layers.Dense(inputs.shape[-1])(x)  # [Batch, Input Length, Channel]\n  x = layers.Dropout(0.7)(x)\n  return x + res\n```", "```py\nfrom tensorflow.keras import layers\n\ndef res_block(inputs, ff_dim):\n\n  norm = layers.BatchNormalization\n\n  # Time mixing\n  x = norm(axis=[-2, -1])(inputs)\n  x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Channel, Input Length]\n  x = layers.Dense(x.shape[-1], activation='relu')(x)\n  x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Input Length, Channel]\n  x = layers.Dropout(0.7)(x)\n  res = x + inputs\n\n  # Feature mixing\n  x = norm(axis=[-2, -1])(res)\n  x = layers.Dense(ff_dim, activation='relu')(x)  # [Batch, Input Length, FF_Dim]\n  x = layers.Dropout(0.7)(x)\n  x = layers.Dense(inputs.shape[-1])(x)  # [Batch, Input Length, Channel]\n  x = layers.Dropout(0.7)(x)\n  return x + res\n```", "```py\ndef build_model(\n    input_shape,\n    pred_len,\n    n_block,\n    ff_dim,\n    target_slice,\n):\n\n  inputs = tf.keras.Input(shape=input_shape)\n  x = inputs  # [Batch, Input Length, Channel]\n  for _ in range(n_block):\n    x = res_block(x, norm_type, activation, dropout, ff_dim)\n\n  if target_slice:\n    x = x[:, :, target_slice]\n\n  # Temporal projection\n  x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Channel, Input Length]\n  x = layers.Dense(pred_len)(x)  # [Batch, Channel, Output Length]\n  outputs = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Output Length, Channel])\n\n  return tf.keras.Model(inputs, outputs)\n```", "```py\nmodel = build_model(\n    input_shape=(512, data_loader.n_feature),\n    pred_len=96,\n    n_block=8,\n    ff_dim=64,\n    target_slice=data_loader.target_slice\n)\n```", "```py\ntf.keras.utils.set_random_seed(42)\n\noptimizer = tf.keras.optimizers.Adam(1e-4)\n\nmodel.compile(optimizer, loss='mse', metrics=['mae'])\n\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath='tsmixer_checkpoints/',\n    vebose=1,\n    save_best_only=True,\n    save_weights_only=True\n)\n\nearly_stop_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=3\n)\n\nhistory = model.fit(\n    train_data,\n    epochs= 30,\n    validation_data=val_data,\n    callbacks=[checkpoint_callback, early_stop_callback]\n)\n```", "```py\nbest_epoch = np.argmin(history.history['val_loss'])\n\nmodel.load_weights(\"tsmixer_checkpoints/\")\n```", "```py\npredictions = model.predict(test_data)\n\nscaled_preds = predictions[-1,:,:]\n```", "```py\ncols = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']\n\nscaled_preds_df = pd.DataFrame(scaled_preds)\nscaled_preds_df.columns = cols\n\npreds = data_loader.inverse_transform(scaled_preds)\n\npreds_df = pd.DataFrame(preds)\npreds_df.columns = cols\n```", "```py\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.models import NHITS\n\ndf = pd.read_csv('data/ETTh1_original.csv')\n\ncolumns_to_melt = ['date', 'HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']\n\nmelted_df = df.melt(id_vars=['date'], value_vars=columns_to_melt, var_name='unique_id', value_name='y')\n\nmelted_df.rename(columns={'date': 'ds'}, inplace=True)\n\nmelted_df['ds'] = pd.to_datetime(melted_df['ds'])\n```", "```py\nhorizon = 96\n\nmodels = [\n    NHITS(h=horizon, input_size=512, max_steps=30)\n]\n\nnf = NeuralForecast(models=models, freq='H')\n\nn_preds_df = nf.cross_validation(\n  df=melted_df, \n  val_size=int(0.2*len(df)), \n  test_size=int(0.1*len(df)), \n  n_windows=None)\n```", "```py\ndf['date'][-96:] = pd.to_datetime(df['date'][-96:])\n\nmax_date = df['date'][-96:].max()\nmin_date = df['date'][-96:].min()\n\nlast_n_preds_df = n_preds_df[(n_preds_df['ds'] >= min_date) & (n_preds_df['ds'] <= max_date)]\n\ncols = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']\n\nclean_last_n_preds_df = pd.DataFrame()\n\nfor col in cols:\n    temp_df = last_n_preds_df[last_n_preds_df['unique_id'] == col].drop_duplicates(subset='ds', keep='first')\n    clean_last_n_preds_df = pd.concat([clean_last_n_preds_df, temp_df], ignore_index=True)\n```", "```py\nnhits_preds = pd.read_csv('data/nhits_preds_etth1_h96.csv')\ntsmixer_preds = pd.read_csv('data/tsmixer_preds_etth1_h96.csv')\n\ncols_to_plot = ['HUFL', 'HULL', 'MUFL', 'MULL']\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12,8))\n\nfor i, ax in enumerate(axes.flatten()):\n    col = cols_to_plot[i]\n\n    nhits_df = nhits_preds[nhits_preds['unique_id'] == col] \n\n    ax.plot(df['date'][-96:], df[col][-96:])\n    ax.plot(df['date'][-96:], tsmixer_preds[col], label='TSMixer', ls='--', color='green')\n    ax.plot(df['date'][-96:], nhits_df['NHITS'], label='N-HiTS', ls=':', color='black')\n\n    ax.legend(loc='best')\n    ax.set_xlabel('Time steps')\n    ax.set_ylabel('Value')\n    ax.set_title(col)\n\nplt.tight_layout()\nfig.autofmt_xdate()\n```", "```py\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\ny_actual = df.drop('date', axis=1)[-96:]\n\ndata = {'N-HiTS': \n            [mean_absolute_error(nhits_preds['y'], nhits_preds['NHITS']), \n             mean_squared_error(nhits_preds['y'], nhits_preds['NHITS'])],\n       'TSMixer': \n            [mean_absolute_error(y_actual, tsmixer_preds), \n             mean_squared_error(y_actual, tsmixer_preds)]}\n\nmetrics_df = pd.DataFrame(data=data)\nmetrics_df.index = ['mae', 'mse']\n\nmetrics_df.style.highlight_min(color='lightgreen', axis=1)\n```"]