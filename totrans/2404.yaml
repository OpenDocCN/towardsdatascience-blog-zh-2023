- en: 'XGBoost: How Deep Learning Can Replace Gradient Boosting and Decision Trees
    — Part 1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-291dc9365656](https://towardsdatascience.com/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-291dc9365656)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@guillaume.saupin?source=post_page-----291dc9365656--------------------------------)[![Saupin
    Guillaume](../Images/d9112d3cdfe6f335b6ff2c875fba6bb5.png)](https://medium.com/@guillaume.saupin?source=post_page-----291dc9365656--------------------------------)[](https://towardsdatascience.com/?source=post_page-----291dc9365656--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----291dc9365656--------------------------------)
    [Saupin Guillaume](https://medium.com/@guillaume.saupin?source=post_page-----291dc9365656--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----291dc9365656--------------------------------)
    ·7 min read·May 29, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d836a60a327155b897054d6830cd01dc.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Preethi Viswanathan](https://unsplash.com/@sallybrad2016?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In this article, you will learn about rewriting decision trees using a Differentiable
    Programming approach, as proposed by the NODE paper, enabling the reformulation
    of this model in a manner similar to Neural Networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deriving this formulation is an excellent exercise, as it raises many issues
    common when building and training Custom Neural Networks:'
  prefs: []
  type: TYPE_NORMAL
- en: How to avoid the vanishing gradient problem?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a good choice for initial weights?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why use batch normalization?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But before answering these questions, let’s see how to rephrase Decision Tree
    in the mathematical framework of Neural Networks: Differentiable Programming.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Update — the second article is online:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-part-2-training-b432620750f8?source=post_page-----291dc9365656--------------------------------)
    [## XGBoost: How Deep Learning Can Replace Gradient Boosting and Decision Trees
    — Part 2: Training'
  prefs: []
  type: TYPE_NORMAL
- en: A world without if
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-part-2-training-b432620750f8?source=post_page-----291dc9365656--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost is highly efficient, but…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have read my previous articles on Gradient Boosting and Decision Trees,
    you are aware that Gradient Boosting, combined with Ensembles of Decision Trees,
    has achieved excellent performance in classification or regression tasks involving
    tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://amzn.to/3IKyqTF?source=post_page-----291dc9365656--------------------------------)
    [## Practical Gradient Boosting: An deep dive into Gradient Boosting in Python'
  prefs: []
  type: TYPE_NORMAL
- en: This book on Gradient Boosting methods is intended for students, academics,
    engineers, and data scientists who wish to…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: amzn.to](https://amzn.to/3IKyqTF?source=post_page-----291dc9365656--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: However, despite the efforts made by XGBoost, LightGBM, or CatBoost to enhance
    the construction of Ensemble of Decision Trees, training such a model still relies
    on a brute force approach. It involves systematically evaluating the gain associated
    with splitting values and features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to my previous article on the subject:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/diy-xgboost-library-in-less-than-200-lines-of-python-69b6bf25e7d9?source=post_page-----291dc9365656--------------------------------)
    [## DIY XGBoost library in less than 200 lines of python'
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost explained as well as gradient boosting method and HP tuning by building
    your own gradient boosting library for…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/diy-xgboost-library-in-less-than-200-lines-of-python-69b6bf25e7d9?source=post_page-----291dc9365656--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'The modern approach to training a Machine Learning model, and programs in general,
    involves using Differentiable Programming. If you are unfamiliar with the concept,
    I have written an introductory article on the subject:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/differentiable-programming-from-scratch-abba0ebebc1c?source=post_page-----291dc9365656--------------------------------)
    [## Differentiable Programming from Scratch'
  prefs: []
  type: TYPE_NORMAL
- en: Last year I had the great opportunity to attend a talk with Yann Lecun, at the
    Facebook Artificial Intelligence…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/differentiable-programming-from-scratch-abba0ebebc1c?source=post_page-----291dc9365656--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, the idea is to identify the parameters to optimize in your model/program
    and use a gradient-based approach to find the optimal parameters. However, for
    this to work, your model must be differentiable with respect to the parameters
    of interest.
  prefs: []
  type: TYPE_NORMAL
- en: Although Ensemble of Decision Trees employ a Gradient-Based approach for training,
    their construction process is not differentiable.
  prefs: []
  type: TYPE_NORMAL
- en: Building Decision Trees is not a differentiable process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A decision tree is a simple yet powerful data structure with numerous applications.
    When training such a structure for regression or classification tasks, we aim
    to identify three main parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The feature to use for each decision node to minimize the error, which involves
    feature selection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The threshold used to split the input data at each node. Data with a selected
    feature value greater than the threshold will be assigned to the right child node,
    while the rest will be assigned to the left child node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The value assigned to the leaves of the tree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first two steps are non-differentiable. In current implementations of Gradient
    Boosted Trees, the first step requires iterating over features and is non-smooth.
    The second step involves evaluating the gain using the resulting data partition
    and the lists of possible values. This process is also non-differentiable.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, in 2019, Popov, Morozov, and Babenko introduced a differentiable
    formulation of Decision Trees.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://arxiv.org/abs/1909.06312?source=post_page-----291dc9365656--------------------------------)
    [## Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data'
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, deep neural networks (DNNs) have become the main instrument for machine
    learning tasks within a wide range of…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: arxiv.org](https://arxiv.org/abs/1909.06312?source=post_page-----291dc9365656--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: I highly recommend reading their paper, as it is always fruitful to gather information
    from the source. However, to facilitate comprehension, I have written some Python
    code using Jax.
  prefs: []
  type: TYPE_NORMAL
- en: 'The questions addressed in the paper are:'
  prefs: []
  type: TYPE_NORMAL
- en: How can we reformulate feature selection to make it smooth and differentiable?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we reformulate comparison to make it smooth and differentiable?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feature selection regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned earlier, feature selection in standard Gradient Boosting algorithms
    is performed using an exhaustive search: we try each feature and keep the one
    that provides the highest gain.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of this iterative process, we need to reformulate the selection as a
    function that returns the value of the feature ensuring maximal gain.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s assume that the features are gathered in a vector. We need a way
    to extract the value of the selected feature from this array, ideally using vector
    arithmetic. Starting with the assumption that we already know the feature to retain,
    we can achieve this with a simple dot product:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Hence, we can retrieve a specific value from an array of features by multiplying
    all of them by 0, except the one we need to keep, and summing all the products.
  prefs: []
  type: TYPE_NORMAL
- en: Learning the feature that achieves the highest performance involves learning
    the vector `selection_weights` with the additional constraint that the sum of
    its elements must be equal to 1\. Ideally, this 1 should be concentrated in a
    single element of the vector.
  prefs: []
  type: TYPE_NORMAL
- en: Enforcing this constraint on the norm of the selection vector can be achieved
    using a function. An obvious candidate is the softmax function, which enforces
    the norm of the weight vector to be 1\. However, softmax generates a smooth distribution
    and cannot generate a 1 for the highest weight and 0 for the others.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet illustrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Not only is the norm not exactly equal to 1.0, but the distribution is spread
    across all dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the NODE paper proposes using the entmax function instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This appears to be a good choice, as it generates the expected result!
  prefs: []
  type: TYPE_NORMAL
- en: By utilizing the dot product in conjunction with the `entmax` function, we can
    retrieve the value of a given feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Differentiable feature selection can be as simple as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: During the training phase, the weights will be trained to learn which feature
    to keep for a given decision.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing a threshold
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The other parameter used in a decision node, which must be learned, is a threshold.
    If the value of the selected feature is greater than or equal to this threshold,
    the prediction follows the right path of the tree. Conversely, if the value is
    strictly lower, the left path is taken.
  prefs: []
  type: TYPE_NORMAL
- en: A similar approach can be used to generate a 1 if the value is above the threshold
    or below it, by using a dot product and the *entmax* function, but with a vector
    of size 2\. The first element represents the difference between the selected value
    and the threshold, while the second element is 0.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to the *entmax* function, if the value is below the threshold, the first
    element of the vector is negative. When applying the *entmax* function, the 0
    is transformed into a 1, and the negative part becomes equal to 0\. Conversely,
    when the value is greater than the threshold, the first element is 1 and the second
    element is 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: During the training process, the parameters to be learned are not only the threshold
    but also the weights attached to the right and left leaves of this simple 1-level
    tree.
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/646ded91846ff6aa7fa8814985c9837d.png)](https://www.buymeacoffee.com/guillaumes0)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.buymeacoffee.com/guillaumes0](https://www.buymeacoffee.com/guillaumes0)'
  prefs: []
  type: TYPE_NORMAL
- en: Next steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this first article, we have seen how to regularize the highly non-smooth
    structure of Gradient Boosting, namely the decision tree. This is the minimum
    requirement to transition from the Gradient Boosting method to the Differentiable
    Programming paradigm that underlies Neural Networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there are still three problems that need to be addressed:'
  prefs: []
  type: TYPE_NORMAL
- en: How to extend the method to support multi-level decision trees?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to learn the parameters of such trees?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we ensure that we stay in the linear part of the `entmax`function, and
    ensure that gradient does not vanish?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I will provide further details on these topics in a forthcoming article.
  prefs: []
  type: TYPE_NORMAL
- en: 'Update — the second article is online:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-part-2-training-b432620750f8?source=post_page-----291dc9365656--------------------------------)
    [## XGBoost: How Deep Learning Can Replace Gradient Boosting and Decision Trees
    — Part 2: Training'
  prefs: []
  type: TYPE_NORMAL
- en: A world without if
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-part-2-training-b432620750f8?source=post_page-----291dc9365656--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'In the meantime, I you want to improve your mastery of Gradient Boosting, check
    out my book on the subject:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://amzn.to/3IKyqTF?source=post_page-----291dc9365656--------------------------------)
    [## Practical Gradient Boosting: An deep dive into Gradient Boosting in Python'
  prefs: []
  type: TYPE_NORMAL
- en: This book on Gradient Boosting methods is intended for students, academics,
    engineers, and data scientists who wish to…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: amzn.to](https://amzn.to/3IKyqTF?source=post_page-----291dc9365656--------------------------------)
  prefs: []
  type: TYPE_NORMAL
