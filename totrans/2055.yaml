- en: The Power of the Dot Product in Artificial Intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-power-of-the-dot-product-in-artificial-intelligence-c002331e1829](https://towardsdatascience.com/the-power-of-the-dot-product-in-artificial-intelligence-c002331e1829)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How a simple tool gives rise to astonishing complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://manuel-brenner.medium.com/?source=post_page-----c002331e1829--------------------------------)[![Manuel
    Brenner](../Images/f62843c79a9b378494cb83caf3ddc792.png)](https://manuel-brenner.medium.com/?source=post_page-----c002331e1829--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c002331e1829--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c002331e1829--------------------------------)
    [Manuel Brenner](https://manuel-brenner.medium.com/?source=post_page-----c002331e1829--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c002331e1829--------------------------------)
    ·9 min read·May 15, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Taking something simple and scaling it up to achieve something more complex
    is a powerful idea, laying the foundation of [life](https://manuel-brenner.medium.com/the-importance-of-noise-327fcab7c4fb),
    computers (based on the simplicity of the [Turing machine](https://medium.com/discourse/a-non-technical-guide-to-turing-machines-f8c6da9596e5)),
    and deep learning techniques (based on the idea of the neurons in a neural network).
  prefs: []
  type: TYPE_NORMAL
- en: '[More is frequently different](https://manuel-brenner.medium.com/more-is-different-a49e833260b3),
    which we continue observing as state-of-the-art architectures are increasingly
    scaled (with large language models like [GPT](https://arxiv.org/abs/2005.14165)
    changing the world as we speak), leading to better generalization and impressive
    results.'
  prefs: []
  type: TYPE_NORMAL
- en: Researchers claim that large language models showcase [emergent abilities](https://openreview.net/pdf?id=yzkSU5zdwD)
    that are achieved without too much architectural innovation, but (arguably) mostly
    from an increase of computational resources and the repetition of simple operations
    at scale (for a caveat, see [this article](https://arxiv.org/abs/2304.15004)).
  prefs: []
  type: TYPE_NORMAL
- en: Although training deep learning models requires ingenuity and techniques that
    the community has built up over years, the fundamental building blocks of most
    deep learning approaches remain quite simple, being composed of only a small number
    of mathematical operations.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the most important one is the dot product. In the rest of this article,
    I want to explore what the dot product does, why it is so important, and why scaling
    it up has achieved levels of AI that astonish everyone that comes into closer
    contact with it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00f9a2a7d5ef38fedd5289f12d30fc1b.png)'
  prefs: []
  type: TYPE_IMG
- en: Artificial Intelligence as the product of dots (DALL-E might have misunderstood
    me a little but it still looks cool).
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you are working at a company like Netflix, and are tasked with picking
    out movies to recommend to millions of different users. Every user has his/her
    own unique taste, and there are millions of movies to select from, so it’s quite
    the daunting task.
  prefs: []
  type: TYPE_NORMAL
- en: One way to tackle this problem is to list a couple of different properties of
    the movies, such as how much the movie aligns with certain genres (e.g. to rate
    how much of an action film or an emotionally taxing drama it is). One could also
    list other kinds of information, such as its comedy content, some external factors
    like its length, whether it was produced by Marvel or Disney, and so on and so
    on. The list of all of the movies’ properties can then be put into a vector.
  prefs: []
  type: TYPE_NORMAL
- en: A simple way of getting an idea of a user’s taste is to just take a look at
    that person’s favorite movie(s), or a list of the last couple of movies the user
    watched, which leads to a second vector that gives us an idea of the user’s taste.
  prefs: []
  type: TYPE_NORMAL
- en: New movies can then simply be recommended by computing how similar they are
    to the user’s favorite movies, according to some kind of similarity measure in
    the vector space of the movie’s properties.
  prefs: []
  type: TYPE_NORMAL
- en: This is where the dot product, AI’s favorite similarity measure, enters the
    stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dot product is a simple mathematical operation that measures the similarity
    between two vectors. Mathematically, the dot product of two vectors x and y is
    calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: x · y = Σ (x_i * y_i) for i = 1 to n
  prefs: []
  type: TYPE_NORMAL
- en: where x_i and y_i are the components of vectors x and y, respectively, and n
    is the dimension of the vectors. It can also be written as
  prefs: []
  type: TYPE_NORMAL
- en: x · y = |x| * |y| * cos(θ)
  prefs: []
  type: TYPE_NORMAL
- en: where θ is the angle between the two vectors and || denotes their length. In
    this interpretation, the dot product measures the extent to which x and y are
    aligned. If the two vectors are parallel, the dot product corresponds to their
    length. If the vectors are perpendicular to each other, it goes to zero.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a9b8065e1560712994dc2bcb34a33232.png)'
  prefs: []
  type: TYPE_IMG
- en: BenFrantzDale at the English Wikipedia, CC BY-SA 3.0
  prefs: []
  type: TYPE_NORMAL
- en: 'The dot product is often combined with the cosine similarity, which normalizes
    the dot product by the magnitudes of the vectors, providing a similarity measure
    that is invariant to their magnitudes:'
  prefs: []
  type: TYPE_NORMAL
- en: cosine_similarity(y, x) = (y · x) / (|y| * |x|)
  prefs: []
  type: TYPE_NORMAL
- en: Geometrically, the dot product can be thought of as the projection of one vector
    onto another. When you compute the dot product of two vectors x and y, you can
    think of it as projecting vector x onto vector y (or vice versa) and then multiplying
    the length of the projection by the length of vector y.
  prefs: []
  type: TYPE_NORMAL
- en: 'Projections are an important concept, and have a nice interpretation: they
    can be thought of as dimension-wise comparisons of different features that the
    vectors encode.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returning to our earlier example, consider two movies, Movie A and Movie B.
    For the sake of simplicitiy, let’s say these a characterized by a two-dimensional
    vector based on two distinct properties: Action Intensity (AI) and Emotional Depth
    (ED) (usually, these vectors can of course be much larger). The first component
    of the vector represents Action Intensity, and the second component represents
    Emotional Depth. Let’s say both properties are measured on a scale of -5 to 5,
    with -5 being the least intense or emotionally deep and 5 being the most intense
    or emotionally deep.'
  prefs: []
  type: TYPE_NORMAL
- en: One movie, a relatively shallow popcorn entertainment action film, has an Action
    Intensity of 4 and an Emotional Depth of -3\. Its vector representation would
    be A = [4, -3].
  prefs: []
  type: TYPE_NORMAL
- en: Movie B is a 4 hour black and white meditation on the Serbian government, [seen
    through the eyes of a pigeon](https://knowyourmeme.com/memes/film-bros-when-you-tell-them-you-want-to-watch-a-marvel-movie).
    It is quite emotionally demanding but does not have a lot of action scenes, scoring
    a rating of B = [-4, 4].
  prefs: []
  type: TYPE_NORMAL
- en: We can compare both movies using the dot product, evaluating to
  prefs: []
  type: TYPE_NORMAL
- en: A · B = (4 * -4) + (-3 * 4)= -28
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing by the size of both vectors, this gives
  prefs: []
  type: TYPE_NORMAL
- en: -28/(sqrt(25)*sqrt(32))= -0.99.
  prefs: []
  type: TYPE_NORMAL
- en: As would be expected, the two movies are extremely dissimilar, so their cosine
    (dis)similarity score is close to -1.
  prefs: []
  type: TYPE_NORMAL
- en: So why is this simple operation of comparing similarity of two vectors so important
    in AI?
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning, and maybe at its heart (artificial) intelligence more generally,
    relies on comparing patterns and measuring their similarity on an enormous scale.
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s not quite trivial to come up with patterns to compare.
  prefs: []
  type: TYPE_NORMAL
- en: The part I glossed over in the earlier example is how to come up with the labels
    for the movies. The ratings Action Intensity and Emotional Depth are not straightforward
    to extract from the ground-truth data of the movie, but are human assessments,
    who perform the complex cognitive (subjective) task of watching the movie and
    transforming it into a significantly lower-dimensional representation in which
    it becomes apparent that the movies are quite different. From the perspective
    of the pixel space of the movies (for a 2 hour movie in 4K, these are something
    like 1.4*10¹² pixels), this is a tremendous dimensionality reduction that includes
    individual biases and priors.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of complex nonlinear transformations is a second important ingredient
    necessary for making dot products powerful. It transforms inputs into representations
    in which comparing their similarity becomes meaningful in the context of the respective
    learning problem, such as a classification task or regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: Combining nonlinear transformation with dot products is at the heart of what
    neural networks do. Here the dot product is used in calculating the weighted sum
    of inputs in each neuron which, combined with a nonlinear transformation (the
    activation function, such as the tanh or ReLU function, plus the learned weights
    of the network), constitute the basic mechanism by which all neural networks operate.
  prefs: []
  type: TYPE_NORMAL
- en: This combination becomes even clearer in the context of kernel functions.
  prefs: []
  type: TYPE_NORMAL
- en: A kernel function allows us to compare the similarity of points in a new, transformed
    space. It does so by computing a dot product between the transformed data points
    in a (usually higher-dimensional) projection space. The neat thing is that the
    “kernel trick” allows this computation to be performed without explicitly calculating
    the transformation, which can save a lot of resources, and finds application in
    Support Vector Machines (SVMs), which, combined with the kernel trick, constitutes
    one of the most powerful and influential algorithms in recent decades.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a21754c52d3435b783d0cfd3741c690.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Kernel machines combine a nonlinear transformation to find a feature space
    in which the class labels are easily separable. Original: Alisneaky Vector: Zirguezi,
    CC BY-SA 4.0'
  prefs: []
  type: TYPE_NORMAL
- en: 'These basic insights can be connected to the hottest models on the market right
    now: large language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A standard task for a large language model is the translation of a sentence
    between two languages, say between English and German:'
  prefs: []
  type: TYPE_NORMAL
- en: “I am reading an article about the importance of the dot product in AI.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “Ich lese einen Artikel über die Bedeutung des Skalarproduktes für die KI.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Both sentences carry approximately the same meaning but are significantly different
    in their representation.
  prefs: []
  type: TYPE_NORMAL
- en: The translation task can be framed as finding a nonlinear transformation of
    the words that correspond to approximately the same location in the latent semantic
    space that captures their “meaning”. The quality of the translation can then be
    measured by the achieved similarity.
  prefs: []
  type: TYPE_NORMAL
- en: If “measuring similarity” doesn’t make your spidey senses tingle by now, I haven’t
    done a good job with this article.
  prefs: []
  type: TYPE_NORMAL
- en: And indeed, the dot product shows up at the heart of transformer models, which
    have become the foundation of modern natural language processing (NLP) and many
    other machine-learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The self-attention mechanism is a key component of transformers. Self-attention
    allows the model to weigh the importance of different input elements with respect
    to each other. This allows them to capture long-range dependencies and complex
    relationships in the data. In the self-attention mechanism, the dot product is
    used in calculating the attention scores and forming context-aware representations
    of the input elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input elements (usually embeddings/tokenized versions of the input text)
    are first linearly projected into three different spaces: Query (Q), Key (K),
    and Value (V) using separate learned weight matrices. This results in three sets
    of vectors for each input element: query vectors, key vectors, and value vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: The dot product is then used to compute attention scores between each pair of
    query and key vectors (score_ij = q_i · k_j).
  prefs: []
  type: TYPE_NORMAL
- en: This measures the similarity between the query and key vectors, which determines
    how much attention the model pays to each input element with respect to all the
    other elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'After computing all similarity scores, the scores are scaled and sent through
    a ***similarity function***, and can then be used to compute a context function,
    which is again a simple sum over attention scores and values: (context_i = Σ (attention_ij
    * v_j))'
  prefs: []
  type: TYPE_NORMAL
- en: '[The usual choice for the similarity function is the softmax, which can be
    thought of as a kernel function](https://arxiv.org/pdf/2112.04035.pdf), a nonlinear
    transformation that enables the comparison between elements, and that estimates
    which elements might be useful for prediction. Other kernel functions are possible,
    depending on the problem at hand. In a more fundamental sense, transformers can
    be viewed as kernel machines (more precisely as [Deep Infinite-Dimensional Non-Mercer
    Binary Kernel Machines](https://arxiv.org/abs/2106.01506), as this paper discusses).'
  prefs: []
  type: TYPE_NORMAL
- en: As with the other examples, the dot product, combined with nonlinear transformations
    of the input and output text and projections, defines the self-attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: I hope I could convince you that the dot product plays a pivotal role in AI,
    especially in its most recent instantiations. While in some specific contexts,
    other distance/similarity measures between vectors are more appropriate (Euclidian,
    Chebyshev, Mahalanobis distance, etc.), the dot product is probably the most widely
    used across a wide range of applications, and constitutes the fundamental building
    block for quantifying similarity, relationships, and dependencies within all kinds
    of data. When combined with nonlinear transformations, the dot product sits at
    the heart of a wide array of algorithms and models, from Support Vector Machines
    to neural networks to the self-attention mechanism in Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: With the impressive strides AI has been taking in recent years, I find it fascinating
    to reflect on the underlying simplicity and adaptability of the operations that
    make it up.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can at times be a little scary to see how effective this strategy has been:
    [in a recent podcast](https://www.youtube.com/watch?v=VcVfceTsD0A), Max Tegmark
    commented on the surprising simplicity/stupidity behind training large language
    models (just give them loads of text and make them predict the next words/sentences
    using transformers with self-attention). In some ways, [intelligence might be
    simpler than we think](/why-intelligence-might-be-simpler-than-we-think-1d3d7feb5d34),
    as most who have played with ChatGPT confirm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This has important implications: computers are very good at doing something
    very simple reliably and fast and scaling it up to the Nth degree. With the promise
    of Moore’s law, we will continue scaling these models.'
  prefs: []
  type: TYPE_NORMAL
- en: These are fascinating and possibly dangerous times ahead, and it’s likely that
    the dot product will sit right at their center.
  prefs: []
  type: TYPE_NORMAL
