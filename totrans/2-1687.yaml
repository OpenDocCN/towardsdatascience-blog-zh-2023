- en: Probabilistic Logistic Regression with TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ TensorFlow è¿›è¡Œæ¦‚ç‡é€»è¾‘å›å½’
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/probabilistic-logistic-regression-with-tensorflow-73e18f0ddc48](https://towardsdatascience.com/probabilistic-logistic-regression-with-tensorflow-73e18f0ddc48)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/probabilistic-logistic-regression-with-tensorflow-73e18f0ddc48](https://towardsdatascience.com/probabilistic-logistic-regression-with-tensorflow-73e18f0ddc48)
- en: Probabilistic Deep Learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚ç‡æ·±åº¦å­¦ä¹ 
- en: '[](https://medium.com/@luisroque?source=post_page-----73e18f0ddc48--------------------------------)[![LuÃ­s
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----73e18f0ddc48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----73e18f0ddc48--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----73e18f0ddc48--------------------------------)
    [LuÃ­s Roque](https://medium.com/@luisroque?source=post_page-----73e18f0ddc48--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@luisroque?source=post_page-----73e18f0ddc48--------------------------------)[![LuÃ­s
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----73e18f0ddc48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----73e18f0ddc48--------------------------------)[![æ•°æ®ç§‘å­¦å‰æ²¿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----73e18f0ddc48--------------------------------)
    [LuÃ­s Roque](https://medium.com/@luisroque?source=post_page-----73e18f0ddc48--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----73e18f0ddc48--------------------------------)
    Â·9 min readÂ·Jan 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [æ•°æ®ç§‘å­¦å‰æ²¿](https://towardsdatascience.com/?source=post_page-----73e18f0ddc48--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 9 åˆ†é’ŸÂ·2023å¹´1æœˆ25æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: This article belongs to the series â€œProbabilistic Deep Learningâ€. This weekly
    series covers probabilistic approaches to deep learning. The main goal is to extend
    deep learning models to quantify uncertainty, i.e., know what they do not know.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å±äºâ€œæ¦‚ç‡æ·±åº¦å­¦ä¹ â€ç³»åˆ—ã€‚è¯¥ç³»åˆ—æ¯å‘¨ä»‹ç»æ·±åº¦å­¦ä¹ ä¸­çš„æ¦‚ç‡æ–¹æ³•ã€‚ä¸»è¦ç›®æ ‡æ˜¯æ‰©å±•æ·±åº¦å­¦ä¹ æ¨¡å‹ä»¥é‡åŒ–ä¸ç¡®å®šæ€§ï¼Œå³äº†è§£å®ƒä»¬ä¸çŸ¥é“çš„å†…å®¹ã€‚
- en: In this article, we will introduce the concept of probabilistic logistic regression,
    a powerful technique that allows for the inclusion of uncertainty in the prediction
    process. We will explore how this approach can lead to more robust and accurate
    predictions, especially in cases where the data is noisy, or the model is overfitting.
    Additionally, by incorporating a prior distribution on the model parameters, we
    can regularize the model and prevent overfitting. This approach serves as a great
    first step into the exciting world of Bayesian Deep Learning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»æ¦‚ç‡é€»è¾‘å›å½’çš„æ¦‚å¿µï¼Œè¿™æ˜¯ä¸€ç§å¼ºå¤§çš„æŠ€æœ¯ï¼Œå…è®¸åœ¨é¢„æµ‹è¿‡ç¨‹ä¸­çº³å…¥ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬å°†æ¢è®¨è¿™ç§æ–¹æ³•å¦‚ä½•åœ¨æ•°æ®å™ªå£°å¤§æˆ–æ¨¡å‹è¿‡æ‹Ÿåˆçš„æƒ…å†µä¸‹æä¾›æ›´ç¨³å¥å’Œå‡†ç¡®çš„é¢„æµ‹ã€‚æ­¤å¤–ï¼Œé€šè¿‡åœ¨æ¨¡å‹å‚æ•°ä¸Šå¼•å…¥å…ˆéªŒåˆ†å¸ƒï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æ¨¡å‹è¿›è¡Œæ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚è¿™ç§æ–¹æ³•æ˜¯è¿›å…¥è´å¶æ–¯æ·±åº¦å­¦ä¹ æ¿€åŠ¨äººå¿ƒçš„ä¸–ç•Œçš„ç»ä½³èµ·ç‚¹ã€‚
- en: 'Articles published so far:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å·²å‘å¸ƒçš„æ–‡ç« ï¼š
- en: '[Gentle Introduction to TensorFlow Probability: Distribution Objects](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow Probability è½»æ¾å…¥é—¨ï¼šåˆ†å¸ƒå¯¹è±¡](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1)'
- en: '[Gentle Introduction to TensorFlow Probability: Trainable Parameters](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow Probability è½»æ¾å…¥é—¨ï¼šå¯è®­ç»ƒå‚æ•°](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15)'
- en: '[Maximum Likelihood Estimation from scratch in TensorFlow Probability](/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ä»é›¶å¼€å§‹çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡åœ¨ TensorFlow Probability ä¸­](/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2)'
- en: '[Probabilistic Linear Regression from scratch in TensorFlow](/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ä»é›¶å¼€å§‹çš„ TensorFlow æ¦‚ç‡çº¿æ€§å›å½’](/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00)'
- en: '[Probabilistic vs. Deterministic Regression with Tensorflow](https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Tensorflow ä¸­çš„æ¦‚ç‡å›å½’ä¸ç¡®å®šæ€§å›å½’](https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef)'
- en: '[Frequentist vs. Bayesian Statistics with Tensorflow](https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[é¢‘ç‡å­¦æ´¾ä¸è´å¶æ–¯ç»Ÿè®¡å­¦åœ¨ Tensorflow ä¸­çš„å¯¹æ¯”](https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5)'
- en: '[Deterministic vs. Probabilistic Deep Learning](/deterministic-vs-probabilistic-deep-learning-5325769dc758)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ç¡®å®šæ€§ä¸æ¦‚ç‡æ·±åº¦å­¦ä¹ ](/deterministic-vs-probabilistic-deep-learning-5325769dc758)'
- en: '[Naive Bayes from scratch with TensorFlow](/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ä»å¤´å¼€å§‹ä½¿ç”¨ TensorFlow å®ç°æœ´ç´ è´å¶æ–¯](/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947)'
- en: Probabilistic Logistic Regression with TensorFlow
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ TensorFlow è¿›è¡Œ**æ¦‚ç‡é€»è¾‘å›å½’**
- en: '![](../Images/93f68e2e7ac8cbc8894c52dd44805819.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93f68e2e7ac8cbc8894c52dd44805819.png)'
- en: 'Figure 1: The motto for today: lines can separate more things than we give
    them credit for ([source](https://unsplash.com/photos/lIdS6_XiAR0))'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šä»Šå¤©çš„åº§å³é“­ï¼šç›´çº¿å¯ä»¥åˆ†éš”æ›´å¤šçš„äº‹ç‰©ï¼Œæ¯”æˆ‘ä»¬æƒ³è±¡çš„è¦å¤š ([source](https://unsplash.com/photos/lIdS6_XiAR0))
- en: As usual, the code is available on my [GitHub](https://github.com/luisroque/probabilistic_deep_learning_with_TFP).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¾€å¸¸ä¸€æ ·ï¼Œä»£ç å¯ä»¥åœ¨æˆ‘çš„ [GitHub](https://github.com/luisroque/probabilistic_deep_learning_with_TFP)
    ä¸Šæ‰¾åˆ°ã€‚
- en: Preliminary Work
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆæ­¥å·¥ä½œ
- en: In our previous article in this series we built the Naive Bayes algorithm from
    scratch and used it to classify wine samples based on selected characteristics.
    This time, we will be using a probabilistic logistic regression approach. Since
    we already followed the end-to-end approach, I will skip most of the Exploratory
    Data Analysis section and the class prior distribution definition.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬ç³»åˆ—æ–‡ç« çš„å‰ä¸€ç¯‡ä¸­ï¼Œæˆ‘ä»¬ä»å¤´å¼€å§‹æ„å»ºäº†æœ´ç´ è´å¶æ–¯ç®—æ³•ï¼Œå¹¶ä½¿ç”¨å®ƒæ ¹æ®é€‰æ‹©çš„ç‰¹å¾å¯¹è‘¡è„é…’æ ·æœ¬è¿›è¡Œåˆ†ç±»ã€‚è¿™ä¸€æ¬¡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨**æ¦‚ç‡é€»è¾‘å›å½’**æ–¹æ³•ã€‚ç”±äºæˆ‘ä»¬å·²ç»é‡‡ç”¨äº†ç«¯åˆ°ç«¯çš„æ–¹æ³•ï¼Œæˆ‘å°†è·³è¿‡å¤§éƒ¨åˆ†æ¢ç´¢æ€§æ•°æ®åˆ†æéƒ¨åˆ†å’Œç±»å…ˆéªŒåˆ†å¸ƒå®šä¹‰ã€‚
- en: The only thing to note is that there is a difference in the features that we
    selected for this model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å”¯ä¸€éœ€è¦æ³¨æ„çš„æ˜¯æˆ‘ä»¬ä¸ºæ­¤æ¨¡å‹é€‰æ‹©çš„ç‰¹å¾æœ‰æ‰€ä¸åŒã€‚
- en: '![](../Images/a2e8b7b5c2fe0281cf88364c78545fd9.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2e8b7b5c2fe0281cf88364c78545fd9.png)'
- en: 'Figure 2: Target samples distribution by alcohol and hue.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2ï¼šæ ¹æ®é…’ç²¾å’Œè‰²è°ƒçš„ç›®æ ‡æ ·æœ¬åˆ†å¸ƒã€‚
- en: We will use hue and flavanoids as our independent variables. Notice how these
    features are more effective in separating the target variable than alcohol and
    hue.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨è‰²è°ƒå’Œç±»é»„é…®ä½œä¸ºè‡ªå˜é‡ã€‚æ³¨æ„è¿™äº›ç‰¹å¾åœ¨åˆ†éš”ç›®æ ‡å˜é‡æ–¹é¢æ¯”é…’ç²¾å’Œè‰²è°ƒæ›´æœ‰æ•ˆã€‚
- en: '![](../Images/e7bfa9bd6d04e51dfc8d656ea2f977cf.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7bfa9bd6d04e51dfc8d656ea2f977cf.png)'
- en: 'Figure 3: Target samples distribution by flavanoids and hue.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3ï¼šæ ¹æ®ç±»é»„é…®å’Œè‰²è°ƒçš„ç›®æ ‡æ ·æœ¬åˆ†å¸ƒã€‚
- en: Probabilistic Logistic Regression
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**æ¦‚ç‡é€»è¾‘å›å½’**'
- en: Logistic regression is a widely-used statistical method for binary classification,
    which is used to model the relationship between a binary response variable and
    one or more predictors. Logistic regression can be used to model the probability
    of a binary outcome as a function of the predictor variables. The traditional
    logistic regression model is a deterministic model, which assumes that the relationship
    between the predictor variables and the response variable is fixed and known.
    However, in many real-world applications, the true relationship between the predictors
    and the response is uncertain, and it is more appropriate to use a probabilistic
    approach.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: é€»è¾‘å›å½’æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„äºŒå…ƒåˆ†ç±»ç»Ÿè®¡æ–¹æ³•ï¼Œç”¨äºå»ºæ¨¡äºŒå…ƒå“åº”å˜é‡ä¸ä¸€ä¸ªæˆ–å¤šä¸ªé¢„æµ‹å˜é‡ä¹‹é—´çš„å…³ç³»ã€‚é€»è¾‘å›å½’å¯ä»¥ç”¨æ¥å»ºæ¨¡äºŒå…ƒç»“æœçš„æ¦‚ç‡ä½œä¸ºé¢„æµ‹å˜é‡çš„å‡½æ•°ã€‚ä¼ ç»Ÿçš„é€»è¾‘å›å½’æ¨¡å‹æ˜¯ä¸€ä¸ªç¡®å®šæ€§æ¨¡å‹ï¼Œå‡è®¾é¢„æµ‹å˜é‡ä¸å“åº”å˜é‡ä¹‹é—´çš„å…³ç³»æ˜¯å›ºå®šä¸”å·²çŸ¥çš„ã€‚ç„¶è€Œï¼Œåœ¨è®¸å¤šå®é™…åº”ç”¨ä¸­ï¼Œé¢„æµ‹å˜é‡ä¸å“åº”å˜é‡ä¹‹é—´çš„çœŸå®å…³ç³»æ˜¯ä¸ç¡®å®šçš„ï¼Œå› æ­¤ä½¿ç”¨**æ¦‚ç‡æ–¹æ³•**æ›´ä¸ºåˆé€‚ã€‚
- en: Probabilistic logistic regression models the relationship between the predictor
    variables and the binary response variable using a probabilistic framework, and
    is able to account for uncertainty in the data and the model parameters. This
    is achieved by placing probability distributions over the model parameters, rather
    than assuming fixed values for them. In this way, probabilistic logistic regression
    models can provide more accurate predictions and better uncertainty quantification
    compared to traditional logistic regression models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¦‚ç‡é€»è¾‘å›å½’**æ¨¡å‹é€šè¿‡æ¦‚ç‡æ¡†æ¶å»ºæ¨¡é¢„æµ‹å˜é‡ä¸äºŒå…ƒå“åº”å˜é‡ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶èƒ½å¤Ÿè€ƒè™‘æ•°æ®å’Œæ¨¡å‹å‚æ•°çš„ä¸ç¡®å®šæ€§ã€‚è¿™æ˜¯é€šè¿‡å¯¹æ¨¡å‹å‚æ•°æ–½åŠ æ¦‚ç‡åˆ†å¸ƒæ¥å®ç°çš„ï¼Œè€Œä¸æ˜¯å‡è®¾å›ºå®šå€¼ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œä¸ä¼ ç»Ÿé€»è¾‘å›å½’æ¨¡å‹ç›¸æ¯”ï¼Œ**æ¦‚ç‡é€»è¾‘å›å½’**æ¨¡å‹å¯ä»¥æä¾›æ›´å‡†ç¡®çš„é¢„æµ‹å’Œæ›´å¥½çš„ä¸ç¡®å®šæ€§é‡åŒ–ã€‚'
- en: One of the most popular probabilistic models for logistic regression is the
    Bayesian logistic regression model. These models are based on Bayesâ€™ theorem,
    which states that the posterior probability of a model parameter given the data
    is proportional to the product of the likelihood of the data given the parameter
    and the prior probability of the parameter. Often, Bayesian logistic regression
    models use a conjugate prior distribution for the model parameters, which allows
    for closed-form solutions for the posterior distributions. This enables the computation
    of the probability of the response variable given the predictor variables, which
    is known as the posterior predictive distribution.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€æµè¡Œçš„é€»è¾‘å›å½’æ¦‚ç‡æ¨¡å‹ä¹‹ä¸€æ˜¯è´å¶æ–¯é€»è¾‘å›å½’æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹åŸºäºè´å¶æ–¯å®šç†ï¼Œè¯¥å®šç†æŒ‡å‡ºï¼Œç»™å®šæ•°æ®çš„æ¨¡å‹å‚æ•°çš„åéªŒæ¦‚ç‡ä¸æ•°æ®ç»™å®šå‚æ•°çš„ä¼¼ç„¶å’Œå‚æ•°çš„å…ˆéªŒæ¦‚ç‡çš„ä¹˜ç§¯æˆæ­£æ¯”ã€‚é€šå¸¸ï¼Œè´å¶æ–¯é€»è¾‘å›å½’æ¨¡å‹ä½¿ç”¨å…±è½­å…ˆéªŒåˆ†å¸ƒæ¥å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œå»ºæ¨¡ï¼Œè¿™å…è®¸å¯¹åéªŒåˆ†å¸ƒè¿›è¡Œå°é—­å½¢å¼çš„è§£æ³•ã€‚è¿™ä½¿å¾—è®¡ç®—å“åº”å˜é‡ç»™å®šé¢„æµ‹å˜é‡çš„æ¦‚ç‡æˆä¸ºå¯èƒ½ï¼Œè¿™è¢«ç§°ä¸ºåéªŒé¢„æµ‹åˆ†å¸ƒã€‚
- en: Likelihood
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¼¼ç„¶
- en: In this section, we present a method for computing the class-conditional densities
    in the probabilistic approach to logistic regression. Our method is based on the
    maximum likelihood estimate for the means, which is given by
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åœ¨é€»è¾‘å›å½’çš„æ¦‚ç‡æ–¹æ³•ä¸­è®¡ç®—ç±»åˆ«æ¡ä»¶å¯†åº¦çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºå‡å€¼çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œå…¶ç”±ä»¥ä¸‹å…¬å¼ç»™å‡º
- en: '![](../Images/3b98d5052f470207f04229ae1581cee8.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b98d5052f470207f04229ae1581cee8.png)'
- en: where ğ‘‹(ğ‘›)ğ‘– is the i-th feature of the n-th sample, ğ‘Œ(ğ‘›) is the target label
    of the n-th sample, ğ‘˜ is the class label, and ğ›¿(ğ‘Œ(ğ‘›)=ğ‘¦ğ‘˜) is an indicator function
    that equals 1 if ğ‘Œ(ğ‘›)=ğ‘¦ğ‘˜, and 0 otherwise.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ ğ‘‹(ğ‘›)ğ‘– æ˜¯ç¬¬ n ä¸ªæ ·æœ¬çš„ç¬¬ i ä¸ªç‰¹å¾ï¼Œğ‘Œ(ğ‘›) æ˜¯ç¬¬ n ä¸ªæ ·æœ¬çš„ç›®æ ‡æ ‡ç­¾ï¼Œğ‘˜ æ˜¯ç±»åˆ«æ ‡ç­¾ï¼Œğ›¿(ğ‘Œ(ğ‘›)=ğ‘¦ğ‘˜) æ˜¯ä¸€ä¸ªæŒ‡ç¤ºå‡½æ•°ï¼Œå¦‚æœ ğ‘Œ(ğ‘›)=ğ‘¦ğ‘˜
    åˆ™ç­‰äº 1ï¼Œå¦åˆ™ä¸º 0ã€‚
- en: To estimate the standard deviations ğœğ‘–, instead of using the closed-form solution
    we will be learning these parameters from the data. We achieve this by implementing
    a custom training loop, which optimizes the values of the standard deviations
    by minimizing the average per-example negative log-likelihood of the data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä¼°è®¡æ ‡å‡†å·® ğœğ‘–ï¼Œæˆ‘ä»¬å°†é€šè¿‡ä»æ•°æ®ä¸­å­¦ä¹ è¿™äº›å‚æ•°æ¥å®ç°ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å°é—­å½¢å¼çš„è§£æ³•ã€‚æˆ‘ä»¬é€šè¿‡å®ç°è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè¯¥å¾ªç¯é€šè¿‡æœ€å°åŒ–æ•°æ®çš„å¹³å‡æ¯ä¸ªç¤ºä¾‹è´Ÿå¯¹æ•°ä¼¼ç„¶æ¥ä¼˜åŒ–æ ‡å‡†å·®çš„å€¼ã€‚
- en: Our function computes the means ğœ‡ğ‘–ğ‘˜ of the class-conditional Gaussians according
    to the above equation. Then, it creates a multivariate Gaussian distribution object
    using MultivariateNormalDiag with the means set to ğœ‡ğ‘–ğ‘˜ and the scales set to the
    TensorFlow Variable.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å‡½æ•°æ ¹æ®ä¸Šè¿°æ–¹ç¨‹è®¡ç®—ç±»åˆ«æ¡ä»¶é«˜æ–¯åˆ†å¸ƒçš„å‡å€¼ ğœ‡ğ‘–ğ‘˜ã€‚ç„¶åï¼Œå®ƒä½¿ç”¨ MultivariateNormalDiag åˆ›å»ºä¸€ä¸ªå¤šå˜é‡é«˜æ–¯åˆ†å¸ƒå¯¹è±¡ï¼Œå°†å‡å€¼è®¾ç½®ä¸º
    ğœ‡ğ‘–ğ‘˜ï¼Œå°†å°ºåº¦è®¾ç½®ä¸º TensorFlow å˜é‡ã€‚
- en: The function runs a custom training loop for a specified number of epochs, in
    which the average per-example negative log-likelihood is computed. Next, the gradients
    are propagated and the scales variables updated accordingly. At each iteration,
    the values of the scales variable and the loss are saved.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å‡½æ•°è¿è¡Œä¸€ä¸ªè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œå¾ªç¯æ¬¡æ•°ä¸ºæŒ‡å®šçš„çºªå…ƒï¼Œå…¶ä¸­è®¡ç®—å¹³å‡æ¯ä¸ªç¤ºä¾‹çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ã€‚æ¥ä¸‹æ¥ï¼Œæ¢¯åº¦è¢«ä¼ æ’­ï¼Œå°ºåº¦å˜é‡ç›¸åº”åœ°æ›´æ–°ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå°ºåº¦å˜é‡çš„å€¼å’ŒæŸå¤±éƒ½ä¼šè¢«ä¿å­˜ã€‚
- en: 'It returns a tuple of three objects: the loss values, the scales variable at
    each iteration, and the final learned batched MultivariateNormalDiag distribution
    object.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒè¿”å›ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªå¯¹è±¡çš„å…ƒç»„ï¼šæŸå¤±å€¼ã€æ¯æ¬¡è¿­ä»£çš„å°ºåº¦å˜é‡å’Œæœ€ç»ˆå­¦ä¹ åˆ°çš„æ‰¹é‡ MultivariateNormalDiag åˆ†å¸ƒå¯¹è±¡ã€‚
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Letâ€™s create our variables to be trained.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åˆ›å»ºè¦è®­ç»ƒçš„å˜é‡ã€‚
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We are now ready to start the training procedure.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å‡†å¤‡å¼€å§‹è®­ç»ƒè¿‡ç¨‹ã€‚
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Finally, we can check how the model separates our classes of wine.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥æ¨¡å‹å¦‚ä½•åŒºåˆ†æˆ‘ä»¬çš„è‘¡è„é…’ç±»åˆ«ã€‚
- en: '![](../Images/be1367ed1a5cbbc932d24d31412fb536.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be1367ed1a5cbbc932d24d31412fb536.png)'
- en: 'Figure 4: Class-conditional density contours.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4ï¼šç±»åˆ«æ¡ä»¶å¯†åº¦è½®å»“ã€‚
- en: Using the function that we defined in the previous article, we can generate
    predictions for out test set. In the plot above we can see that the classes are
    well separated and thus we get a good accuracy from our model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æˆ‘ä»¬åœ¨å‰ä¸€ç¯‡æ–‡ç« ä¸­å®šä¹‰çš„å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºæµ‹è¯•é›†ç”Ÿæˆé¢„æµ‹ã€‚åœ¨ä¸Šé¢çš„å›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç±»åˆ«è¢«å¾ˆå¥½åœ°åˆ†å¼€ï¼Œå› æ­¤æˆ‘ä»¬ä»æ¨¡å‹ä¸­è·å¾—äº†è‰¯å¥½çš„å‡†ç¡®ç‡ã€‚
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In order to quantitatively assess the performance of our probabilistic logistic
    regression model, we plot the decision regions. These regions, defined by the
    boundaries that separate the two classes, provide insight into the ability of
    the model to separate the classes. Our analysis shows that the model is able to
    effectively separate the two classes in the dataset, as evidenced by the visually
    distinct regions. However, it is important to note that the decision boundary
    is constrained to be linear, as per the assumptions of the logistic regression
    model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®šé‡è¯„ä¼°æˆ‘ä»¬çš„æ¦‚ç‡é€»è¾‘å›å½’æ¨¡å‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬ç»˜åˆ¶äº†å†³ç­–åŒºåŸŸã€‚è¿™äº›åŒºåŸŸç”±åˆ†éš”ä¸¤ä¸ªç±»åˆ«çš„è¾¹ç•Œå®šä¹‰ï¼Œä¸ºæ¨¡å‹åŒºåˆ†ç±»åˆ«çš„èƒ½åŠ›æä¾›äº†è§è§£ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ†éš”æ•°æ®é›†ä¸­çš„ä¸¤ä¸ªç±»åˆ«ï¼Œè¿™ä»è§†è§‰ä¸Šæ˜æ˜¾çš„åŒºåŸŸå¯ä»¥çœ‹å‡ºã€‚ç„¶è€Œï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå†³ç­–è¾¹ç•Œè¢«é™åˆ¶ä¸ºçº¿æ€§ï¼Œè¿™æ˜¯é€»è¾‘å›å½’æ¨¡å‹çš„å‡è®¾ã€‚
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/9181695e5c5d25a63646a2eab35ea724.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9181695e5c5d25a63646a2eab35ea724.png)'
- en: 'Figure 5: Class-conditional decision regions.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5ï¼šç±»åˆ«æ¡ä»¶å†³ç­–åŒºåŸŸã€‚
- en: The missing link with Logistic Regression
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é€»è¾‘å›å½’ä¸­çš„ç¼ºå¤±é“¾æ¥
- en: In this section, we link the above definitions of the class-conditional densities
    to logistic regression. We show that the predictive distribution ğ‘ƒ(ğ‘Œ=ğ‘¦0|ğ‘‹) can
    be written as
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä¸Šè¿°ç±»åˆ«æ¡ä»¶å¯†åº¦çš„å®šä¹‰ä¸é€»è¾‘å›å½’è”ç³»èµ·æ¥ã€‚æˆ‘ä»¬å±•ç¤ºäº†é¢„æµ‹åˆ†å¸ƒ ğ‘ƒ(ğ‘Œ=ğ‘¦0|ğ‘‹) å¯ä»¥å†™ä½œ
- en: '![](../Images/7db37d902814496db2cfec21a921cc07.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7db37d902814496db2cfec21a921cc07.png)'
- en: where ğ‘ƒ(ğ‘‹|ğ‘Œ=ğ‘¦0) and ğ‘ƒ(ğ‘‹|ğ‘Œ=ğ‘¦1) are the class-conditional densities, and ğ‘ƒ(ğ‘Œ=ğ‘¦0)
    and ğ‘ƒ(ğ‘Œ=ğ‘¦1) are the class priors.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ ğ‘ƒ(ğ‘‹|ğ‘Œ=ğ‘¦0) å’Œ ğ‘ƒ(ğ‘‹|ğ‘Œ=ğ‘¦1) æ˜¯ç±»åˆ«æ¡ä»¶å¯†åº¦ï¼Œğ‘ƒ(ğ‘Œ=ğ‘¦0) å’Œ ğ‘ƒ(ğ‘Œ=ğ‘¦1) æ˜¯ç±»åˆ«å…ˆéªŒã€‚
- en: This equation can be re-arranged to give ğ‘ƒ(ğ‘Œ=ğ‘¦0|ğ‘‹)=ğœ(ğ‘) where
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ–¹ç¨‹å¯ä»¥é‡æ–°æ’åˆ—ä¸º ğ‘ƒ(ğ‘Œ=ğ‘¦0|ğ‘‹)=ğœ(ğ‘)ï¼Œå…¶ä¸­
- en: '![](../Images/269d6513225ebe05b27a1e7ba8b33184.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/269d6513225ebe05b27a1e7ba8b33184.png)'
- en: is the sigmoid function, and
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯ sigmoid å‡½æ•°ï¼Œä»¥åŠ
- en: '![](../Images/06aaf728416430df589244b420b559e9.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06aaf728416430df589244b420b559e9.png)'
- en: is the log-odds.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯å¯¹æ•°èµ”ç‡ã€‚
- en: With our additional modeling assumption of a shared covariance matrix Î£, it
    can be shown, using the Gaussian pdf, that ğ‘ is in fact a linear function of ğ‘‹,
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æˆ‘ä»¬é¢å¤–çš„å»ºæ¨¡å‡è®¾ï¼Œå³å…±äº«åæ–¹å·®çŸ©é˜µ Î£ï¼Œå¯ä»¥ä½¿ç”¨é«˜æ–¯æ¦‚ç‡å¯†åº¦å‡½æ•°æ˜¾ç¤º ğ‘ å®é™…ä¸Šæ˜¯ ğ‘‹ çš„çº¿æ€§å‡½æ•°ï¼Œ
- en: '![](../Images/162844021b49c68e46b9aa7be22d906e.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/162844021b49c68e46b9aa7be22d906e.png)'
- en: where
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­
- en: '![](../Images/be8553acfb09ac4ae9a6a406e41435ea.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be8553acfb09ac4ae9a6a406e41435ea.png)'
- en: This linear function, ğ‘=ğ‘¤ğ‘‡ğ‘‹+ğ‘¤0, explains the reason behind the decision boundary
    of a logistic regression being linear. It can be seen that the parameters ğ‘¤ and
    ğ‘¤0 are functions of the class-conditional densities ğ‘ƒ(ğ‘‹|ğ‘Œ=ğ‘¦0) and ğ‘ƒ(ğ‘‹|ğ‘Œ=ğ‘¦1) and
    the class priors ğ‘ƒ(ğ‘Œ=ğ‘¦0) and ğ‘ƒ(ğ‘Œ=ğ‘¦1). These parameters are typically estimated
    with maximum likelihood, as we have done in previous sections.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªçº¿æ€§å‡½æ•° ğ‘=ğ‘¤ğ‘‡ğ‘‹+ğ‘¤0 è§£é‡Šäº†é€»è¾‘å›å½’çš„å†³ç­–è¾¹ç•Œä¸ºä½•æ˜¯çº¿æ€§çš„ã€‚å¯ä»¥çœ‹å‡ºï¼Œå‚æ•° ğ‘¤ å’Œ ğ‘¤0 æ˜¯ç±»åˆ«æ¡ä»¶å¯†åº¦ ğ‘ƒ(ğ‘‹|ğ‘Œ=ğ‘¦0) å’Œ ğ‘ƒ(ğ‘‹|ğ‘Œ=ğ‘¦1)
    ä»¥åŠç±»åˆ«å…ˆéªŒ ğ‘ƒ(ğ‘Œ=ğ‘¦0) å’Œ ğ‘ƒ(ğ‘Œ=ğ‘¦1) çš„å‡½æ•°ã€‚è¿™äº›å‚æ•°é€šå¸¸é€šè¿‡æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œå¦‚æˆ‘ä»¬åœ¨å‰é¢ç« èŠ‚ä¸­æ‰€åšçš„é‚£æ ·ã€‚
- en: Generative Logistic Regression Model
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”Ÿæˆé€»è¾‘å›å½’æ¨¡å‹
- en: In this section, we use the equations derived in previous sections to directly
    parameterize the output Bernoulli distribution of the generative logistic regression
    model. Specifically, we use the prior distribution and class-conditional distributions
    to compute the weights and bias terms ğ‘¤ and ğ‘¤0.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å‰é¢ç« èŠ‚ä¸­æ¨å¯¼å‡ºçš„æ–¹ç¨‹æ¥ç›´æ¥å‚æ•°åŒ–ç”Ÿæˆé€»è¾‘å›å½’æ¨¡å‹çš„è¾“å‡ºä¼¯åŠªåˆ©åˆ†å¸ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨å…ˆéªŒåˆ†å¸ƒå’Œç±»åˆ«æ¡ä»¶åˆ†å¸ƒæ¥è®¡ç®—æƒé‡å’Œåç½®é¡¹ ğ‘¤ å’Œ
    ğ‘¤0ã€‚
- en: To achieve this, we write a new function that takes the prior distribution and
    the class-conditional distributions as inputs. The function uses the parameters
    of these distributions to compute the weights and bias terms according to the
    equations derived in previous sections.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ç¼–å†™äº†ä¸€ä¸ªæ–°å‡½æ•°ï¼Œè¯¥å‡½æ•°ä»¥å…ˆéªŒåˆ†å¸ƒå’Œç±»åˆ«æ¡ä»¶åˆ†å¸ƒä½œä¸ºè¾“å…¥ã€‚è¯¥å‡½æ•°ä½¿ç”¨è¿™äº›åˆ†å¸ƒçš„å‚æ•°æ¥è®¡ç®—æƒé‡å’Œåç½®é¡¹ï¼Œæ ¹æ®å‰é¢ç« èŠ‚ä¸­æ¨å¯¼å‡ºçš„æ–¹ç¨‹ã€‚
- en: The inputs to the function are the prior distribution prior over the two classes
    and the class-conditional distributions.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•°çš„è¾“å…¥æ˜¯å¯¹ä¸¤ä¸ªç±»åˆ«çš„å…ˆéªŒåˆ†å¸ƒå’Œç±»åˆ«æ¡ä»¶åˆ†å¸ƒã€‚
- en: The function then uses these inputs to compute the weights and bias terms as
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œå‡½æ•°ä½¿ç”¨è¿™äº›è¾“å…¥æ¥è®¡ç®—æƒé‡å’Œåç½®é¡¹ï¼Œå¦‚ä¸‹æ‰€ç¤º
- en: '![](../Images/be8553acfb09ac4ae9a6a406e41435ea.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be8553acfb09ac4ae9a6a406e41435ea.png)'
- en: The function returns ğ‘¤ and ğ‘¤0, which can be used to directly parameterize the
    output Bernoulli distribution of the generative logistic regression model. This
    allows for a more direct and transparent understanding of the model parameters
    and their relationship to the prior and class-conditional distributions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å‡½æ•°è¿”å›ğ‘¤å’Œğ‘¤0ï¼Œè¿™å¯ä»¥ç”¨æ¥ç›´æ¥å‚æ•°åŒ–ç”Ÿæˆé€»è¾‘å›å½’æ¨¡å‹çš„è¾“å‡ºä¼¯åŠªåˆ©åˆ†å¸ƒã€‚è¿™å…è®¸å¯¹æ¨¡å‹å‚æ•°åŠå…¶ä¸å…ˆéªŒå’Œç±»æ¡ä»¶åˆ†å¸ƒçš„å…³ç³»æœ‰æ›´ç›´æ¥å’Œé€æ˜çš„ç†è§£ã€‚
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can now use these parameters to make a contour plot to display the predictive
    distribution of our logistic regression model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å¯ä»¥ä½¿ç”¨è¿™äº›å‚æ•°åˆ¶ä½œè½®å»“å›¾ï¼Œä»¥æ˜¾ç¤ºæˆ‘ä»¬é€»è¾‘å›å½’æ¨¡å‹çš„é¢„æµ‹åˆ†å¸ƒã€‚
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/7148914bfdb2d7c07d2ef4e63c054c23.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7148914bfdb2d7c07d2ef4e63c054c23.png)'
- en: 'Figure 6: Density contours of the predictive distribution of our logistic regression
    model.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 6: æˆ‘ä»¬é€»è¾‘å›å½’æ¨¡å‹é¢„æµ‹åˆ†å¸ƒçš„å¯†åº¦è½®å»“ã€‚'
- en: Is our approach fully Bayesian?
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æ–¹æ³•æ˜¯å¦å®Œå…¨æ˜¯è´å¶æ–¯çš„ï¼Ÿ
- en: The above approach can be considered a form of Bayesian inference, as it involves
    incorporating prior knowledge about the model parameters through the prior distribution
    and updating this knowledge using the observed data through the class-conditional
    distributions. This is a key aspect of Bayesian inference, which aims to incorporate
    prior knowledge and uncertainty about the model parameters into the inference
    process.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°æ–¹æ³•å¯ä»¥è¢«è§†ä¸ºä¸€ç§è´å¶æ–¯æ¨æ–­å½¢å¼ï¼Œå› ä¸ºå®ƒæ¶‰åŠé€šè¿‡å…ˆéªŒåˆ†å¸ƒå¼•å…¥å…³äºæ¨¡å‹å‚æ•°çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶é€šè¿‡ç±»æ¡ä»¶åˆ†å¸ƒä½¿ç”¨è§‚æµ‹æ•°æ®æ›´æ–°è¿™äº›çŸ¥è¯†ã€‚è¿™æ˜¯è´å¶æ–¯æ¨æ–­çš„ä¸€ä¸ªå…³é”®æ–¹é¢ï¼Œæ—¨åœ¨å°†å…³äºæ¨¡å‹å‚æ•°çš„å…ˆéªŒçŸ¥è¯†å’Œä¸ç¡®å®šæ€§èå…¥æ¨æ–­è¿‡ç¨‹ã€‚
- en: In Bayesian inference, the goal is to compute the posterior distribution of
    the model parameters given the observed data. The above approach can be seen as
    a form of approximate Bayesian inference, as it involves using the maximum likelihood
    estimates of the class-conditional densities and the prior distribution to compute
    the weights and biases of the model. Also, the approach incorporates uncertainty
    through the shared covariance matrix, which serves as a regularization term.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è´å¶æ–¯æ¨æ–­ä¸­ï¼Œç›®æ ‡æ˜¯è®¡ç®—ç»™å®šè§‚æµ‹æ•°æ®ä¸‹æ¨¡å‹å‚æ•°çš„åéªŒåˆ†å¸ƒã€‚ä¸Šè¿°æ–¹æ³•å¯ä»¥çœ‹ä½œæ˜¯ä¸€ç§è¿‘ä¼¼è´å¶æ–¯æ¨æ–­ï¼Œå› ä¸ºå®ƒæ¶‰åŠä½¿ç”¨ç±»æ¡ä»¶å¯†åº¦å’Œå…ˆéªŒåˆ†å¸ƒçš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ¥è®¡ç®—æ¨¡å‹çš„æƒé‡å’Œåç½®ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•é€šè¿‡å…±äº«åæ–¹å·®çŸ©é˜µæ¥å¼•å…¥ä¸ç¡®å®šæ€§ï¼Œè¿™ä½œä¸ºæ­£åˆ™åŒ–é¡¹ã€‚
- en: It is worth noting that the above approach is not fully Bayesian, as it does
    not provide a closed form for the posterior of the model parameters. Instead,
    it uses an approximation based on the maximum likelihood estimates.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸Šè¿°æ–¹æ³•å¹¶éå®Œå…¨è´å¶æ–¯ï¼Œå› ä¸ºå®ƒæ²¡æœ‰æä¾›æ¨¡å‹å‚æ•°çš„åéªŒåˆ†å¸ƒçš„å°é—­å½¢å¼ã€‚ç›¸åï¼Œå®ƒä½¿ç”¨åŸºäºæœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„è¿‘ä¼¼ã€‚
- en: Conclusions
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this article, we proposed a probabilistic approach to logistic regression
    that addresses aleatoric uncertainty in the prediction process. Through the incorporation
    of a prior distribution on the model parameters, our approach regularizes the
    model and prevents overfitting. We show how to implement the approach using TensorFlow
    Probability and how to analyse its results.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¦‚ç‡æ–¹æ³•æ¥å¤„ç†é¢„æµ‹è¿‡ç¨‹ä¸­çš„å†…åœ¨ä¸ç¡®å®šæ€§ã€‚é€šè¿‡åœ¨æ¨¡å‹å‚æ•°ä¸Šå¼•å…¥å…ˆéªŒåˆ†å¸ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯¹æ¨¡å‹è¿›è¡Œæ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨
    TensorFlow Probability å®ç°è¯¥æ–¹æ³•ä»¥åŠå¦‚ä½•åˆ†æå…¶ç»“æœã€‚
- en: It is worth noting that while our approach incorporates Bayesian principles,
    it is not a full Bayesian approach as we do not have a full posterior distribution
    of the model parameters. Nevertheless, accounting for aleatoric uncertainty in
    the prediction process already gives us more confidence about our predictive process.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡æˆ‘ä»¬çš„æ–¹æ³•åŒ…å«äº†è´å¶æ–¯åŸåˆ™ï¼Œä½†å®ƒå¹¶ä¸æ˜¯å®Œå…¨è´å¶æ–¯çš„æ–¹æ³•ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰æ¨¡å‹å‚æ•°çš„å®Œæ•´åéªŒåˆ†å¸ƒã€‚ç„¶è€Œï¼Œè€ƒè™‘åˆ°é¢„æµ‹è¿‡ç¨‹ä¸­çš„å†…åœ¨ä¸ç¡®å®šæ€§ï¼Œå·²ç»ä½¿æˆ‘ä»¬å¯¹é¢„æµ‹è¿‡ç¨‹æ›´æœ‰ä¿¡å¿ƒã€‚
- en: 'Keep in touch: [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¿æŒè”ç³»: [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)'
- en: References and Materials
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®å’Œææ–™
- en: '[1] â€” [Wine Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] â€” [è‘¡è„é…’æ•°æ®é›†](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine)'
- en: '[2] â€” [Coursera: Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] â€” [Coursera: æ·±åº¦å­¦ä¹ ä¸“é¡¹è¯¾ç¨‹](https://www.coursera.org/specializations/deep-learning)'
- en: '[3] â€” [Coursera: TensorFlow 2 for Deep Learning](https://www.coursera.org/specializations/tensorflow2-deeplearning)
    Specialization'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] â€” [Coursera: TensorFlow 2 æ·±åº¦å­¦ä¹ ä¸“é¡¹è¯¾ç¨‹](https://www.coursera.org/specializations/tensorflow2-deeplearning)'
- en: '[4] â€” [TensorFlow Probability Guides and Tutorials](https://www.tensorflow.org/probability/overview)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] â€” [TensorFlow æ¦‚ç‡æŒ‡å—å’Œæ•™ç¨‹](https://www.tensorflow.org/probability/overview)'
- en: '[5] â€” [TensorFlow Probability Posts in TensorFlow Blog](https://blog.tensorflow.org/search?label=TensorFlow+Probability&max-results=20)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] â€” [TensorFlow åšå®¢ä¸­çš„ TensorFlow æ¦‚ç‡å¸–å­](https://blog.tensorflow.org/search?label=TensorFlow+Probability&max-results=20)'
