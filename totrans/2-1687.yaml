- en: Probabilistic Logistic Regression with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/probabilistic-logistic-regression-with-tensorflow-73e18f0ddc48](https://towardsdatascience.com/probabilistic-logistic-regression-with-tensorflow-73e18f0ddc48)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Probabilistic Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@luisroque?source=post_page-----73e18f0ddc48--------------------------------)[![Lu√≠s
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----73e18f0ddc48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----73e18f0ddc48--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----73e18f0ddc48--------------------------------)
    [Lu√≠s Roque](https://medium.com/@luisroque?source=post_page-----73e18f0ddc48--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----73e18f0ddc48--------------------------------)
    ¬∑9 min read¬∑Jan 25, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article belongs to the series ‚ÄúProbabilistic Deep Learning‚Äù. This weekly
    series covers probabilistic approaches to deep learning. The main goal is to extend
    deep learning models to quantify uncertainty, i.e., know what they do not know.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will introduce the concept of probabilistic logistic regression,
    a powerful technique that allows for the inclusion of uncertainty in the prediction
    process. We will explore how this approach can lead to more robust and accurate
    predictions, especially in cases where the data is noisy, or the model is overfitting.
    Additionally, by incorporating a prior distribution on the model parameters, we
    can regularize the model and prevent overfitting. This approach serves as a great
    first step into the exciting world of Bayesian Deep Learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Articles published so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Gentle Introduction to TensorFlow Probability: Distribution Objects](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Gentle Introduction to TensorFlow Probability: Trainable Parameters](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Maximum Likelihood Estimation from scratch in TensorFlow Probability](/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Probabilistic Linear Regression from scratch in TensorFlow](/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Probabilistic vs. Deterministic Regression with Tensorflow](https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Frequentist vs. Bayesian Statistics with Tensorflow](https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Deterministic vs. Probabilistic Deep Learning](/deterministic-vs-probabilistic-deep-learning-5325769dc758)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Naive Bayes from scratch with TensorFlow](/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Probabilistic Logistic Regression with TensorFlow
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/93f68e2e7ac8cbc8894c52dd44805819.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The motto for today: lines can separate more things than we give
    them credit for ([source](https://unsplash.com/photos/lIdS6_XiAR0))'
  prefs: []
  type: TYPE_NORMAL
- en: As usual, the code is available on my [GitHub](https://github.com/luisroque/probabilistic_deep_learning_with_TFP).
  prefs: []
  type: TYPE_NORMAL
- en: Preliminary Work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our previous article in this series we built the Naive Bayes algorithm from
    scratch and used it to classify wine samples based on selected characteristics.
    This time, we will be using a probabilistic logistic regression approach. Since
    we already followed the end-to-end approach, I will skip most of the Exploratory
    Data Analysis section and the class prior distribution definition.
  prefs: []
  type: TYPE_NORMAL
- en: The only thing to note is that there is a difference in the features that we
    selected for this model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a2e8b7b5c2fe0281cf88364c78545fd9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Target samples distribution by alcohol and hue.'
  prefs: []
  type: TYPE_NORMAL
- en: We will use hue and flavanoids as our independent variables. Notice how these
    features are more effective in separating the target variable than alcohol and
    hue.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e7bfa9bd6d04e51dfc8d656ea2f977cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Target samples distribution by flavanoids and hue.'
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression is a widely-used statistical method for binary classification,
    which is used to model the relationship between a binary response variable and
    one or more predictors. Logistic regression can be used to model the probability
    of a binary outcome as a function of the predictor variables. The traditional
    logistic regression model is a deterministic model, which assumes that the relationship
    between the predictor variables and the response variable is fixed and known.
    However, in many real-world applications, the true relationship between the predictors
    and the response is uncertain, and it is more appropriate to use a probabilistic
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic logistic regression models the relationship between the predictor
    variables and the binary response variable using a probabilistic framework, and
    is able to account for uncertainty in the data and the model parameters. This
    is achieved by placing probability distributions over the model parameters, rather
    than assuming fixed values for them. In this way, probabilistic logistic regression
    models can provide more accurate predictions and better uncertainty quantification
    compared to traditional logistic regression models.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most popular probabilistic models for logistic regression is the
    Bayesian logistic regression model. These models are based on Bayes‚Äô theorem,
    which states that the posterior probability of a model parameter given the data
    is proportional to the product of the likelihood of the data given the parameter
    and the prior probability of the parameter. Often, Bayesian logistic regression
    models use a conjugate prior distribution for the model parameters, which allows
    for closed-form solutions for the posterior distributions. This enables the computation
    of the probability of the response variable given the predictor variables, which
    is known as the posterior predictive distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Likelihood
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we present a method for computing the class-conditional densities
    in the probabilistic approach to logistic regression. Our method is based on the
    maximum likelihood estimate for the means, which is given by
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3b98d5052f470207f04229ae1581cee8.png)'
  prefs: []
  type: TYPE_IMG
- en: where ùëã(ùëõ)ùëñ is the i-th feature of the n-th sample, ùëå(ùëõ) is the target label
    of the n-th sample, ùëò is the class label, and ùõø(ùëå(ùëõ)=ùë¶ùëò) is an indicator function
    that equals 1 if ùëå(ùëõ)=ùë¶ùëò, and 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: To estimate the standard deviations ùúéùëñ, instead of using the closed-form solution
    we will be learning these parameters from the data. We achieve this by implementing
    a custom training loop, which optimizes the values of the standard deviations
    by minimizing the average per-example negative log-likelihood of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Our function computes the means ùúáùëñùëò of the class-conditional Gaussians according
    to the above equation. Then, it creates a multivariate Gaussian distribution object
    using MultivariateNormalDiag with the means set to ùúáùëñùëò and the scales set to the
    TensorFlow Variable.
  prefs: []
  type: TYPE_NORMAL
- en: The function runs a custom training loop for a specified number of epochs, in
    which the average per-example negative log-likelihood is computed. Next, the gradients
    are propagated and the scales variables updated accordingly. At each iteration,
    the values of the scales variable and the loss are saved.
  prefs: []
  type: TYPE_NORMAL
- en: 'It returns a tuple of three objects: the loss values, the scales variable at
    each iteration, and the final learned batched MultivariateNormalDiag distribution
    object.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs create our variables to be trained.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready to start the training procedure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we can check how the model separates our classes of wine.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be1367ed1a5cbbc932d24d31412fb536.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Class-conditional density contours.'
  prefs: []
  type: TYPE_NORMAL
- en: Using the function that we defined in the previous article, we can generate
    predictions for out test set. In the plot above we can see that the classes are
    well separated and thus we get a good accuracy from our model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In order to quantitatively assess the performance of our probabilistic logistic
    regression model, we plot the decision regions. These regions, defined by the
    boundaries that separate the two classes, provide insight into the ability of
    the model to separate the classes. Our analysis shows that the model is able to
    effectively separate the two classes in the dataset, as evidenced by the visually
    distinct regions. However, it is important to note that the decision boundary
    is constrained to be linear, as per the assumptions of the logistic regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9181695e5c5d25a63646a2eab35ea724.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Class-conditional decision regions.'
  prefs: []
  type: TYPE_NORMAL
- en: The missing link with Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we link the above definitions of the class-conditional densities
    to logistic regression. We show that the predictive distribution ùëÉ(ùëå=ùë¶0|ùëã) can
    be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7db37d902814496db2cfec21a921cc07.png)'
  prefs: []
  type: TYPE_IMG
- en: where ùëÉ(ùëã|ùëå=ùë¶0) and ùëÉ(ùëã|ùëå=ùë¶1) are the class-conditional densities, and ùëÉ(ùëå=ùë¶0)
    and ùëÉ(ùëå=ùë¶1) are the class priors.
  prefs: []
  type: TYPE_NORMAL
- en: This equation can be re-arranged to give ùëÉ(ùëå=ùë¶0|ùëã)=ùúé(ùëé) where
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/269d6513225ebe05b27a1e7ba8b33184.png)'
  prefs: []
  type: TYPE_IMG
- en: is the sigmoid function, and
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06aaf728416430df589244b420b559e9.png)'
  prefs: []
  type: TYPE_IMG
- en: is the log-odds.
  prefs: []
  type: TYPE_NORMAL
- en: With our additional modeling assumption of a shared covariance matrix Œ£, it
    can be shown, using the Gaussian pdf, that ùëé is in fact a linear function of ùëã,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/162844021b49c68e46b9aa7be22d906e.png)'
  prefs: []
  type: TYPE_IMG
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be8553acfb09ac4ae9a6a406e41435ea.png)'
  prefs: []
  type: TYPE_IMG
- en: This linear function, ùëé=ùë§ùëáùëã+ùë§0, explains the reason behind the decision boundary
    of a logistic regression being linear. It can be seen that the parameters ùë§ and
    ùë§0 are functions of the class-conditional densities ùëÉ(ùëã|ùëå=ùë¶0) and ùëÉ(ùëã|ùëå=ùë¶1) and
    the class priors ùëÉ(ùëå=ùë¶0) and ùëÉ(ùëå=ùë¶1). These parameters are typically estimated
    with maximum likelihood, as we have done in previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: Generative Logistic Regression Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we use the equations derived in previous sections to directly
    parameterize the output Bernoulli distribution of the generative logistic regression
    model. Specifically, we use the prior distribution and class-conditional distributions
    to compute the weights and bias terms ùë§ and ùë§0.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this, we write a new function that takes the prior distribution and
    the class-conditional distributions as inputs. The function uses the parameters
    of these distributions to compute the weights and bias terms according to the
    equations derived in previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: The inputs to the function are the prior distribution prior over the two classes
    and the class-conditional distributions.
  prefs: []
  type: TYPE_NORMAL
- en: The function then uses these inputs to compute the weights and bias terms as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be8553acfb09ac4ae9a6a406e41435ea.png)'
  prefs: []
  type: TYPE_IMG
- en: The function returns ùë§ and ùë§0, which can be used to directly parameterize the
    output Bernoulli distribution of the generative logistic regression model. This
    allows for a more direct and transparent understanding of the model parameters
    and their relationship to the prior and class-conditional distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can now use these parameters to make a contour plot to display the predictive
    distribution of our logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7148914bfdb2d7c07d2ef4e63c054c23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Density contours of the predictive distribution of our logistic regression
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: Is our approach fully Bayesian?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The above approach can be considered a form of Bayesian inference, as it involves
    incorporating prior knowledge about the model parameters through the prior distribution
    and updating this knowledge using the observed data through the class-conditional
    distributions. This is a key aspect of Bayesian inference, which aims to incorporate
    prior knowledge and uncertainty about the model parameters into the inference
    process.
  prefs: []
  type: TYPE_NORMAL
- en: In Bayesian inference, the goal is to compute the posterior distribution of
    the model parameters given the observed data. The above approach can be seen as
    a form of approximate Bayesian inference, as it involves using the maximum likelihood
    estimates of the class-conditional densities and the prior distribution to compute
    the weights and biases of the model. Also, the approach incorporates uncertainty
    through the shared covariance matrix, which serves as a regularization term.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that the above approach is not fully Bayesian, as it does
    not provide a closed form for the posterior of the model parameters. Instead,
    it uses an approximation based on the maximum likelihood estimates.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we proposed a probabilistic approach to logistic regression
    that addresses aleatoric uncertainty in the prediction process. Through the incorporation
    of a prior distribution on the model parameters, our approach regularizes the
    model and prevents overfitting. We show how to implement the approach using TensorFlow
    Probability and how to analyse its results.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that while our approach incorporates Bayesian principles,
    it is not a full Bayesian approach as we do not have a full posterior distribution
    of the model parameters. Nevertheless, accounting for aleatoric uncertainty in
    the prediction process already gives us more confidence about our predictive process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep in touch: [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)'
  prefs: []
  type: TYPE_NORMAL
- en: References and Materials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] ‚Äî [Wine Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] ‚Äî [Coursera: Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] ‚Äî [Coursera: TensorFlow 2 for Deep Learning](https://www.coursera.org/specializations/tensorflow2-deeplearning)
    Specialization'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] ‚Äî [TensorFlow Probability Guides and Tutorials](https://www.tensorflow.org/probability/overview)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] ‚Äî [TensorFlow Probability Posts in TensorFlow Blog](https://blog.tensorflow.org/search?label=TensorFlow+Probability&max-results=20)'
  prefs: []
  type: TYPE_NORMAL
