- en: DINO — A Foundation Model for Computer Vision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/dino-a-foundation-model-for-computer-vision-4cb08e821b18](https://towardsdatascience.com/dino-a-foundation-model-for-computer-vision-4cb08e821b18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[🚀Sascha’s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Emerging Properties in Self-Supervised Vision Transformers by M. Caron et. al.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4cb08e821b18--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4cb08e821b18--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4cb08e821b18--------------------------------)
    ·13 min read·Sep 27, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: It is an exciting decade for computer vision. Great successes from the natural
    language domain are transferred to the vision domain including the introduction
    of the ViT (vision transformer) and lately large-scale self-supervised pre-training
    techniques have made headlines under the name of foundation models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'Today we are looking into a framework called DINO (self **DI**stillation, **N**O
    labels), a visual foundation model built on interesting properties of ViTs. It
    is also the predecessor of one of today’s best performing foundation models: [DINOv2](https://arxiv.org/abs/2304.07193).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/826ce8cc3feae5497593efe2c4e9631c.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: Image created from [publication](https://arxiv.org/abs/2104.14294) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '**Paper:** [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294),
    by [Mathilde Caron](https://arxiv.org/search/cs?searchtype=author&query=Caron%2C+M)
    et.al., 29\. Apr. 2021'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/facebookresearch/dino) — [Blog Post](https://ai.meta.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/)'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** foundation model, computer vision, vision transformer, knowledge
    distillation, similarity learning, self-supervised learning'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    — [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    — [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    — [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    — [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    — [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    — [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    — [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    — [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
- en: Outline
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大纲
- en: Context & Background
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 背景与背景
- en: Method
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 方法
- en: Experiments
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实验
- en: Ablations
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 消融测试
- en: Conclusion
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结论
- en: Further Readings & Resources
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进一步阅读与资源
- en: Context & Background
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景与背景
- en: The year is 2021, April to be precise. It has been four years since the release
    of the transformer model with [attention is all you need](https://arxiv.org/abs/1706.03762).
    Self-supervised pre-training is long being practiced in NLP by models such as
    [BERT](https://arxiv.org/pdf/1810.04805.pdf) and the term foundation model is
    not yet known for the next few months until the release of *“*[*on the opportunities
    and Risks of Foundation Models*](https://arxiv.org/abs/2108.07258)*”*. Six months
    earlier the [Vision transformer (ViT)](https://arxiv.org/abs/2010.11929) was first
    published on arxiv and it is still one month until ICLR 2021 where it will be
    presented.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 时间是 2021 年，准确地说是 4 月。自从发布了带有 [Attention is All You Need](https://arxiv.org/abs/1706.03762)
    的 Transformer 模型已经过去了四年。自监督预训练在 NLP 中已经由 [BERT](https://arxiv.org/pdf/1810.04805.pdf)
    等模型长期实践，而“基础模型”这一术语在接下来的几个月中尚未被知晓，直到 *“*[*关于基础模型的机遇与风险*](https://arxiv.org/abs/2108.07258)*”*
    的发布。六个月前，[Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929) 首次发布在 arxiv
    上，距离 ICLR 2021 还有一个月，它将在那里进行展示。
- en: 'Let that sink in for a moment: ViT had its debut on arxiv.org in October 2020
    and was presented on ICLR2021 in May 2021\. DINO was released on arxiv in April
    2021\. So, one month before it was actually presented on a conference. This would
    mean they only had 5 months if they had started right away to come up with the
    project’s idea, compile a team, lay out the theoretical foundation, train the
    model, perform experiments and ablations, and write the paper. No wonder PhD students
    these days feel constantly anxious. At least that’s what’s happening to me sometimes
    *😅*'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们稍微消化一下这个信息：ViT 于 2020 年 10 月在 arxiv.org 上首次发布，并在 2021 年 5 月的 ICLR2021 上进行了展示。DINO
    于 2021 年 4 月在 arxiv 上发布。所以，实际在会议上展示前的一个月。这意味着他们只有 5 个月的时间，如果他们立即开始的话，来构思项目的想法、组建团队、奠定理论基础、训练模型、进行实验和消融测试，并撰写论文。难怪现在的博士生感到不断的焦虑。至少这就是我有时的感受
    *😅*。
- en: While ViTs were very competitive with convolutional networks, they are demanding
    in terms of computational resources and amount of training data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 ViT 与卷积网络非常具有竞争力，但它们在计算资源和训练数据量方面的要求很高。
- en: 'The authors of DINO made a simple observation: the success of transformers
    in NLP was coupled with self-supervised pre-training and current self-supervised
    methods in the vision domain are built from convnets, like e.g. [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: DINO 的作者做出了一个简单的观察：变换器在 NLP 中的成功与自监督预训练相关，而目前视觉领域的自监督方法是基于卷积网络的，比如 [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)。
- en: '[](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----4cb08e821b18--------------------------------)
    [## BYOL -The Alternative to Contrastive Self-Supervised Learning'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----4cb08e821b18--------------------------------)
    [## BYOL -对比自监督学习的替代方案'
- en: 'Paper Analysis— Bootstrap Your Own Latent: A New Approach to Self-Supervised
    Learning'
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '论文分析——《Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning》'
- en: towardsdatascience.com](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----4cb08e821b18--------------------------------)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----4cb08e821b18--------------------------------)'
- en: 'Inspired by [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)
    and the [mean teacher](https://arxiv.org/abs/1703.01780), the authors came up
    with a framework to train a ViT in a self-supervised fashion and found:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 受到 [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)
    和 [mean teacher](https://arxiv.org/abs/1703.01780) 的启发，作者提出了一个框架来以自监督的方式训练 ViT，并发现：
- en: Self-supervised ViT features explicitly contain the scene layout and, in particular,
    object boundaries.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自监督 ViT 特征明确包含场景布局，特别是对象边界。
- en: Self-supervised ViT features perform particularly well with a basic nearest
    neighbors classifier (k-NN) without any fine-tuning, linear classifier nor data
    augmentation.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自监督 ViT 特征在没有任何微调、线性分类器或数据增强的情况下，与基础的最近邻分类器 (k-NN) 一起表现尤为出色。
- en: In contrast to BYOL and mean teacher, DINO implements a knowledge-distillation
    framework consisting of a student and teacher model that acts upon different views
    of the same image and adds extra measures to deal with inherent instabilities
    of similarity-learning approaches, where solutions often collapse.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与 BYOL 和 mean teacher 相比，DINO 实现了一个知识蒸馏框架，包括一个学生模型和一个教师模型，作用于同一图像的不同视角，并采取额外措施应对相似性学习方法的固有不稳定性，其中解决方案通常会崩溃。
- en: 'An interesting finding of the underlying vision transformer architecture (ViT)
    is that when trained with unsupervised learning techniques its features contain
    explicit information about the semantic segmentation of an image. One can simply
    visualize the self-attention map of selected heads of the multi-head attention
    layer as shown in the video bellow:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 底层视觉变换器架构 (ViT) 的一个有趣发现是，当使用无监督学习技术进行训练时，其特征包含有关图像语义分割的显著信息。可以简单地可视化多头注意力层中选择的头部的自注意力图，如下方视频所示：
- en: '![](../Images/98c910b938eac30dc3417917e4e55d08.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98c910b938eac30dc3417917e4e55d08.png)'
- en: 'Fig. 1: Self-attention maps for selected heads. [Source](https://arxiv.org/abs/2104.14294)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：选择的头部的自注意力图。[来源](https://arxiv.org/abs/2104.14294)
- en: Let us unpack another layer of abstraction and let’s try to understand how DINO
    implements its framework, tackles instabilities and how it performs compared to
    previous methods!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨一下 DINO 实现其框架的方式，如何应对不稳定性，以及与以前的方法相比它的表现如何！
- en: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
- en: '[Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)'
- en: Paper Walkthroughs by Sascha Kirch
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 由 Sascha Kirch 进行的论文解读
- en: '[View list](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2?source=post_page-----4cb08e821b18--------------------------------)7
    stories![“DDPM — Denoising Diffusion Probabilistic Models “ paper illustration
    by Sascha Kirch](../Images/6e785c0a911386676abebe0fa646f483.png)![“Depth Anything”
    paper illustration by Sascha Kirch](../Images/bd8cd71a02e42cf64d0afd39f41f48e0.png)![](../Images/8708d91a4a1902cef889ced95d46fc39.png)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2?source=post_page-----4cb08e821b18--------------------------------)7
    个故事！[“DDPM — 去噪扩散概率模型”论文插图，作者：Sascha Kirch](../Images/6e785c0a911386676abebe0fa646f483.png)![“Depth
    Anything”论文插图，作者：Sascha Kirch](../Images/bd8cd71a02e42cf64d0afd39f41f48e0.png)![](../Images/8708d91a4a1902cef889ced95d46fc39.png)'
- en: Method
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: The DINO framework shares the same overall structure with other similarity-learning
    frameworks like [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)
    or the [mean teacher](https://arxiv.org/abs/1703.01780) but also with knowledge
    distillation. Let’s first have a look on how DINO does it and the differentiate
    between the other frameworks.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: DINO 框架与其他相似性学习框架（如 [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)
    或 [mean teacher](https://arxiv.org/abs/1703.01780)）以及知识蒸馏具有相同的整体结构。让我们首先看看 DINO
    是如何做到这一点的，并与其他框架进行区分。
- en: '![](../Images/2dacf33beadadf8b5604e4842686f18f.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2dacf33beadadf8b5604e4842686f18f.png)'
- en: 'Fig. 2: DINO architecture. [Source](https://arxiv.org/abs/2104.14294) + annotations
    by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '**Networks and Update Rule**'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start from the middle. DINO implements two networks with the exact same
    architecture but a different set of weights. Those are the student and the teacher.
    The student is trained with back propagation and the teacher updates its weights
    with an exponential moving average of its own weights and those of the student.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2fe06104b0d6ee43b9013c3b398e4e8.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: 'Equation 1: Update rule of the teacher’s weights. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Backbones are either a ResNet50 or [DeiT](https://arxiv.org/abs/2012.12877)
    (which is a [ViT](https://arxiv.org/abs/2010.11929) adapted for knowledge distillation).
    An MLP-based projection head is connected to the backbone to reduce the dimensionality
    of the features, but is removed for inference.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '***Nice, but which model is used for inference: student or teacher?*** — Well
    that’s a good question and funny enough not a single word is mentioned in the
    paper. Intuitively you might think the student, at least I did at first. But as
    we will see later, the teacher outperforms the student throughout the training.
    The only hint beside the better performance is that in the code implementation
    the teacher checkpoint is the default one for the evaluation of for example [video
    segmentation](https://github.com/facebookresearch/dino/blob/7c446df5b9f45747937fb0d72314eb9f7b66930a/eval_video_segmentation.py#L257C9-L257C9),
    [linear probing](https://github.com/facebookresearch/dino/blob/7c446df5b9f45747937fb0d72314eb9f7b66930a/eval_linear.py#L264C34-L264C34)
    and [k-NN](https://github.com/facebookresearch/dino/blob/7c446df5b9f45747937fb0d72314eb9f7b66930a/eval_knn.py#L203C32-L203C32).
    Since this parameter can be changed though, I cannot tell you with certainty.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '**Inputs and Outputs**'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From an input image *x* different views *x1* and *x2* are created by cropping
    and applying image augmentations like in [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)
    (e.g. color jitter, Gaussian blur and solarization). The technique used for cropping
    is called [multi-crop](https://arxiv.org/abs/2006.09882) where multiple crops
    of different sizes are generated to save memory while providing more data. Small
    crops are called local views and consist of 96x96 pixels that are exclusively
    feed into the student. Larger crops are called global views and consists of 224x224
    pixels that are exclusively fed into the teacher. As we will see later in the
    ablation section, 2 global views and 10 local views have been used during training.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE: The paper is a bit confusing regarding the multi-crop technique because
    neither the provided pseudo-code nor the architecture shown in Fig. 3 above reflect
    it. The pseudo code even suggests that x1 and x2 are feed into both, the student
    and the teacher like in BYOL, which is not the case when using multi-crop.'
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：论文对于多裁剪技术有点混乱，因为提供的伪代码和上面的图 3 所示的架构都没有反映出来。伪代码甚至建议 x1 和 x2 像在 BYOL 中一样输入到学生和教师中，这在使用多裁剪时并非如此。
- en: In contrast to similarity learning where the objective is to maximize the similarity
    of embeddings, DINO minimizes the cross-entropy between the teacher’s and the
    student’s output distribution. As indicated by the equation bellow, the cross-entropy
    is calculated for each pair of global and local views and is then summed up.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与相似性学习的目标是最大化嵌入的相似性不同，DINO 最小化教师和学生输出分布之间的交叉熵。如下面的方程所示，交叉熵是对每对全局和局部视图计算的，然后汇总。
- en: '![](../Images/b999355c87ae211c90653c509cea3da3.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b999355c87ae211c90653c509cea3da3.png)'
- en: 'Equation 2: Optimization objective. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2：优化目标。 [来源](https://arxiv.org/abs/2104.14294) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    的注解
- en: '***And what do the models output?*** — Like in similarity learning, the student
    and the teacher output an embedding for a given image, rather than a prediction
    score. Like in knowledge distillation, the output is transformed via a SoftMax
    transformation into a probability distribution. The SoftMax has a temperature
    parameter that controls the smoothing or sharpening of the resulting distribution.
    This temperature plays a crucial role in knowledge distillation because it allows
    to control the balance between transferring general knowledge and fine-grained
    details from a teacher network to a student network, making the distillation process
    more effective for different tasks.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '***模型的输出是什么？*** — 就像相似性学习中，学生和教师为给定的图像输出一个嵌入，而不是预测分数。就像在知识蒸馏中，输出通过 SoftMax
    转换为概率分布。SoftMax 有一个温度参数，它控制结果分布的平滑或锐化。这个温度在知识蒸馏中起着关键作用，因为它可以控制从教师网络到学生网络转移一般知识和细粒度细节之间的平衡，使蒸馏过程对不同任务更有效。'
- en: '![](../Images/4d9f933db5fb3f0e5dfba164632ebfcd.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d9f933db5fb3f0e5dfba164632ebfcd.png)'
- en: 'Fig. 3: Effect of temperature value on the SoftMax output. Illustration by
    [Sascha Kirch](https://medium.com/@SaschaKirch) created with [this python notebook](https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：温度值对 SoftMax 输出的影响。 [Sascha Kirch](https://medium.com/@SaschaKirch) 的插图，使用
    [这个 Python 笔记本](https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb)
    创建
- en: 'I created a notebook for you so you can investigate the impact of the temperature
    on the resulting distribution:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我为你创建了一个笔记本，以便你可以调查温度对结果分布的影响：
- en: '[](https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb?source=post_page-----4cb08e821b18--------------------------------)
    [## ML_Notebooks/Softmax_Temperature.ipynb at main · sascha-kirch/ML_Notebooks'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb?source=post_page-----4cb08e821b18--------------------------------)
    [## ML_Notebooks/Softmax_Temperature.ipynb 在 main 分支 · sascha-kirch/ML_Notebooks'
- en: Collection of machine learning related notebooks to share. — ML_Notebooks/Softmax_Temperature.ipynb
    at main ·…
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习相关笔记的集合用于共享。— ML_Notebooks/Softmax_Temperature.ipynb 在 main 分支 ·…
- en: github.com](https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb?source=post_page-----4cb08e821b18--------------------------------)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb?source=post_page-----4cb08e821b18--------------------------------)
- en: '**Avoiding Collapse**'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**避免崩溃**'
- en: 'As mentioned earlier, student and teacher have the exact same architecture.
    This kind of setup is unstable (if no counter measures are implemented) and might
    result in collapsing solutions, where all features are mapped to a certain region
    in the latent space, e.g. a single point in the worst case. BYOL addressed this
    issue with an extra prediction head for only one of the models introducing an
    asymmetry. Since DINO has symmetric models another trick is required: centering
    and sharpening. Both are applied to the teacher network only. Centering is a technique
    that prevents a single dimension in the latent space to dominate, by adding a
    bias term *c* to the teachers output *g(x) = g(x)+c*, where'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/268f8c0249d46fa219f3dcad096e9332.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: 'Equation 3: Update rule of the centering term. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: While centering has a positive effect, it also encourages the output to collapse
    into a uniform distribution. Sharpening has the opposite effect hence applying
    both balances their effect and stabilizes training. Sharpening is achieved by
    using a smaller temperature in the SoftMax (see Fig. 3) for the teacher as for
    the student.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: To avoid collapsing the hyperparameter *m* from equation 3 and the temperature
    of the teacher are crucial. In their ablation study in the appendix section the
    authors show that *m=0.9…0.999* works best and the temperature value is linearly
    increased from *0.04* to *0.07* during warm-up.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '**What does DINO do? Knowledge Distillation or Similarity Learning?**'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The answer is a little bit of both!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: While knowledge distillation usually distils knowledge from an already trained,
    larger and more accurate teacher model into a smaller student model, it could
    also be seen as some sort of similarity learning because it encourages the student
    network to produce predictions that are similar to those of the teacher. In similarity
    learning, the two models are usually trained jointly and often align their latent
    space predictions rather than probability distributions.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the authors of DINO phrase their objective as knowledge distillation,
    let’s have a look on some differences compared with “standard” knowledge distillation:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: DINO’s teacher is not available a priori but “trained” alongside the student.
    It can even be considered as a co-distillation since knowledge is also distilled
    from the student into the teacher.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DINO’s teacher and student are not acting on the same input but on different
    views of the image cropped to different sizes.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DINO uses different temperatures in the SoftMax of both models to perform sharpening.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DINO calculates the cross-entropy over the temperature-scaled SoftMax of the
    embeddings rather than prediction scores.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'And how is it similar to knowledge distillation?:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: DINO consists of a student and a teacher network, where the teacher performs
    better than the student as we will see in the experiments.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rather than maximizing a similarity metric, DINO minimizes the cross-entropy
    loss of a temperature scaled SoftMax output.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----4cb08e821b18--------------------------------)
    [## Get an email whenever Sascha Kirch publishes 🚀'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Get an email whenever Sascha Kirch publishes 🚀 Looking to learn more about deep
    learning or simply stay up to date…
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----4cb08e821b18--------------------------------)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Experiments
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The paper presents a vast number of experiments. They pre-train the model on
    ImageNet, a commonly used dataset in representation learning.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: For the evaluation, common techniques usually either train a linear classifier
    on top of frozen features or fine-tune the model to new downstream tasks, where
    the parameters of the model are adapted.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: The authors of DINO claim that those techniques are very sensitive to hyperparameters
    which makes comparisons unfair and hard to reproduce. Hence, they propose to use
    a simple nearest neighbor clustering algorithm on the features of the pre-trained
    model.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Linear and k-NN Classification on ImageNet
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a this experiment the models are tested on their image classification accuracy
    on ImageNet. A variety of self-supervised pre-trained models are tested with either
    a ResNet or a ViT backbone. The classification is done either with linear probing
    or k-NN clustering.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8411c4b87ac4d9fab26ed43dfe033c82.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: 'Table 1: Linear and k-NN classification on ImageNet. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'I guess the key take-aways are:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: K-NN performs better on ViT features than on ResNet features.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decreasing patch size in the ViT has larger improvement as larger backbone,
    but at the cost of slower inference.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Video Instance Segmentation
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An important experiment has been the video segmentation task, since the paper
    is about the ViT’s capability to capture semantic segmentation in its features
    when trained with unsupervised methods. Or let’s say that’s what is claimed 😁
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9480e3a017e4d94de1b778797fc40226.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: 'Table 2: Video Instance segmentation. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'Observing those results I am missing two further experiments:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: It would be nice to see a comparison of a supervised ResNet50 and a self-supervised
    ResNet50 in the DINO framework to support their claim that the ViT is superior
    to the ResNet architecture.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It would also be great to see the same set of ViT backbones for supervised as
    for self-supervised to see the impact on patch-size and model size.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'But as I always say: asking questions is easy 😁 In real-world projects the
    authors often face resource constraints and project deadlines so not every single
    little detail can be covered!'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Probing the Self-Attention Map
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this experiment the authors investigated the self-attention maps of different
    heads in the multi-head self-attention layers of the ViT. They visualize the attention
    maps from selected heads from the last layer of ViT-S/8, those of the learned
    [CLS] token to be precise.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，作者调查了ViT的多头自注意力层中不同头部的自注意力图。他们可视化了ViT-S/8最后一层中选定头部的注意力图，精确来说是学习到的[CLS]令牌。
- en: '![](../Images/133db7e45053a51d9d4e14bf554844d1.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/133db7e45053a51d9d4e14bf554844d1.png)'
- en: 'Fig. 4: Attention maps from selected heads. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：来自选定头部的注意力图。 [来源](https://arxiv.org/abs/2104.14294) + [Sascha Kirch](https://medium.com/@SaschaKirch)的注释
- en: Other Experiments
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他实验
- en: In other experiments, DINO improved compared against the supervised baseline.
    Those tasks include image retrieval and copy detection.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他实验中，DINO在与监督基线的比较中有所改进。这些任务包括图像检索和复制检测。
- en: Ablations
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消融实验
- en: For their ablation study the authors experiment with the ViT-S model.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的消融研究中，作者对ViT-S模型进行了实验。
- en: Importance of Patch Size
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 补丁大小的重要性
- en: Recall that a vision transformer inputs a patchified version of an input image,
    transforms each patch into a token and then applies a transformer with its self-attention
    mechanism. This was a trick by the authors of ViT to reduce the compute requirements
    for trading-off performance, making transformers applicable to image data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，视觉变换器输入的是一个补丁化的输入图像，将每个补丁转化为令牌，然后应用具有自注意力机制的变换器。这是ViT作者的一项技巧，用于减少性能权衡的计算需求，使变换器适用于图像数据。
- en: DINO claims that smaller size of the patches increases the performance while
    decreasing the throughput (number of images that can be processed per second),
    which is exactly what ViT claims.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: DINO声称，较小的补丁大小提高了性能，同时降低了吞吐量（每秒可以处理的图像数量），这正是ViT所声称的。
- en: '![](../Images/2103ddc8e040677295c3880a1ff19051.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2103ddc8e040677295c3880a1ff19051.png)'
- en: 'Fig. 5: Impact of patch size on accuracy and throughput. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：补丁大小对准确性和吞吐量的影响。 [来源](https://arxiv.org/abs/2104.14294) + [Sascha Kirch](https://medium.com/@SaschaKirch)的注释
- en: Intuitively I’d say it is no surprise since you increase the input resolution
    and you end up with more tokens to attend to, so you end up with a fine-grained
    attention map.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，这并不令人惊讶，因为你增加了输入分辨率，结果是需要处理更多的令牌，因此你得到一个更细粒度的注意力图。
- en: Different Teacher Update Rules
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同的教师更新规则
- en: The teacher in DINO is updated by calculating the exponential moving average
    from the updated student and the current teacher. This is the “momentum encoder”
    approach they refer to.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: DINO中的教师通过计算从更新后的学生和当前教师的指数移动平均来更新。这就是他们所称的“动量编码器”方法。
- en: 'Using a momentum encoder and plotting the accuracy of the teacher and the student
    during training, the teacher performs better throughout the entire process. From
    this we can hypothesize:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用动量编码器并绘制教师和学生在训练过程中的准确性，教师在整个过程中表现更好。由此我们可以假设：
- en: the teacher can provide a strong learning signal to the student.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 教师可以为学生提供强有力的学习信号。
- en: an improving student improves the teacher due to the EMA update rule (co-distillation).
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改进的学生由于EMA更新规则（共同蒸馏）使教师得到提升。
- en: One can use the Teacher as final model which has better performance but the
    same architecture as the student, hence no change in compute requirements.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用教师作为最终模型，该模型具有更好的性能，但与学生具有相同的架构，因此计算需求没有变化。
- en: '![](../Images/36bd26e2335351a2980f6f518e29211d.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36bd26e2335351a2980f6f518e29211d.png)'
- en: 'Fig. 6: Teacher performance. [Source](https://arxiv.org/abs/2104.14294) + annotations
    by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：教师性能。 [来源](https://arxiv.org/abs/2104.14294) + [Sascha Kirch](https://medium.com/@SaschaKirch)的注释
- en: 'They also experiment with 3 other update rules: copying the weights from the
    student to the teacher, use the student weights from the previous iteration of
    the optimizer and use the student weights from the previous epoch.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 他们还实验了另外3种更新规则：将权重从学生复制到教师，使用优化器前一个迭代的学生权重，和使用前一个时代的学生权重。
- en: Multi-Crop vs. Time and GPU Memory
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多裁剪与时间和GPU内存
- en: As mentioned earlier, DINO inputs multiple cropped views of the same image and
    feeds the global views into the teacher and the local views into the student.
    In this ablation, the authors experiment with different amounts of local views
    and report the impact on performance, training time and peak memory per GPU.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，DINO输入相同图像的多个裁剪视图，并将全局视图输入到教师模型中，将局部视图输入到学生模型中。在这项消融实验中，作者试验了不同数量的局部视图，并报告了对性能、训练时间和每GPU峰值内存的影响。
- en: '![](../Images/1150cc8dc59aeb8b950bce9b7b52a6be.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1150cc8dc59aeb8b950bce9b7b52a6be.png)'
- en: 'Table 3: Multi-Crop vs. Time and GPU Memory. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：多裁剪与时间和GPU内存。[来源](https://arxiv.org/abs/2104.14294) + [Sascha Kirch](https://medium.com/@SaschaKirch)的注释
- en: Avoiding Collapse
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 避免崩溃
- en: 'In this ablation the authors evaluated the role of their stabilizing measures
    to avoid collapsing solutions: centering and sharpening.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项消融实验中，作者评估了其稳定措施在避免崩溃解决方案中的作用：中心化和锐化。
- en: To do so, they decomposed the cross-entropy into an entropy term and a Kullback-Leibler
    (KL) divergence term. KL divergence is a measure of difference of two probability
    distributions. If KL is 0, two distributions are considered equal.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，他们将交叉熵分解为熵项和Kullback-Leibler（KL）散度项。KL散度是两个概率分布差异的度量。如果KL为0，则认为两个分布相等。
- en: 'The intuition behind this is the following: if the KL divergence of the output
    distribution of the teacher and the student is constant throughout the training,
    there is no learning signal for updating the weights of the student.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 其直观的理解是：如果教师和学生的输出分布的KL散度在整个训练过程中保持不变，那么学生的权重更新就没有学习信号。
- en: '![](../Images/65a0d4da56fdaf6c580d5c58625de2aa.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65a0d4da56fdaf6c580d5c58625de2aa.png)'
- en: 'Fig. 7: Analysis of collapsing solutions. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：崩溃解决方案分析。[来源](https://arxiv.org/abs/2104.14294) + [Sascha Kirch](https://medium.com/@SaschaKirch)的注释
- en: Effect of Batch Size
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量大小的影响
- en: An interesting property is that DINO can be trained with small batch sizes without
    a large drop in performance. This was actually one of BYOL’s motivation, a paper
    DINO builds upon, to be less dependent on batch size compared to contrastive self-supervised
    learning.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的特性是，DINO可以用较小的批量大小进行训练，而不会大幅下降性能。这实际上是BYOL的一个动机，DINO基于此论文，减少了对批量大小的依赖，相比对比自监督学习方法。
- en: '![](../Images/9e0fa8d51ff10f8d989b51eb03eb122c.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e0fa8d51ff10f8d989b51eb03eb122c.png)'
- en: 'Table 4: Batch size vs. accuracy. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：批量大小与准确率。[来源](https://arxiv.org/abs/2104.14294) + [Sascha Kirch](https://medium.com/@SaschaKirch)的注释
- en: Contrastive methods like CLIP and GLIP provide a lot of negative samples for
    a given positive sample to avoid collapsing solutions. The more negative samples
    per optimizer update step (hence per batch) the better it works.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 类似CLIP和GLIP的对比方法提供了大量的负样本以避免崩溃解决方案。每次优化器更新步骤（因此每批次）的负样本越多，效果越好。
- en: Conclusion
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, DINO is a knowledge-distillation framework. It is a visual foundation
    model that exploits interesting properties of ViTs and is the predecessor of one
    of today’s best-performing foundation models, DINOv2\. DINO‘s framework consists
    of a student and teacher model that acts upon different views of the same image
    and adds extra measures to deal with inherent instabilities of similarity-learning
    approaches. The experiments show that DINO outperforms other self-supervised pre-trained
    models on various tasks.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，DINO是一个知识蒸馏框架。它是一个视觉基础模型，利用了ViTs的有趣特性，并且是今天表现最好的基础模型之一DINOv2的前身。DINO的框架由学生模型和教师模型组成，作用于相同图像的不同视图，并采取额外措施来处理相似性学习方法的固有不稳定性。实验表明，DINO在各种任务上优于其他自监督预训练模型。
- en: Further Readings & Resources
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读与资源
- en: Papers
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 论文
- en: 'In the meantime an improved version of DINO has been released:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，DINO的改进版本已经发布：
- en: '[DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[DINOv2: 在没有监督的情况下学习鲁棒的视觉特征](https://arxiv.org/abs/2304.07193)'
- en: '[Blog post DINOv2 by Meta](https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/)'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Meta的DINOv2博客文章](https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/)'
- en: Paper Walkthroughs
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 论文解读
- en: 'You might also like my other paper walkthroughs covering concepts we discussed
    in this article:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还会喜欢我其他的论文解读，涵盖了我们在本文中讨论的概念：
- en: '[](/the-clip-foundation-model-7770858b487d?source=post_page-----4cb08e821b18--------------------------------)
    [## The CLIP Foundation Model'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Paper Summary— Learning Transferable Visual Models From Natural Language Supervision
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/the-clip-foundation-model-7770858b487d?source=post_page-----4cb08e821b18--------------------------------)
    [](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----4cb08e821b18--------------------------------)
    [## GLIP: Introducing Language-Image Pre-Training to Object Detection'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper Summary: Grounded Language-Image Pre-training'
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----4cb08e821b18--------------------------------)
    [](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----4cb08e821b18--------------------------------)
    [## BYOL -The Alternative to Contrastive Self-Supervised Learning
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper Analysis— Bootstrap Your Own Latent: A New Approach to Self-Supervised
    Learning'
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----4cb08e821b18--------------------------------)
    [](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?source=post_page-----4cb08e821b18--------------------------------)
    [## Segment Anything — Promptable Segmentation of Arbitrary Objects
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Paper Walkthrough — Segment Anything
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?source=post_page-----4cb08e821b18--------------------------------)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
