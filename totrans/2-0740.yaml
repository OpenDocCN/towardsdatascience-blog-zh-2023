- en: DINO â€” A Foundation Model for Computer Vision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DINO â€” è®¡ç®—æœºè§†è§‰çš„åŸºç¡€æ¨¡å‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/dino-a-foundation-model-for-computer-vision-4cb08e821b18](https://towardsdatascience.com/dino-a-foundation-model-for-computer-vision-4cb08e821b18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/dino-a-foundation-model-for-computer-vision-4cb08e821b18](https://towardsdatascience.com/dino-a-foundation-model-for-computer-vision-4cb08e821b18)
- en: '[ğŸš€Saschaâ€™s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[ğŸš€Sascha çš„è®ºæ–‡ä¿±ä¹éƒ¨](https://towardsdatascience.com/tagged/saschas-paper-club)'
- en: Emerging Properties in Self-Supervised Vision Transformers by M. Caron et. al.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è‡ªç›‘ç£è§†è§‰å˜æ¢å™¨ä¸­çš„æ–°å…´ç‰¹æ€§ï¼Œä½œè€… M. Caron ç­‰ã€‚
- en: '[](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4cb08e821b18--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4cb08e821b18--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4cb08e821b18--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4cb08e821b18--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4cb08e821b18--------------------------------)
    Â·13 min readÂ·Sep 27, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4cb08e821b18--------------------------------)
    Â·13 åˆ†é’Ÿé˜…è¯»Â·2023å¹´9æœˆ27æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: It is an exciting decade for computer vision. Great successes from the natural
    language domain are transferred to the vision domain including the introduction
    of the ViT (vision transformer) and lately large-scale self-supervised pre-training
    techniques have made headlines under the name of foundation models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—æœºè§†è§‰æ­£è¿æ¥ä¸€ä¸ªä»¤äººå…´å¥‹çš„åå¹´ã€‚æ¥è‡ªè‡ªç„¶è¯­è¨€é¢†åŸŸçš„å·¨å¤§æˆåŠŸè¢«è½¬ç§»åˆ°è§†è§‰é¢†åŸŸï¼ŒåŒ…æ‹¬å¼•å…¥ ViTï¼ˆè§†è§‰å˜æ¢å™¨ï¼‰ï¼Œæœ€è¿‘å¤§è§„æ¨¡çš„è‡ªç›‘ç£é¢„è®­ç»ƒæŠ€æœ¯åœ¨åŸºç¡€æ¨¡å‹çš„åä¹‰ä¸‹æˆä¸ºå¤´æ¡æ–°é—»ã€‚
- en: 'Today we are looking into a framework called DINO (self **DI**stillation, **N**O
    labels), a visual foundation model built on interesting properties of ViTs. It
    is also the predecessor of one of todayâ€™s best performing foundation models: [DINOv2](https://arxiv.org/abs/2304.07193).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Šå¤©æˆ‘ä»¬å°†æ¢è®¨ä¸€ä¸ªåä¸º DINOï¼ˆè‡ª**DI**è’¸é¦ï¼Œ**N**O æ ‡ç­¾ï¼‰çš„æ¡†æ¶ï¼Œå®ƒæ˜¯å»ºç«‹åœ¨ ViTs æœ‰è¶£ç‰¹æ€§åŸºç¡€ä¸Šçš„è§†è§‰åŸºç¡€æ¨¡å‹ã€‚å®ƒä¹Ÿæ˜¯ä»Šå¤©è¡¨ç°æœ€ä½³çš„åŸºç¡€æ¨¡å‹ä¹‹ä¸€çš„å‰èº«ï¼š[DINOv2](https://arxiv.org/abs/2304.07193)ã€‚
- en: '![](../Images/826ce8cc3feae5497593efe2c4e9631c.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/826ce8cc3feae5497593efe2c4e9631c.png)'
- en: Image created from [publication](https://arxiv.org/abs/2104.14294) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºäº [å‡ºç‰ˆç‰©](https://arxiv.org/abs/2104.14294)ï¼Œä½œè€… [Sascha Kirch](https://medium.com/@SaschaKirch)
- en: '**Paper:** [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294),
    by [Mathilde Caron](https://arxiv.org/search/cs?searchtype=author&query=Caron%2C+M)
    et.al., 29\. Apr. 2021'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**è®ºæ–‡:** [è‡ªç›‘ç£è§†è§‰å˜æ¢å™¨ä¸­çš„æ–°å…´ç‰¹æ€§](https://arxiv.org/abs/2104.14294)ï¼Œä½œè€… [Mathilde Caron](https://arxiv.org/search/cs?searchtype=author&query=Caron%2C+M)
    ç­‰ï¼Œ2021å¹´4æœˆ29æ—¥'
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/facebookresearch/dino) â€” [Blog Post](https://ai.meta.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/)'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**èµ„æº:** [GitHub](https://github.com/facebookresearch/dino) â€” [åšå®¢æ–‡ç« ](https://ai.meta.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/)'
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** foundation model, computer vision, vision transformer, knowledge
    distillation, similarity learning, self-supervised learning'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ç±»åˆ«:** åŸºç¡€æ¨¡å‹ï¼Œè®¡ç®—æœºè§†è§‰ï¼Œè§†è§‰å˜æ¢å™¨ï¼ŒçŸ¥è¯†è’¸é¦ï¼Œç›¸ä¼¼æ€§å­¦ä¹ ï¼Œè‡ªç›‘ç£å­¦ä¹ '
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[**å…¶ä»–è¯¦ç»†è®²è§£**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
- en: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    â€” [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    â€” [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    â€” [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    â€” [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    â€” [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    â€” [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    â€” [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    â€” [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
- en: Outline
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤§çº²
- en: Context & Background
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: èƒŒæ™¯ä¸èƒŒæ™¯
- en: Method
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ–¹æ³•
- en: Experiments
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®éªŒ
- en: Ablations
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¶ˆèæµ‹è¯•
- en: Conclusion
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Further Readings & Resources
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»ä¸èµ„æº
- en: Context & Background
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: èƒŒæ™¯ä¸èƒŒæ™¯
- en: The year is 2021, April to be precise. It has been four years since the release
    of the transformer model with [attention is all you need](https://arxiv.org/abs/1706.03762).
    Self-supervised pre-training is long being practiced in NLP by models such as
    [BERT](https://arxiv.org/pdf/1810.04805.pdf) and the term foundation model is
    not yet known for the next few months until the release of *â€œ*[*on the opportunities
    and Risks of Foundation Models*](https://arxiv.org/abs/2108.07258)*â€*. Six months
    earlier the [Vision transformer (ViT)](https://arxiv.org/abs/2010.11929) was first
    published on arxiv and it is still one month until ICLR 2021 where it will be
    presented.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¶é—´æ˜¯ 2021 å¹´ï¼Œå‡†ç¡®åœ°è¯´æ˜¯ 4 æœˆã€‚è‡ªä»å‘å¸ƒäº†å¸¦æœ‰ [Attention is All You Need](https://arxiv.org/abs/1706.03762)
    çš„ Transformer æ¨¡å‹å·²ç»è¿‡å»äº†å››å¹´ã€‚è‡ªç›‘ç£é¢„è®­ç»ƒåœ¨ NLP ä¸­å·²ç»ç”± [BERT](https://arxiv.org/pdf/1810.04805.pdf)
    ç­‰æ¨¡å‹é•¿æœŸå®è·µï¼Œè€Œâ€œåŸºç¡€æ¨¡å‹â€è¿™ä¸€æœ¯è¯­åœ¨æ¥ä¸‹æ¥çš„å‡ ä¸ªæœˆä¸­å°šæœªè¢«çŸ¥æ™“ï¼Œç›´åˆ° *â€œ*[*å…³äºåŸºç¡€æ¨¡å‹çš„æœºé‡ä¸é£é™©*](https://arxiv.org/abs/2108.07258)*â€*
    çš„å‘å¸ƒã€‚å…­ä¸ªæœˆå‰ï¼Œ[Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929) é¦–æ¬¡å‘å¸ƒåœ¨ arxiv
    ä¸Šï¼Œè·ç¦» ICLR 2021 è¿˜æœ‰ä¸€ä¸ªæœˆï¼Œå®ƒå°†åœ¨é‚£é‡Œè¿›è¡Œå±•ç¤ºã€‚
- en: 'Let that sink in for a moment: ViT had its debut on arxiv.org in October 2020
    and was presented on ICLR2021 in May 2021\. DINO was released on arxiv in April
    2021\. So, one month before it was actually presented on a conference. This would
    mean they only had 5 months if they had started right away to come up with the
    projectâ€™s idea, compile a team, lay out the theoretical foundation, train the
    model, perform experiments and ablations, and write the paper. No wonder PhD students
    these days feel constantly anxious. At least thatâ€™s whatâ€™s happening to me sometimes
    *ğŸ˜…*'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç¨å¾®æ¶ˆåŒ–ä¸€ä¸‹è¿™ä¸ªä¿¡æ¯ï¼šViT äº 2020 å¹´ 10 æœˆåœ¨ arxiv.org ä¸Šé¦–æ¬¡å‘å¸ƒï¼Œå¹¶åœ¨ 2021 å¹´ 5 æœˆçš„ ICLR2021 ä¸Šè¿›è¡Œäº†å±•ç¤ºã€‚DINO
    äº 2021 å¹´ 4 æœˆåœ¨ arxiv ä¸Šå‘å¸ƒã€‚æ‰€ä»¥ï¼Œå®é™…åœ¨ä¼šè®®ä¸Šå±•ç¤ºå‰çš„ä¸€ä¸ªæœˆã€‚è¿™æ„å‘³ç€ä»–ä»¬åªæœ‰ 5 ä¸ªæœˆçš„æ—¶é—´ï¼Œå¦‚æœä»–ä»¬ç«‹å³å¼€å§‹çš„è¯ï¼Œæ¥æ„æ€é¡¹ç›®çš„æƒ³æ³•ã€ç»„å»ºå›¢é˜Ÿã€å¥ å®šç†è®ºåŸºç¡€ã€è®­ç»ƒæ¨¡å‹ã€è¿›è¡Œå®éªŒå’Œæ¶ˆèæµ‹è¯•ï¼Œå¹¶æ’°å†™è®ºæ–‡ã€‚éš¾æ€ªç°åœ¨çš„åšå£«ç”Ÿæ„Ÿåˆ°ä¸æ–­çš„ç„¦è™‘ã€‚è‡³å°‘è¿™å°±æ˜¯æˆ‘æœ‰æ—¶çš„æ„Ÿå—
    *ğŸ˜…*ã€‚
- en: While ViTs were very competitive with convolutional networks, they are demanding
    in terms of computational resources and amount of training data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ ViT ä¸å·ç§¯ç½‘ç»œéå¸¸å…·æœ‰ç«äº‰åŠ›ï¼Œä½†å®ƒä»¬åœ¨è®¡ç®—èµ„æºå’Œè®­ç»ƒæ•°æ®é‡æ–¹é¢çš„è¦æ±‚å¾ˆé«˜ã€‚
- en: 'The authors of DINO made a simple observation: the success of transformers
    in NLP was coupled with self-supervised pre-training and current self-supervised
    methods in the vision domain are built from convnets, like e.g. [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: DINO çš„ä½œè€…åšå‡ºäº†ä¸€ä¸ªç®€å•çš„è§‚å¯Ÿï¼šå˜æ¢å™¨åœ¨ NLP ä¸­çš„æˆåŠŸä¸è‡ªç›‘ç£é¢„è®­ç»ƒç›¸å…³ï¼Œè€Œç›®å‰è§†è§‰é¢†åŸŸçš„è‡ªç›‘ç£æ–¹æ³•æ˜¯åŸºäºå·ç§¯ç½‘ç»œçš„ï¼Œæ¯”å¦‚ [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)ã€‚
- en: '[](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----4cb08e821b18--------------------------------)
    [## BYOL -The Alternative to Contrastive Self-Supervised Learning'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----4cb08e821b18--------------------------------)
    [## BYOL -å¯¹æ¯”è‡ªç›‘ç£å­¦ä¹ çš„æ›¿ä»£æ–¹æ¡ˆ'
- en: 'Paper Analysisâ€” Bootstrap Your Own Latent: A New Approach to Self-Supervised
    Learning'
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'è®ºæ–‡åˆ†æâ€”â€”ã€ŠBootstrap Your Own Latent: A New Approach to Self-Supervised Learningã€‹'
- en: towardsdatascience.com](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----4cb08e821b18--------------------------------)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----4cb08e821b18--------------------------------)'
- en: 'Inspired by [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)
    and the [mean teacher](https://arxiv.org/abs/1703.01780), the authors came up
    with a framework to train a ViT in a self-supervised fashion and found:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å—åˆ° [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)
    å’Œ [mean teacher](https://arxiv.org/abs/1703.01780) çš„å¯å‘ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªæ¡†æ¶æ¥ä»¥è‡ªç›‘ç£çš„æ–¹å¼è®­ç»ƒ ViTï¼Œå¹¶å‘ç°ï¼š
- en: Self-supervised ViT features explicitly contain the scene layout and, in particular,
    object boundaries.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è‡ªç›‘ç£ ViT ç‰¹å¾æ˜ç¡®åŒ…å«åœºæ™¯å¸ƒå±€ï¼Œç‰¹åˆ«æ˜¯å¯¹è±¡è¾¹ç•Œã€‚
- en: Self-supervised ViT features perform particularly well with a basic nearest
    neighbors classifier (k-NN) without any fine-tuning, linear classifier nor data
    augmentation.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è‡ªç›‘ç£ ViT ç‰¹å¾åœ¨æ²¡æœ‰ä»»ä½•å¾®è°ƒã€çº¿æ€§åˆ†ç±»å™¨æˆ–æ•°æ®å¢å¼ºçš„æƒ…å†µä¸‹ï¼Œä¸åŸºç¡€çš„æœ€è¿‘é‚»åˆ†ç±»å™¨ (k-NN) ä¸€èµ·è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚
- en: In contrast to BYOL and mean teacher, DINO implements a knowledge-distillation
    framework consisting of a student and teacher model that acts upon different views
    of the same image and adds extra measures to deal with inherent instabilities
    of similarity-learning approaches, where solutions often collapse.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ BYOL å’Œ mean teacher ç›¸æ¯”ï¼ŒDINO å®ç°äº†ä¸€ä¸ªçŸ¥è¯†è’¸é¦æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹å’Œä¸€ä¸ªæ•™å¸ˆæ¨¡å‹ï¼Œä½œç”¨äºåŒä¸€å›¾åƒçš„ä¸åŒè§†è§’ï¼Œå¹¶é‡‡å–é¢å¤–æªæ–½åº”å¯¹ç›¸ä¼¼æ€§å­¦ä¹ æ–¹æ³•çš„å›ºæœ‰ä¸ç¨³å®šæ€§ï¼Œå…¶ä¸­è§£å†³æ–¹æ¡ˆé€šå¸¸ä¼šå´©æºƒã€‚
- en: 'An interesting finding of the underlying vision transformer architecture (ViT)
    is that when trained with unsupervised learning techniques its features contain
    explicit information about the semantic segmentation of an image. One can simply
    visualize the self-attention map of selected heads of the multi-head attention
    layer as shown in the video bellow:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åº•å±‚è§†è§‰å˜æ¢å™¨æ¶æ„ (ViT) çš„ä¸€ä¸ªæœ‰è¶£å‘ç°æ˜¯ï¼Œå½“ä½¿ç”¨æ— ç›‘ç£å­¦ä¹ æŠ€æœ¯è¿›è¡Œè®­ç»ƒæ—¶ï¼Œå…¶ç‰¹å¾åŒ…å«æœ‰å…³å›¾åƒè¯­ä¹‰åˆ†å‰²çš„æ˜¾è‘—ä¿¡æ¯ã€‚å¯ä»¥ç®€å•åœ°å¯è§†åŒ–å¤šå¤´æ³¨æ„åŠ›å±‚ä¸­é€‰æ‹©çš„å¤´éƒ¨çš„è‡ªæ³¨æ„åŠ›å›¾ï¼Œå¦‚ä¸‹æ–¹è§†é¢‘æ‰€ç¤ºï¼š
- en: '![](../Images/98c910b938eac30dc3417917e4e55d08.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98c910b938eac30dc3417917e4e55d08.png)'
- en: 'Fig. 1: Self-attention maps for selected heads. [Source](https://arxiv.org/abs/2104.14294)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šé€‰æ‹©çš„å¤´éƒ¨çš„è‡ªæ³¨æ„åŠ›å›¾ã€‚[æ¥æº](https://arxiv.org/abs/2104.14294)
- en: Let us unpack another layer of abstraction and letâ€™s try to understand how DINO
    implements its framework, tackles instabilities and how it performs compared to
    previous methods!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ·±å…¥æ¢è®¨ä¸€ä¸‹ DINO å®ç°å…¶æ¡†æ¶çš„æ–¹å¼ï¼Œå¦‚ä½•åº”å¯¹ä¸ç¨³å®šæ€§ï¼Œä»¥åŠä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”å®ƒçš„è¡¨ç°å¦‚ä½•ï¼
- en: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
- en: '[Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)'
- en: Paper Walkthroughs by Sascha Kirch
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”± Sascha Kirch è¿›è¡Œçš„è®ºæ–‡è§£è¯»
- en: '[View list](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2?source=post_page-----4cb08e821b18--------------------------------)7
    stories![â€œDDPMâ€Šâ€”â€ŠDenoising Diffusion Probabilistic Models â€œ paper illustration
    by Sascha Kirch](../Images/6e785c0a911386676abebe0fa646f483.png)![â€œDepth Anythingâ€
    paper illustration by Sascha Kirch](../Images/bd8cd71a02e42cf64d0afd39f41f48e0.png)![](../Images/8708d91a4a1902cef889ced95d46fc39.png)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2?source=post_page-----4cb08e821b18--------------------------------)7
    ä¸ªæ•…äº‹ï¼[â€œDDPMâ€Šâ€”â€Šå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹â€è®ºæ–‡æ’å›¾ï¼Œä½œè€…ï¼šSascha Kirch](../Images/6e785c0a911386676abebe0fa646f483.png)![â€œDepth
    Anythingâ€è®ºæ–‡æ’å›¾ï¼Œä½œè€…ï¼šSascha Kirch](../Images/bd8cd71a02e42cf64d0afd39f41f48e0.png)![](../Images/8708d91a4a1902cef889ced95d46fc39.png)'
- en: Method
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–¹æ³•
- en: The DINO framework shares the same overall structure with other similarity-learning
    frameworks like [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)
    or the [mean teacher](https://arxiv.org/abs/1703.01780) but also with knowledge
    distillation. Letâ€™s first have a look on how DINO does it and the differentiate
    between the other frameworks.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: DINO æ¡†æ¶ä¸å…¶ä»–ç›¸ä¼¼æ€§å­¦ä¹ æ¡†æ¶ï¼ˆå¦‚ [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)
    æˆ– [mean teacher](https://arxiv.org/abs/1703.01780)ï¼‰ä»¥åŠçŸ¥è¯†è’¸é¦å…·æœ‰ç›¸åŒçš„æ•´ä½“ç»“æ„ã€‚è®©æˆ‘ä»¬é¦–å…ˆçœ‹çœ‹ DINO
    æ˜¯å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹çš„ï¼Œå¹¶ä¸å…¶ä»–æ¡†æ¶è¿›è¡ŒåŒºåˆ†ã€‚
- en: '![](../Images/2dacf33beadadf8b5604e4842686f18f.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2dacf33beadadf8b5604e4842686f18f.png)'
- en: 'Fig. 2: DINO architecture. [Source](https://arxiv.org/abs/2104.14294) + annotations
    by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2ï¼šDINO æ¶æ„ã€‚ [æ¥æº](https://arxiv.org/abs/2104.14294) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    çš„æ³¨é‡Š
- en: '**Networks and Update Rule**'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç½‘ç»œå’Œæ›´æ–°è§„åˆ™**'
- en: Letâ€™s start from the middle. DINO implements two networks with the exact same
    architecture but a different set of weights. Those are the student and the teacher.
    The student is trained with back propagation and the teacher updates its weights
    with an exponential moving average of its own weights and those of the student.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»ä¸­é—´å¼€å§‹ã€‚DINO å®ç°äº†ä¸¤ä¸ªå…·æœ‰å®Œå…¨ç›¸åŒæ¶æ„ä½†æƒé‡ä¸åŒçš„ç½‘ç»œã€‚è¿™äº›ç½‘ç»œåˆ†åˆ«æ˜¯å­¦ç”Ÿç½‘ç»œå’Œæ•™å¸ˆç½‘ç»œã€‚å­¦ç”Ÿç½‘ç»œé€šè¿‡åå‘ä¼ æ’­è¿›è¡Œè®­ç»ƒï¼Œè€Œæ•™å¸ˆç½‘ç»œåˆ™é€šè¿‡å…¶è‡ªèº«æƒé‡å’Œå­¦ç”Ÿç½‘ç»œæƒé‡çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡æ¥æ›´æ–°å…¶æƒé‡ã€‚
- en: '![](../Images/b2fe06104b0d6ee43b9013c3b398e4e8.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2fe06104b0d6ee43b9013c3b398e4e8.png)'
- en: 'Equation 1: Update rule of the teacherâ€™s weights. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 1ï¼šæ•™å¸ˆæƒé‡çš„æ›´æ–°è§„åˆ™ã€‚ [æ¥æº](https://arxiv.org/abs/2104.14294) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    çš„æ³¨é‡Š
- en: Backbones are either a ResNet50 or [DeiT](https://arxiv.org/abs/2012.12877)
    (which is a [ViT](https://arxiv.org/abs/2010.11929) adapted for knowledge distillation).
    An MLP-based projection head is connected to the backbone to reduce the dimensionality
    of the features, but is removed for inference.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: éª¨å¹²ç½‘ç»œå¯ä»¥æ˜¯ ResNet50 æˆ– [DeiT](https://arxiv.org/abs/2012.12877)ï¼ˆè¿™æ˜¯ä¸ºçŸ¥è¯†è’¸é¦è€Œè°ƒæ•´çš„ [ViT](https://arxiv.org/abs/2010.11929)ï¼‰ã€‚ä¸€ä¸ªåŸºäº
    MLP çš„æŠ•å½±å¤´è¿æ¥åˆ°éª¨å¹²ç½‘ç»œï¼Œä»¥å‡å°‘ç‰¹å¾çš„ç»´åº¦ï¼Œä½†åœ¨æ¨ç†æ—¶ä¼šè¢«ç§»é™¤ã€‚
- en: '***Nice, but which model is used for inference: student or teacher?*** â€” Well
    thatâ€™s a good question and funny enough not a single word is mentioned in the
    paper. Intuitively you might think the student, at least I did at first. But as
    we will see later, the teacher outperforms the student throughout the training.
    The only hint beside the better performance is that in the code implementation
    the teacher checkpoint is the default one for the evaluation of for example [video
    segmentation](https://github.com/facebookresearch/dino/blob/7c446df5b9f45747937fb0d72314eb9f7b66930a/eval_video_segmentation.py#L257C9-L257C9),
    [linear probing](https://github.com/facebookresearch/dino/blob/7c446df5b9f45747937fb0d72314eb9f7b66930a/eval_linear.py#L264C34-L264C34)
    and [k-NN](https://github.com/facebookresearch/dino/blob/7c446df5b9f45747937fb0d72314eb9f7b66930a/eval_knn.py#L203C32-L203C32).
    Since this parameter can be changed though, I cannot tell you with certainty.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '***å¾ˆå¥½ï¼Œä½†ç”¨äºæ¨ç†çš„æ˜¯å“ªä¸ªæ¨¡å‹ï¼šå­¦ç”Ÿè¿˜æ˜¯æ•™å¸ˆï¼Ÿ*** â€” å¥½é—®é¢˜ï¼Œå®é™…ä¸Šè®ºæ–‡ä¸­å¹¶æ²¡æœ‰æåˆ°è¿™ä¸ªé—®é¢˜çš„ä»»ä½•ä¿¡æ¯ã€‚ç›´è§‚ä¸Šä½ å¯èƒ½ä¼šè®¤ä¸ºæ˜¯å­¦ç”Ÿï¼Œè‡³å°‘æˆ‘æœ€å¼€å§‹ä¹Ÿæ˜¯è¿™æ ·æƒ³çš„ã€‚ä½†æ­£å¦‚æˆ‘ä»¬åç»­å°†çœ‹åˆ°çš„ï¼Œæ•™å¸ˆåœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­è¡¨ç°ä¼˜äºå­¦ç”Ÿã€‚é™¤äº†æ›´å¥½çš„æ€§èƒ½ä¹‹å¤–ï¼Œå”¯ä¸€çš„çº¿ç´¢æ˜¯ï¼Œåœ¨ä»£ç å®ç°ä¸­ï¼Œæ•™å¸ˆæ£€æŸ¥ç‚¹æ˜¯ç”¨äºä¾‹å¦‚
    [è§†é¢‘åˆ†å‰²](https://github.com/facebookresearch/dino/blob/7c446df5b9f45747937fb0d72314eb9f7b66930a/eval_video_segmentation.py#L257C9-L257C9)ã€[çº¿æ€§æ¢æµ‹](https://github.com/facebookresearch/dino/blob/7c446df5b9f45747937fb0d72314eb9f7b66930a/eval_linear.py#L264C34-L264C34)
    å’Œ [k-NN](https://github.com/facebookresearch/dino/blob/7c446df5b9f45747937fb0d72314eb9f7b66930a/eval_knn.py#L203C32-L203C32)
    çš„é»˜è®¤è¯„ä¼°ç‚¹ã€‚ç”±äºæ­¤å‚æ•°å¯ä»¥æ›´æ”¹ï¼Œå› æ­¤æˆ‘ä¸èƒ½ç»™å‡ºç¡®åˆ‡çš„ç­”æ¡ˆã€‚'
- en: '**Inputs and Outputs**'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**è¾“å…¥å’Œè¾“å‡º**'
- en: From an input image *x* different views *x1* and *x2* are created by cropping
    and applying image augmentations like in [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)
    (e.g. color jitter, Gaussian blur and solarization). The technique used for cropping
    is called [multi-crop](https://arxiv.org/abs/2006.09882) where multiple crops
    of different sizes are generated to save memory while providing more data. Small
    crops are called local views and consist of 96x96 pixels that are exclusively
    feed into the student. Larger crops are called global views and consists of 224x224
    pixels that are exclusively fed into the teacher. As we will see later in the
    ablation section, 2 global views and 10 local views have been used during training.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¾“å…¥å›¾åƒ *x* åˆ›å»ºä¸åŒçš„è§†å›¾ *x1* å’Œ *x2*ï¼Œæ–¹æ³•æ˜¯é€šè¿‡è£å‰ªå’Œåº”ç”¨å›¾åƒå¢å¼ºï¼Œå¦‚ [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)ï¼ˆä¾‹å¦‚è‰²å½©æŠ–åŠ¨ã€é«˜æ–¯æ¨¡ç³Šå’Œå¤ªé˜³åŒ–ï¼‰ã€‚ç”¨äºè£å‰ªçš„æŠ€æœ¯ç§°ä¸º
    [multi-crop](https://arxiv.org/abs/2006.09882)ï¼Œé€šè¿‡ç”Ÿæˆä¸åŒå¤§å°çš„å¤šä¸ªè£å‰ªæ¥èŠ‚çœå†…å­˜ï¼ŒåŒæ—¶æä¾›æ›´å¤šæ•°æ®ã€‚å°è£å‰ªè¢«ç§°ä¸ºå±€éƒ¨è§†å›¾ï¼Œç”±
    96x96 åƒç´ ç»„æˆï¼Œè¿™äº›è§†å›¾ä¸“é—¨è¾“å…¥åˆ°å­¦ç”Ÿç½‘ç»œä¸­ã€‚è¾ƒå¤§çš„è£å‰ªè¢«ç§°ä¸ºå…¨å±€è§†å›¾ï¼Œç”± 224x224 åƒç´ ç»„æˆï¼Œè¿™äº›è§†å›¾ä¸“é—¨è¾“å…¥åˆ°æ•™å¸ˆç½‘ç»œä¸­ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨æ¶ˆèéƒ¨åˆ†å°†çœ‹åˆ°çš„ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨äº†
    2 ä¸ªå…¨å±€è§†å›¾å’Œ 10 ä¸ªå±€éƒ¨è§†å›¾ã€‚
- en: 'NOTE: The paper is a bit confusing regarding the multi-crop technique because
    neither the provided pseudo-code nor the architecture shown in Fig. 3 above reflect
    it. The pseudo code even suggests that x1 and x2 are feed into both, the student
    and the teacher like in BYOL, which is not the case when using multi-crop.'
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šè®ºæ–‡å¯¹äºå¤šè£å‰ªæŠ€æœ¯æœ‰ç‚¹æ··ä¹±ï¼Œå› ä¸ºæä¾›çš„ä¼ªä»£ç å’Œä¸Šé¢çš„å›¾ 3 æ‰€ç¤ºçš„æ¶æ„éƒ½æ²¡æœ‰åæ˜ å‡ºæ¥ã€‚ä¼ªä»£ç ç”šè‡³å»ºè®® x1 å’Œ x2 åƒåœ¨ BYOL ä¸­ä¸€æ ·è¾“å…¥åˆ°å­¦ç”Ÿå’Œæ•™å¸ˆä¸­ï¼Œè¿™åœ¨ä½¿ç”¨å¤šè£å‰ªæ—¶å¹¶éå¦‚æ­¤ã€‚
- en: In contrast to similarity learning where the objective is to maximize the similarity
    of embeddings, DINO minimizes the cross-entropy between the teacherâ€™s and the
    studentâ€™s output distribution. As indicated by the equation bellow, the cross-entropy
    is calculated for each pair of global and local views and is then summed up.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ç›¸ä¼¼æ€§å­¦ä¹ çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–åµŒå…¥çš„ç›¸ä¼¼æ€§ä¸åŒï¼ŒDINO æœ€å°åŒ–æ•™å¸ˆå’Œå­¦ç”Ÿè¾“å‡ºåˆ†å¸ƒä¹‹é—´çš„äº¤å‰ç†µã€‚å¦‚ä¸‹é¢çš„æ–¹ç¨‹æ‰€ç¤ºï¼Œäº¤å‰ç†µæ˜¯å¯¹æ¯å¯¹å…¨å±€å’Œå±€éƒ¨è§†å›¾è®¡ç®—çš„ï¼Œç„¶åæ±‡æ€»ã€‚
- en: '![](../Images/b999355c87ae211c90653c509cea3da3.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b999355c87ae211c90653c509cea3da3.png)'
- en: 'Equation 2: Optimization objective. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 2ï¼šä¼˜åŒ–ç›®æ ‡ã€‚ [æ¥æº](https://arxiv.org/abs/2104.14294) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    çš„æ³¨è§£
- en: '***And what do the models output?*** â€” Like in similarity learning, the student
    and the teacher output an embedding for a given image, rather than a prediction
    score. Like in knowledge distillation, the output is transformed via a SoftMax
    transformation into a probability distribution. The SoftMax has a temperature
    parameter that controls the smoothing or sharpening of the resulting distribution.
    This temperature plays a crucial role in knowledge distillation because it allows
    to control the balance between transferring general knowledge and fine-grained
    details from a teacher network to a student network, making the distillation process
    more effective for different tasks.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '***æ¨¡å‹çš„è¾“å‡ºæ˜¯ä»€ä¹ˆï¼Ÿ*** â€” å°±åƒç›¸ä¼¼æ€§å­¦ä¹ ä¸­ï¼Œå­¦ç”Ÿå’Œæ•™å¸ˆä¸ºç»™å®šçš„å›¾åƒè¾“å‡ºä¸€ä¸ªåµŒå…¥ï¼Œè€Œä¸æ˜¯é¢„æµ‹åˆ†æ•°ã€‚å°±åƒåœ¨çŸ¥è¯†è’¸é¦ä¸­ï¼Œè¾“å‡ºé€šè¿‡ SoftMax
    è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚SoftMax æœ‰ä¸€ä¸ªæ¸©åº¦å‚æ•°ï¼Œå®ƒæ§åˆ¶ç»“æœåˆ†å¸ƒçš„å¹³æ»‘æˆ–é”åŒ–ã€‚è¿™ä¸ªæ¸©åº¦åœ¨çŸ¥è¯†è’¸é¦ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œå› ä¸ºå®ƒå¯ä»¥æ§åˆ¶ä»æ•™å¸ˆç½‘ç»œåˆ°å­¦ç”Ÿç½‘ç»œè½¬ç§»ä¸€èˆ¬çŸ¥è¯†å’Œç»†ç²’åº¦ç»†èŠ‚ä¹‹é—´çš„å¹³è¡¡ï¼Œä½¿è’¸é¦è¿‡ç¨‹å¯¹ä¸åŒä»»åŠ¡æ›´æœ‰æ•ˆã€‚'
- en: '![](../Images/4d9f933db5fb3f0e5dfba164632ebfcd.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d9f933db5fb3f0e5dfba164632ebfcd.png)'
- en: 'Fig. 3: Effect of temperature value on the SoftMax output. Illustration by
    [Sascha Kirch](https://medium.com/@SaschaKirch) created with [this python notebook](https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3ï¼šæ¸©åº¦å€¼å¯¹ SoftMax è¾“å‡ºçš„å½±å“ã€‚ [Sascha Kirch](https://medium.com/@SaschaKirch) çš„æ’å›¾ï¼Œä½¿ç”¨
    [è¿™ä¸ª Python ç¬”è®°æœ¬](https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb)
    åˆ›å»º
- en: 'I created a notebook for you so you can investigate the impact of the temperature
    on the resulting distribution:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸ºä½ åˆ›å»ºäº†ä¸€ä¸ªç¬”è®°æœ¬ï¼Œä»¥ä¾¿ä½ å¯ä»¥è°ƒæŸ¥æ¸©åº¦å¯¹ç»“æœåˆ†å¸ƒçš„å½±å“ï¼š
- en: '[](https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb?source=post_page-----4cb08e821b18--------------------------------)
    [## ML_Notebooks/Softmax_Temperature.ipynb at main Â· sascha-kirch/ML_Notebooks'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb?source=post_page-----4cb08e821b18--------------------------------)
    [## ML_Notebooks/Softmax_Temperature.ipynb åœ¨ main åˆ†æ”¯ Â· sascha-kirch/ML_Notebooks'
- en: Collection of machine learning related notebooks to share. â€” ML_Notebooks/Softmax_Temperature.ipynb
    at main Â·â€¦
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ ç›¸å…³ç¬”è®°çš„é›†åˆç”¨äºå…±äº«ã€‚â€” ML_Notebooks/Softmax_Temperature.ipynb åœ¨ main åˆ†æ”¯ Â·â€¦
- en: github.com](https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb?source=post_page-----4cb08e821b18--------------------------------)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb?source=post_page-----4cb08e821b18--------------------------------)
- en: '**Avoiding Collapse**'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**é¿å…å´©æºƒ**'
- en: 'As mentioned earlier, student and teacher have the exact same architecture.
    This kind of setup is unstable (if no counter measures are implemented) and might
    result in collapsing solutions, where all features are mapped to a certain region
    in the latent space, e.g. a single point in the worst case. BYOL addressed this
    issue with an extra prediction head for only one of the models introducing an
    asymmetry. Since DINO has symmetric models another trick is required: centering
    and sharpening. Both are applied to the teacher network only. Centering is a technique
    that prevents a single dimension in the latent space to dominate, by adding a
    bias term *c* to the teachers output *g(x) = g(x)+c*, where'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œå­¦ç”Ÿå’Œæ•™å¸ˆå…·æœ‰å®Œå…¨ç›¸åŒçš„æ¶æ„ã€‚è¿™ç§è®¾ç½®æ˜¯ä¸ç¨³å®šçš„ï¼ˆå¦‚æœæ²¡æœ‰é‡‡å–å¯¹ç­–ï¼‰ï¼Œå¯èƒ½ä¼šå¯¼è‡´å´©æºƒè§£å†³æ–¹æ¡ˆï¼Œå³æ‰€æœ‰ç‰¹å¾éƒ½æ˜ å°„åˆ°æ½œåœ¨ç©ºé—´ä¸­çš„æŸä¸ªåŒºåŸŸï¼Œä¾‹å¦‚æœ€åæƒ…å†µä¸‹çš„ä¸€ä¸ªç‚¹ã€‚BYOL
    é€šè¿‡ä¸ºå…¶ä¸­ä¸€ä¸ªæ¨¡å‹å¼•å…¥é¢å¤–çš„é¢„æµ‹å¤´æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä»è€Œå¼•å…¥äº†ä¸å¯¹ç§°æ€§ã€‚ç”±äº DINO å…·æœ‰å¯¹ç§°æ¨¡å‹ï¼Œå› æ­¤éœ€è¦å¦ä¸€ç§æŠ€å·§ï¼šä¸­å¿ƒåŒ–å’Œé”åŒ–ã€‚ä¸¤è€…ä»…åº”ç”¨äºæ•™å¸ˆç½‘ç»œã€‚ä¸­å¿ƒåŒ–æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œé€šè¿‡å‘æ•™å¸ˆè¾“å‡ºæ·»åŠ åç½®é¡¹*c*æ¥é˜²æ­¢æ½œåœ¨ç©ºé—´ä¸­çš„å•ä¸€ç»´åº¦ä¸»å¯¼ï¼Œå³*g(x)
    = g(x)+c*ã€‚
- en: '![](../Images/268f8c0249d46fa219f3dcad096e9332.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/268f8c0249d46fa219f3dcad096e9332.png)'
- en: 'Equation 3: Update rule of the centering term. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 3ï¼šä¸­å¿ƒåŒ–é¡¹çš„æ›´æ–°è§„åˆ™ã€‚[æ¥æº](https://arxiv.org/abs/2104.14294) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    çš„æ³¨é‡Š
- en: While centering has a positive effect, it also encourages the output to collapse
    into a uniform distribution. Sharpening has the opposite effect hence applying
    both balances their effect and stabilizes training. Sharpening is achieved by
    using a smaller temperature in the SoftMax (see Fig. 3) for the teacher as for
    the student.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶ä¸­å¿ƒåŒ–å…·æœ‰ç§¯ææ•ˆæœï¼Œä½†å®ƒä¹Ÿé¼“åŠ±è¾“å‡ºå´©æºƒä¸ºå‡åŒ€åˆ†å¸ƒã€‚é”åŒ–å…·æœ‰ç›¸åçš„æ•ˆæœï¼Œå› æ­¤åº”ç”¨ä¸¤è€…å¹³è¡¡å®ƒä»¬çš„æ•ˆæœå¹¶ç¨³å®šè®­ç»ƒã€‚é€šè¿‡ä½¿ç”¨è¾ƒå°çš„æ¸©åº¦æ¥å®ç°é”åŒ–ï¼ˆè§å›¾ 3ï¼‰ï¼Œæ•™å¸ˆçš„
    SoftMax æ¸©åº¦æ¯”å­¦ç”Ÿçš„ä½ã€‚
- en: To avoid collapsing the hyperparameter *m* from equation 3 and the temperature
    of the teacher are crucial. In their ablation study in the appendix section the
    authors show that *m=0.9â€¦0.999* works best and the temperature value is linearly
    increased from *0.04* to *0.07* during warm-up.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†é¿å…æ–¹ç¨‹ 3 ä¸­çš„è¶…å‚æ•°*m*å’Œæ•™å¸ˆçš„æ¸©åº¦å´©æºƒæ˜¯è‡³å…³é‡è¦çš„ã€‚åœ¨é™„å½•éƒ¨åˆ†çš„æ¶ˆèç ”ç©¶ä¸­ï¼Œä½œè€…å±•ç¤ºäº†*m=0.9â€¦0.999*çš„æ•ˆæœæœ€ä½³ï¼Œå¹¶ä¸”æ¸©åº¦å€¼åœ¨é¢„çƒ­æœŸé—´ä»*0.04*çº¿æ€§å¢åŠ åˆ°*0.07*ã€‚
- en: '**What does DINO do? Knowledge Distillation or Similarity Learning?**'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**DINO æ˜¯åšä»€ä¹ˆçš„ï¼ŸçŸ¥è¯†è’¸é¦è¿˜æ˜¯ç›¸ä¼¼æ€§å­¦ä¹ ï¼Ÿ**'
- en: The answer is a little bit of both!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”æ¡ˆæ˜¯ä¸¤è€…å…¼æœ‰ï¼
- en: While knowledge distillation usually distils knowledge from an already trained,
    larger and more accurate teacher model into a smaller student model, it could
    also be seen as some sort of similarity learning because it encourages the student
    network to produce predictions that are similar to those of the teacher. In similarity
    learning, the two models are usually trained jointly and often align their latent
    space predictions rather than probability distributions.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶çŸ¥è¯†è’¸é¦é€šå¸¸æ˜¯å°†çŸ¥è¯†ä»å·²ç»è®­ç»ƒå¥½çš„ã€æ›´å¤§ä¸”æ›´å‡†ç¡®çš„æ•™å¸ˆæ¨¡å‹è’¸é¦åˆ°è¾ƒå°çš„å­¦ç”Ÿæ¨¡å‹ä¸­ï¼Œä½†å®ƒä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯ä¸€ç§ç›¸ä¼¼æ€§å­¦ä¹ ï¼Œå› ä¸ºå®ƒé¼“åŠ±å­¦ç”Ÿç½‘ç»œç”Ÿæˆä¸æ•™å¸ˆç›¸ä¼¼çš„é¢„æµ‹ã€‚åœ¨ç›¸ä¼¼æ€§å­¦ä¹ ä¸­ï¼Œä¸¤ä¸ªæ¨¡å‹é€šå¸¸æ˜¯è”åˆè®­ç»ƒçš„ï¼Œå¹¶ä¸”é€šå¸¸å¯¹é½å®ƒä»¬çš„æ½œåœ¨ç©ºé—´é¢„æµ‹ï¼Œè€Œä¸æ˜¯æ¦‚ç‡åˆ†å¸ƒã€‚
- en: 'Since the authors of DINO phrase their objective as knowledge distillation,
    letâ€™s have a look on some differences compared with â€œstandardâ€ knowledge distillation:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äº DINO çš„ä½œè€…å°†ä»–ä»¬çš„ç›®æ ‡è¡¨è¿°ä¸ºçŸ¥è¯†è’¸é¦ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ä¸â€œæ ‡å‡†â€çŸ¥è¯†è’¸é¦ç›¸æ¯”çš„ä¸€äº›å·®å¼‚ï¼š
- en: DINOâ€™s teacher is not available a priori but â€œtrainedâ€ alongside the student.
    It can even be considered as a co-distillation since knowledge is also distilled
    from the student into the teacher.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DINO çš„æ•™å¸ˆä¸æ˜¯äº‹å…ˆå¯ç”¨çš„ï¼Œè€Œæ˜¯ä¸å­¦ç”Ÿä¸€èµ·â€œè®­ç»ƒâ€çš„ã€‚å®ƒç”šè‡³å¯ä»¥è¢«è®¤ä¸ºæ˜¯ä¸€ç§å…±åŒè’¸é¦ï¼Œå› ä¸ºçŸ¥è¯†ä¹Ÿä»å­¦ç”Ÿè’¸é¦åˆ°æ•™å¸ˆã€‚
- en: DINOâ€™s teacher and student are not acting on the same input but on different
    views of the image cropped to different sizes.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DINO çš„æ•™å¸ˆå’Œå­¦ç”Ÿä¸æ˜¯å¯¹ç›¸åŒçš„è¾“å…¥è¿›è¡Œæ“ä½œï¼Œè€Œæ˜¯å¯¹è£å‰ªåˆ°ä¸åŒå°ºå¯¸çš„å›¾åƒçš„ä¸åŒè§†å›¾è¿›è¡Œæ“ä½œã€‚
- en: DINO uses different temperatures in the SoftMax of both models to perform sharpening.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DINO åœ¨ä¸¤ä¸ªæ¨¡å‹çš„ SoftMax ä¸­ä½¿ç”¨ä¸åŒçš„æ¸©åº¦æ¥è¿›è¡Œé”åŒ–ã€‚
- en: DINO calculates the cross-entropy over the temperature-scaled SoftMax of the
    embeddings rather than prediction scores.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DINO è®¡ç®—çš„æ˜¯åµŒå…¥çš„æ¸©åº¦ç¼©æ”¾ SoftMax ä¸Šçš„äº¤å‰ç†µï¼Œè€Œä¸æ˜¯é¢„æµ‹åˆ†æ•°ã€‚
- en: 'And how is it similar to knowledge distillation?:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä¸çŸ¥è¯†è’¸é¦çš„ç›¸ä¼¼ä¹‹å¤„åœ¨å“ªé‡Œï¼Ÿï¼š
- en: DINO consists of a student and a teacher network, where the teacher performs
    better than the student as we will see in the experiments.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DINO ç”±ä¸€ä¸ªå­¦ç”Ÿç½‘ç»œå’Œä¸€ä¸ªæ•™å¸ˆç½‘ç»œç»„æˆï¼Œå…¶ä¸­æ•™å¸ˆçš„è¡¨ç°ä¼˜äºå­¦ç”Ÿï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨å®éªŒä¸­å°†çœ‹åˆ°çš„é‚£æ ·ã€‚
- en: Rather than maximizing a similarity metric, DINO minimizes the cross-entropy
    loss of a temperature scaled SoftMax output.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DINO ä¸æ˜¯æœ€å¤§åŒ–ç›¸ä¼¼æ€§åº¦é‡ï¼Œè€Œæ˜¯æœ€å°åŒ–æ¸©åº¦ç¼©æ”¾çš„ SoftMax è¾“å‡ºçš„äº¤å‰ç†µæŸå¤±ã€‚
- en: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----4cb08e821b18--------------------------------)
    [## Get an email whenever Sascha Kirch publishes ğŸš€'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----4cb08e821b18--------------------------------)
    [## æ¯å½“ Sascha Kirch å‘å¸ƒæ–°å†…å®¹æ—¶éƒ½ä¼šæ”¶åˆ°é‚®ä»¶ ğŸš€'
- en: Get an email whenever Sascha Kirch publishes ğŸš€ Looking to learn more about deep
    learning or simply stay up to dateâ€¦
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¯å½“ Sascha Kirch å‘å¸ƒæ–°å†…å®¹æ—¶éƒ½ä¼šæ”¶åˆ°é‚®ä»¶ ğŸš€ æƒ³è¦äº†è§£æ›´å¤šæ·±åº¦å­¦ä¹ ç›¸å…³çš„å†…å®¹æˆ–åªæ˜¯ä¿æŒæœ€æ–°åŠ¨æ€â€¦â€¦
- en: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----4cb08e821b18--------------------------------)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----4cb08e821b18--------------------------------)
- en: Experiments
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®éªŒ
- en: The paper presents a vast number of experiments. They pre-train the model on
    ImageNet, a commonly used dataset in representation learning.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡å±•ç¤ºäº†å¤§é‡çš„å®éªŒã€‚ä»–ä»¬åœ¨ ImageNet ä¸Šé¢„è®­ç»ƒæ¨¡å‹ï¼ŒImageNet æ˜¯ä¸€ä¸ªåœ¨è¡¨å¾å­¦ä¹ ä¸­å¸¸ç”¨çš„æ•°æ®é›†ã€‚
- en: For the evaluation, common techniques usually either train a linear classifier
    on top of frozen features or fine-tune the model to new downstream tasks, where
    the parameters of the model are adapted.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¯„ä¼°ï¼Œå¸¸è§çš„æŠ€æœ¯é€šå¸¸è¦ä¹ˆåœ¨å†»ç»“ç‰¹å¾ä¸Šè®­ç»ƒçº¿æ€§åˆ†ç±»å™¨ï¼Œè¦ä¹ˆå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥é€‚åº”æ–°çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹çš„å‚æ•°ä¼šè¢«è°ƒæ•´ã€‚
- en: The authors of DINO claim that those techniques are very sensitive to hyperparameters
    which makes comparisons unfair and hard to reproduce. Hence, they propose to use
    a simple nearest neighbor clustering algorithm on the features of the pre-trained
    model.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: DINO çš„ä½œè€…å£°ç§°è¿™äº›æŠ€æœ¯å¯¹è¶…å‚æ•°éå¸¸æ•æ„Ÿï¼Œè¿™ä½¿å¾—æ¯”è¾ƒä¸å…¬å¹³ä¸”éš¾ä»¥é‡ç°ã€‚å› æ­¤ï¼Œä»–ä»¬å»ºè®®å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹å¾ä½¿ç”¨ç®€å•çš„æœ€è¿‘é‚»èšç±»ç®—æ³•ã€‚
- en: Linear and k-NN Classification on ImageNet
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImageNet ä¸Šçš„çº¿æ€§å’Œ k-NN åˆ†ç±»
- en: In a this experiment the models are tested on their image classification accuracy
    on ImageNet. A variety of self-supervised pre-trained models are tested with either
    a ResNet or a ViT backbone. The classification is done either with linear probing
    or k-NN clustering.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªå®éªŒä¸­ï¼Œæ¨¡å‹åœ¨ ImageNet ä¸Šçš„å›¾åƒåˆ†ç±»å‡†ç¡®æ€§ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚æµ‹è¯•äº†å¤šç§è‡ªç›‘ç£é¢„è®­ç»ƒæ¨¡å‹ï¼Œéª¨å¹²ç½‘åŒ…æ‹¬ ResNet æˆ– ViTã€‚åˆ†ç±»æ˜¯é€šè¿‡çº¿æ€§æ¢æµ‹æˆ–
    k-NN èšç±»å®Œæˆçš„ã€‚
- en: '![](../Images/8411c4b87ac4d9fab26ed43dfe033c82.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8411c4b87ac4d9fab26ed43dfe033c82.png)'
- en: 'Table 1: Linear and k-NN classification on ImageNet. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 1ï¼šåœ¨ ImageNet ä¸Šçš„çº¿æ€§å’Œ k-NN åˆ†ç±»ã€‚ [æ¥æº](https://arxiv.org/abs/2104.14294) + [Sascha
    Kirch](https://medium.com/@SaschaKirch) çš„æ³¨é‡Š
- en: 'I guess the key take-aways are:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºä¸»è¦çš„æ”¶è·æ˜¯ï¼š
- en: K-NN performs better on ViT features than on ResNet features.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K-NN åœ¨ ViT ç‰¹å¾ä¸Šçš„è¡¨ç°ä¼˜äº ResNet ç‰¹å¾ã€‚
- en: Decreasing patch size in the ViT has larger improvement as larger backbone,
    but at the cost of slower inference.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ ViT ä¸­å‡å°‘è¡¥ä¸å¤§å°æ¯”å¢åŠ éª¨å¹²ç½‘å¸¦æ¥çš„æ”¹è¿›æ›´å¤§ï¼Œä½†ä»£ä»·æ˜¯æ¨ç†é€Ÿåº¦å˜æ…¢ã€‚
- en: Video Instance Segmentation
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§†é¢‘å®ä¾‹åˆ†å‰²
- en: An important experiment has been the video segmentation task, since the paper
    is about the ViTâ€™s capability to capture semantic segmentation in its features
    when trained with unsupervised methods. Or letâ€™s say thatâ€™s what is claimed ğŸ˜
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé‡è¦çš„å®éªŒæ˜¯è§†é¢‘åˆ†å‰²ä»»åŠ¡ï¼Œå› ä¸ºè®ºæ–‡è®¨è®ºäº† ViT åœ¨ç”¨è‡ªç›‘ç£æ–¹æ³•è®­ç»ƒæ—¶æ•æ‰è¯­ä¹‰åˆ†å‰²èƒ½åŠ›çš„ç‰¹æ€§ã€‚æˆ–è€…è¯´è¿™æ˜¯è®ºæ–‡æ‰€å£°ç§°çš„ ğŸ˜
- en: '![](../Images/9480e3a017e4d94de1b778797fc40226.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9480e3a017e4d94de1b778797fc40226.png)'
- en: 'Table 2: Video Instance segmentation. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 2ï¼šè§†é¢‘å®ä¾‹åˆ†å‰²ã€‚ [æ¥æº](https://arxiv.org/abs/2104.14294) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    çš„æ³¨é‡Š
- en: 'Observing those results I am missing two further experiments:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è§‚å¯Ÿè¿™äº›ç»“æœåï¼Œæˆ‘è§‰å¾—è¿˜ç¼ºå°‘ä¸¤ä¸ªè¿›ä¸€æ­¥çš„å®éªŒï¼š
- en: It would be nice to see a comparison of a supervised ResNet50 and a self-supervised
    ResNet50 in the DINO framework to support their claim that the ViT is superior
    to the ResNet architecture.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœèƒ½çœ‹åˆ°åœ¨ DINO æ¡†æ¶ä¸‹ç›‘ç£çš„ ResNet50 å’Œè‡ªç›‘ç£çš„ ResNet50 ä¹‹é—´çš„å¯¹æ¯”ï¼Œå°†ä¼šå¾ˆå¥½ï¼Œè¿™å¯ä»¥æ”¯æŒä»–ä»¬å…³äº ViT ä¼˜äº ResNet
    æ¶æ„çš„ä¸»å¼ ã€‚
- en: It would also be great to see the same set of ViT backbones for supervised as
    for self-supervised to see the impact on patch-size and model size.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœèƒ½çœ‹åˆ°ç›¸åŒçš„ ViT éª¨å¹²ç½‘åœ¨ç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£å­¦ä¹ ä¸‹çš„æ•ˆæœå¯¹æ¯”ï¼Œå°†ä¼šéå¸¸æ£’ï¼Œè¿™æ ·å¯ä»¥è§‚å¯Ÿåˆ°å¯¹è¡¥ä¸å¤§å°å’Œæ¨¡å‹å¤§å°çš„å½±å“ã€‚
- en: 'But as I always say: asking questions is easy ğŸ˜ In real-world projects the
    authors often face resource constraints and project deadlines so not every single
    little detail can be covered!'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿‡æ­£å¦‚æˆ‘æ€»æ˜¯è¯´çš„ï¼šæå‡ºé—®é¢˜å¾ˆå®¹æ˜“ ğŸ˜ åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œä½œè€…ä»¬å¸¸å¸¸é¢ä¸´èµ„æºé™åˆ¶å’Œé¡¹ç›®æˆªæ­¢æ—¥æœŸï¼Œæ‰€ä»¥ä¸å¯èƒ½æ¶µç›–æ¯ä¸€ä¸ªç»†èŠ‚ï¼
- en: Probing the Self-Attention Map
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¢ç´¢è‡ªæ³¨æ„åŠ›å›¾
- en: In this experiment the authors investigated the self-attention maps of different
    heads in the multi-head self-attention layers of the ViT. They visualize the attention
    maps from selected heads from the last layer of ViT-S/8, those of the learned
    [CLS] token to be precise.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªå®éªŒä¸­ï¼Œä½œè€…è°ƒæŸ¥äº†ViTçš„å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚ä¸­ä¸åŒå¤´éƒ¨çš„è‡ªæ³¨æ„åŠ›å›¾ã€‚ä»–ä»¬å¯è§†åŒ–äº†ViT-S/8æœ€åä¸€å±‚ä¸­é€‰å®šå¤´éƒ¨çš„æ³¨æ„åŠ›å›¾ï¼Œç²¾ç¡®æ¥è¯´æ˜¯å­¦ä¹ åˆ°çš„[CLS]ä»¤ç‰Œã€‚
- en: '![](../Images/133db7e45053a51d9d4e14bf554844d1.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/133db7e45053a51d9d4e14bf554844d1.png)'
- en: 'Fig. 4: Attention maps from selected heads. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4ï¼šæ¥è‡ªé€‰å®šå¤´éƒ¨çš„æ³¨æ„åŠ›å›¾ã€‚ [æ¥æº](https://arxiv.org/abs/2104.14294) + [Sascha Kirch](https://medium.com/@SaschaKirch)çš„æ³¨é‡Š
- en: Other Experiments
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…¶ä»–å®éªŒ
- en: In other experiments, DINO improved compared against the supervised baseline.
    Those tasks include image retrieval and copy detection.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å…¶ä»–å®éªŒä¸­ï¼ŒDINOåœ¨ä¸ç›‘ç£åŸºçº¿çš„æ¯”è¾ƒä¸­æœ‰æ‰€æ”¹è¿›ã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬å›¾åƒæ£€ç´¢å’Œå¤åˆ¶æ£€æµ‹ã€‚
- en: Ablations
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¶ˆèå®éªŒ
- en: For their ablation study the authors experiment with the ViT-S model.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»–ä»¬çš„æ¶ˆèç ”ç©¶ä¸­ï¼Œä½œè€…å¯¹ViT-Sæ¨¡å‹è¿›è¡Œäº†å®éªŒã€‚
- en: Importance of Patch Size
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¡¥ä¸å¤§å°çš„é‡è¦æ€§
- en: Recall that a vision transformer inputs a patchified version of an input image,
    transforms each patch into a token and then applies a transformer with its self-attention
    mechanism. This was a trick by the authors of ViT to reduce the compute requirements
    for trading-off performance, making transformers applicable to image data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: è®°ä½ï¼Œè§†è§‰å˜æ¢å™¨è¾“å…¥çš„æ˜¯ä¸€ä¸ªè¡¥ä¸åŒ–çš„è¾“å…¥å›¾åƒï¼Œå°†æ¯ä¸ªè¡¥ä¸è½¬åŒ–ä¸ºä»¤ç‰Œï¼Œç„¶ååº”ç”¨å…·æœ‰è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å˜æ¢å™¨ã€‚è¿™æ˜¯ViTä½œè€…çš„ä¸€é¡¹æŠ€å·§ï¼Œç”¨äºå‡å°‘æ€§èƒ½æƒè¡¡çš„è®¡ç®—éœ€æ±‚ï¼Œä½¿å˜æ¢å™¨é€‚ç”¨äºå›¾åƒæ•°æ®ã€‚
- en: DINO claims that smaller size of the patches increases the performance while
    decreasing the throughput (number of images that can be processed per second),
    which is exactly what ViT claims.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: DINOå£°ç§°ï¼Œè¾ƒå°çš„è¡¥ä¸å¤§å°æé«˜äº†æ€§èƒ½ï¼ŒåŒæ—¶é™ä½äº†ååé‡ï¼ˆæ¯ç§’å¯ä»¥å¤„ç†çš„å›¾åƒæ•°é‡ï¼‰ï¼Œè¿™æ­£æ˜¯ViTæ‰€å£°ç§°çš„ã€‚
- en: '![](../Images/2103ddc8e040677295c3880a1ff19051.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2103ddc8e040677295c3880a1ff19051.png)'
- en: 'Fig. 5: Impact of patch size on accuracy and throughput. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5ï¼šè¡¥ä¸å¤§å°å¯¹å‡†ç¡®æ€§å’Œååé‡çš„å½±å“ã€‚ [æ¥æº](https://arxiv.org/abs/2104.14294) + [Sascha Kirch](https://medium.com/@SaschaKirch)çš„æ³¨é‡Š
- en: Intuitively Iâ€™d say it is no surprise since you increase the input resolution
    and you end up with more tokens to attend to, so you end up with a fine-grained
    attention map.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´è§‚åœ°è¯´ï¼Œè¿™å¹¶ä¸ä»¤äººæƒŠè®¶ï¼Œå› ä¸ºä½ å¢åŠ äº†è¾“å…¥åˆ†è¾¨ç‡ï¼Œç»“æœæ˜¯éœ€è¦å¤„ç†æ›´å¤šçš„ä»¤ç‰Œï¼Œå› æ­¤ä½ å¾—åˆ°ä¸€ä¸ªæ›´ç»†ç²’åº¦çš„æ³¨æ„åŠ›å›¾ã€‚
- en: Different Teacher Update Rules
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸åŒçš„æ•™å¸ˆæ›´æ–°è§„åˆ™
- en: The teacher in DINO is updated by calculating the exponential moving average
    from the updated student and the current teacher. This is the â€œmomentum encoderâ€
    approach they refer to.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: DINOä¸­çš„æ•™å¸ˆé€šè¿‡è®¡ç®—ä»æ›´æ–°åçš„å­¦ç”Ÿå’Œå½“å‰æ•™å¸ˆçš„æŒ‡æ•°ç§»åŠ¨å¹³å‡æ¥æ›´æ–°ã€‚è¿™å°±æ˜¯ä»–ä»¬æ‰€ç§°çš„â€œåŠ¨é‡ç¼–ç å™¨â€æ–¹æ³•ã€‚
- en: 'Using a momentum encoder and plotting the accuracy of the teacher and the student
    during training, the teacher performs better throughout the entire process. From
    this we can hypothesize:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åŠ¨é‡ç¼–ç å™¨å¹¶ç»˜åˆ¶æ•™å¸ˆå’Œå­¦ç”Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‡†ç¡®æ€§ï¼Œæ•™å¸ˆåœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­è¡¨ç°æ›´å¥½ã€‚ç”±æ­¤æˆ‘ä»¬å¯ä»¥å‡è®¾ï¼š
- en: the teacher can provide a strong learning signal to the student.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ•™å¸ˆå¯ä»¥ä¸ºå­¦ç”Ÿæä¾›å¼ºæœ‰åŠ›çš„å­¦ä¹ ä¿¡å·ã€‚
- en: an improving student improves the teacher due to the EMA update rule (co-distillation).
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ”¹è¿›çš„å­¦ç”Ÿç”±äºEMAæ›´æ–°è§„åˆ™ï¼ˆå…±åŒè’¸é¦ï¼‰ä½¿æ•™å¸ˆå¾—åˆ°æå‡ã€‚
- en: One can use the Teacher as final model which has better performance but the
    same architecture as the student, hence no change in compute requirements.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨æ•™å¸ˆä½œä¸ºæœ€ç»ˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·æœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œä½†ä¸å­¦ç”Ÿå…·æœ‰ç›¸åŒçš„æ¶æ„ï¼Œå› æ­¤è®¡ç®—éœ€æ±‚æ²¡æœ‰å˜åŒ–ã€‚
- en: '![](../Images/36bd26e2335351a2980f6f518e29211d.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36bd26e2335351a2980f6f518e29211d.png)'
- en: 'Fig. 6: Teacher performance. [Source](https://arxiv.org/abs/2104.14294) + annotations
    by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾6ï¼šæ•™å¸ˆæ€§èƒ½ã€‚ [æ¥æº](https://arxiv.org/abs/2104.14294) + [Sascha Kirch](https://medium.com/@SaschaKirch)çš„æ³¨é‡Š
- en: 'They also experiment with 3 other update rules: copying the weights from the
    student to the teacher, use the student weights from the previous iteration of
    the optimizer and use the student weights from the previous epoch.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬è¿˜å®éªŒäº†å¦å¤–3ç§æ›´æ–°è§„åˆ™ï¼šå°†æƒé‡ä»å­¦ç”Ÿå¤åˆ¶åˆ°æ•™å¸ˆï¼Œä½¿ç”¨ä¼˜åŒ–å™¨å‰ä¸€ä¸ªè¿­ä»£çš„å­¦ç”Ÿæƒé‡ï¼Œå’Œä½¿ç”¨å‰ä¸€ä¸ªæ—¶ä»£çš„å­¦ç”Ÿæƒé‡ã€‚
- en: Multi-Crop vs. Time and GPU Memory
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤šè£å‰ªä¸æ—¶é—´å’ŒGPUå†…å­˜
- en: As mentioned earlier, DINO inputs multiple cropped views of the same image and
    feeds the global views into the teacher and the local views into the student.
    In this ablation, the authors experiment with different amounts of local views
    and report the impact on performance, training time and peak memory per GPU.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼ŒDINOè¾“å…¥ç›¸åŒå›¾åƒçš„å¤šä¸ªè£å‰ªè§†å›¾ï¼Œå¹¶å°†å…¨å±€è§†å›¾è¾“å…¥åˆ°æ•™å¸ˆæ¨¡å‹ä¸­ï¼Œå°†å±€éƒ¨è§†å›¾è¾“å…¥åˆ°å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚åœ¨è¿™é¡¹æ¶ˆèå®éªŒä¸­ï¼Œä½œè€…è¯•éªŒäº†ä¸åŒæ•°é‡çš„å±€éƒ¨è§†å›¾ï¼Œå¹¶æŠ¥å‘Šäº†å¯¹æ€§èƒ½ã€è®­ç»ƒæ—¶é—´å’Œæ¯GPUå³°å€¼å†…å­˜çš„å½±å“ã€‚
- en: '![](../Images/1150cc8dc59aeb8b950bce9b7b52a6be.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1150cc8dc59aeb8b950bce9b7b52a6be.png)'
- en: 'Table 3: Multi-Crop vs. Time and GPU Memory. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨3ï¼šå¤šè£å‰ªä¸æ—¶é—´å’ŒGPUå†…å­˜ã€‚[æ¥æº](https://arxiv.org/abs/2104.14294) + [Sascha Kirch](https://medium.com/@SaschaKirch)çš„æ³¨é‡Š
- en: Avoiding Collapse
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¿å…å´©æºƒ
- en: 'In this ablation the authors evaluated the role of their stabilizing measures
    to avoid collapsing solutions: centering and sharpening.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é¡¹æ¶ˆèå®éªŒä¸­ï¼Œä½œè€…è¯„ä¼°äº†å…¶ç¨³å®šæªæ–½åœ¨é¿å…å´©æºƒè§£å†³æ–¹æ¡ˆä¸­çš„ä½œç”¨ï¼šä¸­å¿ƒåŒ–å’Œé”åŒ–ã€‚
- en: To do so, they decomposed the cross-entropy into an entropy term and a Kullback-Leibler
    (KL) divergence term. KL divergence is a measure of difference of two probability
    distributions. If KL is 0, two distributions are considered equal.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼Œä»–ä»¬å°†äº¤å‰ç†µåˆ†è§£ä¸ºç†µé¡¹å’ŒKullback-Leiblerï¼ˆKLï¼‰æ•£åº¦é¡¹ã€‚KLæ•£åº¦æ˜¯ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒå·®å¼‚çš„åº¦é‡ã€‚å¦‚æœKLä¸º0ï¼Œåˆ™è®¤ä¸ºä¸¤ä¸ªåˆ†å¸ƒç›¸ç­‰ã€‚
- en: 'The intuition behind this is the following: if the KL divergence of the output
    distribution of the teacher and the student is constant throughout the training,
    there is no learning signal for updating the weights of the student.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ç›´è§‚çš„ç†è§£æ˜¯ï¼šå¦‚æœæ•™å¸ˆå’Œå­¦ç”Ÿçš„è¾“å‡ºåˆ†å¸ƒçš„KLæ•£åº¦åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒä¸å˜ï¼Œé‚£ä¹ˆå­¦ç”Ÿçš„æƒé‡æ›´æ–°å°±æ²¡æœ‰å­¦ä¹ ä¿¡å·ã€‚
- en: '![](../Images/65a0d4da56fdaf6c580d5c58625de2aa.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65a0d4da56fdaf6c580d5c58625de2aa.png)'
- en: 'Fig. 7: Analysis of collapsing solutions. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾7ï¼šå´©æºƒè§£å†³æ–¹æ¡ˆåˆ†æã€‚[æ¥æº](https://arxiv.org/abs/2104.14294) + [Sascha Kirch](https://medium.com/@SaschaKirch)çš„æ³¨é‡Š
- en: Effect of Batch Size
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰¹é‡å¤§å°çš„å½±å“
- en: An interesting property is that DINO can be trained with small batch sizes without
    a large drop in performance. This was actually one of BYOLâ€™s motivation, a paper
    DINO builds upon, to be less dependent on batch size compared to contrastive self-supervised
    learning.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæœ‰è¶£çš„ç‰¹æ€§æ˜¯ï¼ŒDINOå¯ä»¥ç”¨è¾ƒå°çš„æ‰¹é‡å¤§å°è¿›è¡Œè®­ç»ƒï¼Œè€Œä¸ä¼šå¤§å¹…ä¸‹é™æ€§èƒ½ã€‚è¿™å®é™…ä¸Šæ˜¯BYOLçš„ä¸€ä¸ªåŠ¨æœºï¼ŒDINOåŸºäºæ­¤è®ºæ–‡ï¼Œå‡å°‘äº†å¯¹æ‰¹é‡å¤§å°çš„ä¾èµ–ï¼Œç›¸æ¯”å¯¹æ¯”è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚
- en: '![](../Images/9e0fa8d51ff10f8d989b51eb03eb122c.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e0fa8d51ff10f8d989b51eb03eb122c.png)'
- en: 'Table 4: Batch size vs. accuracy. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨4ï¼šæ‰¹é‡å¤§å°ä¸å‡†ç¡®ç‡ã€‚[æ¥æº](https://arxiv.org/abs/2104.14294) + [Sascha Kirch](https://medium.com/@SaschaKirch)çš„æ³¨é‡Š
- en: Contrastive methods like CLIP and GLIP provide a lot of negative samples for
    a given positive sample to avoid collapsing solutions. The more negative samples
    per optimizer update step (hence per batch) the better it works.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼CLIPå’ŒGLIPçš„å¯¹æ¯”æ–¹æ³•æä¾›äº†å¤§é‡çš„è´Ÿæ ·æœ¬ä»¥é¿å…å´©æºƒè§£å†³æ–¹æ¡ˆã€‚æ¯æ¬¡ä¼˜åŒ–å™¨æ›´æ–°æ­¥éª¤ï¼ˆå› æ­¤æ¯æ‰¹æ¬¡ï¼‰çš„è´Ÿæ ·æœ¬è¶Šå¤šï¼Œæ•ˆæœè¶Šå¥½ã€‚
- en: Conclusion
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In conclusion, DINO is a knowledge-distillation framework. It is a visual foundation
    model that exploits interesting properties of ViTs and is the predecessor of one
    of todayâ€™s best-performing foundation models, DINOv2\. DINOâ€˜s framework consists
    of a student and teacher model that acts upon different views of the same image
    and adds extra measures to deal with inherent instabilities of similarity-learning
    approaches. The experiments show that DINO outperforms other self-supervised pre-trained
    models on various tasks.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼ŒDINOæ˜¯ä¸€ä¸ªçŸ¥è¯†è’¸é¦æ¡†æ¶ã€‚å®ƒæ˜¯ä¸€ä¸ªè§†è§‰åŸºç¡€æ¨¡å‹ï¼Œåˆ©ç”¨äº†ViTsçš„æœ‰è¶£ç‰¹æ€§ï¼Œå¹¶ä¸”æ˜¯ä»Šå¤©è¡¨ç°æœ€å¥½çš„åŸºç¡€æ¨¡å‹ä¹‹ä¸€DINOv2çš„å‰èº«ã€‚DINOçš„æ¡†æ¶ç”±å­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹ç»„æˆï¼Œä½œç”¨äºç›¸åŒå›¾åƒçš„ä¸åŒè§†å›¾ï¼Œå¹¶é‡‡å–é¢å¤–æªæ–½æ¥å¤„ç†ç›¸ä¼¼æ€§å­¦ä¹ æ–¹æ³•çš„å›ºæœ‰ä¸ç¨³å®šæ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒDINOåœ¨å„ç§ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–è‡ªç›‘ç£é¢„è®­ç»ƒæ¨¡å‹ã€‚
- en: Further Readings & Resources
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»ä¸èµ„æº
- en: Papers
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®ºæ–‡
- en: 'In the meantime an improved version of DINO has been released:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ­¤åŒæ—¶ï¼ŒDINOçš„æ”¹è¿›ç‰ˆæœ¬å·²ç»å‘å¸ƒï¼š
- en: '[DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[DINOv2: åœ¨æ²¡æœ‰ç›‘ç£çš„æƒ…å†µä¸‹å­¦ä¹ é²æ£’çš„è§†è§‰ç‰¹å¾](https://arxiv.org/abs/2304.07193)'
- en: '[Blog post DINOv2 by Meta](https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/)'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Metaçš„DINOv2åšå®¢æ–‡ç« ](https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/)'
- en: Paper Walkthroughs
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®ºæ–‡è§£è¯»
- en: 'You might also like my other paper walkthroughs covering concepts we discussed
    in this article:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½è¿˜ä¼šå–œæ¬¢æˆ‘å…¶ä»–çš„è®ºæ–‡è§£è¯»ï¼Œæ¶µç›–äº†æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­è®¨è®ºçš„æ¦‚å¿µï¼š
- en: '[](/the-clip-foundation-model-7770858b487d?source=post_page-----4cb08e821b18--------------------------------)
    [## The CLIP Foundation Model'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/the-clip-foundation-model-7770858b487d?source=post_page-----4cb08e821b18--------------------------------)
    [## CLIP åŸºç¡€æ¨¡å‹'
- en: Paper Summaryâ€” Learning Transferable Visual Models From Natural Language Supervision
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ€»ç»“â€”ä»è‡ªç„¶è¯­è¨€ç›‘ç£ä¸­å­¦ä¹ å¯è½¬ç§»çš„è§†è§‰æ¨¡å‹
- en: 'towardsdatascience.com](/the-clip-foundation-model-7770858b487d?source=post_page-----4cb08e821b18--------------------------------)
    [](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----4cb08e821b18--------------------------------)
    [## GLIP: Introducing Language-Image Pre-Training to Object Detection'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/the-clip-foundation-model-7770858b487d?source=post_page-----4cb08e821b18--------------------------------)
    [](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----4cb08e821b18--------------------------------)
    [## GLIPï¼šå¼•å…¥è¯­è¨€-å›¾åƒé¢„è®­ç»ƒåˆ°ç›®æ ‡æ£€æµ‹
- en: 'Paper Summary: Grounded Language-Image Pre-training'
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ€»ç»“ï¼š**åŸºäºè¯­å¢ƒçš„è¯­è¨€-å›¾åƒé¢„è®­ç»ƒ**
- en: towardsdatascience.com](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----4cb08e821b18--------------------------------)
    [](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----4cb08e821b18--------------------------------)
    [## BYOL -The Alternative to Contrastive Self-Supervised Learning
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----4cb08e821b18--------------------------------)
    [](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----4cb08e821b18--------------------------------)
    [## BYOL -å¯¹æ¯”è‡ªæˆ‘ç›‘ç£å­¦ä¹ çš„æ›¿ä»£æ–¹æ¡ˆ
- en: 'Paper Analysisâ€” Bootstrap Your Own Latent: A New Approach to Self-Supervised
    Learning'
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®ºæ–‡åˆ†æâ€”**è‡ªæˆ‘ç›‘ç£å­¦ä¹ çš„æ–°æ–¹æ³•**ï¼šBootstrap Your Own Latent
- en: towardsdatascience.com](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----4cb08e821b18--------------------------------)
    [](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?source=post_page-----4cb08e821b18--------------------------------)
    [## Segment Anything â€” Promptable Segmentation of Arbitrary Objects
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----4cb08e821b18--------------------------------)
    [](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?source=post_page-----4cb08e821b18--------------------------------)
    [## Segment Anything â€” å¯æç¤ºçš„ä»»æ„å¯¹è±¡åˆ†å‰²
- en: Paper Walkthrough â€” Segment Anything
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®ºæ–‡è®²è§£â€”Segment Anything
- en: towardsdatascience.com](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?source=post_page-----4cb08e821b18--------------------------------)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?source=post_page-----4cb08e821b18--------------------------------)
