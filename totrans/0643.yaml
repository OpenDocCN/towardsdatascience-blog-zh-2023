- en: Data Integration Strategies for Time Series Databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/data-integration-strategies-for-time-series-databases-f96cab274820](https://towardsdatascience.com/data-integration-strategies-for-time-series-databases-f96cab274820)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exploring popular data integration strategies for TSDBs including ETL, ELT,
    and CDC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://yitaek.medium.com/?source=post_page-----f96cab274820--------------------------------)[![Yitaek
    Hwang](../Images/8e85bfb443a1df284607fb5e5fb2afa5.png)](https://yitaek.medium.com/?source=post_page-----f96cab274820--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f96cab274820--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f96cab274820--------------------------------)
    [Yitaek Hwang](https://yitaek.medium.com/?source=post_page-----f96cab274820--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f96cab274820--------------------------------)
    ·6 min read·Jan 31, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/58b09c28f46fa6ad8943b13548433aec.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [fabio](https://unsplash.com/@fabioha?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: As digital transformation reaches more industries, the number of data points
    generated is growing exponentially. As such, data integration strategies to collect
    such large volumes of data from different sources in varying formats and structures
    are now a primary concern for data engineering teams. Traditional approaches to
    data integration, which have largely focused on curating highly structured data
    into data warehouses, struggle to deal with the volume and heterogeneity of new
    data sets.
  prefs: []
  type: TYPE_NORMAL
- en: Time series data present an additional layer of complexity. By nature, the value
    of each time series data point diminishes over time as the granularity of the
    data loses relevance as it gets stale. So it is crucial for teams to carefully
    plan data integration strategies into time series databases (TSDBs) to ensure
    that the analysis reflects the trends and situation in near real-time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we’ll examine some of the most popular approaches to data
    integration for TSDBs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ETL** (Extract, Transform, Load)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ELT** (Extract, Load, Transform)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Streaming with CDC** (Change Data Capture)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the need for real-time insights for time series data, most modern event-driven
    architectures now implement data streaming with CDC. To illustrate how it works
    in practice, we will walk through a reference implementation with [QuestDB](https://questdb.io/)
    (a fast TSDB) to show that CDC can flexibly handle the needs of a time series
    data source.
  prefs: []
  type: TYPE_NORMAL
- en: Extract, Transform, Load (ETL)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ETL is a traditional and popular data integration strategy that involves first
    transforming the data into a predetermined structure, before loading the data
    into the target system (typically a data warehouse).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7d5a31d9b7a8cd6137b9e2c0d17d0f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: One of the main advantages of ETL is that it provides the highest degree of
    customization. Since the data is first extracted to a staging area where it is
    transformed into a clean, standardized format, ETL systems can handle a wide range
    of formats and structures. Also, once the data is loaded into the data warehouse,
    data science teams can run efficient queries and analyses. Finally, given the
    maturity of the ETL ecosystem, there is a plethora of enterprise-grade tools to
    choose from.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, ETL is both time- and resource-intensive to maintain. The
    logic for sanitizing and transforming the data can be complex and computationally
    expensive. This is why most ETL systems are typically batch-oriented, only loading
    the data into the warehouse periodically. As the volume of data and the sources
    of data grows, this can become a bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: Given these qualities, ETL systems are most used for datasets that require complex
    transformation logic before analysis. It can also work well for datasets that
    do not require real-time insights and can be stored for long-term trend analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Extract, Load, Transform (ELT)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ELT, as the name suggests, loads the data first into the target system (typically
    a data lake) and performs the transformation within the system itself. Given the
    responsibilities of the target system to handle both fast loads and transformations,
    ELT pipelines usually leverage modern, cloud-based data lakes that can deal with
    the processing requirements
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4a554c0daf7d71f8254d42f4ced5db5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Compared to ETL pipelines, ELT systems can provide more real-time analysis of
    the data since raw data is ingested and transformed on the fly. Most cloud-based
    data lakes provide SDKs or endpoints to efficiently ingest data in micro-batches
    and provide almost limitless scalability. However, ELT is not without downsides.
    Since transformation is done by the target system, such operations are limited
    by the capabilities supported by the data lakes. If you need a more complex transformation
    logic, additional steps may be needed to re-extract data and store it in a more
    friendly format.
  prefs: []
  type: TYPE_NORMAL
- en: For most use cases, ELT is a more efficient data integration strategy than ETL.
    If your application can leverage cloud-based tools and does not require complex
    processing, ELT can be a great choice to handle large amounts of data in near
    real-time.
  prefs: []
  type: TYPE_NORMAL
- en: Change Data Capture (CDC)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For new projects, teams can plan to utilize TSDBs as one of the target systems
    in an ETL or ELT pipeline. However, for existing projects, either migrating to
    or adding a TSDB into the mix can be a challenge. For one, work may be required
    to either modify or use a new driver/SDK to stream data into the TSDB. Even if
    the same drivers are supported, data formats may also need to change to fully
    take advantage of TSDB capabilities. CDC tools can be useful to bridge this gap.
  prefs: []
  type: TYPE_NORMAL
- en: 'CDC is simple in principle: CDC tools such as Debezium continuously monitor
    changes in the source system and notify your data pipeline whenever there is a
    change. The application causing the change is often not even aware there is a
    CDC process listening on changes. This makes CDC a good fit for integrating new
    real-time data pipelines into existing architectures because it requires small
    or no changes in the existing applications. As such, CDC can be used in conjunction
    with either ETL or ELT pipelines. For example, the source system can be a SQL
    RDBMS (e.g., MySQL, PostgreSQL, etc) or NoSQL DB (e.g., MongoDB, Casandra) and
    one of the target systems can be a TSDB along with other data lakes or warehouses.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd1e89ed545b347b5ace7779fc175dc8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of using CDC for data integration is that it provides real-time
    data replication. Unlike traditional ETL and ELT systems that work with batches,
    the changes to the source system are continuously streamed into one or more target
    systems. This can be useful to replicate either a subset or the entire data across
    multiple databases in near real-time. The target databases may even be in different
    geographical regions or serve different purposes (i.e., long-term storage vs.
    real-time analytics). For time series data where the changes in value over time
    are often most useful, CDC efficiently captures that delta for real-time insights.
  prefs: []
  type: TYPE_NORMAL
- en: Reference Implementation for CDC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To illustrate how CDC works more concretely, let’s take the [reference implementation](https://github.com/questdb/kafka-questdb-connector)
    I wrote about recently to stream stock prices into QuestDB.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2818fe615a4f7ec0ffd9fba5adeaebec.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, a Java Spring App publishes stock price information into PostgreSQL.
    Debezium Connector then reads the changes from PostgreSQL and publishes them onto
    Kafka. On the other side, QuestDB’s Kafka Connector reads from the Kafka topics
    and streams them onto QuestDB.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a deeper dive, please refer to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://questdb.io/blog/2023/01/03/change-data-capture-with-questdb-and-debezium?source=post_page-----f96cab274820--------------------------------)
    [## Change Data Capture with QuestDB and Debezium | QuestDB: the database for
    time series'
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, our community contributor, Yitaek Hwang, shows us how to stream
    data into QuestDB with change data…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: questdb.io](https://questdb.io/blog/2023/01/03/change-data-capture-with-questdb-and-debezium?source=post_page-----f96cab274820--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: In this reference architecture, the Java Spring App is transforming and loading
    the data onto PostgreSQL before it’s replicated to TSDB for further analysis.
    If a more ELT-like pipeline is desired, raw data from another provider could have
    been loaded directly onto PostgreSQL and transformed later in QuestDB as well.
  prefs: []
  type: TYPE_NORMAL
- en: The important thing to note with this architecture is that CDC can seamlessly
    integrate with existing systems. From the application standpoint, it can retain
    the transactional guarantees of PostgreSQL, while adding a new TSDB component
    down the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data integration plays an important role for organizations that use a TSDB to
    store and analyze time series data. In this article, we looked at some advantages
    and disadvantages of using ETL or ELT. We also examined how CDC can be used in
    conjunction with those pipelines to provide real-time replication into TSDBs.
  prefs: []
  type: TYPE_NORMAL
- en: Given the special qualities of time series data, using a TSDB to properly store
    and analyze them is important. If you are starting fresh, look to build an ELT
    pipeline to stream data into a TSDB. To integrate with an existing system, look
    at utilizing CDC tools to limit the disruption to the current architecture.
  prefs: []
  type: TYPE_NORMAL
