- en: An Introduction to Deep Learning for Sequential Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/an-introduction-to-deep-learning-for-sequential-data-ac966b9b9b67](https://towardsdatascience.com/an-introduction-to-deep-learning-for-sequential-data-ac966b9b9b67)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Highlighting the similarities between time series and NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://donatoriccio.medium.com/?source=post_page-----ac966b9b9b67--------------------------------)[![Donato
    Riccio](../Images/0af2a026e72a023db4635522cbca50eb.png)](https://donatoriccio.medium.com/?source=post_page-----ac966b9b9b67--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ac966b9b9b67--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ac966b9b9b67--------------------------------)
    [Donato Riccio](https://donatoriccio.medium.com/?source=post_page-----ac966b9b9b67--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ac966b9b9b67--------------------------------)
    ·8 min read·Nov 14, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9bd71d38282448e4edea2f653b4701a1.png)'
  prefs: []
  type: TYPE_IMG
- en: How I imagine myself when I load a time series dataset. Image by the author.
    (AI assisted)
  prefs: []
  type: TYPE_NORMAL
- en: Sequential data like time series and natural language require models that can
    capture ordering and context. While time series analysis focuses on forecasting
    based on temporal patterns, natural language processing aims to extract semantic
    meaning from word sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Though distinct tasks, both data types have long-range dependencies where distant
    elements influence predictions. As deep learning has advanced, model architectures
    initially developed for one domain have been adapted to the other.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Time series and natural language have both a sequential structure, where the
    position of an observation in the sequence matters greatly.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9a269b83f4cb166a2c118aed8691e4ea.png)![](../Images/9f6877653538823f1d05aaebe893604c.png)'
  prefs: []
  type: TYPE_IMG
- en: A time series is a sequence of values. (left) Text is a sequence of words. (right)
    Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'A time series is a set of observations over time that are ordered chronologically
    and sampled at fixed time intervals. Some examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: Stock prices every day
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Server metrics every hour
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Temperature readings every second
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key attribute of time series data is that the ordering of observations is
    meaningful. Values nearby in time are usually highly dependent — knowing recent
    values gives insight into predicting the next value. Time series analysis aims
    to model these temporal dependencies to understand patterns and make forecasts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Text data is also sequential — the order of words conveys meaning and context.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: John threw the ball
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ball threw John
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While both sentences contain the same words, their meaning changes entirely
    based on word order. These temporal relationships are represented in language
    models and are the key to natural language tasks like translation and summarization.
  prefs: []
  type: TYPE_NORMAL
- en: Both time series and text exhibit **long-range dependencies —** values far apart
    in the sequence still influence each other. Also, local patterns repeat across
    different locations.
  prefs: []
  type: TYPE_NORMAL
- en: Time series and text representation in neural network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text data need to be converted to embeddings to make them readable from a machine.
  prefs: []
  type: TYPE_NORMAL
- en: Vector representations called embeddings are learned from large datasets to
    capture semantic meaning and relationships between words or data points. The embedding
    vectors encode different semantic properties in each element, representing words/data
    in a dense, low-dimensional way for machine learning models. Embeddings can be
    pre-trained on large corpora and then fine-tuned for specific tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/add12597b4b65fb74b6ec4c16c42731c.png)'
  prefs: []
  type: TYPE_IMG
- en: From words to embeddings. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: There are more things to keep in mind while analyzing time series, like trends
    and seasonality. But when it comes down to how these data are represented in a
    neural network, the difference between text and time series ultimately comes down
    to the fact that time series are a sequence of values, while text is a sequence
    of vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Tasks for sequential data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When examining sequential data, the most intuitive next step would be to predict
    what comes next in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '**In Time series forecasting** you’re trying to predict a continuous value
    (like tomorrow’s stock price or the temperature next week) based on past data.
    The model is trained to minimize the difference between its predictions and the
    actual values, a common characteristic of regression tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text generation** — or, more appropriately, **next-token prediction —** consists
    in training a model to predict the next token given the previous ones. Autoregressive
    language modeling can be viewed as a multi-class classification problem, where
    you can think of each possible token as a separate class. The output is a probability
    distribution over all possible tokens in the vocabulary.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d0d2354dd7912f8cd312ff0ca365622.png)'
  prefs: []
  type: TYPE_IMG
- en: Text classification (sentiment analysis). Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Other tasks involve **sentence classification** — categorizing sentences or
    documents into predefined classes, and **time series classification.**
  prefs: []
  type: TYPE_NORMAL
- en: An example is sentiment analysis, a task where each text is categorized in a
    *Positive* and *Negative* class. Time series can be classified too, for example
    one can classify heartbeats are *Healthy* or *Disease* to detect anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/76161ab75060a0fb57b9033c560bb7fd.png)'
  prefs: []
  type: TYPE_IMG
- en: Time series classification (ECG). Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the models require training on datasets of manually annotated examples
    to learn how to map textual or time series features to categorical labels.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling sequential data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before today’s powerful neural networks were created for time series forecasting
    and natural language processing, different models were typically used for these
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical methods like autoregressive integrated moving average **(ARIMA)**
    and exponential smoothing models were popular for time series forecasting before
    2010s. These rely on mathematical relationships between past values in a time
    series to predict future values. While effective on some data, they make rigid
    assumptions that limit performance on complex real-world time series.
  prefs: []
  type: TYPE_NORMAL
- en: In NLP, tasks like language translation and speech recognition were historically
    addressed using rule-based systems. These encode human-crafted rules and grammar,
    requiring extensive manual effort and struggling with the nuance and variability
    of real human language. Alternately, naive Bayes, logistic regression, and other
    classical machine learning models were sometimes applied but could not effectively
    capture long-term context and dependencies in textual data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38b2d5628855c440a291a0b147fb174f.png)'
  prefs: []
  type: TYPE_IMG
- en: Models for sequential data. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The introduction of **RNN** and **LSTM** networks allowed contextual learning
    for time series forecasting and NLP. Rather than relying on rigid statistical
    assumptions or simple input-output mappings, RNNs can learn long-range dependencies
    from sequential data. This breakthrough enabled them to excel on problems like
    language modeling, sentiment analysis, and non-linear forecasting where classical
    approaches wouldn’t work. Though introduced in the 1980s, these models have only
    become practical in the last decade, as computational power has dramatically increased.
    Google started using LSTM in Google Voice in 2015\. [1]
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RNNs contain recursive connections that allow information to persist across
    timesteps.
  prefs: []
  type: TYPE_NORMAL
- en: When working on forecasting, the RNN can be trained on past observations from
    a time series to learn the temporal patterns. The RNN processes the sequence by
    updating its hidden state at each time step based on the current input and previous
    hidden state.
  prefs: []
  type: TYPE_NORMAL
- en: For next token prediction, the RNN is trained on textual sequences like sentences
    where each token is a word. The RNN learns to predict the next word based on the
    previous words. The hidden state maintains the context of earlier words to inform
    the next prediction. At each step, the RNN outputs a probability distribution
    over the next token.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ea1cb73f038ecc40b2030344a36e3f4.png)![](../Images/1cda611cdeffde3dbe484bf67a0748fd.png)'
  prefs: []
  type: TYPE_IMG
- en: An LSTM model can be used both for forecasting and next-word prediction. Image
    by author.
  prefs: []
  type: TYPE_NORMAL
- en: The ability of RNNs to remember past context has been transformational for sequence
    tasks in NLP and time series analysis. However, they can struggle with long-term
    dependencies due to issues like vanishing and exploding gradients. This issue
    motivated architectural advances like **LSTMs** to improve gradient flow across
    many timesteps and has been further enhanced using attention-based models.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Attention** mechanisms made possible all the amazing LLMs we know today.
    They were initially introduced to augment RNNs by allowing models to focus on
    relevant parts of the input sequence when making predictions. Attention functions
    score the importance of each timestep and use these weights to extract relevant
    context.'
  prefs: []
  type: TYPE_NORMAL
- en: Attention has become an indispensable component for sequence tasks across NLP
    and time series modeling. It improves model accuracy and interpretability by focusing
    on relevant inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d36cd0e2b60f90673e27409a09c55aec.png)'
  prefs: []
  type: TYPE_IMG
- en: The original Transformer architecture. Positional encoding allows the capture
    of the order of the sequence. [2]
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer architecture relying entirely on self-attention has led to breakthrough
    results in NLP and time series modeling. The self-attention layers allow the modeling
    of dependencies regardless of the distance between sequence elements as long as
    the sequence fits in the context length.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers have become state-of-the-art for sequential data, **with this architecture
    adapted to NLP as BERT and time series as Temporal Fusion Transformer.**
  prefs: []
  type: TYPE_NORMAL
- en: Towards Foundation Models for Time Series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **foundation model** is a large machine learning model that can be trained
    on vast amount of data and then adapted to various tasks. Foundation models differ
    from traditional machine learning models, which usually perform specific tasks.
    They are more general and flexible and can be used as a starting point for developing
    more specialized applications. Avoiding expensive training from scratch can significantly
    reduce the time and cost of building new applications.
  prefs: []
  type: TYPE_NORMAL
- en: In NLP, Large Language Models allow ***in-context learning*** — they can perform
    new tasks they weren’t explicitly trained for. This revolutionary capability makes
    ChatGPT and other LLMs so powerful, as they can generalize to a wide variety of
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Most current forecasting approaches must be individually fit to each new dataset.
    This process is time-consuming and requires domain expertise. To address this
    problem, the concept of foundation models has lately been applied to time series
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4199e7c68d4e012ff1e51eaec8e63fd3.png)'
  prefs: []
  type: TYPE_IMG
- en: TimeGPT is pre-trained on massive datasets and can generate new data. [3]
  prefs: []
  type: TYPE_NORMAL
- en: '**TimeGPT i**s a Transformer-based neural network pre-trained on a diverse
    dataset of over 100 billion time series data points encompassing domains like
    economics, weather, transport, retail sales, etc. The key innovation is that,
    like GPT-3, TimeGPT can generalize to make accurate forecasts on new time series
    data without retraining on each new dataset. This *zero-shot* ability provides
    immense time and resource savings compared to traditional forecasting pipelines.
    A foundation model simplifies forecasting to a single model that can be applied
    to any time series with just a few lines of code. [2]'
  prefs: []
  type: TYPE_NORMAL
- en: Takeaways
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When doing deep learning, think outside the box. Data and models have more in
    common than they appear — everything is interconnected. Both time series analysis
    and NLP are rapidly innovating and sharing ideas.
  prefs: []
  type: TYPE_NORMAL
- en: Time series and NLP share many parallels as sequential data types. We model
    both with architectures such as RNN, LSTM, and Transformers. As deep learning
    advances, we expect techniques to continue crossing over between these domains.
  prefs: []
  type: TYPE_NORMAL
- en: The 2010s were the decade of neural networks conquering domains once dominated
    by statistical models. The 2020s look set to be the decade of transformers cementing
    their dominance, and researchers continue pushing the boundaries of these formidable
    models.
  prefs: []
  type: TYPE_NORMAL
- en: '*Enjoyed this article? Get weekly data science interview questions delivered
    to your inbox by subscribing to my newsletter,* [*The Data Interview*](https://thedatainterview.substack.com/)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Also, you can find me on* [*LinkedIn*](https://www.linkedin.com/in/driccio/)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [Long short-term memory — Wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2][[1706.03762] Attention Is All You Need (arxiv.org)](https://arxiv.org/abs/1706.03762)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [[2310.03589] TimeGPT-1 (arxiv.org)](https://arxiv.org/abs/2310.03589?ref=emergentmind)'
  prefs: []
  type: TYPE_NORMAL
