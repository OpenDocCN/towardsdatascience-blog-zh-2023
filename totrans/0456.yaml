- en: Building Pipelines In Apache Airflow - For Beginners
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/building-pipelines-in-apache-airflow-for-beginners-58f87a1512d5](https://towardsdatascience.com/building-pipelines-in-apache-airflow-for-beginners-58f87a1512d5)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A quick and simple demo for running DAGs on Airflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@aashishnair?source=post_page-----58f87a1512d5--------------------------------)[![Aashish
    Nair](../Images/23f4b3839e464419332b690a4098d824.png)](https://medium.com/@aashishnair?source=post_page-----58f87a1512d5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----58f87a1512d5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----58f87a1512d5--------------------------------)
    [Aashish Nair](https://medium.com/@aashishnair?source=post_page-----58f87a1512d5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----58f87a1512d5--------------------------------)
    ·9 min read·Mar 15, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4fbe919cb98598fa62ac516da138073a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Kelly Sikkema](https://unsplash.com/@kellysikkema?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Apache Airflow is quite popular in the data science and data engineering space.
    It boasts many features that enable users to programmatically create, manage,
    and monitor complex workflows.
  prefs: []
  type: TYPE_NORMAL
- en: However, the platform’s range of features may inadvertently become a detriment
    to beginners. New users that explore Apache Airflow’s documentation and tutorials
    can easily become inundated by new terminology, tools, and concepts.
  prefs: []
  type: TYPE_NORMAL
- en: With the aim of creating a more digestible introduction to this platform, we
    offer a bare-bones version of an Apache Airflow demo that entails coding and running
    an Airflow pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Terminology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It will be easier to follow the demo after becoming familiar with the following
    Airflow terminology.
  prefs: []
  type: TYPE_NORMAL
- en: '**DAG:** A DAG, short for directed acyclic graph, is what Airflow uses to represent
    a workflow. DAGs are composed of nodes that denote tasks and arrows that denote
    the relationships between the tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/4ced24b547d325303b4af3419555b16a.png)'
  prefs: []
  type: TYPE_IMG
- en: Example DAG (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Task:** A task is a single unit of work within a DAG. Tasks in a DAG
    can be linked together by dependencies, ensuring that they are executed in a specific
    order.'
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Operator:** Operators are tools used to instantiate the tasks in a DAG.
    Airflow offers a plethora of operators that can perform basic jobs, such as executing
    Python code, bash commands, and SQL queries.'
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Metadata Database:** The metadata database stores metadata about the
    Airflow pipelines that the users create and run.'
  prefs: []
  type: TYPE_NORMAL
- en: '**5\. Webserver:** The webserver is a convenient UI that enables users to run,
    monitor, and debug pipelines.'
  prefs: []
  type: TYPE_NORMAL
- en: '**6\. Scheduler:** The scheduler is responsible for monitoring DAGs and running
    tasks when their dependencies are met.'
  prefs: []
  type: TYPE_NORMAL
- en: Demo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of the demo is to create a DAG with Python and get it running on Airflow.
    Since we are prioritizing simplicity, we will create a DAG that only consists
    of two tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The first task runs a Python function named `pull_jokes`, which procures random
    jokes from the [official joke API](https://github.com/15Dkatz/official_joke_api)
    and stores them in a text file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second task runs a bash command that prints a single sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we want the second task (i.e., bash command) to be executed right
    after the first task (i.e., Python function). If we were to visualize this DAG,
    it would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c00bb65a67db2ad82e0197a5c0112356.png)'
  prefs: []
  type: TYPE_IMG
- en: Demo DAG (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: Pretty simple, right? Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: Part 1 — Setting Up Airflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we can begin creating any DAGs, we will need to set up Airflow on our
    machine, which will require the use of the command line interface.
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways to install Apache Airflow, as explained in the [documentation](https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html).
    Here, we will perform the installation with the PyPI route.
  prefs: []
  type: TYPE_NORMAL
- en: '**Activate Windows Subsystem for Linux (For Windows Users)**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you’re working with Windows, you will need to activate Windows Subsystem
    for Linux (WSL), which will require installing Ubuntu first.
  prefs: []
  type: TYPE_NORMAL
- en: To activate WSL, enter the `wsl` command in PowerShell.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Create a virtual environment**'
  prefs: []
  type: TYPE_NORMAL
- en: Next, create the virtual environment in which we will be working. Enter the
    following command to install python-venv.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For the demo, we will create a virtual environment named “airflow_venv” and
    activate it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f42ccc28d2d7c1fa8fdb4ee9e3202e67.png)'
  prefs: []
  type: TYPE_IMG
- en: Activate Virtual Environment (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Select the home directory for Airflow (optional)**'
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to know where the Airflow project will be stored, as this is
    where the created workflows should be stored and configured.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, the home directory for Airflow will be the `~/airflow` directory.
    However, users can change the home directory for Airflow using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: For the case study, the project will be stored in the `~/airflowhome` directory.
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Install Apache Airflow**'
  prefs: []
  type: TYPE_NORMAL
- en: 'After confirming the home directory, install Apache Airflow using pip with
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This installation will create a project in the home directory with the following
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c5d572ad0a24fb128dd0b479d742bbab.png)'
  prefs: []
  type: TYPE_IMG
- en: Airflow Home Directory (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: The file named `airflow.cfg` contains all of Airflow’s configurations. It includes
    a parameter named `dags_folder`, which shows the path of the folder where all
    created DAGs *must* be located.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the currently assigned directory by entering the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`vim airflow.cfg`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c6a2fa637293999b8efb865aed86198.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: As shown by the output, all airflow DAGs for this demo have to be in the `dags`
    subdirectory in the `airflowhome` directory.
  prefs: []
  type: TYPE_NORMAL
- en: So, we will create a folder named `dags` in the same directory, which will contain
    the subsequently created pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Note: You can also change the path assigned to the `*dags_folder*` parameter
    in the airflow.cfg file to the path of your liking.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The project structure should be updated to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**5\. Initialize the metadata database**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, initialize the database with the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: After initializing the database, the airflow home directory should now have
    a file named `airflow.db`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/37d6655933bb2e834954d517db334e0f.png)'
  prefs: []
  type: TYPE_IMG
- en: Airflow Home Directory (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: '**6\. Create a user.**'
  prefs: []
  type: TYPE_NORMAL
- en: A user account is needed to access the webserver, so we can create one using
    the following command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: For the case study, we will be creating an admin account.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/14fed9c9a3ec0504fcf456f03f252550.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating a User (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a user account is created, it can be verified using the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8d107cb3b974b69f8abfd574d1e33060.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: '**7\. Start the web server and scheduler**'
  prefs: []
  type: TYPE_NORMAL
- en: The Airflow webserver can be opened with a one-liner.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Note: the default port is 8080, but you can choose a different port.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In a new terminal, start the scheduler with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '**9\. Login to the web server**'
  prefs: []
  type: TYPE_NORMAL
- en: After starting the webserver, visit “[http://localhost:8080/](http://localhost:8080/)”
    (or whichever port you choose) in a web browser, and you will be directed to the
    login page, which is where you will enter the login credentials from your newly
    created user account.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/83e11f0dffb65f825020998d0a465d79.png)'
  prefs: []
  type: TYPE_IMG
- en: Login Page (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: After entering your details, you will be directed to the home page.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7adf1824ac6b0001301fe5dc1b46e039.png)'
  prefs: []
  type: TYPE_IMG
- en: Webserver (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: At the moment, there will only be samples DAGs provided by Airflow, but the
    webserver will also contain your DAGs once they are created.
  prefs: []
  type: TYPE_NORMAL
- en: Part 2 — Creating the DAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have Airflow set up, we can build the DAG using Python. The DAG
    created in this demo will be called “pulling_jokes_dag”.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Python script named `pull_jokes.py` creates a DAG instance as
    well as the tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s a small snippet of code, but there is a lot going on. Let’s break everything
    down step by step.
  prefs: []
  type: TYPE_NORMAL
- en: '**Create a DAG instance**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, we create a DAG instance, in which we determine the configuration of
    the DAG.
  prefs: []
  type: TYPE_NORMAL
- en: 'A DAG instance must have assigned values for two parameters: `dag_id` and `start_date`.
    The `dag_id` parameter is the unique identifier of the DAG, while the `start_date`
    parameter is the date the DAG is set to be scheduled.'
  prefs: []
  type: TYPE_NORMAL
- en: To keep things simple, we will only specify a few parameters. The `schedule`
    parameter defines the rules for when the DAG should be run. The `end_date` parameter
    indicates when the DAG runs should stop running. The `catchup` parameter indicates
    whether the scheduler should start a DAG run for any data interval that has not
    been run since the last data interval. The `tag` parameter assigns tags to the
    DAG, which will make it easier to find in the UI.
  prefs: []
  type: TYPE_NORMAL
- en: For those interested in all available parameters for DAG instances, feel free
    to visit the Airflow [documentation](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/dag/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Create the first task**'
  prefs: []
  type: TYPE_NORMAL
- en: After creating the DAG instance, we can create the first task, which runs the
    `pull_jokes` function, by using a [Python Operator](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html).
  prefs: []
  type: TYPE_NORMAL
- en: The `task_id` parameter is the unique identifier for the task, whereas the `python_callable`
    parameter contains the function that should be executed.
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Create the second task**'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create the second task, which executes the bash command, using the
    [Bash Operator](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/bash.html).
  prefs: []
  type: TYPE_NORMAL
- en: Once again, the `task_id` parameter is the unique identifier for the task, whereas
    the `bash_command` parameter contains the bash script that should be executed.
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Establish dependencies**'
  prefs: []
  type: TYPE_NORMAL
- en: After creating the tasks, the dependencies need to be set up to determine the
    order in which the tasks are executed.
  prefs: []
  type: TYPE_NORMAL
- en: We can set task 2 to be executed *after* task 1 by using the `>>` operator.
  prefs: []
  type: TYPE_NORMAL
- en: Part 3 — Running the DAG on Airflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the Python script for the DAG is created, the next step is to run the DAG
    on Airflow.
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, the DAG script *must* be placed in the location specified in
    the `airflow.cfg` file. The `pull_jokes.py` file used for this case study should
    be in this location.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c6a2fa637293999b8efb865aed86198.png)'
  prefs: []
  type: TYPE_IMG
- en: DAGs directory (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'After moving the `pull_jokes.py` file to the correct location, the project
    directory tree should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, Airflow should have access to the DAG. To confirm this, simply enter the
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We should now be able to view the newly added DAG in the webserver. Start the
    webserver with the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In a separate terminal, start the scheduler
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: You should be able to see the DAG named “pulling_jokes_dag” in the UI.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/855937da6b3b9b509c67088cdc7c6094.png)'
  prefs: []
  type: TYPE_IMG
- en: DAG in Webserver (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Pro tip: Although it’s optional, it’s best to assign tags to your DAGs. This
    will make them easier to find in the webserver.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Technically speaking, DAGs can be managed using the command line interface alone,
    but the features in the web server make it much easier to access and monitor the
    created pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the features include, but are not limited to:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A breakdown of the DAG in the form of a grid:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/6f643dcbedc72faa7406db3767612042.png)'
  prefs: []
  type: TYPE_IMG
- en: Grid View (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: '**A breakdown of the DAG in the form of a graph**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/a6b891390bf49296a1037bc21d992d9d.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph View (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: '**The underlying Python code for the DAG**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/5927e333bfe4235973ac523bf7fde95d.png)'
  prefs: []
  type: TYPE_IMG
- en: Code (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: '**The audit log**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/9506444453c5e3fad536d8b1b1172471.png)'
  prefs: []
  type: TYPE_IMG
- en: Audit Log (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: Once Airflow has access to the DAG, it will perform DAG runs based on the provided
    schedule.
  prefs: []
  type: TYPE_NORMAL
- en: A DAG can also be run manually by clicking the “play button” in the top right
    of the web server.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7419f3018372c13c9cabc5116b61cdf.png)'
  prefs: []
  type: TYPE_IMG
- en: Run DAG (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, a DAG can be manually triggered using the CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Voila! Just like that, we have our DAG up and running on Airflow!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/84653505509c3468ec21803d5b42b311.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Prateek Katyal](https://unsplash.com/es/@prateekkatyal?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, this has been a useful primer for those looking to get hands-on experience
    with Apache Airflow.
  prefs: []
  type: TYPE_NORMAL
- en: This demo is by no means a comprehensive demonstration of Airflow’s features
    and capabilities (we have barely scratched the surface), but it should be able
    to help people get started, which oftentimes is the hardest part.
  prefs: []
  type: TYPE_NORMAL
- en: After building and running your first DAG, you will have established a solid
    foundation, which will make it easier to design and run more complex workflows
    with a more frictionless experience.
  prefs: []
  type: TYPE_NORMAL
- en: I wish you the best of luck in your data science endeavors!
  prefs: []
  type: TYPE_NORMAL
