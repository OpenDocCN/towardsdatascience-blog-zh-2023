- en: 'Unlock the Power of Audio Data: Advanced Transcription and Diarization with
    Whisper, WhisperX, and PyAnnotate'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解锁音频数据的潜力：使用 Whisper、WhisperX 和 PyAnnotate 进行高级转录和语音分段
- en: 原文：[https://towardsdatascience.com/unlock-the-power-of-audio-data-advanced-transcription-and-diarization-with-whisper-whisperx-and-ed9424307281](https://towardsdatascience.com/unlock-the-power-of-audio-data-advanced-transcription-and-diarization-with-whisper-whisperx-and-ed9424307281)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/unlock-the-power-of-audio-data-advanced-transcription-and-diarization-with-whisper-whisperx-and-ed9424307281](https://towardsdatascience.com/unlock-the-power-of-audio-data-advanced-transcription-and-diarization-with-whisper-whisperx-and-ed9424307281)
- en: Streamline Audio Analysis with State-of-the-Art Speech Recognition and Speaker
    Attribution Technologies
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用最先进的语音识别和说话人归属技术简化音频分析
- en: '[](https://medium.com/@luisroque?source=post_page-----ed9424307281--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----ed9424307281--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ed9424307281--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ed9424307281--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----ed9424307281--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@luisroque?source=post_page-----ed9424307281--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----ed9424307281--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ed9424307281--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ed9424307281--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----ed9424307281--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ed9424307281--------------------------------)
    ·9 min read·Apr 17, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ed9424307281--------------------------------)
    ·阅读时间 9 分钟·2023年4月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In our fast-paced world, we generate enormous amounts of audio data. Think about
    your favorite podcast or conference calls at work. The data is already rich in
    its raw form; we, as humans, can understand it. Even so, we could go further and,
    for example, convert it into a written format to search for it later.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们快节奏的世界中，我们生成了大量的音频数据。想想你最喜欢的播客或工作中的会议电话。数据在原始形式中已经非常丰富，我们作为人类可以理解它。即便如此，我们仍然可以进一步处理，例如将其转换为书面格式以便后续搜索。
- en: To better understand the task at hand, we are introducing two concepts. The
    first is transcription, simply converting spoken language into text. A second
    one that we explore in this article is diarization. Diarization helps us give
    additional structure to the unstructured content. In this case, we are interested
    in attributing specific speech segments to different speakers.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解当前任务，我们引入了两个概念。第一个是转录，即将口语转化为文本。第二个是我们在本文中探讨的**语音分段**。语音分段帮助我们为无结构的内容提供额外的结构。在这种情况下，我们关注的是将特定的语音片段归属于不同的说话人。
- en: With the above context, we address both tasks by using different tools. We use
    Whisper, a general-purpose speech recognition model developed by OpenAI. It was
    training on a diverse dataset of audio samples, and the researchers developed
    it to perform multiple tasks. Secondly, we use PyAnnotate, a library for speaker
    diarization. Finally, we use WhisperX, a research project that helps combine the
    two while solving some limitations of Whisper.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述背景下，我们通过使用不同的工具来解决这两个任务。我们使用了 Whisper，这是一个由 OpenAI 开发的通用语音识别模型。它在各种音频样本数据集上进行了训练，研究人员开发了它以执行多种任务。其次，我们使用了
    PyAnnotate，这是一个用于说话人归属的库。最后，我们使用了 WhisperX，这是一个研究项目，旨在结合这两者，同时解决 Whisper 的一些局限性。
- en: '![](../Images/20859fc38b69b3c538c6e1338a5e81ab.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20859fc38b69b3c538c6e1338a5e81ab.png)'
- en: 'Figure 1: Speak to the machines ([source](https://unsplash.com/photos/6dDHofabCQ8)).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：与机器对话（[来源](https://unsplash.com/photos/6dDHofabCQ8)）。
- en: 'This article belongs to “Large Language Models Chronicles: Navigating the NLP
    Frontier”, a new weekly series of articles that will explore how to leverage the
    power of large models for various NLP tasks. By diving into these cutting-edge
    technologies, we aim to empower developers, researchers, and enthusiasts to harness
    the potential of NLP and unlock new possibilities.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文属于“**大型语言模型纪实：探索 NLP 前沿**”系列文章，该系列每周更新，旨在探讨如何利用大型模型的力量来完成各种 NLP 任务。通过深入了解这些前沿技术，我们的目标是赋能开发者、研究人员和爱好者，充分发挥
    NLP 的潜力，开启新的可能性。
- en: 'Articles published so far:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 目前已发布的文章：
- en: '[Summarizing the latest Spotify releases with ChatGPT](https://medium.com/towards-data-science/summarizing-the-latest-spotify-releases-with-chatgpt-553245a6df88)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[用 ChatGPT 总结最新的 Spotify 发布](https://medium.com/towards-data-science/summarizing-the-latest-spotify-releases-with-chatgpt-553245a6df88)'
- en: '[Master Semantic Search at Scale: Index Millions of Documents with Lightning-Fast
    Inference Times using FAISS and Sentence Transformers](https://medium.com/towards-data-science/master-semantic-search-at-scale-index-millions-of-documents-with-lightning-fast-inference-times-fa395e4efd88)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[掌握大规模语义搜索：使用 FAISS 和 Sentence Transformers 快速索引数百万文档](https://medium.com/towards-data-science/master-semantic-search-at-scale-index-millions-of-documents-with-lightning-fast-inference-times-fa395e4efd88)'
- en: As always, the code is available on my [Github](https://github.com/luisroque/large_laguage_models).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，代码可在我的 [Github](https://github.com/luisroque/large_laguage_models) 上获得。
- en: 'Whisper: A General-Purpose Speech Recognition Model'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Whisper: 一个通用的语音识别模型'
- en: Whisper is a general-purpose speech recognition model performing very well in
    various speech-processing tasks. It is reasonably robust at multilingual speech
    recognition, speech translation, spoken language identification, and voice activity
    detection.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 是一个通用的语音识别模型，在各种语音处理任务中表现非常好。它在多语言语音识别、语音翻译、口语语言识别和语音活动检测方面相当稳健。
- en: At the core of Whisper lies a Transformer sequence-to-sequence model. The model
    jointly represents various speech-processing tasks as a sequence of tokens to
    be predicted by the decoder. The model can replace multiple stages of a traditional
    speech-processing pipeline by employing special tokens as task specifiers or classification
    targets. We can think of it as a meta-model for speech-processing tasks.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 的核心是一个 Transformer 序列到序列模型。该模型将各种语音处理任务共同表示为由解码器预测的令牌序列。该模型通过使用特殊令牌作为任务说明符或分类目标，取代了传统语音处理管道的多个阶段。我们可以将其视为语音处理任务的元模型。
- en: Whisper comes in five model sizes, targeting edge devices or large computing
    machines. It allows users to select the appropriate model for their use case and
    the capacity of their systems. Note that the English-only versions of some models
    perform better for English use cases.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 有五种模型大小，针对边缘设备或大型计算机。它允许用户根据他们的使用情况和系统的容量选择合适的模型。请注意，一些仅支持英语的版本在处理英语用例时表现更好。
- en: 'PyAnnotate: Speaker Diarization Library'
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'PyAnnotate: 说话人分离库'
- en: Speaker diarization is the process of identifying and segmenting speech by different
    speakers. The task can be beneficial, for example, when we are analyzing data
    from a call center, and we want to separate the customer and the agent’s voices.
    Companies can then use it to improve customer service and ensure company policy
    compliance.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 说话人分离是通过不同说话人识别和分割语音的过程。这项任务在分析呼叫中心的数据时非常有用，例如，当我们想要分开客户和代理的声音时。公司可以利用它来改善客户服务并确保公司政策的合规性。
- en: PyAnnotate is a Python library specifically designed to support this task. The
    process is relatively simple. It preprocesses the data, allowing us to extract
    features from the raw audio file. Next, it produces clusters of similar speech
    segments based on the extracted features. Finally, it attributes the generated
    clusters to individual speakers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: PyAnnotate 是一个专门设计用于支持这项任务的 Python 库。该过程相对简单。它对数据进行预处理，允许我们从原始音频文件中提取特征。接下来，它根据提取的特征生成相似的语音段集群。最后，它将生成的集群分配给不同的说话人。
- en: 'WhisperX: Long-Form Audio Transcription with Voice Activity Detection and Forced
    Phoneme Alignment'
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'WhisperX: 带有语音活动检测和强制音素对齐的长音频转录'
- en: As we saw in the previous sections, Whisper is a large-scale and weakly supervised
    model trained to perform several tasks in the speech-processing field. While it
    performs well in different domains and even in different languages it falls short
    when it comes to long audio transcription. The limitation comes from the fact
    that the training procedure uses a sliding window approach, which can result in
    drifting or even hallucination. In addition, it has severe limitations when it
    comes to aligning the transcription with the audio timestamps. This is particularly
    important to us when performing the speaker diarization.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面部分看到的，Whisper 是一个大规模的、弱监督的模型，训练用于在语音处理领域执行多个任务。虽然它在不同领域甚至不同语言中表现良好，但在长音频转录时却有所不足。其限制在于训练过程使用了滑动窗口方法，这可能导致漂移甚至幻觉。此外，它在对齐转录与音频时间戳时也存在严重限制。在进行说话人分离时，这一点尤为重要。
- en: 'To tackle these limitations, an Oxford research group is actively developing
    WhisperX. The Arxiv pre-print paper was published last month. It uses Voice Activity
    Detection (VAD), which detects the presence or absence of human speech and pre-segments
    the input audio file. It then cuts and merges these segments into windows of approximately
    30 seconds by defining the boundaries on the regions where there is a low probability
    of speech (yield from the voice model). This step has an additional advantage:
    it allows using batch transcriptions with Whisper. It increases performance while
    reducing the probability of drifting or hallucination we discussed above. The
    final step is called forced alignment. WhisperX uses a phoneme model to align
    the transcription with the audio. Phoneme-based Automatic Speech Recognition (ASR)
    recognizes the smallest unit of speech, e.g., the element “g” in “big.” This post-processing
    operation aligns the generated transcription with the audio timestamps at the
    word level.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些限制，牛津大学的一个研究小组正在积极开发WhisperX。Arxiv预印本论文上个月发布。它使用语音活动检测（VAD），检测人声的存在与否，并对输入音频文件进行预分段。然后，它将这些片段裁剪并合并成大约30秒的窗口，通过定义在语音概率较低的区域（从语音模型得出的）上界限来实现。这一步还有一个额外的好处：允许使用批量转录与Whisper配合。这提高了性能，同时减少了我们上面讨论的漂移或幻觉的概率。最后一步称为强制对齐。WhisperX使用音素模型将转录与音频对齐。基于音素的自动语音识别（ASR）识别语音的最小单位，例如“big”中的“g”元素。这种后处理操作将生成的转录与音频时间戳在单词级别对齐。
- en: Integrating WhisperX, Whisper, and PyAnnotate
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整合WhisperX、Whisper和PyAnnotate
- en: In this section, we integrate WhisperX, Whisper, and PyAnnotate to create our
    own ASR system. We designed our approach to handle long-form audio transcriptions
    while being able to segment the speech and attribute a specific speaker to each
    segment. In addition, it reduces the probability of hallucination, increases inference
    efficiency, and ensures proper alignment between the transcription and the audio.
    Let’s build a pipeline to perform the different tasks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们整合了WhisperX、Whisper和PyAnnotate来创建我们自己的ASR系统。我们设计了我们的方法，以处理长形式的音频转录，同时能够分段语音并为每个片段分配特定的说话人。此外，它减少了幻觉的概率，提高了推理效率，并确保转录与音频之间的正确对齐。让我们构建一个流程来执行不同的任务。
- en: We start with transcription, converting the speech recognized from the audio
    file into written text. The `transcribe`function loads a Whisper model specified
    by `model_name` and transcribes the audio file. It then returns a dictionary containing
    the transcript segments and language code. OpenAI designed Whisper also to perform
    language detection, being a multilanguage model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从转录开始，将从音频文件中识别出的语音转换为书面文本。`transcribe`函数加载由`model_name`指定的Whisper模型并转录音频文件。它然后返回一个包含转录片段和语言代码的字典。OpenAI设计的Whisper还具有语言检测功能，是一个多语言模型。
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we align the transcript segments using the `align_segments` function.
    As we discussed previously, this step is essencial for accurate speaker diarization,
    as it ensures that each segment corresponds to the correct speaker:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`align_segments`函数对齐转录片段。如前所述，这一步对准确的说话人分离至关重要，因为它确保每个片段对应到正确的说话人：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'With the transcript segments aligned, we can now perform speaker diarization.
    We use the `diarize` function, which leverages the PyAnnotate library:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐转录片段后，我们可以进行说话人分离。我们使用`diarize`函数，它利用了PyAnnotate库：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After diarization, we assign speakers to each transcript segment using the
    `assign_speakers` function. It is the final step in our pipeline and completes
    our process of transforming the raw audio file into a transcript with speaker
    information:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在分离说话人后，我们使用`assign_speakers`函数为每个转录片段分配说话人。这是我们流程中的最后一步，完成了将原始音频文件转换为包含说话人信息的转录文本的过程：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Finally, we combine all the steps into a single `transcribe_and_diarize` function.
    This function returns a list of dictionaries representing each transcript segment,
    including the start and end times, spoken text, and speaker identifier. Note that
    you need a Hugging Face API token to run the pipeline.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将所有步骤合并到一个`transcribe_and_diarize`函数中。该函数返回一个字典列表，表示每个转录片段，包括开始和结束时间、发言文本以及说话人标识符。请注意，你需要一个Hugging
    Face API令牌才能运行该流程。
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Assessing the Performance of the Integrated ASR System
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估集成ASR系统的性能
- en: 'Let’s start by testing our pipeline with a short audio clip that I recorded
    myself. There are two speakers in the video that we need to identify. Also, notice
    the several hesitations in the speech of one of the speakers, making it hard to
    transcribe. We will use the *base* model from Whisper to assess its capabilities.
    For better accuracy, you can use the *medium* or *large* ones. The transcription
    is presented below:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从测试我自己录制的一个短音频片段开始。视频中有两个发言者，我们需要识别它们。此外，注意到其中一个发言者在讲话中有多个犹豫，使得转录变得困难。我们将使用Whisper的*base*模型来评估其能力。为了更好的准确性，你可以使用*medium*或*large*模型。转录文本如下：
- en: 'Segment 1:'
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 段落 1：
- en: 'Start time: 0.95'
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 开始时间：0.95
- en: 'End time: 2.44'
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 结束时间：2.44
- en: 'Speaker: SPEAKER_01'
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 发言者：SPEAKER_01
- en: 'Transcript: What TV show are you watching?'
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 转录文本：你在看什么电视节目？
- en: ''
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Segment 2:'
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 段落 2：
- en: 'Start time: 3.56'
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 开始时间：3.56
- en: 'End time: 5.40'
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 结束时间：5.40
- en: 'Speaker: SPEAKER_00'
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 发言者：SPEAKER_00
- en: 'Transcript: Currently I’m watching This Is Us.'
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 转录文本：目前我在看《我们这一天》。
- en: ''
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Segment 3:'
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 段落 3：
- en: 'Start time: 6.18'
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 开始时间：6.18
- en: 'End time: 6.93'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 结束时间：6.93
- en: 'Speaker: SPEAKER_01'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 发言者：SPEAKER_01
- en: 'Transcript: What is it about?'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 转录文本：内容是什么？
- en: ''
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Segment 4:'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 段落 4：
- en: 'Start time: 8.30'
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 开始时间：8.30
- en: 'End time: 15.44'
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 结束时间：15.44
- en: 'Speaker: SPEAKER_00'
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 发言者：SPEAKER_00
- en: 'Transcript: It is about the life of a family throughout several generations.'
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 转录文本：这是关于一个家庭在几代人中的生活。
- en: ''
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Segment 5:'
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 段落 5：
- en: 'Start time: 15.88'
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 开始时间：15.88
- en: 'End time: 21.42'
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 结束时间：21.42
- en: 'Speaker: SPEAKER_00'
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 发言者：SPEAKER_00
- en: 'Transcript: And you can somehow live their lives through the series.'
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 转录文本：你可以通过这个系列某种程度上体验他们的生活。
- en: ''
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Segment 6:'
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 段落 6：
- en: 'Start time: 22.34'
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 开始时间：22.34
- en: 'End time: 23.55'
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 结束时间：23.55
- en: 'Speaker: SPEAKER_01'
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 发言者：SPEAKER_01
- en: 'Transcript: What will be the next one?'
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 转录文本：下一步会是什么？
- en: ''
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Segment 7:'
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 段落 7：
- en: 'Start time: 25.48'
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 开始时间：25.48
- en: 'End time: 28.81'
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 结束时间：28.81
- en: 'Speaker: SPEAKER_00'
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 发言者：SPEAKER_00
- en: 'Transcript: Maybe beef I’ve been hearing very good things about it.'
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 转录文本：也许是牛肉，我听说关于它的评价很好。
- en: ''
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Execution time for base: 8.57 seconds'
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 基础模型的执行时间：8.57秒
- en: 'Memory usage for base: 3.67GB'
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 基础模型的内存使用：3.67GB
- en: Our approach achieves its main goals with the transcription above. First, notice
    that the transcription is accurate and that we could ignore the speech hesitations
    successfully. We produced text with the correct syntax, which helps readability.
    The segments were well separated and correctly aligned with the audio timestamps.
    Finally, the speaker diarization was also executed adequately, with the two speakers
    attributed accurately to each speech segment.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法通过上述转录实现了其主要目标。首先，注意到转录是准确的，我们成功地忽略了说话中的犹豫。我们生成了语法正确的文本，这有助于可读性。段落被很好地分隔，并且与音频时间戳对齐。最后，发言者分离也得到了妥善执行，两个发言者被准确地分配到每个语音段落。
- en: 'Another important aspect is the computation efficiency of the various models
    on long-format audio when running inference on CPU and GPU. We selected an audio
    file of around 30 minutes. Below, you can find the results:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要方面是各种模型在长格式音频上的计算效率，特别是在 CPU 和 GPU 上运行推断时。我们选择了一个约 30 分钟的音频文件。以下是结果：
- en: '![](../Images/d22c0e5b35642a811e90fae90ad26fc3.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d22c0e5b35642a811e90fae90ad26fc3.png)'
- en: 'Figure 2: Execution time for the various models using CPU and GPU (Image by
    Author).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：使用 CPU 和 GPU 的各种模型的执行时间（图片由作者提供）。
- en: The main takeaway is that these models are very heavy and need to be more efficient
    to run at scale. For 30 minutes of video, we take around 70–75 minutes to run
    the transcriptions on the CPU and approximately 15 minutes on GPU. Also, remember
    that we need about 10GB of VRAM to run the large model. We should expect these
    results since the models are still in the research phase.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 主要结论是这些模型非常庞大，需要更高效才能在大规模运行。对于 30 分钟的视频，我们在 CPU 上需要大约 70–75 分钟来完成转录，而在 GPU 上则大约需要
    15 分钟。此外，请记住，我们需要大约 10GB 的 VRAM 来运行大型模型。考虑到这些模型仍处于研究阶段，我们应该预期这些结果。
- en: Conclusion
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This article provides a comprehensive step-by-step guide to analyzing audio
    data using state-of-the-art speech recognition and speaker diarization technologies.
    We introduced Whisper, PyAnnotate, and WhisperX, forming a powerful integrated
    ASR system together — our approach produces promising results when working with
    long-form audio transcriptions. It also solves the main limitations of Whisper,
    hallucinating on long-form audio transcriptions, ensuring alignment between transcription
    and audio, accurately segmenting the speech, and attributing speakers to each
    segment.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了使用最先进的语音识别和说话人分离技术分析音频数据的全面逐步指南。我们介绍了 Whisper、PyAnnotate 和 WhisperX，这些工具共同形成了一个强大的集成
    ASR 系统——我们的方案在处理长格式音频转录时产生了有希望的结果。它还解决了 Whisper 在长格式音频转录中出现的主要限制问题，包括确保转录与音频对齐、准确分割语音以及将说话人归属到每个片段。
- en: Nonetheless, the computational efficiency of these models remains a challenge,
    particularly for long-format audio and when running inference on limited hardware.
    Even so, the integration of Whisper, WhisperX, and PyAnnotate demonstrates the
    potential of these tools to transform the way we process and analyze audio data,
    unlocking new possibilities for applications across various industries and use
    cases.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，这些模型的计算效率仍然是一个挑战，尤其是对于长格式音频以及在有限硬件上运行推理时。即便如此，Whisper、WhisperX 和 PyAnnotate
    的集成展示了这些工具在处理和分析音频数据方面的潜力，为各种行业和用例带来了新的应用可能性。
- en: About me
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于我
- en: Serial entrepreneur and leader in the AI space. I develop AI products for businesses
    and invest in AI-focused startups.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: serial entrepreneur 和 AI 领域的领导者。我开发面向企业的 AI 产品，并投资于专注于 AI 的初创公司。
- en: '[Founder @ ZAAI](http://zaai.ai) | [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)
    | [X/Twitter](https://x.com/luisbrasroque)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[创始人 @ ZAAI](http://zaai.ai) | [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)
    | [X/Twitter](https://x.com/luisbrasroque)'
