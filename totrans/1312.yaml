- en: Implement and Train a CNN from Scratch with PyTorch Lightning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/implement-and-train-a-cnn-from-scratch-with-pytorch-lightning-ce22f7dfad83](https://towardsdatascience.com/implement-and-train-a-cnn-from-scratch-with-pytorch-lightning-ce22f7dfad83)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you are not using PyTorch Lightning, you should give it a try.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@betty.ld?source=post_page-----ce22f7dfad83--------------------------------)[![Betty
    LD](../Images/1a908f5bcdb6cbc3be5f889f52d743e6.png)](https://medium.com/@betty.ld?source=post_page-----ce22f7dfad83--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ce22f7dfad83--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ce22f7dfad83--------------------------------)
    [Betty LD](https://medium.com/@betty.ld?source=post_page-----ce22f7dfad83--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ce22f7dfad83--------------------------------)
    ·19 min read·Aug 8, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c624d26acc13295bc8fc840a43add42.png)'
  prefs: []
  type: TYPE_IMG
- en: The abstract idea of PyTorch Lightning. From [Marc Sendra Martorell](https://unsplash.com/@marcsm).
  prefs: []
  type: TYPE_NORMAL
- en: This article is a gentle introduction to Convolution Neural Networks (CNNs).
    This article details why PyTorch Lightning is so great, then makes a brief theoretical
    walkthrough of CNN components, and then describes the implementation of a training
    loop for a simple CNN architecture coded from scratch using the PyTorch Lightning
    library.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why PyTorch Lightning?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyTorch is a flexible and user-friendly library. If PyTorch is great for research,
    I found Lightning even greater for engineering. The main advantages are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Less code.** When running an ML project, so many things can go wrong that
    it is helpful to delegate the boilerplate code and focus on what is relevant for
    solving my specific problem. Using the built-in functionalities is reducing the
    amount of written code, and therefore the probability of bugs. The development
    (and debugging) time is reduced.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Well-structured code**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency and fast training**. Lightning also allows the use of all the
    multiprocessing and parallel workers tricks (like [DDP](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html))
    from PyTorch without coding an extra line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Built-in **development tools** like sanity checks (for validation and training
    loops as well as for the model architecture), creation of overfit dataset on the
    fly, early stopping callbacks, best weights management, etc. For example [https://lightning.ai/docs/pytorch/stable/debug/debugging_basic.html](https://lightning.ai/docs/pytorch/stable/debug/debugging_basic.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more official reasons why it is great, check out [there](https://pytorch-lightning.readthedocs.io/en/0.10.0/introduction_guide.html#why-pytorch-lightning).
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, when using PyTorch Lightning I found it easy to code, easy to
    read, and easy to debug. These activities consist of what takes most of my time
    as a Machine Learning Engineer. Moreover, the documentation is well-written and
    contains many tutorials so it is also easy to [learn](https://lightning.ai/docs/pytorch/stable/levels/core_skills.html).
  prefs: []
  type: TYPE_NORMAL
- en: CNN models refresher
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LeNet is a good starting point when learning or rehearsing deep learning architectures
    for computer vision. LeNet was the first successful Convolutional Neural Network
    (CNN) architecture designed by Yann LeCun et al. in 1998, designed for handwritten
    digit recognition.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll explain first the main components of the standard CNN module through the
    components of LeNet architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'LeNet is composed of three types of layers:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully Connected layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1\. Convolutional Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The convolutional layers are responsible for extracting features from an input
    layer. With CNN, this first “layer” is often an image. Each convolutional layer
    consists of a set of learnable **filters** (also called “kernels”) that slide
    over the input layer, applying an operation called “convolution”. The convolution
    performs an element-wise multiplication and summation between the filter and the
    local area in the image. Since a (nonlinear) **activation function** is applied
    on the output feature maps, this output is also called “activation”. In this article,
    we use the most popular activation function: ReLU, shown in blue below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b53e41645b4b10b07065d17a70413a68.png)'
  prefs: []
  type: TYPE_IMG
- en: Rectified Linear Unit (ReLU) f(x)=max(0,x) and its variant GeLU. [Image under
    Creative Common License](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#/media/File:ReLU_and_GELU.svg).
  prefs: []
  type: TYPE_NORMAL
- en: Each convolutional layer is followed by an activation function mainly in order
    to add non-linearity. Without such the model would behave like a regular linear
    model regardless of its depth.
  prefs: []
  type: TYPE_NORMAL
- en: More about convolutional layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next section explains how the convolution layers are used in CNN and how
    they enable CNN to perform extremely well on a range of computer vision tasks.
    In a nutshell, the **hierarchical** nature of successive convolutional layers
    makes it so efficient for image recognition tasks. One convolutional layer is
    not efficient, but a stack of them is.
  prefs: []
  type: TYPE_NORMAL
- en: The first layers enable the model to simple and generic patterns in a local
    area of the input image and then, the deeper layers grasp more complex and abstract
    representations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f08fc6fe0e3d4d48fb3e3d67813b752d.png)'
  prefs: []
  type: TYPE_IMG
- en: The convolutional [operation is illustrated](https://commons.wikimedia.org/wiki/File:Convolution_arithmetic_-_Full_padding_no_strides.gif).
    The filter size is 3x3, the stride is 1 and there is zero padding amount is 2\.
    Image under MIT license.
  prefs: []
  type: TYPE_NORMAL
- en: The **first convolutional layers** of a CNN have typically a small **spatial
    extent** (for example 3x3 pixels for the first convolutional layer of VGG16, 5x5
    pixels for LeNet, 11x11 for AlexNet).
  prefs: []
  type: TYPE_NORMAL
- en: After training, these layers would detect simple patterns like edges and corners
    similar to [classical computer vision technique](https://www.cs.toronto.edu/~urtasun/courses/CV/lecture02.pdf)s.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the filters become more sophisticated in the **intermediate convolutional
    layers**. Their relative size to the input layer is bigger than earlier layers,
    therefore they have more context (as they see a broader part of the image than
    the previous convolutional layer) and they can start detecting higher-level features
    like complex textures, shapes, and object parts.
  prefs: []
  type: TYPE_NORMAL
- en: For example in an image of a [Japanese set meal](https://kome-academy.com/en/teishoku_library/),
    these layers may detect the chopsticks, the rice bowl, and the miso soup.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/daead1a1554228f49844911728e308ef.png)'
  prefs: []
  type: TYPE_IMG
- en: '[LeNet architecture](https://www.researchgate.net/figure/LeNet-is-a-classic-convolutional-neural-network-employing-the-use-of-convolutions_fig1_369803371).
    Image under Creative Common license.'
  prefs: []
  type: TYPE_NORMAL
- en: In the **last convolutional layers**, the filters have an even larger spatial
    extent (they can see more context at once in the input layer) and are more specialized
    and abstract than the generic earlier edge, color, and object parts’ detector
    layers. They represent high-level features and are essential to make an accurate
    decision in image recognition tasks. They are used to encode complex object representations
    and capture the distinctive characteristics of the different classes in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: It can for example recognize a Japanese set meal from a French lunch.
  prefs: []
  type: TYPE_NORMAL
- en: The following picture summarizes what each layer can “see”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4df73b66459f78b7555a71345c3cac25.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of the output feature maps at different convolution stages. By
    [Stanford CS231](https://cs231n.github.io/convolutional-networks/). Image from
    [https://github.com/cs231n/cs231n.github.io/blob/master/convolutional-networks.md](https://github.com/cs231n/cs231n.github.io/blob/master/convolutional-networks.md)
    under MIT license.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is common to add a pooling layer between two successive convolutional layers.
    The pooling layer downsamples the feature map obtained from the previous convolutional
    layer reducing the spatial dimension of the data while retaining the import information.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why are pooling layers effective?**'
  prefs: []
  type: TYPE_NORMAL
- en: They downsize the feature maps and therefore allow faster computation and lower
    memory requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This information aggregation step reduces the number of parameters in the model
    and also prevents overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They introduce a degree of translation invariance such that the network can
    recognize certain features even though they are slightly shifted, rotated, or
    distorted in the image. Being robust to spatial variation helps the model to generalize
    better.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most common pooling operations are MAX pooling (select the maximum value
    from the element covered by the window as the value for this window) and AVERAGE
    pooling (same but takes the average value instead).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/165816e82847faf5c3a030a9c351fdc1.png)'
  prefs: []
  type: TYPE_IMG
- en: Pooling operation. On the feature map on the left and on a feature slice on
    the right. Image from [Stanford CS231.](https://cs231n.github.io/convolutional-networks/)
    [MIT license](https://github.com/cs231n/cs231n.github.io/blob/master/convolutional-networks.md).
  prefs: []
  type: TYPE_NORMAL
- en: Fully Connected Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Fully Connected (FC) layers are the last layers of the model. They are responsible
    for making high-level decisions based on the extracted features from earlier layers.
    Unlike the convolutional filters that have local visibility of the input layer,
    the FC layers connect all the activations at once from the previous output feature
    maps to the next output maps activation, as seen in regular Neural Networks. It
    basically consists of a matric multiplication followed by a bias offset.
  prefs: []
  type: TYPE_NORMAL
- en: In the LeNet model, there are three FC layers at the very end of the model.
    The last layer is an FC layer used for the classification task. Its dimension
    is (pevious_layer_output_dimension, number_of_classes).
  prefs: []
  type: TYPE_NORMAL
- en: Other classical architectures such as AlexNet, VGG, ResNet, and Inception include
    an FC layer at the end of the architecture. However, recent architectures dropped
    this layer like MobileNet, YOLO, EfficientNet, and Vision Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reasons to drop the FC from CNN architectures are:'
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the number of parameters (preventing overfitting, especially on smaller
    datasets).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FC discards the spatial information present in the feature maps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flexibility. CNN architecture without FC can handle input of varying sizes,
    and avoid resizing the input image to a fixed size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generic CNN architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most common form of a CNN architecture is a stack of {Convolutional layer+non
    linear activation function} layers followed by {Pooling layers} applied successively
    until the image has been spatially downsized to a smaller size with more channels.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, it is common to have the final FC layer outputting the class scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, a CNN is structured as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'With usually ([source](https://cs231n.github.io/convolutional-networks/)):'
  prefs: []
  type: TYPE_NORMAL
- en: 0 ≤ N ≤ 3 stacked convolutions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: M≥0 pooled blocks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0≤ K< 3 FC stacked layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model constraints ([source](https://cs231n.github.io/convolutional-networks/))
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is usually recommended to ML practitioners to use existing state-of-the-art
    architectures rather than creating their own. However, it is important to be aware
    of the spatial constraints when using convolutional nets. For example, after applying
    a convolution layer with W as the input layer size (width or height), F as the
    filter size, P as the padding size, and S as the stride size, the output feature
    maps have a size Output size = ((W -F + 2P) / S ) +1\. For each convolution, the
    parameters W, F, P and S need to be chosen such as the output size is an integer.
    Usually adding padding solves most issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other common constraints are:'
  prefs: []
  type: TYPE_NORMAL
- en: The input image should be divisible by 2 many times, depending on the depth
    of the model. This requirement comes from the pooling layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The convolutional layers should use small filters and small strides.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using “same-size” padding is recommended. If we keep the same feature size before
    and after applying a convolution, we delegate all the downsizing operations to
    the pooling layers and the architecture becomes easier to understand.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeNet model implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enough theory, now we’ll implement LetNet CNN with PyTorch Lightning. LeNet
    has been chosen as an example due to its simplicity and its small size.
  prefs: []
  type: TYPE_NORMAL
- en: Model implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In PyTorch, a new module inherits from a `[pytorch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#module)`.
    In PyTorch Lighthing, the model class inherits from `ligthning.pytorch.LightningModule`.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the `ligthning.pytorch.LightningModule` exactly as the `nn.Module`
    class, it just includes more functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model’s module takes two arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of input channels (which is 1 for grayscale images).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of classes in the classifier (which is 10 for the MNIST dataset).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In PyTorch, the model is described in two parts `__init__()` and `forward()`.
    `__init__()` declares every component having learnable parameters as the initialization
    method. It optionally also contains more declarations like activation functions.
    Then, the forward method `forward()` applies all the layers and functions successively
    on the input image.
  prefs: []
  type: TYPE_NORMAL
- en: LeNet architecture is composed of two stacked convolutional blocks, each followed
    by a Pooling layer. Then the result is given to successive FC layers which output
    a tensor of size (batch_size, out_channels) where `out_channels` represents the
    number of classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the implementation block below, some miscellaneous attributes are first
    initialized:'
  prefs: []
  type: TYPE_NORMAL
- en: The `example_input_array` tensor used for [displaying a simulation of the tensor
    size](https://lightning.ai/docs/pytorch/stable/debug/debugging_basic.html) between
    each layer when running `print(model)`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/ce02dbe770a43ff01540d989c373967f.png)'
  prefs: []
  type: TYPE_IMG
- en: Automatic model logging when running `print(model)`. Image from author.
  prefs: []
  type: TYPE_NORMAL
- en: From the table above, we can confirm that the output tensor has the size (batch_size=16,
    num_classes=10).
  prefs: []
  type: TYPE_NORMAL
- en: 2\. The`Accuracy()` metric will be used during training and validation.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. The layers with learnable parameters are initialized as well. First the
    two{Convolutional + Max Pool} blocks and then the Fully Connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Notes about the implementation above**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Notes about the convolution layers**'
  prefs: []
  type: TYPE_NORMAL
- en: In order to simplify the forward call, it is common to represent the stacked
    layers inside a `nn.Sequential()` submodule.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first conv layer takes (32x32) sized images and outputs a (16x16) sized
    image after dividing the size by 2 in the pooling layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeNet expects an input image size of (32x32) but the available MNIST dataset
    has images of size (28x28). You can either resize the image or increase the padding
    size of the first convolutional layer (as written in the comment). Otherwise,
    the convolutional layers can take variable-size images but after two downsampling
    there is a dimension mismatch between the output activation from the last conv
    layer and the matrix multiplication (which dimension is fixed as shown below)
    of the first FC layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The second convolutional layer takes as input the same number of channels as
    the first conv has output filters (6).
  prefs: []
  type: TYPE_NORMAL
- en: '**The order ReLU and MaxPool does not matter here.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ReLU activations are not called before pooling, unlike it was mentioned
    in the earlier section. In this implementation, the ReLU activations functions
    are called in the `forward()`call only.
  prefs: []
  type: TYPE_NORMAL
- en: A convolutional layer should always be followed by an activation function in
    order to add non-linearity. But if the convolutional layer is also followed by
    a Pooling layer, the order does not matter. Both operations are commutative MaxPool(Relu(x))
    = Relu(MaxPool(x)). Indeed, we can take the maximum of a local patch and set 0
    for all the negative values or set 0 for all the negative values and take the
    maximum of each local patch.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note about the FC layers.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first FC layer takes as a tensor of size (number_output_filter_from_conv2
    * previous_activation_width * previous_activation_height). The size of the output
    activation is gradually decreasing through the three FC layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'All these layers are called during the forward pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The backward function which computes the gradient is automatically defined when
    using `autograd` .
  prefs: []
  type: TYPE_NORMAL
- en: In most PyTorch implementations, the last layer (also called the softmax layer
    sometimes) outputs the raw activation values where each number corresponds to
    a score. Here the softmax function is not called in the forward pass but is built
    in the [Cross-Entropy loss function](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the training, validation, and test steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the same file as before, under the `class LeNet(pl.LightningModule)` we override
    all the core functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'the optimizer and scheduler: `configure_optimizers()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'the training loop: `training_step()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'the validation loop: `validation_step()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you may notice, the functions above are pretty short. There is no need to
    move the variable to `to(device)` , to delete the gradients with `optimizer.zero_grad()`
    or compute the new gradients with `loss.backward()` . Switching the model mode
    is also handled by the PyTorch Lightning library itself `model.eval()` ,`model.train()`
    .
  prefs: []
  type: TYPE_NORMAL
- en: You can notice that the `log()` method is called here. This method saves and
    displays the results when appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to customize it, the [documentation](https://pytorch-lightning.readthedocs.io/en/0.10.0/introduction_guide.html#logging)
    explains well how to use the logging correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `log()` method has a few options:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- on_step (logs the metric at that step in training)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- on_epoch (automatically accumulates and logs at the end of the epoch)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- prog_bar (logs to the progress bar)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- logger (logs to the logger like Tensorboard)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Depending on where log is called from, Lightning auto-determines the correct
    mode for you. But of course you can override the default behavior by manually
    setting the flags
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Another good feature of PyTorch Lightning is the Validation Sanity check:'
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed the words **Validation sanity check** logged. This is because
    Lightning runs 2 batches of validation before starting to train. This is a kind
    of unit test to make sure that if you have a bug in the validation loop, you won’t
    need to potentially wait a full epoch to find out.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Finally, the methods for testing and prediction are also implemented under
    the same class:'
  prefs: []
  type: TYPE_NORMAL
- en: 'the test loop: `test_step()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'the prediction loop: `predict_step()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model can either load its weights from a checkpoint or if called after a
    training loop, the model fetchs automatically the weights from the last or best
    (if a callback has been implemented) epochs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Managing the MNIST dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use either the regular PyTorch DataLoader class or a PyTorch Lightning
    DataModule. In this article, I implemented the dataset and dataloading with the
    PyTorch Lightning DataModule. It aims at centralizing all the information relative
    to one dataset within one single file. It includes data downloading, data splitting,
    data loading, etc.
  prefs: []
  type: TYPE_NORMAL
- en: For this tutorial, we use MNIST made of images of size 28x28\. The MNIST dataset
    is made available under the terms of the [Creative Commons Attribution-Share Alike
    3.0 license](https://creativecommons.org/licenses/by-sa/3.0/) ([source](https://keras.io/api/datasets/mnist/)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/335b143ea684822c964c16b1e9b765c8.png)'
  prefs: []
  type: TYPE_IMG
- en: MNIST dataset visualization. [Image under CC license](https://en.wikipedia.org/wiki/MNIST_database#/media/File:MnistExamplesModified.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the implementation of the data module to manage the MNIST dataset.
    It includes setting the standard parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Path to the data directory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensor transforms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As well as downloading and processing functionality in `prepare_data()` `setup()`
    .
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'When starting training, the following functions are called in this order:'
  prefs: []
  type: TYPE_NORMAL
- en: The DataModule `prepare_data()` and `setup()` methods. The method `prepare_data()`
    runs on one CPU and it is used for downloading the data locally. Whereas the method`setup()`
    is a parallel process that can run data processing jobs. These methods are called
    each time when calling a method from the trainer like `trainer.fit()`, `trainer.validate()`,
    etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pl.LightningModule `configure_optimizers()` initializes the optimizer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, under the same class `MNISTDataModule`we implement the different dataloaders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The DataModule `train_dataloader()` retrieves the training DataLoader.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pl.LightningModule `training_step()` runs a forward and backward pass on
    a mini-batch obtained from the training DataLoader. The method is repeatedly called
    until all samples from the training DataLoader are seen once.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pl.LightningModule `validation_step()` computes the loss and the metrics
    on the validation dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training stops once the max number of epochs has been hit or if the validation
    loss stops decreasing (early stop).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the training loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, the only missing piece is the training script itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training script includes:'
  prefs: []
  type: TYPE_NORMAL
- en: Parsing CLI arguments and calling the main function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The main function includes the model selection, the creation of an early stopping
    callback, and the calls to the trainer for training `trainer.fit(model, datamodule=data_module)`,
    validating `trainer.validate(datamodule=data_module)`, testing `trainer.test(datamodule=data_module)`,
    and predicting `output_preds = trainer.predict(datamodule=data_module, ckpt_path=”best”)`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The function to save predicted images (mostly for debugging).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Altogether with the imports and constant declarations, the script looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Results**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Running `python -m train --early-stopping`for 10 epochs of training (batch size
    of 64) takes less than two minutes on my machine with a GPU NVIDIA GeForce RTX
    3070.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the training reaches the default max_epochs (10), PyTorch Lightning outputs
    the loss and accuracy results on the validation and test datasets respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cb93f5e958f8ae8ea6e5cc233d669683.png)![](../Images/e575a76d7f7db272a93622747b292ad3.png)'
  prefs: []
  type: TYPE_IMG
- en: Result of the model on unseen data after 10 epochs of training.
  prefs: []
  type: TYPE_NORMAL
- en: The trained model obtains almost 99% of accuracy on the unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The script saves 10 images from the test set as well as the predicted class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/54dfd8fc9be7924ac5569c5a6b4a456d.png)'
  prefs: []
  type: TYPE_IMG
- en: Predicted as a 6
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we have discovered the magic of PyTorch Lightning, we have
    then rehearsed the key technical concepts of CNNs and we have walked through a
    complete implementation of a training loop for a simple CNN architecture from
    scratch.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this introduction-level article was helpful in your journey to implement
    a basic architecture fast and reliably and helped you build a stronger foundation
    in your learning journey. You can check my public deep-learning repository for
    more content [https://github.com/bledem/deep-learning](https://github.com/bledem/deep-learning).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stanford Computer Vision classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Andrew NG [https://www.youtube.com/watch?v=c1RBQzKsDCk&list=PLpFsSf5Dm-pd5d3rjNtIXUHT-v7bdaEIe&index=115&ab_channel=DeepLearningAI](https://www.youtube.com/watch?v=c1RBQzKsDCk&list=PLpFsSf5Dm-pd5d3rjNtIXUHT-v7bdaEIe&index=115&ab_channel=DeepLearningAI)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch Lightning docs [https://www.pytorchlightning.ai/tutorials](https://www.pytorchlightning.ai/tutorials)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
