- en: What are Query, Key, and Value in the Transformer Architecture and Why Are They
    Used?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer 架构中的 Query、Key 和 Value 是什么？它们为什么被使用？
- en: 原文：[https://towardsdatascience.com/what-are-query-key-and-value-in-the-transformer-architecture-and-why-are-they-used-acbe73f731f2](https://towardsdatascience.com/what-are-query-key-and-value-in-the-transformer-architecture-and-why-are-they-used-acbe73f731f2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/what-are-query-key-and-value-in-the-transformer-architecture-and-why-are-they-used-acbe73f731f2](https://towardsdatascience.com/what-are-query-key-and-value-in-the-transformer-architecture-and-why-are-they-used-acbe73f731f2)
- en: An analysis of the intuition behind the notion of Key, Query, and Value in Transformer
    architecture and why is it used.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对 Transformer 架构中 Key、Query 和 Value 概念背后的直觉进行分析，以及为何使用它们。
- en: '[](https://ebrahimpichka.medium.com/?source=post_page-----acbe73f731f2--------------------------------)[![Ebrahim
    Pichka](../Images/8add6e8e875d9e921caf7f5eaa77d545.png)](https://ebrahimpichka.medium.com/?source=post_page-----acbe73f731f2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----acbe73f731f2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----acbe73f731f2--------------------------------)
    [Ebrahim Pichka](https://ebrahimpichka.medium.com/?source=post_page-----acbe73f731f2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ebrahimpichka.medium.com/?source=post_page-----acbe73f731f2--------------------------------)[![Ebrahim
    Pichka](../Images/8add6e8e875d9e921caf7f5eaa77d545.png)](https://ebrahimpichka.medium.com/?source=post_page-----acbe73f731f2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----acbe73f731f2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----acbe73f731f2--------------------------------)
    [Ebrahim Pichka](https://ebrahimpichka.medium.com/?source=post_page-----acbe73f731f2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----acbe73f731f2--------------------------------)
    ·10 min read·Oct 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----acbe73f731f2--------------------------------)
    ·阅读时间 10 分钟·2023年10月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/36bd08a0784d8f29ebfaca4463809896.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36bd08a0784d8f29ebfaca4463809896.png)'
- en: Image by author — generated by [**Midjourney**](https://www.midjourney.com/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像 — 由 [**Midjourney**](https://www.midjourney.com/) 生成
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Recent years have seen the Transformer architecture make waves in the field
    of natural language processing (NLP), achieving state-of-the-art results in a
    variety of tasks including machine translation, language modeling, and text summarization,
    as well as other domains of AI i.e. Vision, Speech, RL, etc.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，Transformer 架构在自然语言处理（NLP）领域引起了轰动，在机器翻译、语言建模、文本摘要等多种任务中取得了最先进的成果，并且在其他 AI
    领域（如视觉、语音、强化学习等）也有所应用。
- en: Vaswani et al. (2017), first introduced the transformer in their paper *“Attention
    Is All You Need”*, in which they used the self-attention mechanism without incorporating
    recurrent connections while the model can focus selectively on specific portions
    of input sequences.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Vaswani 等人（2017年）首次在他们的论文*“Attention Is All You Need”*中介绍了 Transformer，其中他们使用了自注意力机制，而没有加入递归连接，同时模型可以选择性地关注输入序列的特定部分。
- en: '![](../Images/5c8d9226a835a3ef8723261c98b8ff73.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c8d9226a835a3ef8723261c98b8ff73.png)'
- en: 'The Transformer model architecture — Image from the Vaswani et al. (2017) paper
    (Source: [**arXiv:1706.03762v7**](https://arxiv.org/abs/1706.03762v7))'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型架构 — 来自 Vaswani 等人（2017年）论文的图像（来源：[**arXiv:1706.03762v7**](https://arxiv.org/abs/1706.03762v7)）
- en: In particular, previous sequence models, such as recurrent encoder-decoder models,
    were limited in their ability to capture long-term dependencies and parallel computations.
    In fact, right before the Transformers paper came out in 2017, state-of-the-art
    performance in most NLP tasks was obtained by using RNNs with an attention mechanism
    on top, so attention kind of existed before transformers. By introducing the multi-head
    attention mechanism on its own, and dropping the RNN part, the transformer architecture
    resolves these issues by allowing multiple independent attention mechanisms.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，之前的序列模型，例如递归编码器-解码器模型，在捕捉长期依赖关系和并行计算方面存在限制。实际上，在 2017 年 Transformer 论文发布之前，大多数
    NLP 任务的最先进性能是通过在 RNN 上使用注意力机制获得的，所以注意力在 Transformer 之前就已存在。通过引入多头注意力机制，并去掉 RNN
    部分，Transformer 架构通过允许多个独立的注意力机制解决了这些问题。
- en: In this post, we will go over one of the details of this architecture, namely
    the Query, Key, and Values, and try to make sense of the intuition used behind
    this part.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将深入探讨该架构的一个细节，即查询（Query）、键（Key）和值（Value），并尝试理解这部分所使用的直觉。
- en: Note that this post assumes you are already familiar with some basic concepts
    in NLP and deep learning such as **embeddings**, **Linear (dense) layers**, and
    in general how a simple neural network works.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这篇文章假设你已经熟悉一些基本的自然语言处理和深度学习概念，如**嵌入**、**线性（密集）层**，以及一般的简单神经网络的工作原理。
- en: Attention!
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意！
- en: First, let’s start understanding what the attention mechanism is trying to achieve.
    And for the sake of simplicity, let’s start with **a simple case** of sequential
    data to understand what problem exactly we are going to solve, without going through
    all the jargon of the *attention mechanism*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们了解注意力机制试图实现什么。为了简化起见，我们从**一个简单的例子**开始，了解我们要解决的具体问题，而不深入讨论*注意力机制*的所有术语。
- en: Context Matters
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文很重要
- en: Consider the case of **smoothing time-series data**. Time series are known to
    be one of the most basic kinds of sequential data due to the fact that it is already
    in a numerical and structured form, and is usually in low-dimensional space. So
    it would be suitable to lay out a good starting example.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑**平滑时间序列数据**的情况。时间序列已知是最基本的序列数据之一，因为它已经是数值化和结构化的形式，并且通常处于低维空间。因此，提出一个好的起始示例是合适的。
- en: To smooth a highly variant time series, a common technique is to calculate a
    “**weighted average”** of the proximate timesteps for each timestep, as shown
    in image 1, the weights are usually chosen based on how close the proximate timesteps
    are to our desired timestep. For instance, in Gaussian Smoothing, these weights
    are drawn from a Gaussian function that is centered at our current step.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了平滑一个高度变化的时间序列，一种常见的技术是计算每个时间步的“**加权平均值**”，如图1所示，权重通常基于相邻时间步距离目标时间步的远近来选择。例如，在高斯平滑中，这些权重是从以当前步为中心的高斯函数中提取的。
- en: '![](../Images/16b29fa0cd09627df288a00c2d248be8.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16b29fa0cd09627df288a00c2d248be8.png)'
- en: time-series smoothing example — Image by author.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列平滑示例 — 作者提供的图像。
- en: 'What we have done here, in a sense, is that:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种意义上说，我们所做的是：
- en: We took **a sequence of values**,
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们取了**一系列值**，
- en: And for each step of this sequence, we **added (a weighted) context** from its
    proximate values, while the proportion of added context (**the weight)** is only
    related to their **proximity** to the target value.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于序列中的每一步，我们**添加了（加权的）上下文**来自其邻近值，而添加的上下文（**权重**）仅与它们的**接近度**相关。
- en: And finally, we attained a new **contextualized** sequence, which we can understand
    and analyze more easily.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们获得了一个新的**上下文化**的序列，这使得我们可以更容易地理解和分析。
- en: '**There are two key points/issues in this example:**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**这个示例中有两个关键点/问题：**'
- en: It only uses the **proximity** and **ordinal position** of the values to obtain
    the weights of the context.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它仅使用**接近度**和**顺序位置**的值来获取上下文的权重。
- en: The weights are calculated by fixed arbitrary rules for all points.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重是通过对所有点应用固定的任意规则来计算的。
- en: The Case of Language
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言的案例
- en: 'In machine learning, textual data always have to be represented by vectors
    of real-valued numbers AKA **Embeddings**. So we assume that the primary meanings
    of tokens (or words) are encoded in these vectors. Now in the case of textual
    sequence data, if we would like to apply the same kind of technique to **contextualize**
    each token of the sequence as the above example so that each token’s new embedding
    would contain more information about its context, we would encounter some issues
    which we will discuss now:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，文本数据通常需要用实值向量（即**嵌入**）来表示。因此，我们假设标记（或单词）的主要含义被编码在这些向量中。现在，对于文本序列数据，如果我们想要像上述示例那样对序列中的每个标记进行**上下文化**，使每个标记的新嵌入包含更多关于其上下文的信息，我们将遇到一些问题，接下来我们将讨论这些问题：
- en: Firstly, in the example above, we **only** used the **proximity of tokens**
    to determine the importance (weights) of the context to be added, while **words
    do not work like that.**
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在上面的示例中，我们**仅仅**使用了**标记的接近度**来确定要添加的上下文的重要性（权重），而**单词的工作方式并非如此**。
- en: In language, the context of a word in a sentence is not based only on the ordinal
    distance and proximity. We can’t just blindly use proximity to incorporate context
    from other words.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言中，单词在句子中的上下文不仅仅基于顺序距离和接近度。我们不能仅仅依赖接近度来融入其他单词的上下文。
- en: Secondly, adding the context only by taking the (weighted) average of the embeddings
    of the context tokens itself may not be entirely intuitive. A token’s embedding
    may contain information about different syntactical, semantical, or lexical aspects
    of that token. All of this information may not be relevant to the target token
    to be added. So it’s better not to add all the information as a whole as context.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，仅通过对上下文标记的嵌入的（加权）平均值来添加上下文可能不够直观。一个标记的嵌入可能包含有关该标记的不同句法、语义或词汇方面的信息。所有这些信息可能与要添加的目标标记无关。因此，最好不要将所有信息作为整体添加为上下文。
- en: So if we have some (vector) representation of words in a sequence, **how** do
    we **obtain the weights** and **the relevant context** to re-weight and contextualize
    each token of the sequence?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果我们有一些（向量）表示序列中的词，**我们如何获得权重**和**相关的上下文**以重新加权和上下文化序列中的每个标记？
- en: The answer, in a broad sense, is that we have to “**search”** for it, based
    on some specific aspects of the tokens meaning (could be semantic, syntactic,
    or anything). And during this **search**, assign the **weights** and the **context
    information** based on relevance or importance.
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从广义上讲，答案是我们必须基于标记意义的某些特定方面（可以是语义的、句法的或其他任何方面）“**搜索**”它。在此**搜索**过程中，根据相关性或重要性分配**权重**和**上下文信息**。
- en: ''
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It means that for each of the tokens in a sequence, we have to **go through
    all other tokens** in the sequence, and assign them **weights** and the **context
    information,** based on a similarity metric that we use to compare our target
    token with others. **The more similar they are in terms of the desired context,
    the larger the weight it gets**.
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这意味着，对于序列中的每个标记，我们必须**遍历序列中的所有其他标记**，并根据我们用来比较目标标记与其他标记的相似性度量，为它们分配**权重**和**上下文信息**。**它们在期望上下文中的相似性越大，权重就越大**。
- en: So, in general, we could say that the attention mechanism is basically (1) **assigning
    weights** toand (2) extracting **relevant context** from other tokens of a sequence
    based on their relevance or importance to a target token (i.e. attending to them).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一般来说，我们可以说注意力机制基本上是（1）**为序列中的其他标记分配权重**，（2）从其他标记中提取**相关上下文**，以根据它们与目标标记的相关性或重要性（即关注它们）。
- en: And we said that in order to find this relevance/importance we need to **search
    through our sequence** and compare tokens one-to-one.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说过，为了找到相关性/重要性，我们需要**在序列中进行搜索**并逐一比较标记。
- en: '![](../Images/55d8c5d3622b879762f626cf777d50a0.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55d8c5d3622b879762f626cf777d50a0.png)'
- en: Searching through the sequence for relevant context to a token for adding —
    Image by author.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在序列中搜索与标记相关的上下文以进行添加——图片由作者提供。
- en: '**This is where the *Query*, *Key*, and *Values* find meaning.**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**这就是*查询*、*键*和*值*找到意义的地方。**'
- en: '***Query*, *Key*, and *Value***'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '***查询*、*键*和*值***'
- en: To make more sense, think of when you search for something on YouTube, for example.
    Assume YouTube stores all its videos as a pair of “*video title*” and the “*video
    file*” itself. Which we call a **Key-Value pair**, with the Key being the video
    title and the Value being the video itself.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地理解，可以想象一下你在 YouTube 上搜索内容的情形。例如，假设 YouTube 将所有视频存储为一对“*视频标题*”和“*视频文件*”本身。我们称之为**键值对**，其中键是视频标题，值是视频本身。
- en: The text you put in the search box is called a **Query** in search terms. So
    in a sense, when you search for something, **YouTube compares your search Query
    with the Keys of all its videos**, then measures the similarity between them,
    and ranks their **Values** from the highest similarity down.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你在搜索框中输入的文本在搜索术语中称为**查询**。所以从某种意义上讲，当你搜索某个内容时，**YouTube 将你的搜索查询与所有视频的键进行比较**，然后衡量它们之间的相似性，并根据相似性从高到低排序它们的**值**。
- en: '![](../Images/0387c1a513e915155c6bb6fdefeb750d.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0387c1a513e915155c6bb6fdefeb750d.png)'
- en: A basic search procedure illustrating the notion of Key, Quey, and Value — Image
    by author.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一个基本的搜索过程展示了键、查询和值的概念——图片由作者提供。
- en: 'In our problem, we have a sequence of token vectors, and we want to search
    for **the weights** to re-weight and contextualize each token (word) embedding
    of the sequence, we can think in terms of:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的问题中，我们有一个标记向量的序列，我们想要搜索**权重**以重新加权和上下文化序列中的每个标记（词）嵌入，我们可以从以下方面考虑：
- en: What you want to look for is the **Query**.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你想要查找的是**查询**。
- en: What you are searching among is **Key-Value** pairs.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你正在搜索的是**键值对**。
- en: The query is compared to all the Keys to measure the **relevance/importance/similarity**.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查询会与所有的键进行比较，以衡量**相关性/重要性/相似性**。
- en: The **Values** are utilized based on the assigned similarity measure.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**值**根据分配的相似性度量来使用。'
- en: Another helpful relevant analogy is a dictionary (or hashmap) data structure.
    A dictionary stores data in key-value pairs and it maps keys to their respective
    value pairs. When you try to **get a specific** value from the dictionary, you
    have to provide a **query** to match its corresponding **key**, then it **searches
    among those keys**, **compares them with the query**, and **if matched**, the
    desired value will be returned.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的相关类比是字典（或哈希表）数据结构。字典以键值对的形式存储数据，将键映射到其相应的值对。当你尝试**获取特定的**值时，你必须提供一个**查询**来匹配其对应的**键**，然后它**在这些键中搜索**，**与查询进行比较**，**如果匹配**，则返回所需的值。
- en: However, the difference here is that this is a “hard-matching” case, where the
    Query either **exactly matches** the Key or it doesn’t and an in-between similarity
    is not measured between them.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里的区别在于这是一个“硬匹配”情况，其中查询要么**完全匹配**键，要么不匹配，中间的相似性不会被测量。
- en: '![](../Images/311e0dba10b97d888a7104f7029b7847.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/311e0dba10b97d888a7104f7029b7847.png)'
- en: Dictionary Query matching with Key-Value pairs — Image by author
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 字典查询与键值对匹配 — 图片来源于作者
- en: We earlier mentioned that we are only working with real-valued vectors (token
    embeddings). So the Query, Key, and Value also need to be vectors. However, so
    far we only have **one vector for each token** which is its embedding vector.
    So, how should we obtain the Query, Key, and Value vectors?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到，我们只处理实值向量（标记嵌入）。因此，查询、键和值也需要是向量。然而，到目前为止，我们只有**每个标记的一个向量**，即其嵌入向量。那么，我们应该如何获得查询、键和值向量呢？
- en: 'We **Construct them** using linear projections (linear transformations aka
    single dense layer with separate sets of weights: Wq, Wₖ, Wᵥ) of the embedding
    vector of each token. This means we use a **learnable** vector of weights for
    each of the Query, Key, and Value to do a linear transformation on the word embedding
    to **obtain the corresponding Query, Key, and Value vectors**.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们**构造它们**使用线性投影（线性变换即单个密集层，具有不同的权重集合：Wq、Wₖ、Wᵥ）对每个标记的嵌入向量进行变换。这意味着我们为查询、键和值分别使用**可学习**的权重向量，对词嵌入进行线性变换，以**获取相应的查询、键和值向量**。
- en: '![](../Images/466cfc4ec0ca8ce6bf781237a111f9c4.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/466cfc4ec0ca8ce6bf781237a111f9c4.png)'
- en: Linear transformation of the word embedding to obtain Query, Key, and Value
    vectors — Image by author
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对词嵌入进行线性变换以获取查询、键和值向量 — 图片来源于作者
- en: 'An embedding of a token may represent different contextual, structural, and
    syntactical, aspects or meanings of that token. By using **learnable linear transformation
    layers** to construct these vectors from the token’s embedding, we allow the network
    to:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一个标记的嵌入可能代表该标记的不同上下文、结构和语法方面或含义。通过使用**可学习的线性变换层**来从标记的嵌入构造这些向量，我们允许网络：
- en: '**Extract** and pass a limited specific part of that information into the ***Q***,
    ***K***, and ***V*** vectors.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提取**并将信息的有限特定部分传递到***Q***、***K***和***V***向量中。'
- en: Determine a narrower context in which the search and match is going to be done.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定一个更狭窄的上下文，在其中进行搜索和匹配。
- en: '**Learn** what information in an embedding is more important to attend to.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**学习**在嵌入中哪些信息更重要。'
- en: Now, having the ***Q***, ***K***, and ***V*** vectors in hand, we are able to
    perform the “**search and compare”** procedure that was discussed before, with
    these vectors. This results in the final derivation of the attention mechanism
    proposed in the proposed in (Vaswani et al 2017).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，手中有了***Q***、***K***和***V***向量，我们能够执行之前讨论的“**搜索和比较**”过程。这导致了注意力机制的最终推导，如（Vaswani
    et al 2017）中所提。
- en: 'For each token:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个标记：
- en: We compare its **Query vector** to all other tokens’ **Key vectors**.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将其**查询向量**与所有其他标记的**键向量**进行比较。
- en: Calculate a **vector similarity** score between each two (i.e. the dot-product
    similarity in the original paper)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算每两个之间的**向量相似性**分数（即原始论文中的点积相似性）
- en: Transform these similarity **scores** into **weights** by scaling them into
    [0,1] (i.e. Softmax)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些相似性**分数**转换为**权重**，通过将它们缩放到 [0,1]（即Softmax）
- en: And add the weighted context by weighting their corresponding **value vectors**.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并通过加权其对应的**值向量**来增加加权的上下文。
- en: '![](../Images/d292c611f2a18909aba5f8e76835fca3.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d292c611f2a18909aba5f8e76835fca3.png)'
- en: Dot-product attention procedure —Image by author
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 点积注意力过程 — 图片来源于作者
- en: So the whole notion of the ***Q***, ***K***, and ***V*** vectors is like a **soft**
    dictionary to mimic a ***search-and-match procedure*** from which we learn how
    much two tokens in a sequence are relevant (the weights), and **what should be
    added** as the context (the values). Also, note that this process does not have
    to happen sequentially (one token at a time). This all happens in parallel by
    using matrix operations.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，***Q***、***K***和***V***向量的整个概念就像是一个**软**词典，用于模拟***搜索与匹配过程***，从中我们学习两个序列令牌的相关性（权重）以及**应该添加什么**作为上下文（值）。还要注意，这一过程不必按顺序进行（一个令牌一次）。这一切通过矩阵操作并行进行。
- en: Note that in the illustration below, the matrix dimensions are switched compared
    to that of the original paper (***n_tokens*** by ***dim*** instead of ***dim***
    by ***n_tokens***). Later in this post, you will see the original and complete
    formulation of the attention mechanism which is the other way around.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在下面的插图中，矩阵的维度与原始论文中的维度有所不同（***n_tokens*** 乘 ***dim*** 而不是 ***dim*** 乘 ***n_tokens***）。稍后在本文中，您将看到注意力机制的原始和完整公式，其顺序与此相反。
- en: '![](../Images/eae164765934b254aeedd32b94c5f5cc.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eae164765934b254aeedd32b94c5f5cc.png)'
- en: Matrix form of the dot-product attention — Image by author
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 点积注意力的矩阵形式 — 作者绘制的图片
- en: 'This results in a more context-aware embedding of each token, where the added
    context is based on the relevance of the tokens to each other and it is learned
    through ***Q***, ***K***, ***V*** vector transformation. Hence, the dot-product
    attention mechanism. The original attention mechanism in (Vaswani et al, 2017)
    also scales the dot-product of ***K*** and ***Q*** vectors, meaning it divides
    the resulting vector by ***sqrt(d)***, where ***d*** is the dimension of the Query
    vector. Hence the name, ***“scaled dot-product attention”***. This scaling helps
    with reducing the variance of the dot-product before being passed to the Softmax
    function:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了对每个令牌的上下文感知嵌入，其中添加的上下文基于令牌之间的相关性，并通过***Q***、***K***、***V***向量转换学习得到。因此，使用了点积注意力机制。Vaswani等人（2017）的原始注意力机制也缩放了***K***和***Q***向量的点积，即将结果向量除以***sqrt(d)***，其中***d***是查询向量的维度。因此得名***“缩放点积注意力”***。这种缩放有助于在传递到Softmax函数之前减少点积的方差。
- en: '![](../Images/e602bdf1db623fd467c9c072ed50f509.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e602bdf1db623fd467c9c072ed50f509.png)'
- en: 'Attention mechanism formula — Vaswani et al. (2017) (Source: [**arXiv:1706.03762v7**](https://arxiv.org/abs/1706.03762v7))'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制公式 — Vaswani等人（2017）（来源：[**arXiv:1706.03762v7**](https://arxiv.org/abs/1706.03762v7)）
- en: Finally, we mentioned that the linear layers that transform the embedding into
    ***Q***, ***K***, ***V,*** may extract only a specific pattern in the embedding
    for finding the attention weights. To enable the model to learn different complex
    relations between the sequence tokens, create and use multiple different versions
    of these ***Q***, ***K***, ***V***, so that each will focus on different patterns
    existing in our embeddings. These multiple versions are called **attention heads**
    resulting in the name “multi-head attention”. These heads can also be vectorized
    and computed in parallel using current popular deep learning frameworks.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们提到将嵌入转换为***Q***、***K***、***V***的线性层可能只提取嵌入中的特定模式以找到注意力权重。为了使模型能够学习序列令牌之间的不同复杂关系，创建并使用这些***Q***、***K***、***V***的多个不同版本，以便每个版本关注嵌入中存在的不同模式。这些多个版本称为**注意力头**，因此得名“多头注意力”。这些头部也可以向量化，并使用当前流行的深度学习框架并行计算。
- en: '![](../Images/2cdd64a6737faef6b55fa00cf48e24e5.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2cdd64a6737faef6b55fa00cf48e24e5.png)'
- en: 'Multi-head scaled dot-product attention — Image from the Vaswani et al. (2017)
    paper (Source: [**arXiv:1706.03762v7**](https://arxiv.org/abs/1706.03762v7))'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 多头缩放点积注意力 — 图片来自Vaswani等人（2017）的论文（来源：[**arXiv:1706.03762v7**](https://arxiv.org/abs/1706.03762v7)）
- en: Conclusion
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: So to wrap up, in this post I tried to picture and analyze the intuition behind
    the use of Query, Key, and Value which are key components in the attention mechanism
    and may be a little difficult to make sense of, at first encounters.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，在这篇文章中，我试图描绘和分析Query、Key和Value的直觉，这些是注意力机制中的关键组件，初次接触时可能有点难以理解。
- en: The attention mechanism discussed in this post was proposed in the transformer
    architecture that is introduced in the (Vaswani et al, 2017) paper “Attention
    is all you need” and has been one of the top-performing architectures since, in
    several different tasks and benchmarks in deep learning. With its vast use cases
    and applicability, it would be helpful to have an understanding of the intuition
    behind the nuts and bolts used in this architecture and know why we use it.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本文讨论的注意力机制是提出在变换器架构中，这种架构在(Vaswani et al, 2017)的论文“Attention is all you need”中介绍，并且自此以来，在深度学习的多个任务和基准中一直是表现最优的架构之一。鉴于其广泛的应用和适用性，了解这一架构中的细节和使用原因将会很有帮助。
- en: I attempted to be as clear and as basic as possible while explaining this topic
    by laying down examples and illustrations wherever possible.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我在解释这个话题时尽量做到清晰和基础，通过举例和插图尽可能地说明。
- en: References
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “*Attention Is All You Need.*”
    arXiv, August 1, 2023\. [https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762.).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “*Attention Is All You Need.*”
    arXiv, 2023年8月1日。 [https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)。'
