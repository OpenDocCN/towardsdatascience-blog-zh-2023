- en: Top 10 Pre-Trained Models for Image Embedding every Data Scientist Should Know
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/top-10-pre-trained-models-for-image-embedding-every-data-scientist-should-know-88da0ef541cd](https://towardsdatascience.com/top-10-pre-trained-models-for-image-embedding-every-data-scientist-should-know-88da0ef541cd)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Essential guide to transfer learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://satyam-kumar.medium.com/?source=post_page-----88da0ef541cd--------------------------------)[![Satyam
    Kumar](../Images/2360baa87ea7a20f41589c5f8d783288.png)](https://satyam-kumar.medium.com/?source=post_page-----88da0ef541cd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----88da0ef541cd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----88da0ef541cd--------------------------------)
    [Satyam Kumar](https://satyam-kumar.medium.com/?source=post_page-----88da0ef541cd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----88da0ef541cd--------------------------------)
    ·9 min read·Apr 19, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bcf5c3734418b63acaacc768035b72e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Chen](https://pixabay.com/users/chenspec-7784448/?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=5692896)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=5692896)
  prefs: []
  type: TYPE_NORMAL
- en: The rapid developments in Computer Vision — image classification use cases have
    been further accelerated by the advent of transfer learning. It takes a lot of
    computational resources and time to train a computer vision neural network model
    on a large dataset of images.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, this time and resources can be shortened by using pre-trained models.
    The technique of leveraging feature representation from a pre-trained model is
    called transfer learning. The pre-trained are generally trained using high-end
    computational resources and on massive datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pre-trained models can be used in various ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the pre-trained weights and directly making predictions on the test data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the pre-trained weights for initialization and training the model using
    the custom dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using only the architecture of the pre-trained network, and training it from
    scratch on the custom dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This article walks through the top 10 state-of-the-art pre-trained models to
    get image embedding. All these pre-trained models can be loaded as keras models
    using the [keras.application](https://keras.io/api/applications/) API.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**License:** All the images used in this article are from [paperwithcode.com](https://paperswithcode.com/)
    which is licensed under CC BY-SA 4.0.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '1) VGG:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The VGG-16/19 networks were introduced at the ILSVRC 2014 conference since it
    is one of the most popular pre-trained models. It was developed by the Visual
    Graphics Group at the University of Oxford.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two variations of the VGG model: 16 and 19 layers network, VGG-19
    (19-layer network) being an improvement of the VGG-16 (16-layer network) model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Architecture:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf14e8e28334a24a8ae22c55a4e1de6d.png)'
  prefs: []
  type: TYPE_IMG
- en: ([Source](https://paperswithcode.com/method/vgg), Free-to-use license under
    [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)), VGG-16 Network
    architecture
  prefs: []
  type: TYPE_NORMAL
- en: The VGG network is simple and sequential in nature and uses a lot of filters.
    At each stage, small (3*3) filters are used to reduce the number of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The VGG-16 network has the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Layers = 13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling Layers = 5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully Connected Dense Layers = 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input:** Image of dimensions (224, 224, 3)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output:** Image embedding of 1000-dimension'
  prefs: []
  type: TYPE_NORMAL
- en: '**Other Details for VGG-16/19:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper Link: [https://arxiv.org/pdf/1409.1556.pdf](https://arxiv.org/pdf/1409.1556.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GitHub: [VGG](https://paperswithcode.com/paper/very-deep-convolutional-networks-for-large#code)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Published On: April 2015'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Performance on ImageNet Dataset: 71% (Top 1 Accuracy), 90% (Top 5 Accuracy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of Parameters: ~140M'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of Layers: 16/19'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Size on Disk: ~530MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation:**'
  prefs: []
  type: TYPE_NORMAL
- en: Call `[tf.keras.applications.vgg16.preprocess_input](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/preprocess_input)`
    on your input data to convert input images to BGR with zero-center for each color
    channel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Instantiate the VGG16 model using the below-mentioned code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The above-mentioned code is for VGG-16 implementation, keras offers a similar
    API for VGG-19 implementation, for more details refer to [this documentation](https://keras.io/api/applications/vgg/#vgg16-function).
  prefs: []
  type: TYPE_NORMAL
- en: '2) Xception:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Xception is a deep CNN architecture that involves depthwise separable convolutions.
    A depthwise separable convolution can be understood as an Inception model with
    a maximally large number of towers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Architecture:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cca16ef8eae292510d58bc3c94f14bb5.png)'
  prefs: []
  type: TYPE_IMG
- en: ([Source](https://paperswithcode.com/paper/xception-deep-learning-with-depthwise),
    Free-to-use license under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)),
    Xception architecture
  prefs: []
  type: TYPE_NORMAL
- en: '**Input:** Image of dimensions (299, 299, 3)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output:** Image embedding of 1000-dimension'
  prefs: []
  type: TYPE_NORMAL
- en: '**Other Details for Xception:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper Link: [https://arxiv.org/pdf/1409.1556.pdf](https://arxiv.org/pdf/1610.02357.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GitHub: [Xception](https://paperswithcode.com/paper/xception-deep-learning-with-depthwise#code)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Published On: April 2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Performance on ImageNet Dataset: 79% (Top 1 Accuracy), 94.5% (Top 5 Accuracy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of Parameters: ~30M'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depth: 81'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Size on Disk: 88MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the Xception model using the below-mentioned code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The above-mentioned code is for Xception implementation, for more details refer
    to [this documentation](https://keras.io/api/applications/xception/).
  prefs: []
  type: TYPE_NORMAL
- en: '3) ResNet:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous CNN architectures were not designed to scale to many convolutional
    layers. It resulted in a vanishing gradient problem and limited performance upon
    adding new layers to the existing architecture.
  prefs: []
  type: TYPE_NORMAL
- en: ResNets architecture offers to skip connections to solve the vanishing gradient
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: '**Architecture:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ffc35dcf643069854d00c4f511e8ab1a.png)'
  prefs: []
  type: TYPE_IMG
- en: ([Source](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-09-25_at_10.26.40_AM_SAB79fQ.png),
    Free-to-use license under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)),
    ResNet architecture
  prefs: []
  type: TYPE_NORMAL
- en: This ResNet model uses a 34-layer network architecture inspired by the VGG-19
    model to which the shortcut connections are added. These shortcut connections
    then convert the architecture into a residual network.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several versions of ResNet architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: ResNet50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet50V2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet101V2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet152V2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input:** Image of dimensions (224, 224, 3)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output:** Image embedding of 1000-dimension'
  prefs: []
  type: TYPE_NORMAL
- en: '**Other Details for ResNet models:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper Link: [https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GitHub: [ResNet](https://paperswithcode.com/paper/deep-residual-learning-for-image-recognition#code)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Published On: Dec 2015'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Performance on ImageNet Dataset: 75–78% (Top 1 Accuracy), 92–93% (Top 5 Accuracy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of Parameters: 25–60M'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depth: 107–307'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Size on Disk: ~100–230MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the ResNet50 model using the below-mentioned code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The above-mentioned code is for ResNet50 implementation, keras offers a similar
    API to other ResNet architecture implementations, for more details refer to [this
    documentation](https://keras.io/api/applications/resnet/#resnet101-function).
  prefs: []
  type: TYPE_NORMAL
- en: '4) Inception:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Multiple deep layers of convolutions resulted in the overfitting of the data.
    To avoid overfitting, the inception model uses parallel layers or multiple filters
    of different sizes on the same level, to make the model wider rather than making
    it deeper. The Inception V1 model is made of 4 parallel layers with: (1*1), (3*3),
    (5*5) convolutions, and (3*3) max pooling.'
  prefs: []
  type: TYPE_NORMAL
- en: Inception (V1/V2/V3) is deep learning model-based CNN network developed by a
    team at Google. InceptionV3 is an advanced and optimized version of the InceptionV1
    and V2 models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Architecture:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The InceptionV3 model is made up of 42 layers. The architecture of InceptionV3
    is progressively step-by-step built as:'
  prefs: []
  type: TYPE_NORMAL
- en: Factorized Convolutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smaller Convolutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asymmetric Convolutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auxilliary Convolutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grid Size Reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All these concepts are consolidated into the final architecture mentioned below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c6221bb9bc94860e18662fde362d19d.png)'
  prefs: []
  type: TYPE_IMG
- en: ([Source](https://paperswithcode.com/method/inception-v3), Free-to-use license
    under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)), InceptionV3
    architecture
  prefs: []
  type: TYPE_NORMAL
- en: '**Input:** Image of dimensions (299, 299, 3)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output:** Image embedding of 1000-dimension'
  prefs: []
  type: TYPE_NORMAL
- en: '**Other Details for InceptionV3 models:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper Link: [https://arxiv.org/pdf/1512.00567.pdf](https://arxiv.org/pdf/1512.00567.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GitHub: [InceptionV3](https://paperswithcode.com/paper/what-do-deep-networks-like-to-see#code)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Published On: Dec 2015'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Performance on ImageNet Dataset: 78% (Top 1 Accuracy), 94% (Top 5 Accuracy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of Parameters: 24M'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depth: 189'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Size on Disk: 92MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the InceptionV3 model using the below-mentioned code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The above-mentioned code is for InceptionV3 implementation, for more details
    refer to [this documentation](https://keras.io/api/applications/inceptionv3/).
  prefs: []
  type: TYPE_NORMAL
- en: '5) InceptionResNet:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: InceptionResNet-v2 is a CNN model developed by researchers at Google. The target
    of this model was to reduce the complexity of InceptionV3 and explore the possibility
    of using residual networks on the Inception model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Architecture:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a49ccb18b6aae5e423a69f62a3f40e91.png)'
  prefs: []
  type: TYPE_IMG
- en: ([Source](https://paperswithcode.com/method/inception-resnet-v2), Free-to-use
    license under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)),
    Inception-ResNet-V2 architecture
  prefs: []
  type: TYPE_NORMAL
- en: '**Input:** Image of dimensions (299, 299, 3)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output:** Image embedding of 1000-dimension'
  prefs: []
  type: TYPE_NORMAL
- en: '**Other Details for Inception-ResNet-V2 models:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper Link: [https://arxiv.org/pdf/1602.07261.pdf](https://arxiv.org/pdf/1602.07261.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GitHub: [Inception-ResNet-V](https://paperswithcode.com/paper/inception-v4-inception-resnet-and-the-impact#code)2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Published On: Aug 2016'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Performance on ImageNet Dataset: 80% (Top 1 Accuracy), 95% (Top 5 Accuracy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of Parameters: 56M'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depth: 189'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Size on Disk: 215MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the Inception-ResNet-V2 model using the below-mentioned code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The above-mentioned code is for Inception-ResNet-V2 implementation, for more
    details refer to [this documentation](https://keras.io/api/applications/inceptionresnetv2/).
  prefs: []
  type: TYPE_NORMAL
- en: '6) MobileNet:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MobileNet is a streamlined architecture that uses depthwise separable convolutions
    to construct deep convolutional neural networks and provides an efficient model
    for mobile and embedded vision applications.
  prefs: []
  type: TYPE_NORMAL
- en: '**Architecture:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72ad57d5161cf1660e5501abfde689bc.png)'
  prefs: []
  type: TYPE_IMG
- en: ([Source](https://www.hindawi.com/journals/misy/2020/7602384/), Free-to-use
    license under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)),
    Mobile-Net architecture
  prefs: []
  type: TYPE_NORMAL
- en: '**Input:** Image of dimensions (224, 224, 3)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output:** Image embedding of 1000-dimension'
  prefs: []
  type: TYPE_NORMAL
- en: '**Other Details for MobileNet models:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper Link: [https://arxiv.org/pdf/1602.07261.pdf](https://arxiv.org/pdf/1704.04861.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GitHub: [MobileNet-V3](https://paperswithcode.com/paper/searching-for-mobilenetv3#code),
    [MobileNet-V2](https://paperswithcode.com/paper/mobilenetv2-inverted-residuals-and-linear#code)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Published On: Apr 2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Performance on ImageNet Dataset: 71% (Top 1 Accuracy), 90% (Top 5 Accuracy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of Parameters: 3.5–4.3M'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depth: 55–105'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Size on Disk: 14–16MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the MobileNet model using the below-mentioned code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The above-mentioned code is for MobileNet implementation, keras offers a similar
    API to other MobileNet architecture (MobileNet-V2, MobileNet-V3) implementation,
    for more details refer to [this documentation](https://keras.io/api/applications/mobilenet/).
  prefs: []
  type: TYPE_NORMAL
- en: '7) DenseNet:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DenseNet is a CNN model developed to improve accuracy caused by the vanishing
    gradient in high-level neural networks due to the long distance between input
    and output layers and the information vanishes before reaching the destination.
  prefs: []
  type: TYPE_NORMAL
- en: '**Architecture:**'
  prefs: []
  type: TYPE_NORMAL
- en: A DenseNet architecture has 3 dense blocks. The layers between two adjacent
    blocks are referred to as transition layers and change feature-map sizes via convolution
    and pooling.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ad573e101eebd6f3ecb20c10bcda321.png)'
  prefs: []
  type: TYPE_IMG
- en: ([Source](https://paperswithcode.com/method/densenet), Free-to-use license under
    [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)), DenseNet architecture
  prefs: []
  type: TYPE_NORMAL
- en: '**Input:** Image of dimensions (224, 224, 3)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output:** Image embedding of 1000-dimension'
  prefs: []
  type: TYPE_NORMAL
- en: '**Other Details for DenseNet models:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper Link: [https://arxiv.org/pdf/1608.06993.pdf](https://arxiv.org/pdf/1608.06993.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GitHub: [DenseNet-169](https://paperswithcode.com/paper/densely-connected-convolutional-networks#code),
    [DenseNet-201](https://paperswithcode.com/paper/densely-connected-convolutional-networks),
    [DenseNet-264](https://paperswithcode.com/paper/densely-connected-convolutional-networks#code)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Published On: Jan 2018'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Performance on ImageNet Dataset: 75–77% (Top 1 Accuracy), 92–94% (Top 5 Accuracy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of Parameters: 8–20M'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depth: 240–400'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Size on Disk: 33–80MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the DenseNet121 model using the below-mentioned code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The above-mentioned code is for DenseNet implementation, keras offers a similar
    API to other DenseNet architecture (DenseNet-169, DenseNet-201) implementation,
    for more details refer to [this documentation](https://keras.io/api/applications/mobilenet/).
  prefs: []
  type: TYPE_NORMAL
- en: '8) NasNet:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google researchers designed a NasNet model that framed the problem to find the
    best CNN architecture as a Reinforcement Learning approach. The idea is to search
    for the best combination of parameters of the given search space of a number of
    layers, filter sizes, strides, output channels, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '**Input:** Image of dimensions (331, 331, 3)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Other Details for NasNet models:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper Link: [https://arxiv.org/pdf/1608.06993.pdf](https://arxiv.org/pdf/1707.07012.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Published On: Apr 2018'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Performance on ImageNet Dataset: 75–83% (Top 1 Accuracy), 92–96% (Top 5 Accuracy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of Parameters: 5–90M'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depth: 389–533'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Size on Disk: 23–343MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the NesNetLarge model using the below-mentioned code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The above-mentioned code is for NesNet implementation, keras offers a similar
    API to other NasNet architecture (NasNetLarge, NasNetMobile) implementation, for
    more details refer to [this documentation](https://keras.io/api/applications/nasnet/#nasnetmobile-function).
  prefs: []
  type: TYPE_NORMAL
- en: '9) EfficientNet:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: EfficientNet is a CNN architecture from the researchers of Google, that can
    achieve better performance by a scaling method called compound scaling. This scaling
    method uniformly scales all dimensions of depth/width/resolution by a fixed amount
    (compound coefficient) uniformly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Architecture:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c66322e34001b6b1958742fb0b98839d.png)'
  prefs: []
  type: TYPE_IMG
- en: ([Source](https://paperswithcode.com/method/efficientnet), Free-to-use license
    under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)), Efficient-B0
    architecture
  prefs: []
  type: TYPE_NORMAL
- en: '**Other Details for EfficientNet Models:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper Link: [https://arxiv.org/pdf/1905.11946v5.pdf](https://arxiv.org/pdf/1905.11946v5.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GitHub: [EfficientNet](https://paperswithcode.com/paper/efficientnet-rethinking-model-scaling-for#code)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Published On: Sep 2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Performance on ImageNet Dataset: 77–84% (Top 1 Accuracy), 93–97% (Top 5 Accuracy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of Parameters: 5–67M'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depth: 132–438'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Size on Disk: 29–256MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the EfficientNet-B0 model using the below-mentioned code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The above-mentioned code is for EfficientNet-B0 implementation, keras offers
    a similar API for other EfficientNet architecture (EfficientNet-B0 to B7, EfficientNet-V2-B0
    to B3) implementation, for more details refer to [this documentation](https://keras.io/api/applications/efficientnet/#efficientnetb0-function),
    and [this documentation](https://keras.io/api/applications/efficientnet_v2/#efficientnetv2b0-function).
  prefs: []
  type: TYPE_NORMAL
- en: '10) ConvNeXt:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ConvNeXt CNN model was proposed as a pure convolutional model (ConvNet),
    inspired by the design of Vision Transformers, that claims to outperform them.
  prefs: []
  type: TYPE_NORMAL
- en: '**Architecture:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec14809f89d123eb8425466d17393e20.png)'
  prefs: []
  type: TYPE_IMG
- en: ([Source](https://arxiv.org/pdf/2201.03545.pdf), Free-to-use license under [CC
    BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)), ConvNeXt architecture
  prefs: []
  type: TYPE_NORMAL
- en: '**Other Details for ConvNeXt models:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper Link: [https://arxiv.org/pdf/1905.11946v5.pdf](https://arxiv.org/pdf/1905.11946v5.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GitHub: [ConvNeXt](https://paperswithcode.com/paper/a-convnet-for-the-2020s#code)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Published On: March 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Performance on ImageNet Dataset: 81–87% (Top 1 Accuracy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of Parameters: 29–350M'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Size on Disk: 110–1310MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the ConvNeXt-Tiny model using the below-mentioned code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The above-mentioned code is for ConvNeXt-Tiny implementation, keras offers a
    similar API of the other EfficientNet architecture (ConvNeXt-Small, ConvNeXt-Base,
    ConvNeXt-Large, ConvNeXt-XLarge) implementation, for more details refer to [this
    documentation](https://keras.io/api/applications/convnext/#convnexttiny-function).
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I have discussed 10 popular CNN architectures that can generate embeddings using
    transfer learning. These pre-trained CNN models have outperformed the ImageNet
    dataset and proved the best. Keras library offers APIs to load the architecture
    and weights of the discussed pre-trained models. The image embeddings generated
    from these models can be used for various use cases.
  prefs: []
  type: TYPE_NORMAL
- en: However, this is a continuously growing domain and there is always a new CNN
    architecture to look forward to.
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Papers with code: [https://paperswithcode.com/sota/image-classification-on-imagenet](https://paperswithcode.com/sota/image-classification-on-imagenet)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Keras Documentation: [https://keras.io/api/applications/](https://keras.io/api/applications/)'
  prefs: []
  type: TYPE_NORMAL
- en: Thank You for Reading
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
