- en: Introduction to Clustering Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/introduction-to-clustering-algorithms-76da35b5670a](https://towardsdatascience.com/introduction-to-clustering-algorithms-76da35b5670a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A comprehensive guide to 10 clustering algorithms commonly used for Hierarchical,
    Partitional, and Density-Based Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://kevindbabitz.medium.com/?source=post_page-----76da35b5670a--------------------------------)[![Kevin
    Babitz](../Images/271c0bda1324f51e3b61670ae869edf8.png)](https://kevindbabitz.medium.com/?source=post_page-----76da35b5670a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----76da35b5670a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----76da35b5670a--------------------------------)
    [Kevin Babitz](https://kevindbabitz.medium.com/?source=post_page-----76da35b5670a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----76da35b5670a--------------------------------)
    ·9 min read·Nov 12, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/66a16934e96383d4113c300de64f6477.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Rod Long](https://unsplash.com/@rodlong?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/penguin-walking-on-gray-sand-during-daytime-BSz0jJ172vM?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering algorithms play an important role in data analysis. These unsupervised
    learning, exploratory data analysis tools provide systems for knowledge discovery
    by categorizing data points into distinct groups based on shared characteristics.
    This allows for the identification of relationships and trends that may be hard
    to see in the raw data. They facilitate more informed decision making by systematically
    adding more understanding to complex and intricate datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we will cover the basics of three types of clustering algorithms:
    Hierarchical, Partitional, and Density-Based Clustering models. We will begin
    by defining each of these categories. Next, we will dive into 10 different clustering
    algorithms, providing definitions, links to the original or interesting research
    papers, strengths of the algorithms, and python code-snippets for each.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table of Contents**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Hierarchical Clustering Algorithms](#1c12)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Birch](#d9e2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cure](#9353)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ROCK](#6754)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chameleon](#1306)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Partitional Clustering Algorithms](#286d)'
  prefs: []
  type: TYPE_NORMAL
- en: '[K-Means](#8604)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[K-Medoids (PAM)](#2370)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[CLARANS](#e4d4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ISODATA](#355b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Density-Based Clustering Algorithms](#a380)'
  prefs: []
  type: TYPE_NORMAL
- en: '[DBSCAN](#43eb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[DENCLUE](#d560)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical Clustering Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Definition:** Hierarchical clustering is a method of cluster analysis that
    builds a hierarchy of clusters. It can be visualized as a tree structure (dendrogram)
    where the leaves represent individual data points and the root represents a single
    cluster containing all data points.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use Cases:**'
  prefs: []
  type: TYPE_NORMAL
- en: Taxonomy Problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When Vertical relationships are important in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strengths:**'
  prefs: []
  type: TYPE_NORMAL
- en: Provides a hierarchical structure of clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No need to specify the number of clusters beforehand.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weaknesses:**'
  prefs: []
  type: TYPE_NORMAL
- en: Prone to noise and outliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computationally intensive for large datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Birch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Summary:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Birch algorithm, short for “Balanced Iterative Reducing and Clustering
    using Hierarchies,” is a hierarchical clustering algorithm designed for scalability
    and efficiency, particularly suited for large datasets. It uses a two-step process
    to build clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: Construct a feature-based tree called a Clustering Feature (CF) tree, summarizing
    the dataset’s distribution. This CF tree allows for efficient memory usage and
    incremental updates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply a clustering mechanism based on the leaf nodes of the CF tree to form
    cohesive clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Original Paper / Helpful Research:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://dl.acm.org/doi/10.1145/235968.233324?source=post_page-----76da35b5670a--------------------------------)
    [## BIRCH: an efficient data clustering method for very large databases: ACM SIGMOD
    Record: Vol 25, No…'
  prefs: []
  type: TYPE_NORMAL
- en: Finding useful patterns in large datasets has attracted considerable interest
    recently, and one of the most widely…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: dl.acm.org](https://dl.acm.org/doi/10.1145/235968.233324?source=post_page-----76da35b5670a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Strengths:**'
  prefs: []
  type: TYPE_NORMAL
- en: Birch is known for its ability to handle large volumes of data and its resilience
    to outliers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python Code:**'
  prefs: []
  type: TYPE_NORMAL
- en: Birch algorithm using [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html)
  prefs: []
  type: TYPE_NORMAL
- en: Cure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Summary:**'
  prefs: []
  type: TYPE_NORMAL
- en: The CURE (Clustering Using Representatives) algorithm is an [agglomerative hierarchical
    clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering#Agglomerative_clustering_example)
    method designed to address the limitations of traditional centroid-based algorithms
    like K-Means, especially when dealing with non-spherical and arbitrarily shaped
    clusters. CURE takes a unique approach by representing clusters with a fixed number
    of points, called representatives, which are randomly selected from each cluster.
    These representatives are then “shrunk” toward the center of mass, effectively
    capturing the cluster’s geometry. CURE is known for its robustness against outliers,
    ability to handle clusters of varying shapes and sizes, and improved performance
    in scenarios where traditional centroid-based algorithms might fail.
  prefs: []
  type: TYPE_NORMAL
- en: '**Original Paper:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://dl.acm.org/doi/10.1145/276305.276312?source=post_page-----76da35b5670a--------------------------------)
    [## CURE: an efficient clustering algorithm for large databases: ACM SIGMOD Record:
    Vol 27, No 2'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering, in data mining, is useful for discovering groups and identifying
    interesting distributions in the…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: dl.acm.org](https://dl.acm.org/doi/10.1145/276305.276312?source=post_page-----76da35b5670a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Strengths:**'
  prefs: []
  type: TYPE_NORMAL
- en: Cure is robust to outliers and is able to identify clusters with varied shapes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python Code:**'
  prefs: []
  type: TYPE_NORMAL
- en: Cure algorithm using [pyclustering](https://pyclustering.github.io/docs/0.8.2/html/dc/d6d/classpyclustering_1_1cluster_1_1cure_1_1cure.html)
  prefs: []
  type: TYPE_NORMAL
- en: ROCK
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Summary:**'
  prefs: []
  type: TYPE_NORMAL
- en: The ROCK (Robust Clustering using Links) algorithm is an agglomerative hierarchical
    clustering method designed to address challenges in clustering datasets with categorical
    attributes. It introduces the concept of “links,” which measure the proximity
    between data points with categorical attributes. Utilizing a “goodness measure,”
    ROCK aims to identify clusters by evaluating the similarity of objects within
    them. The algorithm is particularly useful for datasets with mixed attribute types,
    providing a global approach to clustering. However, ROCK may produce ambiguous
    results if parameter choices in the static model differ significantly from the
    dataset being clustered, and it may struggle to accurately define clusters with
    different sizes and shapes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Original Paper / Helpful Research:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.sciencedirect.com/science/article/abs/pii/S0306437900000223?source=post_page-----76da35b5670a--------------------------------)
    [## Rock: A robust clustering algorithm for categorical attributes'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering, in data mining, is useful to discover distribution patterns in the
    underlying data. Clustering algorithms…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S0306437900000223?source=post_page-----76da35b5670a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Strengths:**'
  prefs: []
  type: TYPE_NORMAL
- en: ROCK scales well with increasing dimensionality and can measure similarity within
    clusters using a global approach. This algorithm works well with categorical variables.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python Code:**'
  prefs: []
  type: TYPE_NORMAL
- en: ROCK algorithm using [pyclustering](https://pyclustering.github.io/docs/0.8.2/html/d8/dde/classpyclustering_1_1cluster_1_1rock_1_1rock.html)
  prefs: []
  type: TYPE_NORMAL
- en: Chameleon
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Summary:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chameleon is a dynamic clustering algorithm designed to measure the similarity
    of two clusters based on a dynamic model. It works in two phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a graph with links between each point and its N-nearest neighbors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the graph by a graph partitioning algorithm, resulting in many small unconnected
    subgraphs. Chameleon iteratively combines the two most similar clusters, considering
    their connectivity and closeness, making it more functional than some other algorithms
    when dealing with arbitrary-shaped clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/4e6bcdafa4b3d99b385c4826607aed2a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure from the original Chameleon algorithm paper linked below
  prefs: []
  type: TYPE_NORMAL
- en: '**Original Paper / Helpful Research:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.researchgate.net/publication/2955136_CHAMELEON_A_hierarchical_clustering_algorithm_using_dynamic_modeling](https://www.researchgate.net/publication/2955136_CHAMELEON_A_hierarchical_clustering_algorithm_using_dynamic_modeling)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Strengths:**'
  prefs: []
  type: TYPE_NORMAL
- en: Chameleon can measures cluster similarity based on a dynamic model and it is
    strong in dealing with arbitrary-shaped clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python Code:**'
  prefs: []
  type: TYPE_NORMAL
- en: There are no existing libraries to run this algorithm. However, Moonpuck has
    implemented the chameleon algorithm and their code can be explored [here on Github](https://github.com/Moonpuck/chameleon_cluster).
  prefs: []
  type: TYPE_NORMAL
- en: Partitional Clustering Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Definition:** Partitional clustering divides data into non-overlapping subsets
    (partitions) based on similarity. The most common method, which you’ve likely
    heard of, is the K-Means algorithm, where data points are assigned to the nearest
    centroid.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use-Cases:**'
  prefs: []
  type: TYPE_NORMAL
- en: Large datasets because these algorithms have lower computational requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the coherence of clustering is more critical than hierarchical structure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strengths:**'
  prefs: []
  type: TYPE_NORMAL
- en: Efficient for large datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suitable for spherical-shaped clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weaknesses:**'
  prefs: []
  type: TYPE_NORMAL
- en: Sensitive to noise and outliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assumes hyper-ellipsoidal shapes for clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-Means
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Summary:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The K-Means algorithm is a partitioning-based clustering technique widely used
    for grouping data points into K distinct clusters. The steps of the algorithm
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly select K centroids in the feature space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign each data point to the nearest centroid.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iteratively update the centroids based on the mean of the points within each
    cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Continue the above steps until convergence, where the centroids no longer change
    significantly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: K-Means is computationally efficient and simple to implement, making it one
    of the most popular clustering algorithms; however, its performance can be sensitive
    to the initial placement of centroids and is influenced by outliers. Additionally,
    K-Means assumes clusters with a spherical shape and struggles with clusters of
    varying sizes and densities.
  prefs: []
  type: TYPE_NORMAL
- en: '**Original Paper / Helpful Research:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.semanticscholar.org/paper/Least-squares-quantization-in-PCM-Lloyd/9241ea3d8cb85633d314ecb74b31567b8e73f6af?source=post_page-----76da35b5670a--------------------------------)
    [## [PDF] Least squares quantization in PCM | Semantic Scholar'
  prefs: []
  type: TYPE_NORMAL
- en: The corresponding result for any finite number of quanta is derived; that is,
    necessary conditions are found that the…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.semanticscholar.org](https://www.semanticscholar.org/paper/Least-squares-quantization-in-PCM-Lloyd/9241ea3d8cb85633d314ecb74b31567b8e73f6af?source=post_page-----76da35b5670a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Strengths:**'
  prefs: []
  type: TYPE_NORMAL
- en: K-Means is efficient, widely studied and understood, applicable to various domains,
    and is strong for spherical clusters (an assumption of the model).
  prefs: []
  type: TYPE_NORMAL
- en: '**Python Code:**'
  prefs: []
  type: TYPE_NORMAL
- en: K-Means algorithm using [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)
  prefs: []
  type: TYPE_NORMAL
- en: K-Medoids (PAM)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Summary:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The K-Medoids algorithm, also known as Partitioning Around Medoids (PAM), is
    a clustering technique similar to K-Means but with a crucial difference: instead
    of using the mean (centroid) to represent a cluster, it employs the actual data
    point (medoid) that minimizes the average dissimilarity to all other points in
    the cluster. This makes K-Medoids more robust against outliers and resistant to
    the influence of extreme values. The algorithm iteratively refines cluster assignments
    by choosing data points as medoids and updating until a stable configuration is
    reached. While K-Medoids is more robust to noise than K-Means, it can still be
    computationally expensive due to its exhaustive search for the best medoids.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Original Paper / Helpful Research:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.sciencedirect.com/science/article/pii/S1877050916000971?source=post_page-----76da35b5670a--------------------------------)
    [## Analysis of K-Means and K-Medoids Algorithm For Big Data'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering plays a very vital role in exploring data, creating predictions and
    to overcome the anomalies in the data…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1877050916000971?source=post_page-----76da35b5670a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Strengths:**'
  prefs: []
  type: TYPE_NORMAL
- en: K-Medoids is better against outliers compared to centroids (which K-Means uses)
    and it identifies cluster centers using actual data points.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python Code:**'
  prefs: []
  type: TYPE_NORMAL
- en: K-Medoids algorithm Using [pyclustering](https://pyclustering.github.io/docs/0.9.0/html/d0/dd3/classpyclustering_1_1cluster_1_1kmedoids_1_1kmedoids.html)
  prefs: []
  type: TYPE_NORMAL
- en: CLARANS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Summary:**'
  prefs: []
  type: TYPE_NORMAL
- en: CLARANS (Clustering Large Applications based on Randomized Search) is a clustering
    algorithm that combines sampling techniques with the Partitioning Around Medoids
    (PAM) method. It employs a randomized search to discover clusters without relying
    on additional data structures. CLARANS is particularly robust in high-dimensional
    spaces, as it doesn’t assume a specific distance function, and it can effectively
    identify clusters with non-convex shapes. However, its efficiency comes at the
    cost of increased computational complexity, making it potentially slower than
    other partitioning methods.
  prefs: []
  type: TYPE_NORMAL
- en: '**Original Paper / Helpful Research:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://ieeexplore.ieee.org/abstract/document/1033770](https://ieeexplore.ieee.org/abstract/document/1033770)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Strengths:**'
  prefs: []
  type: TYPE_NORMAL
- en: CLARANS is robust against increases in dimensionality and can identify polygon-shaped
    objects effectively.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python Code:**'
  prefs: []
  type: TYPE_NORMAL
- en: CLARANS algorithm using [*pyclustering*](https://pyclustering.github.io/docs/0.9.0/html/d6/d42/classpyclustering_1_1cluster_1_1clarans_1_1clarans.html)
  prefs: []
  type: TYPE_NORMAL
- en: '**ISODATA**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Summary:**'
  prefs: []
  type: TYPE_NORMAL
- en: The ISODATA (Iterative Self-Organizing Data Analysis Technique) algorithm is
    an iterative and adaptive clustering method, considered a variant of the K-Means
    algorithm. It dynamically adjusts clusters through splitting and merging based
    on user-defined thresholds, such as minimum points for each cluster, maximum variance
    for splitting clusters, and minimum distance for merging clusters. This adaptability
    allows ISODATA to handle outliers effectively through its splitting procedure
    and prevent the formation of elongated clusters. Despite its advantages in handling
    noise and varying cluster shapes, ISODATA’s sensitivity to input parameters and
    the need for careful tuning are notable considerations.
  prefs: []
  type: TYPE_NORMAL
- en: '**Original Paper / Helpful Research:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.sciencedirect.com/topics/computer-science/isodata-algorithm?source=post_page-----76da35b5670a--------------------------------)
    [## Isodata Algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: The first automatic threshold selecting method was probably by isodata algorithm,
    which was originally proposed by…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.sciencedirect.com](https://www.sciencedirect.com/topics/computer-science/isodata-algorithm?source=post_page-----76da35b5670a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Strengths:**'
  prefs: []
  type: TYPE_NORMAL
- en: ISODATA handles outliers better than k-means and adjusts clusters dynamically
    during iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python Code:**'
  prefs: []
  type: TYPE_NORMAL
- en: ISODATA is a version of K-Means that can be implemented using the code in the
    k-means section above with some adaptations You can also use this [Github code](https://github.com/PyRadar/pyradar/blob/master/pyradar/classifiers/isodata.py)
    by PyRadar.
  prefs: []
  type: TYPE_NORMAL
- en: Density-Based Clustering Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Definition:** Density-based clustering identifies clusters as contiguous
    regions of high data point density separated by regions of lower density. It is
    based on the idea that clusters are areas of higher density compared to their
    surroundings.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use-Cases:**'
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with arbitrary-shaped clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with noisy data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strengths:**'
  prefs: []
  type: TYPE_NORMAL
- en: Capable of discovering clusters of arbitrary shapes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robust against noise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weaknesses:**'
  prefs: []
  type: TYPE_NORMAL
- en: Efficiency and performance may struggle with high-dimensional data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBSCAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Summary:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm
    is a density-based clustering technique that identifies clusters based on the
    density of data points in the feature space. It classifies points into three categories:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Core Points: dense regions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Border Points: points on the outskirts of a cluster.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Noise Points: isolated points.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DBSCAN efficiently discovers clusters with arbitrary shapes, requires no predefined
    number of clusters, and is robust to noise. However, its performance can be influenced
    by the choice of parameters such as the neighborhood radius and minimum number
    of points required to form a dense region.
  prefs: []
  type: TYPE_NORMAL
- en: '**Original Paper / Helpful Research:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://dl.acm.org/doi/10.5555/3001460.3001507?source=post_page-----76da35b5670a--------------------------------)
    [## A density-based algorithm for discovering clusters in large spatial databases
    with noise |…'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering algorithms are attractive for the task of class identification in
    spatial databases. However, the…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: dl.acm.org](https://dl.acm.org/doi/10.5555/3001460.3001507?source=post_page-----76da35b5670a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Strengths:**'
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN is effective against noise and discovers clusters of arbitrary shapes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python Code:**'
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN algorithm using [sklean](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)
  prefs: []
  type: TYPE_NORMAL
- en: DENCLUE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Summary:**'
  prefs: []
  type: TYPE_NORMAL
- en: The DENCLUE (Density Clustering) algorithm is a density-based clustering technique
    that determines clusters based on the local density attractors, representing local
    maxima in an overall density function. It employs an influence function to calculate
    the distance between data points, and the density function is the cumulative sum
    of these influences. DENCLUE is designed to identify clusters with arbitrary shapes
    and exhibits good scalability, making it suitable for datasets with unpredictable
    structures. However, DENCLUE is sensitive to input parameters and can be affected
    by the curse of dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: '**Original Paper / Helpful Research:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://dl.acm.org/doi/10.1145/3368691.3368724?source=post_page-----76da35b5670a--------------------------------)
    [## An overview of various enhancements of DENCLUE algorithm | Proceedings of
    the Second International…'
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTICE OF CONCERN: ACM has received evidence that casts doubt on the integrity
    of the peer review process for the DATA…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: dl.acm.org](https://dl.acm.org/doi/10.1145/3368691.3368724?source=post_page-----76da35b5670a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Strengths:**'
  prefs: []
  type: TYPE_NORMAL
- en: DENCLUE is scalable and spots clusters with unpredictable shapes and is effective
    against noise.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python Code:**'
  prefs: []
  type: TYPE_NORMAL
- en: There are no libraries that support DENCLUE. See mgarrett57’s implementation
    on [Github here](https://github.com/mgarrett57/DENCLUE/blob/master/denclue.py).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we covered 10 of the most commonly used clustering algorithms
    within the areas of Hierarchical, Partitional, and Density-Based Clustering. While
    this article is meant to be exhaustive of the basics, I will be going in depth
    on each of these algorithms and providing more context in future articles dedicated
    specifically to each algorithm providing a full review of the literature surrounding
    them. We have also linked to research papers for each algorithm as helpful starting
    points for more exploration for each. I hope this article has helped serve as
    a good starting point for your data exploration projects using clustering.
  prefs: []
  type: TYPE_NORMAL
