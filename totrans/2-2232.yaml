- en: 'Unraveling the Design Pattern of Physics-Informed Neural Networks: Part 06'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭开物理信息神经网络设计模式的面纱：第06部分
- en: 原文：[https://towardsdatascience.com/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-06-bcb3557199e2](https://towardsdatascience.com/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-06-bcb3557199e2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-06-bcb3557199e2](https://towardsdatascience.com/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-06-bcb3557199e2)
- en: Bring causality to PINN training
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将因果关系引入PINN训练
- en: '[](https://shuaiguo.medium.com/?source=post_page-----bcb3557199e2--------------------------------)[![Shuai
    Guo](../Images/d673c066f8006079be5bf92757e73a59.png)](https://shuaiguo.medium.com/?source=post_page-----bcb3557199e2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bcb3557199e2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bcb3557199e2--------------------------------)
    [Shuai Guo](https://shuaiguo.medium.com/?source=post_page-----bcb3557199e2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shuaiguo.medium.com/?source=post_page-----bcb3557199e2--------------------------------)[![Shuai
    Guo](../Images/d673c066f8006079be5bf92757e73a59.png)](https://shuaiguo.medium.com/?source=post_page-----bcb3557199e2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bcb3557199e2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bcb3557199e2--------------------------------)
    [Shuai Guo](https://shuaiguo.medium.com/?source=post_page-----bcb3557199e2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bcb3557199e2--------------------------------)
    ·9 min read·Jun 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----bcb3557199e2--------------------------------)
    ·阅读时间9分钟·2023年6月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0c5a39882aa4755f7434a2421e21550e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c5a39882aa4755f7434a2421e21550e.png)'
- en: Photo by [Delano Ramdas](https://unsplash.com/@delanodzr?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Delano Ramdas](https://unsplash.com/@delanodzr?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Welcome to the 6th blog of this series, where we continue our exciting journey
    of exploring ***design patterns*** of physics-informed neural networks (PINN)🙌
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到本系列的第六篇博客，我们将继续探索物理信息神经网络（PINN）的***设计模式***🙌
- en: 'In this episode, we will talk about bringing **causality** to the training
    of physics-informed neural nets. As suggested by the paper we will look at today:
    respecting causality is all you need!'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一集里，我们将讨论将**因果关系**引入物理信息神经网络训练的内容。正如我们今天将要查看的论文所建议的：尊重因果关系就是你所需的一切！
- en: As always, let’s begin by talking about the current matters in question, then
    move on to the suggested remedies, the evaluation procedure, and the advantages
    and disadvantages of the proposed method. Finally, we will conclude the blog by
    exploring potential opportunities that lie ahead.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，让我们首先讨论当前的问题，然后转到建议的解决方案、评估程序以及所提方法的优缺点。最后，我们将通过探索潜在的机会来结束博客。
- en: 'As this series continues to expand, the collection of PINN design patterns
    grows even richer*🙌* Here’s a sneak peek at what awaits you:'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 随着这一系列的不断扩展，PINN设计模式的集合变得更加丰富*🙌* 这里是一些即将到来的内容的预览：
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PINN design pattern 01: Optimizing the residual point distribution](/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[PINN设计模式01：优化残差点分布](/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)'
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PINN design pattern 02: Dynamic solution interval expansion](/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-02-2156516f2791)'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[PINN设计模式02：动态解区间扩展](/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-02-2156516f2791)'
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PINN design pattern 03: Training PINN with gradient boosting](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-03-fe365ef480d9)'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[PINN设计模式03：使用梯度提升训练PINN](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-03-fe365ef480d9)'
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PINN design pattern 04: Gradient-enhanced PINN learning](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-04-c778f4829dde)'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[PINN设计模式04：梯度增强的PINN学习](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-04-c778f4829dde)'
- en: ''
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PINN design pattern 05: Automated hyperparameter tuning](/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-05-67a35a984b23)'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[PINN设计模式05：自动化超参数调整](/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-05-67a35a984b23)'
- en: ''
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PINN design pattern 07: Active learning with PINN](/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-07-4ecb543b616a)'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[PINN设计模式07：使用PINN的主动学习](/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-07-4ecb543b616a)'
- en: Let’s dive in!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨！
- en: 1\. Paper at a glance 🔍
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 论文概述 🔍
- en: '**Title**: Respecting causality is all you need for training physics-informed
    neural networks'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标题**：尊重因果性是训练物理信息神经网络所需的一切'
- en: '**Authors**: S. Wang, S. Sankaran, P. Perdikaris'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**作者**：S. Wang, S. Sankaran, P. Perdikaris'
- en: '**Institutes**: University of Pennsylvania'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机构**：宾夕法尼亚大学'
- en: '**Link**: [arXiv](https://arxiv.org/abs/2203.07404), [GitHub](https://github.com/PredictiveIntelligenceLab/CausalPINNs)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**链接**：[arXiv](https://arxiv.org/abs/2203.07404), [GitHub](https://github.com/PredictiveIntelligenceLab/CausalPINNs)'
- en: 2\. Design pattern 🎨
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 设计模式 🎨
- en: 2.1 Problem 🎯
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 问题 🎯
- en: Physics-informed neural networks (PINNs) are a major leap in combining observational
    data and physical laws across various fields. In practice, however, they are often
    observed to be unable to tackle high nonlinearity, multi-scale dynamics, or chaotic
    problems, and tend to converge to erroneous solutions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 物理信息神经网络（PINNs）在结合观察数据和物理法则方面是一个重大进展。然而，在实践中，它们常常无法处理高度非线性、多尺度动态或混沌问题，并趋向于收敛到错误的解。
- en: Why this is the case?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会这样呢？
- en: Well, the fundamental issue lies in the **violation of causality** in the PINN
    formulations, as revealed by the current paper.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，根本问题在于当前论文揭示的PINN公式中的**因果性违背**。
- en: Causality, in the physical sense, implies that the state at a future time point
    depends on the state at the current or past time points. In PINN training, however,
    this principle may not hold true; these networks might be implicitly biased towards
    first approximating PDE solutions at future states before even resolving initial
    conditions, essentially “jumping ahead” in time and thereby violating causality.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 物理意义上的因果性意味着未来时刻的状态依赖于当前或过去时刻的状态。然而，在PINN训练中，这一原则可能不成立；这些网络可能隐含偏向于首先在未来状态下近似PDE解，然后才解决初始条件，实质上是“跳过”时间，从而违反因果性。
- en: In contrast, traditional numerical methods inherently preserve causality via
    a time-marching strategy. For instance, when discretizing PDE in time, these methods
    ensure the solution at time *t* is resolved before approximating the solution
    at time *t* + ∆*t*. Hence, each future state is sequentially built upon the resolved
    past states, thus preserving the principle of causality.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，传统的数值方法通过时间推进策略固有地保持因果性。例如，在时间上离散化PDE时，这些方法确保在近似时间 *t* + ∆*t* 时的解之前解决时间
    *t* 时的解。因此，每个未来状态是依赖于已解决的过去状态，从而保持因果性原则。
- en: 'This understanding of the problem brings us to an intriguing question: how
    do we rectify this violation of causality in PINNs, bringing them in line with
    fundamental physical laws?'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对这个问题的理解引出了一个有趣的问题：我们如何纠正PINN中的因果性违背，使其符合基本物理法则？
- en: '![](../Images/3bb498e14a463d651a5fcfaa8d26ffba.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3bb498e14a463d651a5fcfaa8d26ffba.png)'
- en: PINN workflow. Naive PINN does not have “causality” baked in. One simple yet
    effective strategy is to dynamically weight the PDE residual losses at different
    time instances. (Image by this blog author)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: PINN工作流程。简单的PINN没有“因果性”嵌入其中。一种简单而有效的策略是动态加权不同时间实例的PDE残差损失。（图像由本博客作者提供）
- en: 2.2 Solution 💡
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 解决方案 💡
- en: The key idea here is to **re-formulate the PINN loss function**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键思想是**重新制定PINN损失函数**。
- en: Specifically, we can introduce a dynamic weighting scheme to account for different
    contributions of PDE residual loss evaluated at different temporal locations.
    Let’s break it down using illustrations.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们可以引入动态加权方案，以考虑在不同时间位置评估的PDE残差损失的不同贡献。让我们通过插图来详细分析。
- en: 'For simplicity, let’s assume the collocation points are uniformly sampled in
    the spatial-temporal domain of our simulation, as illustrated in the figure below:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我们假设在空间-时间域中的配点是均匀采样的，如下图所示：
- en: '![](../Images/c1ca3613665e64381681b8090b2bf771.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1ca3613665e64381681b8090b2bf771.png)'
- en: Total PDE residual loss is calculated over all collocation points, and its gradient
    values are used to drive network parameter optimization. (Image by this blog author)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 总PDE残差损失是计算在所有配点上的，其梯度值用于驱动网络参数优化。（图像由本博客作者提供）
- en: To proceed with one step of gradient descent, we must first calculate the cumulative
    PDE residual loss across all collocation points. One specific way to do that is
    by first calculating the losses related to the collocation points sampled at individual
    time instances, and then performing a “simple sum” to get the total loss. The
    following gradient descent step can then be conducted based on the calculated
    total loss to optimize the PINN weights.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行一步梯度下降，我们必须首先计算所有配点的累计PDE残差损失。具体做法是首先计算与在各个时间点采样的配点相关的损失，然后进行“简单的求和”以获得总损失。接下来的梯度下降步骤可以基于计算得到的总损失来优化PINN权重。
- en: Of course, the exact order of summation over collocation points doesn’t influence
    the total loss computation; all methods yield the same result. However, the decision
    to group loss calculations by temporal order is purposeful, designed to emphasize
    the element of ‘temporality’. This concept is crucial for understanding the proposed
    causal training strategy.
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当然，配点的求和顺序不会影响总损失的计算；所有方法都会得到相同的结果。然而，按时间顺序分组损失计算的决定是有目的的，旨在强调‘时间性’的元素。这个概念对于理解提出的因果训练策略至关重要。
- en: In this process, the PDE residual losses evaluated at different temporal locations
    are treated equally. meaning that all temporal residual losses are simultaneously
    minimized.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，评估在不同时间位置的PDE残差损失被视为同等重要。这意味着所有时间残差损失同时被最小化。
- en: This approach, however, risks the PINN violating temporal causality, as it doesn’t
    enforce a chronological regularization for minimizing the temporal residual loss
    at successive time intervals.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法存在风险，可能会导致PINN违反时间因果关系，因为它没有对在连续时间间隔最小化时间残差损失进行时间上的规范化。
- en: So, how can we coax PINN to adhere to the temporal precedence during training?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何引导PINN在训练过程中遵循时间优先原则呢？
- en: The secret is in **selectively weighting individual temporal residual losses**.
    For instance, suppose that at the current iteration, we want the PINN to focus
    on approximating the solutions at time instance *t*₁. Then, we could simply put
    a higher weight on Lᵣ(*t*₁), which is the temporal residual lossat *t*₁. This
    way, Lᵣ(*t*₁) will become a dominant component in the final total loss, and as
    a result, the optimization algorithm will prioritize minimizing Lᵣ(*t*₁), which
    aligns with our goal of approximating solutions at time instance *t*₁ first.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 秘诀在于**有选择地加权各个时间残差损失**。例如，假设在当前迭代中，我们希望PINN专注于在时间点*t*₁处逼近解。那么，我们可以简单地在Lᵣ(*t*₁)上加上更高的权重，这就是在*t*₁处的时间残差损失。这样，Lᵣ(*t*₁)将成为最终总损失中的主导成分，结果是优化算法将优先最小化Lᵣ(*t*₁)，这与我们首先在时间点*t*₁逼近解的目标一致。
- en: '![](../Images/3e565859efb6486eea4297bd96b2a18d.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e565859efb6486eea4297bd96b2a18d.png)'
- en: By assigning weights to temporal residual loss at different time instances,
    we can steer the optimizer to focus on minimizing loss at our desired time instances.
    (Image by this blog author)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在不同时间点分配时间残差损失的权重，我们可以引导优化器专注于在我们期望的时间点最小化损失。（图片由本博客作者提供）
- en: In the subsequent iteration, we shift our focus to the solutions at time instance
    t₂. By increasing the weight on Lᵣ(t₂), it now becomes the main factor in the
    total loss calculation. The optimization algorithm is thus directed towards minimizing
    Lᵣ(t₂), improving the prediction accuracy of the solutions at t₂.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在随后的迭代中，我们将注意力转向时间点*t*₂处的解。通过增加Lᵣ(*t*₂)的权重，它现在成为总损失计算中的主要因素。因此，优化算法被引导去最小化Lᵣ(*t*₂)，从而提高了在*t*₂处解的预测准确性。
- en: '![](../Images/838ffec64f1ec82607de97a20824cbe4.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/838ffec64f1ec82607de97a20824cbe4.png)'
- en: (Image by this blog author)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: （图片由本博客作者提供）
- en: As can be seen from our previous walk-through, varying the weights assigned
    to temporal residual losses at different time instances enables us to direct the
    PINN to approximate solutions at our chosen time instances.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们之前的演示可以看出，通过改变在不同时间点分配的时间残差损失权重，我们可以引导PINN在我们选择的时间点逼近解。
- en: So, how does this assist in incorporating a causal structure into PINN training?
    It turns out, we can design a causal training algorithm (as proposed in the paper),
    such that **the weight for the temporal residual loss at time *t*, *i.e.*,Lᵣ(*t*),is
    significant only when the losses before *t* (Lᵣ(*t-1*), Lᵣ(*t-2*), etc.) are sufficiently
    small**. This effectively means that the neural network begins minimizing Lᵣ(t)
    only when it has achieved satisfactory approximation accuracy for prior steps.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这如何帮助将因果结构纳入PINN训练中呢？事实证明，我们可以设计一种因果训练算法（如论文中提出的），使得**时间*t*的时间残差损失的权重Lᵣ(*t*)，只有在*t*之前的损失（Lᵣ(*t-1*),
    Lᵣ(*t-2*), 等）足够小时才显著**。这有效地意味着神经网络仅在对先前步骤的近似准确度令人满意时才开始最小化Lᵣ(t)。
- en: 'To determine the weight, the paper proposed a simple formula: the weight ωᵢ
    is set to be inversely exponentially proportional to the magnitude of the cumulative
    temporal residual loss from all the previous time instances. This ensures that
    the weight ωᵢ will only be active (i.e., with a sufficiently large value) when
    the cumulative loss from all previous time instances is small, i.e., PINN can
    already accurately approximate solutions at previous time steps. This is how *temporal
    causality* is reflected in the PINN training.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定权重，论文提出了一个简单的公式：权重ωᵢ设置为与所有之前时间实例的累计时间残差损失的大小成反向指数比例。这确保了当所有之前时间实例的累计损失较小时，权重ωᵢ才会活跃（即，具有足够大的值），即PINN已经能够准确地近似之前时间步的解。这就是*时间因果关系*在PINN训练中体现的方式。
- en: '![](../Images/757433ba91f9576b3faf5815e50f1312.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/757433ba91f9576b3faf5815e50f1312.png)'
- en: (Image by this blog author)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来自博客作者）
- en: 'With all components explained, we can piece together the full causal training
    algorithm as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 解释了所有组件后，我们可以将完整的因果训练算法拼凑如下：
- en: '![](../Images/3f46749a291db166ce65e1224516938d.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f46749a291db166ce65e1224516938d.png)'
- en: Illustration of the proposed causal training algorithm in the paper. (Image
    by this blog author)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中提出的因果训练算法的示意图。（图片来自博客作者）
- en: 'Before we conclude this section, there are two remarks worth mentioning:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本节之前，有两个值得提及的备注：
- en: The paper suggested using the magnitude of ωᵢ as the stopping criterion for
    PINN training. Specifically, when all ωᵢ’s are larger than a pre-defined threshold
    δ, the training may be deemed completed. The recommended value for δ is 0.99.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 论文建议使用ωᵢ的大小作为PINN训练的停止标准。具体来说，当所有ωᵢ的值都大于预定义的阈值δ时，训练可以认为完成。推荐的δ值为0.99。
- en: Selecting a proper value for ε is important. Although this value can be tuned
    via conventional hyperparameter tuning, the paper recommended an annealing strategy
    for adjusting ε. Details can be found in the [original paper](https://arxiv.org/abs/2203.07404)
    (section 3).
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择合适的ε值很重要。虽然可以通过传统的超参数调整来调整此值，但论文推荐了一种退火策略来调整ε。详细信息请参见[原始论文](https://arxiv.org/abs/2203.07404)（第3节）。
- en: 2.3 Why the solution might work 🛠️
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 为什么这个解决方案可能有效 🛠️
- en: By dynamically weighting temporal residual losses evaluated at different time
    instances, the proposed algorithm is able to steer the PINN training to first
    approximate PDE solutions at earlier times before even trying to resolve the solution
    at later times.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过动态加权在不同时间实例评估的时间残差损失，提出的算法能够引导PINN训练首先在较早的时间近似PDE解，然后再尝试解决较晚时间的解。
- en: This property facilitates the explicit incorporation of temporal causality into
    the PINN training and constitutes the key factor in potentially more accurate
    simulations of physical systems.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个属性有助于将时间因果关系明确地融入PINN训练中，并构成潜在更准确的物理系统模拟的关键因素。
- en: 2.4 Benchmark ⏱️
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 基准 ⏱️
- en: The paper considered a total of 3 different benchmark equations. All problems
    are forward problems where PINN is used to solve the PDEs.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 论文考虑了总共3个不同的基准方程。所有问题都是前向问题，其中PINN用于求解PDE。
- en: 'Lorenz system: these equations arise in studies of convection and instability
    in planetary atmospheric convection. Lorenz system exhibits strong sensitivity
    to its initial conditions, and it is known to be challenging for vanilla PINN.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 洛伦兹系统：这些方程出现在行星大气对流和不稳定性的研究中。洛伦兹系统对其初始条件具有强烈的敏感性，且对普通PINN来说是具有挑战性的。
- en: '![](../Images/24033d97a935bef0fb3fd3f9d1aa3c90.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24033d97a935bef0fb3fd3f9d1aa3c90.png)'
- en: 'Kuramoto–Sivashinsky equation: this equation describes the dynamics of various
    wave-like patterns, such as flames, chemical reactions, and surface waves. It
    is known to exhibit a wealth of spatiotemporal chaotic behaviors.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuramoto–Sivashinsky 方程：该方程描述了各种波动模式的动态，如火焰、化学反应和表面波。它被认为表现出丰富的时空混沌行为。
- en: '![](../Images/780bc2f9867af18779e1983004bc910d.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/780bc2f9867af18779e1983004bc910d.png)'
- en: 'Navier-Stokes equation: this set of partial differential equations describes
    the motion of fluid substances and constitutes the fundamental equations in fluid
    mechanics. The current paper considered a classical two-dimensional decaying turbulence
    example in a square domain with periodic boundary conditions.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Navier-Stokes 方程：这组偏微分方程描述了流体物质的运动，并构成了流体力学的基本方程。当前论文考虑了一个经典的二维衰减湍流示例，位于一个具有周期性边界条件的方形区域内。
- en: '![](../Images/2587329316d8589be36ab58853699baa.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2587329316d8589be36ab58853699baa.png)'
- en: 'The benchmark studies yielded that:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 基准研究表明：
- en: The proposed causal training algorithm was able to achieve 10–100x improvements
    in accuracy compared to the vanilla PINN training scheme.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与原始PINN训练方案相比，提出的因果训练算法能够实现10到100倍的准确性改进。
- en: Demonstrated that PINNs equipped with causal training algorithm can successfully
    simulate highly nonlinear, multi-scale, and chaotic systems.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演示了配备因果训练算法的PINNs能够成功模拟高度非线性、多尺度和混沌系统。
- en: 2.5 Strengths and Weaknesses ⚡
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 优势与劣势 ⚡
- en: '**Strengths** 💪'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**优势** 💪'
- en: Respects the causality principle and makes PINN training more transparent.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尊重因果性原则，使PINN训练更加透明。
- en: Introduces significant accuracy improvements, allowing it to tackle problems
    that have remained elusive to PINNs.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入了显著的准确性改进，使其能够解决那些对PINNs仍然难以处理的问题。
- en: Provides a practical quantitative criterion for assessing the training convergence
    of PINNs.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供了一个实用的定量标准，用于评估PINNs的训练收敛性。
- en: Negligible added computational cost compared to the vanilla PINN training strategy.
    The only added cost is to compute the ωᵢ’s, which is negligible compared to auto-diff
    operations.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与原始PINN训练策略相比，计算成本几乎可以忽略不计。唯一的额外成本是计算ωᵢ，这与自动微分操作相比几乎可以忽略。
- en: '**Weaknesses** 📉'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**劣势** 📉'
- en: Introduced a new hyperparameter ε, which controls the scheduling of the weights
    for temporal residual losses. Although the authors proposed an annealing strategy
    as an alternative to avoid the tedious hyper-parameter tuning.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入了新的超参数ε，该参数控制时间残差损失权重的调度。尽管作者提出了一种退火策略作为替代方案，以避免繁琐的超参数调优。
- en: Complicated the PINN training workflow. Special attention should be given to
    the temporal weights ωᵢ’s, as they are now functions of the network trainable
    parameters (e.g., layer weights and bias), and the gradient associated with the
    computation of ωᵢ should not be back-propagated.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使PINN训练工作流程复杂化。应特别关注时间权重ωᵢ，因为它们现在是网络可训练参数（如层权重和偏差）的函数，计算ωᵢ的梯度不应反向传播。
- en: 2.6 Alternatives 🔀
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 替代方案 🔀
- en: 'There are a couple of alternative methods that are trying to address the same
    issue as the current “causal training algorithm”:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种替代方法试图解决与当前“因果训练算法”相同的问题：
- en: 'Adaptive time sampling strategy ([Wight et al.](https://arxiv.org/abs/2007.04542)):
    instead of weighting the collocation points at different time instances, this
    strategy modifies the sampling density of collocation points. This has a similar
    effect of shifting the focus of the optimizer on minimizing temporal losses at
    different time instances.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自适应时间采样策略 ([Wight et al.](https://arxiv.org/abs/2007.04542))：该策略不是对不同时间点的配点进行加权，而是修改配点的采样密度。这种方法类似于将优化器的关注点转移到不同时间点的时间损失最小化上。
- en: '“Time-marching”/“Curriculum training” strategy (e.g., [Krishnapriyan et al.](https://arxiv.org/abs/2109.01050)):
    the temporal causality is respected via learning the solution sequentially within
    separate time windows.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “时间推进”/“课程训练”策略（例如，[Krishnapriyan et al.](https://arxiv.org/abs/2109.01050)）：通过在不同的时间窗口内顺序学习解决方案来尊重时间因果性。
- en: However, compared to those alternative approaches, the “causal training algorithm”
    put temporal causality front and center, is more adaptable to a variety of problems,
    and enjoys low added computational cost.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与那些替代方法相比，“因果训练算法”将时间因果性置于核心，更适应各种问题，且计算成本低。
- en: 3 Potential Future Improvements 🌟
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 潜在的未来改进 🌟
- en: 'There are several possibilities to further improve the proposed strategy:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种可能性可以进一步改进提出的策略：
- en: Incorporating more sophisticated data sampling strategies, such as adaptive-
    and residual-based sampling methods, to further improve the training efficiency
    and accuracy.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合更复杂的数据采样策略，如自适应和基于残差的采样方法，以进一步提高训练效率和准确性。
- en: To learn more about how to optimize the residual points distribution, check
    out [this blog](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)
    in the PINN design pattern series.
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 欲了解如何优化残差点分布，请查看[此博客](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)中的PINN设计模式系列。
- en: Extend to inverse problem settings. How to ensure casualty when point sources
    of information (i.e., observational data) are available would require an extension
    of the currently proposed training strategy.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展到逆问题设置。当信息点源（即观测数据）可用时，如何确保因果关系将需要对目前提出的训练策略进行扩展。
- en: 4 Takeaways 📝
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4个要点📝
- en: 'In this blog, we looked at how to bring causality to PINN training with a reformulation
    of the training objectives. Here are the highlights of the design pattern proposed
    in the paper:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客中，我们探讨了如何通过重新制定训练目标将因果关系引入PINN训练。以下是论文中提出的设计模式的亮点：
- en: '[Problem]: How to make PINNs respect the causality principle underpinning the
    physical systems?'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[问题]：如何让PINN遵守支撑物理系统的因果关系原则？'
- en: '[Solution]: **Re-formulating the PINN training objective**, where a dynamic
    weighting scheme is introduced to gradually shift the training focus from earlier
    time steps to later time steps.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[解决方案]：**重新制定PINN训练目标**，引入动态加权方案，以逐步将训练重点从早期时间步骤转移到后期时间步骤。'
- en: '[Potential benefits]: 1\. Significantly improved PINNs’ accuracy. 2\. Expanded
    the applicability of PINNs to complex problems.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[潜在收益]：1\. 显著提高PINN的准确性。2\. 扩展PINN对复杂问题的适用性。'
- en: 'Here is the PINN design card to summarize the takeaways:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这是PINN设计卡，总结了主要收获：
- en: '![](../Images/f44122165c8b0f741c7006072f4a7695.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f44122165c8b0f741c7006072f4a7695.png)'
- en: PINN design pattern proposed in the paper. (Image by this blog author)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中提出的PINN设计模式。（图片由此博客作者提供）
- en: 'I hope you found this blog useful! To learn more about PINN design patterns,
    feel free to check out other posts in this series:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你觉得这篇博客有用！要了解更多关于PINN设计模式的信息，请随时查看此系列中的其他帖子：
- en: '[PINN design pattern 01: Optimizing the residual point distribution](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PINN设计模式01：优化残差点分布](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)'
- en: '[PINN design pattern 02: Dynamic solution interval expansion](/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-02-2156516f2791)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PINN设计模式02：动态解决方案区间扩展](/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-02-2156516f2791)'
- en: '[PINN design pattern 03: PINN training with gradient boost](/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-03-fe365ef480d9)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PINN设计模式03：带有梯度提升的PINN训练](/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-03-fe365ef480d9)'
- en: '[PINN design pattern 04: Gradient-enhanced PINN learning](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-04-c778f4829dde)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PINN设计模式04：增强梯度的PINN学习](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-04-c778f4829dde)'
- en: '[PINN design pattern 05: Hyperparameter tuning for PINN](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-05-67a35a984b23)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PINN设计模式05：PINN的超参数调优](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-05-67a35a984b23)'
- en: '[PINN design pattern 07: Active learning with PINN](/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-07-4ecb543b616a)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PINN设计模式07：与PINN的主动学习](/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-07-4ecb543b616a)'
- en: Looking forward to sharing more insights with you in the upcoming blogs!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 期待在即将到来的博客中与您分享更多见解！
- en: Reference 📑
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考📑
- en: '[1] Wang et al., Respecting causality is all you need for training physics-informed
    neural networks, [arXiv](https://arxiv.org/abs/2203.07404), 2022.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wang等人，尊重因果关系是训练物理信息神经网络所需的一切](https://arxiv.org/abs/2203.07404)，[arXiv](https://arxiv.org/abs/2203.07404)，2022年。'
