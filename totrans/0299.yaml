- en: An Intuition for How Models like ChatGPT Work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/an-intuition-for-how-models-like-chatgpt-work-c7f01616bd6d](https://towardsdatascience.com/an-intuition-for-how-models-like-chatgpt-work-c7f01616bd6d)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Providing an intuition on the ideas behind popular transformer models like ChatGPT
    and other large language models (LLMs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dkhundley.medium.com/?source=post_page-----c7f01616bd6d--------------------------------)[![David
    Hundley](../Images/1779ef96ec3d338f8fe4a9567ba7b194.png)](https://dkhundley.medium.com/?source=post_page-----c7f01616bd6d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c7f01616bd6d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c7f01616bd6d--------------------------------)
    [David Hundley](https://dkhundley.medium.com/?source=post_page-----c7f01616bd6d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c7f01616bd6d--------------------------------)
    ·10 min read·Dec 30, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6298bdd024c30f5750f9870f35f92f5a.png)'
  prefs: []
  type: TYPE_IMG
- en: Title card created by the author
  prefs: []
  type: TYPE_NORMAL
- en: As we wind down 2023, it’s incredible to think about how much Generative AI
    has already impacted our daily lives. Starting with ChatGPT’s release in November
    2022, this space has evolved so quickly that it’s hard to believe that it’s been
    just one year in which all these advancements have come out.
  prefs: []
  type: TYPE_NORMAL
- en: While the results are quite amazing, the underlying complexity has led a lot
    of people to publicly speculate on how these large language models (LLMs) work.
    Some people have speculated that these models are pulling from a preformulated
    database of responses, and some have gone as far to speculate that these LLMs
    have gained a human-level of sentience. These are extreme stances, and as you
    might guess, both are incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: You may have heard that these **LLMs are next-word predictors, meaning that
    they use probability to determine the next word that should come in a sentence**.
    This understanding is technically correct, but it’s a little too high level to
    sufficiently understand these models. In order to build a stronger intuition,
    we need to go deeper. **The intention of this post is to provide business leaders
    with a deep enough understanding of these models that they can make educated decisions
    on how to appropriately approach Generative AI for their respective companies**.
    We’ll keep things at more of a conceptual and intuitive level and stray away from
    the deep math behind these models.
  prefs: []
  type: TYPE_NORMAL
- en: Making Sense of Language
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider the sentence, “I like to drink _______ in the morning.” How might
    you discern how to fill in that blank? Most reasonable people might fill in answers
    like coffee, water, or juice. The more silly among us might say something like
    beer or sour milk, but all these various options fixate on one important context
    clue: drinking. That alone narrows down what that blank could be, but those who
    took in the full context of the sentence also noticed the word “morning” and thus
    narrowed the context even further. In other words, “drink” + “morning” = something
    in the neighborhood of a breakfast beverage.'
  prefs: []
  type: TYPE_NORMAL
- en: This was simple for us to do because the phrase I gave was in English, and you’re
    presumably reading this in English. But what if we don’t have that direct understanding
    of those contextual words like “drink” and “morning” to understand how to properly
    fill in that blank?
  prefs: []
  type: TYPE_NORMAL
- en: This is precisely the dilemma that computers face. Computers have no semantic
    understanding of the world because at the core of the computer’s CPU or GPU, it’s
    blasting through a blazing amount of one’s and zero’s. In other words, it has
    no intuition that the sky is blue, what morning is, nor how delicious pizza is.
  prefs: []
  type: TYPE_NORMAL
- en: So you might be wondering, how does a computer get around this problem?
  prefs: []
  type: TYPE_NORMAL
- en: I actually got to explore this notion firsthand by playing a new indie video
    game called [*Chants of Sennaar*](https://youtu.be/__hzPH3tcvA?si=5diH-3hoT179hy7J).
    In the game, the player takes control of a character who is in a land of people
    where people speak in decipherable glyphs. As the game progresses, the player
    becomes acclimated by context clues demonstrated in the environment. For example,
    two of the early words you learn are “you” and “me”, and that’s because one of
    non-playable characters (NPCs) points at itself while stating the glyph for “me”
    and points at the player character while stating the glyph for “you.” It’s very
    much how you might imagine historians translating Egyptian hieroglyphics into
    more modern languages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice in these cases, it did not matter at all what the individual characters
    were. In *Chants of Sennaar*, you can get around learning this glyph-like language
    no matter what language the player speaks. These characters were made up, and
    what mattered were two things: **consistency and context**. Let’s explore each
    of these concepts in the following sections.'
  prefs: []
  type: TYPE_NORMAL
- en: Consistency via Sequencing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **sequence** in which you use words has to be relatively consistent over
    time. In the English language, almost all sentences follow a typical subject-verb
    structure. While languages like Spanish tend to alter their verbs based on the
    subject in question, it still works as just a valid a language as English. For
    example, consider the sentence, “I am going to the store.” In Spanish, this translates
    to, “Me voy a la tienda.” Both of these languages use different words and a slightly
    different sequencing of the words, but both convey the same idea of going to the
    store. This makes both languages equally valid.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this same principle transcends English and Spanish and works for
    all languages, verbal and written. Why? **The sequencing of the words is more
    important than the words themselves.**
  prefs: []
  type: TYPE_NORMAL
- en: This is good news for a computer that operates in binary ones and zeroes. If
    the words themselves don’t matter, that means we can redefine them however we
    need. In our computer’s case, it wants to work with numbers, so it converts the
    words into vectors of numbers. We call this process of changing words into vectors
    as the **encoding** process, and we refer to the output of this process — the
    number vectors — as **embeddings**.
  prefs: []
  type: TYPE_NORMAL
- en: It probably comes as no surprise to you that this simple encoding process is
    not new. Getting the words into something a computer can fiddle around with has
    never been the problem. The challenge has been that researchers have had to spend
    decades trying to use different mathematical algorithms to make sense of the embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Without going into a whole lot of detail, this effort became known as the field
    of **natural language processing (NLP)**, and it has a rich history that we’ll
    save for another day. Many NLP techniques have been introduced over the years,
    and many are still effective solutions for certain scenarios today. In fact, several
    of these techniques can be even more effective than LLMs for certain use cases,
    making them still advisable for some of today’s problems.
  prefs: []
  type: TYPE_NORMAL
- en: One major NLP breakthrough centered on this same idea of consistency of word
    sequencing being important. These researchers at Google in 2014 discovered a way
    to effectively encode a sentence and then later “decode” it in a specific way
    for fruitful results. For example, they could take an English sentence, pass it
    through this **encoder-decoder architecture**, and on the other side would pop
    out that same sentence but in Spanish. They referred to this sequence-to-sequence
    architecture as **seq2seq**.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, I noted that when it comes to language, the two most important characteristics
    are consistency (in sequencing) and context. The seq2seq helps to satisfy the
    consistency matter, but what about context?
  prefs: []
  type: TYPE_NORMAL
- en: Context Clues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Context remained a slippery idea the next few years as early encoder-decoder
    architectures like seq2seq didn’t have a good way of managing the context. Researchers
    tried different things, including introducing the somewhat effective **long, short-term
    memory (LSTM)** cell, where the neural network essentially tried keeping a running
    context of the full sentence history. For example, consider the following sentence,
    “Don’t let the burglar in so that he may steal all our precious goods and belongings.”
    Imagine if a model made context of every word in that sentence except for the
    first. Well, now you have the opposite intention of the sentence! The LSTM sought
    to right those wrongs by keeping as much early context as possible. Mathematically
    speaking, it meant holding onto some numbers to represent the “long term memory”
    while giving equal or extra weight to the more “short term memory.”
  prefs: []
  type: TYPE_NORMAL
- en: Still, the LSTM didn’t quite cut it, and if you think about context, you’ll
    understand this is because not all words have an equal weighting. Let’s revisit
    our original example, “I like to drink ______ in the morning.” In this case, the
    most important word is “drink” in helping us to fill in the blank. Thus, we should
    give more weight to that word, or you might say, we should pay more **attention**
    to that word. “Attention” is precisely how the Google researchers referred to
    it in their infamous 2017 paper “Attention is All You Need.”
  prefs: []
  type: TYPE_NORMAL
- en: Attention in neural networks is a complex mathematical process, but we can still
    understand it at an intuitive level. As we already touched on, information is
    first **encoded** through the embedding process, that way we can do an apples-to-apples
    comparison on different sequences of words. This information is later passed through
    a **decoder** that produces a new sequence of words in response to the input we
    give the model. While we demonstrated that we can input one sentence in English
    and produce a Spanish translation, we can also pass through an input like, “What
    is the capital of Illinois?” and still receive an appropriately decoded output
    of, “The capital of Illinois is Springfield.”
  prefs: []
  type: TYPE_NORMAL
- en: '**Remember, attention is all about giving focus to the important words in a
    context to produce a more accurate output**. This is done by assessing the similarity
    of a word during the decoding process to all the words encoded in the encoding
    process. Consider the example from our previous paragraph. Let’s say the decoder
    model has already produced the output, “The capital of Illinois is _______.” How
    does it know how to fill in this final blank? It’s going to look at how similar
    the current decoded words are to the encoding process. Specifically, it’s going
    to see a strong correlation / similarity between the words “capital” and “Illinois”
    to derive the most probabilistic answer, which in this case is “Springfield.”'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you can imagine that LLMs need a LOT of examples in order to effectively
    produce more generalized outputs, hence why we refer to them as “large” language
    models. Generally speaking, the more examples (or **parameters**) that we provide
    to an LLM during the training process, the greater chance it has at predicting
    the correct words. Of course, we’re oversimplifying this concept, and there are
    certainly other nuances that go into influencing a Generative AI model’s output.
    For example, people are beginning to find effectiveness in **mixture of expert
    (MoE)** models, where “smaller” models are trained on a more specific domain of
    knowledge and later combined to produce more fruitful results. In fact, it’s rumored
    that this is how OpenAI’s popular GPT-4 works under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing Pitfalls & Myths
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As cool as LLMs can be, they are not perfect and do not always produce the most
    correct or relevant results. We refer to these confident but incorrect responses
    as **hallucinations**, but technically speaking, it is not fair to the LLM to
    refer to hallucinations as incorrect. Remember, as we’ve explored the intuition
    behind these models throughout this post, we have a general understanding of how
    results are derived using very fancy probability methods. **There is no sentient
    reasoning going on in these models; they’re simply predicting what the probabilities
    are based on the training process.** You can’t really blame the model for adhering
    to empirically derived probabilities!
  prefs: []
  type: TYPE_NORMAL
- en: This is especially apparent if you use different kinds of LLMs. You might be
    only familiar with ChatGPT, but if you play around with enough LLMs, you’ll notice
    drastically different results. For example, it is rumored that ChatGPT’s underlying
    model, `gpt-3.5-turbo`, was trained on 70 billion parameters. We now have models
    like Meta’s Llama 2 with flavors that go as low as 7 billion parameters, one tenth
    that of `gpt-3.5-turbo`. And in practice, it’s very obvious the performance difference
    between these models. (Granted, smaller models like Mistral are getting better
    and even coming close to matching that of ChatGPT!)
  prefs: []
  type: TYPE_NORMAL
- en: The tendency for models to hallucinate should give a business leader pause when
    applying Generative AI to business processes. In these earlier days of LLMs, it’s
    perhaps more appropriate to include a “human in the loop” mechanism, where a human
    has their work processes augmented with Generative AI but can ultimately “overrule”
    an LLM’s response if the human feels that the model didn’t give a good result.
    Of course, these models are going to get better and better over time, so perhaps
    a business leader may relax some of these restrictions as time goes on. That will
    be up to the risk appetite of the business.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, it should be noted that these large language models can exhibit
    an unfair bias, albeit this is *NOT* to say that this bias is intended. I like
    to think of LLMs as “zeitgeist machines.” If LLMs are next-word predictors based
    on advanced probabilities, **the predictions are only as good as the data it saw
    at the time of training**. So if you were to train an LLM only on a bunch of text
    talking about how nasty pizza tastes, don’t be surprised when the LLM has a tendency
    to talk negatively about pizza! Likewise, the complaints from people online that
    LLMs exhibit bias in unfair ways are correct, but it’s only because the “zeitgeist”
    of the training data inclined it that way. For example, it should come as no surprise
    that an LLM may be unfairly biased against a certain political candidate if the
    training data contained many articles that were critical about that candidate.
    It’s a myth to believe that this bias was intentionally baked into the model.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we should address the concern of copyright infringement. Because LLMs
    measure advanced probabilities between words, it should come as no surprise when
    these LLMs are able to emulate a piece of copyrighted work. Intentionally use
    the word “emulate” because LLMs are unable to reproduce full bodies of copyrighted
    work simply due to complex probabities. So I can have an LLM talk to me like Hagrid
    from the *Harry Potter* series or Jar Jar Binks from *Star Wars*, but it will
    struggle to fully reproduce the dialogue from those movie scripts. In other words,
    don’t expect the LLM to be able to sufficiently reproduce the entire *Harry Potter*
    book series. It’s just too long, and the probabilities of these words get too
    messy.
  prefs: []
  type: TYPE_NORMAL
- en: Still, the copyright infringement space is a messy one, and it seems to be a
    problem more beholden to these companies training LLMs and less a problem for
    the companies simply using them. Now granted, a company making use of another’s
    LLM should still seek to put up guardrails that would not intentionally try to
    reproduce copyrighted work, which is easy enough to do with appropriate prompt
    engineering. But companies like OpenAI are currently facing this legal predicament
    on whether or not they were legally allowed to use others’ data for training purposes
    in the first place. At the time of this article’s posting, [*The New York Time*s
    is suing OpenAI and Microsoft alleging that their news data was inappropriately
    used in the training of the model](https://fortune.com/2023/12/27/openai-microsoft-new-york-times-lawsuit-ai-copyright-infringement/).
    I am not an expert on these sorts of legal matters, but it will be important to
    watch cases like these to see how legislation may impact the evolution of LLMs
    going forward.
  prefs: []
  type: TYPE_NORMAL
- en: The Generative AI space is an extremely fascinating one, and I’m excited to
    see how the space continues to evolve in the future. We are very much still in
    the early days of this revolution, and I anticipate we will continue to see advancements
    in adoption, technological evolution, and legal understanding. I hope that this
    post provided you with enough intuition that you can make better informed decisions
    on how to approach this exciting domain with the appropriate level of caution!
    😃
  prefs: []
  type: TYPE_NORMAL
