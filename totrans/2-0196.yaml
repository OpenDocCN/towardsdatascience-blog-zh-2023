- en: A Gentle Introduction to Open Source Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-gentle-introduction-to-open-source-large-language-models-3643f5ca774](https://towardsdatascience.com/a-gentle-introduction-to-open-source-large-language-models-3643f5ca774)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Open Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Why everyone is talking about Llamas, Alpacas, Falcons and other animals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://donatoriccio.medium.com/?source=post_page-----3643f5ca774--------------------------------)[![Donato
    Riccio](../Images/0af2a026e72a023db4635522cbca50eb.png)](https://donatoriccio.medium.com/?source=post_page-----3643f5ca774--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3643f5ca774--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3643f5ca774--------------------------------)
    [Donato Riccio](https://donatoriccio.medium.com/?source=post_page-----3643f5ca774--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3643f5ca774--------------------------------)
    ·11 min read·Aug 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b78c1db423fd8ee4ce173a12b054118c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author (generated with Midjourney)
  prefs: []
  type: TYPE_NORMAL
- en: Unless you’ve been living under a rock for the last year, you’ve witnessed the
    ChatGPT revolution and to how everyone seems unable to stop using it. In this
    article, we’ll explore its alternatives, jumping into the world of open source
    models. This first article of the series *Open Language Models* is helpful for
    people looking to get started and understand Open Source Large Language Models,
    and how and why to use them.
  prefs: []
  type: TYPE_NORMAL
- en: Table of contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: — [Why do we need Open Source Models?](#c41d)
  prefs: []
  type: TYPE_NORMAL
- en: — [The bigger the better? Training L](#031f)arge Language Models
  prefs: []
  type: TYPE_NORMAL
- en: — [Fine-tuning Large Language Models](#9419)
  prefs: []
  type: TYPE_NORMAL
- en: — [The Best Open Source Large Language Models](#1f3c)
  prefs: []
  type: TYPE_NORMAL
- en: — [Running a Large Language Model on your computer](#2d37)
  prefs: []
  type: TYPE_NORMAL
- en: — [Limitations](#d1aa)
  prefs: []
  type: TYPE_NORMAL
- en: — [Conclusion](#2c45)
  prefs: []
  type: TYPE_NORMAL
- en: What is a Large Language Model?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **Large Language Model (LLM)** is an AI capable of understanding and generating
    human language. At the heart, there’s a type of neural network called a transformer,
    that works by predicting what word comes next in a sentence. The word *large*
    describes these models’ extensive nature since they canhave billions or even trillions
    of parameters. What differentiates them is their ability to specialize in particular
    tasks, such as code generation or translation, or be applied to general instruction-following
    chatbots. One of the groundbreaking aspects of these models is that they enable
    *zero-shot* and *few-shot learning,* as they exhibit an unprecedented ability
    to learn tasks they have not been explicitly trained for. [1]
  prefs: []
  type: TYPE_NORMAL
- en: '**Why do we need Open Source Models?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose you use GPT API to create an innovative app that quickly gains traction.
    Everything is going smoothly until OpenAI changes their course of action. They
    might halt the service, escalate the cost, or even decrease the capability of
    their models — which is already happening. [2]
  prefs: []
  type: TYPE_NORMAL
- en: Currently, your only solution would be to adjust to their new policies. Moreover,
    relying on a third-party API results in your data being transmitted to their server.
    While OpenAI may not utilize data from GPT APIs for model training, [3] deploying
    your own language model guarantees you total control over these operations. Even
    if this may seem like an ideal plan, **deploying your private LLM also comes with
    its own set of limitations and challenges**, which will be addressed in this article.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07203eb84cbde74e5569f1097fc7ce03.png)![](../Images/9308c09c7eb3d9aff77ddabd9f5aed2e.png)![](../Images/537439fafa4ae2af699eb2f04ab98635.png)'
  prefs: []
  type: TYPE_IMG
- en: '**(Left)** A Llama. Photo by Sébastien Goldberg | **(Middle)** A Vicuña. Photo
    by Dušan veverkolog | **(Right)** An Alpaca. Photo by Adrian Dascal. All from
    Unsplash.'
  prefs: []
  type: TYPE_NORMAL
- en: The bigger the better? Training Large Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you’ve stumbled upon something like **LLaMA 65B**, you’re likely wondering
    what the meaning of ***65B***. Well, it simply refers to the number of parameters
    in the model. As the size of the model increases, it requires more time to train
    and uses up more memory for inference. Unlike what you’re used to hearing in machine
    learning, complex models can more easily generalize to different tasks. Some have
    an enormous number of parameters: GPT-3 has 175 billion, while GPT-4 has more
    than 1 trillion. Training them from scratch has been estimated to cost millions
    of dollars. For example, **Google’s PaLM 540B was trained on 2240 GPUs** [4].
    For comparison, EfficientNet-B7, one of the most popular deep learning models
    for image classification, has only 66M parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f79c3dbb127b44da8aa89b3bb2e17224.png)'
  prefs: []
  type: TYPE_IMG
- en: Clearly, not something you can train on your laptop. [5]
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2022, Google claimed:'
  prefs: []
  type: TYPE_NORMAL
- en: As the scale of the model increases, the performance improves across tasks while
    also unlocking new capabilities.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the current state of LLMs, more parameters are usually better [4]. Companies
    focus on building bigger models, but the current trend in open source is to create
    smaller, more efficient models. While the most popular open source models usually
    have up to 70B parameters, it’s not uncommon to see smaller, fine-tuned models
    performing better than bigger models on a specific task. Also, larger models require
    more resources for training and inference, making them challenging to deploy.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, within a year, even Google changed their perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Open-source models are faster, more customizable, more private, and pound-for-pound
    more capable. They are doing things with $100 and 13B params that we struggle
    with at $10M and 540B. And they are doing so in weeks, not months.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From the leaked document titled *We have no moat, and neither does OpenAI* [6]*,*
    where they acknowledge the incredible evolution of open source models, that are
    quickly catching up using smaller and cheaper models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/99f1fc4b148f9e351c0b0ca75c36b518.png)'
  prefs: []
  type: TYPE_IMG
- en: ChatGPT took it well.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to the outstanding work of the open source community in the last year,
    there are now accessible and free alternatives. Less than a year after Google’s
    PaLM, **LLaMA** was released; In the paper, the authors claim that their biggest
    model, **LLaMA 65B**, surpasses GPT-3 (175B) and PaLM (540B) on many tasks [5].
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning Large Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs have earned their reputation for their versatility in handling many language
    tasks within a single framework. Yet, your specific application might require
    the model to excel in a single task. To achieve this, you can fine-tune a pre-trained
    model using a dataset specific to your task — say, text summarization. The fascinating
    part is that good results are attainable even with a small dataset. You might
    need just 500 to 1,000 examples to improve performance significantly despite the
    model initially being trained on billions of pieces of text.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1334709edb870c678d16360f80f24d4f.png)'
  prefs: []
  type: TYPE_IMG
- en: A sample from the [Alpaca Dataset.](https://github.com/gururise/AlpacaDataCleaned)
    The model is fine-tuned to follow an instruction given by the user.
  prefs: []
  type: TYPE_NORMAL
- en: A popular technique is *instruction fine-tuning*. This method involves training
    the model using examples that illustrate how it should respond to a particular
    instruction. The outcome of this process is an instruct model — an enhanced version
    of the base model that excels at following instructions, rather than just completing
    text. Examples of instruct models are **Alpaca** and **Vicuna**.
  prefs: []
  type: TYPE_NORMAL
- en: The Best Open Source Large Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In February 2023, Meta’s **LLaMA** model hit the open-source market in various
    sizes, including 7B, 13B, 33B, and 65B. Initially, the model was only available
    to researchers under a non-commercial license, but in less than a week its weights
    were leaked. This event sparked a revolution in the open-source LLMs world as
    its training code was freely accessible under GPL 3 license. Consequently, several
    powerful fine-tuned variations have been released.
  prefs: []
  type: TYPE_NORMAL
- en: The first was **Alpaca**, released by Stanford University. This model utilized
    GPT’s generated instructions for its fine-tuning on 52K examples. It was soon
    followed by **Vicuna**, which surprisingly outperformed Alpaca by measuring a
    whopping 90% ChatGPT quality on many tasks. Its distinguishing feature is that
    it was fine-tuned on ShareGPT data.
  prefs: []
  type: TYPE_NORMAL
- en: Adding to these powerful models is **GPT4All** — inspired by its vision to make
    LLMs easily accessible, it features a range of consumer CPU-friendly models along
    with an interactive GUI application.
  prefs: []
  type: TYPE_NORMAL
- en: '**WizardLM** also joined these remarkable LLaMa-based models. Through a new
    and unique method named Evol-Instruct, it underwent fine-tuning on complex instruction
    data and has shown similar performance to ChatGPT at an average of 97.8%.'
  prefs: []
  type: TYPE_NORMAL
- en: Not all recent models are based on LLaMA, though. Models like **MPT** are known
    for achieving impressive context lengths with their variants capable of generating
    up to a staggering amount of 65k context length — a whole book at once!
  prefs: []
  type: TYPE_NORMAL
- en: '**Falcon** also joins this bandwagon in both 7B and 40B variants. Surprisingly
    it outperforms LLaMA on the OpenLLM leaderboard due to its high-quality training
    dataset called RefinedWeb.'
  prefs: []
  type: TYPE_NORMAL
- en: However, Falcon’s reign on top of HuggingFace’s OpenLLM leaderboard was short-lived.
    In July 2023, Meta unveiled the successor to their famous model, **LLaMa 2**.
    This next-generation model doubled its predecessor’s token limit and increased
    its context length to 4K tokens. Simultaneously featuring **llama-2-chat** — an
    optimized version for dialogue applications — it made a significant impact. At
    the time of writing, **fine-tuned versions of LLaMa 2 such as StableBeluga2, Airoboros
    and Guanaco** are the most powerful open source large language models, dominating
    the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd9a5f03805a71f719c3309616366dbd.png)'
  prefs: []
  type: TYPE_IMG
- en: The top 10 models on the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    are mostly LLaMa 2 based.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re curious about a model and you want to try it, the easiest way is probably
    going to an HuggingFace model page, and opening a Space using it. They are simple
    Gradio interfaces that allow you to send an input to a model and receive back
    an output. Since the models run on their servers and there are many requests,
    often you’ll have to wait in queue for a while. Nonetheless, it’s not a big deal
    to wait a couple of minutes sometimes, given how great their service is.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ba02b0c371638a0c1bc44d625997a1f.png)'
  prefs: []
  type: TYPE_IMG
- en: HuggingFace Spaces makes it very easy to try language models. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Hardware requirements and optimizations**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Last year, you needed a high-end GPU to run a LLM locally. Even small ones required
    many gigabytes of memory, and you had to load them on your graphics card memory.
    The situation changed with the release of [**llama.cpp**](https://github.com/ggerganov/llama.cpp)**.**
    Initially a port of Facebook’s LLaMA model in C/C++, it now supports many other
    models. It allows to convert an LLM from PyTorch to **GGML**, their new format
    that allows fast inference on CPU. Being compiled in C++, the inference is multithreaded.
  prefs: []
  type: TYPE_NORMAL
- en: '*Thanks to this amazing work, it’s now possible to run many LLMs on your desktop
    computer, or even on a MacBook.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The main limitation is the memory they consume. For example, a 7B model weights
    around 14GB. A 65B model would require around 130GB of memory (RAM and disk space),
    which is more than most of us have on our computers. Fortunately, there is a way
    to compress them.
  prefs: []
  type: TYPE_NORMAL
- en: Enter **Quantization.**
  prefs: []
  type: TYPE_NORMAL
- en: In every machine learning model, a parameter is a number. This number is usually
    represented as a float32, a 32bits (4 bytes) representation.
  prefs: []
  type: TYPE_NORMAL
- en: Since every parameter takes two bytes of space, a model with 65B parameters
    would take approximately **4*65*10⁹ = 260GB** of memory.
  prefs: []
  type: TYPE_NORMAL
- en: Model quantization refers to the idea of reducing the model weight by representing
    its parameters with a lower precision number, using rounding. For mathematical
    details about quantization, there is a great article on HuggingFace’s blog. [7]
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf72f9eb4f926c1a71b27b04bed85faf.png)'
  prefs: []
  type: TYPE_IMG
- en: 8-bit integer quantization. [7]
  prefs: []
  type: TYPE_NORMAL
- en: Being a compression process, quantization involves some loss in performance.
    With the recent introduction of **LLM.int8()** and newer techniques, this loss
    has been greatly reduced, making it a must for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Llama.cpp** supports up to 4-bit integer quantization. Using **Q4_0**, one
    of the available options, it’s possible to **reduce the model size up to 4 times.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa79944cc873862e975dacbd39970ada.png)'
  prefs: []
  type: TYPE_IMG
- en: Quantization effect on model perplexity and file size. Image by the author.
    Data from llama.cpp GitHub repo.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q4_0** reduced file size **from 25.8 to 6.8 GB**, while reducing perplexity
    by approximately 2%. It’s possible to load a 13B model on a computer with 16GB
    of RAM, while if you have 64GB, you can run even the biggest 70B models.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db20407583125c5b5d5b0f52e656f616.png)'
  prefs: []
  type: TYPE_IMG
- en: LLaMa 2 models with different quantization techniques. [TheBloke](https://huggingface.co/TheBloke)’s
    HuggingFace profile has many pre-trained models.
  prefs: []
  type: TYPE_NORMAL
- en: Another quantization technique worth mentioning is **GPTQ,** that can reduce
    VRAM usage up to 75% while preserving accuracy. [8] Released in March 2023, it
    made possible for the first time to run a 175B model on a single GPU. Talking
    about consumer hardware, you can run up to a 30B model with a single RTX 3090
    card.
  prefs: []
  type: TYPE_NORMAL
- en: Running a Large Language Model on your computer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most similar experience to ChatGPT is the **GPT4All** app, a chat interface
    that allows you to chat with your favorite model. It’s not limited to GPT4All
    models but supports many of the most popular ones. The app runs on Windows, Mac
    OS, and Linux and it’s completely open source.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37d1240ec81d290f33f8c83c375518cf.png)'
  prefs: []
  type: TYPE_IMG
- en: LLaMA-2 7B inference on a Ryzen 5600 CPU. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can use a GUI tool like [**text-generation-webui**](https://github.com/oobabooga/text-generation-webui)
    or [**openplayground**](https://github.com/nat/openplayground)**.** While they
    both offer a graphical interface that can be easily used to generate text, the
    first is probably the most complete tool available: it offers many features such
    as chat, training, support for GPU and HTML/Markdown output, and more. The second
    is very similar to OpenAI’s Playground, and it’s a great tool for testing and
    comparing LLMs quickly with different parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3f42c748cf8dff9cd9146f7458816a99.png)'
  prefs: []
  type: TYPE_IMG
- en: Text generation web UI is an advanced web interface built with Gradio.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to use your LLMs in Python there are several options, such as [**llama-cpp-python**](https://github.com/abetlen/llama-cpp-python),
    or the **HuggingFace Transformers** library, which offers an high-level syntax
    to interact with any HF model. It supports PyTorch, Tensorflow, and Jax backend
    and it can be used with any pre-trained model you find on HF, for text, images
    or audio. Using transformers, generating text with your favorite LLM it’s as simple
    as writing two lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Among the thousands freely available models on the HF hub, there are different
    formats. The *Transformers* library require a model in **HF** format and *llama.cpp*
    requires a **GGML** model.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While we have shown that you don’t need powerful computers for most models,
    **scalability** is the first concern if you are planning to build a system that
    allows hundreds or thousands of users to interact with your model. An advantage
    of using GPT is that OpenAI offers cheap, high rate-limit APIs that can scale
    your app easily.
  prefs: []
  type: TYPE_NORMAL
- en: Another limitation is **safety and moderation**. Since you can be held responsible
    for the model’s output, you must be extra careful of what the model is generating.
    Commercial LLMs have advanced moderation filters built using Reinforcement Learning
    with Human Feedback (RLHF), for limiting harmful content.
  prefs: []
  type: TYPE_NORMAL
- en: Remember to check the **model’s license** before using it. *Open Source* doesn’t
    always mean that the model can be used commercially.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The article has shown that open source models are viable and free alternatives
    that are getting better quickly. This year has witnessed incredible advancements
    in the development of these models, and it is now even possible to run them on
    your laptop.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you found this article useful if you are considering an open source LLM
    for your next project. The following articles in the series will go more in-depth
    on *Open Language Models* different aspects and challenges.
  prefs: []
  type: TYPE_NORMAL
- en: See you in the next one!
  prefs: []
  type: TYPE_NORMAL
- en: '*If you enjoyed this article, join* [***Text Generation***](https://textgeneration.substack.com/)
    *— our newsletter has two weekly posts with the latest insights on Generative
    AI and Large Language Models.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Also, you can find me on* [***LinkedIn***](https://www.linkedin.com/in/driccio/)***.***'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [Stanford Scientists Find That Yes, ChatGPT Is Getting Stupider](https://futurism.com/the-byte/stanford-chatgpt-getting-dumber)
    (2023), (futurism.com)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] T. Brown at al, [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
    (2020), arXiv.org'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] M. Schade, [How your data is used to improve model performance](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance)
    (2023), OpenAI Help Center'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Google AI Blog, [Pathways Language Model (PaLM): Scaling to 540 Billion
    Parameters for Breakthrough Performance](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)
    (2022), ai.googleblog.com'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] A. Fan et al., [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
    (2023), arXiv.org'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] D. Patel and A. Ahmad, [Google: We Have No Moat (And Neither Does OpenAI)](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)
    (2023), semianalysis.com'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Y. Belkada and T. Dettmers, [A Gentle Introduction to 8-bit Matrix Multiplication
    for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration)
    (2022), Hugging Face Blog'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] E. Frantar et al., [GPTQ: Accurate Post-Training Quantization for Generative
    Pre-trained Transformers](https://arxiv.org/abs/2210.17323) (2023), arXiv:2210.17323v2
    [cs.LG]'
  prefs: []
  type: TYPE_NORMAL
