- en: What Are the Data-Centric AI Concepts behind GPT Models?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT模型背后的数据中心AI概念是什么？
- en: 原文：[https://towardsdatascience.com/what-are-the-data-centric-ai-concepts-behind-gpt-models-a590071bb727](https://towardsdatascience.com/what-are-the-data-centric-ai-concepts-behind-gpt-models-a590071bb727)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/what-are-the-data-centric-ai-concepts-behind-gpt-models-a590071bb727](https://towardsdatascience.com/what-are-the-data-centric-ai-concepts-behind-gpt-models-a590071bb727)
- en: Unpacking the data-centric AI techniques used in ChatGPT and GPT-4
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解读ChatGPT和GPT-4中使用的数据中心AI技术
- en: '[](https://medium.com/@a0987284901?source=post_page-----a590071bb727--------------------------------)[![Henry
    Lai](../Images/eaa1b4eb6f6cebc131f4cf0cfdd4cda7.png)](https://medium.com/@a0987284901?source=post_page-----a590071bb727--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a590071bb727--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a590071bb727--------------------------------)
    [Henry Lai](https://medium.com/@a0987284901?source=post_page-----a590071bb727--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@a0987284901?source=post_page-----a590071bb727--------------------------------)[![Henry
    Lai](../Images/eaa1b4eb6f6cebc131f4cf0cfdd4cda7.png)](https://medium.com/@a0987284901?source=post_page-----a590071bb727--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a590071bb727--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a590071bb727--------------------------------)
    [Henry Lai](https://medium.com/@a0987284901?source=post_page-----a590071bb727--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a590071bb727--------------------------------)
    ·8 min read·Mar 29, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a590071bb727--------------------------------)
    ·阅读时间 8 分钟·2023年3月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/fa65e6643cb2712bc70bc825d8ba03a3.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa65e6643cb2712bc70bc825d8ba03a3.png)'
- en: '[https://arxiv.org/abs/2303.10158](https://arxiv.org/abs/2303.10158). Image
    by the author.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/2303.10158](https://arxiv.org/abs/2303.10158)。作者提供的图像。'
- en: Artificial Intelligence (AI) has made incredible strides in transforming the
    way we live, work, and interact with technology. Recently, that one area that
    has seen significant progress is the development of Large Language Models (LLMs),
    such as [GPT-3](https://arxiv.org/abs/2005.14165), [ChatGPT](https://openai.com/blog/chatgpt),
    and [GPT-4](https://cdn.openai.com/papers/gpt-4.pdf). These models are capable
    of performing tasks such as language translation, text summarization, and question-answering
    with impressive accuracy.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能（AI）在改变我们的生活、工作和与技术互动的方式上取得了令人难以置信的进展。最近，取得显著进展的一个领域是大型语言模型（LLMs）的发展，例如
    [GPT-3](https://arxiv.org/abs/2005.14165)、[ChatGPT](https://openai.com/blog/chatgpt)
    和 [GPT-4](https://cdn.openai.com/papers/gpt-4.pdf)。这些模型能够以令人印象深刻的准确性执行语言翻译、文本摘要和问答等任务。
- en: While it’s difficult to ignore the increasing model size of LLMs, it’s also
    important to recognize that their success is due largely to the large amount and
    high-quality data used to train them.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管很难忽视大型语言模型（LLMs）模型尺寸的不断增长，但同样重要的是认识到它们的成功在很大程度上归功于用于训练它们的大量高质量数据。
- en: 'In this article, we will present an overview of the recent advancements in
    LLMs from a data-centric AI perspective, drawing upon insights from our recent
    survey papers [1,2] with corresponding technical resources on [GitHub](https://github.com/daochenzha/data-centric-AI).
    Particularly, we will take a closer look at GPT models through the lens of [data-centric
    AI](https://github.com/daochenzha/data-centric-AI), a growing concept in the data
    science community. We’ll unpack the data-centric AI concepts behind GPT models
    by discussing three data-centric AI goals: [training data development, inference
    data development, and data maintenance](https://arxiv.org/abs/2303.10158).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将从数据中心AI的角度概述大型语言模型（LLMs）的最新进展，借鉴我们最近的调查论文 [1,2] 以及在 [GitHub](https://github.com/daochenzha/data-centric-AI)
    上的相关技术资源。特别是，我们将通过 [数据中心AI](https://github.com/daochenzha/data-centric-AI) 的视角，深入探讨GPT模型。我们将通过讨论三个数据中心AI目标来解读GPT模型背后的数据中心AI概念：
    [训练数据开发、推理数据开发和数据维护](https://arxiv.org/abs/2303.10158)。
- en: Large Language Models (LLMs) and GPT Models
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）和GPT模型
- en: LLMs are a type of Natual Language Processing model that are trained to infer
    words within a context. For example, the most basic function of an LLM is to predict
    missing tokens given the context. To do this, LLMs are trained to predict the
    probability of each token candidate from massive data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 是一种自然语言处理模型，经过训练以在上下文中推断单词。例如，LLM 的最基本功能是根据上下文预测缺失的标记。为此，LLM 通过海量数据训练来预测每个标记候选的概率。
- en: '![](../Images/960f620655f6281a5eab620cc245b37c.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/960f620655f6281a5eab620cc245b37c.png)'
- en: An illustrative example of predicting the probabilities of missing tokens with
    an LLM within a context. Image by the author.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例，展示了如何在上下文中用 LLM 预测缺失标记的概率。图片由作者提供。
- en: GPT models refer to a series of LLMs created by OpenAI, such as [GPT-1](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf),
    [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf),
    [GPT-3](https://arxiv.org/abs/2005.14165), [InstructGPT](https://arxiv.org/abs/2203.02155),
    and [ChatGPT/GPT-4.](https://cdn.openai.com/papers/gpt-4.pdf) Just like other
    LLMs, GPT models’ architectures are largely based on [Transformers](https://arxiv.org/abs/1706.03762),
    which use text and positional embeddings as input, and attention layers to model
    tokens’ relationships.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 模型指的是一系列由 OpenAI 创建的 LLM，例如 [GPT-1](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)、[GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)、[GPT-3](https://arxiv.org/abs/2005.14165)、[InstructGPT](https://arxiv.org/abs/2203.02155)
    和 [ChatGPT/GPT-4](https://cdn.openai.com/papers/gpt-4.pdf)。与其他 LLM 一样，GPT 模型的架构主要基于
    [Transformers](https://arxiv.org/abs/1706.03762)，该架构使用文本和位置嵌入作为输入，并通过注意力层建模标记之间的关系。
- en: '![](../Images/256a7a43daaf08de5607d27bf8348635.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/256a7a43daaf08de5607d27bf8348635.png)'
- en: GPT-1 model architecture. Image from the paper [https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-1 模型架构。图片来自论文 [https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- en: The later GPT models use similar architectures as GPT-1, except for using more
    model parameters with more layers, larger context length, hidden layer size, etc.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 后来的 GPT 模型使用了与 GPT-1 相似的架构，但模型参数更多，层数更多，上下文长度更大，隐藏层大小等也有所增加。
- en: '![](../Images/48108b80ae1227f06f83795294cd1a56.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48108b80ae1227f06f83795294cd1a56.png)'
- en: Models size comparison of GPT models. Image by the author.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 模型的模型大小比较。图片由作者提供。
- en: What is data-centric AI?
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是数据驱动的 AI？
- en: '[Data-centric AI](https://github.com/daochenzha/data-centric-AI) is an emerging
    new way of thinking about how to build AI systems. It has been advocated by Andrew
    Ng, an AI pioneer.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[数据驱动的 AI](https://github.com/daochenzha/data-centric-AI) 是一种新兴的思考方式，旨在构建 AI
    系统。它得到了 AI 先驱 Andrew Ng 的倡导。'
- en: '*Data-centric AI is the discipline of systematically engineering the data used
    to build an AI system. — Andrew Ng*'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*数据驱动的 AI 是系统地工程化用于构建 AI 系统的数据的学科。 — Andrew Ng*'
- en: In the past, we mainly focused on creating better models with data largely unchanged
    (model-centric AI). However, this approach can lead to problems in the real world
    because it doesn’t consider the different problems that may arise in the data,
    such as inaccurate labels, duplicates, and biases. As a result, “overfitting”
    a dataset may not necessarily lead to better model behaviors.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，我们主要关注在数据基本不变的情况下创建更好的模型（模型驱动 AI）。然而，这种方法可能会在现实世界中导致问题，因为它没有考虑到数据中可能出现的不同问题，如标签不准确、重复和偏差。因此，"过拟合"
    一个数据集不一定会带来更好的模型表现。
- en: In contrast, data-centric AI focuses on improving the quality and quantity of
    data used to build AI systems. This means that the attention is on the data itself,
    and the models are relatively more fixed. Developing AI systems with a data-centric
    approach holds more potential in real-world scenarios, as the data used for training
    ultimately determines the maximum capability of a model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，数据驱动的 AI 侧重于提高用于构建 AI 系统的数据的质量和数量。这意味着关注点在于数据本身，而模型则相对固定。采用数据驱动的方法开发 AI
    系统在现实世界场景中具有更大的潜力，因为用于训练的数据最终决定了模型的最大能力。
- en: It is important to note that “data-centric” differs fundamentally from “data-driven”,
    as the latter only emphasizes the use of data to guide AI development, which typically
    still centers on developing models rather than engineering data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，“数据中心”与“数据驱动”在根本上有所不同，因为后者仅强调使用数据来指导 AI 开发，通常仍然侧重于开发模型，而不是工程化数据。
- en: '![](../Images/682d043538c4955991ee62d93f9ad98a.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/682d043538c4955991ee62d93f9ad98a.png)'
- en: Comparison between data-centric AI and model-centric AI. [https://arxiv.org/abs/2301.04819](https://arxiv.org/abs/2301.04819)
    Image by the author.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中心 AI 与模型中心 AI 的比较。 [https://arxiv.org/abs/2301.04819](https://arxiv.org/abs/2301.04819)
    图像由作者提供。
- en: 'The [data-centric AI framework](https://github.com/daochenzha/data-centric-AI)
    consists of three goals:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[数据中心 AI 框架](https://github.com/daochenzha/data-centric-AI) 由三个目标组成：'
- en: '**Training data development** is to collect and produce rich and high-quality
    data to support the training of machine learning models.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据开发**是为了收集和生成丰富且高质量的数据，以支持机器学习模型的训练。'
- en: '**Inference data development** is to create novel evaluation sets that can
    provide more granular insights into the model or trigger a specific capability
    of the model with engineered data inputs.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推断数据开发**是为了创建新的评估集，这些评估集可以提供对模型更细致的见解，或通过工程化的数据输入触发模型的特定能力。'
- en: '**Data maintenance** is to ensure the quality and reliability of data in a
    dynamic environment. Data maintenance is critical as data in the real world is
    not created once but rather necessitates continuous maintenance.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据维护**是为了确保在动态环境中数据的质量和可靠性。数据维护至关重要，因为现实世界中的数据不是一次性创建的，而是需要持续的维护。'
- en: '![](../Images/8b70bbf9b3ae3d73ee0a67793217d787.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b70bbf9b3ae3d73ee0a67793217d787.png)'
- en: Data-centric AI framework. [https://arxiv.org/abs/2303.10158](https://arxiv.org/abs/2303.10158).
    Image by the author.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中心 AI 框架。 [https://arxiv.org/abs/2303.10158](https://arxiv.org/abs/2303.10158)。图像由作者提供。
- en: Why Data-centric AI Made GPT Models Successful
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据中心 AI 如何让 GPT 模型取得成功
- en: Months earlier, Yann LeCun tweeted that ChatGPT was nothing new. Indeed, all
    techniques (transformer, reinforcement learning from human feedback, etc.) used
    in ChatGPT and GPT-4 are not new at all. However, they did achieve incredible
    results that previous models couldn’t. So, what is the driving force of their
    success?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 几个月前，Yann LeCun 在推特上表示 ChatGPT 并没有什么新意。确实，ChatGPT 和 GPT-4 中使用的所有技术（变换器、从人类反馈中学习的强化学习等）都并不新颖。然而，它们确实取得了之前模型无法达到的惊人结果。那么，它们成功的驱动力是什么呢？
- en: '**Training data development.** The quantity and quality of the data used for
    training GPT models have seen a significant increase through better data collection,
    data labeling, and data preparation strategies.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练数据开发。** 通过更好的数据收集、数据标注和数据准备策略，用于训练 GPT 模型的数据的数量和质量有了显著提升。'
- en: '**GPT-1:** [BooksCorpus dataset](https://huggingface.co/datasets/bookcorpus)
    is used in training. This dataset contains 4629.00 MB of raw text, covering books
    from a range of genres such as Adventure, Fantasy, and Romance.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT-1：** [BooksCorpus 数据集](https://huggingface.co/datasets/bookcorpus) 被用于训练。该数据集包含
    4629.00 MB 的原始文本，涵盖了来自冒险、奇幻和浪漫等多种类型的书籍。'
- en: '**- *Data-centric AI strategies*:** None.'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**- *数据中心的 AI 策略*:** 无。'
- en: '**- *Result:*** Pertaining GPT-1 on this dataset can increase performances
    on downstream tasks with fine-tuning.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**- *结果：*** 关于这个数据集的 GPT-1 可以通过微调提高下游任务的表现。'
- en: '**GPT-2:** [WebText](https://paperswithcode.com/dataset/webtext) is used in
    training. This is an internal dataset in OpenAI created by scraping outbound links
    from Reddit.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT-2：** [WebText](https://paperswithcode.com/dataset/webtext) 被用于训练。这是一个由
    OpenAI 创建的内部数据集，通过抓取 Reddit 上的外链生成。'
- en: '**- *Data-centric AI strategies:*** (1) Curate/filter data by only using the
    outbound links from Reddit, which received at least 3 karma. (2) Use tools [Dragnet](https://dl.acm.org/doi/abs/10.1145/2487788.2487828)
    and [Newspaper](https://github.com/codelucas/newspaper) to extract clean contents.
    (3) Adopt de-duplication and some other heuristic-based cleaning (details not
    mentioned in the paper)'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**- *数据中心的 AI 策略：*** (1) 通过仅使用来自 Reddit 的外链（获得至少 3 个 Karma）来策划/筛选数据。 (2) 使用工具
    [Dragnet](https://dl.acm.org/doi/abs/10.1145/2487788.2487828) 和 [Newspaper](https://github.com/codelucas/newspaper)
    提取干净的内容。 (3) 采用去重和一些其他基于启发式的清洗方法（论文中未提及具体细节）。'
- en: '**- *Result:*** 40 GB of text is obtained after filtering. GPT-2 achieves strong
    zero-shot results without fine-tuning.'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**- *结果：*** 经过筛选后获得了 40 GB 的文本。GPT-2 在未经过微调的情况下实现了强大的零样本结果。'
- en: '**GPT-3:** The training of GPT-3 is mainly based on [Common Crawl](https://commoncrawl.org/the-data/).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT-3：** GPT-3 的训练主要基于 [Common Crawl](https://commoncrawl.org/the-data/)。'
- en: '**- *Data-centric AI strategies:***(1) Train a classifier to filter out low-quality
    documents based on the similarity of each document to WebText, a proxy for high-quality
    documents. (2) Use Spark’s MinHashLSH to fuzzily deduplicate documents. (3) Augment
    the data with WebText, books corpora, and Wikipedia.'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**- *数据中心 AI 策略：***（1）训练分类器，根据每个文档与 WebText 的相似性来筛选低质量文档，WebText 是高质量文档的代理。
    （2）使用 Spark 的 MinHashLSH 进行模糊去重。 （3）用 WebText、书籍语料库和维基百科来增强数据。'
- en: '***- Result:*** 570GB of text is obtained after filtering from 45TB of plaintext
    (only 1.27% of data is selected in this quality filtering). GPT-3 significantly
    outperforms GPT-2 in the zero-shot setting.'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '***- 结果：*** 经过筛选后，从 45TB 的纯文本中获得了 570GB 的文本（仅 1.27% 的数据在此质量筛选中被选择）。GPT-3 在零样本设置中显著优于
    GPT-2。'
- en: '**InstructGPT:** Let humans evaluate the answer to tune GPT-3 so that it can
    better align with human expectations. They have designed tests for annotators,
    and only those who can pass the tests are eligible to annotate. They have even
    designed a survey to ensure that the annotators enjoy the annotating process.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**InstructGPT：** 让人类评估答案以调整 GPT-3，使其更好地符合人类期望。他们为标注员设计了测试，只有通过测试的标注员才有资格进行标注。他们甚至设计了一项调查，以确保标注员喜欢标注过程。'
- en: '**- *Data-centric AI strategies:***(1) Use human-provided answers to prompts
    to tune the model with supervised training. (2) Collect comparison data to train
    a reward model and then use this reward model to tune GPT-3 with reinforcement
    learning from human feedback (RLHF).'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**- *数据中心 AI 策略：***（1）使用人工提供的答案来调整模型，进行监督训练。 （2）收集比较数据来训练奖励模型，然后使用此奖励模型通过人类反馈强化学习（RLHF）来调整
    GPT-3。'
- en: '***- Result:*** InstructGPT shows better truthfulness and less bias, i.e.,
    better alignment.'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '***- 结果：*** InstructGPT 显示出更好的真实性和更少的偏见，即更好的对齐。'
- en: '**ChatGPT/GPT-4:** The details are not disclosed by OpenAI. But it is known
    that ChatGPT/GPT-4 largely follow the design of previous GPT models, and they
    still use RLHF to tune models (with potentially more and higher quality data/labels).
    It is commonly believed that GPT-4 used an even larger dataset, as the model weights
    have been increased.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ChatGPT/GPT-4:** OpenAI 并未公开详细信息。但已知 ChatGPT/GPT-4 大体上遵循之前 GPT 模型的设计，并且仍然使用
    RLHF 来调整模型（可能使用了更多、更高质量的数据/标签）。普遍认为 GPT-4 使用了更大的数据集，因为模型权重有所增加。'
- en: '**Inference data development.** As recent GPT models are already sufficiently
    powerful, we can achieve various goals by tuning prompts (or tuning inference
    data) with the model fixed. For example, we can conduct text summarization by
    offering the text to be summarized alongside an instruction like “summarize it”
    or “TL;DR” to steer the inference process.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**推理数据开发。** 由于最近的 GPT 模型已经足够强大，我们可以通过调整提示（或调整推理数据）来实现各种目标，而不需要调整模型。例如，我们可以通过将待总结的文本与“总结一下”或“TL;DR”这样的指令一起提供来进行文本摘要，从而引导推理过程。'
- en: '![](../Images/002c7658b5abc0122c062efbae7b3852.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/002c7658b5abc0122c062efbae7b3852.png)'
- en: Prompt tuning. [https://arxiv.org/abs/2303.10158](https://arxiv.org/abs/2303.10158).
    Image by the author.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 提示调优。 [https://arxiv.org/abs/2303.10158](https://arxiv.org/abs/2303.10158)。图像由作者提供。
- en: Designing the proper prompts for inference is a challenging task. It heavily
    relies on heuristics. A nice [survey](https://arxiv.org/abs/2107.13586) has summarized
    different promoting methods. Sometimes, even semantically similar prompts can
    have very diverse outputs. In this case, [Soft Prompt-Based Calibration](https://arxiv.org/abs/2303.13035v1)
    may be required to reduce variance.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 设计适当的推理提示是一项具有挑战性的任务。这在很大程度上依赖于启发式方法。一项不错的 [调查](https://arxiv.org/abs/2107.13586)
    总结了不同的促销方法。有时，即使是语义上相似的提示也可能产生非常不同的输出。在这种情况下，可能需要 [软提示基础校准](https://arxiv.org/abs/2303.13035v1)
    来减少方差。
- en: '![](../Images/8ec843a99cc26c78f25d50e5ac8ebd5a.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ec843a99cc26c78f25d50e5ac8ebd5a.png)'
- en: Soft prompt-based calibration. Image from the paper [https://arxiv.org/abs/2303.13035v1](https://arxiv.org/abs/2303.13035v1)
    with original authors’ permission.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 基于软提示的校准。图像来自论文 [https://arxiv.org/abs/2303.13035v1](https://arxiv.org/abs/2303.13035v1)，已获原作者许可。
- en: The research of inference data development for LLMs is still in its early stage.
    More [inference data development techniques that have been used in other tasks](https://arxiv.org/abs/2303.10158)
    could be applied in LLMs in the near future.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的推理数据开发研究仍处于初期阶段。更多 [推理数据开发技术在其他任务中](https://arxiv.org/abs/2303.10158) 的应用可能会在不久的将来应用于
    LLM。
- en: '**Data maintenance.** ChatGPT/GPT-4, as a commercial product, is not only trained
    once but rather is updated continuously and maintained. Clearly, we can’t know
    how data maintenance is executed outside of OpenAI. So, we discuss some general
    data-centric AI strategies that are or will be very likely used for GPT models:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据维护。** ChatGPT/GPT-4 作为一种商业产品，不仅仅是训练一次，而是会持续更新和维护。显然，我们无法了解 OpenAI 外部的数据维护是如何执行的。因此，我们讨论一些用于
    GPT 模型的数据中心 AI 策略，这些策略现在或将来很可能会被使用：'
- en: '***- Continuous data collection:*** When we use ChatGPT/GPT-4, our prompts/feedback
    could be, in turn, used by OpenAI to further advance their models. Quality metrics
    and assurance strategies may have been designed and implemented to collect high-quality
    data in this process.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '***- 持续的数据收集：*** 当我们使用 ChatGPT/GPT-4 时，我们的提示/反馈可能会被 OpenAI 进一步用于提升他们的模型。质量指标和保证策略可能已经被设计和实施，以在此过程中收集高质量的数据。'
- en: '***- Data understanding tools:*** Various tools could have been developed to
    visualize and comprehend user data, facilitating a better understanding of users’
    requirements and guiding the direction of future improvements.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '***- 数据理解工具：*** 可能已经开发出各种工具来可视化和理解用户数据，从而更好地理解用户的需求并指导未来的改进方向。'
- en: '***- Efficient data processing:*** As the number of users of ChatGPT/GPT-4
    grows rapidly, an efficient data administration system is required to enable fast
    data acquisition.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '***- 高效的数据处理：*** 随着 ChatGPT/GPT-4 用户数量的快速增长，需要一个高效的数据管理系统来实现快速的数据获取。'
- en: '![](../Images/ce19edca589650b43eca2390c3615710.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce19edca589650b43eca2390c3615710.png)'
- en: ChatGPT/GPT-4 collects user feedback with “thumb up” and “thumb down” to further
    evolve their system. Screenshot from [https://chat.openai.com/chat](https://chat.openai.com/chat).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT/GPT-4 使用“赞”和“踩”来收集用户反馈，以进一步发展他们的系统。截图来自 [https://chat.openai.com/chat](https://chat.openai.com/chat)。
- en: What Can the Data Science Community Learn from this Wave of LLMs?
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据科学社区能从这波 LLM 中学到什么？
- en: 'The success of LLMs has revolutionized AI. Looking forward, LLMs could further
    revolutionize the data science lifecycle. We make two predictions:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的成功已经彻底改变了 AI。展望未来，LLM 可能会进一步革新数据科学生命周期。我们做出两个预测：
- en: '**Data-centric AI becomes even more important.** After years of research, the
    model design is already very mature, especially after Transformer. Engineering
    data becomes a crucial (or possibly the only) way to improve AI systems in the
    future. Also, when the model becomes sufficiently powerful, we don’t need to train
    models in our daily work. Instead, we only need to design the proper inference
    data (prompt engineering) to probe knowledge from the model. Thus, the research
    and development of data-centric AI will drive future advancements.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据中心 AI 变得更加重要。** 经过多年的研究，模型设计已经非常成熟，特别是在 Transformer 之后。工程数据将成为未来提高 AI 系统的关键（或可能是唯一）途径。此外，当模型变得足够强大时，我们在日常工作中无需训练模型。相反，我们只需设计合适的推理数据（提示工程）以从模型中探测知识。因此，数据中心
    AI 的研究与发展将推动未来的进步。'
- en: '**LLMs will enable better data-centric AI solutions.** Many of the tedious
    data science works could be performed much more efficiently with the help of LLMs.
    For example, ChaGPT/GPT-4 can already write workable codes to process and clean
    data. Additionally, LLMs can even be used to create data for training. For example,
    [recent work](https://arxiv.org/abs/2303.04360) has shown that generating synthetic
    data with LLMs can boost model performance in clinical text mining.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLM 将推动更好的数据中心 AI 解决方案。** 许多繁琐的数据科学工作可以借助 LLM 更高效地完成。例如，ChatGPT/GPT-4 已经能够编写可用的代码来处理和清理数据。此外，LLM
    甚至可以用于创建训练数据。例如，[最近的工作](https://arxiv.org/abs/2303.04360) 表明，使用 LLM 生成合成数据可以提升临床文本挖掘中的模型性能。'
- en: '![](../Images/0ba4a3065f5e08b2d7c60e0b9da4c6d1.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ba4a3065f5e08b2d7c60e0b9da4c6d1.png)'
- en: Generating synthetic data with LLMs to train the model. Image from the paper
    [https://arxiv.org/abs/2303.04360](https://arxiv.org/abs/2303.04360) with the
    original authors’ permission.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LLM 生成合成数据以训练模型。图片来自论文 [https://arxiv.org/abs/2303.04360](https://arxiv.org/abs/2303.04360)
    并获得原作者的许可。
- en: Resources
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: 'I hope this article can inspire you in your own work. You can learn more about
    the data-centric AI framework and how it benefits LLMs in the following papers:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章能激发你在自己的工作中。你可以通过以下论文了解更多关于数据中心 AI 框架以及它如何使 LLM 受益的信息：
- en: '[1] [Data-centric Artificial Intelligence: A Survey](https://arxiv.org/abs/2303.10158)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] [数据中心人工智能：综述](https://arxiv.org/abs/2303.10158)'
- en: '[2] [Data-centric AI: Perspectives and Challenges](https://arxiv.org/abs/2301.04819)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] [数据驱动 AI：视角与挑战](https://arxiv.org/abs/2301.04819)'
- en: We have maintained a [GitHub repo](https://github.com/daochenzha/data-centric-AI),
    which will regularly update the relevant data-centric AI resources. Stay tuned!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们维护了一个[GitHub 仓库](https://github.com/daochenzha/data-centric-AI)，该仓库将定期更新相关的数据驱动
    AI 资源。敬请关注！
- en: In the later articles, I will delve into the three goals of data-centric AI
    (training data development, inference data development, and data maintenance)
    and introduce the representative methods.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续文章中，我将深入探讨数据驱动 AI 的三个目标（训练数据开发、推理数据开发和数据维护），并介绍代表性的方法。
