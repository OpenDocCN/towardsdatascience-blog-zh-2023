- en: LoRA — Intuitively and Exhaustively Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LoRA — 直观且详尽的解释
- en: 原文：[https://towardsdatascience.com/lora-intuitively-and-exhaustively-explained-e944a6bff46b](https://towardsdatascience.com/lora-intuitively-and-exhaustively-explained-e944a6bff46b)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/lora-intuitively-and-exhaustively-explained-e944a6bff46b](https://towardsdatascience.com/lora-intuitively-and-exhaustively-explained-e944a6bff46b)
- en: Natural Language Processing | Machine Learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理 | 机器学习
- en: Exploring the modern wave of machine learning with cutting edge fine tuning
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索现代机器学习的前沿，通过前沿微调
- en: '[](https://medium.com/@danielwarfield1?source=post_page-----e944a6bff46b--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----e944a6bff46b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e944a6bff46b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e944a6bff46b--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----e944a6bff46b--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@danielwarfield1?source=post_page-----e944a6bff46b--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----e944a6bff46b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e944a6bff46b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e944a6bff46b--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----e944a6bff46b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e944a6bff46b--------------------------------)
    ·18 min read·Nov 7, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e944a6bff46b--------------------------------)
    ·18分钟阅读·2023年11月7日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ea13e6af7448529f375da8c5e914e006.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea13e6af7448529f375da8c5e914e006.png)'
- en: “Lora The Tuner” By Daniel Warfield using MidJourney. All images by the author
    unless otherwise specified.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: “Lora The Tuner” 作者 Daniel Warfield 使用 MidJourney 制作。所有图片均为作者所用，除非另有说明。
- en: Fine tuning is the process of tailoring a machine learning model to a specific
    application, which can be vital in achieving consistent and high quality performance.
    In this article we’ll discuss “Low-Rank Adaptation” (LoRA), one of the most popular
    fine tuning strategies. First we’ll cover the theory, then we’ll use LoRA to fine
    tune a language model, improving its question answering abilities.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是将机器学习模型调整到特定应用的过程，这在实现一致和高质量的性能中可能至关重要。在本文中，我们将讨论“低秩适应”（LoRA），这是最受欢迎的微调策略之一。首先我们将介绍理论，然后使用LoRA微调语言模型，提高其问答能力。
- en: '![](../Images/88f897196d79cc9be45b43f42a722cb2.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88f897196d79cc9be45b43f42a722cb2.png)'
- en: The results of fine tuning. Before fine tuning the output is gibberish, the
    model repeats the question and a bogus answers repeatedly. After fine tuning the
    output is clear, concise, and accurate.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 微调的结果。在微调之前，输出是乱码，模型不断重复问题和虚假的答案。微调后，输出清晰、简洁且准确。
- en: '**Who is this useful for?** Anyone interested in learning state of the art
    machine learning approaches. We’ll be focusing on language modeling in this article,
    but LoRA is a popular choice in many machine learning applications.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**这对谁有用？** 任何对学习最新机器学习方法感兴趣的人。本文将重点关注语言建模，但LoRA在许多机器学习应用中是一个受欢迎的选择。'
- en: '**How advanced is this post?** This article should be approachable to novice
    data scientists and enthusiasts, but contains topics which are critical in advanced
    applications.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**这篇文章有多先进？** 本文应该对新手数据科学家和爱好者可读，但包含在高级应用中至关重要的主题。'
- en: '**Pre-requisites:** While not required, a solid working understanding of large
    language models (LLMs) would probably be useful. Feel free to refer to my article
    on transformers, a common form of language model, for more information:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**前提条件：** 虽然不是必需的，但对大型语言模型（LLMs）有一个扎实的工作理解可能会有帮助。欢迎参考我的关于变换器的文章，变换器是语言模型的一种常见形式，了解更多信息：'
- en: '[](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----e944a6bff46b--------------------------------)
    [## Transformers — Intuitively and Exhaustively Explained'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----e944a6bff46b--------------------------------)
    [## 变换器 — 直观且详尽的解释'
- en: 'Exploring the modern wave of machine learning: taking apart the transformer
    step by step'
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索现代机器学习的前沿：逐步拆解变换器
- en: towardsdatascience.com](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----e944a6bff46b--------------------------------)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----e944a6bff46b--------------------------------)'
- en: 'You’ll also probably want to have an idea of what a gradient is. I also have
    an article on that:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可能想了解什么是梯度。我还有一篇相关的文章：
- en: '[](/what-are-gradients-and-why-do-they-explode-add23264d24b?source=post_page-----e944a6bff46b--------------------------------)
    [## What Are Gradients, and Why Do They Explode?'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 什么是梯度？它们为什么会爆炸？](/what-are-gradients-and-why-do-they-explode-add23264d24b?source=post_page-----e944a6bff46b--------------------------------)'
- en: By reading this post you will have a firm understanding of the most important
    concept in deep learning
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过阅读这篇文章，你将对深度学习中最重要的概念有一个坚实的理解。
- en: towardsdatascience.com](/what-are-gradients-and-why-do-they-explode-add23264d24b?source=post_page-----e944a6bff46b--------------------------------)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/what-are-gradients-and-why-do-they-explode-add23264d24b?source=post_page-----e944a6bff46b--------------------------------)'
- en: If you don’t feel confident on either of these topics you can still get a lot
    from this article, but they exist if you get confused.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这两个主题都不太自信，你仍然可以从这篇文章中获益，但如果感到困惑，它们会存在。
- en: What, and Why, is Fine Tuning?
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是微调？为什么要微调？
- en: As the state of the art of machine learning has evolved, expectations of model
    performance have increased; requiring more complex machine learning approaches
    to match the demand for heightened performance. In the earlier days of machine
    learning it was feasible to build a model and train it in a single pass.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习技术的进步，对模型性能的期望也有所提高；这要求更复杂的机器学习方法来满足对更高性能的需求。在早期的机器学习中，构建一个模型并在一次训练中完成是可行的。
- en: '![](../Images/f38a87df35c70af383c252be5c3a7263.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f38a87df35c70af383c252be5c3a7263.png)'
- en: Training, in its simplest sense. You take an untrained model, give it data,
    and get a performant model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 训练，最简单的定义就是：你拿一个未经训练的模型，给它数据，然后得到一个性能优越的模型。
- en: This is still a popular strategy for simple problems, but for more complex problems
    it can be useful to think of training as two parts; “pre-training” then “fine
    tuning”. The general idea is to do an initial training pass on a bulk dataset
    and to then refine the model on a tailored dataset.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略仍然是解决简单问题的流行方法，但对于更复杂的问题，将训练分为“预训练”和“微调”两个部分可能更有用。一般思路是在大数据集上进行初始训练，然后在量身定制的数据集上对模型进行优化。
- en: '![](../Images/eae0e38b25446fe393550adca71bc765.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eae0e38b25446fe393550adca71bc765.png)'
- en: Pre Training and Fine Tuning, a refinement of the typical single-shot training
    strategy.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练和微调，是对典型单次训练策略的改进。
- en: This “pre-training” then “fine tuning” strategy can allow data scientists to
    leverage multiple forms of data and use large pre-trained models for specific
    tasks. As a result, pre-training then fine tuning is a common and incredibly powerful
    paradigm. It comes with a few difficulties, though, which we’ll discuss in the
    following section.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这种“预训练”然后“微调”的策略可以让数据科学家利用多种数据形式，并将大型预训练模型用于特定任务。因此，预训练然后微调是一种常见且极其强大的范式。然而，它也有一些困难，我们将在下一节讨论。
- en: Difficulties with Fine Tuning
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调中的困难
- en: The most basic form of fine tuning is to use the same exact process you used
    to pre-train a model to then fine tune that model on new data. You might train
    a model on a huge corpus of general text data, for instance, then fine tune that
    model using the same training strategy on a more specific dataset.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 微调的最基本形式是使用你用于预训练模型的完全相同的过程，然后在新数据上微调该模型。例如，你可能会在一个巨大的通用文本数据集上训练一个模型，然后使用相同的训练策略在一个更具体的数据集上进行微调。
- en: '![](../Images/eae0e38b25446fe393550adca71bc765.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eae0e38b25446fe393550adca71bc765.png)'
- en: In it’s simplest form, pre-training and fine tuning are proceduraly identical.
    You pre-train a model on one set of data, then fine tune on another set of data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的形式下，预训练和微调在程序上是相同的。你在一组数据上预训练模型，然后在另一组数据上微调。
- en: This strategy can be expensive. LLMs are absolutely massive, to fine tune using
    this strategy you would need enough memory to store not only the entire model,
    but also gradients for every parameter in the entire model (gradients being the
    thing that lets the model know what direction to tweak its parameters). Both the
    parameters and the gradients need to live on a GPU, which is why training LLMs
    requires so much GPU memory.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略可能很昂贵。LLM（大规模语言模型）绝对庞大，要使用这种策略进行微调，你需要足够的内存来存储不仅是整个模型，还要存储整个模型中每个参数的梯度（梯度是让模型知道如何调整其参数方向的东西）。参数和梯度都需要存储在GPU上，这就是为什么训练LLM需要如此多的GPU内存。
- en: '![](../Images/a448ee196ed4fdcbceb751d6aabc73e1.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a448ee196ed4fdcbceb751d6aabc73e1.png)'
- en: Back propagation, which is the strategy used in training machine learning models.
    Machine learning models are “differentiable”, which means you can calculate “gradients”,
    which can tell you how a small change to a certain parameter will impact model
    output. We generate a prediction, calculate gradients, calculate how wrong the
    prediction is, then use the gradients to improve the parameters of the model.
    Both pre-training and fine tuning employ back propagation, which requires the
    computation of a gradient for ever learnable parameter in the model. This means,
    if you have a 100 billion parameter model, you need to store 100 billion gradients
    as well. This cycle is done repeatedly, perhaps billions of times, to train a
    model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是训练机器学习模型时使用的策略。机器学习模型是“可微分的”，这意味着你可以计算“梯度”，梯度可以告诉你对某个参数的小变化会如何影响模型输出。我们生成预测，计算梯度，计算预测的错误程度，然后使用梯度来改善模型的参数。预训练和微调都采用反向传播，这需要计算模型中每个可学习参数的梯度。这意味着，如果你有一个1000亿参数的模型，你还需要存储1000亿个梯度。这一过程会重复进行，可能达到数十亿次，以训练模型。
- en: On top of the issue of storing gradients, it’s common to save “checkpoints”,
    which are copies of the model at a particular state throughout the training process.
    This is a great strategy, allowing one to experiment with the model at different
    phases of the fine-tuning process, but it means we need to store numerous full-size
    copies of the model. Falcon 180B, a popular modern LLM, requires around 360GB
    in storage. If we wanted to store a checkpoint of the model ten times throughout
    the fine-tuning process it would consume 3.6 terabytes of storage, which is a
    lot. Perhaps even more importantly, it takes time to save such a large amount
    of data. The data typically has to come off the GPU, into RAM, then onto storage;
    potentially adding significant delay to the fine-tuning process.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 除了存储梯度的问题，通常还会保存“检查点”，即在训练过程中模型在特定状态下的副本。这是一种很好的策略，可以在微调过程的不同阶段实验模型，但这意味着我们需要存储大量完整尺寸的模型副本。流行的现代LLM
    Falcon 180B需要大约360GB的存储空间。如果我们希望在微调过程中保存模型的十个检查点，这将消耗3.6TB的存储，这是一笔巨大的开支。也许更重要的是，保存如此大量的数据需要时间。数据通常需要从GPU转移到RAM，然后再到存储器，这可能会显著延长微调过程的时间。
- en: LoRA can help us deal with these issues and more. Less GPU Memory usage, smaller
    file sizes, faster fine-tuning times, the list goes on and on. In a practical
    sense one can generally consider LoRA a direct upgrade of the traditional style
    of fine-tuning. We’ll cover exactly how LoRA works and how it can achieve such
    a remarkable improvements in the following sections.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA可以帮助我们解决这些问题以及更多。减少GPU内存使用，缩小文件大小，加快微调时间，等等。从实际意义上讲，LoRA通常被认为是传统微调方法的直接升级。我们将在接下来的部分详细介绍LoRA如何工作以及它如何实现如此显著的改进。
- en: LoRA in a Nutshell
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LoRA简介
- en: '“Low-Rank Adaptation” (LoRA) is a form of “parameter efficient fine tuning”
    (PEFT), which allows one to fine tune a large model using a small number of learnable
    parameters. LoRA employs a few concepts which, when used together, massively improve
    fine tuning:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: “低秩适配”（LoRA）是一种“参数高效微调”（PEFT）的形式，它允许使用少量可学习的参数来微调大型模型。LoRA使用了一些概念，这些概念结合在一起，可以大幅度提高微调效果。
- en: We can think of fine tuning as learning changes to parameters, instead of adjusting
    parameters themselves.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以将微调视为学习对参数的更改，而不是直接调整参数本身。
- en: We can try to compress those changes into a smaller representation by removing
    duplicate information.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以尝试通过删除重复信息将这些更改压缩为更小的表示形式。
- en: We can “load” our changes by simply adding them to the pre-trained parameters.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过简单地将更改添加到预训练的参数中来“加载”我们的更改。
- en: Don’t worry if that’s confusing; in the following sections we’ll go over these
    ideas step by step.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这让你感到困惑，不必担心；在接下来的部分中，我们将一步步讲解这些概念。
- en: 1) Fine Tuning as Parameter Changes
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1) 微调作为参数变化
- en: As we previously discussed, the most basic approach to fine tuning consists
    of iteratively updating parameters. Just like normal model training, you have
    the model make an inference, then update the parameters of the model based on
    how wrong that inference was.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，最基本的微调方法包括迭代更新参数。就像正常的模型训练一样，你让模型进行推断，然后根据该推断的错误程度来更新模型的参数。
- en: '![](../Images/a448ee196ed4fdcbceb751d6aabc73e1.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a448ee196ed4fdcbceb751d6aabc73e1.png)'
- en: Recall the back propagation diagram previously discussed. This is the basic
    form of fine tuning.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾之前讨论的反向传播图。这是微调的基本形式。
- en: LoRA thinks of this slightly differently. Instead of thinking of fine tuning
    as learning better parameters, you can think of fine tuning as learning parameter
    changes. You can freeze the model parameters, exactly how they are, and learn
    the changes to those parameters necessary to make the model perform better at
    the fine tuned task.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA对此有不同的看法。与其把微调看作是学习更好的参数，不如把微调看作是学习参数的变化。你可以将模型参数固定不变，学习使模型在微调任务上表现更好的参数变化。
- en: This is done very similarly to training; you have the model make an inference,
    then update based on how wrong the inference was. However, instead of updating
    the model parameters, you update the change in the model parameters.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这与训练的方式非常相似；你让模型进行推断，然后根据推断的错误程度进行更新。然而，不是更新模型参数，而是更新模型参数的变化。
- en: '![](../Images/d1c4548035490a428d0a9549a5f9254b.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1c4548035490a428d0a9549a5f9254b.png)'
- en: In LoRA we freeze the model parameters, and create a new set of values which
    describes the change in those parameters. We then learn the parameter changes
    necessary to perform better on the fine tuning task.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在LoRA中，我们冻结模型参数，并创建一组新的值来描述这些参数的变化。然后，我们学习使模型在微调任务上表现更好的参数变化。
- en: You might be thinking this is a bit of a silly abstraction. The whole point
    of LoRA is that we want to make fine tuning smaller and faster, how does adding
    more data and extra steps allow us to do that? In the next section we’ll discuss
    exactly that.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会觉得这有点儿抽象。LoRA的核心目的是使微调变得更小更快，如何通过增加更多的数据和额外的步骤来实现呢？在下一节中我们将详细讨论这一点。
- en: 2) Parameter Change Compression
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2) 参数变化压缩
- en: For the sake of illustration many represent dense networks as a series of weighted
    connections. Each input gets multiplied by some weight, and then added together
    to create outputs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明问题，许多人将密集网络表示为一系列加权连接。每个输入乘以某个权重，然后加在一起以产生输出。
- en: '![](../Images/abb14b4b1f2a4074d288a20a84c0aeb2.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/abb14b4b1f2a4074d288a20a84c0aeb2.png)'
- en: A conceptual diagram of a dense network as a list of neurons connected by weights.
    The value of a particular neuron would be the sum of all inputs multiplied by
    the inputs respective weight.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 密集网络的概念图，展示了一系列由权重连接的神经元。某个特定神经元的值将是所有输入乘以相应权重后的总和。
- en: This is a completely accurate visualization from a conceptual perspective, but
    under the hood this actually happens via matrix multiplication. A matrix of values,
    called a weight matrix, gets multiplied by a vector of inputs to create the vector
    of outputs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，这是一种完全准确的可视化，但实际上，这是通过矩阵乘法来实现的。一个值矩阵（称为权重矩阵）与输入向量相乘，生成输出向量。
- en: '![](../Images/ec97bb13a3ee76c3aec028567378d9bd.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec97bb13a3ee76c3aec028567378d9bd.png)'
- en: A conceptual diagram of matrix multiplication. [Source](https://en.wikipedia.org/wiki/Matrix_multiplication#/media/File:Matrix_multiplication_diagram_2.svg)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法的概念图。[来源](https://en.wikipedia.org/wiki/Matrix_multiplication#/media/File:Matrix_multiplication_diagram_2.svg)
- en: To give you an idea of how matrix multiplication works. In the example above
    the red dot is equal to a₁₁•b₁₂ + a₁₂•b₂₂. As you can see, this combination of
    multiplication and addition is very similar to that found in the neuron example.
    If we create the correctly shaped matices, matrix multiplication ends up shaking
    out exactly identically to the concept of weighted connections.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让你了解矩阵乘法的工作原理。在上面的例子中，红点等于a₁₁•b₁₂ + a₁₂•b₂₂。正如你所见，这种乘法和加法的组合与神经元示例中的非常相似。如果我们创建正确形状的矩阵，矩阵乘法最终会与加权连接的概念完全一致。
- en: '![](../Images/6e209e6a8f4ccf37c5172622475043c8.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e209e6a8f4ccf37c5172622475043c8.png)'
- en: Thinking of a dense network as weighted connections on the left, and as matrix
    multiplication on the right. On the right hand side diagram, the vector on the
    left would be the input, the matrix in the center would be the weight matrix,
    and the vector on the right would be the output. Only a portion of values are
    included for readability.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 将密集网络视为左侧的加权连接，右侧则视为矩阵乘法。在右侧的图示中，左侧的向量是输入，中心的矩阵是权重矩阵，右侧的向量是输出。为了可读性，只包含了一部分值。
- en: From the perspective of LoRA, understanding that weights are actually a matrix
    is incredibly important, as a matrix has certain properties which we can be leveraged
    to condense information.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从LoRA的角度来看，理解权重实际上是一个矩阵非常重要，因为矩阵具有某些属性，可以用来压缩信息。
- en: Matrix Property 1) Linear Independence
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵属性 1) 线性独立性
- en: 'You can think of a matrix, which is a two dimensional array of values, as either
    rows or columns of vectors. For now let’s just think of matrices as rows of vectors.
    Say we have a matrix consisting of two vectors which look something like this:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将矩阵（即二维值数组）视为向量的行或列。现在我们只需将矩阵视为向量的行。假设我们有一个由两个向量组成的矩阵，其大致如下所示：
- en: '![](../Images/1ef49f62e375e874e64ba7bbcd69c7a6.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ef49f62e375e874e64ba7bbcd69c7a6.png)'
- en: A matrix consisting of two vectors, represented as rows in the matrix.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一个由两个向量组成的矩阵，这些向量在矩阵中被表示为行。
- en: Each of these vectors point in different directions. You can’t squash and stretch
    one vector to be equal to the other vector.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 每个向量指向不同的方向。你不能通过压缩和扩展一个向量来使其与另一个向量相等。
- en: '![](../Images/6a9d31bad32542015e594d707e794946.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a9d31bad32542015e594d707e794946.png)'
- en: Each row as a matrix, plotted as a vector. No matter how the blue vector gets
    squashed or stretched, it will never point in the same direction as the red vector,
    and vice versa.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行作为一个矩阵，绘制为一个向量。无论蓝色向量如何被压缩或扩展，它都不会指向与红色向量相同的方向，反之亦然。
- en: Let’s add a third vector into the mix.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加入第三个向量。
- en: '![](../Images/67cd87845c2c930313d0319d04fd0e5e.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67cd87845c2c930313d0319d04fd0e5e.png)'
- en: Vectors `A` and `B`are pointing in the same exact direction, while vector `C`
    is pointing in a different direction. As a result, no matter how you squash and
    stretch either `A`or `B`, they can never be used to describe `C`. Therefore, `C`
    is linearly independent from `A`and `B`. However, you can stretch `A` to equal
    `B` , and vice versa, so `A` and `B` are linearly dependent.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 向量`A`和`B`指向完全相同的方向，而向量`C`指向不同的方向。因此，无论你如何压缩或扩展`A`或`B`，它们都无法用来描述`C`。因此，`C`与`A`和`B`是线性独立的。然而，你可以将`A`扩展为等于`B`，反之亦然，因此`A`和`B`是线性相关的。
- en: Let's say `A` and `B` pointed in slightly different directions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 假设`A`和`B`指向稍有不同的方向。
- en: '![](../Images/a9531adc7177a666feaa82cea38828a5.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9531adc7177a666feaa82cea38828a5.png)'
- en: Now `A` and `B` can be used together (With some squashing and stretching) to
    describe `C` , and likewise `A` and `B` can be described by the other vectors.
    In this situation we would say none of the vectors are linearly independent, because
    all vectors can be described with other vectors in the matrix.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在`A`和`B`可以一起使用（经过一些压缩和扩展）来描述`C`，同样`A`和`B`也可以被其他向量描述。在这种情况下，我们会说这些向量没有线性独立性，因为所有向量都可以用矩阵中的其他向量来描述。
- en: '![](../Images/eb9a3b70424742ff0a0dfe754902fc88.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb9a3b70424742ff0a0dfe754902fc88.png)'
- en: Using A and B to describe C. B’s magnitude can be multiplied by a negative number
    to flip it’s magnitude, the added to A.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`A`和`B`来描述`C`。`B`的大小可以通过乘以一个负数来翻转其大小，然后加到`A`上。
- en: Conceptually speaking, linearly independent vectors can be thought of as containing
    different information, while linearly dependent vectors contain some duplicate
    information between them.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，线性独立的向量可以被视为包含不同的信息，而线性相关的向量则包含一些重复的信息。
- en: Matrix Property 2) Rank
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵属性 2) 秩
- en: 'The idea of rank is to quantify the amount of linear independence within a
    matrix. I’ll skip the nitty gritty details and get straight to the point: We can
    break a matrix down into some number of linearly independent vectors; This form
    of the matrix is called “reduced row echelon form”.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 秩的概念是量化矩阵中的线性独立性。我将跳过详细的内容，直接进入要点：我们可以将矩阵分解为一些线性独立的向量；这种形式的矩阵称为“简化行最简形”。
- en: '![](../Images/5c4ba81672157aced67c5768d471cbbc.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c4ba81672157aced67c5768d471cbbc.png)'
- en: A matrix (left) and that same matrix in reduced row echelon form (right). in
    the RREF matrix you can see that there are four linearly independent vectors (rows).
    Each of these vectors can be used in combination to describe all vectors in the
    input matrix.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一个矩阵（左）和同一个矩阵的简化行最简形式（右）。在 RREF 矩阵中，你可以看到有四个线性独立的向量（行）。这些向量可以组合使用来描述输入矩阵中的所有向量。
- en: By breaking the matrix down into this form (I won’t describe how because this
    is only useful to us conceptually), you can count how many linearly independent
    vectors can be used to describe the original matrix. The number of linearly independent
    vectors is the “rank” of the matrix. The rank of the RREF matrix above would be
    four, as there are four linearly independent vectors.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将矩阵分解成这种形式（我不会描述如何做，因为这只是对我们概念上有用），你可以计算出多少个线性独立的向量可以用来描述原始矩阵。线性独立向量的数量就是矩阵的“秩”。上述
    RREF 矩阵的秩为四，因为有四个线性独立的向量。
- en: 'A little note I’ll drop in here: regardless of if you consider a matrix in
    terms of rows of vectors or columns of vectors, the rank is always the same. This
    is a mathy little detail which isn’t super important, but does have conceptual
    implications for the next section.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里插一句：无论你是否将矩阵视为向量的行还是列，秩始终是相同的。这是一个数学上的小细节，虽然不是特别重要，但对下一节有概念上的影响。
- en: Matrix Property 3) Matrix Factors
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵属性 3) 矩阵因子
- en: So, matrices can contain some level of “duplicate information” in the form of
    linear dependence. We can exploit this idea using factorization to represent a
    large matrix in terms of two smaller matrices. Similarly to how a large number
    can be represented as the multiplication of two smaller numbers, a matrix can
    be thought of as the multiplication of two smaller matrices.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，矩阵可以包含某种程度的“重复信息”，这种信息以线性依赖的形式存在。我们可以利用这一点，通过因式分解来表示一个大矩阵为两个较小矩阵的乘积。类似于如何将一个大数字表示为两个小数字的乘积，矩阵也可以被视为两个较小矩阵的乘积。
- en: '![](../Images/5221bbcd2d03d7997c44e075a2b36960.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5221bbcd2d03d7997c44e075a2b36960.png)'
- en: The two vectors on the right, when multiplied together, are equivalent to the
    matrix on the left. Even though they have the same value, the vectors on the left
    occupy 40% of the size that the matrix on the right occupies. The larger the matrix
    becomes, the more factors have a tendency to save on space.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的两个向量相乘等于左侧的矩阵。尽管它们的值相同，但左侧的向量占据了右侧矩阵的 40% 的大小。矩阵越大，因子就越能节省空间。
- en: If you have a large matrix, with a significant degree of linear dependence (and
    thus a low rank), you can express that matrix as a factor of two comparatively
    small matrices. This idea of factorization is what allows LoRA to occupy such
    a small memory footprint.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个大矩阵，且具有显著的线性依赖（因此秩较低），你可以将该矩阵表示为两个相对较小矩阵的因子。这种因式分解的想法使得 LoRA 能够占用如此小的内存空间。
- en: The Core Idea Behind LoRA
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LoRA 背后的核心思想
- en: LoRA thinks of tuning not as adjusting parameters, but as learning parameter
    changes. With LoRA we don’t learn the parameter changes directly, however; we
    learn the factors of the parameter change matrix.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 认为调整不是改变参数，而是学习参数的变化。使用 LoRA 时，我们不会直接学习参数变化；相反，我们学习参数变化矩阵的因子。
- en: '![](../Images/9c1c8d573808ae5aeb587611e83083cd.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c1c8d573808ae5aeb587611e83083cd.png)'
- en: Diagram of LoRA, from the [LoRA paper](https://arxiv.org/pdf/2106.09685.pdf).
    matrices A and B are trained to find optimal changes to the pretrained weights.
    We’ll talk about “r” in a future section.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 的示意图，来自于 [LoRA 论文](https://arxiv.org/pdf/2106.09685.pdf)。矩阵 A 和 B 经过训练以找到对预训练权重的最佳调整。我们将在未来的章节中讨论“r”。
- en: This idea of learning factors of the change matrix relies on the core assumption
    that weight matrices within a large language model have a lot of linear dependence,
    as a result of having significantly more parameters than is theoretically required.
    Over parameterization has been shown to be beneficial in pre-training (which is
    why modern machine learning models are so large). The idea behind LoRA is that,
    once you’ve learned the general task with pre-training, you can do fine tuning
    with significantly less information.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 学习变化矩阵因子的这一想法依赖于核心假设，即大型语言模型中的权重矩阵具有大量的线性依赖，因为参数数量远远超出理论所需的数量。过度参数化已被证明在预训练中是有益的（这就是现代机器学习模型如此庞大的原因）。LoRA
    的理念是，一旦你通过预训练学习了通用任务，你可以用显著更少的信息进行微调。
- en: learned over-parametrized models in fact reside on a low intrinsic dimension.
    We hypothesize that the change in weights during model adaptation also has a low
    “intrinsic rank”, leading to our proposed Low-Rank Adaptation (LoRA) approach.
    LoRA allows us to train some dense layers in a neural network indirectly by optimizing
    rank decomposition matrices of the dense layers’ change during adaptation instead,
    while keeping the pre-trained weights frozen — [The LoRA Paper](https://arxiv.org/pdf/2106.09685.pdf)
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 过度参数化的模型实际上位于较低的内在维度。我们假设模型适应过程中的权重变化也具有低的“内在秩”，这导致我们提出了低秩适应（LoRA）方法。LoRA允许我们通过优化密集层变化的秩分解矩阵来间接训练神经网络中的一些密集层，同时保持预训练权重冻结
    — [LoRA 论文](https://arxiv.org/pdf/2106.09685.pdf)
- en: This results in a significantly smaller amount of parameters being trained which
    means an overall faster and more storage and memory efficient fine tuning process.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致训练的参数数量显著减少，从而使微调过程整体上更快且在存储和内存上更高效。
- en: Fine-Tuning Flow with LoRA
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LoRA的微调流程
- en: Now that we understand how the pieces of LoRA generally work, let’s put it all
    together.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了LoRA的各个部分如何工作，让我们将它们结合起来。
- en: So, first, we freeze the model parameters. We’ll be using these parameters to
    make inferences, but we won’t update them.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们冻结模型参数。我们将使用这些参数进行推理，但不会更新它们。
- en: '![](../Images/805a248157862c4a47b8eb05be022483.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/805a248157862c4a47b8eb05be022483.png)'
- en: We create two matrices. These are sized in such a way that, when they’re multiplied
    together, they’ll be the same size as the weight matrices of the model we’re fine
    tuning. In a large model, with multiple weight matrices, you would create one
    of these pairs for each weight matrix.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建两个矩阵。这些矩阵的大小设置为，当它们相乘时，它们将与我们微调的模型的权重矩阵大小相同。在大型模型中，具有多个权重矩阵时，你将为每个权重矩阵创建一对这些矩阵。
- en: '![](../Images/360179d813e340fbe00f3c7a84d80582.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/360179d813e340fbe00f3c7a84d80582.png)'
- en: the LoRA paper refferes to these matrices as matrix “A” and “B”. Together, these
    matices represent the leranable parameters during LoRA fine tuning.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 论文将这些矩阵称为矩阵“A”和“B”。这两个矩阵代表了LoRA微调过程中的可学习参数。
- en: We calculate the change matrix
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算变化矩阵
- en: '![](../Images/d33e12aa69588e985f2589cbaa8e00d1.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d33e12aa69588e985f2589cbaa8e00d1.png)'
- en: Then we pass our input through both the frozen weights and the change matrix.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将输入通过冻结的权重和变化矩阵。
- en: '![](../Images/7cf1aea3b66b42543777fd4945aa481a.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7cf1aea3b66b42543777fd4945aa481a.png)'
- en: We calculate a loss based on the combination of both outputs then we update
    matrix A and B based on the loss
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据两个输出的组合计算损失，然后根据损失更新矩阵A和B
- en: '![](../Images/f5fa3f569c4a109c504078a33dd5e305.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5fa3f569c4a109c504078a33dd5e305.png)'
- en: Note, while the change matrix is displayed here for illustrative purposes, in
    reality it’s computed on the fly and never stored, which is why LoRA has such
    a small memory footprint. In reality, only the Model Parameters, the matrices
    A and B, and the gradients of A and B are stored during training.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，虽然这里显示了变化矩阵以便于说明，但实际上它是即时计算的，从未存储，这就是为什么LoRA具有如此小的内存占用。实际上，仅在训练期间存储模型参数、矩阵A和B，以及A和B的梯度。
- en: We do this operation until we’ve optimized the factors of the change matrix
    for our fine tuning task. The backpropagation step to update the matrices A and
    B is much faster than the process to update the full set of model parameters,
    on account of A and B being significantly smaller. This is why, despite more operations
    in the training process, LoRA is still typically faster than traditional fine-tuning.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行这个操作，直到我们优化了用于微调任务的变化矩阵的因子。由于A和B显著较小，因此更新矩阵A和B的反向传播步骤比更新整个模型参数集的过程要快。这就是为什么尽管训练过程中的操作更多，LoRA仍然通常比传统微调更快的原因。
- en: When we ultimately want to make inferences with this fine tuned model, we can
    simply compute the change matrix, and add the changes to the weights. This means
    the LoRA does not change the inference time of the model.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们最终想要使用这个微调模型进行推理时，我们可以简单地计算变化矩阵，并将这些变化添加到权重中。这意味着LoRA不会改变模型的推理时间。
- en: '![](../Images/1ba5cbdd78ac0b0fd793703503f74f80.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ba5cbdd78ac0b0fd793703503f74f80.png)'
- en: A cool little note, we can even multiply the change matrix by a scaling factor,
    allowing us to control the level of impact tha change matrix has on the model.
    In theory, we could use a bit of this LoRA and a dash of that LoRA at the same
    time, an approach which is common in image generation.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的小提示，我们甚至可以将变化矩阵乘以一个缩放因子，从而控制变化矩阵对模型的影响程度。理论上，我们可以同时使用一点这种LoRA和一点那种LoRA，这种方法在图像生成中很常见。
- en: A Note on LoRA For Transformers
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于变换器的LoRA的说明
- en: When researching this article I found a conceptual disconnect which a lot of
    people didn’t discuss. It’s fine to treat a machine learning model as a big box
    of weights, but in actuality many models have a complex structure which isn’t
    very box like. It wasn’t obvious to me how, exactly, this concept of a change
    matrix applies to the parameters in something like a transformer.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究这篇文章时，我发现了一个许多人没有讨论的概念性脱节。把机器学习模型当作一个大权重箱子来处理是可以的，但实际上许多模型具有复杂的结构，这种结构并不是非常“箱子型”的。我不太明白这个变化矩阵的概念如何确切地应用于像变换器这样的参数。
- en: '![](../Images/3356829911ae5e55d3884270aca500b5.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3356829911ae5e55d3884270aca500b5.png)'
- en: The transformer diagram, which I cover in [another article](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb).
    The “Nx” symbol represents the fact that both the left and right side get repeated
    numerous times. This is not a clean square of weights, and thus it’s not obvious
    how LoRA might be applied. Image [source](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器的图示，我在[另一篇文章](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)中进行了讲解。符号“Nx”表示左侧和右侧的重复次数。这不是一个干净的权重方阵，因此不明显如何应用LoRA。图像[来源](https://arxiv.org/pdf/1706.03762.pdf)
- en: 'Based on my current understanding, for transformers specifically, there are
    two things to keep in mind:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我目前的理解，针对变换器，需牢记两点：
- en: Typically the dense network in a transformer’s multi-headed self attention layer
    (the one that construct the query, key, and value) is only of depth one. That
    is, there’s only an input layer and an output layer connected by weights.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通常，变换器的多头自注意层（构建查询、键和值的层）中的密集网络只有一层深度。也就是说，只有一个输入层和一个通过权重连接的输出层。
- en: These shallow dense networks, which comprise most of the learnable parameters
    in a transformer, are very very large. There might be over 100,000 input neurons
    being connected to 100,000 output neurons, meaning a single weight matrix, describing
    one of these networks, might have 10B parameters. So, even though these networks
    might be of depth one, they’re super duper wide, and thus the weight matrix describing
    them is super duper large.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些浅层密集网络构成了变换器中大部分可学习参数，非常非常庞大。可能有超过100,000个输入神经元连接到100,000个输出神经元，这意味着描述这些网络的单个权重矩阵可能有10B个参数。所以，尽管这些网络可能只有一层深度，但它们却极其宽广，因此描述它们的权重矩阵也非常庞大。
- en: From the perspective of LoRA on transformer models, these are the chief parameters
    being optimized; you’re learning factorized changes for each of these incredibly
    large, yet shallow, dense layers which exist within the model. Each of these shallow
    dense layers, as previously discussed, has weights which can be represented as
    a matrix.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 从LoRA在变换器模型中的角度来看，这些是主要的优化参数；你在学习这些在模型内部存在的巨大但浅层的密集层的因子化变化。正如之前讨论的，每个浅层密集层都有可以表示为矩阵的权重。
- en: A Note on LoRA Rank
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于LoRA秩的说明
- en: LoRA has a hyperparameter, named `r`, which describes the depth of the `A` and
    `B` matrix used to construct the change matrix discussed previously. Higher `r`
    values mean larger `A` and `B` matrices, which means they can encode more linearly
    independent information in the change matrix.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA有一个名为`r`的超参数，它描述了用于构建前面讨论的变化矩阵的`A`和`B`矩阵的深度。较高的`r`值意味着`A`和`B`矩阵更大，这意味着它们可以在变化矩阵中编码更多线性独立的信息。
- en: '![](../Images/9c1c8d573808ae5aeb587611e83083cd.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c1c8d573808ae5aeb587611e83083cd.png)'
- en: Diagram of LoRA, from the [LoRA paper](https://arxiv.org/pdf/2106.09685.pdf).
    the “r” parameter can be thought of as an “information bottleneck”. Low r values
    mean A and B can encode less information with a smaller memory footprint. Larger
    r values mean A and B can encode more information, but with a larger memory footprint.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA的图示，来自[LoRA论文](https://arxiv.org/pdf/2106.09685.pdf)。参数“r”可以被视为一个“信息瓶颈”。较低的r值意味着A和B可以用较小的内存占用编码较少的信息。较大的r值意味着A和B可以编码更多的信息，但需要较大的内存占用。
- en: '![](../Images/02ca9251cdea02f6b2418273158f8181.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02ca9251cdea02f6b2418273158f8181.png)'
- en: A conceptual diagram of LoRA with an r value equal to 1 and 2\. In both examples
    the decomposed A and B matrices result in the same sized change matrix, but r=2
    is able to encode more linearly independent information into the change matrix,
    due to having more information in the A and B matrices
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 r 值等于 1 和 2 的 LoRA 概念图。在这两个示例中，分解后的 A 和 B 矩阵会导致相同大小的变化矩阵，但 r=2 能够将更多线性独立的信息编码到变化矩阵中，因为
    A 和 B 矩阵中包含更多信息。
- en: It turns out the core assumption the LoRA paper makes, that the change to model
    parameters is of low implicit rank, is a pretty strong assumption. The folks at
    Microsoft (the publishers of LoRA) tried out a few `r` values and found that even
    `A` and `B` matrices of rank one perform surprisingly well.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，LoRA 论文中提出的核心假设，即模型参数的变化具有较低的隐含秩，是一个相当强的假设。微软的团队（LoRA 的出版者）尝试了一些 `r` 值，发现即使
    `A` 和 `B` 矩阵的秩为 1，也表现得相当好。
- en: '![](../Images/b84ee54032aada28f9506edb24441c28.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b84ee54032aada28f9506edb24441c28.png)'
- en: from the [LoRA paper](https://arxiv.org/pdf/2106.09685.pdf)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 来源于 [LoRA 论文](https://arxiv.org/pdf/2106.09685.pdf)
- en: 'Generally, in selecting `r`, the advice I’ve heard is the following: When the
    data is similar to the data used in pre-training, a low `r` value is probably
    sufficient. When fine tuning on very new tasks, which might require substantial
    logical changes within the model, a high `r` value may be required.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在选择 `r` 时，我听到的建议是：当数据与预训练中使用的数据类似时，低 `r` 值可能足够。当在非常新的任务上进行微调，这可能需要模型内部的 substantial
    logical changes 时，可能需要高 `r` 值。
- en: LoRA in Python
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python 中的 LoRA
- en: Considering how much theory we went over you might be expecting a pretty long
    tutorial, but I have good news! HuggingFace has a module which makes LoRA super
    duper easy.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们讨论了很多理论，你可能期望一个相当长的教程，但我有好消息！HuggingFace 有一个模块可以让 LoRA 变得非常简单。
- en: 'In this example we’ll be fine tuning a pre-trained model for question answering.
    Let’s go ahead and jump right in. Full code can be found here:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将对一个预训练的模型进行问答微调。让我们直接开始。完整代码可以在这里找到：
- en: '[](https://github.com/DanielWarfield1/MLWritingAndResearch/blob/main/LoRA.ipynb?source=post_page-----e944a6bff46b--------------------------------)
    [## MLWritingAndResearch/LoRA.ipynb at main · DanielWarfield1/MLWritingAndResearch'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/DanielWarfield1/MLWritingAndResearch/blob/main/LoRA.ipynb?source=post_page-----e944a6bff46b--------------------------------)
    [## MLWritingAndResearch/LoRA.ipynb 在主分支 · DanielWarfield1/MLWritingAndResearch]'
- en: Notebook Examples used in machine learning writing and research - MLWritingAndResearch/LoRA.ipynb
    at main ·…
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习写作和研究中使用的笔记本示例 - MLWritingAndResearch/LoRA.ipynb 在主分支 ·…
- en: github.com](https://github.com/DanielWarfield1/MLWritingAndResearch/blob/main/LoRA.ipynb?source=post_page-----e944a6bff46b--------------------------------)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/DanielWarfield1/MLWritingAndResearch/blob/main/LoRA.ipynb?source=post_page-----e944a6bff46b--------------------------------)
- en: 1) Downloading Dependencies
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1) 下载依赖项
- en: 'We’ll be using a few modules which are beyond a simple PyTorch project. This
    is what they do:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一些超出简单 PyTorch 项目的模块。这些模块的作用如下：
- en: '**bitsandbytes:** for representing models using smaller datatypes, saving on
    memory.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**bitsandbytes：** 用于使用较小的数据类型表示模型，从而节省内存。'
- en: '**datasets:** for downloading datasets'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**datasets：** 用于下载数据集'
- en: '**accelerate:** required dependency for machine learning interoperability for
    some of the modules'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**accelerate：** 一些模块所需的机器学习互操作性依赖项。'
- en: '**loralib:** LoRA implementation'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**loralib：** LoRA 实现'
- en: '**peft:** a general “parameter efficient fine tuning” module, our interface
    for LoRA'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**peft：** 一般的“参数高效微调”模块，我们的 LoRA 接口。'
- en: '**transformers:** for downloading and using pre-trained transformers from huggingface.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**transformers：** 用于从 huggingface 下载和使用预训练的 transformers。'
- en: '[PRE0]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 2) Loading Pre-Trained Model
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2) 加载预训练模型
- en: We’ll be using BLOOM, an open source and permissively licensed language model.
    We’ll be using the 560 million parameter version to save on memory, but you could
    apply this same strategy to larger versions of BLOOM.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 BLOOM，这是一个开源且许可宽松的语言模型。我们将使用 5.6 亿参数版本以节省内存，但你也可以将此策略应用于 BLOOM 的更大版本。
- en: '[PRE1]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 3) Setting Up LoRA
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3) 设置 LoRA
- en: 'Configuring LoRA with the following parameters:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下参数配置 LoRA：
- en: '**r:** the rank of the A and B matrices'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**r：** A 和 B 矩阵的秩'
- en: '**lora_alpha:** this is a pretty controversial parameter. A lot of people have
    a lot of ideas about it. You can consider it a scaling factor, and by default
    it should be equal to `r`, as far as I understand.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**lora_alpha：** 这是一个相当有争议的参数。很多人对它有很多想法。你可以将它视为一个缩放因子，根据我的理解，它的默认值应该等于 `r`。'
- en: '**target_modules:** the portions of the model we want to optimize with LoRA.
    the BLOOM module has parameters named `query_key_value` which we want to optimize.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标模块（target_modules）：** 我们希望用 LoRA 优化的模型部分。BLOOM 模块有名为 `query_key_value`
    的参数，我们希望优化这些参数。'
- en: '**lora_dropout:** dropout is a technique which hides inputs to suppress the
    model from overfitting (called regularization). This is a probability of being
    hidden.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**lora_dropout：** dropout 是一种技术，通过隐藏输入来抑制模型的过拟合（称为正则化）。这是一个被隐藏的概率。'
- en: '**bias:** neural networks typically have two paramet per connection, a “weight”
    and a “bias”. We’re only training weights in this example.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏置（bias）：** 神经网络通常每个连接有两个参数，一个是“权重”（weight），另一个是“偏置”（bias）。在这个示例中，我们只训练权重。'
- en: '**task_type:** not super necessary, used in the superclass `PeftConfig`. Setting
    to `CAUSAL_LM` because the specific language model we''re using is "causal".'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务类型（task_type）：** 并不是非常必要，主要用于超类 `PeftConfig` 中。设置为 `CAUSAL_LM` 是因为我们使用的特定语言模型是“因果型”。'
- en: '[PRE2]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 4) Examining Memory Savings
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4) 检查内存节省
- en: One of the big ideas of LoRA is that training contains significantly fewer training
    parameters, meaning a large savings in terms of memory consumption. Let’s see
    exactly how much we’re saving in this particular example.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 的一个重要概念是训练包含的训练参数显著减少，这意味着在内存消耗方面有很大的节省。让我们看看在这个特定示例中我们节省了多少。
- en: '[PRE3]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/f77890c5ac04c09519ce9e0e51c95bdc.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f77890c5ac04c09519ce9e0e51c95bdc.png)'
- en: The results of comparing the trainable parameters in LoRA to the parameters
    in the original model. In this example, we’re training on just over one tenth
    of a percent.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 比较 LoRA 中可训练参数与原始模型参数的结果。在这个示例中，我们只训练了不到千分之一的数据。
- en: 5) Loading Fine Tuning Dataset
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5) 加载微调数据集
- en: We’ll be using the SQUAD dataset to improve our language model’s performance
    on question answering. The Stanford Question Answering Dataset (SQUAD) is a high
    quality, commonly used, and permissively licensed dataset.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 SQUAD 数据集来提高我们语言模型在问答任务中的表现。斯坦福问答数据集（SQUAD）是一个高质量、常用且许可宽松的数据集。
- en: '[PRE4]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 6) Re-Structuring Data
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6) 重新结构化数据
- en: 'We’re going to be fine tuning the language model on a specific structure of
    data. The model will expect text in this general form:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对特定结构的数据进行语言模型的微调。模型将期望文本呈现这种一般形式：
- en: '[PRE5]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We’ll provide to the model the context and question, and the model will be expected
    to provide an answer to us. So, we’ll be reformatting the data in SQUAD to respect
    this format.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将向模型提供上下文和问题，模型将被期望为我们提供答案。因此，我们将重新格式化 SQUAD 中的数据以符合这种格式。
- en: '[PRE6]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 7) Fine Tuning on SQUAD using LoRA
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7) 使用 LoRA 在 SQUAD 上进行微调
- en: This code is largly co-opted. In the absence of a rigid validation procedure,
    the best practice is to just copy a successful tutorial or, better yet, directly
    from the documentation. If you were to train an actual model for an actual use
    case, you would probably want to research and potentially optimize some of these
    parameters.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码大多被借用。在缺乏严格验证程序的情况下，最佳实践是直接复制成功的教程，或者更好的是，直接从文档中获取。如果你要为实际用例训练一个实际模型，你可能需要研究并优化这些参数。
- en: '[PRE7]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/9438d812f70c7c66b9b131c20d87a0b7.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9438d812f70c7c66b9b131c20d87a0b7.png)'
- en: The loss (how much error is in the model). In this example we don’t have to
    look at loss super closely, but it serves as a good metric. In this example we
    train for 100 steps and, while there’s some random variation in loss across steps,
    the loss generally goes down throughout the course of training, which is good.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 损失（模型中的错误量）。在这个示例中，我们不需要非常仔细地查看损失，但它作为一个良好的度量指标。在这个示例中，我们训练了 100 步，虽然损失在步骤之间有一些随机变化，但损失通常在训练过程中下降，这是好的。
- en: 8) Checking LoRA Size
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8) 检查 LoRA 大小
- en: Let’s go ahead and save our LoRA optimization
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们保存我们的 LoRA 优化成果。
- en: '[PRE8]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Then check how large the file is in our file system
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 然后检查文件系统中文件的大小。
- en: '[PRE9]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/670a2dc01836e53a9128f0062a7c8b3c.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/670a2dc01836e53a9128f0062a7c8b3c.png)'
- en: The BLOOM 560m model, in it’s float 16 datatype, is over 1 gigabyte in total
    size. With LoRA, and us only needing to save the decomposed matrices, our checkpoint
    size is a mere 3 megabytes. That’s like compressing the entirety of the game “Plants
    vs Zombies” down into a single image taken on an iPhone.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: BLOOM 560m 模型，在其 float 16 数据类型下，总大小超过 1 GB。使用 LoRA，我们只需要保存分解后的矩阵，我们的检查点大小仅为
    3 MB。这就像把整个游戏“植物大战僵尸”压缩成一张 iPhone 拍摄的图片一样。
- en: 9) Testing
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9) 测试
- en: Ok, so we have a LoRA fine tuned model, let’s ask it a few questions. First,
    we’ll define a helper function which will take in a context and question, run
    predictions, and generate a response.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们有一个 LoRA 精细调整后的模型，让我们问它几个问题。首先，我们定义一个辅助函数，该函数将接收上下文和问题，运行预测，并生成响应。
- en: '[PRE10]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s take a look at a few examples, and see just how much better our fine
    tuned model is at question answering:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些示例，了解我们精细调整后的模型在回答问题方面表现如何：
- en: '**Example 1)**'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例 1)**'
- en: '[PRE11]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/88f897196d79cc9be45b43f42a722cb2.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88f897196d79cc9be45b43f42a722cb2.png)'
- en: '**Example 2)**'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例 2)**'
- en: '[PRE12]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/a6c199e9674737d64bc786ff66865eb6.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a6c199e9674737d64bc786ff66865eb6.png)'
- en: We’re only using a 560M parameter model, so it’s not a surprise that it’s not
    very good at basic reasoning. asking it what 1+1 was might have been a bit of
    a stretch, but at least it failed more elegantly.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅使用了一个 560M 参数的模型，因此它在基本推理方面表现不佳也不奇怪。问它 1+1 是什么可能有些勉强，但至少它失败得更优雅。
- en: '**Example 3)**'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例 3)**'
- en: '[PRE13]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/f1cbcfc3c285bc7cab772fbaa8e81ed4.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1cbcfc3c285bc7cab772fbaa8e81ed4.png)'
- en: Again, we’re only using a 560M parameter model. That said, the fine-tuned model
    failed to answer the question significantly more elegantly.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们仅使用了一个 560M 参数的模型。尽管如此，经过精细调整的模型在回答问题时显得更加优雅。
- en: Conclusion
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: And that’s it! We covered the concept of fine tuning, and how LoRA thinks of
    fine tuning as learning changes in parameters, rather than iteratively learning
    new parameters. We learned about linear independence and rank, and how the change
    matrix can be represented by small factors because of how low rank most weight
    matrices are. We put it all together, went through LoRA step by step, then used
    the HuggingFace PEFT module to implement LoRA on a question answering task.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些！我们讨论了精细调整的概念，以及 LoRA 如何将精细调整视为学习参数的变化，而不是迭代地学习新参数。我们了解了线性独立性和秩，以及由于权重矩阵的低秩，变化矩阵如何通过小因子来表示。我们将这些知识结合在一起，逐步了解了
    LoRA，然后使用 HuggingFace PEFT 模块在问答任务中实现了 LoRA。
- en: Follow For More!
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关注以获取更多更新！
- en: I describe papers and concepts in the ML space, with an emphasis on practical
    and intuitive explanations.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我描述了机器学习领域的论文和概念，重点是实用和直观的解释。
- en: '[](https://medium.com/@danielwarfield1/subscribe?source=post_page-----e944a6bff46b--------------------------------)
    [## Get an email whenever Daniel Warfield publishes'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@danielwarfield1/subscribe?source=post_page-----e944a6bff46b--------------------------------)
    [## 每当 Daniel Warfield 发布新内容时获取邮件提醒'
- en: High quality data science articles straight to your inbox. Get an email whenever
    Daniel Warfield publishes. By signing up, you…
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高质量的数据科学文章直达你的邮箱。每当 Daniel Warfield 发布新内容时获取邮件提醒。通过注册，你可以…
- en: medium.com](https://medium.com/@danielwarfield1/subscribe?source=post_page-----e944a6bff46b--------------------------------)
    ![](../Images/1f6f4c8a07d69cf53e055e0130a85b03.png)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@danielwarfield1/subscribe?source=post_page-----e944a6bff46b--------------------------------)
    ![](../Images/1f6f4c8a07d69cf53e055e0130a85b03.png)
- en: Never expected, always appreciated. By donating you allow me to allocate more
    time and resources towards more frequent and higher quality articles. [Link](https://www.buymeacoffee.com/danielwarfield)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 从未预料到，总是受到赞赏。通过捐赠，你可以让我分配更多的时间和资源来制作更频繁、更高质量的文章。 [链接](https://www.buymeacoffee.com/danielwarfield)
- en: '**Attribution:** All of the resources in this document were created by Daniel
    Warfield, unless a source is otherwise provided. You can use any resource in this
    post for your own non-commercial purposes, so long as you reference this article,
    [https://danielwarfield.dev](https://danielwarfield.dev/), or both. An explicit
    commercial license may be granted upon request.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**归属：** 除非另有来源提供，否则本文档中的所有资源均由 Daniel Warfield 创建。你可以将此帖子中的任何资源用于非商业目的，只要你引用本文或
    [https://danielwarfield.dev](https://danielwarfield.dev/)，或者两者均引用。可根据请求提供明确的商业许可。'
