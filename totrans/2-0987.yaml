- en: 'GLIP: Introducing Language-Image Pre-Training to Object Detection'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GLIP：将语言-图像预训练引入物体检测
- en: 原文：[https://towardsdatascience.com/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa](https://towardsdatascience.com/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa](https://towardsdatascience.com/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa)
- en: '[🚀Sascha’s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[🚀Sascha’s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
- en: Grounded Language-Image Pre-training by L. H. Li et. al.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Grounded Language-Image Pre-training 由 L. H. Li 等人著
- en: '[](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)
    ·9 min read·Sep 1, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)
    ·阅读时间9分钟·2023年9月1日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Today we will dive into a paper that builds upon the great success of [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    in language-image pre-training and extends it to the task of object detection:
    GLIP — **G**rounded **L**anguage-**I**mage **P**re-training. We will cover the
    key concepts and findings of the paper and make them easy to understand by providing
    further context and adding annotations to images and experiment results. Let’s
    go!'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 今天我们将深入探讨一篇建立在**[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)**语言-图像预训练成功基础上的论文，并将其扩展到物体检测任务：GLIP——**G**rounded
    **L**anguage-**I**mage **P**re-training。我们将涵盖论文的关键概念和发现，通过提供更多背景信息和对图像及实验结果的注释，帮助大家更容易理解。让我们开始吧！
- en: '![](../Images/d7ef9f457ada5a99c0936ce420e24274.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7ef9f457ada5a99c0936ce420e24274.png)'
- en: Image created from [publication](https://arxiv.org/abs/2112.03857) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [publication](https://arxiv.org/abs/2112.03857) 由 [Sascha Kirch](https://medium.com/@SaschaKirch)
    创作
- en: '**Paper:** [Grounded Language-Image Pre-training](https://arxiv.org/abs/2112.03857)
    by'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**论文：** [Grounded Language-Image Pre-training](https://arxiv.org/abs/2112.03857)
    由'
- en: Liunian Harold Li et. al., 7\. Dec. 2021
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Liunian Harold Li 等，2021年12月7日
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/microsoft/GLIP)'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**资源：** [GitHub](https://github.com/microsoft/GLIP)'
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** representation learning, object detection, phrase-grounding,
    foundation models'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**类别：** 表示学习、物体检测、短语定位、基础模型'
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[**其他讲解**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**：**'
- en: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    — [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    — [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    — [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    — [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    — [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    — [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
- en: Outline
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大纲
- en: Context & Background
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 背景与上下文
- en: Claimed Contributions
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 声明的贡献
- en: Method
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 方法
- en: Experiments
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实验
- en: Further Readings & Resources
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进一步阅读与资源
- en: Context & Background
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景与上下文
- en: GLIP **(G**rounded **L**anguage-**I**mage **P**re-training) is a multi-modal
    language-image model. Similar to [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    (**C**ontrastive **L**anguage-**I**mage **P**re-Training), it performs contrastive
    pre-training to learn semantically rich representations and aligns them across
    its modalities. While [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    learns these representation on an image level, which means one sentence describes
    the entire image, GLIP aims to extend this approach to object-level representations,
    meaning one sentence might correspond to multiple objects within the image. The
    task of identifying correspondences between single tokens in a text-prompt and
    objects or regions in an image is called *phrase grounding*. Hence the word “Grounded”
    in GLIP.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: GLIP **（G**rounded **L**anguage-**I**mage **P**re-training）是一个多模态语言-图像模型。类似于
    [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    (**C**ontrastive **L**anguage-**I**mage **P**re-Training)，它通过对比预训练学习语义丰富的表示，并在其模态间进行对齐。虽然
    [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    在图像级别上学习这些表示，这意味着一句话描述整张图像，GLIP 旨在将这种方法扩展到对象级别的表示，即一句话可能对应图像中的多个对象。识别文本提示中的单个令牌与图像中对象或区域的对应关系的任务称为
    *短语基础*。因此，GLIP 中的“Grounded”一词。
- en: 'Therefore, GLIP aims to:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，GLIP 的目标是：
- en: Unify phrase grounding and object detection for large-scale pre-training.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 统一短语基础和对象检测，用于大规模预训练。
- en: Provide a flexible framework for zero-shot object detection, where flexible
    means it is not restricted to a fixed set of classes.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供一个灵活的框架进行零-shot 对象检测，其中“灵活”意味着不局限于固定的类别集合。
- en: Build one pre-trained model that seamlessly transfers to various tasks and domains,
    in a zero-shot or few-shot manner.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个预训练模型，可以无缝迁移到各种任务和领域，以零-shot 或少-shot 的方式。
- en: '*What can you do with such a model?* You could use text prompts to find objects
    or regions of interest within a given input image. And the best part: you are
    not restricted to pre-defined classes.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以用这样的模型做什么？* 你可以使用文本提示在给定的输入图像中找到对象或感兴趣的区域。最棒的是：你不受限于预定义的类别。'
- en: '![](../Images/af97d1d27d14bad39edef24dd33dbeca.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af97d1d27d14bad39edef24dd33dbeca.png)'
- en: 'Fig. 1: Output of GLIP for different images and prompt formats. [Image source](https://arxiv.org/abs/2112.03857)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：GLIP 对不同图像和提示格式的输出。 [图片来源](https://arxiv.org/abs/2112.03857) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    的注释
- en: You could further process these detections (e.g. feeding those into a tracking
    system) or create a custom dataset with certain classes of interest and use those
    to train your own supervised detection system. Not only that you could cover rare
    or very specific classes, but you could also save a lot of time and money for
    the creation of manual labels. As we will see later, the authors of GLIP had a
    similar idea to boost the performance even further by introducing a teacher-student
    framework.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: GLIP has been adopted by many other projects and domains in deep learning. For
    example, [GLIGEN (Grounded-Language-to-Image-Generation)](https://arxiv.org/abs/2301.07093)
    uses GLIP as to condition the image generation of a latent diffusion model to
    increase the controllability. Additionally, GLIP has been combined with other
    models such as [DINO (**D**ETR with **I**mproved de**N**oising anch**O**r
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: boxes](https://arxiv.org/abs/2203.03605)) and [SAM (Segment Anything Model)](https://arxiv.org/abs/2304.02643)
    to [GroundingDINO](https://arxiv.org/abs/2303.05499) and [Grounded-Segment-Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything)
    respectively. [GLIPv2](https://arxiv.org/abs/2206.05836) extends the initial GLIP
    model with vision-language understanding to not only improve phrase grounding
    but also enable visual question answering tasks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: '[Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Paper Walkthroughs by Sascha Kirch
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2?source=post_page-----5ddb601873aa--------------------------------)7
    stories![“DDPM — Denoising Diffusion Probabilistic Models “ paper illustration
    by Sascha Kirch](../Images/6e785c0a911386676abebe0fa646f483.png)![“Depth Anything”
    paper illustration by Sascha Kirch](../Images/bd8cd71a02e42cf64d0afd39f41f48e0.png)![](../Images/8708d91a4a1902cef889ced95d46fc39.png)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Claimed Contributions
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large scale pre-training for combined phrase grounding and object detection
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Providing a unified view on object detection and phrase grounding
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deep cross-modality fusion to learn high-quality language-aware visual representations
    and to achieve superior transfer learning performance.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Presenting that prompt-tuning is more effective in deep vision-language fusion
    (e.g. GLIP) as in shallow fused networks (e.g. [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500))
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Method
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having a rough idea of what can be done with GLIP, let’s have a closer look
    into the details of the paper.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Architectural Overview
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On a high level, GLIP’s architecture is quite similar to [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)’s
    in a sense that it also consists of a text encoder, an image encoder and some
    sort of contrastive learning on the similarity of text and image features. The
    architecture of GLIP is shown in Fig. 2.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，GLIP 的架构在某种程度上与[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)的架构非常相似，因为它也包括一个文本编码器、一个图像编码器以及某种对文本和图像特征相似性的对比学习。GLIP
    的架构如图 2 所示。
- en: '![](../Images/03fe6d986334bdeeb0e40e962cc55592.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03fe6d986334bdeeb0e40e962cc55592.png)'
- en: 'Fig. 2: Framework architecture. [Image source](https://arxiv.org/abs/2112.03857)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：框架架构。 [图片来源](https://arxiv.org/abs/2112.03857) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    注释
- en: GLIP adds a language-image aware deep fusion module after the text and image
    encoder. This module performs cross-modal attention and extracts further features.
    A cosine similarity is calculated over the resulting region features and word
    features. During training, the similarity of matching pairs is maximized, while
    minimized for incorrect pairs. In contrast to [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500),
    where the matching pairs are located on the diagonal of the similarity matrix,
    in GLIP the matching is not performed on sentence level, but on (sub)word level
    resulting in usually off-diagonal positions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: GLIP 在文本和图像编码器之后添加了一个语言-图像感知的深度融合模块。该模块执行跨模态注意力并提取进一步的特征。对生成的区域特征和单词特征计算余弦相似性。在训练过程中，匹配对的相似性被最大化，而不匹配对的相似性被最小化。与[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)中匹配对位于相似性矩阵对角线上的情况不同，在
    GLIP 中，匹配不是在句子级别进行的，而是在（子）词级别进行的，通常导致非对角线位置。
- en: Phrase Grounding Formulated as Object Detection Problem
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 短语定位表述为对象检测问题
- en: 'The authors noted that the problem of phrase grounding (= associating words
    with objects/regions in an image) can be formulated as Object detection Objective,
    where the standard loss objective is:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作者指出，短语定位（即将单词与图像中的对象/区域关联起来）的问题可以表述为对象检测目标，其中标准的损失目标是：
- en: '![](../Images/7f7962cd88754b158fc687760fa7be2c.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f7962cd88754b158fc687760fa7be2c.png)'
- en: The localization loss is concerned with the quality of the predicted bounding
    box, which depending on the format, might be the size and location of the box.
    The classification loss is the key part in the unification. By calculating the
    logits over the similarity score of text-image features instead of over the logits
    from an image classifier, the same loss objective can be used for training.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 定位损失关注的是预测边界框的质量，这取决于格式，可能是框的大小和位置。分类损失是在统一过程中关键的一部分。通过计算文本-图像特征的相似性得分的 logits，而不是图像分类器的
    logits，可以使用相同的损失目标进行训练。
- en: '![](../Images/f659414dd9a3a8b27fe30f950211e69b.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f659414dd9a3a8b27fe30f950211e69b.png)'
- en: Different Model Variants
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同的模型变体
- en: 'Five different models are trained to show the effect of the authors’ design
    choices and model scale:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 训练了五种不同的模型，以展示作者设计选择和模型规模的效果：
- en: '![](../Images/e1c6bb6395dde8d9cf1d0c547a63f76f.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1c6bb6395dde8d9cf1d0c547a63f76f.png)'
- en: 'Fig. 3: Model variants. [Image source](https://arxiv.org/abs/2112.03857) +
    annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：模型变体。 [图片来源](https://arxiv.org/abs/2112.03857) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    注释
- en: Teacher-Student Pre-Training
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 教师-学生预训练
- en: To boost the performance of GLIP, the authors train the GLIP-T (C) model (see
    Fig.3) on human annotated data, called GoldG, to generate grounding data from
    text-image pairs from the internet. They call this model the teacher model and
    subsequently train a student model feeding it the with the data used to train
    the teacher plus the data the teacher generated. See Fig. 4 for an illustration.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提升 GLIP 的性能，作者在人工标注的数据 GoldG 上训练了 GLIP-T (C) 模型（见图 3），以从互联网的文本-图像对中生成定位数据。他们称这个模型为教师模型，并随后训练一个学生模型，将教师模型使用的数据加上教师生成的数据输入给学生模型。见图
    4 了解更多说明。
- en: 'Note: Even though the terms teacher and student are used, it is not the same
    process as in knowledge distillation, where a smaller student model is trained
    to match the output of a larger teacher model.'
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：尽管使用了教师和学生的术语，但这与知识蒸馏中的过程不同，知识蒸馏中一个较小的学生模型被训练以匹配较大教师模型的输出。
- en: '![](../Images/07473682c441e2c3a26e1397649cc19e.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07473682c441e2c3a26e1397649cc19e.png)'
- en: Fig. 4\. Teacher-Student Pre-Training. Image by [Sascha Kirch](https://medium.com/@SaschaKirch)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：教师-学生预训练。图片由[Sascha Kirch](https://medium.com/@SaschaKirch) 提供
- en: Interestingly, as we will see in the experiments, the student surpasses the
    teacher on many (but not all) datasets for both; zero-shot and few-shot detection.
    *Why is that?* The paper hypothesizes, that even though the teacher provides a
    prediction with low confidence (they call it an “educated guess”), it becomes
    the ground truth (they call it “supervised signal”) in the generated dataset consumed
    by the student.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，正如我们在实验中看到的，学生在许多（但不是所有）数据集上超越了教师，无论是零样本检测还是少样本检测。*为什么会这样？* 论文假设，即使教师提供了低置信度的预测（他们称之为“有根据的猜测”），它仍然会成为学生所使用的生成数据集中的真实值（他们称之为“监督信号”）。
- en: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----5ddb601873aa--------------------------------)
    [## Get an email whenever Sascha Kirch publishes 🚀'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----5ddb601873aa--------------------------------)
    [## 每当Sascha Kirch发布新内容时获取电子邮件 🚀'
- en: Get an email whenever Sascha Kirch publishes 🚀 Looking to learn more about deep
    learning or simply stay up to date…
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每当Sascha Kirch发布新内容时获取电子邮件 🚀 如果你想了解更多关于深度学习的内容或只是想保持更新……
- en: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----5ddb601873aa--------------------------------)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----5ddb601873aa--------------------------------)
- en: Experiments
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验
- en: 'The GLIP paper presents various experiments and ablation studies, mainly concerned
    with:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: GLIP论文展示了各种实验和消融研究，主要关注于：
- en: Zero-Shot Domain Transfer
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 零样本领域转移
- en: Data Efficiency
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据效率
- en: Prompt Engineering
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提示工程
- en: I have some doubts for some of the results and the way they are presented, and
    I will point them out in the annotations. I don’t want to diminish the achievements
    of GLIP and rather view it with a critical eye.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我对一些结果和它们的呈现方式有一些疑问，我会在注释中指出这些问题。我不想贬低GLIP的成就，而是持批判的眼光来看待它。
- en: Now let’s jump into the details!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入细节！
- en: Zero-Shot Domain Transfer
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零样本领域转移
- en: First, we will have a look into the results from the zero-shot domain transfer.
    In this task the objective is to analyze how well the pre-trained GLIP models
    perform on a different dataset (i.e. COCO and LVIS) as used during pre-training
    and compare it against a baseline with models that have been trained in a supervised
    fashion. Then, the pre-trained GLIP is further fine-tuned and evaluated on the
    dataset under test.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看零样本领域转移的结果。在这个任务中，目标是分析预训练的GLIP模型在不同数据集（即COCO和LVIS）上的表现，与那些经过监督训练的模型的基线进行比较。然后，对预训练的GLIP进行进一步的微调，并在测试数据集上进行评估。
- en: In Fig.5 we see the results from the zero-shot domain transfer on [COCO](https://cocodataset.org/#home).
    We see that all GLIP models have a better 0-shot performance as a supervised Faster
    RCNN. We are also presented with the result, that GLIP-L outperforms the previous
    SOTA (at the time of the paper’s release). We see that the larger student GLIP-L
    outperforms the teacher model GLIP-T (C).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在图5中，我们看到COCO上的零样本领域转移结果。我们看到所有GLIP模型在零样本性能上优于监督的Faster RCNN。我们还看到GLIP-L超越了之前的SOTA（在论文发布时）。我们看到较大的学生模型GLIP-L超越了教师模型GLIP-T（C）。
- en: '![](../Images/6f347562bca6718e7a4d2ba0a643c1cd.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f347562bca6718e7a4d2ba0a643c1cd.png)'
- en: 'Fig. 5: Zero-shot domain transfer and fine-tuning on COCO. [Image source](https://arxiv.org/abs/2112.03857)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：COCO上的零样本领域转移和微调。[图片来源](https://arxiv.org/abs/2112.03857) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    的注释
- en: Following I list my doubts when reading these results and the claims made in
    the paper, where it is said that GLIP-L surpasses the best supervised model SoftTeacher.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我列出了在阅读这些结果和论文中提出的声明时的疑虑，论文中提到GLIP-L超越了最佳监督模型SoftTeacher。
- en: The model that has better metrics than SoftTeacher is GLIP-L, which is better
    by 0.2 points. This small margin might not be the result of the new method of
    GLIP but might be due to some differences in training hyperparameters.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 具有比SoftTeacher更好指标的模型是GLIP-L，领先0.2分。这一小的差距可能不是GLIP新方法的结果，而可能是由于训练超参数的一些差异。
- en: GLIP-L does not even use the data (Cap4M or Cap24M) generated from teacher model
    which they presented as a good solution.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GLIP-L甚至没有使用从教师模型生成的数据（Cap4M或Cap24M），他们将其展示为一个好的解决方案。
- en: GLIP-L has been trained on a much larger corpus of training data as SoftTeacher.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GLIP-L在比SoftTeacher更大的训练数据集上进行训练。
- en: In my opinion the results comparing the different GLIP models and the DyHead-T
    they trained themselves are completely fine, I just have my doubts in general
    when different methods and models are compared under unclear or different constraints.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为不同 GLIP 模型和他们自己训练的 DyHead-T 的结果比较是完全可以的，只是在不同方法和模型在不明确或不同约束下比较时，我总是有一些疑虑。
- en: In Fig.6, we see the zero-shot domain transfer performance on [LVIS](https://www.lvisdataset.org/)
    dataset. We can see that the largest GLIP model, GLIP-L, outperforms all other
    presented supervised models.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在图6中，我们可以看到 [LVIS](https://www.lvisdataset.org/) 数据集上的零样本领域迁移性能。我们可以看到，最大的 GLIP
    模型 GLIP-L 超越了所有其他呈现的监督模型。
- en: '![](../Images/26ec45482630c9a95d494b079b0500ce.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26ec45482630c9a95d494b079b0500ce.png)'
- en: 'Fig. 6: Zero-shot domain transfer to LVIS. [Image source](https://arxiv.org/abs/2112.03857)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：零样本领域迁移到LVIS。 [图片来源](https://arxiv.org/abs/2112.03857) + 注释由 [Sascha Kirch](https://medium.com/@SaschaKirch)
    提供
- en: Finally, GLIP has been compared on its phrase grounding performance on the Flickr30K
    entities against MDETR (see Fig.7). Both student models, GLIP-T and GLIP-L, surpass
    the MDETR baselines.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，GLIP 在Flickr30K 实体上的短语定位性能与 MDETR 进行了比较（见图7）。两种学生模型，GLIP-T 和 GLIP-L，都超越了
    MDETR 基线。
- en: '![](../Images/ce1a12825e5e44ed1f6bf5b2026ead2d.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce1a12825e5e44ed1f6bf5b2026ead2d.png)'
- en: 'Fig. 7: Phrase grounding performance on Flickr30K entities. [Image source](https://arxiv.org/abs/2112.03857)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：Flickr30K 实体上的短语定位性能。 [图片来源](https://arxiv.org/abs/2112.03857) + 注释由 [Sascha
    Kirch](https://medium.com/@SaschaKirch) 提供
- en: Data Efficiency
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据效率
- en: Another experiment is concerned with the data efficiency. This experiment aims
    to show how the performance (in terms of average precision) changes when fine-tuning
    a pre-trained model on a certain number of task specific data. In Fig.8, the models
    are evaluated on 13 different datasets and their performance is reported as average
    precision averaged over the 13 datasets. Results are reported for 0-shot, 1-shot,
    3-shot, 5-shot, 10-shot and “all”-shot (I doubt that’s an official term for complete
    fine-tuning, but I guess you get the point 😅).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个实验关注数据效率。这个实验旨在展示当在一定数量的任务特定数据上微调一个预训练模型时，性能（以平均精度衡量）如何变化。在图8中，这些模型在13个不同的数据集上进行了评估，其性能作为在13个数据集上的平均精度进行报告。结果包括0-shot、1-shot、3-shot、5-shot、10-shot和“all”-shot（我怀疑“all”-shot是否是完整微调的官方术语，但我想你明白我的意思
    😅）。
- en: '![](../Images/a402f0df992ed3145a417aaee59c5a7b.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a402f0df992ed3145a417aaee59c5a7b.png)'
- en: 'Fig. 8: Data Efficiency. [Image source](https://arxiv.org/abs/2112.03857) +
    annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：数据效率。 [图片来源](https://arxiv.org/abs/2112.03857) + 注释由 [Sascha Kirch](https://medium.com/@SaschaKirch)
    提供
- en: Prompt Engineering
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示工程
- en: 'Similar as in [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500),
    the authors also report a correlation of the model’s performance and the formulation
    of the input text prompt. They propose two techniques to improve the performance
    of a pre-trained model, without the need to retrain the model’s weights:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与 [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    类似，作者还报告了模型性能与输入文本提示的表述之间的相关性。他们提出了两种技术来提高预训练模型的性能，无需重新训练模型的权重：
- en: Manual prompt tuning
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 手动提示调整
- en: Prompt Tuning
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提示调整
- en: 'The idea of manual prompt tuning is to provide further context in form of additional
    descriptive words, see Fig. 9:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 手动提示调整的想法是提供额外的描述性词语来提供更多的上下文，见图9：
- en: '![](../Images/7b797264c781c99cadefc232338c9843.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b797264c781c99cadefc232338c9843.png)'
- en: 'Fig. 9: Manual prompt tning example. [Image source](https://arxiv.org/abs/2112.03857)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：手动提示调整示例。 [图片来源](https://arxiv.org/abs/2112.03857) + 注释由 [Sascha Kirch](https://medium.com/@SaschaKirch)
    提供
- en: Manual prompt tuning can always be used to improve the performance, meaning
    it does not matter if the model is fully fine-tuned or if the model is used in
    a zero-shot or few-shot scenario.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 手动提示调整始终可以用来提高性能，这意味着无论模型是完全微调还是在零样本或少样本场景中使用，效果都不会受到影响。
- en: The second approach, prompt tuning, requires access to ground truth labels of
    a downstream task and is especially suitable for scenarios, where each detection
    task has a single prompt (e.g. “Detect car”). In that scenario, this prompt would
    first be translated into a feature embedding using the text encoder. Then, the
    image encoder and the deep fusion module are frozen and only the input embedding
    is optimized using the ground truth labels. The optimized embeddings would then
    serve as input to the model and the text encoder could be removed.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法，提示调整，需要访问下游任务的真实标签，特别适用于每个检测任务只有一个提示的场景（例如：“检测汽车”）。在这种情况下，这个提示会首先通过文本编码器转换成特征嵌入。然后，图像编码器和深度融合模块被冻结，仅使用真实标签优化输入嵌入。优化后的嵌入将作为模型输入，文本编码器可以被移除。
- en: Fig.10 shows the result of this prompt tuning for various GLIP models. When
    applied to models that have a deep fusion module, prompt tuning achieves almost
    the same performance as fine-tuning the model’s weights.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10 显示了对各种 GLIP 模型进行此提示调整的结果。当应用于具有深度融合模块的模型时，提示调整几乎与微调模型权重的效果相同。
- en: '![](../Images/a73b1b8b25722d15c6911a39848c351f.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a73b1b8b25722d15c6911a39848c351f.png)'
- en: 'Fig. 10: Effectiveness of prompt tuning. [Image source](https://arxiv.org/abs/2112.03857)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：提示调整的有效性。[图片来源](https://arxiv.org/abs/2112.03857) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    的注释
- en: Further Readings & Resources
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读和资源
- en: As mentioned at the beginning of this article, GLIP has been widely adopted
    by a vast number of projects.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如本文开头所述，GLIP 已被广泛应用于大量项目中。
- en: 'Following a list of papers that built upon GLIP:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些建立在 GLIP 基础上的论文：
- en: '[GLIPv2: Unifying Localization and Vision-Language Understanding](https://arxiv.org/abs/2206.05836)'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[GLIPv2：统一定位和视觉语言理解](https://arxiv.org/abs/2206.05836)'
- en: '[GLIGEN: Open-Set Grounded Text-to-Image Generation](https://arxiv.org/abs/2301.07093)'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[GLIGEN：开放集基础文本到图像生成](https://arxiv.org/abs/2301.07093)'
- en: '[Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object
    Detection](https://arxiv.org/abs/2303.05499)'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Grounding DINO：将 DINO 与 Grounded 预训练结合以进行开放集物体检测](https://arxiv.org/abs/2303.05499)'
- en: 'Here is a list of repositories if you want to dive into the implementation
    of GLIP and other interesting projects that built upon GLIP:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入了解 GLIP 的实现以及其他建立在 GLIP 基础上的有趣项目，这里有一个仓库列表：
- en: '[Official implementation of GLIP](https://github.com/microsoft/GLIP)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GLIP 官方实现](https://github.com/microsoft/GLIP)'
- en: '[Python Notebook to play around with GLIP](https://colab.research.google.com/drive/12x7v-_miN7-SRiziK3Cx4ffJzstBJNqb?usp=sharing)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用于玩弄 GLIP 的 Python Notebook](https://colab.research.google.com/drive/12x7v-_miN7-SRiziK3Cx4ffJzstBJNqb?usp=sharing)'
- en: '[GroundingDINO: combining GLIP and DINO](https://github.com/IDEA-Research/GroundingDINO)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GroundingDINO：结合 GLIP 和 DINO](https://github.com/IDEA-Research/GroundingDINO)'
- en: '[Grounded-Segment-Anything: combining GroundingDINO and SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Grounded-Segment-Anything：结合 GroundingDINO 和 SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything)'
- en: 'Here is one of my articles about the [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    foundation model, following the same approach of summary as this article:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我关于[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)基础模型的文章之一，采用与本文相同的总结方法：
- en: '[](/the-clip-foundation-model-7770858b487d?source=post_page-----5ddb601873aa--------------------------------)
    [## The CLIP Foundation Model'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/the-clip-foundation-model-7770858b487d?source=post_page-----5ddb601873aa--------------------------------)
    [## CLIP 基础模型'
- en: Paper Summary— Learning Transferable Visual Models From Natural Language Supervision
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 论文总结——从自然语言监督中学习可转移的视觉模型
- en: towardsdatascience.com](/the-clip-foundation-model-7770858b487d?source=post_page-----5ddb601873aa--------------------------------)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/the-clip-foundation-model-7770858b487d?source=post_page-----5ddb601873aa--------------------------------)
