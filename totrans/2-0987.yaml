- en: 'GLIP: Introducing Language-Image Pre-Training to Object Detection'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GLIPï¼šå°†è¯­è¨€-å›¾åƒé¢„è®­ç»ƒå¼•å…¥ç‰©ä½“æ£€æµ‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa](https://towardsdatascience.com/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa](https://towardsdatascience.com/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa)
- en: '[ğŸš€Saschaâ€™s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[ğŸš€Saschaâ€™s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
- en: Grounded Language-Image Pre-training by L. H. Li et. al.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Grounded Language-Image Pre-training ç”± L. H. Li ç­‰äººè‘—
- en: '[](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)
    Â·9 min readÂ·Sep 1, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)
    Â·é˜…è¯»æ—¶é—´9åˆ†é’ŸÂ·2023å¹´9æœˆ1æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Today we will dive into a paper that builds upon the great success of [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    in language-image pre-training and extends it to the task of object detection:
    GLIP â€” **G**rounded **L**anguage-**I**mage **P**re-training. We will cover the
    key concepts and findings of the paper and make them easy to understand by providing
    further context and adding annotations to images and experiment results. Letâ€™s
    go!'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Šå¤©æˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ä¸€ç¯‡å»ºç«‹åœ¨**[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)**è¯­è¨€-å›¾åƒé¢„è®­ç»ƒæˆåŠŸåŸºç¡€ä¸Šçš„è®ºæ–‡ï¼Œå¹¶å°†å…¶æ‰©å±•åˆ°ç‰©ä½“æ£€æµ‹ä»»åŠ¡ï¼šGLIPâ€”â€”**G**rounded
    **L**anguage-**I**mage **P**re-trainingã€‚æˆ‘ä»¬å°†æ¶µç›–è®ºæ–‡çš„å…³é”®æ¦‚å¿µå’Œå‘ç°ï¼Œé€šè¿‡æä¾›æ›´å¤šèƒŒæ™¯ä¿¡æ¯å’Œå¯¹å›¾åƒåŠå®éªŒç»“æœçš„æ³¨é‡Šï¼Œå¸®åŠ©å¤§å®¶æ›´å®¹æ˜“ç†è§£ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼
- en: '![](../Images/d7ef9f457ada5a99c0936ce420e24274.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7ef9f457ada5a99c0936ce420e24274.png)'
- en: Image created from [publication](https://arxiv.org/abs/2112.03857) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºäº [publication](https://arxiv.org/abs/2112.03857) ç”± [Sascha Kirch](https://medium.com/@SaschaKirch)
    åˆ›ä½œ
- en: '**Paper:** [Grounded Language-Image Pre-training](https://arxiv.org/abs/2112.03857)
    by'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**è®ºæ–‡ï¼š** [Grounded Language-Image Pre-training](https://arxiv.org/abs/2112.03857)
    ç”±'
- en: Liunian Harold Li et. al., 7\. Dec. 2021
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Liunian Harold Li ç­‰ï¼Œ2021å¹´12æœˆ7æ—¥
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/microsoft/GLIP)'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**èµ„æºï¼š** [GitHub](https://github.com/microsoft/GLIP)'
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** representation learning, object detection, phrase-grounding,
    foundation models'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ç±»åˆ«ï¼š** è¡¨ç¤ºå­¦ä¹ ã€ç‰©ä½“æ£€æµ‹ã€çŸ­è¯­å®šä½ã€åŸºç¡€æ¨¡å‹'
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[**å…¶ä»–è®²è§£**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**ï¼š**'
- en: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    â€” [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    â€” [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    â€” [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    â€” [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    â€” [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    â€” [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
- en: Outline
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤§çº²
- en: Context & Background
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: èƒŒæ™¯ä¸ä¸Šä¸‹æ–‡
- en: Claimed Contributions
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å£°æ˜çš„è´¡çŒ®
- en: Method
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ–¹æ³•
- en: Experiments
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®éªŒ
- en: Further Readings & Resources
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»ä¸èµ„æº
- en: Context & Background
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: èƒŒæ™¯ä¸ä¸Šä¸‹æ–‡
- en: GLIP **(G**rounded **L**anguage-**I**mage **P**re-training) is a multi-modal
    language-image model. Similar to [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    (**C**ontrastive **L**anguage-**I**mage **P**re-Training), it performs contrastive
    pre-training to learn semantically rich representations and aligns them across
    its modalities. While [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    learns these representation on an image level, which means one sentence describes
    the entire image, GLIP aims to extend this approach to object-level representations,
    meaning one sentence might correspond to multiple objects within the image. The
    task of identifying correspondences between single tokens in a text-prompt and
    objects or regions in an image is called *phrase grounding*. Hence the word â€œGroundedâ€
    in GLIP.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: GLIP **ï¼ˆG**rounded **L**anguage-**I**mage **P**re-trainingï¼‰æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€è¯­è¨€-å›¾åƒæ¨¡å‹ã€‚ç±»ä¼¼äº
    [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    (**C**ontrastive **L**anguage-**I**mage **P**re-Training)ï¼Œå®ƒé€šè¿‡å¯¹æ¯”é¢„è®­ç»ƒå­¦ä¹ è¯­ä¹‰ä¸°å¯Œçš„è¡¨ç¤ºï¼Œå¹¶åœ¨å…¶æ¨¡æ€é—´è¿›è¡Œå¯¹é½ã€‚è™½ç„¶
    [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    åœ¨å›¾åƒçº§åˆ«ä¸Šå­¦ä¹ è¿™äº›è¡¨ç¤ºï¼Œè¿™æ„å‘³ç€ä¸€å¥è¯æè¿°æ•´å¼ å›¾åƒï¼ŒGLIP æ—¨åœ¨å°†è¿™ç§æ–¹æ³•æ‰©å±•åˆ°å¯¹è±¡çº§åˆ«çš„è¡¨ç¤ºï¼Œå³ä¸€å¥è¯å¯èƒ½å¯¹åº”å›¾åƒä¸­çš„å¤šä¸ªå¯¹è±¡ã€‚è¯†åˆ«æ–‡æœ¬æç¤ºä¸­çš„å•ä¸ªä»¤ç‰Œä¸å›¾åƒä¸­å¯¹è±¡æˆ–åŒºåŸŸçš„å¯¹åº”å…³ç³»çš„ä»»åŠ¡ç§°ä¸º
    *çŸ­è¯­åŸºç¡€*ã€‚å› æ­¤ï¼ŒGLIP ä¸­çš„â€œGroundedâ€ä¸€è¯ã€‚
- en: 'Therefore, GLIP aims to:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼ŒGLIP çš„ç›®æ ‡æ˜¯ï¼š
- en: Unify phrase grounding and object detection for large-scale pre-training.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç»Ÿä¸€çŸ­è¯­åŸºç¡€å’Œå¯¹è±¡æ£€æµ‹ï¼Œç”¨äºå¤§è§„æ¨¡é¢„è®­ç»ƒã€‚
- en: Provide a flexible framework for zero-shot object detection, where flexible
    means it is not restricted to a fixed set of classes.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æä¾›ä¸€ä¸ªçµæ´»çš„æ¡†æ¶è¿›è¡Œé›¶-shot å¯¹è±¡æ£€æµ‹ï¼Œå…¶ä¸­â€œçµæ´»â€æ„å‘³ç€ä¸å±€é™äºå›ºå®šçš„ç±»åˆ«é›†åˆã€‚
- en: Build one pre-trained model that seamlessly transfers to various tasks and domains,
    in a zero-shot or few-shot manner.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯ä»¥æ— ç¼è¿ç§»åˆ°å„ç§ä»»åŠ¡å’Œé¢†åŸŸï¼Œä»¥é›¶-shot æˆ–å°‘-shot çš„æ–¹å¼ã€‚
- en: '*What can you do with such a model?* You could use text prompts to find objects
    or regions of interest within a given input image. And the best part: you are
    not restricted to pre-defined classes.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä½ å¯ä»¥ç”¨è¿™æ ·çš„æ¨¡å‹åšä»€ä¹ˆï¼Ÿ* ä½ å¯ä»¥ä½¿ç”¨æ–‡æœ¬æç¤ºåœ¨ç»™å®šçš„è¾“å…¥å›¾åƒä¸­æ‰¾åˆ°å¯¹è±¡æˆ–æ„Ÿå…´è¶£çš„åŒºåŸŸã€‚æœ€æ£’çš„æ˜¯ï¼šä½ ä¸å—é™äºé¢„å®šä¹‰çš„ç±»åˆ«ã€‚'
- en: '![](../Images/af97d1d27d14bad39edef24dd33dbeca.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af97d1d27d14bad39edef24dd33dbeca.png)'
- en: 'Fig. 1: Output of GLIP for different images and prompt formats. [Image source](https://arxiv.org/abs/2112.03857)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šGLIP å¯¹ä¸åŒå›¾åƒå’Œæç¤ºæ ¼å¼çš„è¾“å‡ºã€‚ [å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2112.03857) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    çš„æ³¨é‡Š
- en: You could further process these detections (e.g. feeding those into a tracking
    system) or create a custom dataset with certain classes of interest and use those
    to train your own supervised detection system. Not only that you could cover rare
    or very specific classes, but you could also save a lot of time and money for
    the creation of manual labels. As we will see later, the authors of GLIP had a
    similar idea to boost the performance even further by introducing a teacher-student
    framework.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: GLIP has been adopted by many other projects and domains in deep learning. For
    example, [GLIGEN (Grounded-Language-to-Image-Generation)](https://arxiv.org/abs/2301.07093)
    uses GLIP as to condition the image generation of a latent diffusion model to
    increase the controllability. Additionally, GLIP has been combined with other
    models such as [DINO (**D**ETR with **I**mproved de**N**oising anch**O**r
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: boxes](https://arxiv.org/abs/2203.03605)) and [SAM (Segment Anything Model)](https://arxiv.org/abs/2304.02643)
    to [GroundingDINO](https://arxiv.org/abs/2303.05499) and [Grounded-Segment-Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything)
    respectively. [GLIPv2](https://arxiv.org/abs/2206.05836) extends the initial GLIP
    model with vision-language understanding to not only improve phrase grounding
    but also enable visual question answering tasks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: '[Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Paper Walkthroughs by Sascha Kirch
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2?source=post_page-----5ddb601873aa--------------------------------)7
    stories![â€œDDPMâ€Šâ€”â€ŠDenoising Diffusion Probabilistic Models â€œ paper illustration
    by Sascha Kirch](../Images/6e785c0a911386676abebe0fa646f483.png)![â€œDepth Anythingâ€
    paper illustration by Sascha Kirch](../Images/bd8cd71a02e42cf64d0afd39f41f48e0.png)![](../Images/8708d91a4a1902cef889ced95d46fc39.png)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Claimed Contributions
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large scale pre-training for combined phrase grounding and object detection
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Providing a unified view on object detection and phrase grounding
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deep cross-modality fusion to learn high-quality language-aware visual representations
    and to achieve superior transfer learning performance.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Presenting that prompt-tuning is more effective in deep vision-language fusion
    (e.g. GLIP) as in shallow fused networks (e.g. [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500))
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Method
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having a rough idea of what can be done with GLIP, letâ€™s have a closer look
    into the details of the paper.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Architectural Overview
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On a high level, GLIPâ€™s architecture is quite similar to [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)â€™s
    in a sense that it also consists of a text encoder, an image encoder and some
    sort of contrastive learning on the similarity of text and image features. The
    architecture of GLIP is shown in Fig. 2.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é«˜å±‚æ¬¡æ¥çœ‹ï¼ŒGLIP çš„æ¶æ„åœ¨æŸç§ç¨‹åº¦ä¸Šä¸[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)çš„æ¶æ„éå¸¸ç›¸ä¼¼ï¼Œå› ä¸ºå®ƒä¹ŸåŒ…æ‹¬ä¸€ä¸ªæ–‡æœ¬ç¼–ç å™¨ã€ä¸€ä¸ªå›¾åƒç¼–ç å™¨ä»¥åŠæŸç§å¯¹æ–‡æœ¬å’Œå›¾åƒç‰¹å¾ç›¸ä¼¼æ€§çš„å¯¹æ¯”å­¦ä¹ ã€‚GLIP
    çš„æ¶æ„å¦‚å›¾ 2 æ‰€ç¤ºã€‚
- en: '![](../Images/03fe6d986334bdeeb0e40e962cc55592.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03fe6d986334bdeeb0e40e962cc55592.png)'
- en: 'Fig. 2: Framework architecture. [Image source](https://arxiv.org/abs/2112.03857)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2ï¼šæ¡†æ¶æ¶æ„ã€‚ [å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2112.03857) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    æ³¨é‡Š
- en: GLIP adds a language-image aware deep fusion module after the text and image
    encoder. This module performs cross-modal attention and extracts further features.
    A cosine similarity is calculated over the resulting region features and word
    features. During training, the similarity of matching pairs is maximized, while
    minimized for incorrect pairs. In contrast to [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500),
    where the matching pairs are located on the diagonal of the similarity matrix,
    in GLIP the matching is not performed on sentence level, but on (sub)word level
    resulting in usually off-diagonal positions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: GLIP åœ¨æ–‡æœ¬å’Œå›¾åƒç¼–ç å™¨ä¹‹åæ·»åŠ äº†ä¸€ä¸ªè¯­è¨€-å›¾åƒæ„ŸçŸ¥çš„æ·±åº¦èåˆæ¨¡å—ã€‚è¯¥æ¨¡å—æ‰§è¡Œè·¨æ¨¡æ€æ³¨æ„åŠ›å¹¶æå–è¿›ä¸€æ­¥çš„ç‰¹å¾ã€‚å¯¹ç”Ÿæˆçš„åŒºåŸŸç‰¹å¾å’Œå•è¯ç‰¹å¾è®¡ç®—ä½™å¼¦ç›¸ä¼¼æ€§ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒåŒ¹é…å¯¹çš„ç›¸ä¼¼æ€§è¢«æœ€å¤§åŒ–ï¼Œè€Œä¸åŒ¹é…å¯¹çš„ç›¸ä¼¼æ€§è¢«æœ€å°åŒ–ã€‚ä¸[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)ä¸­åŒ¹é…å¯¹ä½äºç›¸ä¼¼æ€§çŸ©é˜µå¯¹è§’çº¿ä¸Šçš„æƒ…å†µä¸åŒï¼Œåœ¨
    GLIP ä¸­ï¼ŒåŒ¹é…ä¸æ˜¯åœ¨å¥å­çº§åˆ«è¿›è¡Œçš„ï¼Œè€Œæ˜¯åœ¨ï¼ˆå­ï¼‰è¯çº§åˆ«è¿›è¡Œçš„ï¼Œé€šå¸¸å¯¼è‡´éå¯¹è§’çº¿ä½ç½®ã€‚
- en: Phrase Grounding Formulated as Object Detection Problem
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: çŸ­è¯­å®šä½è¡¨è¿°ä¸ºå¯¹è±¡æ£€æµ‹é—®é¢˜
- en: 'The authors noted that the problem of phrase grounding (= associating words
    with objects/regions in an image) can be formulated as Object detection Objective,
    where the standard loss objective is:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æŒ‡å‡ºï¼ŒçŸ­è¯­å®šä½ï¼ˆå³å°†å•è¯ä¸å›¾åƒä¸­çš„å¯¹è±¡/åŒºåŸŸå…³è”èµ·æ¥ï¼‰çš„é—®é¢˜å¯ä»¥è¡¨è¿°ä¸ºå¯¹è±¡æ£€æµ‹ç›®æ ‡ï¼Œå…¶ä¸­æ ‡å‡†çš„æŸå¤±ç›®æ ‡æ˜¯ï¼š
- en: '![](../Images/7f7962cd88754b158fc687760fa7be2c.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f7962cd88754b158fc687760fa7be2c.png)'
- en: The localization loss is concerned with the quality of the predicted bounding
    box, which depending on the format, might be the size and location of the box.
    The classification loss is the key part in the unification. By calculating the
    logits over the similarity score of text-image features instead of over the logits
    from an image classifier, the same loss objective can be used for training.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä½æŸå¤±å…³æ³¨çš„æ˜¯é¢„æµ‹è¾¹ç•Œæ¡†çš„è´¨é‡ï¼Œè¿™å–å†³äºæ ¼å¼ï¼Œå¯èƒ½æ˜¯æ¡†çš„å¤§å°å’Œä½ç½®ã€‚åˆ†ç±»æŸå¤±æ˜¯åœ¨ç»Ÿä¸€è¿‡ç¨‹ä¸­å…³é”®çš„ä¸€éƒ¨åˆ†ã€‚é€šè¿‡è®¡ç®—æ–‡æœ¬-å›¾åƒç‰¹å¾çš„ç›¸ä¼¼æ€§å¾—åˆ†çš„ logitsï¼Œè€Œä¸æ˜¯å›¾åƒåˆ†ç±»å™¨çš„
    logitsï¼Œå¯ä»¥ä½¿ç”¨ç›¸åŒçš„æŸå¤±ç›®æ ‡è¿›è¡Œè®­ç»ƒã€‚
- en: '![](../Images/f659414dd9a3a8b27fe30f950211e69b.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f659414dd9a3a8b27fe30f950211e69b.png)'
- en: Different Model Variants
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸åŒçš„æ¨¡å‹å˜ä½“
- en: 'Five different models are trained to show the effect of the authorsâ€™ design
    choices and model scale:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒäº†äº”ç§ä¸åŒçš„æ¨¡å‹ï¼Œä»¥å±•ç¤ºä½œè€…è®¾è®¡é€‰æ‹©å’Œæ¨¡å‹è§„æ¨¡çš„æ•ˆæœï¼š
- en: '![](../Images/e1c6bb6395dde8d9cf1d0c547a63f76f.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1c6bb6395dde8d9cf1d0c547a63f76f.png)'
- en: 'Fig. 3: Model variants. [Image source](https://arxiv.org/abs/2112.03857) +
    annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3ï¼šæ¨¡å‹å˜ä½“ã€‚ [å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2112.03857) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    æ³¨é‡Š
- en: Teacher-Student Pre-Training
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•™å¸ˆ-å­¦ç”Ÿé¢„è®­ç»ƒ
- en: To boost the performance of GLIP, the authors train the GLIP-T (C) model (see
    Fig.3) on human annotated data, called GoldG, to generate grounding data from
    text-image pairs from the internet. They call this model the teacher model and
    subsequently train a student model feeding it the with the data used to train
    the teacher plus the data the teacher generated. See Fig. 4 for an illustration.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æå‡ GLIP çš„æ€§èƒ½ï¼Œä½œè€…åœ¨äººå·¥æ ‡æ³¨çš„æ•°æ® GoldG ä¸Šè®­ç»ƒäº† GLIP-T (C) æ¨¡å‹ï¼ˆè§å›¾ 3ï¼‰ï¼Œä»¥ä»äº’è”ç½‘çš„æ–‡æœ¬-å›¾åƒå¯¹ä¸­ç”Ÿæˆå®šä½æ•°æ®ã€‚ä»–ä»¬ç§°è¿™ä¸ªæ¨¡å‹ä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œå¹¶éšåè®­ç»ƒä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹ï¼Œå°†æ•™å¸ˆæ¨¡å‹ä½¿ç”¨çš„æ•°æ®åŠ ä¸Šæ•™å¸ˆç”Ÿæˆçš„æ•°æ®è¾“å…¥ç»™å­¦ç”Ÿæ¨¡å‹ã€‚è§å›¾
    4 äº†è§£æ›´å¤šè¯´æ˜ã€‚
- en: 'Note: Even though the terms teacher and student are used, it is not the same
    process as in knowledge distillation, where a smaller student model is trained
    to match the output of a larger teacher model.'
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šå°½ç®¡ä½¿ç”¨äº†æ•™å¸ˆå’Œå­¦ç”Ÿçš„æœ¯è¯­ï¼Œä½†è¿™ä¸çŸ¥è¯†è’¸é¦ä¸­çš„è¿‡ç¨‹ä¸åŒï¼ŒçŸ¥è¯†è’¸é¦ä¸­ä¸€ä¸ªè¾ƒå°çš„å­¦ç”Ÿæ¨¡å‹è¢«è®­ç»ƒä»¥åŒ¹é…è¾ƒå¤§æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºã€‚
- en: '![](../Images/07473682c441e2c3a26e1397649cc19e.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07473682c441e2c3a26e1397649cc19e.png)'
- en: Fig. 4\. Teacher-Student Pre-Training. Image by [Sascha Kirch](https://medium.com/@SaschaKirch)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4ï¼šæ•™å¸ˆ-å­¦ç”Ÿé¢„è®­ç»ƒã€‚å›¾ç‰‡ç”±[Sascha Kirch](https://medium.com/@SaschaKirch) æä¾›
- en: Interestingly, as we will see in the experiments, the student surpasses the
    teacher on many (but not all) datasets for both; zero-shot and few-shot detection.
    *Why is that?* The paper hypothesizes, that even though the teacher provides a
    prediction with low confidence (they call it an â€œeducated guessâ€), it becomes
    the ground truth (they call it â€œsupervised signalâ€) in the generated dataset consumed
    by the student.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è¶£çš„æ˜¯ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨å®éªŒä¸­çœ‹åˆ°çš„ï¼Œå­¦ç”Ÿåœ¨è®¸å¤šï¼ˆä½†ä¸æ˜¯æ‰€æœ‰ï¼‰æ•°æ®é›†ä¸Šè¶…è¶Šäº†æ•™å¸ˆï¼Œæ— è®ºæ˜¯é›¶æ ·æœ¬æ£€æµ‹è¿˜æ˜¯å°‘æ ·æœ¬æ£€æµ‹ã€‚*ä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼Ÿ* è®ºæ–‡å‡è®¾ï¼Œå³ä½¿æ•™å¸ˆæä¾›äº†ä½ç½®ä¿¡åº¦çš„é¢„æµ‹ï¼ˆä»–ä»¬ç§°ä¹‹ä¸ºâ€œæœ‰æ ¹æ®çš„çŒœæµ‹â€ï¼‰ï¼Œå®ƒä»ç„¶ä¼šæˆä¸ºå­¦ç”Ÿæ‰€ä½¿ç”¨çš„ç”Ÿæˆæ•°æ®é›†ä¸­çš„çœŸå®å€¼ï¼ˆä»–ä»¬ç§°ä¹‹ä¸ºâ€œç›‘ç£ä¿¡å·â€ï¼‰ã€‚
- en: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----5ddb601873aa--------------------------------)
    [## Get an email whenever Sascha Kirch publishes ğŸš€'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----5ddb601873aa--------------------------------)
    [## æ¯å½“Sascha Kirchå‘å¸ƒæ–°å†…å®¹æ—¶è·å–ç”µå­é‚®ä»¶ ğŸš€'
- en: Get an email whenever Sascha Kirch publishes ğŸš€ Looking to learn more about deep
    learning or simply stay up to dateâ€¦
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¯å½“Sascha Kirchå‘å¸ƒæ–°å†…å®¹æ—¶è·å–ç”µå­é‚®ä»¶ ğŸš€ å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºæ·±åº¦å­¦ä¹ çš„å†…å®¹æˆ–åªæ˜¯æƒ³ä¿æŒæ›´æ–°â€¦â€¦
- en: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----5ddb601873aa--------------------------------)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----5ddb601873aa--------------------------------)
- en: Experiments
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®éªŒ
- en: 'The GLIP paper presents various experiments and ablation studies, mainly concerned
    with:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: GLIPè®ºæ–‡å±•ç¤ºäº†å„ç§å®éªŒå’Œæ¶ˆèç ”ç©¶ï¼Œä¸»è¦å…³æ³¨äºï¼š
- en: Zero-Shot Domain Transfer
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é›¶æ ·æœ¬é¢†åŸŸè½¬ç§»
- en: Data Efficiency
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ•°æ®æ•ˆç‡
- en: Prompt Engineering
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æç¤ºå·¥ç¨‹
- en: I have some doubts for some of the results and the way they are presented, and
    I will point them out in the annotations. I donâ€™t want to diminish the achievements
    of GLIP and rather view it with a critical eye.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¯¹ä¸€äº›ç»“æœå’Œå®ƒä»¬çš„å‘ˆç°æ–¹å¼æœ‰ä¸€äº›ç–‘é—®ï¼Œæˆ‘ä¼šåœ¨æ³¨é‡Šä¸­æŒ‡å‡ºè¿™äº›é—®é¢˜ã€‚æˆ‘ä¸æƒ³è´¬ä½GLIPçš„æˆå°±ï¼Œè€Œæ˜¯æŒæ‰¹åˆ¤çš„çœ¼å…‰æ¥çœ‹å¾…å®ƒã€‚
- en: Now letâ€™s jump into the details!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬æ·±å…¥ç»†èŠ‚ï¼
- en: Zero-Shot Domain Transfer
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é›¶æ ·æœ¬é¢†åŸŸè½¬ç§»
- en: First, we will have a look into the results from the zero-shot domain transfer.
    In this task the objective is to analyze how well the pre-trained GLIP models
    perform on a different dataset (i.e. COCO and LVIS) as used during pre-training
    and compare it against a baseline with models that have been trained in a supervised
    fashion. Then, the pre-trained GLIP is further fine-tuned and evaluated on the
    dataset under test.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å°†æŸ¥çœ‹é›¶æ ·æœ¬é¢†åŸŸè½¬ç§»çš„ç»“æœã€‚åœ¨è¿™ä¸ªä»»åŠ¡ä¸­ï¼Œç›®æ ‡æ˜¯åˆ†æé¢„è®­ç»ƒçš„GLIPæ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ï¼ˆå³COCOå’ŒLVISï¼‰ä¸Šçš„è¡¨ç°ï¼Œä¸é‚£äº›ç»è¿‡ç›‘ç£è®­ç»ƒçš„æ¨¡å‹çš„åŸºçº¿è¿›è¡Œæ¯”è¾ƒã€‚ç„¶åï¼Œå¯¹é¢„è®­ç»ƒçš„GLIPè¿›è¡Œè¿›ä¸€æ­¥çš„å¾®è°ƒï¼Œå¹¶åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚
- en: In Fig.5 we see the results from the zero-shot domain transfer on [COCO](https://cocodataset.org/#home).
    We see that all GLIP models have a better 0-shot performance as a supervised Faster
    RCNN. We are also presented with the result, that GLIP-L outperforms the previous
    SOTA (at the time of the paperâ€™s release). We see that the larger student GLIP-L
    outperforms the teacher model GLIP-T (C).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å›¾5ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°COCOä¸Šçš„é›¶æ ·æœ¬é¢†åŸŸè½¬ç§»ç»“æœã€‚æˆ‘ä»¬çœ‹åˆ°æ‰€æœ‰GLIPæ¨¡å‹åœ¨é›¶æ ·æœ¬æ€§èƒ½ä¸Šä¼˜äºç›‘ç£çš„Faster RCNNã€‚æˆ‘ä»¬è¿˜çœ‹åˆ°GLIP-Lè¶…è¶Šäº†ä¹‹å‰çš„SOTAï¼ˆåœ¨è®ºæ–‡å‘å¸ƒæ—¶ï¼‰ã€‚æˆ‘ä»¬çœ‹åˆ°è¾ƒå¤§çš„å­¦ç”Ÿæ¨¡å‹GLIP-Lè¶…è¶Šäº†æ•™å¸ˆæ¨¡å‹GLIP-Tï¼ˆCï¼‰ã€‚
- en: '![](../Images/6f347562bca6718e7a4d2ba0a643c1cd.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f347562bca6718e7a4d2ba0a643c1cd.png)'
- en: 'Fig. 5: Zero-shot domain transfer and fine-tuning on COCO. [Image source](https://arxiv.org/abs/2112.03857)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5ï¼šCOCOä¸Šçš„é›¶æ ·æœ¬é¢†åŸŸè½¬ç§»å’Œå¾®è°ƒã€‚[å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2112.03857) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    çš„æ³¨é‡Š
- en: Following I list my doubts when reading these results and the claims made in
    the paper, where it is said that GLIP-L surpasses the best supervised model SoftTeacher.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘åˆ—å‡ºäº†åœ¨é˜…è¯»è¿™äº›ç»“æœå’Œè®ºæ–‡ä¸­æå‡ºçš„å£°æ˜æ—¶çš„ç–‘è™‘ï¼Œè®ºæ–‡ä¸­æåˆ°GLIP-Lè¶…è¶Šäº†æœ€ä½³ç›‘ç£æ¨¡å‹SoftTeacherã€‚
- en: The model that has better metrics than SoftTeacher is GLIP-L, which is better
    by 0.2 points. This small margin might not be the result of the new method of
    GLIP but might be due to some differences in training hyperparameters.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å…·æœ‰æ¯”SoftTeacheræ›´å¥½æŒ‡æ ‡çš„æ¨¡å‹æ˜¯GLIP-Lï¼Œé¢†å…ˆ0.2åˆ†ã€‚è¿™ä¸€å°çš„å·®è·å¯èƒ½ä¸æ˜¯GLIPæ–°æ–¹æ³•çš„ç»“æœï¼Œè€Œå¯èƒ½æ˜¯ç”±äºè®­ç»ƒè¶…å‚æ•°çš„ä¸€äº›å·®å¼‚ã€‚
- en: GLIP-L does not even use the data (Cap4M or Cap24M) generated from teacher model
    which they presented as a good solution.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GLIP-Lç”šè‡³æ²¡æœ‰ä½¿ç”¨ä»æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„æ•°æ®ï¼ˆCap4Mæˆ–Cap24Mï¼‰ï¼Œä»–ä»¬å°†å…¶å±•ç¤ºä¸ºä¸€ä¸ªå¥½çš„è§£å†³æ–¹æ¡ˆã€‚
- en: GLIP-L has been trained on a much larger corpus of training data as SoftTeacher.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GLIP-Låœ¨æ¯”SoftTeacheræ›´å¤§çš„è®­ç»ƒæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚
- en: In my opinion the results comparing the different GLIP models and the DyHead-T
    they trained themselves are completely fine, I just have my doubts in general
    when different methods and models are compared under unclear or different constraints.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºä¸åŒ GLIP æ¨¡å‹å’Œä»–ä»¬è‡ªå·±è®­ç»ƒçš„ DyHead-T çš„ç»“æœæ¯”è¾ƒæ˜¯å®Œå…¨å¯ä»¥çš„ï¼Œåªæ˜¯åœ¨ä¸åŒæ–¹æ³•å’Œæ¨¡å‹åœ¨ä¸æ˜ç¡®æˆ–ä¸åŒçº¦æŸä¸‹æ¯”è¾ƒæ—¶ï¼Œæˆ‘æ€»æ˜¯æœ‰ä¸€äº›ç–‘è™‘ã€‚
- en: In Fig.6, we see the zero-shot domain transfer performance on [LVIS](https://www.lvisdataset.org/)
    dataset. We can see that the largest GLIP model, GLIP-L, outperforms all other
    presented supervised models.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å›¾6ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° [LVIS](https://www.lvisdataset.org/) æ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬é¢†åŸŸè¿ç§»æ€§èƒ½ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæœ€å¤§çš„ GLIP
    æ¨¡å‹ GLIP-L è¶…è¶Šäº†æ‰€æœ‰å…¶ä»–å‘ˆç°çš„ç›‘ç£æ¨¡å‹ã€‚
- en: '![](../Images/26ec45482630c9a95d494b079b0500ce.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26ec45482630c9a95d494b079b0500ce.png)'
- en: 'Fig. 6: Zero-shot domain transfer to LVIS. [Image source](https://arxiv.org/abs/2112.03857)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾6ï¼šé›¶æ ·æœ¬é¢†åŸŸè¿ç§»åˆ°LVISã€‚ [å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2112.03857) + æ³¨é‡Šç”± [Sascha Kirch](https://medium.com/@SaschaKirch)
    æä¾›
- en: Finally, GLIP has been compared on its phrase grounding performance on the Flickr30K
    entities against MDETR (see Fig.7). Both student models, GLIP-T and GLIP-L, surpass
    the MDETR baselines.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼ŒGLIP åœ¨Flickr30K å®ä½“ä¸Šçš„çŸ­è¯­å®šä½æ€§èƒ½ä¸ MDETR è¿›è¡Œäº†æ¯”è¾ƒï¼ˆè§å›¾7ï¼‰ã€‚ä¸¤ç§å­¦ç”Ÿæ¨¡å‹ï¼ŒGLIP-T å’Œ GLIP-Lï¼Œéƒ½è¶…è¶Šäº†
    MDETR åŸºçº¿ã€‚
- en: '![](../Images/ce1a12825e5e44ed1f6bf5b2026ead2d.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce1a12825e5e44ed1f6bf5b2026ead2d.png)'
- en: 'Fig. 7: Phrase grounding performance on Flickr30K entities. [Image source](https://arxiv.org/abs/2112.03857)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾7ï¼šFlickr30K å®ä½“ä¸Šçš„çŸ­è¯­å®šä½æ€§èƒ½ã€‚ [å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2112.03857) + æ³¨é‡Šç”± [Sascha
    Kirch](https://medium.com/@SaschaKirch) æä¾›
- en: Data Efficiency
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®æ•ˆç‡
- en: Another experiment is concerned with the data efficiency. This experiment aims
    to show how the performance (in terms of average precision) changes when fine-tuning
    a pre-trained model on a certain number of task specific data. In Fig.8, the models
    are evaluated on 13 different datasets and their performance is reported as average
    precision averaged over the 13 datasets. Results are reported for 0-shot, 1-shot,
    3-shot, 5-shot, 10-shot and â€œallâ€-shot (I doubt thatâ€™s an official term for complete
    fine-tuning, but I guess you get the point ğŸ˜…).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå®éªŒå…³æ³¨æ•°æ®æ•ˆç‡ã€‚è¿™ä¸ªå®éªŒæ—¨åœ¨å±•ç¤ºå½“åœ¨ä¸€å®šæ•°é‡çš„ä»»åŠ¡ç‰¹å®šæ•°æ®ä¸Šå¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæ€§èƒ½ï¼ˆä»¥å¹³å‡ç²¾åº¦è¡¡é‡ï¼‰å¦‚ä½•å˜åŒ–ã€‚åœ¨å›¾8ä¸­ï¼Œè¿™äº›æ¨¡å‹åœ¨13ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå…¶æ€§èƒ½ä½œä¸ºåœ¨13ä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡ç²¾åº¦è¿›è¡ŒæŠ¥å‘Šã€‚ç»“æœåŒ…æ‹¬0-shotã€1-shotã€3-shotã€5-shotã€10-shotå’Œâ€œallâ€-shotï¼ˆæˆ‘æ€€ç–‘â€œallâ€-shotæ˜¯å¦æ˜¯å®Œæ•´å¾®è°ƒçš„å®˜æ–¹æœ¯è¯­ï¼Œä½†æˆ‘æƒ³ä½ æ˜ç™½æˆ‘çš„æ„æ€
    ğŸ˜…ï¼‰ã€‚
- en: '![](../Images/a402f0df992ed3145a417aaee59c5a7b.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a402f0df992ed3145a417aaee59c5a7b.png)'
- en: 'Fig. 8: Data Efficiency. [Image source](https://arxiv.org/abs/2112.03857) +
    annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8ï¼šæ•°æ®æ•ˆç‡ã€‚ [å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2112.03857) + æ³¨é‡Šç”± [Sascha Kirch](https://medium.com/@SaschaKirch)
    æä¾›
- en: Prompt Engineering
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æç¤ºå·¥ç¨‹
- en: 'Similar as in [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500),
    the authors also report a correlation of the modelâ€™s performance and the formulation
    of the input text prompt. They propose two techniques to improve the performance
    of a pre-trained model, without the need to retrain the modelâ€™s weights:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    ç±»ä¼¼ï¼Œä½œè€…è¿˜æŠ¥å‘Šäº†æ¨¡å‹æ€§èƒ½ä¸è¾“å…¥æ–‡æœ¬æç¤ºçš„è¡¨è¿°ä¹‹é—´çš„ç›¸å…³æ€§ã€‚ä»–ä»¬æå‡ºäº†ä¸¤ç§æŠ€æœ¯æ¥æé«˜é¢„è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹çš„æƒé‡ï¼š
- en: Manual prompt tuning
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‰‹åŠ¨æç¤ºè°ƒæ•´
- en: Prompt Tuning
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æç¤ºè°ƒæ•´
- en: 'The idea of manual prompt tuning is to provide further context in form of additional
    descriptive words, see Fig. 9:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰‹åŠ¨æç¤ºè°ƒæ•´çš„æƒ³æ³•æ˜¯æä¾›é¢å¤–çš„æè¿°æ€§è¯è¯­æ¥æä¾›æ›´å¤šçš„ä¸Šä¸‹æ–‡ï¼Œè§å›¾9ï¼š
- en: '![](../Images/7b797264c781c99cadefc232338c9843.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b797264c781c99cadefc232338c9843.png)'
- en: 'Fig. 9: Manual prompt tning example. [Image source](https://arxiv.org/abs/2112.03857)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9ï¼šæ‰‹åŠ¨æç¤ºè°ƒæ•´ç¤ºä¾‹ã€‚ [å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2112.03857) + æ³¨é‡Šç”± [Sascha Kirch](https://medium.com/@SaschaKirch)
    æä¾›
- en: Manual prompt tuning can always be used to improve the performance, meaning
    it does not matter if the model is fully fine-tuned or if the model is used in
    a zero-shot or few-shot scenario.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰‹åŠ¨æç¤ºè°ƒæ•´å§‹ç»ˆå¯ä»¥ç”¨æ¥æé«˜æ€§èƒ½ï¼Œè¿™æ„å‘³ç€æ— è®ºæ¨¡å‹æ˜¯å®Œå…¨å¾®è°ƒè¿˜æ˜¯åœ¨é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬åœºæ™¯ä¸­ä½¿ç”¨ï¼Œæ•ˆæœéƒ½ä¸ä¼šå—åˆ°å½±å“ã€‚
- en: The second approach, prompt tuning, requires access to ground truth labels of
    a downstream task and is especially suitable for scenarios, where each detection
    task has a single prompt (e.g. â€œDetect carâ€). In that scenario, this prompt would
    first be translated into a feature embedding using the text encoder. Then, the
    image encoder and the deep fusion module are frozen and only the input embedding
    is optimized using the ground truth labels. The optimized embeddings would then
    serve as input to the model and the text encoder could be removed.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒç§æ–¹æ³•ï¼Œæç¤ºè°ƒæ•´ï¼Œéœ€è¦è®¿é—®ä¸‹æ¸¸ä»»åŠ¡çš„çœŸå®æ ‡ç­¾ï¼Œç‰¹åˆ«é€‚ç”¨äºæ¯ä¸ªæ£€æµ‹ä»»åŠ¡åªæœ‰ä¸€ä¸ªæç¤ºçš„åœºæ™¯ï¼ˆä¾‹å¦‚ï¼šâ€œæ£€æµ‹æ±½è½¦â€ï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿™ä¸ªæç¤ºä¼šé¦–å…ˆé€šè¿‡æ–‡æœ¬ç¼–ç å™¨è½¬æ¢æˆç‰¹å¾åµŒå…¥ã€‚ç„¶åï¼Œå›¾åƒç¼–ç å™¨å’Œæ·±åº¦èåˆæ¨¡å—è¢«å†»ç»“ï¼Œä»…ä½¿ç”¨çœŸå®æ ‡ç­¾ä¼˜åŒ–è¾“å…¥åµŒå…¥ã€‚ä¼˜åŒ–åçš„åµŒå…¥å°†ä½œä¸ºæ¨¡å‹è¾“å…¥ï¼Œæ–‡æœ¬ç¼–ç å™¨å¯ä»¥è¢«ç§»é™¤ã€‚
- en: Fig.10 shows the result of this prompt tuning for various GLIP models. When
    applied to models that have a deep fusion module, prompt tuning achieves almost
    the same performance as fine-tuning the modelâ€™s weights.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 10 æ˜¾ç¤ºäº†å¯¹å„ç§ GLIP æ¨¡å‹è¿›è¡Œæ­¤æç¤ºè°ƒæ•´çš„ç»“æœã€‚å½“åº”ç”¨äºå…·æœ‰æ·±åº¦èåˆæ¨¡å—çš„æ¨¡å‹æ—¶ï¼Œæç¤ºè°ƒæ•´å‡ ä¹ä¸å¾®è°ƒæ¨¡å‹æƒé‡çš„æ•ˆæœç›¸åŒã€‚
- en: '![](../Images/a73b1b8b25722d15c6911a39848c351f.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a73b1b8b25722d15c6911a39848c351f.png)'
- en: 'Fig. 10: Effectiveness of prompt tuning. [Image source](https://arxiv.org/abs/2112.03857)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 10ï¼šæç¤ºè°ƒæ•´çš„æœ‰æ•ˆæ€§ã€‚[å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2112.03857) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    çš„æ³¨é‡Š
- en: Further Readings & Resources
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»å’Œèµ„æº
- en: As mentioned at the beginning of this article, GLIP has been widely adopted
    by a vast number of projects.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ¬æ–‡å¼€å¤´æ‰€è¿°ï¼ŒGLIP å·²è¢«å¹¿æ³›åº”ç”¨äºå¤§é‡é¡¹ç›®ä¸­ã€‚
- en: 'Following a list of papers that built upon GLIP:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸€äº›å»ºç«‹åœ¨ GLIP åŸºç¡€ä¸Šçš„è®ºæ–‡ï¼š
- en: '[GLIPv2: Unifying Localization and Vision-Language Understanding](https://arxiv.org/abs/2206.05836)'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[GLIPv2ï¼šç»Ÿä¸€å®šä½å’Œè§†è§‰è¯­è¨€ç†è§£](https://arxiv.org/abs/2206.05836)'
- en: '[GLIGEN: Open-Set Grounded Text-to-Image Generation](https://arxiv.org/abs/2301.07093)'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[GLIGENï¼šå¼€æ”¾é›†åŸºç¡€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ](https://arxiv.org/abs/2301.07093)'
- en: '[Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object
    Detection](https://arxiv.org/abs/2303.05499)'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Grounding DINOï¼šå°† DINO ä¸ Grounded é¢„è®­ç»ƒç»“åˆä»¥è¿›è¡Œå¼€æ”¾é›†ç‰©ä½“æ£€æµ‹](https://arxiv.org/abs/2303.05499)'
- en: 'Here is a list of repositories if you want to dive into the implementation
    of GLIP and other interesting projects that built upon GLIP:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³æ·±å…¥äº†è§£ GLIP çš„å®ç°ä»¥åŠå…¶ä»–å»ºç«‹åœ¨ GLIP åŸºç¡€ä¸Šçš„æœ‰è¶£é¡¹ç›®ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªä»“åº“åˆ—è¡¨ï¼š
- en: '[Official implementation of GLIP](https://github.com/microsoft/GLIP)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GLIP å®˜æ–¹å®ç°](https://github.com/microsoft/GLIP)'
- en: '[Python Notebook to play around with GLIP](https://colab.research.google.com/drive/12x7v-_miN7-SRiziK3Cx4ffJzstBJNqb?usp=sharing)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ç”¨äºç©å¼„ GLIP çš„ Python Notebook](https://colab.research.google.com/drive/12x7v-_miN7-SRiziK3Cx4ffJzstBJNqb?usp=sharing)'
- en: '[GroundingDINO: combining GLIP and DINO](https://github.com/IDEA-Research/GroundingDINO)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GroundingDINOï¼šç»“åˆ GLIP å’Œ DINO](https://github.com/IDEA-Research/GroundingDINO)'
- en: '[Grounded-Segment-Anything: combining GroundingDINO and SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Grounded-Segment-Anythingï¼šç»“åˆ GroundingDINO å’Œ SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything)'
- en: 'Here is one of my articles about the [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    foundation model, following the same approach of summary as this article:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘å…³äº[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)åŸºç¡€æ¨¡å‹çš„æ–‡ç« ä¹‹ä¸€ï¼Œé‡‡ç”¨ä¸æœ¬æ–‡ç›¸åŒçš„æ€»ç»“æ–¹æ³•ï¼š
- en: '[](/the-clip-foundation-model-7770858b487d?source=post_page-----5ddb601873aa--------------------------------)
    [## The CLIP Foundation Model'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/the-clip-foundation-model-7770858b487d?source=post_page-----5ddb601873aa--------------------------------)
    [## CLIP åŸºç¡€æ¨¡å‹'
- en: Paper Summaryâ€” Learning Transferable Visual Models From Natural Language Supervision
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ€»ç»“â€”â€”ä»è‡ªç„¶è¯­è¨€ç›‘ç£ä¸­å­¦ä¹ å¯è½¬ç§»çš„è§†è§‰æ¨¡å‹
- en: towardsdatascience.com](/the-clip-foundation-model-7770858b487d?source=post_page-----5ddb601873aa--------------------------------)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/the-clip-foundation-model-7770858b487d?source=post_page-----5ddb601873aa--------------------------------)
