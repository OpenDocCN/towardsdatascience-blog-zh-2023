- en: 'GLIP: Introducing Language-Image Pre-Training to Object Detection'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa](https://towardsdatascience.com/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[üöÄSascha‚Äôs Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Grounded Language-Image Pre-training by L. H. Li et. al.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)
    ¬∑9 min read¬∑Sep 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'Today we will dive into a paper that builds upon the great success of [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    in language-image pre-training and extends it to the task of object detection:
    GLIP ‚Äî **G**rounded **L**anguage-**I**mage **P**re-training. We will cover the
    key concepts and findings of the paper and make them easy to understand by providing
    further context and adding annotations to images and experiment results. Let‚Äôs
    go!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d7ef9f457ada5a99c0936ce420e24274.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created from [publication](https://arxiv.org/abs/2112.03857) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  prefs: []
  type: TYPE_NORMAL
- en: '**Paper:** [Grounded Language-Image Pre-training](https://arxiv.org/abs/2112.03857)
    by'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Liunian Harold Li et. al., 7\. Dec. 2021
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/microsoft/GLIP)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** representation learning, object detection, phrase-grounding,
    foundation models'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    ‚Äî [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    ‚Äî [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    ‚Äî [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    ‚Äî [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    ‚Äî [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Outline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Context & Background
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Claimed Contributions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Method
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiments
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further Readings & Resources
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Context & Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GLIP **(G**rounded **L**anguage-**I**mage **P**re-training) is a multi-modal
    language-image model. Similar to [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    (**C**ontrastive **L**anguage-**I**mage **P**re-Training), it performs contrastive
    pre-training to learn semantically rich representations and aligns them across
    its modalities. While [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    learns these representation on an image level, which means one sentence describes
    the entire image, GLIP aims to extend this approach to object-level representations,
    meaning one sentence might correspond to multiple objects within the image. The
    task of identifying correspondences between single tokens in a text-prompt and
    objects or regions in an image is called *phrase grounding*. Hence the word ‚ÄúGrounded‚Äù
    in GLIP.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, GLIP aims to:'
  prefs: []
  type: TYPE_NORMAL
- en: Unify phrase grounding and object detection for large-scale pre-training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide a flexible framework for zero-shot object detection, where flexible
    means it is not restricted to a fixed set of classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build one pre-trained model that seamlessly transfers to various tasks and domains,
    in a zero-shot or few-shot manner.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*What can you do with such a model?* You could use text prompts to find objects
    or regions of interest within a given input image. And the best part: you are
    not restricted to pre-defined classes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af97d1d27d14bad39edef24dd33dbeca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 1: Output of GLIP for different images and prompt formats. [Image source](https://arxiv.org/abs/2112.03857)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: You could further process these detections (e.g. feeding those into a tracking
    system) or create a custom dataset with certain classes of interest and use those
    to train your own supervised detection system. Not only that you could cover rare
    or very specific classes, but you could also save a lot of time and money for
    the creation of manual labels. As we will see later, the authors of GLIP had a
    similar idea to boost the performance even further by introducing a teacher-student
    framework.
  prefs: []
  type: TYPE_NORMAL
- en: GLIP has been adopted by many other projects and domains in deep learning. For
    example, [GLIGEN (Grounded-Language-to-Image-Generation)](https://arxiv.org/abs/2301.07093)
    uses GLIP as to condition the image generation of a latent diffusion model to
    increase the controllability. Additionally, GLIP has been combined with other
    models such as [DINO (**D**ETR with **I**mproved de**N**oising anch**O**r
  prefs: []
  type: TYPE_NORMAL
- en: boxes](https://arxiv.org/abs/2203.03605)) and [SAM (Segment Anything Model)](https://arxiv.org/abs/2304.02643)
    to [GroundingDINO](https://arxiv.org/abs/2303.05499) and [Grounded-Segment-Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything)
    respectively. [GLIPv2](https://arxiv.org/abs/2206.05836) extends the initial GLIP
    model with vision-language understanding to not only improve phrase grounding
    but also enable visual question answering tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Paper Walkthroughs by Sascha Kirch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2?source=post_page-----5ddb601873aa--------------------------------)7
    stories![‚ÄúDDPM‚Ää‚Äî‚ÄäDenoising Diffusion Probabilistic Models ‚Äú paper illustration
    by Sascha Kirch](../Images/6e785c0a911386676abebe0fa646f483.png)![‚ÄúDepth Anything‚Äù
    paper illustration by Sascha Kirch](../Images/bd8cd71a02e42cf64d0afd39f41f48e0.png)![](../Images/8708d91a4a1902cef889ced95d46fc39.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Claimed Contributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large scale pre-training for combined phrase grounding and object detection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Providing a unified view on object detection and phrase grounding
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deep cross-modality fusion to learn high-quality language-aware visual representations
    and to achieve superior transfer learning performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Presenting that prompt-tuning is more effective in deep vision-language fusion
    (e.g. GLIP) as in shallow fused networks (e.g. [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having a rough idea of what can be done with GLIP, let‚Äôs have a closer look
    into the details of the paper.
  prefs: []
  type: TYPE_NORMAL
- en: Architectural Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On a high level, GLIP‚Äôs architecture is quite similar to [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)‚Äôs
    in a sense that it also consists of a text encoder, an image encoder and some
    sort of contrastive learning on the similarity of text and image features. The
    architecture of GLIP is shown in Fig. 2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03fe6d986334bdeeb0e40e962cc55592.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 2: Framework architecture. [Image source](https://arxiv.org/abs/2112.03857)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: GLIP adds a language-image aware deep fusion module after the text and image
    encoder. This module performs cross-modal attention and extracts further features.
    A cosine similarity is calculated over the resulting region features and word
    features. During training, the similarity of matching pairs is maximized, while
    minimized for incorrect pairs. In contrast to [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500),
    where the matching pairs are located on the diagonal of the similarity matrix,
    in GLIP the matching is not performed on sentence level, but on (sub)word level
    resulting in usually off-diagonal positions.
  prefs: []
  type: TYPE_NORMAL
- en: Phrase Grounding Formulated as Object Detection Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The authors noted that the problem of phrase grounding (= associating words
    with objects/regions in an image) can be formulated as Object detection Objective,
    where the standard loss objective is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7f7962cd88754b158fc687760fa7be2c.png)'
  prefs: []
  type: TYPE_IMG
- en: The localization loss is concerned with the quality of the predicted bounding
    box, which depending on the format, might be the size and location of the box.
    The classification loss is the key part in the unification. By calculating the
    logits over the similarity score of text-image features instead of over the logits
    from an image classifier, the same loss objective can be used for training.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f659414dd9a3a8b27fe30f950211e69b.png)'
  prefs: []
  type: TYPE_IMG
- en: Different Model Variants
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Five different models are trained to show the effect of the authors‚Äô design
    choices and model scale:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e1c6bb6395dde8d9cf1d0c547a63f76f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 3: Model variants. [Image source](https://arxiv.org/abs/2112.03857) +
    annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: Teacher-Student Pre-Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To boost the performance of GLIP, the authors train the GLIP-T (C) model (see
    Fig.3) on human annotated data, called GoldG, to generate grounding data from
    text-image pairs from the internet. They call this model the teacher model and
    subsequently train a student model feeding it the with the data used to train
    the teacher plus the data the teacher generated. See Fig. 4 for an illustration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Even though the terms teacher and student are used, it is not the same
    process as in knowledge distillation, where a smaller student model is trained
    to match the output of a larger teacher model.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/07473682c441e2c3a26e1397649cc19e.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 4\. Teacher-Student Pre-Training. Image by [Sascha Kirch](https://medium.com/@SaschaKirch)
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, as we will see in the experiments, the student surpasses the
    teacher on many (but not all) datasets for both; zero-shot and few-shot detection.
    *Why is that?* The paper hypothesizes, that even though the teacher provides a
    prediction with low confidence (they call it an ‚Äúeducated guess‚Äù), it becomes
    the ground truth (they call it ‚Äúsupervised signal‚Äù) in the generated dataset consumed
    by the student.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----5ddb601873aa--------------------------------)
    [## Get an email whenever Sascha Kirch publishes üöÄ'
  prefs: []
  type: TYPE_NORMAL
- en: Get an email whenever Sascha Kirch publishes üöÄ Looking to learn more about deep
    learning or simply stay up to date‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----5ddb601873aa--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The GLIP paper presents various experiments and ablation studies, mainly concerned
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Shot Domain Transfer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data Efficiency
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prompt Engineering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I have some doubts for some of the results and the way they are presented, and
    I will point them out in the annotations. I don‚Äôt want to diminish the achievements
    of GLIP and rather view it with a critical eye.
  prefs: []
  type: TYPE_NORMAL
- en: Now let‚Äôs jump into the details!
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Shot Domain Transfer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we will have a look into the results from the zero-shot domain transfer.
    In this task the objective is to analyze how well the pre-trained GLIP models
    perform on a different dataset (i.e. COCO and LVIS) as used during pre-training
    and compare it against a baseline with models that have been trained in a supervised
    fashion. Then, the pre-trained GLIP is further fine-tuned and evaluated on the
    dataset under test.
  prefs: []
  type: TYPE_NORMAL
- en: In Fig.5 we see the results from the zero-shot domain transfer on [COCO](https://cocodataset.org/#home).
    We see that all GLIP models have a better 0-shot performance as a supervised Faster
    RCNN. We are also presented with the result, that GLIP-L outperforms the previous
    SOTA (at the time of the paper‚Äôs release). We see that the larger student GLIP-L
    outperforms the teacher model GLIP-T (C).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6f347562bca6718e7a4d2ba0a643c1cd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 5: Zero-shot domain transfer and fine-tuning on COCO. [Image source](https://arxiv.org/abs/2112.03857)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: Following I list my doubts when reading these results and the claims made in
    the paper, where it is said that GLIP-L surpasses the best supervised model SoftTeacher.
  prefs: []
  type: TYPE_NORMAL
- en: The model that has better metrics than SoftTeacher is GLIP-L, which is better
    by 0.2 points. This small margin might not be the result of the new method of
    GLIP but might be due to some differences in training hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GLIP-L does not even use the data (Cap4M or Cap24M) generated from teacher model
    which they presented as a good solution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GLIP-L has been trained on a much larger corpus of training data as SoftTeacher.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In my opinion the results comparing the different GLIP models and the DyHead-T
    they trained themselves are completely fine, I just have my doubts in general
    when different methods and models are compared under unclear or different constraints.
  prefs: []
  type: TYPE_NORMAL
- en: In Fig.6, we see the zero-shot domain transfer performance on [LVIS](https://www.lvisdataset.org/)
    dataset. We can see that the largest GLIP model, GLIP-L, outperforms all other
    presented supervised models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26ec45482630c9a95d494b079b0500ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 6: Zero-shot domain transfer to LVIS. [Image source](https://arxiv.org/abs/2112.03857)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, GLIP has been compared on its phrase grounding performance on the Flickr30K
    entities against MDETR (see Fig.7). Both student models, GLIP-T and GLIP-L, surpass
    the MDETR baselines.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ce1a12825e5e44ed1f6bf5b2026ead2d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 7: Phrase grounding performance on Flickr30K entities. [Image source](https://arxiv.org/abs/2112.03857)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: Data Efficiency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another experiment is concerned with the data efficiency. This experiment aims
    to show how the performance (in terms of average precision) changes when fine-tuning
    a pre-trained model on a certain number of task specific data. In Fig.8, the models
    are evaluated on 13 different datasets and their performance is reported as average
    precision averaged over the 13 datasets. Results are reported for 0-shot, 1-shot,
    3-shot, 5-shot, 10-shot and ‚Äúall‚Äù-shot (I doubt that‚Äôs an official term for complete
    fine-tuning, but I guess you get the point üòÖ).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a402f0df992ed3145a417aaee59c5a7b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 8: Data Efficiency. [Image source](https://arxiv.org/abs/2112.03857) +
    annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar as in [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500),
    the authors also report a correlation of the model‚Äôs performance and the formulation
    of the input text prompt. They propose two techniques to improve the performance
    of a pre-trained model, without the need to retrain the model‚Äôs weights:'
  prefs: []
  type: TYPE_NORMAL
- en: Manual prompt tuning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prompt Tuning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The idea of manual prompt tuning is to provide further context in form of additional
    descriptive words, see Fig. 9:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b797264c781c99cadefc232338c9843.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 9: Manual prompt tning example. [Image source](https://arxiv.org/abs/2112.03857)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: Manual prompt tuning can always be used to improve the performance, meaning
    it does not matter if the model is fully fine-tuned or if the model is used in
    a zero-shot or few-shot scenario.
  prefs: []
  type: TYPE_NORMAL
- en: The second approach, prompt tuning, requires access to ground truth labels of
    a downstream task and is especially suitable for scenarios, where each detection
    task has a single prompt (e.g. ‚ÄúDetect car‚Äù). In that scenario, this prompt would
    first be translated into a feature embedding using the text encoder. Then, the
    image encoder and the deep fusion module are frozen and only the input embedding
    is optimized using the ground truth labels. The optimized embeddings would then
    serve as input to the model and the text encoder could be removed.
  prefs: []
  type: TYPE_NORMAL
- en: Fig.10 shows the result of this prompt tuning for various GLIP models. When
    applied to models that have a deep fusion module, prompt tuning achieves almost
    the same performance as fine-tuning the model‚Äôs weights.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a73b1b8b25722d15c6911a39848c351f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 10: Effectiveness of prompt tuning. [Image source](https://arxiv.org/abs/2112.03857)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: Further Readings & Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned at the beginning of this article, GLIP has been widely adopted
    by a vast number of projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following a list of papers that built upon GLIP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[GLIPv2: Unifying Localization and Vision-Language Understanding](https://arxiv.org/abs/2206.05836)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[GLIGEN: Open-Set Grounded Text-to-Image Generation](https://arxiv.org/abs/2301.07093)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object
    Detection](https://arxiv.org/abs/2303.05499)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is a list of repositories if you want to dive into the implementation
    of GLIP and other interesting projects that built upon GLIP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Official implementation of GLIP](https://github.com/microsoft/GLIP)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python Notebook to play around with GLIP](https://colab.research.google.com/drive/12x7v-_miN7-SRiziK3Cx4ffJzstBJNqb?usp=sharing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GroundingDINO: combining GLIP and DINO](https://github.com/IDEA-Research/GroundingDINO)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Grounded-Segment-Anything: combining GroundingDINO and SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is one of my articles about the [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    foundation model, following the same approach of summary as this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/the-clip-foundation-model-7770858b487d?source=post_page-----5ddb601873aa--------------------------------)
    [## The CLIP Foundation Model'
  prefs: []
  type: TYPE_NORMAL
- en: Paper Summary‚Äî Learning Transferable Visual Models From Natural Language Supervision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/the-clip-foundation-model-7770858b487d?source=post_page-----5ddb601873aa--------------------------------)
  prefs: []
  type: TYPE_NORMAL
