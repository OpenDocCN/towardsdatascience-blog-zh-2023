- en: Comparing and Explaining Diffusion Models in HuggingFace Diffusers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/comparing-and-explaining-diffusion-models-in-huggingface-diffusers-a83d64348d90](https://towardsdatascience.com/comparing-and-explaining-diffusion-models-in-huggingface-diffusers-a83d64348d90)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: DDPM, Stable Diffusion, DALL·E-2, Imagen, Kandinsky 2, SDEdit, ControlNet, InstructPix2Pix,
    and more
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mnslarcher.medium.com/?source=post_page-----a83d64348d90--------------------------------)[![Mario
    Larcher](../Images/b5b443807fe06f096ed4fc5139b3cb42.png)](https://mnslarcher.medium.com/?source=post_page-----a83d64348d90--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a83d64348d90--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a83d64348d90--------------------------------)
    [Mario Larcher](https://mnslarcher.medium.com/?source=post_page-----a83d64348d90--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a83d64348d90--------------------------------)
    ·33 min read·Aug 24, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62e4b9c064053a36fd3c580645994bbc.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated with Diffusers. Continue reading to discover how and the theory
    behind.
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Introduction](#6595)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Prerequisites and Suggested Materials](#a412)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Diffusers Pipelines](#8611)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pipeline: DDPM (Diffusion Models)](#0693)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pipeline: Stable Diffusion Text-to-Image](#4aeb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pipeline: Stable Diffusion Image-to-Image (SDEdit)](#8692)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pipeline: Stable Diffusion Image Variation](#246c)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pipeline: Stable Diffusion Upscale](#f3b1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pipeline: Stable Diffusion Latent Upscale](#ed67)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pipeline: unCLIP (Karlo/DALL·E-2)](#a504)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pipeline: DeepFloyd IF (Imagen)](#730d)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pipeline: Kandinsky](#f933)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pipeline: ControlNet](#695a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pipeline: Instruct Pix2Pix](#ce6b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Appendix — CLIP](#13e9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Appendix — VQGAN](#9eb6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Appendix — Prompt-to-Prompt](#ec8a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Conclusions](#59d1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Acknowledgments](#ffc4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Embracing the ever-growing interest in Generative AI, including image generation,
    many excellent resources are starting to become available, some of which I’ll
    highlight below. However, based on my experience, progressing beyond foundational
    courses demands significant effort, as resources on advanced topics become more
    scattered.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will list the most popular diffusion models from the [Hugging
    Face](https://huggingface.co/) [Diffusers](https://huggingface.co/docs/diffusers/index)
    library, which is the primary tool for utilizing this technology. We’ll provide
    brief explanations of these models, compare them, and outline their strengths
    and weaknesses.
  prefs: []
  type: TYPE_NORMAL
- en: 'The structure of this article is as follows: we will start by reviewing a few
    valuable resources for those who are just beginning to study diffusion models.
    Afterward, we’ll provide a brief explanation of the HuggingFace pipelines. Finally,
    we will delve deep into each pipeline listed in the [Popular Tasks & Pipelines](https://github.com/huggingface/diffusers#popular-tasks--pipelines)
    section of the Diffusers GitHub repository.'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this article, I hope you will have a solid grasp of the primary
    diffusion models and associated techniques, and will be in a good position to
    apply them effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites and Suggested Materials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To fully appreciate this article, though I will strive to keep the explanations
    at an intuitive level, I recommend having a general background in these topics.
    In this section, I list three resources that I have found useful in my own journey.
  prefs: []
  type: TYPE_NORMAL
- en: Practical Deep Learning for Coders — Part 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://course.fast.ai/Lessons/part2.html?source=post_page-----a83d64348d90--------------------------------)
    [## Practical Deep Learning for Coders - Part 2 overview'
  prefs: []
  type: TYPE_NORMAL
- en: In this course, containing over 30 hours of video content, we implement the
    astounding Stable Diffusion algorithm from…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: course.fast.ai](https://course.fast.ai/Lessons/part2.html?source=post_page-----a83d64348d90--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Undoubtedly one of my favorite resources, this course not only provides essential
    insights into diffusion models but also serves as an excellent entry point to
    acquiring fundamental programming skills in Python and deep learning. [Jeremy
    Howard](https://jeremy.fast.ai/), the instructor, adopts a highly effective approach
    by starting with practical applications before delving into the theoretical intricacies.
    This approach ensures a clear understanding without overwhelming learners with
    complex mathematical formulas typically encountered in standard courses.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the course serves as a seamless continuation from [Part 1](https://course.fast.ai/),
    requiring no special prerequisites beyond its predecessor. Whether you’re a novice
    or a seasoned learner, this course is a valuable asset in your journey of mastering
    deep learning and diffusion models.
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face Diffusion Models Course
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://github.com/huggingface/diffusion-models-class?source=post_page-----a83d64348d90--------------------------------)
    [## GitHub - huggingface/diffusion-models-class: Materials for the Hugging Face
    Diffusion Models Course'
  prefs: []
  type: TYPE_NORMAL
- en: 'Materials for the Hugging Face Diffusion Models Course - GitHub - huggingface/diffusion-models-class:
    Materials for the…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/huggingface/diffusion-models-class?source=post_page-----a83d64348d90--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: In an article about the Diffusers library, it would be crazy not to mention
    the official Hugging Face course. This course, which currently has four lectures,
    dives into diffusion models, teaches you how to guide their generation, tackles
    Stable Diffusion, and wraps up with some cool advanced stuff, including applying
    these concepts to a different realm — audio generation.
  prefs: []
  type: TYPE_NORMAL
- en: Generative Deep Learning, 2nd Edition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/?source=post_page-----a83d64348d90--------------------------------)
    [## Generative Deep Learning, 2nd Edition'
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI is the hottest topic in tech. This practical book teaches machine
    learning engineers and data scientists…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.oreilly.com](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/?source=post_page-----a83d64348d90--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: For book enthusiasts, this is one of my favorite reads on the subject. As the
    title suggests, this book doesn’t just dive into diffusion models; it also covers
    the broader realm of generative AI. It delves into image generation models like
    Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), which
    are sources of inspiration for diffusion models and used on their side. The second
    edition covers arguments up until the beginning of 2023, exploring more recent
    algorithms like DALL·E-2, CLIP, Imagen, Stable Diffusion, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: If you’ve explored any of these resources or similar ones, you’re well-equipped
    for what lies ahead. If not, you can either go and explore them or proceed in
    any case with the article; I’ll try to keep the explanations as simple as possible,
    I promise.
  prefs: []
  type: TYPE_NORMAL
- en: Useful Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**MOOC**: [Practical Deep Learning for Coders — Part 2](https://course.fast.ai/Lessons/part2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MOOC**: [Hugging Face Diffusion Models Course](https://github.com/huggingface/diffusion-models-class)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Book**: [Generative Deep Learning, 2nd Edition](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bonus**: I’d like to introduce another resource that I’ve used to refresh
    some concepts while writing this article. I’m confident you’ll appreciate it as
    well. If you’re interested in understanding AI in a fun, concise, and clear manner,
    I highly recommend checking out “[**AI Coffee Break with Letitia**](https://www.youtube.com/c/aicoffeebreak)”.
    Trust me, it’s absolutely worth exploring and subscribing to!'
  prefs: []
  type: TYPE_NORMAL
- en: Diffusers Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are Diffusers Pipelines?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From the [Diffusers documentation](https://huggingface.co/docs/diffusers/api/pipelines/overview#pipelines):'
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines provide a simple way to run state-of-the-art diffusion models in inference
    by bundling all of the necessary components (multiple independently-trained models,
    schedulers, and processors) into a single end-to-end class. Pipelines are flexible
    and they can be adapted to use different scheduler or even model components.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In this article, we will discuss the models behind the most popular pipelines
    of the Diffusers library. Even though pipelines are meant for inference, the theory
    behind them is equally relevant for the training of these models. There are a
    couple of popular training techniques that don’t have a dedicated inference pipeline,
    mainly [LoRA](https://arxiv.org/abs/2106.09685) and [DreamBooth](https://arxiv.org/abs/2208.12242).
    We will not cover them in this article, but for the latter, I’ve already written
    a dedicated article. Feel free to check it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/demystifying-dreambooth-a-new-tool-for-personalizing-text-to-image-generation-70f8bb0cfa30?source=post_page-----a83d64348d90--------------------------------)
    [## Demystifying DreamBooth: A New Tool for Personalizing Text-To-Image Generation'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the technology that turns boring images into creative masterpieces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/demystifying-dreambooth-a-new-tool-for-personalizing-text-to-image-generation-70f8bb0cfa30?source=post_page-----a83d64348d90--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: How to Use Diffusion Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s learn from a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This code snippet is all that I used to generate the cover image for this article.
    We can already observe a few things:'
  prefs: []
  type: TYPE_NORMAL
- en: Even though I am using [Stable Diffusion XL](https://stability.ai/blog/stable-diffusion-sdxl-1-announcement),
    it’s not necessary to use the `[StableDiffusionXLPipeline](https://huggingface.co/docs/diffusers/v0.20.0/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLPipeline)`
    specifically; instead you can use the more general class `[DiffusionPipeline](https://huggingface.co/docs/diffusers/api/diffusion_pipeline#diffusers.DiffusionPipeline)`.
    The `from_pretrained` function will return the correct class object based on the
    repository ID ("stabilityai/stable-diffusion-xl-base-1.0" in this case) or the
    path to a local directory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s easily possible, and generally recommended to speed up the process, to
    change the weight to half precision (float16), specifying the corresponding variant.
    For example, you can check [here](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/tree/main/unet)
    that both a `diffusion_pytorch_model.f16` and a non-f16 model exist for the U-Net
    of Stable Diffusion XL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s advised to use the weights in the [safetensors](https://huggingface.co/docs/safetensors/index)
    format whenever possible. This format avoids the security issues of pickling and
    is faster at the same time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s highly recommended to execute this code on a GPU (`to("cuda")`), as diffusion
    models are computationally intensive. Generating a single prediction often requires
    around 20–50 forward passes of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you were to rerun this code, you would obtain a different result. Diffusion
    model inference is inherently non-deterministic, implying that each execution
    produces a diverse outcome (unless you intentionally enforce consistency fixing
    random seeds, etc.).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In conclusion, as observed, it’s remarkably straightforward to utilize these
    pipelines. This is why I chose to focus on the theory behind them; understanding
    it at an intuitive level is instrumental in fully harnessing the capabilities
    of these powerful tools.
  prefs: []
  type: TYPE_NORMAL
- en: Useful Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Diffusers documentation**: [Pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview#pipelines)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diffusers documentation**: [Safetensors](https://huggingface.co/docs/safetensors/index)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pipeline: DDPM (Diffusion Models)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Unraveling the Theory**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '“[Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)”
    (DDPM) marked the initial spotlight on diffusion models. While often hailed as
    the seminal paper on this theme, the concept of diffusion models was introduced
    back in 2015 within the paper titled “[Deep Unsupervised Learning using Nonequilibrium
    Thermodynamics](https://arxiv.org/abs/1503.03585)”. The figure below encapsulates
    the core concept of **diffusion models**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/860896a0d6a0a75f1292d0dbdfa9b330.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 2 from [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239).
  prefs: []
  type: TYPE_NORMAL
- en: Reading the image from right to left, we observe the **forward** or **diffusion
    process**, in which we progressively add noise to an image — a straightforward
    procedure in Python or any other programming language.
  prefs: []
  type: TYPE_NORMAL
- en: Now, imagine having an instrument that can partially remove noise from a noisy
    image. This tool would facilitate transforming an image composed entirely of noise
    into a less noisy version, progressing from left to right in the figure above
    — the **reverse process**. But how do we create this predictive model? According
    to DDPM, we utilize a U-Net. Given a noisy image, the U-Net works to predict the
    added noise (or alternatively directly the denoised image). As we introduce the
    noise ourselves, we have the target variable for free, allowing us to train the
    model in a self-supervised manner.
  prefs: []
  type: TYPE_NORMAL
- en: The **U-Net** used here isn’t the [2015 version](https://arxiv.org/abs/1505.04597);
    it’s a modern adaptation designed specifically for this task. To simplify the
    U-Net’s task, we provide not only the noisy image but also the **timestep** `t`
    as input. A higher `t` corresponds to a noisier image. This timestep is incorporated
    into the model using a sinusoidal position embedding, inspired by the [Transformer](https://arxiv.org/abs/1706.03762).
    From the Transformer derives also the self-attention mechanism, tailored for images
    in this case. Self-attention allows pixels in the 16x16 resolution blocks to attend
    to all other pixels, boosting the model's capability to generate globally consistent
    images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s introduce the concept of **sampler** or **scheduler**. According
    to the Diffusers documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: The schedule functions, denoted *Schedulers* in the library take in the output
    of a trained model, a sample which the diffusion process is iterating on, and
    a timestep to return a denoised sample. That’s why schedulers may also be called
    *Samplers* in other diffusion models implementations.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In practical terms, schedulers determine the number of steps required to generate
    the final image and establish the method for converting a noisy image into a less
    noisy variant, utilizing the model’s output. These schedulers can be categorized
    as either discrete or continuous, as elucidated in the documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: Different algorithms use timesteps that can be discrete (accepting `int` inputs),
    such as the [DDPMScheduler](https://huggingface.co/docs/diffusers/v0.19.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)
    or [PNDMScheduler](https://huggingface.co/docs/diffusers/v0.19.3/en/api/schedulers/pndm#diffusers.PNDMScheduler),
    or continuous (accepting `float` inputs), such as the score-based schedulers [ScoreSdeVeScheduler](https://huggingface.co/docs/diffusers/v0.19.3/en/api/schedulers/score_sde_ve#diffusers.ScoreSdeVeScheduler)
    or `ScoreSdeVpScheduler`.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In a similar vein, the sampling process can have either a stochastic or deterministic
    nature.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re curious to dive deeper into samplers, that would need a whole separate
    article. If that sounds intriguing, just give me a shout, and I’ll be happy to
    explore it further!
  prefs: []
  type: TYPE_NORMAL
- en: '**Applications and Limitations**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[DDPMPipeline](https://huggingface.co/docs/diffusers/api/pipelines/ddpm#diffusers.DDPMPipeline)
    is a pipeline for **unconditional image generation**, hence its practical application
    is limited compared to other techniques we will explore that allow for greater
    control over the generated output. Furthermore, the image denoising process using
    DDPM’s scheduler is quite slow; by default, it requires 1000 steps, which translate
    to 1000 predictions by the U-Net. Given these considerations, the current interest
    in DDPM is mainly historical, as subsequent works build upon this foundation.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Useful Resources**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Diffusers documentation**: [DDPM](https://huggingface.co/docs/diffusers/api/pipelines/ddpm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific paper**: [Deep Unsupervised Learning using Nonequilibrium Thermodynamics](https://arxiv.org/abs/1503.03585)
    (diffusion models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific paper**: [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)
    (DDPM)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pipeline: Stable Diffusion Text-to-Image'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unraveling the Theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To date, the primary open-source algorithm for image generation is [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release)
    in its various iterations. The initial version of Stable Diffusion is the outcome
    of a collaboration between [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/),
    [Runway](https://runwayml.com/about/), and [LAION](https://laion.ai/). The primary
    feature of this model is being a **latent diffusion** **model** (**LDM**), where
    the diffusion process occurs not directly in the image/pixel space but within
    a latent space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/86efd18764d1d440d1484f13fcdec929.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 3 from [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752).
  prefs: []
  type: TYPE_NORMAL
- en: In practice, before being fed into the U-Net, the image is compressed into a
    latent space using a [**Variational Autoencoder**](https://arxiv.org/abs/1312.6114)
    (**VAE**). After the denoising process, the latent representation is transformed
    back into an image using the decoder of the same VAE.
  prefs: []
  type: TYPE_NORMAL
- en: Another crucial point to underline is Stable Diffusion’s capability to take
    **textual prompts** as input, partly controlling the generated content. The text
    is first embedded using a Transformer-based model and then mapped into the U-Net
    using a **cross-attention** mechanism. Specifically, Stable Diffusion v1 utilizes
    the OpenAI **CLIP** text encoder (see [Appendix — CLIP](#13e9)).
  prefs: []
  type: TYPE_NORMAL
- en: Two more versions of Stable Diffusion currently exist, each with its own sub-variants.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stable Diffusion v2** stands out from the original mainly due to a shift
    in the text encoder to [OpenCLIP](https://arxiv.org/abs/2212.07143), the open-source
    counterpart of CLIP. While one might generally anticipate improved performance
    in a later version, this assertion is uncertain in the context of Stable Diffusion
    v2\. Notably, **OpenCLIP**’s training on a subset of [**LAION-5B**](https://laion.ai/blog/laion-5b/),
    different from OpenAI’s private dataset, along with the use of a highly restrictive
    NSFW filter, led v2 to noticeably lag behind v1 in representing concepts like
    celebrities or emulating styles of renowned artists. Some of these limitations
    were partially addressed in version v2.1, which introduced a less stringent filter
    and other modifications. For further insight, I found the [AssemblyAI](https://www.assemblyai.com/)
    article “[Stable Diffusion 1 vs 2 — What you need to know](https://www.assemblyai.com/blog/stable-diffusion-1-vs-2-what-you-need-to-know/)”
    particularly informative.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, Stability AI recently introduced [**Stable Diffusion XL**](https://arxiv.org/abs/2307.01952)(**SD-XL**),
    a substantial leap from v2\. This iteration competes with leading closed-source
    models like [Midjourney](https://www.midjourney.com/home/?callbackUrl=%2Fapp%2F)
    in terms of output quality.
  prefs: []
  type: TYPE_NORMAL
- en: The upgrades of this version include the merging of CLIP and OpenCLIP outputs,
    the retraining of the VAE with a larger batch size, and the implementation of
    weight tracking through the **Exponential Moving Average** (**EMA**) technique.
    The EMA weights can be employed in place of the final weights during inference,
    leading to a general improvement in performance. This technique aids in reducing
    some of the overfitting that typically arises in the final iterations, often resulting
    in the generation of slightly improved weights for inference.
  prefs: []
  type: TYPE_NORMAL
- en: Equally important is SD-XL’s effort to address the issues that arise from the
    use of squared random cropping during training. To enhance this aspect, it employs
    **cropping parameter conditioning**, which involves informing the model about
    the parameters that determine how the image has been cropped, similar to what
    is done for the timestep. This prevents issues like generating headless figures.
    Simultaneously, the SD-XL version follows modern practices and it’s fine-tuned
    to handle **multiple aspect-ratios** using [aspect ratio bucketing](https://github.com/NovelAI/novelai-aspect-ratio-bucketing).
    This, along with cropping parameter conditioning, significantly enhances the model’s
    ability to portray horizontal and vertical scenes.
  prefs: []
  type: TYPE_NORMAL
- en: SD-XL also introduces a **refinement stage** where another LDM specialized in
    high-quality images uses a noise-denoising process introduced by SDEdit, which
    we will cover in the next pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, another technique that is not always presented in introductory courses
    is [**offset noise**](https://www.crosslabs.org/blog/diffusion-with-offset-noise).
    The primary reason why we need to modify the initial noise is that, in reality,
    the image is never completely erased during the forward process (as we perform
    a finite number of steps). As a result, the model encounters difficulties in learning
    from pure noise. Quoting from the SD-XL paper:'
  prefs: []
  type: TYPE_NORMAL
- en: Our model is trained in the discrete-time formulation of [14], and requires
    offset-noise [11, 25] for aesthetically pleasing results.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Applications and Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[StableDiffusionPipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)
    (text-to-image) allows the generation of images based on textual prompts. As of
    today, my recommendation is to use the SD-XL version, which can produce truly
    astonishing results. Although SD-XL is undeniably exceptional, there are still
    various cases of failure. The model sometimes faces challenges with very complex
    prompts that involve detailed spatial arrangements and intricate descriptions.
    Complicated structures, such as human hands, can still be generated deformed at
    times. While the photorealism is quite good, it’s not yet perfect. Occasionally,
    a phenomenon known as “**concept bleeding**” occurs, where, for example, a color
    associated with one element in the prompt is mistaken or extended to another element.
    The texts generated by SD-XL are significantly better than in the past, but sometimes,
    especially for longer ones, they contain errors like random characters or inconsistencies.
    Lastly, it’s important to remember that like all generative models, this can inadvertently
    introduce social and racial biases.'
  prefs: []
  type: TYPE_NORMAL
- en: Useful Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Diffusers documentation**: [Text-to-image](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific paper**: [High-Resolution Image Synthesis with Latent Diffusion
    Models](https://arxiv.org/abs/2112.10752) (Stable Diffusion v1, check out my article
    below, which breaks down this paper for you)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific paper**: [SDXL: Improving Latent Diffusion Models for High-Resolution
    Image Synthesis](https://arxiv.org/abs/2307.01952)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific paper**: [Reproducible scaling laws for contrastive language-image
    learning](https://arxiv.org/abs/2212.07143) (OpenCLIP)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific blog**: [Stable Diffusion 1 vs 2 — What you need to know](https://www.assemblyai.com/blog/stable-diffusion-1-vs-2-what-you-need-to-know/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific blog**: [Diffusion With Offset Noise](https://www.crosslabs.org/blog/diffusion-with-offset-noise)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GitHub page**: [NovelAI Aspect Ratio Bucketing](https://github.com/NovelAI/novelai-aspect-ratio-bucketing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Appendix**: [CLIP](#13e9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you’re curious to explore the mechanics behind Stable Diffusion further,
    take a peek at my earlier article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----a83d64348d90--------------------------------)
    [## Paper Explained — High-Resolution Image Synthesis with Latent Diffusion Models'
  prefs: []
  type: TYPE_NORMAL
- en: While OpenAI has dominated the field of natural language processing with their
    generative text models, their image…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----a83d64348d90--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipeline: Stable Diffusion Image-to-Image (SDEdit)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unraveling the Theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes we would like to start from a starting image, which can also be made
    up of our coarse colored strokes, and generate another image that respects the
    structure of the initial image but whose content is determined by the textual
    prompt. The simplest technique to achieve this is [**SDEdit**](https://arxiv.org/abs/2108.01073),
    which corresponds to the **Image-to-Image** pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c413fe10cf4ff56171e7ed31e2fd2402.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 2 from [SDEdit: Guided Image Synthesis and Editing with Stochastic Differential
    Equations](https://arxiv.org/abs/2108.01073)'
  prefs: []
  type: TYPE_NORMAL
- en: During the diffusion process, rather than beginning from random noise, there’s
    nothing stopping us from starting from a later step of the forward process, where
    we generate the input by incorporating noise based on the chosen starting timestep.
    As you can see in the figure above, even in the instance of **strokes**, the addition
    of noise enables the resulting image to be included within the distribution of
    typical images. This final point is important as it enable training the model
    solely with images, but then employing our strokes as input during inference.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that this technique presents a trade-off between **faithfulness**
    and **realism**, depending on which point of the forward process we start from.
    In practice, if we use a “strength” parameter equal to 1 during generation, the
    input image will essentially be ignored, while if “strength” is equal to 0, we
    will get the same image. The current default is `strength=0.8`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally SDEdit can also be used for **inpainting**, simply by masking the portion
    of the image that you do not want to modify.
  prefs: []
  type: TYPE_NORMAL
- en: Applications and Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[StableDiffusionImg2ImgPipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img#diffusers.StableDiffusionImg2ImgPipeline)
    is a good pipeline to use when you want to generate an image from some strokes
    or modify a starting image based on a textual prompt. It’s worth noting that the
    main limitation of this technique is that you can’t request significant variations
    in the structure of the generated image through the textual prompt. The generated
    image’s structure will remain conditioned by the starting structure (unless you
    choose a strength value very close to 1).'
  prefs: []
  type: TYPE_NORMAL
- en: Useful Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Diffusers documentation**: [Image-to-image](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific paper**: [SDEdit: Guided Image Synthesis and Editing with Stochastic
    Differential Equations](https://arxiv.org/abs/2108.01073)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pipeline: Stable Diffusion Image Variation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unraveling the Theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[StableDiffusionImageVariationPipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/image_variation#diffusers.StableDiffusionImageVariationPipeline)
    is a pipeline developed by [Lambda](https://lambdalabs.com/) which, like Image-to-Image,
    allows generating variations of an input image.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/78c8df9d7b1cc06eb090485d676ea1ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from the [Stable Diffusion Image Variations Model Card](https://huggingface.co/lambdalabs/sd-image-variations-diffusers).
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, in a task such as text-to-image, generation is conditioned by a
    textual prompt that is transformed into an embedding through a dedicated encoder.
    As you can check in [Appendix — CLIP](#13e9), CLIP has two encoders: one for text
    and one for images. Both of these encoders map the input in such a way that text
    describing an image has an embedding close to those that don’t describe it, and
    vice versa. This pipeline simply **replaces the CLIP text encoder with the CLIP
    image encoder**. In this way, generation isn’t governed by a textual prompt, but
    rather by an image that will be decoded by the model not exactly into itself,
    but into a variant. This is unless the model has overfit to that specific concept
    and can reproduce it exactly from its latent representation.'
  prefs: []
  type: TYPE_NORMAL
- en: Applications and Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is an interesting pipeline for obtaining images similar to the input. The
    generated images may not necessarily retain the exact structure of the original
    image, as with image-to-image approaches, but they might, for example, retain
    its style or key subject characteristics. One of the main limitations of this
    technique is that there’s not a great deal of control over the generated variations.
  prefs: []
  type: TYPE_NORMAL
- en: Useful Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Diffusers documentation**: [Image variation](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/image_variation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model card**: [Stable Diffusion Image Variations Model Card](https://huggingface.co/lambdalabs/sd-image-variations-diffusers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Appendix**: [CLIP](#13e9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pipeline: Stable Diffusion Upscale'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unraveling the Theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[StableDiffusionUpscalePipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline)
    is a **super-resolution** pipeline that enhances the resolution of input images
    **by a factor of 4**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9bc91302fb8eb954d59340d66f22b5e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from the [Stable Diffusion x4 Upscaler Model Card](https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler).
  prefs: []
  type: TYPE_NORMAL
- en: The method employed, previously introduced in the original Latent Diffusion
    paper, involves **concatenating the low-resolution image with the latents generated
    by the VAE encoder**. The model is then trained to generate the high-resolution
    image based on this input. This model was created by the researchers and engineers
    from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/),
    and [LAION](https://laion.ai/).
  prefs: []
  type: TYPE_NORMAL
- en: Applications and Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The application of this pipeline is quite straightforward: increasing the resolution
    of an input image.'
  prefs: []
  type: TYPE_NORMAL
- en: Useful Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Diffusers documentation**: [Super-resolution](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/upscale)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model card**: [Stable Diffusion x4 Upscaler Model Card](https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pipeline: Stable Diffusion Latent Upscale'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unraveling the Theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unfortunately, I didn’t find many references about this **latent upscaler**
    trained by [Katherine Crowson](https://github.com/crowsonkb) in collaboration
    with [Stability AI](https://stability.ai/). In any case, I think it’s a safe bet
    to assume it was trained in a similar way to how the super-resolution model was
    trained. So, what’s the difference? This model, instead of only accepting an image,
    also accepts latents. It can be used directly on the latents generated in a previous
    step, without needing to start from an image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6cc20941a2f8dd9db645dda8a50c2283.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Tanishq Abraham from [Stability AI](https://stability.ai/) originating
    from [this tweet](https://twitter.com/StabilityAI/status/1590531958815064065).
  prefs: []
  type: TYPE_NORMAL
- en: The [StableDiffusionLatentUpscalePipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/latent_upscale#diffusers.StableDiffusionLatentUpscalePipeline)
    enhances the resolution of input images **by a factor of 2**.
  prefs: []
  type: TYPE_NORMAL
- en: Applications and Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This pipeline can serve as an alternative to the super-resolution pipeline when
    we intend to start from latents rather than an image.
  prefs: []
  type: TYPE_NORMAL
- en: Useful Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Diffusers documentation**: [Latent upscaler](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/latent_upscale#diffusers.StableDiffusionLatentUpscalePipeline)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model card**: [Stable Diffusion x2 Latent Upscaler Model Card](https://huggingface.co/stabilityai/sd-x2-latent-upscaler)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pipeline: unCLIP (Karlo/**DALL·E-2**)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unraveling the Theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You’ve probably heard of [unCLIP](https://huggingface.co/docs/diffusers/api/pipelines/unclip)
    under a different name: DALL·E-2\. The version present in Diffusers is derived
    from [kakaobrain](https://kakaobrain.com/)’s [Karlo](https://github.com/KAKAOBRAIN/KARLO).'
  prefs: []
  type: TYPE_NORMAL
- en: Less describe how the original **unCLIP** work.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3fdaf40b1f8416fbd3777cf8976aac01.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 2 from [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125).
  prefs: []
  type: TYPE_NORMAL
- en: 'To grasp unCLIP, it’s important to know about CLIP. If you’re not familiar
    with CLIP, feel free to check out the [Appendix — CLIP](#13e9). In the image above,
    the portion above the dashed line represents CLIP itself. Below, we observe unCLIP.
    unCLIP employs a model called a “prior” to predict the CLIP image embedding from
    the CLIP text embedding of the provided prompt. The predicted CLIP image embedding
    is then input into a decoder, that transforms it into an image. This generated
    image is subsequently enlarged twice using two upsamplers: first from 64x64 to
    256x256, then from 256x256 to 1024x1024.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The paper describes the “**prior**” model as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the diffusion prior, we train a decoder-only Transformer with a causal
    attention mask on a sequence consisting of, in order: the encoded text, the CLIP
    text embedding, an embedding for the diffusion timestep, the noised CLIP image
    embedding, and a final embedding whose output from the Transformer is used to
    predict the unnoised CLIP image embedding. […]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Well, quite confusing, isn’t it? First question: **why do we need a prior model
    at all?** Wasn’t CLIP trained to make text embeddings close to their respective
    image embeddings? Why can’t we use them directly? Well, we can, as shown in the
    paper, but the results will be worse (though not terribly worse). In short, while
    the embedding of “dog” will be closer to the embedding of the image “dog” than
    to that of the image “cat”, the clusters of text embeddings and image embeddings
    do not overlap but maintain a gap between them. This phenomenon is quite complex,
    and if you’re interested in delving into it, I suggest you take a look at “[Mind
    the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation
    Learning](https://arxiv.org/abs/2203.02053)”. That said, although we understand
    that there isn’t a strict equivalence between image and text embeddings, where
    only the same concept in both modes is closer than different concepts, there isn’t,
    in my opinion, a strong theoretical reason why directly using the text embedding
    shouldn’t yield analogous results — it’s more of an experimental matter.'
  prefs: []
  type: TYPE_NORMAL
- en: Alright, they employed a Transformer instead of a U-Net for this diffusion process
    (since the aim here is to predict a 1D embedding rather than an image). However,
    **why they utilized a causal attention mask?** I’m unsure about this, and even
    the skilled [luciddrains](https://github.com/lucidrains) who adapted [DALL·E-2
    to PyTorch](https://github.com/lucidrains/DALLE2-pytorch) doesn’t appear to have
    a solid rationale for it. You can find his response on this topic [here](https://github.com/lucidrains/DALLE2-pytorch/issues/46).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another doubt you might have is: **how** on earth do **we input the noised
    CLIP image embedding** if the CLIP image embedding is exactly what we want to
    predict? To answer this question, it’s sufficient to remember that we are dealing
    with an iterative diffusion process, where at the beginning, the noised image
    embedding will simply be… noise.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, two other tricks.
  prefs: []
  type: TYPE_NORMAL
- en: The first one is to **predict** not one but **two CLIP image embeddings** **and**
    then **choose the one closer to the CLIP text embedding**.
  prefs: []
  type: TYPE_NORMAL
- en: The second trick is the use of [classifier-free guidance](https://arxiv.org/abs/2207.12598).
    **Classifier-free guidance** is now a technique used by practically all diffusion
    models, including Stable Diffusion. During training, it involves occasionally
    removing text conditioning (in this specific case, 10% of the time). During inference,
    this means generating one sample with text conditioning and another without it.
    The difference between the two provides us with the direction in which we want
    to guide the model (the direction given by our textual prompt). This difference
    can be used to adjust the sample for the next step in the diffusion process.
  prefs: []
  type: TYPE_NORMAL
- en: The **decoder** is inspired by the architecture of [**GLIDE**](https://arxiv.org/abs/2112.10741)
    (**G**uided **L**anguage to **I**mage **D**iffusion for Generation and **E**diting),
    to which a conditioning based on CLIP embeddings is added. GLIDE, in turn, is
    inspired by [**ADM**](https://arxiv.org/abs/2105.05233) (**A**blated **D**iffusion
    **M**odel), to which it adds text conditioning by encoding the prompt with a Transformer.
    ADM is an enhanced U-Net with additional attention layers and other enhancements
    compared to the version used in the paper that introduced popular diffusion models.
  prefs: []
  type: TYPE_NORMAL
- en: The **upsamplers** are also diffusion models (ADM), where **noise is added to
    the conditioning** through the low-resolution image to make them more robust.
  prefs: []
  type: TYPE_NORMAL
- en: Alright, up until now, we have discussed the original unCLIP/DALL·E-2\. However,
    we pointed out that the implementation found in Diffusers is derived from Karlo.
    So, **what are the differences between Karlo and DALL·E-2?** The main architectural
    distinction between Karlo and DALL·E-2 is that **Karlo** includes an enhancement
    in the super-resolution module designed to upscale from 64px to 256px. This enhancement
    involves a process consisting of only 7 steps. After the initial 6 steps performed
    using the standard super-resolution module, the additional super-resolution module
    is further fine-tuned using a [VQGAN](https://arxiv.org/abs/2012.09841)-style
    loss, see [Appendix — VQGAN](#9eb6).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it’s important to emphasize that while Karlo shares a very similar
    architecture, **it is not the original OpenAI’s DALL·E-2**. Karlo and DALL·E-2were
    trained on different datasets, and there might also be variations in other training
    details. Consequently, the generated outputs from Karlo could potentially exhibit
    significant differences in quality when compared to those produced by the original
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Applications and Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The limitations and applications of unCLIP are more or less similar to those
    of Stable Diffusion. One additional possibility provided by this model is that
    tasks like generating **variations** of a starting image become trivial: take
    an image, pass it through the CLIP text encoder, and decode it through the unCLIP
    decoder. You might now be asking yourself the question: **Is Stable Diffusion
    better, or is unCLIP, or other models that we are about to see?**'
  prefs: []
  type: TYPE_NORMAL
- en: The answer to this question is not straightforward. First, as of today, **there
    are no robust metrics to automatically measure the performance of these models**.
    If you’re interested, I can write another article about it, but for now, know
    that close to metrics like [**Fréchet inception distance**](https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance)(**FID**),
    the best papers always report **human evaluations** for this reason. Second, as
    we say in Italy, “Non è bello quel che è bello ma è bello ciò che piace” (what’s
    beautiful is not what’s beautiful, but what’s liked), meaning that beauty is relative,
    and different people might prefer the “style” of different models depending on
    their tastes and the use of the images in question.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here one data point to let you judge what you prefer between the text-to-image
    models presented in this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5f0e244d094c05ebcef84afca5935ef7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From left to right: [SD-XL 1.0 Base](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0),
    [Karlo v1 alpha](https://huggingface.co/kakaobrain/karlo-v1-alpha) (unCLIP) and
    [Kandinsky 2.2](https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder).'
  prefs: []
  type: TYPE_NORMAL
- en: I generated the images using the prompt “Astronaut in a jungle, cold color palette,
    muted colors, detailed, 8k”, choosing my favorite image out of four generations,
    while keeping all parameters as default. I didn’t include DeepFloyd IF because
    it requires accepting specific terms and conditions to be used.
  prefs: []
  type: TYPE_NORMAL
- en: In this specific case, in my opinion, the result from SD-XL is the best, closely
    followed by Kandinsky 2, while the unCLIP output is the least preferable, even
    considering that the remaining three images, not included here, were significantly
    worse. It’s worth noting that the default image size of unCLIP (Karlo) is 256x256,
    whereas SD-XL generates 1024x1024 images and Kandinsky 2 generates 512x512 images
    (if we use the Diffusers implementations of these models).
  prefs: []
  type: TYPE_NORMAL
- en: As a final disclaimer, please be aware that this test is conducted using only
    one specific prompt and without utilizing other available parameters to control
    the generation. Each model possesses unique strengths and can produce outputs
    that are more or less appealing depending on the subject. Considering that we’re
    discussing altering just a few lines of code, I highly recommend **experimenting
    with all of them before determining which one aligns best with your requirements**.
  prefs: []
  type: TYPE_NORMAL
- en: Useful Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Diffusers documentation**: [unCLIP](https://huggingface.co/docs/diffusers/api/pipelines/unclip)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific paper**: [Hierarchical Text-Conditional Image Generation with
    CLIP Latents](https://arxiv.org/abs/2204.06125) (unCLIP/DALL·E-2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific paper**: [Mind the Gap: Understanding the Modality Gap in Multi-modal
    Contrastive Representation Learning](https://arxiv.org/abs/2203.02053)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific paper**: [GLIDE: Towards Photorealistic Image Generation and Editing
    with Text-Guided Diffusion Models](https://arxiv.org/abs/2112.10741)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific paper**: [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific paper**: [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GitHub page**: [Karlo](https://github.com/kakaobrain/karlo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GitHub page**: [DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Appendix**: [CLIP](#13e9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Appendix**: [VQGAN](#9eb6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pipeline: DeepFloyd IF (Imagen)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unraveling the Theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[DeepFloyd IF](https://www.deepfloyd.ai/deepfloyd-if) is a model inspired by
    [**Imagen**](https://imagen.research.google/), a Google text-to-image model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/846c2621afec019bdb4787a4c122b4fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from the [Google blog post about Imagen](https://imagen.research.google/).
  prefs: []
  type: TYPE_NORMAL
- en: We have already seen all the elements of these models; both use a text-to-image
    diffusion model that generates a low-resolution image, 64x64\. This image is then
    upscaled to higher resolutions, first to 256x256 and then to 1024x1024 by other
    two models.
  prefs: []
  type: TYPE_NORMAL
- en: As the **text encoder**, both models employ a large pre-trained [**Text-To-Text
    Transfer Transformer**](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)
    (**T5**) by Google, which reframe all NLP tasks into a unified text-to-text format
    where input and output are always text strings. The text encoder used appears
    to be a crucial element in DeepFloyd IF/Imagen, as T5 possesses a broader language
    understanding compared to CLIP.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the previously presented models, also in this case, diffusion models
    are implemented as U-Nets. For super-resolution models, Imagen introduces an **Efficient
    U-Net**, claimed to be simpler, faster converging, and more memory-efficient compared
    to prior implementations. The changes made to the U-Net in comparison to previous
    diffusion models involve “shifting” some of the parameters from high-resolution
    blocks to low-resolution ones (which have more channels and contain more semantic
    knowledge respect to initial blocks), using more residual blocks at low resolution,
    and altering the order of convolution operations concerning up/downsampling. In
    the case of Imagen, downsampling is performed before convolution, and the opposite
    is true for upsampling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, Imagen emphasizes the significance of **classifier-free guidance**.
    According to the paper:'
  prefs: []
  type: TYPE_NORMAL
- en: We corroborate the results of recent text-guided diffusion work [ 16 , 41 ,
    54] and find that increasing the classifier-free guidance weight improves image-text
    alignment, but damages image fidelity producing highly saturated and unnatural
    images [ 27 ].
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To benefit from improved image-text alignment without compromising fidelity,
    two types of thresholding are discussed. The first, previously used by other works,
    is **static thresholding**, which clips the x-prediction to the range [-1, 1],
    the same range as the training data x. It is exactly the discrepancy between what
    the model has seen during training and what it encounters during inference that
    causes the issue. Static thresholding is essential with large guidance weights
    but still results in oversaturated and less detailed images as the weight increase.
    Therefore, the authors introduce **dynamic thresholding**. This technique involves
    initially choosing a certain percentile of absolute pixel value, let’s say 80%.
    If the value of this percentile, s, exceeds 1 (i.e., more than 20% of pixels are
    greater than 1 in absolute value), all pixels outside the range [-s, s] are clipped.
    After this, the values are scaled by s, bringing everything into the range [-1,
    1]. Discarding the extreme pixels before normalization helps mitigate the oversaturation
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: '**DeepFloyd IF** seems to closely resemble Imagen, but without a paper that
    delves into the details of this architecture, it’s uncertain whether there are
    any significant modifications I might have missed. According to the authors, DeepFloyd
    IF outperforms the original Imagen.'
  prefs: []
  type: TYPE_NORMAL
- en: Applications and Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DeepFloyd IF can be employed for all the mentioned applications. However, unlike
    Stable Diffusion and unCLIP, currently, users need to accept the DeepFloyd LICENSE
    AGREEMENT before utilizing it.
  prefs: []
  type: TYPE_NORMAL
- en: Useful Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Diffusers documentation**: [DeepFloyd IF](https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific blog**: [DeepFloyd IF](https://www.deepfloyd.ai/deepfloyd-if)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific blog**: [Imagen](https://imagen.research.google/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific blog**: [Exploring Transfer Learning with T5: the Text-To-Text
    Transfer Transformer](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pipeline: Kandinsky'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unraveling the Theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kandinsky is an [AI Forever](https://github.com/ai-forever) model that inherits
    best practices from DALL·E-2 and Latent Diffusion while introducing some new ideas.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e78587e375847c021a4e0970b2143ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from the Kandinsky GitHub page.
  prefs: []
  type: TYPE_NORMAL
- en: Just like DALL·E-2, Kandinsky incorporates a **prior** model (**Diffusion Mapping**)
    to predict CLIP image embeddings based on CLIP text embeddings. Furthermore, akin
    to Latent Diffusion, the diffusion model doesn’t **operate** in pixel space like
    DALL·E-2/Imagen, but rather **in the** **latent space**.
  prefs: []
  type: TYPE_NORMAL
- en: An important difference is that the latest versions, 2.2 and 2.1, of Kandinsky
    use [**XLM-RoBERTa**](https://arxiv.org/abs/1911.02116) as the text encoder, thus
    making the model **multilingual**.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to DALL·E-2, **the output of the prior model doesn’t go directly
    into a decoder**; instead, it’s first directed towards a latent diffusion model.
  prefs: []
  type: TYPE_NORMAL
- en: The **decoder** is [**MoVQ**](https://arxiv.org/abs/2209.09002), a model similar
    to VQGAN (refer to [Appendix — VQGAN](#9eb6)), which is improved by incorporating
    **spatially conditional normalization** to tackle the issue of mapping similar
    neighboring patches to the same codebook index. This addressing prevents the occurrence
    of repeated artifacts in regions with similar adjacent content. Moreover, the
    model incorporates multichannel quantization to enhance its flexibility. For the
    second stage, the autoregressive transformer is replaced with a significantly
    faster, thanks to its parallel rather than sequential nature, [**Masked Generative
    Image Transformer**](https://arxiv.org/abs/2202.04200) (**MaskGIT**).
  prefs: []
  type: TYPE_NORMAL
- en: Applications and Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have already seen an output from Kandinsky; this model is undoubtedly among
    the most promising at this moment and competes with the best current available
    diffusion models. Its usage and limitations are similar to those of Stable Diffusion.
  prefs: []
  type: TYPE_NORMAL
- en: Useful Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Diffusers documentation**: [Kandinsky](https://huggingface.co/docs/diffusers/api/pipelines/kandinsky)/[Kandinsky
    2.2](https://huggingface.co/docs/diffusers/api/pipelines/kandinsky_v22)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific paper**: [Unsupervised Cross-lingual Representation Learning at
    Scale](https://arxiv.org/abs/1911.02116) (XLM-RoBERTa)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific paper**: [MoVQ: Modulating Quantized Vectors for High-Fidelity
    Image Generation](https://arxiv.org/abs/2209.09002)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific paper**: [MaskGIT: Masked Generative Image Transformer](https://arxiv.org/abs/2202.04200)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific blog**: [Kandinsky 2.1, or When +0.1 means a lot](https://habr.com/ru/companies/sberbank/articles/725282/)
    (thanks Google Translate!)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GitHub page**: [Kandinsky 2.2](https://github.com/ai-forever/Kandinsky-2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Appendix**: [CLIP](#13e9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Appendix**: [VQGAN](#9eb6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pipeline: ControlNet'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unraveling the Theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[**ControlNet**](https://arxiv.org/abs/2302.05543) is a technique for conditioning
    the generation of a diffusion model, controlling the structure of what is generated.
    To some extent, and although the two techniques are complementary, it’s like an
    enhanced SDEdit. The main idea is to automatically generate conditional inputs
    such as edge maps, segmentation maps, keypoints, etc., and then teach the diffusion
    model to generate outputs that adhere to the structure of these conditioning inputs.
    In the original ControlNet paper, Stable Diffusion is used as the base, but this
    technique can be applied to any model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/939630e907f176840ec1ac9429c82de8.png)'
  prefs: []
  type: TYPE_IMG
- en: Firstly, a copy of the original model is created. The original model is frozen,
    while the copy is linked to it through a series of zero convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: A **zero convolution** is simply a 1x1 convolution where both the weights and
    biases are initialized with zero. This type of initialization, along with the
    fact that the weights of the original model are frozen, ensures that initially,
    the system is identical to the starting model and only gradually starts to use
    the conditioning to guide the generation, without forgetting what was originally
    learned during the extensive training process.
  prefs: []
  type: TYPE_NORMAL
- en: Conditioning involves some form of processing (usually automated) of the input.
    For instance, we can extract edges from the initial image using a [Canny edge
    detector](https://en.wikipedia.org/wiki/Canny_edge_detector) and teach the model
    to generate variations faithful to the structure of the original image but with
    different characteristics that can be guided through textual prompts.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/81808fa8aeb080241d90a17cd22d7837.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The potential **conditioning inputs** are limited only by our imagination;
    the authors mention more than a dozen, and over time, the community is inventing
    new ones. To name a few: edges (e.g., extracted with [Canny](https://en.wikipedia.org/wiki/Canny_edge_detector)),
    human poses (e.g., extracted with [OpenPifPaf](https://openpifpaf.github.io/intro.html)
    or [OpenPose](https://cmu-perceptual-computing-lab.github.io/openpose/web/html/doc/index.html)),
    semantic maps, depth maps, and so on. Clearly, during training, automation of
    the extraction is important to speed up the creation of the initial dataset. During
    inference, there’s no restriction on hand-drawing a segmentation maps or even
    sketching what we want, as it’s possible to provide a similar input automatically
    during training using [HED](https://arxiv.org/abs/1504.06375) boundary detection
    and a range of robust data augmentations or alternative techniques, mimicking
    human sketches.'
  prefs: []
  type: TYPE_NORMAL
- en: Applications and Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ControlNet is one of the tools that those who appreciate generative art must
    have in their toolkit. It’s possible to train your own ControlNet from scratch
    with reasonably limited resources, but often it’s not necessary, and you can use
    ControlNets already trained by the community. The main limitation of this technique
    is that conditioning often depends on having a starting image with a structure
    similar to the desired result, or the ability to manually produce an equivalent
    conditioning. Finally, it’s worth noting that it’s also possible to [combine multiple
    ControlNets](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#combining-multiple-conditionings),
    conditioning, for example, one part of an image on edges and another on a human
    pose.
  prefs: []
  type: TYPE_NORMAL
- en: Useful Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Diffusers documentation**: [ControlNet](https://huggingface.co/docs/diffusers/api/pipelines/controlnet)/[ControlNet
    with Stable Diffusion XL](https://huggingface.co/docs/diffusers/api/pipelines/controlnet_sdxl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific paper**: [Adding Conditional Control to Text-to-Image Diffusion
    Models](https://arxiv.org/abs/2302.05543) (ControlNet)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific blog**: [Ultra fast ControlNet with 🧨 Diffusers](https://huggingface.co/blog/controlnet)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pipeline: InstructPix2Pix'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unraveling the Theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[**InstructPix2Pix**](https://arxiv.org/abs/2211.09800) is a method for teaching
    a generative model to follow human-written instructions for image editing.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6cda1261db244191c6ca7de661a888cd.png)'
  prefs: []
  type: TYPE_IMG
- en: The method consists of three phases. Firstly, generate a set of input captions,
    edit instructions, and edited captions. Then, employ another technique called
    Prompt-to-Prompt (see [Appendix — Prompt-to-Prompt](#ec8a)) to generate a dataset
    of image pairs associated with the input and edited captions. Finally, train a
    generative model to produce the requested modification based on the given instruction.
  prefs: []
  type: TYPE_NORMAL
- en: The **instructions and edited captions**, as shown in the figure, **are generated
    semi-automatically**. [GPT-3](https://arxiv.org/abs/2005.14165), the powerful
    language model by OpenAI, is fine-tuned on a small sample of LAION captions, to
    which manually crafted edit instructions and resulting edited captions are added.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have all the components to **generate variations of the original
    images using Prompt-to-Prompt**. An important aspect here is that, depending on
    the type of instruction given, it may be required for the generated image to remain
    more or less faithful to the original image. For instance, consider the difference
    between requesting “make the hair blonde” versus “make it a Miro painting”. Fortunately,
    Prompt-to-Prompt has a parameter to adjust how much attention should be given
    to the original image versus the prompt. Unfortunately, this parameter varies
    case by case.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address this issue, InstructPix2Pix generates 100 pairs of images for each
    caption in the training set, varying this parameter. These **pairs are** **then
    filtered** using a CLIP-based metric: [**directional similarity in CLIP**](https://arxiv.org/abs/2108.00946).
    This metric measures how consistent the change between two images (in CLIP space)
    is with the change between the two image captions. Besides enhancing the quality
    of the generated dataset, this filtering also enhances the model’s robustness
    to Prompt-to-Prompt and Stable Diffusion failures.'
  prefs: []
  type: TYPE_NORMAL
- en: To input the text edit instruction, the authors reuse the same text conditioning
    mechanism that was initially intended for captions. Meanwhile, for the input image
    to be modified, they simply add input channels to the first convolutional layer.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, they employ a form of **classifier-free diffusion guidance** to weigh
    the image more or less with respect to the text, allowing some control over how
    closely it adheres to the input image in comparison to following the edit instruction.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d54d98809c0b1193dfc2c6846e470c74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Eq. 3 from [InstructPix2Pix: Learning to Follow Image Editing Instructions](https://arxiv.org/abs/2211.09800).'
  prefs: []
  type: TYPE_NORMAL
- en: Applications and Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: InstructPix2Pix is a very useful technique when one wants to modify an image
    through text without significantly altering elements unrelated to the requested
    modification. This is different from generating two images where the second one
    has only a slightly modified prompt. Clearly, this technique doesn’t work flawlessly
    100% of the time and encounters issues when asked to change the viewpoint, swap
    object positions, and sometimes, although not as frequently as other techniques,
    it can lead to unintended excessive changes to the image.
  prefs: []
  type: TYPE_NORMAL
- en: Useful Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Diffusers documentation**: [InstructPix2Pix](https://huggingface.co/docs/diffusers/api/pipelines/pix2pix)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific paper**: [InstructPix2Pix: Learning to Follow Image Editing Instructions](https://arxiv.org/abs/2211.09800)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific paper**: [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
    (GPT-3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific paper**: [StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image
    Generators](https://arxiv.org/abs/2108.00946) (directional similarity in CLIP)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Appendix**: [CLIP](#13e9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Appendix**: [Prompt-to-Prompt](#ec8a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CLIP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The fundamental idea behind [**CLIP**](https://openai.com/research/clip) is
    as simple as it is powerful: training two Transformer encoders, one for images
    and one for text, on a dataset of associated images and texts to produce similar
    embeddings when the text refers to the image, and dissimilar embeddings otherwise.
    With respect to the matrix shown in the figure, the objective is to maximize the
    sum of scalar products along the diagonal and minimize those off the diagonal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62c39422a94ea89b7caf7688871b53b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from the [OpenAI blog post about CLIP](https://openai.com/research/clip).
  prefs: []
  type: TYPE_NORMAL
- en: Since the outputs of the encoders are normalized before scalar products, these
    correspond to measuring the [**cosine similarity**](https://en.wikipedia.org/wiki/Cosine_similarity)
    between two vectors, i.e., how much the embeddings “point in the same direction”.
  prefs: []
  type: TYPE_NORMAL
- en: Useful Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Hugging Face documentation**: [CLIP](https://huggingface.co/docs/transformers/model_doc/clip)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific blog**: [CLIP: Connecting text and images](https://openai.com/research/clip)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VQGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, I’ll introduce [VQGAN](https://arxiv.org/abs/2012.09841) and
    touch on [VQVAE](https://arxiv.org/abs/1711.00937).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b1a63f5d379c08a1afece814f0ce45f.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 2\. from [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841).
  prefs: []
  type: TYPE_NORMAL
- en: In the image above, if we consider only *E*, *ẑ*, and *G*, what we have is an
    Autoencoder. VQGAN builds upon **VQVAE** and employs a regularization technique
    known as **Vector Quantization** (**VQ**). For each spatial position of the encoder
    output *ẑ*, the corresponding vector (whose size depends on the number of channels
    in *ẑ*) is substituted with the nearest vector from a learnable “codebook”. This
    effectively limits the possible inputs to the decoder during inference, allowing
    them to only be combinations of the learned “codes” and then quantizing the latent
    space.
  prefs: []
  type: TYPE_NORMAL
- en: The **loss** function employed by **VQVAE**, *L*VQ, comprises three terms.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/84bce52f1fb5bdedc608d9f2885219f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq. 4 from [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841).
  prefs: []
  type: TYPE_NORMAL
- en: The first is a **reconstruction loss**, *L*rec; the second term penalizes the
    codebook when its elements are distant from the outputs of the encoder. The third
    term, also called the “**commitment loss**”, penalizes the encoder when its output
    embeddings are distant from the codes in the codebook (we want the encoder to
    “commit” to a certain codebook).
  prefs: []
  type: TYPE_NORMAL
- en: '**VQGAN** replaces the reconstruction loss with a **perceptual loss**. Specifically,
    it employs the [**Learned Perceptual Image Patch Similarity**](https://arxiv.org/abs/1801.03924)
    (**LPIPS**), which uses a pre-trained VGG16 network to extract features from both
    the generated and target images, and then calculates the differences between these
    features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, it introduces an **adversarial** training procedure with a **patch-based**
    discriminator *D*, aiming to differentiate between real (*x*) and reconstructed
    (*x̂*) images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ccbd7c2793fb7d841f2b695dfad8a46.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq. 5 from [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841).
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete objective is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad3547e09294735a0b0404199e85cc0b.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq. 6 from [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, λ represents an **adaptive weight** calculated using the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38b09aa49805d736485dc03a4427e227.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq. 7 from [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841).
  prefs: []
  type: TYPE_NORMAL
- en: This weight increases as the gradient of *L*rec (which, for VQGAN, corresponds
    to the perceptual loss) with respect to the last layer of the decoder intensifies.
    Conversely, it decreases when the same occurs for *L*GAN. In practice, this means
    that if *L*GAN is too sensitive to the output of the decoder, its importance is
    decreased. Conversely, if the perceptual loss (*L*rec) exhibits strong gradients,
    the importance of *L*GAN is increased, ensuring that a balance is maintained between
    the two. This approach prevents either term from being entirely ignored when one
    of them has strong gradients, thus achieving equilibrium between the two objectives.
  prefs: []
  type: TYPE_NORMAL
- en: VQGAN uses a **two-stage approach**. We have already seen the first stage where
    an encoder, a codebook, and a decoder are learned. In the second stage, as implied
    by the title of the paper referring to “taming Transformers”, this architecture
    uses a **Transformer** to predict autoregressively the indices corresponding to
    codes in the codebook. Since we know the ground truth indices during training
    (those generated by the encoder), we can train the Transformer using maximum likelihood.
    During inference, we don’t utilize the encoder (as we don’t have an input image,
    our goal is to generate one), and we leverage the trained Transformer to generate
    sequences of indices, which are then mapped to codes that the decoder transforms
    into images.
  prefs: []
  type: TYPE_NORMAL
- en: Useful Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Diffusers documentation**: [VQModel](https://huggingface.co/docs/diffusers/api/models/vq)
    (VQVAE)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific paper**: [Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937)
    (VQVAE)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific paper**: [The Unreasonable Effectiveness of Deep Features as a
    Perceptual Metric](https://arxiv.org/abs/1801.03924) (LPIPS)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific paper**: [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841)
    (VQGAN)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt-to-Prompt
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Prompt-to-Prompt** arises from a key observation by the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: we analyze a text-conditioned model in depth and observe that the cross-attention
    layers are the key to controlling the relation between the spatial layout of the
    image to each word in the prompt
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Based on this, the method essentially involves **manipulating the cross-attention
    maps**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b8e5531efd51768982b665c36283193.png)'
  prefs: []
  type: TYPE_IMG
- en: For example, let’s say we want to alter an image generated with the prompt “Photo
    of a cat riding on a bicycle”, replacing the bicycle with a car while keeping
    all other elements as unchanged as possible. In this case, we can generate a new
    image with the updated prompt but fix the cross-attention maps to those of the
    previous prompt, where the weights associated with the word “car” become the ones
    that were originally associated with the word “bicycle”.
  prefs: []
  type: TYPE_NORMAL
- en: With this framework, it’s now possible to do much more than just simple word
    replacements. We can use it to give more or less emphasis to a given word, or
    even add parts to the prompt that weren’t present before. In this case, we reuse
    the attention maps only for the shared parts.
  prefs: []
  type: TYPE_NORMAL
- en: However, keeping the attention maps fixed could overly constrain the geometry
    of the scene, which, for certain modifications to the prompt, might become too
    restrictive. To control how much attention is given to the modification and how
    much of the initial scene geometry is retained, **the injection is limited up
    to a certain timestep** *τ*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0cbfa4e61d7f075735f0e0bed0a3bafe.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq. from p. 7 of [Prompt-to-Prompt Image Editing with Cross Attention Control](https://arxiv.org/abs/2208.01626).
  prefs: []
  type: TYPE_NORMAL
- en: This ensures that after capturing the overall composition of the scene in the
    initial steps, the model can, if needed, alter the geometries in the later steps
    of the diffusion process.
  prefs: []
  type: TYPE_NORMAL
- en: Useful Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Scientific paper**: [Prompt-to-Prompt Image Editing with Cross Attention
    Control](https://arxiv.org/abs/2208.01626)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s summarize what we have covered in this article. To explain what lies behind
    the most popular pipelines of Diffusers, we have learned about diffusion models,
    analyzing key ones such as DDPM, Stable Diffusion, unCLIP (Karlo/DALL·E-2), DeepFloyd
    IF (Imagen), and Kandinsky. Additionally, we’ve explored techniques for gaining
    greater control over image generation, like SDEdit, ControlNet, or InstructPix2Pix.
    To truly understand these techniques, we’ve also examined important non-diffusion
    models like CLIP, VQGAN, or techniques like Prompt-to-Prompt ([with a pipeline
    for the latter that might be ready by the time you read this article](https://github.com/huggingface/diffusers/issues/2121)).
    Finally, diffusion model training is somewhat of an art, which is why we have
    also delved into important tricks like classifier-free guidance, offset noise,
    CLIP filtering, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you found this article helpful. Feel free to share your thoughts in whichever
    way you prefer, I appreciate and consider feedback. If you’d like to show your
    support, sharing this article on social networks is the best way to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First of all, special thanks go to Letitia Parcalabescu of [AI Coffee Break](https://twitter.com/AICoffeeBreak?s=20).
    She was invaluable in two ways: first, her videos ([check them out](https://www.youtube.com/AICoffeeBreak),
    they are great!) were helpful in refreshing or clarifying some concepts for this
    article; and second, she took the time to read the first draft and provide me
    with very valuable feedback. Regarding this point, I would also like to express
    my gratitude to the reviewers at Towards Data Science who are always available
    for any inquiries and, thanks to their insights, improve the quality of the articles
    I write. Finally, thanks to you, reading up to this point is no small feat 😊!'
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for taking the time to read this article, and please feel free to
    leave a comment or connect with me to share your thoughts or ask any questions.
    To stay updated on my latest articles, you can follow me on [Medium](https://medium.com/@mnslarcher),
    [LinkedIn](https://www.linkedin.com/in/mnslarcher/) or [Twitter](https://twitter.com/mnslarcher).
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mnslarcher/membership?source=post_page-----a83d64348d90--------------------------------)
    [## Join Medium with my referral link - Mario Namtao Shianti Larcher'
  prefs: []
  type: TYPE_NORMAL
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@mnslarcher/membership?source=post_page-----a83d64348d90--------------------------------)
  prefs: []
  type: TYPE_NORMAL
