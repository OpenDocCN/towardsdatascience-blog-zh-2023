- en: Comparing and Explaining Diffusion Models in HuggingFace Diffusers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较和解释HuggingFace扩散模型
- en: 原文：[https://towardsdatascience.com/comparing-and-explaining-diffusion-models-in-huggingface-diffusers-a83d64348d90](https://towardsdatascience.com/comparing-and-explaining-diffusion-models-in-huggingface-diffusers-a83d64348d90)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/comparing-and-explaining-diffusion-models-in-huggingface-diffusers-a83d64348d90](https://towardsdatascience.com/comparing-and-explaining-diffusion-models-in-huggingface-diffusers-a83d64348d90)
- en: DDPM, Stable Diffusion, DALL·E-2, Imagen, Kandinsky 2, SDEdit, ControlNet, InstructPix2Pix,
    and more
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DDPM、稳定扩散、DALL·E-2、Imagen、康定斯基2、SDEdit、ControlNet、InstructPix2Pix等
- en: '[](https://mnslarcher.medium.com/?source=post_page-----a83d64348d90--------------------------------)[![Mario
    Larcher](../Images/b5b443807fe06f096ed4fc5139b3cb42.png)](https://mnslarcher.medium.com/?source=post_page-----a83d64348d90--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a83d64348d90--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a83d64348d90--------------------------------)
    [Mario Larcher](https://mnslarcher.medium.com/?source=post_page-----a83d64348d90--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mnslarcher.medium.com/?source=post_page-----a83d64348d90--------------------------------)[![Mario
    Larcher](../Images/b5b443807fe06f096ed4fc5139b3cb42.png)](https://mnslarcher.medium.com/?source=post_page-----a83d64348d90--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a83d64348d90--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a83d64348d90--------------------------------)
    [Mario Larcher](https://mnslarcher.medium.com/?source=post_page-----a83d64348d90--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a83d64348d90--------------------------------)
    ·33 min read·Aug 24, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a83d64348d90--------------------------------)
    ·33分钟阅读·2023年8月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/62e4b9c064053a36fd3c580645994bbc.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62e4b9c064053a36fd3c580645994bbc.png)'
- en: Image generated with Diffusers. Continue reading to discover how and the theory
    behind.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用扩散器生成的图像。继续阅读以发现生成方法及其背后的理论。
- en: Table of Contents
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: '[Introduction](#6595)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍](#6595)'
- en: '[Prerequisites and Suggested Materials](#a412)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[前提条件和建议材料](#a412)'
- en: '[Diffusers Pipelines](#8611)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[扩散器管道](#8611)'
- en: '[Pipeline: DDPM (Diffusion Models)](#0693)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[管道：DDPM（扩散模型）](#0693)'
- en: '[Pipeline: Stable Diffusion Text-to-Image](#4aeb)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[管道：稳定扩散文本到图像](#4aeb)'
- en: '[Pipeline: Stable Diffusion Image-to-Image (SDEdit)](#8692)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[管道：稳定扩散图像到图像（SDEdit）](#8692)'
- en: '[Pipeline: Stable Diffusion Image Variation](#246c)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[管道：稳定扩散图像变异](#246c)'
- en: '[Pipeline: Stable Diffusion Upscale](#f3b1)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[管道：稳定扩散放大](#f3b1)'
- en: '[Pipeline: Stable Diffusion Latent Upscale](#ed67)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[管道：稳定扩散潜在放大](#ed67)'
- en: '[Pipeline: unCLIP (Karlo/DALL·E-2)](#a504)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[管道：unCLIP（Karlo/DALL·E-2）](#a504)'
- en: '[Pipeline: DeepFloyd IF (Imagen)](#730d)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[管道：DeepFloyd IF（Imagen）](#730d)'
- en: '[Pipeline: Kandinsky](#f933)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[管道：康定斯基](#f933)'
- en: '[Pipeline: ControlNet](#695a)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[管道：ControlNet](#695a)'
- en: '[Pipeline: Instruct Pix2Pix](#ce6b)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[管道：指导Pix2Pix](#ce6b)'
- en: '[Appendix — CLIP](#13e9)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[附录 — CLIP](#13e9)'
- en: '[Appendix — VQGAN](#9eb6)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[附录 — VQGAN](#9eb6)'
- en: '[Appendix — Prompt-to-Prompt](#ec8a)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[附录 — 提示到提示](#ec8a)'
- en: '[Conclusions](#59d1)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[结论](#59d1)'
- en: '[Acknowledgments](#ffc4)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[致谢](#ffc4)'
- en: Introduction
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Embracing the ever-growing interest in Generative AI, including image generation,
    many excellent resources are starting to become available, some of which I’ll
    highlight below. However, based on my experience, progressing beyond foundational
    courses demands significant effort, as resources on advanced topics become more
    scattered.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对生成性AI，包括图像生成的兴趣日益增长，许多优秀的资源开始变得可用，其中一些我将在下文中突出介绍。然而，根据我的经验，超越基础课程的进展需要付出大量的努力，因为高级主题的资源变得更加零散。
- en: In this article, we will list the most popular diffusion models from the [Hugging
    Face](https://huggingface.co/) [Diffusers](https://huggingface.co/docs/diffusers/index)
    library, which is the primary tool for utilizing this technology. We’ll provide
    brief explanations of these models, compare them, and outline their strengths
    and weaknesses.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将列出来自 [Hugging Face](https://huggingface.co/) [Diffusers](https://huggingface.co/docs/diffusers/index)
    库的最流行的扩散模型，这是使用这项技术的主要工具。我们将简要解释这些模型，比较它们，并概述它们的优缺点。
- en: 'The structure of this article is as follows: we will start by reviewing a few
    valuable resources for those who are just beginning to study diffusion models.
    Afterward, we’ll provide a brief explanation of the HuggingFace pipelines. Finally,
    we will delve deep into each pipeline listed in the [Popular Tasks & Pipelines](https://github.com/huggingface/diffusers#popular-tasks--pipelines)
    section of the Diffusers GitHub repository.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的结构如下：我们将首先回顾一些对刚开始学习扩散模型的人员有价值的资源。之后，我们将简要解释 HuggingFace 的管道。最后，我们将深入探讨 [流行任务与管道](https://github.com/huggingface/diffusers#popular-tasks--pipelines)
    部分中列出的每个管道。
- en: By the end of this article, I hope you will have a solid grasp of the primary
    diffusion models and associated techniques, and will be in a good position to
    apply them effectively.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 到本文末尾，我希望你对主要的扩散模型及相关技术有一个扎实的掌握，并能够有效地应用它们。
- en: Prerequisites and Suggested Materials
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 先决条件和建议材料
- en: To fully appreciate this article, though I will strive to keep the explanations
    at an intuitive level, I recommend having a general background in these topics.
    In this section, I list three resources that I have found useful in my own journey.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分理解本文，尽管我会尽力保持解释的直观性，但我建议对这些主题有一个基本的背景。在这一部分，我列出了我在自己学习过程中发现有用的三个资源。
- en: Practical Deep Learning for Coders — Part 2
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践深度学习编程者 - 第二部分
- en: '[](https://course.fast.ai/Lessons/part2.html?source=post_page-----a83d64348d90--------------------------------)
    [## Practical Deep Learning for Coders - Part 2 overview'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://course.fast.ai/Lessons/part2.html?source=post_page-----a83d64348d90--------------------------------)
    [## 实践深度学习编程者 - 第二部分概述](https://course.fast.ai/Lessons/part2.html?source=post_page-----a83d64348d90--------------------------------)'
- en: In this course, containing over 30 hours of video content, we implement the
    astounding Stable Diffusion algorithm from…
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在这门包含超过 30 小时视频内容的课程中，我们实现了令人惊叹的稳定扩散算法……
- en: course.fast.ai](https://course.fast.ai/Lessons/part2.html?source=post_page-----a83d64348d90--------------------------------)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: course.fast.ai](https://course.fast.ai/Lessons/part2.html?source=post_page-----a83d64348d90--------------------------------)
- en: Undoubtedly one of my favorite resources, this course not only provides essential
    insights into diffusion models but also serves as an excellent entry point to
    acquiring fundamental programming skills in Python and deep learning. [Jeremy
    Howard](https://jeremy.fast.ai/), the instructor, adopts a highly effective approach
    by starting with practical applications before delving into the theoretical intricacies.
    This approach ensures a clear understanding without overwhelming learners with
    complex mathematical formulas typically encountered in standard courses.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这无疑是我最喜欢的资源之一，这门课程不仅提供了对扩散模型的基本见解，还作为获取 Python 和深度学习基础编程技能的绝佳入门点。[Jeremy Howard](https://jeremy.fast.ai/)
    教授采用了一种高效的方法，通过从实际应用开始，再深入理论复杂性。这种方法确保了清晰的理解，而不会使学习者被通常在标准课程中遇到的复杂数学公式所困扰。
- en: Moreover, the course serves as a seamless continuation from [Part 1](https://course.fast.ai/),
    requiring no special prerequisites beyond its predecessor. Whether you’re a novice
    or a seasoned learner, this course is a valuable asset in your journey of mastering
    deep learning and diffusion models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，本课程作为 [第 1 部分](https://course.fast.ai/) 的无缝延续，不需要额外的特殊先决条件。无论你是新手还是经验丰富的学习者，这门课程都是掌握深度学习和扩散模型过程中宝贵的资产。
- en: Hugging Face Diffusion Models Course
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Face 扩散模型课程
- en: '[](https://github.com/huggingface/diffusion-models-class?source=post_page-----a83d64348d90--------------------------------)
    [## GitHub - huggingface/diffusion-models-class: Materials for the Hugging Face
    Diffusion Models Course'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/huggingface/diffusion-models-class?source=post_page-----a83d64348d90--------------------------------)
    [## GitHub - huggingface/diffusion-models-class: Hugging Face 扩散模型课程的资料](https://github.com/huggingface/diffusion-models-class?source=post_page-----a83d64348d90--------------------------------)'
- en: 'Materials for the Hugging Face Diffusion Models Course - GitHub - huggingface/diffusion-models-class:
    Materials for the…'
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Hugging Face 扩散模型课程的资料 - GitHub - huggingface/diffusion-models-class: 课程的资料……'
- en: github.com](https://github.com/huggingface/diffusion-models-class?source=post_page-----a83d64348d90--------------------------------)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/huggingface/diffusion-models-class?source=post_page-----a83d64348d90--------------------------------)
- en: In an article about the Diffusers library, it would be crazy not to mention
    the official Hugging Face course. This course, which currently has four lectures,
    dives into diffusion models, teaches you how to guide their generation, tackles
    Stable Diffusion, and wraps up with some cool advanced stuff, including applying
    these concepts to a different realm — audio generation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在关于 Diffusers 库的文章中，不提及官方 Hugging Face 课程简直是疯狂的。这个课程目前有四讲，深入探讨了扩散模型，教你如何引导它们的生成，讨论了稳定扩散，并且最后介绍了一些很酷的高级内容，包括将这些概念应用到另一个领域——音频生成。
- en: Generative Deep Learning, 2nd Edition
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成式深度学习，第2版
- en: '[](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/?source=post_page-----a83d64348d90--------------------------------)
    [## Generative Deep Learning, 2nd Edition'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/?source=post_page-----a83d64348d90--------------------------------)
    [## 生成式深度学习，第2版'
- en: Generative AI is the hottest topic in tech. This practical book teaches machine
    learning engineers and data scientists…
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成式 AI 是科技领域最热门的话题。这本实用书教导机器学习工程师和数据科学家……
- en: www.oreilly.com](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/?source=post_page-----a83d64348d90--------------------------------)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: www.oreilly.com](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/?source=post_page-----a83d64348d90--------------------------------)
- en: For book enthusiasts, this is one of my favorite reads on the subject. As the
    title suggests, this book doesn’t just dive into diffusion models; it also covers
    the broader realm of generative AI. It delves into image generation models like
    Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), which
    are sources of inspiration for diffusion models and used on their side. The second
    edition covers arguments up until the beginning of 2023, exploring more recent
    algorithms like DALL·E-2, CLIP, Imagen, Stable Diffusion, and much more.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于书籍爱好者来说，这是我在这一主题上的最爱之一。正如书名所示，这本书不仅仅探讨了扩散模型；它还涵盖了生成 AI 的广泛领域。它深入研究了图像生成模型，如生成对抗网络（GANs）和变分自编码器（VAEs），这些都是扩散模型的灵感来源并被应用于其中。第二版涵盖了截至2023年初的内容，探讨了如
    DALL·E-2、CLIP、Imagen、稳定扩散等更近期的算法。
- en: If you’ve explored any of these resources or similar ones, you’re well-equipped
    for what lies ahead. If not, you can either go and explore them or proceed in
    any case with the article; I’ll try to keep the explanations as simple as possible,
    I promise.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经探索过这些资源或类似的资源，你已经为接下来的内容做好了充分准备。如果没有，你可以去探索这些资源，或者继续阅读本文；我会尽量保持解释的简洁明了，我保证。
- en: Useful Resources
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实用资源
- en: '**MOOC**: [Practical Deep Learning for Coders — Part 2](https://course.fast.ai/Lessons/part2.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MOOC**：[实用深度学习课程 — 第2部分](https://course.fast.ai/Lessons/part2.html)'
- en: '**MOOC**: [Hugging Face Diffusion Models Course](https://github.com/huggingface/diffusion-models-class)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MOOC**：[Hugging Face 扩散模型课程](https://github.com/huggingface/diffusion-models-class)'
- en: '**Book**: [Generative Deep Learning, 2nd Edition](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**书籍**：[生成式深度学习，第2版](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/)'
- en: '**Bonus**: I’d like to introduce another resource that I’ve used to refresh
    some concepts while writing this article. I’m confident you’ll appreciate it as
    well. If you’re interested in understanding AI in a fun, concise, and clear manner,
    I highly recommend checking out “[**AI Coffee Break with Letitia**](https://www.youtube.com/c/aicoffeebreak)”.
    Trust me, it’s absolutely worth exploring and subscribing to!'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**额外推荐**：我想介绍另一个我在撰写这篇文章时用来刷新一些概念的资源。我相信你也会喜欢。如果你有兴趣以一种有趣、简洁和清晰的方式了解 AI，我强烈推荐你去看看“[**AI
    Coffee Break with Letitia**](https://www.youtube.com/c/aicoffeebreak)”。相信我，它绝对值得探索和订阅！'
- en: Diffusers Pipelines
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Diffusers Pipelines
- en: What are Diffusers Pipelines?
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是 Diffusers Pipelines？
- en: 'From the [Diffusers documentation](https://huggingface.co/docs/diffusers/api/pipelines/overview#pipelines):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [Diffusers 文档](https://huggingface.co/docs/diffusers/api/pipelines/overview#pipelines)：
- en: Pipelines provide a simple way to run state-of-the-art diffusion models in inference
    by bundling all of the necessary components (multiple independently-trained models,
    schedulers, and processors) into a single end-to-end class. Pipelines are flexible
    and they can be adapted to use different scheduler or even model components.
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Pipelines 提供了一种简单的方式，通过将所有必要的组件（多个独立训练的模型、调度器和处理器）打包到一个端到端的类中，从而运行最先进的扩散模型进行推理。Pipelines
    是灵活的，可以适应使用不同的调度器或甚至模型组件。
- en: 'In this article, we will discuss the models behind the most popular pipelines
    of the Diffusers library. Even though pipelines are meant for inference, the theory
    behind them is equally relevant for the training of these models. There are a
    couple of popular training techniques that don’t have a dedicated inference pipeline,
    mainly [LoRA](https://arxiv.org/abs/2106.09685) and [DreamBooth](https://arxiv.org/abs/2208.12242).
    We will not cover them in this article, but for the latter, I’ve already written
    a dedicated article. Feel free to check it out:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将讨论 Diffusers 库中最流行的管道背后的模型。尽管管道用于推理，但它们背后的理论对于这些模型的训练同样重要。有几种流行的训练技术没有专门的推理管道，主要是
    [LoRA](https://arxiv.org/abs/2106.09685) 和 [DreamBooth](https://arxiv.org/abs/2208.12242)。我们在本文中不会涵盖它们，但对于后者，我已经写了专门的文章。随时查看：
- en: '[](/demystifying-dreambooth-a-new-tool-for-personalizing-text-to-image-generation-70f8bb0cfa30?source=post_page-----a83d64348d90--------------------------------)
    [## Demystifying DreamBooth: A New Tool for Personalizing Text-To-Image Generation'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/demystifying-dreambooth-a-new-tool-for-personalizing-text-to-image-generation-70f8bb0cfa30?source=post_page-----a83d64348d90--------------------------------)
    [## 揭秘 DreamBooth：一个个性化文本到图像生成的新工具'
- en: Exploring the technology that turns boring images into creative masterpieces
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索将无聊图像转化为创意杰作的技术
- en: towardsdatascience.com](/demystifying-dreambooth-a-new-tool-for-personalizing-text-to-image-generation-70f8bb0cfa30?source=post_page-----a83d64348d90--------------------------------)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/demystifying-dreambooth-a-new-tool-for-personalizing-text-to-image-generation-70f8bb0cfa30?source=post_page-----a83d64348d90--------------------------------)
- en: How to Use Diffusion Pipelines
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用扩散管道
- en: 'Let’s learn from a simple example:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个简单的例子学习：
- en: '[PRE0]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This code snippet is all that I used to generate the cover image for this article.
    We can already observe a few things:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码就是我用来生成本文封面图像的全部内容。我们已经可以观察到一些东西：
- en: Even though I am using [Stable Diffusion XL](https://stability.ai/blog/stable-diffusion-sdxl-1-announcement),
    it’s not necessary to use the `[StableDiffusionXLPipeline](https://huggingface.co/docs/diffusers/v0.20.0/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLPipeline)`
    specifically; instead you can use the more general class `[DiffusionPipeline](https://huggingface.co/docs/diffusers/api/diffusion_pipeline#diffusers.DiffusionPipeline)`.
    The `from_pretrained` function will return the correct class object based on the
    repository ID ("stabilityai/stable-diffusion-xl-base-1.0" in this case) or the
    path to a local directory.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管我使用的是 [Stable Diffusion XL](https://stability.ai/blog/stable-diffusion-sdxl-1-announcement)，但不必特定使用
    `[StableDiffusionXLPipeline](https://huggingface.co/docs/diffusers/v0.20.0/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLPipeline)`；你可以使用更通用的类
    `[DiffusionPipeline](https://huggingface.co/docs/diffusers/api/diffusion_pipeline#diffusers.DiffusionPipeline)`。`from_pretrained`
    函数将根据仓库 ID（在这种情况下为 "stabilityai/stable-diffusion-xl-base-1.0"）或本地目录路径返回正确的类对象。
- en: It’s easily possible, and generally recommended to speed up the process, to
    change the weight to half precision (float16), specifying the corresponding variant.
    For example, you can check [here](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/tree/main/unet)
    that both a `diffusion_pytorch_model.f16` and a non-f16 model exist for the U-Net
    of Stable Diffusion XL.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改权重为半精度（float16）以加速处理是完全可能且通常推荐的，指定相应的变体。例如，你可以查看 [这里](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/tree/main/unet)，Stable
    Diffusion XL 的 U-Net 有 `diffusion_pytorch_model.f16` 和非 f16 模型。
- en: It’s advised to use the weights in the [safetensors](https://huggingface.co/docs/safetensors/index)
    format whenever possible. This format avoids the security issues of pickling and
    is faster at the same time.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建议尽可能使用 [safetensors](https://huggingface.co/docs/safetensors/index) 格式的权重。该格式避免了
    pickle 的安全问题，并且速度更快。
- en: It’s highly recommended to execute this code on a GPU (`to("cuda")`), as diffusion
    models are computationally intensive. Generating a single prediction often requires
    around 20–50 forward passes of the model.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强烈建议在 GPU 上执行此代码（`to("cuda")`），因为扩散模型计算密集。生成一次预测通常需要大约 20–50 次模型前向传播。
- en: If you were to rerun this code, you would obtain a different result. Diffusion
    model inference is inherently non-deterministic, implying that each execution
    produces a diverse outcome (unless you intentionally enforce consistency fixing
    random seeds, etc.).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你重新运行此代码，将会得到不同的结果。扩散模型推理本质上是非确定性的，这意味着每次执行都会产生不同的结果（除非你故意强制一致性，例如固定随机种子等）。
- en: In conclusion, as observed, it’s remarkably straightforward to utilize these
    pipelines. This is why I chose to focus on the theory behind them; understanding
    it at an intuitive level is instrumental in fully harnessing the capabilities
    of these powerful tools.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，正如所观察到的，使用这些管道非常简单。这就是我选择专注于它们背后的理论的原因；在直观层面理解它对充分利用这些强大工具的能力至关重要。
- en: Useful Resources
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有用的资源
- en: '**Diffusers documentation**: [Pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview#pipelines)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Diffusers 文档**: [管道](https://huggingface.co/docs/diffusers/api/pipelines/overview#pipelines)'
- en: '**Diffusers documentation**: [Safetensors](https://huggingface.co/docs/safetensors/index)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Diffusers 文档**: [Safetensors](https://huggingface.co/docs/safetensors/index)'
- en: 'Pipeline: DDPM (Diffusion Models)'
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '管道: DDPM (扩散模型)'
- en: '**Unraveling the Theory**'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**解密理论**'
- en: '“[Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)”
    (DDPM) marked the initial spotlight on diffusion models. While often hailed as
    the seminal paper on this theme, the concept of diffusion models was introduced
    back in 2015 within the paper titled “[Deep Unsupervised Learning using Nonequilibrium
    Thermodynamics](https://arxiv.org/abs/1503.03585)”. The figure below encapsulates
    the core concept of **diffusion models**:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: “[去噪扩散概率模型](https://arxiv.org/abs/2006.11239)” (DDPM) 让扩散模型首次受到关注。尽管常被称为这一主题的开创性论文，但扩散模型的概念早在
    2015 年就在论文 “[使用非平衡热力学的深度无监督学习](https://arxiv.org/abs/1503.03585)” 中提出。下图概述了**扩散模型**的核心概念：
- en: '![](../Images/860896a0d6a0a75f1292d0dbdfa9b330.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/860896a0d6a0a75f1292d0dbdfa9b330.png)'
- en: Fig. 2 from [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2 来源于 [去噪扩散概率模型](https://arxiv.org/abs/2006.11239)。
- en: Reading the image from right to left, we observe the **forward** or **diffusion
    process**, in which we progressively add noise to an image — a straightforward
    procedure in Python or any other programming language.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 从右向左读取图像，我们观察到 **前向** 或 **扩散过程**，在此过程中我们逐步向图像中添加噪声 — 这是 Python 或任何其他编程语言中的简单过程。
- en: Now, imagine having an instrument that can partially remove noise from a noisy
    image. This tool would facilitate transforming an image composed entirely of noise
    into a less noisy version, progressing from left to right in the figure above
    — the **reverse process**. But how do we create this predictive model? According
    to DDPM, we utilize a U-Net. Given a noisy image, the U-Net works to predict the
    added noise (or alternatively directly the denoised image). As we introduce the
    noise ourselves, we have the target variable for free, allowing us to train the
    model in a self-supervised manner.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象有一个工具可以部分去除噪声。这种工具可以将完全由噪声组成的图像转换为较少噪声的版本，从上图的左侧到右侧 — **反向过程**。但我们如何创建这个预测模型呢？根据
    DDPM，我们使用 U-Net。给定一张有噪声的图像，U-Net 会预测添加的噪声（或直接预测去噪图像）。由于我们自己引入噪声，我们可以免费获得目标变量，从而以自监督的方式训练模型。
- en: The **U-Net** used here isn’t the [2015 version](https://arxiv.org/abs/1505.04597);
    it’s a modern adaptation designed specifically for this task. To simplify the
    U-Net’s task, we provide not only the noisy image but also the **timestep** `t`
    as input. A higher `t` corresponds to a noisier image. This timestep is incorporated
    into the model using a sinusoidal position embedding, inspired by the [Transformer](https://arxiv.org/abs/1706.03762).
    From the Transformer derives also the self-attention mechanism, tailored for images
    in this case. Self-attention allows pixels in the 16x16 resolution blocks to attend
    to all other pixels, boosting the model's capability to generate globally consistent
    images.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 此处使用的 **U-Net** 不是 [2015 版本](https://arxiv.org/abs/1505.04597)；它是专门为此任务设计的现代改编版。为了简化
    U-Net 的任务，我们不仅提供噪声图像，还将 **时间步** `t` 作为输入。较高的 `t` 对应于更有噪声的图像。这个时间步通过正弦位置嵌入引入到模型中，灵感来自
    [Transformer](https://arxiv.org/abs/1706.03762)。Transformer 还派生出自注意力机制，在这种情况下专门针对图像。自注意力允许
    16x16 分辨率块中的像素关注所有其他像素，提高了模型生成全球一致图像的能力。
- en: 'Finally, let’s introduce the concept of **sampler** or **scheduler**. According
    to the Diffusers documentation:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们介绍 **采样器** 或 **调度器** 的概念。根据 Diffusers 文档：
- en: The schedule functions, denoted *Schedulers* in the library take in the output
    of a trained model, a sample which the diffusion process is iterating on, and
    a timestep to return a denoised sample. That’s why schedulers may also be called
    *Samplers* in other diffusion models implementations.
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 调度函数，在库中表示为*Schedulers*，接受训练模型的输出、扩散过程正在迭代的样本以及一个时间步，以返回去噪样本。这就是为什么调度器在其他扩散模型实现中也可能被称为*Samplers*。
- en: 'In practical terms, schedulers determine the number of steps required to generate
    the final image and establish the method for converting a noisy image into a less
    noisy variant, utilizing the model’s output. These schedulers can be categorized
    as either discrete or continuous, as elucidated in the documentation:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，调度器确定生成最终图像所需的步骤数，并建立将噪声图像转换为较少噪声变体的方法，利用模型的输出。这些调度器可以分为离散或连续两类，如文档中所述：
- en: Different algorithms use timesteps that can be discrete (accepting `int` inputs),
    such as the [DDPMScheduler](https://huggingface.co/docs/diffusers/v0.19.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)
    or [PNDMScheduler](https://huggingface.co/docs/diffusers/v0.19.3/en/api/schedulers/pndm#diffusers.PNDMScheduler),
    or continuous (accepting `float` inputs), such as the score-based schedulers [ScoreSdeVeScheduler](https://huggingface.co/docs/diffusers/v0.19.3/en/api/schedulers/score_sde_ve#diffusers.ScoreSdeVeScheduler)
    or `ScoreSdeVpScheduler`.
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 不同的算法使用可以是离散的（接受`int`输入），例如[DDPMScheduler](https://huggingface.co/docs/diffusers/v0.19.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)或[PNDMScheduler](https://huggingface.co/docs/diffusers/v0.19.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)，也可以是连续的（接受`float`输入），例如基于分数的调度器[ScoreSdeVeScheduler](https://huggingface.co/docs/diffusers/v0.19.3/en/api/schedulers/score_sde_ve#diffusers.ScoreSdeVeScheduler)或`ScoreSdeVpScheduler`。
- en: In a similar vein, the sampling process can have either a stochastic or deterministic
    nature.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，采样过程可以是随机的或确定性的。
- en: If you’re curious to dive deeper into samplers, that would need a whole separate
    article. If that sounds intriguing, just give me a shout, and I’ll be happy to
    explore it further!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入了解采样器，那将需要一整篇独立的文章。如果这听起来很有趣，随时告诉我，我会很高兴进一步探讨！
- en: '**Applications and Limitations**'
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**应用与局限性**'
- en: '[DDPMPipeline](https://huggingface.co/docs/diffusers/api/pipelines/ddpm#diffusers.DDPMPipeline)
    is a pipeline for **unconditional image generation**, hence its practical application
    is limited compared to other techniques we will explore that allow for greater
    control over the generated output. Furthermore, the image denoising process using
    DDPM’s scheduler is quite slow; by default, it requires 1000 steps, which translate
    to 1000 predictions by the U-Net. Given these considerations, the current interest
    in DDPM is mainly historical, as subsequent works build upon this foundation.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[DDPMPipeline](https://huggingface.co/docs/diffusers/api/pipelines/ddpm#diffusers.DDPMPipeline)是用于**无条件图像生成**的流程，因此与我们将要探讨的允许更大控制的技术相比，其实际应用受到限制。此外，使用DDPM调度器的图像去噪过程相当慢；默认情况下，它需要1000步，即U-Net的1000次预测。鉴于这些考虑，目前对DDPM的兴趣主要是历史性的，因为后续工作在此基础上进行扩展。'
- en: '**Useful Resources**'
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**有用的资源**'
- en: '**Diffusers documentation**: [DDPM](https://huggingface.co/docs/diffusers/api/pipelines/ddpm)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩散器文档**: [DDPM](https://huggingface.co/docs/diffusers/api/pipelines/ddpm)'
- en: '**Scientific paper**: [Deep Unsupervised Learning using Nonequilibrium Thermodynamics](https://arxiv.org/abs/1503.03585)
    (diffusion models)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**: [使用非平衡热力学的深度无监督学习](https://arxiv.org/abs/1503.03585)（扩散模型）'
- en: '**Scientific paper**: [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)
    (DDPM)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**: [去噪扩散概率模型](https://arxiv.org/abs/2006.11239)（DDPM）'
- en: 'Pipeline: Stable Diffusion Text-to-Image'
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '流程: 稳定扩散文本到图像'
- en: Unraveling the Theory
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 揭示理论
- en: To date, the primary open-source algorithm for image generation is [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release)
    in its various iterations. The initial version of Stable Diffusion is the outcome
    of a collaboration between [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/),
    [Runway](https://runwayml.com/about/), and [LAION](https://laion.ai/). The primary
    feature of this model is being a **latent diffusion** **model** (**LDM**), where
    the diffusion process occurs not directly in the image/pixel space but within
    a latent space.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，主要的开源图像生成算法是[Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release)及其各种版本。Stable
    Diffusion的初始版本是[CompVis](https://github.com/CompVis)、[Stability AI](https://stability.ai/)、[Runway](https://runwayml.com/about/)和[LAION](https://laion.ai/)的合作成果。该模型的主要特性是作为一个**潜在扩散**
    **模型** (**LDM**)，其扩散过程不是直接在图像/像素空间中进行，而是在潜在空间中进行。
- en: '![](../Images/86efd18764d1d440d1484f13fcdec929.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/86efd18764d1d440d1484f13fcdec929.png)'
- en: Fig. 3 from [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图3来自[高分辨率图像合成与潜在扩散模型](https://arxiv.org/abs/2112.10752)。
- en: In practice, before being fed into the U-Net, the image is compressed into a
    latent space using a [**Variational Autoencoder**](https://arxiv.org/abs/1312.6114)
    (**VAE**). After the denoising process, the latent representation is transformed
    back into an image using the decoder of the same VAE.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 实际操作中，在输入到U-Net之前，图像会使用[**变分自编码器**](https://arxiv.org/abs/1312.6114) (**VAE**)
    压缩到潜在空间中。在去噪过程后，潜在表示会通过同一VAE的解码器转回图像。
- en: Another crucial point to underline is Stable Diffusion’s capability to take
    **textual prompts** as input, partly controlling the generated content. The text
    is first embedded using a Transformer-based model and then mapped into the U-Net
    using a **cross-attention** mechanism. Specifically, Stable Diffusion v1 utilizes
    the OpenAI **CLIP** text encoder (see [Appendix — CLIP](#13e9)).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要点是稳定扩散（Stable Diffusion）能够接受**文本提示**作为输入，部分控制生成的内容。文本首先使用基于Transformer的模型进行嵌入，然后通过**交叉注意力**机制映射到U-Net中。具体来说，Stable
    Diffusion v1 使用了OpenAI的**CLIP**文本编码器（参见[附录 — CLIP](#13e9)）。
- en: Two more versions of Stable Diffusion currently exist, each with its own sub-variants.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 目前存在两个更多版本的Stable Diffusion，每个版本都有其子变体。
- en: '**Stable Diffusion v2** stands out from the original mainly due to a shift
    in the text encoder to [OpenCLIP](https://arxiv.org/abs/2212.07143), the open-source
    counterpart of CLIP. While one might generally anticipate improved performance
    in a later version, this assertion is uncertain in the context of Stable Diffusion
    v2\. Notably, **OpenCLIP**’s training on a subset of [**LAION-5B**](https://laion.ai/blog/laion-5b/),
    different from OpenAI’s private dataset, along with the use of a highly restrictive
    NSFW filter, led v2 to noticeably lag behind v1 in representing concepts like
    celebrities or emulating styles of renowned artists. Some of these limitations
    were partially addressed in version v2.1, which introduced a less stringent filter
    and other modifications. For further insight, I found the [AssemblyAI](https://www.assemblyai.com/)
    article “[Stable Diffusion 1 vs 2 — What you need to know](https://www.assemblyai.com/blog/stable-diffusion-1-vs-2-what-you-need-to-know/)”
    particularly informative.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**Stable Diffusion v2**之所以与原版不同，主要在于文本编码器转移到了[OpenCLIP](https://arxiv.org/abs/2212.07143)，这是CLIP的开源对应版本。尽管一般来说，可以预期后续版本的性能会有所提高，但在Stable
    Diffusion v2中这一断言并不确定。值得注意的是，**OpenCLIP**在[**LAION-5B**](https://laion.ai/blog/laion-5b/)子集上的训练不同于OpenAI的私有数据集，加上使用了高度限制的NSFW过滤器，使得v2在表示名人或模仿著名艺术家风格方面明显落后于v1。这些限制在v2.1版本中得到了部分解决，后者引入了较不严格的过滤器和其他修改。有关更多见解，我发现[AssemblyAI](https://www.assemblyai.com/)的文章“[Stable
    Diffusion 1 vs 2 — 你需要知道的](https://www.assemblyai.com/blog/stable-diffusion-1-vs-2-what-you-need-to-know/)”特别具有信息量。'
- en: Finally, Stability AI recently introduced [**Stable Diffusion XL**](https://arxiv.org/abs/2307.01952)(**SD-XL**),
    a substantial leap from v2\. This iteration competes with leading closed-source
    models like [Midjourney](https://www.midjourney.com/home/?callbackUrl=%2Fapp%2F)
    in terms of output quality.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Stability AI 最近推出了[**Stable Diffusion XL**](https://arxiv.org/abs/2307.01952)
    (**SD-XL**)，这是v2的重大跃进。这个版本在输出质量上与领先的闭源模型如[Midjourney](https://www.midjourney.com/home/?callbackUrl=%2Fapp%2F)竞争。
- en: The upgrades of this version include the merging of CLIP and OpenCLIP outputs,
    the retraining of the VAE with a larger batch size, and the implementation of
    weight tracking through the **Exponential Moving Average** (**EMA**) technique.
    The EMA weights can be employed in place of the final weights during inference,
    leading to a general improvement in performance. This technique aids in reducing
    some of the overfitting that typically arises in the final iterations, often resulting
    in the generation of slightly improved weights for inference.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 本版本的升级包括合并 CLIP 和 OpenCLIP 输出，使用更大的批量大小重新训练 VAE，并通过**指数移动平均**（**EMA**）技术实现权重跟踪。EMA
    权重可以在推理过程中替代最终权重，从而普遍提升性能。这项技术有助于减少在最终迭代中通常出现的一些过拟合现象，通常会生成略微改进的推理权重。
- en: Equally important is SD-XL’s effort to address the issues that arise from the
    use of squared random cropping during training. To enhance this aspect, it employs
    **cropping parameter conditioning**, which involves informing the model about
    the parameters that determine how the image has been cropped, similar to what
    is done for the timestep. This prevents issues like generating headless figures.
    Simultaneously, the SD-XL version follows modern practices and it’s fine-tuned
    to handle **multiple aspect-ratios** using [aspect ratio bucketing](https://github.com/NovelAI/novelai-aspect-ratio-bucketing).
    This, along with cropping parameter conditioning, significantly enhances the model’s
    ability to portray horizontal and vertical scenes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是 SD-XL 努力解决在训练期间使用平方随机裁剪所产生的问题。为增强这一方面，它采用了**裁剪参数条件**，这涉及到将决定图像如何裁剪的参数信息提供给模型，类似于时间步的处理。这可以防止生成无头图像等问题。同时，SD-XL
    版本遵循现代实践，并且经过微调以处理**多种长宽比**，使用了[长宽比分桶](https://github.com/NovelAI/novelai-aspect-ratio-bucketing)。这与裁剪参数条件一起，显著提升了模型呈现横向和纵向场景的能力。
- en: SD-XL also introduces a **refinement stage** where another LDM specialized in
    high-quality images uses a noise-denoising process introduced by SDEdit, which
    we will cover in the next pipeline.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: SD-XL 还引入了一个**精炼阶段**，其中另一个专注于高质量图像的 LDM 使用 SDEdit 引入的去噪过程，我们将在下一个管道中讨论这一点。
- en: 'Finally, another technique that is not always presented in introductory courses
    is [**offset noise**](https://www.crosslabs.org/blog/diffusion-with-offset-noise).
    The primary reason why we need to modify the initial noise is that, in reality,
    the image is never completely erased during the forward process (as we perform
    a finite number of steps). As a result, the model encounters difficulties in learning
    from pure noise. Quoting from the SD-XL paper:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有一种技术并不总是在入门课程中介绍，即[**偏移噪声**](https://www.crosslabs.org/blog/diffusion-with-offset-noise)。我们需要修改初始噪声的主要原因是，实际上图像在前向过程中从未完全被擦除（因为我们执行了有限数量的步骤）。因此，模型在从纯噪声中学习时会遇到困难。引用
    SD-XL 论文：
- en: Our model is trained in the discrete-time formulation of [14], and requires
    offset-noise [11, 25] for aesthetically pleasing results.
  id: totrans-112
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们的模型在[14]的离散时间公式下进行训练，并且需要偏移噪声[11, 25]以获得令人满意的结果。
- en: Applications and Limitations
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用与局限
- en: '[StableDiffusionPipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)
    (text-to-image) allows the generation of images based on textual prompts. As of
    today, my recommendation is to use the SD-XL version, which can produce truly
    astonishing results. Although SD-XL is undeniably exceptional, there are still
    various cases of failure. The model sometimes faces challenges with very complex
    prompts that involve detailed spatial arrangements and intricate descriptions.
    Complicated structures, such as human hands, can still be generated deformed at
    times. While the photorealism is quite good, it’s not yet perfect. Occasionally,
    a phenomenon known as “**concept bleeding**” occurs, where, for example, a color
    associated with one element in the prompt is mistaken or extended to another element.
    The texts generated by SD-XL are significantly better than in the past, but sometimes,
    especially for longer ones, they contain errors like random characters or inconsistencies.
    Lastly, it’s important to remember that like all generative models, this can inadvertently
    introduce social and racial biases.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[StableDiffusionPipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)（文本到图像）允许基于文本提示生成图像。截至今天，我推荐使用
    SD-XL 版本，它能够产生真正令人惊叹的结果。尽管 SD-XL 无疑是杰出的，但仍有各种失败情况。模型有时面临涉及详细空间排列和复杂描述的非常复杂提示的挑战。复杂的结构，例如人类手部，有时仍可能生成变形。尽管照片级真实感相当好，但仍未完美。偶尔会出现一种称为“**概念溢出**”的现象，例如，提示中的一种颜色被误认为或扩展到另一元素。SD-XL
    生成的文本明显比过去更好，但有时，尤其是对于较长的文本，可能包含随机字符或不一致性。最后，重要的是要记住，像所有生成模型一样，这可能会无意中引入社会和种族偏见。'
- en: Useful Resources
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有用的资源
- en: '**Diffusers documentation**: [Text-to-image](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Diffusers 文档**: [文本到图像](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img)'
- en: '**Scientific paper**: [High-Resolution Image Synthesis with Latent Diffusion
    Models](https://arxiv.org/abs/2112.10752) (Stable Diffusion v1, check out my article
    below, which breaks down this paper for you)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**: [使用潜在扩散模型进行高分辨率图像合成](https://arxiv.org/abs/2112.10752)（Stable Diffusion
    v1，请查看下面的文章，它为你解析了这篇论文）'
- en: '**Scientific paper**: [SDXL: Improving Latent Diffusion Models for High-Resolution
    Image Synthesis](https://arxiv.org/abs/2307.01952)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**: [SDXL: 改进高分辨率图像合成的潜在扩散模型](https://arxiv.org/abs/2307.01952)'
- en: '**Scientific paper**: [Reproducible scaling laws for contrastive language-image
    learning](https://arxiv.org/abs/2212.07143) (OpenCLIP)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**: [对比语言-图像学习的可重复缩放定律](https://arxiv.org/abs/2212.07143)（OpenCLIP）'
- en: '**Scientific blog**: [Stable Diffusion 1 vs 2 — What you need to know](https://www.assemblyai.com/blog/stable-diffusion-1-vs-2-what-you-need-to-know/)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学博客**: [Stable Diffusion 1 与 2 — 你需要了解的内容](https://www.assemblyai.com/blog/stable-diffusion-1-vs-2-what-you-need-to-know/)'
- en: '**Scientific blog**: [Diffusion With Offset Noise](https://www.crosslabs.org/blog/diffusion-with-offset-noise)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学博客**: [带偏移噪声的扩散](https://www.crosslabs.org/blog/diffusion-with-offset-noise)'
- en: '**GitHub page**: [NovelAI Aspect Ratio Bucketing](https://github.com/NovelAI/novelai-aspect-ratio-bucketing)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GitHub 页面**: [NovelAI 纵横比分桶](https://github.com/NovelAI/novelai-aspect-ratio-bucketing)'
- en: '**Appendix**: [CLIP](#13e9)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**附录**: [CLIP](#13e9)'
- en: 'If you’re curious to explore the mechanics behind Stable Diffusion further,
    take a peek at my earlier article:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣进一步探索 Stable Diffusion 背后的机制，可以查看我之前的文章：
- en: '[](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----a83d64348d90--------------------------------)
    [## Paper Explained — High-Resolution Image Synthesis with Latent Diffusion Models'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----a83d64348d90--------------------------------)
    [## 论文解析 — 高分辨率图像合成与潜在扩散模型'
- en: While OpenAI has dominated the field of natural language processing with their
    generative text models, their image…
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虽然OpenAI凭借其生成文本模型主导了自然语言处理领域，但他们的图像……
- en: towardsdatascience.com](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----a83d64348d90--------------------------------)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----a83d64348d90--------------------------------)
- en: 'Pipeline: Stable Diffusion Image-to-Image (SDEdit)'
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流程：Stable Diffusion 图像到图像（SDEdit）
- en: Unraveling the Theory
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解开理论
- en: Sometimes we would like to start from a starting image, which can also be made
    up of our coarse colored strokes, and generate another image that respects the
    structure of the initial image but whose content is determined by the textual
    prompt. The simplest technique to achieve this is [**SDEdit**](https://arxiv.org/abs/2108.01073),
    which corresponds to the **Image-to-Image** pipeline.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们希望从起始图像开始，该图像也可以由我们的粗略彩色笔触组成，并生成另一张图像，该图像尊重初始图像的结构，但其内容由文本提示决定。实现这一点的最简单技术是
    [**SDEdit**](https://arxiv.org/abs/2108.01073)，它对应于**图像到图像**管道。
- en: '![](../Images/c413fe10cf4ff56171e7ed31e2fd2402.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c413fe10cf4ff56171e7ed31e2fd2402.png)'
- en: 'Fig. 2 from [SDEdit: Guided Image Synthesis and Editing with Stochastic Differential
    Equations](https://arxiv.org/abs/2108.01073)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2 来自 [SDEdit: 使用随机微分方程引导图像合成与编辑](https://arxiv.org/abs/2108.01073)'
- en: During the diffusion process, rather than beginning from random noise, there’s
    nothing stopping us from starting from a later step of the forward process, where
    we generate the input by incorporating noise based on the chosen starting timestep.
    As you can see in the figure above, even in the instance of **strokes**, the addition
    of noise enables the resulting image to be included within the distribution of
    typical images. This final point is important as it enable training the model
    solely with images, but then employing our strokes as input during inference.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩散过程中，我们可以选择从正向过程的较晚步骤开始，而不是从随机噪声开始，这样我们可以通过根据选定的起始时间步长加入噪声来生成输入。正如上图所示，即使在**笔触**的情况下，添加噪声也能使生成的图像包含在典型图像的分布中。这一点很重要，因为它使得模型可以仅用图像进行训练，但在推断时则使用我们的笔触作为输入。
- en: It’s worth noting that this technique presents a trade-off between **faithfulness**
    and **realism**, depending on which point of the forward process we start from.
    In practice, if we use a “strength” parameter equal to 1 during generation, the
    input image will essentially be ignored, while if “strength” is equal to 0, we
    will get the same image. The current default is `strength=0.8`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，这种技术在**忠实性**和**现实性**之间存在权衡，具体取决于我们从正向过程中的哪个点开始。实际上，如果在生成过程中使用“强度”参数为
    1，则输入图像将被忽略，而如果“强度”参数为 0，我们将获得相同的图像。当前的默认值是 `strength=0.8`。
- en: Finally SDEdit can also be used for **inpainting**, simply by masking the portion
    of the image that you do not want to modify.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，SDEdit 也可以用于**修复**，只需遮掩不希望修改的图像部分即可。
- en: Applications and Limitations
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用与限制
- en: '[StableDiffusionImg2ImgPipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img#diffusers.StableDiffusionImg2ImgPipeline)
    is a good pipeline to use when you want to generate an image from some strokes
    or modify a starting image based on a textual prompt. It’s worth noting that the
    main limitation of this technique is that you can’t request significant variations
    in the structure of the generated image through the textual prompt. The generated
    image’s structure will remain conditioned by the starting structure (unless you
    choose a strength value very close to 1).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[StableDiffusionImg2ImgPipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img#diffusers.StableDiffusionImg2ImgPipeline)
    是一个很好的管道，用于从一些笔触生成图像或基于文本提示修改起始图像。值得注意的是，这种技术的主要限制是无法通过文本提示请求生成图像结构的显著变化。生成图像的结构将继续受到起始结构的限制（除非选择非常接近
    1 的强度值）。'
- en: Useful Resources
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有用资源
- en: '**Diffusers documentation**: [Image-to-image](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Diffusers 文档**：[图像到图像](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img)'
- en: '**Scientific paper**: [SDEdit: Guided Image Synthesis and Editing with Stochastic
    Differential Equations](https://arxiv.org/abs/2108.01073)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**：[SDEdit: 使用随机微分方程引导图像合成与编辑](https://arxiv.org/abs/2108.01073)'
- en: 'Pipeline: Stable Diffusion Image Variation'
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管道：稳定扩散图像变体
- en: Unraveling the Theory
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解开理论
- en: '[StableDiffusionImageVariationPipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/image_variation#diffusers.StableDiffusionImageVariationPipeline)
    is a pipeline developed by [Lambda](https://lambdalabs.com/) which, like Image-to-Image,
    allows generating variations of an input image.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[StableDiffusionImageVariationPipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/image_variation#diffusers.StableDiffusionImageVariationPipeline)
    是由 [Lambda](https://lambdalabs.com/) 开发的一个管道，类似于 Image-to-Image，允许生成输入图像的变体。'
- en: '![](../Images/78c8df9d7b1cc06eb090485d676ea1ba.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78c8df9d7b1cc06eb090485d676ea1ba.png)'
- en: Image from the [Stable Diffusion Image Variations Model Card](https://huggingface.co/lambdalabs/sd-image-variations-diffusers).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来自[Stable Diffusion Image Variations Model Card](https://huggingface.co/lambdalabs/sd-image-variations-diffusers)。
- en: 'Typically, in a task such as text-to-image, generation is conditioned by a
    textual prompt that is transformed into an embedding through a dedicated encoder.
    As you can check in [Appendix — CLIP](#13e9), CLIP has two encoders: one for text
    and one for images. Both of these encoders map the input in such a way that text
    describing an image has an embedding close to those that don’t describe it, and
    vice versa. This pipeline simply **replaces the CLIP text encoder with the CLIP
    image encoder**. In this way, generation isn’t governed by a textual prompt, but
    rather by an image that will be decoded by the model not exactly into itself,
    but into a variant. This is unless the model has overfit to that specific concept
    and can reproduce it exactly from its latent representation.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在像文本到图像这样的任务中，生成是由文本提示条件控制的，该提示通过专门的编码器转换为嵌入。正如你可以在[附录 — CLIP](#13e9)中检查的那样，CLIP有两个编码器：一个用于文本，一个用于图像。这两个编码器都以这样的方式映射输入，使得描述图像的文本具有与不描述图像的文本接近的嵌入，反之亦然。这个流程只是**用CLIP图像编码器替代CLIP文本编码器**。这样，生成不再由文本提示来控制，而是由一个图像控制，模型将图像解码为一个变体，而不是完全相同的图像。除非模型对特定概念过度拟合并能够从其潜在表示中准确再现。
- en: Applications and Limitations
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用与局限性
- en: This is an interesting pipeline for obtaining images similar to the input. The
    generated images may not necessarily retain the exact structure of the original
    image, as with image-to-image approaches, but they might, for example, retain
    its style or key subject characteristics. One of the main limitations of this
    technique is that there’s not a great deal of control over the generated variations.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有趣的管道，用于获取与输入图像相似的图像。生成的图像可能不会完全保留原始图像的结构，如图像到图像的方法，但它们可能会保留其风格或关键特征。该技术的主要局限之一是对生成变异的控制不大。
- en: Useful Resources
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有用的资源
- en: '**Diffusers documentation**: [Image variation](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/image_variation)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Diffusers文档**：[图像变异](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/image_variation)'
- en: '**Model card**: [Stable Diffusion Image Variations Model Card](https://huggingface.co/lambdalabs/sd-image-variations-diffusers)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型卡**：[Stable Diffusion Image Variations Model Card](https://huggingface.co/lambdalabs/sd-image-variations-diffusers)'
- en: '**Appendix**: [CLIP](#13e9)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**附录**：[CLIP](#13e9)'
- en: 'Pipeline: Stable Diffusion Upscale'
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管道：Stable Diffusion Upscale
- en: Unraveling the Theory
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解开理论
- en: '[StableDiffusionUpscalePipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline)
    is a **super-resolution** pipeline that enhances the resolution of input images
    **by a factor of 4**.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[StableDiffusionUpscalePipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline)是一个**超分辨率**管道，通过**4倍**的因子增强输入图像的分辨率。'
- en: '![](../Images/9bc91302fb8eb954d59340d66f22b5e5.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9bc91302fb8eb954d59340d66f22b5e5.png)'
- en: Image from the [Stable Diffusion x4 Upscaler Model Card](https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来自[Stable Diffusion x4 Upscaler Model Card](https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler)。
- en: The method employed, previously introduced in the original Latent Diffusion
    paper, involves **concatenating the low-resolution image with the latents generated
    by the VAE encoder**. The model is then trained to generate the high-resolution
    image based on this input. This model was created by the researchers and engineers
    from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/),
    and [LAION](https://laion.ai/).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 所采用的方法，在原始潜在扩散论文中已介绍，涉及**将低分辨率图像与由VAE编码器生成的潜在变量串联**。然后模型基于这一输入训练以生成高分辨率图像。该模型由[CompVis](https://github.com/CompVis)、[Stability
    AI](https://stability.ai/)和[LAION](https://laion.ai/)的研究人员和工程师创建。
- en: Applications and Limitations
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用与局限性
- en: 'The application of this pipeline is quite straightforward: increasing the resolution
    of an input image.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这个管道的应用非常简单：提高输入图像的分辨率。
- en: Useful Resources
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有用的资源
- en: '**Diffusers documentation**: [Super-resolution](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/upscale)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Diffusers文档**：[超分辨率](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/upscale)'
- en: '**Model card**: [Stable Diffusion x4 Upscaler Model Card](https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型卡**：[Stable Diffusion x4 Upscaler Model Card](https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler)'
- en: 'Pipeline: Stable Diffusion Latent Upscale'
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流程：稳定扩散潜在上采样
- en: Unraveling the Theory
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解开理论
- en: Unfortunately, I didn’t find many references about this **latent upscaler**
    trained by [Katherine Crowson](https://github.com/crowsonkb) in collaboration
    with [Stability AI](https://stability.ai/). In any case, I think it’s a safe bet
    to assume it was trained in a similar way to how the super-resolution model was
    trained. So, what’s the difference? This model, instead of only accepting an image,
    also accepts latents. It can be used directly on the latents generated in a previous
    step, without needing to start from an image.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我没有找到很多关于 **潜在上采样器** 的参考资料，这个模型由 [Katherine Crowson](https://github.com/crowsonkb)
    与 [Stability AI](https://stability.ai/) 合作训练。无论如何，我认为可以安全地假设它的训练方式与超分辨率模型类似。那么，有什么不同呢？这个模型除了接受图像外，还接受潜在变量。它可以直接用于前一步生成的潜在变量，而无需从图像开始。
- en: '![](../Images/6cc20941a2f8dd9db645dda8a50c2283.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6cc20941a2f8dd9db645dda8a50c2283.png)'
- en: Image by Tanishq Abraham from [Stability AI](https://stability.ai/) originating
    from [this tweet](https://twitter.com/StabilityAI/status/1590531958815064065).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 Tanishq Abraham 提供，来源于 [Stability AI](https://stability.ai/)，源自 [这条推文](https://twitter.com/StabilityAI/status/1590531958815064065)。
- en: The [StableDiffusionLatentUpscalePipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/latent_upscale#diffusers.StableDiffusionLatentUpscalePipeline)
    enhances the resolution of input images **by a factor of 2**.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[StableDiffusionLatentUpscalePipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/latent_upscale#diffusers.StableDiffusionLatentUpscalePipeline)
    **将输入图像的分辨率提高 2 倍**。'
- en: Applications and Limitations
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用与限制
- en: This pipeline can serve as an alternative to the super-resolution pipeline when
    we intend to start from latents rather than an image.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们打算从潜在变量而不是图像开始时，这个流程可以作为超分辨率流程的替代方案。
- en: Useful Resources
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有用的资源
- en: '**Diffusers documentation**: [Latent upscaler](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/latent_upscale#diffusers.StableDiffusionLatentUpscalePipeline)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Diffusers 文档**: [潜在上采样器](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/latent_upscale#diffusers.StableDiffusionLatentUpscalePipeline)'
- en: '**Model card**: [Stable Diffusion x2 Latent Upscaler Model Card](https://huggingface.co/stabilityai/sd-x2-latent-upscaler)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型卡**: [Stable Diffusion x2 潜在上采样器模型卡](https://huggingface.co/stabilityai/sd-x2-latent-upscaler)'
- en: 'Pipeline: unCLIP (Karlo/**DALL·E-2**)'
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流程：unCLIP (Karlo/**DALL·E-2**)
- en: Unraveling the Theory
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解开理论
- en: 'You’ve probably heard of [unCLIP](https://huggingface.co/docs/diffusers/api/pipelines/unclip)
    under a different name: DALL·E-2\. The version present in Diffusers is derived
    from [kakaobrain](https://kakaobrain.com/)’s [Karlo](https://github.com/KAKAOBRAIN/KARLO).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能听说过 [unCLIP](https://huggingface.co/docs/diffusers/api/pipelines/unclip)
    的另一个名字：DALL·E-2。Diffusers 中的版本源自 [kakaobrain](https://kakaobrain.com/) 的 [Karlo](https://github.com/KAKAOBRAIN/KARLO)。
- en: Less describe how the original **unCLIP** work.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 较少描述原始 **unCLIP** 的工作原理。
- en: '![](../Images/3fdaf40b1f8416fbd3777cf8976aac01.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3fdaf40b1f8416fbd3777cf8976aac01.png)'
- en: Fig. 2 from [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2 来自 [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125)。
- en: 'To grasp unCLIP, it’s important to know about CLIP. If you’re not familiar
    with CLIP, feel free to check out the [Appendix — CLIP](#13e9). In the image above,
    the portion above the dashed line represents CLIP itself. Below, we observe unCLIP.
    unCLIP employs a model called a “prior” to predict the CLIP image embedding from
    the CLIP text embedding of the provided prompt. The predicted CLIP image embedding
    is then input into a decoder, that transforms it into an image. This generated
    image is subsequently enlarged twice using two upsamplers: first from 64x64 to
    256x256, then from 256x256 to 1024x1024.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 unCLIP，了解 CLIP 是很重要的。如果你对 CLIP 不熟悉，可以查看 [附录 — CLIP](#13e9)。在上面的图片中，虚线上的部分代表
    CLIP 本身。下面，我们观察 unCLIP。unCLIP 使用一种叫做“prior”的模型来预测 CLIP 图像嵌入，基于提供的提示的 CLIP 文本嵌入。预测的
    CLIP 图像嵌入被输入到解码器中，转化为图像。生成的图像随后使用两个上采样器进行两次放大：第一次从 64x64 放大到 256x256，然后从 256x256
    放大到 1024x1024。
- en: 'The paper describes the “**prior**” model as:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 论文将“**prior**”模型描述为：
- en: 'For the diffusion prior, we train a decoder-only Transformer with a causal
    attention mask on a sequence consisting of, in order: the encoded text, the CLIP
    text embedding, an embedding for the diffusion timestep, the noised CLIP image
    embedding, and a final embedding whose output from the Transformer is used to
    predict the unnoised CLIP image embedding. […]'
  id: totrans-183
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于扩散先验，我们训练一个仅解码的Transformer，并在一个序列上使用因果注意力掩码，序列的顺序是：编码文本、CLIP文本嵌入、扩散时间步的嵌入、加噪的CLIP图像嵌入，以及最终的嵌入，Transformer的输出用于预测未加噪的CLIP图像嵌入。[…]
- en: 'Well, quite confusing, isn’t it? First question: **why do we need a prior model
    at all?** Wasn’t CLIP trained to make text embeddings close to their respective
    image embeddings? Why can’t we use them directly? Well, we can, as shown in the
    paper, but the results will be worse (though not terribly worse). In short, while
    the embedding of “dog” will be closer to the embedding of the image “dog” than
    to that of the image “cat”, the clusters of text embeddings and image embeddings
    do not overlap but maintain a gap between them. This phenomenon is quite complex,
    and if you’re interested in delving into it, I suggest you take a look at “[Mind
    the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation
    Learning](https://arxiv.org/abs/2203.02053)”. That said, although we understand
    that there isn’t a strict equivalence between image and text embeddings, where
    only the same concept in both modes is closer than different concepts, there isn’t,
    in my opinion, a strong theoretical reason why directly using the text embedding
    shouldn’t yield analogous results — it’s more of an experimental matter.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这相当令人困惑，不是吗？第一个问题：**我们为什么需要一个先验模型？** CLIP不是训练来使文本嵌入接近其对应的图像嵌入吗？为什么不能直接使用它们？其实可以，如论文中所示，但结果会更差（虽然不是差得特别厉害）。简而言之，虽然“狗”的嵌入会比“猫”的图像嵌入更接近“狗”图像的嵌入，但文本嵌入和图像嵌入的簇并不重叠，而是保持着间隙。这一现象相当复杂，如果你有兴趣深入了解，我建议你看看“[注意间隙：理解多模态对比表示学习中的模态间隙](https://arxiv.org/abs/2203.02053)”。尽管如此，虽然我们明白图像和文本嵌入之间没有严格的等价关系，只是在两种模式下相同的概念比不同的概念更接近，但在我看来，没有强有力的理论理由说明直接使用文本嵌入不会产生类似的结果——这更是一个实验性的问题。
- en: Alright, they employed a Transformer instead of a U-Net for this diffusion process
    (since the aim here is to predict a 1D embedding rather than an image). However,
    **why they utilized a causal attention mask?** I’m unsure about this, and even
    the skilled [luciddrains](https://github.com/lucidrains) who adapted [DALL·E-2
    to PyTorch](https://github.com/lucidrains/DALLE2-pytorch) doesn’t appear to have
    a solid rationale for it. You can find his response on this topic [here](https://github.com/lucidrains/DALLE2-pytorch/issues/46).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，他们在这个扩散过程中使用了Transformer而不是U-Net（因为目标是预测一个1D嵌入而不是图像）。然而，**他们为何使用了因果注意力掩码？**
    我对此不太确定，即使是熟练的[ luciddrains](https://github.com/lucidrains)也似乎没有一个明确的理由。他的回应可以在[这里](https://github.com/lucidrains/DALLE2-pytorch/issues/46)找到。
- en: 'Another doubt you might have is: **how** on earth do **we input the noised
    CLIP image embedding** if the CLIP image embedding is exactly what we want to
    predict? To answer this question, it’s sufficient to remember that we are dealing
    with an iterative diffusion process, where at the beginning, the noised image
    embedding will simply be… noise.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个你可能有的疑问是：**我们如何输入加噪的CLIP图像嵌入**，如果CLIP图像嵌入正是我们想要预测的？回答这个问题，只需记住我们处理的是一个迭代的扩散过程，在开始时，加噪的图像嵌入将仅仅是……噪声。
- en: Finally, two other tricks.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有两个其他技巧。
- en: The first one is to **predict** not one but **two CLIP image embeddings** **and**
    then **choose the one closer to the CLIP text embedding**.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是**预测**不仅一个而是**两个 CLIP 图像嵌入**，**然后选择与 CLIP 文本嵌入更接近的那个**。
- en: The second trick is the use of [classifier-free guidance](https://arxiv.org/abs/2207.12598).
    **Classifier-free guidance** is now a technique used by practically all diffusion
    models, including Stable Diffusion. During training, it involves occasionally
    removing text conditioning (in this specific case, 10% of the time). During inference,
    this means generating one sample with text conditioning and another without it.
    The difference between the two provides us with the direction in which we want
    to guide the model (the direction given by our textual prompt). This difference
    can be used to adjust the sample for the next step in the diffusion process.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: The **decoder** is inspired by the architecture of [**GLIDE**](https://arxiv.org/abs/2112.10741)
    (**G**uided **L**anguage to **I**mage **D**iffusion for Generation and **E**diting),
    to which a conditioning based on CLIP embeddings is added. GLIDE, in turn, is
    inspired by [**ADM**](https://arxiv.org/abs/2105.05233) (**A**blated **D**iffusion
    **M**odel), to which it adds text conditioning by encoding the prompt with a Transformer.
    ADM is an enhanced U-Net with additional attention layers and other enhancements
    compared to the version used in the paper that introduced popular diffusion models.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: The **upsamplers** are also diffusion models (ADM), where **noise is added to
    the conditioning** through the low-resolution image to make them more robust.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Alright, up until now, we have discussed the original unCLIP/DALL·E-2\. However,
    we pointed out that the implementation found in Diffusers is derived from Karlo.
    So, **what are the differences between Karlo and DALL·E-2?** The main architectural
    distinction between Karlo and DALL·E-2 is that **Karlo** includes an enhancement
    in the super-resolution module designed to upscale from 64px to 256px. This enhancement
    involves a process consisting of only 7 steps. After the initial 6 steps performed
    using the standard super-resolution module, the additional super-resolution module
    is further fine-tuned using a [VQGAN](https://arxiv.org/abs/2012.09841)-style
    loss, see [Appendix — VQGAN](#9eb6).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it’s important to emphasize that while Karlo shares a very similar
    architecture, **it is not the original OpenAI’s DALL·E-2**. Karlo and DALL·E-2were
    trained on different datasets, and there might also be variations in other training
    details. Consequently, the generated outputs from Karlo could potentially exhibit
    significant differences in quality when compared to those produced by the original
    model.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Applications and Limitations
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The limitations and applications of unCLIP are more or less similar to those
    of Stable Diffusion. One additional possibility provided by this model is that
    tasks like generating **variations** of a starting image become trivial: take
    an image, pass it through the CLIP text encoder, and decode it through the unCLIP
    decoder. You might now be asking yourself the question: **Is Stable Diffusion
    better, or is unCLIP, or other models that we are about to see?**'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: unCLIP 的局限性和应用与 Stable Diffusion 的相似。这个模型提供的一个额外可能性是，生成**变化**图像的任务变得非常简单：获取一张图像，通过
    CLIP 文本编码器传递，然后通过 unCLIP 解码器解码。你可能会问：**Stable Diffusion 更好，还是 unCLIP，更好，或者是我们即将看到的其他模型？**
- en: The answer to this question is not straightforward. First, as of today, **there
    are no robust metrics to automatically measure the performance of these models**.
    If you’re interested, I can write another article about it, but for now, know
    that close to metrics like [**Fréchet inception distance**](https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance)(**FID**),
    the best papers always report **human evaluations** for this reason. Second, as
    we say in Italy, “Non è bello quel che è bello ma è bello ciò che piace” (what’s
    beautiful is not what’s beautiful, but what’s liked), meaning that beauty is relative,
    and different people might prefer the “style” of different models depending on
    their tastes and the use of the images in question.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题的答案并不简单。首先，截至今天，**没有可靠的指标可以自动测量这些模型的性能**。如果你感兴趣，我可以写另一篇文章来讨论这个问题，但目前请知道，接近于像[**Fréchet
    inception distance**](https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance)（**FID**）这样的指标，最佳的论文总是报告**人工评估**，原因就是如此。其次，正如我们在意大利所说，“Non
    è bello quel che è bello ma è bello ciò che piace”（美丽的不是美丽的东西，而是被喜欢的东西），这意味着美是相对的，不同的人可能会根据自己的口味和图像的使用情况，偏好不同模型的“风格”。
- en: 'Here one data point to let you judge what you prefer between the text-to-image
    models presented in this article:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个数据点，让你判断在本文介绍的文本到图像模型中你更喜欢哪个。
- en: '![](../Images/5f0e244d094c05ebcef84afca5935ef7.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f0e244d094c05ebcef84afca5935ef7.png)'
- en: 'From left to right: [SD-XL 1.0 Base](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0),
    [Karlo v1 alpha](https://huggingface.co/kakaobrain/karlo-v1-alpha) (unCLIP) and
    [Kandinsky 2.2](https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 从左到右：[SD-XL 1.0 Base](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)、[Karlo
    v1 alpha](https://huggingface.co/kakaobrain/karlo-v1-alpha)（unCLIP）和[Kandinsky
    2.2](https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder)。
- en: I generated the images using the prompt “Astronaut in a jungle, cold color palette,
    muted colors, detailed, 8k”, choosing my favorite image out of four generations,
    while keeping all parameters as default. I didn’t include DeepFloyd IF because
    it requires accepting specific terms and conditions to be used.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用提示“宇航员在丛林中，冷色调，柔和的颜色，详细，8k”生成了图像，从四次生成中选择了我最喜欢的图像，同时保持所有参数为默认设置。我没有包括 DeepFloyd
    IF，因为它需要接受特定的条款和条件才能使用。
- en: In this specific case, in my opinion, the result from SD-XL is the best, closely
    followed by Kandinsky 2, while the unCLIP output is the least preferable, even
    considering that the remaining three images, not included here, were significantly
    worse. It’s worth noting that the default image size of unCLIP (Karlo) is 256x256,
    whereas SD-XL generates 1024x1024 images and Kandinsky 2 generates 512x512 images
    (if we use the Diffusers implementations of these models).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种特定情况下，我认为 SD-XL 的结果最好，其次是 Kandinsky 2，而 unCLIP 的输出最不受欢迎，即使考虑到剩下的三张图像（未包括在此）明显更差。值得注意的是，unCLIP（Karlo）的默认图像大小为
    256x256，而 SD-XL 生成 1024x1024 的图像，Kandinsky 2 生成 512x512 的图像（如果我们使用这些模型的 Diffusers
    实现）。
- en: As a final disclaimer, please be aware that this test is conducted using only
    one specific prompt and without utilizing other available parameters to control
    the generation. Each model possesses unique strengths and can produce outputs
    that are more or less appealing depending on the subject. Considering that we’re
    discussing altering just a few lines of code, I highly recommend **experimenting
    with all of them before determining which one aligns best with your requirements**.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后的免责声明，请注意，这项测试仅使用了一个特定的提示，并且没有利用其他可用的参数来控制生成。每个模型都有其独特的优势，并且可以根据主题生成更具吸引力或不那么吸引人的输出。考虑到我们讨论的仅仅是改变几行代码，我强烈建议**在确定哪一个最符合你的需求之前，先对所有模型进行实验**。
- en: Useful Resources
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有用资源
- en: '**Diffusers documentation**: [unCLIP](https://huggingface.co/docs/diffusers/api/pipelines/unclip)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Diffusers 文档**: [unCLIP](https://huggingface.co/docs/diffusers/api/pipelines/unclip)'
- en: '**Scientific paper**: [Hierarchical Text-Conditional Image Generation with
    CLIP Latents](https://arxiv.org/abs/2204.06125) (unCLIP/DALL·E-2)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**: [基于 CLIP 潜变量的层次化文本条件图像生成](https://arxiv.org/abs/2204.06125) (unCLIP/DALL·E-2)'
- en: '**Scientific paper**: [Mind the Gap: Understanding the Modality Gap in Multi-modal
    Contrastive Representation Learning](https://arxiv.org/abs/2203.02053)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**: [注意差距: 理解多模态对比表示学习中的模态差距](https://arxiv.org/abs/2203.02053)'
- en: '**Scientific paper**: [GLIDE: Towards Photorealistic Image Generation and Editing
    with Text-Guided Diffusion Models](https://arxiv.org/abs/2112.10741)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**: [GLIDE: 基于文本引导的扩散模型用于逼真图像生成与编辑](https://arxiv.org/abs/2112.10741)'
- en: '**Scientific paper**: [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**: [扩散模型在图像合成上超越 GANs](https://arxiv.org/abs/2105.05233)'
- en: '**Scientific paper**: [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**: [无分类器扩散引导](https://arxiv.org/abs/2207.12598)'
- en: '**GitHub page**: [Karlo](https://github.com/kakaobrain/karlo)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GitHub 页面**: [Karlo](https://github.com/kakaobrain/karlo)'
- en: '**GitHub page**: [DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GitHub 页面**: [DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch)'
- en: '**Appendix**: [CLIP](#13e9)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**附录**: [CLIP](#13e9)'
- en: '**Appendix**: [VQGAN](#9eb6)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**附录**: [VQGAN](#9eb6)'
- en: 'Pipeline: DeepFloyd IF (Imagen)'
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '流程: DeepFloyd IF (Imagen)'
- en: Unraveling the Theory
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解开理论
- en: '[DeepFloyd IF](https://www.deepfloyd.ai/deepfloyd-if) is a model inspired by
    [**Imagen**](https://imagen.research.google/), a Google text-to-image model.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[DeepFloyd IF](https://www.deepfloyd.ai/deepfloyd-if) 是一个受 [**Imagen**](https://imagen.research.google/)
    启发的模型，Imagen 是 Google 的文本到图像模型。'
- en: '![](../Images/846c2621afec019bdb4787a4c122b4fb.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/846c2621afec019bdb4787a4c122b4fb.png)'
- en: Image from the [Google blog post about Imagen](https://imagen.research.google/).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [Google 关于 Imagen 的博客文章](https://imagen.research.google/) 的图像。
- en: We have already seen all the elements of these models; both use a text-to-image
    diffusion model that generates a low-resolution image, 64x64\. This image is then
    upscaled to higher resolutions, first to 256x256 and then to 1024x1024 by other
    two models.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到这些模型的所有元素；它们都使用一个文本到图像的扩散模型生成一个低分辨率图像，64x64。然后，这个图像通过另外两个模型被放大到更高的分辨率，首先是
    256x256，然后是 1024x1024。
- en: As the **text encoder**, both models employ a large pre-trained [**Text-To-Text
    Transfer Transformer**](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)
    (**T5**) by Google, which reframe all NLP tasks into a unified text-to-text format
    where input and output are always text strings. The text encoder used appears
    to be a crucial element in DeepFloyd IF/Imagen, as T5 possesses a broader language
    understanding compared to CLIP.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 作为**文本编码器**，两个模型都使用由 Google 提供的大型预训练[**文本到文本转换 Transformer**](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)
    (**T5**)，它将所有 NLP 任务重新框定为统一的文本到文本格式，其中输入和输出始终是文本字符串。所使用的文本编码器在 DeepFloyd IF/Imagen
    中似乎是一个关键要素，因为 T5 比 CLIP 具有更广泛的语言理解能力。
- en: Similar to the previously presented models, also in this case, diffusion models
    are implemented as U-Nets. For super-resolution models, Imagen introduces an **Efficient
    U-Net**, claimed to be simpler, faster converging, and more memory-efficient compared
    to prior implementations. The changes made to the U-Net in comparison to previous
    diffusion models involve “shifting” some of the parameters from high-resolution
    blocks to low-resolution ones (which have more channels and contain more semantic
    knowledge respect to initial blocks), using more residual blocks at low resolution,
    and altering the order of convolution operations concerning up/downsampling. In
    the case of Imagen, downsampling is performed before convolution, and the opposite
    is true for upsampling.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前提出的模型类似，本案例中的扩散模型也实现为 U-Net。对于超分辨率模型，Imagen 引入了一个**高效 U-Net**，声称比之前的实现更简单、收敛更快、内存使用更高效。与之前的扩散模型相比，U-Net
    的变化包括将一些参数从高分辨率块“转移”到低分辨率块（这些块具有更多通道，包含更多语义知识），在低分辨率下使用更多残差块，以及更改卷积操作相对于上下采样的顺序。在
    Imagen 中，卷积之前进行下采样，反之对于上采样。
- en: 'Finally, Imagen emphasizes the significance of **classifier-free guidance**.
    According to the paper:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Imagen 强调了**无分类器引导**的重要性。根据论文：
- en: We corroborate the results of recent text-guided diffusion work [ 16 , 41 ,
    54] and find that increasing the classifier-free guidance weight improves image-text
    alignment, but damages image fidelity producing highly saturated and unnatural
    images [ 27 ].
  id: totrans-223
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们验证了近期的文本引导扩散工作 [16, 41, 54] 的结果，并发现增加无分类器指导权重可以改善图像-文本对齐，但会损害图像保真度，产生高度饱和和不自然的图像
    [27]。
- en: To benefit from improved image-text alignment without compromising fidelity,
    two types of thresholding are discussed. The first, previously used by other works,
    is **static thresholding**, which clips the x-prediction to the range [-1, 1],
    the same range as the training data x. It is exactly the discrepancy between what
    the model has seen during training and what it encounters during inference that
    causes the issue. Static thresholding is essential with large guidance weights
    but still results in oversaturated and less detailed images as the weight increase.
    Therefore, the authors introduce **dynamic thresholding**. This technique involves
    initially choosing a certain percentile of absolute pixel value, let’s say 80%.
    If the value of this percentile, s, exceeds 1 (i.e., more than 20% of pixels are
    greater than 1 in absolute value), all pixels outside the range [-s, s] are clipped.
    After this, the values are scaled by s, bringing everything into the range [-1,
    1]. Discarding the extreme pixels before normalization helps mitigate the oversaturation
    problem.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在不影响图像保真度的情况下改善图像-文本对齐，讨论了两种阈值处理方法。第一种是**静态阈值处理**，它将 x 预测值裁剪到范围 [-1, 1]，这是与训练数据
    x 相同的范围。正是模型在训练期间见过的内容与推断过程中遇到的内容之间的差异导致了这个问题。静态阈值处理在大型指导权重下是必要的，但随着权重的增加，仍然会导致图像过度饱和和细节减少。因此，作者引入了**动态阈值处理**。这项技术最初选择绝对像素值的某个百分位数，例如
    80%。如果这个百分位数的值 s 超过 1（即超过 20% 的像素绝对值大于 1），则所有超出范围 [-s, s] 的像素都会被裁剪。之后，值会通过 s 进行缩放，将所有内容带入范围
    [-1, 1]。在归一化之前丢弃极端像素有助于缓解过度饱和问题。
- en: '**DeepFloyd IF** seems to closely resemble Imagen, but without a paper that
    delves into the details of this architecture, it’s uncertain whether there are
    any significant modifications I might have missed. According to the authors, DeepFloyd
    IF outperforms the original Imagen.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '**DeepFloyd IF** 看起来与 Imagen 非常相似，但由于没有深入探讨该架构细节的论文，因此不确定是否存在我可能遗漏的重要修改。根据作者的说法，DeepFloyd
    IF 的表现优于原始的 Imagen。'
- en: Applications and Limitations
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用与局限
- en: DeepFloyd IF can be employed for all the mentioned applications. However, unlike
    Stable Diffusion and unCLIP, currently, users need to accept the DeepFloyd LICENSE
    AGREEMENT before utilizing it.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: DeepFloyd IF 可以用于所有上述应用。然而，与 Stable Diffusion 和 unCLIP 不同，目前用户在使用之前需要接受 DeepFloyd
    LICENSE AGREEMENT。
- en: Useful Resources
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有用资源
- en: '**Diffusers documentation**: [DeepFloyd IF](https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩散器文档**: [DeepFloyd IF](https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if)'
- en: '**Scientific blog**: [DeepFloyd IF](https://www.deepfloyd.ai/deepfloyd-if)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学博客**: [DeepFloyd IF](https://www.deepfloyd.ai/deepfloyd-if)'
- en: '**Scientific blog**: [Imagen](https://imagen.research.google/)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学博客**: [Imagen](https://imagen.research.google/)'
- en: '**Scientific blog**: [Exploring Transfer Learning with T5: the Text-To-Text
    Transfer Transformer](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学博客**: [探索 T5 的迁移学习: 文本到文本转换器](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)'
- en: 'Pipeline: Kandinsky'
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '流水线: Kandinsky'
- en: Unraveling the Theory
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解开理论
- en: Kandinsky is an [AI Forever](https://github.com/ai-forever) model that inherits
    best practices from DALL·E-2 and Latent Diffusion while introducing some new ideas.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Kandinsky 是一个 [AI Forever](https://github.com/ai-forever) 模型，它继承了 DALL·E-2 和潜在扩散的最佳实践，同时引入了一些新想法。
- en: '![](../Images/9e78587e375847c021a4e0970b2143ba.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e78587e375847c021a4e0970b2143ba.png)'
- en: Image from the Kandinsky GitHub page.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 Kandinsky GitHub 页面。
- en: Just like DALL·E-2, Kandinsky incorporates a **prior** model (**Diffusion Mapping**)
    to predict CLIP image embeddings based on CLIP text embeddings. Furthermore, akin
    to Latent Diffusion, the diffusion model doesn’t **operate** in pixel space like
    DALL·E-2/Imagen, but rather **in the** **latent space**.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 DALL·E-2 一样，Kandinsky 采用了一个**先验**模型（**扩散映射**）来基于 CLIP 文本嵌入预测 CLIP 图像嵌入。此外，类似于潜在扩散，这个扩散模型并不像
    DALL·E-2/Imagen 那样**在像素空间中操作**，而是**在潜在空间中**。
- en: An important difference is that the latest versions, 2.2 and 2.1, of Kandinsky
    use [**XLM-RoBERTa**](https://arxiv.org/abs/1911.02116) as the text encoder, thus
    making the model **multilingual**.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的区别是Kandinsky的最新版本2.2和2.1使用了[**XLM-RoBERTa**](https://arxiv.org/abs/1911.02116)作为文本编码器，从而使模型**多语言**。
- en: In contrast to DALL·E-2, **the output of the prior model doesn’t go directly
    into a decoder**; instead, it’s first directed towards a latent diffusion model.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 与DALL·E-2相比，**前一个模型的输出不会直接进入解码器**；而是首先导向一个潜在的扩散模型。
- en: The **decoder** is [**MoVQ**](https://arxiv.org/abs/2209.09002), a model similar
    to VQGAN (refer to [Appendix — VQGAN](#9eb6)), which is improved by incorporating
    **spatially conditional normalization** to tackle the issue of mapping similar
    neighboring patches to the same codebook index. This addressing prevents the occurrence
    of repeated artifacts in regions with similar adjacent content. Moreover, the
    model incorporates multichannel quantization to enhance its flexibility. For the
    second stage, the autoregressive transformer is replaced with a significantly
    faster, thanks to its parallel rather than sequential nature, [**Masked Generative
    Image Transformer**](https://arxiv.org/abs/2202.04200) (**MaskGIT**).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '**解码器**是[**MoVQ**](https://arxiv.org/abs/2209.09002)，一个类似于VQGAN的模型（参考[附录 —
    VQGAN](#9eb6)），通过引入**空间条件归一化**来解决将相似的相邻补丁映射到相同代码本索引的问题。这种处理方法防止了在相邻内容相似的区域出现重复的伪影。此外，该模型还结合了多通道量化，以增强其灵活性。第二阶段中，自回归变换器被一个显著更快的[**掩码生成图像变换器**](https://arxiv.org/abs/2202.04200)
    (**MaskGIT**)所替代，这得益于其并行而非顺序的特性。'
- en: Applications and Limitations
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用与局限性
- en: We have already seen an output from Kandinsky; this model is undoubtedly among
    the most promising at this moment and competes with the best current available
    diffusion models. Its usage and limitations are similar to those of Stable Diffusion.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到Kandinsky的一个输出；这个模型无疑是目前最有前途的模型之一，并与当前最好的扩散模型竞争。它的使用和局限性类似于Stable Diffusion。
- en: Useful Resources
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有用的资源
- en: '**Diffusers documentation**: [Kandinsky](https://huggingface.co/docs/diffusers/api/pipelines/kandinsky)/[Kandinsky
    2.2](https://huggingface.co/docs/diffusers/api/pipelines/kandinsky_v22)'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Diffusers文档**: [Kandinsky](https://huggingface.co/docs/diffusers/api/pipelines/kandinsky)/[Kandinsky
    2.2](https://huggingface.co/docs/diffusers/api/pipelines/kandinsky_v22)'
- en: '**Scientific paper**: [Unsupervised Cross-lingual Representation Learning at
    Scale](https://arxiv.org/abs/1911.02116) (XLM-RoBERTa)'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**: [无监督跨语言表示学习的规模](https://arxiv.org/abs/1911.02116) (XLM-RoBERTa)'
- en: '**Scientific paper**: [MoVQ: Modulating Quantized Vectors for High-Fidelity
    Image Generation](https://arxiv.org/abs/2209.09002)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**: [MoVQ: 调制量化向量以生成高保真图像](https://arxiv.org/abs/2209.09002)'
- en: '**Scientific paper**: [MaskGIT: Masked Generative Image Transformer](https://arxiv.org/abs/2202.04200)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**: [MaskGIT: 掩码生成图像变换器](https://arxiv.org/abs/2202.04200)'
- en: '**Scientific blog**: [Kandinsky 2.1, or When +0.1 means a lot](https://habr.com/ru/companies/sberbank/articles/725282/)
    (thanks Google Translate!)'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学博客**: [Kandinsky 2.1，或者+0.1意味着很多](https://habr.com/ru/companies/sberbank/articles/725282/)（感谢谷歌翻译！）'
- en: '**GitHub page**: [Kandinsky 2.2](https://github.com/ai-forever/Kandinsky-2)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GitHub 页面**: [Kandinsky 2.2](https://github.com/ai-forever/Kandinsky-2)'
- en: '**Appendix**: [CLIP](#13e9)'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**附录**: [CLIP](#13e9)'
- en: '**Appendix**: [VQGAN](#9eb6)'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**附录**: [VQGAN](#9eb6)'
- en: 'Pipeline: ControlNet'
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '流程: ControlNet'
- en: Unraveling the Theory
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解开理论
- en: '[**ControlNet**](https://arxiv.org/abs/2302.05543) is a technique for conditioning
    the generation of a diffusion model, controlling the structure of what is generated.
    To some extent, and although the two techniques are complementary, it’s like an
    enhanced SDEdit. The main idea is to automatically generate conditional inputs
    such as edge maps, segmentation maps, keypoints, etc., and then teach the diffusion
    model to generate outputs that adhere to the structure of these conditioning inputs.
    In the original ControlNet paper, Stable Diffusion is used as the base, but this
    technique can be applied to any model.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[**ControlNet**](https://arxiv.org/abs/2302.05543)是一种条件生成扩散模型的技术，控制生成内容的结构。在一定程度上，尽管这两种技术是互补的，但它类似于增强版的SDEdit。主要思想是自动生成条件输入，如边缘图、分割图、关键点等，然后教导扩散模型生成符合这些条件输入结构的输出。在原始ControlNet论文中，使用了Stable
    Diffusion作为基础，但该技术可以应用于任何模型。'
- en: '![](../Images/939630e907f176840ec1ac9429c82de8.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/939630e907f176840ec1ac9429c82de8.png)'
- en: Firstly, a copy of the original model is created. The original model is frozen,
    while the copy is linked to it through a series of zero convolutions.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: A **zero convolution** is simply a 1x1 convolution where both the weights and
    biases are initialized with zero. This type of initialization, along with the
    fact that the weights of the original model are frozen, ensures that initially,
    the system is identical to the starting model and only gradually starts to use
    the conditioning to guide the generation, without forgetting what was originally
    learned during the extensive training process.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Conditioning involves some form of processing (usually automated) of the input.
    For instance, we can extract edges from the initial image using a [Canny edge
    detector](https://en.wikipedia.org/wiki/Canny_edge_detector) and teach the model
    to generate variations faithful to the structure of the original image but with
    different characteristics that can be guided through textual prompts.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/81808fa8aeb080241d90a17cd22d7837.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
- en: 'The potential **conditioning inputs** are limited only by our imagination;
    the authors mention more than a dozen, and over time, the community is inventing
    new ones. To name a few: edges (e.g., extracted with [Canny](https://en.wikipedia.org/wiki/Canny_edge_detector)),
    human poses (e.g., extracted with [OpenPifPaf](https://openpifpaf.github.io/intro.html)
    or [OpenPose](https://cmu-perceptual-computing-lab.github.io/openpose/web/html/doc/index.html)),
    semantic maps, depth maps, and so on. Clearly, during training, automation of
    the extraction is important to speed up the creation of the initial dataset. During
    inference, there’s no restriction on hand-drawing a segmentation maps or even
    sketching what we want, as it’s possible to provide a similar input automatically
    during training using [HED](https://arxiv.org/abs/1504.06375) boundary detection
    and a range of robust data augmentations or alternative techniques, mimicking
    human sketches.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Applications and Limitations
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ControlNet is one of the tools that those who appreciate generative art must
    have in their toolkit. It’s possible to train your own ControlNet from scratch
    with reasonably limited resources, but often it’s not necessary, and you can use
    ControlNets already trained by the community. The main limitation of this technique
    is that conditioning often depends on having a starting image with a structure
    similar to the desired result, or the ability to manually produce an equivalent
    conditioning. Finally, it’s worth noting that it’s also possible to [combine multiple
    ControlNets](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#combining-multiple-conditionings),
    conditioning, for example, one part of an image on edges and another on a human
    pose.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Useful Resources
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Diffusers documentation**: [ControlNet](https://huggingface.co/docs/diffusers/api/pipelines/controlnet)/[ControlNet
    with Stable Diffusion XL](https://huggingface.co/docs/diffusers/api/pipelines/controlnet_sdxl)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Diffusers 文档**：[ControlNet](https://huggingface.co/docs/diffusers/api/pipelines/controlnet)/[带有
    Stable Diffusion XL 的 ControlNet](https://huggingface.co/docs/diffusers/api/pipelines/controlnet_sdxl)'
- en: '**Scientific paper**: [Adding Conditional Control to Text-to-Image Diffusion
    Models](https://arxiv.org/abs/2302.05543) (ControlNet)'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**：[将条件控制添加到文本到图像扩散模型](https://arxiv.org/abs/2302.05543)（ControlNet）'
- en: '**Scientific blog**: [Ultra fast ControlNet with 🧨 Diffusers](https://huggingface.co/blog/controlnet)'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学博客**：[超快 ControlNet 与 🧨 Diffusers](https://huggingface.co/blog/controlnet)'
- en: 'Pipeline: InstructPix2Pix'
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流程：InstructPix2Pix
- en: Unraveling the Theory
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解开理论
- en: '[**InstructPix2Pix**](https://arxiv.org/abs/2211.09800) is a method for teaching
    a generative model to follow human-written instructions for image editing.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '[**InstructPix2Pix**](https://arxiv.org/abs/2211.09800) 是一种教学生成模型跟随人工编写指令进行图像编辑的方法。'
- en: '![](../Images/6cda1261db244191c6ca7de661a888cd.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6cda1261db244191c6ca7de661a888cd.png)'
- en: The method consists of three phases. Firstly, generate a set of input captions,
    edit instructions, and edited captions. Then, employ another technique called
    Prompt-to-Prompt (see [Appendix — Prompt-to-Prompt](#ec8a)) to generate a dataset
    of image pairs associated with the input and edited captions. Finally, train a
    generative model to produce the requested modification based on the given instruction.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法包括三个阶段。首先，生成一组输入字幕、编辑指令和编辑字幕。然后，使用另一种叫做 Prompt-to-Prompt 的技术（见 [附录 — Prompt-to-Prompt](#ec8a)）生成与输入和编辑字幕相关联的图像对数据集。最后，训练生成模型以根据给定的指令产生请求的修改。
- en: The **instructions and edited captions**, as shown in the figure, **are generated
    semi-automatically**. [GPT-3](https://arxiv.org/abs/2005.14165), the powerful
    language model by OpenAI, is fine-tuned on a small sample of LAION captions, to
    which manually crafted edit instructions and resulting edited captions are added.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '**指令和编辑的字幕**，如图所示，**是半自动生成的**。[GPT-3](https://arxiv.org/abs/2005.14165)，由 OpenAI
    开发的强大语言模型，经过了少量 LAION 字幕的微调，添加了手动制作的编辑指令和生成的编辑字幕。'
- en: At this point, we have all the components to **generate variations of the original
    images using Prompt-to-Prompt**. An important aspect here is that, depending on
    the type of instruction given, it may be required for the generated image to remain
    more or less faithful to the original image. For instance, consider the difference
    between requesting “make the hair blonde” versus “make it a Miro painting”. Fortunately,
    Prompt-to-Prompt has a parameter to adjust how much attention should be given
    to the original image versus the prompt. Unfortunately, this parameter varies
    case by case.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们拥有了所有的组件来**使用 Prompt-to-Prompt 生成原始图像的变体**。一个重要的方面是，根据给定的指令类型，生成的图像可能需要更多或更少地忠于原始图像。例如，考虑请求“将头发变成金色”和“将其变成米罗画”的区别。幸运的是，Prompt-to-Prompt
    有一个参数可以调整对原始图像与提示之间的关注程度。不幸的是，这个参数因情况而异。
- en: 'To address this issue, InstructPix2Pix generates 100 pairs of images for each
    caption in the training set, varying this parameter. These **pairs are** **then
    filtered** using a CLIP-based metric: [**directional similarity in CLIP**](https://arxiv.org/abs/2108.00946).
    This metric measures how consistent the change between two images (in CLIP space)
    is with the change between the two image captions. Besides enhancing the quality
    of the generated dataset, this filtering also enhances the model’s robustness
    to Prompt-to-Prompt and Stable Diffusion failures.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，InstructPix2Pix 为训练集中每个字幕生成了 100 对图像，改变这个参数。这些**对** **然后通过** CLIP 基于度量进行过滤：[**CLIP中的方向相似度**](https://arxiv.org/abs/2108.00946)。这个度量衡量两个图像（在
    CLIP 空间）之间的变化与两个图像字幕之间的变化的一致性。除了提高生成数据集的质量外，这种过滤还增强了模型对 Prompt-to-Prompt 和 Stable
    Diffusion 失败的鲁棒性。
- en: To input the text edit instruction, the authors reuse the same text conditioning
    mechanism that was initially intended for captions. Meanwhile, for the input image
    to be modified, they simply add input channels to the first convolutional layer.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 为了输入文本编辑指令，作者重新使用了最初为字幕设计的相同文本条件机制。同时，对于需要修改的输入图像，他们仅在第一个卷积层中添加了输入通道。
- en: Finally, they employ a form of **classifier-free diffusion guidance** to weigh
    the image more or less with respect to the text, allowing some control over how
    closely it adheres to the input image in comparison to following the edit instruction.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，他们采用一种 **无分类器的扩散引导** 形式，根据文本对图像进行加权，从而在遵循编辑指令时，能够对图像如何紧密符合输入图像进行一定控制。
- en: '![](../Images/d54d98809c0b1193dfc2c6846e470c74.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d54d98809c0b1193dfc2c6846e470c74.png)'
- en: 'Eq. 3 from [InstructPix2Pix: Learning to Follow Image Editing Instructions](https://arxiv.org/abs/2211.09800).'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '来自 [InstructPix2Pix: Learning to Follow Image Editing Instructions](https://arxiv.org/abs/2211.09800)
    的公式 3。'
- en: Applications and Limitations
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用与局限性
- en: InstructPix2Pix is a very useful technique when one wants to modify an image
    through text without significantly altering elements unrelated to the requested
    modification. This is different from generating two images where the second one
    has only a slightly modified prompt. Clearly, this technique doesn’t work flawlessly
    100% of the time and encounters issues when asked to change the viewpoint, swap
    object positions, and sometimes, although not as frequently as other techniques,
    it can lead to unintended excessive changes to the image.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: InstructPix2Pix 是一种非常有用的技术，当你希望通过文本修改图像时，而不显著改变与请求的修改无关的元素。这与生成两张图像，其中第二张图像只有略微修改的提示不同。显然，这种技术并非
    100% 完美，且在要求更改视角、交换物体位置时会遇到问题，有时，尽管不如其他技术那样频繁，但它可能会导致图像发生意外的过度变化。
- en: Useful Resources
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有用的资源
- en: '**Diffusers documentation**: [InstructPix2Pix](https://huggingface.co/docs/diffusers/api/pipelines/pix2pix)'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Diffusers 文档**：[InstructPix2Pix](https://huggingface.co/docs/diffusers/api/pipelines/pix2pix)'
- en: '**Scientific paper**: [InstructPix2Pix: Learning to Follow Image Editing Instructions](https://arxiv.org/abs/2211.09800)'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**：[InstructPix2Pix: Learning to Follow Image Editing Instructions](https://arxiv.org/abs/2211.09800)'
- en: '**Scientific paper**: [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
    (GPT-3)'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**：[语言模型是少样本学习者](https://arxiv.org/abs/2005.14165)（GPT-3）'
- en: '**Scientific paper**: [StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image
    Generators](https://arxiv.org/abs/2108.00946) (directional similarity in CLIP)'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**：[StyleGAN-NADA: CLIP 指导的图像生成器领域适应](https://arxiv.org/abs/2108.00946)（CLIP
    中的方向相似性）'
- en: '**Appendix**: [CLIP](#13e9)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**附录**：[CLIP](#13e9)'
- en: '**Appendix**: [Prompt-to-Prompt](#ec8a)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**附录**：[Prompt-to-Prompt](#ec8a)'
- en: Appendix
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录
- en: CLIP
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CLIP
- en: 'The fundamental idea behind [**CLIP**](https://openai.com/research/clip) is
    as simple as it is powerful: training two Transformer encoders, one for images
    and one for text, on a dataset of associated images and texts to produce similar
    embeddings when the text refers to the image, and dissimilar embeddings otherwise.
    With respect to the matrix shown in the figure, the objective is to maximize the
    sum of scalar products along the diagonal and minimize those off the diagonal:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '[**CLIP**](https://openai.com/research/clip) 的基本理念既简单又强大：训练两个 Transformer 编码器，一个用于图像，一个用于文本，在一个关联图像和文本的数据集上进行训练，以便当文本指代图像时产生相似的嵌入，而在其他情况下产生不同的嵌入。对于图中所示的矩阵，目标是最大化对角线上的标量积总和，并最小化对角线外的标量积：'
- en: '![](../Images/62c39422a94ea89b7caf7688871b53b5.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62c39422a94ea89b7caf7688871b53b5.png)'
- en: Image from the [OpenAI blog post about CLIP](https://openai.com/research/clip).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [OpenAI 关于 CLIP 的博客文章](https://openai.com/research/clip) 的图像。
- en: Since the outputs of the encoders are normalized before scalar products, these
    correspond to measuring the [**cosine similarity**](https://en.wikipedia.org/wiki/Cosine_similarity)
    between two vectors, i.e., how much the embeddings “point in the same direction”.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 由于编码器的输出在进行标量积之前已被标准化，因此这些输出相当于测量两个向量之间的 [**余弦相似度**](https://en.wikipedia.org/wiki/Cosine_similarity)，即嵌入“指向相同方向”的程度。
- en: Useful Resources
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有用的资源
- en: '**Hugging Face documentation**: [CLIP](https://huggingface.co/docs/transformers/model_doc/clip)'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hugging Face 文档**：[CLIP](https://huggingface.co/docs/transformers/model_doc/clip)'
- en: '**Scientific blog**: [CLIP: Connecting text and images](https://openai.com/research/clip)'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学博客**：[CLIP: Connecting text and images](https://openai.com/research/clip)'
- en: VQGAN
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VQGAN
- en: In this section, I’ll introduce [VQGAN](https://arxiv.org/abs/2012.09841) and
    touch on [VQVAE](https://arxiv.org/abs/1711.00937).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将介绍 [VQGAN](https://arxiv.org/abs/2012.09841) 并简要提及 [VQVAE](https://arxiv.org/abs/1711.00937)。
- en: '![](../Images/4b1a63f5d379c08a1afece814f0ce45f.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b1a63f5d379c08a1afece814f0ce45f.png)'
- en: Fig. 2\. from [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 来自 [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841)。
- en: In the image above, if we consider only *E*, *ẑ*, and *G*, what we have is an
    Autoencoder. VQGAN builds upon **VQVAE** and employs a regularization technique
    known as **Vector Quantization** (**VQ**). For each spatial position of the encoder
    output *ẑ*, the corresponding vector (whose size depends on the number of channels
    in *ẑ*) is substituted with the nearest vector from a learnable “codebook”. This
    effectively limits the possible inputs to the decoder during inference, allowing
    them to only be combinations of the learned “codes” and then quantizing the latent
    space.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，如果我们只考虑 *E*、*ẑ* 和 *G*，我们得到的是一个自编码器。VQGAN 在 **VQVAE** 的基础上建立，并采用一种称为**向量量化**
    (**VQ**) 的正则化技术。对于编码器输出 *ẑ* 的每个空间位置，对应的向量（其大小取决于 *ẑ* 中的通道数）会被替换为来自可学习“代码本”的最近向量。这有效地限制了推理过程中解码器的可能输入，使其只能是学习到的“代码”的组合，从而对潜在空间进行量化。
- en: The **loss** function employed by **VQVAE**, *L*VQ, comprises three terms.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '**VQVAE** 使用的**损失**函数 *L*VQ 由三部分组成。'
- en: '![](../Images/84bce52f1fb5bdedc608d9f2885219f4.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84bce52f1fb5bdedc608d9f2885219f4.png)'
- en: Eq. 4 from [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841)
    的公式 4。
- en: The first is a **reconstruction loss**, *L*rec; the second term penalizes the
    codebook when its elements are distant from the outputs of the encoder. The third
    term, also called the “**commitment loss**”, penalizes the encoder when its output
    embeddings are distant from the codes in the codebook (we want the encoder to
    “commit” to a certain codebook).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是 **重建损失**，*L*rec；第二项是当代码本中的元素距离编码器输出较远时对代码本的惩罚。第三项，也称为“**承诺损失**”，在编码器的输出嵌入距离代码本中的代码较远时对编码器进行惩罚（我们希望编码器“承诺”某个代码本）。
- en: '**VQGAN** replaces the reconstruction loss with a **perceptual loss**. Specifically,
    it employs the [**Learned Perceptual Image Patch Similarity**](https://arxiv.org/abs/1801.03924)
    (**LPIPS**), which uses a pre-trained VGG16 network to extract features from both
    the generated and target images, and then calculates the differences between these
    features.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '**VQGAN** 用**感知损失**替代了重建损失。具体而言，它使用 [**Learned Perceptual Image Patch Similarity**](https://arxiv.org/abs/1801.03924)
    (**LPIPS**)，利用预训练的 VGG16 网络从生成图像和目标图像中提取特征，然后计算这些特征之间的差异。'
- en: 'Second, it introduces an **adversarial** training procedure with a **patch-based**
    discriminator *D*, aiming to differentiate between real (*x*) and reconstructed
    (*x̂*) images:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，它引入了一种**对抗**训练过程，使用 **基于块的** 判别器 *D*，旨在区分真实图像 (*x*) 和重建图像 (*x̂*)：
- en: '![](../Images/6ccbd7c2793fb7d841f2b695dfad8a46.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ccbd7c2793fb7d841f2b695dfad8a46.png)'
- en: Eq. 5 from [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841)
    的公式 5。
- en: 'The complete objective is as follows:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的目标如下：
- en: '![](../Images/ad3547e09294735a0b0404199e85cc0b.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad3547e09294735a0b0404199e85cc0b.png)'
- en: Eq. 6 from [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841).
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841)
    的公式 6。
- en: 'Here, λ represents an **adaptive weight** calculated using the formula:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，λ 代表使用以下公式计算的**自适应权重**：
- en: '![](../Images/38b09aa49805d736485dc03a4427e227.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38b09aa49805d736485dc03a4427e227.png)'
- en: Eq. 7 from [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841)
    的公式 7。
- en: This weight increases as the gradient of *L*rec (which, for VQGAN, corresponds
    to the perceptual loss) with respect to the last layer of the decoder intensifies.
    Conversely, it decreases when the same occurs for *L*GAN. In practice, this means
    that if *L*GAN is too sensitive to the output of the decoder, its importance is
    decreased. Conversely, if the perceptual loss (*L*rec) exhibits strong gradients,
    the importance of *L*GAN is increased, ensuring that a balance is maintained between
    the two. This approach prevents either term from being entirely ignored when one
    of them has strong gradients, thus achieving equilibrium between the two objectives.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *L*rec（对于 VQGAN，等同于感知损失）的梯度相对于解码器最后一层的梯度增强时，这个权重会增加。相反，当 *L*GAN 的梯度增强时，权重会减少。在实践中，这意味着如果
    *L*GAN 对解码器输出过于敏感，其重要性会降低。反之，如果感知损失 (*L*rec) 展现出强烈的梯度，则 *L*GAN 的重要性会增加，从而确保两者之间的平衡。这种方法防止在一个项具有强梯度时另一个项被完全忽视，从而实现两个目标之间的平衡。
- en: VQGAN uses a **two-stage approach**. We have already seen the first stage where
    an encoder, a codebook, and a decoder are learned. In the second stage, as implied
    by the title of the paper referring to “taming Transformers”, this architecture
    uses a **Transformer** to predict autoregressively the indices corresponding to
    codes in the codebook. Since we know the ground truth indices during training
    (those generated by the encoder), we can train the Transformer using maximum likelihood.
    During inference, we don’t utilize the encoder (as we don’t have an input image,
    our goal is to generate one), and we leverage the trained Transformer to generate
    sequences of indices, which are then mapped to codes that the decoder transforms
    into images.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: VQGAN 使用 **两阶段方法**。我们已经看到第一阶段，其中学习了编码器、代码本和解码器。在第二阶段，如论文标题所示的“驯化 Transformer”，该架构使用
    **Transformer** 自回归地预测代码本中代码的索引。由于我们在训练过程中知道真实的索引（由编码器生成的），我们可以使用最大似然法训练 Transformer。在推断阶段，我们不使用编码器（因为我们没有输入图像，我们的目标是生成一张），而是利用训练好的
    Transformer 生成索引序列，然后将其映射到代码，解码器将其转换为图像。
- en: Useful Resources
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有用的资源
- en: '**Diffusers documentation**: [VQModel](https://huggingface.co/docs/diffusers/api/models/vq)
    (VQVAE)'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Diffusers 文档**: [VQModel](https://huggingface.co/docs/diffusers/api/models/vq)
    (VQVAE)'
- en: '**Scientific paper**: [Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937)
    (VQVAE)'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**: [神经离散表示学习](https://arxiv.org/abs/1711.00937) (VQVAE)'
- en: '**Scientific paper**: [The Unreasonable Effectiveness of Deep Features as a
    Perceptual Metric](https://arxiv.org/abs/1801.03924) (LPIPS)'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**: [深度特征作为感知度量的非理性有效性](https://arxiv.org/abs/1801.03924) (LPIPS)'
- en: '**Scientific paper**: [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841)
    (VQGAN)'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**: [高分辨率图像合成中的 Transformer 驯化](https://arxiv.org/abs/2012.09841) (VQGAN)'
- en: Prompt-to-Prompt
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Prompt-to-Prompt
- en: '**Prompt-to-Prompt** arises from a key observation by the authors:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '**Prompt-to-Prompt** 源于作者的一个关键观察：'
- en: we analyze a text-conditioned model in depth and observe that the cross-attention
    layers are the key to controlling the relation between the spatial layout of the
    image to each word in the prompt
  id: totrans-326
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们深入分析了一个文本条件模型，并观察到交叉注意力层是控制图像空间布局与提示中每个词汇之间关系的关键。
- en: Based on this, the method essentially involves **manipulating the cross-attention
    maps**.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，该方法实质上涉及 **操控交叉注意力图**。
- en: '![](../Images/7b8e5531efd51768982b665c36283193.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b8e5531efd51768982b665c36283193.png)'
- en: For example, let’s say we want to alter an image generated with the prompt “Photo
    of a cat riding on a bicycle”, replacing the bicycle with a car while keeping
    all other elements as unchanged as possible. In this case, we can generate a new
    image with the updated prompt but fix the cross-attention maps to those of the
    previous prompt, where the weights associated with the word “car” become the ones
    that were originally associated with the word “bicycle”.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想要修改一张用“猫骑自行车”的提示生成的图像，将自行车替换为汽车，同时尽可能保持其他元素不变。在这种情况下，我们可以生成一张更新提示的新图像，但将交叉注意力图固定为之前提示的图像，其中与“汽车”相关的权重变为原本与“自行车”相关的权重。
- en: With this framework, it’s now possible to do much more than just simple word
    replacements. We can use it to give more or less emphasis to a given word, or
    even add parts to the prompt that weren’t present before. In this case, we reuse
    the attention maps only for the shared parts.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个框架，现在可以做的不仅仅是简单的词汇替换。我们可以用它来对给定的词汇给予更多或更少的强调，甚至可以在提示中添加之前不存在的部分。在这种情况下，我们仅对共享部分重用注意力图。
- en: However, keeping the attention maps fixed could overly constrain the geometry
    of the scene, which, for certain modifications to the prompt, might become too
    restrictive. To control how much attention is given to the modification and how
    much of the initial scene geometry is retained, **the injection is limited up
    to a certain timestep** *τ*.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，保持注意力图固定可能会过度限制场景的几何形状，这对于某些提示修改可能变得过于严格。为了控制对修改的关注程度以及保留初始场景几何的程度，**注入被限制在某个时间步**
    *τ*。
- en: '![](../Images/0cbfa4e61d7f075735f0e0bed0a3bafe.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0cbfa4e61d7f075735f0e0bed0a3bafe.png)'
- en: Eq. from p. 7 of [Prompt-to-Prompt Image Editing with Cross Attention Control](https://arxiv.org/abs/2208.01626).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 见于 [Prompt-to-Prompt 图像编辑与交叉注意力控制](https://arxiv.org/abs/2208.01626) 第 7 页的方程式。
- en: This ensures that after capturing the overall composition of the scene in the
    initial steps, the model can, if needed, alter the geometries in the later steps
    of the diffusion process.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了在初步捕捉到场景的整体构图后，模型可以在扩散过程的后续步骤中（如有需要）修改几何形状。
- en: Useful Resources
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实用资源
- en: '**Scientific paper**: [Prompt-to-Prompt Image Editing with Cross Attention
    Control](https://arxiv.org/abs/2208.01626)'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学论文**: [基于跨注意力控制的 Prompt-to-Prompt 图像编辑](https://arxiv.org/abs/2208.01626)'
- en: Conclusions
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Let’s summarize what we have covered in this article. To explain what lies behind
    the most popular pipelines of Diffusers, we have learned about diffusion models,
    analyzing key ones such as DDPM, Stable Diffusion, unCLIP (Karlo/DALL·E-2), DeepFloyd
    IF (Imagen), and Kandinsky. Additionally, we’ve explored techniques for gaining
    greater control over image generation, like SDEdit, ControlNet, or InstructPix2Pix.
    To truly understand these techniques, we’ve also examined important non-diffusion
    models like CLIP, VQGAN, or techniques like Prompt-to-Prompt ([with a pipeline
    for the latter that might be ready by the time you read this article](https://github.com/huggingface/diffusers/issues/2121)).
    Finally, diffusion model training is somewhat of an art, which is why we have
    also delved into important tricks like classifier-free guidance, offset noise,
    CLIP filtering, and so on.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下本文所涵盖的内容。为了揭示 Diffusers 中最受欢迎的管道背后的原理，我们了解了扩散模型，分析了关键的模型，如 DDPM、Stable
    Diffusion、unCLIP（Karlo/DALL·E-2）、DeepFloyd IF（Imagen）和 Kandinsky。此外，我们还探索了图像生成的控制技术，如
    SDEdit、ControlNet 或 InstructPix2Pix。为了真正理解这些技术，我们还研究了重要的非扩散模型，如 CLIP、VQGAN 或像
    Prompt-to-Prompt ([相关管道可能在你阅读本文时已准备好](https://github.com/huggingface/diffusers/issues/2121))。最后，扩散模型训练在某种程度上是一门艺术，因此我们还探讨了重要的技巧，如无分类器引导、偏移噪声、CLIP
    过滤等。
- en: I hope you found this article helpful. Feel free to share your thoughts in whichever
    way you prefer, I appreciate and consider feedback. If you’d like to show your
    support, sharing this article on social networks is the best way to do so.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你觉得这篇文章有帮助。欢迎以任何方式分享你的想法，我非常重视和考虑反馈。如果你想表示支持，分享这篇文章到社交网络是最好的方式。
- en: Acknowledgments
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: 'First of all, special thanks go to Letitia Parcalabescu of [AI Coffee Break](https://twitter.com/AICoffeeBreak?s=20).
    She was invaluable in two ways: first, her videos ([check them out](https://www.youtube.com/AICoffeeBreak),
    they are great!) were helpful in refreshing or clarifying some concepts for this
    article; and second, she took the time to read the first draft and provide me
    with very valuable feedback. Regarding this point, I would also like to express
    my gratitude to the reviewers at Towards Data Science who are always available
    for any inquiries and, thanks to their insights, improve the quality of the articles
    I write. Finally, thanks to you, reading up to this point is no small feat 😊!'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，特别感谢 [AI Coffee Break](https://twitter.com/AICoffeeBreak?s=20) 的 Letitia
    Parcalabescu。她的帮助有两方面：首先，她的视频（[查看一下](https://www.youtube.com/AICoffeeBreak)，非常棒！）有助于刷新或澄清本文的一些概念；其次，她花时间阅读了初稿并提供了非常宝贵的反馈。关于这一点，我还要感谢
    Towards Data Science 的审稿人，他们总是随时回答任何询问，并通过他们的见解提高了我写的文章的质量。最后，感谢你阅读到这里，真不容易 😊！
- en: Thank you for taking the time to read this article, and please feel free to
    leave a comment or connect with me to share your thoughts or ask any questions.
    To stay updated on my latest articles, you can follow me on [Medium](https://medium.com/@mnslarcher),
    [LinkedIn](https://www.linkedin.com/in/mnslarcher/) or [Twitter](https://twitter.com/mnslarcher).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你花时间阅读本文，欢迎留言或与我联系，分享你的想法或提出任何问题。要保持对我最新文章的更新，你可以关注我的 [Medium](https://medium.com/@mnslarcher)、[LinkedIn](https://www.linkedin.com/in/mnslarcher/)
    或 [Twitter](https://twitter.com/mnslarcher)。
- en: '[](https://medium.com/@mnslarcher/membership?source=post_page-----a83d64348d90--------------------------------)
    [## Join Medium with my referral link - Mario Namtao Shianti Larcher'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mnslarcher/membership?source=post_page-----a83d64348d90--------------------------------)
    [## 通过我的推荐链接加入 Medium - 马里奥·南塔奥·夏恩提·拉尔切尔'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为 Medium 的会员，你的会员费用的一部分将用于支持你阅读的作者，并且你可以完全访问所有故事……
- en: medium.com](https://medium.com/@mnslarcher/membership?source=post_page-----a83d64348d90--------------------------------)
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@mnslarcher/membership?source=post_page-----a83d64348d90--------------------------------)
