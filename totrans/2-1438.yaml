- en: Load Testing SageMaker Multi-Model Endpoints
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载测试 SageMaker 多模型端点
- en: 原文：[https://towardsdatascience.com/load-testing-sagemaker-multi-model-endpoints-f0db7b305770](https://towardsdatascience.com/load-testing-sagemaker-multi-model-endpoints-f0db7b305770)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/load-testing-sagemaker-multi-model-endpoints-f0db7b305770](https://towardsdatascience.com/load-testing-sagemaker-multi-model-endpoints-f0db7b305770)
- en: Utilize Locust to Distribute Traffic Weight Across Models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用 Locust 在模型之间分配流量权重
- en: '[](https://ram-vegiraju.medium.com/?source=post_page-----f0db7b305770--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----f0db7b305770--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f0db7b305770--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f0db7b305770--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----f0db7b305770--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ram-vegiraju.medium.com/?source=post_page-----f0db7b305770--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----f0db7b305770--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f0db7b305770--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f0db7b305770--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----f0db7b305770--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f0db7b305770--------------------------------)
    ·9 min read·Feb 24, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f0db7b305770--------------------------------)
    ·9 分钟阅读·2023年2月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/2d73100b5f9f21fc0048bf4d480216c2.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d73100b5f9f21fc0048bf4d480216c2.png)'
- en: Image from [Unsplash](https://unsplash.com/photos/mTorQ9gFfOg) by Luis Reyes
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Unsplash](https://unsplash.com/photos/mTorQ9gFfOg) 作者：Luis Reyes
- en: Productionizing Machine Learning models is a complicated practice. There’s a
    lot of iteration around different model parameters, hardware configurations, traffic
    patterns that you will have to test to try to finalize a production grade deployment.
    [Load testing](https://www.blazemeter.com/blog/performance-testing-vs-load-testing-vs-stress-testing#:~:text=but%20remain%20stable.-,What%20is%20Load%20Testing%3F,systems%20handle%20heavy%20load%20volumes.)
    is an essential software engineering practice, but also crucial to apply in the
    MLOps space to see how performant your model is in a real-world setting.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 将机器学习模型投入生产是一个复杂的过程。你需要测试不同的模型参数、硬件配置和流量模式，以尽可能地确定一个生产级别的部署。[负载测试](https://www.blazemeter.com/blog/performance-testing-vs-load-testing-vs-stress-testing#:~:text=but%20remain%20stable.-,What%20is%20Load%20Testing%3F,systems%20handle%20heavy%20load%20volumes.)
    是一种必不可少的软件工程实践，但在 MLOps 领域应用同样至关重要，以评估你的模型在实际环境中的表现。
- en: '**How can we load test?** A simple yet highly effective framework is the Python
    package: [Locust](https://locust.io/). Locust can be used in both a vanilla and
    distributed mode to simulate up to thousands of Transactions Per Second (TPS).
    For today’s blog we will assume basic understanding of this package and cover
    the fundamentals briefly, but for a more general introduction please reference
    this [article](/why-load-testing-is-essential-to-take-your-ml-app-to-production-faab0df1c4e1).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们如何进行负载测试？** 一个简单而高效的框架是 Python 包：[Locust](https://locust.io/)。Locust 可以在普通模式和分布式模式下使用，模拟每秒高达数千次的事务（TPS）。在今天的博客中，我们假设你对这个包有基本的了解，并将简要介绍其基础知识，但有关更一般的介绍，请参考这篇[文章](/why-load-testing-is-essential-to-take-your-ml-app-to-production-faab0df1c4e1)。'
- en: '**What model/endpoint will we be testing?** [SageMaker Real-Time Inference](https://aws.amazon.com/blogs/machine-learning/part-2-model-hosting-patterns-in-amazon-sagemaker-getting-started-with-deploying-real-time-models-on-sagemaker/)
    is one of the best options for serving your ML models on REST endpoints tailored
    for low latency, high throughput workloads. In this blog we’ll specifically take
    a look at an advanced hosting option known as [**SageMaker Multi-Model Endpoints**](https://aws.amazon.com/blogs/machine-learning/part-3-model-hosting-patterns-in-amazon-sagemaker-run-and-optimize-multi-model-inference-with-amazon-sagemaker-multi-model-endpoints/).
    Here we can host thousands of models behind a singular REST endpoint and specify
    a target model we want to invoke for each API call. Load testing becomes challenging
    here because we are dealing with multiple points of invoke not a singular model/endpoint.
    While it’s possible to randomly generate traffic across all models, sometimes
    users will want to control what models receive more traffic. In this example,
    we’ll take a look at how you can **distribute traffic weight across specific models**
    so you can simulate your real-world use case as closely as possible.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们将测试什么模型/终端节点？** [SageMaker 实时推断](https://aws.amazon.com/blogs/machine-learning/part-2-model-hosting-patterns-in-amazon-sagemaker-getting-started-with-deploying-real-time-models-on-sagemaker/)
    是在低延迟、高吞吐量工作负载下服务 ML 模型的最佳选择之一。在这篇博客中，我们将特别关注一种称为[**SageMaker 多模型终端节点**](https://aws.amazon.com/blogs/machine-learning/part-3-model-hosting-patterns-in-amazon-sagemaker-run-and-optimize-multi-model-inference-with-amazon-sagemaker-multi-model-endpoints/)
    的高级托管选项。在这里，我们可以在单一 REST 终端节点后托管数千个模型，并为每个 API 调用指定我们想要调用的目标模型。由于我们处理的是多个调用点而非单一模型/终端节点，负载测试变得具有挑战性。虽然可以随机生成所有模型的流量，有时用户会希望控制哪些模型接收更多流量。在这个示例中，我们将探讨如何**分配特定模型的流量权重**，以便尽可能接近模拟你的实际使用情况。'
- en: '**NOTE**: This article will assume basic knowledge of AWS and SageMaker, on
    the coding side Python fluency will be assumed along with a basic understanding
    of the Locust package. To understand load testing single model endpoints of SageMaker
    with Locust as a starter please reference this [article](https://aws.amazon.com/blogs/machine-learning/best-practices-for-load-testing-amazon-sagemaker-real-time-inference-endpoints/).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：本文假设你对 AWS 和 SageMaker 有基本的了解，编程方面假设你熟悉 Python，并对 Locust 包有基本了解。要了解如何使用
    Locust 对 SageMaker 单模型终端节点进行负载测试，请参考这篇[文章](https://aws.amazon.com/blogs/machine-learning/best-practices-for-load-testing-amazon-sagemaker-real-time-inference-endpoints/)。'
- en: Dataset Citation
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集引用
- en: In this example we’ll be using the Abalone dataset for a regression problem,
    this dataset is sourced from the UCI ML Repository (CC BY 4.0) and you can find
    the official citation [here](https://archive.ics.uci.edu/ml/citation_policy.html).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用 Abalone 数据集进行回归问题，这个数据集来源于 UCI ML Repository（CC BY 4.0），你可以在[这里](https://archive.ics.uci.edu/ml/citation_policy.html)找到官方引用。
- en: Creating A SageMaker Multi-Model Endpoint
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 SageMaker 多模型终端节点
- en: Before we can get started with load testing, we have to create our SageMaker
    Multi-Model Endpoint. All development for the creation of the endpoint will occur
    on a **SageMaker Notebook Instance** on a **conda_python3 kernel**.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始负载测试之前，我们必须创建我们的 SageMaker 多模型终端节点。所有终端节点创建的开发工作将在一个**SageMaker Notebook
    实例**上进行，使用**conda_python3 内核**。
- en: For this example we’ll utilize the Abalone dataset and run a [SageMaker XGBoost
    algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) on it
    for a regression model. You can download the dataset from the publicly available
    Amazon datasets. We will utilize this dataset to run training and create copies
    of our model artifacts to create a Multi-Model Endpoint.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将利用 Abalone 数据集，并对其运行[SageMaker XGBoost 算法](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html)以创建一个回归模型。你可以从公开的
    Amazon 数据集中下载这个数据集。我们将利用这个数据集进行训练，并创建我们模型工件的副本以创建一个多模型终端节点。
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We can first kick of a training job using the built-in SageMaker XGBoost algorithm,
    for a full guide on this process please reference this [article](https://aws.plainenglish.io/end-to-end-example-of-sagemaker-xgboost-eb9eae8a5207).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以首先使用内置的 SageMaker XGBoost 算法启动一个训练任务，关于这个过程的详细指南请参考这篇[文章](https://aws.plainenglish.io/end-to-end-example-of-sagemaker-xgboost-eb9eae8a5207)。
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: After we’ve completed this training job we will grab the generated model artifacts
    (model.tar.gz format in SageMaker) and create another copy of this artifact to
    have **two models behind our Multi-Model Endpoint**. Obviously in a real-world
    use case these models may be trained on different datasets or scale up to thousands
    of models behind the endpoint.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此训练任务后，我们将获取生成的模型工件（SageMaker 中为 model.tar.gz 格式），并创建另一个此工件的副本，以拥有**两个模型在我们的多模型端点后面**。显然，在实际使用情况下，这些模型可能在不同的数据集上进行训练，或者在端点后面扩展到成千上万个模型。
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: After we’ve made these two copies, we can specify the S3 path with our model
    artifacts for both models in our [create_model](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model)
    Boto3 API call.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在制作这两个副本后，我们可以在 [create_model](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model)
    Boto3 API 调用中指定我们两个模型的 S3 路径。
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can define our instance type and count behind the endpoint in the [endpoint
    configuration](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint_config)
    object, which we then feed to our [create_endpoint](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint)
    API call.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 [端点配置](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint_config)
    对象中定义端点后的实例类型和数量，然后将其提供给我们的 [create_endpoint](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint)
    API 调用。
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We can validate that our endpoint works with a sample data point invocation
    from the Abalone dataset. Notice that we specify a target model for a Multi-Model
    Endpoint, here we specify the model.tar.gz or model artifact that we want to invoke.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用 Abalone 数据集中的一个样本数据点调用来验证我们的端点是否有效。请注意，我们为多模型端点指定了一个目标模型，这里我们指定了我们想要调用的
    model.tar.gz 或模型工件。
- en: '[PRE7]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This [invoke_endpoint](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html#SageMakerRuntime.Client.invoke_endpoint)
    API call is essential as it is the point of contact we are evaluating in our load
    tests. We now have a functioning Multi-Model Endpoint, let’s get to testing!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 [invoke_endpoint](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html#SageMakerRuntime.Client.invoke_endpoint)
    API 调用是至关重要的，因为这是我们在负载测试中评估的接触点。我们现在有了一个功能齐全的多模型端点，让我们开始测试吧！
- en: Load Testing With Locust
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Locust 进行负载测试
- en: Let’s do a quick primer on Locust before we dive into setup of our script. Locust
    is a Python framework that let’s you define user behavior with Python code. Locust
    defines an execution as a Task. A Task in Locust is essentially the API or in
    our case the invoke_endpoint call that we want to test. Each user will run the
    tasks that we define for them in a [Python script](https://docs.locust.io/en/stable/writing-a-locustfile.html)
    that we build.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入设置脚本之前，让我们快速了解一下 Locust。Locust 是一个 Python 框架，可以让你用 Python 代码定义用户行为。Locust
    将执行定义为任务。Locust 中的任务本质上是我们要测试的 API，或在我们的案例中是 invoke_endpoint 调用。每个用户将运行我们在 [Python
    脚本](https://docs.locust.io/en/stable/writing-a-locustfile.html) 中为他们定义的任务。
- en: Locust has a vanilla mode that utilizes a single process to run your tests,
    but when you want to scale up it also has a distributed [load generation feature](https://docs.locust.io/en/stable/running-distributed.html)
    that essentially allows you to work with multiple processes and even multiple
    client machines.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Locust 有一个普通模式，利用单个进程来运行你的测试，但当你想要扩展时，它还具有分布式 [负载生成特性](https://docs.locust.io/en/stable/running-distributed.html)，本质上允许你使用多个进程甚至多个客户端机器。
- en: In this case we want to bombard our Multi-Model Endpoint with above 1000 TPS,
    so we need a powerful client machine that can handle the load that we are trying
    to generate. We can spin up an **EC2 instance**, in this case we use an **ml.c5d.18xlarge**
    and we will conduct our load testing in this environment to ensure we don’t run
    out of client side juice. To understand how to setup an EC2 instance, please read
    the following [documentation](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/connect.html).
    For our AMI we utilize a “Deep Learning AMI GPU TensorFlow 2.9.1 (Ubuntu 20.04)”,
    these Deep Learning AMIs come with a lot of pre-installed ML frameworks so I find
    them handy in these use-cases. Note that while we are using EC2 to test and invoke
    our endpoint, you can also use another client source as long as it has adequate
    compute power to handle the TPS Locust will generate.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们希望对我们的多模型端点施加超过 1000 TPS 的负载，因此我们需要一台能够处理我们试图生成的负载的强大客户端机器。我们可以启动一个**EC2
    实例**，在这种情况下，我们使用**ml.c5d.18xlarge**，并将在这个环境中进行负载测试，以确保客户端不出现性能瓶颈。要了解如何设置 EC2 实例，请阅读以下[文档](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/connect.html)。对于我们的
    AMI，我们使用“Deep Learning AMI GPU TensorFlow 2.9.1 (Ubuntu 20.04)”这一镜像，这些深度学习 AMI
    自带了很多预装的机器学习框架，因此在这些用例中非常方便。请注意，虽然我们使用 EC2 来测试和调用我们的端点，你也可以使用其他客户端，只要它具有足够的计算能力来处理
    Locust 生成的 TPS。
- en: Once you are SSHd into your EC2 instance we can get into defining our locust
    script. We first define the boto3 client that will be conducting our invoke_endpoint
    call that we are measuring. We’ve parameterized a few of these with a distributed
    shell script that we will cover later.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你通过 SSH 进入你的 EC2 实例，我们可以开始定义我们的 locust 脚本。我们首先定义了一个 boto3 客户端，该客户端将进行我们要测量的
    invoke_endpoint 调用。我们将用一个分布式 shell 脚本对其中的一些参数进行参数化，稍后会详细介绍。
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now is when we get specific for Multi-Model Endpoints. We define two methods,
    **each method will hit one of our two target models**.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始具体讨论多模型端点。我们定义了两种方法，**每种方法将触及我们两个目标模型中的一个**。
- en: '[PRE9]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now in the future if you have 200 models do you need a method for each? Not
    necessarily, you can specify the target model string to fit into the models you
    need. For example if you have 200 models and wanted 5 models to be invoked for
    a specific method we can set the TargetModel parameter to something like the following
    snippet.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你有200个模型，是否需要为每个模型都准备一种方法？不一定，你可以指定目标模型字符串以适应你需要的模型。例如，如果你有200个模型，并且希望特定方法调用5个模型，我们可以将
    TargetModel 参数设置为如下片段。
- en: '[PRE11]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The more specific you want to get the more methods you may have to define, but
    if you have a general idea that a certain number of models will receive the majority
    of the traffic then some string manipulation like the above will suffice.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要更具体，可能需要定义更多的方法，但如果你大致了解某些模型会接收到大多数流量，那么像上述的字符串操作就足够了。
- en: Finally we can define the task weight via a decorator. Our first model now is
    three times more likely to receive traffic than the second.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过装饰器定义任务权重。我们第一个模型现在比第二个模型更有可能接收到流量，概率是前者的三倍。
- en: '[PRE12]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: With the task decorator we can define the weight and you can expand and manipulate
    these depending on your traffic pattern.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任务装饰器，我们可以定义权重，你可以根据流量模式扩展和调整这些权重。
- en: Lastly, there is also a shell script we have defined in this repository that
    you can utilize to increase or decrease traffic.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在这个仓库中定义了一个可以用来增加或减少流量的 shell 脚本。
- en: '[PRE13]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here we are defining the parameters our locust script is reading, but also most
    importantly two Locust specific parameters in users and workers. Here you can
    define a number of users that will be distributed across different workers. You
    can scale these as multiples either up or down to try to achieve your target TPS.
    We can execute our distributed test by running the following command.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了 locust 脚本读取的参数，但更重要的是，定义了两个 Locust 特有的参数：用户和工作者。在这里，你可以定义一个用户数量，这些用户将分布在不同的工作者上。你可以根据需要将这些数量扩大或缩小，以尝试实现目标
    TPS。我们可以通过运行以下命令来执行我们的分布式测试。
- en: '[PRE14]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Once we kick this off we can see in our EC2 instance CLI a load test is up and
    running.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启动测试，我们可以在我们的 EC2 实例 CLI 中看到负载测试正在进行。
- en: '![](../Images/57ad767aac4225776ea988c962613822.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57ad767aac4225776ea988c962613822.png)'
- en: Locust Distributed Load Test (Screenshot by Author)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Locust 分布式负载测试（作者截图）
- en: Monitoring
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控
- en: Before we conclude there’s a few different ways you can monitor your load tests.
    One is via Locust, as seen in the above screenshot you can track your TPS and
    latency in live. In the end a general results file is generated containing your
    end to end latency percentile metrics and TPS. To adjust the duration of the test
    please check the RUN_TIME flag in your distributed.sh script.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在总结之前，有几种不同的方法可以监控你的负载测试。一种是通过 Locust，如上截图所示，你可以实时跟踪你的 TPS 和延迟。最后，生成一个包含端到端延迟百分位数指标和
    TPS 的一般结果文件。要调整测试的持续时间，请检查你在 distributed.sh 脚本中的 RUN_TIME 标志。
- en: Lastly, to validate your load test results you can cross check with SageMaker
    CloudWatch Metrics which can be found in the Console.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了验证你的负载测试结果，你可以通过 SageMaker CloudWatch 指标进行交叉检查，这些指标可以在控制台中找到。
- en: '![](../Images/1eabf031087f78631b911163a09fcdb1.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1eabf031087f78631b911163a09fcdb1.png)'
- en: Monitor Endpoint (Screenshot by Author)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 监控端点（截图由作者提供）
- en: With Invocation Metrics we can get an idea of invocations as well as latency
    numbers. With Instance Metrics we can see how well our hardware is saturated and
    if we need to scale up or down. To fully understand how to interpret these metrics
    please reference this [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用指标，我们可以了解调用情况及延迟数据。通过实例指标，我们可以查看硬件的饱和程度以及是否需要扩展或缩减。要全面理解如何解读这些指标，请参考此[文档](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html)。
- en: '![](../Images/792de5d6cf7598f12d0baf56a8b95812.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/792de5d6cf7598f12d0baf56a8b95812.png)'
- en: Hardware Metrics (Screenshot by Author)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件指标（截图由作者提供）
- en: '![](../Images/1dfaa859b7171b5e379c83770a72dff8.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1dfaa859b7171b5e379c83770a72dff8.png)'
- en: Invocation Metrics (Screenshot by Author)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 调用指标（截图由作者提供）
- en: Here we can see we’ve scaled to nearly 77,000 invokes per minute which comes
    to a little above 1000 TPS as our Locust metrics showed. It’s best practice to
    track these metrics at both the instance and invocation level so that you can
    properly define [AutoScaling](/autoscaling-sagemaker-real-time-endpoints-b1b6e6731c59)
    for your hardware as necessary.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们可以看到我们已扩展到每分钟近 77,000 次调用，这比我们 Locust 指标显示的 1000 TPS 略高。最佳实践是跟踪实例和调用级别的这些指标，以便根据需要为你的硬件定义[自动扩展](/autoscaling-sagemaker-real-time-endpoints-b1b6e6731c59)。
- en: Additional Resources & Conclusion
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外资源与结论
- en: '[](https://github.com/RamVegiraju/weighted-mme-load-test?source=post_page-----f0db7b305770--------------------------------)
    [## GitHub - RamVegiraju/weighted-mme-load-test: Weighted load traffic distribution
    across models…'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[## GitHub - RamVegiraju/weighted-mme-load-test: Weighted load traffic distribution
    across models…](https://github.com/RamVegiraju/weighted-mme-load-test?source=post_page-----f0db7b305770--------------------------------)'
- en: You can't perform that action at this time. You signed in with another tab or
    window. You signed out in another tab or…
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你现在无法执行该操作。你在另一个标签或窗口中登录了。你在另一个标签或窗口中注销了……
- en: github.com](https://github.com/RamVegiraju/weighted-mme-load-test?source=post_page-----f0db7b305770--------------------------------)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/RamVegiraju/weighted-mme-load-test?source=post_page-----f0db7b305770--------------------------------)'
- en: The entire code for the example can be found in the link above. Once again if
    you are new to Locust and SageMaker Real-Time Inference, I strongly recommend
    you check out the starting blogs linked for both features. The load test scripts
    attached in this repository can easily be adjusted not just for SageMaker endpoints,
    but any APIs that you are hosting and need to be tested. As always any feedback
    is appreciate and feel free to reach out with any questions or comments, thank
    you for reading!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 示例的完整代码可以在上述链接中找到。如果你对 Locust 和 SageMaker 实时推理不熟悉，我强烈建议你查看与这两个功能相关的起始博客。本存储库中附带的负载测试脚本不仅可以轻松调整以适应
    SageMaker 端点，还可以用于测试你托管的任何 API。任何反馈都受到欢迎，随时联系我提问或评论，谢谢阅读！
- en: '*If you enjoyed this article feel free to connect with me on* [*LinkedIn*](https://www.linkedin.com/in/ram-vegiraju-81272b162/)
    *and subscribe to my Medium* [*Newsletter*](https://ram-vegiraju.medium.com/subscribe)*.
    If you’re new to Medium, sign up using my* [*Membership Referral*](https://ram-vegiraju.medium.com/membership)*.*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这篇文章，请随时在* [*LinkedIn*](https://www.linkedin.com/in/ram-vegiraju-81272b162/)
    *上与我联系，并订阅我的 Medium* [*Newsletter*](https://ram-vegiraju.medium.com/subscribe)*。如果你是
    Medium 新用户，请使用我的* [*会员推荐链接*](https://ram-vegiraju.medium.com/membership)*注册。*'
