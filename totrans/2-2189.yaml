- en: 'Understanding Large Language Models: The Physics of (Chat)GPT and BERT'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/understanding-large-language-models-the-physics-of-chat-gpt-and-bert-ea512bcc6a64](https://towardsdatascience.com/understanding-large-language-models-the-physics-of-chat-gpt-and-bert-ea512bcc6a64)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Insights from a physicist, on how particles and forces can help us understand
    LLMs.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://tim-lou.medium.com/?source=post_page-----ea512bcc6a64--------------------------------)[![Tim
    Lou, PhD](../Images/e4931bb6d59e27730529ceaf00a23822.png)](https://tim-lou.medium.com/?source=post_page-----ea512bcc6a64--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ea512bcc6a64--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ea512bcc6a64--------------------------------)
    [Tim Lou, PhD](https://tim-lou.medium.com/?source=post_page-----ea512bcc6a64--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ea512bcc6a64--------------------------------)
    ¬∑12 min read¬∑Jul 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/471184f0f0e17bba6faaa3b9fca21a6c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'ChatGPT and ice crystals may have more in common than one might think (credit:
    [15414483@pixabay](https://pixabay.com/photos/ice-frost-winter-snow-snowflakes-6538605/))'
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT, or more broadly Large Language AI Models (LLMs), have become ubiquitous
    in our lives. Yet, most of the mathematics and internal structures of LLMs are
    obscure knowledge to the general public.
  prefs: []
  type: TYPE_NORMAL
- en: So, how can we move beyond perceiving LLMs like ChatGPT as magical black boxes?
    Physics may provide an answer.
  prefs: []
  type: TYPE_NORMAL
- en: Everyone is somewhat familiar with our physical world. Objects such as cars,
    tables, and planets are composed of trillions of atoms, governed by a simple set
    of physical laws. Similarly, complex organisms, like ChatGPT, have emerged and
    are capable of generating highly sophisticated concepts like arts and sciences.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that the equations of the building blocks of LLMs, are analogous
    to our physical laws. So that by understanding how complexity arises from our
    simple physical laws, we might be able to gleam some insight on how and why LLMs
    work.
  prefs: []
  type: TYPE_NORMAL
- en: Complexity from Simplicity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/22f0277b4b16b574f6da4ceaa92c46ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Complex structures, like bubble films and the convection cells within, are generated
    by simple physical laws (Photo by [chuttersnap](https://unsplash.com/@chuttersnap?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral))
  prefs: []
  type: TYPE_NORMAL
- en: Our world is inherently complex, yet it can be described by a remarkably small
    number of fundamental interactions. For instance, complicated snowflakes and bubble
    films can be linked to simple attractive forces between molecules.
  prefs: []
  type: TYPE_NORMAL
- en: So, what is the commonality in how complex structures arise? In physics, complexity
    is generated when we zoom out from the smallest to the largest scale.
  prefs: []
  type: TYPE_NORMAL
- en: Drawing an analogy to language, English starts with a modest number of fundamental
    constituents ‚Äî 26 symbols. These symbols can combine to form around 100,000 usable
    words, each carrying a distinctive meaning. From these words, a countless number
    of sentences, passages, books, and volumes can be generated.
  prefs: []
  type: TYPE_NORMAL
- en: This linguistic hierarchy is similar to the ones found in physics. Our current
    fundamental law (the [Standard Model](https://en.wikipedia.org/wiki/Standard_Model))
    starts with a limited number of elementary particles such as quarks and electrons,
    along with a few interactions mediated by force carriers like photons. These particles
    and forces combine to form atoms, each carrying a distinctive chemical property.
    From these atoms, an immense number of molecules, structures, cells, and beings
    are created.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our physical world, there is a sort of emergent universality: even though
    many complex systems have totally different origins, they often share some universal
    characteristics. For instance, many liquids, despite having distinctive chemical
    properties, share three common phases (liquid, solid, and gas). As a more extreme
    example, the physics behind certain materials ([Type-I superconductors](https://en.wikipedia.org/wiki/Type-I_superconductor))
    can be borrowed to describe fundamental physics (the celebrated [Higgs Mechanism](https://en.wikipedia.org/wiki/Higgs_mechanism)).'
  prefs: []
  type: TYPE_NORMAL
- en: Although it is important to keep in mind the difference between language and
    physics ‚Äî that physical laws are dictated and constrained by nature, while languages
    are human creations that seem unconstrained ‚Äî there is no requirement for linguistic
    complexity to resemble physical complexity in our world.
  prefs: []
  type: TYPE_NORMAL
- en: However, as we will argue below, ChatGPT, and other LLMs alike, contain particle-physics-like
    structures. If we believe that these structures are the key to the successes of
    LLMs, it may provide hints that linguistic complexity shares some commonalities
    with physical complexities. Additionally, this could provide valuable insight
    into why and how LLMs work.
  prefs: []
  type: TYPE_NORMAL
- en: The Physics of Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/32cbf598a9a5f6c0d34534c91e3ad0da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Physical laws are governed by equations, but what about LLMs like ChatGPT?
    (credit: author‚Äôs own work)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to connect LLMs to physics, we need to relate their underlying mathematical
    structures. In physics, the movement of particles (or more generally, fields or
    states), can be written schematically as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56b8c41531d0692d1f9388058e2da595.png)'
  prefs: []
  type: TYPE_IMG
- en: A schematic form of physical equations
  prefs: []
  type: TYPE_NORMAL
- en: '(** technical note: the [Hamiltonian](https://en.wikipedia.org/wiki/Hamiltonian_mechanics)
    formalism makes it more precise, although the derivative part needs to be modified
    a bit)'
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, it states that particles move because of forces, which come from
    the slopes of some abstract object called *potential*. This is analogous to water
    flowing down a stream, where the potential would then come from gravity and fluid
    dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out, the structure of LLMs is very similar: they break down sentences
    into fundamental constituents called *tokens*, and the AI model modifies these
    tokens layer-by-layer in an analogous way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/abcfdcd9b5410a45702255ab7e0ee267.png)'
  prefs: []
  type: TYPE_IMG
- en: A schematic equation describing the essence in LLMs
  prefs: []
  type: TYPE_NORMAL
- en: This will be made more precise in the technical section below. From this, we
    can draw an analogy
  prefs: []
  type: TYPE_NORMAL
- en: Transformer based language models are treating words as particles, which move
    around under the influence of each other, generating intriguing patterns.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this way, just like how water molecules can build beautiful snowflakes, or
    how liquid-soap mixtures can create intricate bubble patterns, interesting results
    from ChatGPT might be attributed to its physics-like behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: In the following optional section, we‚Äôll describe in more details how this analogy
    come to be more rigorously, and then dive into the specifics on how this insight
    could help us understand LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Detour
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Below we provide a more detailed explanation on how one can think of LLMs as
    physics models.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the physics side, on a microscopic level, each particle is typically influenced
    by all the particles in the system. For instance, let‚Äôs consider a hypothetical
    world with only 3 particles; in this case, there would be a total of 3 √ó 3 = 9
    possible potentials between one particle and another. Schematically, we can represent
    this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b97def960af4cac94cd1b822874070f.png)'
  prefs: []
  type: TYPE_IMG
- en: A sketch for the equation of motion for 3 particles
  prefs: []
  type: TYPE_NORMAL
- en: (**In physics, the potentials are usually symmetric i.e. Potential‚ÇÅ‚ÇÇ = Potential‚ÇÇ‚ÇÅ,
    we are relaxing this constraint here)
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how this relates to LLMs, let‚Äôs recall some basic facts:'
  prefs: []
  type: TYPE_NORMAL
- en: To feed data into LLMs, a document or text is broken down into tokens. Tokens
    typically consist of one word or part of a word. Like particles, tokens are thought
    of as the smallest indivisible constituents in an LLM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LLMs have multiple layers, and in each layer, all the tokens are modified by
    self-attention modules.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final output layer aggregates the tokens to form predictions, which can
    be used for classifications or for generating texts/images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we take a three token example (say from the sentence ‚ÄúI like physics‚Äù), what
    would the equations look like?
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some small differences depending on the specific types of LLMs we‚Äôre
    working with: [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) or [GPT](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer).'
  prefs: []
  type: TYPE_NORMAL
- en: BERT Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For BERT-like models (typically used for classification), each layer would
    modify the tokens schematically as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/85c31a78fc2af9e22b893756aabfe8c4.png)'
  prefs: []
  type: TYPE_IMG
- en: (** the reason layer‚ÇÅ is involved is due to the residual layer)
  prefs: []
  type: TYPE_NORMAL
- en: If we think of the layer as analogous to the time dimension, then the structure
    of the equation is similar to the equations governing the movements of three particles,
    although in LLMs the layers are discrete, as opposed to continuous in physics.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the analogy complete, we still need to convert the attention portion
    into a sort of potential. Let‚Äôs dig deeper mathematically. Pick a specific token,
    *t·µ¢*, at each layer, it gets modified accordingly to the self-attention mechanism
    (ignoring multiple attention-head):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1147a27a12307df49c1e04d4f1ca5b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *Q*, *K*, *V* are the Query, Key, Value matrices typically seen in an
    attention module. For now we are ignoring normalization layers. The crux is that
    the exponential form can be re-written as the derivative of a kind of potential
    term!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/335668f50a4aa9d7610bb24c203c5e18.png)'
  prefs: []
  type: TYPE_IMG
- en: '(** while *Q*·µÄ*K* may not always be invertible and this equation may not be
    exact, *V* is an arbitrary weight in our model: So we can always trade *M* for
    *V* in our attention module to achieve the same model performance)'
  prefs: []
  type: TYPE_NORMAL
- en: In this way, passing tokens through layers in LLMs is analogous to having groups
    of particles interacting under some pair-wise interactions! It‚Äôs sort of like
    gas molecules bumping into one another and forming weather patterns.
  prefs: []
  type: TYPE_NORMAL
- en: (** in this view, we can interpret the normalization and matrix multiplication
    *M* as a sort of projection, so that the token-particles are properly constrained
    in the system. It‚Äôs analogous to a roller coaster being confined to its tracks.)
  prefs: []
  type: TYPE_NORMAL
- en: GPT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For (Chat)[GPT](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer)-like
    models, the discussion gets modified. The attention module has an extra casual
    structure ‚Äî that tokens can only get modified by the ones before them. This means
    that the equations are missing a few terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c4c08ba6336b93785a5854e24b98fd0f.png)'
  prefs: []
  type: TYPE_IMG
- en: Following our analogy, this means that particles are coming in one at a time,
    and each one would get stuck after going through all the interaction layers. It‚Äôs
    sort of like growing a crystal one-atom at a time.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to keep in mind is that our physics analogy isn‚Äôt 100% exact, as fundamental
    features like symmetries and laws like energy/momentum conservations that are
    so ubiquitous in physics do not apply to LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Emergence in Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/6f6106610e4d44312c28c9e4356f78c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Like a beautiful snowflake, the output of an LLM might hinge on its physics-like
    properties (Photo by [Aaron Burden](https://unsplash.com/@aaronburden?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral))
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our physics analogies, how does it help us understand LLMs?
    The hope is that, like complex physical systems, we can draw analogies from other,
    more familiar and well-understood systems to gain insights into LLMs. However,
    I have to caution the reader that most of our discussions below will be speculative,
    as confirming them would require conducting detailed experimental studies on LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: (* Indeed if I have more resources, I‚Äôd imagine some of these ideas might be
    fruitful academic research projects)
  prefs: []
  type: TYPE_NORMAL
- en: Below is a sampler of how we may use the language of physics to reframe our
    understanding of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the language of thermal physics, we can think of LLMs as a tunable physical
    system, and model training is analogous to applying thermal pressure to the system
    to adjust its parameters. This viewpoint was described in my other article, ‚Äú[The
    Thermodynamics of Machine Learning](https://medium.com/towards-data-science/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1),‚Äù
    so I won‚Äôt delve too much into the details here.
  prefs: []
  type: TYPE_NORMAL
- en: Emergence of Intelligence?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While there is plenty of discussion on whether systems like ChatGPT are intelligent
    or not, I will refrain from adding to this controversial topic as I am not even
    sure how one may define intelligence. Nevertheless, it is clear that ChatGPT can
    consistently produce sophisticated and interesting outputs.
  prefs: []
  type: TYPE_NORMAL
- en: If we subscribe to our physics analogy, this should not be surprising. From
    snowflakes to tornadoes, we know that even simple laws can give rise to highly
    complex behaviors, and from complex behaviors, structures that appear intelligent
    could arise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Complexity as a concept is not easily defined, so in order to proceed further,
    we can try to examine some key features of complex systems: Phase transition is
    one of them.'
  prefs: []
  type: TYPE_NORMAL
- en: Phase Transition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many complex physical systems possess distinctive phases, each with a highlighted
    set of physical properties. Thus, it is reasonable to suggest that within LLMs,
    there could be distinctive phases as well, with each phase tuned to be helpful
    in specific tasks (such as coding vs proofreading).
  prefs: []
  type: TYPE_NORMAL
- en: 'How might we verify or refute such a claim? This is where things could get
    interesting. In physics, phases arise when interactions start to form interesting
    structures. Some examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: When water cools down, the attractive forces between molecules get stronger,
    causing the molecules to stick together and form solids.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When a metal is cooled to extremely low temperatures, electrons may become attracted
    to each other through sound-wave (phonon) interactions, forming Type-I superconductors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perhaps something analogous could occur in LLMs? For instance, in ChatGPT, one
    might speculate that certain groups of tokens from ‚Äúcode‚Äù or ‚Äúproofread‚Äù could
    trigger an avalanche of particular forces that drive specific types of output.
  prefs: []
  type: TYPE_NORMAL
- en: Another technical aspect of phase transitions is the modification of symmetries.
    This is related to the creation of structures, such as ice crystal patterns from
    water vapor. While LLMs do not possess physical symmetries, they should contain
    some sort of permutation symmetries of the model weights. This is because model
    performance should be the same as long as they are initialized with the same statistics
    and trained in the same paradigm. The specific values of a particular weight only
    become important during training. This can be thought of as the freezing-in of
    the weights. However, to continue this discussion, we would need to delve into
    the technical subject of [spontaneous symmetry breaking](https://en.wikipedia.org/wiki/Spontaneous_symmetry_breaking),
    which we‚Äôll save for a later date.
  prefs: []
  type: TYPE_NORMAL
- en: Are LLMs Efficient?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While there are many criticisms regarding the perceived inefficiency of LLMs
    due to their large number of parameters (especially when compared to physical
    models), these criticisms may not be fully warranted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Why? It comes down to the technical limitations of our computers, which result
    in significant differences between physics and LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: Physical laws have infinite precision, while LLMs have finite precision.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Physics exhibits huge hierarchies, with some forces being tiny and others large.
    In LLMs, we attempt to make all outputs/weights similar in size through normalization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In physics, tiny effects can accumulate to enormous influences (such as Earth‚Äôs
    gravity). In LLMs, these tiny effects are often rounded away and eliminated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Nature is an incredibly efficient computer, with interactions computed instantaneously
    across all scales with infinite precisions. LLMs, on the other hand, are relatively
    slow computers limited by finite precisions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This means that while we can strive to make LLMs mimic physics better and create
    more powerful models, in practice, computers are simply incapable of fully simulating
    our world (as discussed in ‚Äú[Why We Don‚Äôt Live In a Simulation](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d)‚Äù).
    Therefore, resorting to a large number of parameters may be a last-resort tactic
    to address some of these deficiencies.
  prefs: []
  type: TYPE_NORMAL
- en: It is even plausible that given finite precision, there may be an upper limit
    to the complexity achievable with standard computers. This could make it very
    challenging to significantly reduce the number of parameters (although advancements
    in [quantum computing](https://en.wikipedia.org/wiki/Quantum_computing) might
    change this in the future).
  prefs: []
  type: TYPE_NORMAL
- en: Improvements to LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Could our physics analogy help provide hints for the next generation of LLMs?
    I believe it‚Äôs possible. Logically, there are two possible directions to pursue
    based on our beliefs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Physics-like features are desirable**: We should draw more inspiration from
    physics to create better model structures.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Physics-like features are undesirable**: Physics-like features may actually
    limit the capabilities of LLMs due to inherent computational limits, so we should
    avoid them.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since we are using physics to understand LLMs, I‚Äôll focus on the first possibility.
    Under this assumption, how could we address the shortcomings of LLMs like ChatGPT?
  prefs: []
  type: TYPE_NORMAL
- en: '**Preserving Hierarchies**: Instead of solely focusing on normalizing weights
    and reducing precision, we should explore alternative approaches to account for
    diverse interactions with different strengths and scales. We could draw inspiration
    from how electromagnetism (which is very strong) and gravity (which is very weak)
    are combined in nature.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Accommodating Different Phases**: Describing both ice and water using the
    same basic molecular equations is inefficient. It‚Äôs more efficient to use difference
    descriptions for difference phases (say sounds waves vs water waves). We could
    create a better structure that naturally accommodates the macroscopic differences
    within the model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Advanced Physics Techniques**: In physics, we don‚Äôt study emergent phenomena
    using only fundamental equations. Techniques like [thermodynamics](https://en.wikipedia.org/wiki/Thermodynamics),
    [mean-field theory](https://en.wikipedia.org/wiki/Mean-field_theory) and [renormalization](https://en.wikipedia.org/wiki/Renormalization)
    can be used to help us simplify the problem. Incorporating some of these ideas
    into the building blocks of LLMs could improve their efficiency. For example,
    recent advances on [linear attention](https://arxiv.org/abs/2006.16236) (A. Katharopoulos
    et. al.) may already be interpreted as a sort of mean-field approach.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By exploring these avenues, we may be able to enhance the capabilities and efficiencies
    of LLMs, leveraging insights from physics to advance the field further.
  prefs: []
  type: TYPE_NORMAL
- en: Epilogue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In summary, we have showcased how the mathematics of LLMs resemble those in
    physics. This allows us to use our intuitions about everyday physical systems
    to understand these new emergent phenomena, such as ChatGPT. I hope that this
    helps demystify the reasons behind the characteristics of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: More generally, I hope I have conveyed to you how physics can provide valuable
    insights into complex subjects like LLMs. I firmly believe that science is most
    effective when we borrow insights from seemingly disparate fields
  prefs: []
  type: TYPE_NORMAL
- en: If you enjoyed this article, you might be interested in my other pieces on similar
    topics, such as the link between physics and AI.
  prefs: []
  type: TYPE_NORMAL
- en: Please leave a comment or provide feedback, as it encourages me to write more
    insightful pieces! üëã
  prefs: []
  type: TYPE_NORMAL
- en: '[](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----ea512bcc6a64--------------------------------)
    [## A Physicist‚Äôs View of Machine Learning: The Thermodynamics of Machine Learning'
  prefs: []
  type: TYPE_NORMAL
- en: Complex systems in nature can be successfully studied using thermodynamics.
    What about Machine Learning?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----ea512bcc6a64--------------------------------)
    [](/the-meaning-behind-logistic-classification-from-physics-291774fda579?source=post_page-----ea512bcc6a64--------------------------------)
    [## The Meaning Behind Logistic Classification, from Physics
  prefs: []
  type: TYPE_NORMAL
- en: Why do we use the logistic and softmax functions? Thermal physics may have an
    answer.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/the-meaning-behind-logistic-classification-from-physics-291774fda579?source=post_page-----ea512bcc6a64--------------------------------)
    [](/why-causation-is-correlation-a-physicists-perspective-part-1-742696d130e8?source=post_page-----ea512bcc6a64--------------------------------)
    [## Why Causation Is Correlation: A Physicist‚Äôs Perspective (Part 1)'
  prefs: []
  type: TYPE_NORMAL
- en: We‚Äôve all heard the phrase, ‚Äúcorrelation does not imply causation‚Äù, but no one
    ever talks about what causation really‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/why-causation-is-correlation-a-physicists-perspective-part-1-742696d130e8?source=post_page-----ea512bcc6a64--------------------------------)
    [](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----ea512bcc6a64--------------------------------)
    [## Why We Don‚Äôt Live in a Simulation
  prefs: []
  type: TYPE_NORMAL
- en: Describing reality as a simulation vastly understates the complexities of our
    world. Here‚Äôs why the simulation‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----ea512bcc6a64--------------------------------)
  prefs: []
  type: TYPE_NORMAL
