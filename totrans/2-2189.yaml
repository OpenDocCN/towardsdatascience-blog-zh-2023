- en: 'Understanding Large Language Models: The Physics of (Chat)GPT and BERT'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解大型语言模型：**(Chat)GPT 和 BERT 的物理学**
- en: 原文：[https://towardsdatascience.com/understanding-large-language-models-the-physics-of-chat-gpt-and-bert-ea512bcc6a64](https://towardsdatascience.com/understanding-large-language-models-the-physics-of-chat-gpt-and-bert-ea512bcc6a64)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-large-language-models-the-physics-of-chat-gpt-and-bert-ea512bcc6a64](https://towardsdatascience.com/understanding-large-language-models-the-physics-of-chat-gpt-and-bert-ea512bcc6a64)
- en: Insights from a physicist, on how particles and forces can help us understand
    LLMs.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从物理学家的角度探讨粒子和力量如何帮助我们理解大型语言模型（LLMs）。
- en: '[](https://tim-lou.medium.com/?source=post_page-----ea512bcc6a64--------------------------------)[![Tim
    Lou, PhD](../Images/e4931bb6d59e27730529ceaf00a23822.png)](https://tim-lou.medium.com/?source=post_page-----ea512bcc6a64--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ea512bcc6a64--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ea512bcc6a64--------------------------------)
    [Tim Lou, PhD](https://tim-lou.medium.com/?source=post_page-----ea512bcc6a64--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://tim-lou.medium.com/?source=post_page-----ea512bcc6a64--------------------------------)[![Tim
    Lou, PhD](../Images/e4931bb6d59e27730529ceaf00a23822.png)](https://tim-lou.medium.com/?source=post_page-----ea512bcc6a64--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ea512bcc6a64--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ea512bcc6a64--------------------------------)
    [Tim Lou, PhD](https://tim-lou.medium.com/?source=post_page-----ea512bcc6a64--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ea512bcc6a64--------------------------------)
    ·12 min read·Jul 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ea512bcc6a64--------------------------------)
    ·12 分钟阅读·2023年7月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/471184f0f0e17bba6faaa3b9fca21a6c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/471184f0f0e17bba6faaa3b9fca21a6c.png)'
- en: 'ChatGPT and ice crystals may have more in common than one might think (credit:
    [15414483@pixabay](https://pixabay.com/photos/ice-frost-winter-snow-snowflakes-6538605/))'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 和冰晶之间可能有更多的相似之处（来源：[15414483@pixabay](https://pixabay.com/photos/ice-frost-winter-snow-snowflakes-6538605/))
- en: ChatGPT, or more broadly Large Language AI Models (LLMs), have become ubiquitous
    in our lives. Yet, most of the mathematics and internal structures of LLMs are
    obscure knowledge to the general public.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT，或者更广泛的说，大型语言 AI 模型（LLMs），已经在我们的生活中无处不在。然而，LLMs 的大部分数学和内部结构对于普通大众来说是模糊的知识。
- en: So, how can we move beyond perceiving LLMs like ChatGPT as magical black boxes?
    Physics may provide an answer.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何才能超越将 LLMs，如 ChatGPT，视为神秘黑箱的观念呢？物理学或许能提供答案。
- en: Everyone is somewhat familiar with our physical world. Objects such as cars,
    tables, and planets are composed of trillions of atoms, governed by a simple set
    of physical laws. Similarly, complex organisms, like ChatGPT, have emerged and
    are capable of generating highly sophisticated concepts like arts and sciences.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 每个人对我们的物理世界都有一定的了解。像汽车、桌子和行星这样的物体由数万亿个原子组成，受一套简单物理法则的支配。同样，像 ChatGPT 这样的复杂生物体也已经出现，并能够生成像艺术和科学这样高度复杂的概念。
- en: It turns out that the equations of the building blocks of LLMs, are analogous
    to our physical laws. So that by understanding how complexity arises from our
    simple physical laws, we might be able to gleam some insight on how and why LLMs
    work.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，LLMs 的构建模块的方程类似于我们的物理法则。因此，通过理解复杂性如何从简单的物理法则中产生，我们可能能够获得一些关于 LLMs 工作原理和原因的见解。
- en: Complexity from Simplicity
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从简单中看复杂
- en: '![](../Images/22f0277b4b16b574f6da4ceaa92c46ca.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22f0277b4b16b574f6da4ceaa92c46ca.png)'
- en: Complex structures, like bubble films and the convection cells within, are generated
    by simple physical laws (Photo by [chuttersnap](https://unsplash.com/@chuttersnap?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral))
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂的结构，如气泡膜和其中的对流单元，是由简单的物理法则生成的（照片来源：[chuttersnap](https://unsplash.com/@chuttersnap?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)）
- en: Our world is inherently complex, yet it can be described by a remarkably small
    number of fundamental interactions. For instance, complicated snowflakes and bubble
    films can be linked to simple attractive forces between molecules.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的世界本质上是复杂的，但它可以通过极少量的基本相互作用来描述。例如，复杂的雪花和气泡膜可以与分子之间简单的吸引力联系起来。
- en: So, what is the commonality in how complex structures arise? In physics, complexity
    is generated when we zoom out from the smallest to the largest scale.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，复杂结构产生的共同点是什么？在物理学中，复杂性是当我们从最小尺度放大到最大尺度时产生的。
- en: Drawing an analogy to language, English starts with a modest number of fundamental
    constituents — 26 symbols. These symbols can combine to form around 100,000 usable
    words, each carrying a distinctive meaning. From these words, a countless number
    of sentences, passages, books, and volumes can be generated.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以语言为类比，英语从有限数量的基本成分开始——26个符号。这些符号可以组合成大约100,000个可用的单词，每个单词都携带独特的意义。从这些单词中，可以生成无数的句子、段落、书籍和卷册。
- en: This linguistic hierarchy is similar to the ones found in physics. Our current
    fundamental law (the [Standard Model](https://en.wikipedia.org/wiki/Standard_Model))
    starts with a limited number of elementary particles such as quarks and electrons,
    along with a few interactions mediated by force carriers like photons. These particles
    and forces combine to form atoms, each carrying a distinctive chemical property.
    From these atoms, an immense number of molecules, structures, cells, and beings
    are created.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种语言学层次结构类似于物理学中的层次结构。我们目前的基本法则（[标准模型](https://en.wikipedia.org/wiki/Standard_Model)）以有限数量的基本粒子开始，如夸克和电子，以及由光子等力子介导的一些相互作用。这些粒子和力结合形成原子，每种原子具有独特的化学性质。从这些原子中，产生了大量的分子、结构、细胞和生物。
- en: 'In our physical world, there is a sort of emergent universality: even though
    many complex systems have totally different origins, they often share some universal
    characteristics. For instance, many liquids, despite having distinctive chemical
    properties, share three common phases (liquid, solid, and gas). As a more extreme
    example, the physics behind certain materials ([Type-I superconductors](https://en.wikipedia.org/wiki/Type-I_superconductor))
    can be borrowed to describe fundamental physics (the celebrated [Higgs Mechanism](https://en.wikipedia.org/wiki/Higgs_mechanism)).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的物理世界中，有一种涌现的普遍性：尽管许多复杂系统有着完全不同的起源，但它们常常共享一些普遍特征。例如，许多液体尽管具有不同的化学性质，却共享三种共同的相态（液态、固态和气态）。作为一个更极端的例子，某些材料的物理学（[I型超导体](https://en.wikipedia.org/wiki/Type-I_superconductor)）可以借用来描述基本物理学（著名的[希格斯机制](https://en.wikipedia.org/wiki/Higgs_mechanism)）。
- en: Although it is important to keep in mind the difference between language and
    physics — that physical laws are dictated and constrained by nature, while languages
    are human creations that seem unconstrained — there is no requirement for linguistic
    complexity to resemble physical complexity in our world.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管需要牢记语言和物理学之间的区别——物理定律由自然规定并受到限制，而语言是看似不受约束的人类创造——但语言复杂性不必与我们世界中的物理复杂性相似。
- en: However, as we will argue below, ChatGPT, and other LLMs alike, contain particle-physics-like
    structures. If we believe that these structures are the key to the successes of
    LLMs, it may provide hints that linguistic complexity shares some commonalities
    with physical complexities. Additionally, this could provide valuable insight
    into why and how LLMs work.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如我们将要论证的那样，ChatGPT和其他类似的LLMs包含类似粒子物理学的结构。如果我们认为这些结构是LLMs成功的关键，它可能会提供语言复杂性与物理复杂性共享一些共同点的线索。此外，这也可能为我们提供关于LLMs如何工作的有价值见解。
- en: The Physics of Language Models
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言模型的物理学
- en: '![](../Images/32cbf598a9a5f6c0d34534c91e3ad0da.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32cbf598a9a5f6c0d34534c91e3ad0da.png)'
- en: 'Physical laws are governed by equations, but what about LLMs like ChatGPT?
    (credit: author’s own work)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 物理定律由方程 govern，但像ChatGPT这样的LLMs呢？（来源：作者自身的工作）
- en: 'In order to connect LLMs to physics, we need to relate their underlying mathematical
    structures. In physics, the movement of particles (or more generally, fields or
    states), can be written schematically as:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将LLMs与物理学联系起来，我们需要将它们的基础数学结构进行关联。在物理学中，粒子的运动（或更一般地说，场或状态）可以示意性地表示为：
- en: '![](../Images/56b8c41531d0692d1f9388058e2da595.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56b8c41531d0692d1f9388058e2da595.png)'
- en: A schematic form of physical equations
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 物理方程的示意图
- en: '(** technical note: the [Hamiltonian](https://en.wikipedia.org/wiki/Hamiltonian_mechanics)
    formalism makes it more precise, although the derivative part needs to be modified
    a bit)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (** 技术说明：[哈密顿量](https://en.wikipedia.org/wiki/Hamiltonian_mechanics)形式使其更精确，尽管导数部分需要稍作修改)
- en: Intuitively, it states that particles move because of forces, which come from
    the slopes of some abstract object called *potential*. This is analogous to water
    flowing down a stream, where the potential would then come from gravity and fluid
    dynamics.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，它表明粒子因力的作用而移动，这些力来源于一些抽象对象叫做*势能*的斜率。这类似于水流下山，势能来自重力和流体动力学。
- en: 'It turns out, the structure of LLMs is very similar: they break down sentences
    into fundamental constituents called *tokens*, and the AI model modifies these
    tokens layer-by-layer in an analogous way:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，LLMs的结构非常相似：它们将句子拆解为基本组成部分，即*tokens*，并以类似的方式逐层修改这些tokens：
- en: '![](../Images/abcfdcd9b5410a45702255ab7e0ee267.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/abcfdcd9b5410a45702255ab7e0ee267.png)'
- en: A schematic equation describing the essence in LLMs
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 描述LLMs本质的示意方程
- en: This will be made more precise in the technical section below. From this, we
    can draw an analogy
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在下面的技术部分中更为准确地说明。由此，我们可以进行类比。
- en: Transformer based language models are treating words as particles, which move
    around under the influence of each other, generating intriguing patterns.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 基于Transformer的语言模型将词语视作粒子，这些粒子在相互影响下移动，生成引人入胜的模式。
- en: In this way, just like how water molecules can build beautiful snowflakes, or
    how liquid-soap mixtures can create intricate bubble patterns, interesting results
    from ChatGPT might be attributed to its physics-like behaviors.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，就像水分子可以构建美丽的雪花，或液体肥皂混合物可以创造复杂的气泡图案一样，ChatGPT的有趣结果可能归因于其类似物理的行为。
- en: In the following optional section, we’ll describe in more details how this analogy
    come to be more rigorously, and then dive into the specifics on how this insight
    could help us understand LLMs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的可选部分中，我们将更详细地描述这一类比如何变得更加严谨，然后深入探讨这一洞见如何帮助我们理解LLMs。
- en: Technical Detour
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术绕道
- en: Below we provide a more detailed explanation on how one can think of LLMs as
    physics models.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们将更详细地解释如何将LLMs视作物理模型。
- en: 'From the physics side, on a microscopic level, each particle is typically influenced
    by all the particles in the system. For instance, let’s consider a hypothetical
    world with only 3 particles; in this case, there would be a total of 3 × 3 = 9
    possible potentials between one particle and another. Schematically, we can represent
    this as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从物理学的角度来看，在微观层面上，每个粒子通常会受到系统中所有粒子的影响。例如，假设一个只有3个粒子的假想世界；在这种情况下，一个粒子和另一个粒子之间会有总共3
    × 3 = 9种可能的势能。从示意图来看，我们可以这样表示：
- en: '![](../Images/4b97def960af4cac94cd1b822874070f.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b97def960af4cac94cd1b822874070f.png)'
- en: A sketch for the equation of motion for 3 particles
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 描述三个粒子运动方程的示意图
- en: (**In physics, the potentials are usually symmetric i.e. Potential₁₂ = Potential₂₁,
    we are relaxing this constraint here)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: (**在物理学中，势能通常是对称的，即Potential₁₂ = Potential₂₁，我们在这里放宽了这一约束**)
- en: 'To see how this relates to LLMs, let’s recall some basic facts:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解这与LLMs的关系，让我们回顾一些基本事实：
- en: To feed data into LLMs, a document or text is broken down into tokens. Tokens
    typically consist of one word or part of a word. Like particles, tokens are thought
    of as the smallest indivisible constituents in an LLM.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将数据输入LLMs，文档或文本会被拆分成tokens。tokens通常由一个词或词的一部分组成。像粒子一样，tokens被视为LLM中最小的不可分割的组成部分。
- en: LLMs have multiple layers, and in each layer, all the tokens are modified by
    self-attention modules.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLMs具有多个层级，在每一层中，所有的token都被自注意力模块所修改。
- en: The final output layer aggregates the tokens to form predictions, which can
    be used for classifications or for generating texts/images.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终输出层汇聚tokens形成预测，这些预测可用于分类或生成文本/图像。
- en: If we take a three token example (say from the sentence “I like physics”), what
    would the equations look like?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们以三个token的例子（比如来自句子“I like physics”）进行分析，这些方程会是什么样的？
- en: 'There are some small differences depending on the specific types of LLMs we’re
    working with: [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) or [GPT](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们所处理的LLMs的具体类型，有一些小的差异：[BERT](https://en.wikipedia.org/wiki/BERT_(language_model))或[GPT](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer)。
- en: BERT Models
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT模型
- en: 'For BERT-like models (typically used for classification), each layer would
    modify the tokens schematically as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于类似BERT的模型（通常用于分类），每一层会按如下方式示意性地修改tokens：
- en: '![](../Images/85c31a78fc2af9e22b893756aabfe8c4.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85c31a78fc2af9e22b893756aabfe8c4.png)'
- en: (** the reason layer₁ is involved is due to the residual layer)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: (** 层₁ 的参与是由于残差层)
- en: If we think of the layer as analogous to the time dimension, then the structure
    of the equation is similar to the equations governing the movements of three particles,
    although in LLMs the layers are discrete, as opposed to continuous in physics.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们把层看作类似于时间维度，那么这个方程的结构类似于控制三个粒子运动的方程，尽管在 LLM 中，层是离散的，而在物理学中则是连续的。
- en: 'To make the analogy complete, we still need to convert the attention portion
    into a sort of potential. Let’s dig deeper mathematically. Pick a specific token,
    *tᵢ*, at each layer, it gets modified accordingly to the self-attention mechanism
    (ignoring multiple attention-head):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使类比完整，我们仍需要将注意力部分转换为某种潜在量。让我们从数学上更深入地挖掘。选取一个特定的令牌 *tᵢ*，在每一层中，它会根据自注意力机制进行相应的修改（忽略多个注意力头）：
- en: '![](../Images/d1147a27a12307df49c1e04d4f1ca5b9.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1147a27a12307df49c1e04d4f1ca5b9.png)'
- en: Where *Q*, *K*, *V* are the Query, Key, Value matrices typically seen in an
    attention module. For now we are ignoring normalization layers. The crux is that
    the exponential form can be re-written as the derivative of a kind of potential
    term!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *Q*、*K*、*V* 是在注意力模块中通常看到的查询、键、值矩阵。现在我们忽略了归一化层。关键是指数形式可以被重写为某种潜在项的导数！
- en: '![](../Images/335668f50a4aa9d7610bb24c203c5e18.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/335668f50a4aa9d7610bb24c203c5e18.png)'
- en: '(** while *Q*ᵀ*K* may not always be invertible and this equation may not be
    exact, *V* is an arbitrary weight in our model: So we can always trade *M* for
    *V* in our attention module to achieve the same model performance)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: (** 尽管 *Q*ᵀ*K* 可能并不总是可逆的，这个方程式可能并不完全准确，但 *V* 是我们模型中的一个任意权重：因此我们总是可以在注意力模块中用
    *V* 代替 *M* 以实现相同的模型性能)
- en: In this way, passing tokens through layers in LLMs is analogous to having groups
    of particles interacting under some pair-wise interactions! It’s sort of like
    gas molecules bumping into one another and forming weather patterns.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，通过 LLM 的各层传递令牌类似于粒子在某些成对相互作用下的相互作用！这有点像气体分子互相碰撞并形成天气模式。
- en: (** in this view, we can interpret the normalization and matrix multiplication
    *M* as a sort of projection, so that the token-particles are properly constrained
    in the system. It’s analogous to a roller coaster being confined to its tracks.)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: (** 从这个角度来看，我们可以将归一化和矩阵乘法 *M* 解释为一种投影，以便令牌粒子在系统中得到适当约束。这类似于过山车被限制在轨道上。)
- en: GPT
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT
- en: 'For (Chat)[GPT](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer)-like
    models, the discussion gets modified. The attention module has an extra casual
    structure — that tokens can only get modified by the ones before them. This means
    that the equations are missing a few terms:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于类似 [Chat](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer)
    的模型，讨论会有所修改。注意力模块具有额外的因果结构——即令牌只能被之前的令牌修改。这意味着方程中缺少一些项：
- en: '![](../Images/c4c08ba6336b93785a5854e24b98fd0f.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4c08ba6336b93785a5854e24b98fd0f.png)'
- en: Following our analogy, this means that particles are coming in one at a time,
    and each one would get stuck after going through all the interaction layers. It’s
    sort of like growing a crystal one-atom at a time.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 按照我们的类比，这意味着粒子一次一个地进入，每个粒子在经过所有的相互作用层后会被卡住。这有点像逐个原子地生长晶体。
- en: One thing to keep in mind is that our physics analogy isn’t 100% exact, as fundamental
    features like symmetries and laws like energy/momentum conservations that are
    so ubiquitous in physics do not apply to LLMs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的一点是，我们的物理类比并不是100%准确的，因为物理学中普遍存在的对称性和能量/动量守恒等基本特性并不适用于 LLM。
- en: Emergence in Language Models
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言模型中的涌现
- en: '![](../Images/6f6106610e4d44312c28c9e4356f78c4.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f6106610e4d44312c28c9e4356f78c4.png)'
- en: Like a beautiful snowflake, the output of an LLM might hinge on its physics-like
    properties (Photo by [Aaron Burden](https://unsplash.com/@aaronburden?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral))
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 像一片美丽的雪花一样，LLM 的输出可能依赖于其类似物理的属性（图片来源：[Aaron Burden](https://unsplash.com/@aaronburden?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)）
- en: Now that we have our physics analogies, how does it help us understand LLMs?
    The hope is that, like complex physical systems, we can draw analogies from other,
    more familiar and well-understood systems to gain insights into LLMs. However,
    I have to caution the reader that most of our discussions below will be speculative,
    as confirming them would require conducting detailed experimental studies on LLMs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们有了物理学的类比，它如何帮助我们理解LLMs？希望的是，像复杂物理系统一样，我们可以从其他更熟悉且理解得更透彻的系统中获得对LLMs的洞见。然而，我必须提醒读者，以下大部分讨论将是推测性的，因为确认这些观点需要对LLMs进行详细的实验研究。
- en: (* Indeed if I have more resources, I’d imagine some of these ideas might be
    fruitful academic research projects)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: (* 事实上，如果我有更多资源，我会想象这些想法可能会成为有成果的学术研究项目)
- en: Below is a sampler of how we may use the language of physics to reframe our
    understanding of LLMs.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何利用物理学语言重新框定我们对LLMs理解的示例。
- en: LLMs Training
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMs训练
- en: Using the language of thermal physics, we can think of LLMs as a tunable physical
    system, and model training is analogous to applying thermal pressure to the system
    to adjust its parameters. This viewpoint was described in my other article, “[The
    Thermodynamics of Machine Learning](https://medium.com/towards-data-science/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1),”
    so I won’t delve too much into the details here.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用热物理学的语言，我们可以将大型语言模型（LLMs）视为一个可调节的物理系统，模型训练类似于对系统施加热压以调整其参数。这一观点在我的另一篇文章“[机器学习的热力学](https://medium.com/towards-data-science/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1)”中有描述，因此我在这里不会详细讨论。
- en: Emergence of Intelligence?
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 智能的出现？
- en: While there is plenty of discussion on whether systems like ChatGPT are intelligent
    or not, I will refrain from adding to this controversial topic as I am not even
    sure how one may define intelligence. Nevertheless, it is clear that ChatGPT can
    consistently produce sophisticated and interesting outputs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管关于ChatGPT是否智能有很多讨论，但我将避免对此有争议的话题进行更多探讨，因为我甚至不确定如何定义智能。然而，很明显，ChatGPT可以持续产生复杂且有趣的输出。
- en: If we subscribe to our physics analogy, this should not be surprising. From
    snowflakes to tornadoes, we know that even simple laws can give rise to highly
    complex behaviors, and from complex behaviors, structures that appear intelligent
    could arise.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们接受物理学类比，这并不令人惊讶。从雪花到龙卷风，我们知道即使是简单的定律也可以产生高度复杂的行为，而从复杂行为中，可以出现看似智能的结构。
- en: 'Complexity as a concept is not easily defined, so in order to proceed further,
    we can try to examine some key features of complex systems: Phase transition is
    one of them.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂性作为一个概念并不容易定义，因此为了进一步探讨，我们可以尝试检查复杂系统的一些关键特征：相变就是其中之一。
- en: Phase Transition
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相变
- en: Many complex physical systems possess distinctive phases, each with a highlighted
    set of physical properties. Thus, it is reasonable to suggest that within LLMs,
    there could be distinctive phases as well, with each phase tuned to be helpful
    in specific tasks (such as coding vs proofreading).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 许多复杂的物理系统具有独特的相，每个相都有一组突出的物理属性。因此，合理的猜测是，在LLMs中也可能存在独特的相，每个相都被调整以在特定任务中（例如编码与校对）提供帮助。
- en: 'How might we verify or refute such a claim? This is where things could get
    interesting. In physics, phases arise when interactions start to form interesting
    structures. Some examples include:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何验证或反驳这种说法？这就是事情可能变得有趣的地方。在物理学中，相位出现时，交互作用开始形成有趣的结构。一些例子包括：
- en: When water cools down, the attractive forces between molecules get stronger,
    causing the molecules to stick together and form solids.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当水冷却时，分子之间的吸引力变得更强，使分子粘在一起形成固体。
- en: When a metal is cooled to extremely low temperatures, electrons may become attracted
    to each other through sound-wave (phonon) interactions, forming Type-I superconductors.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当金属被冷却到极低的温度时，电子可能通过声波（声子）相互吸引，形成I型超导体。
- en: Perhaps something analogous could occur in LLMs? For instance, in ChatGPT, one
    might speculate that certain groups of tokens from “code” or “proofread” could
    trigger an avalanche of particular forces that drive specific types of output.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs中是否可能发生类似的现象？例如，在ChatGPT中，人们可能会推测“代码”或“校对”中的某些令牌组合可能会触发一系列特定的力量，从而驱动特定类型的输出。
- en: Another technical aspect of phase transitions is the modification of symmetries.
    This is related to the creation of structures, such as ice crystal patterns from
    water vapor. While LLMs do not possess physical symmetries, they should contain
    some sort of permutation symmetries of the model weights. This is because model
    performance should be the same as long as they are initialized with the same statistics
    and trained in the same paradigm. The specific values of a particular weight only
    become important during training. This can be thought of as the freezing-in of
    the weights. However, to continue this discussion, we would need to delve into
    the technical subject of [spontaneous symmetry breaking](https://en.wikipedia.org/wiki/Spontaneous_symmetry_breaking),
    which we’ll save for a later date.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 相变的另一个技术方面是对对称性的修改。这与结构的创建有关，例如从水蒸气中形成的冰晶图案。尽管LLMs不具备物理对称性，但它们应该包含某种模型权重的排列对称性。这是因为模型性能应该是相同的，只要它们以相同的统计数据初始化并在相同的范式下训练。特定权重的具体值只有在训练过程中才变得重要。这可以看作是权重的“冻结”。然而，要继续讨论这个话题，我们需要深入探讨[自发对称破缺](https://en.wikipedia.org/wiki/Spontaneous_symmetry_breaking)的技术内容，我们将在以后再讨论。
- en: Are LLMs Efficient?
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMs高效吗？
- en: While there are many criticisms regarding the perceived inefficiency of LLMs
    due to their large number of parameters (especially when compared to physical
    models), these criticisms may not be fully warranted.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有许多关于LLMs由于其大量参数而被认为效率低下的批评（特别是与物理模型相比时），这些批评可能并不完全成立。
- en: 'Why? It comes down to the technical limitations of our computers, which result
    in significant differences between physics and LLMs:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么？这归结于我们计算机的技术限制，这导致了物理学和LLMs之间存在显著差异：
- en: Physical laws have infinite precision, while LLMs have finite precision.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 物理定律具有无限精度，而LLMs具有有限精度。
- en: Physics exhibits huge hierarchies, with some forces being tiny and others large.
    In LLMs, we attempt to make all outputs/weights similar in size through normalization.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 物理学表现出巨大的层级结构，一些力非常微小，而另一些力则很大。在LLMs中，我们通过归一化尝试使所有输出/权重大小相似。
- en: In physics, tiny effects can accumulate to enormous influences (such as Earth’s
    gravity). In LLMs, these tiny effects are often rounded away and eliminated.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在物理学中，微小的效应可以累积成巨大的影响（例如地球的引力）。在LLMs中，这些微小的效应通常会被舍去和消除。
- en: Nature is an incredibly efficient computer, with interactions computed instantaneously
    across all scales with infinite precisions. LLMs, on the other hand, are relatively
    slow computers limited by finite precisions.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自然是一个极其高效的计算机，能够以无限精度瞬时计算所有尺度上的相互作用。另一方面，LLMs则是受限于有限精度的相对较慢的计算机。
- en: This means that while we can strive to make LLMs mimic physics better and create
    more powerful models, in practice, computers are simply incapable of fully simulating
    our world (as discussed in “[Why We Don’t Live In a Simulation](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d)”).
    Therefore, resorting to a large number of parameters may be a last-resort tactic
    to address some of these deficiencies.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，虽然我们可以努力使LLMs更好地模拟物理现象并创建更强大的模型，但在实际应用中，计算机本质上无法完全模拟我们的世界（如在“[我们为什么不生活在模拟中](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d)”中讨论的）。因此，
    resorting to a large number of parameters 可能是一种应对这些不足的最后手段。
- en: It is even plausible that given finite precision, there may be an upper limit
    to the complexity achievable with standard computers. This could make it very
    challenging to significantly reduce the number of parameters (although advancements
    in [quantum computing](https://en.wikipedia.org/wiki/Quantum_computing) might
    change this in the future).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 即使考虑到有限的精度，也可以认为标准计算机能够达到的复杂度可能存在上限。这可能会使得显著减少参数数量变得非常具有挑战性（尽管[量子计算](https://en.wikipedia.org/wiki/Quantum_computing)的进展可能会在未来改变这种情况）。
- en: Improvements to LLMs
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对LLMs的改进
- en: 'Could our physics analogy help provide hints for the next generation of LLMs?
    I believe it’s possible. Logically, there are two possible directions to pursue
    based on our beliefs:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的物理类比是否可以为下一代LLMs提供一些线索？我认为这是可能的。从逻辑上讲，根据我们的信念，有两个可能的方向可以探索：
- en: '**Physics-like features are desirable**: We should draw more inspiration from
    physics to create better model structures.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**物理类似的特征是值得追求的**：我们应该从物理学中获得更多灵感，以创造更好的模型结构。'
- en: '**Physics-like features are undesirable**: Physics-like features may actually
    limit the capabilities of LLMs due to inherent computational limits, so we should
    avoid them.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**物理学特征是不可取的**：物理学特征可能由于固有的计算限制而限制 LLM 的能力，因此我们应该避免它们。'
- en: Since we are using physics to understand LLMs, I’ll focus on the first possibility.
    Under this assumption, how could we address the shortcomings of LLMs like ChatGPT?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们使用物理学来理解大型语言模型（LLMs），那么我将重点关注第一个可能性。在这种假设下，我们如何解决类似 ChatGPT 的 LLMs 的不足之处？
- en: '**Preserving Hierarchies**: Instead of solely focusing on normalizing weights
    and reducing precision, we should explore alternative approaches to account for
    diverse interactions with different strengths and scales. We could draw inspiration
    from how electromagnetism (which is very strong) and gravity (which is very weak)
    are combined in nature.'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**保持层次结构**：我们不应仅仅专注于规范化权重和降低精度，而应探索替代方法，以考虑不同强度和规模的多样化交互。我们可以借鉴自然界中电磁力（非常强）和重力（非常弱）如何结合的方式。'
- en: '**Accommodating Different Phases**: Describing both ice and water using the
    same basic molecular equations is inefficient. It’s more efficient to use difference
    descriptions for difference phases (say sounds waves vs water waves). We could
    create a better structure that naturally accommodates the macroscopic differences
    within the model.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**适应不同阶段**：使用相同的基本分子方程描述冰和水是低效的。使用不同的描述来应对不同的阶段（例如声波与水波）会更有效。我们可以创建一种更好的结构，能够自然地适应模型中的宏观差异。'
- en: '**Advanced Physics Techniques**: In physics, we don’t study emergent phenomena
    using only fundamental equations. Techniques like [thermodynamics](https://en.wikipedia.org/wiki/Thermodynamics),
    [mean-field theory](https://en.wikipedia.org/wiki/Mean-field_theory) and [renormalization](https://en.wikipedia.org/wiki/Renormalization)
    can be used to help us simplify the problem. Incorporating some of these ideas
    into the building blocks of LLMs could improve their efficiency. For example,
    recent advances on [linear attention](https://arxiv.org/abs/2006.16236) (A. Katharopoulos
    et. al.) may already be interpreted as a sort of mean-field approach.'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**高级物理学技术**：在物理学中，我们不仅仅使用基本方程研究涌现现象。技术如 [热力学](https://en.wikipedia.org/wiki/Thermodynamics)、[均场理论](https://en.wikipedia.org/wiki/Mean-field_theory)
    和 [重整化](https://en.wikipedia.org/wiki/Renormalization) 可以帮助我们简化问题。将这些思想的一部分融入
    LLM 的构建模块中，可能会提高它们的效率。例如，最近对 [线性注意力](https://arxiv.org/abs/2006.16236)（A. Katharopoulos
    等）的进展，可能已经被解释为一种均场方法。'
- en: By exploring these avenues, we may be able to enhance the capabilities and efficiencies
    of LLMs, leveraging insights from physics to advance the field further.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 通过探索这些途径，我们或许能够提升 LLM 的能力和效率，利用物理学的洞察力进一步推动该领域的发展。
- en: Epilogue
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结语
- en: In summary, we have showcased how the mathematics of LLMs resemble those in
    physics. This allows us to use our intuitions about everyday physical systems
    to understand these new emergent phenomena, such as ChatGPT. I hope that this
    helps demystify the reasons behind the characteristics of LLMs.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们展示了 LLM 的数学如何与物理学中的数学相似。这使我们能够利用对日常物理系统的直觉来理解这些新兴现象，例如 ChatGPT。我希望这有助于揭示
    LLM 特征背后的原因。
- en: More generally, I hope I have conveyed to you how physics can provide valuable
    insights into complex subjects like LLMs. I firmly believe that science is most
    effective when we borrow insights from seemingly disparate fields
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地说，我希望我已经向你传达了物理学如何为像 LLMs 这样的复杂主题提供有价值的见解。我坚信，当我们从看似不相关的领域中借鉴洞察力时，科学最为有效。
- en: If you enjoyed this article, you might be interested in my other pieces on similar
    topics, such as the link between physics and AI.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章，你可能会对我关于类似主题的其他文章感兴趣，例如物理学与人工智能之间的联系。
- en: Please leave a comment or provide feedback, as it encourages me to write more
    insightful pieces! 👋
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 请留下评论或提供反馈，这鼓励我写出更多有见地的文章！👋
- en: '[](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----ea512bcc6a64--------------------------------)
    [## A Physicist’s View of Machine Learning: The Thermodynamics of Machine Learning'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----ea512bcc6a64--------------------------------)
    [## 物理学家对机器学习的看法：机器学习的热力学'
- en: Complex systems in nature can be successfully studied using thermodynamics.
    What about Machine Learning?
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自然界中的复杂系统可以通过热力学进行成功研究。那么，机器学习呢？
- en: towardsdatascience.com](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----ea512bcc6a64--------------------------------)
    [](/the-meaning-behind-logistic-classification-from-physics-291774fda579?source=post_page-----ea512bcc6a64--------------------------------)
    [## The Meaning Behind Logistic Classification, from Physics
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[逻辑回归背后的意义，从物理学角度看](/the-meaning-behind-logistic-classification-from-physics-291774fda579?source=post_page-----ea512bcc6a64--------------------------------)
    [## 逻辑回归背后的意义，从物理学角度看]'
- en: Why do we use the logistic and softmax functions? Thermal physics may have an
    answer.
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么我们使用逻辑回归和softmax函数？热物理学或许能提供答案。
- en: 'towardsdatascience.com](/the-meaning-behind-logistic-classification-from-physics-291774fda579?source=post_page-----ea512bcc6a64--------------------------------)
    [](/why-causation-is-correlation-a-physicists-perspective-part-1-742696d130e8?source=post_page-----ea512bcc6a64--------------------------------)
    [## Why Causation Is Correlation: A Physicist’s Perspective (Part 1)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[为什么因果关系是相关性的体现：物理学家的视角（第一部分）](/why-causation-is-correlation-a-physicists-perspective-part-1-742696d130e8?source=post_page-----ea512bcc6a64--------------------------------)
    [## 为什么因果关系是相关性的体现：物理学家的视角（第一部分）]'
- en: We’ve all heard the phrase, “correlation does not imply causation”, but no one
    ever talks about what causation really…
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们都听说过“相关性不代表因果性”这句话，但没有人真正谈论因果性到底是什么…
- en: towardsdatascience.com](/why-causation-is-correlation-a-physicists-perspective-part-1-742696d130e8?source=post_page-----ea512bcc6a64--------------------------------)
    [](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----ea512bcc6a64--------------------------------)
    [## Why We Don’t Live in a Simulation
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[为什么我们不生活在模拟中](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----ea512bcc6a64--------------------------------)
    [## 为什么我们不生活在模拟中]'
- en: Describing reality as a simulation vastly understates the complexities of our
    world. Here’s why the simulation…
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将现实描述为模拟极大地低估了我们世界的复杂性。以下是为什么模拟…
- en: medium.com](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----ea512bcc6a64--------------------------------)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----ea512bcc6a64--------------------------------)'
