- en: 'Understanding Large Language Models: The Physics of (Chat)GPT and BERT'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼š**(Chat)GPT å’Œ BERT çš„ç‰©ç†å­¦**
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/understanding-large-language-models-the-physics-of-chat-gpt-and-bert-ea512bcc6a64](https://towardsdatascience.com/understanding-large-language-models-the-physics-of-chat-gpt-and-bert-ea512bcc6a64)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/understanding-large-language-models-the-physics-of-chat-gpt-and-bert-ea512bcc6a64](https://towardsdatascience.com/understanding-large-language-models-the-physics-of-chat-gpt-and-bert-ea512bcc6a64)
- en: Insights from a physicist, on how particles and forces can help us understand
    LLMs.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»ç‰©ç†å­¦å®¶çš„è§’åº¦æ¢è®¨ç²’å­å’ŒåŠ›é‡å¦‚ä½•å¸®åŠ©æˆ‘ä»¬ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚
- en: '[](https://tim-lou.medium.com/?source=post_page-----ea512bcc6a64--------------------------------)[![Tim
    Lou, PhD](../Images/e4931bb6d59e27730529ceaf00a23822.png)](https://tim-lou.medium.com/?source=post_page-----ea512bcc6a64--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ea512bcc6a64--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ea512bcc6a64--------------------------------)
    [Tim Lou, PhD](https://tim-lou.medium.com/?source=post_page-----ea512bcc6a64--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://tim-lou.medium.com/?source=post_page-----ea512bcc6a64--------------------------------)[![Tim
    Lou, PhD](../Images/e4931bb6d59e27730529ceaf00a23822.png)](https://tim-lou.medium.com/?source=post_page-----ea512bcc6a64--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ea512bcc6a64--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ea512bcc6a64--------------------------------)
    [Tim Lou, PhD](https://tim-lou.medium.com/?source=post_page-----ea512bcc6a64--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ea512bcc6a64--------------------------------)
    Â·12 min readÂ·Jul 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ea512bcc6a64--------------------------------)
    Â·12 åˆ†é’Ÿé˜…è¯»Â·2023å¹´7æœˆ20æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/471184f0f0e17bba6faaa3b9fca21a6c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/471184f0f0e17bba6faaa3b9fca21a6c.png)'
- en: 'ChatGPT and ice crystals may have more in common than one might think (credit:
    [15414483@pixabay](https://pixabay.com/photos/ice-frost-winter-snow-snowflakes-6538605/))'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT å’Œå†°æ™¶ä¹‹é—´å¯èƒ½æœ‰æ›´å¤šçš„ç›¸ä¼¼ä¹‹å¤„ï¼ˆæ¥æºï¼š[15414483@pixabay](https://pixabay.com/photos/ice-frost-winter-snow-snowflakes-6538605/))
- en: ChatGPT, or more broadly Large Language AI Models (LLMs), have become ubiquitous
    in our lives. Yet, most of the mathematics and internal structures of LLMs are
    obscure knowledge to the general public.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPTï¼Œæˆ–è€…æ›´å¹¿æ³›çš„è¯´ï¼Œå¤§å‹è¯­è¨€ AI æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå·²ç»åœ¨æˆ‘ä»¬çš„ç”Ÿæ´»ä¸­æ— å¤„ä¸åœ¨ã€‚ç„¶è€Œï¼ŒLLMs çš„å¤§éƒ¨åˆ†æ•°å­¦å’Œå†…éƒ¨ç»“æ„å¯¹äºæ™®é€šå¤§ä¼—æ¥è¯´æ˜¯æ¨¡ç³Šçš„çŸ¥è¯†ã€‚
- en: So, how can we move beyond perceiving LLMs like ChatGPT as magical black boxes?
    Physics may provide an answer.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•æ‰èƒ½è¶…è¶Šå°† LLMsï¼Œå¦‚ ChatGPTï¼Œè§†ä¸ºç¥ç§˜é»‘ç®±çš„è§‚å¿µå‘¢ï¼Ÿç‰©ç†å­¦æˆ–è®¸èƒ½æä¾›ç­”æ¡ˆã€‚
- en: Everyone is somewhat familiar with our physical world. Objects such as cars,
    tables, and planets are composed of trillions of atoms, governed by a simple set
    of physical laws. Similarly, complex organisms, like ChatGPT, have emerged and
    are capable of generating highly sophisticated concepts like arts and sciences.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªäººå¯¹æˆ‘ä»¬çš„ç‰©ç†ä¸–ç•Œéƒ½æœ‰ä¸€å®šçš„äº†è§£ã€‚åƒæ±½è½¦ã€æ¡Œå­å’Œè¡Œæ˜Ÿè¿™æ ·çš„ç‰©ä½“ç”±æ•°ä¸‡äº¿ä¸ªåŸå­ç»„æˆï¼Œå—ä¸€å¥—ç®€å•ç‰©ç†æ³•åˆ™çš„æ”¯é…ã€‚åŒæ ·ï¼Œåƒ ChatGPT è¿™æ ·çš„å¤æ‚ç”Ÿç‰©ä½“ä¹Ÿå·²ç»å‡ºç°ï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆåƒè‰ºæœ¯å’Œç§‘å­¦è¿™æ ·é«˜åº¦å¤æ‚çš„æ¦‚å¿µã€‚
- en: It turns out that the equations of the building blocks of LLMs, are analogous
    to our physical laws. So that by understanding how complexity arises from our
    simple physical laws, we might be able to gleam some insight on how and why LLMs
    work.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœè¡¨æ˜ï¼ŒLLMs çš„æ„å»ºæ¨¡å—çš„æ–¹ç¨‹ç±»ä¼¼äºæˆ‘ä»¬çš„ç‰©ç†æ³•åˆ™ã€‚å› æ­¤ï¼Œé€šè¿‡ç†è§£å¤æ‚æ€§å¦‚ä½•ä»ç®€å•çš„ç‰©ç†æ³•åˆ™ä¸­äº§ç”Ÿï¼Œæˆ‘ä»¬å¯èƒ½èƒ½å¤Ÿè·å¾—ä¸€äº›å…³äº LLMs å·¥ä½œåŸç†å’ŒåŸå› çš„è§è§£ã€‚
- en: Complexity from Simplicity
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»ç®€å•ä¸­çœ‹å¤æ‚
- en: '![](../Images/22f0277b4b16b574f6da4ceaa92c46ca.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22f0277b4b16b574f6da4ceaa92c46ca.png)'
- en: Complex structures, like bubble films and the convection cells within, are generated
    by simple physical laws (Photo by [chuttersnap](https://unsplash.com/@chuttersnap?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral))
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å¤æ‚çš„ç»“æ„ï¼Œå¦‚æ°”æ³¡è†œå’Œå…¶ä¸­çš„å¯¹æµå•å…ƒï¼Œæ˜¯ç”±ç®€å•çš„ç‰©ç†æ³•åˆ™ç”Ÿæˆçš„ï¼ˆç…§ç‰‡æ¥æºï¼š[chuttersnap](https://unsplash.com/@chuttersnap?utm_source=medium&utm_medium=referral)
    åœ¨ [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)ï¼‰
- en: Our world is inherently complex, yet it can be described by a remarkably small
    number of fundamental interactions. For instance, complicated snowflakes and bubble
    films can be linked to simple attractive forces between molecules.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ä¸–ç•Œæœ¬è´¨ä¸Šæ˜¯å¤æ‚çš„ï¼Œä½†å®ƒå¯ä»¥é€šè¿‡æå°‘é‡çš„åŸºæœ¬ç›¸äº’ä½œç”¨æ¥æè¿°ã€‚ä¾‹å¦‚ï¼Œå¤æ‚çš„é›ªèŠ±å’Œæ°”æ³¡è†œå¯ä»¥ä¸åˆ†å­ä¹‹é—´ç®€å•çš„å¸å¼•åŠ›è”ç³»èµ·æ¥ã€‚
- en: So, what is the commonality in how complex structures arise? In physics, complexity
    is generated when we zoom out from the smallest to the largest scale.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œå¤æ‚ç»“æ„äº§ç”Ÿçš„å…±åŒç‚¹æ˜¯ä»€ä¹ˆï¼Ÿåœ¨ç‰©ç†å­¦ä¸­ï¼Œå¤æ‚æ€§æ˜¯å½“æˆ‘ä»¬ä»æœ€å°å°ºåº¦æ”¾å¤§åˆ°æœ€å¤§å°ºåº¦æ—¶äº§ç”Ÿçš„ã€‚
- en: Drawing an analogy to language, English starts with a modest number of fundamental
    constituents â€” 26 symbols. These symbols can combine to form around 100,000 usable
    words, each carrying a distinctive meaning. From these words, a countless number
    of sentences, passages, books, and volumes can be generated.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥è¯­è¨€ä¸ºç±»æ¯”ï¼Œè‹±è¯­ä»æœ‰é™æ•°é‡çš„åŸºæœ¬æˆåˆ†å¼€å§‹â€”â€”26ä¸ªç¬¦å·ã€‚è¿™äº›ç¬¦å·å¯ä»¥ç»„åˆæˆå¤§çº¦100,000ä¸ªå¯ç”¨çš„å•è¯ï¼Œæ¯ä¸ªå•è¯éƒ½æºå¸¦ç‹¬ç‰¹çš„æ„ä¹‰ã€‚ä»è¿™äº›å•è¯ä¸­ï¼Œå¯ä»¥ç”Ÿæˆæ— æ•°çš„å¥å­ã€æ®µè½ã€ä¹¦ç±å’Œå·å†Œã€‚
- en: This linguistic hierarchy is similar to the ones found in physics. Our current
    fundamental law (the [Standard Model](https://en.wikipedia.org/wiki/Standard_Model))
    starts with a limited number of elementary particles such as quarks and electrons,
    along with a few interactions mediated by force carriers like photons. These particles
    and forces combine to form atoms, each carrying a distinctive chemical property.
    From these atoms, an immense number of molecules, structures, cells, and beings
    are created.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è¯­è¨€å­¦å±‚æ¬¡ç»“æ„ç±»ä¼¼äºç‰©ç†å­¦ä¸­çš„å±‚æ¬¡ç»“æ„ã€‚æˆ‘ä»¬ç›®å‰çš„åŸºæœ¬æ³•åˆ™ï¼ˆ[æ ‡å‡†æ¨¡å‹](https://en.wikipedia.org/wiki/Standard_Model)ï¼‰ä»¥æœ‰é™æ•°é‡çš„åŸºæœ¬ç²’å­å¼€å§‹ï¼Œå¦‚å¤¸å…‹å’Œç”µå­ï¼Œä»¥åŠç”±å…‰å­ç­‰åŠ›å­ä»‹å¯¼çš„ä¸€äº›ç›¸äº’ä½œç”¨ã€‚è¿™äº›ç²’å­å’ŒåŠ›ç»“åˆå½¢æˆåŸå­ï¼Œæ¯ç§åŸå­å…·æœ‰ç‹¬ç‰¹çš„åŒ–å­¦æ€§è´¨ã€‚ä»è¿™äº›åŸå­ä¸­ï¼Œäº§ç”Ÿäº†å¤§é‡çš„åˆ†å­ã€ç»“æ„ã€ç»†èƒå’Œç”Ÿç‰©ã€‚
- en: 'In our physical world, there is a sort of emergent universality: even though
    many complex systems have totally different origins, they often share some universal
    characteristics. For instance, many liquids, despite having distinctive chemical
    properties, share three common phases (liquid, solid, and gas). As a more extreme
    example, the physics behind certain materials ([Type-I superconductors](https://en.wikipedia.org/wiki/Type-I_superconductor))
    can be borrowed to describe fundamental physics (the celebrated [Higgs Mechanism](https://en.wikipedia.org/wiki/Higgs_mechanism)).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„ç‰©ç†ä¸–ç•Œä¸­ï¼Œæœ‰ä¸€ç§æ¶Œç°çš„æ™®éæ€§ï¼šå°½ç®¡è®¸å¤šå¤æ‚ç³»ç»Ÿæœ‰ç€å®Œå…¨ä¸åŒçš„èµ·æºï¼Œä½†å®ƒä»¬å¸¸å¸¸å…±äº«ä¸€äº›æ™®éç‰¹å¾ã€‚ä¾‹å¦‚ï¼Œè®¸å¤šæ¶²ä½“å°½ç®¡å…·æœ‰ä¸åŒçš„åŒ–å­¦æ€§è´¨ï¼Œå´å…±äº«ä¸‰ç§å…±åŒçš„ç›¸æ€ï¼ˆæ¶²æ€ã€å›ºæ€å’Œæ°”æ€ï¼‰ã€‚ä½œä¸ºä¸€ä¸ªæ›´æç«¯çš„ä¾‹å­ï¼ŒæŸäº›ææ–™çš„ç‰©ç†å­¦ï¼ˆ[Iå‹è¶…å¯¼ä½“](https://en.wikipedia.org/wiki/Type-I_superconductor)ï¼‰å¯ä»¥å€Ÿç”¨æ¥æè¿°åŸºæœ¬ç‰©ç†å­¦ï¼ˆè‘—åçš„[å¸Œæ ¼æ–¯æœºåˆ¶](https://en.wikipedia.org/wiki/Higgs_mechanism)ï¼‰ã€‚
- en: Although it is important to keep in mind the difference between language and
    physics â€” that physical laws are dictated and constrained by nature, while languages
    are human creations that seem unconstrained â€” there is no requirement for linguistic
    complexity to resemble physical complexity in our world.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡éœ€è¦ç‰¢è®°è¯­è¨€å’Œç‰©ç†å­¦ä¹‹é—´çš„åŒºåˆ«â€”â€”ç‰©ç†å®šå¾‹ç”±è‡ªç„¶è§„å®šå¹¶å—åˆ°é™åˆ¶ï¼Œè€Œè¯­è¨€æ˜¯çœ‹ä¼¼ä¸å—çº¦æŸçš„äººç±»åˆ›é€ â€”â€”ä½†è¯­è¨€å¤æ‚æ€§ä¸å¿…ä¸æˆ‘ä»¬ä¸–ç•Œä¸­çš„ç‰©ç†å¤æ‚æ€§ç›¸ä¼¼ã€‚
- en: However, as we will argue below, ChatGPT, and other LLMs alike, contain particle-physics-like
    structures. If we believe that these structures are the key to the successes of
    LLMs, it may provide hints that linguistic complexity shares some commonalities
    with physical complexities. Additionally, this could provide valuable insight
    into why and how LLMs work.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæ­£å¦‚æˆ‘ä»¬å°†è¦è®ºè¯çš„é‚£æ ·ï¼ŒChatGPTå’Œå…¶ä»–ç±»ä¼¼çš„LLMsåŒ…å«ç±»ä¼¼ç²’å­ç‰©ç†å­¦çš„ç»“æ„ã€‚å¦‚æœæˆ‘ä»¬è®¤ä¸ºè¿™äº›ç»“æ„æ˜¯LLMsæˆåŠŸçš„å…³é”®ï¼Œå®ƒå¯èƒ½ä¼šæä¾›è¯­è¨€å¤æ‚æ€§ä¸ç‰©ç†å¤æ‚æ€§å…±äº«ä¸€äº›å…±åŒç‚¹çš„çº¿ç´¢ã€‚æ­¤å¤–ï¼Œè¿™ä¹Ÿå¯èƒ½ä¸ºæˆ‘ä»¬æä¾›å…³äºLLMså¦‚ä½•å·¥ä½œçš„æœ‰ä»·å€¼è§è§£ã€‚
- en: The Physics of Language Models
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯­è¨€æ¨¡å‹çš„ç‰©ç†å­¦
- en: '![](../Images/32cbf598a9a5f6c0d34534c91e3ad0da.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32cbf598a9a5f6c0d34534c91e3ad0da.png)'
- en: 'Physical laws are governed by equations, but what about LLMs like ChatGPT?
    (credit: authorâ€™s own work)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰©ç†å®šå¾‹ç”±æ–¹ç¨‹ governï¼Œä½†åƒChatGPTè¿™æ ·çš„LLMså‘¢ï¼Ÿï¼ˆæ¥æºï¼šä½œè€…è‡ªèº«çš„å·¥ä½œï¼‰
- en: 'In order to connect LLMs to physics, we need to relate their underlying mathematical
    structures. In physics, the movement of particles (or more generally, fields or
    states), can be written schematically as:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å°†LLMsä¸ç‰©ç†å­¦è”ç³»èµ·æ¥ï¼Œæˆ‘ä»¬éœ€è¦å°†å®ƒä»¬çš„åŸºç¡€æ•°å­¦ç»“æ„è¿›è¡Œå…³è”ã€‚åœ¨ç‰©ç†å­¦ä¸­ï¼Œç²’å­çš„è¿åŠ¨ï¼ˆæˆ–æ›´ä¸€èˆ¬åœ°è¯´ï¼Œåœºæˆ–çŠ¶æ€ï¼‰å¯ä»¥ç¤ºæ„æ€§åœ°è¡¨ç¤ºä¸ºï¼š
- en: '![](../Images/56b8c41531d0692d1f9388058e2da595.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56b8c41531d0692d1f9388058e2da595.png)'
- en: A schematic form of physical equations
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰©ç†æ–¹ç¨‹çš„ç¤ºæ„å›¾
- en: '(** technical note: the [Hamiltonian](https://en.wikipedia.org/wiki/Hamiltonian_mechanics)
    formalism makes it more precise, although the derivative part needs to be modified
    a bit)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (** æŠ€æœ¯è¯´æ˜ï¼š[å“ˆå¯†é¡¿é‡](https://en.wikipedia.org/wiki/Hamiltonian_mechanics)å½¢å¼ä½¿å…¶æ›´ç²¾ç¡®ï¼Œå°½ç®¡å¯¼æ•°éƒ¨åˆ†éœ€è¦ç¨ä½œä¿®æ”¹)
- en: Intuitively, it states that particles move because of forces, which come from
    the slopes of some abstract object called *potential*. This is analogous to water
    flowing down a stream, where the potential would then come from gravity and fluid
    dynamics.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´è§‚åœ°è¯´ï¼Œå®ƒè¡¨æ˜ç²’å­å› åŠ›çš„ä½œç”¨è€Œç§»åŠ¨ï¼Œè¿™äº›åŠ›æ¥æºäºä¸€äº›æŠ½è±¡å¯¹è±¡å«åš*åŠ¿èƒ½*çš„æ–œç‡ã€‚è¿™ç±»ä¼¼äºæ°´æµä¸‹å±±ï¼ŒåŠ¿èƒ½æ¥è‡ªé‡åŠ›å’Œæµä½“åŠ¨åŠ›å­¦ã€‚
- en: 'It turns out, the structure of LLMs is very similar: they break down sentences
    into fundamental constituents called *tokens*, and the AI model modifies these
    tokens layer-by-layer in an analogous way:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœè¡¨æ˜ï¼ŒLLMsçš„ç»“æ„éå¸¸ç›¸ä¼¼ï¼šå®ƒä»¬å°†å¥å­æ‹†è§£ä¸ºåŸºæœ¬ç»„æˆéƒ¨åˆ†ï¼Œå³*tokens*ï¼Œå¹¶ä»¥ç±»ä¼¼çš„æ–¹å¼é€å±‚ä¿®æ”¹è¿™äº›tokensï¼š
- en: '![](../Images/abcfdcd9b5410a45702255ab7e0ee267.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/abcfdcd9b5410a45702255ab7e0ee267.png)'
- en: A schematic equation describing the essence in LLMs
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æè¿°LLMsæœ¬è´¨çš„ç¤ºæ„æ–¹ç¨‹
- en: This will be made more precise in the technical section below. From this, we
    can draw an analogy
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†åœ¨ä¸‹é¢çš„æŠ€æœ¯éƒ¨åˆ†ä¸­æ›´ä¸ºå‡†ç¡®åœ°è¯´æ˜ã€‚ç”±æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œç±»æ¯”ã€‚
- en: Transformer based language models are treating words as particles, which move
    around under the influence of each other, generating intriguing patterns.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹å°†è¯è¯­è§†ä½œç²’å­ï¼Œè¿™äº›ç²’å­åœ¨ç›¸äº’å½±å“ä¸‹ç§»åŠ¨ï¼Œç”Ÿæˆå¼•äººå…¥èƒœçš„æ¨¡å¼ã€‚
- en: In this way, just like how water molecules can build beautiful snowflakes, or
    how liquid-soap mixtures can create intricate bubble patterns, interesting results
    from ChatGPT might be attributed to its physics-like behaviors.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼Œå°±åƒæ°´åˆ†å­å¯ä»¥æ„å»ºç¾ä¸½çš„é›ªèŠ±ï¼Œæˆ–æ¶²ä½“è‚¥çš‚æ··åˆç‰©å¯ä»¥åˆ›é€ å¤æ‚çš„æ°”æ³¡å›¾æ¡ˆä¸€æ ·ï¼ŒChatGPTçš„æœ‰è¶£ç»“æœå¯èƒ½å½’å› äºå…¶ç±»ä¼¼ç‰©ç†çš„è¡Œä¸ºã€‚
- en: In the following optional section, weâ€™ll describe in more details how this analogy
    come to be more rigorously, and then dive into the specifics on how this insight
    could help us understand LLMs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„å¯é€‰éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†æ›´è¯¦ç»†åœ°æè¿°è¿™ä¸€ç±»æ¯”å¦‚ä½•å˜å¾—æ›´åŠ ä¸¥è°¨ï¼Œç„¶åæ·±å…¥æ¢è®¨è¿™ä¸€æ´è§å¦‚ä½•å¸®åŠ©æˆ‘ä»¬ç†è§£LLMsã€‚
- en: Technical Detour
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æŠ€æœ¯ç»•é“
- en: Below we provide a more detailed explanation on how one can think of LLMs as
    physics models.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æˆ‘ä»¬å°†æ›´è¯¦ç»†åœ°è§£é‡Šå¦‚ä½•å°†LLMsè§†ä½œç‰©ç†æ¨¡å‹ã€‚
- en: 'From the physics side, on a microscopic level, each particle is typically influenced
    by all the particles in the system. For instance, letâ€™s consider a hypothetical
    world with only 3 particles; in this case, there would be a total of 3 Ã— 3 = 9
    possible potentials between one particle and another. Schematically, we can represent
    this as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ç‰©ç†å­¦çš„è§’åº¦æ¥çœ‹ï¼Œåœ¨å¾®è§‚å±‚é¢ä¸Šï¼Œæ¯ä¸ªç²’å­é€šå¸¸ä¼šå—åˆ°ç³»ç»Ÿä¸­æ‰€æœ‰ç²’å­çš„å½±å“ã€‚ä¾‹å¦‚ï¼Œå‡è®¾ä¸€ä¸ªåªæœ‰3ä¸ªç²’å­çš„å‡æƒ³ä¸–ç•Œï¼›åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸€ä¸ªç²’å­å’Œå¦ä¸€ä¸ªç²’å­ä¹‹é—´ä¼šæœ‰æ€»å…±3
    Ã— 3 = 9ç§å¯èƒ½çš„åŠ¿èƒ½ã€‚ä»ç¤ºæ„å›¾æ¥çœ‹ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·è¡¨ç¤ºï¼š
- en: '![](../Images/4b97def960af4cac94cd1b822874070f.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b97def960af4cac94cd1b822874070f.png)'
- en: A sketch for the equation of motion for 3 particles
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æè¿°ä¸‰ä¸ªç²’å­è¿åŠ¨æ–¹ç¨‹çš„ç¤ºæ„å›¾
- en: (**In physics, the potentials are usually symmetric i.e. Potentialâ‚â‚‚ = Potentialâ‚‚â‚,
    we are relaxing this constraint here)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: (**åœ¨ç‰©ç†å­¦ä¸­ï¼ŒåŠ¿èƒ½é€šå¸¸æ˜¯å¯¹ç§°çš„ï¼Œå³Potentialâ‚â‚‚ = Potentialâ‚‚â‚ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæ”¾å®½äº†è¿™ä¸€çº¦æŸ**)
- en: 'To see how this relates to LLMs, letâ€™s recall some basic facts:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†äº†è§£è¿™ä¸LLMsçš„å…³ç³»ï¼Œè®©æˆ‘ä»¬å›é¡¾ä¸€äº›åŸºæœ¬äº‹å®ï¼š
- en: To feed data into LLMs, a document or text is broken down into tokens. Tokens
    typically consist of one word or part of a word. Like particles, tokens are thought
    of as the smallest indivisible constituents in an LLM.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸ºäº†å°†æ•°æ®è¾“å…¥LLMsï¼Œæ–‡æ¡£æˆ–æ–‡æœ¬ä¼šè¢«æ‹†åˆ†æˆtokensã€‚tokensé€šå¸¸ç”±ä¸€ä¸ªè¯æˆ–è¯çš„ä¸€éƒ¨åˆ†ç»„æˆã€‚åƒç²’å­ä¸€æ ·ï¼Œtokensè¢«è§†ä¸ºLLMä¸­æœ€å°çš„ä¸å¯åˆ†å‰²çš„ç»„æˆéƒ¨åˆ†ã€‚
- en: LLMs have multiple layers, and in each layer, all the tokens are modified by
    self-attention modules.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLMså…·æœ‰å¤šä¸ªå±‚çº§ï¼Œåœ¨æ¯ä¸€å±‚ä¸­ï¼Œæ‰€æœ‰çš„tokenéƒ½è¢«è‡ªæ³¨æ„åŠ›æ¨¡å—æ‰€ä¿®æ”¹ã€‚
- en: The final output layer aggregates the tokens to form predictions, which can
    be used for classifications or for generating texts/images.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ€ç»ˆè¾“å‡ºå±‚æ±‡èštokenså½¢æˆé¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹å¯ç”¨äºåˆ†ç±»æˆ–ç”Ÿæˆæ–‡æœ¬/å›¾åƒã€‚
- en: If we take a three token example (say from the sentence â€œI like physicsâ€), what
    would the equations look like?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬ä»¥ä¸‰ä¸ªtokençš„ä¾‹å­ï¼ˆæ¯”å¦‚æ¥è‡ªå¥å­â€œI like physicsâ€ï¼‰è¿›è¡Œåˆ†æï¼Œè¿™äº›æ–¹ç¨‹ä¼šæ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿ
- en: 'There are some small differences depending on the specific types of LLMs weâ€™re
    working with: [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) or [GPT](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®æˆ‘ä»¬æ‰€å¤„ç†çš„LLMsçš„å…·ä½“ç±»å‹ï¼Œæœ‰ä¸€äº›å°çš„å·®å¼‚ï¼š[BERT](https://en.wikipedia.org/wiki/BERT_(language_model))æˆ–[GPT](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer)ã€‚
- en: BERT Models
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERTæ¨¡å‹
- en: 'For BERT-like models (typically used for classification), each layer would
    modify the tokens schematically as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç±»ä¼¼BERTçš„æ¨¡å‹ï¼ˆé€šå¸¸ç”¨äºåˆ†ç±»ï¼‰ï¼Œæ¯ä¸€å±‚ä¼šæŒ‰å¦‚ä¸‹æ–¹å¼ç¤ºæ„æ€§åœ°ä¿®æ”¹tokensï¼š
- en: '![](../Images/85c31a78fc2af9e22b893756aabfe8c4.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85c31a78fc2af9e22b893756aabfe8c4.png)'
- en: (** the reason layerâ‚ is involved is due to the residual layer)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: (** å±‚â‚ çš„å‚ä¸æ˜¯ç”±äºæ®‹å·®å±‚)
- en: If we think of the layer as analogous to the time dimension, then the structure
    of the equation is similar to the equations governing the movements of three particles,
    although in LLMs the layers are discrete, as opposed to continuous in physics.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æŠŠå±‚çœ‹ä½œç±»ä¼¼äºæ—¶é—´ç»´åº¦ï¼Œé‚£ä¹ˆè¿™ä¸ªæ–¹ç¨‹çš„ç»“æ„ç±»ä¼¼äºæ§åˆ¶ä¸‰ä¸ªç²’å­è¿åŠ¨çš„æ–¹ç¨‹ï¼Œå°½ç®¡åœ¨ LLM ä¸­ï¼Œå±‚æ˜¯ç¦»æ•£çš„ï¼Œè€Œåœ¨ç‰©ç†å­¦ä¸­åˆ™æ˜¯è¿ç»­çš„ã€‚
- en: 'To make the analogy complete, we still need to convert the attention portion
    into a sort of potential. Letâ€™s dig deeper mathematically. Pick a specific token,
    *táµ¢*, at each layer, it gets modified accordingly to the self-attention mechanism
    (ignoring multiple attention-head):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿ç±»æ¯”å®Œæ•´ï¼Œæˆ‘ä»¬ä»éœ€è¦å°†æ³¨æ„åŠ›éƒ¨åˆ†è½¬æ¢ä¸ºæŸç§æ½œåœ¨é‡ã€‚è®©æˆ‘ä»¬ä»æ•°å­¦ä¸Šæ›´æ·±å…¥åœ°æŒ–æ˜ã€‚é€‰å–ä¸€ä¸ªç‰¹å®šçš„ä»¤ç‰Œ *táµ¢*ï¼Œåœ¨æ¯ä¸€å±‚ä¸­ï¼Œå®ƒä¼šæ ¹æ®è‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œç›¸åº”çš„ä¿®æ”¹ï¼ˆå¿½ç•¥å¤šä¸ªæ³¨æ„åŠ›å¤´ï¼‰ï¼š
- en: '![](../Images/d1147a27a12307df49c1e04d4f1ca5b9.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1147a27a12307df49c1e04d4f1ca5b9.png)'
- en: Where *Q*, *K*, *V* are the Query, Key, Value matrices typically seen in an
    attention module. For now we are ignoring normalization layers. The crux is that
    the exponential form can be re-written as the derivative of a kind of potential
    term!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *Q*ã€*K*ã€*V* æ˜¯åœ¨æ³¨æ„åŠ›æ¨¡å—ä¸­é€šå¸¸çœ‹åˆ°çš„æŸ¥è¯¢ã€é”®ã€å€¼çŸ©é˜µã€‚ç°åœ¨æˆ‘ä»¬å¿½ç•¥äº†å½’ä¸€åŒ–å±‚ã€‚å…³é”®æ˜¯æŒ‡æ•°å½¢å¼å¯ä»¥è¢«é‡å†™ä¸ºæŸç§æ½œåœ¨é¡¹çš„å¯¼æ•°ï¼
- en: '![](../Images/335668f50a4aa9d7610bb24c203c5e18.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/335668f50a4aa9d7610bb24c203c5e18.png)'
- en: '(** while *Q*áµ€*K* may not always be invertible and this equation may not be
    exact, *V* is an arbitrary weight in our model: So we can always trade *M* for
    *V* in our attention module to achieve the same model performance)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: (** å°½ç®¡ *Q*áµ€*K* å¯èƒ½å¹¶ä¸æ€»æ˜¯å¯é€†çš„ï¼Œè¿™ä¸ªæ–¹ç¨‹å¼å¯èƒ½å¹¶ä¸å®Œå…¨å‡†ç¡®ï¼Œä½† *V* æ˜¯æˆ‘ä»¬æ¨¡å‹ä¸­çš„ä¸€ä¸ªä»»æ„æƒé‡ï¼šå› æ­¤æˆ‘ä»¬æ€»æ˜¯å¯ä»¥åœ¨æ³¨æ„åŠ›æ¨¡å—ä¸­ç”¨
    *V* ä»£æ›¿ *M* ä»¥å®ç°ç›¸åŒçš„æ¨¡å‹æ€§èƒ½)
- en: In this way, passing tokens through layers in LLMs is analogous to having groups
    of particles interacting under some pair-wise interactions! Itâ€™s sort of like
    gas molecules bumping into one another and forming weather patterns.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼Œé€šè¿‡ LLM çš„å„å±‚ä¼ é€’ä»¤ç‰Œç±»ä¼¼äºç²’å­åœ¨æŸäº›æˆå¯¹ç›¸äº’ä½œç”¨ä¸‹çš„ç›¸äº’ä½œç”¨ï¼è¿™æœ‰ç‚¹åƒæ°”ä½“åˆ†å­äº’ç›¸ç¢°æ’å¹¶å½¢æˆå¤©æ°”æ¨¡å¼ã€‚
- en: (** in this view, we can interpret the normalization and matrix multiplication
    *M* as a sort of projection, so that the token-particles are properly constrained
    in the system. Itâ€™s analogous to a roller coaster being confined to its tracks.)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: (** ä»è¿™ä¸ªè§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†å½’ä¸€åŒ–å’ŒçŸ©é˜µä¹˜æ³• *M* è§£é‡Šä¸ºä¸€ç§æŠ•å½±ï¼Œä»¥ä¾¿ä»¤ç‰Œç²’å­åœ¨ç³»ç»Ÿä¸­å¾—åˆ°é€‚å½“çº¦æŸã€‚è¿™ç±»ä¼¼äºè¿‡å±±è½¦è¢«é™åˆ¶åœ¨è½¨é“ä¸Šã€‚)
- en: GPT
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT
- en: 'For (Chat)[GPT](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer)-like
    models, the discussion gets modified. The attention module has an extra casual
    structure â€” that tokens can only get modified by the ones before them. This means
    that the equations are missing a few terms:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç±»ä¼¼ [Chat](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer)
    çš„æ¨¡å‹ï¼Œè®¨è®ºä¼šæœ‰æ‰€ä¿®æ”¹ã€‚æ³¨æ„åŠ›æ¨¡å—å…·æœ‰é¢å¤–çš„å› æœç»“æ„â€”â€”å³ä»¤ç‰Œåªèƒ½è¢«ä¹‹å‰çš„ä»¤ç‰Œä¿®æ”¹ã€‚è¿™æ„å‘³ç€æ–¹ç¨‹ä¸­ç¼ºå°‘ä¸€äº›é¡¹ï¼š
- en: '![](../Images/c4c08ba6336b93785a5854e24b98fd0f.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4c08ba6336b93785a5854e24b98fd0f.png)'
- en: Following our analogy, this means that particles are coming in one at a time,
    and each one would get stuck after going through all the interaction layers. Itâ€™s
    sort of like growing a crystal one-atom at a time.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‰ç…§æˆ‘ä»¬çš„ç±»æ¯”ï¼Œè¿™æ„å‘³ç€ç²’å­ä¸€æ¬¡ä¸€ä¸ªåœ°è¿›å…¥ï¼Œæ¯ä¸ªç²’å­åœ¨ç»è¿‡æ‰€æœ‰çš„ç›¸äº’ä½œç”¨å±‚åä¼šè¢«å¡ä½ã€‚è¿™æœ‰ç‚¹åƒé€ä¸ªåŸå­åœ°ç”Ÿé•¿æ™¶ä½“ã€‚
- en: One thing to keep in mind is that our physics analogy isnâ€™t 100% exact, as fundamental
    features like symmetries and laws like energy/momentum conservations that are
    so ubiquitous in physics do not apply to LLMs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦è®°ä½çš„ä¸€ç‚¹æ˜¯ï¼Œæˆ‘ä»¬çš„ç‰©ç†ç±»æ¯”å¹¶ä¸æ˜¯100%å‡†ç¡®çš„ï¼Œå› ä¸ºç‰©ç†å­¦ä¸­æ™®éå­˜åœ¨çš„å¯¹ç§°æ€§å’Œèƒ½é‡/åŠ¨é‡å®ˆæ’ç­‰åŸºæœ¬ç‰¹æ€§å¹¶ä¸é€‚ç”¨äº LLMã€‚
- en: Emergence in Language Models
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯­è¨€æ¨¡å‹ä¸­çš„æ¶Œç°
- en: '![](../Images/6f6106610e4d44312c28c9e4356f78c4.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f6106610e4d44312c28c9e4356f78c4.png)'
- en: Like a beautiful snowflake, the output of an LLM might hinge on its physics-like
    properties (Photo by [Aaron Burden](https://unsplash.com/@aaronburden?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral))
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: åƒä¸€ç‰‡ç¾ä¸½çš„é›ªèŠ±ä¸€æ ·ï¼ŒLLM çš„è¾“å‡ºå¯èƒ½ä¾èµ–äºå…¶ç±»ä¼¼ç‰©ç†çš„å±æ€§ï¼ˆå›¾ç‰‡æ¥æºï¼š[Aaron Burden](https://unsplash.com/@aaronburden?utm_source=medium&utm_medium=referral)
    åœ¨ [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)ï¼‰
- en: Now that we have our physics analogies, how does it help us understand LLMs?
    The hope is that, like complex physical systems, we can draw analogies from other,
    more familiar and well-understood systems to gain insights into LLMs. However,
    I have to caution the reader that most of our discussions below will be speculative,
    as confirming them would require conducting detailed experimental studies on LLMs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¢ç„¶æˆ‘ä»¬æœ‰äº†ç‰©ç†å­¦çš„ç±»æ¯”ï¼Œå®ƒå¦‚ä½•å¸®åŠ©æˆ‘ä»¬ç†è§£LLMsï¼Ÿå¸Œæœ›çš„æ˜¯ï¼Œåƒå¤æ‚ç‰©ç†ç³»ç»Ÿä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥ä»å…¶ä»–æ›´ç†Ÿæ‚‰ä¸”ç†è§£å¾—æ›´é€å½»çš„ç³»ç»Ÿä¸­è·å¾—å¯¹LLMsçš„æ´è§ã€‚ç„¶è€Œï¼Œæˆ‘å¿…é¡»æé†’è¯»è€…ï¼Œä»¥ä¸‹å¤§éƒ¨åˆ†è®¨è®ºå°†æ˜¯æ¨æµ‹æ€§çš„ï¼Œå› ä¸ºç¡®è®¤è¿™äº›è§‚ç‚¹éœ€è¦å¯¹LLMsè¿›è¡Œè¯¦ç»†çš„å®éªŒç ”ç©¶ã€‚
- en: (* Indeed if I have more resources, Iâ€™d imagine some of these ideas might be
    fruitful academic research projects)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: (* äº‹å®ä¸Šï¼Œå¦‚æœæˆ‘æœ‰æ›´å¤šèµ„æºï¼Œæˆ‘ä¼šæƒ³è±¡è¿™äº›æƒ³æ³•å¯èƒ½ä¼šæˆä¸ºæœ‰æˆæœçš„å­¦æœ¯ç ”ç©¶é¡¹ç›®)
- en: Below is a sampler of how we may use the language of physics to reframe our
    understanding of LLMs.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯å¦‚ä½•åˆ©ç”¨ç‰©ç†å­¦è¯­è¨€é‡æ–°æ¡†å®šæˆ‘ä»¬å¯¹LLMsç†è§£çš„ç¤ºä¾‹ã€‚
- en: LLMs Training
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMsè®­ç»ƒ
- en: Using the language of thermal physics, we can think of LLMs as a tunable physical
    system, and model training is analogous to applying thermal pressure to the system
    to adjust its parameters. This viewpoint was described in my other article, â€œ[The
    Thermodynamics of Machine Learning](https://medium.com/towards-data-science/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1),â€
    so I wonâ€™t delve too much into the details here.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨çƒ­ç‰©ç†å­¦çš„è¯­è¨€ï¼Œæˆ‘ä»¬å¯ä»¥å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è§†ä¸ºä¸€ä¸ªå¯è°ƒèŠ‚çš„ç‰©ç†ç³»ç»Ÿï¼Œæ¨¡å‹è®­ç»ƒç±»ä¼¼äºå¯¹ç³»ç»Ÿæ–½åŠ çƒ­å‹ä»¥è°ƒæ•´å…¶å‚æ•°ã€‚è¿™ä¸€è§‚ç‚¹åœ¨æˆ‘çš„å¦ä¸€ç¯‡æ–‡ç« â€œ[æœºå™¨å­¦ä¹ çš„çƒ­åŠ›å­¦](https://medium.com/towards-data-science/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1)â€ä¸­æœ‰æè¿°ï¼Œå› æ­¤æˆ‘åœ¨è¿™é‡Œä¸ä¼šè¯¦ç»†è®¨è®ºã€‚
- en: Emergence of Intelligence?
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ™ºèƒ½çš„å‡ºç°ï¼Ÿ
- en: While there is plenty of discussion on whether systems like ChatGPT are intelligent
    or not, I will refrain from adding to this controversial topic as I am not even
    sure how one may define intelligence. Nevertheless, it is clear that ChatGPT can
    consistently produce sophisticated and interesting outputs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å…³äºChatGPTæ˜¯å¦æ™ºèƒ½æœ‰å¾ˆå¤šè®¨è®ºï¼Œä½†æˆ‘å°†é¿å…å¯¹æ­¤æœ‰äº‰è®®çš„è¯é¢˜è¿›è¡Œæ›´å¤šæ¢è®¨ï¼Œå› ä¸ºæˆ‘ç”šè‡³ä¸ç¡®å®šå¦‚ä½•å®šä¹‰æ™ºèƒ½ã€‚ç„¶è€Œï¼Œå¾ˆæ˜æ˜¾ï¼ŒChatGPTå¯ä»¥æŒç»­äº§ç”Ÿå¤æ‚ä¸”æœ‰è¶£çš„è¾“å‡ºã€‚
- en: If we subscribe to our physics analogy, this should not be surprising. From
    snowflakes to tornadoes, we know that even simple laws can give rise to highly
    complex behaviors, and from complex behaviors, structures that appear intelligent
    could arise.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æ¥å—ç‰©ç†å­¦ç±»æ¯”ï¼Œè¿™å¹¶ä¸ä»¤äººæƒŠè®¶ã€‚ä»é›ªèŠ±åˆ°é¾™å·é£ï¼Œæˆ‘ä»¬çŸ¥é“å³ä½¿æ˜¯ç®€å•çš„å®šå¾‹ä¹Ÿå¯ä»¥äº§ç”Ÿé«˜åº¦å¤æ‚çš„è¡Œä¸ºï¼Œè€Œä»å¤æ‚è¡Œä¸ºä¸­ï¼Œå¯ä»¥å‡ºç°çœ‹ä¼¼æ™ºèƒ½çš„ç»“æ„ã€‚
- en: 'Complexity as a concept is not easily defined, so in order to proceed further,
    we can try to examine some key features of complex systems: Phase transition is
    one of them.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å¤æ‚æ€§ä½œä¸ºä¸€ä¸ªæ¦‚å¿µå¹¶ä¸å®¹æ˜“å®šä¹‰ï¼Œå› æ­¤ä¸ºäº†è¿›ä¸€æ­¥æ¢è®¨ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•æ£€æŸ¥å¤æ‚ç³»ç»Ÿçš„ä¸€äº›å…³é”®ç‰¹å¾ï¼šç›¸å˜å°±æ˜¯å…¶ä¸­ä¹‹ä¸€ã€‚
- en: Phase Transition
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç›¸å˜
- en: Many complex physical systems possess distinctive phases, each with a highlighted
    set of physical properties. Thus, it is reasonable to suggest that within LLMs,
    there could be distinctive phases as well, with each phase tuned to be helpful
    in specific tasks (such as coding vs proofreading).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šå¤æ‚çš„ç‰©ç†ç³»ç»Ÿå…·æœ‰ç‹¬ç‰¹çš„ç›¸ï¼Œæ¯ä¸ªç›¸éƒ½æœ‰ä¸€ç»„çªå‡ºçš„ç‰©ç†å±æ€§ã€‚å› æ­¤ï¼Œåˆç†çš„çŒœæµ‹æ˜¯ï¼Œåœ¨LLMsä¸­ä¹Ÿå¯èƒ½å­˜åœ¨ç‹¬ç‰¹çš„ç›¸ï¼Œæ¯ä¸ªç›¸éƒ½è¢«è°ƒæ•´ä»¥åœ¨ç‰¹å®šä»»åŠ¡ä¸­ï¼ˆä¾‹å¦‚ç¼–ç ä¸æ ¡å¯¹ï¼‰æä¾›å¸®åŠ©ã€‚
- en: 'How might we verify or refute such a claim? This is where things could get
    interesting. In physics, phases arise when interactions start to form interesting
    structures. Some examples include:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•éªŒè¯æˆ–åé©³è¿™ç§è¯´æ³•ï¼Ÿè¿™å°±æ˜¯äº‹æƒ…å¯èƒ½å˜å¾—æœ‰è¶£çš„åœ°æ–¹ã€‚åœ¨ç‰©ç†å­¦ä¸­ï¼Œç›¸ä½å‡ºç°æ—¶ï¼Œäº¤äº’ä½œç”¨å¼€å§‹å½¢æˆæœ‰è¶£çš„ç»“æ„ã€‚ä¸€äº›ä¾‹å­åŒ…æ‹¬ï¼š
- en: When water cools down, the attractive forces between molecules get stronger,
    causing the molecules to stick together and form solids.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å½“æ°´å†·å´æ—¶ï¼Œåˆ†å­ä¹‹é—´çš„å¸å¼•åŠ›å˜å¾—æ›´å¼ºï¼Œä½¿åˆ†å­ç²˜åœ¨ä¸€èµ·å½¢æˆå›ºä½“ã€‚
- en: When a metal is cooled to extremely low temperatures, electrons may become attracted
    to each other through sound-wave (phonon) interactions, forming Type-I superconductors.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å½“é‡‘å±è¢«å†·å´åˆ°æä½çš„æ¸©åº¦æ—¶ï¼Œç”µå­å¯èƒ½é€šè¿‡å£°æ³¢ï¼ˆå£°å­ï¼‰ç›¸äº’å¸å¼•ï¼Œå½¢æˆIå‹è¶…å¯¼ä½“ã€‚
- en: Perhaps something analogous could occur in LLMs? For instance, in ChatGPT, one
    might speculate that certain groups of tokens from â€œcodeâ€ or â€œproofreadâ€ could
    trigger an avalanche of particular forces that drive specific types of output.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨LLMsä¸­æ˜¯å¦å¯èƒ½å‘ç”Ÿç±»ä¼¼çš„ç°è±¡ï¼Ÿä¾‹å¦‚ï¼Œåœ¨ChatGPTä¸­ï¼Œäººä»¬å¯èƒ½ä¼šæ¨æµ‹â€œä»£ç â€æˆ–â€œæ ¡å¯¹â€ä¸­çš„æŸäº›ä»¤ç‰Œç»„åˆå¯èƒ½ä¼šè§¦å‘ä¸€ç³»åˆ—ç‰¹å®šçš„åŠ›é‡ï¼Œä»è€Œé©±åŠ¨ç‰¹å®šç±»å‹çš„è¾“å‡ºã€‚
- en: Another technical aspect of phase transitions is the modification of symmetries.
    This is related to the creation of structures, such as ice crystal patterns from
    water vapor. While LLMs do not possess physical symmetries, they should contain
    some sort of permutation symmetries of the model weights. This is because model
    performance should be the same as long as they are initialized with the same statistics
    and trained in the same paradigm. The specific values of a particular weight only
    become important during training. This can be thought of as the freezing-in of
    the weights. However, to continue this discussion, we would need to delve into
    the technical subject of [spontaneous symmetry breaking](https://en.wikipedia.org/wiki/Spontaneous_symmetry_breaking),
    which weâ€™ll save for a later date.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸å˜çš„å¦ä¸€ä¸ªæŠ€æœ¯æ–¹é¢æ˜¯å¯¹å¯¹ç§°æ€§çš„ä¿®æ”¹ã€‚è¿™ä¸ç»“æ„çš„åˆ›å»ºæœ‰å…³ï¼Œä¾‹å¦‚ä»æ°´è’¸æ°”ä¸­å½¢æˆçš„å†°æ™¶å›¾æ¡ˆã€‚å°½ç®¡LLMsä¸å…·å¤‡ç‰©ç†å¯¹ç§°æ€§ï¼Œä½†å®ƒä»¬åº”è¯¥åŒ…å«æŸç§æ¨¡å‹æƒé‡çš„æ’åˆ—å¯¹ç§°æ€§ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹æ€§èƒ½åº”è¯¥æ˜¯ç›¸åŒçš„ï¼Œåªè¦å®ƒä»¬ä»¥ç›¸åŒçš„ç»Ÿè®¡æ•°æ®åˆå§‹åŒ–å¹¶åœ¨ç›¸åŒçš„èŒƒå¼ä¸‹è®­ç»ƒã€‚ç‰¹å®šæƒé‡çš„å…·ä½“å€¼åªæœ‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ‰å˜å¾—é‡è¦ã€‚è¿™å¯ä»¥çœ‹ä½œæ˜¯æƒé‡çš„â€œå†»ç»“â€ã€‚ç„¶è€Œï¼Œè¦ç»§ç»­è®¨è®ºè¿™ä¸ªè¯é¢˜ï¼Œæˆ‘ä»¬éœ€è¦æ·±å…¥æ¢è®¨[è‡ªå‘å¯¹ç§°ç ´ç¼º](https://en.wikipedia.org/wiki/Spontaneous_symmetry_breaking)çš„æŠ€æœ¯å†…å®¹ï¼Œæˆ‘ä»¬å°†åœ¨ä»¥åå†è®¨è®ºã€‚
- en: Are LLMs Efficient?
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMsé«˜æ•ˆå—ï¼Ÿ
- en: While there are many criticisms regarding the perceived inefficiency of LLMs
    due to their large number of parameters (especially when compared to physical
    models), these criticisms may not be fully warranted.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æœ‰è®¸å¤šå…³äºLLMsç”±äºå…¶å¤§é‡å‚æ•°è€Œè¢«è®¤ä¸ºæ•ˆç‡ä½ä¸‹çš„æ‰¹è¯„ï¼ˆç‰¹åˆ«æ˜¯ä¸ç‰©ç†æ¨¡å‹ç›¸æ¯”æ—¶ï¼‰ï¼Œè¿™äº›æ‰¹è¯„å¯èƒ½å¹¶ä¸å®Œå…¨æˆç«‹ã€‚
- en: 'Why? It comes down to the technical limitations of our computers, which result
    in significant differences between physics and LLMs:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆï¼Ÿè¿™å½’ç»“äºæˆ‘ä»¬è®¡ç®—æœºçš„æŠ€æœ¯é™åˆ¶ï¼Œè¿™å¯¼è‡´äº†ç‰©ç†å­¦å’ŒLLMsä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼š
- en: Physical laws have infinite precision, while LLMs have finite precision.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç‰©ç†å®šå¾‹å…·æœ‰æ— é™ç²¾åº¦ï¼Œè€ŒLLMså…·æœ‰æœ‰é™ç²¾åº¦ã€‚
- en: Physics exhibits huge hierarchies, with some forces being tiny and others large.
    In LLMs, we attempt to make all outputs/weights similar in size through normalization.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç‰©ç†å­¦è¡¨ç°å‡ºå·¨å¤§çš„å±‚çº§ç»“æ„ï¼Œä¸€äº›åŠ›éå¸¸å¾®å°ï¼Œè€Œå¦ä¸€äº›åŠ›åˆ™å¾ˆå¤§ã€‚åœ¨LLMsä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å½’ä¸€åŒ–å°è¯•ä½¿æ‰€æœ‰è¾“å‡º/æƒé‡å¤§å°ç›¸ä¼¼ã€‚
- en: In physics, tiny effects can accumulate to enormous influences (such as Earthâ€™s
    gravity). In LLMs, these tiny effects are often rounded away and eliminated.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ç‰©ç†å­¦ä¸­ï¼Œå¾®å°çš„æ•ˆåº”å¯ä»¥ç´¯ç§¯æˆå·¨å¤§çš„å½±å“ï¼ˆä¾‹å¦‚åœ°çƒçš„å¼•åŠ›ï¼‰ã€‚åœ¨LLMsä¸­ï¼Œè¿™äº›å¾®å°çš„æ•ˆåº”é€šå¸¸ä¼šè¢«èˆå»å’Œæ¶ˆé™¤ã€‚
- en: Nature is an incredibly efficient computer, with interactions computed instantaneously
    across all scales with infinite precisions. LLMs, on the other hand, are relatively
    slow computers limited by finite precisions.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è‡ªç„¶æ˜¯ä¸€ä¸ªæå…¶é«˜æ•ˆçš„è®¡ç®—æœºï¼Œèƒ½å¤Ÿä»¥æ— é™ç²¾åº¦ç¬æ—¶è®¡ç®—æ‰€æœ‰å°ºåº¦ä¸Šçš„ç›¸äº’ä½œç”¨ã€‚å¦ä¸€æ–¹é¢ï¼ŒLLMsåˆ™æ˜¯å—é™äºæœ‰é™ç²¾åº¦çš„ç›¸å¯¹è¾ƒæ…¢çš„è®¡ç®—æœºã€‚
- en: This means that while we can strive to make LLMs mimic physics better and create
    more powerful models, in practice, computers are simply incapable of fully simulating
    our world (as discussed in â€œ[Why We Donâ€™t Live In a Simulation](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d)â€).
    Therefore, resorting to a large number of parameters may be a last-resort tactic
    to address some of these deficiencies.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€ï¼Œè™½ç„¶æˆ‘ä»¬å¯ä»¥åŠªåŠ›ä½¿LLMsæ›´å¥½åœ°æ¨¡æ‹Ÿç‰©ç†ç°è±¡å¹¶åˆ›å»ºæ›´å¼ºå¤§çš„æ¨¡å‹ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè®¡ç®—æœºæœ¬è´¨ä¸Šæ— æ³•å®Œå…¨æ¨¡æ‹Ÿæˆ‘ä»¬çš„ä¸–ç•Œï¼ˆå¦‚åœ¨â€œ[æˆ‘ä»¬ä¸ºä»€ä¹ˆä¸ç”Ÿæ´»åœ¨æ¨¡æ‹Ÿä¸­](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d)â€ä¸­è®¨è®ºçš„ï¼‰ã€‚å› æ­¤ï¼Œ
    resorting to a large number of parameters å¯èƒ½æ˜¯ä¸€ç§åº”å¯¹è¿™äº›ä¸è¶³çš„æœ€åæ‰‹æ®µã€‚
- en: It is even plausible that given finite precision, there may be an upper limit
    to the complexity achievable with standard computers. This could make it very
    challenging to significantly reduce the number of parameters (although advancements
    in [quantum computing](https://en.wikipedia.org/wiki/Quantum_computing) might
    change this in the future).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿è€ƒè™‘åˆ°æœ‰é™çš„ç²¾åº¦ï¼Œä¹Ÿå¯ä»¥è®¤ä¸ºæ ‡å‡†è®¡ç®—æœºèƒ½å¤Ÿè¾¾åˆ°çš„å¤æ‚åº¦å¯èƒ½å­˜åœ¨ä¸Šé™ã€‚è¿™å¯èƒ½ä¼šä½¿å¾—æ˜¾è‘—å‡å°‘å‚æ•°æ•°é‡å˜å¾—éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ï¼ˆå°½ç®¡[é‡å­è®¡ç®—](https://en.wikipedia.org/wiki/Quantum_computing)çš„è¿›å±•å¯èƒ½ä¼šåœ¨æœªæ¥æ”¹å˜è¿™ç§æƒ…å†µï¼‰ã€‚
- en: Improvements to LLMs
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¹LLMsçš„æ”¹è¿›
- en: 'Could our physics analogy help provide hints for the next generation of LLMs?
    I believe itâ€™s possible. Logically, there are two possible directions to pursue
    based on our beliefs:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç‰©ç†ç±»æ¯”æ˜¯å¦å¯ä»¥ä¸ºä¸‹ä¸€ä»£LLMsæä¾›ä¸€äº›çº¿ç´¢ï¼Ÿæˆ‘è®¤ä¸ºè¿™æ˜¯å¯èƒ½çš„ã€‚ä»é€»è¾‘ä¸Šè®²ï¼Œæ ¹æ®æˆ‘ä»¬çš„ä¿¡å¿µï¼Œæœ‰ä¸¤ä¸ªå¯èƒ½çš„æ–¹å‘å¯ä»¥æ¢ç´¢ï¼š
- en: '**Physics-like features are desirable**: We should draw more inspiration from
    physics to create better model structures.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç‰©ç†ç±»ä¼¼çš„ç‰¹å¾æ˜¯å€¼å¾—è¿½æ±‚çš„**ï¼šæˆ‘ä»¬åº”è¯¥ä»ç‰©ç†å­¦ä¸­è·å¾—æ›´å¤šçµæ„Ÿï¼Œä»¥åˆ›é€ æ›´å¥½çš„æ¨¡å‹ç»“æ„ã€‚'
- en: '**Physics-like features are undesirable**: Physics-like features may actually
    limit the capabilities of LLMs due to inherent computational limits, so we should
    avoid them.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç‰©ç†å­¦ç‰¹å¾æ˜¯ä¸å¯å–çš„**ï¼šç‰©ç†å­¦ç‰¹å¾å¯èƒ½ç”±äºå›ºæœ‰çš„è®¡ç®—é™åˆ¶è€Œé™åˆ¶ LLM çš„èƒ½åŠ›ï¼Œå› æ­¤æˆ‘ä»¬åº”è¯¥é¿å…å®ƒä»¬ã€‚'
- en: Since we are using physics to understand LLMs, Iâ€™ll focus on the first possibility.
    Under this assumption, how could we address the shortcomings of LLMs like ChatGPT?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¢ç„¶æˆ‘ä»¬ä½¿ç”¨ç‰©ç†å­¦æ¥ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œé‚£ä¹ˆæˆ‘å°†é‡ç‚¹å…³æ³¨ç¬¬ä¸€ä¸ªå¯èƒ½æ€§ã€‚åœ¨è¿™ç§å‡è®¾ä¸‹ï¼Œæˆ‘ä»¬å¦‚ä½•è§£å†³ç±»ä¼¼ ChatGPT çš„ LLMs çš„ä¸è¶³ä¹‹å¤„ï¼Ÿ
- en: '**Preserving Hierarchies**: Instead of solely focusing on normalizing weights
    and reducing precision, we should explore alternative approaches to account for
    diverse interactions with different strengths and scales. We could draw inspiration
    from how electromagnetism (which is very strong) and gravity (which is very weak)
    are combined in nature.'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä¿æŒå±‚æ¬¡ç»“æ„**ï¼šæˆ‘ä»¬ä¸åº”ä»…ä»…ä¸“æ³¨äºè§„èŒƒåŒ–æƒé‡å’Œé™ä½ç²¾åº¦ï¼Œè€Œåº”æ¢ç´¢æ›¿ä»£æ–¹æ³•ï¼Œä»¥è€ƒè™‘ä¸åŒå¼ºåº¦å’Œè§„æ¨¡çš„å¤šæ ·åŒ–äº¤äº’ã€‚æˆ‘ä»¬å¯ä»¥å€Ÿé‰´è‡ªç„¶ç•Œä¸­ç”µç£åŠ›ï¼ˆéå¸¸å¼ºï¼‰å’Œé‡åŠ›ï¼ˆéå¸¸å¼±ï¼‰å¦‚ä½•ç»“åˆçš„æ–¹å¼ã€‚'
- en: '**Accommodating Different Phases**: Describing both ice and water using the
    same basic molecular equations is inefficient. Itâ€™s more efficient to use difference
    descriptions for difference phases (say sounds waves vs water waves). We could
    create a better structure that naturally accommodates the macroscopic differences
    within the model.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é€‚åº”ä¸åŒé˜¶æ®µ**ï¼šä½¿ç”¨ç›¸åŒçš„åŸºæœ¬åˆ†å­æ–¹ç¨‹æè¿°å†°å’Œæ°´æ˜¯ä½æ•ˆçš„ã€‚ä½¿ç”¨ä¸åŒçš„æè¿°æ¥åº”å¯¹ä¸åŒçš„é˜¶æ®µï¼ˆä¾‹å¦‚å£°æ³¢ä¸æ°´æ³¢ï¼‰ä¼šæ›´æœ‰æ•ˆã€‚æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ç§æ›´å¥½çš„ç»“æ„ï¼Œèƒ½å¤Ÿè‡ªç„¶åœ°é€‚åº”æ¨¡å‹ä¸­çš„å®è§‚å·®å¼‚ã€‚'
- en: '**Advanced Physics Techniques**: In physics, we donâ€™t study emergent phenomena
    using only fundamental equations. Techniques like [thermodynamics](https://en.wikipedia.org/wiki/Thermodynamics),
    [mean-field theory](https://en.wikipedia.org/wiki/Mean-field_theory) and [renormalization](https://en.wikipedia.org/wiki/Renormalization)
    can be used to help us simplify the problem. Incorporating some of these ideas
    into the building blocks of LLMs could improve their efficiency. For example,
    recent advances on [linear attention](https://arxiv.org/abs/2006.16236) (A. Katharopoulos
    et. al.) may already be interpreted as a sort of mean-field approach.'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é«˜çº§ç‰©ç†å­¦æŠ€æœ¯**ï¼šåœ¨ç‰©ç†å­¦ä¸­ï¼Œæˆ‘ä»¬ä¸ä»…ä»…ä½¿ç”¨åŸºæœ¬æ–¹ç¨‹ç ”ç©¶æ¶Œç°ç°è±¡ã€‚æŠ€æœ¯å¦‚ [çƒ­åŠ›å­¦](https://en.wikipedia.org/wiki/Thermodynamics)ã€[å‡åœºç†è®º](https://en.wikipedia.org/wiki/Mean-field_theory)
    å’Œ [é‡æ•´åŒ–](https://en.wikipedia.org/wiki/Renormalization) å¯ä»¥å¸®åŠ©æˆ‘ä»¬ç®€åŒ–é—®é¢˜ã€‚å°†è¿™äº›æ€æƒ³çš„ä¸€éƒ¨åˆ†èå…¥
    LLM çš„æ„å»ºæ¨¡å—ä¸­ï¼Œå¯èƒ½ä¼šæé«˜å®ƒä»¬çš„æ•ˆç‡ã€‚ä¾‹å¦‚ï¼Œæœ€è¿‘å¯¹ [çº¿æ€§æ³¨æ„åŠ›](https://arxiv.org/abs/2006.16236)ï¼ˆA. Katharopoulos
    ç­‰ï¼‰çš„è¿›å±•ï¼Œå¯èƒ½å·²ç»è¢«è§£é‡Šä¸ºä¸€ç§å‡åœºæ–¹æ³•ã€‚'
- en: By exploring these avenues, we may be able to enhance the capabilities and efficiencies
    of LLMs, leveraging insights from physics to advance the field further.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ¢ç´¢è¿™äº›é€”å¾„ï¼Œæˆ‘ä»¬æˆ–è®¸èƒ½å¤Ÿæå‡ LLM çš„èƒ½åŠ›å’Œæ•ˆç‡ï¼Œåˆ©ç”¨ç‰©ç†å­¦çš„æ´å¯ŸåŠ›è¿›ä¸€æ­¥æ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•ã€‚
- en: Epilogue
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è¯­
- en: In summary, we have showcased how the mathematics of LLMs resemble those in
    physics. This allows us to use our intuitions about everyday physical systems
    to understand these new emergent phenomena, such as ChatGPT. I hope that this
    helps demystify the reasons behind the characteristics of LLMs.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“ä¸€ä¸‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº† LLM çš„æ•°å­¦å¦‚ä½•ä¸ç‰©ç†å­¦ä¸­çš„æ•°å­¦ç›¸ä¼¼ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨å¯¹æ—¥å¸¸ç‰©ç†ç³»ç»Ÿçš„ç›´è§‰æ¥ç†è§£è¿™äº›æ–°å…´ç°è±¡ï¼Œä¾‹å¦‚ ChatGPTã€‚æˆ‘å¸Œæœ›è¿™æœ‰åŠ©äºæ­ç¤º
    LLM ç‰¹å¾èƒŒåçš„åŸå› ã€‚
- en: More generally, I hope I have conveyed to you how physics can provide valuable
    insights into complex subjects like LLMs. I firmly believe that science is most
    effective when we borrow insights from seemingly disparate fields
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´ä¸€èˆ¬åœ°è¯´ï¼Œæˆ‘å¸Œæœ›æˆ‘å·²ç»å‘ä½ ä¼ è¾¾äº†ç‰©ç†å­¦å¦‚ä½•ä¸ºåƒ LLMs è¿™æ ·çš„å¤æ‚ä¸»é¢˜æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚æˆ‘åšä¿¡ï¼Œå½“æˆ‘ä»¬ä»çœ‹ä¼¼ä¸ç›¸å…³çš„é¢†åŸŸä¸­å€Ÿé‰´æ´å¯ŸåŠ›æ—¶ï¼Œç§‘å­¦æœ€ä¸ºæœ‰æ•ˆã€‚
- en: If you enjoyed this article, you might be interested in my other pieces on similar
    topics, such as the link between physics and AI.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œä½ å¯èƒ½ä¼šå¯¹æˆ‘å…³äºç±»ä¼¼ä¸»é¢˜çš„å…¶ä»–æ–‡ç« æ„Ÿå…´è¶£ï¼Œä¾‹å¦‚ç‰©ç†å­¦ä¸äººå·¥æ™ºèƒ½ä¹‹é—´çš„è”ç³»ã€‚
- en: Please leave a comment or provide feedback, as it encourages me to write more
    insightful pieces! ğŸ‘‹
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·ç•™ä¸‹è¯„è®ºæˆ–æä¾›åé¦ˆï¼Œè¿™é¼“åŠ±æˆ‘å†™å‡ºæ›´å¤šæœ‰è§åœ°çš„æ–‡ç« ï¼ğŸ‘‹
- en: '[](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----ea512bcc6a64--------------------------------)
    [## A Physicistâ€™s View of Machine Learning: The Thermodynamics of Machine Learning'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----ea512bcc6a64--------------------------------)
    [## ç‰©ç†å­¦å®¶å¯¹æœºå™¨å­¦ä¹ çš„çœ‹æ³•ï¼šæœºå™¨å­¦ä¹ çš„çƒ­åŠ›å­¦'
- en: Complex systems in nature can be successfully studied using thermodynamics.
    What about Machine Learning?
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è‡ªç„¶ç•Œä¸­çš„å¤æ‚ç³»ç»Ÿå¯ä»¥é€šè¿‡çƒ­åŠ›å­¦è¿›è¡ŒæˆåŠŸç ”ç©¶ã€‚é‚£ä¹ˆï¼Œæœºå™¨å­¦ä¹ å‘¢ï¼Ÿ
- en: towardsdatascience.com](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----ea512bcc6a64--------------------------------)
    [](/the-meaning-behind-logistic-classification-from-physics-291774fda579?source=post_page-----ea512bcc6a64--------------------------------)
    [## The Meaning Behind Logistic Classification, from Physics
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[é€»è¾‘å›å½’èƒŒåçš„æ„ä¹‰ï¼Œä»ç‰©ç†å­¦è§’åº¦çœ‹](/the-meaning-behind-logistic-classification-from-physics-291774fda579?source=post_page-----ea512bcc6a64--------------------------------)
    [## é€»è¾‘å›å½’èƒŒåçš„æ„ä¹‰ï¼Œä»ç‰©ç†å­¦è§’åº¦çœ‹]'
- en: Why do we use the logistic and softmax functions? Thermal physics may have an
    answer.
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆæˆ‘ä»¬ä½¿ç”¨é€»è¾‘å›å½’å’Œsoftmaxå‡½æ•°ï¼Ÿçƒ­ç‰©ç†å­¦æˆ–è®¸èƒ½æä¾›ç­”æ¡ˆã€‚
- en: 'towardsdatascience.com](/the-meaning-behind-logistic-classification-from-physics-291774fda579?source=post_page-----ea512bcc6a64--------------------------------)
    [](/why-causation-is-correlation-a-physicists-perspective-part-1-742696d130e8?source=post_page-----ea512bcc6a64--------------------------------)
    [## Why Causation Is Correlation: A Physicistâ€™s Perspective (Part 1)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[ä¸ºä»€ä¹ˆå› æœå…³ç³»æ˜¯ç›¸å…³æ€§çš„ä½“ç°ï¼šç‰©ç†å­¦å®¶çš„è§†è§’ï¼ˆç¬¬ä¸€éƒ¨åˆ†ï¼‰](/why-causation-is-correlation-a-physicists-perspective-part-1-742696d130e8?source=post_page-----ea512bcc6a64--------------------------------)
    [## ä¸ºä»€ä¹ˆå› æœå…³ç³»æ˜¯ç›¸å…³æ€§çš„ä½“ç°ï¼šç‰©ç†å­¦å®¶çš„è§†è§’ï¼ˆç¬¬ä¸€éƒ¨åˆ†ï¼‰]'
- en: Weâ€™ve all heard the phrase, â€œcorrelation does not imply causationâ€, but no one
    ever talks about what causation reallyâ€¦
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éƒ½å¬è¯´è¿‡â€œç›¸å…³æ€§ä¸ä»£è¡¨å› æœæ€§â€è¿™å¥è¯ï¼Œä½†æ²¡æœ‰äººçœŸæ­£è°ˆè®ºå› æœæ€§åˆ°åº•æ˜¯ä»€ä¹ˆâ€¦
- en: towardsdatascience.com](/why-causation-is-correlation-a-physicists-perspective-part-1-742696d130e8?source=post_page-----ea512bcc6a64--------------------------------)
    [](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----ea512bcc6a64--------------------------------)
    [## Why We Donâ€™t Live in a Simulation
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸ç”Ÿæ´»åœ¨æ¨¡æ‹Ÿä¸­](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----ea512bcc6a64--------------------------------)
    [## ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸ç”Ÿæ´»åœ¨æ¨¡æ‹Ÿä¸­]'
- en: Describing reality as a simulation vastly understates the complexities of our
    world. Hereâ€™s why the simulationâ€¦
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å°†ç°å®æè¿°ä¸ºæ¨¡æ‹Ÿæå¤§åœ°ä½ä¼°äº†æˆ‘ä»¬ä¸–ç•Œçš„å¤æ‚æ€§ã€‚ä»¥ä¸‹æ˜¯ä¸ºä»€ä¹ˆæ¨¡æ‹Ÿâ€¦
- en: medium.com](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----ea512bcc6a64--------------------------------)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----ea512bcc6a64--------------------------------)'
