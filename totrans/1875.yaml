- en: 'Sklearn Tutorial: Module 1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/sklearn-tutorial-module-1-f31b3964a3b4](https://towardsdatascience.com/sklearn-tutorial-module-1-f31b3964a3b4)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**I took the official sklearn MOOC tutorial. Here are my takeaways.**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mocquin.medium.com/?source=post_page-----f31b3964a3b4--------------------------------)[![Yoann
    Mocquin](../Images/b30a0f70c56972aabd2bc0a74baa90bb.png)](https://mocquin.medium.com/?source=post_page-----f31b3964a3b4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f31b3964a3b4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f31b3964a3b4--------------------------------)
    [Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----f31b3964a3b4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f31b3964a3b4--------------------------------)
    ·9 min read·Nov 22, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: After years of playing with the Python scientific stack (NumPy, Matplotlib,
    SciPy, Pandas, and Seaborn), it became obvious to me that the next step was scikit-learn,
    or “sklearn”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8bd6d36b3e9ecfab66a7bc1ed7c91f80.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Thought Catalog](https://unsplash.com/@thoughtcatalog?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: But why sklearn ?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Among the ML libraries, scikit-learn is the de facto simplest and easiest framework
    to learn ML. It is based on the scientific stack (mostly NumPy), focuses on traditional
    yet powerful algorithms like linear regression/support vector machines/dimensionality
    reductions, and provides lots of tools to build around those algorithms (like
    model evaluation and selection, hyperparameter optimization, data preprocessing,
    and feature selection).
  prefs: []
  type: TYPE_NORMAL
- en: '**But its main advantage is, without a doubt, its documentation and user guide.
    You can literally learn almost everything just from the scikit-learn website,
    with lots of examples.**'
  prefs: []
  type: TYPE_NORMAL
- en: Note that other popular frameworks are TensorFlow and PyTorch, but they have
    steeper learning curves and focus on more complex subjects like computer vision
    and neural networks. Since this my first real contact with ML, I figured I’d start
    with sklearn.
  prefs: []
  type: TYPE_NORMAL
- en: I already started reading the documentation a few months ago but was kinda lost
    given its size. While the documentation is huge and very well written, I am not
    sure the best way to learn scikit-learn is to follow through the whole documentation
    one page after another.
  prefs: []
  type: TYPE_NORMAL
- en: The good news, and the thing that triggered my intent to learn scikit-learn
    further, was the start of the “official” MOOC of scikit-learn, created by the
    actual team of scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.fun-mooc.fr/fr/cours/machine-learning-python-scikit-learn/?source=post_page-----f31b3964a3b4--------------------------------)
    [## Machine learning in Python with scikit-learn'
  prefs: []
  type: TYPE_NORMAL
- en: Build predictive models with scikit-learn and gain a practical understanding
    of the strengths and limitations of…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.fun-mooc.fr](https://www.fun-mooc.fr/fr/cours/machine-learning-python-scikit-learn/?source=post_page-----f31b3964a3b4--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**In this series, I will try to summarize what I learned from each of the 6
    modules that compose the MOOC.** This is an excellent exercise for me to practice
    my memory and summarize what I learned, and a good introduction for you if you
    want to get in touch with sklearn.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the MOOC is free, so if you like what you read below, you should definitely
    subscribe! Note that these posts are my curated vision of the MOOC, which is itself
    just an introduction to scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Module 1: Machine Learning Concepts'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This first module focuses on introducing the following notions:'
  prefs: []
  type: TYPE_NORMAL
- en: splitting data into train/test set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: column selector/transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: model, pipeline, and the estimator API with the `.fit()`, `.transform`, `.predict()`,
    `.score()` methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So our program for today is to review those concepts with mostly words and very
    little code. If you want to go further, I strongly recommend heading to the documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting data into train set and test set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most important best practices in ML is to split the data into train
    sets and test sets. The idea is that given a fixed-size input data, we are going
    to train the model with a subpart of the whole data — the train set — and test
    its performance on the other part — the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'This method is very important for many reasons: **the whole point of ML and
    models is to be able to guess the output from new, unseen data**. If we use the
    whole data to train the model, we have no other choice than to use the same data
    to test its performance. Obviously, this seems like a biased exercise: of course,
    the model would be able to guess the outputs based on inputs it has already seen,
    all the more since it also had access to the corresponding outputs. This concept
    is also known as **generalization versus memorization**: we want the model to
    generalize (extrapolate outputs for new input data), not just memorize the data
    it was trained with.'
  prefs: []
  type: TYPE_NORMAL
- en: Another reason (actually another way to put this) is to avoid overfitting. Overfitting
    is a very important concept in ML and will be studied more in another module.
    For now, let’s just say that a model “overfits” when it learns too much, too precisely
    the data it is trained with. Having a test set, different from the train set,
    lets us check that the performance of the model is about the same on the train
    set as on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, a simple yet important function provided by scikit-learn is `train_test_split`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The idea then is to use `data_train` and `target_train` to train our model and
    test its performance on unseen data using `data_test` and `target_test`.
  prefs: []
  type: TYPE_NORMAL
- en: Column transformer/preprocessor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Often, the raw input data is not very well formatted and needs some kind
    of preprocessing steps before actually going into a typical model**. For example,
    if the input data contains a categorical column stored as a string column, and
    the model only uses numerical inputs, we need to convert this string column into
    a numerical column (that encodes the same information) for the model to leverage
    the information of this feature.'
  prefs: []
  type: TYPE_NORMAL
- en: Another typical example is when several numerical features live on very different
    scales and/or units. Models usually benefit from having data with the same scales-
    meaning having more or less the same mean and or variation scale around their
    mean.
  prefs: []
  type: TYPE_NORMAL
- en: Once those preprocessing steps are applied, the transformed data is sent to
    the actual model.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve these preprocessing steps, scikit-learn proposes some useful tools.
    The first are the preprocessing functions (actually stored in classes), that help
    improve the scales of each feature or encode a categorical feature into numerical
    format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once instantiated, those objects can be used to preprocess the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s review the `ColumnTransformer` class: it allows you to specify the
    mapping between some columns with some preprocessors. The basic example is to
    map numerical columns to a standard scaler and map categorical columns to a one-hot
    encoder. So let''s say we have a list of numerical columns and a list of categorical
    columns, we create a new preprocessor object like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This way, `preprocessor` is now a new preprocessor like `StandardScaler` in
    the sense that it has a fit/transform API. We can use this new preprocessor as
    a global preprocessor for our model, that can be applied directly to the full
    data matrix. Note that the `make_column_transformer` does the same thing, without
    having to specify the names for each preprocessor.
  prefs: []
  type: TYPE_NORMAL
- en: 'But what if we don’t know the names of the columns in the data matrix beforehand?
    or what if we don’t want to review all the columns and just make a mapping based
    on their dtypes? For this, we can use the `make_column_selector` helper function,
    that basically creates filters to extract columns for a whole data matrix, to
    map a preprocessor in a `ColumnTransformer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This way, we created pretty much the same `ColumnTransformer` preprocessor object,
    but with no assumption on the columns' names.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We now have all the tools to define a pipeline: **a pipeline is basically the
    concatenation of various processors**. Since we already have some preprocessor,
    we just need to add a predictive model, like a linear regression or support vector
    classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, I’d recommend using the `Pipeline` constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The newly created pipeline object, again, exposes the fit/transform API (like
    the preprocessor we saw above, and also like the LinearRegression() instance).
    We can use this to train and test the performance of our model (the pipeline)
    on our splitted data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final subject I want to introduce for this first module is cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: Remember in the beginning when we split the input data into a train set and
    a test set? Well, the actual split might seem kinda arbitrary. What if, by luck
    or bad luck, the data was split in a particular way that would be advantageous
    or disadvantageous for the performance of the model?
  prefs: []
  type: TYPE_NORMAL
- en: 'To circumvent this risk, we can use what is called **cross-validation: the
    idea is to split the data in different ways and train and test the model for each
    of them. The overall performance of the model is given by the mean of the performance
    of each split**. For example, the first split will use the first 75% entries to
    train and the last 25% to test the model. Starting over, the second split will
    use the 25%-100% entries to train and the first 25% entries to test. And so on.
    For each split, the model is fitted and tested from the beginning.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The output is a dictionary that contains a lot of information, including the
    test-score and model, for each split of the data. Note that there are several
    approaches to split the data, including random split or the most common KFold
    that splits the data into continuous subgroups.
  prefs: []
  type: TYPE_NORMAL
- en: Complete working example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s review all we saw in a full example using the iris toy dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the example below, we create 5 pipelines based on 5 classification type of
    models, namely logistic regression, decision tree, random forest, support vector,
    and K-nearest neighbors. The idea here is not to understand how each of these
    models works, but rather see the overall process of creating a pipeline that include
    preprocessors and models, and how to compute their performance in a robust way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, instead of using the `Pipeline` constructor that can seem a bit verbose,
    we use the helper function `make_pipeline`. Also, instead of just specifying the
    number of splits for the cross-validation, we explicitely specify the kind of
    folds we want to use: here 10-fold randomized splits.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Takeaway
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this first post, we saw :'
  prefs: []
  type: TYPE_NORMAL
- en: what it means to **split the data in a training and test sets**, why we do this,
    and how
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: how to create **column transformer/preprocessor**, that are used to apply transformations
    to the input features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the concept of pipeline, which means to concatenate various steps like preprocessor
    and model, in order to create complex models from basic tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'and finaly what is cross-validation: why and how we must evaluate our model
    performance in a robust way'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, please give this post:'
  prefs: []
  type: TYPE_NORMAL
- en: 1 clap if it was just ok (meh!)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10 claps if you think it was clearly written (noice!)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 50 claps if it was very clear and interesting (daaaamn!)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You might like some of my other posts, make sure to check them out:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Yoann Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----f31b3964a3b4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Scientific/numerical python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://mocquin.medium.com/list/scientificnumerical-python-9ce115122ab6?source=post_page-----f31b3964a3b4--------------------------------)3
    stories![Ironicaly, an array of containers](../Images/4ecd0326a3efdda93947f60872018d41.png)![](../Images/f11076a724463f7b11d819d95bcf0ea4.png)![](../Images/e340b22f444d2bd311537341cf1a105a.png)![Yoann
    Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----f31b3964a3b4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Data science and Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://mocquin.medium.com/list/data-science-and-machine-learning-ba3fb2206051?source=post_page-----f31b3964a3b4--------------------------------)3
    stories![](../Images/c078e74fd67e0141c2b54b82823c78d4.png)![](../Images/79988eda04a078da9373f03d7db51c51.png)![](../Images/6a5966e529bf4ba9b16c592fec7b591a.png)![Yoann
    Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----f31b3964a3b4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Fourier-transforms for time-series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://mocquin.medium.com/list/fouriertransforms-for-timeseries-ed423e3f38ad?source=post_page-----f31b3964a3b4--------------------------------)4
    stories![](../Images/86efd63d329650eb9b6d7c33625d6884.png)![](../Images/c693e4e596df5c1a8ef1b0fb3777d7ac.png)![](../Images/b6bc5330fb2d92bc3aad36f5bbc950da.png)'
  prefs: []
  type: TYPE_NORMAL
