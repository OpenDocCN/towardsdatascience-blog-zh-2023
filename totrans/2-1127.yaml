- en: How to Choose the Best Evaluation Metric for Classification Problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-choose-the-best-evaluation-metric-for-classification-problems-638e845da334](https://towardsdatascience.com/how-to-choose-the-best-evaluation-metric-for-classification-problems-638e845da334)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A comprehensive guide covering the most commonly used evaluation metrics for
    supervised classification and their utility in different scenarios
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://thomasdorfer.medium.com/?source=post_page-----638e845da334--------------------------------)[![Thomas
    A Dorfer](../Images/9258a1735cee805f1d9b02e2adf01096.png)](https://thomasdorfer.medium.com/?source=post_page-----638e845da334--------------------------------)[](https://towardsdatascience.com/?source=post_page-----638e845da334--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----638e845da334--------------------------------)
    [Thomas A Dorfer](https://thomasdorfer.medium.com/?source=post_page-----638e845da334--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----638e845da334--------------------------------)
    ·9 min read·Apr 17, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c6a92a3e2d07ecf632a0435a02146a6d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author.
  prefs: []
  type: TYPE_NORMAL
- en: In order to properly evaluate a classification model, it is important to carefully
    consider which evaluation metric is the most suitable.
  prefs: []
  type: TYPE_NORMAL
- en: This article will cover the most commonly used evaluation metrics for classification
    tasks, including relevant example cases, and will provide you with the information
    necessary to help you choose among them.
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A classification problem is characterized by the prediction of the category
    or class of a given observation based on its corresponding features. The choice
    of the most appropriate evaluation metric will depend on the aspects of model
    performance the user would like to optimize.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a prediction model aiming to diagnose a particular disease. If this
    model fails to detect the disease, it can lead to serious consequences, such as
    delayed treatment and patient harm. On the other hand, if the model falsely diagnoses
    a healthy patient, that can also result in costly consequences by subjecting a
    healthy patient to unnecessary tests and treatments.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, the decision on which error to minimize will depend on the particular
    use case and the costs associated with it. Let’s go through some of the most commonly
    used metrics to shed some more light on this.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When the classes in a dataset are balanced—meaning if there is roughly an equal
    number of samples in each class — accuracy can serve as a simple and intuitive
    metric to evaluate a model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: In simple terms, **accuracy** measures the proportion of correct predictions
    made by the model.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'To illustrate this, let’s have a look at the following table, showing both
    actual and predicted classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c70ffa6575d1b09fd17e7c862da1b11.png)'
  prefs: []
  type: TYPE_IMG
- en: Columns shaded in green indicate correct predictions. Table by the Author.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we have a total of 10 samples, of which 6 have been predicted
    correctly (green shading).
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, our accuracy can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a01277b7429e87ede8b69763a0afd30e.png)'
  prefs: []
  type: TYPE_IMG
- en: In order to prepare ourselves for what’s about to come with the metrics below,
    it is worth noting that *correct predictions* are the sum of *true positives*
    and *true negatives*.
  prefs: []
  type: TYPE_NORMAL
- en: A **true positive (TP)** occurs when the model correctly predicts the positive
    class.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A **true negative (TN)** occurs when the model correctly predicts the negative
    class.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In our example, a true positive is an outcome where both actual and predicted
    classes are 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e005e302187378cf1ce6a813c9c1911e.png)'
  prefs: []
  type: TYPE_IMG
- en: Columns shaded in green indicate true positives. Table by the Author.
  prefs: []
  type: TYPE_NORMAL
- en: Likewise, a true negative occurs when both actual and predicted classes are
    0.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3814599cd210b37642bc1f601488daa0.png)'
  prefs: []
  type: TYPE_IMG
- en: Columns shaded in green indicate true negatives. Table by the Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, you may occasionally see the formula for accuracy being written
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d49ce17a8d2df106c52432843f682fb7.png)'
  prefs: []
  type: TYPE_IMG
- en: '***Example:*** Face detection. In order to detect the absence or presence of
    a face in an image, accuracy can be a suitable metric as the cost of a false positive
    (identifying a non-face as a face) or a false negative (failing to identify a
    face) is approximately equal. *Note:* the distribution of the class labels in
    the dataset should be balanced in order for accuracy to be an appropriate measure.'
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The precision metric is suitable for measuring the proportion of correct positive
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, **precision** provides a measure of the model’s ability to correctly
    identify true positive samples.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As a result, it is often used when the goal is to minimize false positives,
    as is the case in domains like credit card fraud detection or disease diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: A **false positive (FP)** occurs when the model incorrectly predicts the positive
    class, indicating that a given condition exists when in reality it does not.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In our example, a false positive is an outcome where the predicted class should
    have been 0, but was actually 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9faf63ad891151da3fd7683a3bed65b6.png)'
  prefs: []
  type: TYPE_IMG
- en: Columns shaded in red indicate false positives. Table by the Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since precision measures the proportion of positive predictions that are actually
    true positives, it is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5b14eb56c075ada4798df1c93cabc7d0.png)'
  prefs: []
  type: TYPE_IMG
- en: '***Example:*** Anomaly detection. In fraud detection, for instance, precision
    can be a suitable evaluation metric, particularly when the cost of false positives
    is high. Identifying non-fraudulent activities as fraudulent can lead not only
    to additional costs for investigation expenses, but also to high levels of customer
    dissatisfaction and increased churn rates.'
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When the goal of a prediction task is to minimize false negatives, recall serves
    as an appropriate evaluation metric.
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall** measures the proportion of true positives that are correctly identified
    by the model.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is particularly useful in situations where false negatives are more costly
    than false positives.
  prefs: []
  type: TYPE_NORMAL
- en: A **false negative (FN)** occurs when the model incorrectly predicts the negative
    class, indicating that a given condition is absent when in fact it is present.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In our example, a false negative is an outcome where the predicted class should
    have been 1, but was actually 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7330595e075597530bbf669ac9926949.png)'
  prefs: []
  type: TYPE_IMG
- en: Columns in red indicate false negatives. Table by the Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91a0b0a0437343ea9a6ab9356fa0cf7d.png)'
  prefs: []
  type: TYPE_IMG
- en: '***Example:*** Disease diagnosis. In COVID-19 testing, for instance, recall
    is a good choice when the goal is to detect as many positive cases as possible.
    In this case, a higher number of false positives is tolerated since the priority
    is to minimize false negatives in order to prevent the spread of the disease.
    Arguably, the cost of missing a positive case is much higher than misclassifying
    a negative case as positive.'
  prefs: []
  type: TYPE_NORMAL
- en: F1 Score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In cases when both false positives and false negatives are important aspects
    to consider, such as in spam detection, the F1 score comes in as a handy metric.
  prefs: []
  type: TYPE_NORMAL
- en: The **F1 score** is the harmonic mean of precision and recall and provides a
    balanced measure of the model’s performance by taking into account both false
    positives and false negatives.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'It is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/711f3107f60623ae534fa8212966bf4f.png)'
  prefs: []
  type: TYPE_IMG
- en: '***Example:*** Document classification.In spam detection, for instance, the
    F1 score is an appropriate evaluation metric, as the goal is to strike a balance
    between precision and recall. A spam email classifier should correctly classify
    as many spam emails as possible (recall), whilst also avoid the incorrect classification
    of legitimate emails as spam (precision).'
  prefs: []
  type: TYPE_NORMAL
- en: Area under the ROC curve (AUC)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The receiver operating characteristic curve, or [ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic),
    is a graph that illustrates the performance of a binary classifier at all classification
    thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: The area under the ROC curve, or **AUC**, measures how well a binary classifier
    can tell apart positive and negative classes across different thresholds.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is a particularly useful metric when the cost of false positives and false
    negatives is different. This is because it considers the trade-off between true
    positive rate (sensitivity) and false positive rate (1-specificity) at different
    thresholds. By adjusting the threshold, we can get a classifier that prioritizes
    either sensitivity or specificity, depending on the cost of false positives and
    false negatives of a specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: The **true positive rate (TPR)**, or **sensitivity**, measures the proportion
    of actual positive cases that are correctly identified by the model. It is exactly
    the same as recall.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'It is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ca469388208b10fe61e2441ea3fb123.png)'
  prefs: []
  type: TYPE_IMG
- en: The **false positive rate (FPR)**, or **1-specificity**, measures the proportion
    of actual negative cases that are incorrectly classified as positive by the model.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'It is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/438366a0d2b086ce9b7c798eaa950304.png)'
  prefs: []
  type: TYPE_IMG
- en: By varying the classification threshold from 0 to 1, and calculating TPR and
    FPR for each of these thresholds, a ROC curve and corresponding AUC value can
    be produced. The diagonal line represents the performance of a random classifier
    — that is, a classifier that makes random guesses about the class label of each
    sample.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0aa2053330b313a4b0eb751b23d7af0d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author.
  prefs: []
  type: TYPE_NORMAL
- en: The closer the ROC curve is to the top left corner, the better the performance
    of the classifier. A corresponding AUC of 1 indicates perfect classification,
    whereas an AUC of 0.5 indicates random classification performance.
  prefs: []
  type: TYPE_NORMAL
- en: '***Example:*** Ranking problems. When the task is to rank samples by their
    likelihood of being in one class or another, AUC is a suitable metric as it reflects
    the model’s ability to correctly rank samples rather than just classify them.
    For instance, it can be used in online advertising, as it would evaluate the model’s
    ability to correctly rank users by their likelihood of clicking on an ad, rather
    than just predicting a binary click/no-click outcome.'
  prefs: []
  type: TYPE_NORMAL
- en: Log Loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logarithmic loss, also known as log loss or cross-entropy loss, is a useful
    evaluation metric for classification problems where probabilistic estimates are
    important.
  prefs: []
  type: TYPE_NORMAL
- en: The **log loss** measures the difference between the predicted probabilities
    of the classes and the actual class labels.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is a particularly useful metric when the goal is to penalize the model for
    being overly confident about predicting the wrong class. The metric is also used
    as a loss function in the training of logistic regressors and neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a single sample, whereby *y* denotes the true label and *p* denotes the
    probability estimate, the log loss is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f4fea9355413799f1d8e8822cb14c60.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When the true label is 1, the log loss as a function of predicted probabilities
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8f7bf57f84d7d980d0905a47e718cff2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author.
  prefs: []
  type: TYPE_NORMAL
- en: It can be clearly seen that the log loss gets smaller the more certain the classifier
    is about the correct label being 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'The log loss can also be generalized to multi-class classification problems.
    For a single sample, where *k* denotes the class label and *K* corresponds to
    the total number of classes, it can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/523414c7cddf07dc23e0286461a1c7bd.png)'
  prefs: []
  type: TYPE_IMG
- en: In both binary and multi-class classification, the log loss is a helpful measure
    that determines how well the predicted probabilities match the true class labels.
  prefs: []
  type: TYPE_NORMAL
- en: '***Example:*** Credit risk assessment. For instance, the log loss can be used
    to evaluate the performance of a credit risk model that predicts how likely a
    borrower is to default on a loan. The cost of a false negative (predicting a reliable
    borrower as unreliable) could be much higher than that of a false positive (predicting
    an unreliable borrower as reliable). Thus, minimizing the log loss can help minimize
    the financial risk of lending in this scenario.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to accurately assess the performance of a classifier and to make informed
    decisions based on its predictions, it is crucial to choose an appropriate evaluation
    metric. In most situations, this choice will highly depend on the specific problem
    at hand. Important factors to consider are the balance of the classes in a dataset,
    whether it’s more important to minimize false positives, false negatives, or both,
    and the significance of ranking and probabilistic estimates.
  prefs: []
  type: TYPE_NORMAL
- en: Liked this article?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s connect! You can find me on [Twitter](https://twitter.com/ThomasADorfer),
    [LinkedIn](https://www.linkedin.com/in/thomasdorfer/) and [Substack](https://thomasdorfer.substack.com/).
  prefs: []
  type: TYPE_NORMAL
- en: If you like to support my writing, you can do so through a [Medium Membership](https://thomasdorfer.medium.com/membership),
    which provides you access to all my stories as well as those of thousands of other
    writers on Medium.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@thomasdorfer/membership?source=post_page-----638e845da334--------------------------------)
    [## Join Medium with my referral link - Thomas A Dorfer'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Thomas A Dorfer (and thousands of other writers on Medium).
    Your membership fee directly supports…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@thomasdorfer/membership?source=post_page-----638e845da334--------------------------------)
  prefs: []
  type: TYPE_NORMAL
