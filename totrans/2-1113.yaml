- en: How to Become a Data Engineer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何成为数据工程师
- en: 原文：[https://towardsdatascience.com/how-to-become-a-data-engineer-c0319cb226c2](https://towardsdatascience.com/how-to-become-a-data-engineer-c0319cb226c2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-become-a-data-engineer-c0319cb226c2](https://towardsdatascience.com/how-to-become-a-data-engineer-c0319cb226c2)
- en: A shortcut for beginners in 2024
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2024年初学者的捷径
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----c0319cb226c2--------------------------------)[![💡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----c0319cb226c2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c0319cb226c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c0319cb226c2--------------------------------)
    [💡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----c0319cb226c2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mshakhomirov.medium.com/?source=post_page-----c0319cb226c2--------------------------------)[![💡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----c0319cb226c2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c0319cb226c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c0319cb226c2--------------------------------)
    [💡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----c0319cb226c2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c0319cb226c2--------------------------------)
    ·17 min read·Oct 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布在[数据科学前沿](https://towardsdatascience.com/?source=post_page-----c0319cb226c2--------------------------------)
    ·17分钟阅读·2023年10月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b49c1deff780a74750c4dfda9056b149.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b49c1deff780a74750c4dfda9056b149.png)'
- en: Photo by [Gabriel Vasiliu](https://unsplash.com/@gabimedia?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Gabriel Vasiliu](https://unsplash.com/@gabimedia?utm_source=medium&utm_medium=referral)提供，[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)上发布
- en: This story explains an accelerated way of getting into the data engineering
    role by learning the required skills and familiarising yourself with data engineering
    tools and techniques. It will be useful for beginner-level IT practitioners and
    intermediate software engineers who would like to make a career change. Through
    my years as a Head of Data Engineering for one of the most successful start-ups
    in the UK and mid-east, I learned a lot from my career and I would like to share
    this knowledge and experience with you. This is a reflection of my personal experience
    in the data engineering field I acquired over the last 12 years. I hope it will
    be useful for you.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这个故事解释了一种加速进入数据工程角色的方法，通过学习所需的技能并熟悉数据工程工具和技术。这对初级IT从业者和希望转行的中级软件工程师将非常有用。通过我作为英国和中东地区最成功初创公司的数据工程主管的多年经验，我从职业生涯中学到了很多，我希望与您分享这些知识和经验。这是我在数据工程领域获得的个人经验的反映。我希望这对您有用。
- en: Data engineer — the role
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据工程师——角色
- en: First and foremost, why data engineer?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为什么选择数据工程师？
- en: Data engineering is an exciting and very rewarding field. It’s a fascinating
    job where we have a chance to work with everything that touches data — APIs, data
    connectors, data platforms, business intelligence and dozens of data tools available
    in the market. Data engineering is closely connected with Machine learning (ML).
    You will create and deploy all sorts of data and ML pipelines.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程是一个令人兴奋且非常有回报的领域。这是一份迷人的工作，我们有机会处理所有与数据相关的事物——API、数据连接器、数据平台、商业智能以及市场上数十种数据工具。数据工程与机器学习（ML）紧密相关。你将创建和部署各种数据和ML管道。
- en: It definitely won’t be boring and it pays well.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这份工作绝对不会无聊，并且薪资优厚。
- en: It pays well because it’s not easy to build a good data platform. It starts
    with requirements gathering and design and requires considerable experience. It’s
    not an easy task and requires some really good programming skills as well. The
    job itself is secure because as long businesses generate data this job will be
    in high demand.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这份工作回报丰厚，因为建立一个良好的数据平台并不容易。它从需求收集和设计开始，需要相当的经验。这不是一项简单的任务，也需要一些真正出色的编程技能。工作本身是安全的，因为只要企业产生数据，这份工作就会有很高的需求。
- en: The companies will always hire someone who knows how to process (ETL) data efficiently.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 公司总是会聘用那些知道如何高效处理（ETL）数据的人。
- en: Data engineering has been one of the fastest-growing careers in the UK over
    the last five years, ranking 13 on LinkedIn’s list of the most in-demand jobs
    in 2023 [1]. The other reason to join is the scarcity. In IT space it is incredibly
    difficult to find a good data engineer these days.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程在过去五年里成为了英国增长最快的职业之一，在2023年LinkedIn的最受欢迎职业榜单中排名第13 [1]。另一个加入的理由是稀缺性。在IT领域，如今找到一个优秀的数据工程师是非常困难的。
- en: As a “Head of Data Engineering” I receive 4 job interview invites on LinkedIn
    each week. On average. Entry level data engineering roles are in higher demand.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 作为“数据工程负责人”，我每周在LinkedIn上收到4个职位面试邀请。平均而言，初级数据工程角色的需求更高。
- en: 'According to tech job research conducted by DICE Data Engineer is the fastest
    growing tech occupation:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 根据DICE的技术职位研究，数据工程师是增长最快的技术职业：
- en: '![](../Images/ee08b504be177749be9112bfa0f2dd1c.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee08b504be177749be9112bfa0f2dd1c.png)'
- en: 'Source: [DICE](http://marketing.dice.com/pdf/2020/Dice_2020_Tech_Job_Report.pdf)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[DICE](http://marketing.dice.com/pdf/2020/Dice_2020_Tech_Job_Report.pdf)
- en: Modern data stack
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现代数据栈
- en: Modern data stack refers to a collection of data processing tools and data platform
    types.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现代数据栈指的是一系列数据处理工具和数据平台类型。
- en: Are you in the space?
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你在这个领域吗？
- en: “Are you in the space?” — This is the question I was asked during one of my
    job interviews. You would want to be able to answer this one, and be aware of
    the news, IPOs, recent developments, breakthroughs, tools and techniques.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: “你在这个领域吗？”——这是我在一次面试中被问到的问题。你会希望能够回答这个问题，并了解相关新闻、首次公开募股、最近的发展、突破、工具和技术。
- en: 'Familiarise yourself with common data platform architecture types, i.e. data
    lake, lakehouse, data warehouse and be ready to answer which tools they use. Check
    this article for some examples:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉常见的数据平台架构类型，即数据湖、湖仓、数据仓库，并准备好回答它们使用哪些工具。查看这篇文章了解一些示例：
- en: '[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----c0319cb226c2--------------------------------)
    [## Data Platform Architecture Types'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 数据平台架构类型](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----c0319cb226c2--------------------------------)'
- en: How well does it answer your business needs? Dilemma of a choice.
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它在多大程度上满足了你的业务需求？选择的困境。
- en: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----c0319cb226c2--------------------------------)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 数据平台架构类型](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----c0319cb226c2--------------------------------)'
- en: Data pipelines
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据管道
- en: As a data engineer, you will be tasked with data pipeline design almost every
    day. You would want to familiarise yourself with data pipeline design patterns
    and be able to explain when to use them. It is crucial to apply this knowledge
    in practice as it defines which tools to use. The right set of data transformation
    tools can make the data pipeline extremely efficient.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名数据工程师，你几乎每天都会被要求进行数据管道设计。你会希望熟悉数据管道设计模式，并能够解释何时使用它们。将这些知识应用于实践是至关重要的，因为它决定了使用哪些工具。正确的数据转换工具组合可以使数据管道极其高效。
- en: 'So, we need to know exactly when to apply streaming data processing and when
    to apply batch. One can be very expensive and the other one can save thousands.
    However, business requirements might be different in each case. This article has
    a comprehensive list of data pipeline design patterns:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要准确知道何时应用流数据处理，何时应用批处理。一种可能非常昂贵，而另一种则可能节省数千。然而，业务需求可能在每种情况下都不同。本文提供了全面的数据管道设计模式列表：
- en: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----c0319cb226c2--------------------------------)
    [## Data pipeline design patterns'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 数据管道设计模式](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----c0319cb226c2--------------------------------)'
- en: Choosing the right architecture with examples
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择合适的架构及示例
- en: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----c0319cb226c2--------------------------------)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 数据管道设计模式](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----c0319cb226c2--------------------------------)'
- en: Data modelling
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据建模
- en: I would say data modelling is an essential part of data engineering. Many data
    platforms are designed in a way that data is being loaded into the data warehouse
    solution “as is”. This is called the ELT approach. Data engineers are tasked to
    create data transformation pipelines using standard SQL dialect very often. Good
    SQL skills are a must. Indeed, SQL is natural for analytics querying and pretty
    much is a standard this day. It helps to query data efficiently and enables all
    business stakeholders with the power of analytics easily.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为数据建模是数据工程的一个关键部分。许多数据平台的设计方式是将数据“原样”加载到数据仓库解决方案中。这被称为 ELT 方法。数据工程师经常需要使用标准
    SQL 方言创建数据转换管道。良好的 SQL 技能是必不可少的。确实，SQL 对于分析查询是自然的，现如今几乎已经成为标准。它有助于高效查询数据，并让所有业务利益相关者轻松使用分析功能。
- en: 'Data engineers must know how to **cleanse, enrich and update datasets.** For
    example, using MERGE to perform incremental updates. Run this SQL in your workbench
    or data warehouse (DWH). It explains how it worls:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师必须知道如何**清洗、丰富和更新数据集**。例如，使用 MERGE 执行增量更新。在你的工作台或数据仓库（DWH）中运行此 SQL。它解释了它的工作原理：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Some advanced SQL hints and tricks can be found here:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一些高级 SQL 提示和技巧可以在这里找到：
- en: '[](/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----c0319cb226c2--------------------------------)
    [## Advanced SQL techniques for beginners'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 面向初学者的高级 SQL 技巧](https://towardsdatascience.com/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----c0319cb226c2--------------------------------)'
- en: On a scale from 1 to 10 how good are your data warehousing skills?
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 1 到 10 的尺度上，你的数据仓库技能有多好？
- en: towardsdatascience.com](/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----c0319cb226c2--------------------------------)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----c0319cb226c2--------------------------------)'
- en: Coding
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码
- en: This is really important as data engineering is not only about data modelling
    and SQL. Consider data engineers as software engineers instead. They must have
    good knowledge of ETL/ELT techniques and also must be able to code at least in
    Python. Yes, it is obvious that Python is no doubt, the most convenient programming
    language for data engineering but everything you can do with Python can be easily
    done with any other language, i.e. Java Script or Java. Don’t limit yourself,
    you’ll have time to learn any language your company chose as the main one for
    their stack.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常重要，因为数据工程不仅仅涉及数据建模和 SQL。可以将数据工程师视为软件工程师。他们必须具备 ETL/ELT 技术的良好知识，并且至少能够使用 Python
    编码。是的，显然 Python 无疑是数据工程最方便的编程语言，但你用 Python 可以做的任何事情，都可以用其他语言轻松完成，比如 JavaScript
    或 Java。不要限制自己，你会有时间学习公司选择作为其技术栈主要语言的任何语言。
- en: I would recommend to start with **data APIs and requests**. Combining this knowledge
    with Cloud services gives us a very good foundation for any ETL processes we might
    need in the future.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议从**数据 API 和请求**开始。将这些知识与云服务结合，为未来可能需要的任何 ETL 过程提供了一个很好的基础。
- en: We can’t know everything and it’s not required to be a coding guru but we must
    know how to process data.
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们不能知道一切，也不要求成为编码专家，但我们必须知道如何处理数据。
- en: 'Consider this example of loading data into BigQuery data warehouse. It will
    use BigQuery client libraries [5] to insert rows into a table:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以将数据加载到 BigQuery 数据仓库为例。它将使用 BigQuery 客户端库 [5] 将行插入到表中：
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We can’t know everything and it’s not required to be a coding guru but we must
    know how to process data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能知道一切，也不要求成为编码专家，但我们必须知道如何处理数据。
- en: We can run it locally or deploy it in the cloud as a serverless application.
    It can be triggered by any other service we choose. For example, deploying an
    AWS Lambda or GCP Cloud Function can be very efficient. It will process our data
    pipeline events with ease and almost at zero cost. There is plenty of articles
    in my blog explaining how easy and flexible it can be.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在本地运行它，也可以将其部署到云端作为无服务器应用程序。它可以由我们选择的任何其他服务触发。例如，部署 AWS Lambda 或 GCP Cloud
    Function 会非常高效。它将轻松处理我们的数据管道事件，几乎不产生成本。我的博客中有很多文章解释了它的简便性和灵活性。
- en: Airflow, Airbyte, Luidgi, Hudi…
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Airflow、Airbyte、Luigi、Hudi……
- en: Play with 3rd party frameworks and libraries that help to manage data platforms
    and orchestrate data pipelines. Many of them are open-source such as Apache Hudi
    [6] and help to understand what is data platform management from different angles.
    Many of them are really good at managing batch and streaming workloads. I learned
    a lot simply by using them. Apache Airflow, for example, offers a lot of ready
    data connectors. We can use them to run our ETL tasks with ease with any cloud
    vendor (AWS, GCP, Azure).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用帮助管理数据平台和编排数据管道的第三方框架和库进行实验。许多框架是开源的，如Apache Hudi [6]，并且从不同角度帮助理解数据平台管理。它们中的许多在管理批处理和流处理工作负载方面表现出色。我通过使用它们学到了很多东西。例如，Apache
    Airflow 提供了很多现成的数据连接器。我们可以使用它们轻松地运行任何云供应商（AWS、GCP、Azure）的ETL任务。
- en: It’s very easy to create batch data processing jobs using these frameworks.
    If we take a look under the hood it definitely makes things much clearer in terms
    of what the actual ETL is.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些框架创建批处理数据处理作业非常容易。如果我们深入了解，它确实可以使ETL的实际操作更加清晰。
- en: 'Consider this example of ML pipeline I built using airflow connectors to train
    the recommendation engine:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我使用 airflow 连接器构建了一个机器学习管道来训练推荐引擎：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It will create a simple data pipeline graph to export data into cloud storage
    bucket and then train the ML model using MLEngineTrainingOperator.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 它将创建一个简单的数据管道图，将数据导出到云存储桶中，然后使用MLEngineTrainingOperator训练ML模型。
- en: '![](../Images/e316a43ba542db4191a0a78ccfa00721.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e316a43ba542db4191a0a78ccfa00721.png)'
- en: ML model training using Airflow. Image by author.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Airflow进行ML模型训练。图片由作者提供。
- en: Orchestrate data pipelines
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编排数据管道
- en: Frameworks are great but data engineers must know how to create their own frameworks
    to orchestrate data pipelines. This brings us back to raw vanilla coding and working
    with client libraries and APIs. A good advice here will be to familiarise yourself
    with data tools and their API endpoints. Very often it is much more intuitive
    and easier to create and deploy our own microservice to perform ETL/ELT jobs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 框架非常重要，但数据工程师必须知道如何创建自己的框架来编排数据管道。这将我们带回到原始的基础编码以及使用客户端库和API。在这里，一个好的建议是熟悉数据工具及其API端点。通常，创建并部署我们自己的微服务来执行ETL/ELT任务更加直观和容易。
- en: Orchestrate data pipelines with your own tools
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用你自己的工具编排数据管道
- en: For example, we can create a simple serverless application that will consume
    data from the message broker, such as SNS. Then it can process these events and
    orchestrate other microservices we create to perform ETL tasks. Another example
    is a simple AWS Lambda that is being triggered by new files created in the data
    lake, then based on the information it reads from the pipeline configuration file
    it can decide which service to invoke or which table to load data into.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以创建一个简单的无服务器应用程序，它将从消息代理（如SNS）中获取数据。然后，它可以处理这些事件并编排我们创建的其他微服务来执行ETL任务。另一个例子是一个简单的AWS
    Lambda，它由数据湖中创建的新文件触发，然后根据它从管道配置文件中读取的信息，它可以决定调用哪个服务或将数据加载到哪个表中。
- en: Consider this application below. It’s a very simple AWS Lambda that can be run
    locally or when it’s deployed in the cloud.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑下面的这个应用程序。它是一个非常简单的AWS Lambda，可以在本地运行或在云中部署时运行。
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`app.py` can be anything with ETL, we just need to add some logic like we did
    with data loading into BigQuery in the previous example using a couple of client
    libraries:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`app.py` 可以是任何ETL任务，我们只需添加一些逻辑，就像在前面的示例中使用几个客户端库将数据加载到BigQuery中一样：'
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'No we can run it locally or deploy using infrastructure as code. This command
    line script will run this service locally:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在本地运行它或使用基础设施即代码进行部署。这个命令行脚本将在本地运行此服务：
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Alternatively it can be deployed in the cloud and we can invoke it from there.
    This brings us into the Cloud.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，它也可以部署到云中，我们可以从那里调用它。这将我们带入了云环境。
- en: Cloud services providers
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云服务提供商
- en: Everything is managed in the cloud these days. That’s why learning at least
    one vendor is crucial. It can be AWS, GCP or Azure. They are the leaders and we
    would want to focus on one of them. It will be ideal to get a Cloud certification,
    such as “[Google Cloud Professional Data Engineer](https://cloud.google.com/learn/certification/data-engineer)”
    [7] or similar. These exams are difficult but it is worth getting one as it gives
    a good overview of data processing tools and makes us look very credible. I did
    one, have written down my experience in an article and you can find it in my stories.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切都在云端管理。这就是为什么至少学习一种供应商的服务至关重要。可以是AWS、GCP或Azure。它们是领导者，我们希望专注于其中之一。获得云认证是理想的，比如“[Google
    Cloud Professional Data Engineer](https://cloud.google.com/learn/certification/data-engineer)”
    [7] 或类似的认证。这些考试很难，但值得获得，因为它提供了数据处理工具的良好概述，并使我们看起来非常可信。我考过一次，已在文章中记录了我的经验，你可以在我的故事中找到。
- en: 'Everything data engineers create with cloud functions and/or docker can be
    deployed in the cloud. Consider this AWS CloudFormation stack template below.
    We can use it to deploy our simple ETL microservice:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师使用云功能和/或docker创建的所有内容都可以部署在云中。考虑以下AWS CloudFormation堆栈模板。我们可以使用它来部署我们的简单ETL微服务：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If we run this shell script in our command line it will deploy our service
    and all required resources such as IAM policies, in the cloud:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在命令行中运行这个shell脚本，它将部署我们的服务和所有所需的资源，如IAM策略到云中：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After that we can invoke our service using SDK by running this CLI command
    from our command line tool:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可以使用SDK通过在命令行工具中运行此CLI命令来调用我们的服务：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Master command line tools
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精通命令行工具
- en: Command line tools from Cloud vendors are very useful and help to create scripts
    to test our ETL services when they are deployed in the cloud. Data engineers use
    it a lot. Working with data lakes we would want to master CLI commands that help
    us manage Cloud storage, i.e. upload, download and copy files and objects. Why
    do we do this? Very often files in cloud storage trigger various ETL services.
    Batch processing is a very common data transformation pattern and in order to
    investigate bugs and errors we might need to download or copy files between buckets.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 云供应商的命令行工具非常有用，有助于创建测试我们在云中部署的ETL服务的脚本。数据工程师经常使用它。与数据湖一起工作时，我们希望掌握帮助我们管理云存储的CLI命令，即上传、下载和复制文件和对象。为什么我们要这样做？因为云存储中的文件经常触发各种ETL服务。批处理是一个非常常见的数据转换模式，为了调查错误和问题，我们可能需要在桶之间下载或复制文件。
- en: '![](../Images/c59684bded8474031b90c752aed36ea9.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c59684bded8474031b90c752aed36ea9.png)'
- en: ETL on Data lake objects with AWS Lambda. Image by author.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据湖对象上使用AWS Lambda进行ETL。作者提供的图像。
- en: In this example, we can see that the service outputs data into Kinesis and then
    it is stored in the data lake. When file objects are being created in S3 they
    trigger the ETL process handled by AWS Lambda. The result is being saved in the
    S3 bucket to consume by AWS Athena in order to generate a BI report with AWS Quicksight.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们可以看到服务将数据输出到Kinesis，然后存储在数据湖中。当S3中创建文件对象时，它们会触发由AWS Lambda处理的ETL过程。结果被保存到S3桶中，以供AWS
    Athena使用，从而生成一个使用AWS Quicksight的BI报告。
- en: 'Here is the set of AWS CLI commands we might want to use at some point:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们可能在某个时刻想要使用的一组AWS CLI命令：
- en: Copy and upload file
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复制和上传文件
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Recursively copy/upload/download all files in the folder
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 递归复制/上传/下载文件夹中的所有文件
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Recursively delete all contents
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 递归删除所有内容
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Delete a bucket
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除一个桶
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: There are more advanced examples but I think the idea is clear.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 还有更多高级示例，但我认为这个概念已经很清楚了。
- en: We would want to manage cloud storage efficiently with scripts.
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们希望使用脚本有效地管理云存储。
- en: We can chain these commands into shell scripts which makes CLI a very powerful
    tool.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些命令链接成shell脚本，这使得CLI成为一个非常强大的工具。
- en: 'Consider this shell script for example. It will check if storage bucket for
    lambda package exists, upload and deploy our ETL service as a Lambda Function:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑这个shell脚本。它将检查是否存在lambda包的存储桶，上传并部署我们的ETL服务作为Lambda函数：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: More advanced examples can be found in my previous stories.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 更高级的示例可以在我之前的故事中找到。
- en: Data quality
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据质量
- en: 'Now when we know how to deploy ETL services, perform requests and pull data
    from external APIs we need to learn how to observe the data we have in our data
    platform. Ideally, we would want to check data quality in real-time as data flows
    into our data platform. It can be done both ways using the ETL or ELT approach.
    Streaming applications built with Kafka or Kinesis have libraries to analyze data
    quality as data flows in the data pipeline. ELT approach is preferable when data
    engineers delegate data observability and data quality management to other stakeholders
    working with the data warehouse. Personally, I like the latter approach as it
    saves time. Consider data warehouse solutions as a single source of truth for
    everyone in the company. Finance, marketing and customer services teams can access
    data and check for **any potential issues.** Among these we typically see the
    following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们知道如何部署ETL服务、执行请求并从外部API拉取数据时，我们需要学习如何观察我们在数据平台上拥有的数据。理想情况下，我们希望在数据流入数据平台时实时检查数据质量。可以通过ETL或ELT方法来实现。使用Kafka或Kinesis构建的流应用程序具有分析数据质量的库，数据在数据管道中流动时可以进行分析。当数据工程师将数据可观察性和数据质量管理委派给其他在数据仓库中工作的利益相关者时，ELT方法更为可取。就个人而言，我喜欢后者，因为它节省了时间。将数据仓库解决方案视为公司中每个人的单一真实来源。财务、市场营销和客户服务团队可以访问数据并检查**任何潜在问题**。这些中我们通常会看到以下情况：
- en: missing data
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失数据
- en: data source outages
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据源中断
- en: data source changes when schema fields are updated
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据源在模式字段更新时发生变化
- en: various data anomalies such as outliers or unusual application/user behaviour.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种数据异常，如异常值或不寻常的应用程序/用户行为。
- en: Data engineers create alarms and schedule notifications for any potential data
    issues so data users stay always informed.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师创建警报并安排通知，以便对潜在的数据问题保持关注。
- en: 'Consider this example when a daily email is sent to stakeholders informing
    them about data outage:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个例子，当每天发送邮件通知利益相关者有关数据中断的情况：
- en: '![](../Images/c9b2a060d7071e8e6c95c403b0901690.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9b2a060d7071e8e6c95c403b0901690.png)'
- en: Email alarm. Image by author.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 邮件警报。图片由作者提供。
- en: In my stories you can find an article explaining how to schedule such data monitoring
    workflow using SQL.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的故事中，你可以找到一篇文章，解释如何使用SQL安排这样的数据监控工作流。
- en: '[](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0?source=post_page-----c0319cb226c2--------------------------------)
    [## Automated emails and data quality checks for your data'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0?source=post_page-----c0319cb226c2--------------------------------)
    [## 自动化邮件和数据质量检查'
- en: Data warehouse guide for better and cleaner data with scheduled emails
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据仓库指南以便更好、更干净的数据和定期邮件
- en: towardsdatascience.com](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0?source=post_page-----c0319cb226c2--------------------------------)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0?source=post_page-----c0319cb226c2--------------------------------)
- en: 'Consider this snippet below. It ewill check yeterday data ffor any missing
    field using **row conditions** and send a notification alarm if any were found:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下代码片段。它将检查昨天的数据是否有任何缺失字段，并在发现时发送通知警报：
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Data environments
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据环境
- en: Data engineers test data pipelines. There are various ways of doing this. In
    general, it requires a data environment split between production and staging pipelines.
    Often we might need an extra sandbox for testing purposes or to run data transformation
    unit tests when our ETL services trigger CI/CD workflows.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师测试数据管道。有多种方法来实现这一点。通常，它需要将数据环境分为生产和预发布管道。我们通常可能需要一个额外的沙盒用于测试目的，或者在我们的ETL服务触发CI/CD工作流时运行数据转换单元测试。
- en: This is a common practice and job interviewers might ask a couple of questions
    regarding this. It might seem a little tricky in the beginning but this diagram
    below exlpains how it works.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种常见做法，面试官可能会问一些相关问题。刚开始可能有点棘手，但下面的图解解释了它是如何工作的。
- en: '![](../Images/081619c52103b0f55e7e9c6dca8bd0f8.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/081619c52103b0f55e7e9c6dca8bd0f8.png)'
- en: CI/CD workflow example for ETL services. Image by author.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: CI/CD工作流示例，用于ETL服务。图片由作者提供。
- en: For example, we can use infrastructure as code and GitHub Actions to deploy
    and test staging resources on any pull request from the development branch. When
    all tests are passed and we are happy with our ETL service we can promote it to
    production by merging into the master branch.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用基础设施即代码和GitHub Actions来部署和测试任何来自开发分支的拉取请求中的预发布资源。当所有测试通过且我们对ETL服务满意时，我们可以通过合并到主分支将其提升到生产环境。
- en: Consider this GitHub action workflow below. It will deploy our ETL service on
    staging and do the testing. Suc approach helps reduce errors and deliver data
    piplelines faster.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 请看下面这个 GitHub Action 工作流。它将会在预生产环境中部署我们的 ETL 服务并进行测试。这样的做法有助于减少错误并更快地交付数据管道。
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: There is a full solution example in one of my stories.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的一个故事中有一个完整的解决方案示例。
- en: Machine learning
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习
- en: Adding a machine learning component will make us a machine learning engineer.
    Data engineering and ML are very close as data engineers create data pipelines
    that are consumed by ML services often.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 添加一个机器学习组件将使我们成为机器学习工程师。数据工程与机器学习非常接近，因为数据工程师创建的数据管道通常被机器学习服务使用。
- en: We don’t need to know every machine-learning model
  id: totrans-124
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们不需要了解每一种机器学习模型。
- en: We can’t compete with cloud service providers such as Amazon ang Google in machine
    learning and data science but we need to know how to use it.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法与像亚马逊和谷歌这样的云服务提供商在机器学习和数据科学领域竞争，但我们需要知道如何使用它。
- en: There are numerous managed ML services provided by cloud vendors and we would
    want to familiarize ourselves with them. Data engineers prepare datasets for these
    services and it will definitely be useful to do a couple of tutorials on this.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 云供应商提供了许多托管的机器学习服务，我们希望熟悉这些服务。数据工程师为这些服务准备数据集，做几个相关的教程肯定会很有用。
- en: 'For example, consider a churn prediction project to understand user churn and
    how to use managed ML services to generate predictions for users. This can easily
    done with BigQuery ML [9] by creating a simple logistic regression model like
    so:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个用户流失预测项目，以了解用户流失情况以及如何使用托管的机器学习服务为用户生成预测。这可以通过 BigQuery ML [9] 轻松完成，只需创建一个简单的逻辑回归模型，如下所示：
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'And then we can generate predictions using SQL:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用 SQL 生成预测：
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Conclusion
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: I’ve tried to summarise a set of data engineering skills and techniques that
    are typically required for entry-level data engineering roles. Based on my experience
    these skills can be acquired within two to three months of active learning. I
    would recommend starting with cloud service providers and Python to build a simple
    ETL service with a CI/CD pipeline for staging and production split. It doesn’t
    cost anything and we can learn fast by running them both locally and when they
    are deployed in the cloud. Data engineers are in high demand in the market right
    now. I hope this article will help you to learn a couple of new things and prepare
    for job interviews.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我试图总结一组数据工程技能和技术，这些技能通常是入门级数据工程师角色所需的。根据我的经验，这些技能可以在两到三个月的积极学习中掌握。我建议从云服务提供商和
    Python 开始，建立一个简单的 ETL 服务，并为预生产和生产环境设置 CI/CD 管道。这不需要花费任何费用，我们可以通过在本地和云中运行这些服务来快速学习。当前市场上对数据工程师的需求很高。我希望这篇文章能帮助你学习一些新知识，并为工作面试做准备。
- en: Recommedned read
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推荐阅读
- en: '[1] [https://www.linkedin.com/pulse/linkedin-jobs-rise-2023-25-uk-roles-growing-demand-linkedin-news-uk/](https://www.linkedin.com/pulse/linkedin-jobs-rise-2023-25-uk-roles-growing-demand-linkedin-news-uk/)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://www.linkedin.com/pulse/linkedin-jobs-rise-2023-25-uk-roles-growing-demand-linkedin-news-uk/](https://www.linkedin.com/pulse/linkedin-jobs-rise-2023-25-uk-roles-growing-demand-linkedin-news-uk/)'
- en: '[2] [https://towardsdatascience.com/data-platform-architecture-types-f255ac6e0b7](/data-platform-architecture-types-f255ac6e0b7)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://towardsdatascience.com/data-platform-architecture-types-f255ac6e0b7](/data-platform-architecture-types-f255ac6e0b7)'
- en: '[3] [https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3](/data-pipeline-design-patterns-100afa4b93e3)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3](/data-pipeline-design-patterns-100afa4b93e3)'
- en: '[4] [https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488](https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488](https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488)'
- en: '[5] [https://cloud.google.com/python/docs/reference/bigquery/latest](https://cloud.google.com/python/docs/reference/bigquery/latest)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [https://cloud.google.com/python/docs/reference/bigquery/latest](https://cloud.google.com/python/docs/reference/bigquery/latest)'
- en: '[6] [https://hudi.apache.org/docs/overview/](https://hudi.apache.org/docs/overview/)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [https://hudi.apache.org/docs/overview/](https://hudi.apache.org/docs/overview/)'
- en: '[7] [https://cloud.google.com/learn/certification/data-engineer](https://cloud.google.com/learn/certification/data-engineer)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [https://cloud.google.com/learn/certification/data-engineer](https://cloud.google.com/learn/certification/data-engineer)'
- en: '[8] [https://towardsdatascience.com/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [https://towardsdatascience.com/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0)'
- en: '[9] [https://cloud.google.com/bigquery/docs/bqml-introduction](https://cloud.google.com/bigquery/docs/bqml-introduction)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] [https://cloud.google.com/bigquery/docs/bqml-introduction](https://cloud.google.com/bigquery/docs/bqml-introduction)'
