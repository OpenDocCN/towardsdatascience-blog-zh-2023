- en: 'Use Deep Learning to Generate Fantasy Names: Build a Language Model from Scratch'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习生成奇幻名字：从零构建语言模型
- en: 原文：[https://towardsdatascience.com/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa](https://towardsdatascience.com/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa](https://towardsdatascience.com/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa)
- en: Can a language model invent unique fantasy character names? Let’s build it from
    scratch
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个语言模型能否发明独特的奇幻角色名字？让我们从零开始构建它
- en: '[](https://medium.com/@riccardo.andreoni?source=post_page-----792b13629efa--------------------------------)[![Riccardo
    Andreoni](../Images/5e22581e419639b373019a809d6e65c1.png)](https://medium.com/@riccardo.andreoni?source=post_page-----792b13629efa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----792b13629efa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----792b13629efa--------------------------------)
    [Riccardo Andreoni](https://medium.com/@riccardo.andreoni?source=post_page-----792b13629efa--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@riccardo.andreoni?source=post_page-----792b13629efa--------------------------------)[![Riccardo
    Andreoni](../Images/5e22581e419639b373019a809d6e65c1.png)](https://medium.com/@riccardo.andreoni?source=post_page-----792b13629efa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----792b13629efa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----792b13629efa--------------------------------)
    [Riccardo Andreoni](https://medium.com/@riccardo.andreoni?source=post_page-----792b13629efa--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----792b13629efa--------------------------------)
    ·11 min read·Sep 22, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----792b13629efa--------------------------------)
    ·阅读时间 11 分钟·2023年9月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f64a5c60ba6b75f99b1f391913f9f8be.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f64a5c60ba6b75f99b1f391913f9f8be.png)'
- en: 'Source: [pixabay.com](https://pixabay.com/illustrations/book-old-surreal-fantasy-pages-863418/)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[pixabay.com](https://pixabay.com/illustrations/book-old-surreal-fantasy-pages-863418/)
- en: To truly grasp the intricacies of [**Language Models**](https://en.wikipedia.org/wiki/Language_model#:~:text=A%20language%20model%20is%20a,feedforward%20neural%20networks%20and%20transformers.)
    (LM) and become familiar with their underlying principles, there is no other way
    than rolling up our sleeves and starting to write code. In this article, I present
    the creation of a [**Recurrent Neural Network**](https://en.wikipedia.org/wiki/Recurrent_neural_network)
    (RNN) built entirely from scratch, without the aid of any deep learning library.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 要真正掌握 [**语言模型**](https://en.wikipedia.org/wiki/Language_model#:~:text=A%20language%20model%20is%20a,feedforward%20neural%20networks%20and%20transformers.)
    (LM) 的复杂性并熟悉其基本原理，唯一的方法就是卷起袖子开始编写代码。在本文中，我展示了一个从零开始完全构建的 [**递归神经网络**](https://en.wikipedia.org/wiki/Recurrent_neural_network)
    (RNN)，没有使用任何深度学习库。
- en: Tensorflow, Keras, Pytorch make building deep and complex neural networks effortless.
    Undoubtedly, this is a great advantage for Machine Learning practitioners, however,
    this approach has the massive downside of leaving the functioning of those networks
    unclear as they happen “under the hood”.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Tensorflow、Keras、Pytorch 使得构建深层复杂的神经网络变得轻而易举。这无疑是机器学习从业者的一大优势，但这种方法也有一个巨大的缺点，那就是这些网络的运行机制往往不够清晰，因为它们是在“引擎盖下”发生的。
- en: This is why today we will perform the inspiring exercise of building a Language
    Model using only the [Numpy Python](https://numpy.org/) library!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么今天我们将进行一个鼓舞人心的练习，仅使用 [Numpy Python](https://numpy.org/) 库来构建一个语言模型！
- en: Understanding Recurrent Neural Networks and Language Models
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解递归神经网络和语言模型
- en: 'Standard fully connected neural networks are not suited for [Natural Language
    Processing](https://en.wikipedia.org/wiki/Natural_language_processing) (NLP) tasks
    like text generation. The main reasons are:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 标准全连接神经网络不适用于像文本生成这样的 [自然语言处理](https://en.wikipedia.org/wiki/Natural_language_processing)
    (NLP) 任务。主要原因如下：
- en: For NLP tasks, input and outputs may take different forms and dimensions.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 NLP 任务，输入和输出可能采用不同的形式和维度。
- en: Standard neural networks don’t simultaneously use features learned at different
    steps of the network.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准神经网络不会同时使用网络不同步骤中学到的特征。
- en: The main breakthrough in AI application within the NLP field is undeniably represented
    by Recurrent Neural Networks (RNN).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AI 应用于 NLP 领域的主要突破无疑是递归神经网络 (RNN)。
- en: 'RNNs are a class of artificial neural networks particularly well suited for
    NLP tasks and text generation. The reason for their efficacy lies in their ability
    of capturing sequential dependencies in data. Human language deeply relies on
    considering the context and linking the first words in a sentence to the last
    ones. Consider these sentences:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 是一类特别适合 NLP 任务和文本生成的人工神经网络。它们的有效性在于能够捕捉数据中的序列依赖关系。人类语言深深依赖于考虑上下文并将句子的第一个词与最后一个词联系起来。考虑这些句子：
- en: '*He said, “Teddy Roosevelt was the US President.”*'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*他说：“西奥多·罗斯福曾是美国总统。”*'
- en: '*He said, “Teddy bears are on sale!”*'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*他说：“泰迪熊正在打折！”*'
- en: The word “*Teddy*” has a completely different meaning in the two sentences.
    We humans easily understand that by considering the context and the words written
    in the opposite part of the sentence. Surprisingly enough, **RNNs can do the same**!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 单词 “*Teddy*” 在这两个句子中具有完全不同的含义。我们人类通过考虑上下文和句子另一部分的词语，很容易理解这一点。令人惊讶的是，**RNN 也能做到这一点**！
- en: RNN Architecture
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNN 架构
- en: The architecture of RNNs is fairly straightforward. They are made of sequential
    cells, each one of them takes as input a word *x* (or a single character), outputs
    a word/character *y*, and passes an activation *a* to the next cell.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的架构相当简单。它们由一系列顺序单元组成，每个单元接收一个词 *x*（或一个字符）作为输入，输出一个词/字符 *y*，并将激活值 *a* 传递给下一个单元。
- en: '![](../Images/f30ec7ec395dd7ba0d5daf116c16fa72.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f30ec7ec395dd7ba0d5daf116c16fa72.png)'
- en: RNN flowchart. Image by the author.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 流程图。图片由作者提供。
- en: 'What happens inside a RNN cell is more interesting. The steps are the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 单元内部发生的事情更有趣。步骤如下：
- en: The activation of the previous cell is multiplied by some weights *W_aa*
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前一个单元的激活值乘以一些权重 *W_aa*
- en: The input *x* is multiplied by some weights *W_ax*
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入 *x* 乘以一些权重 *W_ax*
- en: The results of the previous steps are summed together and with a bias *b_a*
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面步骤的结果会相加，并与偏置 *b_a* 一起处理
- en: An activation function like the hyperbolic tangent is applied to compute the
    activation which is passed to the next cell
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用像双曲正切这样的激活函数来计算激活值，然后传递给下一个单元
- en: The activation is multiplied by some weights *W_ya* and summed with a bias *b_y*
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活值乘以一些权重 *W_ya* 并与偏置 *b_y* 相加
- en: Finally, a softmax function is applied to the resulting vector and the output
    *y_hat* is returned
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，对结果向量应用 softmax 函数，并返回输出 *y_hat*
- en: '![](../Images/f6ef5bc9f480a77f4f03431f6e1bf08c.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6ef5bc9f480a77f4f03431f6e1bf08c.png)'
- en: RNN cell flowchart. Image by the author.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 单元流程图。图片由作者提供。
- en: 'To summarize, the formulas for the activation and output are as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，激活和输出的公式如下：
- en: '![](../Images/da06e7631ca8be19b539fdc905837300.png)![](../Images/2b8de8a940b0bf252f1bd5f35b04c267.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da06e7631ca8be19b539fdc905837300.png)![](../Images/2b8de8a940b0bf252f1bd5f35b04c267.png)'
- en: I’m unable to provide a comprehensive theoretical introduction of RNNs and LMs
    here. For that, I refer you to the resources listed at the end of this article.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我无法在这里提供 RNN 和语言模型的全面理论介绍。有关内容，请参见本文末尾列出的资源。
- en: 'Let’s dive now into the actual Python code. The most important parts of the
    code will be explained in detail in this article. In order to keep the article
    concise, some intuitive parts are omitted. The entire commented code can be accessed
    in my [GitHub repository](https://github.com/andreoniriccardo/articles/tree/main/names-generator-RNN):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来深入了解实际的 Python 代码。本文将详细解释代码中最重要的部分。为了使文章简洁，一些直观的部分被省略。整个带注释的代码可以在我的 [GitHub
    仓库](https://github.com/andreoniriccardo/articles/tree/main/names-generator-RNN)
    中获取：
- en: '[](https://github.com/andreoniriccardo/articles/tree/main/names-generator-RNN?source=post_page-----792b13629efa--------------------------------)
    [## articles/names-generator-RNN at main · andreoniriccardo/articles'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/andreoniriccardo/articles/tree/main/names-generator-RNN?source=post_page-----792b13629efa--------------------------------)
    [## articles/names-generator-RNN at main · andreoniriccardo/articles'
- en: Contribute to andreoniriccardo/articles development by creating an account on
    GitHub.
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过在 GitHub 上创建账户来为 andreoniriccardo/articles 开发做贡献。
- en: github.com](https://github.com/andreoniriccardo/articles/tree/main/names-generator-RNN?source=post_page-----792b13629efa--------------------------------)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/andreoniriccardo/articles/tree/main/names-generator-RNN?source=post_page-----792b13629efa--------------------------------)
- en: Data Preparation
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: Our goal is to teach a Language Model to invent novel fantasy character names.
    For this reason, the first thing we need to provide our Language Model is a database
    of actual fantasy names. The LM will be trained and will take inspiration from
    that.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是教会语言模型发明新颖的奇幻角色名称。因此，我们需要为语言模型提供一个实际奇幻名称的数据库。语言模型将从中获得训练和灵感。
- en: 'Thanks to this [Wikipedia page](https://en.wikipedia.org/wiki/List_of_Middle-earth_characters),
    we can easily access a list of all the characters mentioned in the [*Lord Of The
    Ring*](https://en.wikipedia.org/wiki/The_Lord_of_the_Rings) or [*The Hobbit*](https://en.wikipedia.org/wiki/The_Hobbit)
    books. Using the [BeautifulSoup](https://pypi.org/project/beautifulsoup4/)and
    [Regex](https://docs.python.org/3/library/re.html) libraries, the following lines
    of code will collect the data:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个 [维基百科页面](https://en.wikipedia.org/wiki/List_of_Middle-earth_characters)，我们可以轻松访问提到的所有角色的列表，这些角色出现在
    [*指环王*](https://en.wikipedia.org/wiki/The_Lord_of_the_Rings) 或 [*霍比特人*](https://en.wikipedia.org/wiki/The_Hobbit)
    书中。使用 [BeautifulSoup](https://pypi.org/project/beautifulsoup4/) 和 [Regex](https://docs.python.org/3/library/re.html)
    库，以下代码将收集数据：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For the explanation of the Regex functions, I leave you with this comprehensive
    [guide](https://pub.towardsai.net/regex-for-the-modern-data-scientists-37bd528d343a).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Regex 函数的解释，我为你提供了这份详尽的 [指南](https://pub.towardsai.net/regex-for-the-modern-data-scientists-37bd528d343a)。
- en: 'At this point, our dataset will look like this:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们的数据集将如下所示：
- en: '![](../Images/269aec886667a1e0f9c59c4948f61e04.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/269aec886667a1e0f9c59c4948f61e04.png)'
- en: I aim to simplify our vocabulary by replacing unwanted characters such as ä,
    é, î.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我的目标是通过替换不需要的字符如 ä、é、î 来简化我们的词汇。
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, our vocabulary consists in 27 characters:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们的词汇由 27 个字符组成：
- en: '*[‘x’, ‘j’, ‘f’, ‘t’, ‘c’, ‘b’, ‘o’, ‘l’, ‘y’, ‘w’, ‘i’, ‘e’, ‘g’, ‘m’, ‘k’,
    ‘d’, ‘v’, ’n’, ‘u’, ‘a’, ‘z’, ‘r’, ‘\n’, ‘s’, ‘ ‘, ‘p’, ‘h’]*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*[‘x’，‘j’，‘f’，‘t’，‘c’，‘b’，‘o’，‘l’，‘y’，‘w’，‘i’，‘e’，‘g’，‘m’，‘k’，‘d’，‘v’，‘n’，‘u’，‘a’，‘z’，‘r’，‘\n’，‘s’，‘
    ‘，‘p’，‘h’]*'
- en: Note that the special character *‘\n’* serves as an End of Name token, indicating
    when the generated name should terminate.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，特殊字符 *‘\n’* 作为名称终止符，表示生成的名称应何时终止。
- en: The dataset is ready and we can now focus on modeling the Recurrent Neural Network.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集已经准备好，我们现在可以专注于建模递归神经网络。
- en: Building the Language Model
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建语言模型
- en: 'In this section, I present the Python implementation of the Language Model.
    The whole code can be divided into 2 sections:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我展示了语言模型的 Python 实现。整个代码可以分为两个部分：
- en: Forward Propagation
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正向传播
- en: Backprop
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Backprop
- en: In the following sections, I will present the code that actually trains the
    model and how to generate fantasy character names.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我将展示实际训练模型的代码以及如何生成奇幻角色名称。
- en: Forward Propagation
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正向传播
- en: As illustrated above, a RNN is a network composed of multiple cells. It is advantageous
    to model a single RNN cell in Python and then integrate its output through multiple
    cells.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，RNN 是由多个单元组成的网络。在 Python 中建模单个 RNN 单元然后通过多个单元集成其输出是有利的。
- en: 'The code that models a single RNN cell is the following:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟单个 RNN 单元的代码如下：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, it is not complex at all. We simply take as input the model’s
    parameters and multiply them by the cell’s input and previous activation function.
    Finally, we apply the softmax function to return a vector of probabilities for
    what the output character should be.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这一点也不复杂。我们只是将模型的参数作为输入，并将其与单元的输入和先前的激活函数相乘。最后，我们应用 softmax 函数返回一个表示输出字符概率的向量。
- en: The next step is to iterate through several RNN cells. This is exactly what
    the `RNN_roward_prop()` function does.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是遍历多个 RNN 单元。这正是 `RNN_roward_prop()` 函数的作用。
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: It calls the previous `RNN_roward_pro_step()` function *T* times, where *T*
    is the number of characters of the input word. At the end, a loss function and
    the output is returned.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 它调用之前的 `RNN_roward_pro_step()` 函数 *T* 次，其中 *T* 是输入词的字符数。最后，返回损失函数和输出结果。
- en: Backprop
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Backprop
- en: Backprop, short for backward propagation, is the process of adjusting the network’s
    weights to get closer and closer to the desired output, i.e. reducing the model
    loss function. This is done by applying gradient descent.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Backprop，即反向传播，是调整网络权重以逐渐接近期望输出的过程，即减少模型损失函数。这是通过应用梯度下降来完成的。
- en: Detailing the formulas of gradient descent or other viable optimization algorithms
    is out of this article’s scope. I will leave instead with [**this article**](/choose-the-right-optimization-algorithm-for-your-neural-network-cb86c15d7328),
    which deals exactly with the selection of the right Optimization Algorithm for
    your Deep Networks.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 详细说明梯度下降或其他可行优化算法的公式超出了本文的范围。我将推荐[**这篇文章**](/choose-the-right-optimization-algorithm-for-your-neural-network-cb86c15d7328)，它正好涉及如何选择适合你深度网络的优化算法。
- en: '[](/choose-the-right-optimization-algorithm-for-your-neural-network-cb86c15d7328?source=post_page-----792b13629efa--------------------------------)
    [## Choose the Right Optimization Algorithm for your Neural Network'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/choose-the-right-optimization-algorithm-for-your-neural-network-cb86c15d7328?source=post_page-----792b13629efa--------------------------------)
    [## 选择适合你神经网络的优化算法'
- en: As the nature of neural networks’ developing process is iterative, we need to
    take advantage of each possible expedient…
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 由于神经网络的开发过程是迭代性的，我们需要利用每一个可能的便捷方法……
- en: towardsdatascience.com](/choose-the-right-optimization-algorithm-for-your-neural-network-cb86c15d7328?source=post_page-----792b13629efa--------------------------------)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/choose-the-right-optimization-algorithm-for-your-neural-network-cb86c15d7328?source=post_page-----792b13629efa--------------------------------)
- en: 'The strategy I used to model the backprop flow is the same as the one for the
    forward propagation: code the backprop of a single RNN cell and iterate it multiple
    times.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我用来建模反向传播流的策略与前向传播相同：编码单个RNN单元的反向传播并多次迭代。
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The above function takes as input the output of the forward propagation and
    the previous gradients and it computes the updated gradients.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 上述函数接受前向传播的输出和之前的梯度作为输入，并计算更新后的梯度。
- en: 'The ideration of the `RNN_back_prop_function()` function is performed with
    these lines of code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`RNN_back_prop_function()`函数的迭代通过以下代码行执行：'
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Training the Language Model
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练语言模型
- en: At this point, all the model’s components are set and ready to be executed.
    I wrote the following code to put together the functions we saw above.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，所有模型的组件都已设置好并准备执行。我写了以下代码来整合我们上面看到的函数。
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The actual magic happens in the following code snipped. This is the main part
    of the whole algorithm and once this function is called, the Language Model will
    be trained.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的魔法发生在以下代码片段中。这是整个算法的主要部分，一旦调用此函数，语言模型将被训练。
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The result is a model able to mimics the creative process of Tolkien and effortlessly
    produce unique character names.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个能够模仿托尔金创作过程的模型，轻松生成独特的角色名字。
- en: Generating Fantasy Character Names
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成奇幻角色名字
- en: To sample novel character names from our trained Language Model, I developed
    two functions.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从我们训练的语言模型中采样新的字符名字，我开发了两个函数。
- en: The `sample()` function takes as input the network’s parameters and the vocabulary
    mapping the characters to numbers. The idea is to apply the forward propagation
    steps multiple times until either the *‘\n’* special character is return, or we
    reach an upper bound for the number of generated characters (in this case set
    to 50).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`sample()`函数接受网络的参数和将字符映射到数字的词汇表作为输入。其思想是多次应用前向传播步骤，直到返回*‘\n’*特殊字符，或者达到生成字符的上限（在此设置为50）。'
- en: Finally, it returns a list of indices that encode the generated fantasy character
    name.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，它返回一个索引列表，用于编码生成的奇幻角色名字。
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To print the generated fantasy names in a human-readable form, we need to call
    the `get_sample()` function, which takes as input the list of indices previously
    generated and the decoding dictionary mapping indices to characters.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以人类可读的形式打印生成的奇幻名字，我们需要调用`get_sample()`函数，它接受先前生成的索引列表和将索引映射到字符的解码字典作为输入。
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'With everything in place, you can now appreciate some original fantasy character
    names, as shown below:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一切就绪后，你现在可以欣赏一些原创的奇幻角色名字，如下所示：
- en: '![](../Images/2f212619fc0a1124dde79edd34f56699.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f212619fc0a1124dde79edd34f56699.png)'
- en: Image by the author.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: Evaluating Model Performance
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型性能
- en: 'During the first phases of the training process, the model is still unable
    to effectively mimic Tolkien’s style. If we sample random names at this stage
    we obtain pure gibberish such as:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程的初期，模型仍然无法有效模仿托尔金的风格。如果我们在这个阶段采样随机名字，我们会得到纯粹的胡言乱语，如下所示：
- en: “Orvnnvfufufiiubx” at iteration 2000 and loss 27.96
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第2000次迭代和损失27.96时的“Orvnnvfufufiiubx”
- en: “Aotvux” at iteration 4000 and loss 26.43
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第4000次迭代和损失26.43时的“Aotvux”
- en: 'We can clearly see how these names don’t meet the iconic Middle Earth patterns
    and sounds. However, by letting the model learn the dataset features for a longer
    period of time, we start to obtain progressively more plausible generated names:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到这些名字不符合标志性的中土世界模式和声音。然而，通过让模型在数据集特征上学习更长时间，我们开始逐渐获得更可信的生成名字：
- en: “Furun I” at iteration 14000 and loss 21.53
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “Furun I” 在第 14000 次迭代和损失 21.53
- en: “Flutto Balger” at iteration 16000 and loss 21.11
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “Flutto Balger” 在第 16000 次迭代和损失 21.11
- en: Finally, the trained model seems to be able to emulate the characters’ name
    style.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，经过训练的模型似乎能够模拟角色的名字风格。
- en: The improvement I described qualitatively can be also quantitatively visualized
    as follows.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我所描述的改进在定量上也可以通过以下方式可视化。
- en: By plotting the loss function through the iterations, we can clearly see how
    the [optimization algorithm](/choose-the-right-optimization-algorithm-for-your-neural-network-cb86c15d7328)
    progressively tunes the weights and biases in the right direction.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 通过绘制迭代过程中的损失函数，我们可以清楚地看到 [优化算法](/choose-the-right-optimization-algorithm-for-your-neural-network-cb86c15d7328)
    如何逐步调整权重和偏差到正确的方向。
- en: '![](../Images/0ce95057b5b483abdad50878102acc54.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ce95057b5b483abdad50878102acc54.png)'
- en: Image by the author.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: The oscillating behavior of the loss function is motivated by the fact that
    we are using a single training example in each iteration step to train the model.
    Using a larger batch would result in a smoother loss curve.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的振荡行为是因为我们在每次迭代步骤中使用了一个单一的训练示例来训练模型。使用较大的批次将会导致更平滑的损失曲线。
- en: Conclusion
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, building a Language Model for fantasy name generation has brought
    us several insights.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，为幻想名称生成构建语言模型给我们带来了几个见解。
- en: We discovered how RNNs and LMs are capable of appreciating sequential dependencies
    in the data, making them essential elements for NLP and all the tasks that involve
    text.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现 RNN 和语言模型能够识别数据中的序列依赖性，使它们成为自然语言处理以及涉及文本的所有任务的关键元素。
- en: Moreover, I cannot enphasize more the importance of an hands-on approach to
    gain experience in any Data Science related subject. Studying the theory is fundamental,
    but provides you only half of what you need.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我不能再强调实践方法在任何数据科学相关主题中获得经验的重要性。学习理论是基础，但只提供了你所需的一半知识。
- en: Finally, I want to stretch again the flexibility of the tool we just created.
    I recommend training it with a different set of names. Try using Disney character
    names or typical pet names as the input dataset and share what you generate with
    me!
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想再次强调我们刚刚创建的工具的灵活性。我推荐用不同的名字集进行训练。尝试使用迪士尼角色名字或典型的宠物名字作为输入数据集，并与我分享你生成的内容！
- en: If you liked this story, consider following me to be notified of my upcoming
    projects and articles!
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这个故事，可以关注我，以便及时了解我的最新项目和文章！
- en: 'Here are some of my past projects:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我的一些过去项目：
- en: '[](/ensemble-learning-with-scikit-learn-a-friendly-introduction-5dd64650de6c?source=post_page-----792b13629efa--------------------------------)
    [## Ensemble Learning with Scikit-Learn: A Friendly Introduction'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/ensemble-learning-with-scikit-learn-a-friendly-introduction-5dd64650de6c?source=post_page-----792b13629efa--------------------------------)
    [## 使用 Scikit-Learn 的集成学习：一个友好的介绍'
- en: Ensemble learning algorithms like XGBoost or Random Forests are among the top-performing
    models in Kaggle competitions…
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 像 XGBoost 或随机森林这样的集成学习算法是 Kaggle 比赛中表现最好的模型之一……
- en: 'towardsdatascience.com](/ensemble-learning-with-scikit-learn-a-friendly-introduction-5dd64650de6c?source=post_page-----792b13629efa--------------------------------)
    [](/euro-trip-optimization-genetic-algorithms-and-google-maps-python-api-solve-the-traveling-salesman-4ad8e1548207?source=post_page-----792b13629efa--------------------------------)
    [## Euro Trip Optimization: Genetic Algorithms and Google Maps API Solve the Traveling
    Salesman Problem'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/ensemble-learning-with-scikit-learn-a-friendly-introduction-5dd64650de6c?source=post_page-----792b13629efa--------------------------------)
    [](/euro-trip-optimization-genetic-algorithms-and-google-maps-python-api-solve-the-traveling-salesman-4ad8e1548207?source=post_page-----792b13629efa--------------------------------)
    [## 欧洲旅行优化：遗传算法和 Google Maps API 解决旅行推销员问题
- en: Navigate the charm of Europe’s 50 most visited cities using genetic algorithms
    and Google Maps API, unlocking efficient…
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用遗传算法和 Google Maps API 探索欧洲 50 个最受欢迎城市的魅力，解锁高效的……
- en: 'towardsdatascience.com](/euro-trip-optimization-genetic-algorithms-and-google-maps-python-api-solve-the-traveling-salesman-4ad8e1548207?source=post_page-----792b13629efa--------------------------------)
    [](/the-birth-of-data-science-historys-first-hypothesis-test-python-insights-4745dccaf6d?source=post_page-----792b13629efa--------------------------------)
    [## The Birth of Data Science: History’s First Hypothesis Test & Python Insights'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/euro-trip-optimization-genetic-algorithms-and-google-maps-python-api-solve-the-traveling-salesman-4ad8e1548207?source=post_page-----792b13629efa--------------------------------)
    [](/the-birth-of-data-science-historys-first-hypothesis-test-python-insights-4745dccaf6d?source=post_page-----792b13629efa--------------------------------)
    [## 数据科学的诞生：历史上的首次假设检验与Python见解
- en: Dive into Python-powered insights that every data scientist needs to know
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深入了解每位数据科学家需要知道的Python驱动的见解
- en: towardsdatascience.com](/the-birth-of-data-science-historys-first-hypothesis-test-python-insights-4745dccaf6d?source=post_page-----792b13629efa--------------------------------)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/the-birth-of-data-science-historys-first-hypothesis-test-python-insights-4745dccaf6d?source=post_page-----792b13629efa--------------------------------)
- en: Resources
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '[GitHub Repository](https://github.com/andreoniriccardo/articles/tree/main/names-generator-RNN)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GitHub 仓库](https://github.com/andreoniriccardo/articles/tree/main/names-generator-RNN)'
- en: '[LOTR character names](https://www.behindthename.com/namesakes/list/tolkien/alpha)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[《指环王》角色名字](https://www.behindthename.com/namesakes/list/tolkien/alpha)'
- en: '[Deep Learning Course](https://www.coursera.org/specializations/deep-learning)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习课程](https://www.coursera.org/specializations/deep-learning)'
- en: '[Recurrent Neural Network](https://en.wikipedia.org/wiki/Recurrent_neural_network)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[递归神经网络](https://en.wikipedia.org/wiki/Recurrent_neural_network)'
- en: '[BeautifulSoup](https://pypi.org/project/beautifulsoup4/)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BeautifulSoup](https://pypi.org/project/beautifulsoup4/)'
