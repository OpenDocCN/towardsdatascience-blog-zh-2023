- en: TensorFlow Model Training Using GradientTape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/tensorflow-model-training-using-gradienttape-f2093646ab13](https://towardsdatascience.com/tensorflow-model-training-using-gradienttape-f2093646ab13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/fbe16d8537174389bce863e509126468.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Sivani Bandaru](https://unsplash.com/@agni11?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Use of GradientTape to Update the Weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://rashida00.medium.com/?source=post_page-----f2093646ab13--------------------------------)[![Rashida
    Nasrin Sucky](../Images/42bd057e8eca255907c43c29a498f2ca.png)](https://rashida00.medium.com/?source=post_page-----f2093646ab13--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f2093646ab13--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f2093646ab13--------------------------------)
    [Rashida Nasrin Sucky](https://rashida00.medium.com/?source=post_page-----f2093646ab13--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f2093646ab13--------------------------------)
    ·7 min read·Oct 17, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow is arguably the most popular library for deep learning. I wrote so
    many tutorials on TensorFlow before and still continuing. TensorFlow is very well
    organized and easy to use package where you do not need to worry about model development
    and model training too much. Pretty much most of the stuff is taken care of by
    the package itself. That is probably the reason why it has gotten so popular in
    the industry. But at the same time, sometimes it is nice to have control over
    the behind-the-scenes functionalities. It gives you a lot of power to experiment
    with the models. If you are job seeker, some extra knowledge may give you an edge.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, I wrote an article on [how to develop custom activation functions,
    layers, and loss functions](/how-to-define-custom-layer-activation-function-and-loss-function-in-tensorflow-bdd7e78eb67).
    In this article, we will see how you can train the model manually and update the
    weights yourself. But don’t worry. You don’t have to remember the differential
    calculus all over again. We have GradientTape() method available in TensorFlow
    itself to take care of that part.
  prefs: []
  type: TYPE_NORMAL
- en: 'If GradientTape() is totally new to you, please feel free to check this exercises
    on GradientTape() that shows you, how GradientTape() works: [Introduction to GradientTape
    in TensorFlow — Regenerative (regenerativetoday.com)](https://regenerativetoday.com/introduction-to-gradienttape-in-tensorflow/)'
  prefs: []
  type: TYPE_NORMAL
- en: Data Preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this article we work on a simple classification algorithm in TensorFlow
    using GradientTape(). Please download the dataset from this link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Heart Failure Prediction Dataset (kaggle.com)](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)'
  prefs: []
  type: TYPE_NORMAL
- en: This dataset has an open database license.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the necessary imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Creating the DataFrame with the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61a3bb01c99a35bbda05fe057c85d556.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the picture above, there are multiple columns with string data
    type. Let’s check the data types of all the columns in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you are here, I can assume that you know the machine learning basics and
    already have learned that the data types need to be numeric for TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: This code below loops through the columns and if the data type of a column is
    ‘object’, it changes that to numeric data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'All the columns have become the numeric type now. Define the training features
    and target variable to move forward with the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We should keep a part of the dataset to evaluate the model. So we will use
    the train_test_split method here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Before diving into the model development part, it is also necessary to scale
    the data. I am using standard scaling method here that requires mean and standard
    deviation. Remember, we need the mean and standard deviation of the training data
    only. To scale the test data, the mean and standard deviation of training data
    needs to be used again as we shouldn’t reveal any information about the test data
    to the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the .describe() method to find out the statistical parameters of
    each column of the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/aaba121317fe5e39bff8a35db170ab88.png)'
  prefs: []
  type: TYPE_IMG
- en: We have the parameters for scaling now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Target variables for training and testing data need to be separated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following function is defined to scale one column of a dataset using the
    standard scaling formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the function ‘norm’ scaling both training and testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If you check the X_train_norm and X_test_norm, they are actually arrays. Converting
    them to tensors here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Shuffling the datasets by each batch where batch_size is set to be 32:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Model Development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will take a simple TensorFlow model for this as the dataset is so simple.
    The model is defined as a function base_model where two fully connected Dense
    layers of 64 neurons and one output layers are added. In the last line, the function
    base_model called and saved in a variable called ’model’.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: I chose RMSprop optimizer and BinaryCrossentropy() loss function for this example.
    Please feel free to use any other Optimizer of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Model Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The GradientTape part is going to be useful in the model training part. During
    the model training we need the differentiation to update the weights. The function
    below will use the GradientTape to calculate the gradients. First it calculates
    the outputs using the model which is called the logits. Using the true label and
    predicted label, loss was calculated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then the gradients were calculated taking the differential of loss with respect
    to the weights and applied the gradients to the optimizers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The next function is to train the network for one epoch. Training one epoch
    includes:'
  prefs: []
  type: TYPE_NORMAL
- en: the process of gradient calculation and applying as in the ‘gradient_calc’ function,
  prefs: []
  type: TYPE_NORMAL
- en: saving the losses and
  prefs: []
  type: TYPE_NORMAL
- en: updating the true label to the predicted label by the model after one epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: One more function is necessary before we start the model training. That is the
    function for the validation loss. This function is pretty self explanatory. It
    calculates the validation loss using the true label and the predicted label, append
    the loss to the list of losses, finally updates the true label for the validation
    data to the new calculated labels.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'For this model I decided to use the accuracy as the evaluation metric. Calling
    the BinaryAccuracy() method for both training and validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: It’s model training time. I am training this model for 60 epochs. We will loop
    through the epochs,
  prefs: []
  type: TYPE_NORMAL
- en: we can get the training losses by calling the ‘training_one_epoch’ function
    in each epoch, and validation loss by calling the ‘validation_loss’ function.
    These functions will provide up with a list of losses for each epoch because they
    will calculate the losses for all the data separately. We will take the mean of
    all the losses to get only one loss per epoch for both training and validation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output for last few epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: After the 60 epochs, we got 93.3% accuracy for training data and 87.5% aacuracy
    for the validation data. Please feel free to train the model for more epochs to
    check if you can improve the accuracy scores. But please be aware of the overfitting
    issue.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article, we worked on an example to learn how to manually train the
    model instead of using the model.compile() method. This should give you better
    understanding of the TensorFlow library itself and how exactly model training
    works in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: More Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[How to Define Custom Layer, Activation Function, and Loss Function in TensorFlow
    | by Rashida Nasrin Sucky | Towards Data Science (medium.com)](https://medium.com/p/bdd7e78eb67)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Anomaly Detection in TensorFlow and Keras Using the Autoencoder Method | by
    Rashida Nasrin Sucky | Sep, 2023 | Towards Data Science (medium.com)](https://medium.com/p/5600aca29c50)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Using a Keras Tuner for Hyperparameter Tuning of a TensorFlow Model | by Rashida
    Nasrin Sucky | Towards AI (medium.com)](https://medium.com/p/41978f53111)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Implementation of a Siamese Network in Keras and TensorFlow | by Rashida Nasrin
    Sucky | Towards Data Science (medium.com)](https://medium.com/p/aa327418e177)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Complete Implementation of a Mini VGG Network for Image Recognition | by Rashida
    Nasrin Sucky | Towards Data Science (medium.com)](https://medium.com/p/849299480356)'
  prefs: []
  type: TYPE_NORMAL
