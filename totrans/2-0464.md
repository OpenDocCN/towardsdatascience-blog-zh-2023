# 人工智能能克服人类的确认偏差吗？

> 原文：[https://towardsdatascience.com/can-ai-overcome-humans-confirmation-bias-57bee0bc5c8c](https://towardsdatascience.com/can-ai-overcome-humans-confirmation-bias-57bee0bc5c8c)

## 人工智能如何补充人类智慧的不足，并作为人类认知偏差的对冲力量

[](https://jshen9889.medium.com/?source=post_page-----57bee0bc5c8c--------------------------------)[![Stephanie Shen](../Images/857cccbe84f0d3a9886c84acfbbac03e.png)](https://jshen9889.medium.com/?source=post_page-----57bee0bc5c8c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----57bee0bc5c8c--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----57bee0bc5c8c--------------------------------) [Stephanie Shen](https://jshen9889.medium.com/?source=post_page-----57bee0bc5c8c--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----57bee0bc5c8c--------------------------------) ·阅读时间 9 分钟·2023年11月11日

--

![](../Images/f42bd2913152c60b53a12af4000d7d80.png)

图片来源：由 [geralt](https://pixabay.com/users/9301/?tab=photos&order=latest&pagi=1) 拍摄，来自 [Pixabay](https://pixabay.com/illustrations/hand-candle-diwali-4543318/)

从诺贝尔奖得主丹尼尔·卡尼曼的《思考，快与慢》一书中，我们知道人类的大脑在其应有的功能上远非完美。除了情绪冲动、顽固的成瘾以及自人类文明诞生以来的道德挣扎外，卡尼曼在他的书中全面描述了我们认知偏差的细微差别。更令人沮丧的是，许多偏差渗透到我们生活的方方面面、组织和社会中。鉴于此，我们现在面临两个关键问题。首先，我们如何识别我们决策中的这种偏差的现实？其次，我们如何在决策过程中弥补或防止这些偏差？在本文中，我将从数据科学的角度出发，通过关注最普遍的偏差之一——确认偏差，来回答这两个问题。随着机器学习和人工智能的进步，在它们的帮助下，我们现在看到了识别、克服和防止这些偏差的曙光。

# 什么是确认偏差？

确认偏差是指解释和寻找信息以确认或支持现有信念或先前结论的倾向。由于确认偏差，人们倾向于通过片面地测试想法、关注支持/积极证据而忽视可能反驳自己观点的替代/矛盾事实来确认自己的信念或假设。确认偏差是无意的，与故意欺骗相对。这种偏差在许多领域的人类生活和社会中具有广泛的影响，包括科学研究、医学、政治、执法、社会心理学和组织决策。

英国心理学家彼得·凯瑟特·瓦森在1960年代首次系统性地提出并研究了确认偏差。在他的实验中，瓦森给参与者一个4卡片谜题，也称为Wason选择任务。这个谜题可以有任何变体，但结果高度可重复。让我们来看一个例子：

桌子上放着四张卡片，每张卡片的一面有一个数字，另一面有一个颜色。规则是*如果一张卡片的一个面显示偶数，则另一面应该是蓝色*。现在，桌子上你看到四张卡片：3、8、蓝色和红色。你必须翻转哪张卡片来测试规则是否成立？

![](../Images/3bde2605913c4d034b5f7f1a3d832c43.png)

Wason选择任务的一个例子：必须翻转哪个卡片来测试规则“如果一张卡片的一个面显示偶数，则另一面是蓝色”？ 图片来源：[维基百科](https://en.wikipedia.org/wiki/Wason_selection_task)

每个人都知道需要翻转8号卡；有些人可能选择8号和蓝色卡。正确的答案是翻转8号卡和红色卡，而大多数人会遗漏翻转红色卡。如果你翻转3号卡，无论另一面是蓝色还是非蓝色都与规则无关。同样地，如果你翻转蓝色卡发现另一面是奇数，这也不重要，因为规则规定另一面可能是偶数。另一方面，如果你翻转红色卡发现另一面是偶数，那么你就证明了规则被违反。

令人惊讶的是，参与者在各种形式的测试中表现一直很差。人们关注规则的积极支持（另一面是蓝色），但忽视了可能会使规则无效的信息（另一面不是蓝色）。

Wason选择任务的规则都很简单，具备逻辑条件和结果：如果P，则Q。要完全验证规则是否有效，需要确认下面列出的两个标准：

1.  如果P，则Q

1.  如果不是Q，则不是P

平均而言，只有10%的Wason选择任务参与者完全正确地选择了第二种选项，这显示了人类大脑自动关注积极证据来确认结论，但很难检查可能反驳规则的证据。

有趣的是，如果我们在难题中加入社会背景，大多数人能够很快回答正确，主要涉及权限或职责。一个流行的例子是这样的：规则是如果你未满 21 岁，你不能喝酒。假设有四张卡片——啤酒、28、可乐、15。你必须翻转哪张卡片以测试规则是否正确？大多数人会凭直觉快速给出正确答案：翻转啤酒卡以查看另一面是否标注年龄超过 21 岁，并翻转 15 卡以确认另一面是否列出了含酒精的饮料。

# **什么导致确认偏差？**

Wason 选择任务的结果意味着什么？科学家们仍在研究哪些神经回路和机制可以解释确认偏差。但我们可以推导出两点：

1.  人脑不是通过符号和标记来解决这种逻辑问题的逻辑运算器。

1.  当人们有社会情境中的规则的先前经验时，偏差可以通过社会背景克服。

参考生物大脑和人工学习中的神经网络学习机制（见文章 [“从生物学习到人工神经网络：接下来是什么？”](https://medium.com/towards-data-science/from-biological-learning-to-artificial-neural-network-whats-next-c8cf0d351af5)），确认偏差可能是神经网络在模式识别中工作的副产品。对于现有的信念或规则，神经网络通过加强输入前提条件的相关神经连接来进行学习。类似的证据会激活相同的网络，得出相同的结论，同时加强神经连接。相反，要验证对立面，网络需要通过不同的输入数据（非 P）进行单独训练，以得出不同的结论（非 Q）。换句话说，网络可能涉及不同的神经连接来进行独立学习。由于建立另一个神经输出以理解对立效果的障碍和努力，人脑倾向于现有的脑电路。

当人们遵守社会规则时，他们知道如果不遵守会受到惩罚或付出一定的代价。这种对立场景被认为是被纳入大脑的电路中，这解释了为什么在解决社会背景中的难题时，人类没有困难看到另一面。换句话说，大脑首先通过实证数据学习了双方的场景。

然而，还有另一种方法可以防止确认偏差，那就是使用超越我们本能大脑能力的工具。这些通常被称为心理工具。对于 Wason 选择任务，这个工具就是简单的逻辑：

1.  如果 P，那么 Q

1.  如果不是 Q，那么不是 P

假设我们将每个规则的前提（P）和结果（Q）插入上述两个场景；无论是否与社会背景相关，我们将100%正确地解开难题。换句话说，使用正负逻辑这样简单的思维工具可以帮助我们清晰地思考，而不会凭直觉忽视对立面。

然而，在现实世界中，实验室之外的规则更复杂和隐含。数据科学和AI可以通过利用相同的原则和策略来帮助人类克服确认偏差。

# **数据科学和AI如何克服确认偏差？**

鉴于生物神经网络和人工神经网络之间的学习相似性，我们不希望AI一开始就重复人类的偏见。虽然人类克服确认偏差很困难，但AI和机器学习有以下优势来克服这些偏差：

1.  模型和算法以预设的方式解释训练数据。因此，它们不会像人类那样有解释或偏好某些问题而忽略相反的事实。

1.  数据科学家和工程师在AI方面的协作工作使其更加客观，与每个人观点中的人类偏见形成对比。

1.  对于AI来说，增加额外的统计和逻辑工具与流程以防止偏差的发生是很灵活的。

另一方面，由于AI和机器学习依赖于人为策划的训练数据，因此需要额外的预防措施以防止将偏差引入模型。因此，我们应关注三个领域来克服确认偏差：训练数据、模型测试和监控，以及结果的可解释性。

1.  **确保训练数据不带有偏见**

自数据科学开始以来，我们的口号之一就是做出“数据驱动的决策”。但正如我们所知，数据可能是不完整的、垃圾的或完全错误的，这是一大危险，可能导致AI做出糟糕或有偏见的决策。使训练数据正确且完整，包括确认和否认的事实，是消除确认偏差的前提。

例如，假设我们构建一个模型来预测订阅者的增长。除了寻找与订阅者相关的特征外，是否探索过与非订阅者相关的特征？是否有些特征同时对订阅和非订阅有所贡献？训练数据是否限制或偏离了我们的预测和决策？

确保训练数据中所有方面的事实得到均等代表是确保 AI 不继承人类偏差的关键步骤之一。由于 AI 模型依赖于人类收集和整理的数据，而人类倾向于有确认偏差，因此在设计训练模型时，确保数据包含正面和负面场景是必须的，以确保模型没有偏差。然而，这通常涉及不同的数据来源，并需要多种数据收集和整理方法。在某些情况下，如果对立数据不存在或收集成本较高，数据合成可能是模拟对比场景的解决方案。

**2\. 通过彻底测试和验证来防止偏差**

机器学习和人工智能已经具备自动测试过程来验证模型。然而，验证的目的通常围绕预测的可重复性、确保模型泛化能力而不出现过拟合，以及从统计分布中移除异常值。防止确认偏差需要额外步骤来验证训练集和模型的行为及输出。例如，模型能否同时确认和反驳一个假设？由于负面案例样本较少，是否存在任何后果或异常？是否由于人类干预认为不重要而导致某些特征被低估？

确认偏差的识别不是一次性的任务。在不断变化的世界中，新的负面或矛盾的事实可能会意外出现。最近的例外情况可能在未来成为新的常态。数据整理中的例外处理程序应定期检查，以确定是否删除了实际的相反案例。此外，应该定期进行审计和测试，以确保在模型发布后没有引入偏差。

**3\. 演示“思考”过程**

从人类经验来看，我们的思维过程和心理模型对做出正确的决定至关重要。我们应该评估和理解 AI 模型如何得出决定或结论。AI 的一个明显优势是它可以让许多数据科学家和工程师同时合作评估其决策过程，而人类只能在个体的思维中单独进行。

然而，神经网络和深度学习模型以难以解释而闻名。鉴于此，可能需要推理过程和混合方法来理解一个决定或结论是否存在偏差：

1.  彻底了解训练数据的来源、处理方式以及如何用于模型。

1.  使用特定过程和可用库（例如，LIME — 局部可解释模型无关解释，SHAP — Shapley 加性解释）提高模型的可解释性。

1.  利用可视化（例如图表）来展示不仅仅是最终结果，还包括从源数据到训练和模型执行的全过程，例如训练数据的质量、支持每个决策的参数实例、输出类别随时间的一致性、过程中的任何异常或离群点等。通过这种方式，数据工程师和数据科学家更容易识别模型可能在哪个步骤出现偏差，以及还需要哪些数据或训练。

# **结论**

在历史上，人类一直擅长发明工具以克服自身的局限性和约束。鉴于人类和AI智能之间的相似性和差异性，我们应关注AI如何补充人类智能的不足，并防止人类的认知偏差。虽然人类难以克服这些偏差，但数据科学和AI可以帮助我们识别并最小化这些偏差，同时使过程更加透明。尽管本文重点讨论了确认偏差，但类似的原则和方法也可以应用于解决其他认知偏差。
