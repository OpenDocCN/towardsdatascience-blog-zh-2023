- en: Quickly Evaluate your RAG Without Manually Labeling Test Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/quickly-evaluate-your-rag-without-manually-labeling-test-data-43ade0ae187a](https://towardsdatascience.com/quickly-evaluate-your-rag-without-manually-labeling-test-data-43ade0ae187a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Automate the evaluation process of your Retrieval Augment Generation apps without
    any manual intervention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ahmedbesbes.medium.com/?source=post_page-----43ade0ae187a--------------------------------)[![Ahmed
    Besbes](../Images/93804d9291439715e578f204b79c9bdd.png)](https://ahmedbesbes.medium.com/?source=post_page-----43ade0ae187a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----43ade0ae187a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----43ade0ae187a--------------------------------)
    [Ahmed Besbes](https://ahmedbesbes.medium.com/?source=post_page-----43ade0ae187a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----43ade0ae187a--------------------------------)
    ¬∑12 min read¬∑Dec 21, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d058a92e9619f69812d1ccfc71bcb537.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by the user
  prefs: []
  type: TYPE_NORMAL
- en: Today‚Äôs topic is evaluating your RAG without manually labeling test data.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the performance of your RAG is something you should care about especially
    if you‚Äôre building such systems and serving them in production.
  prefs: []
  type: TYPE_NORMAL
- en: Besides giving you a rough idea of how your application behaves, evaluating
    your RAG also provides quantitative feedback that guides experimentations and
    the appropriate selection of parameters (LLMs, embedding models, chunk size, top
    K, etc.)
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating your RAG is also important for your client or stakeholders because
    they are ***always*** expecting performance metrics to validate your project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Less teasing, here‚Äôs what this issue covers:'
  prefs: []
  type: TYPE_NORMAL
- en: Automatically generating a synthetic test set from your RAG‚Äôs data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Overview of popular RAG metrics
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Computing RAG metrics on the synthetic dataset using the Ragas package
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*PS: Some sections of this issue are a bit hands-on. They include the necessary
    coding material to implement dataset generation and evaluate the RAG.'
  prefs: []
  type: TYPE_NORMAL
- en: Everything will also be available in this* [*notebook*](https://colab.research.google.com/drive/1Qr3fUfBvJbDLZ0k6al_axlmFrArlm9-u?usp=drive_link)*.*
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs have a look üîé
  prefs: []
  type: TYPE_NORMAL
- en: 1 ‚Äî Generate a synthetic test set üß™
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let‚Äôs say you‚Äôve just built a RAG and now want to evaluate its performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, you need an evaluation dataset that has the following columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**question** (str): to evaluate the RAG on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ground_truths** (list): the reference (i.e. true) answers to the questions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**answer** (str): the answers predicted by the RAG'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**contexts** (list): the list of relevant contexts the RAG used for each question
    to generate the answer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***‚Üí the first two columns represent ground-truth data and the last two columns
    represent the RAG predictions.***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/103ca833aac503a2d0c58bce1f8c30b8.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot by the author
  prefs: []
  type: TYPE_NORMAL
- en: To build such a dataset, we first need to generate tuples of questions and corresponding
    answers.
  prefs: []
  type: TYPE_NORMAL
- en: Then, in the next step, we need to run the RAG over these questions to make
    the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: üëâ Generate questions and ground-truth answers (the theory)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To generate tuples of (question, answer), we first need to prepare the RAG data,
    split it into chunks, and embed it into a vector database.
  prefs: []
  type: TYPE_NORMAL
- en: Once the splits are created and embedded, we will instruct an LLM to generate
    `N_q` questions from `N_c` topics to finally obtain `N_q x N_c` tuples of questions
    and answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate questions and answers from a given context, we need to go through
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Sample a random split and use it as a root context
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fetch K similar contexts from the vector database
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Concatenate the texts of the root context with its K neighbors to build a larger
    context
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the large `context` and `num_questions` in the following prompt template
    to generate questions and answers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Then, repeat steps 1 to 4 `N_c` times to vary the context each time and generate
    different questions
  prefs: []
  type: TYPE_NORMAL
- en: I‚Äôve used this workflow to produce questions and answers on Python programming
    and here‚Äôs a sample of the results I got.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: üëâ Now the coding part üíª
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs first start by building a vectorstore that includes the data used by the
    RAG.
  prefs: []
  type: TYPE_NORMAL
- en: We can load that from Wikipedia (but if you have any interesting PDF on Python
    programming, you can load that too)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: After loading the data, we split it into chunks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create an index in Pinecone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: and use a LangChain wrapper to index the splits‚Äô embeddings in it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now comes the interesting part: generating the synthetic dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: To do that, we initialize an object from a `TestsetGenerator` class using an
    LLM, the document splits, an embedding model, and the name of the Pinecone index.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we call the `generate` method by passing two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces this following dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Pretty simple, right?
  prefs: []
  type: TYPE_NORMAL
- en: '**If you‚Äôre interested in the implementation details, you can find them in
    the** [**notebook**](https://colab.research.google.com/drive/1Qr3fUfBvJbDLZ0k6al_axlmFrArlm9-u?usp=drive_link)**.**'
  prefs: []
  type: TYPE_NORMAL
- en: We‚Äôll only be halfway through it. Now, we need to use the RAG to predict the
    answers for each question and provide the context list used to ground the response.
  prefs: []
  type: TYPE_NORMAL
- en: 'The RAG is defined in a RAG class, so let‚Äôs initialize it first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Then, we iterate over the synthetic dataset by calling the `predict` method
    over each question and collect the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'and here‚Äôs what the final result looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: üèÜ Congratulations if you‚Äôve made it this far, now you‚Äôre ready to evaluate your
    RAG.
  prefs: []
  type: TYPE_NORMAL
- en: 2 ‚Äî Popular RAG metrics üìä
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before jumping into the code, let‚Äôs cover the **four** basic metrics we‚Äôll use
    to evaluate our RAG.
  prefs: []
  type: TYPE_NORMAL
- en: Each metric examines a different facet. Therefore, when evaluating your application,
    it is crucial to consider multiple metrics for a comprehensive perspective.
  prefs: []
  type: TYPE_NORMAL
- en: '**1 ‚Äî Answer Relevancy**:'
  prefs: []
  type: TYPE_NORMAL
- en: The answer relevance metric is designed to **evaluate the pertinence of the
    generated answer with the provided prompt**. Answers that lack completeness or
    include redundant information receive lower scores. This metric utilizes both
    the question and the answer, generating values between 0 and 1\. Higher scores
    signify better relevance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**'
  prefs: []
  type: TYPE_NORMAL
- en: '‚ùì*Question: What are the key features of a healthy diet?*'
  prefs: []
  type: TYPE_NORMAL
- en: '‚¨áÔ∏è *Low relevance answer*: A healthy diet is important for overall well-being.'
  prefs: []
  type: TYPE_NORMAL
- en: '‚¨ÜÔ∏è *High relevance answer*: A healthy diet should include a variety of fruits,
    vegetables, whole grains, lean proteins, and dairy products, providing essential
    nutrients for optimal health.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2 ‚Äî Faithfulness**'
  prefs: []
  type: TYPE_NORMAL
- en: This metric evaluates **the factual consistency of the generated answer within
    the provided context**. The calculation involves both the answer and the retrieved
    context, with the answer scaled to a range between 0 and 1, where higher values
    indicate better faithfulness.
  prefs: []
  type: TYPE_NORMAL
- en: For an answer to be deemed faithful, all claims made in the response must be
    inferable from the given context.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example:**'
  prefs: []
  type: TYPE_NORMAL
- en: '‚ùì*Question: What are the main accomplishments of Marie Curie?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'üìë *Context*: Marie Curie (1867‚Äì1934) was a pioneering physicist and chemist,
    the first woman to win a Nobel Prize and the only woman to win Nobel Prizes in
    two different fields.'
  prefs: []
  type: TYPE_NORMAL
- en: '‚¨ÜÔ∏è *High faithfulness answer*: Marie Curie won Nobel Prizes in both physics
    and chemistry, making her the first woman to achieve this feat.'
  prefs: []
  type: TYPE_NORMAL
- en: ‚¨áÔ∏è *Low faithfulness answer:* Marie Curie won Nobel Prizes only in physics.
  prefs: []
  type: TYPE_NORMAL
- en: '**3 ‚Äî Context precision**'
  prefs: []
  type: TYPE_NORMAL
- en: Context Precision is a metric that evaluates whether all of the ground-truth
    relevant items present in the `contexts` are ranked higher or not. Ideally, all
    the relevant chunks must appear at the top ranks. This metric is computed using
    the `question` and the `contexts`, with values ranging between 0 and 1, where
    higher scores indicate better precision.
  prefs: []
  type: TYPE_NORMAL
- en: '**4 ‚Äî Answer correctness**'
  prefs: []
  type: TYPE_NORMAL
- en: This metric measures the accuracy of the generated answer in comparison to the
    ground truth. This assessment utilizes both the ground truth and the answer, assigning
    scores within the 0 to 1 range. A higher score indicates a more accurate alignment
    between the generated answer and the ground truth, indicative of superior correctness.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'üü¢ *Ground Truth: The Eiffel Tower was completed in 1889 in Paris, France.*'
  prefs: []
  type: TYPE_NORMAL
- en: '‚¨ÜÔ∏è *High answer correctness*: The construction of the Eiffel Tower concluded
    in 1889 in Paris, France.'
  prefs: []
  type: TYPE_NORMAL
- en: '‚¨áÔ∏è *Low answer correctness*: Completed in 1889, the Eiffel Tower stands in
    London, England.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 ‚Äî Evaluate RAGs with RAGAS üìè
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To evaluate the RAG and compute the four metrics, we can use [Ragas](https://github.com/explodinggradients/ragas).
  prefs: []
  type: TYPE_NORMAL
- en: Ragas (for Rag Assessment), is a framework that helps you evaluate your Retrieval
    Augmented Generation (RAG) pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: It also provides a bunch of [metrics](https://docs.ragas.io/en/latest/concepts/metrics/index.html)
    and utility [functions](https://docs.ragas.io/en/latest/concepts/testset_generation.html)
    to generate synthetic datasets.
  prefs: []
  type: TYPE_NORMAL
- en: To run Ragas over our dataset, you first need to import the metrics and convert
    the dataframe of synthetic data to a Dataset object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Then, we need to configure Ragas to use VertexAI LLMs and embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: This step is important since Ragas is configured to use OpenAI by default.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we call the `evaluate` function on the synthetic dataset and specifcy
    the metrics we want to compute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Once the evaluation is done, you can print the result directly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: or you can convert it to a dataframe to inspect each of these metrics for every
    question.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generating a synthetic dataset to evaluate your RAG is a good start, especially
    when you don‚Äôt have access to labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: However, this solution also comes with its problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the generated answers:'
  prefs: []
  type: TYPE_NORMAL
- en: May lack diversity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are redundant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are plain rephrasing of the original text and need more complexity to reflect
    real questions that need reasoning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be too generic (especially in very technical domains)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To tackle these issues, you can adjust and tune your prompts, filter irrelevant
    questions, create synthetic questions on specific topics, and use Ragas for dataset
    generation.
  prefs: []
  type: TYPE_NORMAL
