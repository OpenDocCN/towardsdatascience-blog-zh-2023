- en: Audio Classification with Deep Learning in Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/audio-classification-with-deep-learning-in-python-cf752b22ba07](https://towardsdatascience.com/audio-classification-with-deep-learning-in-python-cf752b22ba07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[The Kaggle Blueprints](/the-kaggle-blueprints-unlocking-winning-approaches-to-data-science-competitions-24d7416ef5fd)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fine-tuning image models to tackle domain shift and class imbalance with PyTorch
    and torchaudio in audio data
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@iamleonie?source=post_page-----cf752b22ba07--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----cf752b22ba07--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cf752b22ba07--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cf752b22ba07--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----cf752b22ba07--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cf752b22ba07--------------------------------)
    ·10 min read·Apr 4, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5658cabbf5faa49cd94151fa8d262288.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: Classifying bird calls in soundscapes with Machine Learning (Image drawn by
    the author)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to another edition of “[The Kaggle Blueprints](/the-kaggle-blueprints-unlocking-winning-approaches-to-data-science-competitions-24d7416ef5fd)”,
    where we will analyze [Kaggle](https://www.kaggle.com/) competitions’ winning
    solutions for lessons we can apply to our own data science projects.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: This edition will review the techniques and approaches from the [“BirdCLEF 2022”](https://www.kaggle.com/competitions/birdclef-2022)
    competition, which ended in May 2022.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem Statement: Audio Classification with Domain Shift'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The objective of the [“BirdCLEF 2022”](https://www.kaggle.com/competitions/birdclef-2022)
    competition was to identify Hawaiian bird species by sound. The competitors were
    given short audio files of single bird calls and were asked to predict whether
    a specific bird was present in a longer recording.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.kaggle.com/competitions/birdclef-2022/?source=post_page-----cf752b22ba07--------------------------------)
    [## BirdCLEF 2022'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Identify bird calls in soundscapes
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.kaggle.com](https://www.kaggle.com/competitions/birdclef-2022/?source=post_page-----cf752b22ba07--------------------------------)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to a vanilla audio classification problem, this competition added
    flavor with the following challenges:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '**Domain shift** — The training data consisted of clean audio recordings of
    a single bird call separated from any additional sounds (a few seconds, different
    lengths). However, the test data consisted of “unclean” longer (1 minute) recordings
    taken “in the wild” and contained different sounds other than bird calls (e.g.,
    wind, rain, other animals, etc.).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/704679818fe7a7fed4bc9e911776597f.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: Domain shift in audio data
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '**Class imbalance/Few-shot learning** —As some birds are less common than others,
    we are dealing with a long-tailed class distribution where some birds only have
    one sample.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/77ac7d7f41edd2c292be144740016d79.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: Long-tailed class distribution
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '*Insert your data here!* — To follow along in this article, your dataset should
    look something like this:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d498603d50162c179f1679b9a89ea6e.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: 'Insert your data here: How your audio dataset dataframe should be formatted'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Approaching Audio Classification as an Image Classification Problem with Deep
    Learning
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A popular approach among competitors to this audio classification problem was
    to:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[Converting the audio classification problem to an image classification problem](#e2c2)
    by converting the audio from waveform to a Mel spectrogram and applying a Deep
    Learning model'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Applying data augmentations to the audio data in waveform and in spectrograms](#8f8b)
    to tackle the domain shift and class imbalance'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Finetune a pre-trained image classification model](#daf1) to tackle class
    imbalance'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This article will use PyTorch (version 1.13.0) for the Deep Learning framework
    and `[torchaudio](https://pytorch.org/audio/stable/index.html)` (version 0.13.0)
    and `[librosa](https://librosa.org/doc/main/index.html)` (version 0.10.0) for
    audio processing. Additionally, we will be using `[timm](https://timm.fast.ai/)`
    (version 0.6.12) for fine-tuning with pre-trained image models.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Preparations: Getting Familiar with Audio Data'
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before getting started with solving an audio classification problem, let’s first
    get familiar with working with audio data. You can load the audio and its sampling
    rate from different file formats (e.g., .wav, .ogg, etc.) with the `.load()` method
    from the `torchaudio` library or the `librosa` library.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you want to listen to the loaded audio directly in a Jupyter notebook for
    explorations, the following code will provide you with an audio player.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/2bf7e7fca69596cc56af095a4ab4a080.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: Displaying audio player for loaded data in Jupyter notebook
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: The `[librosa](https://librosa.org/doc/main/index.html)` library also provides
    various methods to display the audio data for exploration purposes quickly. If
    you used `[torchaudio](https://pytorch.org/audio/stable/index.html)` to load the
    audio file, make sure to convert the tensors to NumPy arrays.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/5256d3c24fab2123ee1348e4c2b25ae3.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: Original audio data of the word “stop” in waveform from the “Speech Commands”
    dataset [0]
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Convert the Audio Classification Problem to an Image Classification
    Problem'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A popular method to model audio data with a Deep Learning model is to convert
    the *“computer hearing”* problem to a *computer vision* problem [2]. Specifically,
    the waveform audio is converted to a Mel spectrogram (which is a type of image)
    as shown below.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ee81e347b6f6c708e569e49da5e7cbf.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: Converting an audio file from waveform (time domain) to Mel spectrogram (frequency
    domain)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Usually, you would use a Fast Fourier Transform (FFT) to computationally convert
    an audio signal from the time domain (waveform) to the frequency domain (spectrogram).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: However, the FFT will give you the overall frequency components for the entire
    time series of the audio signal as a whole. Thus, you are losing the time information
    when converting audio data from the time domain to the frequency domain.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Instead of the FFT, you can use the Short-Time Fourier Transform (STFT) to preserve
    the time information. The STFT is a variant of the FFT that breaks up the audio
    signal into smaller sections by using a sliding time window. It takes the FFT
    on each section and then combines them.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '`n_fft` —length of the sliding window (default: 2048)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hop_length` — number of samples by which to slide the window (default: 512).
    The `hop_length`will directly impact the resulting image size. If your audio data
    has a fixed length and you want to convert the waveform to a fixed image size,
    you can set `hop_length = audio_length // (image_size[1] — 1)`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/f4ee73d6d2965c96bfb5d8b8b97df960.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: Short-Time Fourier Transform (STFT)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will convert the amplitude to decibels and bin the frequencies according
    to the Mel scale. For this purpose, `n_mels` is the number of frequency bands
    (Mel bins). This will be the height of the resulting spectrogram.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/82405a51d8414924699fa9b74e0da69a.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: Convert amplitude to decibels and apply Mel binning to the spectrum
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'For an in-depth explanation of the Mel spectrogram, I recommend this article:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53?source=post_page-----cf752b22ba07--------------------------------)
    [## Understanding the Mel Spectrogram'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: (and Other Topics in Signal Processing)
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53?source=post_page-----cf752b22ba07--------------------------------)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Below you can see an example PyTorch `Dataset` which loads an audio file and
    converts the waveform to a Mel spectrogram after some preprocessing steps.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Your resulting dataset should produce samples that look something like this
    before we feed it to the neural network:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0c54b387ffda9de0c7cb8c156863f58.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: Sample structure from the Audio Dataset
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Apply Augmentations to Audio Data'
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One technique to tackle this competition’s challenges of domain shift and class
    imbalance was to apply data augmentations to the training data [5, 8, 10, 11].
    You can apply data augmentations for audio data in the waveform and the spectrogram.
    The `[torchaudio](https://pytorch.org/audio/stable/index.html)` library already
    provides a lot of different data augmentations for audio data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 针对本次竞赛的领域转换和类别不平衡挑战，采用了对训练数据应用数据增强的技术 [5, 8, 10, 11]。你可以对波形和谱图的音频数据应用数据增强。`[torchaudio](https://pytorch.org/audio/stable/index.html)`
    库已经提供了许多不同的音频数据增强方法。
- en: 'Popular data augmentation techniques for audio data in **waveform (time domain)**
    are:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在**波形（时间域）**中流行的音频数据增强技术有：
- en: Noise injection like white noise, colored noise, or background noise (`[AddNoise](https://pytorch.org/audio/stable/generated/torchaudio.transforms.AddNoise.html#torchaudio.transforms.AddNoise)`)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 噪声注入，如白噪声、有色噪声或背景噪声 (`[AddNoise](https://pytorch.org/audio/stable/generated/torchaudio.transforms.AddNoise.html#torchaudio.transforms.AddNoise)`)
- en: Shifting time
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间转换
- en: Changing speed (`[Speed](https://pytorch.org/audio/stable/generated/torchaudio.transforms.Speed.html#torchaudio.transforms.Speed)`;
    alternatively use `[TimeStretch](https://pytorch.org/audio/stable/generated/torchaudio.transforms.TimeStretch.html#torchaudio.transforms.TimeStretch)`
    in frequency domain)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变速度 (`[Speed](https://pytorch.org/audio/stable/generated/torchaudio.transforms.Speed.html#torchaudio.transforms.Speed)`；或者使用
    `[TimeStretch](https://pytorch.org/audio/stable/generated/torchaudio.transforms.TimeStretch.html#torchaudio.transforms.TimeStretch)`
    频率域)
- en: Changing pitch (`[PitchShift](https://pytorch.org/audio/stable/generated/torchaudio.transforms.PitchShift.html#torchaudio.transforms.PitchShift)`)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变音高 (`[PitchShift](https://pytorch.org/audio/stable/generated/torchaudio.transforms.PitchShift.html#torchaudio.transforms.PitchShift)`)
- en: '![](../Images/2c9d74a61d35695672531f166551ec88.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c9d74a61d35695672531f166551ec88.png)'
- en: 'Overview of different data augmentation techniques for audio in waveform: Noise
    injection (white noise, colored noise, background noise), shifting time, changing
    speed and pitch'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 不同音频增强技术概述：噪声注入（白噪声、有色噪声、背景噪声）、时间转换、速度和音高变化
- en: 'Popular data augmentation techniques for audio data in the **spectrogram (frequency
    domain)** are:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在**谱图（频率域）**中流行的音频数据增强技术有：
- en: Popular image augmentation techniques like Mixup [13] or Cutmix [3]
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像 Mixup [13] 或 Cutmix [3] 这样的流行图像增强技术
- en: '![](../Images/d0193c5f5d8b516c9d6b5cff68fde6ad.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d0193c5f5d8b516c9d6b5cff68fde6ad.png)'
- en: 'Data Augmentation for Spectrogram: Mixup [13]'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 用于谱图的数据增强：Mixup [13]
- en: SpecAugment [7] (`[FrequencyMasking](https://pytorch.org/audio/stable/generated/torchaudio.transforms.FrequencyMasking.html#torchaudio.transforms.FrequencyMasking)`
    and `[TimeMasking](https://pytorch.org/audio/stable/generated/torchaudio.transforms.TimeMasking.html#torchaudio.transforms.TimeMasking)`)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SpecAugment [7] (`[FrequencyMasking](https://pytorch.org/audio/stable/generated/torchaudio.transforms.FrequencyMasking.html#torchaudio.transforms.FrequencyMasking)`
    和 `[TimeMasking](https://pytorch.org/audio/stable/generated/torchaudio.transforms.TimeMasking.html#torchaudio.transforms.TimeMasking)`)
- en: '![](../Images/23daa65bbcea6fa1c6a6d52efe3baffe.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23daa65bbcea6fa1c6a6d52efe3baffe.png)'
- en: 'Data Augmentation for Spectrogram: SpecAugment [7]'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 用于谱图的数据增强：SpecAugment [7]
- en: As you can see while providing a lot of audio augmentations, `[torchaudio](https://pytorch.org/audio/stable/index.html)`
    doesn’t provide all of the proposed data augmentations.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，虽然提供了很多音频增强方法，`[torchaudio](https://pytorch.org/audio/stable/index.html)`
    并未提供所有提议的数据增强方法。
- en: 'Thus, if you want to inject a specific type of noise, shift the time, or apply
    Mixup [13] or Cutmix [12] augmentations, you must [write a custom data augmentation
    in PyTorch](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#transforms).
    You can reference this [collection of audio data augmentation techniques](/data-augmentation-techniques-for-audio-data-in-python-15505483c63c)
    for their implementations:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你想注入特定类型的噪声、时间转换，或应用 Mixup [13] 或 Cutmix [12] 数据增强，你必须 [编写自定义数据增强代码（PyTorch）](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#transforms)。你可以参考这个
    [音频数据增强技术集合](/data-augmentation-techniques-for-audio-data-in-python-15505483c63c)
    以了解它们的实现：
- en: '[](/data-augmentation-techniques-for-audio-data-in-python-15505483c63c?source=post_page-----cf752b22ba07--------------------------------)
    [## Data Augmentation Techniques for Audio Data in Python'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/data-augmentation-techniques-for-audio-data-in-python-15505483c63c?source=post_page-----cf752b22ba07--------------------------------)
    [## Python中的音频数据增强技术'
- en: How to augment audio in waveform (time domain) and as spectrograms (frequency
    domain) with librosa, numpy, and PyTorch
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何用 librosa、numpy 和 PyTorch 对波形（时间域）和谱图（频率域）进行音频增强
- en: towardsdatascience.com](/data-augmentation-techniques-for-audio-data-in-python-15505483c63c?source=post_page-----cf752b22ba07--------------------------------)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'In the example PyTorch `Dataset` class from before, you can apply the data
    augmentations as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Step 3: Fine-tune a Pretrained Image Classification Model for Few-Shot Learning'
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this competition, we are dealing with a class imbalance. As some classes
    only have one sample, we are dealing with a few-shot learning problem. Nakamura
    and Harada [6] showed in 2019 that fine-tuning could be an effective approach
    to few-shot learning.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: A lot of competitors [2, 5, 8, 10, 11] fine-tuned common pre-trained image classification
    models such as
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: EfficientNet (e.g., `tf_efficientnet_b3_ns`) [9],
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SE-ResNext (e.g., `se_resnext50_32x4d`) [3],
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NFNet (e.g., `eca_nfnet_l0`) [1]
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can load any pre-trained image classification model with the `[timm](https://timm.fast.ai/)`
    library for fine-tuning. Make sure to set `in_chans = 1` as we are not working
    with 3-channel images but 1-channel Mel spectrograms.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Other competitors reported successes from fine-tuning models pre-trained on
    similar audio classification problems [4, 10].
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning is done with a cosine annealing learning rate scheduler (`[CosineAnnealingLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR)`)
    for a few epochs [2, 8].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/9ae13b38a3e843bc46c227a590baa3f2.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: PyTorch Cosine Annealing / Decay Learning Rate Scheduler (Image by the author,
    originally published in [“A Visual Guide to Learning Rate Schedulers in PyTorch”](https://medium.com/towards-data-science/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863#fad1))
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find more tips and best practices in this [guide for fine-tuning Deep
    Learning models](/intermediate-deep-learning-with-transfer-learning-f1aba5a814f):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[](/intermediate-deep-learning-with-transfer-learning-f1aba5a814f?source=post_page-----cf752b22ba07--------------------------------)
    [## Intermediate Deep Learning with Transfer Learning'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: A practical guide for fine-tuning Deep Learning models for computer vision and
    natural language processing
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/intermediate-deep-learning-with-transfer-learning-f1aba5a814f?source=post_page-----cf752b22ba07--------------------------------)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many more lessons to be learned from reviewing the learning resources
    Kagglers have created during the course of the [“BirdCLEF 2022”](https://www.kaggle.com/competitions/birdclef-2022)
    competition. There are also many different solutions for this type of problem
    statement.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we focused on the general approach that was popular among
    many competitors:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[Converting the audio classification problem to an image classification problem](#e2c2)
    by converting the audio from waveform to a Mel spectrogram and applying a Deep
    Learning model'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Applying data augmentations to the audio data in waveform and in spectrograms](#8f8b)
    to tackle the domain shift and class imbalance'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Finetune a pre-trained image classification model](#daf1) to tackle class
    imbalance'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enjoyed This Story?
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Subscribe for free*](https://medium.com/subscribe/@iamleonie) *to get notified
    when I publish a new story.*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----cf752b22ba07--------------------------------)
    [## Get an email whenever Leonie Monigatti publishes.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Get an email whenever Leonie Monigatti publishes. By signing up, you will create
    a Medium account if you don’t already…
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----cf752b22ba07--------------------------------)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '*Find me on* [*LinkedIn*](https://www.linkedin.com/in/804250ab/),[*Twitter*](https://twitter.com/helloiamleonie)*,
    and* [*Kaggle*](https://www.kaggle.com/iamleonie)*!*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dataset
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the original competition data does not allow commercial use, examples are
    done with the following dataset.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[0] Warden P. Speech Commands: A public dataset for single-word speech recognition,
    2017\. Available from [http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz](http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'License: CC-BY-4.0'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Image References
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If not otherwise stated, all images are created by the author.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Web & Literature
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Brock, A., De, S., Smith, S. L., & Simonyan, K. (2021, July). High-performance
    large-scale image recognition without normalization. In *International Conference
    on Machine Learning* (pp. 1059–1071). PMLR.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Chai Time Data Science](https://www.youtube.com/@ChaiTimeDataScience)
    (2022). [BirdCLEF 2022: 11th Pos Gold Solution | Gilles Vandewiele](https://www.youtube.com/watch?v=MXZKNnuoQXw)
    (accessed March 13th, 2023)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Hu, J., Shen, L., & Sun, G. (2018). Squeeze-and-excitation networks. In
    *Proceedings of the IEEE conference on computer vision and pattern recognition*
    (pp. 7132–7141).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Kramarenko Vladislav (2022). [4th place](https://www.kaggle.com/competitions/birdclef-2022/discussion/326987)
    in Kaggle Discussions (accessed March 13th, 2023)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[5] LeonShangguan (2022). [[Public #1 Private #2] + [Private #7/8 (potential)]
    solutions. The host wins.](https://www.kaggle.com/competitions/birdclef-2022/discussion/326950)
    in Kaggle Discussions (accessed March 13th, 2023)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Nakamura, A., & Harada, T. (2019). Revisiting fine-tuning for few-shot
    learning. *arXiv preprint arXiv:1910.00216*.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Park, D. S., Chan, W., Zhang, Y., Chiu, C. C., Zoph, B., Cubuk, E. D.,
    & Le, Q. V. (2019). Specaugment: A simple data augmentation method for automatic
    speech recognition. *arXiv preprint arXiv:1904.08779*.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[8] slime (2022). [3rd place solution](https://www.kaggle.com/competitions/birdclef-2022/discussion/327193)
    in Kaggle Discussions (accessed March 13th, 2023)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Tan, M., & Le, Q. (2019, May). Efficientnet: Rethinking model scaling for
    convolutional neural networks. In *International conference on machine learning*
    (pp. 6105–6114). PMLR.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Volodymyr (2022). [1st place solution models (it’s not all BirdNet)](https://www.kaggle.com/competitions/birdclef-2022/discussion/327047)
    in Kaggle Discussions (accessed March 13th, 2023)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[11] yokuyama (2022). [5th place solution](https://www.kaggle.com/competitions/birdclef-2022/discussion/327044)
    in Kaggle Discussions (accessed March 13th, 2023)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[12] Yun, S., Han, D., Oh, S. J., Chun, S., Choe, J., & Yoo, Y. (2019). Cutmix:
    Regularization strategy to train strong classifiers with localizable features.
    In *Proceedings of the IEEE/CVF international conference on computer vision* (pp.
    6023–6032).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[13] Zhang, H., Cisse, M., Dauphin, Y. N., & Lopez-Paz, D. (2017) mixup: Beyond
    empirical risk minimization. arXiv preprint arXiv:1710.09412.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
