- en: Audio Classification with Deep Learning in Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Python进行深度学习的音频分类
- en: 原文：[https://towardsdatascience.com/audio-classification-with-deep-learning-in-python-cf752b22ba07](https://towardsdatascience.com/audio-classification-with-deep-learning-in-python-cf752b22ba07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/audio-classification-with-deep-learning-in-python-cf752b22ba07](https://towardsdatascience.com/audio-classification-with-deep-learning-in-python-cf752b22ba07)
- en: '[The Kaggle Blueprints](/the-kaggle-blueprints-unlocking-winning-approaches-to-data-science-competitions-24d7416ef5fd)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[Kaggle 蓝图](/the-kaggle-blueprints-unlocking-winning-approaches-to-data-science-competitions-24d7416ef5fd)'
- en: Fine-tuning image models to tackle domain shift and class imbalance with PyTorch
    and torchaudio in audio data
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整图像模型以应对领域迁移和类别不平衡，使用PyTorch和torchaudio处理音频数据
- en: '[](https://medium.com/@iamleonie?source=post_page-----cf752b22ba07--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----cf752b22ba07--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cf752b22ba07--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cf752b22ba07--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----cf752b22ba07--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@iamleonie?source=post_page-----cf752b22ba07--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----cf752b22ba07--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cf752b22ba07--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cf752b22ba07--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----cf752b22ba07--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cf752b22ba07--------------------------------)
    ·10 min read·Apr 4, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----cf752b22ba07--------------------------------)
    ·阅读时间10分钟·2023年4月4日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5658cabbf5faa49cd94151fa8d262288.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5658cabbf5faa49cd94151fa8d262288.png)'
- en: Classifying bird calls in soundscapes with Machine Learning (Image drawn by
    the author)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用机器学习对声音景观中的鸟鸣进行分类（图片由作者绘制）
- en: Welcome to another edition of “[The Kaggle Blueprints](/the-kaggle-blueprints-unlocking-winning-approaches-to-data-science-competitions-24d7416ef5fd)”,
    where we will analyze [Kaggle](https://www.kaggle.com/) competitions’ winning
    solutions for lessons we can apply to our own data science projects.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到另一期的“[Kaggle 蓝图](/the-kaggle-blueprints-unlocking-winning-approaches-to-data-science-competitions-24d7416ef5fd)”，在这里我们将分析[Kaggle](https://www.kaggle.com/)比赛的获胜解决方案，以寻找可以应用于我们自己数据科学项目的经验教训。
- en: This edition will review the techniques and approaches from the [“BirdCLEF 2022”](https://www.kaggle.com/competitions/birdclef-2022)
    competition, which ended in May 2022.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本期将回顾[“BirdCLEF 2022”](https://www.kaggle.com/competitions/birdclef-2022)比赛中的技术和方法，该比赛于2022年5月结束。
- en: 'Problem Statement: Audio Classification with Domain Shift'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题陈述：具有领域迁移的音频分类
- en: The objective of the [“BirdCLEF 2022”](https://www.kaggle.com/competitions/birdclef-2022)
    competition was to identify Hawaiian bird species by sound. The competitors were
    given short audio files of single bird calls and were asked to predict whether
    a specific bird was present in a longer recording.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[“BirdCLEF 2022”](https://www.kaggle.com/competitions/birdclef-2022)比赛的目标是通过声音识别夏威夷鸟类。参赛者提供了单个鸟鸣的短音频文件，并要求预测特定鸟类是否出现在更长的录音中。'
- en: '[](https://www.kaggle.com/competitions/birdclef-2022/?source=post_page-----cf752b22ba07--------------------------------)
    [## BirdCLEF 2022'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.kaggle.com/competitions/birdclef-2022/?source=post_page-----cf752b22ba07--------------------------------)
    [## BirdCLEF 2022'
- en: Identify bird calls in soundscapes
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在声音景观中识别鸟鸣
- en: www.kaggle.com](https://www.kaggle.com/competitions/birdclef-2022/?source=post_page-----cf752b22ba07--------------------------------)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: www.kaggle.com](https://www.kaggle.com/competitions/birdclef-2022/?source=post_page-----cf752b22ba07--------------------------------)
- en: 'In contrast to a vanilla audio classification problem, this competition added
    flavor with the following challenges:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与普通的音频分类问题相比，此次比赛增加了以下挑战：
- en: '**Domain shift** — The training data consisted of clean audio recordings of
    a single bird call separated from any additional sounds (a few seconds, different
    lengths). However, the test data consisted of “unclean” longer (1 minute) recordings
    taken “in the wild” and contained different sounds other than bird calls (e.g.,
    wind, rain, other animals, etc.).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域偏移** — 训练数据由单一鸟类叫声的干净音频录音组成，且没有其他声音（几秒钟，长度不同）。然而，测试数据则由“在野外”录制的较长（1分钟）“不干净”录音组成，并包含了除了鸟叫声之外的其他声音（例如风、雨、其他动物等）。'
- en: '![](../Images/704679818fe7a7fed4bc9e911776597f.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/704679818fe7a7fed4bc9e911776597f.png)'
- en: Domain shift in audio data
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 音频数据中的领域偏移
- en: '**Class imbalance/Few-shot learning** —As some birds are less common than others,
    we are dealing with a long-tailed class distribution where some birds only have
    one sample.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别不平衡/少样本学习** — 由于某些鸟类比其他鸟类更为稀有，我们面临长尾类别分布的问题，其中一些鸟类只有一个样本。'
- en: '![](../Images/77ac7d7f41edd2c292be144740016d79.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77ac7d7f41edd2c292be144740016d79.png)'
- en: Long-tailed class distribution
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 长尾类别分布
- en: '*Insert your data here!* — To follow along in this article, your dataset should
    look something like this:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*在这里插入你的数据！* — 为了跟随本文，你的数据集应如下所示：'
- en: '![](../Images/5d498603d50162c179f1679b9a89ea6e.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d498603d50162c179f1679b9a89ea6e.png)'
- en: 'Insert your data here: How your audio dataset dataframe should be formatted'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里插入你的数据：你的音频数据集数据框应该如何格式化
- en: Approaching Audio Classification as an Image Classification Problem with Deep
    Learning
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将音频分类视为图像分类问题，并使用深度学习方法
- en: 'A popular approach among competitors to this audio classification problem was
    to:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 竞争对手们在解决这个音频分类问题时，通常会采用以下方法：
- en: '[Converting the audio classification problem to an image classification problem](#e2c2)
    by converting the audio from waveform to a Mel spectrogram and applying a Deep
    Learning model'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[将音频分类问题转换为图像分类问题](#e2c2)，方法是将音频从波形转换为梅尔谱图，并应用深度学习模型'
- en: '[Applying data augmentations to the audio data in waveform and in spectrograms](#8f8b)
    to tackle the domain shift and class imbalance'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[对波形和谱图中的音频数据应用数据增强](#8f8b)，以应对领域偏移和类别不平衡'
- en: '[Finetune a pre-trained image classification model](#daf1) to tackle class
    imbalance'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[微调预训练图像分类模型](#daf1)，以应对类别不平衡'
- en: This article will use PyTorch (version 1.13.0) for the Deep Learning framework
    and `[torchaudio](https://pytorch.org/audio/stable/index.html)` (version 0.13.0)
    and `[librosa](https://librosa.org/doc/main/index.html)` (version 0.10.0) for
    audio processing. Additionally, we will be using `[timm](https://timm.fast.ai/)`
    (version 0.6.12) for fine-tuning with pre-trained image models.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将使用 PyTorch（版本 1.13.0）作为深度学习框架，以及 `[torchaudio](https://pytorch.org/audio/stable/index.html)`（版本
    0.13.0）和 `[librosa](https://librosa.org/doc/main/index.html)`（版本 0.10.0）进行音频处理。此外，我们还将使用
    `[timm](https://timm.fast.ai/)`（版本 0.6.12）进行预训练图像模型的微调。
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Preparations: Getting Familiar with Audio Data'
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作：熟悉音频数据
- en: Before getting started with solving an audio classification problem, let’s first
    get familiar with working with audio data. You can load the audio and its sampling
    rate from different file formats (e.g., .wav, .ogg, etc.) with the `.load()` method
    from the `torchaudio` library or the `librosa` library.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始解决音频分类问题之前，让我们首先熟悉处理音频数据。你可以使用 `torchaudio` 库或 `librosa` 库中的 `.load()` 方法从不同的文件格式（例如
    .wav、.ogg 等）加载音频及其采样率。
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you want to listen to the loaded audio directly in a Jupyter notebook for
    explorations, the following code will provide you with an audio player.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想直接在 Jupyter notebook 中收听加载的音频进行探索，可以使用以下代码提供音频播放器。
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/2bf7e7fca69596cc56af095a4ab4a080.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bf7e7fca69596cc56af095a4ab4a080.png)'
- en: Displaying audio player for loaded data in Jupyter notebook
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jupyter notebook 中显示加载数据的音频播放器
- en: The `[librosa](https://librosa.org/doc/main/index.html)` library also provides
    various methods to display the audio data for exploration purposes quickly. If
    you used `[torchaudio](https://pytorch.org/audio/stable/index.html)` to load the
    audio file, make sure to convert the tensors to NumPy arrays.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`[librosa](https://librosa.org/doc/main/index.html)` 库还提供了各种方法来快速显示音频数据以供探索。如果你使用
    `[torchaudio](https://pytorch.org/audio/stable/index.html)` 加载音频文件，请确保将张量转换为 NumPy
    数组。'
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/5256d3c24fab2123ee1348e4c2b25ae3.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5256d3c24fab2123ee1348e4c2b25ae3.png)'
- en: Original audio data of the word “stop” in waveform from the “Speech Commands”
    dataset [0]
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: “Speech Commands” 数据集中的单词“stop”原始音频数据（波形）[0]
- en: 'Step 1: Convert the Audio Classification Problem to an Image Classification
    Problem'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 1：将音频分类问题转换为图像分类问题
- en: A popular method to model audio data with a Deep Learning model is to convert
    the *“computer hearing”* problem to a *computer vision* problem [2]. Specifically,
    the waveform audio is converted to a Mel spectrogram (which is a type of image)
    as shown below.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一种使用深度学习模型对音频数据建模的流行方法是将 *“计算机听觉”* 问题转化为 *计算机视觉* 问题 [2]。具体来说，波形音频被转换为 Mel 频谱图（这是一种图像），如下所示。
- en: '![](../Images/6ee81e347b6f6c708e569e49da5e7cbf.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ee81e347b6f6c708e569e49da5e7cbf.png)'
- en: Converting an audio file from waveform (time domain) to Mel spectrogram (frequency
    domain)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 将音频文件从波形（时域）转换为 Mel 频谱图（频域）
- en: Usually, you would use a Fast Fourier Transform (FFT) to computationally convert
    an audio signal from the time domain (waveform) to the frequency domain (spectrogram).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你会使用快速傅里叶变换（FFT）来计算性地将音频信号从时域（波形）转换到频域（频谱图）。
- en: However, the FFT will give you the overall frequency components for the entire
    time series of the audio signal as a whole. Thus, you are losing the time information
    when converting audio data from the time domain to the frequency domain.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，FFT 会给出整个音频信号时间序列的整体频率成分。因此，在将音频数据从时域转换到频域时，你会丢失时间信息。
- en: Instead of the FFT, you can use the Short-Time Fourier Transform (STFT) to preserve
    the time information. The STFT is a variant of the FFT that breaks up the audio
    signal into smaller sections by using a sliding time window. It takes the FFT
    on each section and then combines them.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 FFT，你还可以使用短时傅里叶变换（STFT）来保留时间信息。STFT 是 FFT 的一种变体，通过使用滑动时间窗口将音频信号分成更小的部分。它对每个部分进行
    FFT，然后将它们组合起来。
- en: '`n_fft` —length of the sliding window (default: 2048)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_fft` — 滑动窗口的长度（默认值：2048）'
- en: '`hop_length` — number of samples by which to slide the window (default: 512).
    The `hop_length`will directly impact the resulting image size. If your audio data
    has a fixed length and you want to convert the waveform to a fixed image size,
    you can set `hop_length = audio_length // (image_size[1] — 1)`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hop_length` — 窗口滑动的样本数（默认值：512）。`hop_length` 将直接影响生成图像的大小。如果你的音频数据具有固定长度，并且你希望将波形转换为固定图像大小，你可以设置
    `hop_length = audio_length // (image_size[1] — 1)`'
- en: '![](../Images/f4ee73d6d2965c96bfb5d8b8b97df960.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4ee73d6d2965c96bfb5d8b8b97df960.png)'
- en: Short-Time Fourier Transform (STFT)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 短时傅里叶变换（STFT）
- en: Next, you will convert the amplitude to decibels and bin the frequencies according
    to the Mel scale. For this purpose, `n_mels` is the number of frequency bands
    (Mel bins). This will be the height of the resulting spectrogram.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将把幅度转换为分贝，并根据 Mel 频率尺度对频率进行分箱。为此，`n_mels` 是频带数量（Mel 频带）。这将决定生成的频谱图的高度。
- en: '![](../Images/82405a51d8414924699fa9b74e0da69a.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82405a51d8414924699fa9b74e0da69a.png)'
- en: Convert amplitude to decibels and apply Mel binning to the spectrum
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 将幅度转换为分贝并对频谱应用 Mel 分箱
- en: 'For an in-depth explanation of the Mel spectrogram, I recommend this article:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Mel 频谱图的详细解释，我推荐这篇文章：
- en: '[](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53?source=post_page-----cf752b22ba07--------------------------------)
    [## Understanding the Mel Spectrogram'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53?source=post_page-----cf752b22ba07--------------------------------)
    [## 了解 Mel 频谱图'
- en: (and Other Topics in Signal Processing)
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: （以及信号处理中的其他主题）
- en: medium.com](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53?source=post_page-----cf752b22ba07--------------------------------)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53?source=post_page-----cf752b22ba07--------------------------------)
- en: Below you can see an example PyTorch `Dataset` which loads an audio file and
    converts the waveform to a Mel spectrogram after some preprocessing steps.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 下面你可以看到一个示例 PyTorch `Dataset`，它加载一个音频文件，并在经过一些预处理步骤后将波形转换为 Mel 频谱图。
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Your resulting dataset should produce samples that look something like this
    before we feed it to the neural network:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你生成的数据集在喂给神经网络之前应产生类似这样的样本：
- en: '![](../Images/e0c54b387ffda9de0c7cb8c156863f58.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0c54b387ffda9de0c7cb8c156863f58.png)'
- en: Sample structure from the Audio Dataset
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 音频数据集中的样本结构
- en: 'Step 2: Apply Augmentations to Audio Data'
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 2：对音频数据应用增强
- en: One technique to tackle this competition’s challenges of domain shift and class
    imbalance was to apply data augmentations to the training data [5, 8, 10, 11].
    You can apply data augmentations for audio data in the waveform and the spectrogram.
    The `[torchaudio](https://pytorch.org/audio/stable/index.html)` library already
    provides a lot of different data augmentations for audio data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 针对本次竞赛的领域转换和类别不平衡挑战，采用了对训练数据应用数据增强的技术 [5, 8, 10, 11]。你可以对波形和谱图的音频数据应用数据增强。`[torchaudio](https://pytorch.org/audio/stable/index.html)`
    库已经提供了许多不同的音频数据增强方法。
- en: 'Popular data augmentation techniques for audio data in **waveform (time domain)**
    are:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在**波形（时间域）**中流行的音频数据增强技术有：
- en: Noise injection like white noise, colored noise, or background noise (`[AddNoise](https://pytorch.org/audio/stable/generated/torchaudio.transforms.AddNoise.html#torchaudio.transforms.AddNoise)`)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 噪声注入，如白噪声、有色噪声或背景噪声 (`[AddNoise](https://pytorch.org/audio/stable/generated/torchaudio.transforms.AddNoise.html#torchaudio.transforms.AddNoise)`)
- en: Shifting time
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间转换
- en: Changing speed (`[Speed](https://pytorch.org/audio/stable/generated/torchaudio.transforms.Speed.html#torchaudio.transforms.Speed)`;
    alternatively use `[TimeStretch](https://pytorch.org/audio/stable/generated/torchaudio.transforms.TimeStretch.html#torchaudio.transforms.TimeStretch)`
    in frequency domain)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变速度 (`[Speed](https://pytorch.org/audio/stable/generated/torchaudio.transforms.Speed.html#torchaudio.transforms.Speed)`；或者使用
    `[TimeStretch](https://pytorch.org/audio/stable/generated/torchaudio.transforms.TimeStretch.html#torchaudio.transforms.TimeStretch)`
    频率域)
- en: Changing pitch (`[PitchShift](https://pytorch.org/audio/stable/generated/torchaudio.transforms.PitchShift.html#torchaudio.transforms.PitchShift)`)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变音高 (`[PitchShift](https://pytorch.org/audio/stable/generated/torchaudio.transforms.PitchShift.html#torchaudio.transforms.PitchShift)`)
- en: '![](../Images/2c9d74a61d35695672531f166551ec88.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c9d74a61d35695672531f166551ec88.png)'
- en: 'Overview of different data augmentation techniques for audio in waveform: Noise
    injection (white noise, colored noise, background noise), shifting time, changing
    speed and pitch'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 不同音频增强技术概述：噪声注入（白噪声、有色噪声、背景噪声）、时间转换、速度和音高变化
- en: 'Popular data augmentation techniques for audio data in the **spectrogram (frequency
    domain)** are:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在**谱图（频率域）**中流行的音频数据增强技术有：
- en: Popular image augmentation techniques like Mixup [13] or Cutmix [3]
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像 Mixup [13] 或 Cutmix [3] 这样的流行图像增强技术
- en: '![](../Images/d0193c5f5d8b516c9d6b5cff68fde6ad.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d0193c5f5d8b516c9d6b5cff68fde6ad.png)'
- en: 'Data Augmentation for Spectrogram: Mixup [13]'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 用于谱图的数据增强：Mixup [13]
- en: SpecAugment [7] (`[FrequencyMasking](https://pytorch.org/audio/stable/generated/torchaudio.transforms.FrequencyMasking.html#torchaudio.transforms.FrequencyMasking)`
    and `[TimeMasking](https://pytorch.org/audio/stable/generated/torchaudio.transforms.TimeMasking.html#torchaudio.transforms.TimeMasking)`)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SpecAugment [7] (`[FrequencyMasking](https://pytorch.org/audio/stable/generated/torchaudio.transforms.FrequencyMasking.html#torchaudio.transforms.FrequencyMasking)`
    和 `[TimeMasking](https://pytorch.org/audio/stable/generated/torchaudio.transforms.TimeMasking.html#torchaudio.transforms.TimeMasking)`)
- en: '![](../Images/23daa65bbcea6fa1c6a6d52efe3baffe.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23daa65bbcea6fa1c6a6d52efe3baffe.png)'
- en: 'Data Augmentation for Spectrogram: SpecAugment [7]'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 用于谱图的数据增强：SpecAugment [7]
- en: As you can see while providing a lot of audio augmentations, `[torchaudio](https://pytorch.org/audio/stable/index.html)`
    doesn’t provide all of the proposed data augmentations.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，虽然提供了很多音频增强方法，`[torchaudio](https://pytorch.org/audio/stable/index.html)`
    并未提供所有提议的数据增强方法。
- en: 'Thus, if you want to inject a specific type of noise, shift the time, or apply
    Mixup [13] or Cutmix [12] augmentations, you must [write a custom data augmentation
    in PyTorch](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#transforms).
    You can reference this [collection of audio data augmentation techniques](/data-augmentation-techniques-for-audio-data-in-python-15505483c63c)
    for their implementations:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你想注入特定类型的噪声、时间转换，或应用 Mixup [13] 或 Cutmix [12] 数据增强，你必须 [编写自定义数据增强代码（PyTorch）](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#transforms)。你可以参考这个
    [音频数据增强技术集合](/data-augmentation-techniques-for-audio-data-in-python-15505483c63c)
    以了解它们的实现：
- en: '[](/data-augmentation-techniques-for-audio-data-in-python-15505483c63c?source=post_page-----cf752b22ba07--------------------------------)
    [## Data Augmentation Techniques for Audio Data in Python'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/data-augmentation-techniques-for-audio-data-in-python-15505483c63c?source=post_page-----cf752b22ba07--------------------------------)
    [## Python中的音频数据增强技术'
- en: How to augment audio in waveform (time domain) and as spectrograms (frequency
    domain) with librosa, numpy, and PyTorch
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何用 librosa、numpy 和 PyTorch 对波形（时间域）和谱图（频率域）进行音频增强
- en: towardsdatascience.com](/data-augmentation-techniques-for-audio-data-in-python-15505483c63c?source=post_page-----cf752b22ba07--------------------------------)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/data-augmentation-techniques-for-audio-data-in-python-15505483c63c?source=post_page-----cf752b22ba07--------------------------------)
- en: 'In the example PyTorch `Dataset` class from before, you can apply the data
    augmentations as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的PyTorch `Dataset` 类示例中，你可以按照以下方式应用数据增强：
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Step 3: Fine-tune a Pretrained Image Classification Model for Few-Shot Learning'
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3步：针对少样本学习微调预训练的图像分类模型
- en: In this competition, we are dealing with a class imbalance. As some classes
    only have one sample, we are dealing with a few-shot learning problem. Nakamura
    and Harada [6] showed in 2019 that fine-tuning could be an effective approach
    to few-shot learning.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次比赛中，我们面临着类别不平衡的问题。由于一些类别只有一个样本，我们正在处理少样本学习问题。Nakamura 和 Harada [6] 在2019年表明，微调可能是少样本学习的有效方法。
- en: A lot of competitors [2, 5, 8, 10, 11] fine-tuned common pre-trained image classification
    models such as
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 很多竞争者[2, 5, 8, 10, 11]微调了常见的预训练图像分类模型，例如
- en: EfficientNet (e.g., `tf_efficientnet_b3_ns`) [9],
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EfficientNet（例如，`tf_efficientnet_b3_ns`）[9]，
- en: SE-ResNext (e.g., `se_resnext50_32x4d`) [3],
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SE-ResNext（例如，`se_resnext50_32x4d`）[3]，
- en: NFNet (e.g., `eca_nfnet_l0`) [1]
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NFNet（例如，`eca_nfnet_l0`）[1]
- en: You can load any pre-trained image classification model with the `[timm](https://timm.fast.ai/)`
    library for fine-tuning. Make sure to set `in_chans = 1` as we are not working
    with 3-channel images but 1-channel Mel spectrograms.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`[timm](https://timm.fast.ai/)`库加载任何预训练的图像分类模型进行微调。确保将`in_chans = 1`设置为1，因为我们处理的是1通道Mel谱图而不是3通道图像。
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Other competitors reported successes from fine-tuning models pre-trained on
    similar audio classification problems [4, 10].
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 其他竞争者报告了在类似音频分类问题上微调预训练模型的成功[4, 10]。
- en: Fine-tuning is done with a cosine annealing learning rate scheduler (`[CosineAnnealingLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR)`)
    for a few epochs [2, 8].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 微调使用余弦退火学习率调度器（`[CosineAnnealingLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR)`）进行几个周期[2,
    8]。
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/9ae13b38a3e843bc46c227a590baa3f2.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ae13b38a3e843bc46c227a590baa3f2.png)'
- en: PyTorch Cosine Annealing / Decay Learning Rate Scheduler (Image by the author,
    originally published in [“A Visual Guide to Learning Rate Schedulers in PyTorch”](https://medium.com/towards-data-science/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863#fad1))
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch余弦退火/衰减学习率调度器（图像由作者提供，原文发表于[“PyTorch学习率调度器视觉指南”](https://medium.com/towards-data-science/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863#fad1)）
- en: 'You can find more tips and best practices in this [guide for fine-tuning Deep
    Learning models](/intermediate-deep-learning-with-transfer-learning-f1aba5a814f):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这个[深度学习模型微调指南](/intermediate-deep-learning-with-transfer-learning-f1aba5a814f)中找到更多提示和最佳实践：
- en: '[](/intermediate-deep-learning-with-transfer-learning-f1aba5a814f?source=post_page-----cf752b22ba07--------------------------------)
    [## Intermediate Deep Learning with Transfer Learning'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/intermediate-deep-learning-with-transfer-learning-f1aba5a814f?source=post_page-----cf752b22ba07--------------------------------)
    [## 中级深度学习与迁移学习'
- en: A practical guide for fine-tuning Deep Learning models for computer vision and
    natural language processing
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这是一本实用指南，介绍了用于计算机视觉和自然语言处理的深度学习模型微调。
- en: towardsdatascience.com](/intermediate-deep-learning-with-transfer-learning-f1aba5a814f?source=post_page-----cf752b22ba07--------------------------------)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/intermediate-deep-learning-with-transfer-learning-f1aba5a814f?source=post_page-----cf752b22ba07--------------------------------)
- en: Summary
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: There are many more lessons to be learned from reviewing the learning resources
    Kagglers have created during the course of the [“BirdCLEF 2022”](https://www.kaggle.com/competitions/birdclef-2022)
    competition. There are also many different solutions for this type of problem
    statement.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 从Kagglers在[“BirdCLEF 2022”](https://www.kaggle.com/competitions/birdclef-2022)比赛期间创建的学习资源中，我们可以学到更多经验。这类问题陈述也有许多不同的解决方案。
- en: 'In this article, we focused on the general approach that was popular among
    many competitors:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们集中讨论了许多竞争者中流行的一般方法：
- en: '[Converting the audio classification problem to an image classification problem](#e2c2)
    by converting the audio from waveform to a Mel spectrogram and applying a Deep
    Learning model'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[通过将音频从波形转换为Mel谱图并应用深度学习模型，将音频分类问题转化为图像分类问题](#e2c2)'
- en: '[Applying data augmentations to the audio data in waveform and in spectrograms](#8f8b)
    to tackle the domain shift and class imbalance'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[将数据增强应用于波形和谱图中的音频数据](#8f8b)以应对领域迁移和类别不平衡'
- en: '[Finetune a pre-trained image classification model](#daf1) to tackle class
    imbalance'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[微调预训练的图像分类模型](#daf1)以应对类别不平衡'
- en: Enjoyed This Story?
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 享受这个故事了吗？
- en: '[*Subscribe for free*](https://medium.com/subscribe/@iamleonie) *to get notified
    when I publish a new story.*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[*免费订阅*](https://medium.com/subscribe/@iamleonie) *以便在我发布新故事时获得通知。*'
- en: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----cf752b22ba07--------------------------------)
    [## Get an email whenever Leonie Monigatti publishes.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----cf752b22ba07--------------------------------)
    [## 当 Leoni Monigatti 发布时，获取电子邮件通知。'
- en: Get an email whenever Leonie Monigatti publishes. By signing up, you will create
    a Medium account if you don’t already…
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 当 Leoni Monigatti 发布时，获取电子邮件通知。通过注册，你将创建一个 Medium 账户（如果你还没有的话）…
- en: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----cf752b22ba07--------------------------------)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----cf752b22ba07--------------------------------)
- en: '*Find me on* [*LinkedIn*](https://www.linkedin.com/in/804250ab/),[*Twitter*](https://twitter.com/helloiamleonie)*,
    and* [*Kaggle*](https://www.kaggle.com/iamleonie)*!*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*在* [*LinkedIn*](https://www.linkedin.com/in/804250ab/)、[*Twitter*](https://twitter.com/helloiamleonie)
    *和* [*Kaggle*](https://www.kaggle.com/iamleonie) *上找到我！*'
- en: References
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Dataset
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: As the original competition data does not allow commercial use, examples are
    done with the following dataset.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 由于原始比赛数据不允许商业使用，示例使用了以下数据集。
- en: '[0] Warden P. Speech Commands: A public dataset for single-word speech recognition,
    2017\. Available from [http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz](http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[0] Warden P. Speech Commands: 一个用于单词语音识别的公共数据集，2017。可从 [http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz](http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz)
    获取'
- en: 'License: CC-BY-4.0'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 许可证：CC-BY-4.0
- en: Image References
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像参考
- en: If not otherwise stated, all images are created by the author.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图像均由作者创建。
- en: Web & Literature
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络与文献
- en: '[1] Brock, A., De, S., Smith, S. L., & Simonyan, K. (2021, July). High-performance
    large-scale image recognition without normalization. In *International Conference
    on Machine Learning* (pp. 1059–1071). PMLR.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Brock, A., De, S., Smith, S. L., & Simonyan, K. (2021年7月). 高性能大规模图像识别，无需归一化。在
    *国际机器学习会议* (第1059–1071页)。PMLR。'
- en: '[2] [Chai Time Data Science](https://www.youtube.com/@ChaiTimeDataScience)
    (2022). [BirdCLEF 2022: 11th Pos Gold Solution | Gilles Vandewiele](https://www.youtube.com/watch?v=MXZKNnuoQXw)
    (accessed March 13th, 2023)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [Chai Time Data Science](https://www.youtube.com/@ChaiTimeDataScience)
    (2022). [BirdCLEF 2022: 第11位Pos Gold解决方案 | Gilles Vandewiele](https://www.youtube.com/watch?v=MXZKNnuoQXw)
    (访问日期：2023年3月13日)'
- en: '[3] Hu, J., Shen, L., & Sun, G. (2018). Squeeze-and-excitation networks. In
    *Proceedings of the IEEE conference on computer vision and pattern recognition*
    (pp. 7132–7141).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Hu, J., Shen, L., & Sun, G. (2018). Squeeze-and-excitation 网络。在 *IEEE计算机视觉与模式识别会议论文集*
    (第7132–7141页)。'
- en: '[4] Kramarenko Vladislav (2022). [4th place](https://www.kaggle.com/competitions/birdclef-2022/discussion/326987)
    in Kaggle Discussions (accessed March 13th, 2023)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Kramarenko Vladislav (2022). [第四名](https://www.kaggle.com/competitions/birdclef-2022/discussion/326987)
    在 Kaggle 讨论中 (访问日期：2023年3月13日)'
- en: '[5] LeonShangguan (2022). [[Public #1 Private #2] + [Private #7/8 (potential)]
    solutions. The host wins.](https://www.kaggle.com/competitions/birdclef-2022/discussion/326950)
    in Kaggle Discussions (accessed March 13th, 2023)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] LeonShangguan (2022). [[公共 #1 私人 #2] + [私人 #7/8 (潜在)] 解决方案。主持人获胜。](https://www.kaggle.com/competitions/birdclef-2022/discussion/326950)
    在 Kaggle 讨论中 (访问日期：2023年3月13日)'
- en: '[6] Nakamura, A., & Harada, T. (2019). Revisiting fine-tuning for few-shot
    learning. *arXiv preprint arXiv:1910.00216*.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Nakamura, A., & Harada, T. (2019). 重新审视少样本学习的微调。*arXiv 预印本 arXiv:1910.00216*。'
- en: '[7] Park, D. S., Chan, W., Zhang, Y., Chiu, C. C., Zoph, B., Cubuk, E. D.,
    & Le, Q. V. (2019). Specaugment: A simple data augmentation method for automatic
    speech recognition. *arXiv preprint arXiv:1904.08779*.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Park, D. S., Chan, W., Zhang, Y., Chiu, C. C., Zoph, B., Cubuk, E. D.,
    & Le, Q. V. (2019). Specaugment: 一种用于自动语音识别的简单数据增强方法。*arXiv 预印本 arXiv:1904.08779*。'
- en: '[8] slime (2022). [3rd place solution](https://www.kaggle.com/competitions/birdclef-2022/discussion/327193)
    in Kaggle Discussions (accessed March 13th, 2023)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] slime (2022). [第三名解决方案](https://www.kaggle.com/competitions/birdclef-2022/discussion/327193)
    在 Kaggle 讨论中 (访问日期：2023年3月13日)'
- en: '[9] Tan, M., & Le, Q. (2019, May). Efficientnet: Rethinking model scaling for
    convolutional neural networks. In *International conference on machine learning*
    (pp. 6105–6114). PMLR.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Tan, M., & Le, Q. (2019年5月). Efficientnet: 重新思考卷积神经网络的模型缩放。在*国际机器学习会议*（第6105–6114页）。PMLR。'
- en: '[10] Volodymyr (2022). [1st place solution models (it’s not all BirdNet)](https://www.kaggle.com/competitions/birdclef-2022/discussion/327047)
    in Kaggle Discussions (accessed March 13th, 2023)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Volodymyr (2022). [第一名解决方案模型（这不是全部 BirdNet）](https://www.kaggle.com/competitions/birdclef-2022/discussion/327047)
    在 Kaggle 讨论区（访问日期：2023年3月13日）'
- en: '[11] yokuyama (2022). [5th place solution](https://www.kaggle.com/competitions/birdclef-2022/discussion/327044)
    in Kaggle Discussions (accessed March 13th, 2023)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] yokuyama (2022). [第五名解决方案](https://www.kaggle.com/competitions/birdclef-2022/discussion/327044)
    在 Kaggle 讨论区（访问日期：2023年3月13日）'
- en: '[12] Yun, S., Han, D., Oh, S. J., Chun, S., Choe, J., & Yoo, Y. (2019). Cutmix:
    Regularization strategy to train strong classifiers with localizable features.
    In *Proceedings of the IEEE/CVF international conference on computer vision* (pp.
    6023–6032).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Yun, S., Han, D., Oh, S. J., Chun, S., Choe, J., & Yoo, Y. (2019). Cutmix:
    一种用于训练强分类器的正则化策略，具有可定位特征。在*IEEE/CVF国际计算机视觉会议论文集*（第6023–6032页）。'
- en: '[13] Zhang, H., Cisse, M., Dauphin, Y. N., & Lopez-Paz, D. (2017) mixup: Beyond
    empirical risk minimization. arXiv preprint arXiv:1710.09412.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Zhang, H., Cisse, M., Dauphin, Y. N., & Lopez-Paz, D. (2017) mixup: 超越经验风险最小化。arXiv
    预印本 arXiv:1710.09412。'
