- en: 'PID Controller Optimization: A Gradient Descent Approach'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PID控制器优化：梯度下降方法
- en: 原文：[https://towardsdatascience.com/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2](https://towardsdatascience.com/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2](https://towardsdatascience.com/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2)
- en: Using machine learning to solve engineering optimization problems
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用机器学习解决工程优化问题
- en: '[](https://medium.com/@callum.bruce1?source=post_page-----58876e14eef2--------------------------------)[![Callum
    Bruce](../Images/4833a199a9449434777fdf5ce913a9cb.png)](https://medium.com/@callum.bruce1?source=post_page-----58876e14eef2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----58876e14eef2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----58876e14eef2--------------------------------)
    [Callum Bruce](https://medium.com/@callum.bruce1?source=post_page-----58876e14eef2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@callum.bruce1?source=post_page-----58876e14eef2--------------------------------)[![Callum
    Bruce](../Images/4833a199a9449434777fdf5ce913a9cb.png)](https://medium.com/@callum.bruce1?source=post_page-----58876e14eef2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----58876e14eef2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----58876e14eef2--------------------------------)
    [Callum Bruce](https://medium.com/@callum.bruce1?source=post_page-----58876e14eef2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----58876e14eef2--------------------------------)
    ·10 min read·Aug 1, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----58876e14eef2--------------------------------)
    ·10分钟阅读·2023年8月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4350abf8767e1ec8d3ce9a121cb47b01.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4350abf8767e1ec8d3ce9a121cb47b01.png)'
- en: The gradient descent algorithm steps downhill to minimize a cost function
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法沿着下坡的方向减少成本函数。
- en: Machine learning. Deep learning. AI. More and more people use these technologies
    every day. This has largely been driven by the rise of Large Language Models deployed
    by the likes of ChatGPT, Bard and others. Despite their widespread use, relatively
    few people are familiar with the methods underpinning these technologies.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习。深度学习。人工智能。越来越多的人每天都在使用这些技术。这在很大程度上是由于像ChatGPT、Bard等大型语言模型的兴起。尽管这些技术被广泛使用，但相对较少的人了解这些技术背后的方法。
- en: 'In this article, we dive into one of the fundamental methods deployed in machine
    learning: the gradient descent algorithm.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们深入探讨了机器学习中使用的基本方法之一：梯度下降算法。
- en: Instead of looking at gradient descent through the lens of neural networks,
    where it is used to optimize network weights and biases, we will instead examine
    the algorithm as a tool for solving classic engineering optimization problems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不从神经网络的角度来审视梯度下降算法，在神经网络中它用于优化网络权重和偏置，而是将其作为解决经典工程优化问题的工具来探讨。
- en: Specifically, we will use gradient descent to tune the gains of a PID (Proportional-Integral-Derivative)
    controller for a car cruise control system.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们将使用梯度下降来调整汽车巡航控制系统中PID（比例-积分-微分）控制器的增益。
- en: 'The motivation for following this approach is twofold:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这种方法的动机有两个：
- en: First, optimizing the weights and biases in a neural network is a high-dimension
    problem. There are a lot of moving parts and I think these distract from the underlying
    utility of gradient descent for solving optimization problems.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，优化神经网络中的权重和偏置是一个高维问题。有许多变量，我认为这些会分散梯度下降在解决优化问题中的基本效用。
- en: Secondly, as you will see, gradient descent can be a powerful tool when applied
    to classic engineering problems like PID controller tuning, inverse kinematics
    in robotics and topology optimization. Gradient descent is a tool that, in my
    opinion, more engineers should be familiar with and able to utilize.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，正如你将看到的，梯度下降在应用于经典工程问题如PID控制器调优、机器人中的逆运动学和拓扑优化时，可以是一个强大的工具。梯度下降是一个在我看来，更多工程师应该熟悉并能够利用的工具。
- en: After reading this article you will understand what a PID controller is, how
    the gradient descent algorithm works and how it can be applied to solve classic
    engineering optimization problems. You might be motivated to use gradient descent
    to tackle optimization challenges of your own.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本文后，你将了解什么是PID控制器，梯度下降算法的工作原理，以及它如何应用于解决经典工程优化问题。你可能会受到激励，使用梯度下降来应对自己的优化挑战。
- en: All code used in this article is available [here on GitHub](https://github.com/c-bruce/pid_controller_gradient_descent).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中使用的所有代码可以在[GitHub 上找到](https://github.com/c-bruce/pid_controller_gradient_descent)。
- en: What is a PID controller?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 PID 控制器？
- en: A PID controller is a widely used feedback control mechanism in engineering
    and automated systems. It aims to maintain a desired setpoint by continuously
    adjusting the control signal based on the error between the setpoint and the system’s
    measured output (the process variable).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: PID 控制器是工程和自动化系统中广泛使用的反馈控制机制。其目标是通过持续调整控制信号以维持期望的设定点，基于设定点和系统测量输出（过程变量）之间的误差。
- en: '![](../Images/4bebce93124edeb5a68f3e150a4cc86f.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bebce93124edeb5a68f3e150a4cc86f.png)'
- en: Typical step response of a PID controller
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: PID 控制器的典型阶跃响应
- en: PID controllers find extensive applications in various industries and domains.
    They are widely used in process control systems, such as temperature control in
    manufacturing, flow control in chemical plants, and pressure control in HVAC systems.
    PID controllers are also employed in robotics for precise positioning and motion
    control, as well as in automotive systems for throttle control, engine speed regulation,
    and anti-lock braking systems. They play a vital role in aerospace and aviation
    applications, including aircraft autopilots and attitude control systems.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: PID 控制器在各个行业和领域中应用广泛。它们被广泛用于过程控制系统，如制造业中的温度控制、化工厂中的流量控制以及 HVAC 系统中的压力控制。PID
    控制器还被应用于机器人技术中以实现精确定位和运动控制，以及汽车系统中的油门控制、发动机速度调节和防抱死刹车系统。它们在航空航天和航空应用中也发挥着重要作用，包括飞机自动驾驶仪和姿态控制系统。
- en: 'A PID controller consists of three components: the proportional term, the integral
    term, and the derivative term. The proportional term provides an immediate response
    to the current error, the integral term accumulates and corrects for past errors,
    and the derivative term predicts and counteracts future error trends.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: PID 控制器由三个组成部分组成：比例项、积分项和微分项。比例项对当前误差做出即时响应，积分项累积并修正过去的误差，微分项预测并抵消未来的误差趋势。
- en: '![](../Images/6fe42cfd42b6ca09246fce764897838c.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6fe42cfd42b6ca09246fce764897838c.png)'
- en: PID controller block diagram
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: PID 控制器框图
- en: The control loop of a PID controller is presented in the block diagram above.
    *r(t)* is the setpoint and *y(t)* is the process variable. The process variable
    is subtracted from the setpoint to get the error signal, *e(t)*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 上图中的 PID 控制器控制回路展示了 *r(t)* 是设定点，*y(t)* 是过程变量。过程变量从设定点中减去以获得误差信号 *e(t)*。
- en: The control signal, *u(t)*, is the sum of the proportional, integral and derivative
    terms. The control signal is input to the process and this, in turn, causes the
    process variable to update.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 控制信号，*u(t)*，是比例、积分和微分项的总和。控制信号被输入到过程当中，这会导致过程变量更新。
- en: PID controller control signal u(t)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: PID 控制器控制信号 u(t)
- en: The gradient descent algorithm
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降算法
- en: Gradient descent is an optimization algorithm commonly used in machine learning
    and mathematical optimization. It aims to find the minimum of a given cost function
    by iteratively adjusting the parameters based on the cost function gradient. The
    gradient points in the direction of the steepest ascent, so by taking steps in
    the opposite direction, the algorithm gradually converges towards the optimal
    solution.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种常用于机器学习和数学优化的优化算法。它通过基于成本函数梯度迭代调整参数，旨在找到给定成本函数的最小值。梯度指向最陡的上升方向，因此通过朝相反方向迈步，算法逐渐收敛到最优解。
- en: 'A single gradient descent update step is defined as:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 单步梯度下降更新定义为：
- en: Gradient descent update step
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降更新步骤
- en: Where ***a****ₙ*is a vector of input parameters. The subscript *n* denotes iteration.
    *f(****a****ₙ)* is a multi-variable cost function and ∇*f(****a****) is* the gradient
    of that cost function. ∇*f(****a****ₙ)* represents the direction of steepest ascent
    so it is subtracted from ***a****ₙ* to reduce the cost function on the next iteration.
    𝛾 is the learning rate which determines the step size at each iteration.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '***a****ₙ* 是输入参数的向量。下标 *n* 表示迭代。*f(****a****ₙ)* 是多变量成本函数，∇*f(****a****)* 是该成本函数的梯度。∇*f(****a****ₙ)*
    代表最陡升高的方向，因此它从 ***a****ₙ* 中减去以在下一次迭代中减少成本函数。𝛾 是学习率，决定了每次迭代的步长。'
- en: An appropriate value for 𝛾 must be selected. Too big and the steps taken at
    each iteration will be too large and cause the gradient descent algorithm to not
    converge. Too small and the gradient descent algorithm will be computationally
    expensive and take a long time to converge.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 必须选择一个合适的𝛾值。如果值过大，每次迭代时采取的步骤将过大，导致梯度下降算法无法收敛。如果值过小，梯度下降算法将计算开销大，收敛时间长。
- en: '![](../Images/aba8bdc7d4833adb595ed30d9acda58e.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aba8bdc7d4833adb595ed30d9acda58e.png)'
- en: Gradient descent algorithm applied to y=x² cost function (initially x=5) for
    𝛾=0.1 (LHS) and 𝛾=1.02 (RHS)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法应用于y=x²代价函数（初始x=5），对于𝛾=0.1（左侧）和𝛾=1.02（右侧）
- en: Gradient descent is applied in a wide range of fields and disciplines. In machine
    learning and deep learning, it is a fundamental optimization algorithm used to
    train neural networks and optimize their parameters. By iteratively updating the
    weights and biases of the network based on the gradient of the cost function,
    gradient descent enables the network to learn and improve its performance over
    time.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降在广泛的领域和学科中应用。在机器学习和深度学习中，它是用于训练神经网络和优化其参数的基本优化算法。通过根据代价函数的梯度迭代更新网络的权重和偏差，梯度下降使网络能够学习并随着时间的推移提高其性能。
- en: Beyond machine learning, gradient descent is utilized in various optimization
    problems across engineering, physics, economics, and other domains. It assists
    in parameter estimation, system identification, signal processing, image reconstruction,
    and many other tasks that require finding the minimum or maximum of a function.
    The versatility and effectiveness of gradient descent make it an essential tool
    for solving optimization problems and improving models and systems across diverse
    fields.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 除了机器学习之外，梯度下降被应用于工程、物理学、经济学及其他领域的各种优化问题中。它帮助进行参数估计、系统识别、信号处理、图像重建以及许多其他需要找到函数最小值或最大值的任务。梯度下降的多功能性和有效性使其成为解决优化问题和改善各领域模型和系统的重要工具。
- en: Optimizing PID controller gains using gradient descent
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用梯度下降优化PID控制器增益
- en: There are several methods available to tune a PID controller. These include
    the manual tuning method and heuristic methods like the [Ziegler-Nichols method](https://en.wikipedia.org/wiki/Ziegler%E2%80%93Nichols_method).
    The manual tuning method can be time-consuming and may require multiple iterations
    to find optimal values while the Ziegler-Nichols method often yields aggressive
    gains and large overshoot which means it is not suitable for certain applications.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以调整PID控制器。这些方法包括手动调节法和像[齐格勒-尼科尔斯法](https://en.wikipedia.org/wiki/Ziegler%E2%80%93Nichols_method)这样的启发式方法。手动调节法可能耗时且可能需要多次迭代才能找到最佳值，而齐格勒-尼科尔斯法往往会产生激进的增益和较大的超调，这意味着它不适合某些应用。
- en: Presented here is a gradient descent approach to PID controller optimization.
    We will optimize the control system of a car cruise control system subject to
    a step change in setpoint.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了一种梯度下降方法来优化PID控制器。我们将优化一个车载巡航控制系统的控制系统，以应对设定点的阶跃变化。
- en: By controlling the pedal position, the controller’s objective is to accelerate
    the car up to the velocity setpoint with minimum overshoot, settling time and
    steady-state error.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过控制踏板位置，控制器的目标是将车辆加速到速度设定点，同时最小化超调、稳定时间和稳态误差。
- en: The car is subject to a driving force proportional to the pedal position. Rolling
    resistance and aerodynamic drag forces act in the opposite direction to the driving
    force. Pedal position is controlled by the PID controller and limited to within
    a range of -50% to 100%. When the pedal position is negative, the car is braking.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 车辆受到与踏板位置成比例的驱动力。滚动阻力和空气阻力作用于与驱动力相反的方向。踏板位置由PID控制器控制，并限制在-50%到100%范围内。当踏板位置为负值时，车辆在制动。
- en: 'It is helpful to have a model of the system when tuning PID controller gains.
    This is so that we can simulate the system response. For this I have implemented
    a `Car` class in Python:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整PID控制器增益时，拥有系统模型是有帮助的。这样我们可以模拟系统响应。为此，我在Python中实现了一个`Car`类：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The`PIDController` class is implemented as:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`PIDController`类的实现如下：'
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Taking this object-oriented programming approach makes it much easier to set
    up and run multiple simulations with different PID controller gains as we must
    do when running the gradient descent algorithm.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 采取这种面向对象编程的方法使得设置和运行多个带有不同PID控制器增益的仿真变得更加容易，这在运行梯度下降算法时是必须的。
- en: 'The`GradientDescent` class is implemented as:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The algorithm is run for a specified number of iterations by calling `execute`
    or `execute_adagrad`. The `execute_adagrad` method executes a modified form of
    gradient descent called AdaGrad (adaptive gradient descent).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: AdaGrad has per-parameter learning rates that increase for sparse parameters
    and decrease for less sparse parameters. The learning rate is updated after each
    iteration based on a historical sum of the gradients squared.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use AdaGrad to optimize the PID controller gains for the car cruise
    control system. Using AdaGrad, the gradient descent update equation becomes:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: AdaGrad gradient descent update step
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to define our cost function. The cost function must take a vector
    of input parameters as input and return a single number; the cost. The objective
    of the car cruise control is to accelerate the car up to the velocity setpoint
    with minimum overshoot, settling time and steady-state error. There are many ways
    we could define the cost function based on this objective. Here we will define
    it as the integral of the error magnitude over time:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Car cruise control cost function
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our cost function is an integral, we can visualize it as the area under
    the error magnitude curve. We expect to see the area under the curve reduce as
    we approach the global minimum. Programmatically, the cost function is defined
    as:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The cost function includes the simulation parameters. The simulation is run
    for 60 seconds. During this time we observe the response of the system to a step
    change in setpoint from 0 m/s to 20 m/s. By integrating error magnitude over time,
    the cost is calculated for every iteration.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, all that is left to do is run the optimization. We will start with initial
    values of *Kp* = 5.0, *Ki* = 1.0 and *Kd* = 0.0\. These values give a steady,
    oscillating response, with overshoot, that eventually converges to the setpoint.
    From this start point we will run the gradient descent algorithm for 500 iterations
    using a base learning rate of 𝛾=0.1:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/aef6802298914bd17d835a1e6233d53f.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: Car cruise control step response (LHS), error magnitude (middle) and cost (RHS)
    as the gradient descent algorithm iterates toward an optimal solution
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: The animated plot above shows the evolution of the car cruise control step response
    as the gradient descent algorithm tunes the *Kp*, *Ki* and *Kd* gains of the PID
    controller.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: By iteration 25, the gradient descent algorithm has eliminated the oscillatory
    response. After this point, something interesting happens. The algorithm wanders
    into a local minimum characterized by an overshoot of ~ 3 m/s. This happens in
    the region of 6.0 < *Kp* < 7.5, *Ki* ~= 0.5, *Kd* = 0.0 and lasts right up to
    iteration 300.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: After iteration 300, the algorithm moves out of the local minimum to find a
    more satisfactory response closer to the global minimum. The response is now characterized
    by zero overshoot, fast settling time and near-zero steady-state error.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Running the gradient descent algorithm for 500 iterations we arrive at our optimized
    PID controller gains; *Kp* = 8.33, *Ki* = 0.12 and *Kd* = 0.00.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 运行梯度下降算法500次迭代后，我们得到了优化的PID控制器增益；*Kp* = 8.33，*Ki* = 0.12 和 *Kd* = 0.00。
- en: The proportional gain is still rising steadily. Running more iterations (not
    shown here), as *Kp* slowly increases, we find further reduction to the cost function
    is possible though this effect becomes increasingly marginal.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 比例增益仍在稳步上升。运行更多的迭代（此处未显示），随着 *Kp* 的缓慢增加，我们发现对成本函数的进一步减少是可能的，尽管这种效果逐渐变得边际化。
- en: Summary
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Adopting a method widely used for solving machine learning and deep learning
    problems, we have successfully optimized PID controller gains for a car cruise
    control system.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 采用一种广泛用于解决机器学习和深度学习问题的方法，我们成功地优化了汽车巡航控制系统的PID控制器增益。
- en: Starting with initial values of *Kp* = 5.0, *Ki* = 1.0 and *Kd* = 0.0 and applying
    the AdaGrad form of the gradient descent algorithm we observed how this low-dimension
    system first wanders into a local minimum before eventually finding a more satisfactory
    response with zero overshoot, fast settling time and near-zero steady-state error.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从初始值 *Kp* = 5.0，*Ki* = 1.0 和 *Kd* = 0.0 开始，应用梯度下降算法的AdaGrad形式，我们观察到该低维系统如何首先进入局部最小值，然后最终找到一个更满意的响应，具有零超调、快速的稳定时间和接近零的稳态误差。
- en: In this article, we have seen how gradient descent can be a powerful tool when
    applied to classic engineering optimization problems. Beyond the example presented
    here, gradient descent can be utilised to solve other engineering problems like
    inverse kinematics in robotics, topology optimization and many more.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们看到梯度下降在应用于经典工程优化问题时可以成为一个强大的工具。除了这里展示的例子外，梯度下降还可以用于解决其他工程问题，如机器人中的逆向运动学、拓扑优化等。
- en: Do you have an optimization problem that you think gradient descent could be
    applied to? Let me know in the comments below.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 您是否有一个认为可以应用梯度下降的优化问题？在下面的评论中告诉我。
- en: '**Enjoyed reading this article?**'
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**喜欢阅读这篇文章吗？**'
- en: ''
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/@callum.bruce1) and [subscribe](https://medium.com/@callum.bruce1/subscribe)
    for more content like this — share it with your network — try applying gradient
    descent to your own optimization problems.'
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/@callum.bruce1)并[订阅](https://medium.com/@callum.bruce1/subscribe)获取更多类似内容——与您的网络分享——尝试将梯度下降应用于您自己的优化问题。'
- en: '*All images unless otherwise noted are by the author.*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有图片，除非另有说明，均由作者提供。*'
- en: References
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Web
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络
- en: '[1] GitHub (2023), [pid_controller_gradient_descent](https://github.com/c-bruce/pid_controller_gradient_descent)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] GitHub (2023), [pid_controller_gradient_descent](https://github.com/c-bruce/pid_controller_gradient_descent)'
- en: '[2] Wikipedia (2023), [Ziegler–Nichols method](https://en.wikipedia.org/wiki/Ziegler–Nichols_method)
    (accessed 10th July 2023)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 维基百科 (2023)，[齐格勒–尼科尔斯法](https://en.wikipedia.org/wiki/Ziegler–Nichols_method)（访问日期：2023年7月10日）'
