- en: 'PID Controller Optimization: A Gradient Descent Approach'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PIDæ§åˆ¶å™¨ä¼˜åŒ–ï¼šæ¢¯åº¦ä¸‹é™æ–¹æ³•
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2](https://towardsdatascience.com/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2](https://towardsdatascience.com/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2)
- en: Using machine learning to solve engineering optimization problems
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æœºå™¨å­¦ä¹ è§£å†³å·¥ç¨‹ä¼˜åŒ–é—®é¢˜
- en: '[](https://medium.com/@callum.bruce1?source=post_page-----58876e14eef2--------------------------------)[![Callum
    Bruce](../Images/4833a199a9449434777fdf5ce913a9cb.png)](https://medium.com/@callum.bruce1?source=post_page-----58876e14eef2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----58876e14eef2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----58876e14eef2--------------------------------)
    [Callum Bruce](https://medium.com/@callum.bruce1?source=post_page-----58876e14eef2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@callum.bruce1?source=post_page-----58876e14eef2--------------------------------)[![Callum
    Bruce](../Images/4833a199a9449434777fdf5ce913a9cb.png)](https://medium.com/@callum.bruce1?source=post_page-----58876e14eef2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----58876e14eef2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----58876e14eef2--------------------------------)
    [Callum Bruce](https://medium.com/@callum.bruce1?source=post_page-----58876e14eef2--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----58876e14eef2--------------------------------)
    Â·10 min readÂ·Aug 1, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----58876e14eef2--------------------------------)
    Â·10åˆ†é’Ÿé˜…è¯»Â·2023å¹´8æœˆ1æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4350abf8767e1ec8d3ce9a121cb47b01.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4350abf8767e1ec8d3ce9a121cb47b01.png)'
- en: The gradient descent algorithm steps downhill to minimize a cost function
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™ç®—æ³•æ²¿ç€ä¸‹å¡çš„æ–¹å‘å‡å°‘æˆæœ¬å‡½æ•°ã€‚
- en: Machine learning. Deep learning. AI. More and more people use these technologies
    every day. This has largely been driven by the rise of Large Language Models deployed
    by the likes of ChatGPT, Bard and others. Despite their widespread use, relatively
    few people are familiar with the methods underpinning these technologies.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ ã€‚æ·±åº¦å­¦ä¹ ã€‚äººå·¥æ™ºèƒ½ã€‚è¶Šæ¥è¶Šå¤šçš„äººæ¯å¤©éƒ½åœ¨ä½¿ç”¨è¿™äº›æŠ€æœ¯ã€‚è¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºåƒChatGPTã€Bardç­‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å…´èµ·ã€‚å°½ç®¡è¿™äº›æŠ€æœ¯è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†ç›¸å¯¹è¾ƒå°‘çš„äººäº†è§£è¿™äº›æŠ€æœ¯èƒŒåçš„æ–¹æ³•ã€‚
- en: 'In this article, we dive into one of the fundamental methods deployed in machine
    learning: the gradient descent algorithm.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†æœºå™¨å­¦ä¹ ä¸­ä½¿ç”¨çš„åŸºæœ¬æ–¹æ³•ä¹‹ä¸€ï¼šæ¢¯åº¦ä¸‹é™ç®—æ³•ã€‚
- en: Instead of looking at gradient descent through the lens of neural networks,
    where it is used to optimize network weights and biases, we will instead examine
    the algorithm as a tool for solving classic engineering optimization problems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä¸ä»ç¥ç»ç½‘ç»œçš„è§’åº¦æ¥å®¡è§†æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œåœ¨ç¥ç»ç½‘ç»œä¸­å®ƒç”¨äºä¼˜åŒ–ç½‘ç»œæƒé‡å’Œåç½®ï¼Œè€Œæ˜¯å°†å…¶ä½œä¸ºè§£å†³ç»å…¸å·¥ç¨‹ä¼˜åŒ–é—®é¢˜çš„å·¥å…·æ¥æ¢è®¨ã€‚
- en: Specifically, we will use gradient descent to tune the gains of a PID (Proportional-Integral-Derivative)
    controller for a car cruise control system.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ¥è°ƒæ•´æ±½è½¦å·¡èˆªæ§åˆ¶ç³»ç»Ÿä¸­PIDï¼ˆæ¯”ä¾‹-ç§¯åˆ†-å¾®åˆ†ï¼‰æ§åˆ¶å™¨çš„å¢ç›Šã€‚
- en: 'The motivation for following this approach is twofold:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: é‡‡ç”¨è¿™ç§æ–¹æ³•çš„åŠ¨æœºæœ‰ä¸¤ä¸ªï¼š
- en: First, optimizing the weights and biases in a neural network is a high-dimension
    problem. There are a lot of moving parts and I think these distract from the underlying
    utility of gradient descent for solving optimization problems.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œä¼˜åŒ–ç¥ç»ç½‘ç»œä¸­çš„æƒé‡å’Œåç½®æ˜¯ä¸€ä¸ªé«˜ç»´é—®é¢˜ã€‚æœ‰è®¸å¤šå˜é‡ï¼Œæˆ‘è®¤ä¸ºè¿™äº›ä¼šåˆ†æ•£æ¢¯åº¦ä¸‹é™åœ¨è§£å†³ä¼˜åŒ–é—®é¢˜ä¸­çš„åŸºæœ¬æ•ˆç”¨ã€‚
- en: Secondly, as you will see, gradient descent can be a powerful tool when applied
    to classic engineering problems like PID controller tuning, inverse kinematics
    in robotics and topology optimization. Gradient descent is a tool that, in my
    opinion, more engineers should be familiar with and able to utilize.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶æ¬¡ï¼Œæ­£å¦‚ä½ å°†çœ‹åˆ°çš„ï¼Œæ¢¯åº¦ä¸‹é™åœ¨åº”ç”¨äºç»å…¸å·¥ç¨‹é—®é¢˜å¦‚PIDæ§åˆ¶å™¨è°ƒä¼˜ã€æœºå™¨äººä¸­çš„é€†è¿åŠ¨å­¦å’Œæ‹“æ‰‘ä¼˜åŒ–æ—¶ï¼Œå¯ä»¥æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ã€‚æ¢¯åº¦ä¸‹é™æ˜¯ä¸€ä¸ªåœ¨æˆ‘çœ‹æ¥ï¼Œæ›´å¤šå·¥ç¨‹å¸ˆåº”è¯¥ç†Ÿæ‚‰å¹¶èƒ½å¤Ÿåˆ©ç”¨çš„å·¥å…·ã€‚
- en: After reading this article you will understand what a PID controller is, how
    the gradient descent algorithm works and how it can be applied to solve classic
    engineering optimization problems. You might be motivated to use gradient descent
    to tackle optimization challenges of your own.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: é˜…è¯»æœ¬æ–‡åï¼Œä½ å°†äº†è§£ä»€ä¹ˆæ˜¯PIDæ§åˆ¶å™¨ï¼Œæ¢¯åº¦ä¸‹é™ç®—æ³•çš„å·¥ä½œåŸç†ï¼Œä»¥åŠå®ƒå¦‚ä½•åº”ç”¨äºè§£å†³ç»å…¸å·¥ç¨‹ä¼˜åŒ–é—®é¢˜ã€‚ä½ å¯èƒ½ä¼šå—åˆ°æ¿€åŠ±ï¼Œä½¿ç”¨æ¢¯åº¦ä¸‹é™æ¥åº”å¯¹è‡ªå·±çš„ä¼˜åŒ–æŒ‘æˆ˜ã€‚
- en: All code used in this article is available [here on GitHub](https://github.com/c-bruce/pid_controller_gradient_descent).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡ä¸­ä½¿ç”¨çš„æ‰€æœ‰ä»£ç å¯ä»¥åœ¨[GitHub ä¸Šæ‰¾åˆ°](https://github.com/c-bruce/pid_controller_gradient_descent)ã€‚
- en: What is a PID controller?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯ PID æ§åˆ¶å™¨ï¼Ÿ
- en: A PID controller is a widely used feedback control mechanism in engineering
    and automated systems. It aims to maintain a desired setpoint by continuously
    adjusting the control signal based on the error between the setpoint and the systemâ€™s
    measured output (the process variable).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: PID æ§åˆ¶å™¨æ˜¯å·¥ç¨‹å’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿä¸­å¹¿æ³›ä½¿ç”¨çš„åé¦ˆæ§åˆ¶æœºåˆ¶ã€‚å…¶ç›®æ ‡æ˜¯é€šè¿‡æŒç»­è°ƒæ•´æ§åˆ¶ä¿¡å·ä»¥ç»´æŒæœŸæœ›çš„è®¾å®šç‚¹ï¼ŒåŸºäºè®¾å®šç‚¹å’Œç³»ç»Ÿæµ‹é‡è¾“å‡ºï¼ˆè¿‡ç¨‹å˜é‡ï¼‰ä¹‹é—´çš„è¯¯å·®ã€‚
- en: '![](../Images/4bebce93124edeb5a68f3e150a4cc86f.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bebce93124edeb5a68f3e150a4cc86f.png)'
- en: Typical step response of a PID controller
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: PID æ§åˆ¶å™¨çš„å…¸å‹é˜¶è·ƒå“åº”
- en: PID controllers find extensive applications in various industries and domains.
    They are widely used in process control systems, such as temperature control in
    manufacturing, flow control in chemical plants, and pressure control in HVAC systems.
    PID controllers are also employed in robotics for precise positioning and motion
    control, as well as in automotive systems for throttle control, engine speed regulation,
    and anti-lock braking systems. They play a vital role in aerospace and aviation
    applications, including aircraft autopilots and attitude control systems.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: PID æ§åˆ¶å™¨åœ¨å„ä¸ªè¡Œä¸šå’Œé¢†åŸŸä¸­åº”ç”¨å¹¿æ³›ã€‚å®ƒä»¬è¢«å¹¿æ³›ç”¨äºè¿‡ç¨‹æ§åˆ¶ç³»ç»Ÿï¼Œå¦‚åˆ¶é€ ä¸šä¸­çš„æ¸©åº¦æ§åˆ¶ã€åŒ–å·¥å‚ä¸­çš„æµé‡æ§åˆ¶ä»¥åŠ HVAC ç³»ç»Ÿä¸­çš„å‹åŠ›æ§åˆ¶ã€‚PID
    æ§åˆ¶å™¨è¿˜è¢«åº”ç”¨äºæœºå™¨äººæŠ€æœ¯ä¸­ä»¥å®ç°ç²¾ç¡®å®šä½å’Œè¿åŠ¨æ§åˆ¶ï¼Œä»¥åŠæ±½è½¦ç³»ç»Ÿä¸­çš„æ²¹é—¨æ§åˆ¶ã€å‘åŠ¨æœºé€Ÿåº¦è°ƒèŠ‚å’Œé˜²æŠ±æ­»åˆ¹è½¦ç³»ç»Ÿã€‚å®ƒä»¬åœ¨èˆªç©ºèˆªå¤©å’Œèˆªç©ºåº”ç”¨ä¸­ä¹Ÿå‘æŒ¥ç€é‡è¦ä½œç”¨ï¼ŒåŒ…æ‹¬é£æœºè‡ªåŠ¨é©¾é©¶ä»ªå’Œå§¿æ€æ§åˆ¶ç³»ç»Ÿã€‚
- en: 'A PID controller consists of three components: the proportional term, the integral
    term, and the derivative term. The proportional term provides an immediate response
    to the current error, the integral term accumulates and corrects for past errors,
    and the derivative term predicts and counteracts future error trends.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: PID æ§åˆ¶å™¨ç”±ä¸‰ä¸ªç»„æˆéƒ¨åˆ†ç»„æˆï¼šæ¯”ä¾‹é¡¹ã€ç§¯åˆ†é¡¹å’Œå¾®åˆ†é¡¹ã€‚æ¯”ä¾‹é¡¹å¯¹å½“å‰è¯¯å·®åšå‡ºå³æ—¶å“åº”ï¼Œç§¯åˆ†é¡¹ç´¯ç§¯å¹¶ä¿®æ­£è¿‡å»çš„è¯¯å·®ï¼Œå¾®åˆ†é¡¹é¢„æµ‹å¹¶æŠµæ¶ˆæœªæ¥çš„è¯¯å·®è¶‹åŠ¿ã€‚
- en: '![](../Images/6fe42cfd42b6ca09246fce764897838c.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6fe42cfd42b6ca09246fce764897838c.png)'
- en: PID controller block diagram
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: PID æ§åˆ¶å™¨æ¡†å›¾
- en: The control loop of a PID controller is presented in the block diagram above.
    *r(t)* is the setpoint and *y(t)* is the process variable. The process variable
    is subtracted from the setpoint to get the error signal, *e(t)*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šå›¾ä¸­çš„ PID æ§åˆ¶å™¨æ§åˆ¶å›è·¯å±•ç¤ºäº† *r(t)* æ˜¯è®¾å®šç‚¹ï¼Œ*y(t)* æ˜¯è¿‡ç¨‹å˜é‡ã€‚è¿‡ç¨‹å˜é‡ä»è®¾å®šç‚¹ä¸­å‡å»ä»¥è·å¾—è¯¯å·®ä¿¡å· *e(t)*ã€‚
- en: The control signal, *u(t)*, is the sum of the proportional, integral and derivative
    terms. The control signal is input to the process and this, in turn, causes the
    process variable to update.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ§åˆ¶ä¿¡å·ï¼Œ*u(t)*ï¼Œæ˜¯æ¯”ä¾‹ã€ç§¯åˆ†å’Œå¾®åˆ†é¡¹çš„æ€»å’Œã€‚æ§åˆ¶ä¿¡å·è¢«è¾“å…¥åˆ°è¿‡ç¨‹å½“ä¸­ï¼Œè¿™ä¼šå¯¼è‡´è¿‡ç¨‹å˜é‡æ›´æ–°ã€‚
- en: PID controller control signal u(t)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: PID æ§åˆ¶å™¨æ§åˆ¶ä¿¡å· u(t)
- en: The gradient descent algorithm
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™ç®—æ³•
- en: Gradient descent is an optimization algorithm commonly used in machine learning
    and mathematical optimization. It aims to find the minimum of a given cost function
    by iteratively adjusting the parameters based on the cost function gradient. The
    gradient points in the direction of the steepest ascent, so by taking steps in
    the opposite direction, the algorithm gradually converges towards the optimal
    solution.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™æ˜¯ä¸€ç§å¸¸ç”¨äºæœºå™¨å­¦ä¹ å’Œæ•°å­¦ä¼˜åŒ–çš„ä¼˜åŒ–ç®—æ³•ã€‚å®ƒé€šè¿‡åŸºäºæˆæœ¬å‡½æ•°æ¢¯åº¦è¿­ä»£è°ƒæ•´å‚æ•°ï¼Œæ—¨åœ¨æ‰¾åˆ°ç»™å®šæˆæœ¬å‡½æ•°çš„æœ€å°å€¼ã€‚æ¢¯åº¦æŒ‡å‘æœ€é™¡çš„ä¸Šå‡æ–¹å‘ï¼Œå› æ­¤é€šè¿‡æœç›¸åæ–¹å‘è¿ˆæ­¥ï¼Œç®—æ³•é€æ¸æ”¶æ•›åˆ°æœ€ä¼˜è§£ã€‚
- en: 'A single gradient descent update step is defined as:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å•æ­¥æ¢¯åº¦ä¸‹é™æ›´æ–°å®šä¹‰ä¸ºï¼š
- en: Gradient descent update step
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™æ›´æ–°æ­¥éª¤
- en: Where ***a****â‚™*is a vector of input parameters. The subscript *n* denotes iteration.
    *f(****a****â‚™)* is a multi-variable cost function and âˆ‡*f(****a****) is* the gradient
    of that cost function. âˆ‡*f(****a****â‚™)* represents the direction of steepest ascent
    so it is subtracted from ***a****â‚™* to reduce the cost function on the next iteration.
    ğ›¾ is the learning rate which determines the step size at each iteration.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '***a****â‚™* æ˜¯è¾“å…¥å‚æ•°çš„å‘é‡ã€‚ä¸‹æ ‡ *n* è¡¨ç¤ºè¿­ä»£ã€‚*f(****a****â‚™)* æ˜¯å¤šå˜é‡æˆæœ¬å‡½æ•°ï¼Œâˆ‡*f(****a****)* æ˜¯è¯¥æˆæœ¬å‡½æ•°çš„æ¢¯åº¦ã€‚âˆ‡*f(****a****â‚™)*
    ä»£è¡¨æœ€é™¡å‡é«˜çš„æ–¹å‘ï¼Œå› æ­¤å®ƒä» ***a****â‚™* ä¸­å‡å»ä»¥åœ¨ä¸‹ä¸€æ¬¡è¿­ä»£ä¸­å‡å°‘æˆæœ¬å‡½æ•°ã€‚ğ›¾ æ˜¯å­¦ä¹ ç‡ï¼Œå†³å®šäº†æ¯æ¬¡è¿­ä»£çš„æ­¥é•¿ã€‚'
- en: An appropriate value for ğ›¾ must be selected. Too big and the steps taken at
    each iteration will be too large and cause the gradient descent algorithm to not
    converge. Too small and the gradient descent algorithm will be computationally
    expensive and take a long time to converge.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å¿…é¡»é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„ğ›¾å€¼ã€‚å¦‚æœå€¼è¿‡å¤§ï¼Œæ¯æ¬¡è¿­ä»£æ—¶é‡‡å–çš„æ­¥éª¤å°†è¿‡å¤§ï¼Œå¯¼è‡´æ¢¯åº¦ä¸‹é™ç®—æ³•æ— æ³•æ”¶æ•›ã€‚å¦‚æœå€¼è¿‡å°ï¼Œæ¢¯åº¦ä¸‹é™ç®—æ³•å°†è®¡ç®—å¼€é”€å¤§ï¼Œæ”¶æ•›æ—¶é—´é•¿ã€‚
- en: '![](../Images/aba8bdc7d4833adb595ed30d9acda58e.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aba8bdc7d4833adb595ed30d9acda58e.png)'
- en: Gradient descent algorithm applied to y=xÂ² cost function (initially x=5) for
    ğ›¾=0.1 (LHS) and ğ›¾=1.02 (RHS)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™ç®—æ³•åº”ç”¨äºy=xÂ²ä»£ä»·å‡½æ•°ï¼ˆåˆå§‹x=5ï¼‰ï¼Œå¯¹äºğ›¾=0.1ï¼ˆå·¦ä¾§ï¼‰å’Œğ›¾=1.02ï¼ˆå³ä¾§ï¼‰
- en: Gradient descent is applied in a wide range of fields and disciplines. In machine
    learning and deep learning, it is a fundamental optimization algorithm used to
    train neural networks and optimize their parameters. By iteratively updating the
    weights and biases of the network based on the gradient of the cost function,
    gradient descent enables the network to learn and improve its performance over
    time.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™åœ¨å¹¿æ³›çš„é¢†åŸŸå’Œå­¦ç§‘ä¸­åº”ç”¨ã€‚åœ¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ä¸­ï¼Œå®ƒæ˜¯ç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œå’Œä¼˜åŒ–å…¶å‚æ•°çš„åŸºæœ¬ä¼˜åŒ–ç®—æ³•ã€‚é€šè¿‡æ ¹æ®ä»£ä»·å‡½æ•°çš„æ¢¯åº¦è¿­ä»£æ›´æ–°ç½‘ç»œçš„æƒé‡å’Œåå·®ï¼Œæ¢¯åº¦ä¸‹é™ä½¿ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å¹¶éšç€æ—¶é—´çš„æ¨ç§»æé«˜å…¶æ€§èƒ½ã€‚
- en: Beyond machine learning, gradient descent is utilized in various optimization
    problems across engineering, physics, economics, and other domains. It assists
    in parameter estimation, system identification, signal processing, image reconstruction,
    and many other tasks that require finding the minimum or maximum of a function.
    The versatility and effectiveness of gradient descent make it an essential tool
    for solving optimization problems and improving models and systems across diverse
    fields.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†æœºå™¨å­¦ä¹ ä¹‹å¤–ï¼Œæ¢¯åº¦ä¸‹é™è¢«åº”ç”¨äºå·¥ç¨‹ã€ç‰©ç†å­¦ã€ç»æµå­¦åŠå…¶ä»–é¢†åŸŸçš„å„ç§ä¼˜åŒ–é—®é¢˜ä¸­ã€‚å®ƒå¸®åŠ©è¿›è¡Œå‚æ•°ä¼°è®¡ã€ç³»ç»Ÿè¯†åˆ«ã€ä¿¡å·å¤„ç†ã€å›¾åƒé‡å»ºä»¥åŠè®¸å¤šå…¶ä»–éœ€è¦æ‰¾åˆ°å‡½æ•°æœ€å°å€¼æˆ–æœ€å¤§å€¼çš„ä»»åŠ¡ã€‚æ¢¯åº¦ä¸‹é™çš„å¤šåŠŸèƒ½æ€§å’Œæœ‰æ•ˆæ€§ä½¿å…¶æˆä¸ºè§£å†³ä¼˜åŒ–é—®é¢˜å’Œæ”¹å–„å„é¢†åŸŸæ¨¡å‹å’Œç³»ç»Ÿçš„é‡è¦å·¥å…·ã€‚
- en: Optimizing PID controller gains using gradient descent
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–PIDæ§åˆ¶å™¨å¢ç›Š
- en: There are several methods available to tune a PID controller. These include
    the manual tuning method and heuristic methods like the [Ziegler-Nichols method](https://en.wikipedia.org/wiki/Ziegler%E2%80%93Nichols_method).
    The manual tuning method can be time-consuming and may require multiple iterations
    to find optimal values while the Ziegler-Nichols method often yields aggressive
    gains and large overshoot which means it is not suitable for certain applications.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å‡ ç§æ–¹æ³•å¯ä»¥è°ƒæ•´PIDæ§åˆ¶å™¨ã€‚è¿™äº›æ–¹æ³•åŒ…æ‹¬æ‰‹åŠ¨è°ƒèŠ‚æ³•å’Œåƒ[é½æ ¼å‹’-å°¼ç§‘å°”æ–¯æ³•](https://en.wikipedia.org/wiki/Ziegler%E2%80%93Nichols_method)è¿™æ ·çš„å¯å‘å¼æ–¹æ³•ã€‚æ‰‹åŠ¨è°ƒèŠ‚æ³•å¯èƒ½è€—æ—¶ä¸”å¯èƒ½éœ€è¦å¤šæ¬¡è¿­ä»£æ‰èƒ½æ‰¾åˆ°æœ€ä½³å€¼ï¼Œè€Œé½æ ¼å‹’-å°¼ç§‘å°”æ–¯æ³•å¾€å¾€ä¼šäº§ç”Ÿæ¿€è¿›çš„å¢ç›Šå’Œè¾ƒå¤§çš„è¶…è°ƒï¼Œè¿™æ„å‘³ç€å®ƒä¸é€‚åˆæŸäº›åº”ç”¨ã€‚
- en: Presented here is a gradient descent approach to PID controller optimization.
    We will optimize the control system of a car cruise control system subject to
    a step change in setpoint.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œå±•ç¤ºäº†ä¸€ç§æ¢¯åº¦ä¸‹é™æ–¹æ³•æ¥ä¼˜åŒ–PIDæ§åˆ¶å™¨ã€‚æˆ‘ä»¬å°†ä¼˜åŒ–ä¸€ä¸ªè½¦è½½å·¡èˆªæ§åˆ¶ç³»ç»Ÿçš„æ§åˆ¶ç³»ç»Ÿï¼Œä»¥åº”å¯¹è®¾å®šç‚¹çš„é˜¶è·ƒå˜åŒ–ã€‚
- en: By controlling the pedal position, the controllerâ€™s objective is to accelerate
    the car up to the velocity setpoint with minimum overshoot, settling time and
    steady-state error.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ§åˆ¶è¸æ¿ä½ç½®ï¼Œæ§åˆ¶å™¨çš„ç›®æ ‡æ˜¯å°†è½¦è¾†åŠ é€Ÿåˆ°é€Ÿåº¦è®¾å®šç‚¹ï¼ŒåŒæ—¶æœ€å°åŒ–è¶…è°ƒã€ç¨³å®šæ—¶é—´å’Œç¨³æ€è¯¯å·®ã€‚
- en: The car is subject to a driving force proportional to the pedal position. Rolling
    resistance and aerodynamic drag forces act in the opposite direction to the driving
    force. Pedal position is controlled by the PID controller and limited to within
    a range of -50% to 100%. When the pedal position is negative, the car is braking.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è½¦è¾†å—åˆ°ä¸è¸æ¿ä½ç½®æˆæ¯”ä¾‹çš„é©±åŠ¨åŠ›ã€‚æ»šåŠ¨é˜»åŠ›å’Œç©ºæ°”é˜»åŠ›ä½œç”¨äºä¸é©±åŠ¨åŠ›ç›¸åçš„æ–¹å‘ã€‚è¸æ¿ä½ç½®ç”±PIDæ§åˆ¶å™¨æ§åˆ¶ï¼Œå¹¶é™åˆ¶åœ¨-50%åˆ°100%èŒƒå›´å†…ã€‚å½“è¸æ¿ä½ç½®ä¸ºè´Ÿå€¼æ—¶ï¼Œè½¦è¾†åœ¨åˆ¶åŠ¨ã€‚
- en: 'It is helpful to have a model of the system when tuning PID controller gains.
    This is so that we can simulate the system response. For this I have implemented
    a `Car` class in Python:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è°ƒæ•´PIDæ§åˆ¶å™¨å¢ç›Šæ—¶ï¼Œæ‹¥æœ‰ç³»ç»Ÿæ¨¡å‹æ˜¯æœ‰å¸®åŠ©çš„ã€‚è¿™æ ·æˆ‘ä»¬å¯ä»¥æ¨¡æ‹Ÿç³»ç»Ÿå“åº”ã€‚ä¸ºæ­¤ï¼Œæˆ‘åœ¨Pythonä¸­å®ç°äº†ä¸€ä¸ª`Car`ç±»ï¼š
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The`PIDController` class is implemented as:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`PIDController`ç±»çš„å®ç°å¦‚ä¸‹ï¼š'
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Taking this object-oriented programming approach makes it much easier to set
    up and run multiple simulations with different PID controller gains as we must
    do when running the gradient descent algorithm.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: é‡‡å–è¿™ç§é¢å‘å¯¹è±¡ç¼–ç¨‹çš„æ–¹æ³•ä½¿å¾—è®¾ç½®å’Œè¿è¡Œå¤šä¸ªå¸¦æœ‰ä¸åŒPIDæ§åˆ¶å™¨å¢ç›Šçš„ä»¿çœŸå˜å¾—æ›´åŠ å®¹æ˜“ï¼Œè¿™åœ¨è¿è¡Œæ¢¯åº¦ä¸‹é™ç®—æ³•æ—¶æ˜¯å¿…é¡»çš„ã€‚
- en: 'The`GradientDescent` class is implemented as:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The algorithm is run for a specified number of iterations by calling `execute`
    or `execute_adagrad`. The `execute_adagrad` method executes a modified form of
    gradient descent called AdaGrad (adaptive gradient descent).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: AdaGrad has per-parameter learning rates that increase for sparse parameters
    and decrease for less sparse parameters. The learning rate is updated after each
    iteration based on a historical sum of the gradients squared.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use AdaGrad to optimize the PID controller gains for the car cruise
    control system. Using AdaGrad, the gradient descent update equation becomes:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: AdaGrad gradient descent update step
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to define our cost function. The cost function must take a vector
    of input parameters as input and return a single number; the cost. The objective
    of the car cruise control is to accelerate the car up to the velocity setpoint
    with minimum overshoot, settling time and steady-state error. There are many ways
    we could define the cost function based on this objective. Here we will define
    it as the integral of the error magnitude over time:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Car cruise control cost function
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our cost function is an integral, we can visualize it as the area under
    the error magnitude curve. We expect to see the area under the curve reduce as
    we approach the global minimum. Programmatically, the cost function is defined
    as:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The cost function includes the simulation parameters. The simulation is run
    for 60 seconds. During this time we observe the response of the system to a step
    change in setpoint from 0 m/s to 20 m/s. By integrating error magnitude over time,
    the cost is calculated for every iteration.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, all that is left to do is run the optimization. We will start with initial
    values of *Kp* = 5.0, *Ki* = 1.0 and *Kd* = 0.0\. These values give a steady,
    oscillating response, with overshoot, that eventually converges to the setpoint.
    From this start point we will run the gradient descent algorithm for 500 iterations
    using a base learning rate of ğ›¾=0.1:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/aef6802298914bd17d835a1e6233d53f.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: Car cruise control step response (LHS), error magnitude (middle) and cost (RHS)
    as the gradient descent algorithm iterates toward an optimal solution
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: The animated plot above shows the evolution of the car cruise control step response
    as the gradient descent algorithm tunes the *Kp*, *Ki* and *Kd* gains of the PID
    controller.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: By iteration 25, the gradient descent algorithm has eliminated the oscillatory
    response. After this point, something interesting happens. The algorithm wanders
    into a local minimum characterized by an overshoot of ~ 3 m/s. This happens in
    the region of 6.0 < *Kp* < 7.5, *Ki* ~= 0.5, *Kd* = 0.0 and lasts right up to
    iteration 300.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: After iteration 300, the algorithm moves out of the local minimum to find a
    more satisfactory response closer to the global minimum. The response is now characterized
    by zero overshoot, fast settling time and near-zero steady-state error.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Running the gradient descent algorithm for 500 iterations we arrive at our optimized
    PID controller gains; *Kp* = 8.33, *Ki* = 0.12 and *Kd* = 0.00.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œæ¢¯åº¦ä¸‹é™ç®—æ³•500æ¬¡è¿­ä»£åï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¼˜åŒ–çš„PIDæ§åˆ¶å™¨å¢ç›Šï¼›*Kp* = 8.33ï¼Œ*Ki* = 0.12 å’Œ *Kd* = 0.00ã€‚
- en: The proportional gain is still rising steadily. Running more iterations (not
    shown here), as *Kp* slowly increases, we find further reduction to the cost function
    is possible though this effect becomes increasingly marginal.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯”ä¾‹å¢ç›Šä»åœ¨ç¨³æ­¥ä¸Šå‡ã€‚è¿è¡Œæ›´å¤šçš„è¿­ä»£ï¼ˆæ­¤å¤„æœªæ˜¾ç¤ºï¼‰ï¼Œéšç€ *Kp* çš„ç¼“æ…¢å¢åŠ ï¼Œæˆ‘ä»¬å‘ç°å¯¹æˆæœ¬å‡½æ•°çš„è¿›ä¸€æ­¥å‡å°‘æ˜¯å¯èƒ½çš„ï¼Œå°½ç®¡è¿™ç§æ•ˆæœé€æ¸å˜å¾—è¾¹é™…åŒ–ã€‚
- en: Summary
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: Adopting a method widely used for solving machine learning and deep learning
    problems, we have successfully optimized PID controller gains for a car cruise
    control system.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: é‡‡ç”¨ä¸€ç§å¹¿æ³›ç”¨äºè§£å†³æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ é—®é¢˜çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æˆåŠŸåœ°ä¼˜åŒ–äº†æ±½è½¦å·¡èˆªæ§åˆ¶ç³»ç»Ÿçš„PIDæ§åˆ¶å™¨å¢ç›Šã€‚
- en: Starting with initial values of *Kp* = 5.0, *Ki* = 1.0 and *Kd* = 0.0 and applying
    the AdaGrad form of the gradient descent algorithm we observed how this low-dimension
    system first wanders into a local minimum before eventually finding a more satisfactory
    response with zero overshoot, fast settling time and near-zero steady-state error.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä»åˆå§‹å€¼ *Kp* = 5.0ï¼Œ*Ki* = 1.0 å’Œ *Kd* = 0.0 å¼€å§‹ï¼Œåº”ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•çš„AdaGradå½¢å¼ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è¯¥ä½ç»´ç³»ç»Ÿå¦‚ä½•é¦–å…ˆè¿›å…¥å±€éƒ¨æœ€å°å€¼ï¼Œç„¶åæœ€ç»ˆæ‰¾åˆ°ä¸€ä¸ªæ›´æ»¡æ„çš„å“åº”ï¼Œå…·æœ‰é›¶è¶…è°ƒã€å¿«é€Ÿçš„ç¨³å®šæ—¶é—´å’Œæ¥è¿‘é›¶çš„ç¨³æ€è¯¯å·®ã€‚
- en: In this article, we have seen how gradient descent can be a powerful tool when
    applied to classic engineering optimization problems. Beyond the example presented
    here, gradient descent can be utilised to solve other engineering problems like
    inverse kinematics in robotics, topology optimization and many more.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°æ¢¯åº¦ä¸‹é™åœ¨åº”ç”¨äºç»å…¸å·¥ç¨‹ä¼˜åŒ–é—®é¢˜æ—¶å¯ä»¥æˆä¸ºä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ã€‚é™¤äº†è¿™é‡Œå±•ç¤ºçš„ä¾‹å­å¤–ï¼Œæ¢¯åº¦ä¸‹é™è¿˜å¯ä»¥ç”¨äºè§£å†³å…¶ä»–å·¥ç¨‹é—®é¢˜ï¼Œå¦‚æœºå™¨äººä¸­çš„é€†å‘è¿åŠ¨å­¦ã€æ‹“æ‰‘ä¼˜åŒ–ç­‰ã€‚
- en: Do you have an optimization problem that you think gradient descent could be
    applied to? Let me know in the comments below.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨æ˜¯å¦æœ‰ä¸€ä¸ªè®¤ä¸ºå¯ä»¥åº”ç”¨æ¢¯åº¦ä¸‹é™çš„ä¼˜åŒ–é—®é¢˜ï¼Ÿåœ¨ä¸‹é¢çš„è¯„è®ºä¸­å‘Šè¯‰æˆ‘ã€‚
- en: '**Enjoyed reading this article?**'
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**å–œæ¬¢é˜…è¯»è¿™ç¯‡æ–‡ç« å—ï¼Ÿ**'
- en: ''
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/@callum.bruce1) and [subscribe](https://medium.com/@callum.bruce1/subscribe)
    for more content like this â€” share it with your network â€” try applying gradient
    descent to your own optimization problems.'
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/@callum.bruce1)å¹¶[è®¢é˜…](https://medium.com/@callum.bruce1/subscribe)è·å–æ›´å¤šç±»ä¼¼å†…å®¹â€”â€”ä¸æ‚¨çš„ç½‘ç»œåˆ†äº«â€”â€”å°è¯•å°†æ¢¯åº¦ä¸‹é™åº”ç”¨äºæ‚¨è‡ªå·±çš„ä¼˜åŒ–é—®é¢˜ã€‚'
- en: '*All images unless otherwise noted are by the author.*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ‰€æœ‰å›¾ç‰‡ï¼Œé™¤éå¦æœ‰è¯´æ˜ï¼Œå‡ç”±ä½œè€…æä¾›ã€‚*'
- en: References
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: Web
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç½‘ç»œ
- en: '[1] GitHub (2023), [pid_controller_gradient_descent](https://github.com/c-bruce/pid_controller_gradient_descent)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] GitHub (2023), [pid_controller_gradient_descent](https://github.com/c-bruce/pid_controller_gradient_descent)'
- en: '[2] Wikipedia (2023), [Zieglerâ€“Nichols method](https://en.wikipedia.org/wiki/Zieglerâ€“Nichols_method)
    (accessed 10th July 2023)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] ç»´åŸºç™¾ç§‘ (2023)ï¼Œ[é½æ ¼å‹’â€“å°¼ç§‘å°”æ–¯æ³•](https://en.wikipedia.org/wiki/Zieglerâ€“Nichols_method)ï¼ˆè®¿é—®æ—¥æœŸï¼š2023å¹´7æœˆ10æ—¥ï¼‰'
