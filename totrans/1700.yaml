- en: 'Prompt Engineering: How to Trick AI into Solving Your Problems'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f](https://towardsdatascience.com/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 7 prompting tricks, LangChain, and Python example code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://shawhin.medium.com/?source=post_page-----7ce1ed3b553f--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----7ce1ed3b553f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7ce1ed3b553f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7ce1ed3b553f--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----7ce1ed3b553f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7ce1ed3b553f--------------------------------)
    ·14 min read·Aug 25, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: This is the fourth article in a [series on using large language models](/a-practical-introduction-to-llms-65194dda1148)
    (LLMs) in practice. Here, I will discuss prompt engineering (PE) and how to use
    it to build LLM-enabled applications. I start by reviewing key PE techniques and
    then walk through Python example code of using LangChain to build an LLM-based
    application.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/51cee05215f7e6b279687f028ed20dcc.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jason Leung](https://unsplash.com/@ninjason?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: When first hearing about prompt engineering, many technical people (including
    myself) tend to scoff at the idea. We might think, “*Prompt engineering? Psssh,
    that’s lame. Tell me how to build an LLM from scratch.*”
  prefs: []
  type: TYPE_NORMAL
- en: However, after diving into it more deeply, I’d caution developers against writing
    off prompt engineering automatically. I’ll go even further and say that **prompt
    engineering can realize 80% of the value** of most LLM use cases with (relatively)
    very low effort.
  prefs: []
  type: TYPE_NORMAL
- en: My goal with this article is to convey this point via a practical review of
    prompt engineering and illustrative examples. While there are surely gaps in what
    prompt engineering can do, it opens the door to discovering simple and clever
    solutions to our problems.
  prefs: []
  type: TYPE_NORMAL
- en: Supplemental Video.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is Prompt Engineering?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the [first article of this series](/a-practical-introduction-to-llms-65194dda1148),
    I defined **prompt engineering** as **any use of an LLM out-of-the-box** (i.e.
    not training any internal model parameters). However, there is much more that
    can be said about it.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Engineering is “*the means by which LLMs are programmed with prompts.*”
    [[1](https://arxiv.org/abs/2302.11382)]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prompt Engineering is “a*n empirical art of composing and formatting the prompt
    to maximize a model’s performance on a desired task.*” [[2](https://arxiv.org/abs/2106.09685)]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*“language models… want to complete documents, and so you can trick them into
    performing tasks just by arranging fake documents*.” [[3](https://www.youtube.com/watch?v=bZQun8Y4L2A)]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first definition conveys the key innovation coming from LLMs, which is that
    **computers can now be programmed using plain English**. The second point frames
    prompt engineering as a largely empirical endeavor, where practitioners, tinkerers,
    and builders are the key explorers of this new way of programming.
  prefs: []
  type: TYPE_NORMAL
- en: The third point (from [Andrej Karpathy](https://medium.com/u/ac9d9a35533e?source=post_page-----7ce1ed3b553f--------------------------------))
    reminds us that **LLMs aren’t explicitly trained to do almost anything we ask
    them to do**. Thus, in some sense, we are “tricking” these language models to
    solve problems. I feel this captures the essence of prompt engineering, which
    relies less on your technical skills and more on your creativity.
  prefs: []
  type: TYPE_NORMAL
- en: '**2 Levels of Prompt Engineering**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two distinct ways in which one can do prompt engineering, which I
    called the “**easy way**” and the “**less easy way**” in the [first article](/a-practical-introduction-to-llms-65194dda1148)
    of this series.
  prefs: []
  type: TYPE_NORMAL
- en: The Easy Way
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is how most of the world does prompt engineering, which is via ChatGPT
    (or something similar). It is an intuitive, no-code, and cost-free way to interact
    with an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: While this is a great approach for something quick and simple, e.g. summarizing
    a page of text, rewriting an email, helping you brainstorm birthday party plans,
    etc., it has its downsides. A big one is that **it’s not easy to integrate this
    approach into a larger automated process or software system**. To do this, we
    need to go one step further.
  prefs: []
  type: TYPE_NORMAL
- en: The Less Easy Way
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This resolves many of the drawbacks of the “easy way” by interacting with LLMs
    programmatically i.e. using Python. We got a sense of how we can do this in the
    previous two articles of this series, where explored [OpenAI’s Python API](/cracking-open-the-openai-python-api-230e4cae7971)
    and the [Hugging Face Transformers library](/cracking-open-the-hugging-face-transformers-library-350aa0ef0161).
  prefs: []
  type: TYPE_NORMAL
- en: While this requires more technical knowledge, **this is where the real power
    of prompt engineering lies** because it allows developers to integrate LLM-based
    modules into larger software systems.
  prefs: []
  type: TYPE_NORMAL
- en: A good (and perhaps ironic) example of this is ChatGPT. The core of this product
    is prompting a pre-trained model (i.e. GPT-3.5-turbo) to act like a chatbot and
    then wrapping it in an easy-to-use web interface.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, developing GPT-3.5-turbo is the hard part, **but that’s not something
    we need to worry about here**. With all the pre-trained LLMs we have at our fingertips,
    almost anyone with basic programming skills can create a powerful AI application
    like ChatGPT without being an AI researcher or a machine learning Ph.D.
  prefs: []
  type: TYPE_NORMAL
- en: '**Building AI Apps with Prompt Engineering**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The less easy way unlocks a **new paradigm of programming and software development**.
    No longer are developers required to define every inch of logic in their software
    systems. They now have the option to offload a non-trivial portion to LLMs. Let’s
    look at a concrete example of what this might look like.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you want to create an **automatic grader for a high school history class**.
    The trouble, however, is that all the questions have written responses, so there
    often can be multiple versions of a correct answer. For example, the following
    responses to “*Who was the 35th president of the United States of America?*” could
    be correct.
  prefs: []
  type: TYPE_NORMAL
- en: John F. Kennedy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JFK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jack Kennedy (a common nickname)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: John Fitzgerald Kennedy (probably trying to get extra credit)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: John F. Kenedy (misspelled last name)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the **traditional programming paradigm**, it was on the developer to figure
    out how to account for all these variations. To do this, they might list all possible
    correct answers and use an exact string-matching algorithm or maybe even use fuzzy
    matching to help with misspelled words.
  prefs: []
  type: TYPE_NORMAL
- en: However, with this new **LLM-enabled paradigm**, **the problem can be solved
    through simple prompt engineering**. For instance, we could use the following
    prompt to evaluate student answers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We can think of this prompt as a function, where given a ***question***, ***correct_answer***,
    and ***student_answer***, it generates the student's grade. This can then be integrated
    into a larger piece of software that implements the automatic grader.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of time-saving, this prompt took me about 2 minutes to write, while
    if I were to try to develop an algorithm to do the same thing, it would take me
    hours (if not days) and probably have worse performance. **So the time savings
    for tasks like this are 100–1000x**.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there are many tasks in which LLMs do not provide any substantial
    benefit, and other existing methods are much better suited (e.g. predicting tomorrow’s
    weather). In no way are LLMs the solution to every problem, but they do create
    a new set of solutions to tasks that require processing natural language effectively—something
    that has been historically difficult for computers to do.
  prefs: []
  type: TYPE_NORMAL
- en: '**7 Tricks for Prompt Engineering**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the prompt example from before may seem like a natural and obvious way
    to frame the automatic grading task, it deliberately employed specific prompt
    engineering heuristics (or “tricks,” as I’ll call them). These (and other) tricks
    have emerged as reliable ways to improve the quality of LLM responses.
  prefs: []
  type: TYPE_NORMAL
- en: Although there are many tips and tricks for writing good prompts, here I restrict
    the discussion to the ones that seem the most fundamental (IMO) based on a handful
    of references [1,3–5]. For a deeper dive, I recommend the reader explore the sources
    cited here.
  prefs: []
  type: TYPE_NORMAL
- en: '**Trick 1: Be Descriptive (More is Better)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A defining feature of LLMs is that they are trained on massive text corpora.
    This equips them with a vast knowledge of the world and the ability to perform
    an enormous variety of tasks. However, this impressive generality may hinder performance
    on a specific task if the proper context is not provided.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s compare two prompts for generating a birthday message for
    my dad.
  prefs: []
  type: TYPE_NORMAL
- en: '***Without Trick***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '***With Trick***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Trick 2: Give Examples**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next trick is to give the LLM example responses to improve its performance
    on a particular task. The technical term for this is **few-shot learning,** and
    has been shown to improve LLM performance significantly [6].
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a specific example. Say we want to write a subtitle for a Towards
    Data Science article. We can use existing examples to help guide the LLM completion.
  prefs: []
  type: TYPE_NORMAL
- en: '***Without Trick***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '***With Trick***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Trick 3: Use Structured Text**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensuring prompts follow an organized structure not only makes them easier to
    read and write, but also tends to help the model generate good completions. We
    employed this technique in the example for **Trick 2**, where we explicitly labeled
    the *title* and *subtitle* for each example.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there are countless ways we can give our prompts structure. Here are
    a handful of examples: use ALL CAPS for emphasis, use delimiters like [PRE5]'
  prefs: []
  type: TYPE_NORMAL
- en: Write me a recipe for chocolate chip cookies.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Create a well-organized recipe for chocolate chip cookies. Use the following
    \
  prefs: []
  type: TYPE_NORMAL
- en: 'formatting elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Title**: Classic Chocolate Chip Cookies'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ingredients**: List the ingredients with precise measurements and formatting.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Instructions**: Provide step-by-step instructions in numbered format, detailing
    the baking process.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tips**: Include a separate section with helpful baking tips and possible
    variations.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Write me a LinkedIn post based on the following Medium blog.
  prefs: []
  type: TYPE_NORMAL
- en: 'Medium blog: {Medium blog text}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Write me a LinkedIn post based on the step-by-step process and Medium blog \
  prefs: []
  type: TYPE_NORMAL
- en: given below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Come up with a one line hook relevant to the blog.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Extract 3 key points from the article'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Compress each point to less than 50 characters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Combine the hook, compressed key points from Step 3, and a call to
    action \'
  prefs: []
  type: TYPE_NORMAL
- en: to generate the final output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Medium blog: {Medium blog text}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Make me a travel itinerary for a weekend in New York City.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Act as an NYC native and cabbie who knows everything about the city. \
  prefs: []
  type: TYPE_NORMAL
- en: Please make me a travel itinerary for a weekend in New York City based on \
  prefs: []
  type: TYPE_NORMAL
- en: your experience. Don't forget to include your charming NY accent in your \
  prefs: []
  type: TYPE_NORMAL
- en: response.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: What is an idea for an LLM-based application?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: I want you to ask me questions to help me come up with an LLM-based \
  prefs: []
  type: TYPE_NORMAL
- en: application idea. Ask me one question at a time to keep things conversational.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Review your previous response, pinpoint areas for enhancement, and offer an
    \
  prefs: []
  type: TYPE_NORMAL
- en: improved version. Then explain your reasoning for how you improved the response.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You are a high school history teacher grading homework assignments. \
  prefs: []
  type: TYPE_NORMAL
- en: Based on the homework question indicated by "Q:" and the correct answer \
  prefs: []
  type: TYPE_NORMAL
- en: indicated by "A:", your task is to determine whether the student's answer is
    \
  prefs: []
  type: TYPE_NORMAL
- en: correct.
  prefs: []
  type: TYPE_NORMAL
- en: Grading is binary; therefore, student answers can be correct or wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Simple misspellings are okay.
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: {question}'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: {correct_answer}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Student Answer: {student_answer}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: from langchain.chat_models import ChatOpenAI
  prefs: []
  type: TYPE_NORMAL
- en: from langchain.prompts import PromptTemplate
  prefs: []
  type: TYPE_NORMAL
- en: from langchain.chains import LLMChain
  prefs: []
  type: TYPE_NORMAL
- en: from langchain.schema import BaseOutputParser
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'from sk import my_sk #importing secret key from another python file'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: define LLM object
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: chat_model = ChatOpenAI(openai_api_key=my_sk, temperature=0)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: define prompt template
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: prompt_template_text = """You are a high school history teacher grading \
  prefs: []
  type: TYPE_NORMAL
- en: homework assignments. Based on the homework question indicated by “**Q:**” \
  prefs: []
  type: TYPE_NORMAL
- en: and the correct answer indicated by “**A:**”, your task is to determine \
  prefs: []
  type: TYPE_NORMAL
- en: whether the student's answer is correct. Grading is binary; therefore, \
  prefs: []
  type: TYPE_NORMAL
- en: student answers can be correct or wrong. Simple misspellings are okay.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** {question}'
  prefs: []
  type: TYPE_NORMAL
- en: '**A:** {correct_answer}'
  prefs: []
  type: TYPE_NORMAL
- en: '**Student''s Answer:** {student_answer}'
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs: []
  type: TYPE_NORMAL
- en: prompt = PromptTemplate(
  prefs: []
  type: TYPE_NORMAL
- en: input_variables=["question", "correct_answer", "student_answer"], \
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: template = prompt_template_text)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: define chain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: chain = LLMChain(llm=chat_model, prompt=prompt)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: define inputs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: question = "Who was the 35th president of the United States of America?"
  prefs: []
  type: TYPE_NORMAL
- en: correct_answer = "John F. Kennedy"
  prefs: []
  type: TYPE_NORMAL
- en: student_answer =  "FDR"
  prefs: []
  type: TYPE_NORMAL
- en: run chain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: chain.run({'question':question, 'correct_answer':correct_answer, \
  prefs: []
  type: TYPE_NORMAL
- en: '''student_answer'':student_answer})'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'output: Student''s Answer is wrong.'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: define output parser
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'class GradeOutputParser(BaseOutputParser):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Determine whether grade was correct or wrong"""'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'def parse(self, text: str):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"""Parse the output of an LLM call."""'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: return "wrong" not in text.lower()
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: update chain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: chain = LLMChain(
  prefs: []
  type: TYPE_NORMAL
- en: llm=chat_model,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: prompt=prompt,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: output_parser=GradeOutputParser()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: run chain in for loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: student_answer_list = ["John F. Kennedy", "JFK", "FDR", "John F. Kenedy", \
  prefs: []
  type: TYPE_NORMAL
- en: '"John Kennedy", "Jack Kennedy", "Jacquelin Kennedy", \'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '"Robert F. Kenedy"]'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'for student_answer in student_answer_list:'
  prefs: []
  type: TYPE_NORMAL
- en: print(student_answer + " - " +
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: str(chain.run({'question':question, 'correct_answer':correct_answer, \
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '''student_answer'':student_answer})))'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: print('\n')
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Output:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: John F. Kennedy - True
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JFK - True
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: FDR - False
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: John F. Kenedy - True
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: John Kennedy - True
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Jack Kennedy - True
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Jacqueline Kennedy - False
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Robert F. Kenedy - False
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/langchain-example?source=post_page-----7ce1ed3b553f--------------------------------)
    [## YouTube-Blog/LLMs/langchain-example at main · ShawhinT/YouTube-Blog'
  prefs: []
  type: TYPE_NORMAL
- en: Codes to complement YouTube videos and blog posts on Medium. - YouTube-Blog/LLMs/langchain-example
    at main ·…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/langchain-example?source=post_page-----7ce1ed3b553f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prompt Engineering is more than asking ChatGPT for help writing an email or
    learning about Quantum Computing. It is a ***new programming paradigm that changes
    how developers can build applications***.
  prefs: []
  type: TYPE_NORMAL
- en: While this is a powerful innovation, it has its limitations. For one, optimal
    prompting strategies are LLM-dependent. For example, prompting GPT-3 to “think
    step-by-step” resulted in significant performance gains on simple mathematical
    reasoning tasks [8]. However, for the latest version of ChatGPT, the same strategy
    doesn’t seem helpful (it already thinks step-by-step).
  prefs: []
  type: TYPE_NORMAL
- en: Another limitation of Prompt Engineering is it requires large-scale general-purpose
    language models such as ChatGPT, which come at significant computational and financial
    costs. This may be overkill for many use cases that are more narrowly defined
    e.g. string matching, sentiment analysis, or text summarization.
  prefs: []
  type: TYPE_NORMAL
- en: We can overcome both these limitations via **fine-tuning** pre-trained language
    models. This is where we **take an existing language model and tweak it for a
    particular use case.** In the [next article](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)
    of this series, we will explore popular fine-tuning techniques supplemented with
    example Python code.
  prefs: []
  type: TYPE_NORMAL
- en: '👉 **More on LLMs**: [Introduction](/a-practical-introduction-to-llms-65194dda1148)
    | [OpenAI API](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)
    | [Hugging Face Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    | [Fine-tuning](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)
    | [Build an LLM](/how-to-build-an-llm-from-scratch-8c477768f1f9) | [QLoRA](/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)
    | [RAG](/how-to-improve-llms-with-rag-abdc132f76ac) | [Text Embeddings](/text-embeddings-classification-and-semantic-search-8291746220be)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Shaw Talebi](../Images/02eefb458c6eeff7cd29d40c212e3b22.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Shaw Talebi](https://shawhin.medium.com/?source=post_page-----7ce1ed3b553f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c?source=post_page-----7ce1ed3b553f--------------------------------)13
    stories![](../Images/82e865594c68f5307e75665842d197bb.png)![](../Images/b9436354721f807e0390b5e301be2119.png)![](../Images/59c8db581de77a908457dec8981f3c37.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Connect**: [My website](https://shawhintalebi.com/) | [Book a call](https://calendly.com/shawhintalebi)
    | [Ask me anything](https://shawhintalebi.com/contact/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Socials**: [YouTube 🎥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)
    | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Support**: [Buy me a coffee](https://www.buymeacoffee.com/shawhint) ☕️'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://shawhin.medium.com/subscribe?source=post_page-----7ce1ed3b553f--------------------------------)
    [## Get FREE access to every new story I write'
  prefs: []
  type: TYPE_NORMAL
- en: Get FREE access to every new story I write P.S. I do not share your email with
    anyone By signing up, you will create a…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----7ce1ed3b553f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '[1] [arXiv:2302.11382](https://arxiv.org/abs/2302.11382) **[cs.SE]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [arXiv:2106.09685](https://arxiv.org/abs/2106.09685) **[cs.CL]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [State of GPT](https://www.youtube.com/watch?v=bZQun8Y4L2A) by [Andrej
    Karpathy](https://medium.com/u/ac9d9a35533e?source=post_page-----7ce1ed3b553f--------------------------------)
    at Microsoft Build 2023'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [arXiv:2206.07682](https://arxiv.org/abs/2206.07682) **[cs.CL]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [ChatGPT Prompt Engineering for Developers](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)
    by deeplearning.ai'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] [arXiv:2005.14165](https://arxiv.org/abs/2005.14165) **[cs.CL]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] [arXiv:2201.11903](https://arxiv.org/abs/2201.11903) **[cs.CL]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] [arXiv:2210.03493](https://arxiv.org/abs/2210.03493) **[cs.CL]**'
  prefs: []
  type: TYPE_NORMAL
