- en: Understanding LLM Hallucinations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/llm-hallucinations-ec831dcd7786](https://towardsdatascience.com/llm-hallucinations-ec831dcd7786)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Opinion
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How LLMs can make stuff up and what to do about it
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://franklyai.medium.com/?source=post_page-----ec831dcd7786--------------------------------)[![Frank
    Neugebauer](../Images/0da70d082d0f9c7ad8ccf574ed215df2.png)](https://franklyai.medium.com/?source=post_page-----ec831dcd7786--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ec831dcd7786--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ec831dcd7786--------------------------------)
    [Frank Neugebauer](https://franklyai.medium.com/?source=post_page-----ec831dcd7786--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ec831dcd7786--------------------------------)
    ·6 min read·May 8, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95f952aadd9b08c63fbd537fcf27100e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: Photo by [Ahmad Dirini](https://unsplash.com/@ahmadirini?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Main Objectives
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working with large language models is not without risks including responses
    based on what’s called a LLM “hallucination.” Hallucinations can be a serious
    problem for LLMs because they can lead to the spread of misinformation, expose
    confidential information, and create unrealistic expectations about what LLMs
    can do. Understanding hallucinations and being critical of the information that
    they generate helps explain and mitigate problems such hallucinations can cause.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: What’s a LLM Hallucination?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs are a type of artificial intelligence (AI) that are trained on massive
    datasets of text and code. They can generate text, translate languages, write
    different kinds of creative content, and answer questions in informative ways.
    However, LLMs are also prone to “hallucinating,” which means that they can generate
    text that is factually incorrect or nonsensical. As has been spoken about regularly,
    “LLMs can be confidently full of sh**.” Such hallucinations happen because LLMs
    are trained on data that is often incomplete or contradictory. As a result, they
    may learn to associate certain words or phrases with certain concepts, even if
    those associations are not accurate or are unintentionally “overly accurate” (by
    this I mean they can make up things that are true but not meant to be shared).
    This can lead to LLMs generating text that is factually incorrect, inadvertently
    overly indulgent, or simply nonsensical.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Types of Hallucinations
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lies! Lies! Lies!
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LLMs can sometimes generate text that is factually incorrect. Here’s an example,
    some of which is correct, but highlighting the part that is simply untrue:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To suggest segregation and discrimination no longer exists is factually untrue.
    For me to call it a “lie” is also technical untrue because the models don’t understand
    truth or lie, just how to assemble words. No matter the reason, LLMs can still
    generate factually untrue content. This doesn’t happen infrequently and it’s to
    fact check everything.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 说隔离和歧视已经不存在是不符合事实的。将其称为“谎言”对我来说也是技术上不准确的，因为模型不理解真理或谎言，只是如何组装单词。无论原因如何，LLMs仍然可能生成事实不准确的内容。这并不少见，因此必须对所有内容进行事实检查。
- en: Nonsense
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 胡说八道
- en: At a very high level, LLMs are using probabilities to arrange words. While the
    range of words and their probabilities is likely to result in something that makes
    sense, that’s not always the case; LLMs can also generate text that is nonsensical.
    For example, if you ask an LLM to write a poem, it might generate something that
    is grammatically correct but does not make any sense.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个很高的层面来看，LLMs使用概率来安排单词。尽管单词和其概率的范围可能会生成有意义的内容，但这并不总是如此；LLMs也可以生成无意义的文本。例如，如果你让LLM写一首诗，它可能会生成语法正确但毫无意义的内容。
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: I can see how the creatives may say that it’s perfectly fine to say that a fish
    could lay in the sand in a poem (poetic license and all) but I can also argue
    the model is making up some nonsense, which is what I’m going with in this case.
    The ideas drifted from the logical to illogical. However, note that you’ll see
    more serious examples if you work with LLMs for any length of time. Again, check
    the model output and make corrections where necessary. In the poem example, I’d
    probably simply change “…we lay down on the beach…” to “…we laid down by the coral
    reef…” or maybe I’d just remove that line since humans can’t really take a nap
    underwater.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我能理解创作者可能会说，在诗中说鱼可以躺在沙子里是完全可以接受的（诗意许可及其所有），但我也可以争论模型是在编造一些胡说八道，这也是我在这种情况下的看法。思想从逻辑偏离到了非逻辑。然而，注意如果你与LLMs合作一段时间，你会看到更严重的例子。再次检查模型输出，并在必要时进行更正。在诗歌示例中，我可能会简单地将“…我们躺在海滩上…”改为“…我们躺在珊瑚礁旁…”或干脆删除那一行，因为人类实际上无法在水下小憩。
- en: Source Conflation
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来源混淆
- en: LLMs can sometimes conflate different sources of information, which can lead
    to them generating text that is inaccurate or misleading. For example, if you
    ask an LLM to write a news article about a current event, it might combine information
    from different news sources, even if those sources contradict each other. Note
    also that combining text that includes inferences made from historic information
    combined with (for example) something like LangChain, can really conflate sources
    (and formats) of information.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs有时会混淆不同来源的信息，这可能导致它们生成不准确或误导性的文本。例如，如果你让LLM写一篇关于当前事件的新闻文章，它可能会将来自不同新闻来源的信息结合在一起，即使这些来源相互矛盾。还要注意，将历史信息的推断与（例如）LangChain等内容结合在一起，可能会真正混淆信息（和格式）。
- en: Here is an example of how conflation can lead to factually untrue (or minimally
    very misleading) information.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示例，说明混淆如何导致事实不准确（或至少非常误导性）的信息。
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The response accurately opens in the first paragraph, describing the race held
    on May 7, 2023\. It then appears the model conflated the 2022 results thereafter.
    Leclerc qualified seventh and did not lead the first 20 laps in 2023, but he did
    finish second in 2022 and may have led the first 20 laps of that race. (Sergio
    Perez finished second in 2023.) It’s possible, in this case, that the conflation
    happened in whatever (e.g., LangChain) was used to combine current events and
    the LLM text, but the same hallucination by conflation idea holds true.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 回应在第一段准确地开头，描述了2023年5月7日举行的比赛。然后，模型似乎在之后混淆了2022年的结果。勒克莱尔在2023年排位第七，并没有在前20圈领跑，但他确实在2022年获得了第二名，并且可能在那场比赛的前20圈中领跑。（2023年塞尔吉奥·佩雷斯获得第二名。）在这种情况下，混淆可能发生在用于结合当前事件和LLM文本的任何工具（例如LangChain）中，但混淆引起的幻觉的想法仍然适用。
- en: Overindulgence
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过度纵容
- en: Given that LLMs can put some fairly significant text together using probabilities,
    and they can conflate information, it’s statistically possible for an LLM to make
    up information that “accidentally” discloses confidential information.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于大型语言模型（LLMs）能够利用概率生成一些相当重要的文本，并且可能会混淆信息，因此统计上来说，LLM有可能编造出“意外”泄露机密信息的内容。
- en: For the sake of protecting confidential information, I can’t provide details
    about the situation I recently ran into where an LLM did just that. However, I
    asked an LLM about a particular topic that I knew should not be disclosed and
    the model guessed at a logically correct but overindulgent reply. While the situation
    I ran into was not a matter of national security, it can be very serious under
    the right circumstances.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保护机密信息，我不能提供关于我最近遇到的LLM出现这种情况的详细信息。然而，我曾询问过一个LLM一个我知道不应该披露的特定话题，模型给出了一个逻辑上正确但过于放纵的回答。虽然我遇到的情况不是国家安全问题，但在适当的情况下可能会非常严重。
- en: How to Manage Hallucinations?
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何管理幻觉？
- en: 'Here are some tips for managing hallucinations:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些管理幻觉的技巧：
- en: Test different settings for things like temperature and TopK (how probabilities
    are managed by the model). This is one of the most important ways to manage model
    output.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试不同的设置，比如温度和TopK（模型如何管理概率）。这是管理模型输出的最重要方法之一。
- en: Don’t trust the output — fact check (don’t worry, you’re still saving massive
    amounts of time!).
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要完全相信输出——进行事实核查（不用担心，你仍然节省了大量时间！）。
- en: Generally treat LLM output as a drafting mechanism — for example, an LLM created
    the basic layout and a little of the content for this article. But I edited it
    significantly.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常将LLM输出视为草稿机制——例如，LLM创建了这篇文章的基本布局和一些内容。但我进行了大量编辑。
- en: Tune the model you’re using. Depending on how you’re using model output, you
    may want to tune the model — there are many ways to do this, including prompt
    engineering, parameter efficient tuning (PET), and full model tuning. There’s
    quite a bit of nuance and complexity in that simple list but if you know how do
    it, such tuning can reduce hallucinations.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整你使用的模型。根据你如何使用模型输出，你可能需要调整模型——这有许多方法，包括提示工程、参数高效调整（PET）和完全模型调整。这个简单列表中有相当多的细微差别和复杂性，但如果你知道如何做，这种调整可以减少幻觉。
- en: Accept the reality that models hallucinate. Unlike humans (in most cases), LLM
    hallucination is usually an unintended consequence and I believe generally the
    positives well outweigh the negatives. Accept this and acknowledge / communicate
    the possibility hallucinations can happen.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接受模型出现幻觉的现实。与人类（在大多数情况下）不同，LLM的幻觉通常是一个无意的结果，我认为通常正面的影响远远超过负面的影响。接受这一点并承认/沟通幻觉发生的可能性。
- en: EXPLORE! While this article provided an overview of LLM hallucinations, what
    this means for you and your application can vary significantly. Additionally,
    your perception of these words may not exactly align with reality. The only way
    to truly understand and appreciate how LLM hallucination affects what you’re trying
    to do is to explore LLMs extensively.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索！虽然这篇文章提供了LLM幻觉的概述，但这对你和你的应用意味着什么可能会有显著差异。此外，你对这些词的理解可能并不完全符合现实。唯一真正理解和欣赏LLM幻觉如何影响你所做的事的方法是广泛探索LLM。
- en: More to be Revealed
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多内容待揭示
- en: The widespread use of LLMs is really in its infancy and the pros/cons have yet
    to be enumerated with any accuracy. In my opinion, an open mind is the best technique
    to understand all the dimensions of an LLM, including hallucinations. Enjoy this
    ride and explore as much as you can because such rapid evolution seldom happens
    (in my experience) and those who embrace the ride gain the most from it.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的广泛使用实际上还处于初期阶段，优缺点尚未准确列举。在我看来，开放的心态是理解LLM的所有维度，包括幻觉的最佳技巧。享受这个过程，尽可能多地探索，因为这种快速演变很少发生（根据我的经验），那些拥抱这一过程的人将从中获得最多。
