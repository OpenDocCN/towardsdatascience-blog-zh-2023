# 动态重连的延迟消息传递 GNNs

> 原文：[https://towardsdatascience.com/dynamically-rewired-delayed-message-passing-gnns-2d5ff18687c2](https://towardsdatascience.com/dynamically-rewired-delayed-message-passing-gnns-2d5ff18687c2)

## 延迟消息传递

## 消息传递图神经网络（MPNNs）往往会受到*过度挤压*现象的影响，这会导致依赖长距离交互的任务性能下降。这在很大程度上归因于消息传递仅在*局部*，即节点的直接邻居之间发生。传统的静态图重连技术通常尝试通过允许远程节点立即通信（在极端情况下，像 Transformer 一样使所有节点在每一层都可访问）来对抗这一效果。然而，这会带来计算成本，并且破坏了输入图结构提供的归纳偏置。在这篇文章中，我们描述了两种新机制来克服过度挤压，同时抵消静态重连方法的副作用：*动态重连*和*延迟*消息传递。这些技术可以被纳入任何 MPNN，并在长距离任务上比图 Transformer 具有更好的性能。

[](https://michael-bronstein.medium.com/?source=post_page-----2d5ff18687c2--------------------------------)[![Michael Bronstein](../Images/1aa876fce70bb07bef159fecb74e85bf.png)](https://michael-bronstein.medium.com/?source=post_page-----2d5ff18687c2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2d5ff18687c2--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2d5ff18687c2--------------------------------) [Michael Bronstein](https://michael-bronstein.medium.com/?source=post_page-----2d5ff18687c2--------------------------------)

·发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2d5ff18687c2--------------------------------) ·9 分钟阅读·2023年6月19日

--

![](../Images/a8144f73a5808c0a411b10b828b6ec09.png)

图片：基于 Shutterstock。

*这篇文章由 Francesco Di Giovanni 和 Ben Gutteridge 共同撰写，基于 B. Gutteridge 等人的论文，DRew:* [*动态重连的延迟消息传递*](https://arxiv.org/pdf/2305.08018.pdf) *(2023)，ICML。*

经典的消息传递图神经网络（MPNNs）通过从每个节点的 1 跳邻居中聚合信息来操作。因此，要求*长距离交互*（即存在一个节点 *v* 其表示需要考虑在最短路径（测地线）距离 *d*(*u*,*v*) = *r* > 1 的某节点 *u* 中包含的信息）的学习任务需要具有多个消息传递层的深度 MPNNs。如果图结构是这样的，使得感受野随跳数指数级扩展 [1]，可能需要将过多的消息“挤压”到固定的节点特征向量中——这一现象称为*过度压缩* [2]。

![](../Images/69c830c6c70d0657852dd68b8b8e2d8d.png)

# **过度压缩**

在我们之前的工作中 [3–4]，我们将过度压缩形式化为 MPNN 在节点 *u* 处对输入于 *r* 距离节点的输出的*敏感性缺乏*。这可以通过对形式的偏导数（*雅可比矩阵*）进行界限量化来实现。

|∂**x***ᵤ*⁽*ʳ*⁾/∂**x***ᵥ*⁽⁰⁾| < *c* (**A***ʳ*)*ᵤᵥ.*

在这里，*c* 是一个依赖于 MPNN 结构的常数（例如，激活函数的 Lipschitz 正则性、深度等），而**A** 是图的归一化邻接矩阵。当**A***ʳ* 的条目随距离 *r* 指数级衰减时，就会发生过度压缩（over-squashing）。实际上，现在已知过度压缩更普遍地是一种现象，可以与图的局部结构（如负曲率 [3]）或超越最短路径距离的全局结构（例如，通勤时间或有效电阻 [4, 5]）相关联。

上述表达式中的 **A***ʳ* 的幂反映了 MPNN 中节点 *u* 和 *v* 之间在距离 *r* 处的通信是由不同路径的相邻节点之间的一系列交互组成的。这些路径连接 *u* 和 *v*。结果，节点 *u* 和 *v* 仅从 *r* 层开始交换信息，并且延迟等于它们的距离 *r*。过度压缩是由于这些信息在沿着这些路径的中间节点上进行重复消息传递时被“稀释”。

# **图重连**

解决过度压缩的问题的方法是部分解耦输入图结构和用于计算消息的支持结构，这一过程称为*图重连* [6]。通常，重连是在预处理步骤中执行的，在此步骤中，输入图 *G* 被替换为根据某种空间或频谱连通性度量“更友好”的其他图 *G’*。

实现这一点的最简单方法是连接所有在一定距离内的节点，从而允许它们*直接*交换信息。这就是多跳消息传递方案的想法 [7]。图 Transformer [8] 将这一点推向极致，通过一个注意力加权的边连接*所有*节点对。

这样，信息不再与沿途其他节点的信息“混合”，可以避免过度压缩。然而，这种重连使图在第一层时变得更加稠密，增加了计算负担，并且部分妥协了输入图所提供的归纳偏差，因为在每一层中，本地和全局节点的交互都是*瞬时的*。

![](../Images/d2061ebebf4f75b3a8a5695c95ba70d4.png)

在经典 MPNN（左侧）中，来自节点 *u* 的信息在经过 3 次消息传递步骤后到达距离 3 步的节点 *v*。因此，节点 *v* 总是以固定的延迟（滞后）“看到”节点 *u*，这个延迟等于它们在图中的距离。在图 Transformers（右侧）中使用的图重连的极端例子中，所有节点都相互连接，使得节点 *u* 的信息能立即在 *v* 上可用；然而，这会以失去图距离所提供的部分顺序为代价，这需要通过特征的位置和结构增强来重新发现。

# 动态图重连

以我们之前的两个距离为 *r* > 1 的节点 *u* 和 *v* 的例子来看，在经典 MPNN 中，必须等待 *r* 层才能使 *u* 和 *v* 进行交互，并且这种交互从未是直接的。我们认为，一旦我们达到第 *r* 层，这两个节点已经等待了“足够长”的时间，因此可以直接交互（通过插入的额外边，而不通过中间的邻居）。

因此，在第一层，我们只在输入图的边上传播消息（如经典 MPNNs），但在每一层，节点 *u* 的感受野扩展一个跳跃 [9]。这允许远距离的节点在不经过中间步骤的情况下交换信息，同时保留输入图拓扑提供的归纳偏差：图在更深层次中会根据距离逐渐变密。

我们称这种机制为*动态图重连*，简称为*DRew* [10]。DRew-MPNNs 可以被视为在局部作用于输入图的经典 MPNNs 和同时考虑所有成对交互的图 Transformer 之间的“中间地带”。

# 延迟消息传递

在经典 MPNNs 中，两个距离为 *r* 的节点 *u* 和 *v* 总是以 *r* 层的固定延迟进行交互，这是信息从一个节点传递到另一个节点所需的最短时间。因此，节点 *v* 从 *r* 层之前“看到”节点 *u* 的状态（混合了其他节点的特征）。而在 DRew-MPNNs 中，当两个节点进行交互时，它们是通过插入的边瞬时进行的，使用它们的*当前状态*。

*延迟消息传递* 是这两种极端情况之间的权衡：我们为节点之间发送的消息添加了一个全局*延迟*（一个超参数 **𝝼**）。

为了简单起见，我们在这里考虑两种简单的情况：没有延迟（如在 DRew 中），或最大延迟的情况，其中两个距离 *r* 的节点 *u* 和 *v* 从层 *r* 开始直接互动，但具有 *r* 的常量延迟（如在经典的 MPNNs 中）：在层 *r*，节点 *u* 可以与节点 *v* 在 *r* 层之前的状态交换信息 [11]。

延迟控制信息在图上的*流动速度*。没有延迟意味着消息传递更快，一旦添加边缘，远离的节点会立即交互；相反，延迟越多，信息流动越慢，远离的节点在添加边缘时会访问过去的状态。

![](../Images/59f7173d373f601f48b14e089d1be7ee.png)

DRew及其延迟变体𝝼DRew的比较。左侧，距离 *r* 的节点通过从层 *r* 开始的额外边缘即时交换信息。右侧显示的是最大延迟的情况（在我们的论文中对应**𝝼** = 1），其中两个节点之间的延迟与它们的距离一致；在距离（层）*r* 的节点之间新增的边缘“看起来像是”过去的，以访问 *r* 层前节点的状态。

# 𝝼DRew 框架

我们称结合动态重连和延迟消息传递的架构为𝝼DRew（发音为“Andrew”）。

观察 𝝼DRew 的一种方式是将其视为一种具有*稀疏跳跃连接*的架构，允许消息不仅“横向”传递（在图的同一层级的节点之间，如经典 MPNN 中），还“纵向”传递（跨不同层级）。在 GNN 中依赖垂直边缘的想法并不新鲜，实际上可以将残差连接视为将每个节点连接到前一层相同节点的垂直链接。

延迟机制通过创建垂直边缘来扩展这种方法，这些边缘连接一个节点 *u* 和一个在某个先前层的 *不同* 节点 *v*，具体取决于 *u* 和 *v* 之间的图距离。这样，我们可以利用深度神经网络的跳跃连接的固有优势，同时基于我们手头的额外几何信息（以图距离的形式）来进行条件化。

𝝼DRew 缓解了过度压缩，因为远离的节点现在可以通过多个（更短的）路径交换信息，绕过了重复本地消息传递的“信息稀释”问题。与静态重连不同，𝝼DRew 通过减缓图的稠密化并使其依赖于层级来实现这一效果，从而减少了内存占用。

𝝼DRew 适用于以不同速度探索图形，处理长程交互，并一般性地增强非常深的 GNN 的能力。由于 𝝼DRew 确定了消息交换的*何处*和*何时*，但不*如何*，因此可以视为一种元架构，可以增强现有的 MPNNs。

# 实验结果

在我们的论文[10]中，我们提供了𝝼DRew与经典MPNN基准、静态重连和Transformer型架构的广泛比较，使用了固定的参数预算。在Vijay Dwivedi及其合著者最近提出的长期基准（LRGB）[11]上，𝝼DRew在大多数情况下超越了上述所有模型。

![](../Images/6dab81c55ebc175e0c273e373a52e3aa.png)

对比各种经典MPNN（GCN、GINE等）、静态图重连（MixHop-GCN、DIGL）和图Transformer型架构（Transformer、SAN、GraphGPS，包括位置Laplacian编码）与𝝼DRew-MPNN变体在四个长期图基准（LRGB）任务上的表现。绿色、橙色和紫色分别表示最佳、第二佳和第三佳模型。

对𝝼DRew在LRGB任务之一上的消融研究揭示了我们框架的另一个重要贡献：调节𝝼以适应任务的能力。我们观察到，使用更多延迟（较低的𝝼值）时，大层数*L*的性能更好，而使用较少延迟（高𝝼值）则确保计算图更快填充，连接的密度在较少层数后更高。因此，在浅层架构（小*L*）中，完全去除延迟（𝝼=∞）的性能更好。相反，在深层架构（大*L*）中，更多延迟（小𝝼）“减缓”了消息传递图的密集化，从而提高了性能。

![](../Images/9dcf49a8950930132e945d2eb173f808.png)

不同层数*L*和不同延迟参数𝝼的𝝼DRew-MPNNs的性能。虽然动态重连在所有情况下对长程任务都有帮助，但延迟显著改善了深层模型的性能。我们的框架还可以根据应用场景控制计算/内存预算，例如在Transformer计算不可行的情况下。

传统的MPNN型架构在*消息*交换的方式上存在差异[12]。图重连技术在图上增加了对*消息*发送位置的额外控制。我们新的动态图重连方法与延迟消息传递进一步控制了*消息*交换的*时间*。

这种方法看起来非常强大，我们的工作仅仅是首次尝试利用根据“几何属性”访问图神经网络中过去状态的想法。我们希望这一新范式能引导出更多理论上有原则的框架，并挑战MPNN无法解决长程任务的观点，除非通过增强的二次注意力层。

[1] 这在“小世界”图（例如社交网络）中很典型。

[2] U. Alon 和 E. Yahav，《图神经网络瓶颈及其实际应用》（2021），*ICLR*。

[3] J. Topping *等*，《通过曲率理解图上的过度压缩和瓶颈》（2022），*ICLR*。

[4] 通勤时间是随机游走从节点 *v* 到节点 *u* 再返回所需的预期时间。见 F. Di Giovanni *等*，[关于消息传递神经网络中过度压缩的影响：宽度、深度和拓扑的影响](https://arxiv.org/pdf/2302.02941.pdf)（2023），*ICML*。

[5] 见 F. Di Giovanni *等* 的定理 4.3，[过度压缩如何影响 GNN 的能力？](https://arxiv.org/pdf/2306.03589.pdf)（2023）*arXiv*:2306.03589。

[6] 图重连在 GNN 社区中有些争议，因为一些人认为输入图是神圣不可侵犯的，不应被触及。*实际上*，大多数现代 GNN 都采用某种形式的图重连，无论是显式的（作为预处理步骤）还是隐式的（例如，通过邻居采样或使用虚拟节点）。

[7] R. Abboud, R. Dimitrov 和 I. Ceylan，[图属性预测的最短路径网络](https://arxiv.org/pdf/2206.01003.pdf)（2022），

*arXiv*:2206.01003。

[8] 见例如 V. P. Dwivedi 和 X. Bresson， [Transformer 网络对图的推广](https://arxiv.org/pdf/2012.09699v2.pdf)（2021），*arXiv*:2012.09699 和 C. Ying 等，[Transformer 对图表示的性能真的很差吗？](https://www.microsoft.com/en-us/research/publication/do-transformers-really-perform-badly-for-graph-representation/)（2021），NeurIPS。

[9] 动态重连结果为以下消息传递公式：

**m***ᵤ*⁽*ˡ ᵏ* ⁾=AGG({**x***ᵥ*⁽*ˡ* ⁾ : *v∈𝒩ₖ*(*u*)})，其中 1 ≤ *k ≤ l*+1

**x***ᵤ*⁽*ˡ* ⁺¹⁾=UP(**x***ᵤ*⁽*ˡ* ⁾, **m***ᵤ*⁽*ˡ* ¹⁾,…, **m***ᵤ*⁽*ˡ ˡ* ⁺¹⁾)

其中，AGG 是一个置换不变的聚合操作符，*𝒩ₖ*(*u*) 是节点 *u* 的 *k*-跳邻域，而 UP 是一个接收来自每个 *k*-跳的消息的更新操作。见[10]中的公式5。

[10] B. Gutteridge 等，DRew: [动态重连消息传递与延迟](https://arxiv.org/pdf/2305.08018.pdf)（2023），*ICML*。

[11] 延迟消息传递呈现以下形式

**m***ᵤ*⁽*ˡ ᵏ* ⁾=AGG({**x***ᵥ*⁽*ˡ* ᐨ*ˢ* ⁽*ᵏ* ⁾⁾ : *v∈𝒩ₖ*(*u*)})，其中 1 ≤ *k ≤ l*+1

**x***ᵤ*⁽*ˡ* ⁺¹⁾=UP(**x***ᵤ*⁽*ˡ* ⁾, **m***ᵤ*⁽*ˡ* ¹⁾,…, **m***ᵤ*⁽*ˡ ˡ* ⁺¹⁾)

其中 *s*(*k*)=max{0,*k*﹣𝝼}，见[10]中的公式6。选择 𝝼=∞ 表示没有延迟（如在 DRew 中），而 𝝼 = 1 表示经典的 MPNN（两个节点 *u* 和 *v* 在距离 *r* 的位置上从第 *r* 层开始直接交互，但有一个恒定的 *r* 延迟）。

[11] V. P. Dwivedi *等*，[长距离图基准](https://arxiv.org/abs/2206.08164)（2022），*arXiv*:2206.08164。

[12] 在我们的原型书中 M. M. Bronstein *等*，[几何深度学习：网格、群体、图、测地线和量规](https://arxiv.org/abs/2104.13478)（2021），我们区分了三种“风格”的 MPNN：卷积型、注意力型和通用消息传递。

*我们感谢* [*Federico Barbero*](https://twitter.com/fedzbar)*,* [*Fabrizio Frasca*](https://twitter.com/ffabffrasca)* 和* [*Emanuele Rossi*](https://twitter.com/emaros96) *为本文进行校对并提供了有见地的评论。有关图形深度学习的更多文章，请参见 Michael 的* [*其他文章*](https://towardsdatascience.com/graph-deep-learning/home) *在 Towards Data Science 上，* [*订阅*](https://michael-bronstein.medium.com/subscribe) *他的文章和* [*YouTube 频道*](https://www.youtube.com/c/MichaelBronsteinGDL)*，获取* [*Medium 会员资格*](https://michael-bronstein.medium.com/membership)*，或者在* [*Twitter*](https://twitter.com/mmbronstein)* 上关注他。*
