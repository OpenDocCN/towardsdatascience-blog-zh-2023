- en: Your Dataset Has Missing Values? Do Nothing!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/your-dataset-has-missing-values-do-nothing-10d1633b3727](https://towardsdatascience.com/your-dataset-has-missing-values-do-nothing-10d1633b3727)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Models can handle missing values out-of-the-box more effectively than imputation
    methods. An empirical proof
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mazzanti.sam?source=post_page-----10d1633b3727--------------------------------)[![Samuele
    Mazzanti](../Images/432477d6418a3f79bf25dec42755d364.png)](https://medium.com/@mazzanti.sam?source=post_page-----10d1633b3727--------------------------------)[](https://towardsdatascience.com/?source=post_page-----10d1633b3727--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----10d1633b3727--------------------------------)
    [Samuele Mazzanti](https://medium.com/@mazzanti.sam?source=post_page-----10d1633b3727--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----10d1633b3727--------------------------------)
    ·10 min read·Oct 9, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d3685f99b6f73b5c92ca622966a838de.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image by Author]'
  prefs: []
  type: TYPE_NORMAL
- en: Missing values are very common in real datasets. Over time, many methods have
    been proposed to deal with this issue. Usually, they consist either in removing
    data that contain missing values or in imputing them with some techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, I will test a third alternative:'
  prefs: []
  type: TYPE_NORMAL
- en: Doing nothing.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Indeed, the best models for tabular datasets (namely, XGBoost, LightGBM, and
    CatBoost) can natively handle missing values. So, the question I will try to answer
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: Can these models handle missing values effectively, or would we obtain a better
    result with a preliminary imputation?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Who said we should care about nulls?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There seems to be a **widespread belief that we must do *something* about missing
    values**. For instance, I asked ChatGPT what should I do if my dataset contain
    missing values, and it suggested 10 different ways to get rid of them (you can
    read the full answer [here](https://chat.openai.com/share/d65dcaff-ce67-4fac-b54a-31b1f00f50ba)).
  prefs: []
  type: TYPE_NORMAL
- en: But where does this belief ​​come from?
  prefs: []
  type: TYPE_NORMAL
- en: Usually, these kinds of opinions originate from historical models, particularly
    from linear regression. This is also the case. Let’s see why.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that we have this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c8c027fa0e94bd6e129ff265794c0b29.png)'
  prefs: []
  type: TYPE_IMG
- en: A dataset with missing values. [Image by Author]
  prefs: []
  type: TYPE_NORMAL
- en: If we tried to train a linear regression on these features, we would get an
    error. In fact, to be able to make predictions, linear regression needs to multiply
    each feature by a numeric coefficient. If one or more features are missing, it’s
    impossible to make a prediction for that row.
  prefs: []
  type: TYPE_NORMAL
- en: This is why many imputation methods have been proposed. For instance, one of
    the simplest possibilities is to replace the nulls with the feature’s mean.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4929f5be7de782e268bd095b09c6a716.png)'
  prefs: []
  type: TYPE_IMG
- en: Imputation with the feature’s mean. [Image by Author]
  prefs: []
  type: TYPE_NORMAL
- en: Another, more sophisticated, approach is to use the relationships between the
    variables to predict the most likely value to fill that specific entry. This entails
    training one predictive model for each feature (using the other features as predictors).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d7070bd6fc47236c66194948f8c3291c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Iterative Imputation: each feature is estimated from all the others. [Image
    by Author]'
  prefs: []
  type: TYPE_NORMAL
- en: However, not all models are like linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, as luck would have it, the best-performing models for tabular tasks
    (namely, tree-based models, such as XGBoost, LightGBM, and CatBoost) can handle
    missing values natively.
  prefs: []
  type: TYPE_NORMAL
- en: How is that possible?
  prefs: []
  type: TYPE_NORMAL
- en: 'Because in tree-like structures, missing values can be treated just like other
    values, i.e. the model can assign them to a branch of the tree. For example, this
    image comes from XGBoost documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1b94dc1f7b9c122ef454db1b0f830d15.png)'
  prefs: []
  type: TYPE_IMG
- en: How tree-based models handle missing values. [Image from [XGBoost documentation](https://xgboost.readthedocs.io/en/stable/tutorials/feature_interaction_constraint.html)]
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, for each split XGBoost chooses a default branch on which missing
    values (if any) are routed.
  prefs: []
  type: TYPE_NORMAL
- en: So, are we saying that just because these models can handle missing values,
    we should avoid imputation? I never said that.
  prefs: []
  type: TYPE_NORMAL
- en: As data scientists, we usually want to know well **in practice**. So, the focus
    of the next paragraphs will be to compare the model performance with and without
    imputation and see which approach performs better.
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with real datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our objective is to compare two approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Train and test the model on the dataset **with missing values**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train and test the model on the dataset with no missing values (after they have
    been **imputed through some imputation method**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I will use 14 real datasets that are available in [Pycaret](https://github.com/pycaret/pycaret)
    (a Python library under [MIT license](https://github.com/pycaret/pycaret/blob/master/LICENSE)).
  prefs: []
  type: TYPE_NORMAL
- en: These datasets do not contain missing values, so we will need to fabricate them.
    I will call this procedure null-sowing because it involves disseminating null
    values on the original dataset (i.e. “canceling” some of the original values).
  prefs: []
  type: TYPE_NORMAL
- en: 'How many values are we going to cancel? To ensure that the experiment is representative,
    we will try with different percentages of nulls: 5%, 10%, 20% and 50%.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f70de3fcb9fb2eafdda9def43362c60.png)'
  prefs: []
  type: TYPE_IMG
- en: Null-sowing with different percentages. [Image by Author]
  prefs: []
  type: TYPE_NORMAL
- en: 'I use two different strategies to create null values:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Random**: a random value between 0 and 1 is attributed to each entry of the
    dataset and, if smaller than the threshold, the entry is canceled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-random**: for each feature, I sort the values either in ascending or
    descending order, and attribute a proportional probability of being canceled based
    on the position of the value in the sorted sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreover, for each combination, I will try 25 different random train/test splits.
    The purpose of doing this is to ensure that the results we observe are consistent
    over many repetitions and not simply due to chance.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, these are all the combinations that I will try.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d89a9ff89269302e399f86efce281bd8.png)'
  prefs: []
  type: TYPE_IMG
- en: Combinations that form our experiment. [Image by Author]
  prefs: []
  type: TYPE_NORMAL
- en: 'For each of these 2,800 combinations, I will compute the average precision
    (on the test set) of LightGBM with three different approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the original dataset (with **no missing values**). I will call this average
    precision `ap_original`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the dataset **containing the missing values I have sown.** I will call
    this average precision`ap_noimpute`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the dataset where the **missing values have been filled** using Scikit-Learn’s
    IterativeImputer. I will call this average precision`ap_impute`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s clarify the difference between these three metrics with the aid of a
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b7d9be40c12e0e1242648b95d2421250.png)'
  prefs: []
  type: TYPE_IMG
- en: 3 versions of the dataset. A different LightGBM has been trained on each version.
    [Image by Author]
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I tried all the 2,800 combinations listed above and tracked `ap_original`,
    `ap_noimpute`, and `ap_impute` for each of them. I stored the results in a table
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6018e60a88ced90fabf8cf304100cf45.png)'
  prefs: []
  type: TYPE_IMG
- en: Results of the experiment. [Image by Author]
  prefs: []
  type: TYPE_NORMAL
- en: This table has 112 rows (i.e. 14 datasets x 2 null-sowing strategies x 4 null
    frequencies). The first three columns indicate which dataset, null-sowing strategy,
    and null frequency identify that row. Then, there are three columns that store
    the average precisions reported by that approach on the 25 bootstrap iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make it even clearer, let’s take the fourth column of the first row. This
    is an array of 25 elements: each of them is the average precision that LightGBM
    realized — for that particular train/test split (i.e. bootstrap iteration) — on
    the original dataset (that’s why it’s called `ap_original`) for dataset “bank”
    with 5% of missing values randomly sown.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we are not interested in each single bootstrap iteration, so we need
    to aggregate the arrays in some way. Since we want to compare the three approaches
    with each other, the most simple aggregation is counting the number of times that
    one approach is superior to another (meaning it has a higher average precision).
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, for each row of the table, I calculated the percentage of times that:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ap_original` > `ap_noimpute`, which means that the model on the original dataset
    is better than the model on the dataset containing missing values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ap_noimpute` > `ap_impute`, which means that the model on the dataset containing
    missing values is better than the model on the dataset filled with IterativeImputer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f88a284d0462b9eb4a1f91106ca88972.png)'
  prefs: []
  type: TYPE_IMG
- en: Results of the experiment. [Image by Author]
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we take the last column of the first row, it means that `ap_noimpute`
    is greater than `ap_impute` in 60% out of the 25 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the column called `original_>_noimpute`. Apparently, it’s 100%
    most of the time. And this makes sense: it’s reasonable that the model trained
    on the original dataset does better than the model trained on the dataset in which
    we have canceled some entries.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But the most important information for us is in the last column. In fact:'
  prefs: []
  type: TYPE_NORMAL
- en: when `noimpute_>_impute` is greater than 50%, it means that the model trained
    on the null dataset outperformed most of the times the model trained on the imputed
    dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: when `noimpute_>_impute` is less than 50%, the opposite.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So we could be tempted to simply take the mean of this column and decide which
    approach works better based on whether the global mean is above or below 50%.
  prefs: []
  type: TYPE_NORMAL
- en: But if we do that, we could be misled by the effect of chance. Indeed, if a
    value is close to 50%, like 48% or 52%, this could be easily due to randomness.
    To account for that, we need to frame this as a statistical test.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a statistical test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To avoid being fooled by chance, I will take a couple of precautions.
  prefs: []
  type: TYPE_NORMAL
- en: First, I will keep only the cases in which the average precision on the original
    dataset is greater than the average precision on the missing dataset more than
    90% of the time (in other words, I will keep only the rows where `original_>_noimpute`
    > .90).
  prefs: []
  type: TYPE_NORMAL
- en: I will do this because I want to keep only the cases where missing values have
    a clear negative impact on the performance of the model. After doing that, out
    of the initial 112 rows of the table, only 48 remain.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, I will need to compute a “significance threshold” that helps us to
    understand whether a value is significant or not.
  prefs: []
  type: TYPE_NORMAL
- en: We already said that when a value is very close to 50%, like 48% or 52%, then
    it’s probably just due to chance. But how close is “very close”? To answer this,
    we need some statistics.
  prefs: []
  type: TYPE_NORMAL
- en: '**The hypothesis we want to test** (a.k.a. the null hypothesis) is that **imputing
    or not imputing makes no difference**. This is like saying that the probability
    of `ap_impute` being greater than `ap_noimpute` (or vice versa) is 50%. Since
    we have 25 independent iterations, we can compute the probability of obtaining
    a specific result through the binomial distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s say that, out of 25 iterations, `ap_impute` has been better
    than `ap_noimpute` in 10 iterations. How compatible is this result with our hypothesis?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: So, the probability of obtaining a result as extreme as 10, given our hypothesis,
    is 42%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that I multiplied the cumulative distribution function of the binomial
    distribution by 2 because we are interested in the two-tailed p-value. We can
    double-check this by looking at the p-value associated with any possible outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/43fb8e6d269971d07c24751c321dec24.png)'
  prefs: []
  type: TYPE_IMG
- en: P-values associated with any possible outcome (out of 25 iterations). [Image
    by Author]
  prefs: []
  type: TYPE_NORMAL
- en: 'The p-value associated with 12 and 13 is exactly 100%. And this makes sense:
    the probability of obtaining a result at least as extreme as 12 or 13 is necessarily
    100% since they are the least extreme results, given our hypothesis.'
  prefs: []
  type: TYPE_NORMAL
- en: So, what is our significance level? As per convention, I will take a significance
    level of 1%. However, we must consider that we have many runs, not just one, so
    we must adjust our significance level accordingly. Since we have 48 runs, I will
    use the [Bonferroni correction](https://en.wikipedia.org/wiki/Bonferroni_correction)
    and simply divide 1% by 48, obtaining a final significance level of 0.0002.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing this number with the p-values listed above, this means we will consider
    a run significant only if it has less than 3 or more than 22 (both inclusive)
    successes.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have taken measures to avoid being fooled by chance, we are ready
    to look at the results.
  prefs: []
  type: TYPE_NORMAL
- en: For simplicity, let’s take as metric the percentage of times that `ap_noimpute`
    is greater than `ap_impute`. For example, if the model based on the dataset containing
    missing values has a better average precision than the model on the imputed dataset
    for 10 out of the 25 iterations, this metric will be 40%.
  prefs: []
  type: TYPE_NORMAL
- en: Since we have 48 runs, we will have 48 values. So let’s create a bar plot to
    see all of them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2cbd44a59a1c5bbce9d11a18adb1fe5f.png)'
  prefs: []
  type: TYPE_IMG
- en: Results of the experiment. [Image by Author]
  prefs: []
  type: TYPE_NORMAL
- en: 'The red dashed lines identify the significance threshold: any value that is
    smaller than 12% (inclusive) or greater than 88% (inclusive) is significant (12%
    and 88% correspond respectively to 3/25 and 22/25).'
  prefs: []
  type: TYPE_NORMAL
- en: From the bar plot, we can see that **when we don’t impute the missing values
    we obtain a better result in 30 out of 48 runs** (63%). In 7 of these 30 cases,
    the result is so extreme that it’s also statistically significant according to
    a 1% p-value with Bonferroni adjustment. Instead, in the 18 cases in which imputing
    wins, the result is never significantly different from pure chance.
  prefs: []
  type: TYPE_NORMAL
- en: Based on these results, we can say that **either the difference between imputing
    and not imputing is not significant or it is significant in favor of not imputing**.
  prefs: []
  type: TYPE_NORMAL
- en: In short, there is no reason to impute missing values.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we empirically proved that the difference between imputing
    and not imputing is either not significant or significant in favor of not imputing.
    If you also consider that not imputing missing values will keep your pipeline
    cleaner and faster, leaving nulls in your dataset should be the standard, when
    you can do that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, this is not always possible. Some models that cannot handle missing
    values: take for instance Linear Regression or K-Means.'
  prefs: []
  type: TYPE_NORMAL
- en: However, the good news is that when you use the most common models for tabular
    tasks (i.e. tree-based models), not imputing the missing values and just letting
    the model handle them is the most effective and efficient approach.
  prefs: []
  type: TYPE_NORMAL
- en: '*You can reproduce all the code used for this article with* [*this notebook*](https://github.com/smazzanti/tds_your_dataset_has_missing_values_do_nothing/blob/main/missing-values-do-nothing.ipynb)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Thank you for reading!*'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you find my work useful, you can subscribe to* [***get an email every time
    that I publish a new article***](https://medium.com/@mazzanti.sam/subscribe) *(usually
    once a month).*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Want to show me your support for my work? You can* [***buy me a cappuccino***](https://ko-fi.com/samuelemazzanti)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you’d like,* [***add me on Linkedin***](https://www.linkedin.com/in/samuelemazzanti/)*!*'
  prefs: []
  type: TYPE_NORMAL
