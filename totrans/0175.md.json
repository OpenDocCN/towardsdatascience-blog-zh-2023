["```py\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nclass WineDataset(Dataset):\n    def __init__(self, data, targets):\n        self.data = data\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.data[idx], dtype=torch.float), torch.tensor(self.targets[idx], dtype=torch.long)\n\nclass SimpleNN(torch.nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n        self.fc1 = torch.nn.Linear(13, 64)\n        self.fc2 = torch.nn.Linear(64, 3)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\nclass Trainer():\n    def __init__(self, model, train_data, optimizer, gpu_id, save_every):\n        self.model = model\n        self.train_data = train_data\n        self.optimizer = optimizer\n        self.gpu_id = gpu_id\n        self.save_every = save_every\n        self.losses = []\n\n    def _run_batch(self, source, targets):\n        self.optimizer.zero_grad()\n        output = self.model(source)\n        loss = F.cross_entropy(output, targets)\n        loss.backward()\n        self.optimizer.step()\n        return loss.item()\n\n    def _run_epoch(self, epoch):\n        total_loss = 0.0\n        num_batches = len(self.train_data)\n        for source, targets in self.train_data:\n            source = source.to(self.gpu_id)\n            targets = targets.to(self.gpu_id)\n            loss = self._run_batch(source, targets)\n            total_loss += loss\n\n        avg_loss = total_loss / num_batches\n        self.losses.append(avg_loss)\n        print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n\n    def _save_checkpoint(self, epoch):\n        checkpoint = self.model.state_dict()\n        PATH = f\"model_{epoch}.pt\"\n        torch.save(checkpoint, PATH)\n        print(f\"Epoch {epoch} | Model saved to {PATH}\")\n\n    def train(self, max_epochs):\n        self.model.train()\n        for epoch in range(max_epochs):\n            self._run_epoch(epoch)\n            if epoch % self.save_every == 0:\n                self._save_checkpoint(epoch)\n\ndef load_train_objs():\n    wine_data = load_wine()\n    X = wine_data.data\n    y = wine_data.target\n\n    # Normalize and split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    scaler = StandardScaler().fit(X_train)\n    X_train = scaler.transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    train_set = WineDataset(X_train, y_train)\n    test_set = WineDataset(X_test, y_test)\n\n    print(\"Sample from dataset:\")\n    sample_data, sample_target = train_set[0]\n    print(f\"Data: {sample_data}\")\n    print(f\"Target: {sample_target}\")\n\n    model = SimpleNN()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    return train_set, model, optimizer\n\ndef prepare_dataloader(dataset, batch_size):\n    return DataLoader(dataset, batch_size=batch_size, pin_memory=True, shuffle=True)\n\ndef main(device, total_epochs, save_every, batch_size):\n    dataset, model, optimizer = load_train_objs()\n    train_data = prepare_dataloader(dataset, batch_size)\n    trainer = Trainer(model, train_data, optimizer, device, save_every)\n    trainer.train(total_epochs)\n\nmain(device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"), total_epochs=100, save_every=50, batch_size=32)\n```", "```py\nimport torch.multiprocessing as mp\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import init_process_group, destroy_process_group\nimport os\n```", "```py\ndef ddp_setup(rank, world_size):\n    \"\"\"\n    Set up the distributed environment.\n\n    Args:\n        rank: The rank of the current process. Unique identifier for each process in the distributed training.\n        world_size: Total number of processes participating in the distributed training.\n    \"\"\"\n\n    # Address of the main node. Since we are doing single-node training, it's set to localhost.\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n\n    # Port on which the master node is expected to listen for communications from workers.\n    os.environ[\"MASTER_PORT\"] = \"12355\"\n\n    # Initialize the process group. \n    # 'backend' specifies the communication backend to be used, \"nccl\" is optimized for GPU training.\n    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n\n    # Set the current CUDA device to the specified device (identified by rank).\n    # This ensures that each process uses a different GPU in a multi-GPU setup.\n    torch.cuda.set_device(rank)\n```", "```py\nclass Trainer():\n    def __init__(self, model, train_data, optimizer, gpu_id, save_every):\n        self.model = model.to(gpu_id)\n        self.train_data = train_data\n        self.optimizer = optimizer\n        self.gpu_id = gpu_id\n        self.save_every = save_every\n        self.losses = []\n\n        # This changes\n        self.model = DDP(self.model, device_ids=[gpu_id])\n```", "```py\ndef prepare_dataloader(dataset: Dataset, batch_size: int):\n    return DataLoader(\n        dataset,\n        batch_size=batch_size,\n        pin_memory=True,\n        shuffle=False,\n        sampler=DistributedSampler(dataset)\n    )\n```", "```py\ndef main(rank: int, world_size: int, save_every: int, total_epochs: int, batch_size: int):\n    \"\"\"\n    Main training function for distributed data parallel (DDP) setup.\n\n    Args:\n        rank (int): The rank of the current process (0 <= rank < world_size). Each process is assigned a unique rank.\n        world_size (int): Total number of processes involved in the distributed training.\n        save_every (int): Frequency of model checkpoint saving, in terms of epochs.\n        total_epochs (int): Total number of epochs for training.\n        batch_size (int): Number of samples processed in one iteration (forward and backward pass).\n    \"\"\"\n\n    # Set up the distributed environment, including setting the master address, port, and backend.\n    ddp_setup(rank, world_size)\n\n    # Load the necessary training objects - dataset, model, and optimizer.\n    dataset, model, optimizer = load_train_objs()\n\n    # Prepare the data loader for distributed training. It partitions the dataset across the processes and handles shuffling.\n    train_data = prepare_dataloader(dataset, batch_size)\n\n    # Initialize the trainer instance with the loaded model, data, and other configurations.\n    trainer = Trainer(model, train_data, optimizer, rank, save_every)\n\n    # Train the model for the specified number of epochs.\n    trainer.train(total_epochs)\n\n    # Cleanup the distributed environment after training is complete.\n    destroy_process_group()\n```", "```py\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser(description='simple distributed training job')\n    parser.add_argument('total_epochs', type=int, help='Total epochs to train the model')\n    parser.add_argument('save_every', type=int, help='How often to save a snapshot')\n    parser.add_argument('--batch_size', default=32, type=int, help='Input batch size on each device (default: 32)')\n    args = parser.parse_args()\n\n    world_size = torch.cuda.device_count()\n    mp.spawn(main, args=(world_size, args.save_every, args.total_epochs, args.batch_size), nprocs=world_size)\n```", "```py\ndef ddp_setup(local_rank, world_size_per_node, node_rank):\n    os.environ[\"MASTER_ADDR\"] = \"MASTER_NODE_IP\"  # <-- Replace with your master node IP\n    os.environ[\"MASTER_PORT\"] = \"12355\"  \n    global_rank = node_rank * world_size_per_node + local_rank\n    init_process_group(backend=\"nccl\", rank=global_rank, world_size=world_size_per_node*torch.cuda.device_count())\n    torch.cuda.set_device(local_rank)\n```", "```py\ndef main(local_rank: int, world_size_per_node: int, save_every: int, total_epochs: int, batch_size: int, node_rank: int):\n    ddp_setup(local_rank, world_size_per_node, node_rank)\n    # ... (rest of the main function)\n```", "```py\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser(description='simple distributed training job')\n    parser.add_argument('total_epochs', type=int, help='Total epochs to train the model')\n    parser.add_argument('save_every', type=int, help='How often to save a snapshot')\n    parser.add_argument('--batch_size', default=32, type=int, help='Input batch size on each device (default: 32)')\n    parser.add_argument('--node_rank', default=0, type=int, help='The rank of the node in multi-node training')\n    args = parser.parse_args()\n\n    world_size_per_node = torch.cuda.device_count()\n    mp.spawn(main, args=(world_size_per_node, args.save_every, args.total_epochs, args.batch_size, args.node_rank), nprocs=world_size_per_node)\n```", "```py\n#!/bin/bash\n#SBATCH --job-name=DDPTraining       # Name of the job\n#SBATCH --nodes=$1                   # Number of nodes specified by the user\n#SBATCH --ntasks-per-node=1          # Ensure only one task runs per node\n#SBATCH --cpus-per-task=1            # Number of CPU cores per task\n#SBATCH --gres=gpu:1                 # Number of GPUs per node\n#SBATCH --time=01:00:00              # Time limit hrs:min:sec (1 hour in this example)\n#SBATCH --mem=4GB                    # Memory limit per GPU\n#SBATCH --output=training_%j.log     # Output and error log name (%j expands to jobId)\n#SBATCH --partition=gpu              # Specify the partition or queue\n\nsrun python3 your_python_script.py --total_epochs 10 --save_every 2 --batch_size 32 --node_rank $SLURM_NODEID\n```", "```py\nsbatch train_net.sh 2  # for using 2 nodes\n```"]