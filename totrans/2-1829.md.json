["```py\n\"\"\"\nDownloading and rendering sample MNIST data\n\"\"\"\n\n#torch setup\nimport torch\nimport torchvision\nimport torchvision.datasets as datasets\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n#downloading mnist\nmnist_trainset = datasets.MNIST(root='./data', train=True,\n                                download=True, transform=None)\nmnist_testset = datasets.MNIST(root='./data', train=False,\n                               download=True, transform=None)\n\n#printing lengths\nprint('length of the training set: {}'.format(len(mnist_trainset)))\nprint('length of the test set: {}'.format(len(mnist_testset)))\n\n#rendering a few examples\nfor i in range(3):\n  print('the number {}:'.format(mnist_trainset[i][1]))\n  mnist_trainset[i][0].show()\n```", "```py\n\"\"\"\nCreating un-labled data, and handling necessary data preprocessing\n\"\"\"\n\nfrom tqdm import tqdm\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\n\n# ========== Data Extraction ==========\n# unlabeling some data, and one hot encoding the labels which remain\n# =====================================\n\npartition_index = 200\n\ndef one_hot(y):\n  #For converting a numpy array of 0-9 into a one hot encoding of vectors of length 10\n  b = np.zeros((y.size, y.max() + 1))\n  b[np.arange(y.size), y] = 1\n  return b\n\nprint('processing labeld training x and y')\ntrain_x = np.asarray([np.asarray(mnist_trainset[i][0]) for i in tqdm(range(partition_index))])\ntrain_y = one_hot(np.asarray([np.asarray(mnist_trainset[i][1]) for i in tqdm(range(partition_index))]))\n\nprint('processing unlabled training data')\ntrain_unlabled = np.asarray([np.asarray(mnist_trainset[i][0]) for i in tqdm(range(partition_index,len(mnist_trainset)))])\n\nprint('processing labeld test x and y')\ntest_x = np.asarray([np.asarray(mnist_testset[i][0]) for i in tqdm(range(len(mnist_testset)))])\ntest_y = one_hot(np.asarray([np.asarray(mnist_testset[i][1]) for i in tqdm(range(len(mnist_testset)))]))\n\n# ========== Data Reformatting ==========\n# adding a channel dimension and converting to pytorch\n# =====================================\n\n#adding a dimension to all X values to put them in the proper shape\n#(batch size, channels, x, y)\nprint('reformatting shape...')\ntrain_x = np.expand_dims(train_x, 1)\ntrain_unlabled = np.expand_dims(train_unlabled, 1)\ntest_x = np.expand_dims(test_x, 1)\n\n#converting data to pytorch type\ntorch_train_x = torch.tensor(train_x.astype(np.float32), requires_grad=True).to(device)\ntorch_train_y = torch.tensor(train_y).to(device)\ntorch_test_x = torch.tensor(test_x.astype(np.float32), requires_grad=True).to(device)\ntorch_test_y = torch.tensor(test_y).to(device)\ntorch_train_unlabled = torch.tensor(train_unlabled.astype(np.float32), requires_grad=True).to(device)\n\nprint('done')\n```", "```py\n\"\"\"\nUsing PyTorch to create a modified, smaller version of AlexNet\n\"\"\"\nimport torch.nn.functional as F\nimport torch.nn as nn\n\n#defining model backbone\nclass Backbone(nn.Module):\n    def __init__(self):\n        super(Backbone, self).__init__()\n        self.conv1 = nn.Conv2d(1, 16, 3)\n        self.conv2 = nn.Conv2d(16, 16, 3)\n        self.conv3 = nn.Conv2d(16, 32, 3)\n\n        if torch.cuda.is_available():\n            self.cuda()\n\n    def forward(self, x):\n        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n        x = torch.flatten(x, 1)\n        return x\n\n#defining model head\nclass Head(nn.Module):\n    def __init__(self, n_class=10):\n        super(Head, self).__init__()\n        self.fc1 = nn.Linear(32, 32)\n        self.fc2 = nn.Linear(32, 16)\n        self.fc3 = nn.Linear(16, n_class)\n\n        if torch.cuda.is_available():\n            self.cuda()\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return F.softmax(x,1)\n\n#defining full model\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.backbone = Backbone()\n        self.head = Head()\n\n        if torch.cuda.is_available():\n            self.cuda()\n\n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.head(x)\n        return x\n\nmodel_baseline = Model()\nprint(model_baseline(torch_train_x[:1]).shape)\nmodel_baseline\n```", "```py\n\"\"\"\nTraining model using only supervised learning, and rendering the results.\nThis supervised training function is reused in the future for fine tuning\n\"\"\"\n\ndef supervised_train(model):\n\n    #defining key hyperparamaters explicitly (instead of hyperparamater search)\n    batch_size = 64\n    lr = 0.001\n    momentum = 0.9\n    num_epochs = 20000\n\n    #defining a stocastic gradient descent optimizer\n    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n\n    #defining loss function\n    loss_fn = torch.nn.CrossEntropyLoss()\n\n    train_hist = []\n    test_hist = []\n    test_accuracy = []\n\n    for epoch in tqdm(range(num_epochs)):\n\n        #iterating over all batches\n        for i in range(int(len(train_x)/batch_size)-1):\n\n            #Put the model in training mode, so that things like dropout work\n            model.train(True)\n\n            # Zero gradients\n            optimizer.zero_grad()\n\n            #extracting X and y values from the batch\n            X = torch_train_x[i*batch_size: (i+1)*batch_size]\n            y = torch_train_y[i*batch_size: (i+1)*batch_size]\n\n            # Make predictions for this batch\n            y_pred = model(X)\n\n            #compute gradients\n            loss_fn(model(X), y).backward()\n\n            # Adjust learning weights\n            optimizer.step()\n\n        with torch.no_grad():\n\n            #Disable things like dropout, if they exist\n            model.train(False)\n\n            #calculating epoch training and test loss\n            train_loss = loss_fn(model(torch_train_x), torch_train_y).cpu().numpy()\n            y_pred_test = model(torch_test_x)\n            test_loss = loss_fn(y_pred_test, torch_test_y).cpu().numpy()\n\n            train_hist.append(train_loss)\n            test_hist.append(test_loss)\n\n            #computing test accuracy\n            matches = np.equal(np.argmax(y_pred_test.cpu().numpy(), axis=1), np.argmax(torch_test_y.cpu().numpy(), axis=1))\n            test_accuracy.append(matches.sum()/len(matches))\n\n    import matplotlib.pyplot as plt\n    plt.plot(train_hist, label = 'train loss')\n    plt.plot(test_hist, label = 'test loss')\n    plt.legend()\n    plt.show()\n    plt.plot(test_accuracy, label = 'test accuracy')\n    plt.legend()\n    plt.show()\n\n    maxacc = max(test_accuracy)\n    print('max accuracy: {}'.format(maxacc))\n\n    return maxacc\n\nsupervised_maxacc = supervised_train(model_baseline)\n```", "```py\nimport torch\nimport torchvision.transforms as T\n\nclass Augment:\n   \"\"\"\n   A stochastic data augmentation module\n   Transforms any given data example randomly\n   resulting in two correlated views of the same example,\n   denoted x ̃i and x ̃j, which we consider as a positive pair.\n   \"\"\"\n\n   def __init__(self):\n\n       blur = T.GaussianBlur((3, 3), (0.1, 2.0))\n\n       self.train_transform = torch.nn.Sequential(\n           T.RandomAffine(degrees = (-50,50), translate = (0.1,0.1), scale=(0.5,1.5), shear=0.2),\n           T.RandomPerspective(0.4,0.5),\n           T.RandomPerspective(0.2,0.5),\n           T.RandomPerspective(0.2,0.5),\n           T.RandomApply([blur], p=0.25),\n           T.RandomApply([blur], p=0.25)\n       )\n\n   def __call__(self, x):\n       return self.train_transform(x), self.train_transform(x)\n\n\"\"\"\nGenerating Test Augmentation\n\"\"\"\na = Augment()\naug = a(torch_train_unlabled[0:100])\n\ni=1\nf, axarr = plt.subplots(2,2)\n#positive pair\naxarr[0,0].imshow(aug[0].cpu().detach().numpy()[i,0])\naxarr[0,1].imshow(aug[1].cpu().detach().numpy()[i,0])\n#another positive pair\naxarr[1,0].imshow(aug[0].cpu().detach().numpy()[i+1,0])\naxarr[1,1].imshow(aug[1].cpu().detach().numpy()[i+1,0])\nplt.show()\n```", "```py\nclass ContrastiveLoss(nn.Module):\n   \"\"\"\n   Vanilla Contrastive loss, also called InfoNceLoss as in SimCLR paper\n   \"\"\"\n   def __init__(self, batch_size, temperature=0.5):\n       \"\"\"\n       Defining certain constants used between calculations. The mask is important\n       in understanding which are positive and negative examples. For more\n       information see https://theaisummer.com/simclr/\n       \"\"\"\n       super().__init__()\n       self.batch_size = batch_size\n       self.temperature = temperature\n       self.mask = (~torch.eye(batch_size * 2, batch_size * 2, dtype=bool)).float().to(device)\n\n   def calc_similarity_batch(self, a, b):\n       \"\"\"\n       Defines the cosin similarity between one example, and all other examples.\n       For more information see https://theaisummer.com/simclr/\n       \"\"\"\n       representations = torch.cat([a, b], dim=0)\n       return F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=2)\n\n   def forward(self, proj_1, proj_2):\n       \"\"\"\n       The actual loss function, where proj_1 and proj_2 are embeddings from the\n       projection head. This function calculates the cosin similarity between\n       all vectors, and rewards closeness between examples which come from the\n       same example, and farness for examples which do not. For more information\n       see https://theaisummer.com/simclr/\n       \"\"\"\n       batch_size = proj_1.shape[0]\n       z_i = F.normalize(proj_1, p=2, dim=1)\n       z_j = F.normalize(proj_2, p=2, dim=1)\n\n       similarity_matrix = self.calc_similarity_batch(z_i, z_j)\n\n       sim_ij = torch.diag(similarity_matrix, batch_size)\n       sim_ji = torch.diag(similarity_matrix, -batch_size)\n\n       positives = torch.cat([sim_ij, sim_ji], dim=0)\n\n       nominator = torch.exp(positives / self.temperature)\n\n       denominator = self.mask * torch.exp(similarity_matrix / self.temperature)\n\n       all_losses = -torch.log(nominator / torch.sum(denominator, dim=1))\n       loss = torch.sum(all_losses) / (2 * self.batch_size)\n       return loss\n\n\"\"\"\ntesting\n\"\"\"\nloss = ContrastiveLoss(torch_train_x.shape[0]).forward\nfake_proj_0, fake_proj_1 = a(torch_train_x)\nfake_proj_0 = fake_proj_0[:,0,:,0]\nfake_proj_1 = fake_proj_1[:,0,:,0]\nloss(fake_proj_0, fake_proj_1)\n```", "```py\nfrom torch.optim.lr_scheduler import ExponentialLR\n\n#degining a new model\nmodel = Model()\nmodel.train()\n\n#defining key hyperparameters\nbatch_size = 512\nepoch_size = round(torch_train_unlabled.shape[0]/batch_size)-1\nnum_epochs = 100\npatience = 5\ncutoff_ratio = 0.001\n\n#defining key learning functions\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nnum_examples = train_unlabled.shape[0]\nlossfn = ContrastiveLoss(batch_size).forward\naugmentfn = Augment() #augment function\n\n#for book keeping\nloss_hist = []\nimprovement_hist = []\nschedule_hist = []\n\n#for exponentially decreasing learning rate\nscheduler = ExponentialLR(optimizer,\n                          gamma = 0.95)\n\n#for early stopping\npatience_count=0\n\n#Training Loop\navg_loss = 1e10\nfor i in range(num_epochs):\n\n    print('epoch {}/{}'.format(i,num_epochs))\n\n    total_loss = 0\n    loss_change = 0\n\n    for j in tqdm(range(epoch_size)):\n\n        #getting random batch\n        X = torch_train_unlabled[j*batch_size: (j+1)*batch_size]\n\n        #creating pairs of augmented batches\n        X_aug_i, X_aug_j = augmentfn(X)\n\n        #ensuring gradients are zero\n        optimizer.zero_grad()\n\n        #passing through the model\n        z_i = model(X_aug_i)\n        z_j = model(X_aug_j)\n\n        #calculating loss on the model embeddings, and computing gradients\n        loss = lossfn(z_i, z_j)\n        loss.backward()\n\n        # Adjust learning weights\n        optimizer.step()\n\n        #checking to see if backpropegation resulted in a reduction of the loss function\n        if True:\n            #passing through the model, now that parameters have been updated\n            z_i = model(X_aug_i)\n            z_j = model(X_aug_j)\n\n            #calculating new loss value\n            new_loss = lossfn(z_i, z_j)\n\n            loss_change += new_loss.cpu().detach().numpy() - loss.cpu().detach().numpy()\n\n        total_loss += loss.cpu().detach().numpy()\n\n        #step learning rate scheduler\n        schedule_hist.append(scheduler.get_last_lr())\n\n    scheduler.step()\n\n    #calculating percentage loss reduction\n    new_avg_loss = total_loss/epoch_size\n    per_loss_reduction = (avg_loss-new_avg_loss)/avg_loss\n    print('Percentage Loss Reduction: {}'.format(per_loss_reduction))\n\n    #deciding to stop if loss is not decreasing fast enough\n    if per_loss_reduction < cutoff_ratio:\n        patience_count+=1\n        print('patience counter: {}'.format(patience_count))\n        if patience_count > patience:\n            break\n    else:\n        patience_count = 0\n\n    #setting new loss as previous loss\n    avg_loss = new_avg_loss\n\n    #book keeping\n    avg_improvement = loss_change/epoch_size\n    loss_hist.append(avg_loss)\n    improvement_hist.append(avg_improvement)\n    print('Average Loss: {}'.format(avg_loss))\n    print('Average Loss change (if calculated): {}'.format(avg_im\n```", "```py\nplt.plot(schedule_hist, label='learning rate')\nplt.legend()\nplt.show()\nplt.plot(loss_hist, label = 'loss')\nplt.legend()\nplt.show()\n```", "```py\nimport copy\n\n#creating duplicate models for finetuning\nmodel_same_head = copy.deepcopy(model)\nmodel_new_head = copy.deepcopy(model)\n\n#replacing the projection head with a randomly initialized head\n#for one of the models\nmodel_new_head.head = Head()\n\n#training models\nsame_head_maxacc = supervised_train(model_same_head)\nnew_head_maxacc = supervised_train(model_new_head)\n```"]