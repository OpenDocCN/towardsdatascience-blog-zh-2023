- en: How 25,000 Computers Trained ChatGPT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何用 25,000 台计算机训练 ChatGPT
- en: 原文：[https://towardsdatascience.com/how-25-000-computers-trained-chatgpt-11104686a24d](https://towardsdatascience.com/how-25-000-computers-trained-chatgpt-11104686a24d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-25-000-computers-trained-chatgpt-11104686a24d](https://towardsdatascience.com/how-25-000-computers-trained-chatgpt-11104686a24d)
- en: '![](../Images/fb497373dd0aa1eee593aec71f7c2eb5.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb497373dd0aa1eee593aec71f7c2eb5.png)'
- en: Photo by [Volodymyr Hryshchenko](https://unsplash.com/@lunarts?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Volodymyr Hryshchenko](https://unsplash.com/@lunarts?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The breakthrough behind ChatGPT
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChatGPT 背后的突破
- en: '[](https://medium.com/@JerryQu?source=post_page-----11104686a24d--------------------------------)[![Jerry
    Qu](../Images/f36a4a13d44c97923fa2b4b7b1290e1b.png)](https://medium.com/@JerryQu?source=post_page-----11104686a24d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----11104686a24d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----11104686a24d--------------------------------)
    [Jerry Qu](https://medium.com/@JerryQu?source=post_page-----11104686a24d--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@JerryQu?source=post_page-----11104686a24d--------------------------------)[![Jerry
    Qu](../Images/f36a4a13d44c97923fa2b4b7b1290e1b.png)](https://medium.com/@JerryQu?source=post_page-----11104686a24d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----11104686a24d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----11104686a24d--------------------------------)
    [Jerry Qu](https://medium.com/@JerryQu?source=post_page-----11104686a24d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----11104686a24d--------------------------------)
    ·5 min read·Aug 29, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----11104686a24d--------------------------------)
    ·阅读时间 5 分钟·2023年8月29日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/017490466c0e04b2cbad388da6184527.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/017490466c0e04b2cbad388da6184527.png)'
- en: Image by Author
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: What word comes after Good?
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “Good” 之后是什么词？
- en: You might think **Good Morning**, or **Good Bye**. But you definitely wouldn’t
    say ***Good Loud.*** That just doesn’t make sense. For decades, computer scientists
    have been training AI to solve this exact problem.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想到**早上好**，或者**再见**。但你绝对不会说 ***声音很大***。这没有意义。几十年来，计算机科学家们一直在训练 AI 解决这个确切的问题。
- en: '![](../Images/7dc5b0ba4db5b44c14f7e911d0c78e4b.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7dc5b0ba4db5b44c14f7e911d0c78e4b.png)'
- en: Image by Author
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Given a word, our AI predicts the next word. Do this several times, & you’ve
    generated a sentence.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个词，我们的 AI 预测下一个词。这样做几次，你就生成了一句完整的句子。
- en: This is how ChatGPT works.
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这就是 ChatGPT 的工作原理。
- en: Trained over the entire internet, ChatGPT has learned how to chat like a human.
    However, this immense feat was only made possible by a breakthrough in the late
    2010s. A breakthrough underpinning ChatGPT & forever shaping the world we live
    in.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 经过整个互联网的训练，ChatGPT 学会了像人类一样聊天。然而，这一巨大的成就只有在 2010 年代末期的突破之后才得以实现。这一突破支撑了 ChatGPT，并永远改变了我们生活的世界。
- en: This is the story of an AI that read & learned from every book, tweet, & website
    across the entire internet. And how it was made possible.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个 AI 从整个互联网的每本书、每条推文、每个网站中阅读并学习的故事。以及它是如何实现的。
- en: Sentences are long.
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 句子很长。
- en: When we move beyond a single word, next word prediction is a lot harder. Take
    this example.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们超越单个词时，下一个词的预测就变得困难得多。看这个例子。
- en: '![](../Images/95206d3ccd94b8360832e3a957bbc553.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95206d3ccd94b8360832e3a957bbc553.png)'
- en: Image by Author
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: In this context, it makes no sense to say **I ate a good *Morning***. But our
    AI only looks at good, and spits out morning. In most cases even humans need many
    words to predict the next word. So an AI needs this extra information as well.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，说**我吃了一个好的 *早晨***是没有意义的。但是我们的AI只关注“good”，并且输出“morning”。即使是人类也需要许多词来预测下一个词。因此，AI也需要这些额外的信息。
- en: Our AI needs to read many words to predict the next word. ChatGPT can read more
    than [8**,000 previous words**](https://platform.openai.com/docs/models/gpt-4)
    at once. The natural way to do this would be to feed each word into the AI, one
    by one.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 AI 需要阅读许多词来预测下一个词。ChatGPT 一次可以读取超过 [8**,000 个先前的词**](https://platform.openai.com/docs/models/gpt-4)。自然的做法是逐个将每个词输入
    AI。
- en: '![](../Images/360c316240c0e46b9028be0312978d1b.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/360c316240c0e46b9028be0312978d1b.png)'
- en: Image by Author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: This is how AIs worked in the past. A **Recurrent Neural Network (RNN)** would
    take one word at a time, storing information as it reads a sentence.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是过去 AI 的工作方式。**递归神经网络（RNN）**一次处理一个词，在读取句子时存储信息。
- en: One of the problems with this AI is that it’s **incredibly slow**. Each word
    has to wait for the last, which is a problem at large scales. Imagine if your
    washing machine could only wash one shirt at a time. This **sequential** process
    would take days. But everyone knows we can throw in all our shirts at the same
    time, finishing in minutes. This is the idea of **parallelism.** By performing
    work in parallel instead of sequentially, we can dramatically speed up washing
    machines, computers, & AI.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 AI 的一个问题是它**极其缓慢**。每个词都必须等待前一个词处理完成，这在大规模时是个问题。想象一下，如果你的洗衣机一次只能洗一件衬衫，这种**顺序**处理将需要几天时间。但每个人都知道我们可以同时放入所有衬衫，几分钟就能完成。这就是**并行**的概念。通过并行执行工作而不是顺序执行，我们可以显著加速洗衣机、计算机和
    AI。
- en: RNNs could only be trained on millions of words, nowhere near the trillions
    across the internet. We needed a faster, more efficient way of reading sentences.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 只能在数百万个词上进行训练，远远不及互联网上的万亿词汇。我们需要一种更快、更高效的句子阅读方法。
- en: The Transformer was the solution.
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer 是解决方案。
- en: In 2017, a paper titled [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
    was published. This paper effectively turned sentences on their side. These researchers
    invented an AI that can read a whole sentence at once.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 2017 年，一篇题为[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)的论文被发表。这篇论文有效地改变了句子的处理方式。这些研究人员发明了一种可以一次读取整句的
    AI。
- en: '![](../Images/57482a64785dad9087764a67ed93030e.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57482a64785dad9087764a67ed93030e.png)'
- en: Image by Author
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: This new AI is called a **Transformer**, and its efficiency allowed it to learn
    from every book & website on the internet. To understand how it does this, we
    need to take a step back & understand how computers read text.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这种新的 AI 被称为**Transformer**，它的高效性使得它能够从互联网上的每本书和每个网站中学习。要理解它是如何做到这一点的，我们需要退一步了解计算机如何读取文本。
- en: How can an AI *read* text?
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI 如何*读取*文本？
- en: Computers work in 1s & 0s. Called Binary, these 1s & 0s make up numbers. Computer
    scientists needed a way to represent words as numbers. And this was made possible
    in 2013, when scientists at Google created [word2vec](https://arxiv.org/pdf/1301.3781.pdf).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机以 1 和 0 的形式工作。这些 1 和 0 被称为二进制，它们组成了数字。计算机科学家需要一种将词语表示为数字的方法。这在 2013 年得以实现，当时
    Google 的科学家们创建了[word2vec](https://arxiv.org/pdf/1301.3781.pdf)。
- en: Words contain semantic meaning. Dogs are related to cats. Kings are related
    to queens. Word2vec was able to represent these semantics in vectors or lists
    of numbers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 词语包含语义意义。狗与猫相关。国王与王后相关。Word2vec 能够在向量或数字列表中表示这些语义。
- en: With word2vec, you could take King, subtract Man, add Woman, and get the word
    Queen.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 word2vec，你可以取“King”，减去“Man”，加上“Woman”，然后得到“Queen”这个词。
- en: '![](../Images/8ad19072bc348084f2cb45c6c4cabb10.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ad19072bc348084f2cb45c6c4cabb10.png)'
- en: Image by Author
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: This vector of numbers is called a **Word Embedding**. They embed the word’s
    meaning into this vector. When training an AI to process text, we actually feed
    it these word embeddings. The AI does some math, transforming these vectors, and
    spits out the next word. Transforming these vectors is what takes a lot of time.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数字向量称为**词嵌入**。它们将词语的意义嵌入到这个向量中。在训练 AI 处理文本时，我们实际上是将这些词嵌入输入给 AI。AI 进行一些数学运算，变换这些向量，并生成下一个词。变换这些向量是非常耗时的。
- en: '![](../Images/d522677b058694cb0e63b6d615679c19.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d522677b058694cb0e63b6d615679c19.png)'
- en: Image by Author
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: The Transformer does this all in parallel.
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer 完成了这一切的并行处理。
- en: Instead of waiting for the previous word to process, we transform all these
    word embeddings at the same time, performing an averaging to put them all together.
    This reduces the number of sequential operations from the length of the sentence,
    to a constant number.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 不再等待前一个词处理完成，我们同时变换所有这些词嵌入，进行平均以将它们汇总。这将顺序操作的数量从句子的长度减少到一个常数。
- en: '![](../Images/c38eade80bd1eb798eba17d4f0092e89.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c38eade80bd1eb798eba17d4f0092e89.png)'
- en: Image by Author
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Lambda Labs [estimated](https://lambdalabs.com/blog/demystifying-gpt-3) that
    training ChatGPT on a single GPU would take **355 years.** But by exploiting its
    parallelism, ChatGPT was trained across **25,000 GPUs** & finished in a matter
    of **days**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda Labs[估计](https://lambdalabs.com/blog/demystifying-gpt-3)训练 ChatGPT 在单个
    GPU 上需要**355 年**。但通过利用其并行性，ChatGPT 在**25,000 个 GPU**上训练，且仅用了**几天**时间。
- en: The Transformer sparked a paradigm shift in AI
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer 引发了人工智能领域的范式转变。
- en: With increased parallelism, larger and larger AIs could be trained. While the
    largest sequential models in the past were trained on millions of words, ChatGPT
    was trained on nearly a **trillion**.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 随着并行性的增加，越来越大的 AI 可以被训练。尽管过去最大的序列模型是基于数百万字进行训练的，但 ChatGPT 是在接近 **一万亿** 字的数据上训练的。
- en: '![](../Images/6a80c42f32564f938ca3dd7df397841e.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a80c42f32564f938ca3dd7df397841e.png)'
- en: Image by Author, Data from original papers
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者，数据来源于原始论文。
- en: ChatGPT was trained on CommonCrawl, a collection of the entire internet since
    2008\. Using **over 25,000 computers**, this model has read & learned from every
    website over the entire internet. Imagine reading every book, tweet, & piece of
    code ever published.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 在 CommonCrawl 上进行了训练，这是自 2008 年以来的整个互联网的集合。使用 **超过 25,000 台计算机**，该模型阅读并学习了整个互联网的每个网站。想象一下阅读每本书、每条推文和每段代码。
- en: Today, ChatGPT is being used to write code, [generate TV commercials](https://openai.com/customer-stories/waymark),
    & assist you with almost anything you can imagine! By turning sentences on their
    side, we’ve created a new era in AI, one that pushes the boundaries of what was
    once thought possible.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，ChatGPT 正在被用来编写代码，[生成电视广告](https://openai.com/customer-stories/waymark)，并帮助你完成几乎可以想象的任何事情！通过将句子转向侧面，我们开创了
    AI 的新时代，这一时代突破了曾经认为可能的界限。
- en: But we may have reached a limit.
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 但我们可能已经达到了极限。
- en: After the release of GPT-4, Sam Altman, OpenAI’s CEO said,
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPT-4 发布后，OpenAI 的 CEO Sam Altman 说，
- en: “I think we’re at the end of the era where it’s going to be these, like, giant,
    giant models…”
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我认为我们正处于这些巨大模型时代的尽头……”
- en: After learning from the entire internet, what comes next? The impact of ChatGPT
    is trickling down into every industry. But like any breakthrough, progress plateaus.
    And AI’s next inflection point, only time will tell.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在从整个互联网学习之后，接下来会是什么？ChatGPT 的影响正在渗透到每个行业。但像任何突破一样，进展会趋于平稳。AI 的下一个拐点，只有时间才能告诉我们。
- en: '![](../Images/54adf62cdc3227169027831f8dd9b06c.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54adf62cdc3227169027831f8dd9b06c.png)'
- en: Image by Author, data from original papers
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者，数据来源于原始论文。
- en: '**If you enjoyed this article:**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你喜欢这篇文章：**'
- en: follow my Medium, [LinkedIn](https://www.linkedin.com/in/jerry-qu/), and [Twitter](https://twitter.com/JerryQu2)
    to stay updated with my progress
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关注我的 Medium， [LinkedIn](https://www.linkedin.com/in/jerry-qu/)，和 [Twitter](https://twitter.com/JerryQu2)
    以获取我的最新进展。
