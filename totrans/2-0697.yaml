- en: 'Deep Learning for Forecasting: Preprocessing and Training'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习用于预测：数据预处理和训练
- en: 原文：[https://towardsdatascience.com/deep-learning-for-forecasting-preprocessing-and-training-49d2198fc0e2](https://towardsdatascience.com/deep-learning-for-forecasting-preprocessing-and-training-49d2198fc0e2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deep-learning-for-forecasting-preprocessing-and-training-49d2198fc0e2](https://towardsdatascience.com/deep-learning-for-forecasting-preprocessing-and-training-49d2198fc0e2)
- en: How to train deep neural networks using several time series
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用多个时间序列训练深度神经网络
- en: '[](https://vcerq.medium.com/?source=post_page-----49d2198fc0e2--------------------------------)[![Vitor
    Cerqueira](../Images/9e52f462c6bc20453d3ea273eb52114b.png)](https://vcerq.medium.com/?source=post_page-----49d2198fc0e2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----49d2198fc0e2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----49d2198fc0e2--------------------------------)
    [Vitor Cerqueira](https://vcerq.medium.com/?source=post_page-----49d2198fc0e2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://vcerq.medium.com/?source=post_page-----49d2198fc0e2--------------------------------)[![Vitor
    Cerqueira](../Images/9e52f462c6bc20453d3ea273eb52114b.png)](https://vcerq.medium.com/?source=post_page-----49d2198fc0e2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----49d2198fc0e2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----49d2198fc0e2--------------------------------)
    [Vitor Cerqueira](https://vcerq.medium.com/?source=post_page-----49d2198fc0e2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----49d2198fc0e2--------------------------------)
    ·8 min read·Mar 22, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----49d2198fc0e2--------------------------------)
    ·8分钟阅读·2023年3月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/be89556b550a8794748eb6a4e9d68f9b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be89556b550a8794748eb6a4e9d68f9b.png)'
- en: Photo by [Tamara Malaniy](https://unsplash.com/de/@tamarushphotos?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Tamara Malaniy](https://unsplash.com/de/@tamarushphotos?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: This article is a follow-up to [a previous one](https://medium.com/towards-data-science/how-to-transform-time-series-for-deep-learning-3b6abbbb3726).
    There, we learned how to transform a time series for deep learning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章是对[上一篇文章](https://medium.com/towards-data-science/how-to-transform-time-series-for-deep-learning-3b6abbbb3726)的后续。在那里，我们学习了如何为深度学习转换时间序列。
- en: 'We continue to explore deep neural networks for forecasting. In this post,
    we’ll:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续探索深度神经网络在预测中的应用。在这篇文章中，我们将：
- en: Learn how to train a global forecasting model using deep learning, including
    basic preprocessing steps;
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何使用深度学习训练全球预测模型，包括基本的预处理步骤；
- en: Explore keras callbacks to drive the training process of a neural network.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索keras回调函数以推动神经网络的训练过程。
- en: Deep Learning for Forecasting
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习用于预测
- en: Deep neural networks tackle forecasting problems using auto-regression. [Auto-regression
    is a modeling technique that involves using past observations to predict future
    ones](https://medium.com/towards-data-science/machine-learning-for-forecasting-transformations-and-feature-extraction-bbbea9de0ac2).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络通过自回归解决预测问题。[自回归是一种建模技术，涉及使用过去的观察值来预测未来的观察值](https://medium.com/towards-data-science/machine-learning-for-forecasting-transformations-and-feature-extraction-bbbea9de0ac2)。
- en: Deep neural networks can be designed in different ways, such as recurrent or
    convolutional architectures. Recurrent neural networks are often preferred for
    time series data. Among other reasons, this type of network excels at modeling
    long-term dependencies. This feature can have a strong impact on forecasting performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络可以设计成不同的结构，如递归网络或卷积网络。递归神经网络通常更适合处理时间序列数据。除此之外，这种网络在建模长期依赖关系方面表现出色。这一特性对预测性能有着强大的影响。
- en: Here’s how to define a specific kind of recurrent neural network called LSTM
    (Long Short-Term Memory). The comments provide a brief description of each model
    element.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这里介绍了一种特定类型的递归神经网络，称为LSTM（长短期记忆）。注释提供了每个模型元素的简要描述。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Before, we learned [how to transform a time series to train this model](https://medium.com/towards-data-science/how-to-transform-time-series-for-deep-learning-3b6abbbb3726).
    But, sometimes you have several time series available.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们学习了[如何转换时间序列以训练此模型](https://medium.com/towards-data-science/how-to-transform-time-series-for-deep-learning-3b6abbbb3726)。但是，有时你会有多个时间序列可用。
- en: How do you handle such cases?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如何处理这些情况？
- en: Using many time series for deep learning
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用多时间序列进行深度学习
- en: The rise of global methods
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全球方法的兴起
- en: Forecasting models are usually created with the historical data of a time series.
    Such models can be referred to as local to that time series. By contrast, global
    methods pool the historical data of many time series to build a model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 预测模型通常是使用时间序列的历史数据创建的。这些模型可以被称为局部模型。相比之下，全球方法则通过汇总多个时间序列的历史数据来建立模型。
- en: The interest in global models surged when a method called ES-RNN won the M4
    contest — a forecasting competition featuring 100000 different time series.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当一种称为ES-RNN的方法赢得M4竞赛——一个包含100000个不同时间序列的预测竞赛——时，对全球模型的兴趣激增。
- en: When and why to use a global model
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时以及为何使用全球模型
- en: Global models can provide considerable value in forecasting problems involving
    many time series. For example, in retail where the goal is to predict the sales
    of many products.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 全球模型在涉及多个时间序列的预测问题中可以提供相当大的价值。例如，在零售领域，目标是预测多种产品的销售量。
- en: Another motivation for using this kind of approach is to have more data. Machine
    learning algorithms are likely to perform better with larger training sets. This
    is especially so with methods with lots of parameters, such as deep neural networks.
    These [are known to be data-hungry](https://medium.com/towards-data-science/machine-learning-for-forecasting-size-matters-b5271ec784dc).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个使用这种方法的动机是获得更多的数据。机器学习算法在训练集更大时表现更好。这一点在具有大量参数的方法中尤为明显，如深度神经网络。这些[已知为数据需求大](https://medium.com/towards-data-science/machine-learning-for-forecasting-size-matters-b5271ec784dc)。
- en: Global forecasting models do not assume that the underlying time series are
    dependent. That is, the lags of one series can be used to forecast the future
    values of another series.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 全球预测模型不假设基础时间序列是相关的。也就是说，一个序列的滞后值可以用来预测另一个序列的未来值。
- en: Rather, these techniques exploit information from many time series to estimate
    the parameters of the model. When forecasting the future of a time series, the
    main input to the model is the past recent lags of that series.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，这些技术利用来自多个时间序列的信息来估计模型的参数。在预测时间序列的未来时，模型的主要输入是该序列的过去近期滞后值。
- en: Hands-On
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践操作
- en: In the rest of this article, we’ll explore how to train a deep neural network
    using many time series.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文的其余部分，我们将探讨如何使用多个时间序列训练深度神经网络。
- en: Data
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据
- en: 'We’ll use a data set about the power consumption in 8 regions across the USA:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用关于美国8个地区电力消耗的数据集：
- en: '![](../Images/67e85f3526f8d72d89c3227caafe353c.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67e85f3526f8d72d89c3227caafe353c.png)'
- en: Daily power consumption (log) in 8 regions across the USA. Data source in reference
    [1]. Image by author.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 美国8个地区的日常电力消耗（对数）。数据来源于参考文献[1]。图像由作者提供。
- en: The goal is to forecast power consumption in the following days. This problem
    is relevant for power systems operators. Accurate predictions help balance the
    supply and demand of energy.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是预测未来几天的电力消耗。这一问题对电力系统操作员至关重要。准确的预测有助于平衡能源的供需。
- en: 'We can read the data as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按如下方式读取数据：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/7b623e9d593f385d13d40a6ec689f169.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b623e9d593f385d13d40a6ec689f169.png)'
- en: Preprocessing steps
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理步骤
- en: 'When training a deep neural network with multiple time series you need to apply
    some preprocessing steps. Here, we’ll explore the following two:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练多个时间序列的深度神经网络时，需要应用一些预处理步骤。在这里，我们将探讨以下两种方法：
- en: Mean-scaling
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均值缩放
- en: Log transformation
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数变换
- en: The available set of time series can have different scales. Thus, it’s important
    to normalize each series into a common value range. For global forecasting models,
    this is usually done by dividing each observation by the mean value of the respective
    series.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的时间序列集可能具有不同的尺度。因此，将每个序列归一化到一个共同的值范围是很重要的。对于全球预测模型，这通常是通过将每个观测值除以相应序列的均值来完成的。
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After mean-scaling, the log transformation can also be helpful.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在均值缩放之后，对数变换也可以是有帮助的。
- en: '[In a previous article](https://medium.com/towards-data-science/3-ways-to-deal-with-heteroskedasticity-in-time-series-831f6499e688),
    we explore how taking the log of time series is a useful transformation to handle
    heteroskedasticity. The log transformation can also help avoid saturation areas
    of the neural network. Saturation occurs when the neural network becomes insensitive
    to different inputs. This hampers the learning process, leading to a poor model.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[在上一篇文章中](https://medium.com/towards-data-science/3-ways-to-deal-with-heteroskedasticity-in-time-series-831f6499e688)，我们探讨了如何通过对时间序列取对数来处理异方差性。对数变换还可以帮助避免神经网络的饱和区域。饱和发生在神经网络对不同输入变得不敏感时。这会阻碍学习过程，导致模型效果不佳。'
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Auto-regression
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自回归
- en: After pre-processing each time series, we need to transform them from sequences
    into a set of observations. For a single time series, you can [check the previous
    article](https://medium.com/towards-data-science/how-to-transform-time-series-for-deep-learning-3b6abbbb3726)
    to learn the details of this process.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在预处理每个时间序列之后，我们需要将它们从序列转换为观测数据集。对于单个时间序列，你可以[查看之前的文章](https://medium.com/towards-data-science/how-to-transform-time-series-for-deep-learning-3b6abbbb3726)以了解此过程的详细信息。
- en: For several time series, the idea is similar. We create a set of observations
    for each series individually. Then, these are concatenated into a single data
    set.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多个时间序列，思路类似。我们为每个序列分别创建一组观测数据。然后，这些数据被连接成一个单一的数据集。
- en: 'Here’s how you can do this:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你可以做到的方法：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'After that, you combine the data of each time series by a row-wise concatenation:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，你通过按行连接的方式将每个时间序列的数据组合在一起。
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/c12f9ea7e71bc52404b4c3477c85910f.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c12f9ea7e71bc52404b4c3477c85910f.png)'
- en: 'Finally, we split the target variables from the explanatory ones [as described
    before](https://medium.com/towards-data-science/how-to-transform-time-series-for-deep-learning-3b6abbbb3726):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将目标变量与解释变量分开，[如前所述](https://medium.com/towards-data-science/how-to-transform-time-series-for-deep-learning-3b6abbbb3726)：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Using Callbacks for Training a Deep Neural Network
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用回调函数训练深度神经网络
- en: '![](../Images/8b4be613bb8df9eeacdd8a0a358c9943.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b4be613bb8df9eeacdd8a0a358c9943.png)'
- en: Photo by [Jack B](https://unsplash.com/@nervum?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Jack B](https://unsplash.com/@nervum?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Deep neural networks are iterative methods. They go over the training dataset
    several times in cycles called epochs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络是迭代方法。它们在训练数据集上循环多次，称为周期。
- en: In the above example, we ran 100 epochs. But, it’s not clear how many epochs
    one should run to train a network. Too few epochs can lead to underfitting; too
    many iterations lead to overfitting.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，我们运行了100个周期。但不清楚应该运行多少个周期来训练一个网络。周期太少可能导致欠拟合；周期太多则可能导致过拟合。
- en: A way to handle this problem is by monitoring the performance of the neural
    network after each epoch. Each time the model improves performance, you save it
    before continuing the training process. Then, after the training is over, you
    get the best model that was saved.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这个问题的一种方法是监控每个周期后神经网络的性能。每当模型性能提高时，你会在继续训练过程之前保存它。然后，在训练结束后，你将得到保存的最佳模型。
- en: In keras, you can use callbacks to handle this process for you. A callback is
    a function that performs some action during the training process. You can check
    [keras documentation](https://keras.io/api/callbacks/) for a complete list of
    the available callbacks. Or how to learn to write your own!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在keras中，你可以使用回调函数来处理这个过程。回调函数是一个在训练过程中执行某些操作的函数。你可以查看[keras 文档](https://keras.io/api/callbacks/)以获取完整的回调函数列表，或者学习如何编写你自己的回调函数！
- en: 'The callback that is used to save the model during training is called ModelCheckPoint:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在训练过程中保存模型的回调函数称为ModelCheckPoint：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Another interesting callback you can use for training is [EarlyStopping](https://keras.io/api/callbacks/early_stopping/).
    It can be used to stop training when performance has stopped improving.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的回调函数是[EarlyStopping](https://keras.io/api/callbacks/early_stopping/)。它可以在性能停止改善时停止训练。
- en: Making predictions
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 做出预测
- en: After training, we can retrieve the best model and make predictions on the test
    set.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们可以检索最佳模型并对测试集进行预测。
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Key Take-Aways
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键要点
- en: Many forecasting problems involve many time series, for example in the retail
    domain. In such cases, global methods are often better to build a model;
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多预测问题涉及多个时间序列，例如零售领域。在这种情况下，全球方法通常更适合构建模型；
- en: Preprocessing the time series is important before training a neural network.
    Mean scaling helps bring all series into a common value range. Taking the log
    stabilizes the variance and avoids saturation areas;
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练神经网络之前，预处理时间序列很重要。均值缩放有助于将所有序列带入一个共同的值范围。取对数可以稳定方差，避免饱和区域；
- en: Use callbacks to drive the training process of neural networks.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用回调函数来驱动神经网络的训练过程。
- en: Thank you for reading, and see you in the next story!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读，下次故事见！
- en: References
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] [Hourly Energy Consumption time series](https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption?resource=download)
    (License: [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/))'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [每小时能源消耗时间序列](https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption?resource=download)（许可证：
    [CC0: 公共领域](https://creativecommons.org/publicdomain/zero/1.0/)）'
- en: '[2] Hewamalage, Hansika, Christoph Bergmeir, and Kasun Bandara. “Recurrent
    neural networks for time series forecasting: Current status and future directions.”
    *International Journal of Forecasting* 37.1 (2021): 388–427.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Hewamalage, Hansika, Christoph Bergmeir 和 Kasun Bandara. “用于时间序列预测的递归神经网络：现状与未来方向。”
    *国际预测学杂志* 37.1 (2021)：388–427。'
- en: '[3] Slawek Smyl, Jai Ranganathan, and Andrea Pasqua. M4 Forecasting Competition:
    Introducing a New Hybrid ES-RNN Model, 2018\. URL [https://eng.uber.com/](https://eng.uber.com/)
    m4-forecasting-competition/.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Slawek Smyl, Jai Ranganathan 和 Andrea Pasqua. M4 预测竞赛：介绍一种新的混合 ES-RNN 模型，2018\.
    URL [https://eng.uber.com/](https://eng.uber.com/) m4-forecasting-competition/。'
