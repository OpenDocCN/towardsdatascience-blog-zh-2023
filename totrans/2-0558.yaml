- en: Controllable Medical Image Generation with ControlNets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可控医学图像生成与ControlNets
- en: 原文：[https://towardsdatascience.com/controllable-medical-image-generation-with-controlnets-48ef33dde652](https://towardsdatascience.com/controllable-medical-image-generation-with-controlnets-48ef33dde652)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/controllable-medical-image-generation-with-controlnets-48ef33dde652](https://towardsdatascience.com/controllable-medical-image-generation-with-controlnets-48ef33dde652)
- en: Guide on using ControlNets to control the generation process of Latent Diffusion
    Models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用ControlNets控制潜在扩散模型生成过程的指南
- en: '[](https://medium.com/@walhugolp?source=post_page-----48ef33dde652--------------------------------)[![Walter
    Hugo Lopez Pinaya](../Images/0c132d0d1321790b0cea880800d231e0.png)](https://medium.com/@walhugolp?source=post_page-----48ef33dde652--------------------------------)[](https://towardsdatascience.com/?source=post_page-----48ef33dde652--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----48ef33dde652--------------------------------)
    [Walter Hugo Lopez Pinaya](https://medium.com/@walhugolp?source=post_page-----48ef33dde652--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@walhugolp?source=post_page-----48ef33dde652--------------------------------)[![Walter
    Hugo Lopez Pinaya](../Images/0c132d0d1321790b0cea880800d231e0.png)](https://medium.com/@walhugolp?source=post_page-----48ef33dde652--------------------------------)[](https://towardsdatascience.com/?source=post_page-----48ef33dde652--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----48ef33dde652--------------------------------)
    [Walter Hugo Lopez Pinaya](https://medium.com/@walhugolp?source=post_page-----48ef33dde652--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----48ef33dde652--------------------------------)
    ·9 min read·Jun 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----48ef33dde652--------------------------------)
    ·阅读时间9分钟·2023年6月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In this post, we will present a guide on training a **ControlNet** to empower
    users with precise control over the generation process of a **Latent Diffusion
    Model** **(like Stable Diffusion!)**. Our aim is to showcase the remarkable capabilities
    of these models in translating brain images across various contrasts. To achieve
    this, we will leverage the power of the recently introduced **open-source extension
    for MONAI,** [**MONAI Generative Models**](https://github.com/Project-MONAI/GenerativeModels)**!**
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将展示如何训练一个**ControlNet**，以使用户能够精准控制**潜在扩散模型**的生成过程（**如 Stable Diffusion!**）。我们的目标是展示这些模型在不同对比度下转换脑部图像的卓越能力。为实现这一目标，我们将利用最近推出的**MONAI开源扩展**，[**MONAI
    Generative Models**](https://github.com/Project-MONAI/GenerativeModels)**!**
- en: '![](../Images/802d4dc3c60ef6342ff0aa9478c0bad4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/802d4dc3c60ef6342ff0aa9478c0bad4.png)'
- en: Generating T1-weighted brain images (right) from FLAIR images (left) using ControlNet
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ControlNet从FLAIR图像（左）生成T1加权脑部图像（右）
- en: '***Our project code is available in this public repository*** [***https://github.com/Warvito/generative_brain_controlnet***](https://github.com/Warvito/generative_brain_controlnet)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '***我们的项目代码可以在这个公开的代码库中找到*** [***https://github.com/Warvito/generative_brain_controlnet***](https://github.com/Warvito/generative_brain_controlnet)'
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In recent years, text-to-image diffusion models have witnessed remarkable advancements,
    enabling the generation of highly realistic images based on open-domain text descriptions.
    These generated images have rich details, well-defined outlines, coherent structures,
    and meaningful contextual representation. However, despite the significant achievements
    of diffusion models, there remains a challenge in achieving precise control over
    the generative process. **Even with lengthy and intricate text descriptions, accurately
    capturing the user’s desired ideas can be a struggle.**
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，文本到图像扩散模型取得了显著进展，使得基于开放领域的文本描述生成高度真实的图像成为可能。这些生成的图像具有丰富的细节、清晰的轮廓、一致的结构和有意义的上下文表示。然而，尽管扩散模型取得了重要成就，但在生成过程中实现精确控制仍然是一个挑战。**即使有冗长而复杂的文本描述，准确捕捉用户想要的想法仍然可能是一项困难的任务。**
- en: The introduction of **ControlNets**, as proposed by Lvmin Zhang and Maneesh
    Agrawala in their groundbreaking paper “[*Adding Conditional Control to Text-to-Image
    Diffusion Models*](https://arxiv.org/abs/2302.05543)” (2023), has significantly
    enhanced the controllability and customization of diffusion models. These neural
    networks act as lightweight adapters, enabling precise control and customization
    while preserving the original generation capability of diffusion models. By fine-tuning
    these adapters while keeping the original diffusion model frozen, text-to-image
    models can be efficiently augmented for a diverse range of image-to-image applications.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**ControlNets**的引入，如Lvmin Zhang和Maneesh Agrawala在其开创性论文“[*将条件控制添加到文本到图像扩散模型*](https://arxiv.org/abs/2302.05543)”（2023）中提出的，显著增强了扩散模型的可控性和定制性。这些神经网络作为轻量级适配器，能够实现精确的控制和定制，同时保留扩散模型的原始生成能力。通过微调这些适配器，同时保持原始扩散模型的冻结，文本到图像模型可以高效地扩展到各种图像到图像应用中。'
- en: What sets ControlNet apart is its solution to the challenge of spatial consistency.
    In contrast to previous methods, ControlNet allows for explicit control over spatial,
    structural, and geometric aspects of generated structures, while retaining semantic
    control derived from textual captions. The original study introduced various models
    that enable conditional generation based on edges, pose, semantic masks, and depth
    maps, paving the way for exciting advancements in the computer vision field.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNet 的独特之处在于其解决了空间一致性的问题。与之前的方法不同，ControlNet 允许对生成结构的空间、结构和几何方面进行显式控制，同时保留了来自文本说明的语义控制。原始研究介绍了各种模型，这些模型基于边缘、姿势、语义掩码和深度图实现条件生成，为计算机视觉领域的激动人心的进展铺平了道路。
- en: 'In the field of medical imaging, numerous image-to-image applications hold
    significant importance. Among these applications, one notable task involves translating
    images between different domains, such as converting computational tomography
    (CT) scans to magnetic resonance imaging (MRI) or transforming images between
    distinct contrasts, for instance, from T1-weighted to T2-weighted MRI images.
    In this post, we will focus on a specific case: using **2D slices of brain images
    obtained from a FLAIR image to generate the correspondent T1-weighted image**.
    Our objective is to demonstrate how our new extension for MONAI ([MONAI Generative
    Models](https://github.com/Project-MONAI/GenerativeModels)) and ControlNets can
    be effectively utilized to train and evaluate generative models on medical data.
    By delving into this example, we aim to provide insights into the practical application
    of these technologies in the medical imaging domain.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在医学影像领域，许多图像到图像应用具有重要意义。在这些应用中，一个显著的任务是跨领域翻译图像，例如将计算机断层扫描（CT）转换为磁共振成像（MRI），或在不同对比度之间转换图像，例如从
    T1 加权图像到 T2 加权图像。在这篇文章中，我们将重点关注一个特定案例：使用从 FLAIR 图像获得的**2D 脑图像切片生成对应的 T1 加权图像**。我们的目标是展示如何有效地利用我们的
    MONAI 扩展（[MONAI Generative Models](https://github.com/Project-MONAI/GenerativeModels)）和
    ControlNets 来训练和评估医学数据上的生成模型。通过深入探讨这个例子，我们旨在提供这些技术在医学影像领域实际应用的见解。
- en: '![](../Images/c375ecdab449c61cfdde33f301c92d7f.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c375ecdab449c61cfdde33f301c92d7f.png)'
- en: FLAIR to T1w Translation
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: FLAIR 到 T1w 翻译
- en: Latent Diffusion Model Training
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在扩散模型训练
- en: '![](../Images/43544d640b73ae5bff77412a659a6e63.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43544d640b73ae5bff77412a659a6e63.png)'
- en: Latent Diffusion Model Architecture
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在扩散模型架构
- en: To generate T1-weighted (T1w) images from FLAIR images, **the initial step involves
    training a diffusion model capable of generating T1w images**. In our example,
    we utilize 2D slices extracted from brain MRI images sourced from the [UK Biobank
    dataset](https://www.ukbiobank.ac.uk/) (available under this [data agreement](https://www.ukbiobank.ac.uk/media/p3zffurf/biobank-mta.pdf)[).](https://www.ukbiobank.ac.uk/media/p3zffurf/biobank-mta.pdf).)
    After having the original 3D brains registered to an MNI space using your favourite
    method (for example, [ANTs](http://stnava.github.io/ANTs/) or [UniRes](https://github.com/brudfors/UniRes)),
    we extract five 2D slices from the central part of the brain. We chose this region
    since it presents various tissues, making it easier to evaluate the image translation
    we are performing. Using this [script](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/preprocessing/create_png_dataset.py),
    we ended with about **190,000 slices** with a spatial dimension of **224 × 160
    pixels**. Next, we divide our image into the train (~180,000 slices), validation
    (~5,000 slices), and test (~5,000 slices) sets using this script. With our dataset
    prepared, we can start to train our Latent Diffusion Model!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要从FLAIR图像生成T1加权（T1w）图像，**第一步涉及训练一个能够生成T1w图像的扩散模型**。在我们的示例中，我们使用从[UK Biobank数据集](https://www.ukbiobank.ac.uk/)（根据此[数据协议](https://www.ukbiobank.ac.uk/media/p3zffurf/biobank-mta.pdf)提供）提取的脑部MRI图像的2D切片。我们将原始的3D脑部图像通过你喜欢的方法（例如，[ANTs](http://stnava.github.io/ANTs/)或[UniRes](https://github.com/brudfors/UniRes)）注册到MNI空间。然后，我们从大脑的中心部分提取五个2D切片。我们选择这个区域是因为它展示了多种组织，使我们更容易评估所进行的图像转换。使用此[脚本](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/preprocessing/create_png_dataset.py)，我们最终获得了大约**190,000个切片**，空间维度为**224
    × 160像素**。接下来，我们使用该脚本将图像划分为训练集（约180,000个切片）、验证集（约5,000个切片）和测试集（约5,000个切片）。数据集准备好后，我们可以开始训练我们的潜在扩散模型！
- en: To optimize computational resources, the latent diffusion model employs an **encoder**
    to transform input image x into a lower-dimensional latent space z, which can
    then be reconstructed by a **decoder**. This approach enables training diffusion
    models even with limited computational capacity, while still preserving their
    original quality and flexibility. Similar to what we did in our previous post
    **(**[***Generating Medical Images with MONAI***](https://medium.com/towards-data-science/generating-medical-images-with-monai-e03310aa35e6)**)**,
    we use the [**Autoencoder with KL-regularization model**](https://github.com/Project-MONAI/GenerativeModels/blob/main/generative/networks/nets/autoencoderkl.py#L579)from
    MONAI Generative models to create our compression model. By using [this configuration](https://github.com/Warvito/generative_brain_controlnet/blob/main/configs/stage1/aekl_v0.yaml)
    and the L1 loss together with the KL-regularisation, [perceptual loss](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/losses/perceptual.py#L21)
    and [adversarial loss](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/losses/adversarial_loss.py#L29),
    **we created an autoencoder capable of encoding and decoding brain images with
    high fidelity** [(with this script)](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/training/train_aekl.py).
    The quality of the autoencoder’s reconstruction is crucial for the performance
    of the Latent Diffusion Model since it defines the ceiling of the quality of our
    generated images. If the autoencoder’s decoder produces blurry or low-quality
    images, our generative model will not be able to generate higher-quality ones.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化计算资源，潜在扩散模型使用**编码器**将输入图像x转换为低维潜在空间z，然后通过**解码器**重建。这种方法使得即使在计算能力有限的情况下也能训练扩散模型，同时保留其原始质量和灵活性。类似于我们在上一篇文章**（[***使用MONAI生成医学图像***](https://medium.com/towards-data-science/generating-medical-images-with-monai-e03310aa35e6)）**中做的，我们使用来自MONAI
    Generative模型的[**带有KL正则化的自编码器模型**](https://github.com/Project-MONAI/GenerativeModels/blob/main/generative/networks/nets/autoencoderkl.py#L579)来创建我们的压缩模型。通过使用[这个配置](https://github.com/Warvito/generative_brain_controlnet/blob/main/configs/stage1/aekl_v0.yaml)和L1损失以及KL正则化，[感知损失](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/losses/perceptual.py#L21)和[对抗性损失](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/losses/adversarial_loss.py#L29)，**我们创建了一个能够高保真编码和解码脑部图像的自编码器**
    [（使用这个脚本）](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/training/train_aekl.py)。自编码器的重建质量对于潜在扩散模型的性能至关重要，因为它定义了我们生成图像的质量上限。如果自编码器的解码器生成模糊或低质量的图像，我们的生成模型将无法生成更高质量的图像。
- en: '![](../Images/ed59d3f19e84b64518ba86a2b5b4d483.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed59d3f19e84b64518ba86a2b5b4d483.png)'
- en: Using this [script](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/testing/compute_msssim_reconstruction.py),
    we can quantify the fidelity of the autoencoder by using the **Multi-scale Structural
    Similarity Index Measurement (MS-SSIM)** between the original images and their
    reconstructions. In this example, we obtain a high performance with an MS-SSIM
    metric equal to 0.9876.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个[脚本](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/testing/compute_msssim_reconstruction.py)，我们可以通过使用原始图像与其重建之间的**多尺度结构相似性指数测量（MS-SSIM）**来量化自编码器的保真度。在这个例子中，我们获得了一个MS-SSIM指标为0.9876的高性能。
- en: After we train the autoencoder, we will train the [**diffusion model**](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/networks/nets/diffusion_model_unet.py#L1632)
    **on the latent space z**. The diffusion model it is a model that is able to generate
    images from a pure noise image by iteratively denoising it over a series of timesteps.
    It usually uses an **U-Net architecture** (that has an encoder-decoder format),
    where we have layers of the encoder skip connected with layers in the decoder
    part (via long **skip connections**), enabling feature reusability and stabilize
    training and convergence.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完自编码器后，我们将训练[**扩散模型**](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/networks/nets/diffusion_model_unet.py#L1632)
    **在潜在空间z上**。扩散模型是一种能够通过在一系列时间步上逐步去噪，从纯噪声图像中生成图像的模型。它通常使用**U-Net架构**（具有编码器-解码器格式），其中编码器的层通过长**跳跃连接**与解码器部分的层相连，从而实现特征重用，稳定训练和收敛。
- en: '![](../Images/1a9a40bef9b19b542c683181e50c34bb.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a9a40bef9b19b542c683181e50c34bb.png)'
- en: Diffusion Model’s U-Net architecture with skip connections between the encoder
    and decoder.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型的U-Net架构，具有编码器和解码器之间的跳跃连接。
- en: During the training, the Latent Diffusion Model learns a conditional noise prediction
    given these prompts. Again, we are using MONAI to create and train this network.
    In this [script](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/training/train_ldm.py),
    we are instantiating the model with this [configuration](https://github.com/Warvito/generative_brain_controlnet/blob/main/configs/ldm/ldm_v0.yaml),
    where the training and evaluation are performed in this [part of the code](https://github.com/Warvito/generative_brain_controlnet/blob/bb47f9c359e2d280b23bda84b5c14b65dd5b7af3/src/python/training/training_functions.py#L408).
    Since we are not too interested in the textual prompts in this tutorial, we are
    using the same one for all the image (a sentence saying [“*T1-weighted image of
    a brain.*”](https://github.com/Warvito/generative_brain_controlnet/blob/bb47f9c359e2d280b23bda84b5c14b65dd5b7af3/src/python/training/util.py#L38)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，潜在扩散模型根据这些提示学习条件噪声预测。我们再次使用MONAI来创建和训练这个网络。在这个[脚本](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/training/train_ldm.py)中，我们使用这个[配置](https://github.com/Warvito/generative_brain_controlnet/blob/main/configs/ldm/ldm_v0.yaml)来实例化模型，其中训练和评估在[代码的这一部分](https://github.com/Warvito/generative_brain_controlnet/blob/bb47f9c359e2d280b23bda84b5c14b65dd5b7af3/src/python/training/training_functions.py#L408)进行。由于在本教程中我们对文本提示不太感兴趣，我们对所有图像使用相同的提示（句子为[“*脑部T1加权图像*”](https://github.com/Warvito/generative_brain_controlnet/blob/bb47f9c359e2d280b23bda84b5c14b65dd5b7af3/src/python/training/util.py#L38)）。
- en: '![](../Images/7f66db9ca3d59810a16ef620b751e791.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f66db9ca3d59810a16ef620b751e791.png)'
- en: Synthetic brain images generated with our Latent Diffusion Model
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的潜在扩散模型生成的合成脑部图像
- en: Again, we can quantify the performance of our trained generative model, this
    time we evaluate the quality of the samples (using the **Fréchet inception distance
    (FID)**) and the diversity of the model (computing the MS-SSIM between all pair
    of samples of a group of 1,000 samples). Using these couple of scripts ([1](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/testing/compute_fid.py)
    and [2](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/testing/compute_msssim_sample.py)),
    we obtained an FID = 2.1986 and an MS-SSIM Diversity = 0.5368.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以量化我们训练过的生成模型的性能，这次我们评估了样本的质量（使用**Fréchet inception distance (FID)**）和模型的多样性（计算1000个样本组中所有样本对的MS-SSIM）。使用这两个脚本（[1](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/testing/compute_fid.py)
    和 [2](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/testing/compute_msssim_sample.py)），我们得到了FID
    = 2.1986和MS-SSIM多样性 = 0.5368。
- en: As you can see in the previous images and results, we now have a model that
    can generate high-resolution images with great quality. However, we do not have
    any spatial control over how the images look like. For this, we will use a ControlNet
    to guide the generation of our Latent Diffusion Model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在之前的图像和结果中看到的，我们现在拥有一个可以生成高分辨率、高质量图像的模型。然而，我们对图像的外观没有任何空间控制。为此，我们将使用ControlNet来指导我们潜在扩散模型的生成。
- en: ControlNet Training
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ControlNet 训练
- en: '![](../Images/e66262faa4ed439dc9139cb1bbe85de4.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e66262faa4ed439dc9139cb1bbe85de4.png)'
- en: ControlNet Architecture
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNet 架构
- en: 'The [ControlNet architecture](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/networks/nets/controlnet.py#L125)
    comprises two main components: a **trainable version** of the encoder from the
    U-Net model, including the middle blocks, and a **pre-trained “locked” version**
    of the diffusion model. Here, the locked copy preserves the generative capability,
    while the trainable copy is trained on specific image-to-image datasets to learn
    conditional control. These two components are interconnected using a **“zero convolution”
    layer** — a 1×1 convolution layer with initialized weights and biases set to zeros.
    The convolution weights gradually transition from zeros to optimized parameters,
    ensuring that during the initial training steps, the outputs of both the trainable
    and locked copies remain consistent with what they would be if the ControlNet
    were absent. In other words, when a ControlNet is applied to certain neural network
    blocks prior to any optimization, it does not introduce any additional influence
    or noise to the deep neural features.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[ControlNet架构](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/networks/nets/controlnet.py#L125)包括两个主要组件：一个**可训练的**来自U-Net模型的编码器版本，包括中间块，以及一个**预训练的“锁定”版本**的扩散模型。这里，锁定副本保留了生成能力，而可训练副本则在特定的图像对图像数据集上进行训练，以学习条件控制。这两个组件通过**“零卷积”层**互联——一个1×1的卷积层，其初始化权重和偏置被设置为零。卷积权重逐渐从零过渡到优化参数，确保在初始训练步骤中，可训练和锁定副本的输出与ControlNet不存在时的输出保持一致。换句话说，当ControlNet应用于某些神经网络块之前，没有引入任何额外的影响或噪声到深层神经特征中。'
- en: By integrating these two components, the ControlNet enables us to govern the
    behaviour of each level in the Diffusion Model’s U-Net.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过整合这两个组件，ControlNet使我们能够控制Diffusion Model的U-Net中每个级别的行为。
- en: In our example, we instantiate the ControlNet in [this script](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/training/train_controlnet.py),
    using the following equivalent snippet.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们在[这个脚本](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/training/train_controlnet.py)中实例化ControlNet，使用以下等效代码片段。
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Since we are using a Latent Diffusion Model, this requires ControlNets to convert
    image-based conditions to the same latent space to match the convolution size.
    For that, we use a [convolutional network](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/networks/nets/controlnet.py#L45)
    trained jointly with the full model. In our case, we have three downsampling levels
    (similar to the autoencoder KL) defined in *“conditioning_embedding_num_channels=[64,
    128, 128, 256]”*. Since our conditional image is a FLAIR image with one channel,
    we also need to specify its input number of channels in *“conditioning_embedding_in_channels=1”*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是潜在扩散模型，这要求ControlNets将基于图像的条件转换为相同的潜在空间以匹配卷积大小。为此，我们使用一个与完整模型共同训练的[卷积网络](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/networks/nets/controlnet.py#L45)。在我们的案例中，我们有三个下采样级别（类似于自动编码器KL），在*“conditioning_embedding_num_channels=[64,
    128, 128, 256]”*中定义。由于我们的条件图像是一个具有单通道的FLAIR图像，我们还需要在*“conditioning_embedding_in_channels=1”*中指定其输入通道数。
- en: After initialising our network, we train it similarly to a diffusion model.
    In the following snippet ([and in this part of the code](https://github.com/Warvito/generative_brain_controlnet/blob/bb47f9c359e2d280b23bda84b5c14b65dd5b7af3/src/python/training/training_functions.py#L623)),
    we can see that first we pass our conditional FLAIR image to the trainable network
    and obtain the outputs from its skip connections. Then, these values are inputted
    into the diffusion model when computing the predicted noise. Internally, the diffusion
    model sums the skip connection from the ControlNets with its own ones before feeding
    the decoder part ([code](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/networks/nets/diffusion_model_unet.py#L1901)).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化我们的网络后，我们像训练扩散模型一样训练它。在以下代码片段中（[以及代码的这一部分](https://github.com/Warvito/generative_brain_controlnet/blob/bb47f9c359e2d280b23bda84b5c14b65dd5b7af3/src/python/training/training_functions.py#L623)），我们可以看到，首先我们将条件FLAIR图像传递到可训练的网络中，并从其跳过连接中获得输出。然后，这些值在计算预测噪声时输入到扩散模型中。在内部，扩散模型将ControlNets的跳过连接与自身的跳过连接相加，然后将其输入到解码器部分中（[代码](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/networks/nets/diffusion_model_unet.py#L1901)）。
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ControlNet Sampling and Evaluation
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ControlNet采样与评估
- en: After training our models, we can sample and evaluate them. [Here](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/testing/sample_flair_to_t1w.py),
    we are using the FLAIR images from the test set to generate conditioned T1w images.
    Similar to our training, the the sampling process is very close to the one used
    with the diffusion model, the only difference is that we pass the condition image
    to the trained ControlNet, and we use its output to feed the diffusion model in
    each sampling timesteps. As we can observe from the figure below, **our generated
    images follow with high spatial fidelity the original conditioning**, with the
    cortex gyri following similar shapes and the images preserving the boundary between
    different tissues.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型后，我们可以对其进行采样和评估。[这里](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/testing/sample_flair_to_t1w.py)，我们使用测试集中的
    FLAIR 图像生成条件 T1w 图像。类似于我们的训练，采样过程与扩散模型使用的非常接近，唯一的区别是我们将条件图像传递给训练好的 ControlNet，并使用其输出在每个采样时间步中馈送给扩散模型。正如我们从下图中观察到的那样，**我们生成的图像在空间上高度忠实于原始条件**，大脑皮层回旋遵循类似的形状，并且图像保持了不同组织之间的边界。
- en: '![](../Images/b2ce5162a27dae654d7bd4739ffca9f7.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2ce5162a27dae654d7bd4739ffca9f7.png)'
- en: Examples from the test set of the original FLAIR image used as input to the
    ControlNet (left), the generated T1-weighted image (middle), and the original
    T1-weighted image, a.k.a. the expected output (right)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集中用于输入到 ControlNet 的原始 FLAIR 图像（左）、生成的 T1 加权图像（中）和原始 T1 加权图像，即期望输出（右）的示例。
- en: After we sample the images of our models, we can quantify the performance of
    our ControlNet when translating the images between different contrasts. Since
    we have the expected T1w images from the test set, we can also check their differences
    and compute the distance between the real and synthetic images using the **mean
    absolute error (MAE)**, **peak** **signal-to-noise ratio (PSNR)**, and **MS-SSIM**.
    In our test set, we got a PSNR= 26.2458+-1.0092, MAE=0.02632+-0.0036 and MSSIM=0.9526+-0.0111
    when executing this [script](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/testing/compute_controlnet_performance.py).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对模型的图像进行采样后，我们可以量化我们 ControlNet 在不同对比度之间翻译图像的性能。由于我们拥有测试集中的期望 T1w 图像，我们还可以检查它们之间的差异，并使用**均方绝对误差
    (MAE)**、**峰值** **信噪比 (PSNR)** 和**MS-SSIM**计算真实图像与合成图像之间的距离。在我们的测试集中，当执行这个 [脚本](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/testing/compute_controlnet_performance.py)
    时，我们得到了 PSNR=26.2458+-1.0092，MAE=0.02632+-0.0036 和 MSSIM=0.9526+-0.0111。
- en: And that is it! ControlNet offers incredible control over our diffusion models
    and recent approaches have extended its method to combine different trained ControlNets
    ([Multi-ControlNet](https://github.com/Mikubill/sd-webui-controlnet#multi-controlnet)),
    work with different types of conditioning in the same model ([T2I adapters](https://arxiv.org/abs/2302.08453)),
    and even condition the model on styles (using methods like ControlNet 1.1 — [reference
    only](https://github.com/Mikubill/sd-webui-controlnet/discussions/1236)). If these
    methods sound interesting, do not forget to follow me for more guides like this
    one! 😁
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些！ControlNet 提供了对我们扩散模型的不可思议的控制，近期的方法已经扩展了其方法，以结合不同训练的 ControlNets ([Multi-ControlNet](https://github.com/Mikubill/sd-webui-controlnet#multi-controlnet))，在同一模型中处理不同类型的条件
    ([T2I adapters](https://arxiv.org/abs/2302.08453))，甚至基于样式调整模型（使用像 ControlNet 1.1
    这样的技术 — [仅供参考](https://github.com/Mikubill/sd-webui-controlnet/discussions/1236)）。如果这些方法听起来很有趣，不要忘记关注我，以获取更多类似的指南！😁
- en: For more MONAI Generative Model’s tutorials and to learn more about our features,
    check out our [Tutorial page](https://github.com/Project-MONAI/GenerativeModels/tree/main/tutorials)!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多 MONAI Generative Model 的教程以及我们的功能，请查看我们的 [教程页面](https://github.com/Project-MONAI/GenerativeModels/tree/main/tutorials)！
- en: '*Note: All images unless otherwise noted are by the author*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：除非另有说明，所有图像均由作者提供*'
