- en: Fine-tune Better Chat Models with Distilled Identity Preference Optimization
    (IPO)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调更好的聊天模型，采用蒸馏身份偏好优化（IPO）
- en: 原文：[https://towardsdatascience.com/fine-tune-better-chat-models-with-distilled-identity-preference-optimization-ipo-99cddc819a48](https://towardsdatascience.com/fine-tune-better-chat-models-with-distilled-identity-preference-optimization-ipo-99cddc819a48)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fine-tune-better-chat-models-with-distilled-identity-preference-optimization-ipo-99cddc819a48](https://towardsdatascience.com/fine-tune-better-chat-models-with-distilled-identity-preference-optimization-ipo-99cddc819a48)
- en: Mistral 7B aligned with IPO
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Mistral 7B与IPO对齐
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----99cddc819a48--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----99cddc819a48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----99cddc819a48--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----99cddc819a48--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----99cddc819a48--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----99cddc819a48--------------------------------)[![本杰明·玛丽](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----99cddc819a48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----99cddc819a48--------------------------------)[![数据科学的方向](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----99cddc819a48--------------------------------)
    [本杰明·玛丽](https://medium.com/@bnjmn_marie?source=post_page-----99cddc819a48--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----99cddc819a48--------------------------------)
    ·6 min read·Dec 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----99cddc819a48--------------------------------)
    ·阅读时间6分钟·2023年12月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e762c4828a29a687b5f127b736a29db8.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e762c4828a29a687b5f127b736a29db8.png)'
- en: Photo by [Rishabh Dharmani](https://unsplash.com/@rishabhdharmani?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Rishabh Dharmani](https://unsplash.com/@rishabhdharmani?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: To become chat models, pre-trained large language models (LLMs) are fine-tuned
    on large datasets of instructions/questions paired with expected answers. While
    this simple fine-tuning yields convincing chat models, their answers may still
    be incoherent, biased, unethical, and unsafe from a human perspective. This is
    why we usually perform an additional training step to better align the LLM with
    humans.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 要成为聊天模型，预训练的大型语言模型（LLMs）会在配对的指令/问题和预期答案的大型数据集上进行微调。虽然这种简单的微调可以产生令人信服的聊天模型，但它们的回答可能仍然不连贯、有偏见、不道德和不安全。这就是为什么我们通常会进行额外的训练步骤，以更好地使LLM与人类对齐。
- en: This alignment can be done using reinforcement learning with human feedback
    (RLHF). As demonstrated by OpenAI and the success of ChatGPT, RLHF can yield state-of-the-art
    chat models. However, RLHF is expensive to run. It requires large datasets annotated
    by humans and the training of several auxiliary models (reference and reward models).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对齐可以通过带有人类反馈的强化学习（RLHF）来完成。正如OpenAI和ChatGPT的成功所示，RLHF可以产生最先进的聊天模型。然而，RLHF的运行成本很高。它需要大量由人类标注的数据集和训练多个辅助模型（参考和奖励模型）。
- en: As a simpler and cheaper alternative to RLHF, [direct preference optimization
    (DPO)](https://kaitchup.substack.com/p/fine-tune-your-own-instruct-version) has
    recently been applied with success to align LLMs, such as Hugging Face’s [Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)
    and Intel’s [Neural Chat](https://huggingface.co/Intel/neural-chat-7b-v3).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 作为RLHF的更简单和更便宜的替代方案，[直接偏好优化（DPO）](https://kaitchup.substack.com/p/fine-tune-your-own-instruct-version)最近已成功应用于对齐LLMs，如Hugging
    Face的[Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)和Intel的[神经聊天](https://huggingface.co/Intel/neural-chat-7b-v3)。
- en: In this article, based on a work by Google DeepMind, we will see that, while
    RLHF and DPO perform well at aligning LLMs, they are far from optimal given the
    datasets used for training. DeepMind also demonstrates why DPO is prone to overfitting.
    I’ll explain, in plain English, how the alternative proposed by DeepMind, the
    identity policy optimization (IPO) objective, is simpler and better designed to
    learn from the training data than RLHF and DPO.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，基于Google DeepMind的研究，我们将看到尽管RLHF和DPO在对齐LLMs方面表现良好，但由于训练中使用的数据集，它们远未达到最终效果。DeepMind还展示了为什么DPO容易过拟合。我将用通俗易懂的语言解释DeepMind提出的替代方案，即身份政策优化（IPO）目标，如何比RLHF和DPO更简单且更好地从训练数据中学习。
- en: In the following sections, I show how to use IPO following a training recipe
    close to the one used by Hugging Face to train the Zephyr models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我将展示如何使用IPO，按照与Hugging Face训练Zephyr模型类似的训练配方。
- en: 'I have also implemented a notebook demonstrating IPO training for Mistral 7B.
    You can find it here:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我还实现了一个演示Mistral 7B IPO训练的笔记本。你可以在这里找到：
- en: '[Get the notebook (#31)](https://kaitchup.substack.com/p/notebooks)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[获取笔记本（#31）](https://kaitchup.substack.com/p/notebooks)'
- en: 'The paper by DeepMind describing IPO is on arXiv:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 描述IPO的DeepMind论文在arXiv上：
- en: '[A General Theoretical Paradigm to Understand Learning from Human Preferences](https://arxiv.org/abs/2310.12036)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[一个理解来自人类偏好的学习的普遍理论范式](https://arxiv.org/abs/2310.12036)'
- en: 'ΨPO: Generalization of Preference Optimization'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ΨPO：偏好优化的泛化
- en: 'RLHF and DPO are trained on similar datasets: prompts paired with at least
    two possible answers rated by humans (or LLMs). The answers are paired so that,
    in a pair, one answer is rated better than the other. For its alignment, we want
    the LLM to learn which answer is preferred by humans. This is the “preference
    optimization”. RLHF learns it with reinforcement learning while DPO learns it
    with a simple classifier.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF和DPO在类似的数据集上进行训练：这些数据集包括至少两个由人类（或LLMs）评分的可能答案。答案被配对，以便在一个配对中，一个答案的评分优于另一个。对于其对齐，我们希望LLM学习到哪个答案更受人类欢迎。这就是“偏好优化”。RLHF通过强化学习来学习这一点，而DPO通过简单的分类器来学习。
- en: Moreover, to avoid overfitting, RLHF and DPO training must be regularized. This
    regularization is achieved with the KL-regularization controlling that the LLM
    gets better at each training step without drifting too far away from the original
    (also called reference) non-aligned model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了避免过拟合，RLHF和DPO训练必须进行正则化。这种正则化是通过KL正则化实现的，控制LLM在每一步训练中变得更好，同时不偏离原始（也称为参考）未对齐模型太远。
- en: 'I explained in more detail how the KL regularization works for RLHF in this
    article:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这篇文章中更详细地解释了KL正则化在RLHF中的工作原理：
- en: '[](https://kaitchup.substack.com/p/train-instruct-llms-on-your-gpu-with-6a5?source=post_page-----99cddc819a48--------------------------------)
    [## Train Instruct LLMs On Your GPU with DeepSpeed Chat - Step #3: Reinforcement
    Learning with Human…'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaitchup.substack.com/p/train-instruct-llms-on-your-gpu-with-6a5?source=post_page-----99cddc819a48--------------------------------)
    [## 在您的GPU上使用DeepSpeed Chat训练Instruct LLMs - 第3步：与人类的强化学习…'
- en: The efficiency of DeepSpeed Chat in action
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DeepSpeed Chat 实际应用的效率
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/train-instruct-llms-on-your-gpu-with-6a5?source=post_page-----99cddc819a48--------------------------------)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: kaitchup.substack.com](https://kaitchup.substack.com/p/train-instruct-llms-on-your-gpu-with-6a5?source=post_page-----99cddc819a48--------------------------------)
- en: Without this regularization, the LLM would find a way to generate answers minimizing
    the training loss while being meaningless, i.e., it would overfit the training
    data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有这种正则化，LLM会找到一种方法来生成回答，从而最小化训练损失，但这些回答可能没有意义，即它会过拟合训练数据。
- en: 'DeepMind demonstrated that RLHF and DPO are special cases of a more general
    learning objective: the Ψ preference optimization (ΨPO). *Note: I don’t know how
    DeepMind pronounces “ΨPO”, but I would guess this is psi-P-O.*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind 证明了RLHF和DPO是更一般学习目标的特殊情况：Ψ偏好优化（ΨPO）。*注意：我不知道DeepMind如何发音“ΨPO”，但我猜这可能是psi-P-O。*
- en: One other weakness highlighted by DeepMind, but also well-known by LLM practitioners,
    is that even with the KL-regularization DPO is prone to overfitting. For instance,
    Hugging Face in [their technical report on training Zephyr](https://arxiv.org/abs/2310.16944)
    found that DPO overfits the training data with perfect accuracy after only one
    training epoch.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind指出的另一个弱点，但LLM从业者也非常熟悉的是，即使有KL正则化，DPO也容易过拟合。例如，Hugging Face在[他们关于训练Zephyr的技术报告](https://arxiv.org/abs/2310.16944)中发现，DPO在仅一个训练周期后就能以完美的准确度过拟合训练数据。
- en: A large part of the DeepMind paper is dedicated to demonstrating why this overfitting
    happens. This is also, in my opinion, the most complicated part of the paper.
    Let’s try to summarize it in plain English.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind论文的大部分内容致力于证明为什么会发生这种过拟合。我认为这是论文中最复杂的部分。让我们试着用简单的英语来总结一下。
- en: 'RLHF and DPO are trained on pairs of answers with preference annotations. Usually,
    in the dataset created for this training, the same answer, or a close one, will
    always be better than the other one. In this situation, we can say that it is
    deterministic: the same input will always yield the same output. It seems intuitive
    but, in practice, humans disagree: Given a pair of answers A and B, one human
    may find A better than B while another human may find B better than A.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF和DPO在带有偏好注释的答案对上进行训练。通常，在为这种训练创建的数据集中，相同的答案或一个接近的答案总是比另一个更好。在这种情况下，我们可以说它是确定性的：相同的输入总会产生相同的输出。这似乎很直观，但实际上，人们的意见不一致：给定一对答案A和B，一个人可能认为A比B好，而另一个人可能认为B比A好。
- en: This would happen very often for instance if we want to compare two LLMs performing
    closely, i.e., generating only slightly different answers. RLHF and DPO are designed
    to learn from this kind of non-deterministic data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想比较两个表现相近的LLM，即生成的答案仅略有不同，这种情况会经常发生。RLHF和DPO设计用来从这种非确定性数据中学习。
- en: RLHF and DPO assign Elo scores for the rated answers, in other words, they convert
    pairwise preferences to point-wise estimates. They don’t directly predict which
    answer is better but instead predict a (complicated) score, in the form of [logit](https://en.wikipedia.org/wiki/Logit)-preferences,
    for each answer.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF和DPO为评分的答案分配Elo分数，换句话说，它们将成对偏好转换为逐点估计。它们不会直接预测哪个答案更好，而是预测每个答案的（复杂的）分数，以[logit](https://en.wikipedia.org/wiki/Logit)偏好的形式。
- en: We could say that it makes the learning objective unnecessarily more complicated.
    Indeed, we don’t need point-wise estimates when the training data are deterministic
    and use only a very small number of answers per prompt.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说这使得学习目标不必要地复杂化。实际上，当训练数据是确定性的，并且每个提示只使用非常少量的回答时，我们不需要逐点估计。
- en: 'That is why DeepMind proposes another simpler learning objective: Identity
    Preference Optimization.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么DeepMind提出了另一个更简单的学习目标：身份偏好优化。
- en: Identity Preference Optimization (IPO)
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 身份偏好优化（IPO）
- en: We saw that DPO is prone to overfitting. Nonetheless, DPO has the advantage
    of being a simple learning objective as it doesn’t need a reward model and doesn’t
    do reinforcement learning.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到DPO容易出现过拟合。然而，DPO的优势在于它是一个简单的学习目标，因为它不需要奖励模型，也不进行强化学习。
- en: With the identity preference optimization (IPO) learning objective, DeepMind
    proposes an alternative, based on DPO, that doesn’t need a reward model while
    better exploiting the KL-regularization.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过身份偏好优化（IPO）学习目标，DeepMind提出了一种基于DPO的替代方案，它不需要奖励模型，同时更好地利用KL-正则化。
- en: Like DPO, IPO is a special case of the ΨPO objective. The main difference between
    IPO and DPO is that IPO directly learns pairwise preferences instead of the logit-preferences,
    i.e., IPO doesn’t use Elo-scores.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 像DPO一样，IPO是ΨPO目标的一个特例。IPO和DPO的主要区别在于IPO直接学习成对的偏好，而不是logit-偏好，即IPO不使用Elo分数。
- en: The complete mathematical proof and formulation for IPO are provided in the
    paper. They also compare DPO and IPO with different coefficients for the KL-regularization
    (see [Figure 1 of the paper](https://arxiv.org/pdf/2310.12036.pdf)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中提供了IPO的完整数学证明和公式。他们还比较了DPO和IPO在KL-正则化下不同系数的表现（见[论文的图1](https://arxiv.org/pdf/2310.12036.pdf)）。
- en: They observed that DPO is almost insensitive to the increase in the coefficient
    of the KL-regularization. On the other hand, IPO is much more impacted by the
    increasing coefficient. Interestingly, on the IPO curves of Figure 1, we can see
    that IPO remains able to distinguish the outputs y1, y2, and y3\. It ranks them
    as y1 > y2 > y3 while DPO considers y1 perfect (nearly 1.0), while y2 and y3 would
    be equally bad. DPO overfits, IPO doesn’t.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 他们观察到DPO对KL-正则化系数的增加几乎不敏感。另一方面，IPO受系数增加的影响要大得多。有趣的是，在图1的IPO曲线中，我们可以看到IPO仍能区分输出y1、y2和y3。它将它们排名为y1
    > y2 > y3，而DPO认为y1是完美的（接近1.0），而y2和y3则被认为是同样糟糕的。DPO出现过拟合，IPO则没有。
- en: Distilled IPO for Mistral 7B
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为Mistral 7B蒸馏IPO
- en: IPO is already available in Hugging Face’s TRL to align LLMs. Since IPO is based
    on DPO, they have directly added it to the [DPOTrainer](https://huggingface.co/docs/trl/main/en/dpo_trainer).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: IPO已经在Hugging Face的TRL中提供，用于对齐LLM。由于IPO是基于DPO的，他们直接将其添加到了[DPOTrainer](https://huggingface.co/docs/trl/main/en/dpo_trainer)中。
- en: 'I aligned Mistral 7B using (almost) the same recipe, inspired by the Zephyr
    models, that I used in this article:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我用（几乎）相同的方法对Mistral 7B进行了对齐，这些方法受到Zephyr模型的启发，详细内容可以参见这篇文章：
- en: '[](https://kaitchup.substack.com/p/a-cheap-zephyr-7b-beta-distilled?source=post_page-----99cddc819a48--------------------------------)
    [## A Cheap Zephyr 7B Beta: Distilled DPO on Consumer Hardware'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaitchup.substack.com/p/a-cheap-zephyr-7b-beta-distilled?source=post_page-----99cddc819a48--------------------------------)
    [## 便宜的Zephyr 7B Beta：在消费级硬件上提炼的DPO'
- en: The recipe for training a Zephyr-like model without using A100 GPUs
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在不使用A100 GPU的情况下训练类似Zephyr的模型的配方
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/a-cheap-zephyr-7b-beta-distilled?source=post_page-----99cddc819a48--------------------------------)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[kaitchup.substack.com](https://kaitchup.substack.com/p/a-cheap-zephyr-7b-beta-distilled?source=post_page-----99cddc819a48--------------------------------)'
- en: The main difference is the use of IPO instead of DPO. The training data, generated
    and rated by other LLMs, remains the same. This is a distilled IPO.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的区别在于使用IPO而非DPO。训练数据由其他LLM生成并评估，保持不变。这是一个提炼过的IPO。
- en: 'I have also made small modifications in the LoRA hyperparameters to use the
    ones proposed in the [Hugging Face alignment handbook](https://github.com/huggingface/alignment-handbook/blob/main/recipes/zephyr-7b-beta/sft/config_lora.yaml):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我还对LoRA超参数做了小的修改，使用了[Hugging Face对齐手册](https://github.com/huggingface/alignment-handbook/blob/main/recipes/zephyr-7b-beta/sft/config_lora.yaml)中提出的参数：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, the DPOTrainer can simply be configured to use IPO like this:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，DPOTrainer可以简单地配置为使用IPO，如下所示：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: “loss_type” is simply set to ‘ipo’. The “beta” hyperparameter is the coefficient
    (tau) of the regularization.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: “loss_type”被简单地设置为‘ipo’。 “beta”超参数是正则化的系数（tau）。
- en: It is also worth noticing that, with DPO, Hugging Face fixed a learning rate
    of 5e-7 which is very low. DPO overfits very quickly or doesn’t converge with
    a higher learning rate. On the other hand with IPO, I observed that a higher learning
    rate can work but that it should be tuned together with the beta (the coefficient
    of the regularization) to prevent overfitting.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得注意的是，使用DPO时，Hugging Face固定了一个非常低的学习率5e-7。DPO很容易过拟合或者在较高学习率下无法收敛。另一方面，使用IPO时，我观察到较高的学习率是可行的，但它应该与beta（正则化的系数）一起调整，以防止过拟合。
- en: Conclusion
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: As demonstrated by DeepMind, IPO is theoretically better than RLHF and DPO given
    the training datasets used to align LLMs. IPO directly learns pairwise preferences
    and better exploits the KL-regularization to avoid overfitting.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 正如DeepMind所展示的那样，IPO在理论上比RLHF和DPO更好，特别是对于用于对齐LLM的训练数据集。IPO直接学习成对的偏好，并更好地利用KL正则化以避免过拟合。
- en: Since IPO preserves the main advantages of DPO while correcting its defects,
    it should become the new standard learning objective for LLM alignment.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于IPO保留了DPO的主要优点，同时修正了其缺陷，它应该成为LLM对齐的新标准学习目标。
