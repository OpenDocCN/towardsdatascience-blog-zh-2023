- en: 'Falcon: The Pinnacle of Open-Source LLMs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 鹰：开源大型语言模型的巅峰
- en: 原文：[https://towardsdatascience.com/falcon-the-pinnacle-of-open-source-llms-600de69c333c](https://towardsdatascience.com/falcon-the-pinnacle-of-open-source-llms-600de69c333c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/falcon-the-pinnacle-of-open-source-llms-600de69c333c](https://towardsdatascience.com/falcon-the-pinnacle-of-open-source-llms-600de69c333c)
- en: The gap between open-source and proprietary LLMs continues to shrink…
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开源 LLMs 与专有 LLMs 之间的差距持续缩小…
- en: '[](https://wolfecameron.medium.com/?source=post_page-----600de69c333c--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----600de69c333c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----600de69c333c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----600de69c333c--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----600de69c333c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----600de69c333c--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----600de69c333c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----600de69c333c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----600de69c333c--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----600de69c333c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----600de69c333c--------------------------------)
    ·14 min read·Oct 24, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----600de69c333c--------------------------------)
    ·阅读时间14分钟·2023年10月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4a5506c6e87e01bb8145fcb5b5d29b13.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a5506c6e87e01bb8145fcb5b5d29b13.png)'
- en: (Photo by [Alan Mersom](https://unsplash.com/@merse?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/aerial-photography-of-bird-NzkjR1pw0Lc?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （照片由 [Alan Mersom](https://unsplash.com/@merse?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    提供，来源于 [Unsplash](https://unsplash.com/photos/aerial-photography-of-bird-NzkjR1pw0Lc?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)）
- en: 'Recent research in open-source large language models (LLMs) has mostly focused
    upon two areas: imitation learning and pre-training open-source base models. Though
    both approaches are viable, the creation of high-quality, open-source base models
    is especially enticing, as these models can be further fine-tuned (at a lower
    cost) and used in a variety of different downstream applications. Initial attempts
    at creating these models failed. Although later models (e.g., LLaMA and MPT-7B)
    perform much better, these models have struggled to match the quality of their
    proprietary counterparts (e.g., GPT-3.5 or GPT-4) until recently.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，开源大型语言模型（LLMs）的研究主要集中在两个方面：模仿学习和预训练开源基础模型。虽然这两种方法都是可行的，但创建高质量的开源基础模型尤其令人振奋，因为这些模型可以进一步微调（成本较低）并用于各种不同的下游应用。最初尝试创建这些模型的结果并不理想。尽管后来的模型（例如，LLaMA
    和 MPT-7B）表现更佳，但这些模型在质量上仍难以匹敌其专有对手（例如，GPT-3.5 或 GPT-4），直到最近才有所改进。
- en: With the release of the Falcon-7B and Falcon-40B LLMs [1], we see — *for the
    first time* — open-source base LLMs that begin to rival the quality of the most
    popular paid models. Trained over a massive textual corpus obtained via a novel
    data pipeline, these models achieve (by a decent margin) new state-of-the-art
    performance among open-source LLMs and are free to use in commercial applications.
    To make things better, the Falcon models adopt several modifications to their
    underlying transformer architecture that significantly accelerate inference and
    can even improve the efficiency of pre-training.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Falcon-7B 和 Falcon-40B LLMs [1] 的发布，我们*第一次*看到开始与最受欢迎的付费模型质量相媲美的开源基础 LLMs。这些模型在通过新颖的数据管道获得的大规模文本语料库上进行训练，取得了（以相当大的优势）新的开源
    LLMs 的最先进性能，并且可以自由用于商业应用。更棒的是，Falcon 模型对其底层的 Transformer 架构进行了若干修改，这些修改显著加快了推理速度，甚至可以提高预训练的效率。
- en: '![](../Images/c9e99a0dc680837e42c6f23d65dcf34e.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9e99a0dc680837e42c6f23d65dcf34e.png)'
- en: (from [1, 2])
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1, 2]）
- en: '**The big picture.** The process of creating an LLM is comprised of several
    steps; see below. The first step of this process (i.e., obtaining a pre-trained
    base model) is widely known to be the most expensive, both in terms of money and
    time.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**大局观。** 创建一个 LLM 的过程包含几个步骤；见下文。这个过程的第一步（即，获得预训练基础模型）被广泛认为是最昂贵的，无论是在金钱还是时间上。'
- en: '![](../Images/1827c9fb6ce35c89268c94f997d784e3.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1827c9fb6ce35c89268c94f997d784e3.png)'
- en: Multi-step process for creating and refining an LLM (from [16, 17])
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 创建和完善 LLM 的多步骤过程（来自 [16, 17]）
- en: Such models were previously kept behind proprietary APIs, but advancements in
    open-source LLMs have made high-performing base LLMs more publicly available.
    Falcon is another model in this category, and it achieves unprecedented levels
    of performance in comparison to other open-source alternatives.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型之前被保留在专有 API 后面，但开源 LLM 的进步使高性能基础 LLM 更加公开。Falcon 是这个类别中的另一个模型，相比其他开源替代品，它达到了前所未有的性能水平。
- en: Using Web Data for LLM Pre-Training
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Web 数据进行 LLM 预训练
- en: '![](../Images/fdc2d6a1345e149a2f4922d4b913e892.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fdc2d6a1345e149a2f4922d4b913e892.png)'
- en: (from [3])
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [3]）
- en: 'When we look at the major differences between pre-training and fine-tuning
    (i.e., SFT and RLHF) a language model, we will notice that pre-training is much
    harder (and more expensive) compared to fine-tuning; see above. There are two
    fundamental properties of pre-training that make it so difficult:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们探讨预训练与微调（即，SFT 和 RLHF）语言模型之间的主要差异时，会发现预训练比微调要困难得多（且更昂贵）；见上文。预训练有两个根本性特征使其如此困难：
- en: The model is being trained from scratch, so it requires a greater number of
    training iterations.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型是从头开始训练的，因此需要更多的训练迭代次数。
- en: The pre-training dataset must be large and diverse (i.e., provide as much “coverage”
    as possible) so the resulting LLM has a sizable knowledge base.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预训练数据集必须大且多样化（即，提供尽可能多的“覆盖”），以便生成的 LLM 拥有较大的知识基础。
- en: Put simply, pre-training datasets are massive (e.g., Chinchilla [6] was trained
    on 1.4 trillion textual tokens), which means that the extent of pre-training is
    unavoidably large. *We must run many training iterations to traverse all of this
    data!*
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，预训练数据集非常庞大（例如，Chinchilla [6] 在 1.4 万亿文本令牌上训练），这意味着预训练的范围不可避免地很大。*我们必须进行大量的训练迭代才能遍历所有这些数据！*
- en: '**Creating a pre-training dataset.** However, it’s not just the size of the
    dataset that makes pre-training such a massive undertaking. Just curating the
    dataset is an intricate process that involves both retrieving the data and executing
    an entire pipeline of filtering (e.g., based on data quality, contaminated data,
    PII, and more) and de-duplication steps. A variety of different processing steps
    have been proposed and explored for curating LLM pre-training data; see [here](https://wandb.ai/wandb_gen/llm-data-processing/reports/Processing-Data-for-Large-Language-Models--VmlldzozMDg4MTM2).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**创建预训练数据集。** 然而，数据集的大小不仅是使预训练成为如此庞大任务的原因。仅仅策划数据集就是一个复杂的过程，涉及到检索数据和执行整个过滤（例如，基于数据质量、污染数据、PII
    等）和去重步骤的管道。已经提出并探索了各种不同的处理步骤来策划 LLM 预训练数据；见 [这里](https://wandb.ai/wandb_gen/llm-data-processing/reports/Processing-Data-for-Large-Language-Models--VmlldzozMDg4MTM2)。'
- en: Although one might initially think that such processing steps could be simplified
    or avoided, research on LLMs has showed us time and time again that the quality
    of data on which these models are trained is incredibly important. For evidence
    of this, we can see, for example, LIMA [7] or Galactica [8], both of which are
    trained over smaller (but higher-quality) text corpora and found to match or exceed
    the performance of identical models trained over noisier datasets at a larger
    scale.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最初可能认为这些处理步骤可以简化或避免，但 LLM 研究一次又一次地向我们展示了模型训练数据的质量是极其重要的。例如，我们可以看到 LIMA [7]
    或 Galactica [8]，这两个模型都在较小（但高质量）的文本语料库上训练，结果与在更大规模的噪声数据集上训练的相同模型的性能相匹配或超越。
- en: '![](../Images/8ee5c6fb504267cab8295ea5e9d225a3.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ee5c6fb504267cab8295ea5e9d225a3.png)'
- en: (from [4, 5])
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [4, 5]）
- en: '**Current LLMs.** Due to the impact of data quality on model quality, pre-training
    data used for most LLMs are obtained from highly curated sources, such as filtered
    textual content, books, code, or technical reports; see above [4, 5]. In fact,
    numerous public sources of curated pre-training data are readily available online
    (e.g., [the Pile](https://huggingface.co/datasets/EleutherAI/pile) or [C4](https://huggingface.co/datasets/c4))
    and have already been used extensively by existing models.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**当前的 LLM。** 由于数据质量对模型质量的影响，大多数 LLM 使用的预训练数据来自高度策划的来源，例如筛选过的文本内容、书籍、代码或技术报告；见上文
    [4, 5]。实际上，许多策划过的预训练数据公共来源在线上随处可见（例如，[the Pile](https://huggingface.co/datasets/EleutherAI/pile)
    或 [C4](https://huggingface.co/datasets/c4)），并且已经被现有模型广泛使用。'
- en: “Curation is believed to be necessary to produce performant models … However,
    as models requiring pretraining on trillions of tokens are considered, it is unclear
    whether curation is scalable and if we will run out of unique high-quality data
    soon.” *— from [2]*
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “策展被认为是产生高性能模型所必需的……然而，随着模型需要在万亿个标记上进行预训练，尚不清楚策展是否具有可扩展性，以及我们是否会很快耗尽独特的高质量数据。”
    *— 来源于 [2]*
- en: However, it is questionable whether the use of curated data sources is actually
    scalable — *fine-grained filtering and curation becomes more difficult with the
    scale of the pre-training dataset*. As such, extensive curation may need to become
    less necessary for larger models and datasets, especially given that locating
    sufficient amounts of data for LLM pre-training is becoming [increasingly difficult](https://epochai.org/blog/will-we-run-out-of-ml-data-evidence-from-projecting-dataset).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用策展数据源是否真的具有可扩展性仍然值得怀疑——*随着预训练数据集规模的扩大，细粒度的过滤和策展变得越来越困难*。因此，对于较大的模型和数据集，广泛的策展可能变得不再那么必要，特别是考虑到为LLM预训练找到足够的数据变得越来越[困难](https://epochai.org/blog/will-we-run-out-of-ml-data-evidence-from-projecting-dataset)。
- en: 'RefinedWeb: Scalable Curation of Text from the Web'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'RefinedWeb: 可扩展的网页文本策展'
- en: '![](../Images/e14fa28eebf85cd442c6f973f4bbdfea.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e14fa28eebf85cd442c6f973f4bbdfea.png)'
- en: (from [2])
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [2]）
- en: Given these limitations, the authors of Falcon [1] explore scalable and efficient
    methods of data curation that generalize to massive amounts of data. The full
    data curation pipeline, which is used to create the [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)
    pre-training dataset over which Falcon-7B [10] and Falcon-40B [11] are pre-trained,
    is detailed in [2]. RefinedWeb is comprised of web-based data that undergoes a
    simplified filtering pipeline and can be used to train models that outperform
    similar models trained over curated sources of data; see above. Such a finding
    indicates that large-scale training corpora can be efficiently created from data
    obtained exclusively from the internet (as opposed to curated sources).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些局限性，Falcon [1]的作者探讨了可扩展和高效的数据策展方法，这些方法可以推广到大量数据。用于创建[RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)预训练数据集的完整数据策展流程，Falcon-7B
    [10]和Falcon-40B [11]即在该数据集上进行预训练，详细描述见[2]。RefinedWeb由经过简化过滤流程的网页数据组成，可以用于训练比在策展数据源上训练的类似模型性能更优的模型；见上文。这一发现表明，大规模训练语料库可以有效地从专门从互联网获得的数据中创建（而不是策展数据源）。
- en: “Challenging existing beliefs on data quality and LLMs, models trained on adequately
    filtered and deduplicated web data alone can match the performance of models trained
    on curated data.” *— from [2]*
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “挑战了对数据质量和LLMs的现有信念，单独使用经过适当过滤和去重的网络数据训练的模型可以达到与训练在策展数据上的模型相当的性能。” *— 来源于 [2]*
- en: '**Simplified curation pipeline.** The pre-training dataset used for Falcon
    is based upon [Common Crawl](https://commoncrawl.org/). Compared to prior work,
    authors in [2] differentiate their data curation pipeline by placing an emphasis
    upon scale, *aiming to produce a pre-training corpus with 3–6 trillion tokens
    of data from the web*. This is much larger than datasets explored in prior work
    — even MassiveText (i.e., the corpus used to pre-train Gopher [9] and Chinchilla
    [6]) contains only 2.3 trillion tokens of text in total. Plus, existing models
    only use a subset of this data during pre-training.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**简化的策展流程**。用于Falcon的预训练数据集基于[Common Crawl](https://commoncrawl.org/)。与之前的工作相比，[2]中的作者通过强调规模来区分他们的数据策展流程，*旨在从网络中生成一个包含3-6万亿个数据标记的预训练语料库*。这远远超过了之前工作的数据集——即使是MassiveText（即用于预训练Gopher
    [9]和Chinchilla [6]的语料库）也仅包含2.3万亿个标记的文本。而且，现有模型在预训练过程中仅使用了这些数据的一个子集。'
- en: '![](../Images/a24030f337f8654514e93e3a46049486.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a24030f337f8654514e93e3a46049486.png)'
- en: (from [2])
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [2]）
- en: The corpus constructed in [2] contains roughly 5 trillion tokens of English-only
    data, obtained exclusively from the web; see above. Despite the scale of this
    data, authors adopt a stringent deduplication policy during the construction process,
    which removes both exact and [fuzzy duplicates](/a-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7)
    at a high rate. Minimal extra filtering is done on top of such deduplication.
    In fact, no machine learning-based filtering is performed aside from language
    identification with a [FastText classifier](https://ai.facebook.com/blog/introducing-many-to-many-multilingual-machine-translation/).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 中构建的语料库包含大约 5 万亿个仅含英语的数据标记，完全从网络上获取；见上文。尽管数据规模庞大，作者在构建过程中采用了严格的去重策略，这一策略以较高的速度去除准确的和
    [模糊重复](https://a-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7)。在这种去重之外，只进行了最小的额外过滤。实际上，除了通过
    [FastText 分类器](https://ai.facebook.com/blog/introducing-many-to-many-multilingual-machine-translation/)
    进行语言识别外，没有进行其他基于机器学习的过滤。'
- en: '![](../Images/31969e7e6acb9ce3ab8fbd6f47f59b30.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31969e7e6acb9ce3ab8fbd6f47f59b30.png)'
- en: (from [9])
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [9]）
- en: 'In addition to filtering out non-English documents, authors in [2] use several
    simple heuristics to filter out unwanted content, such as:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 除了过滤非英语文档，[2] 中的作者还使用了几个简单的启发式方法来过滤不需要的内容，例如：
- en: Filtering content from URLs associated with a blocklist
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从与黑名单关联的 URL 中过滤内容
- en: Using [trafilatura](https://trafilatura.readthedocs.io/en/latest/) to extract
    content from web pages
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 [trafilatura](https://trafilatura.readthedocs.io/en/latest/) 从网页中提取内容
- en: Defining simple rules to identify and filter PII
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义简单规则来识别和过滤 PII
- en: Additionally, several steps from the filtering pipeline of MassiveText [9] are
    adopted; see above. RefinedWeb’s full curation pipeline removes nearly 90% of
    the data that was originally obtained from CommonCrawl, but over 5 trillion tokens
    of textual data are still present in the fully-filtered corpus; see below. Plus,
    this dataset is “multimodal friendly”, as it contains links to images and alt
    text.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，采用了 MassiveText [9] 过滤管道中的几个步骤；见上文。RefinedWeb 的完整策划管道移除了原本从 CommonCrawl 获得的近
    90% 数据，但在完全过滤后的语料库中仍保留了超过 5 万亿个标记的文本数据；见下文。此外，这个数据集是“多模态友好的”，因为它包含了指向图片的链接和替代文本。
- en: '![](../Images/da2f19496120c661797566a662f0ffd2.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da2f19496120c661797566a662f0ffd2.png)'
- en: (from [2])
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [2]）
- en: '**Open-source pre-training data.** The resulting RefinedWeb dataset is quite
    large, indicating that sufficient data is available on the internet to conduct
    LLM pre-training at an unprecedented scale. In other words, *we aren’t quite running
    out of data (at least yet)*. However, authors in [2] only open-source a small,
    600B token subset of this corpus (i.e., 12% of the data). Despite its size, this
    public subset of RefinedWeb is a useful source of pre-training data for any practitioner
    working on LLMs. In fact, smaller variants of Falcon trained on this data in [2]
    are found to perform favorably compared to models trained on curated corpora;
    see below.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**开源预训练数据。** 生成的 RefinedWeb 数据集非常庞大，表明互联网上有足够的数据可以进行前所未有规模的 LLM 预训练。换句话说，*我们并没有（至少暂时没有）数据短缺*。然而，[2]
    中的作者仅公开了该语料库中一个小的 600B 标记子集（即 12% 的数据）。尽管其规模较小，但这个公开的 RefinedWeb 子集仍然是任何从事 LLM
    工作的实践者有用的预训练数据来源。事实上，在 [2] 中，基于这些数据训练的小型 Falcon 变体相比于在策划语料库上训练的模型表现更为优越；详见下文。'
- en: '![](../Images/37ba0534f99ae86567287b7b74d09c5c.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37ba0534f99ae86567287b7b74d09c5c.png)'
- en: (from [2])
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [2]）
- en: The Falcon Suite of LLMs
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Falcon 系列 LLM
- en: '![](../Images/0a83f0e07ea8dde2417e6773023d7c8d.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a83f0e07ea8dde2417e6773023d7c8d.png)'
- en: (from the TII Falcon webpage [1])
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 TII Falcon 网页 [1]）
- en: The Falcon suite of LLMs [1], including Falcon-7B [10] and Falcon-40B [11],
    achieves state-of-the-art performance among open-source models. Furthermore, instruction
    tuned variants of these models, such as [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)
    (i.e., the top model on HuggingFace’s [Open LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)),
    perform even better on public benchmarks. As we will see, these models have several
    key traits (e.g., data, performance, and inference speed) that make them unique
    and practically useful.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon 系列 LLM [1]，包括 Falcon-7B [10] 和 Falcon-40B [11]，在开源模型中实现了最先进的性能。此外，这些模型的指令调优变体，如
    [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)（即 HuggingFace
    的 [Open LLM 排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    上的顶级模型），在公共基准测试中表现更佳。正如我们将看到的，这些模型具有几个关键特性（例如，数据、性能和推理速度），使其独特且实际有用。
- en: '**Commercial license.** Originally, the Falcon models were released under a
    peculiar license that required royalty payments when the models (or any derivatives
    of them) were used in commercial applications. Shortly after this initial release,
    however, the license was modified to a normal [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)
    license, which means that the Falcon base models are now free to use in commercial
    applications! Compared to other open-source and commercially-usable LLMs (even
    MPT-30B), Falcon-40B achieves uniquely impressive performance.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**商业许可证。** 起初，Falcon 模型以一种特殊的许可证发布，该许可证要求在模型（或其任何衍生物）用于商业应用时支付版税。然而，在这一初始发布之后不久，该许可证被修改为普通的
    [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) 许可证，这意味着 Falcon 基础模型现在可以在商业应用中免费使用！与其他开源和可商业使用的
    LLMs（即使是 MPT-30B）相比，Falcon-40B 实现了独特的卓越性能。'
- en: Falcon-7B and 40B Datasets
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Falcon-7B 和 40B 数据集
- en: '![](../Images/2e7873095bb9719b857400439db4124d.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e7873095bb9719b857400439db4124d.png)'
- en: (from [10, 11])
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [10, 11]）
- en: As mentioned previously, the dataset created in [2] is used to pre-train the
    open-source Falcon-7B/40B models. However, these models only pre-train over a
    subset of the full 5 trillion token corpus (i.e., 1.5 trillion tokens for Falcon-7B
    and 1 trillion tokens for Falcon-40B). However, this subset of the corpus is augmented
    with extra, curated data, such as books, code, and technical content; see above.
    Due to its larger size, Falcon-40B is trained over less data than Falcon-7B. Nonetheless,
    the larger model still performs much better and takes over two months to train,
    as opposed to only two weeks for Falcon-7B.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，数据集 [2] 被用于对开源 Falcon-7B/40B 模型进行预训练。然而，这些模型仅在完整的 5 万亿标记语料库的一个子集上进行预训练（即
    Falcon-7B 为 1.5 万亿标记，Falcon-40B 为 1 万亿标记）。然而，这个语料库的子集通过额外的、精心挑选的数据（如书籍、代码和技术内容）进行了增强；见上文。由于其更大的规模，Falcon-40B
    的训练数据量少于 Falcon-7B。尽管如此，较大的模型仍然表现更好，训练时间超过两个月，而 Falcon-7B 仅需两周。
- en: '**Multi-lingual LLMs.** The RefinedWeb corpus is English-only and Falcon-7B
    is trained using only English text. Falcon-40B, however, has the RefinedWeb-Europe
    corpus added to its pre-training set, which contains textual data from a variety
    of common European languages. Although this data only accounts for 7% of the pre-training
    corpus, it injects a small amount of multilingual data into the model’s knowledge
    base, enabling higher levels of performance on public benchmarks that require
    basic multilingual understanding.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**多语言 LLMs。** RefinedWeb 语料库仅包含英文数据，而 Falcon-7B 仅使用英文文本进行训练。然而，Falcon-40B 的预训练集中增加了
    RefinedWeb-Europe 语料库，该语料库包含来自各种常见欧洲语言的文本数据。尽管这些数据仅占预训练语料库的 7%，但它将少量多语言数据注入到模型的知识库中，使其在需要基本多语言理解的公共基准测试中表现更佳。'
- en: Falcon Architecture
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Falcon 架构
- en: 'Both Falcon-7B and Falcon-40B models use a modified decoder-only transformer
    architecture. Modifications made to this architecture include:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon-7B 和 Falcon-40B 模型都使用了修改过的解码器-only transformer 架构。对该架构的修改包括：
- en: Flash Attention
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flash Attention
- en: RoPE embeddings
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RoPE 嵌入
- en: Multi-Query Attention
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Multi-Query Attention
- en: Parallel Attention and Feed-Forward Layers
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行注意力和前馈层
- en: These modifications (some of which are shared with the MPT suite of LLMs) drastically
    improve Falcon’s inference speed. In fact, Falcon-40B is `5X` faster than GPT-3
    at inference time. Plus, pre-training Falcon-40B is less costly due to these modifications;
    e.g., Falcon-40B requires 75% of GPT-3’s [4] compute budget, 40% of Chinchilla’s
    [6] compute budget, and 80% of PaLM-62B’s [5] compute budget. Both Falcon-7B and
    Falcon-40B are trained with a sequence length of 2K tokens, which is arguably
    small compared to recent LLMs (e.g., [StoryWriter](https://www.mosaicml.com/blog/mpt-7b)
    and [Claude](https://www.anthropic.com/index/100k-context-windows)).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些修改（其中一些与 MPT 套件的 LLMs 共享）极大地提高了 Falcon 的推理速度。实际上，Falcon-40B 的推理速度是 GPT-3 的
    `5X`。此外，由于这些修改，预训练 Falcon-40B 的成本较低；例如，Falcon-40B 需要 75% 的 GPT-3 [4] 计算预算，40%
    的 Chinchilla [6] 计算预算和 80% 的 PaLM-62B [5] 计算预算。Falcon-7B 和 Falcon-40B 的训练序列长度为
    2K 标记，相较于最近的 LLMs（如 [StoryWriter](https://www.mosaicml.com/blog/mpt-7b) 和 [Claude](https://www.anthropic.com/index/100k-context-windows)）可以说较小。
- en: '![](../Images/3f28168ffe93c805f9bbc83ced80fccc.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f28168ffe93c805f9bbc83ced80fccc.png)'
- en: (from [10, 11])
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [10, 11]）
- en: Falcon-7B/40B share the same model architecture, but the 40B variant is slightly
    deeper (i.e., 60 layers vs. 32 layers) and has higher-dimensional hidden layers;
    see above. Using Falcon-40B requires ~90Gb of memory, which is less overhead than
    comparable models like LLaMA-65B. However, Falcon-40B still cannot be hosted on
    a single GPU like MPT-30B. Given its smaller size, Falcon-7B only requires ~15Gb
    of memory, making it more accessible for both inference and fine-tuning.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon-7B/40B共享相同的模型架构，但40B变体稍微更深（即60层对比32层）且具有更高维度的隐藏层；见上文。使用Falcon-40B需要约90Gb的内存，这比类似的模型（如LLaMA-65B）有更低的开销。然而，Falcon-40B仍不能像MPT-30B一样在单个GPU上托管。鉴于其较小的规模，Falcon-7B仅需约15Gb的内存，使其在推理和微调中更具可及性。
- en: '**RoPE embeddings.** As we have seen in prior overviews, the self-attention
    operation, which is implemented in every layer of a language model’s decoder-only
    transformer architecture, does not naturally consider the position of each token
    within a sequence. As such, we must inject position information (e.g., via an
    additive positional embedding) into this operation for each token; see below.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**RoPE嵌入。** 正如我们在之前的概述中看到的，自注意力操作（该操作在语言模型的解码器单一转换器架构的每一层中实现）并不会自然地考虑序列中每个标记的位置。因此，我们必须为每个标记注入位置信息（例如，通过加性位置嵌入）到此操作中；见下文。'
- en: '![](../Images/6ec75d8d03d246b5921c190064e5acbf.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ec75d8d03d246b5921c190064e5acbf.png)'
- en: Additive positional embeddings for a transformer (created by author)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器的加性位置嵌入（由作者创建）
- en: 'Several position embedding variants — which may or may not learn each embedding
    during training — have been proposed, including absolute and relative embeddings.
    Rotary positional embeddings (RoPE) [15], however, are an alternative that incorporates
    both the absolute (i.e., global position of a token in the sequence) and relative
    (i.e., defines position based on distances between tokens) position of each token
    into self-attention by:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 提出了几种位置嵌入变体——这些变体可能在训练过程中学习每个嵌入，也可能不学习，包括绝对嵌入和相对嵌入。然而，旋转位置嵌入（RoPE）[15]是一个将每个标记的绝对位置（即序列中的全局位置）和相对位置（即基于标记之间的距离定义位置）结合到自注意力中的替代方案，如下所示：
- en: Encoding absolute position with a [rotation matrix](https://en.wikipedia.org/wiki/Rotation_matrix)
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用[旋转矩阵](https://en.wikipedia.org/wiki/Rotation_matrix)编码绝对位置
- en: Adding relative position information directly into the self-attention operation
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将相对位置信息直接添加到自注意力操作中
- en: Such an approach is found to yield a balance between absolute and relative position
    information, which benefits performance especially on tasks that require longer
    sequence lengths. As such, RoPE embeddings have recently gained in popularity,
    leading to their use in models like PaLM [5]. For a more detailed overview, check
    out the outline of RoPE embeddings [here](https://blog.eleuther.ai/rotary-embeddings/).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被发现能够在绝对位置和相对位置信息之间取得平衡，这对需要较长序列长度的任务尤其有利。因此，RoPE嵌入最近获得了广泛关注，导致其被用于如PaLM
    [5]等模型中。有关更详细的概述，请查看RoPE嵌入的[这里](https://blog.eleuther.ai/rotary-embeddings/)。
- en: '**Multi-query attention.** Falcon models replace the typical, multi-headed
    self-attention operation with an alternative structure called multi-query attention.
    Multi-query attention just shares key and value vectors (highlighted in red below)
    between each of a layer’s attention heads. Instead of performing a separate projection
    for each head, all heads share the same projection matrix for keys and the same
    projection layer for values. Although this change does not make training any faster,
    it significantly improves the inference speed of the resulting LLM.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**多查询注意力。** Falcon模型用一种称为多查询注意力的替代结构替换了典型的多头自注意力操作。多查询注意力仅在每层的注意力头之间共享键和值向量（如下图红色高亮部分）。所有头部共享相同的键的投影矩阵和相同的值的投影层，而不是为每个头部执行单独的投影。尽管这一变化并未加快训练速度，但显著提高了最终LLM的推理速度。'
- en: '![](../Images/039a767ac21dfcdc2004b48b3619e68c.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/039a767ac21dfcdc2004b48b3619e68c.png)'
- en: Multi-query attention in LLMs (from [18])
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: LLM中的多查询注意力（来自[18]）
- en: '**Parallel attention layers.** Finally, the Falcon models make one fundamental
    change to the structure of each layer in their architecture. In contrast to the
    “serial” formulation of the decoder-only transformer layer, each layer of the
    Falcon models performs self-attention and a feed-forward transformation in parallel,
    then follows these operations with a single layer norm operation. The difference
    between this formulation and a standard transformer block is depicted below. Interestingly,
    this parallel formulation does not deteriorate the model’s performance. However,
    it can yield benefits in inference speed due to the fact that both major operations
    of a transformer layer happen in parallel.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**并行注意力层。** 最后，Falcon 模型在其架构的每一层结构中做出了一项基本变化。与“串行”解码器仅限变体的层不同，Falcon 模型的每一层都并行执行自注意力和前馈变换，然后进行单层归一化操作。这个公式与标准的变换器块之间的差异如下所示。有趣的是，这种并行公式并没有降低模型的性能。然而，由于变换器层的两个主要操作是并行进行的，因此它可能在推理速度上带来好处。'
- en: '![](../Images/86b1c65401579465e9f86b1d664926c1.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/86b1c65401579465e9f86b1d664926c1.png)'
- en: Different variants of decoder-only transformer layers (created by author)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器仅限的变体层（由作者创建）
- en: A New Standard for Open-Source LLMs!
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开源语言模型的新标准！
- en: 'At the time of writing, no manuscript has yet been published about the Falcon
    models. However, Falcon-7B and Falcon-40B (along with their instruction tuned
    variants) have been evaluated via the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard),
    which includes several benchmarks such as:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写时，关于 Falcon 模型的手稿尚未发布。然而，Falcon-7B 和 Falcon-40B（以及它们的指令调优变体）已通过 [Open LLM
    Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    进行评估，该榜单包括多个基准测试，例如：
- en: ARC [[link](https://huggingface.co/datasets/ai2_arc)]
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ARC [[链接](https://huggingface.co/datasets/ai2_arc)]
- en: HellaSwag [[link](https://huggingface.co/datasets/hellaswag)]
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HellaSwag [[链接](https://huggingface.co/datasets/hellaswag)]
- en: MMLU [[link](https://huggingface.co/datasets/lukaemon/mmlu)]
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MMLU [[链接](https://huggingface.co/datasets/lukaemon/mmlu)]
- en: TruthfulQA [[link](https://huggingface.co/datasets/truthful_qa)]
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TruthfulQA [[链接](https://huggingface.co/datasets/truthful_qa)]
- en: Evaluations conducted via this leaderboard are incomplete and preliminary. However,
    these evaluations, which do capture model performance to a reasonable extent,
    clearly show that Falcon-40B is the current state-of-the-art for open-source language
    models; see below.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通过该榜单进行的评估是不完整和初步的。然而，这些评估虽然在合理范围内捕捉了模型性能，但清楚地显示了 Falcon-40B 是当前开源语言模型的最先进技术；见下文。
- en: '![](../Images/6ff26ede567d2242b1804df2484e9bae.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ff26ede567d2242b1804df2484e9bae.png)'
- en: (from Open LLM Leaderboard)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 Open LLM Leaderboard）
- en: The instruct variant of Falcon-40B (i.e., [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)),
    which has been instruction tuned on a mixture of data from [Baize](https://github.com/project-baize/baize-chatbot),
    far outperforms a variety of other open-sourced models. Additionally, the pre-trained
    Falcon-40B model performs quite well, even better than notable base models like
    LLaMA-65B and MPT-30B. Going further, Falcon-40B is also commercially-usable,
    whereas many comparable models on the leaderboard (e.g., LLaMA [13], [Guanaco](https://huggingface.co/timdettmers/guanaco-65b)
    [14], and [Lazarus](https://huggingface.co/CalderaAI/30B-Lazarus)) are only available
    for research purposes.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon-40B 的指令变体（即 [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)），它在来自
    [Baize](https://github.com/project-baize/baize-chatbot) 的数据混合上进行了指令调优，远远超越了各种其他开源模型。此外，预训练的
    Falcon-40B 模型表现也相当不错，甚至优于像 LLaMA-65B 和 MPT-30B 这样的著名基础模型。进一步说，Falcon-40B 也可用于商业用途，而榜单上的许多可比模型（例如
    LLaMA [13]、[Guanaco](https://huggingface.co/timdettmers/guanaco-65b) [14] 和 [Lazarus](https://huggingface.co/CalderaAI/30B-Lazarus)）仅可用于研究目的。
- en: '**Practical usage of Falcon.** Given that they perform incredibly well, are
    lightweight to host compared to other LLMs (due to improved inference speed),
    and can be used freely in commercial applications, the Falcon LLMs are an impactful
    open-source tool for any practitioner working on LLMs. Luckily, several detailed
    overviews have been written that outline useful frameworks for both fine-tuning
    and hosting/deploying these models in practice.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**Falcon 的实际使用。** 鉴于其性能非常出色，与其他语言模型相比托管起来轻量（由于改进的推理速度），并且可以在商业应用中自由使用，Falcon
    LLMs 是任何从事 LLM 工作的实践者的重要开源工具。幸运的是，已经撰写了几篇详细的概述，概述了在实践中微调和托管/部署这些模型的有用框架。'
- en: Deploy Falcon-40B on AWS Sagemaker [[link](https://aws.amazon.com/blogs/machine-learning/deploy-falcon-40b-with-large-model-inference-dlcs-on-amazon-sagemaker/)]
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 AWS Sagemaker 上部署 Falcon-40B [[link](https://aws.amazon.com/blogs/machine-learning/deploy-falcon-40b-with-large-model-inference-dlcs-on-amazon-sagemaker/)]
- en: Inference and Parameter-Efficient Fine-Tuning with Falcon [[link](https://huggingface.co/blog/falcon)]
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Falcon 进行推理和参数高效微调 [[link](https://huggingface.co/blog/falcon)]
- en: Fine-Tune Falcon-40B with PyTorch Lightning [[link](https://lightning.ai/blog/falcon-a-guide-to-finetune-and-inference/)]
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PyTorch Lightning 对 Falcon-40B 进行微调 [[link](https://lightning.ai/blog/falcon-a-guide-to-finetune-and-inference/)]
- en: Given that Falcon was trained using AWS, there are currently a decent number
    of explainer articles for deploying and training these models on similar hardware.
    These articles provide a good starting point for anyone looking to leverage Falcon
    in a use case of their own.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于 Falcon 使用 AWS 进行训练，目前有相当数量的解释性文章介绍如何在类似硬件上部署和训练这些模型。这些文章为任何希望在自己的用例中利用 Falcon
    的人提供了一个良好的起点。
- en: Final Thoughts
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终思考
- en: 'The release of Falcon was a major breakthrough for the research and application
    of open-source LLMs. When we examine the contributions that are unique to these
    models, we immediately see a few key components that lead to success:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon 的发布是开源 LLM 研究和应用的重大突破。当我们审视这些模型的独特贡献时，我们立即看到一些关键组件，这些组件导致了成功：
- en: A unique mixture of large-scale pre-training data
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大规模预训练数据的独特混合
- en: An architecture that is optimized for efficiency
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 针对效率优化的架构
- en: The RefinedWeb dataset shows us that textual corpora can be created at a much
    larger scale than was previously explored. To do this, we just download a large
    amount of data from the web and adopt strict deduplication rules along with simpler,
    efficient filtering heuristics. Then, by combining this massive source of data
    with a smaller amount of curated text, we can pre-train an incredibly performant
    open-source LLM. Finally, the modified architecture of the Falcon models makes
    both training and inference more efficient, resulting in a model that both performs
    incredibly well and can quickly generate text when deployed.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: RefinedWeb 数据集显示，文本语料库可以在比以前探索的更大规模上创建。为此，我们只需从网络上下载大量数据，并采用严格的去重规则以及更简单高效的过滤启发式方法。然后，通过将这些庞大的数据源与少量精选文本相结合，我们可以预训练一个性能极佳的开源
    LLM。最后，Falcon 模型的修改架构使得训练和推理更加高效，结果是一个表现出色且在部署时能快速生成文本的模型。
- en: Connect with me!
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与我联系！
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/). I study the empirical
    and theoretical foundations of deep learning. If you liked this overview, subscribe
    to my [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/),
    where I help readers understand AI research via overviews of relevant topics from
    the ground up. You can also follow me on [X](https://twitter.com/cwolferesearch)
    and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/), or
    check out my [other writings](https://medium.com/@wolfecameron) on medium!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢阅读这篇文章。我是 [Cameron R. Wolfe](https://cameronrwolfe.me/)，[Rebuy](https://www.rebuyengine.com/)
    的 AI 总监。我研究深度学习的经验和理论基础。如果你喜欢这篇概述，请订阅我的 [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/)，我通过从头开始概述相关主题帮助读者理解
    AI 研究。你还可以在 [X](https://twitter.com/cwolferesearch) 和 [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)
    上关注我，或者查看我在 medium 上的 [其他著作](https://medium.com/@wolfecameron)！
- en: Bibliography
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] “Introducing Falcon LLM”, *Technology Innovation Institute*, 7 June 2023,
    [https://falconllm.tii.ae/.](https://falconllm.tii.ae/.)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] “Introducing Falcon LLM”, *技术创新研究所*, 2023年6月7日, [https://falconllm.tii.ae/.](https://falconllm.tii.ae/.)'
- en: '[2] Penedo, Guilherme, et al. “The RefinedWeb dataset for Falcon LLM: outperforming
    curated corpora with web data, and web data only.” *arXiv preprint arXiv:2306.01116*
    (2023).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Penedo, Guilherme 等. “The RefinedWeb dataset for Falcon LLM: outperforming
    curated corpora with web data, and web data only.” *arXiv preprint arXiv:2306.01116*
    (2023).'
- en: '[3] “Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable
    Llms.” *MosaicML*, 5 May 2023, [www.mosaicml.com/blog/mpt-7b.](http://www.mosaicml.com/blog/mpt-7b.)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] “Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable
    Llms.” *MosaicML*, 2023年5月5日, [www.mosaicml.com/blog/mpt-7b.](http://www.mosaicml.com/blog/mpt-7b.)'
- en: '[4] Brown, Tom, et al. “Language models are few-shot learners.” *Advances in
    neural information processing systems* 33 (2020): 1877–1901.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Brown, Tom 等. “Language models are few-shot learners.” *Advances in neural
    information processing systems* 33 (2020): 1877–1901.'
- en: '[5] Chowdhery, Aakanksha, et al. “Palm: Scaling language modeling with pathways.”
    *arXiv preprint arXiv:2204.02311* (2022).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Chowdhery, Aakanksha 等. “Palm: 利用路径扩展语言建模。” *arXiv 预印本 arXiv:2204.02311*
    (2022)。'
- en: '[6] Hoffmann, Jordan, et al. “Training compute-optimal large language models.”
    *arXiv preprint arXiv:2203.15556* (2022).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Hoffmann, Jordan 等. “训练计算最优的大型语言模型。” *arXiv 预印本 arXiv:2203.15556* (2022)。'
- en: '[7] Zhou, Chunting, et al. “Lima: Less is more for alignment.” *arXiv preprint
    arXiv:2305.11206* (2023).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Zhou, Chunting 等. “Lima: 对齐的‘少即是多’。” *arXiv 预印本 arXiv:2305.11206* (2023)。'
- en: '[8] Taylor, Ross, et al. “Galactica: A large language model for science.” *arXiv
    preprint arXiv:2211.09085* (2022).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Taylor, Ross 等. “Galactica: 一种用于科学的大型语言模型。” *arXiv 预印本 arXiv:2211.09085*
    (2022)。'
- en: '[9] Rae, Jack W., et al. “Scaling language models: Methods, analysis & insights
    from training gopher.” arXiv preprint arXiv:2112.11446 (2021).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Rae, Jack W. 等. “扩展语言模型：方法、分析与训练 gopher 的见解。” *arXiv 预印本 arXiv:2112.11446*
    (2021)。'
- en: '[10] “Falcon-7B”, *Technology Innovation Institute*, HuggingFace Page, [https://huggingface.co/tiiuae/falcon-7b.](https://huggingface.co/tiiuae/falcon-7b.)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] “Falcon-7B”，*技术创新研究所*，HuggingFace 页面，[https://huggingface.co/tiiuae/falcon-7b.](https://huggingface.co/tiiuae/falcon-7b.)。'
- en: '[11] “Falcon-40B”, *Technology Innovation Institute*, HuggingFace Page, [https://huggingface.co/tiiuae/falcon-40b](https://huggingface.co/tiiuae/falcon-40b).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] “Falcon-40B”，*技术创新研究所*，HuggingFace 页面，[https://huggingface.co/tiiuae/falcon-40b](https://huggingface.co/tiiuae/falcon-40b)。'
- en: '[12] Gao, Leo, et al. “The pile: An 800gb dataset of diverse text for language
    modeling.” *arXiv preprint arXiv:2101.00027* (2020).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Gao, Leo 等. “The pile: 一个 800GB 的多样文本数据集用于语言建模。” *arXiv 预印本 arXiv:2101.00027*
    (2020)。'
- en: '[13] Touvron, Hugo, et al. “Llama: Open and efficient foundation language models.”
    *arXiv preprint arXiv:2302.13971* (2023).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Touvron, Hugo 等. “Llama: 开放且高效的基础语言模型。” *arXiv 预印本 arXiv:2302.13971* (2023)。'
- en: '[14] Dettmers, Tim, et al. “Qlora: Efficient finetuning of quantized llms.”
    *arXiv preprint arXiv:2305.14314* (2023).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Dettmers, Tim 等. “Qlora: 量化语言模型的高效微调。” *arXiv 预印本 arXiv:2305.14314* (2023)。'
- en: '[15] Su, Jianlin, et al. “Roformer: Enhanced transformer with rotary position
    embedding.” *arXiv preprint arXiv:2104.09864* (2021).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Su, Jianlin 等. “Roformer: 增强的带有旋转位置嵌入的变换器。” *arXiv 预印本 arXiv:2104.09864*
    (2021)。'
- en: '[16] Ouyang, Long, et al. “Training language models to follow instructions
    with human feedback.” *Advances in Neural Information Processing Systems* 35 (2022):
    27730–27744.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Ouyang, Long 等. “训练语言模型以遵循指令并获得人类反馈。” *神经信息处理系统进展* 35 (2022): 27730–27744。'
- en: '[17] Glaese, Amelia, et al. “Improving alignment of dialogue agents via targeted
    human judgements.” *arXiv preprint arXiv:2209.14375* (2022).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Glaese, Amelia 等. “通过有针对性的人类判断提高对话代理的对齐。” *arXiv 预印本 arXiv:2209.14375*
    (2022)。'
- en: '[18] Vaswani, Ashish, et al. “Attention is all you need.” *Advances in neural
    information processing systems* 30 (2017).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Vaswani, Ashish 等. “注意力即你所需。” *神经信息处理系统进展* 30 (2017)。'
