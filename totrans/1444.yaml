- en: Multiclass Logistic Regression using Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/logistic-regression-a3a23e169eec](https://towardsdatascience.com/logistic-regression-a3a23e169eec)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An introduction to multiclass logistic regression with theory and Python implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://cookieblues.medium.com/?source=post_page-----a3a23e169eec--------------------------------)[![Stefan
    Hrouda-Rasmussen](../Images/701ac16a5753432d493002208151d89f.png)](https://cookieblues.medium.com/?source=post_page-----a3a23e169eec--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a3a23e169eec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a3a23e169eec--------------------------------)
    [Stefan Hrouda-Rasmussen](https://cookieblues.medium.com/?source=post_page-----a3a23e169eec--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a3a23e169eec--------------------------------)
    ·6 min read·Mar 27, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/119f2a6d5cc440dcfe1bdf8df9510593.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision boundary of Logistic Regression. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This post is a part of a series of posts that I will be making. Underneath you
    can see an overview of the series.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction to machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[(a) What is machine learning?](/what-is-machine-learning-91040db474f9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[(b) Model selection in machine learning](/model-selection-in-machine-learning-813fe2e63ec6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[(c) The curse of dimensionality](/the-curse-of-dimensionality-5673118fe6d2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[(d) What is Bayesian inference?](/what-is-bayesian-inference-4eda9f9e20a6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2\. Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[(a) How linear regression actually works](/how-linear-regression-actually-works-theory-and-implementation-8d8dcae3222c)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[(b) How to improve your linear regression with basis functions and regularization](/how-to-improve-your-linear-regression-with-basis-functions-and-regularization-8a6fcebdc11c)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3\. Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[(a) Overview of Classifiers](/overview-of-classifiers-d0a0d3eecfd1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[(b) Quadratic Discriminant Analysis (QDA)](/quadratic-discriminant-analysis-ae55d8a8148a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[(c) Linear Discriminant Analysis (LDA)](/linear-discriminant-analysis-1894bbf04359)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[(d) (Gaussian) Naive Bayes](/gaussian-naive-bayes-4d2895d139a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**(e) Multiclass Logistic Regression using Gradient Descent**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setup and objective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far we’ve gone over generative classifiers (QDA, LDA, and Naive Bayes),
    but now we’ll turn our eyes to a discriminative classifier: logistic regression.
    As mentioned in [3(a)](/overview-of-classifiers-d0a0d3eecfd1), the overview of
    classifiers, **logistic regression is a discriminative classifier** meaning that
    **it models the conditional probability distribution** of the target *P*(*t*|**x**)
    directly.'
  prefs: []
  type: TYPE_NORMAL
- en: But why is it called logistic *regression* and not logistic *classification*
    if it’s a classification model? Well, the answer is simply that we’re regressing
    the conditional probability — I know, it’s confusing.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on, **I recommend that you have a good grasp on linear regression**
    before moving onto logistic regression, **as they’re very similar**. If you read
    [my post about linear regression](/how-linear-regression-actually-works-theory-and-implementation-8d8dcae3222c),
    you’ll have an easy time, as I’m using the same terminology and notation.
  prefs: []
  type: TYPE_NORMAL
- en: Given a training dataset of *N* input variables **x** with corresponding target
    variables *t*, logistic regression assumes that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da98de87d2d4103e528ae5a0af6aa47d.png)'
  prefs: []
  type: TYPE_IMG
- en: where *c* is any integer from 1 to *C* denoting the number of classes.
  prefs: []
  type: TYPE_NORMAL
- en: The right hand side of (1) is the [softmax function](https://en.wikipedia.org/wiki/Softmax_function).
    Basically, **this is the function that we use to transform our linear combination
    into probabilities** between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: To understand what (1) really means, let’s look at the special case where *C=2*,
    i.e., we have 2 classes. We would now typically rewrite (1) as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c6052154396bbad63ad877aad6ba8f1e.png)'
  prefs: []
  type: TYPE_IMG
- en: where *σ* refers to the [logistic sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function),
    which is where the name logistic regression comes from.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the logistic sigmoid function looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: insert image of sigmoid function*
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the y-values lie between 0 and 1, and the higher the value,
    the closer to 1\. **This means that the larger wᵀx is, the higher the probability
    that point (x,** *t***) is of class 1** — and likewise for more than 2 classes,
    whichever value of *c* is higher in (1) will be the class we predict for *t*,
    or
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/684b87e501bc7dca8b3c3c5bb63503ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Derivation and training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, how do we find the values of **W**? Well, just like with linear regression
    **we’ll use maximum likelihood estimation to find the values of our parameters**.
    This works by writing up the likelihood function, taking its derivative, and finding
    for which values of the parameters the derivative is equal to 0, as this will
    be the maximum of the likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: To make notation easier, let **t**ₙ denote a *C*-dimensional vector, where *t*ₙ꜀
    is 1 if the observation belongs to class *c*, and all other components are 0\.
    We can now write the likelihood as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4c7ebfa35328336d1db3a8fe11636362.png)'
  prefs: []
  type: TYPE_IMG
- en: Just like with linear regression, we’ll now take the negative logarithm of the
    likelihood, as we know **minimizing the negative log-likelihood is equivalent
    to maximizing the likelihood**, to help our derivation
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5f33b8e3dc8d24ab5d2ecb7da99cbe7.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have the negative log-likelihood, **we’re going to find the gradient**
    (the derivative) of it and find for which values of **w** it is equal to 0\. The
    function in (3) is also called the **cross-entropy loss function**.
  prefs: []
  type: TYPE_NORMAL
- en: Before we begin the derivation, let’s define some terms to make our notation
    simpler. Firstly, let us denote the softmax function as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d203f47e0bf01e1a6f8649145e054ab1.png)'
  prefs: []
  type: TYPE_IMG
- en: Secondly, let *E* denote the function from (3)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67557b76fab6c47d3c26cf33984e2aa2.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, using the chain rule, we can determine the gradient of *E*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3b0206027bc938e36e6b2b440fd93b13.png)'
  prefs: []
  type: TYPE_IMG
- en: Starting from the right, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/355c41d7c16bc84ee514e0d0cde02e21.png)'
  prefs: []
  type: TYPE_IMG
- en: Next up, we have the derivative of the softmax function
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1f0cb0906eee4e4f4bf3ac06d8c3baf.png)'
  prefs: []
  type: TYPE_IMG
- en: but this is only when *i=c*. In the case where they are not equal, we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d8c7a9f17bfe7153cd691e4b50b933da.png)'
  prefs: []
  type: TYPE_IMG
- en: We can combine (6) and (7) to
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b178cdfb1f4c43567a29453203ceb515.png)'
  prefs: []
  type: TYPE_IMG
- en: Lastly, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c8ad36d87098e4f5fa8c5fd91bb67a5c.png)'
  prefs: []
  type: TYPE_IMG
- en: and now we finally have all the pieces of (4) to put together. Putting (5),
    (7), and (9) into (4) gives us the following
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fb779b8891711b32a19429d0673d9ccc.png)'
  prefs: []
  type: TYPE_IMG
- en: After all that work we’ve finally found the gradient of the likelihood function.
    What we need to do now is figure out for which values of **w** it is equal to
    0\. The reason for this is that the **maxima and minima of the function will be
    where the gradient is 0**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem is that we cannot find a closed-form solution for this, so we’ll
    need an algorithm that figures out where the function is equal to 0 for us. The
    algorithm we’ll use is called **gradient descent**. It works by using the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52c8ff62af06c96c9cd545d793e51c80.png)'
  prefs: []
  type: TYPE_IMG
- en: where *η* is called the learning rate and is a hyperparameter. I’ll write a
    post detailing gradient descent in the future, but briefly **if the learning rate
    is too high you’ll miss the minimum of the gradient function**, and **if it’s
    too low it will take too long to reach the minimum**.
  prefs: []
  type: TYPE_NORMAL
- en: Python implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code underneath is a simple implementation of (Gaussian) Naive Bayes that
    we just went over.
  prefs: []
  type: TYPE_NORMAL
- en: Underneath is a chart with the data points (color coded to match their respective
    classes) and the decision boundaries generated by the logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36656acf4f0a70eb2da052e9a63a314d.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision boundary produced by Logistic Regression with Gradient Descent optimisation.
    Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logistic **regression** is a **classification model**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression is a **discriminative classifier**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we have 2 classes, we use the **logistic sigmoid function to transform our
    linear function into probabilities**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **softmax function is the generalisation** of the logistic sigmoid function
    **to multiple classes**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **negative log-likelihood in logistic regression can also be referred to
    as the cross-entropy loss function**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is **no closed-form solution to logistic regression**, hence we use gradient
    descent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **learning rate and number of iterations are hyperparameters** that you
    will have to tweak.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
