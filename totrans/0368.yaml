- en: 'BERT vs GPT: Comparing the NLP Giants'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/bert-vs-gpt-comparing-the-nlp-giants-329d105e34ec](https://towardsdatascience.com/bert-vs-gpt-comparing-the-nlp-giants-329d105e34ec)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How different are their structure, and how do the differences impact the model’s
    ability?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@vuphuongthao9611?source=post_page-----329d105e34ec--------------------------------)[![Thao
    Vu](../Images/9d44a2f199cdc9c29da72d9dc4971561.png)](https://medium.com/@vuphuongthao9611?source=post_page-----329d105e34ec--------------------------------)[](https://towardsdatascience.com/?source=post_page-----329d105e34ec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----329d105e34ec--------------------------------)
    [Thao Vu](https://medium.com/@vuphuongthao9611?source=post_page-----329d105e34ec--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----329d105e34ec--------------------------------)
    ·7 min read·Aug 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2a6c1c6d9546726bba7434946b0bcdc0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by the author using Stable Diffusion.
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2018, NLP researchers were all amazed by the BERT paper [1]. The approach
    was simple, yet the result was impressive: it set new benchmarks for 11 NLP tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: In a little over a year, BERT has become a ubiquitous baseline in Natural Language
    Processing (NLP) experiments counting over 150 research publications analysing
    and improving the model. [2]
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In 2022, ChatGPT [3] blew up the whole Internet with its ability to generate
    human-like responses. The model can comprehend a wide range of topics and carry
    the conversation naturally for an extended period, which sets it apart from all
    traditional chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: BERT and ChatGPT are significant breakthroughs in NLP, yet their approaches
    are different. How do their structures differ, and how do they impact the models’
    ability? Let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: Attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We must first recall the commonly-used attention to understand the model structure
    fully. Attention mechanisms are designed to capture and model relationships between
    tokens in a sequence, which is one of the reasons why they have been so successful
    in NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: An intuitive understanding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine you have ***n*** goods stored in boxes ***v1, v2,…,v_n.*** These arecalled
    “values”.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have query ***q*** whichdemands to take some suitable amount ***w*** of goods
    from each box. Let’s call them ***w_1, w_2,..,w_n*** (this is the “attention weight”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to determine ***w_1, w_2,.., w_n***? Or, in other words, how to know among
    ***v_1,v_2, ..,v_n,*** which should be taken more than others?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember, all the values are stored in boxes we cannot peek into. So we can’t
    directly judge ***v_i*** should be taken less or more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luckily, we have a tag on each box, ***k_1, k_2,…,k_n***, which are called “keys”.
    The “keys” represent the characteristic of what is inside the containers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the “similarity” of ***q*** and ***k_i (q*k_i)***, we can then decide
    how important the ***v_i*** is (***w_i***) and how much of ***v_i*** we should
    take(***w_i*v_i***).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/17f21a23443564ae27c1603cc8a1022c.png)'
  prefs: []
  type: TYPE_IMG
- en: Base attention mechanism (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Of course, that is a very abstract explanation of attention, but it helps me
    to remember better the meaning behind “query”, “key”, and “value”.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s take a deeper look at how the Transformer models use different types
    of attention.
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT: Global self-attention and bidirectional encoder'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Global self-attention has the same value for query, key and value. In a sequence
    of tokens, **each token will “attend” all other tokens**, so the information is
    propagated along the sequence. And more important, in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/259c53bf8367e74ea252299f2cbed0b3.png)'
  prefs: []
  type: TYPE_IMG
- en: Global self-attention [4]
  prefs: []
  type: TYPE_NORMAL
- en: This is significant compared with RNN and CNN.
  prefs: []
  type: TYPE_NORMAL
- en: For RNN, each “state” is passed through many steps, which may cause the loss
    of information. Besides, RNN passing is sequentially through each token; we can’t
    make use of GPU parallelism.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For CNN, even though it runs in parallel, each token can only attend to a limited
    field, making assumptions about the tokens’ relationship.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The self-attention is the key component of encoders, the building block of BERT
    [1]. The BERT paper’s authors pointed out the limits of left-to-right language
    models as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Such restrictions are sub-optimal for sentence-level tasks and could be very
    harmful when applying finetuning-based approaches to token-level tasks such as
    question answering, where it is crucial to incorporate context from both directions.
    [1]
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/55d42bc5e549cd57e16db6e008eb0624.png)'
  prefs: []
  type: TYPE_IMG
- en: BERT pre-training [1]
  prefs: []
  type: TYPE_NORMAL
- en: To overcome the shortcoming above, BERT was pre-trained on “masked language
    model” (MLM) and “next sentence prediction” (NSP) tasks.
  prefs: []
  type: TYPE_NORMAL
- en: For the MLM task, 15% of token positions were selected for prediction. So those
    chosen tokens will have 80% replaced with a ***[MASK]*** token, 10% replaced by
    a random token, and 10% not replaced.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the NSP task, given 2 sentences, ***s1*** and ***s2***, the input format
    is “***[CLS]<s1>[SEP]<s2>***”, and the model predicts whether ***s1*** is followed
    by ***s2***. [***CLS]*** and ***[SEP]*** are the special classification and separate
    tokens, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we can see, the model can “peek” at both the left and right contexts of each
    token in both tasks.This allows the model to take advantage of the bidirectional
    word representation and gain a deeper understanding.
  prefs: []
  type: TYPE_NORMAL
- en: But the bidirectional encoding comes with a cost. Lacking decoders, BERT may
    not be suitable for text generation. Therefore, the model requires adding extra
    task-specific architecture to adapt to generative tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT: Causal self-attention and text generative'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compared to global self-attention, causal self-attention allows each token to
    only attend to its left context. This architecture is unsuitable for tasks such
    as textual understanding but makes the model good at text generation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ab61426c1a572cc8f2218f75f3e4566.png)'
  prefs: []
  type: TYPE_IMG
- en: Causal self-attention [4]
  prefs: []
  type: TYPE_NORMAL
- en: Namely, causal self-attention allows the model tolearn the probabilities of
    a series of words, which is the core of a “language model” [8]. Given a sequence
    of symbols x=(s1, s2, …, sn), the model can predict the likelihood of the series
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fafe803ac5306d4d08de022ab1a770aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Joint probabilities over a sequence of symbols [6]
  prefs: []
  type: TYPE_NORMAL
- en: Causal self-attention is the critical component of the Transformer decoder block.
    One of the first pre-trained Transformer decoders is GPT [5] by OpenAI. Like BERT,
    the model also aims to utilise the massive corpus of unlabeled text datasets to
    build a pre-trained language model. Pretraining on Book Corpus[7], the model objective
    is to predict the next token. The pre-trained model is then finetuned to adapt
    to downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2 [6] shares the same approach of building universal word representation
    but is more ambitious. It aims to be a “multitask learner”, performing different
    tasks without fine-tuning. GPT only learns the distribution of ***p(output|input),***
    which makes the model lack context on “what task to do”***.*** The authors wanted
    to adapt GPT-2 to multi-tasks by conditioning the prediction on both input and
    task, ***p(output|input, task)***.
  prefs: []
  type: TYPE_NORMAL
- en: Previous approaches have corporated the “task” information at the architectural
    level, but GPT-2 makes it more flexible by “expressing” the task through natural
    language. For example, a translation task’s input can be “*translate to French,
    <English sentence>*”.
  prefs: []
  type: TYPE_NORMAL
- en: Mining a large amount of unlabeled text with explicit “task” information can
    be challenging. However, the authors believed the model could infer the implicit
    “tasks” expression from natural languages. Therefore, they collected a vast and
    diverse dataset which can demonstrate the “task” in varied domains. Namely, the
    model was trained on the WebText dataset[6] containing the text subset of 45 million
    links.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the less competitive performance on some benchmarks, GPT-2 has laid
    the ground for many LLM laters, such as GPT-3 [9] and ChatGPT. In particular,
    GPT-3 can comprehend tasks and demonstrations solely through text-based interactions.
    For the SuperGLUE benchmark [10], a set of language understanding tasks, GPT-3,
    without gradient-based update, has shown impressive performance compared to fine-tuned
    BERT.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e1626027cc404fdbc4df62c80f60ea0b.png)'
  prefs: []
  type: TYPE_IMG
- en: Performance of GPT-3 and BERT on SuperGLUE [9]
  prefs: []
  type: TYPE_NORMAL
- en: '**Which model to choose?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Based on the models’ structure, we can conclude that BERT excels at understanding
    language and extracting contextual information, making it ideal for tasks like
    sentiment analysis and text classification. In contrast, GPT models are designed
    for generating human-like text, making it a top choice for chatbots and language
    generation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Another important factor is our data resources. We can easily customise recent
    GPT models to specific tasks with only a small amount of data, making them suitable
    for a broader range of applications. On the other hand, BERT finetuning might
    require more effort and data. For finetuning LLM techniques, you can check out
    my post.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/mlearning-ai/a-simple-survey-of-fine-tuning-techniques-for-large-language-models-6c7945e6ee34?source=post_page-----329d105e34ec--------------------------------)
    [## A Quick Guide to Fine-tuning Techniques for Large Language Models'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLM) have transformed the field of natural language processing
    (NLP) with their remarkable text…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/mlearning-ai/a-simple-survey-of-fine-tuning-techniques-for-large-language-models-6c7945e6ee34?source=post_page-----329d105e34ec--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, we also need to consider our computational resources. Although
    there have been many optimisation efforts, finetuning, storing and serving LLM
    still demands substantial resources compared to BERT.
  prefs: []
  type: TYPE_NORMAL
- en: Or you may enjoy the best of both worlds by incorporating them together. I will
    cover this topic in a future article.
  prefs: []
  type: TYPE_NORMAL
- en: For now, I hope you enjoy the reading :-)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers
    for language understanding.” *arXiv preprint arXiv:1810.04805* (2018).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Rogers, Anna, Olga Kovaleva, and Anna Rumshisky. “A primer in BERTology:
    What we know about how BERT works.” Transactions of the Association for Computational
    Linguistics 8 (2021): 842–866.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [](https://openai.com/blog/chatgpt) [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [https://www.tensorflow.org/text/tutorials/transformer](https://www.tensorflow.org/text/tutorials/transformer)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Radford, Alec, et al. “Improving language understanding by generative pre-training.”
    (2018).'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Radford, Alec, et al. “Language models are unsupervised multitask learners.”
    *OpenAI blog* 1.8 (2019): 9.'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Zhu, Yukun, et al. “Aligning books and movies: Towards story-like visual
    explanations by watching movies and reading books.” *Proceedings of the IEEE international
    conference on computer vision*. 2015.'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] [https://en.wikipedia.org/wiki/Language_model](https://en.wikipedia.org/wiki/Language_model)'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Brown, Tom, et al. “Language models are few-shot learners.” *Advances in
    neural information processing systems* 33 (2020): 1877–1901.'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Wang, Alex, et al. “Superglue: A stickier benchmark for general-purpose
    language understanding systems.” *Advances in neural information processing systems*
    32 (2019).'
  prefs: []
  type: TYPE_NORMAL
