- en: 'Web Speech API: What Works, What Doesn’t, and How to Improve It by Linking
    It to a GPT Language Model'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/web-speech-api-what-works-what-doesnt-and-how-to-improve-it-by-linking-it-to-a-gpt-language-dc1afde54ced](https://towardsdatascience.com/web-speech-api-what-works-what-doesnt-and-how-to-improve-it-by-linking-it-to-a-gpt-language-dc1afde54ced)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Part of a series on how modern AI and other technologies could assist more efficient
    human-computer interactions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://lucianosphere.medium.com/?source=post_page-----dc1afde54ced--------------------------------)[![LucianoSphere
    (Luciano Abriata, PhD)](../Images/a8ae3085d094749bbdd1169cca672b86.png)](https://lucianosphere.medium.com/?source=post_page-----dc1afde54ced--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dc1afde54ced--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dc1afde54ced--------------------------------)
    [LucianoSphere (Luciano Abriata, PhD)](https://lucianosphere.medium.com/?source=post_page-----dc1afde54ced--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dc1afde54ced--------------------------------)
    ·15 min read·Dec 6, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a173108cd0621793832a4adc1033b257.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [palesa](https://unsplash.com/@palesa08?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: I am of the idea that modern technologies enable today much simpler and natural
    human-computer interactions than what current software actually proposes. Indeed,
    I think technologies are ripe enough that we could just go without traditional
    interfaces and move forward with a revolution in user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Large language models have certainly triggered one stage of this revolution,
    particularly in how we ask for information. However, I think technologies can
    still provide much more. For example, we are still largely stuck with flat screens
    despite the decreasing costs of VR headsets; we are still using mouse, keyboard,
    and touch gestures to operate devices despite the level of advancement of technologies
    like eye-gazing, speech-recognition and body limb tracking; we are still reading
    out a lot despite great advances in speech synthesis.
  prefs: []
  type: TYPE_NORMAL
- en: I feel current technologies are ripe enough to offer human-computer interactions
    almost like those in Star Trek (if you don’t know what I mean, [check this](https://www.youtube.com/watch?v=Drr6_zikuZQ)),
    yet we want to be stuck in the past.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: With this article I’m starting a short series dedicated to how human-computer
    interactions could change forever thanks to modern technologies that **already
    work very well**, as you will be able to test yourself with the pieces of code
    and example apps I will share.
  prefs: []
  type: TYPE_NORMAL
- en: Faithful to my style, I will talk specifically about web-based implementations
    of all these modern technologies. And I start here with the Web Speech API integrated
    into web browsers, discussing its power, showing some use cases, highlighting
    limitations and exemplifying how some of these limitations can be overcome by
    coupling it to large language models.
  prefs: []
  type: TYPE_NORMAL
- en: 'This series is based on a recent project on which I worked to build a first-of-type
    web app for immersive, multiuser molecular graphics and modeling, HandMol:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://pub.towardsai.net/coupling-four-ai-models-to-deliver-the-ultimate-experience-in-immersive-visualization-and-modeling-9f52a4bd1443?source=post_page-----dc1afde54ced--------------------------------)
    [## Coupling Four AI Models to Deliver the Ultimate Experience in Immersive Visualization
    and Modeling'
  prefs: []
  type: TYPE_NORMAL
- en: Read on about how this new free app called HandMol achieves immersive, collaborative
    visualization and modeling of…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pub.towardsai.net](https://pub.towardsai.net/coupling-four-ai-models-to-deliver-the-ultimate-experience-in-immersive-visualization-and-modeling-9f52a4bd1443?source=post_page-----dc1afde54ced--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: (Modern) Speech Recognition and Speech Synthesis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Two technologies in particular that have been around for almost two decades
    and have a lot to offer in powering more natural human-computer interactions,
    are speech recognition and speech synthesis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Speech recognition, or ASR after Automatic Speech Recognition or STT after
    Speech-To-Text, converts spoken language into written text. It has two main broad
    applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Command a piece of hardware such as a smartphone or computer through spoken
    instructions, best if in a natural way. Think of how you use your smartphone,
    or Alexa or Siri via voice commands.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transcribing (and perhaps then displaying, storing or analyzing) what is being
    said in a conversation. Think about meeting transcription, video captioning, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both applications relate directly to the topic of this blog post, and are materialized
    for web programmers through the Web Speech API.
  prefs: []
  type: TYPE_NORMAL
- en: Speech synthesis, or TTS after Text-To-Speech, converts written text into spoken
    language. It allows computers and other devices to generate human-like speech,
    making information accessible audibly just as you see here on Medium articles.
  prefs: []
  type: TYPE_NORMAL
- en: Speech Synthesis finds application in a variety of scenarios, including accessibility
    features for visually impaired users, interactive voice response systems, and
    multimedia content enrichment.
  prefs: []
  type: TYPE_NORMAL
- en: Together, STT and TTS enable a more inclusive and versatile interaction with
    digital content, opening up opportunities for hands-free and screen-free operation
    and personalized user experiences.
  prefs: []
  type: TYPE_NORMAL
- en: Brief history of ASR and TTS technologies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Speech recognition and synthesis have evolved significantly over the years,
    especially with the integration of modern AI models and advancements in machine
    learning techniques. They date back to the mid-20th century when early attempts
    were made to use computers for language processing. Initial models faced challenges
    with accents, dialects, homophones, and speech nuances, both in producing and
    understanding speech. Advancements in statistical models and symbolic natural
    language processing gradually improved ASR systems, but a breakthrough occurred
    in the late 2010s with the introduction of transformers, especially for ASR: by
    leveraging on attention mechanisms, new methods for speech recognition could capture
    long-range dependencies and contextual understanding, thus dramatically improving
    the accuracy of speech-to-text conversion.'
  prefs: []
  type: TYPE_NORMAL
- en: STT is somewhat more complex than TTS, due to a larger number of variables to
    account for. The most modern methods, however, work remarkably well. Without going
    into details as this would be out of scope, modern STT involves a complex process
    with multiple stages and AI models working together. Key stages include pre-processing
    of audio input, feature extraction, phoneme extraction, language model decision-making,
    and decoding into a sequence of words. Modern ASR models often use transformers
    in all stages, preserving long-range couplings in information for better accuracy
    and coherence. The most advanced ASR systems include language model elements built
    right into their core modules; such native integration has proven essential for
    high transcription and recognition accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Modern ASR and TTS technologies can certainly do much more than mere speech
    recognition or synthesis, actually approaching a kin of “understanding of spoken
    language”. See, the most advanced STT and TTS models can transcribe audio to text
    or synthesize audio from text in multiple languages, some even being able of identifying
    languages automatically, adapting to context, detecting or simulating different
    speakers, annotating transcribed words with timestamps, dealing with punctuation
    and with non-verbal vocalization, allowing for customizable dictionaries of words,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: If you are getting excited about all these features and the possibility of exploiting
    them in your web app from the Web Speech API, there are quite a few limitations
    that you need to learn about, that will kind of disappoint you because what the
    browsers support is not state-of-the-art. Fortunately, though, coupling the Web
    Speech API with large language models can alleviate some of these problems, as
    many modern ASR and TTS technologies do and as I will show you here by using GPT-3.5-turbo
    or GPT-4 programmatically. And if that’s not enough, you will learn that there
    are companies offering their own (paid) APIs to perform ASR and TTS at the state
    of the art.
  prefs: []
  type: TYPE_NORMAL
- en: The Web Speech API, Its Parts, and its Availability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Web Speech API is a web standard that enables web applications to incorporate
    voice data into their functionality. It has two parts: SpeechRecognition, which
    allows speech input and recognition, and SpeechSynthesis, which allows speech
    output and synthesis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Web Speech API was first proposed by Google in 2010, and was implemented
    in Chrome 25 in 2013\. Since then, it has been supported by other browsers, with
    varying degrees of compatibility and functionality. And support as of December
    2023 is very heterogeneous, as caniuse.com reports, especially for speech recognition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/81b20b63c28b44cc97984a8bfb6555d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot from [https://caniuse.com/?search=web%20speech%20api](https://caniuse.com/?search=web+speech+api)
  prefs: []
  type: TYPE_NORMAL
- en: In my own experience, I have seen the API’s speech recognition modules work
    nicely only on Safari and Chrome, including their iOS and Android versions, respectively.
    Note that other chromium-based browsers such as Brave or Oculus Browser (the web
    browser of Meta’s Quest browsers for virtual reality) do not support speech recognition.
    This happens because Google’s speech recognition service is proprietary and requires
    a license to use, which Google does not grant to other browsers. Note on top of
    this that since speech recognition in Chrome is handled by a system based on cloud
    computing, browsers centered in user privacy such as Brave would face a hard decision
    to make, should Google allow them to run speech recognition with their resources.
    Oh and by the way, talking about privacy, note that all speech recognition taking
    place through the Web Speech API needs that the calling webpage be served via
    https!
  prefs: []
  type: TYPE_NORMAL
- en: Speech synthesis, contrary to speech recognition, is handled properly by all
    major browsers in computers (except the old Internet Explorer from Microsoft,
    now replaced by their Edge which does support it) and most browsers for smartphones.
    For TTS, https is not mandatory at the moment.
  prefs: []
  type: TYPE_NORMAL
- en: In what follows, I will focus specifically on the Web Speech API and how you
    can use it in Chrome, although in principle the core elements should work the
    same in all other browsers that support the API.
  prefs: []
  type: TYPE_NORMAL
- en: Using Chrome’s Web Speech API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To use the Web Speech API programmatically, you need to create an instance of
    the SpeechRecognition or SpeechSynthesis interface, depending on whether you want
    to use speech input or output -or you might well create both and use them in the
    same app!
  prefs: []
  type: TYPE_NORMAL
- en: Speech Recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For example, to create a speech recognition object, you can go as simple as
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'However, you better check that ASR is actually enabled in the browser. Then,
    the minimal block of code would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can use the methods and properties of the object as shown above to control
    the speech recognition service and put it to work in the best possible way. The
    most used commands and properties are **start**, **stop**, **abort**, **lang**,
    **interimResults**, etc. You can also add event listeners to handle the events
    that are fired by the object, such as **onstart**, **onend**, **onresult**, **onerror**,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: One particularly interesting property very worthy of attention, unfortunately
    for a negative reason, is that of “grammars”. Speech recognition “grammars” are
    supposed to allow a piece of code doing ASR to understand words that it would
    normally not understand or it would rank low. This functionality is especially
    important to correct pronunciation and the use of names, jargon, local expressions,
    etc. You will find some information [here](https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition/grammars)
    but I won’t dig into details because… it just doesn’t work, and many people are
    complaining about this; moreover, there’s a whole move to just drop grammars from
    the API, as no browser currently supports it well (see [here](https://github.com/WICG/speech-api/pull/58)).
  prefs: []
  type: TYPE_NORMAL
- en: To know more about the properties and events of the SpeechRecognition object,
    [check this out](https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition).
  prefs: []
  type: TYPE_NORMAL
- en: 'And for a global, complete example of how to use the Web Speech API for speech
    recognition, check out this official example that shows how to change a web page’s
    background color through spoken commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/mdn/dom-examples/blob/main/web-speech-api/speech-color-changer/script.js?source=post_page-----dc1afde54ced--------------------------------)
    [## dom-examples/web-speech-api/speech-color-changer/script.js at main · mdn/dom-examples'
  prefs: []
  type: TYPE_NORMAL
- en: Code examples that accompany various MDN DOM and Web API documentation pages
    …
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/mdn/dom-examples/blob/main/web-speech-api/speech-color-changer/script.js?source=post_page-----dc1afde54ced--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Speech Synthesis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similarly, to create a speech synthesis object, you can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you can create instances of the **SpeechSynthesisUtterance** interface,
    which represent the specific speech requests that you want to synthesize. You
    can set the properties of the utterance, such as text, voice, rate, pitch, volume,
    etc. You can use the methods and properties of the object to control the speech
    synthesis service, such as speak, pause, resume, cancel, getVoices, etc. You can
    also add event listeners to handle the events that are fired by the utterance,
    such as onstart, onend, onerror, etc. For example, to synthesize a text and log
    when it starts and ends, you can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'What I really like to do in my web apps is to have a function that can receive
    a string and just speak it up. I call this function speakUp(), and it looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Problems of ASR and TTS via the Web Speech API, and how to Correct Some of Them
    Using a Large Language Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Web Speech API is very easy to use, as you saw. It is also free and doesn’t
    even require any API key, nor does it have any limits in how much and often you
    can call it.
  prefs: []
  type: TYPE_NORMAL
- en: However, in trying it over and over I see that the system is down quite often,
    especially for ASR. Moreover, the API sometimes seems to switch off all by itself,
    with many people asking about this without clear solutions. Worst of all, although
    the free API is OK for some applications, it is far from the state of the art,
    regarding both speech recognition and synthesis. Compared to what I explained
    in the introduction about modern ASR systems in general, Chrome’s service for
    ASR can do very little!
  prefs: []
  type: TYPE_NORMAL
- en: Speech recognition is not very accurate or reliable, nor is it tolerant to accents
    and dialects. It is virtually useless even with just some noise. It doesn’t detect
    language automatically, it doesn’t detect multiple people talking in the same
    conversation, it handles punctuation ok but gets confused with vocalizations,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: The speech recognition service comes with privacy and security issues, as the
    voice data is transmitted to external servers or third parties without the user’s
    consent or knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: The speech synthesis service doesn’t have very natural or expressive voices,
    especially for languages other than English.
  prefs: []
  type: TYPE_NORMAL
- en: Besides, as we saw above the speech recognition and synthesis services are very
    dependent on the browser and the device, and may not be available or consistent
    across different platforms and regions.
  prefs: []
  type: TYPE_NORMAL
- en: And because of their limited power, they may have ethical and social implications
    such as bias, discrimination, deception, manipulation, or impersonation.
  prefs: []
  type: TYPE_NORMAL
- en: As a developer, and also as a user, you should be aware of these limitations
    and challenges, and use the Web Speech API responsibly and appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: Alleviating Problems of Speech Recognition by using a Large Language Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Of the above problems, those related to ASR are the most important ones. Solving
    many of these problems requires action at the most basic level when recognition
    is taking place. However, the most important problems related to transcription
    accuracy and jargon completion can be fixed with a large language model, as I’m
    doing via GPT-3.5-turbo or GPT-4, programmatically and right in JavaScript inside
    the web app.
  prefs: []
  type: TYPE_NORMAL
- en: '***How this works***'
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, one calls the language model with a prompt explaining what the
    inputs will look like and what the outputs should be for those inputs, followed
    by the actual (raw) transcription coming from the speech recognized by the API.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the [web app that triggered all this work](https://medium.com/towards-artificial-intelligence/coupling-four-ai-models-to-deliver-the-ultimate-experience-in-immersive-visualization-and-modeling-9f52a4bd1443),
    speech recognition is used to trigger commands. For example, if the user says
    “zoom in”, or “enlarge”, or a similar order, then the visualization (the app is
    for molecular graphics) must be scaled up. This app’s prompt looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see this in detail. Here’s the **System** element of the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: You receive texts from speech recognition and act accordingly by triggering
    commands, effectively correcting speech recognition as in the examples provided
    below. If you don’t understand the request or the request is not in the list,
    you run the command didntUnderstand().
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Then, see how the User/Assistant pairs provide examples of possible inputs that
    produce the same or a related output, for example “Zoom in” and “Enlarge” both
    result in a call to scale(+), while “Zoom out” calls scale(-).
  prefs: []
  type: TYPE_NORMAL
- en: 'In other pieces of the code you will find entries that “teach” the language
    model to correct inputs in a way that will prevent the program from crashing and
    at the same time increase the chances of producing the right output. For example,
    “minimize” as spoken by me is often recognized as “mini mice”, and “ANI” which
    is not a standard English word gets recognized as “Annie”. Then I can instruct
    the language model with examples like these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The trick works remarkably well, and this is no big wonder because at the end
    of the day, this is at the core of the most modern language models like Whisper,
    which embed language models directly into the speech recognition procedure!
  prefs: []
  type: TYPE_NORMAL
- en: 'On closing this section and just for completion, of course the full prompt
    must be inside the promise resolved from a fetch to the language model, which
    in my case is often GPT-3.5-turbo or GPT-4 from OpenAI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Some case uses for Chrome’s Web Speech API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Besides the app that triggered this series of blog posts ([here](https://pub.towardsai.net/coupling-four-ai-models-to-deliver-the-ultimate-experience-in-immersive-visualization-and-modeling-9f52a4bd1443)),
    I can share with you these other projects that use the speech recognition and/or
    synthesis APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, I used Chrome’s speech recognition capabilities together with
    GPT-3 to create a web app that writes e-mails from your spoken notes and guidelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pub.towardsai.net/a-web-app-for-automated-e-mail-writing-from-voice-notes-using-gpt-3-e8e98e4ffb6f?source=post_page-----dc1afde54ced--------------------------------)
    [## A Web App for Automated E-mail Writing From Voice Notes, Using GPT-3'
  prefs: []
  type: TYPE_NORMAL
- en: I coupled Chrome’s speech recognition engine with GPT-3 to create a web app
    that writes e-mails from your spoken notes…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pub.towardsai.net](https://pub.towardsai.net/a-web-app-for-automated-e-mail-writing-from-voice-notes-using-gpt-3-e8e98e4ffb6f?source=post_page-----dc1afde54ced--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, I show you how to build a chatGPT-like bot that can listen to you and
    reply orally; again, this couples Chrome’s Web Speech API for recognition and
    synthesis with GPT-3 as the language model powerling the chatbot’s “brain”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/coupling-gpt-3-with-speech-recognition-and-synthesis-to-achieve-a-fully-talking-chatbot-that-runs-abfcb7bf580?source=post_page-----dc1afde54ced--------------------------------)
    [## Coupling GPT-3 with speech recognition and synthesis to achieve a fully talking
    chatbot that runs…'
  prefs: []
  type: TYPE_NORMAL
- en: How I created this web app with which you can talk naturally with GPT-3 about
    any topic you want, all web-based in your…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/coupling-gpt-3-with-speech-recognition-and-synthesis-to-achieve-a-fully-talking-chatbot-that-runs-abfcb7bf580?source=post_page-----dc1afde54ced--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'In this other example as use Chrome’s speech recognition capability to control
    a web app, in this case using GPT-3 to cast spoken requests into commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/control-web-apps-via-natural-language-by-casting-speech-to-commands-with-gpt-3-113177f4eab1?source=post_page-----dc1afde54ced--------------------------------)
    [## Control web apps via natural language by casting speech to commands with GPT-3'
  prefs: []
  type: TYPE_NORMAL
- en: One last article showcasing practical applications of GPT3, with a full explanation
    of the workflow and details on the…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/control-web-apps-via-natural-language-by-casting-speech-to-commands-with-gpt-3-113177f4eab1?source=post_page-----dc1afde54ced--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Beyond the Web Speech API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I described above that some web browsers don’t support speech synthesis, most
    don’t support speech recognition. I also explained that Chrome’s built-in speech
    API is not great, especially for speech recognition, and that some of these limitations
    can be overcome with language models; however, not all problems can be solved
    with that strategy, and hence many features that modern ASR systems have are totally
    lacking.
  prefs: []
  type: TYPE_NORMAL
- en: In this context, it is important to know that browsers may support speech recognition
    and synthesis by using alternative services or solutions -paid in contrast to
    Chrome’s free API, but in principle more powerful.
  prefs: []
  type: TYPE_NORMAL
- en: Just to mention some companies offering this API service, check Gladia, Speechly,
    AssemblyAI, Deepgram, Spechmatics (this one showing off with a great example [here](https://www.speechmatics.com/)),
    and some others. Even Google has an ASR system separate from that they offer for
    free in Chrome, that certainly works much better, and other tech giants like Microsoft
    and AWS have their own products made available via APIs. You could even download
    OpenAI’s open-source Whisper and set a service to run it specifically for you,
    even customized to your needs -although for this you rather go straight away with
    a company that provides it via an API, such as [Gladia.io](https://www.gladia.io/).
  prefs: []
  type: TYPE_NORMAL
- en: Of course, unless you do a closed implementation (for example using Whisper
    on a local server), most of these services will require that you send audio to
    their servers, thus potentially compromising privacy. However, for non-sensitive
    tasks, they might be a perfect getaway, even with some quite low costs in many
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: Further reads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API?source=post_page-----dc1afde54ced--------------------------------)
    [## Web Speech API - Web APIs | MDN'
  prefs: []
  type: TYPE_NORMAL
- en: The Web Speech API enables you to incorporate voice data into web apps. The
    Web Speech API has two parts…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: developer.mozilla.org](https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API?source=post_page-----dc1afde54ced--------------------------------)
    [](https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API/Using_the_Web_Speech_API?source=post_page-----dc1afde54ced--------------------------------)
    [## Using the Web Speech API - Web APIs | MDN
  prefs: []
  type: TYPE_NORMAL
- en: Speech recognition involves receiving speech through a device's microphone,
    which is then checked by a speech…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: developer.mozilla.org](https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API/Using_the_Web_Speech_API?source=post_page-----dc1afde54ced--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '[***www.lucianoabriata.com***](https://www.lucianoabriata.com/) *I write about
    everything that lies in my broad sphere of interests: nature, science, technology,
    programming, etc.* [***Subscribe to get my new stories***](https://lucianosphere.medium.com/subscribe)
    ***by email****. To* ***consult about small jobs*** *check my* [***services page
    here***](https://lucianoabriata.altervista.org/services/index.html)*. You can*
    [***contact me here***](https://lucianoabriata.altervista.org/office/contact.html)***.***
    *You can* [***tip me here***](https://paypal.me/LAbriata)*.*'
  prefs: []
  type: TYPE_NORMAL
