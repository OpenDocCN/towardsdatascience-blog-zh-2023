- en: 5 Fantastic Data Pipeline Orchestration Tools For R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/5-fantastic-data-pipeline-orchestration-tools-for-r-f34ab71b1730](https://towardsdatascience.com/5-fantastic-data-pipeline-orchestration-tools-for-r-f34ab71b1730)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explore Excellent Options for Data Pipeline Orchestration for R Users
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://chengzhizhao.medium.com/?source=post_page-----f34ab71b1730--------------------------------)[![Chengzhi
    Zhao](../Images/186bba91822dbcc0f926426e56faf543.png)](https://chengzhizhao.medium.com/?source=post_page-----f34ab71b1730--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f34ab71b1730--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f34ab71b1730--------------------------------)
    [Chengzhi Zhao](https://chengzhizhao.medium.com/?source=post_page-----f34ab71b1730--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f34ab71b1730--------------------------------)
    ¬∑11 min read¬∑Jan 30, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c2f54ada9cd590d65eda4378e24afa28.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Daria Nepriakhina üá∫üá¶](https://unsplash.com/ko/@epicantus?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/kXDHR_bXIZo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: The data pipeline orchestration tool is critical for producing healthy and reliable
    data-driven decisions. R is one of the popular languages for data scientists.
    With R‚Äôs exceptional packages, the R programming language is great for data manipulation,
    statistical analysis, and visualization.
  prefs: []
  type: TYPE_NORMAL
- en: One pattern that often brings data scientists‚Äô R local script to production
    is to rewrite using Python or Scala (Spark), then schedule the data pipeline and
    model building via modern data pipeline orchestration tools like Apache Airflow.
  prefs: []
  type: TYPE_NORMAL
- en: However, many modern data orchestration projects like Apache Airflow, Prefect,
    and Luigi are Python-based. Can they work seamlessly with R? Can you write in
    R to define a DAG? In this article, let‚Äôs explore the popular data pipeline orchestration
    tool for R scripts and review which fits your use case.
  prefs: []
  type: TYPE_NORMAL
- en: The Key Components of the Successful Data Pipeline Orchestration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data pipeline orchestration can be broken down into three main components from
    my experience: **DAG (dependencies), Scheduler, and Plugins.**'
  prefs: []
  type: TYPE_NORMAL
- en: DAG (Directed acyclic graph)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Dag defines the blueprint of the data pipeline**. It provides a direction
    for the execution path. At the same time, you can track back the dependencies
    by looking at the dag.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Why is DAG crucial for the success of any data pipeline?* It is because working
    with data needs to have an ordering to extract the insights from the data. This
    order cannot be changed from a business rule perspective. Otherwise, the output
    from the data is useless or errors out.'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at each node in the directed acyclic graph as an individual function,
    DAG provides an alignment that the current node has to follow the rules defined
    by upstream nodes. For example, the current node is only triggered if all the
    upstream nodes are successful; or the current node is executable when one of the
    upstream nodes fails.
  prefs: []
  type: TYPE_NORMAL
- en: The DAG conveniently provides a view of data lineage. It enhances visibility
    while significantly streamlining the ability to trace errors in the data pipeline.
    When it comes to the time that the data pipeline encounters the unpleasant error
    for the morning on-call, the instance of the DAG execution can quickly point out
    where the error occurs.
  prefs: []
  type: TYPE_NORMAL
- en: The power to visualize DAG as a blueprint and its instance at run time to check
    the job running status is crucial nowadays for any data orchestration tools.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Scheduler is the driver for executing the data pipeline.** A scheduler can
    be simple as a cron job. A more complex scheduler involves building your own,
    like the one in Airflow, which manages all the task states and aggressively snapshots.'
  prefs: []
  type: TYPE_NORMAL
- en: '*What does the scheduler do?* The scheduler is a daemon, which can be treated
    as a background process. It is supposed to run 24/7 and monitor a time or event
    when it reaches that point. If the time or event for scheduling is called, execute
    the task and watch for the next one.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af4e7a2e23d3bf464651a20a259de223.png)'
  prefs: []
  type: TYPE_IMG
- en: Scheduling Cycle | Image By Author
  prefs: []
  type: TYPE_NORMAL
- en: Plugins
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Plugins are for extensibility and are considered as the potential for the
    data pipeline.** It‚Äôs common to leverage existing packages instead of reinventing
    the wheel. The richness of the plugins of the orchestration tools can save you
    time to concentrate on business logic than spending days googling & coding ‚Äú*How
    to write script submit a Spark job to EMR?*‚Äù'
  prefs: []
  type: TYPE_NORMAL
- en: If a data orchestration tool grows, it attracts more vendors and additional
    community developers to add more plugins to draw additional users. It is also
    expensive to migrate to other data pipeline orchestration tools.
  prefs: []
  type: TYPE_NORMAL
- en: Options Not Cover
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[taskscheduleR](https://cran.r-project.org/web/packages/taskscheduleR/readme/README.html):
    a Windows-specific scheduler with the Windows task scheduler. If you are on Windows,
    definitely an option to explore.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/kirillseva/ruigi](https://github.com/kirillseva/ruigi)
    ‚Äî It is an admirable attempt. However, the project seems idle, and no more activities
    since May 26, 2019 Git'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore Data Pipeline Orchestration Tools For R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will discuss the following 5 different tools, which fit distinct use cases.
  prefs: []
  type: TYPE_NORMAL
- en: '[cronR](https://github.com/bnosac/cronR)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[targets](https://github.com/ropensci/targets)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Kestra](https://kestra.io/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Apache Airflow](https://airflow.apache.org/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Mage](https://www.mage.ai/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. cronR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the key components for successful data pipeline orchestration is scheduling.
    A scheduler gives you peace of mind to run a data pipeline without human intervention.
    To schedule an R script, cronR is the first solution you‚Äôd explore.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/bnosac/cronR?source=post_page-----f34ab71b1730--------------------------------)
    [## GitHub - bnosac/cronR: A simple R package for managing your cron jobs.'
  prefs: []
  type: TYPE_NORMAL
- en: Schedule R scripts/processes with the cron scheduler. This allows R users working
    on Unix/Linux to automate R processes‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/bnosac/cronR?source=post_page-----f34ab71b1730--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The package enhanced a set of wrappers to `crontab` make adopting more straightforward
    using R only. Thus, you don‚Äôt need to worry about setting up the crontab, and
    the `cronR` provides an interface that reduces the complexity.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Usage Suggestion**'
  prefs: []
  type: TYPE_NORMAL
- en: This option is a quick and lightweight solution if you only want to schedule
    the R script. It is suitable for use cases such as ad-hoc scheduling, simple dependencies,
    or less state management involved.
  prefs: []
  type: TYPE_NORMAL
- en: '**Limitation**'
  prefs: []
  type: TYPE_NORMAL
- en: Since cronR only gives you a scheduler. You‚Äôd need to build workflow dependencies
    by yourself. It‚Äôs doable if a single R script is manageable. However, if the script‚Äôs
    size becomes enormous and intermedia stages are required, you‚Äôd want to break
    it down. Those are the times cronR isn‚Äôt sufficient as it only handles the scheduling
    part without DAG definition and plugins.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. targets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `[targets](https://github.com/ropensci/targets)` package is a [Make](https://www.gnu.org/software/make/)-like
    pipeline toolkit for Statistics and data science in R.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[targets](https://github.com/ropensci/targets) is initiated as an R programming
    language for data pipelines. You can easily define a DAG to create the dependencies
    graph. The main goal for targets is to provide reproducible workflow. It doesn‚Äôt
    come with a scheduler, but connecting with the other tools I mentioned here shouldn''t
    be challenging to schedule the DAG generated from targets.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The pipeline and the function definition are divided into two R files. You can
    build the dependencies tree within `_targets.R` file and visualize it by `tar_visnetwork()`
    and get a DAG automatically.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b33dea61e11ce0f8c89ea0cf57ee81d.png)'
  prefs: []
  type: TYPE_IMG
- en: targets visualize DAG | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Usage Suggestion**'
  prefs: []
  type: TYPE_NORMAL
- en: You‚Äôd need native support utilizing R for the data pipeline. The targets package
    can help R users to improve their efficiency in their day-to-day data analysis
    work. It doesn‚Äôt require an additional daemon running in the background to get
    a DAG for visualization and running on demand. Suppose you are looking for a solution
    you can run manually and seek a DAG management solution. targets is an excellent
    option for R users.
  prefs: []
  type: TYPE_NORMAL
- en: '**Limitation**'
  prefs: []
  type: TYPE_NORMAL
- en: If you decide to go with targets, you have a powerful DAG management tool. Nevertheless,
    you‚Äôd need to get a scheduler and additional plugins to make it a data pipeline
    orchestrator in production. Combining with a scheduler like cronR can give you
    a purely R solution.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Kestra
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Kestra](https://kestra.io/) is a generic data pipeline orchestration tool.
    It currently only supports three types of scripts: *Bash, Node, and Python*.'
  prefs: []
  type: TYPE_NORMAL
- en: Although the R language isn‚Äôt included as the supported script. You can still
    achieve the goal of orchestrating the R script by using the bash script with `Rscript`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A more complex DAG can be set up using [Flowable Task](https://kestra.io/docs/developer-guide/flowable/)
    in the YAML file. The scheduling is also done in the YAML file by [Schedule](https://kestra.io/docs/developer-guide/triggers/schedule.html).
  prefs: []
  type: TYPE_NORMAL
- en: '**Usage Suggestion**'
  prefs: []
  type: TYPE_NORMAL
- en: As R users, Kestra allows you to orchestrate the R code via the bash command.
    Additionally, Kestra has the flexibility to run the data pipeline in various languages.
    It shows you a feeling of a modern version of [Oozie](https://oozie.apache.org/).
    If you are familiar with Oozie, Kestra should be much simpler to onboard.
  prefs: []
  type: TYPE_NORMAL
- en: '**Limitation**'
  prefs: []
  type: TYPE_NORMAL
- en: Running R isn‚Äôt natively supported. Retrieving the metadata at run time isn‚Äôt
    trivial for just using `Rscript` command. Everything is based on `type` , finding
    the proper core or plugin to develop more effectively might take extra time to
    learn.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Apache Airflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Airflow is by far the most popular data pipeline orchestration tool. However,
    to write a DAG, you‚Äôd need to write Python. You can use the Airflow operator to
    execute your R script. However, to define DAG, R is not part of the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: The Airflow community had a proposal to create an `ROperator`. The proposal
    is to leverage [rpy2](https://rpy2.github.io/), which creates an interface for
    **R** running embedded in a Python process. The core idea is to pass the `r_command`,
    then copy the R script to a temporary file and source it.
  prefs: []
  type: TYPE_NORMAL
- en: There are many people interested in this pull request from the R users.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/apache/airflow/pull/3115?source=post_page-----f34ab71b1730--------------------------------)
    [## [AIRFLOW-2193] Add ROperator for using R by briandconnelly ¬∑ Pull Request
    #3115 ¬∑ apache/airflow'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you have checked all steps below. JIRA My PR addresses the following
    Airflow JIRA issues and references them‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/apache/airflow/pull/3115?source=post_page-----f34ab71b1730--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: However, it didn‚Äôt land. Scanning through the pull request. We noticed the R
    language support isn‚Äôt something Airflow CI does at that point, and other options
    could execute the R scripts, so the priority to have a `ROperator` is low. There
    are a few options if you‚Äôd want to stick with the Airflow ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Use `BashOperator` and to run R code. If you can run your R script from the
    terminal, the `BashOperator` meets that requirement. Using the `BashOperator`
    also makes it easy to build the DAG relationship and pass arguments to `bash_command`
    , and add more complex logic like retry and email alert
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Run R in Docker containers (see [rocker](https://www.rocker-project.org/)) and
    use the `DockerOperator` . This option is similar to `BashOperator` . A nice thing
    about using a Docker container is you spend less time configuring R script to
    execute within the same environment as Airflow. The Docker container gives you
    a fresh environment for R to run each time. It‚Äôs a nice and clean solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*[If you still want to have a dedicated* `*ROperator*`*]* you can copy and
    paste `r_operator.py` from the pull request above and make them add them from
    your Airflow infra team.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar situation for [Prefect](https://github.com/PrefectHQ/prefect). it has
    an open pull request without resolution, and the PR has been marked as low priority.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/PrefectHQ/prefect/issues/5449?source=post_page-----f34ab71b1730--------------------------------)
    [## Add `RTask` to the Task Library ¬∑ Issue #5449 ¬∑ PrefectHQ/prefect'
  prefs: []
  type: TYPE_NORMAL
- en: You can't perform that action at this time. You signed in with another tab or
    window. You signed out in another tab or‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/PrefectHQ/prefect/issues/5449?source=post_page-----f34ab71b1730--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Many modern data pipeline orchestrations are built using Python. However, when
    it comes to leveraging another popular data-related language like R, the possibility
    of prioritizing it becomes low as it didn‚Äôt start with other languages while designing
    it. Later, the project becomes too big to make fundamental changes and requires
    tremendous effort from the ground.
  prefs: []
  type: TYPE_NORMAL
- en: '**Usage Suggestion**'
  prefs: []
  type: TYPE_NORMAL
- en: This option is good if you already have Airflow infrastructure as the data pipeline
    orchestrator. Airflow provides the three key elements of a successful data pipeline
    orchestration platform. You have the foundation and resources set up, and running
    R is possible with the abovementioned options.
  prefs: []
  type: TYPE_NORMAL
- en: '**Limitation**'
  prefs: []
  type: TYPE_NORMAL
- en: R isn‚Äôt the first citizen in Airflow. In Airflow, running R isn‚Äôt natively supported.
    You have multiple workarounds, but R is still treated as a foreign language. Whether
    you decide to go with `BashOperator` or `DockerOperator` or even fork that PR,
    there is still additional support you‚Äôd need to get to the data infra team to
    help you to make the R script runnable in Airflow.
  prefs: []
  type: TYPE_NORMAL
- en: Another limitation is that it is not straightforward to pull the Airflow macros
    (Airflow‚Äôs metadata) at run time with R. You can still use a nontrivial solution
    by querying Airflow's backend. However, it isn‚Äôt user-friendly for R users without
    depth knowledge of Airflow.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Mage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mage is the new player in the data pipeline orchestration. The biggest win for
    our discussion is that **Mage recognizes R as part of its support language by
    default and enables users to define DAG regardless of the choice of languages
    (python/SQL/R at this point).**
  prefs: []
  type: TYPE_NORMAL
- en: This is a milestone for R users. UserswholoveR don‚Äôt have to switch to Python
    syntax when they define a DAG that wraps the R script in a limited supported tool.
  prefs: []
  type: TYPE_NORMAL
- en: Mage allows users to write the main ETL (Extraction, Transformation, and Loading)
    blocks using R. Mage constructs the DAG by maintaining DAG dependencies relationships
    in a YAML file. This becomes a flexible option by bypassing the choices of programming
    language. You can also visualize the DAG along with the R code block while developing
    your DAG
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/173007d39faa27f767b129ceb431a85e.png)'
  prefs: []
  type: TYPE_IMG
- en: Mage Pipeline Edit Mode Using R | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Below are the 3 main blocks I used to demonstrate how to use R to code the pipeline
    and build the DAG in Mage. Furthermore, you can access the scheduler metadata
    like `execution_date` easily in R.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Once the pipeline has been developed in Mage, you can attach a trigger by scheduling
    the pipeline with a crontab.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c0338e2a94a4208e2a79a43b4d2b92c.png)'
  prefs: []
  type: TYPE_IMG
- en: Mage Pipeline Scheduling | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Internally, Mage still uses Python as its core and parses R script into a `tmp`
    file, then run the Rscript command with that file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If you‚Äôd want to learn more about Mage as an alternative to Apache Airflow,
    I wrote an article on it.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://chengzhizhao.medium.com/is-apache-airflow-due-for-replacement-the-first-impression-of-mage-ai-ade8208fb2a0?source=post_page-----f34ab71b1730--------------------------------)
    [## Is Apache Airflow Due for Replacement? The First Impression Of mage-ai'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to mage-ai as the alternative to Apache Airflow for data engineers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: chengzhizhao.medium.com](https://chengzhizhao.medium.com/is-apache-airflow-due-for-replacement-the-first-impression-of-mage-ai-ade8208fb2a0?source=post_page-----f34ab71b1730--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Usage Suggestion**'
  prefs: []
  type: TYPE_NORMAL
- en: R becomes the first citizen in Mage. You can write the R blocks and access the
    scheduler metadata smoothly without worrying about how to inject or query the
    backend. Developing in Mage is also interactive. Engineers can rapidly iterate
    on testing while developing; They can visualize the result than having a giant
    DAG to debug.
  prefs: []
  type: TYPE_NORMAL
- en: '**Limitation**'
  prefs: []
  type: TYPE_NORMAL
- en: Mage is a new project founded in 2021 and is in its early stages. A lot of documentation
    needs to improve. Additionally, the number of plugins in Mage isn‚Äôt comparable
    to Airflow.
  prefs: []
  type: TYPE_NORMAL
- en: '**Final Thoughts**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many data pipeline orchestration options haven‚Äôt been uncovered here. For R
    users, better integration with R language for data pipeline orchestration reduces
    the pressure to move initial data analysis to the data pipeline in production.
    I hope the options here can give you better insights into various data pipeline
    orchestration tools for R users.
  prefs: []
  type: TYPE_NORMAL
- en: 'I hope this story is helpful to you. This article is **part of a series** of
    my engineering & data science stories that currently consist of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chengzhi Zhao](../Images/51b8d26809e870b4733e4e5b6d982a9f.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Chengzhi Zhao](https://chengzhizhao.medium.com/?source=post_page-----f34ab71b1730--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Data Engineering & Data Science Stories
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://chengzhizhao.medium.com/list/data-engineering-data-science-stories-ddab37f718e7?source=post_page-----f34ab71b1730--------------------------------)53
    stories![](../Images/8b5085966553259eef85cc643e6907fa.png)![](../Images/9dcdca1fc00a5694849b2c6f36f038d4.png)![](../Images/2a6b2af56aa4d87fa1c30407e49c78f7.png)'
  prefs: []
  type: TYPE_NORMAL
- en: You can also [**subscribe to my new articles**](https://chengzhizhao.medium.com/subscribe)
    or become a [**referred Medium member**](https://chengzhizhao.medium.com/membership)who
    gets unlimited access to all the stories on Medium.
  prefs: []
  type: TYPE_NORMAL
- en: In case of questions/comments, **do not hesitate to write in the comments**
    of this story or **reach me directly** through [Linkedin](https://www.linkedin.com/in/chengzhizhao/)
    or [Twitter](https://twitter.com/ChengzhiZhao).
  prefs: []
  type: TYPE_NORMAL
