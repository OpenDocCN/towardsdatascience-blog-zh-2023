- en: 'Sklearn Tutorial: Module 4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/sklearn-tutorial-module-4-1e1a50e5247d](https://towardsdatascience.com/sklearn-tutorial-module-4-1e1a50e5247d)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Linear models, handling non-linearity, and regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)[![Yoann
    Mocquin](../Images/b30a0f70c56972aabd2bc0a74baa90bb.png)](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1e1a50e5247d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1e1a50e5247d--------------------------------)
    [Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e1a50e5247d--------------------------------)
    ·14 min read·Dec 22, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the **fourth** post in my scikit-learn tutorial series. If you didn’t
    catch them, I strongly recommend my first three posts — it’ll be way easier to
    follow along:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Yoann Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Sklearn tutorial
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://mocquin.medium.com/list/sklearn-tutorial-2e46a0e06b39?source=post_page-----1e1a50e5247d--------------------------------)9
    stories![](../Images/4ffe6868fb22c241a959bd5d5a9fd5d7.png)![](../Images/8aa32b00faa0ef7376e121ba9c9ffdb7.png)![](../Images/9f986423d7983bc08fc2073534603c35.png)'
  prefs: []
  type: TYPE_NORMAL
- en: This 4th module introduces the concept of **linear models**, using the infamous
    **linear regression** and **logistic regression** models as working examples.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to these basic linear models, we show how to use feature engineering
    to **handle nonlinear problems using only linear models,** as well as the concept
    of **regularization** in order to prevent overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Altogether, these concepts enable us to create very simple yet powerful models,
    capable of handling a lot of ML problems with fine-tuned hyperparameters, without
    overfitting, while handling nonlinear problems.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1fc42d52ce3838258d306c1ef042f157.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Roman Synkevych](https://unsplash.com/@synkevych?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '*All graphs and images are made by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Linear models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Linear models are models that “fit” or “learn” by setting coefficients such
    that they eventually only rely on a linear combination of the input features.**
    In other words, if the input data is made of N features f_1 to f_N, the model
    at some point is based on the linear combination:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8be0af90148c270f2a0b0d1479b1059d.png)'
  prefs: []
  type: TYPE_IMG
- en: The coefficients the model learns are the N+1 coefficients beta. The coefficient
    beta_0 represent an offset, a constant value in the output whatever the values
    in the input. The idea behind such models is that the “truth” can be approximated
    with a linear relationship between the inputs and the output.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of regression problems where we want to predict a numerical value
    from the inputs, one of the simplest and well known linear model is the linear
    regression. You most likely have done hundreds of linear regression already (by
    hand, in excel or python).
  prefs: []
  type: TYPE_NORMAL
- en: In the case of classification problem, where we want to predit a category from
    the inputs, the simplest and well known linear model is the logistic regression
    (don’t get fooled by the “regression” in “logistic regression”, it really deals
    with classification).
  prefs: []
  type: TYPE_NORMAL
- en: Many other linear models exists, for example Support Vector Regression and Support
    Vector Classification, as well as lots of variants of linear regression and logistic
    regression. They could all be the subject of a series of posts. The idea here
    is not to review them in depth, but to show their basic usage and limitations
    (although my completeness syndrom will make me detail them a bit).
  prefs: []
  type: TYPE_NORMAL
- en: '**Important note:** one of the committed stances of sklearn is to provide models
    that work out of the box, so newcomers can have running code quickly (and not
    spend too much time setting up framework gimmicks or have many errors to deal
    with when setting up new models)—I’d still recommend reading the documentation
    extensively as it is well written and you’ll learn a lot both about the python
    API as well as the mathematics and good practices.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear model for regression: linear regression'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Linear regression is the most well known linear regression model. As stated
    above, the idea is to approximate as best as possible the output y from a linear
    combination of the inputs f_i:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8be0af90148c270f2a0b0d1479b1059d.png)'
  prefs: []
  type: TYPE_IMG
- en: One of the reasons of the popularity of linear regression and linear models
    in general is that they can be handle with matrices, since matrices are representations
    of linear operations. In particular, one the possible ways (and the most used
    in science in general, not particularly in ML) to learn the coefficients beta
    is to use the Ordinary Least Squares method.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Ordinary Least Squares method** consists in selecting the one and best
    vector of beta such that the sum of squared errors is minimal: this method has
    the advantage to be easily interpretable (the model “minimizes” the squared distance
    between the data and its predictions) and it has a closed-form solution (so no
    numerical optimization method is needed, it basically is just matrix multiplication
    and inversion). If you check the documention of sklearn LinearRegression, you’ll
    see that its this method that is implemented.'
  prefs: []
  type: TYPE_NORMAL
- en: Many other methods exists in order to fit a linear regression, which all lead
    to “variants” of that simplest model. As we’ll see below, ridge regression and
    lasso regression are part of such variants.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that linear regression allows to do polynomial regression if a small preprocessing
    step is applied. Indeed a polynomial is just like a linear combination of monomial
    of inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8be0af90148c270f2a0b0d1479b1059d.png)'
  prefs: []
  type: TYPE_IMG
- en: so a polynomial can be written
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/48ad8a447622c97a20ae21c581f4a645.png)'
  prefs: []
  type: TYPE_IMG
- en: in the case of an univariate (a single input feature). To do so we simply generate
    a new input matrix made of all the polynomial variables we want (the X¹, X², etc
    for univariate problem, and even cross-variables in the case of multivariate polynomial
    like X_1 X_2, X_1 X_3, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simple example of a linear regression for a 1d input feature, so
    the model actually learn only beta0 and beta1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9bafb0f6b3d9bcf0fae2a03c627e5488.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To give you a bit more information about the linear regression model in sklearn:'
  prefs: []
  type: TYPE_NORMAL
- en: you can tune the linear regression to handle the offset beta0 or not by setting
    the `fit_intercept` hyperparameter `LinearRegression(fit_intercept=True)`. If
    using False, the model expect target y to be centered, that is to have mean 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: once the model has been fitted, it learned the coefficients betas. You can inspect
    them using `model.intercept_` and `model.coefficients_`. Remember that in the
    sklearn API, the learned parameters are suffixed with an underscore “_”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the default score for linear regression is the R² coefficients which translates
    how well the fitted model “explains” the variability of the dataset. Of course,
    you can also import any score function from the metrics module and compute other
    scores using for example `from sklearn.metric import mean_absolute_error; mean_absolute_error(y_true,
    y_pred)`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linear model for classification: logistic regression'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The equivalent of linear regression for classification problem is logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is pretty simple: create a linear combination y that when feeded of
    the logistic function best separates the target classes. Like the linear regression,
    the linear combination y can have any value — but to meet our context of classification
    it is feeded to the logistic function which is an S-shaped function that takes
    any real input and maps it to the [0, 1] interval. This interval is then associated
    with the target classes, where 0 corresponds to a class and +1 to the other.'
  prefs: []
  type: TYPE_NORMAL
- en: In other words, if a sample is mapped to a very negative value from the linear
    combination, it’ll be heavily associated with class 0\. As the y value increases
    and approaches 0.5, the target class becomes “uncertain”. Then if the linear combination
    y keeps increasing above 0.5, it will be mapped to class +1\. In this case, we
    say that 0.5 is the classifcation threshold. Note that some other similar algorithms
    use rather the [-1,1] mapping interval with a threshold value of 0\. Those are
    basicaly jsut conventions and won’t change the model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'So we could write the model as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d7ca4d291f7b3aa0436ea76e0654385.png)'
  prefs: []
  type: TYPE_IMG
- en: where x represents a sample vector of length N with features f_1 to f_N, and
    y is the linear combination of that sample with the model’s coefficients which
    can have any value, and that value is mapped to the [0–1] interval thanks to the
    logistic function.
  prefs: []
  type: TYPE_NORMAL
- en: To express it again differently, the probability of a sample to belong to a
    certain class is linked to its corresponding linear combination value y. The final
    corresponding class is simply the closest -or most probable- class, based on its
    position relative to the threshold.
  prefs: []
  type: TYPE_NORMAL
- en: In sklearn terms, the probability is computed with the `.predict_proba` which
    returns an array of floats that sum to 1 to represent probabilities to belong
    to a class. On the other hand, the `.predict` returns a class, which corresponds
    to the most probable class of the `.predict_proba`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see a simple 1D example: again, the linear model only has a single input
    to work on, so the X-axis can be used to plot either the feature value, or the
    y linear combination (beta1X+beta0):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/cf36894f9cb19d485d8f10f700a62652.png)'
  prefs: []
  type: TYPE_IMG
- en: Here the green line corresponds to the linear combination of the input features.
    It corresponds to the logistic value of the linear combination for a given sample
    with value x. By tunning the linear coefficients, this green lines shape and position
    moves to better match the train samples. It is then used to predict the class
    and probability of new samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand, let’s see a 2d example, which is kinda more suited for
    visual linear classification example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/75acb6e47fa77181a95a9ff429cf6d93.png)'
  prefs: []
  type: TYPE_IMG
- en: This example show how a 2d input feature is splitted by the model by a 1D line.
    This “line” represents the logistic function, so that above the threshold the
    samples belong to one class, and on the other side they belong to the other. The
    important idea here is to expand the reasoning from the previous example to higher
    dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like before, here are additionnal info about the logistic regression model
    in sklearn:'
  prefs: []
  type: TYPE_NORMAL
- en: LogisticRegression takes more hyperparameters, including `fit_intercept` like
    linear regression, but also additional parameters that allow to tune regularization
    — we’ll see about those further below
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: also like linear regression, the learned coefficients can be accessed with `model.coef_`
    and `model.intercept_`. Additionnaly, you can get a list of the encountered classes
    with `model.classes_`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'the default score is the accuracy, which is simply a percentage of good classification:
    0 corresponds to no good prediction and 1 corresponds to all good predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, there are a lot of things to say about linear models for classication,
    but the point here is just to provide simple example. To learn more about LogisticRegression,
    I strongly encourage you to go check the user guide of sklearn.
  prefs: []
  type: TYPE_NORMAL
- en: Handling non-linear data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we have seen examples of linear regression and logistic for synthetic
    data that were indeed linera. In other words, the truth we try to approximate
    with a linear model was indeed linear. But that almost never happens with real
    data, where the systems we try to model and reproduce are pretty non-linear.
  prefs: []
  type: TYPE_NORMAL
- en: So does that means that the linear models fall short ? Actually no, there is
    a workaround.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to using natively non-linear models (models that handle non-linear
    data by design), we can still use our linear models by creating new features in
    the input data that hold some non-linearity.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we are going to use the same models, but with “bigger” input
    data matrix, where we add “ourselves” new features that contains non-linear relations
    between the input features.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good simple example is a polynomial regression as introduced above. Say we
    want to fit a target y that is non-linear with respect to a single feature x.
    With standard linear model, this polynomial regression is simply a beta1 x + beta0
    regression. If instead we create new features, say x² and x³, the input matrix
    has now 3 features, and the linear regression can use the relation y=beta3 x³
    + beta2 x² + beta_1x + beta0 to fit the target. See the following example :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1b4f46459e0f400a89fd1c852e897c02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In particular, let’s inspect the coefficients for both the linear regression
    and the polynomial regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: For the simple linear regression we only get the beta1 value and beta0 intercept,
    but for the polynomial regressor, we get 3 coefficients corresponding to beta3,
    beta2 and beta1, as well as the beta0 intercept.
  prefs: []
  type: TYPE_NORMAL
- en: Using PolynomialFeatures is just one of many possibilities to create new feature
    that contain non-linearity. Other options are using KBinsDiscretizer (especially
    with `encode=’onehot`), the SplineTransformer or kernel approaches with Nystroem
    or the kernel trick implemented in some models (like Support Vector Machines models
    with SVR for regression and SVC for classification).
  prefs: []
  type: TYPE_NORMAL
- en: 'The approach is always the same: create new features that are non-linear and
    add them to the input data so the linear models can use them to fit complex y
    target. And the good news is that in sklearn all the approaches are implemented
    either as preprocessing steps in a pipeline, or are built-in the estimator models.'
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we have seen how to use basic linear model, both on linear problems and
    non-linear problems by adding new non-linear features.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization consists in changing or tweaking the way models learn, usually
    in changing the objective/cost function, in order to keep their complexity not
    to high.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, it is often implemented by adding a term in the cost function
    of the problem. For example one of the simplest example of regularization is that
    of linear regression, in which case it is called a “Ridge regression”. The classic
    linear regression cost function is given by the mean (or sum) of the squared errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f720666faaedd814ba8f2448d761adf8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With regularization, the cost function includes an additional term, that consists
    in the L2 norm of the vector beta:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1537eaa63ad21eaca224d146114108f.png)'
  prefs: []
  type: TYPE_IMG
- en: The norm of the coefficient vector is weighted by the alpha hyperparameter,
    so that we can modify by how much its norm should impute of the final solution.
    This way, in the optimization/learning process, the coefficients of beta won’t
    go arbitrary large and instead a good balance between its norm and the errors
    will be found.
  prefs: []
  type: TYPE_NORMAL
- en: This concept can be applied to pretty much any other models, including logistic
    regression of course.
  prefs: []
  type: TYPE_NORMAL
- en: 'But let’s go a bit further regarding regularization: just like we saw how it
    is important to tune hyperparameters in a pipeline/model, the alpha parameter
    of the ridge regression should be optimized (and this applies to any regularization
    parameter).'
  prefs: []
  type: TYPE_NORMAL
- en: To do so we can a GridSearchCV or RandomSearchCV as seen in the previous module,
    but since it is so common to optimize the alpha parameter of a Ridge regression,
    sklearn provide a RidgeCV model that take a list of alpha values to test and select
    using cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'So let’s sum up the 4 approaches to handle regularization for a linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: No regularization, using LinearRegression()
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Standard, non-optimized ridge regression, using Ridge(), equivalent to alpha=1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimized ridge regression using GridSearch or RandomSearch
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ridge regression with builtin optimization with RidgeCV
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s see visually how regularization influences the coefficients of linear
    regression, hence using a ridge model. In the following example, we use a 5 degree
    polynomial feature expansion to do a linear regression, with a regularization
    term through the Ridge model and its alpha hyperparameter to control the strengh
    of regularization.
  prefs: []
  type: TYPE_NORMAL
- en: '**With alpha=0**, there is no regularization, we get the classic linear regression
    results. Since we use up to degree 5 features to regress a noisy linear relation,
    the model tends to overfit a bit, and the linear coefficients span a great interval
    (here between -1500 to +1500).'
  prefs: []
  type: TYPE_NORMAL
- en: '**With alpha=1**, we get “mild” regularization. The model overfits way less
    and the coefficients amplitudes are quite smaller.'
  prefs: []
  type: TYPE_NORMAL
- en: '**With alpha=100**, we get very hard regularization, so the coefficients are
    not allowed to grow a lot and the model tends to underfit.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7bf1d1f00b3e02d1817902ba37ae5923.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s go a bit further and inspect how the train score and test score evolve
    with the value of alpha:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7c4f8a6d9dbb0f9f923e42481fe67548.png)'
  prefs: []
  type: TYPE_IMG
- en: For a degree 10 polynomial, the optimum regularization coefficient alpha seems
    to be between 0.01 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, remember that just like creating new feature to handle non-linearity
    can be applied to pretty much any model, regularization also can be included in
    most models (including logistic regression with its C parameter).
  prefs: []
  type: TYPE_NORMAL
- en: Wrapup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this 4th post, we saw:'
  prefs: []
  type: TYPE_NORMAL
- en: the most important linear models namely linear regression and logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: how we can handle non-linear problems while using such linear models by creating
    new features. This can be done for example by creating polynomial feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: how regularization can help control the complexity of models by adding a regularization
    term in the objective function, such that the linear coefficients cannot go arbitrarily
    large
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You might like some of my other posts, make sure to check them out:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Yoann Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Fourier-transforms for time-series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://mocquin.medium.com/list/fouriertransforms-for-timeseries-ed423e3f38ad?source=post_page-----1e1a50e5247d--------------------------------)4
    stories![](../Images/86efd63d329650eb9b6d7c33625d6884.png)![](../Images/c693e4e596df5c1a8ef1b0fb3777d7ac.png)![](../Images/b6bc5330fb2d92bc3aad36f5bbc950da.png)![Yoann
    Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Scientific/numerical python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://mocquin.medium.com/list/scientificnumerical-python-9ce115122ab6?source=post_page-----1e1a50e5247d--------------------------------)3
    stories![Ironicaly, an array of containers](../Images/4ecd0326a3efdda93947f60872018d41.png)![](../Images/f11076a724463f7b11d819d95bcf0ea4.png)![](../Images/e340b22f444d2bd311537341cf1a105a.png)![Yoann
    Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Sklearn tutorial
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://mocquin.medium.com/list/sklearn-tutorial-2e46a0e06b39?source=post_page-----1e1a50e5247d--------------------------------)9
    stories![](../Images/4ffe6868fb22c241a959bd5d5a9fd5d7.png)![](../Images/8aa32b00faa0ef7376e121ba9c9ffdb7.png)![](../Images/9f986423d7983bc08fc2073534603c35.png)![Yoann
    Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Data science and Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://mocquin.medium.com/list/data-science-and-machine-learning-ba3fb2206051?source=post_page-----1e1a50e5247d--------------------------------)3
    stories![](../Images/c078e74fd67e0141c2b54b82823c78d4.png)![](../Images/79988eda04a078da9373f03d7db51c51.png)![](../Images/6a5966e529bf4ba9b16c592fec7b591a.png)'
  prefs: []
  type: TYPE_NORMAL
