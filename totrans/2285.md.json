["```py\nimport torch\nimport torch.nn as nn\nfrom torch.nn.functional import relu\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(SimpleNet, self).__init__()\n        self.input_layer = nn.Linear(in_features=10, out_features=20) \n        self.output_layer = nn.Linear(in_features=20, out_features=1) \n\n    def forward(self, x):\n        x = relu(self.input_layer(x))\n        x = self.output_layer(x)\n        return x\n```", "```py\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.input_layer = nn.Linear(in_features=10, out_features=20) \n        self.output_layer = nn.Linear(in_features=20, out_features=1) \n\n        self.leaky_relu = nn.LeakyReLU(negative_slope=0.1)\n\n    def forward(self, x):\n        x = self.leaky_relu(self.input_layer(x))\n        x = self.output_layer(x)\n        return x\n```", "```py\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.input_layer = nn.Linear(in_features=10, out_features=20) \n        self.output_layer = nn.Linear(in_features=20, out_features=1) \n\n        self.elu = nn.ELU()\n\n    def forward(self, x):\n        x = self.elu(self.input_layer(x))\n        x = self.output_layer(x)\n        return x\n```", "```py\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.input_layer = nn.Linear(20, 10) \n        self.bn1 = nn.BatchNorm1d(10)\n        self.hidden_layer = nn.Linear(10, 5)\n        self.bn2 = nn.BatchNorm1d(5)\n        self.output_layer = nn.Linear(5, 1)\n\n    def forward(self, x):\n        x = self.input_layer(x)\n        x = self.bn1(x)\n        x = nn.ReLU()(x)\n\n        x = self.hidden_layer(x)\n        x = self.bn2(x)\n        x = nn.ReLU()(x)\n\n        x = self.output_layer(x)\n        return x\n```", "```py\noptimizer.zero_grad() \noutput = model(data)\nloss = criterion(output, target)\nloss.backward()\n\n# Apply gradient clipping\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n\noptimizer.step()\n```"]