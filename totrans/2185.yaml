- en: Understanding Histograms and Kernel Density Estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understanding-histograms-and-kernel-density-estimation-6f9a1f09f960](https://towardsdatascience.com/understanding-histograms-and-kernel-density-estimation-6f9a1f09f960)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An in-depth exploration of histograms and KDE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://reza-bagheri79.medium.com/?source=post_page-----6f9a1f09f960--------------------------------)[![Reza
    Bagheri](../Images/7c5a7dc9e6e31048ce31c8d49055987c.png)](https://reza-bagheri79.medium.com/?source=post_page-----6f9a1f09f960--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6f9a1f09f960--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6f9a1f09f960--------------------------------)
    [Reza Bagheri](https://reza-bagheri79.medium.com/?source=post_page-----6f9a1f09f960--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6f9a1f09f960--------------------------------)
    ·26 min read·Dec 18, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56ccb5112d3a3de4f81b576c14a5bf05.png)'
  prefs: []
  type: TYPE_IMG
- en: A histogram is a graph that visualizes the frequency of numerical data. It is
    commonly used in data science and statistics to have a raw estimate of the distribution
    of a dataset. Kernel density estimation (KDE) is a method for estimating the probability
    density function (PDF) of a random variable with an unknown distribution using
    a random sample drawn from that distribution. Hence, it allows us to infer the
    probability density of a population, based on a finite dataset sampled from it.
    KDE is often used in signal processing and data science, as an essential tool
    to estimate the probability density. This article discusses the math and intuition
    behind histograms and KDE and their advantages and limitations. It also demonstrates
    how KDE can be implemented in Python from scratch. *All figures in this article
    were created by the author.*
  prefs: []
  type: TYPE_NORMAL
- en: '**Probability density function**'
  prefs: []
  type: TYPE_NORMAL
- en: Let *X* be a continuous random variable. The probability that *X* takes a value
    in the interval [a, b] can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf374bc061fff3dd976a7ab701ffbcd0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *f(x)* is *X*''s probability density function (PDF). The *cumulative
    density function* (CDF) of *X* is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7dc86dc593def60ca610dfc98cf02530.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence the CDF of *X*, evaluated at *x*, is the probability that *X* will take
    a value less than or equal to *x*. Using Equation 1, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0bf17bada42795f6728f44201799ae22.png)'
  prefs: []
  type: TYPE_IMG
- en: Using the fundamental theorem of calculus, we can show that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/65e8f11be5d3e6fc5e0860e7a68000c4.png)'
  prefs: []
  type: TYPE_IMG
- en: which means that the PDF of *X* can be determined by taking the derivative of
    its CDF with respect to *x*. A histogram is the simplest approach to estimate
    the PDF of a dataset, and as we show in the next section it uses Equation 1 for
    this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: '**Histograms**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Listing 1, we create a bimodal distribution as a mixture of two normal distributions
    and draw a random sample of size 1000 from this distribution. Here we mix two
    normal distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5699c23a9452e463032a65f8fae65723.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence, the mean of the normal distributions is 0 and 4 respectively and their
    variance is 1 and 0.8 respectively. The mixing coefficients are 0.7 and 0.3, so
    the PDF of the mixture of these distributions is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ddd8ba262d782e1f8e94d1c406e8133b.png)'
  prefs: []
  type: TYPE_IMG
- en: Listing 1 plots this PDF and sample in Figure 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4af127bc95b9f252673748c4012612ef.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1
  prefs: []
  type: TYPE_NORMAL
- en: Now suppose that we only had the sample dataset, and the bimodal distribution
    was unknown, how could we estimate the PDF of this distribution from the dataset?
    The simplest thing we can do is draw this dataset's histogram. Listing 2 plots
    a histogram of the random sample that we generated in Listing 1 along with the
    PDF of the distribution. Even though it does not have a smooth surface, it mimics
    the shape of the PDF of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/061b2ef5a1ac5cbf6f34cf944c1c2216.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how a histogram is generally constructed. Using the definition of
    derivative and Equations 2 and 3 we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c5645d4d93643319223d57669e5a37c.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, the PDF of *X* at point *x* can be estimated using this equation. A histogram
    uses the same concept to estimate the PDF of a random variable based on a random
    sample. Suppose that *X* is a random variable with an unknown distribution, and
    the random variables *X*₁, *X*₂, … *X_n* generally represent a random sample that
    can be drawn from that distribution (so *X*₁, *X*₂, … *X_n* are independent and
    identically distributed). After drawing a random sample, we denote the observations
    in that by *x*₁, *x*₂, … *x_n*. Hence, *x*₁, *x*₂, … *x_n* form the dataset for
    which we want to generate a histogram, and each *xᵢ* is an observation in this
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first need to define the interval [*a*, *b*] on which the histogram is calculated.
    We divide this interval into *k* equal-length subintervals of length *h*=(*b*−*a*)/*k*.
    These constant-length subintervals are also called *bins*. So, we will have the
    following subintervals:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/313b783b995b484596d65f106ccb2fce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Please note that the first *k*-1 subintervals are half-open and only include
    their left endpoints since we don’t want them to overlap at the endpoints. Now
    we can use Equation 4 to estimate the PDF of the test point *x* which belongs
    to the subinterval *Bᵢ*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4dd0adb7716a2ca7513deb53a442f25d.png)'
  prefs: []
  type: TYPE_IMG
- en: This is an approximation since we are not taking the limit anymore. But how
    can we calculate
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b05ab441da54b41cd8891431c273ef7b.png)'
  prefs: []
  type: TYPE_IMG
- en: We can simply use our random sample for this purpose. We know that our dataset
    has *n* observations. Then
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9256d822bc3863bd0f0a700edd9f2ec2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we denote the number of the observations of the random sample within *Bᵢ*
    by *nᵢ*, then we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72669a86f09e01b31f02d918e97925c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We denote this estimation of the PDF by *f^*(*x*), so we can write the previous
    equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8bd185eee30926114f3279468212c48c.png)'
  prefs: []
  type: TYPE_IMG
- en: To plot a histogram, we need to know the values of *a*, *b*, and *h*. We first
    calculate the endpoints of the bins, and for each bin *Bᵢ­* we draw a rectangular
    bar of width *h* and height equal to *nᵢ­*/(*nh*) over the midpoint of that bin
    (Figure 3).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c837e34b3a201b3a3314d78adf77d98a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3
  prefs: []
  type: TYPE_NORMAL
- en: We know that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/60207606cca238577e54f1f479efea47.png)'
  prefs: []
  type: TYPE_IMG
- en: So, the integral of a PDF over the entire space must be equal to 1
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79d92402f9d87f0781051cdcdac27e8e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'which means that a PDF is normalized, and the area behind the PDF curve is
    equal to 1\. The area of the histogram is equal to the sum of the area of the
    rectangles in Figure 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eaa2504fcbf082a448142cf481068409.png)'
  prefs: []
  type: TYPE_IMG
- en: where *k* is the number of bins. This makes sense since a histogram is an estimator
    for the PDF of a distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'To plot a histogram in `matplotlib`, we use the function `hist()`. Please note
    that we should set the parameter `density` to `True` to get an estimate of PDF.
    Otherwise, we will get a *frequency histogram*. In a frequency histogram, the
    height of each bin is equal to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2bd9ef2f2d49498744bb3829180eaea3.png)'
  prefs: []
  type: TYPE_IMG
- en: The histogram plotted using Equation 6 is also called a *density histogram*.
    In this article by a histogram, we mean a density histogram.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at Listing 2, we only set the number of bins and did not provide a
    value for *a* and *b*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'That is because the minimum and maximum values of `sample` are automatically
    used as a and b respectively. We could plot the same histogram by this code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Based on Equation 6, the value of *f^*(*x*) depends on *n* and *h*. But it also
    depends on the starting point of the beans (*a* in Equation 5). So, if we change
    the value of *a*, it will change the shape of the histogram. Listing 3 plots the
    histogram of the sample defined in Listing 1 with *a*=1.8 and *b*=12 and compares
    it with the histogram of Figure 2 in which *a* and *b* were set to the minimum
    and maximum values of the sample. Both histograms have the same *h*. The result
    is shown in Figure 4.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7b2f3e153c67c93d20a192f1880cd24d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4
  prefs: []
  type: TYPE_NORMAL
- en: As you see by changing the values of *a*, the shape of the histogram changes.
    So, the shape of the histogram depends on the random sample used to generate it.
    We don’t know the maximum and minimum values of the population from which a random
    sample is drawn. So, if we draw a histogram based on the minimum and maximum values
    of a random sample, the sample will affect the shape of the histogram. However,
    the histogram is supposed to mimic the shape of the PDF of the population which
    is the same for all the random samples.
  prefs: []
  type: TYPE_NORMAL
- en: '**Histogram as a machine learning model**'
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the histogram of a sample is supposed to mimic the shape of the
    PDF of the population from which the sample is drawn. In fact, we can think of
    a histogram as a machine learning model that learns the PDF of the population
    based on the sample that we have drawn from that population. This is a very simple
    model since it only has one hyperparameter which is the bin width (*h*). This
    hyperparameter controls the complexity of this model. (A hyperparameter is a parameter
    whose value is used to configure a machine learning model and is determined by
    the user before the start of the training process. By contrast, the values of
    other parameters of the model are determined via training).
  prefs: []
  type: TYPE_NORMAL
- en: 'Like any other machine learning model, we can calculate the bias and variance
    of a histogram. Suppose that *X* is a random variable with an unknown distribution,
    and the random variables *X*₁, *X*₂, … *X_n* generally represent a random sample
    that can be drawn from that distribution. After drawing a random sample, we denote
    the observations in that by *x*₁, *x*₂, … *x_n* (here each *xᵢ* is a specific
    value of *Xᵢ*) and these form the dataset for which is used to generate a histogram.
    Now we want to create a histogram that learns the PDF of the distribution of *X*.
    Remember that from Equation 6, we had:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/339d1e0c359b33c9f49e69e339c34be2.png)'
  prefs: []
  type: TYPE_IMG
- en: But this equation is for one specific dataset. If we want to apply it to the
    random sample *X*₁, *X*₂, … *X_n*, we need to replace *nᵢ* with the random variable
    *Nᵢ*. That is because the actual value of *nᵢ* depends on the values that *X*₁,
    *X*₂, … *X_n* take. So, it follows that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0dc783f7dfb13c106e0f641979a90f40.png)'
  prefs: []
  type: TYPE_IMG
- en: Please note that though the values of *X*₁, *X*₂, … *X_n* can change in each
    sample that we draw, the sample size (*n*), the bin size (*h*), and the subinterval
    *Bᵢ* remain the same. So, they are random variables. Now if *Nᵢ* is a random variable,
    what is its distribution? To answer this question, we first need to calculate
    the probability that each *Xₖ* lies in the interval *Bᵢ*. Assume that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7d004985a7d791d4bc62a85c976a591.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *a* is a constant endpoint and doesn’t change with the samples. Since
    each *Xₖ* is sampled from the same distribution of *X*, the probability that *Xₖ*
    belongs to *Bᵢ* can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/535a047f74e01551b085ba510246bf27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we have *n* random variables, and the probability that each of them lies
    in *Bᵢ* is *pᵢ*, and we want to know the distribution of *Nᵢ* which denotes the
    total number of these random variables that lie in *Bᵢ*. This is similar to the
    problem of finding the total number of heads in *n* tosses of a coin where the
    probability of getting heads is *pᵢ*. We already know that if we denote the total
    number of heads by a random variable, it has a binomial distribution with parameters
    *n* and *pᵢ*. So we conclude that *Nᵢ* should have a binomial distribution with
    parameters *n* and *pᵢ*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be8f82c9595098bd6219a6c34ba7fa84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can calculate the mean and variance of *Nᵢ* by knowing its distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4e7f4e1094e4dc8abc9d2aae51395cac.png)![](../Images/151b4486916be393ff35526744040fc3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And we can also calculate the mean and variance *f*^(*x*) at a test point *x_t*::'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dbb597442e54a8b6559bd2574f23dd9b.png)![](../Images/c306d1aaf74ffafe4a5cda7a382a6ef9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence, the bias of *f*^(*x_t*) will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4c58ff9e358e5515dafa2636e2320daa.png)'
  prefs: []
  type: TYPE_IMG
- en: Based on the mean value theorem for integrals, we know that if *f* is continuous
    over the interval *Bᵢ*, then there exists a point *εᵢ* in *Bᵢ* such that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3fc76a61976e7f33a3e5bc1448d79f2a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *d*₁ and *d*₂ are the endpoints of *Bᵢ* in Equation 7:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/21856c77b6ba7d0bf7208b3ee846f80d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By using this equation, *pᵢ* can be simplified:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d094527e823b28dd9e66f29ffa6d6532.png)'
  prefs: []
  type: TYPE_IMG
- en: So, it follows that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/73c12d1cbb94458443cdf334f9ee8baa.png)'
  prefs: []
  type: TYPE_IMG
- en: So, by decreasing *h*, variance increases. We need to make an assumption before
    further simplifying the bias term. Here, we assume that *f*(*x*) is *Lipschitz
    continuous* over the bin *Bᵢ*. A function is Lipschitz continuous over an interval
    *Bi* if there exists a positive constant *γᵢ* such that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eefca86ab928399b4e8ebf6f742b4ea1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now using this equation, we can simplify the bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57b55b3a9eefefdebded03333fa57994.png)'
  prefs: []
  type: TYPE_IMG
- en: Here we used the fact that both *x* and *εᵢ* are in the interval *Bᵢ* whose
    width is *h*, so their distance cannot be greater than *h*. You see that by decreasing
    *h*, the absolute value of bias decreases.
  prefs: []
  type: TYPE_NORMAL
- en: 'By having the bias and variance, we can easily calculate the Mean squared error
    (MSE) at *x*_*t*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57b55b3a9eefefdebded03333fa57994.png)'
  prefs: []
  type: TYPE_IMG
- en: If we think of a histogram as a model, the bin width *h* is the only hyperparameter
    of this model and is like a measure of the model complexity. As *h* goes to zero,
    the model becomes more complicated. Hence the absolute value of bias decreases
    and the variance increases (overfitting). On the other hand, when *h* increases,
    the model becomes simpler, so the variance decreases, but the absolute value of
    bias increases (underfitting).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we conclude that there is a tradeoff between the absolute value of bias
    and variance: increasing one will decrease the other. This is a manifestation
    of the bias-variance tradeoff in mechanic learning which describes the relationship
    between the model complexity and the accuracy of its predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5 shows a demonstration of the bias-variance tradeoff. Here we start
    with 100 random samples of size 80 from a normal distribution with mean of 5 and
    variance of 1\. We also try a range of values for *h* from 0.15 to 2.7\. Next,
    we pick a test point (*x*_*t*) and calculate the PDF of the normal distribution
    at this point (*f(x_t*)). For each value of *h*, we calculate the histograms of
    all 100 random samples. Then we calculate *f*^(*x_t*) for these histograms. Finally,
    using *f*(*x*_*t*) and the values of *f*^(*x_t*) for all the random samples, we
    can calculate the bias², variance and MSE at the test point *x*_*t* for each value
    of *h*.
  prefs: []
  type: TYPE_NORMAL
- en: However, we don’t want to minimize the prediction error of the histogram for
    only one test point (*x*_*t*). Hence, we pick a range of test points from the
    array `xt_list` and for each value of *h*, calculate the average bias², variance
    and MSE for all the test points. Listing 5 creates a plot of the average bias²,
    variance, and MSE for different values of *h* that is shown in Figure 5.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9241ae2f13992be04f0ba4a2507d3c92.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5
  prefs: []
  type: TYPE_NORMAL
- en: By decreasing the value of *h*, the model complexity and average variance increase,
    but the average bias² decreases. A model that is too complex ends up with overfitting.
    On the other hand, increasing the value of *h* results in a simpler model that
    decreases the average variance and increases the average bias². A model that is
    too simple results in underfitting. As mentioned before MSE is the sum of variance
    and the square of bias, so in both cases (underfitting and overfitting), we have
    a large average MSE for all the test points. As this figure shows *h*=0.8 gives
    the lowest average MSE for all the test points since there is a balance between
    the average bias² and average variance at this point. This point represents a
    model which is neither too simple nor too complex, so it minimizes the average
    MSE.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6 plots the histograms for three values of *h* (Figure 6) The histogram
    is underfitting at *h*=2.3 and overfitting at *h*=0.15, and in both cases, it
    doesn’t estimate the PDF very well. Based on the results of Figure 5, the right
    fit happens at *h*=0.8\. The histogram with this value of *h* gives the best estimation
    of the PDF.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5651941c1886742c264574d3f2c0f512.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6
  prefs: []
  type: TYPE_NORMAL
- en: 'As you see, we can find the best value of *h*, if we have so many training
    datasets, but in reality, we only have one. In that case, we can use Scott’s rule
    to find the best value of *h*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/250ebe0d207f01c6e0493120962bc3cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *σ^* isthe sample standard deviation and *n* is the sample size. This
    rule assumes that the sample has a normal distribution. If we use this rule for
    all the samples in Listing 5, we see that the average of *h** is very close to
    the optimal value of *h* found in Listing 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Please note that in reality, we only have one sample to estimate *h**, so the
    estimation error can be larger.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kernel density estimation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid the dependence of the histogram on its endpoints, we can use a different
    formula to approximate the derivative of the CDF. This formula is known as the
    symmetric difference quotient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7b2521f5720fb1c1f04b2a0235b240d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you see unlike Equation 5, this equation does not depend on *a*. Next, we
    can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e912c2033952aaf6cb297459ded2078b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also use the indicator function to calculate this probability. Let *A*
    be a set*.* The indicator function of *A* at *x* is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8168605bb6d04c9b579bcc3b0104c379.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence, it is equal to 1 for all points in its domain which belong to *A,* and
    is equal to 0 for all the other points. Now, we can use the indicator function
    to count the number of observations that belong to [*x*-*h*, *x*+*h*). Remember
    that the *x*₁, *x*₂, … *x_n* represent the dataset for which we want to generate
    a histogram. So, for each *xᵢ*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/075fe9f67a502fddadc0a55e5adabc6c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'is 1 if *x*-*h*≤*xᵢ*<*x*+*h*. Otherwise, it is zero. Hence, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c735f4b092aa5128a771d13ef282336f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And it follows that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/006da1e5ec0ace8d3dddba845f07752a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we denote this estimation of the PDF by *f^*(*x*), we can write the previous
    equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02ad70a6c37903d3ef5e4e30bc2df11d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c08e4287c3edad0ce22fe8ca63e928c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we can write our estimation of PDF as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69a2929a71fd43753e6667bb86f32508.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let’s look at the expression inside the sum and examine the different values
    that it can take:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1252d260ae1c77025af6e1c9d35af076.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Comparing it with the PDF of a continuous uniform distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/47612392a4409734c29dc6d2fd1c95c2.png)'
  prefs: []
  type: TYPE_IMG
- en: We can observe that *K*((*x*-*xᵢ*)/*h*) is the PDF of a uniform distribution
    on the interval [-1, 1].
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7 uses this method to estimate the PDF of the sample generated in Listing
    1\. The result is shown in Figure 7 and it has been compared with the PDF of the
    distribution from which the sample was drawn.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/62b896127e7fea2059dc65e637b62c1e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7
  prefs: []
  type: TYPE_NORMAL
- en: 'We can generalize Equation 8 and replace *K* with a different PDF. We generally
    define a kernel density estimator *f*^(*x*) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/619911cc1cc16ae22555d90c73aa9575.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this equation, *K*(*x*) is called a *kernel function* and *h* is called
    the *bandwidth*. We can also write the previous equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ec925c78d09360abd8b29be8b794896.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *K_h* is called the *scaled kernel* and is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b00598897c6e97506a481c7c942b3df9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The kernel function should satisfy the following conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e062d6715ecc5b4e050074d558c44cd2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Based on the first two conditions, the kernel function should be non-negative
    and normalized to guarantee that the kernel function is a probability density
    function (PDF). Based on the third condition, it should be a symmetric PDF. This
    condition implies that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4a3a318d8958530878a8a721fc70d0c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Looking at Figure 7, you see that the estimator curve is not smooth because
    the uniform density does not have a smooth shape. We can use the PDF of the standard
    normal distribution as the kernel function which has a smooth shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29777a3abcf26ff58e0cebb80b6f6f96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By plugging this kernel into Equation 9, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/24e1e8bb7352f10b2def98edc4a813c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence the estimator is the average of the PDF of *n* normal distributions with
    mean *xᵢ*, and the bandwidth *h* can be considered the standard deviation of these
    distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8 uses an estimator with the standard normal kernel to estimate the
    PDF of the sample generated in Listing 1\. The result is shown in Figure 8 and
    it has been compared with the PDF of the distribution from which the sample was
    drawn. The *n* scaled kernels (*K*_*h*) are also plotted in this figure Each scaled
    kernel is a normal distribution with a mean of *xᵢ* and a standard deviation of
    *h*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9a6a29db61ecf3171f45f9211d3e9421.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8
  prefs: []
  type: TYPE_NORMAL
- en: '**Kernel density estimator as a machine learning model**'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to a histogram, a kernel density estimator can be considered a machine
    learning model that learns the PDF of the population based on the sample we have
    drawn from that population. The hyperparameters of this model are the bandwidth
    (*h*), the kernel type, and its corresponding parameters. These hyperparameters
    make a kernel density estimator more flexible compared to a histogram.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that we have the kernel estimator given in Equation 9\. To calculate
    the bias and variance of this estimator, we need to use all the possible samples
    that we can draw from a population, so we use the i.i.d. random sample *X*₁, *X*₂,
    … *X_n* to create the density estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4679d11b8d5d12cafb9e9f41bfcf90ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now it can be shown that for a test point *x*_*t*, the bias of *f*^(*x_t*)
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b6e8ecda845ff7cf7640d335c02e47f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *σ_K* is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79c2819880dff958a47f1d1f9d60cfbf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also show that the variance of *f*^(*x_t*) is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/637ebe77e6419254ed0f8162856f525f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *R*(*K*) is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c52627f2ef48c49d6d7a297fb8138e0b.png)'
  prefs: []
  type: TYPE_IMG
- en: The bandwidth (*h*) controls the complexity of this model. As *h* goes to zero,
    the model becomes more complicated, so the absolute value of bias decreases and
    the variance increases (overfitting). Conversely, as *h* increases, the model
    becomes simpler, hence the variance decreases, but the absolute value of bias
    increases (underfitting).
  prefs: []
  type: TYPE_NORMAL
- en: 'The details of calculating the bias and variance are given in the appendix.
    Now, we can calculate the MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d941662778bd8385a9fac07104859c60.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So far, we only considered a single test point *x_t*. However, generally, we
    want to control the overall MSE of the density estimator. So, we need to calculate
    the mean integrated squared error (MISE):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c4f403ddcd650ac8638becffc0c1a874.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now by plugging Equation 13 into this equation, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b37e41f5788fac74c9d0aae8901e60a5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This Equation can be simplified using Equation 7 and Equation 12:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8d88e6a0167ca6ece4df13430fbeae5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The two dominating terms in this equation are called the asymptotical mean
    integrated squared error (AMISE):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7cf80d5d6124bae299870353fd078c50.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By setting the derivative of AMISE to zero, we can find the optimum value of
    *h* that minimizes it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe22b5e153e5ed0e281edb096e78de82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Solving this equation gives us the optimum value of *h*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/32a6192dea23d5cd7a6216e7f67cfde8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Please note that we cannot use the above formula involves the second derivative
    of the unknown PDF that we want to estimate (*f’’*), so we cannot use it directly.
    However, we can make an assumption about the PDF type. If we assume that we want
    to estimate the PDF of a normal distribution with the mean *µ* and variance *σ*²
    then:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/482b3575beb7ff214a0c3506c79a224b.png)'
  prefs: []
  type: TYPE_IMG
- en: It follows that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab0b6659342c314130f8ac5a2a94f7ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d74897b0e9d27f21e5140478e031c134.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And if use the PDF of the standard normal distribution as the kernel function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/957f1747771d26676f327c238a1c60fd.png)'
  prefs: []
  type: TYPE_IMG
- en: Then it follows that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b86329b426a2cdb5590ac06740b9f025.png)'
  prefs: []
  type: TYPE_IMG
- en: And
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f04b28df8125a0c8b4f6336286d1d21e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By plugging these equations into Equation 14 we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e4fb3cad47de94567b67e4d2be60406.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we usually don’t know the standard deviation of the population (*σ*),
    we can use the sample standard deviation (*σ^*) instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a5d3ec63fbdb1892d36c2aeb22ad0c12.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the Scott’s rule for estimating the bandwidth of a KDE. Listing 9 shows
    the bias-variance tradeoff for a kernel density estimator. We have 100 random
    samples of size 100 from a normal distribution with a mean of 5 and a variance
    of 1\. We try a range of values for *h* from 0.14 to 0.52\. We pick a range of
    test points from the array `xt_list`.
  prefs: []
  type: TYPE_NORMAL
- en: For each test point (*x*_*t*) in `xt_list`, the PDF of the normal distribution
    at this point (*f(x_t*)) is calculated. Then the KDE of all 100 random samples
    and *f*^(*x_t*) for these samples is calculated. Finally, using *f*(*x*_*t*) and
    the values of *f*^(*x_t*) for all the random samples, the bias², variance, and
    MSE at the *x*_*t* is calculated. Finally, for each value of *h*, we calculate
    the average bias², variance, and MSE for all the test points in `xt_list`. Listing
    9 creates a plot of the average bias², variance, and MSE for different values
    of *h* that is shown in Figure 9.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b1d8f81cd2264dfc4720ff9e5259fbc7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9
  prefs: []
  type: TYPE_NORMAL
- en: As *h* goes to zero, the model becomes more complicated, and the result is a
    decrease in the average bias² and an increase in the average variance (overfitting).
    On the other hand, as *h* increases, the model becomes simpler and results in
    underfitting. Hence the average variance decreases, but the average bias² increases.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use Equation 15 to estimate the optimal value of *h*. Here we calculate
    the average value of *h* for all the samples in listing 9:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We see that the average of *h** is very close to the optimal value of *h* found
    in Listing 9\. Please note that in reality, we only have one sample to estimate
    *h**. Listing 10 plots the KDEs for three values of *h* (Figure 10). The KDE is
    underfitting at *h*=1 and overfitting at *h*=0.1, and in both cases, it doesn’t
    estimate the PDF very well. The right fit happens at *h*=0.8\. The KDE with this
    value of *h* gives the best estimation of the PDF.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/225a78efe6f9832f70872e221d1c8e7e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10
  prefs: []
  type: TYPE_NORMAL
- en: 'The function `gaussian_kde()` in the `SciPy` library can be used to calculate
    KDE. Other libraries like `seaborn` and `matplotlib` use this function to plot
    the KDE of a dataset. This function uses the standard normal kernel to estimate
    the PDF. The documentation mentions that it uses the Scotts’ rule to estimate
    *h**, however, the formula is a little different:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cea65377ef5c90d62cdb853de959de5a.png)'
  prefs: []
  type: TYPE_IMG
- en: Listing 11 plots the KDE of the sample defined in Listing 1\. It compares Equation
    15 and the SciPy’s rule. The plot is shown in Figure 11.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6a126d00524975d34970bdf513a4413c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we discussed histograms and kernel density estimation. Both
    methods are used to estimate the PDF of the probability distribution of a dataset.
    Hence, both of them can be thought of as machine learning models that learn the
    PDF of a dataset. We showed how we can calculate the bias and variance of these
    models. The MSE is the sum of variance and the square of bias, and we want to
    minimize it on all possible test points by finding the optimal bin width of a
    histogram or the optimal bandwidth of a kernel density estimator. The article
    shows how Scott’s rules can be used to estimate these optimal values.
  prefs: []
  type: TYPE_NORMAL
- en: 'I hope that you enjoyed reading this post. All the Code Listings in this article
    are available for download as a Jupyter Notebook from GitHub at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/reza-bagheri/histograms_kde/blob/main/KDE.ipynb](https://github.com/reza-bagheri/histograms_kde/blob/main/KDE.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Appendix:**'
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the random sample *X*₁, *X*₂, … *X_n* are independent and identically
    distributed and represent a random sample drawn from the distribution of the random
    variable *X* (that we want to estimate its PDF), so each of them alone has the
    same distribution of *X*. Hence the random variables
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f9a90ade00d33e09b873c369de54ee80.png)'
  prefs: []
  type: TYPE_IMG
- en: 'are independent and identically distributed, and each of them has the same
    distribution of:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89738b9041e15873f6419cec97f83a49.png)'
  prefs: []
  type: TYPE_IMG
- en: The kernel estimator
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4679d11b8d5d12cafb9e9f41bfcf90ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'is the mean of these independent and identically distributed random variables,
    so we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8db74fdf003e85c77fe98a83ca5bc3e4.png)![](../Images/88205806cecde3f1bcfc9ebf79f4e6f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can calculate the mean and variance of the kernel estimator at a test
    point *x_t*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1a359166a9f29b1067894792728933f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we do a change of variable *y*=(*x_t*-*x*)/*h*. So, the previous equation
    can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4eff369d36df25006137ed2323eaaf09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can use the Taylor series to estimate *f*(*x_t*-*yh*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/960a960a71f6fff137926ae9b4b2f8e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where o(*h*²) means that it is a smaller order term compared to *h*² when *h*
    goes to zero. By plugging this equation into the previous one we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c6d18ef7b700e8824e27f29b6090c755.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now using Equation 11 (*I*) and (*II*), we can simplify this equation and write
    it as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0bc45a5e51f89cb6218889c48146dfe7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *σ_K* is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79c2819880dff958a47f1d1f9d60cfbf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The bias at *x_t* is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9811d98fcdc121a366cb132364cd2ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, it follows that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6ee1e17ff42f723578eaa550631d95c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Please note that we needed the condition in Equation 11 (*II*) to get this
    equation for bias. Next, we calculate the variance at point *x_t*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5436f880c0a1dbedb35b8376d23fadeb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the definition of variance can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a1fc088d2ceb052f050c83137e451a41.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To calculate the first term on the right-hand side of this equation we can
    write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/27868ec27b0022f5a4503cd7f11fa5db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where we used a change of variable *y*=(*x_t*-*x*)/*h*. Again, we use the Taylor
    series to estimate *f*(*x_t*-*yh*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/51a11f348d1eaf6478b6387a06d26c4c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By plugging this equation into the previous one we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2a2300c67e225e5685857112ad7d7c33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *R*(*K*) is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37c7cf205f382d51735c2f4929f27106.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The second on the right-hand side of this equation term can be calculated using
    Equation *A*.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2a18d5c0527a04965c21eb651923c7e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here if we ignore all the terms and approximate it with zero, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fb87c569ab2d4b4efba6ea7a425dd6c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/40684a7e25472b006a7ac042ac2c30cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Here we used the fact that as *h* goes to zero 1/*h*>>1, So
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d708caa19b97edf341fd73ef2525bc2.png)'
  prefs: []
  type: TYPE_IMG
