- en: Google’s Latest Approaches to Multimodal Foundational Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/googles-latest-approaches-to-multimodal-foundational-model-beedaced32f9](https://towardsdatascience.com/googles-latest-approaches-to-multimodal-foundational-model-beedaced32f9)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Multimodal foundational models are even more exciting than large language models.
    Let’s review Google research’s recent progress to have a glimpse of the bleeding
    edge.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://eileen-code4fun.medium.com/?source=post_page-----beedaced32f9--------------------------------)[![Eileen
    Pangu](../Images/cbdab572af709b6e6b52cb3a078f220d.png)](https://eileen-code4fun.medium.com/?source=post_page-----beedaced32f9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----beedaced32f9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----beedaced32f9--------------------------------)
    [Eileen Pangu](https://eileen-code4fun.medium.com/?source=post_page-----beedaced32f9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----beedaced32f9--------------------------------)
    ·7 min read·Aug 12, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4c992bcc9d7d065534e2c5fec5e58b79.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [https://unsplash.com/photos/U3sOwViXhkY](https://unsplash.com/photos/U3sOwViXhkY)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Background**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: While the hype on large language model (LLM) is still iron hot in the industry,
    the leading research organizations have turned their eyes to multimodal foundational
    models — models that have the same scale and versatility characteristics as LLM
    but can handle data beyond just text, such as images, audio, sensor signals, and
    so on. Multimodal foundational models are believed by many to be the key to unlock
    the next phase of Artificial Intelligence (AI) advance.
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, we take a closer look at how Google approaches multimodal
    foundational models. The content covered in this blog post is drawn from the key
    methods and insights of Google’s recent papers, for which we provide references
    at the end of this article.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why Should You Care**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Multimodal foundational models are exciting, but why should you care? You may
    be:'
  prefs: []
  type: TYPE_NORMAL
- en: an AI/ML practitioner who wants to catch up with the latest research development
    of the field, but you don’t have the patience to go through dozens of new papers
    and hundreds of pages of surveys.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a current or emerging industry leader who is wondering what’s next after large
    language models, and is thinking about how to align your business with the new
    trends in the tech world.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a curious reader who may end up being the consumer of current or future multimodal
    AI products, and wants to get a visual and intuitive understanding of how things
    work behind the scenes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For all the above audiences, this article will provide a good overview to jump-start
    your understanding of multimodal foundational models, which is a corner stone
    for future more accessible and helpful AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'One more thing to note before we dive in: when people talk about multimodal
    foundational models, they often mean the input is multimodal, consisting of text,
    images, videos, signals, etc. The output, however, is always just text. The reason
    is that text is the most generic form of communication format. If we can output
    text, we can output code and feed it to downstream tools to generate whatever
    multimedia formats desired. Not to mention there are many AI tools nowadays that
    take natural language as input and generate images and sounds.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Start With Just Text**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The first format to incorporate is text, or in research terms, language. Researchers
    like to call it language because language implies that the text is drawn from
    a linguistic system with a coherent semantic meaning, be it any natural language,
    code, commands, symbols, and so on. In layman’s terms, we’ll just call it text
    in this blog post.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of materials online about building large language models, so
    we won’t cover it in depth here. Just note that Google’s recent results on LLMs
    and multimodal foundational models are all built on PaLM [1]. If you’re not familiar
    with it, just think of it as a black box similar to ChatGPT, though their internal
    mechanics vary quite a bit.
  prefs: []
  type: TYPE_NORMAL
- en: The fundamental concept to grasp here is that the input text is broken into
    tokens. Each token is then mapped into an embedding space. Effectively each token
    becomes a vector. Then the list of the vectors are fed into a model consisting
    of layers of attention based networks. The output of the model is also a list
    of vectors, which can be converted back to tokens, and then concatenated to form
    the final text output.
  prefs: []
  type: TYPE_NORMAL
- en: The model has hundreds of billions of parameters, and is trained on a massive
    amount of text data for some simple prediction tasks such as given a text prefix,
    and predicting the next token.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting model will need to go through further fine-tuning to become more
    directly useful. For example, Google has adapted its PaLM model to the medical
    domain and created MedPaLM [2]. You can reference this [article](/how-to-use-large-language-models-llm-in-your-own-domains-b4dff2d08464)
    for more insights on how to adapt a large language model to your specific domain.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have a large language model. The next step is to incorporate
    other media formats.
  prefs: []
  type: TYPE_NORMAL
- en: '**Incorporate Image**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To incorporate images, the first thing to do is to convert an image into a format
    that can be processed by the large language model. Because we already have the
    large language model and we want multimedia to be processed uniformly, the natural
    choice is to convert an image into a list of embedding vectors. Recall that text
    is also converted into a list of embedding vectors above. Obviously, the vectors
    from image and text should all have the same dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Google has done impressive work in adopting transformer architecture for image
    processing [3] [4]. That turns out to be a great fit for the image to vectors
    conversion.
  prefs: []
  type: TYPE_NORMAL
- en: Concretely, every image is resized to 224×224 resolution, and then broken into
    14×14 pixels patches. This results in 224×234/14/14=256 patches. Each patch goes
    through the same learnable linear transformation to transform into a vector. Each
    vector is concatenated with a learnable positional embedding. These concatenated
    vectors are then fed into a Transformer model [5]. Google research called this
    model architecture ViT— visual Transformer. The output of the ViT model is also
    256 vectors. A shallow network is then attached to the output vectors from the
    ViT model, and the whole architecture is trained for image classification, image
    captioning, and other image processing tasks. See Figure-1 for an illustration.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/14594b2ba46100f5bda3da94b55c2aa4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure-1: illustration of ViT architecture. Image from author.'
  prefs: []
  type: TYPE_NORMAL
- en: Once the image processing training is done, throw away the shallow network on
    the top, we can use the 256 output vectors from ViT as the embedding representation
    of the image. The core idea is that through large scale training, the ViT model
    has learned to represent the input image by a list of vectors that can be easily
    used as input features for a variety of image processing tasks.
  prefs: []
  type: TYPE_NORMAL
- en: One special note here is that Google research found that ViT’s results on image
    processing is superior to state of the art CNN. They attributed the superior performance
    to the scale (Transformer based architecture is known to scale better) and the
    fact that attention can happen directly between any 2 input patches early on in
    the network.
  prefs: []
  type: TYPE_NORMAL
- en: This image representation learning done in ViT is transferable to multimodal
    settings, as you guessed it. The output vectors from the pretrained ViT can serve
    as input to the large language model.
  prefs: []
  type: TYPE_NORMAL
- en: For an input prompt such as `what happened between <imgA> and <imgB>`, the words
    are converted into embedding vectors for the large language model as usual; each
    image is converted into 256 vectors using ViT. All the vectors together form the
    input to the large language model. Finally, the large language model is fine-tuned
    end to end (or by freezing its various parts) on multimedia training data, and
    the resulting model is the multimodal foundational model. See Figure-2 for an
    illustration.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4989f2a7ec72bf29ad5c19ad46caad0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure-2: illustration of multimodal foundational model architecture. Image
    from author.'
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, the dimension numbers mentioned above are just one configuration
    Google research’s papers used [6]. You can try your own if you want. But be aware
    that the ViT output vector dimension should match the larger language model input
    vector dimension. If different, you can adjust either models or you can insert
    a linear transformation between the ViT output and the large language model input
    to squeeze or stretch the ViT output to match the large language model input dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '**Extend to Other Media and Domain**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now that we know how to incorporate images, we can employ a similar scheme to
    include other media types. They don’t necessarily have to use a Transformer architecture.
    Google research employed a multimodal foundational model in robotics [7] by converting
    states, objects in scene, and other input sources into the same vector embedding
    space. Many of those input formats are actually converted to the embedding space
    using very simple transformations. Google research has also recently adapted the
    methodology to the medical domain and created a multimodal foundational model
    for healthcare [8]. See Figure-3 for an illustration of their utility.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e157128e75cea13e1cdb7696f81c6cff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure-3: illustration of multimodal foundational model use cases in (a) robotics
    and (b) medical field. Image from papers in references [7] and [8]'
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In this blog post, we had a glimpse of where the bleeding edge is in the field
    of multimodal foundational models. The core idea is two-fold: (1) encode multimedia
    in the same embedding space so that they can be processed uniformly by LLM architecture;
    (2) stitch together large pre-trained models (they are often pre-trained in unimodal
    settings) and fine-tune with multimedia training data.'
  prefs: []
  type: TYPE_NORMAL
- en: Some of Google research’s recent breakthrough really starts to resemble an AI
    agent that people would want and could use in day to day life. While there is
    still a long way to go for more accessible and helpful AI, there is no doubt that
    multimodal foundational models are the inflection point of AI, and future development
    is going to be ever more exciting.
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[1] PaLM: Scaling Language Modeling with Pathways [https://arxiv.org/abs/2204.02311](https://arxiv.org/abs/2204.02311)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Large Language Models Encode Clinical Knowledge [https://arxiv.org/abs/2212.13138](https://arxiv.org/abs/2212.13138)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
    [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Scaling Vision Transformers [https://arxiv.org/abs/2106.04560](https://arxiv.org/abs/2106.04560)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Attention is All You Need [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] PaLI: A Jointly-Scaled Multilingual Language-Image Model [https://arxiv.org/abs/2209.06794](https://arxiv.org/abs/2209.06794)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] PaLM-E: An Embodied Multimodal Language Model [https://arxiv.org/abs/2303.03378](https://arxiv.org/abs/2303.03378)'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Towards Generalist Biomedical AI [https://arxiv.org/abs/2307.14334](https://arxiv.org/abs/2307.14334)'
  prefs: []
  type: TYPE_NORMAL
