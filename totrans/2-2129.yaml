- en: Topological Generalisation with Advective Diffusion Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŸºäºå¯¹æµæ‰©æ•£å˜æ¢å™¨çš„æ‹“æ‰‘æ³›åŒ–
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/topological-generalisation-with-advective-diffusion-transformers-70f263a5fec7](https://towardsdatascience.com/topological-generalisation-with-advective-diffusion-transformers-70f263a5fec7)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/topological-generalisation-with-advective-diffusion-transformers-70f263a5fec7](https://towardsdatascience.com/topological-generalisation-with-advective-diffusion-transformers-70f263a5fec7)
- en: Topological Generalisation in GNNs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GNNsä¸­çš„æ‹“æ‰‘æ³›åŒ–
- en: One of the key open questions in the study of graph neural networks (GNNs) is
    their generalisation capabilities, in particular, under changes in the topology
    of the graph. In this post, we study this problem from the perspective of graph
    diffusion equations, which are intimately related to GNNs and have been used in
    the past as a framework for analysing GNN dynamics, expressive power, and justifying
    architectural choices. We describe a new architecture based on advective diffusion
    that combines the computational structure of message-passing neural networks (MPNNs)
    and Transformers and shows superior topological generalisation capabilities.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ç ”ç©¶ä¸­ï¼Œä¸€ä¸ªå…³é”®çš„æœªè§£é—®é¢˜æ˜¯å®ƒä»¬çš„æ³›åŒ–èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å›¾çš„æ‹“æ‰‘å‘ç”Ÿå˜åŒ–æ—¶ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä»å›¾æ‰©æ•£æ–¹ç¨‹çš„è§’åº¦æ¥ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œè¿™äº›æ–¹ç¨‹ä¸GNNså¯†åˆ‡ç›¸å…³ï¼Œè¿‡å»æ›¾ä½œä¸ºåˆ†æGNNåŠ¨æ€ã€è¡¨è¾¾èƒ½åŠ›å’Œè¯æ˜æ¶æ„é€‰æ‹©çš„æ¡†æ¶ã€‚æˆ‘ä»¬æè¿°äº†ä¸€ç§åŸºäºå¯¹æµæ‰©æ•£çš„æ–°æ¶æ„ï¼Œè¯¥æ¶æ„ç»“åˆäº†æ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œï¼ˆMPNNsï¼‰å’ŒTransformersçš„è®¡ç®—ç»“æ„ï¼Œå¹¶æ˜¾ç¤ºå‡ºå“è¶Šçš„æ‹“æ‰‘æ³›åŒ–èƒ½åŠ›ã€‚
- en: '[](https://michael-bronstein.medium.com/?source=post_page-----70f263a5fec7--------------------------------)[![Michael
    Bronstein](../Images/1aa876fce70bb07bef159fecb74e85bf.png)](https://michael-bronstein.medium.com/?source=post_page-----70f263a5fec7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----70f263a5fec7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----70f263a5fec7--------------------------------)
    [Michael Bronstein](https://michael-bronstein.medium.com/?source=post_page-----70f263a5fec7--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://michael-bronstein.medium.com/?source=post_page-----70f263a5fec7--------------------------------)[![Michael
    Bronstein](../Images/1aa876fce70bb07bef159fecb74e85bf.png)](https://michael-bronstein.medium.com/?source=post_page-----70f263a5fec7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----70f263a5fec7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----70f263a5fec7--------------------------------)
    [Michael Bronstein](https://michael-bronstein.medium.com/?source=post_page-----70f263a5fec7--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----70f263a5fec7--------------------------------)
    Â·9 min readÂ·Oct 19, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----70f263a5fec7--------------------------------)
    Â·9åˆ†é’Ÿé˜…è¯»Â·2023å¹´10æœˆ19æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4454e0295635120bca008b7c8f313e5a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4454e0295635120bca008b7c8f313e5a.png)'
- en: 'Image: Unsplash'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼šUnsplash
- en: '*This post was co-authored with* [*Qitian Wu*](https://qitianwu.github.io/)
    *and* [*Chenxiao Yang*](https://chr26195.github.io/) *and is based on the paper
    by Q. Wu et al.,* [*Advective Diffusion Transformer for Topological Generalization
    in Graph Learning*](https://arxiv.org/abs/2310.06417) *(2023) arXiv:2310.06417.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿™ç¯‡æ–‡ç« ä¸* [*Qitian Wu*](https://qitianwu.github.io/) *å’Œ* [*Chenxiao Yang*](https://chr26195.github.io/)
    *å…±åŒä½œè€…ï¼Œå¹¶åŸºäºQ. Wuç­‰äººçš„è®ºæ–‡* [*Advective Diffusion Transformer for Topological Generalization
    in Graph Learning*](https://arxiv.org/abs/2310.06417) *(2023) arXiv:2310.06417ã€‚*'
- en: Graph Neural Networks (GNNs) have emerged in the last decade as a popular architecture
    for machine learning on graph-structured data, with a wide variety of applications
    ranging from social networks and [life sciences](https://arxiv.org/abs/2307.08423)
    to [drug](https://medium.com/towards-data-science/geometric-ml-becomes-real-in-fundamental-sciences-3b0d109883b5)
    and [food design](https://medium.com/towards-data-science/hyperfoods-9582e5d9a8e4).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨è¿‡å»åå¹´ä¸­ä½œä¸ºä¸€ç§æµè¡Œçš„å›¾ç»“æ„æ•°æ®æœºå™¨å­¦ä¹ æ¶æ„å‡ºç°ï¼Œåº”ç”¨èŒƒå›´å¹¿æ³›ï¼Œä»ç¤¾äº¤ç½‘ç»œå’Œ [ç”Ÿå‘½ç§‘å­¦](https://arxiv.org/abs/2307.08423)
    åˆ° [è¯ç‰©](https://medium.com/towards-data-science/geometric-ml-becomes-real-in-fundamental-sciences-3b0d109883b5)
    å’Œ [é£Ÿå“è®¾è®¡](https://medium.com/towards-data-science/hyperfoods-9582e5d9a8e4)ã€‚
- en: Two key theoretical questions regarding GNNs are their *expressive* and *generalisation*
    capabilities. The former question has been addressed extensively in the literature
    by resorting to [variants of the graph isomorphism test](https://medium.com/towards-data-science/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49)
    [1], and more recently, by formulating GNNs as [discretised diffusion-type equations](https://medium.com/towards-data-science/neural-sheaf-diffusion-for-deep-learning-on-graphs-bfa200e6afa6)
    [2]. However, the second question, despite multiple recent approaches [3â€“4], is
    still largely open.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äº GNN çš„ä¸¤ä¸ªå…³é”®ç†è®ºé—®é¢˜æ˜¯å®ƒä»¬çš„ *è¡¨è¾¾èƒ½åŠ›* å’Œ *æ³›åŒ–èƒ½åŠ›*ã€‚å‰ä¸€ä¸ªé—®é¢˜åœ¨æ–‡çŒ®ä¸­å·²æœ‰å¹¿æ³›æ¢è®¨ï¼Œé€šè¿‡å€ŸåŠ© [å›¾åŒæ„æµ‹è¯•çš„å˜ä½“](https://medium.com/towards-data-science/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49)
    [1]ï¼Œä»¥åŠæœ€è¿‘å°† GNN å½¢å¼åŒ–ä¸º [ç¦»æ•£åŒ–æ‰©æ•£å‹æ–¹ç¨‹](https://medium.com/towards-data-science/neural-sheaf-diffusion-for-deep-learning-on-graphs-bfa200e6afa6)
    [2]ã€‚ç„¶è€Œï¼Œå°½ç®¡æœ‰å¤šä¸ªæœ€è¿‘çš„ç ”ç©¶æ–¹æ³• [3â€“4]ï¼Œç¬¬äºŒä¸ªé—®é¢˜ä»ç„¶å¤§éƒ¨åˆ†æ‚¬è€Œæœªå†³ã€‚
- en: Empirically, GNNs are often reported to show poor performance [5â€“7] when the
    training and test data are generated from different distributions (so-called *â€œdistribution
    shiftâ€*), especially when the topology of the graph changes (*â€œtopological shiftâ€*).
    This is a major concern in applications such as chemistry, where ML models are
    typically trained on a limited set of molecular graphs and are expected to learn
    some rules that generalise to previously unseen molecules with different structures.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ç»éªŒä¸Šçœ‹ï¼ŒGNNs åœ¨è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®æ¥æºäºä¸åŒåˆ†å¸ƒï¼ˆå³æ‰€è°“çš„*â€œåˆ†å¸ƒè½¬ç§»â€*ï¼‰æ—¶ï¼Œé€šå¸¸è¡¨ç°è¾ƒå·®ï¼Œå°¤å…¶æ˜¯å½“å›¾çš„æ‹“æ‰‘å‘ç”Ÿå˜åŒ–ï¼ˆ*â€œæ‹“æ‰‘è½¬ç§»â€*ï¼‰æ—¶ã€‚è¿™åœ¨åŒ–å­¦ç­‰åº”ç”¨ä¸­å°¤ä¸ºé‡è¦ï¼Œå…¶ä¸­
    ML æ¨¡å‹é€šå¸¸åœ¨æœ‰é™çš„åˆ†å­å›¾é›†ä¸Šè®­ç»ƒï¼Œå¹¶æœŸæœ›å­¦ä¹ ä¸€äº›è§„åˆ™ï¼Œè¿™äº›è§„åˆ™èƒ½å¤Ÿæ¨å¹¿åˆ°ç»“æ„ä¸åŒçš„æœªè§è¿‡çš„åˆ†å­ã€‚
- en: '![](../Images/e690e59c2c82b6b5ec96935e3b77f35a.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e690e59c2c82b6b5ec96935e3b77f35a.png)'
- en: 'Example of topological shifts in graph-based molecular modeling: three molecules
    on the left have high drug-likeness (QED), whereas the molecules on the right
    do not.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åŸºåˆ†å­å»ºæ¨¡ä¸­çš„æ‹“æ‰‘è½¬å˜ç¤ºä¾‹ï¼šå·¦ä¾§çš„ä¸‰ä¸ªåˆ†å­å…·æœ‰è¾ƒé«˜çš„è¯ç‰©ç›¸ä¼¼æ€§ï¼ˆQEDï¼‰ï¼Œè€Œå³ä¾§çš„åˆ†å­åˆ™æ²¡æœ‰ã€‚
- en: '**Neural Graph Diffusion**'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç¥ç»å›¾æ‰©æ•£**'
- en: In a recent collaboration between Oxford and Shanghai Jiao Tong University [8],
    we used the neural diffusion PDE perspective to study the generalisation capabilities
    of GNNs under topological shifts. [Graph neural diffusion](https://medium.com/towards-data-science/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774)
    approaches [9] replace discrete GNN layers with a continuous-time differential
    equation of the form
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ€è¿‘ç‰›æ´¥å¤§å­¦ä¸ä¸Šæµ·äº¤é€šå¤§å­¦çš„åˆä½œä¸­ [8]ï¼Œæˆ‘ä»¬åˆ©ç”¨ç¥ç»æ‰©æ•£ PDE è§†è§’ç ”ç©¶äº† GNN åœ¨æ‹“æ‰‘è½¬å˜ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚[å›¾ç¥ç»æ‰©æ•£](https://medium.com/towards-data-science/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774)
    æ–¹æ³• [9] ç”¨è¿ç»­æ—¶é—´å¾®åˆ†æ–¹ç¨‹æ›¿æ¢ç¦»æ•£ GNN å±‚ï¼Œå½¢å¼ä¸º
- en: '**áºŠ**(*t*) = div(**S**(t)âˆ‡**X**(t)),'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**áºŠ**(*t*) = div(**S**(t)âˆ‡**X**(t)),'
- en: known as the *graph diffusion equation*. It is initialised with **X**(0) = ENC(**X**áµ¢)
    and run for some time *t*â‰¤*T*, in order to produce the output **X**â‚’ = DEC(**X**(*T*))
    [10]. Here **X**(*t*) is an *n*Ã—*d* matrix of *d*-dimensional node features at
    time *t*, **S** is the matrix-valued diffusivity function [11], and ENC/DEC is
    a feature encoder/decoder pair.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¢«ç§°ä¸º*å›¾æ‰©æ•£æ–¹ç¨‹*ã€‚å®ƒä»¥**X**(0) = ENC(**X**áµ¢) åˆå§‹åŒ–ï¼Œå¹¶è¿è¡Œä¸€æ®µæ—¶é—´ *t*â‰¤*T*ï¼Œä»¥äº§ç”Ÿè¾“å‡º **X**â‚’ = DEC(**X**(*T*))
    [10]ã€‚è¿™é‡Œçš„ **X**(*t*) æ˜¯æ—¶é—´ *t* æ—¶çš„ *n*Ã—*d* ç»´èŠ‚ç‚¹ç‰¹å¾çŸ©é˜µï¼Œ**S** æ˜¯çŸ©é˜µå€¼çš„æ‰©æ•£å‡½æ•° [11]ï¼Œè€Œ ENC/DEC
    æ˜¯ç‰¹å¾ç¼–ç å™¨/è§£ç å™¨å¯¹ã€‚
- en: When **S** depends only on the *structure* of the graph (through its adjacency
    matrix **A**, i.e., **S** = **S**(**A**)), the graph diffusion equation is *linear*
    and referred to as *homogeneous* (in the sense that diffusivity properties are
    â€œthe sameâ€ everywhere on the domain). When **S** also depends on the *features*
    (i.e., **S** = **S**(**X**(*t*),**A**) and is thus time-dependent), the equation
    is *nonlinear* (*non-homogeneous*).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ **S** ä»…ä¾èµ–äºå›¾çš„ *ç»“æ„*ï¼ˆé€šè¿‡å…¶é‚»æ¥çŸ©é˜µ **A**ï¼Œå³ **S** = **S**(**A**ï¼‰ï¼‰æ—¶ï¼Œå›¾æ‰©æ•£æ–¹ç¨‹æ˜¯ *çº¿æ€§çš„* å¹¶ç§°ä¸º
    *å‡åŒ€çš„*ï¼ˆåœ¨äºæ‰©æ•£ç‰¹æ€§åœ¨æ•´ä¸ªåŸŸä¸Šâ€œç›¸åŒâ€ï¼‰ã€‚å½“ **S** è¿˜ä¾èµ–äº *ç‰¹å¾*ï¼ˆå³ **S** = **S**(**X**(*t*),**A**ï¼‰ï¼Œå› æ­¤æ˜¯æ—¶é—´ä¾èµ–çš„ï¼‰ï¼Œæ–¹ç¨‹åˆ™æ˜¯
    *éçº¿æ€§çš„*ï¼ˆ*éå‡åŒ€çš„*ï¼‰ã€‚
- en: In the graph learning setting, **S** is implemented as a parametric function
    whose parameters are learned using backpropagation based on the downstream task.
    Many standard message-passing graph neural networks (MPNNs) can be recovered as
    special settings of the graph diffusion equation by an appropriate choice of diffusivity
    and a time discretisation scheme [12]. The linear setting corresponds to GNNs
    of the *convolutional* type and the nonlinear setting to the *attentional* one.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å›¾å­¦ä¹ ç¯å¢ƒä¸­ï¼Œ**S** è¢«å®ç°ä¸ºä¸€ä¸ªå‚æ•°å‡½æ•°ï¼Œå…¶å‚æ•°é€šè¿‡åŸºäºä¸‹æ¸¸ä»»åŠ¡çš„åå‘ä¼ æ’­å­¦ä¹ å¾—åˆ°ã€‚è®¸å¤šæ ‡å‡†çš„æ¶ˆæ¯ä¼ é€’å›¾ç¥ç»ç½‘ç»œï¼ˆMPNNsï¼‰å¯ä»¥é€šè¿‡é€‚å½“é€‰æ‹©æ‰©æ•£æ€§å’Œæ—¶é—´ç¦»æ•£åŒ–æ–¹æ¡ˆ[12]ä½œä¸ºå›¾æ‰©æ•£æ–¹ç¨‹çš„ç‰¹æ®Šè®¾ç½®è¿›è¡Œæ¢å¤ã€‚çº¿æ€§è®¾ç½®å¯¹åº”äº*å·ç§¯å‹*
    GNNï¼Œè€Œéçº¿æ€§è®¾ç½®å¯¹åº”äº*æ³¨æ„åŠ›å‹* GNNã€‚
- en: '**Generalisation capabilities** of the neural graph diffusion model are related
    to the sensitivity of the solution **X**(*T*) = *f*(**X**(0),**A**) to a perturbation
    of the adjacency matrix **Ãƒ** = **A** + Î´**A**. A model is expected to show good
    topological generalisation if *f*(**X**(0),**A**) â‰ˆ *f*(**X**(0),**Ãƒ**) for a
    small Î´**A**. This is unfortunately not the case for graph diffusion: we show
    [13] that in both linear and nonlinear cases, the change in **X**(*T*) can be
    very large, of ğ’ª(exp(â€–Î´**A**â€–*T*)).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»å›¾æ‰©æ•£æ¨¡å‹çš„**æ³›åŒ–èƒ½åŠ›**ä¸è§£**X**(*T*) = *f*(**X**(0),**A**)å¯¹é‚»æ¥çŸ©é˜µ**Ãƒ** = **A** + Î´**A**çš„æ‰°åŠ¨çš„æ•æ„Ÿæ€§æœ‰å…³ã€‚å¦‚æœ*
    f *(**X**(0),**A**) â‰ˆ * f *(**X**(0),**Ãƒ**) å¯¹äºä¸€ä¸ªå°çš„ Î´**A**ï¼Œæ¨¡å‹é¢„æœŸèƒ½è¡¨ç°å‡ºè‰¯å¥½çš„æ‹“æ‰‘æ³›åŒ–ã€‚ç„¶è€Œï¼Œå¯¹äºå›¾æ‰©æ•£æ¥è¯´ï¼Œè¿™ç§æƒ…å†µå¹¶ä¸æˆç«‹ï¼šæˆ‘ä»¬å±•ç¤ºäº†[13]ï¼Œåœ¨çº¿æ€§å’Œéçº¿æ€§æƒ…å†µä¸‹ï¼Œ**X**(*T*)çš„å˜åŒ–å¯èƒ½éå¸¸å¤§ï¼Œè¾¾åˆ°
    ğ’ª(exp(â€–Î´**A**â€–*T*))ã€‚
- en: '**Graph Transformers** attempt to overcome this problem by allowing for *non-local
    diffusion*, where exchange of information can occur between any pair of nodes
    rather than only those connected by an edge. This effectively decouples the input
    graph from the computational one (which is now fully-connected and learned through
    the attention mechanism) and makes the model agnostic to the input graph. We prove
    that such a non-local diffusion model is capable of topological generalisation
    [14] under a certain data generation model [15] with an additional assumption
    that the node labels and graph topology are statistically independent.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾å˜æ¢å™¨**å°è¯•é€šè¿‡å…è®¸*éå±€éƒ¨æ‰©æ•£*æ¥å…‹æœè¿™ä¸ªé—®é¢˜ï¼Œå…¶ä¸­ä¿¡æ¯äº¤æ¢å¯ä»¥å‘ç”Ÿåœ¨ä»»æ„ä¸€å¯¹èŠ‚ç‚¹ä¹‹é—´ï¼Œè€Œä¸ä»…ä»…æ˜¯é‚£äº›é€šè¿‡è¾¹ç›¸è¿çš„èŠ‚ç‚¹ä¹‹é—´ã€‚è¿™æœ‰æ•ˆåœ°å°†è¾“å…¥å›¾ä¸è®¡ç®—å›¾è§£è€¦ï¼ˆè®¡ç®—å›¾ç°åœ¨æ˜¯å…¨è¿æ¥çš„ï¼Œå¹¶é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œå­¦ä¹ ï¼‰ï¼Œä½¿æ¨¡å‹å¯¹è¾“å…¥å›¾ä¸æ•æ„Ÿã€‚æˆ‘ä»¬è¯æ˜äº†è¿™æ ·ä¸€ä¸ªéå±€éƒ¨æ‰©æ•£æ¨¡å‹åœ¨æŸäº›æ•°æ®ç”Ÿæˆæ¨¡å‹[15]ä¸‹å…·å¤‡æ‹“æ‰‘æ³›åŒ–èƒ½åŠ›[14]ï¼Œé¢å¤–å‡è®¾æ˜¯èŠ‚ç‚¹æ ‡ç­¾å’Œå›¾æ‹“æ‰‘åœ¨ç»Ÿè®¡ä¸Šæ˜¯ç‹¬ç«‹çš„ã€‚'
- en: This independence assumption, however, is often far from reality, since node
    labels typically correlate with the graph structure. This implies that non-local
    diffusion alone, discarding any observed structural information, is insufficient
    for topological generalisation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¿™ä¸€ç‹¬ç«‹æ€§å‡è®¾é€šå¸¸ä¸ç°å®ç›¸å»ç”šè¿œï¼Œå› ä¸ºèŠ‚ç‚¹æ ‡ç­¾é€šå¸¸ä¸å›¾ç»“æ„ç›¸å…³ã€‚è¿™æ„å‘³ç€ä»…ä¾èµ–äºéå±€éƒ¨æ‰©æ•£ï¼Œå¿½è§†ä»»ä½•è§‚å¯Ÿåˆ°çš„ç»“æ„ä¿¡æ¯ï¼Œå¯¹äºæ‹“æ‰‘æ³›åŒ–æ˜¯ä¸å¤Ÿçš„ã€‚
- en: '**Advective Graph Diffusion**'
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**å¹³æµå›¾æ‰©æ•£**'
- en: We consider a more general type of diffusion equations containing an additional
    advective term,
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è€ƒè™‘åŒ…å«é™„åŠ å¹³æµé¡¹çš„æ›´ä¸€èˆ¬ç±»å‹çš„æ‰©æ•£æ–¹ç¨‹ï¼Œ
- en: '**áºŠ**(*t*) = div(**S**(*t*)âˆ‡**X**(*t*)) + *Î²*div(**V**(*t*)**X**(*t*)).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**áºŠ**(*t*) = div(**S**(*t*)âˆ‡**X**(*t*)) + *Î²*div(**V**(*t*)**X**(*t*))ã€‚'
- en: Equations of this type are known as *advective diffusion* and arise in real-world
    physical systems such as saline solutions. The first *diffusive term* is lead
    by the concentration gradient and describes the evolution of salinity due to spatial
    differences in the salt chemical concentration (**S** represents the molecular
    diffusivity in water). The second *advective term* is related to the movement
    of the water (**V** characterises the flow direction).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ç±»å‹çš„æ–¹ç¨‹è¢«ç§°ä¸º*å¹³æµæ‰©æ•£*ï¼Œåœ¨å®é™…ç‰©ç†ç³»ç»Ÿä¸­ï¼Œä¾‹å¦‚ç›æ°´æº¶æ¶²ä¸­å‡ºç°ã€‚ç¬¬ä¸€ä¸ª*æ‰©æ•£é¡¹*ç”±æµ“åº¦æ¢¯åº¦ä¸»å¯¼ï¼Œæè¿°äº†ç”±äºç›åŒ–å­¦æµ“åº¦çš„ç©ºé—´å·®å¼‚è€Œå¯¼è‡´çš„ç›åº¦æ¼”å˜ï¼ˆ**S**è¡¨ç¤ºæ°´ä¸­çš„åˆ†å­æ‰©æ•£ç‡ï¼‰ã€‚ç¬¬äºŒä¸ª*å¹³æµé¡¹*ä¸æ°´çš„è¿åŠ¨æœ‰å…³ï¼ˆ**V**ç‰¹å¾åŒ–æµåŠ¨æ–¹å‘ï¼‰ã€‚
- en: '**Advective Diffusion Transformer.** In our setting, the diffusion process
    led by the feature gradient acts as an *internal* driving force of the evolution,
    in which the diffusivity remains invariant across environments. This corresponds
    to environment-invariant latent interactions among nodes, determined by the underlying
    data manifold, that induce all-pair information flow over a complete graph. Architecturally,
    we implement the diffusive term as a *Transformer* with precomputed global attention
    between the input features of all pairs of nodes, **S** = **S**(**X**(0),**A**).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¹³æ»‘æ‰©æ•£Transformer**ã€‚åœ¨æˆ‘ä»¬çš„è®¾ç½®ä¸­ï¼Œç”±ç‰¹å¾æ¢¯åº¦å¼•å¯¼çš„æ‰©æ•£è¿‡ç¨‹ä½œä¸º*å†…éƒ¨*é©±åŠ¨åŠ›ï¼Œå…¶ä¸­æ‰©æ•£æ€§åœ¨å„ç§ç¯å¢ƒä¸­ä¿æŒä¸å˜ã€‚è¿™å¯¹åº”äºèŠ‚ç‚¹ä¹‹é—´çš„ç¯å¢ƒä¸å˜æ½œåœ¨äº¤äº’ï¼Œè¿™äº›äº¤äº’ç”±åŸºç¡€æ•°æ®æµå½¢å†³å®šï¼Œå¯¼è‡´åœ¨å®Œå…¨å›¾ä¸Šçš„æ‰€æœ‰é…å¯¹ä¿¡æ¯æµã€‚åœ¨æ¶æ„ä¸Šï¼Œæˆ‘ä»¬å°†æ‰©æ•£é¡¹å®ç°ä¸ºå…·æœ‰é¢„è®¡ç®—å…¨å±€æ³¨æ„åŠ›çš„*Transformer*ï¼Œå…¶ä¸­è€ƒè™‘äº†æ‰€æœ‰èŠ‚ç‚¹å¯¹çš„è¾“å…¥ç‰¹å¾ï¼Œ**S**
    = **S**(**X**(0),**A**)ã€‚'
- en: The advection process driven by the directional movement (where we use **V**=**A**)
    is an *external* force, where the velocity depends on the context. This is analogous
    to environment-sensitive graph topology that is informative for prediction in
    specific environments. Architecturally, the advective term is implemented as message-passing
    on the input graph.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±æ–¹å‘æ€§è¿åŠ¨ï¼ˆæˆ‘ä»¬ä½¿ç”¨**V**=**A**ï¼‰é©±åŠ¨çš„å¯¹æµè¿‡ç¨‹æ˜¯ä¸€ç§*å¤–éƒ¨*åŠ›é‡ï¼Œå…¶ä¸­é€Ÿåº¦ä¾èµ–äºä¸Šä¸‹æ–‡ã€‚è¿™ç±»ä¼¼äºç¯å¢ƒæ•æ„Ÿçš„å›¾æ‹“æ‰‘ï¼Œå¯¹ç‰¹å®šç¯å¢ƒä¸­çš„é¢„æµ‹å…·æœ‰ä¿¡æ¯é‡ã€‚åœ¨æ¶æ„ä¸Šï¼Œå¯¹æµé¡¹å®ç°ä¸ºåœ¨è¾“å…¥å›¾ä¸Šçš„æ¶ˆæ¯ä¼ é€’ã€‚
- en: '![](../Images/55094d1fdb20966e20a75a7315c30469.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55094d1fdb20966e20a75a7315c30469.png)'
- en: '*Advective Diffusion Transformer architecture for generalisable learning on
    graph data. The model comprises three modules: a node-wise encoder (ENC), a diffusion
    module, and a node-wise decoder (DEC). The diffusion module is implemented with
    the graph advective diffusion equation, where the diffusion term is instantiated
    as global attention (Transformer-like) and the advection term as local message
    passing.*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¹³æ»‘æ‰©æ•£Transformeræ¶æ„ç”¨äºå›¾æ•°æ®çš„å¯æ³›åŒ–å­¦ä¹ ã€‚æ¨¡å‹åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šèŠ‚ç‚¹ç¼–ç å™¨ï¼ˆENCï¼‰ï¼Œæ‰©æ•£æ¨¡å—å’ŒèŠ‚ç‚¹è§£ç å™¨ï¼ˆDECï¼‰ã€‚æ‰©æ•£æ¨¡å—é€šè¿‡å›¾å¹³æ»‘æ‰©æ•£æ–¹ç¨‹å®ç°ï¼Œå…¶ä¸­æ‰©æ•£é¡¹è¢«å®ä¾‹åŒ–ä¸ºå…¨å±€æ³¨æ„åŠ›ï¼ˆç±»ä¼¼Transformerï¼‰ï¼Œè€Œå¯¹æµé¡¹åˆ™ä½œä¸ºå±€éƒ¨æ¶ˆæ¯ä¼ é€’ã€‚*'
- en: We refer to this architecture combining MPNNs and graph Transformers as the
    *Advective Diffusion Transformer* *(ADiT)*. It requires only learnable weights
    in the feature encoder and decoder neural networks (ENC and DEC) and the attention
    used for the pre-computed diffusivity and is highly parameter-efficient. The use
    of constant diffusivity also yields a closed-form solution of the advective diffusion
    equation, **X**(*t*) = exp(âˆ’(**I**âˆ’**S**âˆ’*Î²***A**)*t*)**X**(0), which can be numerically
    approximated using PadÃ©-Chebyshev rational series [16] with linear complexity
    in the number of nodes *n*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è¿™ç§ç»“åˆäº†MPNNå’Œå›¾Transformerçš„æ¶æ„ç§°ä¸º*å¹³æ»‘æ‰©æ•£Transformer* *(ADiT)*ã€‚å®ƒä»…éœ€è¦åœ¨ç‰¹å¾ç¼–ç å™¨å’Œè§£ç å™¨ç¥ç»ç½‘ç»œï¼ˆENCå’ŒDECï¼‰ä¸­å­¦ä¹ çš„æƒé‡ä»¥åŠç”¨äºé¢„è®¡ç®—æ‰©æ•£æ€§çš„æ³¨æ„åŠ›ï¼Œå¹¶ä¸”å‚æ•°æ•ˆç‡æé«˜ã€‚å¸¸é‡æ‰©æ•£æ€§çš„ä½¿ç”¨è¿˜æä¾›äº†å¹³æ»‘æ‰©æ•£æ–¹ç¨‹çš„é—­å¼è§£ï¼Œ**X**(*t*)
    = exp(âˆ’(**I**âˆ’**S**âˆ’*Î²***A**)*t*)**X**(0)ï¼Œè¯¥è§£å¯ä»¥ä½¿ç”¨å…·æœ‰çº¿æ€§å¤æ‚åº¦çš„PadÃ©-Chebyshevæœ‰ç†çº§æ•°[16]è¿›è¡Œæ•°å€¼è¿‘ä¼¼ã€‚
- en: '**Topological generalisation of Advective Diffusion.** We show [17] that the
    change in **X**(*T*) as a result of graph adjacency perturbation is reduced from
    exponential to polynomial, ğ’ª(Poly(â€–Î´**A**â€–*T*)). This suggests that the advective
    diffusion model incorporating observed structural information is capable of controlling
    the impact of topology variation on node representations to arbitrary rates. Furthermore,
    we show that under our graph generation model [15], this architecture has provable
    topological generalisation capability.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¹³æ»‘æ‰©æ•£çš„æ‹“æ‰‘æ³›åŒ–**ã€‚æˆ‘ä»¬å±•ç¤ºäº†[17]ï¼Œåœ¨å›¾é‚»æ¥æ‰°åŠ¨çš„ç»“æœä¸­ï¼Œ**X**(*T*)çš„å˜åŒ–ä»æŒ‡æ•°çº§é™åˆ°äº†å¤šé¡¹å¼çº§ï¼Œğ’ª(Poly(â€–Î´**A**â€–*T*))ã€‚è¿™è¡¨æ˜ï¼Œç»“åˆè§‚æµ‹ç»“æ„ä¿¡æ¯çš„å¹³æ»‘æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæ§åˆ¶æ‹“æ‰‘å˜åŒ–å¯¹èŠ‚ç‚¹è¡¨ç¤ºçš„å½±å“åˆ°ä»»æ„çš„é€Ÿç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨æˆ‘ä»¬çš„å›¾ç”Ÿæˆæ¨¡å‹[15]ä¸‹ï¼Œè¿™ç§æ¶æ„å…·æœ‰å¯è¯æ˜çš„æ‹“æ‰‘æ³›åŒ–èƒ½åŠ›ã€‚'
- en: '**Experimental Validation**'
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**å®éªŒéªŒè¯**'
- en: We provide an extensive experimental validation of the Advective Diffusion Transformer
    on a variety of node-, edge-, graph-level tasks across datasets ranging from molecules
    to social networks. The main focus of the evaluation is a comparison with state-of-the-art
    MPNN and graph Transformers on challenging training/testing splits in order to
    stress-test the generalisation capabilities.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯¹å¹³æ»‘æ‰©æ•£Transformerè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œæ¶µç›–äº†ä»åˆ†å­åˆ°ç¤¾äº¤ç½‘ç»œçš„å„ç§èŠ‚ç‚¹ã€è¾¹ã€å›¾çº§ä»»åŠ¡ã€‚è¯„ä¼°çš„ä¸»è¦é‡ç‚¹æ˜¯ä¸æœ€å…ˆè¿›çš„MPNNå’Œå›¾Transformeråœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è®­ç»ƒ/æµ‹è¯•åˆ†å‰²ä¸Šçš„æ¯”è¾ƒï¼Œä»¥æµ‹è¯•å…¶æ³›åŒ–èƒ½åŠ›ã€‚
- en: '![](../Images/f932ffd746cfabeaf9cc3c1af92cca5d.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f932ffd746cfabeaf9cc3c1af92cca5d.png)'
- en: 'Results on node classification datasets: citation network Arxiv and social
    network Twitch. The train/validation/test data are split with the publication
    years of papers and geographic domains of users on Arxiv and Twitch, respectively,
    which introduces distribution shifts. OOM means out-of-memory error.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹åˆ†ç±»æ•°æ®é›†ä¸Šçš„ç»“æœï¼šå¼•ç”¨ç½‘ç»œ Arxiv å’Œç¤¾äº¤ç½‘ç»œ Twitchã€‚è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®æ ¹æ® Arxiv å’Œ Twitch ä¸Šçš„è®ºæ–‡å‡ºç‰ˆå¹´ä»½å’Œç”¨æˆ·åœ°ç†åŸŸè¿›è¡Œæ‹†åˆ†ï¼Œè¿™å¼•å…¥äº†åˆ†å¸ƒå˜åŒ–ã€‚OOM
    è¡¨ç¤ºå†…å­˜ä¸è¶³é”™è¯¯ã€‚
- en: '![](../Images/8e9322ad8f8f17ea08bfd7a61cb6315b.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e9322ad8f8f17ea08bfd7a61cb6315b.png)'
- en: 'Dynamic Graph Dataset DPPIN [19] of biological protein interactions where we
    consider three different tasks: node regression, edge regression, and link prediction.
    The performance is measured by RMSE and ROC-AUC, respectively. We consider dataset-level
    data splits for train/valid/test, where different datasets are obtained from distinct
    protein identification methods and have disparate topological patterns.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨æ€å›¾æ•°æ®é›† DPPIN [19] æ¶‰åŠç”Ÿç‰©è›‹ç™½è´¨äº¤äº’ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä¸‰ä¸ªä¸åŒçš„ä»»åŠ¡ï¼šèŠ‚ç‚¹å›å½’ã€è¾¹å›å½’å’Œé“¾æ¥é¢„æµ‹ã€‚æ€§èƒ½é€šè¿‡ RMSE å’Œ ROC-AUC åˆ†åˆ«è¡¡é‡ã€‚æˆ‘ä»¬è€ƒè™‘æ•°æ®é›†çº§çš„æ•°æ®æ‹†åˆ†ç”¨äºè®­ç»ƒ/éªŒè¯/æµ‹è¯•ï¼Œå…¶ä¸­ä¸åŒæ•°æ®é›†æ¥è‡ªä¸åŒçš„è›‹ç™½è´¨è¯†åˆ«æ–¹æ³•ï¼Œå¹¶å…·æœ‰ä¸åŒçš„æ‹“æ‰‘æ¨¡å¼ã€‚
- en: '![](../Images/78c4481446bb4c48f15b1c598ce974cc.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78c4481446bb4c48f15b1c598ce974cc.png)'
- en: Generating molecular coarse-grained mapping operators where the task is to find
    a representation of how atoms are grouped in a molecule that can be modeled as
    a graph segmentation problem [20]. It boils down to predicting the edges for subgraph
    partition (indicated by colors) resembling the expert annotations (ground truth).
    The presented scores for each method are averaged testing accuracy.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆåˆ†å­ç²—ç²’åº¦æ˜ å°„æ“ä½œç¬¦ï¼Œå…¶ä¸­ä»»åŠ¡æ˜¯æ‰¾åˆ°å¦‚ä½•åœ¨åˆ†å­ä¸­å¯¹åŸå­è¿›è¡Œåˆ†ç»„çš„è¡¨ç¤ºï¼Œè¿™å¯ä»¥å»ºæ¨¡ä¸ºå›¾åˆ†å‰²é—®é¢˜ [20]ã€‚è¿™å½’ç»“ä¸ºé¢„æµ‹å­å›¾åˆ†åŒºçš„è¾¹ï¼ˆç”±é¢œè‰²æŒ‡ç¤ºï¼‰ä»¥ç±»ä¼¼ä¸“å®¶æ³¨é‡Šï¼ˆçœŸå®å€¼ï¼‰ã€‚æ¯ç§æ–¹æ³•çš„è¯„åˆ†ä¸ºæµ‹è¯•å‡†ç¡®åº¦çš„å¹³å‡å€¼ã€‚
- en: Overall, our experimental results demonstrate the promising power and wide applicability
    of our model in the challenging generalisation tasks over graph data. More broadly,
    our work points to the possibility of leveraging established physical PDEs for
    understanding the generalisation capabilities of GNNs and designing novel generalisable
    architectures in graph learning.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„å®éªŒç»“æœå±•ç¤ºäº†æˆ‘ä»¬æ¨¡å‹åœ¨å›¾æ•°æ®çš„æŒ‘æˆ˜æ€§æ³›åŒ–ä»»åŠ¡ä¸­çš„è‰¯å¥½è¡¨ç°å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚æ›´å¹¿æ³›åœ°è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œæŒ‡å‡ºäº†åˆ©ç”¨æ—¢æœ‰ç‰©ç†åå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEsï¼‰æ¥ç†è§£å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶è®¾è®¡æ–°é¢–çš„å¯æ³›åŒ–æ¶æ„çš„å¯èƒ½æ€§ã€‚
- en: '[1] K. Xu et al., How powerful are graph neural networks? (2019) *ICLR*, and
    C. Morris et al., Weisfeiler and Leman go neural: Higher-order graph neural networks
    (2019) *AAAI* established the equivalence between message passing and the graph
    isomorphism test described in the classical paper of B. Weisfeiler and A. Lehman,
    The reduction of a graph to canonical form and the algebra which appears therein
    (1968) *Nauchno-Technicheskaya Informatsia* 2(9):12â€“16\. See our [previous blog
    post](https://medium.com/towards-data-science/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49)
    on this topic.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] K. Xu ç­‰äººï¼Œã€Šå›¾ç¥ç»ç½‘ç»œçš„èƒ½åŠ›æœ‰å¤šå¼ºï¼Ÿã€‹ï¼ˆ2019ï¼‰ *ICLR*ï¼Œä»¥åŠ C. Morris ç­‰äººï¼Œã€ŠWeisfeiler å’Œ Leman
    èµ°å‘ç¥ç»ç½‘ç»œï¼šé«˜é˜¶å›¾ç¥ç»ç½‘ç»œã€‹ï¼ˆ2019ï¼‰ *AAAI* ç¡®ç«‹äº†æ¶ˆæ¯ä¼ é€’ä¸ B. Weisfeiler å’Œ A. Lehman ç»å…¸è®ºæ–‡ä¸­æè¿°çš„å›¾åŒæ„æµ‹è¯•ä¹‹é—´çš„ç­‰ä»·æ€§ï¼Œã€Šå›¾çš„è§„èŒƒå½¢å¼ç®€åŒ–åŠå…¶æ‰€æ¶‰åŠçš„ä»£æ•°ã€‹ï¼ˆ1968ï¼‰
    *Nauchno-Technicheskaya Informatsia* 2(9):12â€“16ã€‚è¯·å‚é˜…æˆ‘ä»¬çš„ [ä¹‹å‰çš„åšå®¢æ–‡ç« ](https://medium.com/towards-data-science/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49)ã€‚'
- en: '[2] C. Bodnar et al., Neural Sheaf Diffusion: A Topological Perspective on
    Heterophily and Oversmoothing in GNNs (2022) *NeurIPS* (see also an [accompanying
    blog post](https://medium.com/towards-data-science/neural-sheaf-diffusion-for-deep-learning-on-graphs-bfa200e6afa6))
    showed how the ability of the diffusion equation on a cellular sheaf constructed
    over a graph to separate a certain number of node classes depends on the choice
    of the sheaf. F. Di Giovanni et al., Understanding convolution on graphs via energies
    (2023) *TMLR* (see an [accompanying blog post](https://medium.com/towards-data-science/graph-neural-networks-as-gradient-flows-4dae41fb2e8a)
    and a [talk](https://www.youtube.com/watch?v=sgTTtmwOMgE)) described the conditions
    under which diffusion with channel mixing (discretised as a convolutional-type
    GNN) can cope with heterophilic graphs.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] C. Bodnar ç­‰äººï¼Œã€Šç¥ç»å±‚æ‰©æ•£ï¼šGNNä¸­çš„å¼‚è´¨æ€§å’Œè¿‡å¹³æ»‘çš„æ‹“æ‰‘è§†è§’ã€‹ï¼ˆ2022ï¼‰ *NeurIPS*ï¼ˆå¦è§ [é™„å¸¦çš„åšå®¢æ–‡ç« ](https://medium.com/towards-data-science/neural-sheaf-diffusion-for-deep-learning-on-graphs-bfa200e6afa6)ï¼‰å±•ç¤ºäº†å›¾ä¸Šæ„é€ çš„ç»†èƒå±‚æ‰©æ•£æ–¹ç¨‹åˆ†ç¦»ç‰¹å®šæ•°é‡èŠ‚ç‚¹ç±»åˆ«çš„èƒ½åŠ›å¦‚ä½•å–å†³äºå±‚çš„é€‰æ‹©ã€‚F.
    Di Giovanni ç­‰äººï¼Œã€Šé€šè¿‡èƒ½é‡ç†è§£å›¾ä¸Šçš„å·ç§¯ã€‹ï¼ˆ2023ï¼‰ *TMLR*ï¼ˆå¦è§ [é™„å¸¦çš„åšå®¢æ–‡ç« ](https://medium.com/towards-data-science/graph-neural-networks-as-gradient-flows-4dae41fb2e8a)
    å’Œ [è®²åº§](https://www.youtube.com/watch?v=sgTTtmwOMgE)ï¼‰æè¿°äº†å…·æœ‰é€šé“æ··åˆçš„æ‰©æ•£ï¼ˆç¦»æ•£åŒ–ä¸ºå·ç§¯å‹GNNï¼‰åœ¨å¼‚è´¨å›¾ä¸Šçš„é€‚ç”¨æ¡ä»¶ã€‚'
- en: '[3] V. Garg et al., Generalization and Representational Limits of Graph Neural
    Networks (2020) *ICML*.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] V. Garg ç­‰äººï¼Œã€Šå›¾ç¥ç»ç½‘ç»œçš„æ³›åŒ–å’Œè¡¨ç¤ºæé™ã€‹ï¼ˆ2020ï¼‰ *ICML*ã€‚'
- en: '[4] H. Tang and Y. Liu, Towards Understanding Generalization of Graph Neural
    Networks (2023) *ICML*.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] H. Tang å’Œ Y. Liuï¼Œã€Šç†è§£å›¾ç¥ç»ç½‘ç»œçš„æ³›åŒ–ã€‹ï¼ˆ2023ï¼‰ *ICML*ã€‚'
- en: '[5] P. W. Koh et al., WILDS: A benchmark of in-the-wild distribution shifts
    (2021) *ICML*.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] P. W. Koh ç­‰äººï¼Œã€ŠWILDSï¼šç°å®ç¯å¢ƒä¸­åˆ†å¸ƒå˜åŒ–çš„åŸºå‡†ã€‹ï¼ˆ2021ï¼‰ *ICML*ã€‚'
- en: '[6] W. Hu et al., OGB-LSC: A large-scale challenge for machine learning on
    graphs (2021) *arXiv*:2103.09430.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] W. Hu ç­‰äººï¼Œã€ŠOGB-LSCï¼šå›¾ä¸Šæœºå™¨å­¦ä¹ çš„å¤§è§„æ¨¡æŒ‘æˆ˜ã€‹ï¼ˆ2021ï¼‰ *arXiv*:2103.09430ã€‚'
- en: '[7] G. Bazhenov and D. Kuznedelev, Evaluating robustness and uncertainty of
    graph models under structural distributional shifts (2023) *arXiv*:2302.13875.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] G. Bazhenov å’Œ D. Kuznedelevï¼Œã€Šåœ¨ç»“æ„åˆ†å¸ƒå˜åŒ–ä¸‹è¯„ä¼°å›¾æ¨¡å‹çš„é²æ£’æ€§å’Œä¸ç¡®å®šæ€§ã€‹ï¼ˆ2023ï¼‰ *arXiv*:2302.13875ã€‚'
- en: '[8] Q. Wu et al., Advective Diffusion Transformer for Topological Generalization
    in Graph Learning (2023) *arXiv*:2310.06417.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Q. Wu ç­‰äººï¼Œã€Šç”¨äºå›¾å­¦ä¹ çš„è‡ªæµæ‰©æ•£å˜æ¢å™¨ï¼ˆAdvective Diffusion Transformerï¼‰ä»¥å®ç°æ‹“æ‰‘æ³›åŒ–ã€‹ï¼ˆ2023ï¼‰
    *arXiv*:2310.06417ã€‚'
- en: '[9] There exist multiple diffusion-based GNN models. See, e.g., B. Chamberlain
    et al., GRAND: graph neural diffusion (2021) *ICML,* an [accompanying blog post](https://medium.com/towards-data-science/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774),
    [talk](https://youtu.be/9SMbH18nMUg?si=3aNK6gFEjTcDcCHN),as well as our more general
    blog post on [physics-inspired GNNs](https://medium.com/towards-data-science/graph-neural-networks-beyond-weisfeiler-lehman-and-vanilla-message-passing-bc8605fa59a).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] å­˜åœ¨å¤šç§åŸºäºæ‰©æ•£çš„GNNæ¨¡å‹ã€‚ä¾‹å¦‚ï¼ŒB. Chamberlain ç­‰äººï¼Œã€Š GRAND: å›¾ç¥ç»æ‰©æ•£ã€‹ï¼ˆ2021ï¼‰ *ICML*ï¼Œ[é™„å¸¦çš„åšå®¢æ–‡ç« ](https://medium.com/towards-data-science/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774)ã€[è®²åº§](https://youtu.be/9SMbH18nMUg?si=3aNK6gFEjTcDcCHN)ï¼Œä»¥åŠæˆ‘ä»¬å…³äº
    [ç‰©ç†å¯å‘çš„GNN](https://medium.com/towards-data-science/graph-neural-networks-beyond-weisfeiler-lehman-and-vanilla-message-passing-bc8605fa59a)
    çš„æ›´ä¸€èˆ¬æ€§åšå®¢æ–‡ç« ã€‚'
- en: '[10] Additional boundary conditions can be imposed e.g. to model missing features,
    see E. Rossi et al., On the Unreasonable Effectiveness of Feature propagation
    in Learning on Graphs with Missing Node Features (2022) *LoG* (see also an [accompanying
    blog post](https://medium.com/towards-data-science/learning-on-graphs-with-missing-features-dd34be61b06)
    and a [talk](https://youtu.be/xe5A-xQTBdM?si=ftcUtxZHSHv0wXwq)).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] å¯ä»¥æ–½åŠ é¢å¤–çš„è¾¹ç•Œæ¡ä»¶ï¼Œä¾‹å¦‚ï¼Œæ¥æ¨¡æ‹Ÿç¼ºå¤±çš„ç‰¹å¾ï¼Œè¯·å‚è§ E. Rossi ç­‰äººï¼Œã€Šç¼ºå¤±èŠ‚ç‚¹ç‰¹å¾çš„å›¾å­¦ä¹ ä¸­ç‰¹å¾ä¼ æ’­çš„éç†æ€§æœ‰æ•ˆæ€§ã€‹ï¼ˆ2022ï¼‰
    *LoG*ï¼ˆå¦è§ [é™„å¸¦çš„åšå®¢æ–‡ç« ](https://medium.com/towards-data-science/learning-on-graphs-with-missing-features-dd34be61b06)
    å’Œ [è®²åº§](https://youtu.be/xe5A-xQTBdM?si=ftcUtxZHSHv0wXwq)ï¼‰ã€‚'
- en: '[11] The graph *G* is assumed to have *n* nodes and *m* edges, **W** is the
    *n*Ã—*n* adjacency matrix with *wáµ¤áµ¥*=1 if *u*~*v* and zero otherwise. The gradient
    is an operator assigning to each edge *u*~*v* the difference of the respective
    node feature vectors, (âˆ‡**X**)*áµ¤áµ¥*=**x***áµ¥*âˆ’x*áµ¤* and the divergence at node *u*
    sums the features of edges emanating from it, (div(**X**))*áµ¤*= âˆ‘*áµ¥* *wáµ¤áµ¥* **x***áµ¤áµ¥*.
    Given *d*-dimensional node features arranged into an *n*Ã—*d* matrix **X**, the
    gradient âˆ‡**X** can be represented as a matrix of size *m*Ã—*d*. Similarly, given
    edge features matrix **Y** of size *m*Ã—*d*, the divergence div(**Y**) is an *n*Ã—*d*
    matrix. The two operators are adjoint w.r.t. the appropriate inner products, âŸ¨âˆ‡**X**,**Y**âŸ©=âŸ¨**X**,div(**Y**)âŸ©.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] å›¾ *G* å‡å®šæœ‰ *n* ä¸ªèŠ‚ç‚¹å’Œ *m* æ¡è¾¹ï¼Œ**W** æ˜¯ *n*Ã—*n* çš„é‚»æ¥çŸ©é˜µï¼Œå…¶ä¸­ *wáµ¤áµ¥*=1 å¦‚æœ *u*~*v*ï¼Œå¦åˆ™ä¸ºé›¶ã€‚æ¢¯åº¦æ˜¯ä¸€ä¸ªæ“ä½œç¬¦ï¼Œå°†æ¯æ¡è¾¹
    *u*~*v* åˆ†é…ç»™å„è‡ªèŠ‚ç‚¹ç‰¹å¾å‘é‡çš„å·®å¼‚ï¼Œï¼ˆâˆ‡**X**ï¼‰*áµ¤áµ¥*=**x***áµ¥*âˆ’x*áµ¤*ï¼ŒèŠ‚ç‚¹ *u* çš„æ•£åº¦å°†æ¥è‡ªè¯¥èŠ‚ç‚¹çš„è¾¹çš„ç‰¹å¾ç›¸åŠ ï¼Œï¼ˆdiv(**X**ï¼‰ï¼‰*áµ¤*=
    âˆ‘*áµ¥* *wáµ¤áµ¥* **x***áµ¤áµ¥*ã€‚ç»™å®š *d* ç»´çš„èŠ‚ç‚¹ç‰¹å¾æ’åˆ—æˆ *n*Ã—*d* çŸ©é˜µ **X**ï¼Œæ¢¯åº¦ âˆ‡**X** å¯ä»¥è¡¨ç¤ºä¸º *m*Ã—*d*
    çš„çŸ©é˜µã€‚ç±»ä¼¼åœ°ï¼Œç»™å®šå¤§å°ä¸º *m*Ã—*d* çš„è¾¹ç‰¹å¾çŸ©é˜µ **Y**ï¼Œæ•£åº¦ div(**Y**) æ˜¯ä¸€ä¸ª *n*Ã—*d* çš„çŸ©é˜µã€‚è¿™ä¸¤ä¸ªæ“ä½œç¬¦åœ¨é€‚å½“çš„å†…ç§¯ä¸‹æ˜¯ä¼´éšçš„ï¼ŒâŸ¨âˆ‡**X**ï¼Œ**Y**âŸ©=âŸ¨**X**ï¼Œdiv(**Y**)âŸ©ã€‚'
- en: '[12] For example, **S** can be defined as attention on the connected node features,
    as in P. VeliÄkoviÄ‡ et al., Graph Attention Networks (2018) *ICLR*.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] ä¾‹å¦‚ï¼Œ**S** å¯ä»¥å®šä¹‰ä¸ºå¯¹è¿æ¥èŠ‚ç‚¹ç‰¹å¾çš„æ³¨æ„åŠ›ï¼Œå¦‚ P. VeliÄkoviÄ‡ ç­‰äººæ‰€è¿°ï¼Œå›¾æ³¨æ„åŠ›ç½‘ç»œï¼ˆ2018å¹´ï¼‰*ICLR*ã€‚'
- en: '[13] Propositions 1 and 2 in our paper [8].'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] æˆ‘ä»¬è®ºæ–‡ä¸­çš„å‘½é¢˜ 1 å’Œ 2 [8]ã€‚'
- en: '[14] Proposition 3 in our paper [8].'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] æˆ‘ä»¬è®ºæ–‡ä¸­çš„å‘½é¢˜ 3 [8]ã€‚'
- en: '[15] The data generation hypothesis in our paper [8] is an extension of the
    graphon (continuous graph) model described in Section 3.1\. The general setting
    does not assume the independence between node labels and graph topology.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] æˆ‘ä»¬è®ºæ–‡ä¸­çš„æ•°æ®ç”Ÿæˆå‡è®¾ [8] æ˜¯ç¬¬ 3.1 èŠ‚ä¸­æè¿°çš„å›¾è°±ï¼ˆè¿ç»­å›¾ï¼‰æ¨¡å‹çš„æ‰©å±•ã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œä¸å‡è®¾èŠ‚ç‚¹æ ‡ç­¾ä¸å›¾æ‹“æ‰‘ä¹‹é—´çš„ç‹¬ç«‹æ€§ã€‚'
- en: '[16] See E. Gallopoulos and Y. Saad, Efficient solution of parabolic equations
    by Krylov approximation methods (1992) *SIAM J. Scientific and Statistical Computing*
    13(5):1236â€“1264\. This method has previously been successfully used in geometry
    processing, e.g., G. PatanÃ©, Laplacian spectral distances and kernels on 3D shapes
    (2014) *Pattern Recognition Letters* 47:102â€“110.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] è§ E. Gallopoulos å’Œ Y. Saadï¼Œåˆ©ç”¨ Krylov è¿‘ä¼¼æ–¹æ³•æœ‰æ•ˆè§£å†³æŠ›ç‰©æ–¹ç¨‹ï¼ˆ1992å¹´ï¼‰*SIAM J. Scientific
    and Statistical Computing* 13(5):1236â€“1264ã€‚è¯¥æ–¹æ³•ä»¥å‰åœ¨å‡ ä½•å¤„ç†æ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä¾‹å¦‚ï¼ŒG. PatanÃ©ï¼Œ3D å½¢çŠ¶ä¸Šçš„æ‹‰æ™®æ‹‰æ–¯è°±è·ç¦»å’Œæ ¸ï¼ˆ2014å¹´ï¼‰*Pattern
    Recognition Letters* 47:102â€“110ã€‚'
- en: '[17] Theorem 1 in our paper [8].'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] æˆ‘ä»¬è®ºæ–‡ä¸­çš„å®šç† 1 [8]ã€‚'
- en: '[18] Theorem 2 in our paper [8].'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] æˆ‘ä»¬è®ºæ–‡ä¸­çš„å®šç† 2 [8]ã€‚'
- en: '[19] D. Fu and J. He, DPPIN: A biological repository of dynamic protein-protein
    interaction network data (2022) *Big Data*.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] D. Fu å’Œ J. Heï¼ŒDPPIN: åŠ¨æ€è›‹ç™½è´¨-è›‹ç™½è´¨ç›¸äº’ä½œç”¨ç½‘ç»œæ•°æ®çš„ç”Ÿç‰©åº“ï¼ˆ2022å¹´ï¼‰*Big Data*ã€‚'
- en: '[20] Z. Li et al., Graph neural network based coarse-grained mapping prediction
    (2020) *Chemical Science* 11(35):9524â€“9531.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Z. Li ç­‰ï¼ŒåŸºäºå›¾ç¥ç»ç½‘ç»œçš„ç²—ç²’åº¦æ˜ å°„é¢„æµ‹ï¼ˆ2020å¹´ï¼‰*Chemical Science* 11(35):9524â€“9531ã€‚'
- en: '*We are grateful to Yonatan Gideoni and Fan Nie for proofreading this post.
    For additional articles about deep learning on graphs, see Michaelâ€™s* [*other
    posts*](https://towardsdatascience.com/graph-deep-learning/home) *in Towards Data
    Science,* [*subscribe*](https://michael-bronstein.medium.com/subscribe) *to his
    posts and* [*YouTube channel*](https://www.youtube.com/c/MichaelBronsteinGDL)*,
    get* [*Medium membership*](https://michael-bronstein.medium.com/membership)*,
    or follow* [*Michael*](https://twitter.com/mmbronstein), [*Qitian*](https://twitter.com/qitianwu_),
    *and* [*Chenxiao*](https://twitter.com/Chenxia58917359) *on Twitter.*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬æ„Ÿè°¢ Yonatan Gideoni å’Œ Fan Nie å¯¹è¿™ç¯‡æ–‡ç« çš„æ ¡å¯¹ã€‚æœ‰å…³å›¾ä¸Šçš„æ·±åº¦å­¦ä¹ çš„æ›´å¤šæ–‡ç« ï¼Œè¯·å‚è§ Michael çš„* [*å…¶ä»–æ–‡ç« *](https://towardsdatascience.com/graph-deep-learning/home)
    *åœ¨ Towards Data Science ä¸Šï¼Œ* [*è®¢é˜…*](https://michael-bronstein.medium.com/subscribe)
    *ä»–çš„æ–‡ç« ä»¥åŠ* [*YouTube é¢‘é“*](https://www.youtube.com/c/MichaelBronsteinGDL)*ï¼Œè·å–* [*Medium
    ä¼šå‘˜èµ„æ ¼*](https://michael-bronstein.medium.com/membership)*ï¼Œæˆ–è€…å…³æ³¨* [*Michael*](https://twitter.com/mmbronstein)ï¼Œ[*Qitian*](https://twitter.com/qitianwu_)
    *å’Œ* [*Chenxiao*](https://twitter.com/Chenxia58917359) *åœ¨ Twitter ä¸Šã€‚*'
