- en: Evaluating Uplift Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/evaluating-uplift-models-8a078996a113](https://towardsdatascience.com/evaluating-uplift-models-8a078996a113)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[CAUSAL DATA SCIENCE](https://towardsdatascience.com/tagged/causal-data-science)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How to compare and select the best uplift model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@matteo.courthoud?source=post_page-----8a078996a113--------------------------------)[![Matteo
    Courthoud](../Images/d873eab35a0cf9fc696658c0bee16b33.png)](https://medium.com/@matteo.courthoud?source=post_page-----8a078996a113--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8a078996a113--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8a078996a113--------------------------------)
    [Matteo Courthoud](https://medium.com/@matteo.courthoud?source=post_page-----8a078996a113--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8a078996a113--------------------------------)
    ¬∑18 min read¬∑Jul 13, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8182be522fb1050cb15fce92296a5526.png)'
  prefs: []
  type: TYPE_IMG
- en: Cover, image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most widespread applications of causal inference in the industry
    is **uplift modeling**, a.k.a. the estimation of Conditional Average Treatment
    Effects.
  prefs: []
  type: TYPE_NORMAL
- en: When estimating the causal effect of a **treatment** (a drug, ad, product, ‚Ä¶)
    on an **outcome** of interest (a disease, firm revenue, customer satisfaction,
    ‚Ä¶), we are often not only interested in understanding whether the treatment works
    on average, but we would like to know for which **subjects** (patients, users,
    customers, ‚Ä¶) it works better or worse.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating heterogeneous incremental effects, or uplift, is an essential intermediate
    step to improve the **targeting** of the policy of interest. For example, we might
    want to warn certain people that they are more likely to experience side effects
    from a drug or show an advertisement only to a specific set of customers.
  prefs: []
  type: TYPE_NORMAL
- en: While there exist many methods to model uplift, it is not always clear which
    one to use in a specific application. Crucially, because of the **fundamental
    problem of causal inference**, the objective of interest, the uplift, is never
    observed, and therefore we cannot validate our estimators as we would do with
    a machine learning prediction algorithm. We cannot set aside a validation set
    and pick the best-performing model since we have **no ground truth**, not even
    in the validation set, and not even if we ran a randomized experiment.
  prefs: []
  type: TYPE_NORMAL
- en: What can we do then? In this article, I try to cover the most popular methods
    used to **evaluate uplift models**. If you are unfamiliar with uplift models,
    I suggest first reading my introductory article.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/understanding-meta-learners-8a9c1e340832?source=post_page-----8a078996a113--------------------------------)
    [## Understanding Meta Learners'
  prefs: []
  type: TYPE_NORMAL
- en: Edit description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/understanding-meta-learners-8a9c1e340832?source=post_page-----8a078996a113--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Uplift and Promotional Emails
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine we were working in the marketing department of a product company interested
    in improving our **email marketing campaign**. Historically, we mostly sent emails
    to new customers. However, now we would like to adopt a data-driven approach and
    target customers for whom the email has the highest positive impact on revenue.
    This impact is also called **uplift** or **incrementality**.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs have a look at the data we have at our disposal. I import the data-generating
    process `dgp_promotional_email()` from `[src.dgp](https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py)`.
    I also import some plotting functions and libraries from `[src.utils](https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py)`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/56bf3d1577f8467bf3c41f099cde4155.png)'
  prefs: []
  type: TYPE_IMG
- en: Data snapshot, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: We have information on 500 customers, for whom we observe whether they are `new`
    customers, their `age`, the sales they generated before the email campaign (`sales_old`),
    whether they were sent the `mail`, and the `sales` after the email campaign.
  prefs: []
  type: TYPE_NORMAL
- en: The **outcome** of interest is `sales`, which we denote with the letter *Y*.
    The **treatment** or policy that we would like to improve is the `mail` campaign,
    which we denote with the letter *W*. We call all the remaining variables **confounders**
    or control variables and we denote them with *X*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The Directed Acyclic Graph (DAG) representing the causal relationships between
    the variables is the following. The causal relationship of interest is depicted
    in green.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3cd02967b39e52c491c51e76b93d8fe9.png)'
  prefs: []
  type: TYPE_IMG
- en: Directed Acyclic Graph (DAG) of the data generating process
  prefs: []
  type: TYPE_NORMAL
- en: From the DAG, we see that the `new` customer indicator is a confounder and needs
    to be controlled in order to identify the effect of `mail` on `sales.` `age` and
    `sales_old` instead are not essential for estimation but could be helpful for
    identification. For more information on DAGs and control variables, you can check
    my introductory article.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/controls-b63dc69e3d8c?source=post_page-----8a078996a113--------------------------------)
    [## DAGs and Control Variables'
  prefs: []
  type: TYPE_NORMAL
- en: Edit description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/controls-b63dc69e3d8c?source=post_page-----8a078996a113--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective of uplift modeling is to recover the **Individual Treatment Effects
    (ITE)** *œÑ·µ¢*, i.e. the incremental effect on `sales` of sending the promotional
    `mail`. We can express the ITE as the difference between two hypothetical quantities:
    the potential outcome of the customer if they had received the email, *Y·µ¢‚ÅΩ¬π‚Åæ*,
    minus the potential outcome of the customer if they had *not* received the email,
    *Y·µ¢‚ÅΩ‚Å∞‚Åæ*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aac44a8c581fb6a58429a0ee35e3d2cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Individual Treatment Effect (ITE), image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Note that for each customer, we only observe one of the two realized outcomes,
    depending on whether they actually received the `mail` or not. Therefore, the
    ITE is inherently unobservable. What can be estimated instead is the **Conditional
    Average Treatment Effect (CATE),** i.e., the expected individual treatment effect
    *œÑ·µ¢*, conditional on covariates *X*. For example, the average effect of the `mail`
    on `sales` for older customers (`age` > 50).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c77d68e5b94093232b0aa8a73dfecde.png)'
  prefs: []
  type: TYPE_IMG
- en: Conditional Average Treatment Effect (CATE), image by Author
  prefs: []
  type: TYPE_NORMAL
- en: In order to be able to recover the CATE, we need to make three assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Unconfoundedness**: *Y‚ÅΩ‚Å∞‚Åæ, Y‚ÅΩ¬π‚Åæ ‚ä• W | X*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Overlap**: *0 < e(X) < 1*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Consistency**: *Y = W ‚ãÖ Y‚ÅΩ¬π‚Åæ + (1‚àíW) ‚ãÖ Y‚ÅΩ‚Å∞‚Åæ*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Where *e(X)* is the **propensity score,** i.e., the expected probability of
    being treated, conditional on covariates *X*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/53ea9284b69f5874084c4d3b3c3bbfed.png)'
  prefs: []
  type: TYPE_IMG
- en: Propensity score, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: In what follows, we will use machine learning methods to estimate the CATE *œÑ(x)*,
    the propensity scores *e(x)*, and the conditional expectation function (CEF) of
    the outcome, *Œº(x)*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/348e64d87bf5d9f047e446f0673fac50.png)'
  prefs: []
  type: TYPE_IMG
- en: Outcome Conditional Expectation Function (CEF), image by Author
  prefs: []
  type: TYPE_NORMAL
- en: We use [Random Forest Regression](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)
    algorithms to model the CATE and the outcome CEF, while we use [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
    to model the propensity score.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this article, we do not fine-tune the underlying machine learning models,
    but fine-tuning is strongly recommended to improve the accuracy of uplift models
    (for example, with auto-ml libraries like [FLAML](https://microsoft.github.io/FLAML/)).
  prefs: []
  type: TYPE_NORMAL
- en: Uplift Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There exist **many methods** to model uplift or, in other words, to estimate
    Conditional Average Treatment Effects (CATE). Since the objective of this article
    is to compare methods to *evaluate* uplift models, we will not explain the methods
    in detail. For a gentle introduction, you can check [my introductory article on
    meta-learners](https://medium.com/towards-data-science/understanding-meta-learners-8a9c1e340832).
  prefs: []
  type: TYPE_NORMAL
- en: 'The learners that we will consider are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: S-learner or single-learner, introduced by [Kunzel, Sekhon, Bickel, Yu (2017)](https://arxiv.org/abs/1706.03461)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T-learner or two-learner, introduced by [Kunzel, Sekhon, Bickel, Yu (2017)](https://arxiv.org/abs/1706.03461)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: X-learner or cross-learner, introduced by [Kunzel, Sekhon, Bickel, Yu (2017)](https://arxiv.org/abs/1706.03461)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R-learner or [Robinson](https://www.jstor.org/stable/1912705)-learner introduced
    by [Nie, Wager (2017)](https://arxiv.org/abs/1712.04912)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DR-learner or doubly-robust-learner, introduced by [Kennedy (2022)](https://arxiv.org/abs/2004.14497)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We import all the models from Microsoft‚Äôs [econml](https://econml.azurewebsites.net/)
    library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We `fit()` the models on the data, specifying the outcome variable *Y*, the
    treatment variable *W* and covariates *X*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready to evaluate the models! Which model should we choose?
  prefs: []
  type: TYPE_NORMAL
- en: Oracle Loss Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main problem of evaluating uplift models is that, even with a validation
    set and even with a randomized experiment or AB test, we do **not observe** our
    metric of interest: the Individual Treatment Effects. In fact, we only observe
    the realized outcomes, *Y·µ¢‚ÅΩ‚Å∞‚Åæ* for untreated customers and *Y·µ¢‚ÅΩ¬π‚Åæ* for treated
    customers. Therefore, we **cannot compute** the individual treatment effect, *œÑ·µ¢
    = Y·µ¢‚ÅΩ¬π‚Åæ ‚àí Y·µ¢‚ÅΩ‚Å∞‚Åæ*, in the validation data for any customer.'
  prefs: []
  type: TYPE_NORMAL
- en: Can we still do something to **evaluate** our estimators?
  prefs: []
  type: TYPE_NORMAL
- en: The answer is yes, but before giving more details, let‚Äôs first understand what
    we would do if we **could observe** the Individual Treatment Effects *œÑ·µ¢*.
  prefs: []
  type: TYPE_NORMAL
- en: Oracle MSE Loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we could observe the individual treatment effects (but we don‚Äôt, hence the
    ‚Äúoracle‚Äù attribute), we could try to measure how far our estimates *œÑÃÇ(X·µ¢)* are
    from the true values *œÑ·µ¢*. This is what we normally do in machine learning when
    we want to evaluate a prediction method: we set aside a validation dataset, and
    we compare predicted and true values on that data. There exist plenty of loss
    functions to evaluate prediction accuracy, so let‚Äôs concentrate on the most popular
    one: the **Mean Squared Error (MSE) loss**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38df3b8e20ebf572d3b29f88eb6fdf1b.png)'
  prefs: []
  type: TYPE_IMG
- en: Oracle MSE loss function, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The function `compare_methods` prints and plots evaluation metrics computed
    on a separate validation dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/718c577c72131582e4f5397735a2b60a.png)'
  prefs: []
  type: TYPE_IMG
- en: Oracle MSE loss values, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we see that the T-learner clearly performs worst, with the S-learner
    just behind. On the other hand, the X-, R- and DR-learners perform significantly
    better, with the **DR-learner winning** the race.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this might *not* be the best loss function to evaluate our uplift
    model. In fact, uplift modeling is just an intermediate step towards our ultimate
    goal: improving revenue.'
  prefs: []
  type: TYPE_NORMAL
- en: Oracle Policy Gain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since our ultimate goal is to **improve revenue**, we could evaluate estimators
    by how much they increase revenue, given a certain policy function. Suppose, for
    example, that we had a 0.01$ cost of sending an email. Then, our policy would
    be to treat each customer with a predicted Conditional Average Treatment Effect
    above 0.01$.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'How much would our revenue actually increase? Let‚Äôs define with *d(œÑÃÇ)* our
    policy function, such that *d=1* if *œÑ ‚â• 0.1* and *d=0* otherwise. Then our *gain*
    (higher is better) function is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2d8ad6dd41a3932dc4d3572f1a44ee9b.png)'
  prefs: []
  type: TYPE_IMG
- en: Oracle policy gain function, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Again, this is an ‚Äúoracle‚Äù loss function that **cannot be computed** in reality
    since we do not observe the individual treatment effects.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ef9d36d2a5b8ac807f36c43a6cfa320d.png)'
  prefs: []
  type: TYPE_IMG
- en: Oracle policy gain values, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the S-learner is clearly the worst performer, leading to no effect
    on revenues. The T-learner leads to modest gains, while the X-, R- and DR- learners
    all lead to aggregate gains, with the **X-learner slightly ahead**.
  prefs: []
  type: TYPE_NORMAL
- en: Practical Loss Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we have seen two examples of loss functions that we
    would like to compute if we could observe the Individual Treatment Effects *œÑ·µ¢*.
    However, in practice, even with a randomized experiment and even with a validation
    set, we do not observe the ITE, our object of interest. We will now cover some
    measures that try to evaluate uplift models, given this practical constraint.
  prefs: []
  type: TYPE_NORMAL
- en: Outcome Loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first and simplest approach is to switch to a different loss variable. While
    we cannot observe the Individual Treatment Effects, *œÑ·µ¢*, we can still observe
    our outcome *Y·µ¢*. This is not exactly our object of interest, but we might expect
    an uplift model that performs well in terms of predicting *y* also to produce
    good estimates of *œÑ*.
  prefs: []
  type: TYPE_NORMAL
- en: One such loss function could be the **Outcome MSE loss**, which is the usual
    MSE loss function for prediction methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ae12af05bb02493c11b7263d8a1becb4.png)'
  prefs: []
  type: TYPE_IMG
- en: Outcome MSE loss function, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The problem here is that not all models directly produce an estimate of *Œº(x)*.
    Therefore, we skip this comparison and switch to methods that can evaluate any
    uplift model.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction to Prediction Loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another very simple approach could be to compare the predictions of the model
    trained on the training set with the predictions of another model trained on the
    validation set. While intuitive, this approach could be **extremely misleading**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8c286e12dbdbf510e239ac05e5628488.png)'
  prefs: []
  type: TYPE_IMG
- en: Prediction-to-prediction MSE losss values, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Unsurprisingly, this metric performs extremely badly, and you should **never
    use it** since it rewards models that are consistent, irrespectively of their
    quality. A model that always predicts a random constant CATE for each observation
    would obtain a perfect score.
  prefs: []
  type: TYPE_NORMAL
- en: Distribution Loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A different approach is to ask: how well can we match the distribution of potential
    outcomes? We can do this exercise for either the *treated* or *untreated* potential
    outcomes. Let‚Äôs take the last case. Suppose we take the observed `sales` for customers
    that did *not* receive the `mail` and the observed `sales` *minus* the estimated
    CATE *œÑÃÇ(x)* for customers that did receive the `mail`. By the **unconfoundedness**
    assumption, these two distributions of the untreated potential outcome should
    be similar, conditional on covariates *X*.'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we expect the distance between the two distributions to be close
    if we have correctly estimated the treatment effects.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eeb34245b68476c0bd20dee426c4ca85.png)'
  prefs: []
  type: TYPE_IMG
- en: Untreated potential outcome distance, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: We can also do the same exercise for the *treated* potential outcome.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/453be0804bac6bfa71387084b6c92f4e.png)'
  prefs: []
  type: TYPE_IMG
- en: Treated potential outcome distance, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: We use the [energy distance](https://en.wikipedia.org/wiki/Energy_distance)
    as the distance metric.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/590b5e9ba6819a8d225df4dacc459987.png)'
  prefs: []
  type: TYPE_IMG
- en: Untreated potential outcome distance values, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: This measure is extremely noisy and rewards the S-learner followed by the T-learner,
    which are actually the two worst-performing models.
  prefs: []
  type: TYPE_NORMAL
- en: Above-below Median Difference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The above-below median loss tries to answer the question: is our uplift model
    detecting **any heterogeneity**? In particular, if we take the validation set
    and we split the sample into above-median and below-median predicted uplift *œÑÃÇ(x)*,
    how big is the actual difference in average effect, estimated with a difference-in-means
    estimator? We would expect better estimators to better split the sample into high-effects
    and low-effects.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/47e9b9ab0153043e44df1ca6c0b9b2ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Above-below median difference gain values, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the above-below median difference rewards the T-learner, which
    is among the worst-performing models.
  prefs: []
  type: TYPE_NORMAL
- en: It‚Äôs important to note that the difference-in-means estimators in the two groups
    (above- and below- median *œÑÃÇ(x)*) are **not guaranteed to be unbiased**, even
    if the data came from a randomized experiment. In fact, we have split the two
    groups on a variable, *œÑÃÇ(x)*, that is highly endogenous. Therefore, the method
    should be used with a grain of salt.
  prefs: []
  type: TYPE_NORMAL
- en: Uplift Curve
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An extension of the above-below median test is the **uplift curve**. The idea
    is simple: instead of splitting the sample into two groups based on the median
    (0.5 quantile), why not split the data into more groups (more quantiles)?'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each group, we compute the difference-in-means estimate, and we plot its
    cumulative sum against the corresponding quantile. The result is called the **uplift
    curve**. The interpretation is simple: the higher the curve, the better we are
    able to separate high- from low-effect observations. However, also the same **disclaimer**
    applies: the difference-in-means estimates are not unbiased. Therefore, they should
    be used with a grain of salt.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/abbed6c32127b8eddc36c7d71d636f99.png)'
  prefs: []
  type: TYPE_IMG
- en: Uplift curve, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: While probably not the best method to *evaluate* uplift models, the uplift curve
    is very important in **understanding** and **implementing** them. In fact, for
    each model, it tells us that is the expected average treatment effect (y-axis)
    as we increase the share of the treated population (x-axis).
  prefs: []
  type: TYPE_NORMAL
- en: Nearest Neighbor Match
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last couple of methods we analyzed used aggregated data in order to understand
    whether the methods work on larger groups. The nearest neighbor match tries instead
    to understand how well an uplift model predicts individual treatment effects.
    However, since the ITEs are not observable, it tries to build a **proxy by matching**
    treated and control observations on observable characteristics *X*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we take all treated observations (*i: W·µ¢=1*), and we find the
    nearest neighbor in the control group (*NN‚ÇÄ(X·µ¢)*), the corresponding MSE loss
    function is'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03f3aba85a111b1373906b87985a9ac1.png)'
  prefs: []
  type: TYPE_IMG
- en: Nearest neighbor loss function, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/81248da32377e026d4c4549b11e7c8d5.png)'
  prefs: []
  type: TYPE_IMG
- en: Nearest neighbor loss values, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the nearest neighbor loss performs quite well, identifying the
    two worse performing methods, the S- and T-learner.
  prefs: []
  type: TYPE_NORMAL
- en: IPW Loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Inverse Probability Weighting (IPW) loss function was first proposed by
    [Gutierrez, Gerardy (2017)](https://proceedings.mlr.press/v67/gutierrez17a/gutierrez17a.pdf),
    and it is the first of three metrics that we are going to see that use a **pseudo-outcome**
    *Y** to evaluate the estimator. Pseudo-outcomes are variables whose expected value
    is the Conditional Average Treatment Effect, but that are too volatile to be directly
    used as estimates. For a more detailed explanation of pseudo-outcomes, I suggest
    [my article on causal regression trees](/920177462149). The pseudo-outcome corresponding
    to the IPW loss is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e3772e78ac256bc120239afc7eb513d.png)'
  prefs: []
  type: TYPE_IMG
- en: IPW pseudo-outcome, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: so that the corresponding loss function is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2dfb0b821e20461e93237350a7cd1fb0.png)'
  prefs: []
  type: TYPE_IMG
- en: IPW loss function, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b060450ccdbe90a962d595f5643d4e1d.png)'
  prefs: []
  type: TYPE_IMG
- en: IPW loss function values, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The IPW loss is extremely noisy. A solution is to use its more robust variations,
    the R-loss or the DR-loss which we present next.
  prefs: []
  type: TYPE_NORMAL
- en: R Loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The R-loss was introduced together with the R-learner by [Nie, Wager (2017)](https://arxiv.org/abs/1712.04912),
    and it is essentially the **objective function** of the R-learner. As for the
    IPW-loss, the idea is to try to match a pseudo outcome whose expected value is
    the Conditional Average Treatment Effect.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b507442670b3c32a4c1d0e69fb51b82d.png)'
  prefs: []
  type: TYPE_IMG
- en: R pseudo-outcome, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The corresponding loss function is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c91ec81d39e03bb9ff29f5f35091a185.png)'
  prefs: []
  type: TYPE_IMG
- en: R loss function, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1f3b6b7f182ad7889ec7409a11839ba5.png)'
  prefs: []
  type: TYPE_IMG
- en: R loss values, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The R-loss is sensibly less noisy than the IPW loss, and it clearly isolates
    the S-learner. However, it tends to favor its corresponding learner, the R-learner.
  prefs: []
  type: TYPE_NORMAL
- en: DR Loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The DR-loss is the **objective function** of the DR-learner, and it was first
    introduced by [Saito, Yasui (2020)](https://arxiv.org/abs/1909.05299). As for
    the IPW- and the R-loss, the idea is to try to match a pseudo outcome, whose expected
    value is the Conditional Average Treatment Effect. The DR pseudo-outcome is strongly
    related to the [AIPW estimator](/ed4097dab27a), also known as doubly-robust estimator,
    hence the DR name.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a57cdc115e7e040d659213351418157a.png)'
  prefs: []
  type: TYPE_IMG
- en: DR pseudo-outcome, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The corresponding loss function is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2befa8823be6554364a382d23f223e92.png)'
  prefs: []
  type: TYPE_IMG
- en: DR loss function, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/893b54ab36614552dd00282e3d0b36fe.png)'
  prefs: []
  type: TYPE_IMG
- en: DR loss values, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: As for the R-loss, the DR-loss tends to favor its corresponding learner, the
    DR-learner. However, it provides a more accurate ranking in terms of algorithms‚Äô
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Empirical Policy Gain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The last loss function that we are going to analyze is different from all the
    others we have seen so far since it does *not* focus on how well we are able to
    estimate the treatment effects but rather on how well would the corresponding
    **optimal treatment policy** performs. In particular, [Hitsch, Misra, Zhang (2023)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3111957)
    propose the following gain function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe8feb6a39536a9c05ef4ff018220bfd.png)'
  prefs: []
  type: TYPE_IMG
- en: Empirical policy gain function, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: where *c* is the treatment cost and *d* is the optimal treatment policy given
    the estimated CATE *œÑÃÇ(X·µ¢)*. In our case, we assume an individual treatment cost
    of c=0.01$, so that the optimal policy is to treat every customer with an estimated
    CATE larger than 0.01.
  prefs: []
  type: TYPE_NORMAL
- en: The terms *W·µ¢‚ãÖd(œÑÃÇ)* and (1-*W·µ¢)‚ãÖ(1-d(œÑÃÇ))* imply that we use for the calculation
    only individuals for whom the actual treatment *W* corresponds with the optimal
    one, *d*. However, this means that the metric is computed for a different number
    of observations for each model. The inverse weights *eÃÇ(X)* and *1-eÃÇ(X)* correct
    this imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/629b2d1e0b410b032ee314637d433ee1.png)'
  prefs: []
  type: TYPE_IMG
- en: Empirical policy gain values, image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The empirical policy gain performs well, isolating the two worst-performing
    methods, the S- and T-learners.
  prefs: []
  type: TYPE_NORMAL
- en: Meta Studies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we have introduced a wide variety of methods to evaluate uplift
    models, a.k.a. Conditional Average Treatment Effect estimators. We have also tested
    in our simulated dataset, which is a very special and limited example. How do
    these metrics **perform** in general?
  prefs: []
  type: TYPE_NORMAL
- en: '[Schuler, Baiocchi, Tibshirani, Shah (2018)](https://arxiv.org/abs/1804.05146)
    compares the S-loss, T-loss, R-loss, on **simulated data**, for the corresponding
    estimators. They find that the R-loss ‚Äú*is the validation set metric that, when
    optimized, most consistently leads to the selection of a high-performing model*‚Äù.
    The authors also detect the so-called **congeniality bias**: metrics such as the
    R- or DR-loss tend to be biased towards the corresponding learner.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Curth, van der Schaar (2023)](https://arxiv.org/abs/2302.02923) studies a
    broader array of learners from a **theoretical perspective**. They find that ‚Äú*no
    existing selection criterion is globally best across all experimental conditions
    we consider*‚Äù.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Mahajan, Mitliagkas, Neal, Syrgkanis (2023)](https://arxiv.org/abs/2211.01939)
    is the **most comprehensive** study in terms of scope. The authors compare many
    metrics on 144 datasets and 415 estimators. They find that ‚Äú*no metric significantly
    dominates the rest*‚Äù but ‚Äú*metrics that use DR elements seem to always be among
    the candidate winners*‚Äù.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we have explored multiple methods to evaluate uplift models.
    The **main challenge** is the unobservability of the variable of interest, the
    Individual Treatment Effects. Therefore, different methods try to evaluate uplift
    models either using other variables, using proxy outcomes, or approximating the
    effect of implied optimal policies.
  prefs: []
  type: TYPE_NORMAL
- en: It is hard to recommend using a single method since there is **no consensus**
    on which one performs best, neither from a theoretical nor from an empirical perspective.
    Loss functions that use R- and DR- elements tend to perform **consistently better**,
    but are also biased towards the corresponding learners. Understanding how these
    metrics work, however, can help in understanding their biases and limitations
    in order to make the most appropriate decisions depending on the specific scenario.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Curth, van der Schaar (2023), [‚ÄúIn Search of Insights, Not Magic Bullets: Towards
    Demystification of the Model Selection Dilemma in Heterogeneous Treatment Effect
    Estimation‚Äù](https://arxiv.org/abs/2302.02923)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gutierrez, Gerardy (2017), [‚ÄúCausal Inference and Uplift Modeling: A review
    of the literature‚Äù](https://proceedings.mlr.press/v67/gutierrez17a/gutierrez17a.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hitsch, Misra, Zhang (2023), [‚ÄúHeterogeneous Treatment Effects and Optimal Targeting
    Policy Evaluation‚Äù](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3111957)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kennedy (2022), [‚ÄúTowards optimal doubly robust estimation of heterogeneous
    causal effects‚Äù](https://arxiv.org/abs/2004.14497)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kunzel, Sekhon, Bickel, Yu (2017), [‚ÄúMeta-learners for Estimating Heterogeneous
    Treatment Effects using Machine Learning‚Äù](https://arxiv.org/abs/1706.03461)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahajan, Mitliagkas, Neal, Syrgkanis (2023), [‚ÄúEmpirical Analysis of Model Selection
    for Heterogeneous Causal Effect Estimation‚Äù](https://arxiv.org/abs/2211.01939)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nie, Wager (2017), [‚ÄúQuasi-Oracle Estimation of Heterogeneous Treatment Effects‚Äù](https://arxiv.org/abs/1712.04912)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saito, Yasui (2020), [‚ÄúCounterfactual Cross-Validation: Stable Model Selection
    Procedure for Causal Inference Models‚Äù](https://arxiv.org/abs/1909.05299)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schuler, Baiocchi, Tibshirani, Shah (2018), [‚ÄúA comparison of methods for model
    selection when estimating individual treatment effects‚Äù](https://arxiv.org/abs/1804.05146)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Related Articles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Understanding Meta Learners](/8a9c1e340832)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding AIPW, the Doubly-Robust Estimator](/ed4097dab27a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding Causal Trees](/920177462149)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[From Causal Trees to Forests](/43c4536f1481)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can find the original Jupyter Notebook here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/evaluate_uplift.ipynb?source=post_page-----8a078996a113--------------------------------)
    [## Blog-Posts/notebooks/evaluate_uplift.ipynb at main ¬∑ matteocourthoud/Blog-Posts'
  prefs: []
  type: TYPE_NORMAL
- en: Code and notebooks for my Medium blog posts. Contribute to matteocourthoud/Blog-Posts
    development by creating an‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/evaluate_uplift.ipynb?source=post_page-----8a078996a113--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*I really appreciate it!* ü§ó *If you liked the post and want to see more, consider*
    [***following me***](https://medium.com/@matteo.courthoud)*. I post once a week
    on topics related to causal inference and data analysis. I try to keep my posts
    simple but precise, always providing code, examples, and simulations.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Also, a small* ***disclaimer****: I write to learn, so mistakes are the norm,
    even though I try my best. Please, when you spot them, let me know. I also appreciate
    suggestions on new topics!*'
  prefs: []
  type: TYPE_NORMAL
