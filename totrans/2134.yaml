- en: Towards Unbiased Evaluation of Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/towards-unbiased-evaluation-of-large-language-models-9a7315144389](https://towardsdatascience.com/towards-unbiased-evaluation-of-large-language-models-9a7315144389)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How benchmark leakage and data contamination undermine LLMs evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://donatoriccio.medium.com/?source=post_page-----9a7315144389--------------------------------)[![Donato
    Riccio](../Images/0af2a026e72a023db4635522cbca50eb.png)](https://donatoriccio.medium.com/?source=post_page-----9a7315144389--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9a7315144389--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9a7315144389--------------------------------)
    [Donato Riccio](https://donatoriccio.medium.com/?source=post_page-----9a7315144389--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9a7315144389--------------------------------)
    ·7 min read·Dec 9, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc6cb8fa100c8187fb0ad581ac1e56a3.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author. (AI-assisted)
  prefs: []
  type: TYPE_NORMAL
- en: “Our new LLM beats GPT in every benchmark!”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is becoming increasingly common to hear bold claims like this, as the hype
    around LLMs is huge. There are new models every week, and currently everyone is
    trying to compete with GPT-4, which is still the most powerful LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking is a critical part of evaluating progress in large language models.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarks like **MMLU** and **HellaSwag** are the standard for assessing language
    models on skills like reasoning and comprehension. The scores provide a snapshot
    of progress, with new state-of-the-art results heralded as breakthroughs. LLMs
    are usually evaluated in a zero-shot setting, without explicit training on the
    test set, to gauge their general abilities.
  prefs: []
  type: TYPE_NORMAL
- en: This article shows how easy it is to manipulate benchmark results and offers
    suggestions to maintain evaluation integrity.
  prefs: []
  type: TYPE_NORMAL
- en: The Trouble with Benchmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often, benchmarks don’t reflect usefulness in real-life scenarios. Google’s
    newest model, Gemini Ultra, scores **90.04% on MMLU**. While this is an impressive
    score, taking a closer look at the evaluation methodology, it is ***CoT@32***
    (chain of thought with 32 samples). **It means we have to prompt 32 times to get
    90% accuracy!** Most of us are expecting an accurate answer in the first try,
    especially when interacting with a chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5333f4a1ed66aba0aef0296f21106302.png)'
  prefs: []
  type: TYPE_IMG
- en: Google Gemini technical report. [1]
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, **this issue is just the tip of the iceberg of LLMs evaluation.**
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, models are commonly evaluated by measuring their performance
    on a test set that was not used during training. Typically, this process allows
    for an unbiased estimate of how the model will generalize to new data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Benchmark leakage and data contamination are two terms that both refer to
    a concerning issue**: when the test data somehow leaks into the pretraining data
    of LLMs, leading to inflated performance. It makes comparisons between LLMs unfair
    and provides an unreliable measure of progress.'
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation is compromised if examples from the test set leak into the training
    data. This data contamination essentially allows the model to cheat on the test.
  prefs: []
  type: TYPE_NORMAL
- en: Contamination can occur in various ways. Test data might be intentionally or
    unintentionally included in training data. More subtly, if test data is available
    online, web-scraped training data could inadvertently contain test examples. Models
    may also be explicitly trained to regenerate test datasets based on format and
    characteristics. Regardless of the cause, **contamination renders empirical comparisons
    between models invalid.**
  prefs: []
  type: TYPE_NORMAL
- en: This *benchmark leakage* provides an unfair advantage if one LLM has seen data
    related to the test set, but another has not. It casts doubt on claimed improvements
    and makes comparisons misleading, undermining the purpose of benchmarks. Unfortunately,
    leakage is problematic to detect externally and benefits models that exploit it.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing phi-CTNL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our pretraining data for phi-CTNL is constructed by carefully curating an expert-crafted,
    non-synthetic data mixture. Specifically, we first choose the downstream academic
    benchmarks that we wish to evaluate our model on, then pretrain on those benchmarks.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The hilarious paper titled *Pretraining on the Test Set Is All You Need* highlights
    the pitfalls of relying too heavily on benchmarks for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23ed17e9f2d12f2fb11b42014dace25a.png)'
  prefs: []
  type: TYPE_IMG
- en: Perfect scores in every benchmark. [2]
  prefs: []
  type: TYPE_NORMAL
- en: They show how a **small 1 million parameter LLM called phi-CTNL** pre-trained
    on just 100,000 tokens achieves perfect scores across diverse academic benchmarks,
    outperforming state-of-the-art models like GPT-3\. The key? The pretraining data
    consisted solely of the testing data from those exact benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: This is the risk of benchmark leakage — when test data leaks into the pre-trained
    model, evaluation results become meaningless.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Even if meant as a parody, the paper brings attention to a serious issue typically
    unnoticed by the general public.
  prefs: []
  type: TYPE_NORMAL
- en: Demonstrating the Risks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To concretely demonstrate the risks, Zhou et al. [3] take popular LLMs of varying
    sizes like GPT-Neo (1.3B parameters) and LLaMA (65B parameters) and continue pre-training
    them on data related to test sets. They test increasingly severe forms of leakage
    using the training set, test prompts, and the full test set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e5f3c0df05c1022f18e537dcf20a9a2.png)'
  prefs: []
  type: TYPE_IMG
- en: Adding benchmark data to LLM’s training data improves their score on that benchmark.
    [3]
  prefs: []
  type: TYPE_NORMAL
- en: The results are dramatic. On benchmarks like LAMBADA and MMLU, small models
    leapfrog over far larger ones just by training on associated data, **improving
    20–30% in some cases**. For instance, GPT-Neo surpasses LLaMA on many tasks when
    given the training set, despite having 50x fewer parameters. Even language tasks
    in Chinese see a boost, even though the models have little Chinese data overall.
    Clearly, related training data is hugely valuable.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating test prompts provides another massive gain, with models regularly
    achieving over 90% accuracy by learning the exact test format. And with full test
    set leakage, models can score 100% — they simply memorize all examples.
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, it may seem benchmark leakage only causes misleadingly high
    evaluation scores. However, it can negatively impact LLMs in multiple ways. **Performance
    gains are restricted to leaked benchmarks, sometimes decreasing scores on other
    tests.** The model becomes skewed toward specifics of leaked data at the expense
    of general skills.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmark leakage provides illusory progress on a narrow capability while potentially
    harming broader competence — trading generalization for inflated metrics on a
    single benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: Data contamination and maintaining evaluation integrity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLM developers should rigorously check pretraining data against test sets and
    disclose any risks found. Reporting the full composition of pretraining data also
    helps detect leakage. **Unfortunately, most open-source models don’t publish their
    training data.**
  prefs: []
  type: TYPE_NORMAL
- en: Benchmark leakage is not a new problem, but its scale is magnified with LLMs
    containing trillions of parameters pre-trained on internet-scale data. LLMs are
    enormous black boxes, so we can’t know what data has been used to train them.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarks and independent evaluators must keep pace to prevent misleading claims
    of progress.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/everything-you-should-know-about-evaluating-large-language-models-dce69ef8b2d2?source=post_page-----9a7315144389--------------------------------)
    [## Everything You Should Know About Evaluating Large Language Models'
  prefs: []
  type: TYPE_NORMAL
- en: From perplexity to measuring general intelligence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/everything-you-should-know-about-evaluating-large-language-models-dce69ef8b2d2?source=post_page-----9a7315144389--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Also, there are benchmarks where GPT is the evaluator** *(AlpacaEval),* and
    the evaluation may be less meaningful if the tested model is fine-tuned on data
    generated by GPT itself.'
  prefs: []
  type: TYPE_NORMAL
- en: Finding evidence of data contamination
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Checking for data contamination is straightforward, and you can do it yourself.
  prefs: []
  type: TYPE_NORMAL
- en: First, pick a dataset you want to evaluate the model on. This dataset should
    have defined train/dev/test splits. Popular academic datasets like SQuAD, CoNLL
    2003, etc, are good choices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, prompt the model to generate examples from the dataset. Use a prompt
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: Please generate 3 examples from the {dataset} {split} split in the correct format.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now compare the examples generated by the model to actual examples from the
    dataset. If they match, the model likely memorized parts of that split during
    training.
  prefs: []
  type: TYPE_NORMAL
- en: This process is used in the **LM Contamination index**, where they gather evidence
    of data contamination across different LLMs and benchmarks. There is evidence
    of contamination in many LLMs and datasets. [4]
  prefs: []
  type: TYPE_NORMAL
- en: If the model is not an instruct fine-tuned one (e.g., a model able to answer
    questions), prompt the first half of a benchmark instance and see if it can generate
    the rest. **A user on X found evidence of contamination in phi-1.5 on the GSM8k
    dataset using this process.**
  prefs: []
  type: TYPE_NORMAL
- en: Finding evidence of data contamination.
  prefs: []
  type: TYPE_NORMAL
- en: Returning to the Gemini technical report, they mention the data contamination
    issue.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation on these benchmarks is challenging and may be affected by data contamination.
    We performed an extensive leaked data analysis after training to ensure the results
    we report here are as scientifically sound as possible, but still found some minor
    issues and decided not to report results on [some benchmarks]
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The authors conducted extensive leaked data analysis after training the models
    to identify any potential overlap between the training data and test sets. This
    process involved thoroughly evaluating each benchmark used and checking for contamination
    issues.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0687695ca3518043cdb5dd40534a7184.png)'
  prefs: []
  type: TYPE_IMG
- en: Data contamination is acknowledged by Google too. [1]
  prefs: []
  type: TYPE_NORMAL
- en: The authors took steps to report decontaminated results where minor problems
    were found, such as with the HellaSwag benchmark. For HellaSwag, they measured
    performance using 10-shot prompting instead of fewer shots to avoid relying on
    possible training data overlaps.
  prefs: []
  type: TYPE_NORMAL
- en: The authors also emphasized evaluating the models on completely new held-out
    datasets that had confirmed separation from the training data. Examples include
    using new test sets like WMT23 and AMC math problems from 2022–2023 that were
    verified to have no overlap.
  prefs: []
  type: TYPE_NORMAL
- en: For benchmarks where contamination was identified as an issue after initial
    reporting, such as LAMBADA, the authors decided not to report those problematic
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Future directions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Benchmark leakage allows LLMs to cheat, faking progress through contamination
    rather than true improvements in competence. If left unaddressed, this issue undermines
    trust in both benchmarks and LLMs. Following best practices can mitigate the risks,
    keeping benchmarks robust and comparisons fair.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t trust claims of LLMs being better than other ones based on benchmarks
    run by the authors. Benchmarks and evaluation methodology can be cherry-picked
    to only show favorable scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Always try new models yourself before having an opinion.
  prefs: []
  type: TYPE_NORMAL
- en: Or why not experiment with creating your own benchmark? While not easy, you
    can customize it to your use case.
  prefs: []
  type: TYPE_NORMAL
- en: '*If you enjoyed this article, join* [***Text Generation***](https://textgeneration.substack.com/)
    *— our newsletter has two weekly posts with the latest insights on Generative
    AI and Large Language Models.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Also, you can find me on* [***LinkedIn***](https://www.linkedin.com/in/driccio/)***.***'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Google Gemini Technical Report DeepMind Dec 2023](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[[2309.08632] Pretraining on the Test Set Is All You Need (arxiv.org)](https://arxiv.org/abs/2309.08632)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[[2311.01964] Don’t Make Your LLM an Evaluation Benchmark Cheater (arxiv.org)](https://arxiv.org/abs/2311.01964)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[LM Contamination Index (hitz-zentroa.github.io)](https://hitz-zentroa.github.io/lm-contamination/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
