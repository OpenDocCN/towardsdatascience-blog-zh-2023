# 绿色AI：改进AI可持续性的方法和解决方案

> 原文：[https://towardsdatascience.com/green-ai-methods-and-solutions-to-improve-ai-sustainability-861d69dec658](https://towardsdatascience.com/green-ai-methods-and-solutions-to-improve-ai-sustainability-861d69dec658)

## 对一个长期被忽视的话题进行技术性的审视

[](https://pecciaf.medium.com/?source=post_page-----861d69dec658--------------------------------)[![Federico Peccia](../Images/48ad8401c28e87717718f58336cc64cf.png)](https://pecciaf.medium.com/?source=post_page-----861d69dec658--------------------------------)[](https://towardsdatascience.com/?source=post_page-----861d69dec658--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----861d69dec658--------------------------------) [Federico Peccia](https://pecciaf.medium.com/?source=post_page-----861d69dec658--------------------------------)

·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----861d69dec658--------------------------------) ·阅读时间9分钟·2023年6月26日

--

![](../Images/e26e571fe94f4badd859d8df3d7613de.png)

照片由 [Benjamin Davies](https://unsplash.com/@bendavisual?utm_source=medium&utm_medium=referral) 提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

# 介绍

如果你打开了这篇文章，你可能已经听说过关于当前大型语言模型（LLMs）安全性和可信度的争议。由计算机科学界知名人物如**史蒂夫·沃兹尼亚克**、**加里·马库斯**和**斯图尔特·拉塞尔**签署的公开信表达了他们对这一问题的担忧，并要求暂停LLMs的训练6个月。但还有另一个话题正在慢慢引起大量关注，这可能会在不久的将来促使另一封公开信的出现：AI模型训练和推理的能源消耗和碳足迹。

估计仅**训练**流行的GPT-3模型，这一1750亿参数的LLM，便排放了大约502吨二氧化碳[1]。甚至有[在线计算器](https://mlco2.github.io/impact/)可以估算训练某个模型的排放量。但训练步骤并不是唯一消耗能源的环节。训练后，在推理阶段，AI模型每天会被执行数千次或数百万次。即便每次执行消耗的能源较少，经过数周、数月甚至数年的累计消耗，也可能成为一个巨大的问题。

这就是为什么绿色人工智能（Green AI）这一概念越来越受欢迎的原因。它的主要焦点是寻找解决方案和开发技术，以提高人工智能的可持续性，减少其能源消耗和碳足迹。在本文中，我旨在介绍一些正在积极研究的技术和方法，它们可以用来改进这一概念，并且这些内容通常不会以易于理解的方式讨论。本文末尾，你将找到与所讨论主题相关的资源和参考文献。

尽管本文重点讨论了在部署人工智能算法时实现节能的技术方法，但即使你*不是*研究人员，也需要对这些方法有一个大致了解。你是否负责训练公司人工智能算法？那么，也许你可以在训练过程中考虑一些优化，以改善算法部署后的能源消耗。你是否负责选择将要部署算法的硬件？那么请留意本文提到的概念，因为它们可能是前沿优化硬件的标志。

## 计算机架构基础

为了理解本文，基本了解计算机架构以及软件和硬件如何相互作用是至关重要的。这是一个非常复杂的话题，但在进入本文的主要部分之前，我将尝试提供一个简要的总结。

你可能听说过比特（bit），它是任何计算机中最简单的信息单位，也是数字世界存在的原因。比特只能取两种状态：0 或 1。8 个比特组成一个字节（byte）。为了便于讨论，我们可以将任何计算机架构视为 2 个硬件组件，它们操作和存储这些字节：计算单元和内存。

计算单元负责将一组字节作为输入，并生成另一组字节作为输出。例如，如果我们想要计算 7 x 6，我们会将表示 7 的字节插入到乘法器的一个输入端，将表示 6 的字节插入到另一个输入端。乘法器的输出将给我们表示数字 42 的字节，即乘法的结果。这种乘法操作需要一定的时间和能源，直到结果在乘法器的输出端可用。

内存是存储未来使用字节的地方。从内存中读取和写入字节（也称为“访问”内存）需要时间*和*能量。在计算机架构中，内存层次结构通常有多个“层级”，接近计算单元的层级访问时间最快，每字节读取的能量消耗也较少，而离得较远的层级则是最慢且能量消耗最大的。内存的这种层次组织的主要思想是数据重用。使用非常频繁的数据从最后一级内存传输到最接近的一级，并尽可能多次重用。这一概念称为“缓存”，而这些更快且最接近的内存称为L1和L2缓存。

软件负责协调数据从内存到计算单元的移动，然后将结果存储到内存中。因此，软件的决策可以真正影响系统的能量消耗。例如，如果软件请求的数据在L1缓存中不可用，硬件首先需要从L2级别甚至从最后一级中获取数据，从而带来时间延迟和更多的能量消耗。

# 绿色人工智能的技术

现在计算机架构的基础已经建立，我们可以专注于绿色人工智能中使用的具体技术和方法。这些技术分为两个不同的类别：

1.  硬件优化，例如电压/频率调整或近似计算。这些技术作用于电子电路的实际物理设计和特性。

1.  软件优化，例如剪枝、量化、微调等。

## DVFS：动态电压和频率调整

标准硅基电子电路的功耗直接与电路中使用的电压和工作频率相关。在相同的操作条件下，如果降低这些参数中的任何一个，功耗也会随之减少。我们是否可以利用这种行为使人工智能算法的执行更*绿色*？

当然！设想我们有一个小型嵌入式设备连接到电池上，接收多个请求（每个请求有其自身的重要性和约束），用人工智能算法处理这些请求，然后将结果发送回去。我们希望人工智能算法的处理消耗尽可能少的能量，以便让电池尽可能长时间运行，对吧？我们是否可以在处理不那么重要的任务时动态调整设备的电压和工作频率，然后在需要处理关键任务时恢复到正常工作状态？

根据执行AI算法的设备，这确实是一个完全有效的选项！实际上，这是一个活跃的研究领域。如果你对进一步阅读感兴趣，我建议你查看Kim [2] 的《AutoScale：使用强化学习进行随机边缘推理的能效优化》或Hao [3] 的《通过DNN解耦的多智能体协作推理：中间特征压缩与边缘学习》，这些论文提供了如何利用该技术减少AI算法能耗的良好示例。

## 近似计算

在CPU或GPU上执行数学运算时，我们通常期待得到所请求计算的精确结果，对吧？这在使用消费级硬件时通常是这样的。由于乘法是AI算法中最常用的数学操作之一，我们希望在乘以两个整数时得到精确结果，而在乘以两个浮点数时得到一个非常好的近似（这个近似通常如此精确，以至于对基础用户程序没有问题）。我们为什么还要*考虑*插入两个整数而未获得正确的数学结果的可能性呢？

但在过去几年里，一种新的方法正在积极研究。问题很简单：是否可以通过牺牲乘法结果的准确性，设计出更简单的乘法器，从而减少物理面积和能量消耗？更重要的是，这些新乘法器是否可以在实际应用中使用，而不会显著影响性能？这两个问题的答案实际上都是肯定的。这就是被称为近似计算的计算范式。

这真是太吸引人了！目前已经有研究展示了一些近似乘法器，这些乘法器能够提供两个整数相乘的精确结果，且仅对少数输入组合提供不正确的结果，但能在执行整个模型时实现大约20%的能量减少。如果你对这种令人难以置信的技术感兴趣，我鼓励你查看Zervakis [4] 的《用于机器学习的近似计算：最新进展、挑战与展望》，这本书提供了关于这一主题的具体工作的良好概述。

## 剪枝与量化

对于熟悉AI算法训练，尤其是神经网络的人，这两种技术应该很熟悉。对于那些不熟悉这些术语的人，这些概念非常值得阅读。

剪枝是一种基于这样一个观点的方法：神经网络中的参数存在大量冗余，这些参数包含了网络的知识。因此，许多参数可以被移除，而不会实际影响网络的预测。

量化意味着使用更少的字节表示网络的参数。记得我们说过计算机使用一定数量的字节表示数字吗？通常，网络使用一种称为“浮点”的表示方式进行训练，其中每个数字可以占用4或8个字节。但实际上有技术可以只使用1个字节（“整数”表示法）来表示这些参数，并且仍然能保持类似甚至相同的预测质量。

我相信你已经在想象这两种技术如何帮助减少神经网络的能源消耗了。对于剪枝，如果处理一个输入所需的参数更少，会发生两件事来改善算法的能源消耗。首先，需要在计算单元中执行的计算更少。其次，由于计算较少，从内存中读取的数据也更少。对于量化，使用仅一个字节表示的整数来乘以两个数字需要一个更小、更简单的硬件乘法器，这反过来需要更少的能量来进行实际的乘法运算。最后，如果每个参数的大小从8个字节减少到1个字节，这意味着需要从内存中读取的数据量也减少了8倍，从而大大减少了处理一个输入所需的能源消耗。

想了解更多吗？看看 Zhi [5] 的“轻量级参数剪枝以实现节能深度学习：一种二值化门控模块方法”或 Widmann [6] 的“为电力优化：通过神经网络剪枝优化物联网的能源效率”以获取当前该主题的工作示例。

## 微调

由于许多最新的LLM具有封闭的特性，通常需要大量的计算能力来*复制*这些模型的结果。如果这些模型向公众开放，可以应用被称为微调的技术。这是一种在微调训练过程中只修改预训练模型的一部分参数的方法，以将网络专门化为特定任务。这个过程通常需要较少的训练迭代，因此比从头开始重新训练整个网络消耗更少的能源。

这就是为什么向公众开放这些模型不仅会帮助那些试图利用它们构建产品的人，还会帮助那些从头开始重新训练这些模型的研究人员，从而节省大量可以节省的能源。

# 结论

我希望你觉得这些技术和方法与我一样令人着迷。了解到有人积极研究这些技术并尽可能改进在能源节省和碳足迹这样重要的主题上，感到安心和慰藉。

但我们不能坐下来放松，把寻找优化解决方案的责任完全交给从事这些课题的研究人员。你是否正在启动一个新项目？首先检查是否可以微调一个预训练模型。你的硬件是否优化以运行剪枝算法，但你没有有效应用此技术的专业知识？去学习它，或者找一个已经具备技能的人。从长远来看，这将是值得的，不仅对你和你的公司，而且对我们地球整体。

欢迎在[Twitter](https://twitter.com/PecciaF)或[LinkedIn](https://www.linkedin.com/in/fpecc/)上关注我，并告诉我你对这篇文章的看法，或者如果你真的喜欢这篇文章，可以[请我喝杯咖啡](https://www.buymeacoffee.com/pecciaf)！

感谢阅读！

# 参考文献

[1] [“估算BLOOM的碳足迹，一个1760亿参数的语言模型”](https://arxiv.org/pdf/2211.02001.pdf)

[2] [“AutoScale：使用强化学习优化随机边缘推断的能效”](https://microarch.org/micro53/papers/738300b082.pdf)

[3] [“通过DNN解耦的多智能体协作推断：中间特征压缩和边缘学习”](https://arxiv.org/abs/2205.11854)

[4] [“机器学习的近似计算：最新进展、挑战与愿景”](http://slam.ece.utexas.edu/pubs/aspdac21.AxC.pdf)

[[5] “轻量级参数剪枝以实现能效深度学习：二值化门控模块方法”](https://arxiv.org/abs/2302.10798)

[[6] “优化能效：通过神经网络剪枝提高物联网能效”](https://link.springer.com/chapter/10.1007/978-3-031-34204-2_22)
