- en: 'Understanding AUC Scores in Depth: What’s the Point?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understanding-auc-scores-in-depth-whats-the-point-5f2505eb499f](https://towardsdatascience.com/understanding-auc-scores-in-depth-whats-the-point-5f2505eb499f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exploring alternative metrics alongside for deeper insights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@MahamsMultiverse?source=post_page-----5f2505eb499f--------------------------------)[![Maham
    Haroon](../Images/5a9ac82369ecbf7719b765ec160a70ef.png)](https://medium.com/@MahamsMultiverse?source=post_page-----5f2505eb499f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5f2505eb499f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5f2505eb499f--------------------------------)
    [Maham Haroon](https://medium.com/@MahamsMultiverse?source=post_page-----5f2505eb499f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5f2505eb499f--------------------------------)
    ·11 min read·Sep 2, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3dfdfb8105f958d905f36f02fa339e0a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jonathan Greenaway](https://unsplash.com/@jogaway?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/TNUEQ73frkA?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Hello there!
  prefs: []
  type: TYPE_NORMAL
- en: Today, we are delving into a specific metrics used for evaluating model performance
    — the AUC score. But before we delve into the specifics, have you ever wondered
    why unintuitive scores are at times necessary to assess the performance of our
    models?
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether our model handles a single class or multiple classes, the underlying
    objective remains constant: optimizing accurate predictions while minimizing incorrect
    ones. To explore this basic objective, let’s first look at the obligatory confusion
    matrix encompassing True Positives, False Positives, True Negatives, and False
    Negatives.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4fc6d99b4ef3d5cf0a1584103cfb21c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'For any classification or prediction problem, there are only two outcomes:
    True or False.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Consequently, every metric designed to gauge the performance of a prediction
    or classification algorithm is founded on these two measures. The simplest metric
    that accomplishes this is **Accuracy**.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In context of classification and prediction accuracy signifies the proportion
    of correctly predicted instances amongst the total. It’s a very straightforward
    and intuitive measure of a model’s predictive performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a5fc8223fbf73e0eb5ef4a6e20fa02a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**However, is accuracy truly sufficient?**'
  prefs: []
  type: TYPE_NORMAL
- en: While accuracy is a good general measure of a models performance, it’s inadequacy
    becomes evident when we examine the table below that we will frequently reference
    in this article. The table shows performance metrics of four models, each with
    somewhat suboptimal results but, all these models exhibit high accuracy. For instance
    in the first and second case, there’s a clear bias towards one class, resulting
    in dismal classification for the less common class yet the accuracy is 90% which
    is quite misleading.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b13f52866214becd569a5cf32ec220cb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'This helps us conclude:'
  prefs: []
  type: TYPE_NORMAL
- en: While accuracy is valuable, it can sometimes mislead, especially in scenarios
    of class imbalances or when certain errors carry substantial consequences.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For instance, in situations where the cost of missing positive cases (Type 2
    error) or falsely identifying negatives (Type 1 error) is high, relying solely
    on accuracy might not provide a comprehensive assessment of a model’s effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: The strengths of accuracy lie in its simplicity and its applicability across
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, having considered accuracy, let’s venture a little deeper into the realm
    of prediction and classification, several questions emerge:'
  prefs: []
  type: TYPE_NORMAL
- en: What is our objective?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is our data balanced?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we prioritize one class over the other?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we lean towards avoiding False Positives (Type 2 error), or do we emphasize
    minimizing False Negatives (Type 1 error)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After asking these questions, it seems a little trivial to evaluate model performance
    based on just accuracy so we turn our attention to three other metrics for evaluating
    model performance, namely precision, recall and f1-score
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Precision gauges the accuracy with which our model identifies a specific class,
    in a 2 class scenario that is usually the positive class. It measures the reliability
    of predictions for the said class.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a scenario where a machine learning algorithm predicts loan approvals
    based on borrower characteristics. While occasional loan denials of eligible candidate
    (False Negative) might be acceptable to the company, the primary concern is avoiding
    unwarranted approval of loans to individuals who should not qualify (False Positive).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4aaf3a1d480989f4919823e04d30be3.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: In that essence, precision aims to minimize Type 2 errors — instances where
    items are incorrectly accepted when they should be rejected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s demonstrate this by returning to our table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b13f52866214becd569a5cf32ec220cb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: We observe here from cases 1 and 3 that a higher precision is achieved when
    the ratio of true positive to false positive predictions for a specific (positive)
    class is large irrespective of the actual model performance. Thus,
  prefs: []
  type: TYPE_NORMAL
- en: For a specific class, high precision points towards a low Type 2 error.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Next, we have a counterpart to precision for Type 1 errors:'
  prefs: []
  type: TYPE_NORMAL
- en: Recall, Sensitivity, or TPR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall just like precision centers on our predictive ability for a specific
    class. It quantifies how effectively we can accurately select instances belonging
    to a particular category from the entire pool.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a scenario where our models aim is to prevent credit fraud. We can
    probably handle cases where a non-fraudulent activity is flagged as fraudulent(False
    Positive) but we do not want to miss an activity that might actually be fraudulent
    (False Negative).
  prefs: []
  type: TYPE_NORMAL
- en: In this context, aiming for high recall involves minimizing Type 1 errors —
    ensuring that you capture as many relevant instances as possible, even if it means
    flagging a few innocent ones along the way.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba3b20bc631a9c2205023f879db40e7f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Lets return to our table for the third time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b13f52866214becd569a5cf32ec220cb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: We can see from cases 1 and 4 that, recall excels when positive classifications
    are maximized. In these cases, even if false positives are present or the negative
    class’s performance is subpar, recall remains high. Therefore,
  prefs: []
  type: TYPE_NORMAL
- en: High recall for a specific class ensures the minimization of Type 1 errors.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Now, what if we aim to minimize both types of errors for a class?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where the F1 score comes into play:'
  prefs: []
  type: TYPE_NORMAL
- en: F1-Score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: F1-Score is the harmonic mean between precision and recall. F1 score essentially
    tries to find a balance between precision and recall of a class and in that manner
    also attempts at a balance between type 1 and type 2 errors.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, he F1 score shows a classification model’s overall effectiveness
    for a particular class.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8f8bba75bc0197707df53cdd9eeaba4a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s return to our table for the fourth and final perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b13f52866214becd569a5cf32ec220cb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: This time, we observe that the F1 score performs well when both precision and
    recall excel which is true for case 1\. Note that the model is still performing
    sub-optimally for this scenario but since the number of True Positives is high,
    and both False Positives and False Negatives are small in number, the score for
    F1 is high. We can therefore infer,
  prefs: []
  type: TYPE_NORMAL
- en: A high F1 score, being more conservative, for a specific class, minimizes both
    Type 1 and Type 2 errors.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Having examined the other metrics, it’s evident that each of them possesses
    certain limitations. Further more unlike accuracy, F1, Precision, and Recall are
    not class-agnostic, while accuracy remains vulnerable to class imbalances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, we never asked the question: “What’s the prediction threshold?” In
    other words, is there a clear separation between classes? Are positive instances
    assigned predictions around 0.8–0.9, or are they closer to 0.51?'
  prefs: []
  type: TYPE_NORMAL
- en: This is where the ROC Curve and AUC score come into play, albeit with a fair
    degree of unintuitiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the AUC Score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The AUC Score, also known as the Area Under the Curve, is a score measured by
    calculating the area under the Receiver Operating Characteristic (ROC) Curve.
  prefs: []
  type: TYPE_NORMAL
- en: The ROC curve is a plot with Recall/True Positive Rate (TPR) on the y-axis and
    False Positive Rate (FPR) on the x-axis. The peculiar name of ROC stems from its
    origin in the field of Electrical Engineering.
  prefs: []
  type: TYPE_NORMAL
- en: In order to construct the ROC curve, FPR and TPR are calculated for various
    classification thresholds. A classification threshold refers to the prediction
    value, such as 0.5, above which instances are classified differently than those
    below 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09b994478b7997e5f0ab24c5e007e97c.png)'
  prefs: []
  type: TYPE_IMG
- en: By cmglee, MartinThoma — Roc-draft-xkcd-style.svg, CC BY-SA 4.0, [https://commons.wikimedia.org/w/index.php?curid=109730045](https://commons.wikimedia.org/w/index.php?curid=109730045)
  prefs: []
  type: TYPE_NORMAL
- en: However, since creating the complete curve can be cumbersome and does not provide
    a quantifiable measure, the area under the curve is measured instead.
  prefs: []
  type: TYPE_NORMAL
- en: Below, I show progression of creating a ROC curve by plotting FPR vs TPR values
    for various classification thresholds. I mark each new point added as red and
    the prior ones as blue. We can see that upon joining these independent points
    the curve looks quite a bit similar to the light blue curve in the above image
    and seems consistent with the AUC score of 0.91 for the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8eb9a498e09fc491e27836767e5bd5fe.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The figure also shows how changing thresholds affects other metrics like Accuracy,
    F1, precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code use to create the above is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This code snippet showcases the process of creating an ROC curve and calculating
    the AUC score using Python. It involves generating a synthetic dataset, splitting
    it into training and testing sets, training a logistic regression model, and then
    plotting points on the ROC curve for different classification thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: '**A tangent: Would it matter if instead it was True Negative Rate against False
    Negative Rate?**'
  prefs: []
  type: TYPE_NORMAL
- en: Nope, since TNR = 1-FPR and FNR = 1-TPR
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d9e14e165db77e4dacc68f5a353a16dd.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: What about multiple classes?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Well at a time ROC Curve can be calculated for only two classes so for multiple
    classes it can get a bit complex but can do one-vs-all for every class.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve seen how it’s constructed let’s go a little bit deeper on how
    to interpret the AUC score and what does it mean for the model.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the AUC Score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We want to look at 4 cases and I demonstrate this via images where everything
    at the right side of the classification threshold is predicted as positive and
    everything on the left is predicted as negative. The actual labels for the data
    are in the respective rectangles representing them.
  prefs: []
  type: TYPE_NORMAL
- en: 0 AUC Score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A 0 AUC score would be quite hard to achieve and possibly points towards some
    serious human error. It means that TPR is 0 at every classification threshold
    except when FPR is 1\. At FPR 1, TPR goes from 0–1 at various classification thresholds
    this making area under this curve effectively 0\. Visually it’ll mean that there’s
    perfect class separation but every label is reversed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f725b5762337884cde40295855d90657.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**0.5 AUC Score**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now this is the case where the model didn’t learn anything and classification
    or prediction is just random. In this case the ROC curve shows a linear relationship
    or straight proportional line between TPR and FPR. Visually, it would seem that
    there’s effectively no class separation at all.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c7891467fe1d6a4ab7f3cc2c865275c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**0.5 <AUC Score < 1**'
  prefs: []
  type: TYPE_NORMAL
- en: These are the most common cases if we do things right in our model. An AUC score
    between 0.5 and 1 means that the model has at least learned something superior
    to a random classifier but there are still instances in the data that overlap
    and the model can’t effectively separate the classes completely. The closer the
    AUC score is to one, the greater is the separation between the classes achieved.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3cb76116d4735e976377d8a315fac229.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Perfect AUC Score**'
  prefs: []
  type: TYPE_NORMAL
- en: A perfect AUC score of 1.0 indicates that the model has perfect discrimination
    ability. However, achieving such a perfect AUC score can indeed be a sign of potential
    overfitting and unrealistic model behavior, particularly when dealing with real-world
    datasets and scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: The TPR is effectively 1 for all cases except for when FPR is 0\. Eventually
    leading to an area under curve equalling 1\. Visually this means that there is
    perfect class separation between the two classes and the prediction is also correct.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e805e857c9e26b9a51d2627d3ff6780c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that, real-world data is often noisy and contains inherent
    uncertainty. Expecting a model to produce perfect separation can be unrealistic
    and even if it happens, it’s probably a case of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: In case, where an unlikely AUC score of 1 is achieved, the model will likely
    fail to generalize effectively and perform well in practical scenarios due to
    high probability of overfitting.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A more balanced and robust model is one that achieves a reasonably high AUC
    score while also allowing for some level of uncertainty in its predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at some benefits of using AUC score.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of AUC Score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are quite a few benefits to AUC score compared to other metrics
  prefs: []
  type: TYPE_NORMAL
- en: '**Class Agnostic:** In contrast to metrics like precision, recall, and F1 score,
    which are dependent on a chosen positive class, AUC provides a more global assessment
    of a model’s discriminative power, regardless of class distribution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prediction Threshold Agnostic:** One of the distinguishing features of AUC
    score is that it considers the model’s performance across different classification
    thresholds, providing a comprehensive view of its ability to discriminate between
    classes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Insensitive to Class Imbalance:** Since AUC measures how well the model can
    rank positive and negative instances relative to each other, it is less prone
    to distortions caused by imbalanced class distributions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**ROC Curve Threshold Selection:** While AUC is threshold-agnostic, the ROC
    curve itself can help you visually choose a threshold that gives the best performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Disadvantages of AUC Score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While there aren’t many disadvantage and this isn’t an exhaustive list, it’s
    important to note that under extreme imbalance of data AUC score may be affected.
    Furthermore, AUC treats all misclassifications equally. In many real-world scenarios,
    the costs and benefits associated with different types of errors can vary. AUC
    doesn’t take this into account and might not fully represent the performance in
    cases where one type of error is more critical than another.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping UP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So let’s wrap up by saying that although AUC score is an excellent measure of
    model performance each metric has its strengths under the right context.
  prefs: []
  type: TYPE_NORMAL
- en: As we come to an end I hope this was a valuable read and would like you to walk
    away with clear understanding of what an AUC score is and how to interpret it.
  prefs: []
  type: TYPE_NORMAL
- en: If you found this insightful let me know in the comments. :)
  prefs: []
  type: TYPE_NORMAL
- en: Other Resource for AUC Score…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8204137/?source=post_page-----5f2505eb499f--------------------------------)
    [## Magician''s Corner: 9\. Performance Metrics for Machine Learning Models'
  prefs: []
  type: TYPE_NORMAL
- en: '"There are no solutions; there are only trade-offs." In the previous articles,
    we showed how to process images and…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.ncbi.nlm.nih.gov](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8204137/?source=post_page-----5f2505eb499f--------------------------------)
    [](https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-machine-learning-tips-and-tricks?source=post_page-----5f2505eb499f--------------------------------)
    [## CS 229 — Machine Learning Tips and Tricks Cheatsheet
  prefs: []
  type: TYPE_NORMAL
- en: Teaching page of Shervine Amidi, Graduate Student at Stanford University.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: stanford.edu](https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-machine-learning-tips-and-tricks?source=post_page-----5f2505eb499f--------------------------------)
    [](https://www.statology.org/what-is-a-good-auc-score/?source=post_page-----5f2505eb499f--------------------------------)
    [## What is Considered a Good AUC Score? - Statology
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial explains what is considered to be a good value for AUC (area under
    curve), including several examples.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.statology.org](https://www.statology.org/what-is-a-good-auc-score/?source=post_page-----5f2505eb499f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
