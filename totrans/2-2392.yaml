- en: Winning with Simple, not even Linear Time-Series Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/winning-with-simple-not-even-linear-time-series-models-6ece77be22bc](https://towardsdatascience.com/winning-with-simple-not-even-linear-time-series-models-6ece77be22bc)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If your dataset is small, the subsequent ideas might be useful
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://sarem-seitz.medium.com/?source=post_page-----6ece77be22bc--------------------------------)[![Sarem
    Seitz](../Images/f833f915a0eb061f47524a67685ba76c.png)](https://sarem-seitz.medium.com/?source=post_page-----6ece77be22bc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6ece77be22bc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6ece77be22bc--------------------------------)
    [Sarem Seitz](https://sarem-seitz.medium.com/?source=post_page-----6ece77be22bc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6ece77be22bc--------------------------------)
    ·9 min read·May 10, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df6c32f54e340c91d39bddc987d1cca5.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Thomas Bormans](https://unsplash.com/@thomasbormans?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    from [Unsplash](https://unsplash.com/de/s/fotos/time?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: '**Disclaimer:** Title heavily inspired by [this](https://www.youtube.com/watch?v=68ABAU_V8qI&pp=ygUad2lubmluZyB3aXRoIHNpbXBsZSBtb2RlbHM%3D&ref=sarem-seitz.com)
    great talk.'
  prefs: []
  type: TYPE_NORMAL
- en: As the name implies, today we want to consider almost trivially simple models.
    Although the current trend points towards complex models, even for time-series
    models, I am still a big believer in simplicity. In particular, when your dataset
    is small, the subsequent ideas might be useful.
  prefs: []
  type: TYPE_NORMAL
- en: To be fair, this article will probably be most valuable for people who are just
    starting out with time-series analysis. Anyone else should check the table of
    contents first and decide for themselves if they want to continue.
  prefs: []
  type: TYPE_NORMAL
- en: Personally, I am still quite intrigued by how far you can push even the most
    simplistic time-series models. The upcoming paragraphs show some ideas and thoughts
    that I have been gathering on the topic over time.
  prefs: []
  type: TYPE_NORMAL
- en: Models with pure i.i.d. noise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start with the simplest (probabilistic) way to model a (univariate) time-series.
    Namely, we want to look at plain **i**ndependently, **i**dentically, **d** istributed
    randomness:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e350742cc00edf698579955e7fba37a.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: This implies that all our observations follow the same distribution at any point
    in time ( **identically** distributed). Even more importantly, we presume no interrelation
    between observations at all ( **independently** distributed). Obviously, this
    precludes any autoregressive terms as well.
  prefs: []
  type: TYPE_NORMAL
- en: Probably your first question is if such models aren’t too simplistic to be useful
    for real-world problems. Certainly, most time-series are unlikely to have no statistical
    relationship with their own past.
  prefs: []
  type: TYPE_NORMAL
- en: 'While those concerns are true by all means, we can nevertheless deduce the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Any time-series model that is more complex than a pure-noise model should
    also produce better forecasts than a pure-noise model.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In short, we can at least use random noise as a benchmark model. There is arguably
    no simpler approach to create baseline benchmarks than this one. Even smoothing
    techniques will likely require more parameters to be fitted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides this rather obvious use-case, there is another potential application
    for i.i.d. noise. Due to their simplicity, noise models cand be useful for very
    small datasets. Consider this: If big, complex models require large datasets to
    prevent overfitting, then simple models require only a handful of data.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, it is debatable what dataset size can be seen as ‘small’.
  prefs: []
  type: TYPE_NORMAL
- en: Integrated i.i.d. noise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, things are becoming more interesting. While raw i.i.d. noise cannot account
    for auto-correlation between observations, integrated noise can. Before we do
    a demonstration, let us introduce the *differencing operator*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/40c8d3675a77a45bdaa7c19e19a4b0c5.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: If you haven’t heard about differencing for time-series problems yet — great!
    If you have, then you can hopefully still learn something new.
  prefs: []
  type: TYPE_NORMAL
- en: Definition of an integrated time-series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the difference operator in our toolbox, we can now define an *integrated
    time-series*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a83f91138217de79a9e9a59de98166ec.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several ideas in this definition that we should clarify further:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you probably noticed the concept of exponentiating the difference operator.
    You can simply think of this as performing the differentiation several times.
    For the squared difference operator, this would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1f28bca052a6a8c897dbefeb333e637a.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: As we will see, multiple difference operators allow us to handle different time-series
    patterns at once.
  prefs: []
  type: TYPE_NORMAL
- en: Third, it is common convention to simply write
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c476381732175dc4d357eb52ca29c93.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: We will happily adopt this convention here. Also, we call such time-series *simply
    integrated* without referencing its order or seasonality.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, we also need to re-transform a difference representation back to
    its original domain. In our notation, this means we invert the difference transformation,
    i.e.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1734857993033021621d6f59291aea11.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: must hold for arbitrary difference transformations. If we expand this formula,
    we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/11eeaa68897f9d80151df92640bca120.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: These simplifications follow from the fact the difference operator is a linear
    operator (we won’t cover the details here). Technically, the last equation merely
    says that the next observation is a sum of this observation plus a delta.
  prefs: []
  type: TYPE_NORMAL
- en: In a forecasting problem, we will typically have a prediction for the change
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d24988691a8a2447af9c99a554bd069.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s denote this prediction as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7235096d93091daa9f5101237ab5e812.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: to stress that it is not the actual change, but a predicted one. Thus, the forecast
    for the integrated time-series is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/24ce027e19ab52fd99ff658795e44cf3.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Afterwards, we apply this logic recursively as far into the future as our forecast
    should go:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9c262e7923f30b2a38fa861d847c2a05.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ac71bbec3ccca089b9be39cc7f74b32.png)'
  prefs: []
  type: TYPE_IMG
- en: White noise time-series (left) and corresponding integrated time-series (right).
    Both time-series are related via the simple difference operator and its inverse.
    (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Integrated noise for seemingly complex patterns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By now, you can probably imagine what is meant by an integrated noise model.
    In fact, we can come up with countless variants of an integrated noise model by
    just chaining some difference operators with random noise.
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear trends from integrated time-series:** One possibility would be a simply
    integrated time-series, i.e.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9328b9ed1c5083e0c285e8f9f38007a3.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: It is an interesting exercise to simulate data from such a model using a plain
    standard normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: As it turns out, samples from this time-series appear to exhibit linear trends
    with potential change points. However, it is clear that these trends and change
    points occur completely at random.
  prefs: []
  type: TYPE_NORMAL
- en: This implies that simply fitting piece-wise linear functions to forecast such
    trends can be a dangerous approach. After all, if the changes are occurring at
    random, then all linear trend lines are mere artifacts of the random data-generating
    process.
  prefs: []
  type: TYPE_NORMAL
- en: As an important disclaimer, though, ‘ *unpredictable*’ means unpredictable from
    the time-series itself. An external feature might still be able to accurately
    forecast potential change points. Here, however, we presume that the time-series
    is our solely available source of information.
  prefs: []
  type: TYPE_NORMAL
- en: Below, you can see an example of the described phenomenon. While there appears
    to be a trend change at around t=50, this change is purely random. The upward
    trend after t=50 also stalls at around t=60\. Imagine how your model would have
    performed if you extrapolated the upward trend after t=60.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b144c4b04bf38e474e7371a3ea90bb15.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Generating a time-series with shifting linear trends by integrating standard
    normal noise.* (image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the saying goes ‘never say never’, even in those settings. However,
    you should really know what you are doing if you apply such models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Seasonal patterns:** Similarly to how a simple integration produceds trends,
    we can also create seasonal patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: Formally, we now need the s-th difference of our seasonal process to be a stationary
    process, e.g.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6bffde0336d4fe2c9e6fb477100d35da.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The inverse operation — transforming the i.i.d. process back to the seasonally
    integrated — works similarly to the one before:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa662643396bc45b06b0b8b5c8021251.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'You can think of the inverse operation of seasonal differencing as a *cumsum*
    operation over `s` periods. Since I am not aware of a respective, native Python
    function, I decided to do `reshape->cumsum->reshape` to get the desired outcome.
    Below is an example with `s=4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b84fd311862af1073d4e7e6f9d34c77.png)'
  prefs: []
  type: TYPE_IMG
- en: Seasonality of order `4` from a purely i.i.d. process. (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the generated time-series looks reasonably realistic. We could
    easily sell this as quarterly sales numbers of some product to an unsuspecting
    Data Scientist.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could even combine both types of integration to generate a seasonal time-series
    with trending behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e45a94f2c186f59880b834c93c63407.png)'
  prefs: []
  type: TYPE_IMG
- en: Seasonal and trending patterns emerge by integrating white noise. (image by
    author)
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you will probably realize that the title of this article was
    a little click-baity. Integrated time-series are, in fact, purely linear models.
    However, I believe that most people wouldn’t consider a model with, more-or-less,
    zero parameters a typical linear model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory effects through integration:** Another interesting property of integrated
    time-series is the ability to model memory effects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This effect can be seen particularly well when there are larger shocks or outliers
    in our data. Consider the below example, which shows seasonal integration of order
    `s=12` over i.i.d. draws from a standard [Cauchy distribution](https://en.wikipedia.org/wiki/Cauchy_distribution?ref=sarem-seitz.com):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/70bacb7562011ab40d4a495c2c61a196.png)'
  prefs: []
  type: TYPE_IMG
- en: I.i.d. standard Cauchy series (left) and corresponding seasonally integrated
    time-series. (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The first large shock in the i.i.d. Cauchy series at around t=20 is sustained
    over the whole integrated series on the right. Over time, more shocks occur, which
    are also sustained.
  prefs: []
  type: TYPE_NORMAL
- en: This memory property can be very useful in practice. For example, the economic
    shocks from the pandemic have caused persistent changes in many time-series.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking against NBEATS and NHITS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let us now use the *AirPassengers* dataset from Nixtla’s [neuralforecast](https://github.com/Nixtla/neuralforecast?ref=sarem-seitz.com)
    for a quick evaluation of the above ideas. If you are regularly reading my articles,
    you might remember the general procedure from [this one](https://www.sarem-seitz.com/facebook-prophet-covid-and-why-i-dont-trust-the-prophet/#:~:text=even%20simpler%20forecast-,model,-As%20you%20might).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we split the data into a train and test period, with the latter consisting
    of 36 months of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb84a4fe0b1e32b6cc735158f399e1a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Airline passengers dataset — train and test split. (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to obtain a stationary, i.i.d. series we perform the following transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c5e57f705c9ea4d24507e386d17a240f.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: First, the square-root stabilizes the increasing variance. The two differencing
    operators then remove seasonality and trend. For the respective re-transformation,
    check the code further down below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4154b6f216d065cd0fed490616d53076.png)'
  prefs: []
  type: TYPE_IMG
- en: Stationarity-transformed time-series (training set). (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also check a histogram and density plot of the stabilized time-series:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc4124b3a39c42e1862b1581932ba95f.png)'
  prefs: []
  type: TYPE_IMG
- en: Kernel density and histogram of the stationary time-series. (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Our stationary series looks also somewhat normally distributed, which is always
    a nice property.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let us create the forecast for the test period. Presuming that we don’t
    know the exact distribution of our i.i.d. series, we simply draw from the empirical
    distribution via the training data. Hence, we simulate future values by reintegrating
    random samples from the empirical data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8da3329d2e188ead1ed3a09e3ccce96c.png)'
  prefs: []
  type: TYPE_IMG
- en: Train set (blue), test set (red), mean forecast (green line) and 90% CI (green
    area). (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: This looks very good — the mean forecast is very close to the test data. In
    addition, our simulation allows us to empirically sample the whole forecast distribution.
    Therefore, we can also easily add confidence intervals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let us see how our approach compares against rather complex time-series
    models. To do so, I went with Nixtla’s implementation of [NBEATS](https://arxiv.org/abs/1905.10437?ref=sarem-seitz.com)
    and [NHITS](https://arxiv.org/abs/2201.12886?ref=sarem-seitz.com):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Below are the respective *RMSEs* for the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simple Model:** 25.5021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NBEATS:** 42.6277'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NHITS:** 62.6822'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we can see, our almost trivial model has beaten two sophisticated time-series
    models by a fair margin. Of course, we need to emphasize that this doesn’t allow
    to draw any general conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: Rather, I’d expect the neural models to outperform our simple approach for larger
    datasets. Nevertheless, as a benchmark, those trivial models are always a worthwhile
    consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Takeaways — What do we make of this?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As stated multiple times throughout this article:'
  prefs: []
  type: TYPE_NORMAL
- en: A seemingly complex time-series could still follow a fairly simple data-generating
    process.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the end, you might spend hours trying to fit an overly complex model even
    though the underlying problem is almost trivial. At some point, somebody could
    come along, fit a simple ARIMA(1,0,0), and still outperform your sophisticated
    neural model.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid the above worst-case scenario, consider the following idea:'
  prefs: []
  type: TYPE_NORMAL
- en: When starting out with a new time-series problem, always start with the simplest
    possible model and use it as a benchmark for all other models.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Although this is common knowledge in the Data Science community, I feel like
    it deserves particular emphasis in this context. Especially due to nowadays’ (to
    some extent justified) hype around Deep Learning, it can be tempting to directly
    start with something fancy.
  prefs: []
  type: TYPE_NORMAL
- en: For many problems, this might just be the right way to go. Nobody today would
    consider a Hidden Markov Model for NLP today when LLM embeddings are available
    almost for free now.
  prefs: []
  type: TYPE_NORMAL
- en: Once your time-series becomes large, however, modern Machine Learning will likely
    be better. In particular, [Gradient Boosted Trees](https://lightgbm.readthedocs.io/en/v3.3.2/?ref=sarem-seitz.com)
    are very popular for such large-scale problems.
  prefs: []
  type: TYPE_NORMAL
- en: A more controversial approach would be, you guessed it, Deep Learning for time-series.
    While some people believe that these models don’t work as well here, their popularity
    at [tech firms like Amazon](https://www.amazon.science/videos-and-tutorials/forecasting-big-time-series-theory-and-practice?ref=sarem-seitz.com)
    probably speaks for itself.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**[1]** Hamilton, James Douglas. *Time series analysis*. Princeton university
    press, 2020.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[2]** Hyndman, Rob J., & Athanasopoulos, George. *Forecasting: principles
    and practice*. OTexts, 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Originally published at* [*https://www.sarem-seitz.com*](https://www.sarem-seitz.com/winning-with-simple-not-even-linear-time-series-models/)
    *on May 10, 2023.*'
  prefs: []
  type: TYPE_NORMAL
