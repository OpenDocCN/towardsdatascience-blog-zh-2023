- en: 'A Brief Introduction to Neural Networks: A Classification Problem'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-brief-introduction-to-neural-networks-a-classification-problem-43e68c770081](https://towardsdatascience.com/a-brief-introduction-to-neural-networks-a-classification-problem-43e68c770081)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A practical beginner guide to Neural Networks in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@chimso1994?source=post_page-----43e68c770081--------------------------------)[![Chayma
    Zatout](../Images/341c45f53ddf73dc0851d547cc7cb55a.png)](https://medium.com/@chimso1994?source=post_page-----43e68c770081--------------------------------)[](https://towardsdatascience.com/?source=post_page-----43e68c770081--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----43e68c770081--------------------------------)
    [Chayma Zatout](https://medium.com/@chimso1994?source=post_page-----43e68c770081--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----43e68c770081--------------------------------)
    ·18 min read·Jan 24, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c8d201223863da346293e2b716990b27.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [W T](https://unsplash.com/ja/@goodfunlover?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In a [previous tutorial](https://medium.com/towards-data-science/a-brief-introduction-to-neural-networks-a-regression-problem-c58c26e18008#6dba),
    I covered the basics of neural networks and provided a simple example of using
    them for a regression problem. I briefly outlined the general process for working
    with neural networks. In this tutorial, we will delve deeper and learn how to
    use neural networks for classification tasks. We will follow the same general
    pipeline as before. However, if you need more background information on neural
    networks, I recommend reviewing the previous tutorial where I also briefly discussed
    the concepts of neurons and multi-layer networks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table of contents**'
  prefs: []
  type: TYPE_NORMAL
- en: · [1\. Introduction](#cc0d)
  prefs: []
  type: TYPE_NORMAL
- en: · [2\. Problem understanding](#69f1)
  prefs: []
  type: TYPE_NORMAL
- en: · [3\. Data preparation and preprocessing](#cc9e)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [3.1\. Data description](#1162)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [3.2\. Data Transformation](#ebb1)
  prefs: []
  type: TYPE_NORMAL
- en: · [4\. Model conception](#e1d7)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [4.1\. A single unit output](#1a1c)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [4.2\. One-hot output](#288a)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [4.3\. Convolutional neural networks](#692e)
  prefs: []
  type: TYPE_NORMAL
- en: · [5\. Training](#0fb6)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [5.1\. A single unit output](#c474)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [5.2\. One-hot output](#17aa)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [5.3\. Convolution units](#56e6)
  prefs: []
  type: TYPE_NORMAL
- en: · [6\. Validation](#2608)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [6.1 Making predictions](#d7f1)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [6.2\. Learning curves](#39df)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [6.3\. Evaluation on test set](#4c07)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [6.4\. Evaluation metrics](#2f91)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [6.5\. Display some data](#5a16)
  prefs: []
  type: TYPE_NORMAL
- en: · [7\. Conclusion](#5151)
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As previously discussed, the aim of a machine learning solution is to develop
    a model that can produce the desired output by analyzing a dataset created for
    a specific task. To achieve this, a series of steps must be followed, which include:'
  prefs: []
  type: TYPE_NORMAL
- en: Problem understanding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preparation and pre-processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model conception.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model evaluation and validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2\. Problem understanding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before discussing the classification problem that we will solve in this tutorial,
    it is important to understand that there are several types of classification.
    Specifically:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary classification**, when the number of classes is two, such as classifying
    an email as spam or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-class classification**, when there are more than two different classes,
    such as in the [Iris dataset](https://www.kaggle.com/datasets/uciml/iris) where
    the classes are different types of flowers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-label classification**, when the input has multiple classes, such as
    classifying images with multiple objects in them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, if the input are images, the classification can be **pixel-wise
    classification** (or image segmentation) when each pixel has its own class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowing the type of classification helps us to select the appropriate type of
    models and the appropriate training parameters such as the loss function. For
    example, for binary classification the `binary_crossentropy` function is often
    used as the loss function whereas `categorical_crossentropy` is used for multi-class
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem that we will be solving in this tutorial is handwritten digits
    classification (multi-class classification). In other words, giving a handwriting
    digit as an input (from 0 to 9), the model have to identify it and gives what
    digit is written as an output. We will be testing three types of models: a basic
    straight forward neural network, a basic straight forward neural network with
    its output one-hot encoded and a convolutional neural network (CNN).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start be importing the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'and set the seed so we can regenerate the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Data preparation and preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to train our models, we will be using the MNIST dataset which includes
    a training set of 60,000 examples, and a test set of 10,000 examples. If you wish
    to use the original dataset in its IDX format, you can check [my tutorial](https://medium.com/mlearning-ai/how-to-effortlessly-explore-your-idx-dataset-97753246031f)
    for an easy way to explore it. Or, you can simply use the one provided by Keras
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**3.1\. Data description**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start by displaying the data shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: A single sample is a single channel image (a grayscale image) with a shape of
    28×28 pixels. It’s important to also display the range of pixel values in the
    image to determine if data scaling is necessary later on.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Indeed the data scaling is needed later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important factor to display is number of samples in each class. This
    is important to determine if you are facing imbalanced data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3dc6b5af0373962b9afc5ddd7fc18bcb.png)'
  prefs: []
  type: TYPE_IMG
- en: Imbalanced data is a problem in machine learning where there is a significant
    difference in the number of samples between the different classes. In our case,
    the number of samples is more or less the same.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Data Transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data transformation is one of the data preprocessing techniques. It includes:
    data normalization, data encoding, data imputation that fills the missing values,
    data discretization that transforms continuous features to categorical ones, and
    dimensionality reduction that reduces the number of features in the dataset. For
    this example, we will only apply data normalization and data encoding since there
    is no missing values and there is no need for data discretization and dimensionality
    reduction.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data normalization** : is a technique used to transform the values of a dataset
    features so that they are in a specific range. This is usually done to make sure
    that the data is in a range that is suitable for neural networks or other machine
    learning methods. We normalize the pixels to the range of [0,1]:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Reshaping (from 2D image to row image)**: image reshaping or flattening is
    a common data transformation technique that converts the spatial information into
    a single row of pixels. It is a necessary step before it can be input into some
    machine learning models such as multi-layer perceptron (MLP) or linear regression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/0a71f14dc2ebded306bb315318286d72.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We reshape the images for the first two models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Data encoding: one-hot encoding** is a technique used to represent categorical
    variables with a finite number of categories as binary vectors. For a set of n
    labels, each label is represented by a vector of length n, where each element
    is 0, except for the element corresponding to the label, which is equal to 1\.
    In our case, the variable we want to predict is the digit class from 0 to 9, which
    is a finite number of categories that can be represented using one-hot encoding.
    For example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/69ccc0deea29660305ddfca905e5bf31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The one-hot encoding will be used for the output of the second and third model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Reshaping (to expand dimension).** Reshaping to expand the dimension is usually
    used in CNNs to increase the number of channels in the input image:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s summarize the data processing: data scaling is applied on the input for
    all the models, reshaping the input from 2D images to flattened images (row representation)
    is applied for the first and the second models since they are not CNN based, one-hot
    encoding is applied on the output of the second and the last model and finally,
    the input is expanded only for the CNN-based model so the input image becomes
    a single channel input of size 28×28 pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of the preprocessing stage, the training set is divided into a training
    and validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Our data is reading for training but before that, we need to build our models.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Model conception
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will demonstrate the implementation of three different models, starting
    with a simple fully connected layers model, then gradually improving it. In this
    tutorial, we will focus on covering the basics that were not covered in the previous
    tutorial such as the artificial neuron model, activation functions, layers, and
    multi-layer models.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. A single unit output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first model is a sequence of fully connected layers, followed by a single
    unit output. This model is similar to the one used in the previous tutorial. It
    is easy to implement and can produce good results for this particular example.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have outlined the general architecture of the model, we need to
    consider how it will predict the input class (label) based on the available activation
    functions discussed in the previous tutorial. These functions all return real
    numbers, but we can use them and round the predicted number to an integer. However,
    we need to ensure that the output range of the activation function in the final
    layer includes all possible class values. Therefore, functions such as *sigmoid*,
    *tanh*, and *softsign* cannot be used in this case.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb9450eeea1c16ae222b265c135f4f66.png)'
  prefs: []
  type: TYPE_IMG
- en: Fully connected neural network with a single output to predict the input label
  prefs: []
  type: TYPE_NORMAL
- en: Let’s create our model! It will have 5 hidden layers, each with 224 units and
    using the *sigmoid* activation function. The output layer will have a single unit
    and will use the *relu* activation function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The model summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 4.2\. One-hot output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the previous model produces good results, as we will see later, better
    results can be obtained with a smaller model. The key difference is that the output
    layer is encoded using one-hot representation. Typically, in machine learning,
    one-hot encoding is implemented using a dense layer of n units (where n is the
    number of possible categories) and a *softmax* activation function.
  prefs: []
  type: TYPE_NORMAL
- en: Hmm, I’m not sure that *softmax* was defined in the previous tutorial, so what
    is *softmax*?
  prefs: []
  type: TYPE_NORMAL
- en: “Softmax function converts a vector of values to a probability distribution.
    The elements of the output vector are in range (0, 1) and sum to 1\. Softmax is
    often used as the activation for the last layer of a classification network because
    the result could be interpreted as a probability distribution.” [1]
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The overall model has the following architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a89105340d35f89d551c59de7f19ec4.png)'
  prefs: []
  type: TYPE_IMG
- en: Fully connected neural network with a categorical output
  prefs: []
  type: TYPE_NORMAL
- en: 'To create our model, we will first define the dropout layer which is a regularization
    technique to prevent overfitting (which will be explained later):'
  prefs: []
  type: TYPE_NORMAL
- en: “The Dropout layer randomly sets input units to 0 with a frequency of `rate`
    at each step during training time, which helps prevent overfitting. Note that
    the Dropout layer only applies when training is set to True such that no values
    are dropped during inference.” [2]
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In Keras, the dropout layer is defined as following, where `rate` is a float
    between 0 and 1 that represents the fraction of the input units to drop :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s create our model! It will have 2 hidden layers, each with 224 units
    and using the relu activation function. A dense output layer with 10 units and
    the softmax activation function will be added. A dropout layer is added after
    each dense hidden layer to prevent overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The model summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this model has a smaller number of parameters compared to the
    previous one (377 665 parameters) and you will see that it will provide better
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Convolutional neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have treated the image as a vector. However, if we want to take advantage
    of the fact that the image is a 2D matrix, how should we proceed? One approach
    is to use specialized 2D units. In this section, I will briefly introduce them.
    However, if you want to learn more about them, I suggest you refer to this [tutorial](https://cs231n.github.io/convolutional-networks/).
  prefs: []
  type: TYPE_NORMAL
- en: '**Conv2D** : applies convolutions using kernels on the input to produce the
    output (called filters). During training these kernels are updated (trained).
    Indeed, they play the role of weights in dense layers. In Keras, it is defined
    as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'where: `filters` is the number of kernels and since each convolution kernel
    produces an output it also represents the dimensionality of the output space.
    The `kernel_size` is the size of kernels that is usually an odd number.'
  prefs: []
  type: TYPE_NORMAL
- en: '**MaxPooling2D** : takes the maximum value over an input window. In Keras,
    it is defined as following, where `pool_size` is the window size over which to
    take the maximum:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s create our CNN model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The model summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This model is the smallest one, with only 34,826 parameters which is about 7
    times smaller than the previous one. Additionally, you will see that it also achieves
    the highest performance.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As explained in the previous tutorial, training neural networks is updating
    the weights so the models can fit well on the data. Before starting training,
    a set of parameters needs to be defined including: the optimizer, the loss function,
    the batch size, the number of epochs and other metrics to track during training.
    The choice of loss function and additional metrics heavily depends on the type
    of output, whether it is a regression or classification problem, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: For all the following training, we will set the optimizer to `'adam'` which
    is a good choice if we don’t want to handle the learning rate ourselves and the
    batch size to `128` .
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. A single unit output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As this model has a single output layer, the metrics and the loss function
    that we presented and used in the previous tutorial can be used here. We will
    set the loss function to `mae` , the additional metric to `mse` and the number
    of epochs to 200:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'By observing the verbose display, we can conclude that the model has learned
    well and has also been able to generalize (good results for both the train and
    validation sets):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: However, we can achieve better results by using fewer epochs with the one-hot
    encoding (the second model).
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. One-hot output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike the previous model, we use `'categorical_crossentropy'` as the loss function
    and the `accuracy` as the additional metric. The `'categorical_crossentropy'`
    is a probabilistic loss function that computes the crossentropy loss between the
    labels and predictions used when there are two or more label classes. It expects
    labels to be provided in a `one_hot` representation. As for the `accuracy` , it
    is a metric that is more appropriate for classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'By observing the verbose display, we can conclude that the model has learned
    well and has also been able to generalize (good results for both the train and
    validation sets) in only 20 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 5.3\. Convolution units
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let’s train our last model! We use the same loss function and metric as
    the previous model but with a smaller number of epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'By observing the verbose display, we can say that the model learned well and
    managed to generalize (good result for the train and validation set) better in
    only 15 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 6\. Validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The displayed metrics of the last epochs isn’t enough to conclude whether the
    model has learnt from data or not. The first thing to observe is the learning
    curves. Then, we can evaluate our model using other metrics. We also test it on
    a test set if it’s available.
  prefs: []
  type: TYPE_NORMAL
- en: But before that, let’s see how we can make predictions and get the predicted
    class.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Making predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the model trained, we can start making prediction: predict the class of
    the image(s) input. In this section, we will see how to get the predicted class
    for both types of output.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Keras, the `predict` function is used, where x is the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the model is trained on batches, the first thing to do before predicting
    is to expand the input dimension. We take the first instance of the train set
    as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can make prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '**Single output prediction.** The returned value in this case is a float number
    that can be out the range `[0, 9]` if the model behaved poorly for a given input.
    The first thing to do is the clip the output, so all values are in that range
    the we round to `int` to get the input output as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The predicted value is 6.9955063, in this case the clip function returns the
    same value. Finally the value is rounded to `int` : the input class is 7.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '**Onehot encoding output.** In this case, the function predict returns a list
    of 10 element of probabilities so we need to get the index of the element with
    the highest probability:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the highest probability is the 8th element that corresponds to class
    7:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The reason I showed you how to make predictions is not only to learn how predictions
    are made but also to use it later when we compute some validation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. Learning curves
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Learning curves reveals the model performance during training for the seen
    data (train set) and the unseen data (validation set). It allows to:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Identify overfitting:** when the training loss decreases while the validation
    loss increases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/b7eea11eaf2f3c5cd8974d12e9a93892.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '**Identify underfitting:** when both the training and validation losses are
    high both the training and validation errors are high.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/c4429e733c8908e78a7e23db3c94d830.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Example of underfitting: the loss function is around 2 which is a high value
    in our case.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Compare between models** by comparing the learning curves for different models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/307f1eb5f76ded852cd1838071e39a9f.png)'
  prefs: []
  type: TYPE_IMG
- en: Learning curves of the trained models (with the first row represents the loss
    function and the second row represents the accuracy metric). We can say that the
    three models learned well, and the third model outperforms the other models since
    it is able to predict classes with high accuracy for both the train and the validation
    set. In other words, it generalizes better.
  prefs: []
  type: TYPE_NORMAL
- en: 'The learning curves can be plotted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 6.3\. Evaluation on test set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s evaluate our models on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The first model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The second model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The third model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 6.4\. Evaluation metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After training, the model is evaluated using additional metrics. The metrics
    to be computed strongly depend on the nature of the model output: if it is a regression
    (or a continuous variable) the Mean Square Error (MSE) or Mean Absolute Error
    (MAE) can be used; If it is a classification (or a discrete variable) the precision,
    recall, f-measure, the confusion matrix and ROC curve can be used.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**. The accuracy is the fraction between the correct predictions
    and all predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision**. The precision represents the positive class predictions that
    belong to the positive class. It provides how much examples are actually positive
    out of all the positive classes that have been predicted correctly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall**. the recall represents positive class predictions made out of the
    positive examples in the dataset. It provides how much examples was predicted
    correctly out of all the positive classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F-measure**. It is a single score that balances both precision and recall.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s display these measures for the train, validation and test sets. Please
    note that the following instructions are written for the second and the third
    models. If you want to display the metrics for the first model please refer to
    section 6.1 (making predictions) or you can visit my [GitHub repository](https://github.com/Chim-SO/MNIST_classification).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The first model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The second model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The third model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: As you can see the third model, the CNN-based model, performed better on the
    test and the validation sets.
  prefs: []
  type: TYPE_NORMAL
- en: '**Confusion matrix.** The confusion matrix gives a summary of all the correct
    predictions of each class and all the confusions between each class. It provides
    a detailed insight of how the model performs and which kind of error it makes.
    For instance, thanks to confusion matrix, we can say for a given class what are
    the classes that have confusion with it and how much. In addition, if two classes
    have high confusion between them, we can understand that the model find it difficult
    to distinguish between them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e4953e849d60111f6845a1b898ad8767.png)'
  prefs: []
  type: TYPE_IMG
- en: Confusion matrix of the third model (CNN-based model).
  prefs: []
  type: TYPE_NORMAL
- en: 6.5\. Display some data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is always good to display your model output. In case of classification,
    the misclassified samples are often displayed to gain insight into the types of
    mistakes that the model is making. So let’s display 10 of the misclassified images
    using Matplotlib library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/803af2ba311ffe26b0774ce84eddb55a.png)'
  prefs: []
  type: TYPE_IMG
- en: Some of misclassified data of the third model.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: That’s it for this article! In this article, we have learned how to create neural
    networks and train and validate them for classification problems. This article
    is the second tutorial in the ‘Brief Introduction to Neural Networks’ series;
    other types of neural networks will be presented in the same way. If you want
    to delve deeper, you can try exploring and building models for other classification
    problems (such as the Iris dataset, for example) while following the same pipeline
    I described in my tutorials. Even though this tutorial is intended to introduce
    classification in neural networks, it will serve as a reference for more advanced
    tutorials in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks, I hope you enjoyed reading this. You can find the examples here in my
    [GitHub repository](https://github.com/Chim-SO/MNIST_classification). If you have
    any questions or suggestions feel free to leave me a comment below.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/a-brief-introduction-to-neural-networks-a-regression-problem-c58c26e18008?source=post_page-----43e68c770081--------------------------------)
    [## A Brief Introduction to Neural Networks : a Regression Problem'
  prefs: []
  type: TYPE_NORMAL
- en: A practical beginner guide to Neural Networks in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-brief-introduction-to-neural-networks-a-regression-problem-c58c26e18008?source=post_page-----43e68c770081--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] [https://keras.io/api/layers/activations/#softmax-function](https://keras.io/api/layers/activations/#softmax-function)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [https://keras.io/api/layers/regularization_layers/dropout/](https://keras.io/api/layers/regularization_layers/dropout/)'
  prefs: []
  type: TYPE_NORMAL
- en: Image credits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All images and figures in this article whose source is not mentioned in the
    caption are by the author.
  prefs: []
  type: TYPE_NORMAL
