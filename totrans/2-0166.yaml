- en: 'A Brief Introduction to Neural Networks: A Classification Problem'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络简介：一个分类问题
- en: 原文：[https://towardsdatascience.com/a-brief-introduction-to-neural-networks-a-classification-problem-43e68c770081](https://towardsdatascience.com/a-brief-introduction-to-neural-networks-a-classification-problem-43e68c770081)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-brief-introduction-to-neural-networks-a-classification-problem-43e68c770081](https://towardsdatascience.com/a-brief-introduction-to-neural-networks-a-classification-problem-43e68c770081)
- en: A practical beginner guide to Neural Networks in Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实用的Python神经网络初学者指南
- en: '[](https://medium.com/@chimso1994?source=post_page-----43e68c770081--------------------------------)[![Chayma
    Zatout](../Images/341c45f53ddf73dc0851d547cc7cb55a.png)](https://medium.com/@chimso1994?source=post_page-----43e68c770081--------------------------------)[](https://towardsdatascience.com/?source=post_page-----43e68c770081--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----43e68c770081--------------------------------)
    [Chayma Zatout](https://medium.com/@chimso1994?source=post_page-----43e68c770081--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@chimso1994?source=post_page-----43e68c770081--------------------------------)[![Chayma
    Zatout](../Images/341c45f53ddf73dc0851d547cc7cb55a.png)](https://medium.com/@chimso1994?source=post_page-----43e68c770081--------------------------------)[](https://towardsdatascience.com/?source=post_page-----43e68c770081--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----43e68c770081--------------------------------)
    [Chayma Zatout](https://medium.com/@chimso1994?source=post_page-----43e68c770081--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----43e68c770081--------------------------------)
    ·18 min read·Jan 24, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: · 发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----43e68c770081--------------------------------)
    · 18分钟阅读 · 2023年1月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c8d201223863da346293e2b716990b27.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8d201223863da346293e2b716990b27.png)'
- en: Photo by [W T](https://unsplash.com/ja/@goodfunlover?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源 [W T](https://unsplash.com/ja/@goodfunlover?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In a [previous tutorial](https://medium.com/towards-data-science/a-brief-introduction-to-neural-networks-a-regression-problem-c58c26e18008#6dba),
    I covered the basics of neural networks and provided a simple example of using
    them for a regression problem. I briefly outlined the general process for working
    with neural networks. In this tutorial, we will delve deeper and learn how to
    use neural networks for classification tasks. We will follow the same general
    pipeline as before. However, if you need more background information on neural
    networks, I recommend reviewing the previous tutorial where I also briefly discussed
    the concepts of neurons and multi-layer networks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [上一篇教程](https://medium.com/towards-data-science/a-brief-introduction-to-neural-networks-a-regression-problem-c58c26e18008#6dba)中，我介绍了神经网络的基础，并提供了一个简单的回归问题示例。我简要概述了使用神经网络的一般过程。在本教程中，我们将进一步深入学习如何将神经网络用于分类任务。我们将遵循之前相同的一般流程。然而，如果你需要更多关于神经网络的背景信息，我建议你回顾上一篇教程，在其中我也简要讨论了神经元和多层网络的概念。
- en: '**Table of contents**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**目录**'
- en: · [1\. Introduction](#cc0d)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: · [1. 介绍](#cc0d)
- en: · [2\. Problem understanding](#69f1)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: · [2. 问题理解](#69f1)
- en: · [3\. Data preparation and preprocessing](#cc9e)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: · [3. 数据准备与预处理](#cc9e)
- en: ∘ [3.1\. Data description](#1162)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [3.1. 数据描述](#1162)
- en: ∘ [3.2\. Data Transformation](#ebb1)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [3.2. 数据转换](#ebb1)
- en: · [4\. Model conception](#e1d7)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: · [4. 模型构思](#e1d7)
- en: ∘ [4.1\. A single unit output](#1a1c)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [4.1. 单个单元输出](#1a1c)
- en: ∘ [4.2\. One-hot output](#288a)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [4.2. 单热输出](#288a)
- en: ∘ [4.3\. Convolutional neural networks](#692e)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [4.3. 卷积神经网络](#692e)
- en: · [5\. Training](#0fb6)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: · [5. 训练](#0fb6)
- en: ∘ [5.1\. A single unit output](#c474)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [5.1. 单个单元输出](#c474)
- en: ∘ [5.2\. One-hot output](#17aa)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [5.2. 单热输出](#17aa)
- en: ∘ [5.3\. Convolution units](#56e6)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [5.3. 卷积单元](#56e6)
- en: · [6\. Validation](#2608)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: · [6. 验证](#2608)
- en: ∘ [6.1 Making predictions](#d7f1)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [6.1 预测](#d7f1)
- en: ∘ [6.2\. Learning curves](#39df)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [6.2. 学习曲线](#39df)
- en: ∘ [6.3\. Evaluation on test set](#4c07)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [6.3. 测试集评估](#4c07)
- en: ∘ [6.4\. Evaluation metrics](#2f91)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [6.4. 评估指标](#2f91)
- en: ∘ [6.5\. Display some data](#5a16)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [6.5. 显示一些数据](#5a16)
- en: · [7\. Conclusion](#5151)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: · [7. 结论](#5151)
- en: 1\. Introduction
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. 介绍
- en: 'As previously discussed, the aim of a machine learning solution is to develop
    a model that can produce the desired output by analyzing a dataset created for
    a specific task. To achieve this, a series of steps must be followed, which include:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前讨论的，机器学习解决方案的目标是开发一个能够通过分析为特定任务创建的数据集来生成所需输出的模型。为实现这一目标，必须遵循一系列步骤，包括：
- en: Problem understanding.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题理解。
- en: Data preparation and pre-processing.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备和预处理。
- en: Model conception.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型构思。
- en: Training the model.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型。
- en: Model evaluation and validation
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估和验证
- en: 2\. Problem understanding
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 问题理解
- en: 'Before discussing the classification problem that we will solve in this tutorial,
    it is important to understand that there are several types of classification.
    Specifically:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论我们将在本教程中解决的分类问题之前，理解有几种分类类型是很重要的。具体来说：
- en: '**Binary classification**, when the number of classes is two, such as classifying
    an email as spam or not.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二分类**，当类别数量为两个时，例如将电子邮件分类为垃圾邮件或非垃圾邮件。'
- en: '**Multi-class classification**, when there are more than two different classes,
    such as in the [Iris dataset](https://www.kaggle.com/datasets/uciml/iris) where
    the classes are different types of flowers.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多类分类**，当有两个以上不同类别时，例如在[Iris数据集](https://www.kaggle.com/datasets/uciml/iris)中，类别是不同类型的花。'
- en: '**Multi-label classification**, when the input has multiple classes, such as
    classifying images with multiple objects in them.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多标签分类**，当输入有多个类别时，例如将包含多个对象的图像进行分类。'
- en: Furthermore, if the input are images, the classification can be **pixel-wise
    classification** (or image segmentation) when each pixel has its own class.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，如果输入是图像，当每个像素都有自己的类别时，分类可以是**逐像素分类**（或图像分割）。
- en: Knowing the type of classification helps us to select the appropriate type of
    models and the appropriate training parameters such as the loss function. For
    example, for binary classification the `binary_crossentropy` function is often
    used as the loss function whereas `categorical_crossentropy` is used for multi-class
    classification.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 了解分类类型有助于我们选择合适的模型类型和适当的训练参数，例如损失函数。例如，对于二分类，通常使用`binary_crossentropy`函数作为损失函数，而对于多类分类，则使用`categorical_crossentropy`。
- en: 'The problem that we will be solving in this tutorial is handwritten digits
    classification (multi-class classification). In other words, giving a handwriting
    digit as an input (from 0 to 9), the model have to identify it and gives what
    digit is written as an output. We will be testing three types of models: a basic
    straight forward neural network, a basic straight forward neural network with
    its output one-hot encoded and a convolutional neural network (CNN).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本教程中要解决的问题是手写数字分类（多类分类）。换句话说，给定一个手写数字作为输入（从0到9），模型必须识别它并给出写了什么数字作为输出。我们将测试三种类型的模型：一个基本的直接神经网络、一个输出进行过one-hot编码的基本直接神经网络和一个卷积神经网络（CNN）。
- en: 'Let’s start be importing the required libraries:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从导入所需的库开始：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'and set the seed so we can regenerate the results:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 并设置种子，以便我们可以重新生成结果：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 3\. Data preparation and preprocessing
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 数据准备和预处理
- en: 'In order to train our models, we will be using the MNIST dataset which includes
    a training set of 60,000 examples, and a test set of 10,000 examples. If you wish
    to use the original dataset in its IDX format, you can check [my tutorial](https://medium.com/mlearning-ai/how-to-effortlessly-explore-your-idx-dataset-97753246031f)
    for an easy way to explore it. Or, you can simply use the one provided by Keras
    as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的模型，我们将使用MNIST数据集，该数据集包括60,000个训练样本和10,000个测试样本。如果您希望使用原始的IDX格式数据集，可以查看[我的教程](https://medium.com/mlearning-ai/how-to-effortlessly-explore-your-idx-dataset-97753246031f)，以便轻松探索它。或者，您可以直接使用Keras提供的数据集，如下所示：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**3.1\. Data description**'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.1\. 数据描述**'
- en: 'We start by displaying the data shape:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从显示数据形状开始：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A single sample is a single channel image (a grayscale image) with a shape of
    28×28 pixels. It’s important to also display the range of pixel values in the
    image to determine if data scaling is necessary later on.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 单个样本是一个通道的图像（灰度图像），形状为28×28像素。显示图像中像素值的范围也很重要，以确定是否需要后续的数据缩放。
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Indeed the data scaling is needed later.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，后续需要进行数据缩放。
- en: 'Another important factor to display is number of samples in each class. This
    is important to determine if you are facing imbalanced data:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要因素是显示每个类别中的样本数量。这对于确定是否面对数据不平衡非常重要：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/3dc6b5af0373962b9afc5ddd7fc18bcb.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3dc6b5af0373962b9afc5ddd7fc18bcb.png)'
- en: Imbalanced data is a problem in machine learning where there is a significant
    difference in the number of samples between the different classes. In our case,
    the number of samples is more or less the same.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 数据不平衡是机器学习中的一个问题，其中不同类别之间样本数量差异显著。在我们的案例中，样本数量大致相同。
- en: 3.2\. Data Transformation
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2. 数据转换
- en: 'Data transformation is one of the data preprocessing techniques. It includes:
    data normalization, data encoding, data imputation that fills the missing values,
    data discretization that transforms continuous features to categorical ones, and
    dimensionality reduction that reduces the number of features in the dataset. For
    this example, we will only apply data normalization and data encoding since there
    is no missing values and there is no need for data discretization and dimensionality
    reduction.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换是数据预处理技术之一。它包括：数据归一化、数据编码、填补缺失值的数据插补、将连续特征转化为分类特征的数据离散化，以及减少数据集中特征数量的降维。对于这个例子，我们将只应用数据归一化和数据编码，因为没有缺失值，也不需要数据离散化和降维。
- en: '**Data normalization** : is a technique used to transform the values of a dataset
    features so that they are in a specific range. This is usually done to make sure
    that the data is in a range that is suitable for neural networks or other machine
    learning methods. We normalize the pixels to the range of [0,1]:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据归一化**：是一种将数据集特征的值转换到特定范围内的技术。这通常是为了确保数据在适合神经网络或其他机器学习方法的范围内。我们将像素归一化到[0,1]的范围：'
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Reshaping (from 2D image to row image)**: image reshaping or flattening is
    a common data transformation technique that converts the spatial information into
    a single row of pixels. It is a necessary step before it can be input into some
    machine learning models such as multi-layer perceptron (MLP) or linear regression.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重新塑形（从2D图像到行图像）**：图像重新塑形或展平是一种常见的数据转换技术，将空间信息转换为一行像素。这是输入到一些机器学习模型（如多层感知器（MLP）或线性回归）之前的必要步骤。'
- en: '![](../Images/0a71f14dc2ebded306bb315318286d72.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a71f14dc2ebded306bb315318286d72.png)'
- en: 'We reshape the images for the first two models:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为前两个模型重新塑形图像：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Data encoding: one-hot encoding** is a technique used to represent categorical
    variables with a finite number of categories as binary vectors. For a set of n
    labels, each label is represented by a vector of length n, where each element
    is 0, except for the element corresponding to the label, which is equal to 1\.
    In our case, the variable we want to predict is the digit class from 0 to 9, which
    is a finite number of categories that can be represented using one-hot encoding.
    For example:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据编码：一热编码**是一种用于用二进制向量表示有限类别数量的分类变量的技术。对于一组n个标签，每个标签由长度为n的向量表示，其中每个元素为0，除了对应标签的元素为1。在我们的案例中，我们要预测的变量是从0到9的数字类别，这是一个可以用一热编码表示的有限类别。例如：'
- en: '![](../Images/69ccc0deea29660305ddfca905e5bf31.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69ccc0deea29660305ddfca905e5bf31.png)'
- en: 'The one-hot encoding will be used for the output of the second and third model:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一热编码将用于第二和第三模型的输出：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Reshaping (to expand dimension).** Reshaping to expand the dimension is usually
    used in CNNs to increase the number of channels in the input image:'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重新塑形（以扩展维度）。** 重新塑形以扩展维度通常在 CNN 中使用，以增加输入图像中的通道数：'
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let’s summarize the data processing: data scaling is applied on the input for
    all the models, reshaping the input from 2D images to flattened images (row representation)
    is applied for the first and the second models since they are not CNN based, one-hot
    encoding is applied on the output of the second and the last model and finally,
    the input is expanded only for the CNN-based model so the input image becomes
    a single channel input of size 28×28 pixels.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 总结数据处理：对所有模型的输入应用数据缩放，对输入从2D图像转换为展平图像（行表示）应用于第一个和第二个模型，因为它们不是基于 CNN 的，对第二个和最后一个模型的输出应用一热编码，最后，仅对基于
    CNN 的模型扩展输入，以使输入图像变为大小为28×28像素的单通道输入。
- en: 'At the end of the preprocessing stage, the training set is divided into a training
    and validation set:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在预处理阶段结束时，训练集被分为训练集和验证集：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Our data is reading for training but before that, we need to build our models.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据准备好用于训练，但在此之前，我们需要构建我们的模型。
- en: 4\. Model conception
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. 模型构思
- en: Now, we will demonstrate the implementation of three different models, starting
    with a simple fully connected layers model, then gradually improving it. In this
    tutorial, we will focus on covering the basics that were not covered in the previous
    tutorial such as the artificial neuron model, activation functions, layers, and
    multi-layer models.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将演示三种不同模型的实现，首先是一个简单的全连接层模型，然后逐步改进。在本教程中，我们将重点介绍前一个教程中未涵盖的基础知识，例如人工神经元模型、激活函数、层和多层模型。
- en: 4.1\. A single unit output
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1\. 单一输出单元
- en: The first model is a sequence of fully connected layers, followed by a single
    unit output. This model is similar to the one used in the previous tutorial. It
    is easy to implement and can produce good results for this particular example.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个模型是一个完全连接层的序列，最后跟随一个单一的输出单元。这个模型类似于前一个教程中使用的模型。它易于实现，并且可以为这个特定示例产生良好的结果。
- en: Now that we have outlined the general architecture of the model, we need to
    consider how it will predict the input class (label) based on the available activation
    functions discussed in the previous tutorial. These functions all return real
    numbers, but we can use them and round the predicted number to an integer. However,
    we need to ensure that the output range of the activation function in the final
    layer includes all possible class values. Therefore, functions such as *sigmoid*,
    *tanh*, and *softsign* cannot be used in this case.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经概述了模型的一般架构，我们需要考虑如何根据前一个教程中讨论的可用激活函数来预测输入类别（标签）。这些函数都返回实数，但我们可以使用它们并将预测的数字四舍五入为整数。然而，我们需要确保最终层中激活函数的输出范围包含所有可能的类别值。因此，像*sigmoid*、*tanh*和*softsign*这样的函数在这种情况下不能使用。
- en: '![](../Images/bb9450eeea1c16ae222b265c135f4f66.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb9450eeea1c16ae222b265c135f4f66.png)'
- en: Fully connected neural network with a single output to predict the input label
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 带有单一输出以预测输入标签的全连接神经网络
- en: Let’s create our model! It will have 5 hidden layers, each with 224 units and
    using the *sigmoid* activation function. The output layer will have a single unit
    and will use the *relu* activation function.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建我们的模型！它将有5个隐藏层，每层有224个单元，并使用*sigmoid*激活函数。输出层将有一个单元，并使用*relu*激活函数。
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The model summary:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 模型总结：
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 4.2\. One-hot output
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2\. 一热输出
- en: While the previous model produces good results, as we will see later, better
    results can be obtained with a smaller model. The key difference is that the output
    layer is encoded using one-hot representation. Typically, in machine learning,
    one-hot encoding is implemented using a dense layer of n units (where n is the
    number of possible categories) and a *softmax* activation function.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前面的模型产生了良好的结果，但正如我们稍后看到的那样，通过一个更小的模型可以获得更好的结果。关键的区别在于输出层使用了一热编码。通常，在机器学习中，一热编码是通过具有n个单元的密集层实现的（其中n是可能的类别数量），并且使用*softmax*激活函数。
- en: Hmm, I’m not sure that *softmax* was defined in the previous tutorial, so what
    is *softmax*?
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，我不确定前一个教程中是否定义了*softmax*，那么*softmax*是什么呢？
- en: “Softmax function converts a vector of values to a probability distribution.
    The elements of the output vector are in range (0, 1) and sum to 1\. Softmax is
    often used as the activation for the last layer of a classification network because
    the result could be interpreted as a probability distribution.” [1]
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “Softmax函数将值向量转换为概率分布。输出向量的元素在(0, 1)范围内，并且总和为1。Softmax通常用作分类网络最后一层的激活函数，因为结果可以解释为概率分布。”
    [1]
- en: 'The overall model has the following architecture:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 总体模型具有以下架构：
- en: '![](../Images/7a89105340d35f89d551c59de7f19ec4.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a89105340d35f89d551c59de7f19ec4.png)'
- en: Fully connected neural network with a categorical output
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 带有分类输出的全连接神经网络
- en: 'To create our model, we will first define the dropout layer which is a regularization
    technique to prevent overfitting (which will be explained later):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建我们的模型，我们将首先定义dropout层，这是一种正则化技术，用于防止过拟合（稍后会解释）：
- en: “The Dropout layer randomly sets input units to 0 with a frequency of `rate`
    at each step during training time, which helps prevent overfitting. Note that
    the Dropout layer only applies when training is set to True such that no values
    are dropped during inference.” [2]
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “Dropout层在训练期间以`rate`的频率随机将输入单元设置为0，这有助于防止过拟合。请注意，Dropout层仅在训练设置为True时应用，因此在推断过程中不会丢失任何值。”
    [2]
- en: 'In Keras, the dropout layer is defined as following, where `rate` is a float
    between 0 and 1 that represents the fraction of the input units to drop :'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，dropout层定义如下，其中`rate`是一个介于0和1之间的浮点数，表示要丢弃的输入单元的比例：
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now, let’s create our model! It will have 2 hidden layers, each with 224 units
    and using the relu activation function. A dense output layer with 10 units and
    the softmax activation function will be added. A dropout layer is added after
    each dense hidden layer to prevent overfitting.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建我们的模型！它将有2个隐藏层，每个隐藏层有224个单元，并使用relu激活函数。将添加一个具有10个单元和softmax激活函数的密集输出层。在每个密集隐藏层后添加一个dropout层以防止过拟合。
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The model summary:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 模型摘要：
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As you can see, this model has a smaller number of parameters compared to the
    previous one (377 665 parameters) and you will see that it will provide better
    results.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，此模型相比于之前的模型（377,665个参数）具有较少的参数，你会发现它能提供更好的结果。
- en: 4.3\. Convolutional neural networks
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3\. 卷积神经网络
- en: So far, we have treated the image as a vector. However, if we want to take advantage
    of the fact that the image is a 2D matrix, how should we proceed? One approach
    is to use specialized 2D units. In this section, I will briefly introduce them.
    However, if you want to learn more about them, I suggest you refer to this [tutorial](https://cs231n.github.io/convolutional-networks/).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们将图像视为一个向量。然而，如果我们想利用图像是一个2D矩阵的事实，我们应该怎么做？一种方法是使用专门的2D单元。在本节中，我将简要介绍它们。然而，如果你想了解更多关于它们的内容，我建议你参考这个[教程](https://cs231n.github.io/convolutional-networks/)。
- en: '**Conv2D** : applies convolutions using kernels on the input to produce the
    output (called filters). During training these kernels are updated (trained).
    Indeed, they play the role of weights in dense layers. In Keras, it is defined
    as:'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Conv2D**：使用卷积核对输入应用卷积以产生输出（称为滤波器）。在训练过程中，这些卷积核会被更新（训练）。确实，它们在密集层中扮演着权重的角色。在Keras中，它定义为：'
- en: '[PRE19]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'where: `filters` is the number of kernels and since each convolution kernel
    produces an output it also represents the dimensionality of the output space.
    The `kernel_size` is the size of kernels that is usually an odd number.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：`filters`是卷积核的数量，由于每个卷积核生成一个输出，因此它也代表输出空间的维度。`kernel_size`是卷积核的大小，通常是一个奇数。
- en: '**MaxPooling2D** : takes the maximum value over an input window. In Keras,
    it is defined as following, where `pool_size` is the window size over which to
    take the maximum:'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MaxPooling2D**：在输入窗口上取最大值。在Keras中，它定义如下，其中`pool_size`是用来取最大值的窗口大小：'
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, let’s create our CNN model:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建我们的CNN模型：
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The model summary:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 模型摘要：
- en: '[PRE22]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This model is the smallest one, with only 34,826 parameters which is about 7
    times smaller than the previous one. Additionally, you will see that it also achieves
    the highest performance.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是最小的，仅有34,826个参数，大约是之前模型的7倍小。此外，你会发现它也能实现最高的性能。
- en: 5\. Training
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 训练
- en: 'As explained in the previous tutorial, training neural networks is updating
    the weights so the models can fit well on the data. Before starting training,
    a set of parameters needs to be defined including: the optimizer, the loss function,
    the batch size, the number of epochs and other metrics to track during training.
    The choice of loss function and additional metrics heavily depends on the type
    of output, whether it is a regression or classification problem, etc.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前的教程所解释的，训练神经网络就是更新权重，以便模型能够很好地拟合数据。在开始训练之前，需要定义一组参数，包括：优化器、损失函数、批次大小、训练周期数和其他训练过程中需要跟踪的指标。损失函数和附加指标的选择很大程度上取决于输出的类型，无论是回归问题还是分类问题等。
- en: For all the following training, we will set the optimizer to `'adam'` which
    is a good choice if we don’t want to handle the learning rate ourselves and the
    batch size to `128` .
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的所有训练中，我们将优化器设置为`'adam'`，这是一个不错的选择，如果我们不想自己处理学习率的话，批次大小设置为`128`。
- en: 5.1\. A single unit output
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1\. 单一单元输出
- en: 'As this model has a single output layer, the metrics and the loss function
    that we presented and used in the previous tutorial can be used here. We will
    set the loss function to `mae` , the additional metric to `mse` and the number
    of epochs to 200:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于此模型具有单个输出层，因此我们在之前的教程中介绍和使用的指标和损失函数也可以在这里使用。我们将设置损失函数为`mae`，附加指标为`mse`，训练周期数为200：
- en: '[PRE23]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'By observing the verbose display, we can conclude that the model has learned
    well and has also been able to generalize (good results for both the train and
    validation sets):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察详细显示，我们可以得出结论：模型已经很好地学习了，并且也能够泛化（训练集和验证集上都取得了良好的结果）：
- en: '[PRE24]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: However, we can achieve better results by using fewer epochs with the one-hot
    encoding (the second model).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通过使用较少的训练轮次并进行一热编码（第二个模型），我们可以获得更好的结果。
- en: 5.2\. One-hot output
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2\. 一热编码输出
- en: Unlike the previous model, we use `'categorical_crossentropy'` as the loss function
    and the `accuracy` as the additional metric. The `'categorical_crossentropy'`
    is a probabilistic loss function that computes the crossentropy loss between the
    labels and predictions used when there are two or more label classes. It expects
    labels to be provided in a `one_hot` representation. As for the `accuracy` , it
    is a metric that is more appropriate for classification problems.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的模型不同，我们使用`'categorical_crossentropy'`作为损失函数，并将`accuracy`作为附加指标。`'categorical_crossentropy'`是一个概率损失函数，用于计算标签和预测之间的交叉熵损失，当标签类别有两个或更多时使用。它期望标签以`one_hot`表示。至于`accuracy`，它是一个更适合分类问题的指标。
- en: '[PRE25]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'By observing the verbose display, we can conclude that the model has learned
    well and has also been able to generalize (good results for both the train and
    validation sets) in only 20 epochs:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察详细显示，我们可以得出结论：模型在仅20轮训练中就已经很好地学习了，并且也能够泛化（训练集和验证集上都取得了良好的结果）：
- en: '[PRE26]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 5.3\. Convolution units
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3\. 卷积单元
- en: 'Now let’s train our last model! We use the same loss function and metric as
    the previous model but with a smaller number of epochs:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们训练我们的最后一个模型！我们使用与之前模型相同的损失函数和指标，但轮次更少：
- en: '[PRE27]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'By observing the verbose display, we can say that the model learned well and
    managed to generalize (good result for the train and validation set) better in
    only 15 epochs:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察详细显示，我们可以说模型学习得很好，并且在仅15轮训练中成功地泛化（训练集和验证集上的结果更好）：
- en: '[PRE28]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 6\. Validation
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. 验证
- en: The displayed metrics of the last epochs isn’t enough to conclude whether the
    model has learnt from data or not. The first thing to observe is the learning
    curves. Then, we can evaluate our model using other metrics. We also test it on
    a test set if it’s available.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最后几轮的显示指标不足以得出模型是否从数据中学习的结论。首先需要观察学习曲线。然后，我们可以使用其他指标来评估我们的模型。如果有可用的测试集，我们还会对其进行测试。
- en: But before that, let’s see how we can make predictions and get the predicted
    class.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 不过在这之前，让我们看看如何进行预测并获取预测的类别。
- en: 6.1 Making predictions
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 进行预测
- en: 'Once the model trained, we can start making prediction: predict the class of
    the image(s) input. In this section, we will see how to get the predicted class
    for both types of output.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们可以开始进行预测：预测输入图像的类别。在这一部分，我们将看到如何获取两种输出类型的预测类别。
- en: 'In Keras, the `predict` function is used, where x is the input:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，使用`predict`函数，其中x是输入：
- en: '[PRE29]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Since the model is trained on batches, the first thing to do before predicting
    is to expand the input dimension. We take the first instance of the train set
    as an example:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型是基于批次进行训练的，因此在预测之前需要扩展输入维度。我们以训练集的第一个实例为例：
- en: '[PRE30]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, we can make prediction:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以进行预测：
- en: '[PRE31]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '**Single output prediction.** The returned value in this case is a float number
    that can be out the range `[0, 9]` if the model behaved poorly for a given input.
    The first thing to do is the clip the output, so all values are in that range
    the we round to `int` to get the input output as follows:'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单一输出预测。** 在这种情况下返回的值是一个浮点数，如果模型对给定输入表现不好，该值可能超出范围`[0, 9]`。第一步是对输出进行裁剪，使所有值都在该范围内，然后将其四舍五入为`int`，以得到如下输入输出：'
- en: '[PRE32]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The predicted value is 6.9955063, in this case the clip function returns the
    same value. Finally the value is rounded to `int` : the input class is 7.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值为6.9955063，在这种情况下，clip函数返回相同的值。最后，将该值四舍五入为`int`：输入类别为7。
- en: '[PRE33]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '**Onehot encoding output.** In this case, the function predict returns a list
    of 10 element of probabilities so we need to get the index of the element with
    the highest probability:'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一热编码输出。** 在这种情况下，predict函数返回一个包含10个概率元素的列表，因此我们需要获取具有最高概率的元素的索引：'
- en: '[PRE34]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Here, the highest probability is the 8th element that corresponds to class
    7:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，最高概率是第8个元素，对应于类别7：
- en: '[PRE35]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The reason I showed you how to make predictions is not only to learn how predictions
    are made but also to use it later when we compute some validation metrics.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我向你展示如何进行预测的原因不仅仅是为了了解如何进行预测，还为了在计算一些验证指标时能够使用它。
- en: 6.2\. Learning curves
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2\. 学习曲线
- en: 'Learning curves reveals the model performance during training for the seen
    data (train set) and the unseen data (validation set). It allows to:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线揭示了模型在训练期间对已见数据（训练集）和未见数据（验证集）的性能。它允许：
- en: '**Identify overfitting:** when the training loss decreases while the validation
    loss increases.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**识别过拟合：** 当训练损失下降而验证损失增加时。'
- en: '![](../Images/b7eea11eaf2f3c5cd8974d12e9a93892.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7eea11eaf2f3c5cd8974d12e9a93892.png)'
- en: Example of overfitting.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合示例。
- en: '**Identify underfitting:** when both the training and validation losses are
    high both the training and validation errors are high.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**识别欠拟合：** 当训练和验证损失都很高时，训练和验证错误都很高。'
- en: '![](../Images/c4429e733c8908e78a7e23db3c94d830.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4429e733c8908e78a7e23db3c94d830.png)'
- en: 'Example of underfitting: the loss function is around 2 which is a high value
    in our case.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 欠拟合示例：损失函数约为 2，在我们的案例中这是一个较高的值。
- en: '**Compare between models** by comparing the learning curves for different models.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过比较不同模型的学习曲线来比较模型。**'
- en: '![](../Images/307f1eb5f76ded852cd1838071e39a9f.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/307f1eb5f76ded852cd1838071e39a9f.png)'
- en: Learning curves of the trained models (with the first row represents the loss
    function and the second row represents the accuracy metric). We can say that the
    three models learned well, and the third model outperforms the other models since
    it is able to predict classes with high accuracy for both the train and the validation
    set. In other words, it generalizes better.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型的学习曲线（第一行表示损失函数，第二行表示准确率指标）。我们可以说这三种模型都学得很好，而第三种模型优于其他模型，因为它能够对训练集和验证集中的类进行高准确度预测。换句话说，它的泛化能力更强。
- en: 'The learning curves can be plotted as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线可以如下绘制：
- en: '[PRE36]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 6.3\. Evaluation on test set
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3\. 测试集评估
- en: 'Let’s evaluate our models on the test set:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在测试集上评估我们的模型：
- en: '[PRE37]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The first model:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种模型：
- en: '[PRE38]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The second model:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种模型：
- en: '[PRE39]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The third model:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种模型：
- en: '[PRE40]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 6.4\. Evaluation metrics
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4\. 评估指标
- en: 'After training, the model is evaluated using additional metrics. The metrics
    to be computed strongly depend on the nature of the model output: if it is a regression
    (or a continuous variable) the Mean Square Error (MSE) or Mean Absolute Error
    (MAE) can be used; If it is a classification (or a discrete variable) the precision,
    recall, f-measure, the confusion matrix and ROC curve can be used.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，使用额外的指标评估模型。计算的指标很大程度上取决于模型输出的性质：如果是回归（或连续变量），可以使用均方误差（MSE）或平均绝对误差（MAE）；如果是分类（或离散变量），可以使用精确度、召回率、F-measure、混淆矩阵和
    ROC 曲线。
- en: '**Accuracy**. The accuracy is the fraction between the correct predictions
    and all predictions.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率。** 准确率是正确预测与所有预测的比例。'
- en: '**Precision**. The precision represents the positive class predictions that
    belong to the positive class. It provides how much examples are actually positive
    out of all the positive classes that have been predicted correctly.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确度。** 精确度表示属于正类的正类预测。它提供了所有预测正确的正类中实际为正的样本数量。'
- en: '**Recall**. the recall represents positive class predictions made out of the
    positive examples in the dataset. It provides how much examples was predicted
    correctly out of all the positive classes.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**。召回率表示从数据集中正类示例中得到的正类预测。它提供了从所有正类中正确预测的样本数量。'
- en: '**F-measure**. It is a single score that balances both precision and recall.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F-measure**。它是一个平衡精确度和召回率的单一评分。'
- en: Let’s display these measures for the train, validation and test sets. Please
    note that the following instructions are written for the second and the third
    models. If you want to display the metrics for the first model please refer to
    section 6.1 (making predictions) or you can visit my [GitHub repository](https://github.com/Chim-SO/MNIST_classification).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示这些指标在训练、验证和测试集上的表现。请注意，以下说明是针对第二种和第三种模型编写的。如果你想展示第一种模型的指标，请参阅第 6.1 节（做出预测），或者访问我的
    [GitHub 仓库](https://github.com/Chim-SO/MNIST_classification)。
- en: '[PRE41]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The first model:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种模型：
- en: '[PRE42]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The second model:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种模型：
- en: '[PRE43]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The third model:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种模型：
- en: '[PRE44]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: As you can see the third model, the CNN-based model, performed better on the
    test and the validation sets.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，第三种模型，即基于 CNN 的模型，在测试集和验证集上的表现更好。
- en: '**Confusion matrix.** The confusion matrix gives a summary of all the correct
    predictions of each class and all the confusions between each class. It provides
    a detailed insight of how the model performs and which kind of error it makes.
    For instance, thanks to confusion matrix, we can say for a given class what are
    the classes that have confusion with it and how much. In addition, if two classes
    have high confusion between them, we can understand that the model find it difficult
    to distinguish between them.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混淆矩阵。** 混淆矩阵总结了每个类别的所有正确预测和类别之间的所有混淆情况。它提供了模型表现的详细见解，以及它所犯的错误类型。例如，借助混淆矩阵，我们可以了解给定类别与之混淆的其他类别及其程度。此外，如果两个类别之间的混淆程度较高，我们可以理解模型难以区分它们。'
- en: '[PRE45]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '![](../Images/e4953e849d60111f6845a1b898ad8767.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4953e849d60111f6845a1b898ad8767.png)'
- en: Confusion matrix of the third model (CNN-based model).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 第三模型（基于 CNN 的模型）的混淆矩阵。
- en: 6.5\. Display some data
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5. 显示一些数据
- en: 'It is always good to display your model output. In case of classification,
    the misclassified samples are often displayed to gain insight into the types of
    mistakes that the model is making. So let’s display 10 of the misclassified images
    using Matplotlib library:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 显示模型输出总是很有好处。在分类问题中，通常会显示误分类的样本，以获得对模型错误类型的见解。所以，让我们使用 Matplotlib 库显示 10 张误分类的图像：
- en: '[PRE46]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![](../Images/803af2ba311ffe26b0774ce84eddb55a.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/803af2ba311ffe26b0774ce84eddb55a.png)'
- en: Some of misclassified data of the third model.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 第三模型的部分误分类数据。
- en: 7\. Conclusion
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7. 结论
- en: That’s it for this article! In this article, we have learned how to create neural
    networks and train and validate them for classification problems. This article
    is the second tutorial in the ‘Brief Introduction to Neural Networks’ series;
    other types of neural networks will be presented in the same way. If you want
    to delve deeper, you can try exploring and building models for other classification
    problems (such as the Iris dataset, for example) while following the same pipeline
    I described in my tutorials. Even though this tutorial is intended to introduce
    classification in neural networks, it will serve as a reference for more advanced
    tutorials in the future.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是本文的全部内容！在这篇文章中，我们学习了如何创建神经网络，并对分类问题进行训练和验证。本文是“神经网络简要介绍”系列的第二篇教程；其他类型的神经网络将以相同的方式呈现。如果你想深入了解，可以尝试探索和构建其他分类问题的模型（例如
    Iris 数据集），同时遵循我在教程中描述的相同流程。尽管本教程旨在介绍神经网络中的分类，但它将作为未来更高级教程的参考。
- en: Thanks, I hope you enjoyed reading this. You can find the examples here in my
    [GitHub repository](https://github.com/Chim-SO/MNIST_classification). If you have
    any questions or suggestions feel free to leave me a comment below.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 谢谢，我希望你喜欢阅读这篇文章。你可以在我的[GitHub 仓库](https://github.com/Chim-SO/MNIST_classification)中找到示例。如果你有任何问题或建议，请随时在下方留言。
- en: '[](/a-brief-introduction-to-neural-networks-a-regression-problem-c58c26e18008?source=post_page-----43e68c770081--------------------------------)
    [## A Brief Introduction to Neural Networks : a Regression Problem'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 神经网络的简要介绍：回归问题'
- en: A practical beginner guide to Neural Networks in Python
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python 中神经网络的实用初学者指南
- en: towardsdatascience.com](/a-brief-introduction-to-neural-networks-a-regression-problem-c58c26e18008?source=post_page-----43e68c770081--------------------------------)
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[关于神经网络的简要介绍：回归问题](https://towardsdatascience.com/a-brief-introduction-to-neural-networks-a-regression-problem-c58c26e18008?source=post_page-----43e68c770081--------------------------------)'
- en: '**References**'
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[1] [https://keras.io/api/layers/activations/#softmax-function](https://keras.io/api/layers/activations/#softmax-function)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://keras.io/api/layers/activations/#softmax-function](https://keras.io/api/layers/activations/#softmax-function)'
- en: '[2] [https://keras.io/api/layers/regularization_layers/dropout/](https://keras.io/api/layers/regularization_layers/dropout/)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://keras.io/api/layers/regularization_layers/dropout/](https://keras.io/api/layers/regularization_layers/dropout/)'
- en: Image credits
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图片来源
- en: All images and figures in this article whose source is not mentioned in the
    caption are by the author.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中所有未在图注中提及来源的图像和图表均由作者提供。
