["```py\n# multivariate low-rank normal of mean=[0,0], cov_diag=[1,1], cov_factor=[[1],[0]]\n\n# covariance_matrix = cov_diag + cov_factor @ cov_factor.T\n\nm = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([[1.], [0.]]), \ntorch.ones(2))\nm.sample()  \n#tensor([-0.2102, -0.5429])\n```", "```py\n!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00321/LD2011_2014.txt.zip\n!unzip LD2011_2014.txt.zip\n```", "```py\ndata = pd.read_csv('LD2011_2014.txt', index_col=0, sep=';', decimal=',')\ndata.index = pd.to_datetime(data.index)\ndata.sort_index(inplace=True)\ndata.head(5)\n```", "```py\ndata = data.resample('1h').mean().replace(0., np.nan)\nearliest_time = data.index.min()\ndf=data[['MT_002', 'MT_004', 'MT_005', 'MT_006', 'MT_008' ]]\n```", "```py\ndf_list = []\n\nfor label in df:\n\n    ts = df[label]\n\n    start_date = min(ts.fillna(method='ffill').dropna().index)\n    end_date = max(ts.fillna(method='bfill').dropna().index)\n\n    active_range = (ts.index >= start_date) & (ts.index <= end_date)\n    ts = ts[active_range].fillna(0.)\n\n    tmp = pd.DataFrame({'power_usage': ts})\n    date = tmp.index\n\n    tmp['hours_from_start'] = (date - earliest_time).seconds / 60 / 60 + (date - earliest_time).days * 24\n    tmp['hours_from_start'] = tmp['hours_from_start'].astype('int')\n\n    tmp['days_from_start'] = (date - earliest_time).days\n    tmp['date'] = date\n    tmp['consumer_id'] = label\n    tmp['hour'] = date.hour\n    tmp['day'] = date.day\n    tmp['day_of_week'] = date.dayofweek\n    tmp['month'] = date.month\n\n    #stack all time series vertically\n    df_list.append(tmp)\n\ntime_df = pd.concat(df_list).reset_index(drop=True)\n\n# match results in the original paper\ntime_df = time_df[(time_df['days_from_start'] >= 1096)\n                & (time_df['days_from_start'] < 1346)].copy()\n```", "```py\nmax_prediction_length = 24\nmax_encoder_length = 7*24\ntraining_cutoff = time_df[\"hours_from_start\"].max() - max_prediction_length\n\ntraining = TimeSeriesDataSet(\n    time_df[lambda x: x.hours_from_start <= training_cutoff],\n    time_idx=\"hours_from_start\",\n    target=\"power_usage\",\n    group_ids=[\"consumer_id\"],\n    max_encoder_length=max_encoder_length,\n    max_prediction_length=max_prediction_length,\n    static_categoricals=[\"consumer_id\"],\n    time_varying_known_reals=[\"hours_from_start\",\"day\",\"day_of_week\", \"month\", 'hour'],\n    time_varying_unknown_reals=['power_usage'],\n        target_normalizer=GroupNormalizer(\n        groups=[\"consumer_id\"]\n    ),  # we normalize by group\n    add_relative_time_idx=True,\n    add_target_scales=True,\n    add_encoder_length=True,\n)\n\nvalidation = TimeSeriesDataSet.from_dataset(training, time_df, predict=True, stop_randomization=True, min_prediction_idx=training_cutoff + 1)\n\n# create dataloaders for  our model\nbatch_size = 64 \n# if you have a strong GPU, feel free to increase the number of workers  \ntrain_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=2, batch_sampler=\"synchronized\")\nval_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=2, batch_sampler=\"synchronized\")\n```", "```py\nactuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\nbaseline_predictions = Baseline().predict(val_dataloader)\n(actuals - baseline_predictions).abs().mean().item()\n\n# ➢25.139617919921875\n```", "```py\npl.seed_everything(42)\n\nearly_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=4, verbose=True, mode=\"min\")\nlr_logger = LearningRateMonitor(logging_interval='step', log_momentum=True)  # log the learning rate\nlogger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n\ntrainer = pl.Trainer(\n    max_epochs=50,\n    accelerator='auto', \n    devices=1,\n    enable_model_summary=True,\n    gradient_clip_val=0.1,\n    callbacks=[lr_logger, early_stop_callback],\n    logger=logger,\n)\n\nnet = DeepAR.from_dataset(\n    training, \n    learning_rate=0.001, \n    hidden_size=40, \n    rnn_layers=3, \n    loss=MultivariateNormalDistributionLoss(rank=10)\n)\n\ntrainer.fit(\n    net,\n    train_dataloaders=train_dataloader,\n    val_dataloaders=val_dataloader\n)\n```", "```py\nbest_model_path = trainer.checkpoint_callback.best_model_path\nprint(best_model_path)\nbest_deepvar = DeepAR.load_from_checkpoint(best_model_path)\n\n#path:\n#lightning_logs/lightning_logs/version_0/checkpoints/epoch=6-step=40495.ckpt\n```", "```py\n#zip and download the model\n!zip  -r model.zip lightning_logs/lightning_logs/version_0/*\n```", "```py\n!unzip model.zip\nbest_model_path='lightning_logs/lightning_logs/version_0/checkpoints/epoch=6-step=40495.ckpt'\nbest_deepvar = DeepAR.load_from_checkpoint(best_model_path)\n```", "```py\n# Start tensorboard\n%load_ext tensorboard\n%tensorboard --logdir lightning_logs\n```", "```py\nactuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\npredictions = best_deepvar.predict(val_dataloader)\n\n#average p50 loss overall\nprint((actuals - predictions).abs().mean().item())\n#average p50 loss per time series\nprint((actuals - predictions).abs().mean(axis=1))\n\n# 6.78986\n# tensor([ 1.1948,  6.9811,  2.7990,  8.3856, 14.5888])\n```", "```py\nraw_predictions, x = best_deepvar.predict(val_dataloader, mode=\"raw\", return_x=True)\n\nfor idx in range(5):  # plot all 5 consumers\n    fig, ax = plt.subplots(figsize=(10, 4))\n    best_deepvar.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True,ax=ax);\n```", "```py\nfig, ax = plt.subplots(figsize=(10, 5))\n\nraw_prediction, x = best_deepvar.predict(\n    training.filter(lambda x: (x.consumer_id == \"MT_004\") & (x.time_idx_first_prediction == 26512)),\n    mode=\"raw\",\n    return_x=True,\n)\nbest_deepvar.plot_prediction(x, raw_prediction, idx=0, ax=ax);\n```", "```py\ncov_matrix = best_deepvar.loss.map_x_to_distribution(\n    best_deepvar.predict(val_dataloader, mode=(\"raw\", \"prediction\"), n_samples=None)\n).base_dist.covariance_matrix.mean(0)\n\n# normalize the covariance matrix diagnoal to 1.0\ncorrelation_matrix = cov_matrix / torch.sqrt(torch.diag(cov_matrix)[None] * torch.diag(cov_matrix)[None].T)\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\nax.imshow(correlation_matrix, cmap=\"bwr\");\n```"]