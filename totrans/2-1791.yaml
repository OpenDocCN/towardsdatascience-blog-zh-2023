- en: 'RLHF: Reinforcement Learning from Human Feedback'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/rlhf-reinforcement-learning-from-human-feedback-faa5ff4761d1](https://towardsdatascience.com/rlhf-reinforcement-learning-from-human-feedback-faa5ff4761d1)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'ChatGPT’s success ingredient: The Instruction Data.'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://automata88.medium.com/?source=post_page-----faa5ff4761d1--------------------------------)[![Ms
    Aerin](../Images/21335c7f04e64fa34585950f038f96d0.png)](https://automata88.medium.com/?source=post_page-----faa5ff4761d1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----faa5ff4761d1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----faa5ff4761d1--------------------------------)
    [Ms Aerin](https://automata88.medium.com/?source=post_page-----faa5ff4761d1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----faa5ff4761d1--------------------------------)
    ·24 min read·Oct 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT has captivated the world with its impressive capabilities. But how did
    it get so smart?
  prefs: []
  type: TYPE_NORMAL
- en: I recently spoke to one of my former coworkers, a software engineer I respect
    a lot, and I noticed that he believes ChatGPT is a manifestation of AGI, pointing
    to its ability to simplify complex topics to a six-year-old’s level of understanding
    as evidence. While I don’t entirely disagree with him on its unreasonable intelligence,
    I felt compelled to put down my thoughts. In this article, I’d like to emphasize
    that the magic of ChatGPT is heavily reliant on its training data.
  prefs: []
  type: TYPE_NORMAL
- en: Carefully curated instruction data is the key to ChatGPT’s human-like abilities.
    Things like explaining concepts to a 6-year-old, turning a resume into a LinkedIn
    profile, brainstorming ideas with you, etc., didn’t just emerge—they were deliberately
    encoded into the model in the form of training data.
  prefs: []
  type: TYPE_NORMAL
- en: '[After tweeting like this a few times, maybe it’s time time for a long-form...](https://x.com/aerinykim/status/1705640689139396775)'
  prefs: []
  type: TYPE_NORMAL
- en: Like everyone else, this is the first time I am experiencing closed research.
    Since I was in college, all frontier research has been open and peer-reviewed,
    until recently. And I believe openness ultimately advances science more than closedness.
  prefs: []
  type: TYPE_NORMAL
- en: If we aim to match the performance of ChatGPT through open source, I believe
    we need to start taking training data more seriously. A substantial part of ChatGPT’s
    effectiveness might not come from, say, specific ML architecture, fine-tuning
    techniques, or frameworks. But more likely, it’s from the breadth, scale and quality
    of the instruction data.
  prefs: []
  type: TYPE_NORMAL
- en: To put it bluntly, fine-tuning large language models on mediocre instruction
    data is a waste of compute. Let’s take a look at what has changed in the training
    data and learning paradigm—how we are now formatting the training data differently
    and therefore learning differently than in past large-scale pre-training.
  prefs: []
  type: TYPE_NORMAL
- en: What is RLHF?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RLHF stands for Reinforcement Learning from Human Feedback. It has two main
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning (RL)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Human Feedback (HF)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What exactly is being trained?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Historically, when we talk about LLM training, we just mean updating only the
    parameters of the language model. However, **when we use RLHF, we train the parameters
    of three separate models.** This way, it provides far more freedom because it’s
    not confined by the maximum likelihood framework (see [**[WHY did we try RL in
    LLM?]**](#158d) section for details), and we learn an objective function directly
    from the data itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are three models that are being trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Language Model (SFT model)**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: is a large pre-trained language model like GPT-3\. The model has already been
    trained, and it will be fine-tuned based on **instruction data** later.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Reward Model**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: is trained to predict human preferences and provide reward signals to reinforce
    the agent. It is trained on **human feedback** data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Policy Model** **(Agent)**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: is trained to generate tokens by maximizing the predicted reward. To do so,
    it uses reinforcement learning with the reward model as its source of feedback.
    The policy model is initialized from the SFT model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The LLM’s pre-existing weights are adjusted and fine-tuned in RL phase, where
    the model optimizes its actions (generating tokens) to maximize the rewards (good
    human feedback).
  prefs: []
  type: TYPE_NORMAL
- en: The seminal paper on RLHF is [**InstructGPT**](https://arxiv.org/abs/2203.02155),
    which was published last year by OpenAI. Recognizing just how powerful the InstructGPT
    model is, OpenAI switched all of their public APIs from using vanilla models to
    instruct models. Subsequently, they reduced academic publications detailing further
    progress, shifting research in-house. I will mainly use examples and methods from
    InstructGPT in this blog.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Key Innovation in RLHF: Changing the training data format'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before RLHF / ChatGPT / InstructGPT (I’ll use these three terms interchangeably),
    language models like GPT-3 were trained to predict the next word probabilistically
    using cross-entropy loss.
  prefs: []
  type: TYPE_NORMAL
- en: But is predicting the next token probabilistically our end goal?
  prefs: []
  type: TYPE_NORMAL
- en: Not at all! The most impressive aspect of ChatGPT is that it can do **so many
    different tasks** in natural language, such as paraphrasing, summarization, classification,
    and more. This broad-spectrum capability is what makes ChatGPT great and gives
    it a ‘wow’ factor when compared to more specialized, single-purpose ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Then, **in order for a language model to do a variety of tasks instead of just
    predicting the next word, what do we need to do?**
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, if you want to change the model’s behavior, you need to
    change its training data, either its contents, format, or both. You can also change
    the loss function. ChatGPT changed all three aspects.
  prefs: []
  type: TYPE_NORMAL
- en: Before we get into the specifics of RLHF, I’d like to show how the InstructGPT
    team went to great lengths to create extensive and exhaustive training data to
    make ChatGPT a reality.
  prefs: []
  type: TYPE_NORMAL
- en: '**There are two types of human feedback used in RLHF.** One is the **instruction
    data**, and the other is **human preference data**.'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The Instruction Data (aka Demonstration Data)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instruction data are pairs of inputs and outputs that demonstrate how the model
    should behave given an input.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to train your very first InstructGPT model from scratch, you will
    have to write not only the answers but also the user inputs (use cases). Because
    until last year, GPT-3 API users rarely typed in daring prompts like explaining
    complex concepts to a 6-year-old, etc. Users never thought they could ask models
    such questions. And this is why instruction data is also called "demonstration"
    data. **We first have to demonstrate the use cases to the language model.**
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the variety of use cases (prompts) curated by the InstructGPT
    team.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/863d8e80f14f7ba8e82cbb1906efccd2.png)![](../Images/ed0e70635109e3b78429db899e3ceb97.png)![](../Images/220b6b3586ab6111712e0c8b5a31d22f.png)'
  prefs: []
  type: TYPE_IMG
- en: Use Case Examples provided by InstructGPT
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some interesting use cases to highlight:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Closed Q&A use case has clear right and wrong answers like:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Open Q&A use case will have subjective responses:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Rewriting use case will require the labeler’s creativity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The diligence involved in creating Instruction Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s look at what it takes to generate high-quality instruction data. Here’s
    an excerpt from the labeling instructions for the API prompt distribution from
    InstructGPT.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7dc093498a6c14a7fd18429ad8f630bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Excerpt of labeling instructions on the API prompt distribution from InstructGPT
  prefs: []
  type: TYPE_NORMAL
- en: This lengthy section is intended for the “labelers”. It is a long document that
    seems to have a lot of meanings to figure out. We needed this long set of instructions
    because for the labelers to create the instruction data that we want, they must
    first understand what we want them to do and **stick to those rules.**
  prefs: []
  type: TYPE_NORMAL
- en: 'It seems there are three rules you should follow when writing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Being helpful, truthful, and harmless.**'
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at the criteria for being helpful.
  prefs: []
  type: TYPE_NORMAL
- en: '**“**Answering the question they meant to ask, **even if they mis-asked it”.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is a huge ask. It demands labelers try to truly help users rather than
    dismiss their mis-asked questions with responses like “I don’t understand you.”
    This is similar to how a mother will try to understand what her baby wants, even
    if the baby doesn’t say it exactly right.
  prefs: []
  type: TYPE_NORMAL
- en: '**“Being sensitive to internationality (e.g. “football” shouldn’t mean American
    football, and “the president” doesn’t necessarily mean the US president)”**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Labelers should have a strong grasp of language and a good understanding of
    how different cultures work.
  prefs: []
  type: TYPE_NORMAL
- en: So, **who are these labelers capable of diligently following these sophisticated
    guidelines?** They are certainly not part-time workers on a crowdsourcing platform
    who can commit only 1–2 hours a day. Based on my experience creating large-scale
    training data, casual crowd workers can’t sufficiently produce the natural, nuanced
    conversations that enable ChatGPT’s impressive performance.
  prefs: []
  type: TYPE_NORMAL
- en: I prefer the term **“data writers”** over “labelers,” as it better captures
    the creativity and care involved. To make sure these data writers give you the
    quality work you need, you need to train them, over-communicate with them, keep
    them on the same page, review their submissions, give them feedback, and keep
    only the best writers and let the rest go. You will need to be able to trust your
    writers because the performance of your LLMs (the “wow” factor, the quality of
    ChatGPT’s responses to your questions, etc.) will be based on their work. While
    you are their boss, you are also heavily reliant on them. It’s a fascinating symbiotic
    relationship and an art in itself.
  prefs: []
  type: TYPE_NORMAL
- en: The InstructGPT team deserves a lot of credit for taking this art to the next
    level. Their work tells us that if we desire open-source LLMs on par with ChatGPT’s
    performance, the data aspect needs to be executed impeccably.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Preference Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instruction data is used for [the Supervised Fine-Tuning (SFT) phase (details
    in the upcoming section)](#d6fd). The other critical half of the training data
    is the “preference data”. Preference data is used to train the reward model during
    the RL phase. It involves humans ranking different LLM-generated outputs based
    on their preferences. Preference provides training signals on right vs. wrong
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: When I read the labeling guidelines, criteria like “helpful” or “truthful” seemed
    a bit unclear to me. Also, if I were the labeler, I would be less likely to read
    these directions carefully because they are so long. To address this, the InstructGPT
    team went to great lengths to train the labelers by giving them clear examples.
    **This was a crucial step to influence the desired model behavior.**
  prefs: []
  type: TYPE_NORMAL
- en: Here are the examples presented to labelers to help them understand what "helpful,”
    "truthful," and “harmless” mean.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c12f7ea615392e710b93f81f20442e8c.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of prioritizing harmlessness first. Okay, so safety first.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b7fb645756494f906534aa595d48e5f.png)'
  prefs: []
  type: TYPE_IMG
- en: Read the “Reasoning” section above. **The emphasis on the “helpful” aspect of
    training data, in my opinion, was the most important change in ChatGPT. This novel
    way of annotating the data makes InstructGPT stand out from previous research.
    However, it’s also worth noting that the same “being helpful” factor could lead
    to** [**hallucinations (more on this later)**](#f2de)**.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/304c15822a41c5e1bc75fb544b0aebce.png)'
  prefs: []
  type: TYPE_IMG
- en: The above three examples, drawn from the InstructGPT’s [public documentation](https://docs.google.com/document/d/1viWm6I2hBPFL2zqflj4s2it32FRbkETZpUS3CcVdFvo/edit?usp=sharing),
    illustrate the level of training required for the instruction data writers and
    their significant influence on model’s behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Unreasonable Effectiveness of the Instruction Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s compare the outputs of two models — one that was trained with instruction
    data and one that wasn’t.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f52cd690b29ff27126e989cd305e38f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Model comparison: no instruction training vs. instruction training from [https://openai.com/research/instruction-following](https://openai.com/research/instruction-following)'
  prefs: []
  type: TYPE_NORMAL
- en: On the left side, the vanilla version of DaVinci (a model not trained with instruction
    data) fails to comprehend the prompt “Explain the moon landing to a 6-year-old
    in a few sentences”. It doesn’t seem to understand the user’s ask and instead
    offers multiple irrelevant responses such as explaining evolution.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the instruct-DaVinci model on the right, answers the user’s
    prompt right, although it is less elaborate than gpt4\. :)
  prefs: []
  type: TYPE_NORMAL
- en: Why should I care about instruction data?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 1\. Understanding the format of instruction data helps you write better prompts.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The closer your prompts align with the proprietary model’s instruction data,
    the more effective the output will be. Designing prompts that are similar to the
    model’s training data can save you time by reducing the amount of trial-and-error
    needed.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. It partly explains hallucination tendencies.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Various reasons have been proposed to explain hallucinations in models ([On
    the Origin of Hallucinations in Conversational Models: Is it the Datasets or the
    Models?](https://arxiv.org/abs/2204.07931), [Enabling Large Language Models to
    Generate Text with Citations](https://arxiv.org/abs/2305.14627), [Evaluating the
    Factual Consistency of Large Language Models Through Summarization](https://arxiv.org/abs/2211.08412),
    etc.) Some suggest that language models show pattern completion behavior because
    they’re trained to maximize the likelihood of adjacent text. But is this the only
    reason for hallucinations in RLHF?'
  prefs: []
  type: TYPE_NORMAL
- en: '**I don’t think we can overlook the fact that during the preference data labeling,
    labelers were instructed to prioritize helpfulness to users over truthfulness.
    But when we do our final evaluations, we have labelers put the truth first.**'
  prefs: []
  type: TYPE_NORMAL
- en: Refer again to [Example 2, “Prioritizing helpfulness over truthfulness”](#7e80).
  prefs: []
  type: TYPE_NORMAL
- en: This example shows how putting more weight on “helpful” answers in human preference
    data can lead to hallucinations. To mitigate this, we could generate more training
    data that prioritize truthfulness and harmlessness over being helpful in certain
    scenarios, such as high-stake domains like medicine. Balancing the different priorities
    in different situations can help reduce hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing that could cause hallucinations is that the model doesn’t know
    it is allowed to express uncertainty. An important step toward reducing hallucinations
    is to incentivize the model to express uncertainty in words. This has been a long-standing
    problem in NLP, as demonstrated by the specific attempt made by SQUAD (Stanford
    Question Answering Dataset) V2 to address it by not answering when uncertain.
    So while RLHF is a big step forward, some of NLP’s important problems, like how
    to handle uncertainty, are still not completely solved.
  prefs: []
  type: TYPE_NORMAL
- en: Ok, we’re done with the data part. Now let’s look at the RLHF methods.
  prefs: []
  type: TYPE_NORMAL
- en: Three steps of RLHF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAI always shares this simplified diagram to explain how ChatGPT works. I
    hope now you can better appreciate the implications of the minor sub-step “A labeler
    demonstrated the desired output behavior” in Step 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4a42969c59a69e7ff32e3c0db565205.png)'
  prefs: []
  type: TYPE_IMG
- en: That digram
  prefs: []
  type: TYPE_NORMAL
- en: Step 1\. Supervised Fine-Tuning (SFT) Initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in RLHF is supervised fine-tuning (SFT) to initialize the language
    model weights (the first column in the diagram). SFT trains the model on instruction
    data; cloning demonstrated conversational behavior. This step primes the model
    for subsequent reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: You can start SFT with a pretrained model like GPT-3, as OpenAI did for InstructGPT.
    Or you can train one from scratch and go from there. The SFT output provides the
    input for the next reinforcement learning phase.
  prefs: []
  type: TYPE_NORMAL
- en: Properly initialized weights are crucial for strong downstream task performance,
    not just in RLHF but in general. So the SFT model isn’t selected at random. The
    best SFT model will be chosen based on reward model scores using a validation
    set.
  prefs: []
  type: TYPE_NORMAL
- en: '[Some notable excerpts in InstructGPT]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The final reward model was initialized from a 6B GPT-3 model that was fine-tuned
    on a variety of **public NLP datasets (ARC, BoolQ, CoQA, DROP, MultiNLI, OpenBookQA,
    QuAC, RACE, and Winogrande)**. This was mostly for historical reasons; we find
    similar results when initializing the RM from the GPT-3 or SFT models.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We find that our SFT models overfit on validation loss after 1 epoch; however,
    we find that training for more epochs helps both the RM score and human preference
    ratings, despite this overfitting.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Getting good instruction data can be expensive, especially if you don’t have
    thousands of user-submitted seed prompts. What then can you do if you don’t have
    as much resource as commercial enterprises? One option is to use publicly available
    data. Academic datasets mentioned above, SQUAD V1, V2, StackOverflow, Quora, and
    others can all be helpful. You can transform those data to suit your training
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2\. Training Reward Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The reward model’s job is to return **a scalar that represents the human preference**
    when given a pair of (prompt, answer). A high score means preferred, and a low
    score means not preferred.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c544545039168d09980be7bb4a8d91b4.png)'
  prefs: []
  type: TYPE_IMG
- en: The loss function of the reward model
  prefs: []
  type: TYPE_NORMAL
- en: When you see the equation, it might not look straightforward, but this is actually
    a simple formula. Let’s look at this with real data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e18078ff7edd8e4702a0a6a928562dda.png)'
  prefs: []
  type: TYPE_IMG
- en: '[WordMakeover.com](https://wordmakeover.com) for effective email writing'
  prefs: []
  type: TYPE_NORMAL
- en: '**x** = input, question, or prompt'
  prefs: []
  type: TYPE_NORMAL
- en: '**y_w** = winning output'
  prefs: []
  type: TYPE_NORMAL
- en: '**y_l** = losing output'
  prefs: []
  type: TYPE_NORMAL
- en: '**K** = number of outputs (7 here because there are 7 LLM results)'
  prefs: []
  type: TYPE_NORMAL
- en: '**θ** = reward model parameters being trained'
  prefs: []
  type: TYPE_NORMAL
- en: '**r_θ** = reward score from model (scalar)'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know every variable in the equation, let's understand why this loss
    function looks the way it does. Suppose the rightmost term, the difference between
    **r_θ** (winning pair) and **r_θ** (losing pair), holds a certain value. The sigmoid
    will make that difference fall between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Visualize the log graph between 0 and 1 after the sigmoid. When the input comes
    closer to zero, it plummets to negative infinity, and when the input comes closer
    to one, it rises to zero. From this, you can see why the model will suffer a hefty
    penalty if it assigns a larger reward value to the losing pair compared to the
    winning pair.
  prefs: []
  type: TYPE_NORMAL
- en: You do this for all 7C2 pairs and then take the average. And that is the loss
    you want to minimize.
  prefs: []
  type: TYPE_NORMAL
- en: 'For those who love codes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The reward model is initialized from the SFT model. Then we remove the final
    embedding layer and add a linear layer that gives a scalar.
  prefs: []
  type: TYPE_NORMAL
- en: Size-wise, the reward model is often smaller than the language model. For example,
    InstructGPT used a 175B-parameter language model but a 6B-parameter reward model.
    The team reports that 175B reward model training was unstable, making it less
    suitable for use as the value function during RL.
  prefs: []
  type: TYPE_NORMAL
- en: What’s the purpose of ranking?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ranking makes it easy to compare two outputs. **With n outputs, ranking can
    easily generate nC2 pairs from just one-time labeling.**
  prefs: []
  type: TYPE_NORMAL
- en: One disadvantage of binary options is the lack of granularity. **They don’t
    capture how much better output A is over B.** And without quantifying that difference,
    **errors can’t be precisely penalized based on severity.** The alternative is
    to have a labeler give integers or floats, but that is very subjective and hard
    to calibrate across different labelers.
  prefs: []
  type: TYPE_NORMAL
- en: Can anyone think of a better way to frame the preference problem? :)
  prefs: []
  type: TYPE_NORMAL
- en: Step 3\. Optimize a policy against the reward model using RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This step is in one sentence: **the LLM parameters and policy are jointly optimized**
    to maximize expected rewards from the reward model.'
  prefs: []
  type: TYPE_NORMAL
- en: WHY did we try RL in LLM?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the past, language models rarely used RL for optimization. Instead, they
    relied on information theory loss functions like cross-entropy to optimize using
    maximum likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: While both maximum likelihood and RL are used for learning, the way they update
    parameters is based on different principles. Maximum likelihood is based on minimizing
    error with respect to correct answers **with a fixed loss function**, whereas
    RL is based on **a learnable reward function** while maximizing cumulative reward
    through interaction with an environment.
  prefs: []
  type: TYPE_NORMAL
- en: People (e.g., [John Schulman](https://www.youtube.com/live/hhiLw5Q_UFg?si=r2Cs_4WHoS5KJ6q2),
    [YoavGo](https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81), etc.)
    have given numerous justifications for training LLM with RL, but if I push for
    an intuitive answer, I believe we tried RL **because we wanted the flexibility
    to train the objective function as well.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional language model training optimizes only one thing: the model parameters,
    while keeping the loss function fixed. This approach restricts flexibility, as
    **losses like cross-entropy bring in strong inductive biases with themselves—the
    maximum likelihood.** It rewards the most probable next-token predictions, assuming
    the highest likelihood output is optimal.'
  prefs: []
  type: TYPE_NORMAL
- en: If we use RL, we are training not only the model parameters but also the reward
    function and training policy. **The reward function acts as a learnable loss function**
    tailored to the end goal. This provides greater freedom to optimize because we
    are no longer bound by the maximum likelihood framework. **We can learn an objective
    function from the data.** In RLHF, your objective function is the reward model,
    and you use RL to optimize that objective function.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we tried RL to parameterize and learn the objective function.
    This is still a running experiment.
  prefs: []
  type: TYPE_NORMAL
- en: How can we formulate this as an RL problem?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The end goal of ChatGPT is to generate text that humans prefer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we can define the components of the RL problem as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Agent**: **The language model** acts as the RL agent. It learns to generate
    text that is deemed optimal based on the reward system.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Action space**: The action space in this case is the set of all possible
    language outputs that the LLM can generate. Given the variability of language,
    this space is vast.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Policy**: The policy is **the probability distribution over possible outputs**
    of the model at each generation step. It determines which actions the agent should
    take based on the current state.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Environment**: The environment is what the agent interacts with and where
    it receives feedback on its actions. In the RLHF case, the environment gives feedback
    to the agent by giving them rewards based on a human preference model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reward**: The reward is a scalar signal that comes from the human preference
    model. The agent’s goal in RL is to maximize this expected reward over time, leading
    to better text generation.'
  prefs: []
  type: TYPE_NORMAL
- en: By framing language generation as an RL problem this way, the model can interact
    with the reward model to improve its policy over time.
  prefs: []
  type: TYPE_NORMAL
- en: For those who understand best by reading code, here is [a straightforward implementation
    of the RLHF trainer, generously contributed by our open-source contributor, Phil
    Wang](https://github.com/lucidrains/PaLM-rlhf-pytorch/tree/main).
  prefs: []
  type: TYPE_NORMAL
- en: Anticipating that someone will inevitably abstract this version, I’ve copied
    the trainer script here. This covers most of the PPO training components and flow.
  prefs: []
  type: TYPE_NORMAL
- en: The `generate` function generates sequences of text based on a given prompt.
    It uses the actor-critic model to generate the sequence and the reward model to
    score each sequence. The sequence with the highest score is selected as the best
    sequence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `learn` function batches experiences, computes PPO losses, and updates actor
    & critic networks. Implements core PPO algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `train` loop collects actor experience, evaluates reward, and stores in
    memory. Calls `learn()` periodically to update policy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Proximal Policy Optimization (PPO)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can we take the biggest possible improvement step on a policy using the
    data we currently have without stepping so far that we accidentally cause performance
    collapse?
  prefs: []
  type: TYPE_NORMAL
- en: Proximal Policy Optimization (PPO) is a reinforcement learning algorithm that
    strikes a balance between sample efficiency and ease of implementation. To keep
    the policy from changing too much, its objective function uses a clipped surrogate
    objective. Hence the word “proximal” in its name. This strategy ensures stable
    and consistent learning while avoiding the often complex implementation of other
    algorithms that aim to achieve the same result.
  prefs: []
  type: TYPE_NORMAL
- en: I won’t go into details about policy optimization and its implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'How PPO works deserves its own blog post, so I’ll link to some good, in-depth
    tutorials here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.interconnects.ai/p/specifying-objectives-in-rlhf?source=post_page-----faa5ff4761d1--------------------------------)
    [## Specifying objectives in RLHF'
  prefs: []
  type: TYPE_NORMAL
- en: At ICML, it is obvious that many people are getting value out of RLHF. What
    is limiting the scientific understanding of…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.interconnects.ai](https://www.interconnects.ai/p/specifying-objectives-in-rlhf?source=post_page-----faa5ff4761d1--------------------------------)  [##
    Proximal Policy Optimization - Spinning Up documentation
  prefs: []
  type: TYPE_NORMAL
- en: '(Previously: Background for TRPO) PPO is motivated by the same question as
    TRPO: how can we take the biggest possible…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: spinningup.openai.com](https://spinningup.openai.com/en/latest/algorithms/ppo.html?source=post_page-----faa5ff4761d1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Size Comparison**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data used for InstructGPT was orders of magnitude smaller than the data
    used for pretraining the foundation models.
  prefs: []
  type: TYPE_NORMAL
- en: Pretraining data like the one used for GPT-3 was 300 B tokens. In contrast,
    the InstructGPT uses ~O(10M) tokens.
  prefs: []
  type: TYPE_NORMAL
- en: The supervised fine-tuning (SFT) uses around 15,000 prompts for training and
    1,500 for validation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reward model uses the most training and validation prompts, approximately
    **150,000** and **80,000,** respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reinforcement learning phase uses only ~32,000 prompts for training and
    ~16,000 for validation to optimize the agent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So in total, the RLHF data is on the order of 10 million tokens — dwarfed by
    the hundreds of billions used for general pre-training.
  prefs: []
  type: TYPE_NORMAL
- en: I’ll wrap up this blog post by highlighting InstructGPT’s beautiful and promising
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Result: It’s better to train with the right type of data than to make the model
    100 times bigger.'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/412338746ca97ae340436913631ba7bb.png)'
  prefs: []
  type: TYPE_IMG
- en: From InstructGPT
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at the graph. The red and yellow lines represent Instruct-PPO variants,
    which are RLHF methods.
  prefs: []
  type: TYPE_NORMAL
- en: The ELO rating is on the left, with higher numbers indicating a preference.
  prefs: []
  type: TYPE_NORMAL
- en: The PPO models have only 1.3 billion parameters, whereas SFT and GPT models
    (represented by the green and blue lines) have 175 billion parameters. Despite
    having far smaller parameters, humans significantly prefer the outputs from InstructGPT
    over those from GPT-3.
  prefs: []
  type: TYPE_NORMAL
- en: '**This shows that training with the right kind of data is more helpful than
    just making the model a hundred times bigger.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e5924d00597d4727fd9be97e9c8fa10.png)'
  prefs: []
  type: TYPE_IMG
- en: InstructGPT performs better across several other concrete metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our Holy Grail: The Emergent Generalization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though I dismissed my colleague’s claim about “The Emergent Generalization”
    by reminding him that all the prompts were in training data, InstructGPT team
    did observe the emergence of generalization. They reported a degree of **generalization
    in following instructions extending to new domains.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88acc927659ce2a94108c14a4aa7d15c.png)'
  prefs: []
  type: TYPE_IMG
- en: Despite the fact that 99% of the training data was in English, the InstructGPT
    model occasionally demonstrated the ability to follow instructions in French.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95373cc8156da402187419418b3558b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Furthermore, while there were no programming-specific instructions in the training
    set, there was some generalization shown in the code QA scenarios. GPT-3 doesn’t
    really answer the prompt, but InstructGPT does a reasonably good job.
  prefs: []
  type: TYPE_NORMAL
- en: These hints of generalization indicate the coveted phenomenon of emergence in
    AI. Though InstructGPT’s skills are largely based on its training data, I believe
    the glimpses beyond it point to **the beginnings of learned reasoning**.
  prefs: []
  type: TYPE_NORMAL
- en: I’m optimistic about further breakthroughs as RLHF research scales up. If basic
    reinforcement learning can unlock some generalization, advancing to bigger and
    better models might help with even broader emergent intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Improving Open-Source RLHF Training Data: Action Items'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, I’d like to talk about the actions we can take to make open-source
    RLHF data better.
  prefs: []
  type: TYPE_NORMAL
- en: We’re stuck in a vicious loop right now. Because there aren’t any good open-source
    LLMs like ChatGPT, not as many people are using them. This results in fewer user
    prompts to train and improve upon, leaving us with mediocre models. Meanwhile,
    commercial LLMs gain more users and continually improve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few ways we can break this cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '**One central hub where the prompts, results, and feedback from open-source
    users (who have opted in) are collectively assembled**: At the moment, the only
    platform that I know of where I can try LLama 2 is [POE](https://poe.com/Llama-2-70b).
    However, the open-source maintainers can’t access the user inputs (prompts) and
    the model’s output, which are vital for improving the open-source model. We need
    to **make those data available to people who are working on open-source models.**
    This step alone will make open-source LLMs so much better. We also need to enhance
    the user experience of this platform in order to attract more users, which will
    lead to more data and better models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**A unified repository for data preparation codes:** A single hub where all
    open-source LLM enthusiasts can share their data work, such as cleaning, transformation,
    preparation, and auto-labeling, would be very beneficial. Examples include code
    that converts web content into a trainable format and automates the reformatting
    of some unlabeled data, such as text from textbooks, into prompt-response pairs.
    At the moment, all of the data effort used in open-source RLHF is dispersed and
    untracked. This makes sense, because this core and hard data work is what differentiates
    different LLMs. To leverage the power of community, though, we need to establish
    a single, centralized hub.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Incentivize data sharing.** This is the hardest part, and it is easier said
    than done. I don’t have a good answer for this at the moment. In order for open
    source to progress, people need to be transparent about their training data. We
    need to find a way to incentivize data work and sharing. We also need to figure
    out the close collaboration between the open-source data lead and those training
    the LLMs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we can work out the data and feedback loop parts, I do believe there is potential
    for the community to create better LLMs than those currently available commercially.
    It’s an ambitious goal, but with collective community effort, I believe it’s achievable.
    I hope that after reading this blog post, you’ll be a little more motivated to
    contribute to open-source data.
  prefs: []
  type: TYPE_NORMAL
- en: Really grateful to my reviewers for squeezing me into their packed schedules
    and sharing their thoughts to the blog. Without them, this blog wouldn’t be half
    as good.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shoutout to (in alphabetical order by last name): Nathan Lambert (ex-Huggingface),
    Rosanne Liu (Deepmind, ML Collective), Erin LeDell (AutoML), Joshua Moore (Snap),
    Abhinav Srivastava (Breez), Susan Zhang (OPT-175B)'
  prefs: []
  type: TYPE_NORMAL
