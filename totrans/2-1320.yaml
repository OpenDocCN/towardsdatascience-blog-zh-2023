- en: How To Improve The Performance of Python Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/improve-python-function-performance-587cfd4fd511](https://towardsdatascience.com/improve-python-function-performance-587cfd4fd511)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Speeding up frequently called functions in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://gmyrianthous.medium.com/?source=post_page-----587cfd4fd511--------------------------------)[![Giorgos
    Myrianthous](../Images/ff4b116e4fb9a095ce45eb064fde5af3.png)](https://gmyrianthous.medium.com/?source=post_page-----587cfd4fd511--------------------------------)[](https://towardsdatascience.com/?source=post_page-----587cfd4fd511--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----587cfd4fd511--------------------------------)
    [Giorgos Myrianthous](https://gmyrianthous.medium.com/?source=post_page-----587cfd4fd511--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----587cfd4fd511--------------------------------)
    ·9 min read·Mar 10, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4ef78a78679f6b7ede3800157e16a73.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Esteban Lopez](https://unsplash.com/@exxteban?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/6yjAC0-OwkA?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: In today’s world, where the amount of data being processed is growing at an
    unprecedented rate, having efficient and optimized code has become more important
    than ever. Python, being a popular programming language, offers several built-in
    tools to optimize the performance of your code. One of these tools is the `lru_cache`
    decorator, which can be used to cache the results of a function, thereby improving
    its performance.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will explore the benefits of using the `lru_cache` decorator
    in Python, and how it can help you write faster and more efficient code for frequently
    called functions.
  prefs: []
  type: TYPE_NORMAL
- en: Caching in a nutshell
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Caching is a widely adopted approach to enhance the performance of computer
    programs. It involves temporarily storing information that is costly to compute
    or frequently accessed within a specified time frame. Through caching, developers
    can efficiently store the results of previous computations and reduce the time
    and computational resources required to recompute them. This process can considerably
    improve the response time and overall performance of the system.
  prefs: []
  type: TYPE_NORMAL
- en: Caching can be implemented using various data structures such as arrays, hash
    tables, and linked lists, and can also be managed through different policies such
    as Least Recently Used (LRU) and First In First Out (FIFO).
  prefs: []
  type: TYPE_NORMAL
- en: Managing the size of a cache can be a crucial aspect of its implementation.
    Failure to put in place a mechanism that regularly shrinks the cache could result
    in an ever-increasing memory usage, leading to an eventual application crash.
    To overcome this challenge, programmers must carefully consider the appropriate
    policy or strategy that best suits their specific requirements.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, adaptive policies, which adapt to changing workload patterns,
    may be more suitable. Moreover, cache size management can be done through different
    techniques, such as setting a maximum size, implementing time-based expiry, or
    using a hybrid approach that combines multiple policies.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, selecting an appropriate cache management policy requires careful
    consideration of factors such as performance, memory usage, and the specific needs
    of the application. Overall, caching is an effective technique that allows programmers
    to optimize their code and improve the user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Least Recently Used cache with lru_cache decorator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `lru_cache` decorator is part of the language’s standard library and can
    be imported from `functools` module.
  prefs: []
  type: TYPE_NORMAL
- en: The decorator uses a strategy called **Least Recently Used (LRU)** to prioritize
    objects that were used most recently. Whenever an object is accessed using a cache
    that uses the LRU strategy, the cache places it at the top while shifting all
    the other objects down one position. However, if the cache reaches its maximum
    size, the least recently used object is removed to create room for new objects.
    This ensures that frequently used objects stay in the cache while infrequently
    used ones are gradually pushed out.
  prefs: []
  type: TYPE_NORMAL
- en: .. frequently used objects stay in the cache while infrequently used ones are
    gradually pushed out
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Caches implementing LRU come in handy when dealing with computationally expensive
    or I/O bound functions that are repeatedly called with the same arguments.
  prefs: []
  type: TYPE_NORMAL
- en: The `lru_cache` decorator in Python operates by maintaining a **thread-safe
    cache in the form of a dictionary**. Upon invocation of the decorated function,
    a new entry is created in the cache, where the function call arguments correspond
    to the dictionary key and the returned value to the dictionary value. Subsequent
    calls to the function with the same arguments will not compute a new result; rather,
    the cached value will be retrieved. This feature avoids redundant computations,
    thereby improving the overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: Since a dictionary is used to cache results, the positional and keyword arguments
    to the function must be [hashable](https://docs.python.org/3/glossary.html#term-hashable)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- Python Documentation'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**The size of the** `**lru_cache**` **can be adjusted through the** `**maxsize**`
    **argument**, that defaults to `128`. This value specifies the maximum number
    of entries that the cache can hold at any given time. Once the cache reaches its
    maximum size and a new call is made, the decorator will discard the least recently
    used entry to make room for the most recent one. This ensures that the cache does
    not grow beyond the specified limit, thus preventing excessive memory consumption
    and improving performance. If the `maxsize` is set to `None` the `LRU` feature
    is then disabled which means that the cache can grow indefinitely.'
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that when two function calls have the same keyword arguments
    but in a different order, two separate cache entries will be created. For instance,
    calling `func(x=10, y=5)` and `func(y=5, x=10)` will result in two distinct entries
    in the cache, even if both calls return the same value.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the `lru_cache` decorator accepts another argument called `**typed**`
    that defaults to `False`.
  prefs: []
  type: TYPE_NORMAL
- en: If `*typed*` is set to `true`, function arguments of different types will be
    cached separately. If `*typed*` is `false`, the implementation will usually regard
    them as equivalent calls and only cache a single result.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — [Python Documentation](https://docs.python.org/3/library/functools.html#functools.lru_cache)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For instance, calling `func(a=4)` (`a` holds an integer value) and `func(a=4.)`
    (`a` now holds a float value) when `typed=True`, two distinct entries will be
    created in the cache. Note however that some types such as `str` and `int` might
    be cached in separate cache entries even when `typed` is set to `False`.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s assume we have a very simple Python function that accepts two arguments
    and returns their sum
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s assume that the function is decorated with the `lru_cache` and gets
    called several times, as outlined below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The image below illustrates how a LRU cache with `maxsize=3` is maintained over
    time and how it behaves upon cache hit or miss and how it handles cases when the
    current size reaches the specified maximum size.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0fd75bd5c48f6a5a0689016d786bb922.png)'
  prefs: []
  type: TYPE_IMG
- en: 'LRU cache will prioritize most recently used function calls — Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: Initially, when the function is called with arguments `a=10` and `b=15`, the
    cache is empty and a cache miss occurs. The function is executed, and both the
    input arguments and the resulting value are stored in the cache.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next call, when the function is called with arguments `a=10` and `b=10`,
    a cache miss occurs again since this combination of arguments is not found in
    the cache. The function is executed, and a new entry is created in the cache and
    placed at the top.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subsequently, when the function is called with arguments `a=3` and `b=15`, another
    cache miss occurs. The input arguments and the resulting value are stored in the
    cache after the function is executed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next call, when the function is called with arguments `a=20` and `b=1`,
    a cache miss occurs, and the cache has reached its maximum size. This means that
    the least recently used entry is removed, and the remaining entries are shifted
    down by one position. After executing the function, the latest entry is added
    to the cache and placed at the top.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, when the function is called again with arguments `a=3` and `b=15`,
    a cache hit occurs since this entry is now stored in the cache. The returned value
    is fetched from the cache instead of executing the function. The cache entry is
    then moved to the top.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How and when to use lru_cache decorator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having gained a solid understanding of cache and the LRU caching strategy, it’s
    time to delve deeper into lru_cache and learn how to utilize it to its fullest
    potential, while also assessing its impact.
  prefs: []
  type: TYPE_NORMAL
- en: For the purposes of this tutorial, we will examine a more complex function that
    will greatly benefit from caching, and `lru_cache` decorator in particular. Our
    `fibonacci` function can be used to compute **fibonacci numbers.**
  prefs: []
  type: TYPE_NORMAL
- en: 'In mathematics, the **Fibonacci numbers**, commonly denoted *Fn* , form a sequence,
    the **Fibonacci sequence**, in which each number is the sum of the two preceding
    ones. The sequence commonly starts from 0 and 1, although some authors start the
    sequence from 1 and 1 or sometimes (as did Fibonacci) from 1 and 2\. Starting
    from 0 and 1, the first few values in the sequence are:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144.`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- [Wikipedia](https://en.wikipedia.org/wiki/Fibonacci_number)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is worth noting that the function below computes the Fibonacci number recursively.
    Although we can introduce memoization to optimize the solution further, this is
    beyond the scope of this tutorial. In addition, for the sake of simplicity, I
    will keep things straightforward and allow even those with less experience to
    follow along.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that the implementation is not fully optimized serves our purpose well,
    as it demonstrates how `lru_cache` can enhance the performance of computationally
    expensive function calls.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: To evaluate the time it takes for our function to produce results for several
    distinct inputs, we can use the `timeit` module. We’ll utilize the timeit module
    to measure the performance of the `fibonacci` function, and take the minimum timing
    from 5 repeated function calls with the same argument.
  prefs: []
  type: TYPE_NORMAL
- en: '[When measuring execution time] Use the `min()` rather than the average of
    the timings. That is a recommendation from me, from Tim Peters, and from Guido
    van Rossum. The fastest time represents the best an algorithm can perform when
    the caches are loaded and the system isn’t busy with other tasks. All the timings
    are noisy — the fastest time is the least noisy. It is easy to show that the fastest
    timings are the most reproducible and therefore the most useful when timing two
    different implementations.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Raymond Hettinger [on StackOverflow](https://stackoverflow.com/questions/8220801/how-to-use-timeit-module#comment12782516_8220943)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In essence, our script will measure the time elapsed to executed the 30th fibonacci
    number with `fibonacci`. Out of 5 different calls, the minimum time it took on
    my machine was 1.39 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `fibonacci` function is deterministic, meaning that it will always produce
    the same output given the same input. Therefore, we can take advantage of the
    concept of caching. By adding the `@lru_cache` decorator, the function’s output
    for a given input is now cached, and if the function is called again with the
    same input, it will return the cached result instead of recomputing it. This can
    significantly speed up the function’s execution time, particularly when called
    repeatedly with the same input values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s time the new version of `fibonacci` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: On my machine, the minimum timing was close to zero. `fibonacci(30)` was executed
    only during the first iteration. For the subsequent iterations, the cached result
    of this function call was retrieved, which is an inexpensive operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Final Thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In conclusion, optimizing the performance of your code has become more important
    than ever in today’s data-driven world. One way to achieve this in Python is by
    using the `lru_cache` decorator, which can cache the results of frequently called
    functions and improve their performance.
  prefs: []
  type: TYPE_NORMAL
- en: Caching, in general, involves temporarily storing information that is costly
    to compute or frequently accessed, reducing the time and computational resources
    required to recompute them. The decorator uses a strategy called Least Recently
    Used (LRU) to prioritize objects that were used most recently and discard the
    least recently used objects once the cache reaches its maximum size.
  prefs: []
  type: TYPE_NORMAL
- en: '`lru_cache` decorator is an efficient tool for optimizing Python code performance
    by caching the results of computationally expensive or I/O bound functions.'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Become a member**](https://gmyrianthous.medium.com/membership) **and read
    every story on Medium. Your membership fee directly supports me and other writers
    you read. You’ll also get full access to every story on Medium.**'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://gmyrianthous.medium.com/membership?source=post_page-----587cfd4fd511--------------------------------)
    [## Join Medium with my referral link — Giorgos Myrianthous'
  prefs: []
  type: TYPE_NORMAL
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: gmyrianthous.medium.com](https://gmyrianthous.medium.com/membership?source=post_page-----587cfd4fd511--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Related articles you may also like**'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/python-iterables-vs-iterators-688907fd755f?source=post_page-----587cfd4fd511--------------------------------)
    [## Iterables vs Iterators in Python'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the difference between Iterables and Iterators in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/python-iterables-vs-iterators-688907fd755f?source=post_page-----587cfd4fd511--------------------------------)
    [](/requirements-vs-setuptools-python-ae3ee66e28af?source=post_page-----587cfd4fd511--------------------------------)
    [## requirements.txt vs setup.py in Python
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the purpose of requirements.txt, setup.py and setup.cfg in Python
    when developing and distributing…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/requirements-vs-setuptools-python-ae3ee66e28af?source=post_page-----587cfd4fd511--------------------------------)
    [](/make-class-iterable-python-4d9ec5db9b7a?source=post_page-----587cfd4fd511--------------------------------)
    [## How To Create User-Defined Iterables in Python
  prefs: []
  type: TYPE_NORMAL
- en: Showcasing how to create user-defined iterators and make user-defined classes
    Iterables in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/make-class-iterable-python-4d9ec5db9b7a?source=post_page-----587cfd4fd511--------------------------------)
  prefs: []
  type: TYPE_NORMAL
