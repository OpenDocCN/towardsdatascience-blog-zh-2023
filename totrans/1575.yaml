- en: Neural Basis Models for Interpretability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/neural-basis-models-for-interpretability-fd04ac958ff2](https://towardsdatascience.com/neural-basis-models-for-interpretability-fd04ac958ff2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unpacking the new interpretable model proposed by Meta AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@upadhyan?source=post_page-----fd04ac958ff2--------------------------------)[![Nakul
    Upadhya](../Images/336cb21272e9b1f098177adbde50e92e.png)](https://medium.com/@upadhyan?source=post_page-----fd04ac958ff2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fd04ac958ff2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fd04ac958ff2--------------------------------)
    [Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----fd04ac958ff2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fd04ac958ff2--------------------------------)
    ·6 min read·Oct 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: The widespread application of Machine Learning and Artificial Intelligence across
    various domains brings about heightened challenges regarding risks and ethical
    assessments. As seen in case studies like the [criminal recidivism model reported
    on by ProPublica,](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)
    machine learning algorithms can be incredibly biased and, as a result, robust
    explainability mechanisms are needed to ensure trust and safety when these models
    are deployed in high-stakes areas.
  prefs: []
  type: TYPE_NORMAL
- en: So, how do we balance interpretability with accuracy and model expressivity?
    Well, Meta AI researchers have proposed a new approach they dubbed [Neural Basis
    Models (NBMs)[1]](https://proceedings.neurips.cc/paper_files/paper/2022/hash/37da88965c016dca016514df0e420c72-Abstract-Conference.html),
    a sub-family of generalized additive models that achieve state-of-the-art (SOTA)
    performance on benchmark datasets while retaining glass-box interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I aim to explain the NBM and what makes it a beneficial model.
    As usual, I encourage everyone to read the original paper.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re interested in interpretable machine learning and other aspects of
    ethical AI, consider checking out some of my other articles and following me!
  prefs: []
  type: TYPE_NORMAL
- en: '![Nakul Upadhya](../Images/e62aa67aa11cd0f9bcd1132257fc3773.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----fd04ac958ff2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Interpretable and Ethical AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@upadhyan/list/interpretable-and-ethical-ai-f6ee1f0b476d?source=post_page-----fd04ac958ff2--------------------------------)5
    stories![](../Images/3718151c0f72303f3d1c71f54229bc98.png)![](../Images/eddb4279ebae7fc1ba79cf6dcc6ebd5a.png)![](../Images/7def8e23dad656929857f2a488b1f547.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Background: GAMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NBM is considered a Generalized Additive Model (GAM). GAMs are inherently interpretable
    models that learn a shape function for each feature, and predictions are made
    by “querying” the shape function. Since these shape functions are independent,
    the impact of a feature on the prediction can be understood by visualizing these
    shape functions, making them highly explainable. Interactions between variables
    are modelled by passing multiple variables into the same function and constructing
    the shape function based on that (usually limiting the number of variables to
    2 for interoperability), a configuration that is called a GA2M.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/742a70d6f2f1f6bd50d99c2bca1aab04.png)'
  prefs: []
  type: TYPE_IMG
- en: Equations for GAMs and GA2Ms (Figure from Radenovic et al. [1])
  prefs: []
  type: TYPE_NORMAL
- en: The various GAM and GA2M models use different mechanisms for developing these
    shape functions. The Explainable Boosting Machine (EBM) [2] uses a set of boosted
    trees trained on each feature, Neural Additive Models (NAMs) [3] use a deep neural
    network for each feature, and NODE-GAM [4] uses ensembles of [oblivious neural
    trees](https://medium.com/towards-data-science/node-tabular-focused-neural-trees-ee08c752fcd2)[6].
    I recommend reading the following articles on the [EBM](/the-explainable-boosting-machine-f24152509ebb)
    and [NODE-GAM/NAM](https://medium.com/@chkchang21/interpretable-deep-learning-models-for-tabular-data-neural-gams-500c6ecc0122)
    for a more detailed explanation of these models.
  prefs: []
  type: TYPE_NORMAL
- en: NBM Approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural Basis Models (NBM) is a new subfamily of Generalized Additive Models
    (GAMs) that utilizes basis decomposition of shape functions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ca87a05f2e6c1c910d702e09e31938c9.png)'
  prefs: []
  type: TYPE_IMG
- en: NBM Architecture (Figure from Radenovic et al. [1])
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike other GAM models (like NAM[3]), which effectively train independent
    models for each feature to construct the shape function, the NBM architecture
    instead relies on a small number of basis functions shared among all features
    and learned jointly for a given task. What are these functions? Well, the Swiss-army
    knife of function approximations: the deep neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: Effectively, a common MLP backbone that takes in 1 input and outputs *B* values
    is trained and applied to each input feature. These outputs are then linearly
    combined to form the final prediction for the given feature, and the linear combination
    weights differ for each feature. Another way to think of this architecture is
    through the lens of encoder-decoder networks. All the features share the same
    *encoder* (the common MLP backbone), but each has its own *decoder* (the linear
    transformation of the encoding). The decoded values for each feature are then
    summed together to create the final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: This can easily be extended to include feature interactions as well. If we want
    to model pairwise interactions, we include an MLP that takes two inputs instead
    of one.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/223677c0fc646eb67ffc56c432711152.png)'
  prefs: []
  type: TYPE_IMG
- en: NBM and NB2M Equations (Figure from Radenovic et al. [1])
  prefs: []
  type: TYPE_NORMAL
- en: One benefit of using shared MLP backbones instead of different MLPs for each
    feature is the significantly smaller size of the model. This makes the NBM incredibly
    suited to tasks related to extremely high-dimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: Performance and Benefits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To test their architecture, Radenovic et al. (2022) compared NBM to various
    other models like Linear Regression, the EBM [2], NAM [3], XGBoost[5], and an
    MLP. Their first evaluation was on a mix of tabular and image datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6627de5d6dd91ec42eea5363382b0db2.png)'
  prefs: []
  type: TYPE_IMG
- en: Performance Comparison across Baselines (Figure from Radenovic et al. [1])
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the NBM holds its ground, outperforming the other interpretable models
    of the datasets and even outperforming the MLP on some datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Radenovic et al. (2022) also did another evaluation on purely tabular datasets,
    focusing on getting a good comparison between SOTA GAM models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d9dbebf2587c6c4e2e6d7d53019b098.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Performance comparison against other GAMS: (Figure from Radenovic et al. [1])'
  prefs: []
  type: TYPE_NORMAL
- en: This comparison clearly shows the power of the NBM, beating out the competitors
    in almost every dataset. As mentioned before, the scalability of the NBM is also
    exceptional. As shown below, the number of parameters in an NBM is nearly 70 times
    less than in a NAM for high-dimensional data tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4cb1f55b382cb1d68c132d295e0fd895.png)'
  prefs: []
  type: TYPE_IMG
- en: NAM/NBM Parameter Comparison. X-axis is the dimensionality of the data (Figure
    from Radenovic et al. [1])
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overall, NBMs are incredibly powerful and lightweight models that are inherently
    interpretable due to being a GAM. However, this doesn’t mean it’s a silver-bullet
    solution to high-stakes machine-learning problems. There are still a significant
    number of considerations that need to be taken into account when utilizing these
    models. For one, an inherently interpretable model means almost nothing if the
    [features inputted into the model are not interpretable](https://medium.com/towards-data-science/defining-interpretable-features-ebd7ed94897).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, while the size of NBMs scales well compared to NAMs, the interpretability
    doesn’t. No single human can look at thousands of feature attribution charts,
    especially if those attribution charts also include pairwise interactions. This
    means that pre-processing methods such as feature selection are still needed with
    large parameter spaces, something even the authors acknowledged. However, none
    of this discredits the author, as this is still an incredibly useful model that
    is relatively easy to implement and tune.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that this model is a GAM is also very nice for machine learning applications
    such as on mobile devices and other non-powerful devices as users can train the
    model and deploy the generated feature attribution functions instead of the full
    model for extremely fast and memory-light inference without any loss in accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Resources and References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NBM Code: [https://github.com/facebookresearch/nbm-spam](https://github.com/facebookresearch/nbm-spam)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'NBM Open Review: [https://openreview.net/forum?id=fpfDusqKZF](https://openreview.net/forum?id=fpfDusqKZF)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you are interested in Interpretable Machine Learning or Time Series Forecasting,
    consider following me: [https://medium.com/@upadhyan](https://medium.com/@upadhyan).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'See my other articles on interpretable machine learning: [https://medium.com/@upadhyan/list/interpretable-and-ethical-ai-f6ee1f0b476d](https://medium.com/@upadhyan/list/interpretable-and-ethical-ai-f6ee1f0b476d)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**References**'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Radenovic, F., Dubey, A., & Mahajan, D. (2022). Neural basis models for
    interpretability. *Advances in Neural Information Processing Systems*, *35*, 8414–8426.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Yin L., Rich C., Johannes G., and Giles H. (2013) Accurate intelligible
    models with pairwise interactions. In *Proceedings of the 19th ACM SIGKDD international
    conference on Knowledge discovery and data mining, 623–631\. 2013*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Agarwal, R., Melnick, L., Frosst, N., Zhang, X., Lengerich, B., Caruana,
    R., & Hinton, G. E. (2021). Neural additive models: Interpretable machine learning
    with neural nets. *Advances in neural information processing systems*, *34*, 4699–4711.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Chang, C.H., Caruana, R., & Goldenberg, A. (2022). NODE-GAM: Neural Generalized
    Additive Model for Interpretable Deep Learning. In *International Conference on
    Learning Representations*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Chen, T., & Guestrin, C. (2016, August). Xgboost: A scalable tree boosting
    system. In *Proceedings of the 22nd ACM SIGKDD international conference on knowledge
    discovery and data mining* (pp. 785–794).'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Popov, S., Morozov, S., & Babenko, A. (2019). Neural oblivious decision
    ensembles for deep learning on tabular data. *Eight International Conference on
    Learning Representations.*'
  prefs: []
  type: TYPE_NORMAL
