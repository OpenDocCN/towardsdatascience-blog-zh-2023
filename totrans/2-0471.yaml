- en: Can Transformers Learn to Strategize?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变换器能否学会制定策略？
- en: 原文：[https://towardsdatascience.com/can-transformers-learn-to-strategize-862770c996ea](https://towardsdatascience.com/can-transformers-learn-to-strategize-862770c996ea)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/can-transformers-learn-to-strategize-862770c996ea](https://towardsdatascience.com/can-transformers-learn-to-strategize-862770c996ea)
- en: TicTacGPT for playing simple board games
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TicTacGPT用于玩简单的棋盘游戏
- en: '[](https://charlieoneill.medium.com/?source=post_page-----862770c996ea--------------------------------)[![Charlie
    O''Neill](../Images/17aa117fc5787f93ff1f547b919786c8.png)](https://charlieoneill.medium.com/?source=post_page-----862770c996ea--------------------------------)[](https://towardsdatascience.com/?source=post_page-----862770c996ea--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----862770c996ea--------------------------------)
    [Charlie O''Neill](https://charlieoneill.medium.com/?source=post_page-----862770c996ea--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://charlieoneill.medium.com/?source=post_page-----862770c996ea--------------------------------)[![Charlie
    O''Neill](../Images/17aa117fc5787f93ff1f547b919786c8.png)](https://charlieoneill.medium.com/?source=post_page-----862770c996ea--------------------------------)[](https://towardsdatascience.com/?source=post_page-----862770c996ea--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----862770c996ea--------------------------------)
    [Charlie O''Neill](https://charlieoneill.medium.com/?source=post_page-----862770c996ea--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----862770c996ea--------------------------------)
    ·27 min read·Sep 8, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----862770c996ea--------------------------------)
    ·27分钟阅读·2023年9月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/105981c00690836c0dd2d1ea74863f8b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/105981c00690836c0dd2d1ea74863f8b.png)'
- en: Photo by [Jon Tyson](https://unsplash.com/@jontyson?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Jon Tyson](https://unsplash.com/@jontyson?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Although most board games appeal to the use of convolutional neural nets or
    other geometrically inspired architectures, the fact that we can actually represent
    board states as strings begs the question of whether transformers can naturally
    be applied to the game. Here, we’ll see if we can answer this in the context of
    a simple game of tic-tac-toe. Whilst this may not seem very practical (almost
    everyone knows that there is a closed-form Nash equilibrium strategy in this game
    that is very simple to learn), it is a useful testbed for our questions. The reason
    is that the game is simple enough that we can easily train a transformer to play
    it, but complex enough that it is not immediately obvious what the best strategy
    is.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数棋盘游戏倾向于使用卷积神经网络或其他几何灵感的架构，但我们实际能够将棋盘状态表示为字符串，这就引出了一个问题，即变换器是否可以自然地应用于游戏。在这里，我们将看看是否可以在简单的井字游戏的背景下回答这个问题。虽然这看起来可能不太实际（几乎每个人都知道这个游戏中存在一个简单的闭式纳什均衡策略），但它是我们问题的一个有用的测试平台。原因在于游戏足够简单，我们可以轻松训练一个变换器来玩它，但又足够复杂，不容易立刻看出最佳策略是什么。
- en: Implementing the game
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现游戏
- en: We’ll start by implementing a `TicTacToe` class. This is fairly straightforward.
    We want to be able to represent the board as a string of 9 characters, one for
    each square. We’ll use `X` for the first player, `O` for the second player, and
    `-` for an empty square. We’ll also keep track of whose turn it is to play next,
    and whether the game is over or not. We’ll also keep track of the winner, if there
    is one. Finally, we’ll include a nice method to print the board so we don’t have
    to stare at strings when debugging.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始实现一个`TicTacToe`类。这相当简单。我们希望能够将棋盘表示为9个字符的字符串，每个字符代表一个方格。我们将使用`X`表示第一个玩家，`O`表示第二个玩家，`-`表示空方格。我们还会跟踪轮到谁进行下一步，游戏是否结束。如果有获胜者，我们也会记录下来。最后，我们将包含一个打印棋盘的方法，以便在调试时不必盯着字符串看。
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Creating our training data
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建我们的训练数据
- en: We want our transformer to take in a given board state and output a move, that
    move being an integer from 0 to 8 representing the square it wants to place its
    piece in. To do so, we’ll create a dataset of board states and moves. We’ll do
    this by simulating all the possible winning positions of our player, and then
    iterating over all the combinations of games that could have got us there. This
    means the transformer will learn what constitutes a good move in any given board
    state.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们的变换器能够输入一个给定的棋盘状态，并输出一个走法，该走法是一个从 0 到 8 的整数，表示它希望将棋子放在的位置。为此，我们将创建一个棋盘状态和走法的数据集。我们将通过模拟我们玩家的所有可能获胜位置，然后遍历所有可能使我们达到该位置的游戏组合来做到这一点。这意味着变换器将学习在任何给定的棋盘状态下什么是一个好的走法。
- en: To implement this, the `simulate_all_games` function generates both the training
    and validation data. Specifically, the function simulates all possible Tic-Tac-Toe
    games, exploring every permutation of moves for both players (‘X’ and ‘O’). This
    exhaustive simulation ensures that the model is trained on a comprehensive dataset,
    encompassing all conceivable game scenarios. During each simulated game, the function
    records not just the winning or drawing outcomes but also the sequence of board
    states and the moves that led to those outcomes. These board states and moves
    are then transformed into numerical representations, suitable for training our
    transformer. This ensures that the model learns not just to win, but to output
    an appropriate winning move from any given board state. You can think of this
    as akin to the ability of a regular language transformer to output an appropriate
    token given any length of context, from just one token (i.e. our starting board
    state) to the EOS token (i.e. our winning move yielding the final board state).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，`simulate_all_games` 函数生成了训练和验证数据。具体来说，该函数模拟了所有可能的井字棋游戏，探索了两个玩家（‘X’
    和 ‘O’）的每一种走法排列。这种详尽的模拟确保了模型在一个全面的数据集上进行训练，涵盖了所有可能的游戏情景。在每个模拟的游戏中，函数记录了不仅是获胜或平局的结果，还记录了棋盘状态的序列以及导致这些结果的走法。这些棋盘状态和走法随后被转换为数字表示，适用于训练我们的变换器。这确保了模型不仅学习如何获胜，还能够从任何给定的棋盘状态中输出一个合适的获胜走法。你可以将其视为类似于常规语言变换器在给定任何长度的上下文时输出一个合适的标记，从一个标记（即我们的起始棋盘状态）到
    EOS 标记（即我们的获胜走法生成最终棋盘状态）。
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This gives us about 650,000 tensors to train on. These tensors look roughly
    correct, but it’s a bit hard to tell without seeing the board visually. Let’s
    recycle our `print_board` function to see what the some random board states, and
    the next board state given the move, look like:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们大约 650,000 个张量用于训练。这些张量看起来大致正确，但没有可视化的棋盘很难判断。让我们重用我们的 `print_board` 函数来查看一些随机的棋盘状态，以及给定走法后的下一个棋盘状态是什么样的：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This seems plausible, but I’ve noticed that some games have a winning move available,
    and yet the simulation makes a different move (that still ends up in a win). This
    occurred in the example above. Let’s change the `simulate_all_games` function
    to stop searching once we get to at least one potential winning move.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎是合理的，但我注意到有些游戏有一个可用的获胜走法，但模拟却做出了不同的走法（仍然以胜利告终）。在上面的示例中出现了这种情况。让我们将 `simulate_all_games`
    函数更改为在找到至少一个潜在获胜走法时停止搜索。
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, let’s see how many moves we have to train on:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看我们需要训练多少步：
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: About 150,000 examples. This seems like a reasonable start.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 大约 150,000 个示例。这看起来是一个合理的开始。
- en: Transformer architecture with multi-head attention
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多头注意力的变换器架构
- en: Attention is a mechanism whereby the model can learn to focus on certain parts
    of the input sequence when making predictions. The transformer architecture uses
    multi-head self-attention, which means that the model learns to attend to different
    parts of the input sequence in different ways. This is useful because it allows
    the model to learn different relationships between the input and output sequences.
    For example, it might learn to attend to the first token in the input sequence
    when predicting the first token in the output sequence, but attend to the last
    token in the input sequence when predicting the second token in the output sequence.
    This is a powerful mechanism that allows the model to learn complex relationships
    between the input and output sequences.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力是一种机制，使模型能够在进行预测时专注于输入序列的某些部分。变压器架构使用多头自注意力，这意味着模型学习以不同的方式关注输入序列的不同部分。这很有用，因为它允许模型学习输入序列和输出序列之间的不同关系。例如，当预测输出序列中的第一个词时，它可能会学习关注输入序列中的第一个标记，但在预测输出序列中的第二个词时，则关注输入序列中的最后一个标记。这是一种强大的机制，可以使模型学习输入序列和输出序列之间的复杂关系。
- en: 'But how does this actually work? From the original *Attention is all you need*
    paper, attention defined over a query matrix Q, a key matrix K, and a value matrix
    V is defined as:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 但这到底是怎么工作的呢？从原始的 *Attention is all you need* 论文中，定义在查询矩阵 Q、键矩阵 K 和值矩阵 V 上的注意力定义为：
- en: '![](../Images/f580f3530c711f7a05af38349a4eee13.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f580f3530c711f7a05af38349a4eee13.png)'
- en: 'where we divide by sqrt{d_k} to ensure an appropriate variance in the softmax.
    Let’s break down what’s actually happening here. Suppose we have an input of dimension
    (B,T,C) where B is the batch-size, T is the sequence length, and C is the number
    of channels. We can think of this as a batch of B sequences of length T, each
    with C channels:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们除以 `sqrt{d_k}` 以确保 softmax 的方差适当。让我们分解一下实际发生了什么。假设我们有一个维度为 (B,T,C) 的输入，其中
    B 是批次大小，T 是序列长度，C 是通道数。我们可以把它看作是一个包含 B 个长度为 T 的序列的批次，每个序列有 C 个通道：
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Then, to implement a single head of self-attention, we need to create the query,
    the key, and the value. In reality, these are linear layers with a certain `head_size`,
    which is just how wide we want the linear layer to be. We don’t include a bias
    term because we don’t want to learn a bias term for the attention.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了实现一个自注意力头，我们需要创建查询、键和值。实际上，这些是具有一定 `head_size` 的线性层，`head_size` 就是我们希望线性层的宽度。我们不包括偏置项，因为我们不想为注意力学习一个偏置项。
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: However, if you break down what we’re actually doing, we’re predicting the next
    word in a sequence of words. Since we don’t want to cheat and use parts of the
    sequence we haven’t seen yet (since we can’t do this during generation), we need
    to mask out the parts of the sequence we haven’t seen yet. We do this by creating
    a mask of shape (T,T) where T is the sequence length, and then setting all the
    values in the upper triangle to negative infinity. This ensures that the softmax
    will be 0 for all the masked values, and so the model will not attend to them.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果你拆解我们实际在做的事情，我们是在预测一个单词序列中的下一个词。由于我们不想作弊并使用我们尚未看到的序列部分（因为在生成过程中我们不能这样做），我们需要屏蔽掉尚未看到的序列部分。我们通过创建一个形状为
    (T,T) 的掩码来实现，其中 T 是序列长度，然后将上三角的所有值设置为负无穷。这确保了 softmax 对所有掩码值为 0，因此模型不会关注这些值。
- en: Finally, we multiply the attention weights by the value matrix to get the output
    of the attention layer. This is the output of a single head of self-attention.
    We can then repeat this process for as many heads as we want, and then concatenate
    the outputs of each head to get the final output of the multi-head self-attention
    layer.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将注意力权重与值矩阵相乘，以获得注意力层的输出。这是自注意力的单个头的输出。然后，我们可以根据需要重复此过程多次，然后将每个头的输出拼接在一起，以获得多头自注意力层的最终输出。
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: And that’s a single head of self-attention. To create multiple heads, we simply
    repeat this process multiple times, and then concatenate the outputs of each head
    to get the final output of the multi-head self-attention layer. We also add residual
    connections in order to improve our ability to optimise this relatively deep model.
    For a full walkthrough of similar code and the mechanisms behind decoder-only
    transformers, I highly recommend Andrej Karpathy’s [nanoGPT lecture](https://www.youtube.com/watch?v=kCc8FmEb1nY).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是自注意力的一个头。为了创建多个头，我们只需多次重复这个过程，然后将每个头的输出连接起来以获得多头自注意力层的最终输出。我们还添加了残差连接，以提高我们优化这个相对深层模型的能力。对于类似代码的完整演示以及解码器仅变换器背后的机制，我强烈推荐
    Andrej Karpathy 的[nanoGPT 讲座](https://www.youtube.com/watch?v=kCc8FmEb1nY)。
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To make sure our architecture works as intended, let’s try passing a single
    batch through.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们的架构按预期工作，让我们尝试传递一个单一的批次。
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'A good step when doing this initial forward pass is to test whether the loss
    is about equivalent to what we’d expect for random inputs. Since we have 9-dimensional
    logits and we’re using cross-entropy loss, which is equivalent to the negative
    log-likelihood of the correct class, we’d expect the loss to be about:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行这个初始前向传播时，一个好的步骤是测试损失是否大致等于我们对随机输入的期望。由于我们有 9 维 logits，并且我们使用的交叉熵损失等于正确类别的负对数似然，我们期望损失大致为：
- en: '![](../Images/efc854c3d72c8944f3e5dd24a38a3842.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efc854c3d72c8944f3e5dd24a38a3842.png)'
- en: Training the model
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Using quite a small transformer (about 25,000 parameters), we achieve the following
    loss (note that I’m using a small amount of weight decay and dropout):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相当小的变换器（约 25,000 个参数），我们实现了以下损失（请注意，我使用了少量的权重衰减和丢弃）：
- en: '[PRE10]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: That doesn’t seem very good. Let’s plot it to see what’s going on.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎不太好。让我们绘制图表看看发生了什么。
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/80b03da5de9c17e9e1ce1206541f3705.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80b03da5de9c17e9e1ce1206541f3705.png)'
- en: Initial losses for our transformer trained on the tic-tac-toe data (image by
    author).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的变换器在井字游戏数据上训练的初始损失（图片由作者提供）。
- en: 'We can play the transformer using this code to see if it’s any good:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个`code`来测试变换器的效果：
- en: '[PRE12]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Okay, so I easily beat the model. Something is going wrong.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我轻松打败了模型。某些事情出错了。
- en: Improving the transformer
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进变换器
- en: 'So at the moment, the transformer can’t even reliably learn the simple winning
    moves from any given position. I can think of a few reasons for this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 所以目前，变换器甚至无法从任何给定位置可靠地学习简单的胜利动作。我能想到几个原因：
- en: The transformer is only training on the winning moves, so maybe it’s not learning
    how to play when I play a decent strategy that means no winning moves are available.
    Theoretically, to counter this we should allow it to train on moves when the game
    is pre-destined to be a draw.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器仅在胜利的动作上进行训练，因此当我使用一个良好的策略（即没有胜利的动作可用）时，可能无法学习如何游戏。理论上，为了应对这一点，我们应该允许它在游戏注定为平局时进行训练。
- en: The transformer is overparametrised. Trying to get a few hundred thousand neurons
    to coordinate a simple strategy may take a long time to train and rely on grokking
    and other phenomenon to get into a generalisable part of the optimisation landscape.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器的参数过多。试图让几十万个神经元协调一个简单的策略可能需要很长时间来训练，并依赖于**grokking**和其他现象才能进入优化景观的可泛化部分。
- en: The transformer is underparametrised. Maybe it needs more neurons to learn a
    good strategy. Saying this, I very much doubt that scaling up to a few million
    neurons will help if a few hundred thousand doesn’t do the job.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器的参数过少。也许它需要更多的神经元来学习一个好的策略。这样说来，我非常怀疑如果几百万个神经元都无济于事，那几十万个神经元能否解决问题。
- en: State-space analysis
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 状态空间分析
- en: Before I move on, I want to analyse the number of neurons theoretically required
    to encode the entire winning strategy for tic-tac-toe, we need to consider the
    state space of the game and the complexity of the decision-making process.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我想从理论上分析编码井字游戏中完整胜利策略所需的神经元数量，我们需要考虑游戏的状态空间和决策过程的复杂性。
- en: 'In tic-tac-toe, the game board is a 3 x 3 grid, and each cell can be in one
    of three states: ‘X’, ‘O’, or empty (‘-’). So, the total number of possible board
    states can be calculated as 3⁹ = 19683\. However, not all of these are valid states
    in a real game; some of them are unreachable or illegal (e.g., a board with all
    ‘X’s). The number of legal states is actually around 5,478, but for the purpose
    of this analysis, we’ll consider the upper limit, i.e., 3⁹.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在井字棋中，游戏棋盘是一个3 x 3的网格，每个单元可以处于三种状态之一：‘X’，‘O’，或空（‘-’）。因此，总的可能棋盘状态数量可以计算为3⁹ =
    19683。然而，并非所有这些状态在实际游戏中都是有效的；其中一些是不可达的或非法的（例如，所有单元都是‘X’的棋盘）。合法状态的数量实际上大约为5478，但为了分析的目的，我们将考虑上限，即3⁹。
- en: 'Each state requires a decision: where to place the next ‘X’ (since we’re considering
    a winning strategy for ‘X’). There are 9 possible positions, but the number of
    legal moves is often less than 9, depending on how many cells are already occupied.
    A neural network would have to map each possible board state to a correct move.
    One way to achieve this mapping is through a fully connected layer that takes
    the board state as input and outputs a probability distribution over the 9 possible
    moves. The input layer would have 3x3=9 neurons (one for each cell), and the output
    layer would have 9 neurons (one for each possible move). The hidden layers in
    between would perform the complex task of learning the winning strategy.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 每个状态都需要一个决策：在哪里放置下一个‘X’（因为我们考虑的是‘X’的获胜策略）。有9个可能的位置，但合法的移动数量通常少于9，这取决于已经被占据的单元格数量。一个神经网络需要将每个可能的棋盘状态映射到一个正确的移动。实现这种映射的一种方法是通过一个完全连接的层，该层将棋盘状态作为输入，并输出9个可能移动的概率分布。输入层将有3x3=9个神经元（每个单元一个），输出层将有9个神经元（每个可能移动一个）。中间的隐藏层将执行学习获胜策略的复杂任务。
- en: 'Given that the input layer has 9 neurons and the output layer has 9 neurons,
    the number of neurons in the hidden layers is what we’re interested in. Theoretically,
    we could use a single hidden layer with 3⁹ neurons to map each possible state
    to a winning move. This would be an upper bound and is likely far more than what
    would actually be necessary due to the presence of unreachable/illegal states
    and the inherent symmetries in tic-tac-toe that reduce the actual number of unique
    states. So, in this upper-bound scenario, the total number of neurons would be:
    9 + 19683 + 9 = 19701\. This is a theoretical upper bound and the actual number
    could be much lower due to the factors mentioned earlier. Let’s try implementing
    a simple feed-forward vanilla neural network to see how it does on our task. Rather
    than having one layer with thousands of neurons, we’ll use three hidden layers.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到输入层有9个神经元和输出层有9个神经元，我们关注的是隐藏层中的神经元数量。从理论上讲，我们可以使用一个具有3⁹个神经元的隐藏层将每个可能的状态映射到一个获胜的移动。这将是一个上限，可能远远超过实际需要的数量，因为存在不可达/非法状态以及井字棋的固有对称性会减少实际的唯一状态数量。因此，在这个上限情况下，总的神经元数量将是：9
    + 19683 + 9 = 19701。这是一个理论上的上限，实际数量可能由于前述因素而低得多。让我们尝试实现一个简单的前馈普通神经网络，看看它在我们的任务中的表现。我们将使用三个隐藏层，而不是一个具有数千个神经元的层。
- en: '[PRE13]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And let’s see what the loss looks like:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看损失情况如何：
- en: '![](../Images/bd544cb64db73e78b64a4b96bef6e7a5.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd544cb64db73e78b64a4b96bef6e7a5.png)'
- en: Training loss for a vanilla feed-forward neural network (image by author).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于普通前馈神经网络的训练损失（作者提供的图片）。
- en: Clearly, we’re saturating performance. There’s something about the task and
    how we’ve set it up that prevents the models from learning an appropriate strategy.
    To shift tack a bit, I’m going to try giving the model training data which just
    consists of the optimal strategy.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们正在饱和性能。任务和我们设置的方法有些问题，阻止了模型学习适当的策略。为了改变一下，我打算尝试给模型提供只包含最优策略的训练数据。
- en: Optimal strategy training data
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最优策略训练数据
- en: '[Newell and Simon’s 1972 tic-tac-toe program](https://en.wikipedia.org/wiki/Tic-tac-toe)
    outlines the perfect strategy (to win or at least draw) if we choose the first
    available move from the following preference over moves:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[纽厄尔和西蒙1972年的井字棋程序](https://en.wikipedia.org/wiki/Tic-tac-toe)概述了完美策略（以赢得比赛或至少平局），如果我们从以下移动偏好中选择第一个可用的移动：'
- en: '*Win*: If you have two in a row, play the third to get three in a row.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*获胜*：如果你有两个连成一行，玩第三个以完成三连。'
- en: '*Block*: If the opponent has two in a row, play the third to block them.'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*阻挡*：如果对手有两个连成一行，玩第三个以阻挡他们。'
- en: '*Fork*: Create an opportunity where you can win in two ways.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*分叉*：创造一个你可以通过两种方式获胜的机会。'
- en: '*Block Opponent’s Fork:* we either create two in a row to force a defence (if
    this doesn’t result in a fork for them), or we block their potential fork.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*阻止对手的叉子*：我们可以创建两个连续的棋子以迫使对手防守（如果这样做不会给他们造成叉子），或者阻止他们的潜在叉子。'
- en: '*Centre*: Play the centre.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*中心*：占据中心位置。'
- en: '*Opposite Corner*: If the opponent is in the corner, play the opposite corner.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*对角角落*：如果对手在一个角落里，选择对角的角落。'
- en: '*Empty Corner*: Play an empty corner.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*空角落*：选择一个空的角落。'
- en: '*Empty Side*: Play an empty side.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*空侧*：选择一个空的边侧。'
- en: Let’s rewrite our data generator to obtain all possible moves according to this
    strategy. We’ll also simulate all games for both possible players going first.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重写数据生成器，以根据这一策略获得所有可能的走法。我们还将模拟两种可能的先手玩家的所有游戏。
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Let’s retrain our model on our new training data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在新的训练数据上重新训练我们的模型。
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/4549f446f53811e26528645b2d42a5ce.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4549f446f53811e26528645b2d42a5ce.png)'
- en: New loss with the optimal training data (image by author).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最优训练数据的新损失（作者提供的图像）。
- en: So much better! We’ve not only learned the strategy but it generalises to the
    validation dataset (we get close to 0 loss on both train and val). I’m guessing
    this is due to the inherent symmetries in board states, and the transformer has
    learned a form of modular arithmetic over the board string to be invariant to
    board states.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们不仅学会了策略，而且它在验证数据集上也能泛化（我们在训练和验证集上接近0损失）。我猜这是由于棋盘状态的固有对称性，变换器已经学会了一种对棋盘状态不变的模块算术形式。
- en: 'Let’s try playing the new transformer:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试新的变换器：
- en: '[PRE16]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: It beat me! Used a fantastic fork. It seems our transformer has learned the
    optimal strategy.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 它打败了我！使用了一个绝妙的叉子。看起来我们的变换器已经学会了最优策略。
- en: Conclusion
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: I think the main takeaway here is that transformers are more than capable of
    learning optimal strategies for games. Whilst a vanilla neural net could likely
    have learned the same optimal strategy, the dynamic nature of attention means
    that it might be able to attend to longer sequences representing the game over
    time. These ideas naturally give rise to applying transformers in reinforcement
    learning settings. For instance, [Janner et al. (2021)](https://arxiv.org/pdf/2106.02039.pdf)
    used transformers to model distributions over trajectories and a beam search as
    the planning algorithm.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这里的主要收获是变换器完全能够学习游戏的最优策略。虽然一个普通的神经网络可能也能学到相同的最优策略，但注意力机制的动态特性意味着它可能能够处理表示游戏随时间演变的更长序列。这些想法自然地促使我们在强化学习设置中应用变换器。例如，[Janner等（2021）](https://arxiv.org/pdf/2106.02039.pdf)
    使用变换器来建模轨迹分布，并使用束搜索作为规划算法。
- en: Another thing I learned from this project was that the process of the human
    hand-holding the transformer through the optimal strategy obviously doesn’t scale,
    particularly as games get more complex. For instance, Go isn’t a “solved” game,
    so we couldn’t do what we did above and provide it with the optimal strategy to
    train on. Instead, we’d have to use something like self-play to select good sequences
    to then train the transformer on. I hope to experiment with ideas like this in
    future.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个项目中我学到的另一件事是，人工手动引导变换器通过最优策略的过程显然无法扩展，尤其是当游戏变得更加复杂时。例如，围棋并不是一个“已解决”的游戏，因此我们不能像上面那样提供最优策略进行训练。相反，我们必须使用类似自我对弈的方法来选择好的棋局序列，然后再用这些序列训练变换器。我希望未来能尝试这些想法。
- en: Finally, there’s a ripe field to be explored by looking at hierarchies of prediction
    and planning in transformers. As [Ba et al. (2016)](https://arxiv.org/pdf/1610.06258.pdf)
    noted, deep learning has typically focused on methods to maintain temporary state
    in activation dynamics, whereas our brain seems to operate by regulating temporary
    state informatio via medium-term synaptic plasticity. In other words, there should
    be some form of working memory/prediction that operates between the next-token
    level and the long-term states in LSTMs for instance. The autoregressive decoder-only
    transformer architecture that forms the backbone for GPT-3 is a powerful model
    that can generate realistic-looking text simply by predicting one token into the
    future. However, if we anthropomorphise intelligence, we know that quick intuitive
    look-ahead (by one token) does not a genius make. In my mind, it would thus be
    interesting to try and provide a model with multiple hierarchies of prediction,
    where the model can learn to predict multiple tokens into the future. This would
    be akin to the human ability to plan ahead, and would likely be a useful skill
    for transformers to learn.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过查看变换器中的预测和规划层级，仍有一个待开发的领域。正如[Ba 等人 (2016)](https://arxiv.org/pdf/1610.06258.pdf)
    所指出的，深度学习通常关注于在激活动态中保持临时状态的方法，而我们的大脑似乎是通过中期突触可塑性来调节临时状态信息。换句话说，应该有某种形式的工作记忆/预测，在下一个标记级别和例如
    LSTM 中的长期状态之间操作。作为 GPT-3 支撑骨架的自回归解码器仅变换器架构是一个强大的模型，可以通过预测一个标记的未来生成逼真的文本。然而，如果我们将智能拟人化，我们知道快速的直观预测（仅预测一个标记）并不能使人变成天才。因此，在我看来，尝试为模型提供多个预测层级，让模型学会预测多个未来标记将会很有趣。这将类似于人类的提前规划能力，并且可能是变换器学习的一个有用技能。
- en: 'What does it mean to look ahead? There a few different avenues to explore here:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 预测未来意味着什么？这里有几个不同的探索方向：
- en: '**Look ahead in time**: Can the transformer learn to predict not the next token
    in the sequence but the token two, three, or more steps ahead? Is predicting two
    tokens at once just equivalent to predicting one token, then predicting the next
    token, or is there some strategic benefit to predicting two tokens at a time?
    Does it force the transformer to *think* for longer?'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间上的预测**：变换器能否学会预测下一个序列中的标记，而不是预测两个、三个或更多步之后的标记？预测两个标记一次是否等同于先预测一个标记，再预测下一个标记，还是说一次预测两个标记有某种战略上的好处？这是否迫使变换器*思考*更长时间？'
- en: '**Look ahead in space**: There is evidence that humans produce coarse-grained
    semantic representations of tasks they are performing, and then use hierarchical
    modules to “fill in the gaps” in these coarse-grained representations. You might
    think of this as writing an essay: first you create a skeleton of points, then
    you fill in the thesis sentences for each paragraph, and finally you flesh out
    the details. Is it possible that transformers can learn to do the same thing?'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**空间上的预测**：有证据表明，人类会产生粗略的任务语义表示，然后使用层级模块来“填补”这些粗略表示中的空白。你可以将其想象为写一篇文章：首先你创建一个要点的骨架，然后为每一段填入论点句子，最后完善细节。变换器是否可能学会做同样的事情？'
- en: Regardless of whether these are useful questions or not, I hope this post gave
    some clarity and insight into exactly how we can reshape a problem to be suitable
    for a transformer. Happy fine-tuning!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 无论这些问题是否有用，我希望这篇文章能为我们如何将问题重塑为适合变换器的问题提供一些清晰的见解。祝调优愉快！
- en: References
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
    & Polosukhin, I. (2017). Attention is all you need. *Advances in neural information
    processing systems*, *30*.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
    & Polosukhin, I. (2017). 注意力机制是你所需的一切。*神经信息处理系统进展*, *30*。
- en: Janner, M., Li, Q., & Levine, S. (2021). Offline reinforcement learning as one
    big sequence modeling problem. *Advances in neural information processing systems*,
    *34*, 1273–1286.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Janner, M., Li, Q., & Levine, S. (2021). 离线强化学习作为一个大的序列建模问题。*神经信息处理系统进展*, *34*,
    1273–1286。
- en: Ba, J., Hinton, G. E., Mnih, V., Leibo, J. Z., & Ionescu, C. (2016). Using fast
    weights to attend to the recent past. *Advances in neural information processing
    systems*, *29*.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ba, J., Hinton, G. E., Mnih, V., Leibo, J. Z., & Ionescu, C. (2016). 使用快速权重关注最近的过去。*神经信息处理系统进展*,
    *29*。
- en: 'Andrej Karpathy. *Let’s build GPT: from scratch, in code, spelled out.* [https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5076s](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5076s)'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
