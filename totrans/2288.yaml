- en: Vector Representations for Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/vector-representations-for-machine-learning-5047c50aaeff](https://towardsdatascience.com/vector-representations-for-machine-learning-5047c50aaeff)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*How data scientists convert real-world objects in numerical representation
    for the development of machine learning models*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@theDrewDag?source=post_page-----5047c50aaeff--------------------------------)[![Andrea
    D''Agostino](../Images/58c7c218815f25278aae59cea44d8771.png)](https://medium.com/@theDrewDag?source=post_page-----5047c50aaeff--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5047c50aaeff--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5047c50aaeff--------------------------------)
    [Andrea D''Agostino](https://medium.com/@theDrewDag?source=post_page-----5047c50aaeff--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5047c50aaeff--------------------------------)
    ·8 min read·Apr 25, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f867bba38db6c28d77fe559a6a5983f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Sigmund](https://unsplash.com/@sigmund?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning engineers leverage numerical representations of the world to
    build and train predictive algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of supervised learning, these representations allow the computer
    to learn the relationship between them and the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s imagine a vector to just be a list of numbers
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This list is related to the target variable `y`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The machine learning model learns the relationship between the features and
    targets and outputs a prediction — in this case a classification in which one
    of the classes is identified with the number 1.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, **I will write about how vectors can be used to represent complex
    concepts in a number format.**
  prefs: []
  type: TYPE_NORMAL
- en: The rationale is that a machine learning model cannot learn from observations
    that are not provided to it in numerical format.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Text, images, sounds and other input observations must first be transformed
    into a numerical format suitable for learning.
  prefs: []
  type: TYPE_NORMAL
- en: There are various techniques for transforming a phenomenon into vectors, and
    these depend on the type of data we are working with
  prefs: []
  type: TYPE_NORMAL
- en: We will start by introducing the concept of **One-Hot Encoding**, a technique
    used to represent words as numerical vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will discuss the limitations of this technique and introduce the concept
    of **embeddings**, a technique that allows words, images, sounds, and more to
    be represented as smaller numerical vectors than the thousands of categories required
    with One-Hot Encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also mention the **TF-IDF and bag of words models**, which are fundamental
    in text vectorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we encode a phenomenon into a vector?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use text as an example to carry the conversation forward. The example
    is quite obvious because as we can guess, machine learning models cannot directly
    use text for their learning. We need to turn each character or word into a number
    first.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we want to create a numerical representation of words
  prefs: []
  type: TYPE_NORMAL
- en: King
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Queen
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prince
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Princess
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The simplest way to encode these words would be to assign each of them a number,
    sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e3fbf3f3e3dc07a2c6601ceb12ad199c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The words have been correctly transformed into numerical format, following the
    mapping
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: But there’s a problem. If we fed this data to any predictive model, **it would
    assign a higher mathematical value to the prince and princess, making them more
    important than the king and queen.**
  prefs: []
  type: TYPE_NORMAL
- en: Obviously this would provide wrong information to the model, which would learn
    wrong relationships. We need to make our numerical representation more precise.
  prefs: []
  type: TYPE_NORMAL
- en: One-Hot Encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To solve the numeric representation problem described above, the **One-Hot Encoding**
    technique can be used.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, each word would be represented by a numerical vector with a size
    equal to the total number of words to be represented. **The vector would have
    all values equal to zero, except one, which represents the specific word.**
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the case of the four words “King”, “Queen”, “Prince” and “Princess”,
    each word would be represented by an array of four elements, with the value “1”
    in the position corresponding to the word and “0” in all other positions.
  prefs: []
  type: TYPE_NORMAL
- en: This technique solves the problem of assigning a higher mathematical value to
    words that are no more important than others in number representation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/14d80dcb7daf76310e6533078e704018.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Now our model has a “balanced” vectorial representation for each word belonging
    to the dataset (which in this case consists of only 4 words).
  prefs: []
  type: TYPE_NORMAL
- en: But… **what if our vocabulary is made up of thousands or even millions of words**?
    Considering that there are about 270,000 words in the Italian dictionary, applying
    one-hot encoding would be problematic to say the least.
  prefs: []
  type: TYPE_NORMAL
- en: 'The computational resources to carry out this coding would be considerable
    and the final representation would be “only” balanced: *there is no information
    about the relationships between the words.*'
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To overcome the limitations of One-Hot Encoding, the technique called **embedding**
    can be used. This allows words to be represented as numerical vectors of controllable
    size compared to the thousands of categories needed with One-Hot Encoding.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to create a numerical representation of the words that takes into
    account **the semantic relationships between the words themselves.**
  prefs: []
  type: TYPE_NORMAL
- en: In practice, each word is represented as a vector of real numbers, where each
    dimension represents a different aspect of the meaning of the word.
  prefs: []
  type: TYPE_NORMAL
- en: 'Understanding embeddings is simple: related words should appear close together
    in vector space, while unrelated words should appear distant.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s try to create a graph where we capture some of the characteristics of
    the words mentioned before.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc64232e79d3eca25fc0f33f84e1d29a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: We see how close the words *prince* and *princess* are to each other, just like
    *king* and *queen*.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming that the gender variable can assume only two values, M and F (we use
    0 and 1), and that the age variable can assume only three values [Young, Middle-aged,
    Elderly] (we use 0, 1, 2), we see how embeddings can represent these relationships
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1bb9a55ed4d22c41bee362f007bcd25d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: This representation manages to capture the noble status of an individual by
    using the dimensions of gender and age.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving on the X axis we can observe how the two nobles are equidistant from
    a dimension that represents the gender difference (0: male, 1: female). Moving
    on the Y axis, however, we can observe how age is represented by the distance
    of the embedding from the Y axis.'
  prefs: []
  type: TYPE_NORMAL
- en: In this way, **word embeddings can be used as input to machine learning models,
    allowing complex concepts to be more accurately represented in a numerical format**.
  prefs: []
  type: TYPE_NORMAL
- en: In this example we have only two dimensions. In fact, neural networks are trained
    with the specific task of finding these representations on several dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: To put that into perspective, **models like GPT-3 use more than 12,000 dimensions.**
  prefs: []
  type: TYPE_NORMAL
- en: A milestone in the industry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Embeddings can be used not only for words, but also to represent images, sounds,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: The use of vector representations is critical in today’s machine learning. The
    various innovations and technologies in the field of deep learning cascade from
    the concept of vectorization.
  prefs: []
  type: TYPE_NORMAL
- en: Models like GPT-3.5 are born by crossing vector representations, well-studied
    optimization algorithms and large amounts of computational resources.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There is theoretically no limit to this approach.
  prefs: []
  type: TYPE_NORMAL
- en: '**More data → Higher quality vectors → Models that will use those vectors for
    better training.**'
  prefs: []
  type: TYPE_NORMAL
- en: Limits of embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although embeddings are a very useful technique for representing complex concepts
    in numerical format, they also have limitations.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, it is important to underline that the embeddings are built starting
    from the training data, **and therefore can be influenced by any bias present
    in the data.**
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, the quality of the embeddings depends on the quality of the training
    data. If the training data is not representative of the domain in which the model
    will be used, the embeddings may not be able to capture all semantic relationships
    between concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Also, embeddings can require a lot of memory to store, especially if the number
    of dimensions is large. This can be especially problematic for machine learning
    models that need to run on resource-constrained devices, such as mobile devices.
  prefs: []
  type: TYPE_NORMAL
- en: Other ways of representing text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since text is the most common data format around us (just think of the huge
    amount of textual data on the internet), some text vectorization techniques are
    common and well known.
  prefs: []
  type: TYPE_NORMAL
- en: One of these is the **TF-IDF transformation** which is a text vectorization
    technique that assigns a weight to each word based on its frequency within a document
    and its overall frequency within the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: This way, words that appear frequently within a document but rarely within the
    corpus will have more weight than those that appear frequently everywhere. This
    technique is widely used in the field of Natural Language Processing for text
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: I invite the interested reader to learn more about the TF-IDF model by reading
    the following article
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/mlearning-ai/text-clustering-with-tf-idf-in-python-c94cd26a31e7?source=post_page-----5047c50aaeff--------------------------------)
    [## Text Clustering with TF-IDF in Python'
  prefs: []
  type: TYPE_NORMAL
- en: Explanation of a simple pipeline for text clustering. Full example and code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/mlearning-ai/text-clustering-with-tf-idf-in-python-c94cd26a31e7?source=post_page-----5047c50aaeff--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF is based on the **bag of words model which represents a document as an
    unordered set of words, ignoring sentence structure and word order.**
  prefs: []
  type: TYPE_NORMAL
- en: In this way, the bag of words can be used to represent any document as an array
    of numeric values, where each value represents the frequency of a word within
    the document. Of course, there is no adequate representation of the relationship
    between words, which is provided by embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post we have seen how vectors can be used to represent complex concepts
    in a numeric format.
  prefs: []
  type: TYPE_NORMAL
- en: It is important for a data scientist to think in terms of vectorization. Questions
    like
  prefs: []
  type: TYPE_NORMAL
- en: '*how can i convert this stimulus into a number?*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*how is this data interpreted by the neural network?*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*How can I improve this representation?*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: are critical, and the team that can adequately answer these questions will create
    better systems.
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists see the world in terms of vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '**If you want to support my content creation activity, feel free to follow
    my referral link below and join Medium’s membership program**. I will receive
    a portion of your investment and you’ll be able to access Medium’s plethora of
    articles on data science and more in a seamless way.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@theDrewDag/membership?source=post_page-----5047c50aaeff--------------------------------)
    [## Join Medium with my referral link - Andrea D''Agostino'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Andrea D'Agostino (and thousands of other writers on Medium).
    Your membership fee directly…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@theDrewDag/membership?source=post_page-----5047c50aaeff--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Recommended Reads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the interested, here are a list of books that I recommended for each ML-related
    topic. There are ESSENTIAL books in my opinion and have greatly impacted my professional
    career.
  prefs: []
  type: TYPE_NORMAL
- en: '*Disclaimer: these are Amazon affiliate links. I will receive a small commission
    from Amazon for referring you these items. Your experience won’t change and you
    won’t be charged more, but it will help me scale my business and produce even
    more content around AI.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Intro to ML:** [*Confident Data Skills: Master the Fundamentals of Working
    with Data and Supercharge Your Career*](https://amzn.to/3ZzKTz6)by Kirill Eremenko'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sklearn / PyTorch:** [*Machine Learning with PyTorch and Scikit-Learn: Develop
    machine learning and deep learning models with Python*](https://amzn.to/3Gcavve)
    by Sebastian Raschka'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sklearn / TensorFlow:** [*Hands-On Machine Learning with Scikit-Learn, Keras,
    and TensorFlow*](https://amzn.to/433F4Nm) by Aurelien Géron'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NLP:** [*Text as Data: A New Framework for Machine Learning and the Social
    Sciences*](https://amzn.to/3zvH43j)by Justin Grimmer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Viz:** [*Storytelling with Data: A Data Visualization Guide for Business
    Professionals*](https://amzn.to/3HUtGtB) by Cole Knaflic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Useful Links (written by me)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Learn how to perform a top-tier Exploratory Data Analysis in Python:** [*Exploratory
    Data Analysis in Python — A Step-by-Step Process*](/exploratory-data-analysis-in-python-a-step-by-step-process-d0dfa6bf94ee)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learn the basics of PyTorch:** [*Introduction to PyTorch: from training loop
    to prediction*](/introduction-to-pytorch-from-training-loop-to-prediction-a70372764432)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learn the basics of TensorFlow:** [*Get started with TensorFlow 2.0 — Introduction
    to deep learning*](https://medium.com/towards-data-science/a-comprehensive-introduction-to-tensorflows-sequential-api-and-model-for-deep-learning-c5e31aee49fa)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perform text clustering with TF-IDF in Python:** [*Text Clustering with TF-IDF
    in Python*](https://medium.com/mlearning-ai/text-clustering-with-tf-idf-in-python-c94cd26a31e7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
