- en: Convenient Reinforcement Learning With Stable-Baselines3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/convenient-reinforcement-learning-with-stable-baselines3-dccf466b7585](https://towardsdatascience.com/convenient-reinforcement-learning-with-stable-baselines3-dccf466b7585)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Reinforcement Learning](https://medium.com/tag/reinforcement-learning)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement learning without the boilerplate code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dr-robert-kuebler.medium.com/?source=post_page-----dccf466b7585--------------------------------)[![Dr.
    Robert Kübler](../Images/3b8d8b88f76c0c43d9c305e3885e7ab9.png)](https://dr-robert-kuebler.medium.com/?source=post_page-----dccf466b7585--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dccf466b7585--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dccf466b7585--------------------------------)
    [Dr. Robert Kübler](https://dr-robert-kuebler.medium.com/?source=post_page-----dccf466b7585--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dccf466b7585--------------------------------)
    ·10 min read·Dec 9, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3a8b5fb8229e66fec0ddd0fbefcafc4.png)'
  prefs: []
  type: TYPE_IMG
- en: Created by the author with Leonardo Ai**.**
  prefs: []
  type: TYPE_NORMAL
- en: In my previous articles about reinforcement learning, I have shown you how to
    implement (deep) Q-learning using nothing but a bit of numpy and TensorFlow. While
    this was an important step towards understanding how these algorithms work under
    the hood, the code tended to get lengthy — and I even merely implemented one of
    the most basic versions of deep Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/hands-on-deep-q-learning-9073040ce841?source=post_page-----dccf466b7585--------------------------------)
    [## Hands-On Deep Q-Learning'
  prefs: []
  type: TYPE_NORMAL
- en: Level up your agent to win more difficult games!
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/hands-on-deep-q-learning-9073040ce841?source=post_page-----dccf466b7585--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Given the explanations in this article, understanding the code should be quite
    straightforward. However, if we *really* want to get things done, we should rely
    on well-documented, maintained, and optimized libraries. Just as we don’t want
    to implement linear regression over and over again, we don’t want to do the same
    for reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will show you the reinforcement library [**Stable-Baselines3**](https://stable-baselines3.readthedocs.io/)
    which is as easy to use as scikit-learn. Instead of training models to predict
    labels, though, we get trained agents that can navigate well in their environment.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the code and my trained best models on [my Github](https://github.com/Garve/towards_data_science/tree/09b9a701d660776d557e1662e5703a7c5c4d4ab5/Convenient%20Reinforcement%20Learning%20With%20Stable-Baselines3).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Short Recap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are not sure what (deep) Q-learning is about, I suggest reading my previous
    articles. On a high level, we want to train an agent that interacts with its environment
    with the goal of maximizing its total reward. The most important part of reinforcement
    learning is to find a good reward function for the agent.
  prefs: []
  type: TYPE_NORMAL
- en: I usually imagine a character in a game searching its way to get the highest
    score, e.g., Mario running from start to finish without dying and — in the best
    case — as fast as possible.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04732bea1083f1b6d190fab271aa1034.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: In order to do so, in Q-learning, we learn **quality values** for each pair
    (*s*, *a*) where *s* is a state and *a* is an action the agent can take. Q(*s*,
    *a*) is the **expected discounted future reward** when doing action *a* in state
    *s*. As an example, being in the state *s* = “standing in front of a cliff” and
    doing the action *a* = “do one step forward” should have a very low value of Q(*s*,
    *a*).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can turn this Q-function into a **policy** then; imagine a magical compass
    that tells us what to do in any given state. The method is simple: if we are in
    state *s*, just compute Q(*s*, *a*) for all possible actions *a* and pick the
    action with the highest value. Done!'
  prefs: []
  type: TYPE_NORMAL
- en: In my other articles, we have seen how to get these Q-values using a table or
    neural networks. Now, we want to sit back and just enjoy the simplicity of Stable-Baselines3\.
    We deserved it.
  prefs: []
  type: TYPE_NORMAL
- en: Enter Stable-Baselines3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already developed agents that play a variety of games, such as *Frozen
    Lake* (get the present without falling into the lakes), *Taxi* (pick up a customer
    and bring them to the hotel)*,* or *Cart Pole* (balance a stick).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/31380e3184ae94afd0209dc0faba3a41.png)![](../Images/814de8803609a1cc697593aa8992137a.png)![](../Images/81820681c69eae49c10ed02d32aa0160.png)'
  prefs: []
  type: TYPE_IMG
- en: Frozen Lake, Taxi, and *Cart Pole*. Images by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could recreate agents that master these games, but let us start with something
    different: **Mountain Car**!'
  prefs: []
  type: TYPE_NORMAL
- en: The Mountain Car game
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this game, we steer a car that should go up a mountain. The actions we can
    take are going left, going right, or doing nothing. Our training goal is to go
    from here…
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ef03509bd58be1694f32620cd861898b.png)'
  prefs: []
  type: TYPE_IMG
- en: A greedy agent that only wants to move directly to the top of the hill. Image
    by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '… to here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7bf56674fef1402a4da70a093832ebde.png)'
  prefs: []
  type: TYPE_IMG
- en: A smart agent that gains momentum first to reach its goal. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training the model is extremely simple with Stable-Baselines3\. Just look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The magic is about finding good hyperparameters for the `config` , but this
    is something we as machine learning practitioners have to figure out. Or let dedicated
    hyperparameter optimization tools handle it.
  prefs: []
  type: TYPE_NORMAL
- en: Behind the curtains
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We know most things that happen in the `.learn` method already. If you check
    out the source code, you will see many old friends from my other articles. For
    example, if you [look here](https://github.com/DLR-RM/stable-baselines3/blob/373166d6ac30561c378bdd46e8dba4ef0760f996/stable_baselines3/dqn/dqn.py#L184),
    you can find code like
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We have a replay memory, there is the Q-value update step (1-step TD target).
    This shouldn’t look too scary anymore. A noteworthy difference is that the library
    uses **double Q-learning**, something that I did not implement. The idea is easy
    though: instead of having one Q-value neural network, we have two.'
  prefs: []
  type: TYPE_NORMAL
- en: In the source code above, `self.q_net` (called the main network) is the one
    that gets normally trained. On the other hand, `self.q_net_target` (called the
    target network) is used for producing the labels to train our main network. Every
    few epochs, the target network gets set to the main network, so you can see the
    target network as a lagged version of the main network.
  prefs: []
  type: TYPE_NORMAL
- en: If both are the same, we use our network (there is only one) to produce labels,
    and then update the network’s weights. But this in turn changes the targets again,
    so essentially we try to learn moving targets — the **training might be unstable**.
    Double Q-learning with its two-network approach fixes this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Callbacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training takes a long time, and it is always sad to lose progress because your
    program crashes. So Stable-Baselines3 offers some nice callbacks to save your
    progress over time. I recommend using `EvalCallback` and `CheckpointCallback`
    .
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You can just pass these callbacks here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `EvalCallback` also saves some nice performance numbers that you can plot.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eaff3b8b72e5f35cdc0e65ffd8242752.png)'
  prefs: []
  type: TYPE_IMG
- en: The mean reward (over 10 runs) over time. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see how for about 40,000 timesteps, the mode did not learn much. A
    reward of -200 indicates that the model did not reach the top — an episode ends
    after 200 timesteps. Then, suddenly the learning took off until the agent consistently
    reached the top of the mountain. You can plot it like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Playing Atari Games
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Okay, cool, so we beat some kindergarten games. Time to tackle something more
    challenging: **Atari games**! For the young folks: Atari was a leader in the video
    game market back in the 80s. They also invented the game [Pong](https://en.wikipedia.org/wiki/Pong),
    our beloved game consisting of two sticks playing tennis.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b7831e7037401aa4a8bf26049d50396.png)'
  prefs: []
  type: TYPE_IMG
- en: An Atari 2600 that I used to play with as a child. Public domain image by [Evan
    Amos](https://commons.wikimedia.org/wiki/User:Evan-Amos).
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of their games are still simple, but at least they feel like real games
    that challenge you already. To spice things up, we will only use the **raw** **screen
    pixels** to train our agent! No more internal game states such as coordinates,
    velocities, or angles of objects. The machine has to learn how to play the game
    in the same way as a human: by looking at the screen and figuring out what to
    do.'
  prefs: []
  type: TYPE_NORMAL
- en: Breakout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As an example, let us use [Breakout](https://en.wikipedia.org/wiki/Breakout_(video_game)),
    a game where we have to destroy blocks using a ball. The ball jumps around, bouncing
    off from the blocks, but also the ship we control. We can steer a “spaceship”
    left and right to keep the ball in play. But let us just look at a game scene
    with our agent in the main role:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69aa76c33103e3e74eb033a4e64fdc82.png)'
  prefs: []
  type: TYPE_IMG
- en: Our deep Q-learning agent playing Breakout. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'This agent was trained for about 3,000,000 frames using GPUs and training on
    4 environments at the same time on GCP (8 vCPUs, 30 GB RAM, NVIDIA T4 x 4). It
    took about **3 hours** to train it. Besides using a big machine, I boosted the
    performance using the `AtariWrapper` that scales down the images to a size of
    84 x 84 pixels and grayscales them since colors are not important in this game.
    We also use a **convolutional neural network** as opposed to a simple feed-forward
    neural network to achieve better results in less time. Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Note:** Jupyterlab usually has problems with multiprocessing, so you might
    have to paste this code into a .py file and run it from the command line. Also
    notice that I feed the network not only single images of the game, but **four
    consecutive images** with the line'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This way, the agent can learn the **direction and speed** of the ball in addition
    to its position. Otherwise, how could it tell what is going on?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/43662c0b45d8caeb42b952bd5381eb3d.png)'
  prefs: []
  type: TYPE_IMG
- en: Where is the ball going? Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The 4 is just a hyperparameter, feel free to try other values as well. This
    little trick makes it possible for the agent to learn how to play this game without
    any internal game information.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, the performance of the agent is quite jumpy over the episodes. Still,
    you can clearly see how the trend goes up over time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cb34eacf37fa536e6c6b57ade64c3e14.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Space Invaders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another classic — that was a response to Breakout — is the game Space Invaders.
    In case you don’t know: you shoot aliens and try not to get shot. By just replacing
    a single line in the code again, we can train an agent that can beat one wave
    of the game before 3,000,000 steps of training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1b263ee60d5443339cc73d272a7cb213.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, I cherry-picked this run. Usually, my agent dies faster, but it is
    still quite good:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f3cc1bf7f2f5b650a192d338b9c8102.png)![](../Images/8a90e55c6d7da58efd63060226aa43b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: You can train it via
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Of course, you can now retrain agents to play all of the Atari games.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article, we have seen a way to train agents without much boilerplate
    code. At the moment, I would consider Stable-Baselines3 the scikit-learn of reinforcement
    learning: you define the model, configure it a bit, and `.learn` the game. It
    cannot get much simpler than this.'
  prefs: []
  type: TYPE_NORMAL
- en: Still, I advocate for **understanding what is going on behind the curtains.**
    Otherwise, you might be lost when things don’t work out of the box. The same goes
    for classical machine learning, or any other algorithm. First, at least understand
    the fundamentals, and then treat yourself to a nice library!
  prefs: []
  type: TYPE_NORMAL
- en: As a last point, if you check out the [library’s documentation](https://stable-baselines3.readthedocs.io/),
    you will see that it supports more learning algorithms, such as
  prefs: []
  type: TYPE_NORMAL
- en: '[Asynchronous Advantage Actor Critic](https://arxiv.org/abs/1602.01783) (A2C)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Proximal Policy Optimization](https://arxiv.org/abs/1707.06347) (PPO), or'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Deterministic Policy Gradient](https://spinningup.openai.com/en/latest/algorithms/ddpg.html)
    (DDPG).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to have a nice alternative to deep Q-learning, from what I have
    seen PPO seems to be popular. Play around with all of them to see if you can find
    something more performant for your learning problem! But make sure to look up
    how these methods work as well — maybe in one of my future articles!
  prefs: []
  type: TYPE_NORMAL
- en: I hope that you learned something new, interesting, and valuable today. Thanks
    for reading!
  prefs: []
  type: TYPE_NORMAL
- en: '*If you have any questions, write me on* [*LinkedIn*](https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/)*!*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And if you want to dive deeper into the world of algorithms, give my new publication
    All About Algorithms a try! I’m still searching for writers!
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://allaboutalgorithms.com/?source=post_page-----dccf466b7585--------------------------------)
    [## All About Algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: From intuitive explanations to in-depth analysis, algorithms come to life with
    examples, code, and awesome…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: allaboutalgorithms.com](https://allaboutalgorithms.com/?source=post_page-----dccf466b7585--------------------------------)
  prefs: []
  type: TYPE_NORMAL
