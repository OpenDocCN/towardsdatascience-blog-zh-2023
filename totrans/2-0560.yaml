- en: Convenient Reinforcement Learning With Stable-Baselines3
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Stable-Baselines3进行便捷的强化学习
- en: 原文：[https://towardsdatascience.com/convenient-reinforcement-learning-with-stable-baselines3-dccf466b7585](https://towardsdatascience.com/convenient-reinforcement-learning-with-stable-baselines3-dccf466b7585)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/convenient-reinforcement-learning-with-stable-baselines3-dccf466b7585](https://towardsdatascience.com/convenient-reinforcement-learning-with-stable-baselines3-dccf466b7585)
- en: '[Reinforcement Learning](https://medium.com/tag/reinforcement-learning)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[强化学习](https://medium.com/tag/reinforcement-learning)'
- en: Reinforcement learning without the boilerplate code
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无需冗余代码的强化学习
- en: '[](https://dr-robert-kuebler.medium.com/?source=post_page-----dccf466b7585--------------------------------)[![Dr.
    Robert Kübler](../Images/3b8d8b88f76c0c43d9c305e3885e7ab9.png)](https://dr-robert-kuebler.medium.com/?source=post_page-----dccf466b7585--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dccf466b7585--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dccf466b7585--------------------------------)
    [Dr. Robert Kübler](https://dr-robert-kuebler.medium.com/?source=post_page-----dccf466b7585--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dr-robert-kuebler.medium.com/?source=post_page-----dccf466b7585--------------------------------)[![Dr.
    Robert Kübler](../Images/3b8d8b88f76c0c43d9c305e3885e7ab9.png)](https://dr-robert-kuebler.medium.com/?source=post_page-----dccf466b7585--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dccf466b7585--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dccf466b7585--------------------------------)
    [Dr. Robert Kübler](https://dr-robert-kuebler.medium.com/?source=post_page-----dccf466b7585--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dccf466b7585--------------------------------)
    ·10 min read·Dec 9, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dccf466b7585--------------------------------)
    ·10分钟阅读·2023年12月9日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b3a8b5fb8229e66fec0ddd0fbefcafc4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3a8b5fb8229e66fec0ddd0fbefcafc4.png)'
- en: Created by the author with Leonardo Ai**.**
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用**Leonardo Ai**创作。
- en: In my previous articles about reinforcement learning, I have shown you how to
    implement (deep) Q-learning using nothing but a bit of numpy and TensorFlow. While
    this was an important step towards understanding how these algorithms work under
    the hood, the code tended to get lengthy — and I even merely implemented one of
    the most basic versions of deep Q-learning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前的强化学习文章中，我展示了如何仅使用少量的numpy和TensorFlow来实现（深度）Q学习。虽然这是理解这些算法工作原理的一个重要步骤，但代码往往变得冗长——我甚至仅实现了深度Q学习的最基本版本之一。
- en: '[](/hands-on-deep-q-learning-9073040ce841?source=post_page-----dccf466b7585--------------------------------)
    [## Hands-On Deep Q-Learning'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/hands-on-deep-q-learning-9073040ce841?source=post_page-----dccf466b7585--------------------------------)
    [## 动手深度Q学习'
- en: Level up your agent to win more difficult games!
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提升你的智能体，赢得更具挑战性的游戏！
- en: towardsdatascience.com](/hands-on-deep-q-learning-9073040ce841?source=post_page-----dccf466b7585--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/hands-on-deep-q-learning-9073040ce841?source=post_page-----dccf466b7585--------------------------------)
- en: Given the explanations in this article, understanding the code should be quite
    straightforward. However, if we *really* want to get things done, we should rely
    on well-documented, maintained, and optimized libraries. Just as we don’t want
    to implement linear regression over and over again, we don’t want to do the same
    for reinforcement learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 根据本文的解释，理解代码应该是相当直接的。然而，如果我们*真正*想要完成任务，我们应该依赖于文档齐全、维护良好且经过优化的库。正如我们不希望一遍遍地实现线性回归一样，我们也不希望对强化学习做同样的事。
- en: In this article, I will show you the reinforcement library [**Stable-Baselines3**](https://stable-baselines3.readthedocs.io/)
    which is as easy to use as scikit-learn. Instead of training models to predict
    labels, though, we get trained agents that can navigate well in their environment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将向你展示一个与scikit-learn一样易于使用的强化学习库[**Stable-Baselines3**](https://stable-baselines3.readthedocs.io/)。不过，我们得到的是经过训练的智能体，能够在环境中良好地导航，而不是训练模型来预测标签。
- en: Here is the code and my trained best models on [my Github](https://github.com/Garve/towards_data_science/tree/09b9a701d660776d557e1662e5703a7c5c4d4ab5/Convenient%20Reinforcement%20Learning%20With%20Stable-Baselines3).
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这里是代码和我训练的最佳模型，放在了[我的Github](https://github.com/Garve/towards_data_science/tree/09b9a701d660776d557e1662e5703a7c5c4d4ab5/Convenient%20Reinforcement%20Learning%20With%20Stable-Baselines3)上。
- en: A Short Recap
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简短回顾
- en: If you are not sure what (deep) Q-learning is about, I suggest reading my previous
    articles. On a high level, we want to train an agent that interacts with its environment
    with the goal of maximizing its total reward. The most important part of reinforcement
    learning is to find a good reward function for the agent.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不确定 (深度) Q 学习是什么，我建议阅读我之前的文章。从高层次来看，我们希望训练一个代理，该代理与其环境互动，目标是最大化其总奖励。强化学习中最重要的部分是为代理找到一个良好的奖励函数。
- en: I usually imagine a character in a game searching its way to get the highest
    score, e.g., Mario running from start to finish without dying and — in the best
    case — as fast as possible.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我通常想象一个游戏中的角色寻找方法以获得最高分，例如，马里奥从开始跑到结束而不死 — 最好是尽可能快。
- en: '![](../Images/04732bea1083f1b6d190fab271aa1034.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04732bea1083f1b6d190fab271aa1034.png)'
- en: Image by the author.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者提供。
- en: In order to do so, in Q-learning, we learn **quality values** for each pair
    (*s*, *a*) where *s* is a state and *a* is an action the agent can take. Q(*s*,
    *a*) is the **expected discounted future reward** when doing action *a* in state
    *s*. As an example, being in the state *s* = “standing in front of a cliff” and
    doing the action *a* = “do one step forward” should have a very low value of Q(*s*,
    *a*).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，在 Q 学习中，我们为每对 (*s*, *a*) 学习 **质量值**，其中 *s* 是状态，*a* 是代理可以采取的动作。Q(*s*, *a*)
    是在状态 *s* 下执行动作 *a* 时的 **期望折扣未来奖励**。举例来说，处于状态 *s* = “站在悬崖前面”并执行动作 *a* = “向前走一步”应该有一个非常低的
    Q(*s*, *a*) 值。
- en: 'We can turn this Q-function into a **policy** then; imagine a magical compass
    that tells us what to do in any given state. The method is simple: if we are in
    state *s*, just compute Q(*s*, *a*) for all possible actions *a* and pick the
    action with the highest value. Done!'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个 Q 函数转化为 **策略**；想象一个神奇的指南针，告诉我们在任何给定状态下该做什么。方法很简单：如果我们处于状态 *s*，只需计算所有可能动作
    *a* 的 Q(*s*, *a*) 并选择值最高的动作。完成！
- en: In my other articles, we have seen how to get these Q-values using a table or
    neural networks. Now, we want to sit back and just enjoy the simplicity of Stable-Baselines3\.
    We deserved it.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在我其他的文章中，我们已经看到如何使用表格或神经网络来获取这些 Q 值。现在，我们只想放松一下，享受 Stable-Baselines3 的简便性。我们值得拥有。
- en: Enter Stable-Baselines3
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进入 Stable-Baselines3
- en: We have already developed agents that play a variety of games, such as *Frozen
    Lake* (get the present without falling into the lakes), *Taxi* (pick up a customer
    and bring them to the hotel)*,* or *Cart Pole* (balance a stick).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经开发了能够玩各种游戏的代理，如 *冰冻湖*（在不掉入湖中的情况下获得礼物）、*出租车*（接客人并送到酒店）或 *摆杆*（平衡一根杆子）。
- en: '![](../Images/31380e3184ae94afd0209dc0faba3a41.png)![](../Images/814de8803609a1cc697593aa8992137a.png)![](../Images/81820681c69eae49c10ed02d32aa0160.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31380e3184ae94afd0209dc0faba3a41.png)![](../Images/814de8803609a1cc697593aa8992137a.png)![](../Images/81820681c69eae49c10ed02d32aa0160.png)'
- en: Frozen Lake, Taxi, and *Cart Pole*. Images by the author.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 冰冻湖、出租车和*摆杆*。图像由作者提供。
- en: 'We could recreate agents that master these games, but let us start with something
    different: **Mountain Car**!'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重新创建掌握这些游戏的代理，但让我们从不同的事情开始：**山地车**！
- en: The Mountain Car game
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 山地车游戏
- en: In this game, we steer a car that should go up a mountain. The actions we can
    take are going left, going right, or doing nothing. Our training goal is to go
    from here…
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个游戏中，我们操控一辆车，车需要上山。我们可以采取的动作是向左走、向右走或什么都不做。我们的训练目标是从这里…
- en: '![](../Images/ef03509bd58be1694f32620cd861898b.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef03509bd58be1694f32620cd861898b.png)'
- en: A greedy agent that only wants to move directly to the top of the hill. Image
    by the author.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一个贪婪的代理，只想直接移动到山顶。图像由作者提供。
- en: '… to here:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: … 到这里：
- en: '![](../Images/7bf56674fef1402a4da70a093832ebde.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7bf56674fef1402a4da70a093832ebde.png)'
- en: A smart agent that gains momentum first to reach its goal. Image by the author.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一个智能代理首先获得动力以达到目标。图像由作者提供。
- en: 'Training the model is extremely simple with Stable-Baselines3\. Just look:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Stable-Baselines3 训练模型极其简单。请看：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The magic is about finding good hyperparameters for the `config` , but this
    is something we as machine learning practitioners have to figure out. Or let dedicated
    hyperparameter optimization tools handle it.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 魔法在于找到 `config` 的良好超参数，但这是我们作为机器学习从业者必须弄清楚的事情。或者让专门的超参数优化工具来处理它。
- en: Behind the curtains
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 幕后的故事
- en: We know most things that happen in the `.learn` method already. If you check
    out the source code, you will see many old friends from my other articles. For
    example, if you [look here](https://github.com/DLR-RM/stable-baselines3/blob/373166d6ac30561c378bdd46e8dba4ef0760f996/stable_baselines3/dqn/dqn.py#L184),
    you can find code like
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道 `.learn` 方法中大部分发生了什么。如果您查看源代码，您会看到我其他文章中的许多老朋友。例如，如果您 [查看这里](https://github.com/DLR-RM/stable-baselines3/blob/373166d6ac30561c378bdd46e8dba4ef0760f996/stable_baselines3/dqn/dqn.py#L184)，您会找到类似的代码
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We have a replay memory, there is the Q-value update step (1-step TD target).
    This shouldn’t look too scary anymore. A noteworthy difference is that the library
    uses **double Q-learning**, something that I did not implement. The idea is easy
    though: instead of having one Q-value neural network, we have two.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个重放记忆，还有 Q 值更新步骤（1 步 TD 目标）。这应该不再显得过于可怕。值得注意的是，库使用了**双重 Q 学习**，这是我没有实现的。其思路很简单：我们不是使用一个
    Q 值神经网络，而是使用两个。
- en: In the source code above, `self.q_net` (called the main network) is the one
    that gets normally trained. On the other hand, `self.q_net_target` (called the
    target network) is used for producing the labels to train our main network. Every
    few epochs, the target network gets set to the main network, so you can see the
    target network as a lagged version of the main network.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的源代码中，`self.q_net`（称为主网络）是通常被训练的网络。另一方面，`self.q_net_target`（称为目标网络）用于生成训练主网络的标签。每隔几个训练周期，目标网络会被设置为主网络，因此您可以将目标网络视为主网络的滞后版本。
- en: If both are the same, we use our network (there is only one) to produce labels,
    and then update the network’s weights. But this in turn changes the targets again,
    so essentially we try to learn moving targets — the **training might be unstable**.
    Double Q-learning with its two-network approach fixes this problem.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两者相同，我们使用我们的网络（只有一个）来生成标签，然后更新网络的权重。但这反过来又改变了目标，因此本质上我们尝试学习移动目标——**训练可能不稳定**。双重
    Q 学习通过其双网络方法解决了这个问题。
- en: Callbacks
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回调函数
- en: Training takes a long time, and it is always sad to lose progress because your
    program crashes. So Stable-Baselines3 offers some nice callbacks to save your
    progress over time. I recommend using `EvalCallback` and `CheckpointCallback`
    .
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 训练耗时较长，如果您的程序崩溃，丢失进度总是令人沮丧。因此，Stable-Baselines3 提供了一些很好的回调函数来保存您的进度。我建议使用 `EvalCallback`
    和 `CheckpointCallback`。
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can just pass these callbacks here:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将这些回调函数直接传递到这里：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `EvalCallback` also saves some nice performance numbers that you can plot.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`EvalCallback` 还保存了一些不错的性能数据，您可以绘制这些数据。'
- en: '![](../Images/eaff3b8b72e5f35cdc0e65ffd8242752.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eaff3b8b72e5f35cdc0e65ffd8242752.png)'
- en: The mean reward (over 10 runs) over time. Image by the author.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 平均奖励（10 次运行的平均）随时间变化。图片由作者提供。
- en: 'You can see how for about 40,000 timesteps, the mode did not learn much. A
    reward of -200 indicates that the model did not reach the top — an episode ends
    after 200 timesteps. Then, suddenly the learning took off until the agent consistently
    reached the top of the mountain. You can plot it like this:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，在大约 40,000 个时间步长内，模型学习得不多。-200 的奖励表示模型没有达到顶部——一个回合在 200 个时间步长后结束。然后，学习突然起飞，直到代理程序稳定地达到了山顶。您可以像这样绘制：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Playing Atari Games
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩 Atari 游戏
- en: 'Okay, cool, so we beat some kindergarten games. Time to tackle something more
    challenging: **Atari games**! For the young folks: Atari was a leader in the video
    game market back in the 80s. They also invented the game [Pong](https://en.wikipedia.org/wiki/Pong),
    our beloved game consisting of two sticks playing tennis.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，很酷，我们打败了一些幼儿园游戏。是时候挑战更具挑战性的内容了：**Atari 游戏**！对年轻人来说：Atari 是 80 年代视频游戏市场的领导者。他们还发明了游戏
    [Pong](https://en.wikipedia.org/wiki/Pong)，我们的心爱游戏，由两个杆子打乒乓球组成。
- en: '![](../Images/9b7831e7037401aa4a8bf26049d50396.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b7831e7037401aa4a8bf26049d50396.png)'
- en: An Atari 2600 that I used to play with as a child. Public domain image by [Evan
    Amos](https://commons.wikimedia.org/wiki/User:Evan-Amos).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一台我小时候常玩的 Atari 2600。公有领域图片由 [Evan Amos](https://commons.wikimedia.org/wiki/User:Evan-Amos)
    提供。
- en: 'Most of their games are still simple, but at least they feel like real games
    that challenge you already. To spice things up, we will only use the **raw** **screen
    pixels** to train our agent! No more internal game states such as coordinates,
    velocities, or angles of objects. The machine has to learn how to play the game
    in the same way as a human: by looking at the screen and figuring out what to
    do.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的大多数游戏仍然很简单，但至少它们已经能挑战你了。为了增加趣味，我们将只使用**原始** **屏幕像素**来训练我们的智能体！不再使用内部游戏状态，比如坐标、速度或物体角度。机器必须像人类一样通过观察屏幕并找出该做什么来学习如何玩游戏。
- en: Breakout
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 《Breakout》
- en: 'As an example, let us use [Breakout](https://en.wikipedia.org/wiki/Breakout_(video_game)),
    a game where we have to destroy blocks using a ball. The ball jumps around, bouncing
    off from the blocks, but also the ship we control. We can steer a “spaceship”
    left and right to keep the ball in play. But let us just look at a game scene
    with our agent in the main role:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个例子，让我们使用[Breakout](https://en.wikipedia.org/wiki/Breakout_(video_game))，这是一个需要用球破坏方块的游戏。球会跳动，从方块上反弹，也会反弹到我们控制的飞船上。我们可以左右控制“飞船”以保持球在游戏中。不过，让我们看看我们的智能体在主要角色中的游戏场景：
- en: '![](../Images/69aa76c33103e3e74eb033a4e64fdc82.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69aa76c33103e3e74eb033a4e64fdc82.png)'
- en: Our deep Q-learning agent playing Breakout. Image by the author.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的深度 Q 学习智能体在玩《Breakout》。图片由作者提供。
- en: 'This agent was trained for about 3,000,000 frames using GPUs and training on
    4 environments at the same time on GCP (8 vCPUs, 30 GB RAM, NVIDIA T4 x 4). It
    took about **3 hours** to train it. Besides using a big machine, I boosted the
    performance using the `AtariWrapper` that scales down the images to a size of
    84 x 84 pixels and grayscales them since colors are not important in this game.
    We also use a **convolutional neural network** as opposed to a simple feed-forward
    neural network to achieve better results in less time. Here is the code:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这个智能体在大约3,000,000帧的训练中使用了GPU，并在GCP（8 vCPUs，30 GB RAM，NVIDIA T4 x 4）上同时训练了4个环境。训练大约花费了**3小时**。除了使用大型机器外，我还利用了`AtariWrapper`来提升性能，该工具将图像缩放到84
    x 84像素并转为灰度，因为在这个游戏中颜色并不重要。我们还使用了**卷积神经网络**，而不是简单的前馈神经网络，以便在更短的时间内获得更好的结果。以下是代码：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Note:** Jupyterlab usually has problems with multiprocessing, so you might
    have to paste this code into a .py file and run it from the command line. Also
    notice that I feed the network not only single images of the game, but **four
    consecutive images** with the line'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** Jupyterlab 通常在多进程处理上存在问题，因此你可能需要将此代码粘贴到 .py 文件中并从命令行运行。还要注意，我将网络的输入不仅仅是单个游戏图像，而是**四张连续图像**，并且使用了以下代码：'
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This way, the agent can learn the **direction and speed** of the ball in addition
    to its position. Otherwise, how could it tell what is going on?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，智能体不仅可以学习球的**方向和速度**，还可以学习它的位置。否则，它怎么能知道发生了什么呢？
- en: '![](../Images/43662c0b45d8caeb42b952bd5381eb3d.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43662c0b45d8caeb42b952bd5381eb3d.png)'
- en: Where is the ball going? Image by the author.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 球会去哪里？图片由作者提供。
- en: The 4 is just a hyperparameter, feel free to try other values as well. This
    little trick makes it possible for the agent to learn how to play this game without
    any internal game information.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 4 只是一个超参数，你可以尝试其他值。这个小技巧使得智能体能够在没有任何内部游戏信息的情况下学习如何玩这个游戏。
- en: 'As usual, the performance of the agent is quite jumpy over the episodes. Still,
    you can clearly see how the trend goes up over time:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，智能体的性能在各个回合中有些波动。不过，你可以清楚地看到趋势是逐渐上升的：
- en: '![](../Images/cb34eacf37fa536e6c6b57ade64c3e14.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb34eacf37fa536e6c6b57ade64c3e14.png)'
- en: Image by the author.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: Space Invaders
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 《太空侵略者》
- en: 'Another classic — that was a response to Breakout — is the game Space Invaders.
    In case you don’t know: you shoot aliens and try not to get shot. By just replacing
    a single line in the code again, we can train an agent that can beat one wave
    of the game before 3,000,000 steps of training:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个经典游戏是《太空侵略者》，这是对《Breakout》的回应。如果你不知道：你需要射击外星人并尽量避免被击中。只需在代码中替换一行，我们就可以训练一个智能体，使其在3,000,000步训练之前击败一波敌人：
- en: '![](../Images/1b263ee60d5443339cc73d272a7cb213.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b263ee60d5443339cc73d272a7cb213.png)'
- en: Image by the author.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: 'However, I cherry-picked this run. Usually, my agent dies faster, but it is
    still quite good:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我挑选了这次运行的结果。通常情况下，我的智能体会死得更快，但仍然相当不错：
- en: '![](../Images/4f3cc1bf7f2f5b650a192d338b9c8102.png)![](../Images/8a90e55c6d7da58efd63060226aa43b4.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f3cc1bf7f2f5b650a192d338b9c8102.png)![](../Images/8a90e55c6d7da58efd63060226aa43b4.png)'
- en: Image by the author.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: You can train it via
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下方式进行训练：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Of course, you can now retrain agents to play all of the Atari games.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你现在可以重新训练代理来玩所有的 Atari 游戏。
- en: Conclusion
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this article, we have seen a way to train agents without much boilerplate
    code. At the moment, I would consider Stable-Baselines3 the scikit-learn of reinforcement
    learning: you define the model, configure it a bit, and `.learn` the game. It
    cannot get much simpler than this.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们看到了一种训练代理而不需要太多样板代码的方法。目前，我认为 Stable-Baselines3 就像是强化学习领域的 scikit-learn：你定义模型，稍微配置一下，然后
    `.learn` 游戏。没有比这更简单的了。
- en: Still, I advocate for **understanding what is going on behind the curtains.**
    Otherwise, you might be lost when things don’t work out of the box. The same goes
    for classical machine learning, or any other algorithm. First, at least understand
    the fundamentals, and then treat yourself to a nice library!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我还是提倡**了解幕后发生了什么。** 否则，当事情不能按预期工作时，你可能会感到迷茫。经典的机器学习或任何其他算法也是如此。首先，至少要理解基础知识，然后再享受使用一个不错的库！
- en: As a last point, if you check out the [library’s documentation](https://stable-baselines3.readthedocs.io/),
    you will see that it supports more learning algorithms, such as
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你查看[库的文档](https://stable-baselines3.readthedocs.io/)，你会发现它支持更多的学习算法，例如
- en: '[Asynchronous Advantage Actor Critic](https://arxiv.org/abs/1602.01783) (A2C)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[异步优势演员评论](https://arxiv.org/abs/1602.01783)（A2C）'
- en: '[Proximal Policy Optimization](https://arxiv.org/abs/1707.06347) (PPO), or'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[近端策略优化](https://arxiv.org/abs/1707.06347)（PPO），或者'
- en: '[Deep Deterministic Policy Gradient](https://spinningup.openai.com/en/latest/algorithms/ddpg.html)
    (DDPG).'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度确定性策略梯度](https://spinningup.openai.com/en/latest/algorithms/ddpg.html)（DDPG）。'
- en: If you want to have a nice alternative to deep Q-learning, from what I have
    seen PPO seems to be popular. Play around with all of them to see if you can find
    something more performant for your learning problem! But make sure to look up
    how these methods work as well — maybe in one of my future articles!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要一个深度 Q 学习的不错替代方案，从我所看到的，PPO 似乎很受欢迎。尝试一下所有这些算法，看看是否能找到更适合你的学习问题的方案！但也要确保了解这些方法的工作原理——也许在我未来的文章中！
- en: I hope that you learned something new, interesting, and valuable today. Thanks
    for reading!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你今天学到了一些新、趣味和有价值的东西。谢谢阅读！
- en: '*If you have any questions, write me on* [*LinkedIn*](https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/)*!*'
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*如果你有任何问题，可以在* [*LinkedIn*](https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/)*上联系我！*'
- en: And if you want to dive deeper into the world of algorithms, give my new publication
    All About Algorithms a try! I’m still searching for writers!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入了解算法的世界，不妨试试我的新出版物《All About Algorithms》！我还在寻找作者！
- en: '[](https://allaboutalgorithms.com/?source=post_page-----dccf466b7585--------------------------------)
    [## All About Algorithms'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[## All About Algorithms](https://allaboutalgorithms.com/?source=post_page-----dccf466b7585--------------------------------)'
- en: From intuitive explanations to in-depth analysis, algorithms come to life with
    examples, code, and awesome…
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从直观的解释到深入的分析，算法通过示例、代码和精彩的内容变得生动起来…
- en: allaboutalgorithms.com](https://allaboutalgorithms.com/?source=post_page-----dccf466b7585--------------------------------)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[allaboutalgorithms.com](https://allaboutalgorithms.com/?source=post_page-----dccf466b7585--------------------------------)'
