- en: Unleashing the ChatGPT Tokenizer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解锁 ChatGPT 分词器
- en: 原文：[https://towardsdatascience.com/chatgpt-tokenizer-chatgpt3-chatgpt4-artificial-intelligence-python-ai-27f78906ea54](https://towardsdatascience.com/chatgpt-tokenizer-chatgpt3-chatgpt4-artificial-intelligence-python-ai-27f78906ea54)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/chatgpt-tokenizer-chatgpt3-chatgpt4-artificial-intelligence-python-ai-27f78906ea54](https://towardsdatascience.com/chatgpt-tokenizer-chatgpt3-chatgpt4-artificial-intelligence-python-ai-27f78906ea54)
- en: Hands-On! How ChatGPT Manages Tokens?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践操作！ChatGPT 如何管理令牌？
- en: '[](https://medium.com/@andvalenzuela?source=post_page-----27f78906ea54--------------------------------)[![Andrea
    Valenzuela](../Images/ddfc1534af92413fd91076f826cc49b6.png)](https://medium.com/@andvalenzuela?source=post_page-----27f78906ea54--------------------------------)[](https://towardsdatascience.com/?source=post_page-----27f78906ea54--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----27f78906ea54--------------------------------)
    [Andrea Valenzuela](https://medium.com/@andvalenzuela?source=post_page-----27f78906ea54--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@andvalenzuela?source=post_page-----27f78906ea54--------------------------------)[![Andrea
    Valenzuela](../Images/ddfc1534af92413fd91076f826cc49b6.png)](https://medium.com/@andvalenzuela?source=post_page-----27f78906ea54--------------------------------)[](https://towardsdatascience.com/?source=post_page-----27f78906ea54--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----27f78906ea54--------------------------------)
    [Andrea Valenzuela](https://medium.com/@andvalenzuela?source=post_page-----27f78906ea54--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----27f78906ea54--------------------------------)
    ·9 min read·Jul 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----27f78906ea54--------------------------------)
    ·阅读时间 9 分钟·2023年7月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d6d00dc92aaafbfe984040c4f28c2509.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6d00dc92aaafbfe984040c4f28c2509.png)'
- en: Self-made gif.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 自制 gif。
- en: '**Have you ever wondered which are the key components behind ChatGPT?**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**你是否曾经好奇 ChatGPT 背后的关键组件是什么？**'
- en: 'We all have been told the same: ChatGPT predicts the next word. But actually,
    there is a bit of a lie in this statement. **It does not predict the next word,
    ChatGPT predicts the next token**.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都被告知 ChatGPT 预测下一个单词。但实际上，这个说法有点误导。**它并不是预测下一个单词，ChatGPT 预测的是下一个令牌**。
- en: '*Token?* Yes, a token is the unit of text for Large Language Models (LLMs).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*令牌？* 是的，令牌是大型语言模型（LLMs）中的文本单元。'
- en: Indeed **one of the first steps that ChatGPT does when processing any prompt
    is splitting the user input into tokens**. And that is the job of the so-called
    **tokenizer**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，**ChatGPT 在处理任何提示时的第一个步骤之一就是将用户输入拆分成令牌**。这就是所谓的 **分词器** 的工作。
- en: In this article, we will uncover how the ChatGPT tokenizer works with hands-on
    practice with the original library used by OpenAI, the `tiktoken` library.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将揭示 ChatGPT 分词器如何使用 OpenAI 的原始库进行实践操作，即 `tiktoken` 库。
- en: '*TikTok-en… Funny enough :)*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*TikTok 风格... 有趣 :)*'
- en: Let’s dive deep and comprehend the actual steps performed by the tokenizer,
    and how its behavior really impacts the quality of the ChatGPT output.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解分词器实际执行的步骤，以及它的行为如何真正影响 ChatGPT 输出的质量。
- en: How the Tokenizer Works
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词器如何工作
- en: 'In the article [Mastering ChatGPT: Effective Summarization with LLMs](/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce)
    we already saw some of the mysteries behind the ChatGPT tokenizer, but let’s start
    from scratch.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在文章[掌握 ChatGPT：使用 LLMs 的有效总结](/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce)中，我们已经看到了
    ChatGPT 分词器背后的一些奥秘，但让我们从头开始。
- en: The tokenizer appears at the first step in the process of text generation. **It
    is responsible for breaking down the piece of text that we input to ChatGPT into
    individual elements**, the tokens, which are then processed by the language model
    to generate new text.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器在文本生成过程的第一步出现。**它负责将我们输入给 ChatGPT 的文本片段拆分为单个元素**，即令牌，这些令牌随后被语言模型处理以生成新的文本。
- en: When the tokenizer breaks down a piece of text into tokens, it does so based
    on a set of rules that are designed to identify the meaningful units of the target
    language.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当分词器将一段文本拆分为令牌时，它是根据一组规则进行的，这些规则旨在识别目标语言中的有意义的单元。
- en: 'For example, when the words that appear in a given sentence are fairly common
    words, chances are that each token corresponds to one word. But if we use a prompt
    with less frequently used words, like in the sentence *“Prompting as powerful
    developer tool”,* we might not get a one-to-one mapping. In this case, the word
    *prompting* is still not that common in the English language, so it is actually
    broken down to three tokens: *“‘prom”*, *“pt”*, and *“ing”* because those three
    are commonly occurring sequences of letters.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当给定句子中出现的词汇是相当常见的词时，每个标记很可能对应一个单词。但如果我们使用一个包含不常见词汇的提示，例如在句子*“Prompting as
    powerful developer tool”*中，我们可能无法获得一对一的映射。在这种情况下，词语*prompting*在英语中仍然不那么常见，因此它实际上被拆分成三个标记：*“‘prom”*、*“pt”*和*“ing”*，因为这三个序列是常见的字母组合。
- en: '*Let’s see another example!*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*让我们看另一个例子！*'
- en: 'Consider the following sentence: *“I want to eat a peanut butter sandwich”.*
    If the tokenizer is configured to split tokens based on spaces and punctuation,
    it may break this sentence down into the following tokens with a total word count
    of 8, equal to the token count.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下句子：*“I want to eat a peanut butter sandwich”*。如果分词器配置为基于空格和标点符号拆分标记，它可能会将此句子拆分为以下标记，总词数为8，与标记数相等。
- en: '![](../Images/372b106853250883ddbc4f64d78a113e.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/372b106853250883ddbc4f64d78a113e.png)'
- en: Self-made image.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 自制图像。
- en: However, if the tokenizer treats *“peanut butter”* as a compound word due to
    the fact that the components appear together quite often, it may break the sentence
    down into the following tokens, **with a total word count of 8, but a token count
    of 7**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果分词器将*“peanut butter”*视为一个复合词，因为这些组件经常一起出现，它可能会将句子拆分为以下标记，**总词数为8，但标记数为7**。
- en: '![](../Images/7e7d6a87f22c074d29ae976ca8071d29.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e7d6a87f22c074d29ae976ca8071d29.png)'
- en: Self-made image.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 自制图像。
- en: In the context of ChatGPT and the management of tokens, the terms **encoding
    and decoding refer to the processes of converting text into tokens** that the
    model can understand (encoding) and converting the model’s completion back into
    human-readable text (decoding).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ChatGPT 和标记管理的上下文中，术语**编码和解码指的是将文本转换为模型可以理解的标记（编码）以及将模型的输出转换回人类可读文本（解码）**的过程。
- en: Tiktoken Library
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tiktoken 库
- en: Knowing the theory behind the ChatGPT tokenizer is necessary, but in this article,
    I would like to focus on some hands-on revelations too.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 了解 ChatGPT 分词器背后的理论是必要的，但在本文中，我也想重点关注一些实际的揭示。
- en: 'The ChatGPT implementation uses the `tiktoken` library for managing tokens.
    We can get it up a running like any other Python library:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 实现使用`tiktoken`库来管理标记。我们可以像使用其他 Python 库一样使它运行起来：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once it is installed, it is very simple to get the same encoding model that
    ChatGPT uses since there is a `encoding_for_model()` method. As inferred by the
    name, this method automatically loads the correct encoding for a given model name.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装完毕，获取与 ChatGPT 相同的编码模型非常简单，因为有一个`encoding_for_model()`方法。顾名思义，这个方法会自动加载给定模型名称的正确编码。
- en: The first time it runs for a given model, it requires an internet connection
    to download the encoding model. Later runs won’t need internet since the encoding
    is already pre-cached.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次为给定模型运行时，需要互联网连接以下载编码模型。之后的运行则不需要互联网，因为编码已经预先缓存。
- en: 'For the widely used `gpt-3.5-turbo` model, we can simply run:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于广泛使用的`gpt-3.5-turbo`模型，我们可以简单地运行：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The output `encoding` is a tokenizer object that we can use to visualize how
    ChatGPT actually sees our prompt.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 输出`encoding`是一个分词器对象，我们可以用来可视化 ChatGPT 实际上如何看到我们的提示。
- en: More specifically, the `tiktoken.encoding_for_model` function initializes a
    tokenization pipeline specifically for the `gpt-3.5-turbo` model. This pipeline
    handles the tokenization and encoding of the text, preparing it for the model’s
    consumption.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，`tiktoken.encoding_for_model`函数初始化一个专门用于`gpt-3.5-turbo`模型的分词管道。这个管道处理文本的分词和编码，为模型的使用做准备。
- en: One important aspect to consider is that the tokens are numerical representations.
    In our *“Prompting as powerful developer tool”* example, the tokens associated
    with the word *prompting* were *“‘prom”*, *“pt”*, and *“ing”*, but what the model
    actually receives is the numerical representation of those sequences.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的方面是，标记是数值表示。在我们的*“Prompting as powerful developer tool”*示例中，与词语*prompting*相关的标记是*“‘prom”*、*“pt”*和*“ing”*，但模型实际接收到的是这些序列的数值表示。
- en: '*No worries!* We will see what this looks like in the hands-on section.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*不必担心！* 我们将在实践部分看到这是什么样的。'
- en: Encoding types
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码类型
- en: 'The `tiktoken` library supports multiple encoding types. In fact, different
    `gpt` models use different encodings. Here is a table with the most common ones:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`tiktoken`库支持多种编码类型。实际上，不同的`gpt`模型使用不同的编码。以下是最常见的编码类型的表格：'
- en: '![](../Images/cb5e491500e9b4c16f5132985f4b63d5.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb5e491500e9b4c16f5132985f4b63d5.png)'
- en: Encoding — Hands-On!
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码 — 实操！
- en: 'Let’s move forward and try to encode our first prompt. Given the prompt *“tiktoken
    is great!”* and the already loaded `encoding`, we can use the method `encoding.encode`
    to split the prompt into tokens and visualize their numerical representation:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续前进，尝试对我们的第一个提示进行编码。给定提示 *“tiktoken is great！”* 和已加载的`encoding`，我们可以使用方法`encoding.encode`将提示分割成标记并可视化其数值表示：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*Yes, that is true.* The output `[83, 1609, 5963, 374, 2294, 0]` does not seem
    very meaningful. But actually, there is something one can guess at first glance.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*是的，这是真的。* 输出 `[83, 1609, 5963, 374, 2294, 0]` 似乎不太有意义。但实际上，从一眼看过去可以猜到一些东西。'
- en: '*Got it?*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*明白了吗？*'
- en: '*The length!* We can quickly see that our prompt *“tiktoken is great!”* is
    split into 6 tokens. In this case, **ChatGPT is not splitting this sample prompt
    based on blank spaces, but on the most frequent sequences of letters.**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*长度！* 我们可以很快看到我们的提示 *“tiktoken is great！”* 被分割成了 6 个标记。在这种情况下，**ChatGPT 并不是基于空格分割这个样本提示，而是基于最常见的字母序列。**'
- en: In our example, each coordinate in the output list corresponds to a specific
    token in the tokenized sequence, the so-called token IDs. **The token IDs are
    integers that uniquely identify each token according to the vocabulary used by
    the model**. IDs are typically mapped to words or subword units in the vocabulary.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，输出列表中的每个坐标对应于标记化序列中的一个特定标记，即所谓的标记 ID。**标记 ID 是整数，根据模型使用的词汇表唯一标识每个标记**。ID
    通常映射到词汇表中的单词或子词单位。
- en: 'Let’s just decode the list of coordinates back to double check it corresponds
    to our original prompt:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解码坐标列表，以便再次检查它是否与原始提示相符：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `.decode()` method converts a list of token integers to a string. Although
    the `.decode()` method can be applied to single tokens, **be aware that it can
    be lossy for tokens that aren’t on** `**utf-8**` **boundaries**.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`.decode()` 方法将标记整数列表转换为字符串。虽然 `.decode()` 方法可以应用于单个标记，但**要注意它对不在 `**utf-8**`
    边界上的标记可能会有损失。**'
- en: And now you might wonder, *is there a way to see the individual tokens?*
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可能想知道，*有没有办法查看单独的标记？*
- en: '*Let’s go for it!*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*让我们开始吧！*'
- en: 'For single tokens, the `.decode_single_token_bytes()` method safely converts
    a single integer token to the bytes it represents. For our sample prompt:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个标记，`.decode_single_token_bytes()` 方法可以安全地将单个整数标记转换为它表示的字节。对于我们的示例提示：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that the `b` in front of the strings indicates that the strings are byte
    strings. For the English language, one token roughly on average, corresponds to
    about four characters or about three-quarters of a word.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，字符串前的`b`表示这些字符串是字节字符串。对于英语，平均每个标记大约对应四个字符或约四分之三的单词。
- en: Knowing how the text is split into tokens is useful because GPT models see text
    in the form of tokens. Knowing how many tokens are in a text string can give you
    useful information like whether the string is too long for a text model to process,
    or how much an OpenAI API call will cost as usage is priced by token, among others.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 了解文本是如何被分割成标记的很有用，因为 GPT 模型以标记的形式查看文本。知道文本字符串中有多少个标记可以提供有用的信息，比如字符串是否对文本模型来说过长，或者
    OpenAI API 调用的成本是多少，因为使用是按标记计价的，等等。
- en: Comparing Encoding Models
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较编码模型
- en: '![](../Images/b567bf0e70a07178a6d46f1b1c2ac962.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b567bf0e70a07178a6d46f1b1c2ac962.png)'
- en: Self-made gif.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 自制 gif。
- en: As we have seen, **different models use different encodings types**. Sometimes,
    there can be a huge difference in the token management between models.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，**不同模型使用不同的编码类型**。有时，不同模型之间的标记管理差异可能非常大。
- en: '**Different encodings vary in how they split words, group spaces, and handle
    non-English characters**. Using the methods above, we can compare different encodings
    for the different `gpt` models available on a few example strings.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**不同的编码在分割单词、分组空格和处理非英语字符方面有所不同**。使用上述方法，我们可以对几种示例字符串比较不同`gpt`模型的编码。'
- en: 'Let’s compare the encodings of the table above (`gpt2`, `p50k_base`, and `cl100k_base`).
    To do so, we can use the following function that contains all the bits and pieces
    we have seen so far:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较一下上表中的编码（`gpt2`、`p50k_base` 和 `cl100k_base`）。为此，我们可以使用以下包含我们迄今所见所有要点的函数：
- en: 'The `compare_encodings` function takes an `example_string` as input and compares
    the encodings of that string using three different encoding schemes: `gpt2`, `p50k_base`,
    and `cl100k_base`. Finally, it prints various information about the encodings,
    including the number of tokens, the token integers, and the token bytes.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`compare_encodings` 函数以 `example_string` 作为输入，比较该字符串使用三种不同编码方案的编码：`gpt2`、`p50k_base`
    和 `cl100k_base`。最后，它打印关于编码的各种信息，包括标记数、标记整数和标记字节。'
- en: '*Let’s try some examples!*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*让我们尝试一些例子！*'
- en: '![](../Images/96a06fc1407200dc733968cc15adec56.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96a06fc1407200dc733968cc15adec56.png)'
- en: In this first example, although the `gpt2` and `p50k_base` models agree on the
    encoding by merging together the math symbols with the blank spaces, the `cl100k_base`
    encoding considers them separate entities.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个第一个例子中，尽管 `gpt2` 和 `p50k_base` 模型在将数学符号与空格合并在一起的编码上达成一致，但 `cl100k_base` 编码则认为它们是分开的实体。
- en: '![](../Images/ab5e9520a298e6044dff60495731c058.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab5e9520a298e6044dff60495731c058.png)'
- en: In this example, the way of tokenizing the word *Prompting* also depends on
    the selected encoding.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，单词 *Prompting* 的标记化方式也取决于所选择的编码。
- en: Tokenizer Limitations
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标记器限制
- en: '![](../Images/a56c3bd7ed2bfe51aa7feb3d52553ff3.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a56c3bd7ed2bfe51aa7feb3d52553ff3.png)'
- en: Self-made gif.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 自制 gif。
- en: '**This way of tokenizing the input prompts is sometimes the cause of some ChatGPT
    completion errors**. For example, if we ask ChatGPT to write the word lollipop
    in reversed order, it does it wrong!'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**这种对输入提示进行标记化的方法有时会导致一些 ChatGPT 完成错误**。例如，如果我们让 ChatGPT 将单词 lollipop 反向书写，它会做错！'
- en: '![](../Images/f049427e75ab89b1f45fee021a553ded.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f049427e75ab89b1f45fee021a553ded.png)'
- en: Self-made screenshot.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 自制截图。
- en: 'What is happening here is that the tokenizer actually breaks the given word
    down into three tokens: *“l”* ,*“oll”* and *“ipop”*. Therefore, **ChatGPT does
    not see the individual letters, instead it sees these three tokens** making it
    more difficult to print out individual letters in reverse order correctly.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生的情况是标记器实际上将给定的单词分解为三个标记：*“l”*、*“oll”* 和 *“ipop”*。因此，**ChatGPT 无法看到单独的字母，而是看到这三个标记**，这使得正确反向打印单独字母更加困难。
- en: 'Being aware of the limitations can make you find workarounds to avoid them.
    In this case, if we add dashes to the word between the individual letters, we
    can force the tokenizer to split the text based on those symbols. By slightly
    modifying the input prompt, it actually does a much better job:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这些限制可以帮助你找到避免这些问题的解决方法。在这种情况下，如果我们在单词的字母之间添加短横线，我们可以迫使标记器根据这些符号拆分文本。通过稍微修改输入提示，它实际上做得更好：
- en: '![](../Images/e16850a86e10532d2a9bce6c756642d0.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e16850a86e10532d2a9bce6c756642d0.png)'
- en: Self-made screenshot.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 自制截图。
- en: '**By using dashes, it is easier for the model to see the individual letters
    and print them out in reverse order**. So keep that in mind: If you ever want
    to use ChatGPT to play a word game, like *Word* or *Scrabble*, or build an application
    around those principles, this nifty trick helps it to better see the individual
    letters of the words.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过使用短横线，模型更容易看到单独的字母并将其反向打印**。所以请记住：如果你想让 ChatGPT 玩字词游戏，如 *Word* 或 *Scrabble*，或者围绕这些原则构建应用程序，这个巧妙的技巧可以帮助它更好地识别单独的字母。'
- en: '**This is just a simple example where the ChatGPT tokenizer causes the model
    to fail in a very simple task**. *Have you encountered any other similar cases?*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**这只是一个简单的例子，其中 ChatGPT 标记器导致模型在非常简单的任务中失败**。*你遇到过其他类似的情况吗？*'
- en: Summary
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this article, we have explored how **ChatGPT sees user prompts and processes
    them to generate a completion output** based on statistical patterns learned from
    vast amounts of language data during its training.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们探讨了**ChatGPT 如何查看用户提示并根据从大量语言数据中学习的统计模式处理这些提示以生成完成输出**。
- en: By using the `tiktoken` library, **we are now able to evaluate any prompt**
    before feeding it into ChatGPT. This can help us to **debug ChatGPT errors** since
    it can happen that by slightly modifying our prompt, we can make ChatGPT better
    complete the task.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 `tiktoken` 库，**我们现在可以在将提示输入 ChatGPT 之前评估任何提示**。这可以帮助我们**调试 ChatGPT 错误**，因为通过稍微修改提示，我们可以让
    ChatGPT 更好地完成任务。
- en: 'There is also an extra take-home message: **Some design decisions made can
    turn into technical debts in the future**. As we have seen in the simple *lollipop*
    example, while the model succeeds in mind-blowing tasks, it cannot complete simple
    exercises. And the reason behind this is not on the model capabilities, but on
    the first tokenization step!'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个额外的信息：**一些设计决策可能会在未来转变为技术债务**。正如我们在简单的*棒棒糖*示例中看到的，尽管模型在令人惊叹的任务中表现出色，但它不能完成简单的练习。这背后的原因不在于模型的能力，而在于最初的分词步骤！
- en: That is all! Many thanks for reading!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了！非常感谢阅读！
- en: I hope this article helps you when **builiding ChatGPT applications!**
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章能帮助你在**构建ChatGPT应用程序时**！
- en: 'You can also subscribe to my [**Newsletter**](https://towardsdatascience.com/@andvalenzuela/subscribe)
    to stay tuned for new content. **Especially**, **if you are interested in articles
    about ChatGPT**:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以订阅我的[**新闻通讯**](https://towardsdatascience.com/@andvalenzuela/subscribe)以获取最新内容。**特别是**，**如果你对ChatGPT相关文章感兴趣**：
- en: '[](/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce?source=post_page-----27f78906ea54--------------------------------)
    [## Mastering ChatGPT: Effective Summarization with LLMs'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce?source=post_page-----27f78906ea54--------------------------------)
    [## 精通ChatGPT：使用LLMs进行有效总结'
- en: How to Prompt ChatGPT to get High-Quality Summaries
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何提示ChatGPT以获得高质量的总结
- en: towardsdatascience.com](/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce?source=post_page-----27f78906ea54--------------------------------)
    [](https://medium.com/geekculture/prompt-engineering-course-openai-inferring-transforming-expanding-chatgpt-chatgpt4-e5f63132f422?source=post_page-----27f78906ea54--------------------------------)
    [## Prompt Engineering Course by OpenAI — Inferring, Transforming, and Expanding
    with ChatGPT
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce?source=post_page-----27f78906ea54--------------------------------)
    [](https://medium.com/geekculture/prompt-engineering-course-openai-inferring-transforming-expanding-chatgpt-chatgpt4-e5f63132f422?source=post_page-----27f78906ea54--------------------------------)
    [## OpenAI提示工程课程 — 使用ChatGPT进行推断、转换和扩展
- en: Maximize ChatGPT’s Potential in your Custom Application
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最大化ChatGPT在你自定义应用程序中的潜力
- en: 'medium.com](https://medium.com/geekculture/prompt-engineering-course-openai-inferring-transforming-expanding-chatgpt-chatgpt4-e5f63132f422?source=post_page-----27f78906ea54--------------------------------)
    [](/chatgpt-text-to-speech-artificial-intelligence-python-data-science-52456f51fad6?source=post_page-----27f78906ea54--------------------------------)
    [## Unlocking a New Dimension of ChatGPT: Text-to-Speech Integration'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/geekculture/prompt-engineering-course-openai-inferring-transforming-expanding-chatgpt-chatgpt4-e5f63132f422?source=post_page-----27f78906ea54--------------------------------)
    [](/chatgpt-text-to-speech-artificial-intelligence-python-data-science-52456f51fad6?source=post_page-----27f78906ea54--------------------------------)
    [## 解锁ChatGPT的新维度：文本到语音集成
- en: Enhancing User Experience in ChatGPT Interactions
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提升ChatGPT交互中的用户体验
- en: towardsdatascience.com](/chatgpt-text-to-speech-artificial-intelligence-python-data-science-52456f51fad6?source=post_page-----27f78906ea54--------------------------------)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/chatgpt-text-to-speech-artificial-intelligence-python-data-science-52456f51fad6?source=post_page-----27f78906ea54--------------------------------)'
- en: '**Feel free to forward any questions** you may have to *forcodesake.hello@gmail.com*
    :)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**随时向*forcodesake.hello@gmail.com*提出任何问题** :)'
