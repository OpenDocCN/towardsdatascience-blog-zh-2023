- en: Interpreting Machine Learning Models Using Data-Centric Explainable AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/interpreting-machine-learning-models-using-data-centric-explainable-ai-8415be070416](https://towardsdatascience.com/interpreting-machine-learning-models-using-data-centric-explainable-ai-8415be070416)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn about data-centric explanation and its different types in this article
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://adib0073.medium.com/?source=post_page-----8415be070416--------------------------------)[![Aditya
    Bhattacharya](../Images/d0f79ad4a85330c58327aea499b7eea0.png)](https://adib0073.medium.com/?source=post_page-----8415be070416--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8415be070416--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8415be070416--------------------------------)
    [Aditya Bhattacharya](https://adib0073.medium.com/?source=post_page-----8415be070416--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8415be070416--------------------------------)
    ·8 min read·Feb 26, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/64e9edc18d4081642554bc54fda2e7a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Pixabay](https://pixabay.com/illustrations/charts-tables-graph-statistics-6246450/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Explainable AI (XAI)**](https://amzn.to/3cY4c2h) is an emerging concept
    that aims to bridge the gap between AI and end-users, thereby increasing AI adoption.
    XAI can make AI/ML models more transparent, trustworthy, and understandable. It
    is a necessity, especially for critical domains such as healthcare, finance, and
    law enforcement.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get an introduction to XAI, the following 45 minutes presentation of mine
    from the **AI Accelerator Festival APAC, 2021** will be very helpful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Popular XAI methods](https://amzn.to/3J2QNnz), such as LIME, SHAP, Saliency
    Maps, etc., are [**model-centric explanation methods**](https://amzn.to/3J2QNnz).
    These methods approximate the important features used by machine learning models
    to generate predictions. However, due to the inductive bias of ML models, an estimation
    of important features considered by the predictive models might not always be
    correct. Consequently, model-centric feature importance methods may not be very
    useful always.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, considering the principles of Data-Centric AI, the quality of
    ML models is only as good as the quality of the data used to train them. Data
    quality issues caused due to correlated features, data drifts, outliers, skewed
    data, and so on can impact the performance of the trained ML models. Yet, non-technical
    consumers of AI hardly have an awareness of the *goodness* of the datasets used
    to train ML models. Hence, [**Data-Centric Explainable AI (DCXAI)**](https://amzn.to/3J2QNnz)
    is a better choice instead of model-centric explanations when potential data issues
    in datasets are detected during the training and inference of ML models.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are interested to know how Data-Centric Explainable AI can be leveraged
    to explain ML models for a high stake domain, such as healthcare, please take
    a look at my research publication — [Directive Explanations for Monitoring the
    Risk of Diabetes Onset: Introducing Directive Data-Centric Explanations and Combinations
    to Support What-If Explorations](https://arxiv.org/abs/2302.10671).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I will also refer to some of the concepts about data-centric explanations discussed
    in my book [**Applied Machine Learning Explainability Techniques**](https://amzn.to/3cY4c2h)**.**
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----8415be070416--------------------------------)
    [## Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for…'
  prefs: []
  type: TYPE_NORMAL
- en: 'Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for practical…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.amazon.com](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----8415be070416--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**What is Data-Centric Explainable AI (DCXAI)?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in my book **Applied Machine Learning Explainability Techniques**,
    Data-Centric Explainable AI (DCXAI) is an XAI method that explains how an ML model
    can behave by generating insights about the underlying dataset used to train them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0edf5ed26dfba88ff72e9855d353598f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Pixabay](https://pixabay.com/illustrations/graph-charts-stats-data-metrics-6249047/)'
  prefs: []
  type: TYPE_NORMAL
- en: Examples of data-centric explanation approaches include summarizing datasets
    using common statistical methods like mean, mode, and variance, visualizing the
    data distributions to compare feature values to those across the remaining dataset,
    and observing changes in model predictions through what-if analysis to probe into
    the sensitivity of the features. Additionally, data-centric explanations include
    data-driven rule-based approaches that are commonly adopted in decision support
    systems. Furthermore, DCXAI includes creating more awareness about the data quality
    by sharing more insights about the various data issues, such as data drift, skewed
    data, outliers, correlated features and etc., that can impact the overall performance
    of the ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Why DCXAI instead of other XAI methods?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More recently, due to the failure of ML models trained on biased, inconsistent,
    and poor-quality data, the ML research community is exploring data-centric approaches
    for training ML models instead of solely relying on hyperparameter tuning and
    exploring different ML algorithms. If the data is consistent, unambiguous, balanced,
    and available in sufficient quantity, ML models can be trained faster with higher
    accuracy and faster deployment for any production-level system.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, all AI and ML systems that exist in production today are not
    in alignment with the principles of data-centric AI. Consequently, there can be
    severe issues with the underlying data that seldom get detected but eventually
    lead to the failure of ML systems. That is why **DCXAI** is important to inspect
    and evaluate the quality of the data being used.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----8415be070416--------------------------------)
    [## Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for…'
  prefs: []
  type: TYPE_NORMAL
- en: 'Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for practical…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.amazon.com](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----8415be070416--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Different approaches of DCXAI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There can be different approaches to data-centric explanations, which can be
    further categorized by the following types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generating insights about the training data** — Exploratory Data Analysis
    (EDA) is an important practice conducted by all data scientists and ML experts
    before building ML models. However, every insight generated from EDA is very rarely
    communicated to the non-technical consumers of ML models. So, one of the approaches
    of DCXAI is to communicate the insights generated to the end-users to explain
    the potential behavior of ML models. This is particularly useful for domain experts
    who may not have ML knowledge but are experts in their own domains.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreover, visualization of the data distribution can indicate how well the dataset
    is balanced. It can also show the presence of skewness and outliers in training
    data which can affect the model. Generating insights by building data profiles
    through statistical measures can also be very useful for local as well as global
    explanations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Highlighting the data quality** — Most of the time, the poor performance
    of ML models is related to the poor quality of data used to train them. However,
    the information on data quality is rarely communicated to end-users. Consequently,
    when ML models fail to generate good predictions, end-users are never aware of
    the issues in the dataset. So, DCXAI involves explaining about the data quality
    by communicating about the potential data issues such as data drifts, correlated
    features, class imbalance, biased datasets and etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is indeed true that some of these data issues are complicated to understand
    as these are technical concepts. But when presented using simplified and interactive
    visualizations, it creates awareness about the data quality, which highlights
    the true reason for the failure of ML models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Estimating data forecastability** — Sometimes, datasets are too noisy. Getting
    beyond a certain amount of accuracy is always difficult with such a dataset. Then,
    how do we gain the trust of our end users if we know that the trained model is
    not extremely accurate in making the correct predictions? I would say that the
    best way to gain trust is by being transparent and clearly communicating what
    is feasible. So, measuring **data forecastability** and communicating the model’s
    efficiency to end users helps to set the right expectation. Data forecastability
    is an estimation of the model’s performance using the underlying data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, we have a model to predict the stock price of a particular company.
    The stock price data that is being modeled by the ML algorithm can predict the
    stock price with a maximum of 60% accuracy. Beyond that point, it is not practically
    possible to generate a more accurate outcome using the given dataset. But let’s
    say that if other external factors are considered to supplement the current data,
    the model’s accuracy can be boosted. This proves that it is not the ML algorithm
    that is limiting the performance of the system, but rather the dataset that is
    used for modeling does not have sufficient information to get a better model performance.
    Hence, it is a limitation of the dataset that can be estimated by measure of data
    forecastability. It is better to perform data forecastability at a granular level
    to give additional insights about the performance of the ML model at different
    values of demographic variables, as illustrated by the following diagram.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/31cf969163572ef30585ca03a20caa18.png)'
  prefs: []
  type: TYPE_IMG
- en: Data forecastability estimate for different demographic variables (image by
    author)
  prefs: []
  type: TYPE_NORMAL
- en: Benefit of DCXAI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have understood the different approaches of DCXAI, let us also summarize
    its benefits as presented in the following points.
  prefs: []
  type: TYPE_NORMAL
- en: Easy detection of biased and unfair data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating awareness about issues with data quality, purity, and integrity to
    explain the failure of ML models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DCXAI is more simple to understand for non-technical consumers of ML than other
    popular model-centric explanation methods such as LIME, SHAP, and saliency maps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Domain experts tend to have more trust in DCXAI than LIME and SHAP, as DCXAI
    creates more transparency about the datasets used to train ML models. They can
    use DCXAI to justify the model generated predictions by referring to the underlying
    training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----8415be070416--------------------------------)
    [## Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for…'
  prefs: []
  type: TYPE_NORMAL
- en: 'Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for practical…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.amazon.com](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----8415be070416--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Making DCXAI more actionable through Directive Data-Centric Explanations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In my recent research publication — [Directive Explanations for Monitoring the
    Risk of Diabetes Onset](https://arxiv.org/pdf/2302.10671.pdf), I presented about
    an elaborate user-centered design process for an XAI dashboard that includes DCXAI.
    We have further made DCXAI more actionable by making the following adaptions to
    tailor these explanations for healthcare experts -
  prefs: []
  type: TYPE_NORMAL
- en: Provided interactive visual explanations for exploring what-if scenarios.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considering only actionable feature variables instead of non-actionable features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing explicit visual indicators that enable users to explore the system
    to understand the working of the ML models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obtaining local explanations with a global perspective.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these additional modifications to traditional data-centric explanation
    approaches, we have designed and developed **Visually Directive Data-Centric Explanations**.
    You can find out more about this research study in the [research paper](https://arxiv.org/abs/2302.10671).
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----8415be070416--------------------------------)
    [## Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for…'
  prefs: []
  type: TYPE_NORMAL
- en: 'Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for practical…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.amazon.com](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----8415be070416--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we have covered Data-Centric Explainable AI (DCXAI) and its
    various approaches. We have also covered how DCXAI is different from other XAI
    methods, such as LIME, SHAP, etc., which provide model-centric explanations. We
    have discussed about the different approaches to provide data-centric explanations.
    Additionally, we have discussed the benefits of DCXAI and how DCXAI can be further
    modified to generate more actionable explanations for domain experts and lay users.
    You can learn more about data-centric explanations from my book [Applied Machine
    Learning Explainability Techniques](https://amzn.to/3cY4c2h) and take a look at
    code examples presented in the [GitHub repo](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/)
    from the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other XAI stories on TDS by the author:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Explainable AI with TCAV from Google AI](/explainable-ai-with-tcav-from-google-ai-5408adf905e)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Essential Explainable AI Python frameworks that you should know about](/essential-explainable-ai-python-frameworks-that-you-should-know-about-84d5063b75e9)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Explainable Machine Learning for Models Trained on Text Data: Combining SHAP
    with Transformer Models](/explainable-machine-learning-for-models-trained-on-text-data-combining-shap-with-transformer-5095ea7f3a8)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[EUCA — An effective XAI framework to bring artificial intelligence closer
    to end-users](/euca-an-effective-xai-framework-to-bring-artificial-intelligence-closer-to-end-users-74bb0136ffb1)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Understand the Workings of SHAP and Shapley Values Used in Explainable AI](/understand-the-working-of-shap-based-on-shapley-values-used-in-xai-in-the-most-simple-way-d61e4947aa4e)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[How to Explain Image Classifiers Using LIME](/how-to-explain-image-classifiers-using-lime-e364097335b4)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**References**:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Directive Explanations for Monitoring the Risk of Diabetes Onset: Introducing
    Directive Data-Centric Explanations and Combinations to Support What-If Explorations](https://arxiv.org/abs/2302.10671)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Applied Machine Learning Explainability Techniques](https://amzn.to/3cY4c2h)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GitHub repo from the book Applied Machine Learning Explainability Techniques
    — [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
