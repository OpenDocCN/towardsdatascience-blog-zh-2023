- en: Understanding Multinomial Distribution using Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understanding-multinomial-distribution-using-python-f48c89e1e29f](https://towardsdatascience.com/understanding-multinomial-distribution-using-python-f48c89e1e29f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The math and intuition behind the multinomial distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://reza-bagheri79.medium.com/?source=post_page-----f48c89e1e29f--------------------------------)[![Reza
    Bagheri](../Images/7c5a7dc9e6e31048ce31c8d49055987c.png)](https://reza-bagheri79.medium.com/?source=post_page-----f48c89e1e29f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f48c89e1e29f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f48c89e1e29f--------------------------------)
    [Reza Bagheri](https://reza-bagheri79.medium.com/?source=post_page-----f48c89e1e29f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f48c89e1e29f--------------------------------)
    ·20 min read·Jan 6, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be421088ce08f6be5825b9f3e0a934fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [https://pixabay.com/vectors/dice-game-die-luck-random-numbers-151867/](https://pixabay.com/vectors/dice-game-die-luck-random-numbers-151867/)'
  prefs: []
  type: TYPE_NORMAL
- en: The multinomial distribution is a generalization of the binomial distribution
    and is used to find the probabilities in experiments with more than two outcomes.
    This article gives an intuitive introduction to multinomial distribution and discusses
    its mathematical properties. In addition, it will teach you how to use the SciPy
    library in Python to model and visualize a multinomial distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Binomial disitrbution**'
  prefs: []
  type: TYPE_NORMAL
- en: Since the multinomial distribution is a generalization of the binomial distribution,
    we will briefly review it here. In [another article](https://medium.com/towards-data-science/understanding-probability-distributions-using-python-9eca9c1d9d38),
    I discussed the univariate probability distributions in more detail, so if you
    are not familiar with binomial distribution or concepts like random variables
    and probability mass function (PMF), I suggest you read [that article](https://medium.com/towards-data-science/understanding-probability-distributions-using-python-9eca9c1d9d38)
    first.
  prefs: []
  type: TYPE_NORMAL
- en: A *random variable* is a variable whose value is determined by the outcome of
    a random experiment. A Random variable is usually denoted by uppercase letters
    however, we use lowercase letters to denote the particular values that it can
    take. For example, we can define a random variable *X* to represent the outcomes
    of a coin toss. To do that we need to assign a numerical value to each outcome.
    We can denote obtaining a head by 1 and obtaining a tail by 0\. Now, *X*=1 means
    that the outcome of a coin toss is a head, and *X*=0 means that it is a tail.
    Such a random variable that can only take certain values is called a *discrete
    random variable*.
  prefs: []
  type: TYPE_NORMAL
- en: We define the probability mass function (PMF) of a discrete random variable
    *X* as a function that gives the probability that *X* is equal to a certain value.
    Mathematically, the PMF of *X* is defined as the function *p*ₓ such that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b9dc00158375170d3900f00361c9b57.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A particular value that *X* can take on is denoted by *x, s*o *P*(*X*=*x*)
    gives the probability of *X*=*x*. We know that the sum of the probabilities for
    all the possible values of *X* should be equal to 1, so it follows that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ee8dc17ba517bf0a650ca5e085fd7f7.png)'
  prefs: []
  type: TYPE_IMG
- en: A discrete random variable *X* with a Bernoulli distribution with parameter
    *p* represents a random experiment that only has two outcomes denoted by 0 and
    1 (we denote it by *X*~Bern(*p*)). Here *X*=1 (also called ‘success’) occurs with
    the probability *p* and *X*=0 (also called ‘failure’) occurs with the probability
    1-*p*. As an example, we can represent the outcome of a coin toss with a random
    variable *X* with a Bernoulli distribution. Now we can assume that *X*=1 and *X*=0
    represent getting a head and a tail respectively, and *p* is the probability of
    obtaining a head.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that we have a sequence of *n* independent random experiments and each
    of them can be represented by a random variable that has a Bernoulli distribution
    with the parameter *p*. So each experiment has only two outcomes denoted by 1
    (success) and 0 (failure) and *X*=1 occurs with the probability *p* (such an experiment
    is called a Bernoulli trial). A discrete random variable *X* with a binomial distribution
    with parameters *n* and *p* represents the total number of successes in this sequence,
    and we can write this as *X ~* Bin(*n, p*).Suppose that we have a coin that lands
    on heads with the probability *p*. We can represent the total number of heads
    in *n* tosses of this coin with a random variable *X* that has a binomial distribution
    with parameters *n* and *p* (Figure 1).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d6a3bae878b0d97a80ad5be3cee561a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1 (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: '**Random vectors**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multinomial disitrbution is a *multivariate disitrbution*. In statistics, a
    *univariate distribution* is a probability distribution of only one random variable.
    The multivariate distributions are a generalization of the univariate distributions
    to two or more random variables. To understand these distributions, we should
    first discuss the random vectors. A random vector is a vector of random variables.
    If we have *n* random variables *X*₁, *X*₂ …, *Xₙ*, we can place them in the random
    vector ***X***:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e683e03b1fed10b01e8e451f6df7576.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We use bold uppercase letters to denote a random vector. To show a possible
    value of a random vector (which is also a vector) we use bold lowercase letters
    such as ***x***. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c97e8a856c14fb5f780279c1e054b847.png)'
  prefs: []
  type: TYPE_IMG
- en: Is a random vector with two elements and
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/54751b6e5765c1a43a5b245df95baabe.png)'
  prefs: []
  type: TYPE_IMG
- en: is a possible value of it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Joint PMF**'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that we have a random vector
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e683e03b1fed10b01e8e451f6df7576.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *X*₁, *X*₂ …, *Xₙ* are discrete random variables. The *joint probability
    mass function* (joint PMF) of ***X*** is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a13b6cfad30af8a257a724d7fe444391.png)'
  prefs: []
  type: TYPE_IMG
- en: where *P*(*X*₁=*x*₁, *X*₂=*x*₂, …, *Xₙ*=*xₙ*) is that probability that *X*₁=*x*₁,
    *X*₂=*x*₂, …, and *Xₙ*=*xₙ* at the same time. If we have the joint PMF of the
    random vector ***X***, we can derive the distribution of one of its components
    *Xᵢ* using *marginalization*. The *marginal* *probability mass function* of *Xᵢ*
    can be derived from the joint PMF of ***X*** as follows
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d97d95a6e6f48d8b00ed6c4b00a3678b.png)'
  prefs: []
  type: TYPE_IMG
- en: Here *R***ₓ** is the support of ***X*** which means that it is the set of all
    values the random vector ***X*** can take. So, to derive the marginal probability
    PMF of *Xᵢ* at point *x*, we need to sum the probabilities of all the vectors
    of *R***ₓ** in which *Xᵢ* is equal to *x*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Multinomial distribution**'
  prefs: []
  type: TYPE_NORMAL
- en: The multinomial distribution is a generalization of the binomial distribution.
    Suppose that we have *n* independent trials. Each trial has *k* different outcomes
    (*k* ≥ 2), and the probability of the *i*th outcome is *pᵢ* and
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2deb3097908cb7aa9a21931d4f8052c4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The vector ***p*** denotes these probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a176a3594ae25731750094f0da0cdd9b.png)'
  prefs: []
  type: TYPE_IMG
- en: Let the discrete random variable *Xᵢ* represent the number of times the outcome
    number *i* is observed over *n* trials. The random vector ***X*** is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2badf37a90938009e80bdb08529e453a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'and we have *x*₁+*x*₂ +…+*xₖ*=*n*. Then ***X*** is said to have the multinomial
    distribution with parameters *n* and ***p****.* The joint PMF of ***X*** is defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/84911c9d7a590ad9b7f5f67978185a6e.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s see if we can prove it. First, we should note that if *x*₁+*x*₂ +…+*xₖ*≠*n*
    then the event *X*₁=*x*₁, *X*₂=*x*₂, …, and *Xₖ*=*xₖ* is impossible, so its probability
    should be equal to zero. If *x*₁+*x*₂ +…+*xₖ*=*n*, the probability of the event
    *X*₁=*x*₁, *X*₂=*x*₂, …, and *Xₖ*=*xₖ* is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/989b65ec4e8fd60dad535f38c3a1a0c0.png)'
  prefs: []
  type: TYPE_IMG
- en: In addition, the total number of different ways in which we can have *n* trials
    with *X*₁=*x*₁, *X*₂=*x*₂, …, and *Xₖ*=*xₖ* is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4dadc5ded2a235c87074699e1b7ba28b.png)'
  prefs: []
  type: TYPE_IMG
- en: So the total probability of having *X*₁=*x*₁, *X*₂=*x*₂, …, and *Xₖ*=*xₖ* in
    *n* trialsis
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a421a215bb8bb3921fd7e258aeb24db.png)'
  prefs: []
  type: TYPE_IMG
- en: Based on this story, the multinomial distribution can be used to describe a
    *k*-sided die. Suppose that we have a *k*-sided die and the probability of getting
    side *i* is *pᵢ*. In addition, let *Xᵢ* represent the total number of times that
    side *i* is observed. Now, if we roll it *n* times, the random vector ***X***
    has a multinomial distribution with parameters *n* and ***p*** (Figure 2).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1f308405453930cb5be7b120f2a8633a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2 (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: We can also express the multinomial distribution in a different way. Suppose
    that we have a population of items of *k* different categories (*k* ≥ 2). The
    proportion of the items in the population that are in category *i* is *pᵢ*, and
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2deb3097908cb7aa9a21931d4f8052c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we randomly select *n* items from the population with replacement, and we
    assume that the random variable *Xᵢ* represents the number of selected items that
    are in category *i* (Figure 3). Since these items are selected at random with
    replacement, the selections will be independent of each other, and the probability
    that an item will be in category *i* is *pᵢ*. So, we can think of each selection
    as an independent trial with *k* different outcomes, and the corresponding probability
    of each outcome *i* is *pᵢ*. Now, if we define the vector ***p*** as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a176a3594ae25731750094f0da0cdd9b.png)'
  prefs: []
  type: TYPE_IMG
- en: then the random vector
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2badf37a90938009e80bdb08529e453a.png)'
  prefs: []
  type: TYPE_IMG
- en: has the multinomial distribution with parameters *n* and ***p***.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea7ca89aad730a7dfbc8ec377f4a75e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3 (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'A random vector ***X*** that has a multinomial distribution with parameters
    *n* and ***p*** can be written as the sum of *n* random vectors that have a multinomial
    distribution with parameters 1 and ***p***:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d5e45128ca1841f3fba42b78165734e.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Proof** (*optional*): The vectors ***Y****ᵢ* can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0c6e2dd5845276a3974b1c4fab0bdad.png)'
  prefs: []
  type: TYPE_IMG
- en: Since *X*₁+*X*₂ +…+*Xₖ*=*n,* we conclude that we have *n* vectors on the right-hand
    side*.* In each ***Y****ᵢ* one element is 1 and the others are 0\. So, each ***Y****ᵢ*
    can represent one trial that has *k* different outcomes, and the element which
    is equal to 1 represents the observed outcome. Therefore, each ***Y****ᵢ* has
    a multinomial distribution with parameters 1 and ***p***.
  prefs: []
  type: TYPE_NORMAL
- en: We can merge multiple elements in a multinomial random vector ***X*** to get
    a new multinomial random vector. For example, let
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c58acc6b94027153a2bedcb84e364443.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we can merge the first three elements to get the new random vector
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/60262144e97738e0fff1cd4dd5450d11.png)'
  prefs: []
  type: TYPE_IMG
- en: And this random vector has multinomial distribution with parameters *n* and
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b5c973d40d92b24e92a10b13ac488ea.png)'
  prefs: []
  type: TYPE_IMG
- en: In fact, by merging *X*₁, *X*₂, and *X*₅, we create a new category, and the
    proportion of this category is simply *p*₁+*p*₂+*p*₅. So, it can create a multinomial
    distribution along the remaining categories 3 and 4 while the proportion of those
    categories remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: The multinomial distribution is a generalization of the binomial distribution.
    If *k*=2, the multinomial distribution reduces to a binomial distribution. Hence,
    if we have
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57b7ea2a92767e4ff4414adcf88d3920.png)'
  prefs: []
  type: TYPE_IMG
- en: then *X*₁ ~ Bin(*n*, *p*₁) and *X*₂ ~ Bin(*n*, *p*₂).
  prefs: []
  type: TYPE_NORMAL
- en: '**Proof** (*optional*): We should note that *X*₂= *n*-*X*₁ and *p*₂=1-*p*₁.
    So, if we call the outcome 1 a success and the outcome 2 a failure, then each
    trial has only two outcomes (success and failure), and success occurs with probability
    *p*₁. Hence, each trial can be represented with a Bernoulli distribution with
    parameter *p*₁ and *X*₁ represents the number of successes in *n* Bernoulli trials
    with the parameter *p*₁. So, we conclude that *X*₁ has a binomial distribution
    with parameters *n* and *p*₁. Similarly, if we call the outcome 2 a success, and
    the outcome 1 a failure, *X*₂ represents the number of successes in *n* Bernoulli
    trials with the parameter *p*₂, so has a binomial distribution with parameters
    *n* and *p*₂.'
  prefs: []
  type: TYPE_NORMAL
- en: We can extend the previous result to a multinomial distribution with *k*>2\.
    If
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25723813b57188dcbe469671c35beb31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'then the marginal distribution of each *Xᵢ* is a binomial distribution with
    parameters *n* and *pᵢ*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ecfea65828c7ad683f07e1f204135a8.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Proof** (*optional*): We assume that the outcome *i* is a success with the
    probability *pᵢ*, and all the remaining outcomes are a failure with the probability
    1-*pᵢ*. So, *Xᵢ* represents the number of successes in *n* Bernoulli trials with
    the parameter *pᵢ*, and it has a binomial distribution with parameters *n* and
    *pᵢ*.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the sum of some of the elements of ***X*** has a binomial distribution.
    Mathematically, if *Xᵢ,*1, *Xᵢ,*₂, …, *Xᵢ,ₘ* are *m* elements of the random vector
    ***X*** (*m*<*k*) and their corresponding probabilities in vector ***p*** are
    *pᵢ,*₁, *pᵢ*,₂, …, *pᵢ,ₘ*, then the sum *Xᵢ,*₁ + *Xᵢ,*₂ + …+ *Xᵢ,ₘ* has a binomial
    distribution with parameters *n* and *pᵢ,*₁ + *pᵢ,*₂ + …+ *pᵢ,ₘ.* For example,
    for *n*=8 and *k*=5, *X*₁+*X*₃+*X*₅ has a binomial distribution with parameters
    8 and *p*₁ + *p*₃ + *p*₅*.*
  prefs: []
  type: TYPE_NORMAL
- en: '**Proof** (*optional*): As mentioned before, we can merge multiple elements
    in a multinomial random vector ***X*** to get a new multinomial random vector.
    So, by merging *Xᵢ,*₁ + *Xᵢ,*₂ + …+ *Xᵢ,ₘ* we get a new random vector ***Y***
    which has a multinomial distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29c7c4a168fde811c124e5a3207519fd.png)'
  prefs: []
  type: TYPE_IMG
- en: We also now showed that the marginal distribution of each element of a multinomial
    random vector has a binomial distribution with parameters *n* and its corresponding
    probability, so we conclude that *Xᵢ,*₁ + *Xᵢ,*₂ + …+ *Xᵢ,ₘ* has a binomial distribution
    with parameters *n* and *pᵢ,*₁ + *pᵢ,*₂ + …+ *pᵢ,ₘ.*
  prefs: []
  type: TYPE_NORMAL
- en: We can use the `scipy` library in Python to generate the multinomial distribution.
    We can create a multinomial distribution using the `multinomial` object in `scipy.stat`,
    and calculate its joint PMF using the `pmf()` method of this object. The object
    takes the parameters `n` and `p` which correspond to *n* and ***p***. `p` is an
    array-like object. Each element of `p` should be in the interval [0, 1] and the
    elements should sum to 1\. If they do not sum to 1, the last element of `p` is
    not used and is replaced with one minus the sum of the earlier elements (so that
    the elements sum to 1).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to import all the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Listing 1 creates the following multinomial distribution
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6f20c8a28ddfd2d8676de07da59f0f6c.png)'
  prefs: []
  type: TYPE_IMG
- en: And calculates its joint PMF at
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98935491670adb821e70e83d624c491c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We could also pass the distribution parameters to the `pmf()` function directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The method `rvs()` can be used to generate random variates. A random variate
    or simply variate is a particular outcome of a random variable or a random vector.
    Using this method, we can draw a random sample of size *m* from a multinomial
    distribution which means that we generate *m* random variates from a random vector
    with a multinomial distribution. For example, to draw a random sample of size
    4 from a multinomial distribution in Equation 1, we can use the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: It returns a 2d array with 4 elements. Each element is an array and is a possible
    value that the random vector can ***X*** in equation 1 can take.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how we can visualize a multinomial distribution defined in Equation
    1\. Here we want to gather all the possible values that ***X*** can take. To do
    that, we only need to calculate all the possible values that the tuple (*X*₁,
    *X*₂) can take. Once we know the value of *X*₁ and *X*₂, the value of *X*₃ can
    be calculated using the fact that *X*₃=*n*-*X*₁-*X*₂. Since *n*=5, *Xᵢ* can take
    an integer valuebetween 0 and 5*.* Listing 2 uses the method `meshgrid()` in `numpy`
    to get all the possible values of (*X*₁, *X*₂) and their corresponding value of
    *X*₃. Finally, the different values of (*X*₁, *X*₂, *X*₃) are stored in the array
    `X_mat`. We end up with some negative values for *X*₃. These negative values are
    impossible to take, but we don’t need to drop them since the joint PMF of a multinomial
    distribution with a negative value for any *Xᵢ* is simply zero.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now we can use `X_mat` to calculate the marginal PMF of *X*₁ at *X*₁=3\. To
    do that, we need to sum the joint PMF of all values of ***X*** in which *X*₁ is
    equal to 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We also know that the marginal distribution of *X*₁ is a binomial distribution
    with parameters *n* and *p*₁*.* So, we can use the PMF of a binomial distribution
    with parameters *n*=5 and *p*₁=0.5\. To calculate the PMF of the binomial distribution,
    we can use the object `binom` in `scipy.stat`. We calculate the value of this
    PMF at *X*₁=3, and it should give us the same result as the previous code snippet.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The result is almost the same but with a small difference due to the numerical
    errors. We can also approximate the marginal PMF of *X*₁ by sampling from the
    multinomial distribution. Listing 3 draws a sample of size 100,000 from the multinomial
    distribution in Equation 1 and plots the bar chart of the values of *X*₁ in that
    sample. The result is shown in Figure 4\. As this figure shows the bar chart of
    the sample matches the PDF of the binomial distribution with parameters *n*=5
    and *p*₁=0.5.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5b24b278425d0bddbadbebc7ec2abb4e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4
  prefs: []
  type: TYPE_NORMAL
- en: If a random variable with a multinomial distribution has only 3 elements (*k*=3),
    then we can easily plot its joint PMF in a 2d or 3d space. Listing 4 plots the
    joint PMF of the previous multinomial distribution in Figure 5 (top). The plot
    is a heatmap of *X*₁ versus *X*₂. We don’t need to include *X*₃ in this plot since
    its value depends on *X*₁ versus *X*₂ (*X*₃=*n*-*X*₁-*X*). The PMF of the binomial
    distribution of *X*₁ is also plotted in Figure 5 (bottom).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0315911b53f77b10d2bf1db799476456.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5 (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Each column of the heatmap at *X*₁=*x* contains the joint PMF of all values
    of ***X*** in which *X*₁ is equal to *x*. So, the marginal PMF of *X*₁ at *x*
    is equal to the sum of the column at *X*₁=*x* (Figure 5). Similarly, the marginal
    PMF of *X*₂ at *x* is equal to the sum of the row at *X*₂=*x.*
  prefs: []
  type: TYPE_NORMAL
- en: We can also use a 3d bar plot to visualize the joint PMF of the multinomial
    distribution in Equation 1\. As mentioned before we only need to include *X*₁
    and *X*₂ in the plot since the value of *X*₃ depends on them. Listing 5 uses the
    `pmf()` method to calculate the joint PMF of the arrays in `X_mat` and creates
    a 3d bar plot of these joint PMF values in Figure 6\. The marginal distributions
    of *X*₁ and *X*₂ are also shown in this figure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d33b8a6a054f9b32e433b817c81db30a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6
  prefs: []
  type: TYPE_NORMAL
- en: We can merge the first two elements of the multinomial distribution in Equation
    1 to get the new random vector
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ccdb1f8a57dafbe62876c6ca8e1a94f.png)'
  prefs: []
  type: TYPE_IMG
- en: And we know that this random vector has a multinomial distribution with parameters
    *n* and
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b50384006f499b737cf4d547e8ca268.png)'
  prefs: []
  type: TYPE_IMG
- en: Listing 6 approximates the marginal PMF of *X*₁+*X*₂ by sampling from the multinomial
    distribution. It draws a sample of size 100,000 from the multinomial distribution
    in Equation 86 and plots the bar chart of the values of *X*₁+*X*₂ in that sample.
    The result is shown in Figure 7, and you can see that the bar chart matches the
    PDF of a binomial distribution with parameters *n*=5 and *p*₁+*p*₂.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e9525bb33f72eca8ce42a7067aaf5c6f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 7 shows how sampling with replacement from a population of items with
    *k* different categories can result in a multinomial distribution (Figure 3).
    Here the population is represented by a list that has 3 unique elements: 1, 2,
    and 3\. The proportion of these elements is 0.5, 0.3, and 0.2\. We can randomly
    select 5 elements from this list and calculate the number of occurrences of 1,
    2, and 3 in that (we denote them by *x*₁, *x*₂, and *x*₃). Now the vector'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38549a3de678717d453ec1ebd4f12bbb.png)'
  prefs: []
  type: TYPE_IMG
- en: is a random variate of the multinomial distribution in Equation 1\. We can also
    call it a random sample of size 1 from that distribution. Please note that we
    could also use the `rvs()` method to generate this random variate. We can now
    repeat the same procedure *n* times to draw a random sample of size *n* from the
    multinomial distribution. Listing 7 plots the bar chart of two random samples
    in Figure 8 (blue bars). The size of these samples is 30 and 50000 respectively.
    The bar chart of the joint PMF of the multinomial distribution defined in Equation
    1 is also plotted in this figure (red bars). As the sample size increases, the
    shape of the sample bar chart gets closer to the shape of the bar chart of the
    joint PMF of the multinomial distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/72f1a508acbc4cf272cfadbe573c3a0e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8
  prefs: []
  type: TYPE_NORMAL
- en: 'In Listing 8, we compare the joint PMF of a random variable that has the multinomial
    distribution defined in Equation 1 with the joint PMF of the sum of *n* random
    vectors that have a multinomial distribution with parameters *n*=1 and ***p***.
    We first draw a sample of size 5000000 (`sample1`) from the multinomial distribution
    defined in Equation 1 which had these parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe9bc3af3e981e14ab2caef9f78eb551.png)'
  prefs: []
  type: TYPE_IMG
- en: Then we draw a sample of size 5000000 from 5 multinomial distributions with
    the same ***p*** and *n*=1\. We add these 5 samples together to have one sample
    of size 5000000 and store in `sample2`. In fact, this sample contains the values
    that the random vector
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/14c16ab241a7664c3d8d21d263270755.png)'
  prefs: []
  type: TYPE_IMG
- en: can take.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We can now compare the probability of getting a specific value in `sample1`
    and `sample2`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As you see the probabilities are very close, and the minor difference is due
    to the sample size. As the sample size goes to infinity, these probabilities tend
    to be the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned before, if we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25723813b57188dcbe469671c35beb31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then each *Xᵢ* has a binomial distribution with parameters *n* and *p*. So,
    using the properties of the binomial distribution we conclude that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c92c6d3bbdd0acd217cc270d544d75bb.png)'
  prefs: []
  type: TYPE_IMG
- en: We define the mean of the random vector ***X*** as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3243c2cfebba7f3ff54062e9231f825e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We also showed that the sum of some of the elements of ***X*** has a binomial
    distribution. So, for each pair of *i*, *j* (*i*, *j*=1…*k*, *i*≠*j*) *Xᵢ* + *Xⱼ*
    has a binomial distribution with parameters *n* and *pᵢ* + *pⱼ*. Hence, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5640dc1bdbd29453063d2d1a1e7835d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It can be shown that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba2228d70ce94cf5cfc8627ddddb04fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And it follows that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d428986f45302172444bbba5baa326d.png)'
  prefs: []
  type: TYPE_IMG
- en: Since *n*, *pᵢ,* and *pⱼ* are positive numbers, we conclude that the covariance
    of *Xᵢ* and *Xⱼ* is always a negative number. Let’s see why the correlation is
    negative. A negative correlation between two random variables means that when
    one of them is high, the other one tends to be low and when one of them increases,
    the other one tends to decrease. We know that *X*₁+*X*₂+ …+ *Xₖ*=*n*, so assuming
    that all other components of ***X*** (all *Xm* where *m* ≠ *i*, *j*) are constant,
    any increase in *Xᵢ* should result in a decrease in *Xⱼ* and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the covariance matrix of ***X*** as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6df9a1be2226f182707e589af3b8862f.png)'
  prefs: []
  type: TYPE_IMG
- en: So, the (*i, j*) element of the covariance matrix is the covariance of variables
    *Xᵢ* and *Xⱼ,* and the *i*th diagonal element gives the variance of *Xᵢ*. Based
    on the definition of covariance, we know that *Cov*(*Xᵢ*, *Xⱼ*)=*Cov*(*Xⱼ*, *Xᵢ*).
    So, the covariance matrix is a symmetric matrix in which the (*i*, *j*) element
    is equal to the (*j*, *i*) element.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can easily calculate the mean and covariance matrix for the multinomial
    distribution defined in Equation 1 using Python. The method `mean()` returns the
    mean vector defined in Equation 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The method `cov()` returns the covariance matrix (Equation 3):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In this article, we discussed the math behind the multinomial distribution and
    showed you how to implement it in Python. The multinomial distribution is widely
    used in science, engineering, and finance. It can be used in applications where
    there are more than two possible outcomes and the system cannot be modeled by
    a success-failure description.
  prefs: []
  type: TYPE_NORMAL
- en: 'I hope that you enjoyed reading this article. Please let me know if you have
    any questions or suggestions. All the Code Listings in this article are available
    for download as a Jupyter notebook from GitHub at: [https://github.com/reza-bagheri/probability_distributions/blob/main/multinomial_distribution.ipynb](https://github.com/reza-bagheri/probability_distributions/blob/main/multinomial_distribution.ipynb)'
  prefs: []
  type: TYPE_NORMAL
