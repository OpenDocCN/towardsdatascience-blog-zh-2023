- en: Three Fundamental Flaws In Common Reinforcement Learning Algorithms (And How
    To Fix Them)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/three-fundamental-flaws-in-common-reinforcement-learning-algorithms-and-how-to-fix-them-951160b7a207](https://towardsdatascience.com/three-fundamental-flaws-in-common-reinforcement-learning-algorithms-and-how-to-fix-them-951160b7a207)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Harness yourself against these shortcomings encountered in everyday RL algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wvheeswijk.medium.com/?source=post_page-----951160b7a207--------------------------------)[![Wouter
    van Heeswijk, PhD](../Images/9c996bccd6fdfb6d9aa8b50b93338eb9.png)](https://wvheeswijk.medium.com/?source=post_page-----951160b7a207--------------------------------)[](https://towardsdatascience.com/?source=post_page-----951160b7a207--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----951160b7a207--------------------------------)
    [Wouter van Heeswijk, PhD](https://wvheeswijk.medium.com/?source=post_page-----951160b7a207--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----951160b7a207--------------------------------)
    ·8 min read·Jan 30, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9dda11eec29ec887f349772c00f43653.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Varvara Grabova](https://unsplash.com/@santabarbara77?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning algorithms such as Q-learning and REINFORCE have been
    around for decades and their textbook implementations are still widely used. Unfortunately,
    they exhibit some fundamental flaws, which greatly increase the struggle of learning
    a good policy.
  prefs: []
  type: TYPE_NORMAL
- en: This article addresses three major shortcomings of classical Reinforcement Learning
    algorithms, along with solutions to overcome them.
  prefs: []
  type: TYPE_NORMAL
- en: I. Selecting overvalued actions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most RL algorithms use some notion of value functions to capture downstream
    rewards, many of them based on the well-known Q-learning algorithm. The mechanism
    driving Q-learning is that it **selects the action that yields the highest expected
    value**. Depending on initialization, this mechanism might get stuck at the first
    action that is tried, so we also select random actions with probability ϵ, typically
    set at 0.05 or so.
  prefs: []
  type: TYPE_NORMAL
- en: In the limit, we would explore each action infinitely often and the Q-values
    would converge to the true values. In practice, however, we work with limited
    samples and biased Q-values. As a result, **Q-learning consistently selects actions
    with overestimated values**!
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a setting in which we play two identical [slot machines](https://medium.com/towards-data-science/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e).
    Machine A happens to give above-average rewards in the early iteration, so its
    Q-value is higher and we keep playing A. As machine B is only rarely selected,
    it would take a long time before we figure out the Q-values are actually identical.
  prefs: []
  type: TYPE_NORMAL
- en: More generally speaking, value functions will always be imperfect, and RL has
    a penchant to perform overvalued actions. In a sense, RL “rewards” poor estimates,
    which is obviously not a desirable property.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5151c5cbcfb468c1ae5daf6599a9f596.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi-armed bandit problems clearly demonstrate the impact of selecting overvalued
    actions [Photo by [Bangyu Wang](https://unsplash.com/es/@bangyuwang?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]
  prefs: []
  type: TYPE_NORMAL
- en: Solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The problems of Q-learning can be traced back to the practice of sampling and
    updating with the same observations. We can decouple these steps by sampling with
    one policy and updating another. This is precisely what **Double Q-learning**
    (Van Hasselt, 2010) does.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26a17cf25f3c9d50f4235a38c3e19f53.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Double Q-learning samples actions with one network, but updates the Q-value
    with the output of the other network. This procedure decouples sampling and learning
    and combats overestimation. [source: Van Hasselt (2010)]'
  prefs: []
  type: TYPE_NORMAL
- en: More generally speaking, it is good practice to work with [**target networks**](/how-to-model-experience-replay-batch-learning-and-target-networks-c1350db93172).
    The target network is a periodical copy of the policy, used to generate target
    values that we train upon (rather than using exactly the same policy to generate
    both observation and target). This approach reduces the correlation between target
    and observation.
  prefs: []
  type: TYPE_NORMAL
- en: Another solution angle is to take into account the uncertainty of our Q-value
    estimates. Rather than solely tabulating the expected values of actions, we can
    also keep track of the variance of observations, indicating how for we might be
    off from the true value. Working with [**uncertainty bounds and knowledge gradients**](/seven-exploration-strategies-in-reinforcement-learning-you-should-know-8eca7dec503b)are
    two ways to achieve this purpose. Instead of simply selecting the action with
    the highest expected Q-value, we also consider how much we can *learn* from a
    new observation. This approach favors exploring actions with high uncertainty,
    while still sampling with intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/seven-exploration-strategies-in-reinforcement-learning-you-should-know-8eca7dec503b?source=post_page-----951160b7a207--------------------------------)
    [## Seven Exploration Strategies In Reinforcement Learning You Should Know'
  prefs: []
  type: TYPE_NORMAL
- en: Pure exploration and -exploitation, ϵ-greedy, Boltzmann exploration, optimistic
    initialization, confidence intervals…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/seven-exploration-strategies-in-reinforcement-learning-you-should-know-8eca7dec503b?source=post_page-----951160b7a207--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: II. Poor policy gradient updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Policy gradient algorithms have been in swing for decades, and are at the root
    of all modern actor-critic models. The vanilla policy gradient algorithms — e.g.,
    REINFORCE — rely on gradients to determine the direction of weight updates. A
    combination of high rewards and high gradients yield a strong update signal.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0f452ca259da52230dadd302edcc7084.png)'
  prefs: []
  type: TYPE_IMG
- en: Traditional policy gradient update function, updating policy weights θ based
    on objective function gradient ∇_θJ(θ) and step size α
  prefs: []
  type: TYPE_NORMAL
- en: The idea seems natural. If the slope of the reward function is steep, you take
    a big step in that direction. If it is small, there is no sense in performing
    big updates. Compelling as it may seem, this logic is also fundamentally flawed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb76f86045facb568f6e99b2e8037c78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Left: example of overshooting behavior, performing a large policy update that
    misses the reward peak. Right: example of stalling behavior, being stuck in a
    local optimum with gradients close to 0\. [image by author]'
  prefs: []
  type: TYPE_NORMAL
- en: A gradient only provides local information. It tells us how steep the slope
    is, but not how far to step in that direction; we might **overshoot**. Furthermore,
    policy gradients do not consider that a lack of gradient signal might get us **stuck
    on a sub-optimal plateau**.
  prefs: []
  type: TYPE_NORMAL
- en: To make matters worse, we cannot control this behavior by forcing the weight
    updates to be within a certain parameter region. In the figure below, for example,
    weight updates of the same magnitude have very different effects on the policy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a02d2bc8a6f52052e211ff468ad74b77.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of two Gaussian policy updates. Although both updates have the same
    size in terms of parameter space, the policy on the left is clearly affected much
    more than the one on the right [image by author]
  prefs: []
  type: TYPE_NORMAL
- en: Solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A simple start is to experiment with various learning algorithms. The traditional
    stochastic gradient descent (SGD) algorithm only considers first moments. Modern
    learning algorithms (e.g., **ADAM**) consider second moments, often substantially
    enhancing performance. Although not fully resolving the problem, the performance
    increase may be remarkable.
  prefs: []
  type: TYPE_NORMAL
- en: '**Entropy regularization** is a common way to prevent premature convergence
    of vanilla policy gradient algorithms. Loosely stated, entropy in RL is a metric
    for the unpredictability of action selection. Entropy regularization adds a bonus
    for exploring unknown actions, which is higher when we have learned less about
    the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/881d33b567b4fb05c37a9fd5e3dcc53f.png)'
  prefs: []
  type: TYPE_IMG
- en: Measure of entropy in Reinforcement Learning
  prefs: []
  type: TYPE_NORMAL
- en: More sophisticated extensions of policy gradient algorithms also consider second-order
    derivatives, which provide information on the local sensitivity of the function.
    At a plateau, we can safely take big steps without consequence. At a steep but
    curved slope, we would prefer cautious steps. Algorithms such as [**natural policy
    gradients**](https://medium.com/towards-data-science/natural-policy-gradients-in-reinforcement-learning-explained-2265864cf43c)**,**
    [**TRPO**](/trust-region-policy-optimization-trpo-explained-4b56bd206fc2) **and**
    [**PPO**](/proximal-policy-optimization-ppo-explained-abed1952457b) take into
    account the sensitivity towards updates, either explicitly or implicitly considering
    second-order derivatives. At the moment, PPO is the go-to policy gradient algorithm,
    striking a fine balance between ease of implementation, speed and performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4a8fed27fa04ca364cff2ee5ddc88057.png)'
  prefs: []
  type: TYPE_IMG
- en: Weight update scheme for natural policy gradients. The Fischer matrix F(θ) contains
    information about local sensitivity, generating dynamic weight updates.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/natural-policy-gradients-in-reinforcement-learning-explained-2265864cf43c?source=post_page-----951160b7a207--------------------------------)
    [## Natural Policy Gradients In Reinforcement Learning Explained'
  prefs: []
  type: TYPE_NORMAL
- en: Traditional policy gradient methods are inherently flawed. Natural gradients
    converge quicker and better, forming the…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/natural-policy-gradients-in-reinforcement-learning-explained-2265864cf43c?source=post_page-----951160b7a207--------------------------------)
    [](/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=post_page-----951160b7a207--------------------------------)
    [## Trust Region Policy Optimization (TRPO) Explained
  prefs: []
  type: TYPE_NORMAL
- en: The Reinforcement Learning algorithm TRPO builds upon natural policy gradient
    algorithms, ensuring updates remain…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=post_page-----951160b7a207--------------------------------)
    [](/proximal-policy-optimization-ppo-explained-abed1952457b?source=post_page-----951160b7a207--------------------------------)
    [## Proximal Policy Optimization (PPO) Explained
  prefs: []
  type: TYPE_NORMAL
- en: The journey from REINFORCE to the go-to algorithm in continuous control
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/proximal-policy-optimization-ppo-explained-abed1952457b?source=post_page-----951160b7a207--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: III. Underperformance off-policy learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Certain algorithms (e.g., these rooted in Q-learning) rely on *off-policy learning*,
    meaning that **updates may be performed with a different action than actually
    observed**. Whereas on-policy learning requires a tuple *(s,a,r,s’,a’)* — indeed,
    like its algorithm namesake SARSA— off-policy learning uses the best known action
    *a** instead of *a’.* Consequently, we only store *(s,a,r,s’)* for the weight
    updates and learn the policy independent of the agent’s actions.
  prefs: []
  type: TYPE_NORMAL
- en: Due to its setup, off-policy learning can re-use prior observations by drawing
    them from an **experience replay buffer,** which is particularly convenient when
    **creating observations is (computationally) expensive**. We simply feed state
    *s’* to our policy to obtain an action *a**, using the resulting value to update
    Q-values. The transition dynamics from *s* to *s’* do not need to be recomputed.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, even after extensively training an off-policy Reinforcement Learning
    algorithm on a large dataset, it often does not work nearly as well when deployed.
    Why is that?
  prefs: []
  type: TYPE_NORMAL
- en: The problem boils down to a common statistical caveat. The assumption is that
    the **training set is representative for the true data set.** When this is not
    the case — which it often isn’t, as updated policies generate different state-action
    pairs — the policy is fit to a dataset that does not reflect the environment the
    agent ultimately operates in.
  prefs: []
  type: TYPE_NORMAL
- en: Solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: True off-policy learning — e.g., learning good policies solely from a static
    dataset — might be fundamentally infeasible in Reinforcement Learning, as updating
    policies inevitably alters the probability of observing state-action pairs. As
    we cannot exhaustively explore the search space, we inevitably **extrapolate values
    towards unseen state-action pairs.**
  prefs: []
  type: TYPE_NORMAL
- en: The most common solution is to not train on a fully static dataset, but **continuously
    enrich the dataset** with observations generated under the new policy. It may
    help to also **remove older samples**, which no longer represent data generated
    under recent policies.
  prefs: []
  type: TYPE_NORMAL
- en: Another solution is **importance sampling**, which is essentially reweighs observations
    based on their likelihood of being generated under the present policy. For each
    observation, we can compute its ratio of the probability being generated under
    the original and present policy, making observations stemming from similar policies
    more likely to be drawn.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad2e71f9d0df5e643786d6da4cc73b93.png)'
  prefs: []
  type: TYPE_IMG
- en: Importance sampling considers the similarity between the original policy and
    the target policy, selecting observations generated under a policy similar to
    the present one with a higher probability.
  prefs: []
  type: TYPE_NORMAL
- en: If you continue to struggle to get your off-policy algorithm to perform well
    out-of-sample, a switch to an **on-policy algorithm** should be considered. Especially
    when generating new observations is cheap, the loss in sample efficiency might
    be offset by the enhanced policy quality.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-to-model-experience-replay-batch-learning-and-target-networks-c1350db93172?source=post_page-----951160b7a207--------------------------------)
    [## How To Model Experience Replay, Batch Learning and Target Networks'
  prefs: []
  type: TYPE_NORMAL
- en: A quick tutorial on three essential tricks for stable and successful Deep Q-learning,
    using TensorFlow 2.0
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-model-experience-replay-batch-learning-and-target-networks-c1350db93172?source=post_page-----951160b7a207--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article addressed three common flaws encountered in traditional RL algorithms,
    along with strategies to address them.
  prefs: []
  type: TYPE_NORMAL
- en: I. Overvalued actions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms based on value function approximation systematically select actions
    with overestimated values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: Use target networks to decrease correlation between target and observation (e.g.,
    as in Double Q-learning).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporate uncertainty of value estimates in action selection (e.g., uncertainty
    bounds, knowledge gradients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**II. Poor policy gradient updates**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Policy gradient algorithms often perform poor update steps, e.g., minor steps
    when stuck in a local optimum or large ones that overshoot and miss the reward
    peak.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: Use learning algorithm that includes, e.g., ADAM — which tracks momentum in
    addition to first-order gradients — instead of the standard Stochastic Gradient
    Descent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add an entropy bonus to the reward signal, encouraging more exploration of unknown
    regions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy algorithms that include second-order derivatives (either explicitly or
    implicitly), such as natural policy gradients, TRPO or PPO.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: III. Underperformance off-policy learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Problem:'
  prefs: []
  type: TYPE_NORMAL
- en: The experiences in the replay buffer may not be representative for out-of-sample
    experiences, such that values are incorrectly extrapolated and performance drops.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: Update replay buffer, adding new experiences and removing older ones.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform importance sampling to increase probability of selecting experiences
    stemming from policies closer to the target policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Switch to on-policy learning (if sampling observations is cheap).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Problem I: Overvalued actions'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hasselt, H. (2010). Double Q-learning. *Advances in neural information processing
    systems*, *23*.
  prefs: []
  type: TYPE_NORMAL
- en: Matiisen, Tambet (2015). Demystifying deep reinforcement learning. *Computational
    Neuroscience Lab.* Retrieved from neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem II: Poor policy gradient updates'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mahmood, A. R., Van Hasselt, H. P., & Sutton, R. S. (2014). Weighted importance
    sampling for off-policy learning with linear function approximation. *Advances
    in Neural Information Processing Systems*, *27*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cornell University Computational Optimization Open Textbook. (2021). ADAM.
    URL: [https://optimization.cbe.cornell.edu/index.php?title=Adam](https://optimization.cbe.cornell.edu/index.php?title=Adam)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem III: Underperformance off-policy learning'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fujimoto, S., Meger, D., & Precup, D. (2019, May). Off-policy deep reinforcement
    learning without exploration. In *International conference on machine learning*
    (pp. 2052–2062). PMLR.
  prefs: []
  type: TYPE_NORMAL
