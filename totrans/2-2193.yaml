- en: Understanding NeRFs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解 NeRFs
- en: 原文：[https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb](https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb](https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb)
- en: A massive breakthrough in scene representation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 场景表示的重大突破
- en: '[](https://wolfecameron.medium.com/?source=post_page-----2a082e13c6eb--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----2a082e13c6eb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2a082e13c6eb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2a082e13c6eb--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----2a082e13c6eb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----2a082e13c6eb--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----2a082e13c6eb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2a082e13c6eb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2a082e13c6eb--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----2a082e13c6eb--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2a082e13c6eb--------------------------------)
    ·11 min read·Apr 28, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2a082e13c6eb--------------------------------)
    ·11 min read·2023年4月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0f3a2a80515f0f2c516a9ede3117054f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f3a2a80515f0f2c516a9ede3117054f.png)'
- en: (Photo by [nuddle](https://unsplash.com/@nuddle?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/3d-scene?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: (照片由 [nuddle](https://unsplash.com/@nuddle?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/s/photos/3d-scene?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
- en: As we have seen with methods like [DeepSDF](https://cameronrwolfe.substack.com/p/3d-generative-modeling-with-deepsdf)
    [2] and [SRNs](https://cameronrwolfe.substack.com/p/scene-representation-networks)
    [4], encoding 3D objects and scenes within the weights of a feed-forward neural
    network is a memory-efficient, implicit representation of 3D data that is both
    accurate and high-resolution. However, the approaches we have seen so far are
    not quite capable of capturing realistic and complex scenes with sufficient fidelity.
    Rather, discrete representations (e.g., triangle meshes or voxel grids) produce
    a more accurate representation, assuming a sufficient allocation of memory.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们通过 [DeepSDF](https://cameronrwolfe.substack.com/p/3d-generative-modeling-with-deepsdf)
    [2] 和 [SRNs](https://cameronrwolfe.substack.com/p/scene-representation-networks)
    [4] 等方法所见，将 3D 对象和场景编码在前馈神经网络的权重中是一种内存高效的隐式 3D 数据表示，既准确又高分辨率。然而，到目前为止，我们看到的方法还无法以足够的保真度捕捉现实和复杂的场景。相反，离散表示（例如三角网格或体素网格）在内存分配充足的情况下，能产生更准确的表示。
- en: This changed with the proposal of Neural Radiance Fields (NeRFs) [1], which
    use a feed-forward neural network to model a continuous representation of scenes
    and objects. The representation used by NeRFs, called a radiance field, is a bit
    different from [prior](https://cameronrwolfe.substack.com/i/94634004/signed-distance-functions)
    [proposals](https://cameronrwolfe.substack.com/i/94842305/occupancy-functions).
    In particular, NeRFs map a five-dimensional coordinate (i.e., spatial location
    and viewing direction) to a volume density and view-dependent RGB color. By accumulating
    this density and appearance information across different viewpoints and locations,
    we can render photorealist, novel views of a scene.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这在 Neural Radiance Fields (NeRFs) [1] 的提议下发生了改变，NeRFs 使用前馈神经网络来建模场景和物体的连续表示。NeRFs
    使用的表示称为辐射场，与 [先前](https://cameronrwolfe.substack.com/i/94634004/signed-distance-functions)
    [提案](https://cameronrwolfe.substack.com/i/94842305/occupancy-functions) 有些不同。特别是，NeRFs
    将五维坐标（即空间位置和视角方向）映射到体积密度和视角相关的 RGB 颜色。通过在不同视角和位置上累积这些密度和外观信息，我们可以渲染出*逼真的*新视角场景。
- en: Like [SRNs](https://cameronrwolfe.substack.com/p/scene-representation-networks)
    [4], NeRFs can be trained using only a set of images (along with their associated
    [camera poses](https://cameronrwolfe.substack.com/i/97472888/background)) of an
    underlying scene. Compared with prior approaches, NeRF renderings are better both
    qualitatively and quantitatively. Notably, NeRFs can even capture complex effects
    such as view-dependent reflections on an object’s surface. By modeling scenes
    implicitly in the weights of a feed-forward neural network, *we match the accuracy
    of discrete scene representations without prohibitive memory costs*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 像[SRNs](https://cameronrwolfe.substack.com/p/scene-representation-networks)
    [4]一样，NeRFs可以仅使用一组图像（以及它们相关的[相机姿态](https://cameronrwolfe.substack.com/i/97472888/background)）对底层场景进行训练。与之前的方法相比，NeRF渲染在质量和数量上都更好。值得注意的是，NeRFs甚至可以捕捉复杂效果，如物体表面的视角依赖反射。通过在前馈神经网络的权重中隐式建模场景，*我们在不需要过多内存成本的情况下实现了离散场景表示的准确性*。
- en: '![](../Images/ef9dd251e54b4cc3a193203106e1d9b3.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef9dd251e54b4cc3a193203106e1d9b3.png)'
- en: (from [1])
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: （摘自[1]）
- en: '**why is this paper important?** This post is part of my series on deep learning
    for 3D shapes and scenes. NeRFs were a revolutionary proposal in this area, as
    they enable incredibly accurate 3D reconstructions of scene from arbitrary viewpoints.
    The quality of scene representations produced by NeRFs is incredible, as we will
    see throughout the remainder of this post.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么这篇论文很重要？** 本帖子是我关于3D形状和场景深度学习系列的一部分。NeRFs在这一领域是一个革命性的提案，因为它们能够从任意视角实现极其准确的3D重建。NeRFs生成的场景表示质量非常高，我们将在本帖的其余部分看到这一点。'
- en: Background
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: 'Most of the background concepts needed to understand NeRFs have been covered
    in prior posts on this topic, including:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 理解NeRFs所需的大部分背景概念已在之前的帖子中涵盖，包括：
- en: Feed-forward neural networks [[link](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)]
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈神经网络 [[link](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)]
- en: Representing 3D objects [[link](https://cameronrwolfe.substack.com/i/94634004/representing-d-shapes)]
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示3D对象 [[link](https://cameronrwolfe.substack.com/i/94634004/representing-d-shapes)]
- en: Problems with discrete representations [[link](https://cameronrwolfe.substack.com/i/94842305/shortcomings-of-d-shape-representations)]
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离散表示的问题 [[link](https://cameronrwolfe.substack.com/i/94842305/shortcomings-of-d-shape-representations)]
- en: We only need to cover a few more background concepts before going over how NeRFs
    work.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍NeRFs的工作原理之前，我们只需要覆盖几个更多的背景概念。
- en: Position Encodings
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 位置编码
- en: Instead of directly using `[x, y, z]` coordinates as input to a neural network,
    NeRFs convert each of these coordinates into higher-dimensional positional embeddings.
    We have discussed positional embeddings in previous posts on the [transformer
    architecture](https://cameronrwolfe.substack.com/i/76273144/berts-architecture),
    as positional embeddings are needed to provide a notion of token ordering and
    position to self-attention modules.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: NeRFs不是直接将`[x, y, z]`坐标作为输入传递给神经网络，而是将这些坐标转换为高维位置嵌入。我们在之前关于[transformer架构](https://cameronrwolfe.substack.com/i/76273144/berts-architecture)的帖子中讨论过位置嵌入，因为位置嵌入用于向自注意力模块提供令牌排序和位置的概念。
- en: '![](../Images/8db5978f193a85ea178779aff90be165.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8db5978f193a85ea178779aff90be165.png)'
- en: (from [1])
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: （摘自[1]）
- en: Put simply, positional embeddings take a scalar number as input (e.g., a coordinate
    value or an index representing position in a sequence) and produce a higher-dimensional
    vector as output. We can either learn these embeddings during training or use
    a fixed function to generate them. For NeRFs, we use the function shown above,
    which takes a scalar `p` as input and produces a `2L`-dimensional position encoding
    as output.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，位置嵌入将一个标量数作为输入（例如，坐标值或表示序列中位置的索引），并产生一个高维向量作为输出。我们可以在训练期间学习这些嵌入，或者使用固定函数生成它们。对于NeRFs，我们使用上面显示的函数，该函数将一个标量`p`作为输入，并产生一个`2L`维的位置编码作为输出。
- en: Other Stuff
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他内容
- en: There are a few other (possibly) unfamiliar terms that we may encounter in this
    overview. Let’s quickly clarify them now.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本概述中，我们可能会遇到一些其他（可能）不熟悉的术语。让我们现在快速澄清这些术语。
- en: '**end-to-end training.** If we say that a neural architecture can be learned
    “end-to-end”, this just means that all components of a system are differentiable.
    As a result, when we compute the output for some data and apply our loss function,
    we can differentiate through the entire system (i.e., end-to-end) and train it
    with gradient descent!'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**端到端训练。** 如果我们说一个神经架构可以“端到端”学习，这仅仅意味着系统的所有组件都是可微分的。因此，当我们为某些数据计算输出并应用我们的损失函数时，我们可以通过整个系统（即，端到端）进行微分，并用梯度下降进行训练！'
- en: Not all systems can be trained end-to-end. For example, if we are modeling tabular
    data, we might perform a feature extraction process (e.g., one-hot encoding),
    then train a machine learning model on top of these features. Because the feature
    extraction process is hand-crafted and not differentiable, we cannot train the
    system end-to-end!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有系统都可以进行端到端训练。例如，如果我们正在建模表格数据，我们可能会执行特征提取过程（例如，一热编码），然后在这些特征上训练机器学习模型。由于特征提取过程是手工设计的且不可微分，我们不能对系统进行端到端训练！
- en: '**Lambertian reflectance.** This term was completely unfamiliar to me prior
    to reading about NeRFs. Lambertian reflectance refers to how reflective an object’s
    surface is. If an object has a matte surface that does not change when viewed
    from different angles, we say this object is Lambertian. Alternatively, a “shiny”
    object that reflects light differently based on the angle from which it is viewed
    would be called non-Lambertian.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**朗伯特反射。** 在阅读 NeRFs 之前，这个术语对我完全陌生。朗伯特反射指的是物体表面的反射特性。如果一个物体的表面是哑光的，并且从不同角度看不会改变，我们称这个物体是朗伯特型的。另一方面，一个“光亮”的物体，其反射光线的方式根据观察角度不同而变化，则被称为非朗伯特型的。'
- en: Modeling NeRFs
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模 NeRFs
- en: '![](../Images/1521cf4ab301fe381bfcc88c3cbe3a69.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1521cf4ab301fe381bfcc88c3cbe3a69.png)'
- en: (from [1])
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: 'The high-level process for rendering scene viewpoints with NeRFs proceeds as
    follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 NeRF 渲染场景视角的高层次过程如下：
- en: Generate samples of 3D points and viewing directions for a scene using a [Ray
    Marching](https://michaelwalczyk.com/blog-ray-marching.html) approach.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 [射线行进](https://michaelwalczyk.com/blog-ray-marching.html) 方法生成场景的 3D 点和视角方向样本。
- en: Provide the points and viewing directions as input to a feed-forward neural
    network to produce color and density output.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将点和视角方向作为输入提供给前馈神经网络，以生成颜色和密度输出。
- en: Perform volume rendering to accumulate colors and densities from the scene into
    a 2D image.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行体积渲染，将场景中的颜色和密度累积到 2D 图像中。
- en: We will now explain each component of this process in more detail.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将详细解释这个过程的每个组件。
- en: '**radiance fields.** As mentioned before, NeRFs model a 5D vector-valued (i.e.,
    meaning the function outputs multiple values) function called a radiance field.
    The input to this function is an `[x, y, z]` spatial location and a 2D viewing
    direction. The viewing direction has two dimensions, corresponding to the two
    angles that can be used to represent a direction in 3D space; see below.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**辐射场。** 如前所述，NeRFs 模型是一个 5D 向量值（即，意味着函数输出多个值）函数，称为辐射场。该函数的输入是一个 `[x, y, z]`
    空间位置和一个 2D 视角方向。视角方向有两个维度，对应于表示 3D 空间中方向的两个角度；见下文。'
- en: '![](../Images/1efbb045b9c31cbf8666d3815f584902.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1efbb045b9c31cbf8666d3815f584902.png)'
- en: Directions in 3D space can be represented with two angles.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 空间中的方向可以用两个角度表示。
- en: In practice, the viewing direction is just represented as a 3D [cartesian](https://en.wikipedia.org/wiki/Cartesian_coordinate_system)
    [unit vector](https://en.wikipedia.org/wiki/Unit_vector).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，视角方向仅表示为一个 3D [笛卡尔](https://en.wikipedia.org/wiki/Cartesian_coordinate_system)
    [单位向量](https://en.wikipedia.org/wiki/Unit_vector)。
- en: 'The output of this function has two components: volume density and color. The
    color is simply an [RGB value](https://en.wikipedia.org/wiki/RGB_color_model).
    However, this value is view-dependent, meaning that the color output might change
    given a different viewing direction as input! Such a property allows NeRFs to
    capture reflections and other view-dependent appearance effects. In contrast,
    volume density is only dependent upon spatial location and captures opacity (i.e.,
    how much light accumulates as it passes through that position).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的输出有两个组成部分：体积密度和颜色。颜色只是一个 [RGB 值](https://en.wikipedia.org/wiki/RGB_color_model)。然而，这个值是视角依赖的，这意味着给定不同的视角方向作为输入，颜色输出可能会变化！这种特性使
    NeRFs 能够捕捉反射和其他视角依赖的外观效果。相比之下，体积密度仅依赖于空间位置，捕捉不透明度（即，光线通过该位置时积累的程度）。
- en: '![](../Images/827472099516f4376326df3d5cca1582.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/827472099516f4376326df3d5cca1582.png)'
- en: NeRFs model radiance fields with feed-forward neural networks (from [1])
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: NeRF使用前馈神经网络建模辐射场（来自[1]）
- en: '**the neural network.** In [1], we model radiance fields with a feed-forward
    neural network, which takes a 5D input and is trained to produce the corresponding
    color and volume density as output; see above. Recall, however, that color is
    view-dependent and volume density is not. To account for this, we first pass the
    input 3D coordinate through several feed-forward layers, which produce both the
    volume density and a feature vector as output. This feature vector is then concatenated
    with the viewing direction and passed through an extra feed-forward layer to predict
    the view-dependent, RGB color; see below.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经网络。** 在[1]中，我们用前馈神经网络建模辐射场，该网络接收5D输入并训练以生成相应的颜色和体积密度作为输出；见上文。然而，请记住，颜色是视角相关的，而体积密度则不是。为了解决这个问题，我们首先将输入的3D坐标通过几个前馈层，这些层产生体积密度和特征向量作为输出。然后，将这个特征向量与视角方向连接，并通过一个额外的前馈层来预测视角相关的RGB颜色；见下文。'
- en: '![](../Images/a3b8c9ef5a75ed6dbfc690b89e1ef431.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3b8c9ef5a75ed6dbfc690b89e1ef431.png)'
- en: Feed-forward architecture for NeRF.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: NeRF的前馈架构。
- en: '**volume rendering (TL;DR).** Volume rendering is too complex of a topic to
    cover here in-depth, but we should know the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**体积渲染（简要说明）。** 体积渲染是一个太复杂的话题，无法在这里深入讨论，但我们应该了解以下内容：'
- en: It can produce an image of an underlying scene from samples of discrete data
    (e.g., color and density values).
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它可以从离散数据样本（例如颜色和密度值）生成底层场景的图像。
- en: It is differentiable.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它是可微的。
- en: For those interested in more details on volume rendering, check out the explanation
    [here](https://www.heavy.ai/technical-glossary/volume-rendering) and Section 4
    of [1].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些对体积渲染的更多细节感兴趣的人，请查看[这里](https://www.heavy.ai/technical-glossary/volume-rendering)的解释和[1]的第4节。
- en: '**the big picture.** NeRFs use the feed-forward network to generate relevant
    information about a scene’s geometry and appearance along numerous different camera
    rays (i.e., a line in 3D space moving from a specific camera viewpoint out into
    a scene along a certain direction), then use rendering to aggregate this information
    into a 2D image.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**大局观。** NeRF使用前馈网络生成关于场景几何和外观的相关信息，这些信息沿着众多不同的相机光线（即从特定相机视点向场景中的某个方向移动的3D空间中的一条线）进行传递，然后使用渲染将这些信息汇聚成一张2D图像。'
- en: Notably, both of these component are differentiable, which means we can train
    this entire system end-to-end! Given a set of images with corresponding camera
    poses, we can train a NeRF to generate novel scene viewpoints by just generating/rendering
    known viewpoints and using [(stochastic) gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)
    to minimize the error between the NeRF’s output and the actual image; see below.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，这两个组件都是可微的，这意味着我们可以端到端地训练整个系统！给定一组带有对应相机姿态的图像，我们可以通过仅生成/渲染已知视角，并使用[(随机)梯度下降](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)来最小化NeRF输出与实际图像之间的误差，来训练NeRF以生成新的场景视点；见下文。
- en: '![](../Images/be05b56459b285dfbba9f7ffb73f4788.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be05b56459b285dfbba9f7ffb73f4788.png)'
- en: (from [1])
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[1]）
- en: '**a few extra details.** We now understand most of the components of a NeRF.
    However, the approach that we’ve described up to this point is actually shown
    in [1] to be inefficient and generally bad at representing scenes. To improve
    the model, we can:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**一些额外细节。** 我们现在了解了NeRF的大部分组件。然而，迄今为止我们描述的方法在[1]中实际上被证明是低效的，并且通常不能很好地表示场景。为了改进模型，我们可以：'
- en: Replace spatial coordinates (for both the spatial location and the viewing direction)
    with positional embeddings.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用位置嵌入替换空间坐标（包括空间位置和视角方向）。
- en: Adopt a hierarchical sampling approach for volume rendering.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 采用层次采样方法进行体积渲染。
- en: By using positional embeddings, we map the feed-forward network’s inputs (i.e.,
    the spatial location and viewing direction coordinates) to a higher-dimension.
    Prior work showed that such an approach, as opposed to using spatial or directional
    coordinates as input directly, allows neural networks to better model high-frequency
    (i.e., changing a lot/quickly) features of a scene [5]. This makes the quality
    of the NeRF’s output much better; see below.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用位置嵌入，我们将前馈网络的输入（即空间位置和视角方向坐标）映射到更高维度。先前的工作表明，与直接使用空间或方向坐标作为输入相比，这种方法使神经网络更好地建模场景的高频（即变化很大/很快）特征[5]。这使得NeRF的输出质量大大提高；见下文。
- en: '![](../Images/27f8532dd0258fc41e28f6cb223ff1d4.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27f8532dd0258fc41e28f6cb223ff1d4.png)'
- en: (from [1])
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: The hierarchical sampling approach used by NeRF makes the rendering process
    more efficient by only sampling (and passing through the feed-forward neural network)
    locations and viewing directions that are likely to impact the final rendering
    result. This way, we only evaluate the neural network where needed and avoid wasting
    computation on empty or occluded areas.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: NeRF 使用的分层采样方法使渲染过程更高效，只对可能影响最终渲染结果的地点和视角方向进行采样（并通过前馈神经网络传递）。这样，我们仅在需要的地方评估神经网络，避免在空白或遮挡区域浪费计算。
- en: Show us some results!
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 给我们展示一些结果吧！
- en: NeRFs are trained to represent only a single scene at once and are evaluated
    across several datasets with synthetic and real objects.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: NeRFs 被训练为一次仅表示一个场景，并在多个数据集上进行评估，这些数据集包括合成和真实对象。
- en: '![](../Images/e16bcb21169bb214ec51806f5e016d3b.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e16bcb21169bb214ec51806f5e016d3b.png)'
- en: (from [1])
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: As shown in the table above, NeRFs outperform alternatives like [SRNs](https://cameronrwolfe.substack.com/p/scene-representation-networks)
    [4] and [LLFF](https://cameronrwolfe.substack.com/p/local-light-field-fusion)
    [6] by a significant, quantitative margin. Beyond quantitative results, it’s really
    informative to look visually at the outputs of a NeRF compared to alternatives.
    First, we can immediately tell that using positional encodings and modeling colors
    in a view-dependent manner is really important; see below.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如上表所示，NeRFs 显著超越了 [SRNs](https://cameronrwolfe.substack.com/p/scene-representation-networks)
    [4] 和 [LLFF](https://cameronrwolfe.substack.com/p/local-light-field-fusion) [6]
    等替代方法。除了定量结果之外，与其他方法的输出进行视觉对比也非常有用。首先，我们可以立即看出，使用位置编码和以视角依赖的方式建模颜色是非常重要的；见下文。
- en: '![](../Images/46a74f559a0638eb5dc6a880d66d8b38.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46a74f559a0638eb5dc6a880d66d8b38.png)'
- en: (from [1])
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: One improvement that we will immediately notice is that NeRFs — because they
    model colors in a view-dependent fashion — can capture complex reflections (i.e.,
    [non-Lambertian features](https://en.wikipedia.org/wiki/Lambertian_reflectance))
    and view-dependent patterns in a scene. Plus, NeRFs are capable of modeling intricate
    aspects of underlying geometries with surprising precision; see below.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会立即注意到的一个改进是，NeRFs — 因为它们以视角依赖的方式建模颜色 — 能够捕捉场景中的复杂反射（即[非朗伯特特征](https://en.wikipedia.org/wiki/Lambertian_reflectance)）和视角依赖的模式。此外，NeRFs
    还能够以惊人的精度建模底层几何体的复杂方面；见下文。
- en: '![](../Images/d60db20c48445b975be02c8b5a2c2397.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d60db20c48445b975be02c8b5a2c2397.png)'
- en: (from [1])
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: The quality of NeRF scene representations is most evident when they are viewed
    as a video. As can be seen in the video below, NeRFs model the underlying scene
    with impressive accuracy and consistency between different viewpoints.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: NeRF 场景表示的质量在以视频形式查看时最为明显。正如下面的视频所示，NeRFs 以令人印象深刻的准确性和一致性建模了底层场景，并在不同视角之间保持一致。
- en: For more examples of the photorealistic scene viewpoints that can be generated
    with NeRF, I highly recommend checking out the project website linked [here](https://www.matthewtancik.com/nerf)!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取更多可以使用 NeRF 生成的逼真场景视角的示例，我强烈建议查看[这里](https://www.matthewtancik.com/nerf)链接的项目网站！
- en: Takeaways
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要结论
- en: As we can see in the evaluation, NeRFs were a massive breakthrough in scene
    representation quality. As a result, the technique gained a lot of popularity
    within the artificial intelligence and computer vision research communities. The
    potential applications of NeRF (e.g., virtual reality, robotics, etc.) are nearly
    endless due to the quality of its scene representations. The main takeaways are
    listed below.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在评估中所见，NeRFs 在场景表示质量上取得了巨大的突破。因此，这项技术在人工智能和计算机视觉研究社区中获得了极大的关注。由于其场景表示的质量，NeRF
    的潜在应用（例如虚拟现实、机器人等）几乎是无穷无尽的。主要结论列在下方。
- en: '![](../Images/cd47d9e7088c4a75273921599f1eca08.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd47d9e7088c4a75273921599f1eca08.png)'
- en: (from [1])
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: '**NeRFs capture complex details.** With NeRFs, we are able to capture fine-grained
    details within a scene, such as the rigging material within a ship; see above.
    Beyond geometric details, NeRFs can also handle non-Lambertian effects (i.e.,
    reflections and changes in color based on viewpoint) due to their modeling of
    color in a view-dependent manner.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**NeRFs 捕捉复杂细节。** 使用 NeRFs，我们能够捕捉场景中的细粒度细节，例如船只中的装配材料；见上文。除了几何细节，NeRFs 还能处理非朗伯特效应（即反射和基于视角的颜色变化），因为它们以视角依赖的方式建模颜色。'
- en: '**we need smart sampling.** All approaches to modeling 3D scenes that we have
    seen so far use neural networks to model a function on 3D space. These neural
    networks are typically evaluated at every spatial location and orientation within
    the volume of space being considered, which can be quite expensive if not handled
    properly. For NeRFs, we use a hierarchical sampling approach that only evaluates
    regions that are likely to impact the final, rendered image, which drastically
    improves sample efficiency. Similar approaches are adopted by prior work; e.g.,
    [ONets](https://cameronrwolfe.substack.com/p/shape-reconstruction-with-onets)
    [3] use an [octree](https://en.wikipedia.org/wiki/Octree)-based [hierarchical
    sampling approach](https://cameronrwolfe.substack.com/i/94842305/occupancy-networks)
    to extract object representations more efficiently.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们需要智能采样。** 到目前为止，我们见过的所有3D场景建模方法都使用神经网络在3D空间上建模一个函数。这些神经网络通常会在所考虑的空间体积内的每个空间位置和方向上进行评估，如果处理不当，可能会非常昂贵。对于NeRF，我们使用一种分层采样方法，仅评估可能影响最终渲染图像的区域，这大大提高了采样效率。类似的方法也被先前的工作采用；例如，
    [ONets](https://cameronrwolfe.substack.com/p/shape-reconstruction-with-onets)
    [3] 使用基于 [octree](https://en.wikipedia.org/wiki/Octree) 的 [分层采样方法](https://cameronrwolfe.substack.com/i/94842305/occupancy-networks)
    更高效地提取对象表示。'
- en: '**positional embeddings are great.** So far, most of the scene representation
    methods we have seen pass coordinate values directly as input to feed-forward
    neural networks. With NeRFs, we see that positionally embedding these coordinates
    is much better. In particular, mapping coordinates to a higher dimension seems
    to allow the neural network to capture high-frequency variations in scene geometry
    and appearance, which makes the resulting scene renderings much more accurate
    and consistent across views.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**位置嵌入效果很好。** 到目前为止，我们见过的大多数场景表示方法都直接将坐标值作为输入传递给前馈神经网络。通过NeRF，我们发现将这些坐标进行位置嵌入要好得多。特别是，将坐标映射到更高维度似乎允许神经网络捕捉场景几何和外观的高频变化，这使得结果场景渲染更加准确且视角一致。'
- en: '**still saving memory.** NeRFs implicitly model a continuous representation
    of the underlying scene. This representation can be evaluated at arbitrary precision
    and has a fixed memory cost — we just need to store the parameters of the neural
    network! As a result, NeRFs yield accurate, high-resolution scene representations
    without using a ton of memory.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**仍然节省内存。** NeRF隐式地建模了基础场景的连续表示。这种表示可以在任意精度下进行评估，并且具有固定的内存成本——我们只需存储神经网络的参数！因此，NeRF可以在不占用大量内存的情况下提供准确的高分辨率场景表示。'
- en: “Crucially, our method overcomes the prohibitive storage costs of discretized
    voxel grids when modeling complex scenes at high-resolutions.” *— from [1]*
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “至关重要的是，我们的方法克服了在高分辨率下建模复杂场景时离散体素网格的高昂存储成本。” *— 来源于 [1]*
- en: '**limitations.** Despite significantly advancing state-of-the-art, NeRFs are
    not perfect — there is room for improvement in representation quality. However,
    the main limitation of NeRFs is that they only model a single scene at a time
    and are expensive to train (i.e., 2 days on a single GPU for each scene). It will
    be interesting to see how future advances in this area can find more efficient
    methods of generating NeRF-quality scene representations.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**局限性。** 尽管NeRF在先进技术方面取得了显著进展，但它们并不完美——表示质量仍有改进空间。然而，NeRF的主要局限性在于它们每次只能建模一个场景，并且训练成本高（即，每个场景在单个GPU上需要2天）。未来在这一领域的进展如何找到更高效的生成NeRF质量场景表示的方法将会非常有趣。'
- en: Closing remarks
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/) and PhD student at Rice
    University. I study the empirical and theoretical foundations of deep learning.
    You can also check out my [other writings](https://medium.com/@wolfecameron) on
    medium! If you liked it, please follow me on [twitter](https://twitter.com/cwolferesearch)
    or subscribe to my [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/),
    where I help readers build a deeper understanding of topics in deep learning research
    via understandable overviews of popular papers on that topic.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢你阅读这篇文章。我是 [Cameron R. Wolfe](https://cameronrwolfe.me/)，[Rebuy](https://www.rebuyengine.com/)
    的人工智能总监，也是莱斯大学的博士生。我研究深度学习的经验和理论基础。你也可以查看我在 medium 上的 [其他文章](https://medium.com/@wolfecameron)！如果你喜欢这篇文章，请关注我的
    [twitter](https://twitter.com/cwolferesearch) 或订阅我的 [Deep (Learning) Focus 新闻通讯](https://cameronrwolfe.substack.com/)，在这里我通过对该领域流行论文的易懂概述，帮助读者深入理解深度学习研究中的主题。
- en: Bibliography
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Mildenhall, Ben, et al. “Nerf: Representing scenes as neural radiance fields
    for view synthesis.” *Communications of the ACM* 65.1 (2021): 99–106.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Mildenhall, Ben 等人。“Nerf：将场景表示为用于视图合成的神经辐射场。” *ACM 通讯* 65.1 (2021): 99–106。'
- en: '[2] Park, Jeong Joon, et al. “Deepsdf: Learning continuous signed distance
    functions for shape representation.” *Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition*. 2019.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Park, Jeong Joon 等人。“Deepsdf：学习用于形状表示的连续符号距离函数。” *IEEE/CVF 计算机视觉与模式识别会议论文集*。2019年。'
- en: '[3] Mescheder, Lars, et al. “Occupancy networks: Learning 3d reconstruction
    in function space.” *Proceedings of the IEEE/CVF conference on computer vision
    and pattern recognition*. 2019.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Mescheder, Lars 等人。“占用网络：在函数空间中学习 3d 重建。” *IEEE/CVF 计算机视觉与模式识别会议论文集*。2019年。'
- en: '[4] Sitzmann, Vincent, Michael Zollhöfer, and Gordon Wetzstein. “Scene representation
    networks: Continuous 3d-structure-aware neural scene representations.” *Advances
    in Neural Information Processing Systems* 32 (2019).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Sitzmann, Vincent、Michael Zollhöfer 和 Gordon Wetzstein。“场景表示网络：连续的 3d 结构感知神经场景表示。”
    *神经信息处理系统进展* 32 (2019)。'
- en: '[5] Rahaman, Nasim, et al. “On the spectral bias of neural networks.” *International
    Conference on Machine Learning*. PMLR, 2019.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Rahaman, Nasim 等人。“神经网络的谱偏差。” *国际机器学习会议*。PMLR，2019年。'
- en: '[6] Mildenhall, Ben, et al. “Local light field fusion: Practical view synthesis
    with prescriptive sampling guidelines.” *ACM Transactions on Graphics (TOG)* 38.4
    (2019): 1–14.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Mildenhall, Ben 等人。“局部光场融合：带有规定性采样指南的实用视图合成。” *ACM 图形学交易 (TOG)* 38.4 (2019):
    1–14。'
