- en: 'Understanding Gradient Boosting: A Data Scientist’s Guide'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understanding-gradient-boosting-a-data-scientists-guide-f5e0e013f441](https://towardsdatascience.com/understanding-gradient-boosting-a-data-scientists-guide-f5e0e013f441)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/ddead02c0234cca8db16b03529386445.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Midjourney
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://louis-chan.medium.com/?source=post_page-----f5e0e013f441--------------------------------)[![Louis
    Chan](../Images/6d8df9a478e929dd521059631f26e081.png)](https://louis-chan.medium.com/?source=post_page-----f5e0e013f441--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f5e0e013f441--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f5e0e013f441--------------------------------)
    [Louis Chan](https://louis-chan.medium.com/?source=post_page-----f5e0e013f441--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f5e0e013f441--------------------------------)
    ·10 min read·Feb 7, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradient boosting machine (GBM) is one of the most significant advances in
    machine learning and data science that has enabled us as practitioners to use
    ensembles of models to best many domain-specific problems. While this tool is
    widely available in python packages like `scikit-learn` and `xgboost`, as a data
    scientist, we should always look into the theory and mathematics of the model
    instead of using it as a black box. In this blog, we will dive into the following
    areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Different backing concepts of GBM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step-by-step illustrated to recreate GBM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pro’s and Con’s
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/babd686e12a9031bbac988669119ad87.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s jump into it — Image from [GIPHY](https://giphy.com/gifs/14uzPzKMOuVIPu)
  prefs: []
  type: TYPE_NORMAL
- en: Fundamentals of Gradient Boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 1\. Weak learners and ensemble learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Weak learners and ensemble learning are the two key concepts that make gradient
    boosting work. A weak learner is a model that is only slightly better than random
    guessing. Combined with many other weak learners, they can form a robust ensemble
    model to make accurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Too wordy, too complicated
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Okay, imagine we are playing 10,000-piece jigsaw with two other friends. (They
    must be excellent friends to sign up for this) Each of us is responsible for piecing
    one of the four quadrants. Although we might only be able to solve a small piece
    of the puzzle on our own, when we work together as a team, we can quickly complete
    the entire puzzle.
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, every one of us is a weak learner, and the team of four is
    the ensemble model. Like how we focus on our quadrant, each weak learner in an
    ensemble model is good at predicting based on certain features and characteristics.
    When all four of us weak learners come together to share our thoughts on whether
    one of the pieces belongs to a quadrant, the ensemble model can pump out more
    accurate predictions than we can individually.
  prefs: []
  type: TYPE_NORMAL
- en: 'In essence, weak learners and ensemble learning are the power of collective
    intelligence. Or as an old saying goes:'
  prefs: []
  type: TYPE_NORMAL
- en: The whole is greater than the sum of its parts.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 2\. Additive Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Gradient Boosting algorithms, weak learner models are add to the ensemble
    iteratively. It will almost look like a Taylor Approximation where the final value
    is predicted using a rough estimate corrected by a series of correction terms.
    As each of the weak learners will contribute one correction terms, this makes
    GBM very flexible in terms of adding models when prediction results indicates
    overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8035aa728960dfd581bc152a68ed333c.png)'
  prefs: []
  type: TYPE_IMG
- en: Taylor Expansion for e^x starts with 1 and get iteratively corrected — Image
    by Author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f03110eaceea6be85a989d4dad4da3e2.png)'
  prefs: []
  type: TYPE_IMG
- en: GBM starts with a baseline and get iteratively corrected by additive models
    — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Loss Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we say a model gets iteratively corrected or improved, we have got to understand
    that not all improvements are equal. For example, me and my brother’s exam scores
    have improved from 10 to 20 and from 80 to 90 respectively out of a full score
    of 100\. Although both of us has improved for 10 points, can we say that the improvements
    are the same?
  prefs: []
  type: TYPE_NORMAL
- en: Here comes loss functions.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: While loss functions are just mathematical ways for measuring the difference
    between two values. In the context of machine learning, it can be used as a scorecard
    for measuring the difference between predicted value the actual value, and hence
    for evaluating model performance. The more significant the loss, the worse our
    model performs.
  prefs: []
  type: TYPE_NORMAL
- en: In our example above, we can say that 10-point improvement from 10 to 20 is
    more significant if we consider the percentage improvement, or it can also be
    less significant if we consider how far our scores are from the full score of
    100.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e33919545929e1ad848b90675d113d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Loss functions on our exam example — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on how we define our model to be successful, we might want to choose
    different loss functions. Some common choices are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**MSE (Mean Square Error):** Commonly used for regression models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MAE (Mean Absolute Error)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log Loss (Cross Entropy):** Commonly used for classification models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4\. Gradient Descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Still following the same example, if we want to be the most efficient in improving
    our score, we might want to study harder where it is easiest to gain maximal additional
    points (e.g. spelling mistakes for an absentminded person like me). Once we have
    conquered this, we then move on to the next easiest item to gain maximal additional
    point. Rinse and repeat until we get to 100.
  prefs: []
  type: TYPE_NORMAL
- en: This is exactly how gradient descent works!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Technically speaking, gradient descent is a mechanism that aims to explore a
    function's minimum value by iteratively moving in the direction of the steepest
    decrease in the function value. In the context of machine learning, by **minimising
    the loss function**, we are trying to identify the best set of parameters for
    our model to make accurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Without digressing too much into a blog post about gradient descent, an important
    issue with this technique is the possibility for the algorithm to be converging
    to a suboptimal minima.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ff5f9838b088583a44b487475973df8.png)'
  prefs: []
  type: TYPE_IMG
- en: Minima — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Rest assured. There are different approaches for handling these situations.
    Examples include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning Rate: This adjusts how much we move at each step. The intuition behind
    this is that if it is not the absolute trough, we could have skipped it by having
    a larger step.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Momentum Approach: This approach takes into account of our previous steps.
    The algorithm will feed forward a fraction of the previous step (hence momentum)
    for smoothing out oscillations. (i.e. instead of a 90-degree right hand turn,
    momentum makes it into a gentle curve)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gradient descent is basically a root searching algorithm. If you are interested
    in learning more about root searching algorithms, you might want to give me blog
    on root searching algorithms a read!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](/mastering-root-searching-algorithms-in-python-7120c335a2a8?source=post_page-----f5e0e013f441--------------------------------)
    [## Efficient Root Searching Algorithms in Python'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing efficient searching algorithms for finding roots, and optimisation
    in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/mastering-root-searching-algorithms-in-python-7120c335a2a8?source=post_page-----f5e0e013f441--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Before we jump into understanding how gradient boosting algorithm work from
    scratch, a quick plug-in for myself.
  prefs: []
  type: TYPE_NORMAL
- en: If you have enjoyed the read so far, you can also support me by subscribing
    to Medium using my affiliate link below. This has been a platform where I have
    found lots of enjoyable reads. Even if you are perfectly content with not subscribing,
    you can also support me and my creation using claps.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://louis-chan.medium.com/membership?source=post_page-----f5e0e013f441--------------------------------)
    [## Join Medium with my referral link - Louis Chan'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Louis Chan (and thousands of other writers on Medium).
    Your membership fee directly supports…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: louis-chan.medium.com](https://louis-chan.medium.com/membership?source=post_page-----f5e0e013f441--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Much appreciated for tolerating that plug-in. Here is a cute wink from a Samoyed
    as my gratitude.
  prefs: []
  type: TYPE_NORMAL
- en: Image from [GIPHY](https://giphy.com/gifs/cute-dog-6MWahPArixa6I)
  prefs: []
  type: TYPE_NORMAL
- en: Now back into business!
  prefs: []
  type: TYPE_NORMAL
- en: The Gradient Boosting Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The algorithm from scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s say we want to predict the rents of apartments in an area using a Gradient
    Boosted machine with **1 tree**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d74dae66d96e5a7fbb4cf18a87e09b7a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: We start by calculating the average rents, which is 688\. This will be our benchmark
    model i.e. predict the rent to be an average of the area. **We can also understand
    this as a naive prediction of the target variable.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/865df0ad8f02e100f0618f35ebcc77d3.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Let’s then calculate the different between the rents and the average — this
    is the gap that our weak learners would try to minimise iteratively. In our case,
    we have just got one weak learner.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c8eb0eb7fa41d8a7ff7de00633882da.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it is time to build up our group of weak learners that predicts the **residual
    from the average**. Let’s start with the decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/836fd45995b88a8101973cc2b5109237.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Wherever there are more than one item in the leave node, the predicted result
    should be the average of the items.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e773dbf367a9a30414dc0f62e7772617.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s try to predict using this new decision tree and compare it with the
    benchmark we have obtained earlier. The formula for calculating the predicted
    value is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a96d6b89562680937ebeee571a6f510.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: By applying the learning rate as a modifier to the predicted residual, we are
    applying the concept from gradient descent where we are taking a stepwise approach
    to minimise the loss function. **In our case, the “loss function” is our residual.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db09f6b37b7e84d909666322787a9e3f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: With the predicted values based on the first tree, we can then compute the new
    residual.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/50bb4d0a557587a6bde4cd6317ed5fba.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: See! The residuals are already getting smaller by “adding” the tree we have
    created to the baseline prediction we made earlier using just the average!
  prefs: []
  type: TYPE_NORMAL
- en: If we are to train a proper Gradient Boosted model, we will need to repeat the
    process of fitting a lot of tree-based models (often over 1,000 trees) to form
    the additive model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, after fitting say 1,000 trees, we can use the following formula
    for calculating the final predicted value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fb6bb8ab28f8fa6032552d72bbb5705e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Based on the above strawman example, we can summarise the characteristics of
    a GBM with the following hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of additive models:** The total number of weak learners in the ensemble.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Learning rate:** The modifier that decides the contribution of the weak learners
    to the final outcome. This also dictates how quickly the model navigates the gradients.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Maximum depth of each model:** The maximum depth of every weak learner. Shallower
    weak learner means more memory efficiency when fitting individual weak learner
    with a drawback of potentially inability to capture more complex interactions
    between variables.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Minimum number of observations in terminal nodes of the weak learners:**
    A smaller value are typical for imbalanced dataset while a larger value can be
    set for a more balanced dataset.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pros of Gradient Boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Strong prediction performance:** While this is not naturally an advantage
    of using gradient boosting, retrospectively speaking, gradient boosting has been
    a very common winner on various competitions on Kaggle. This could be accredited
    to gradient boosting’s characteristics of combining a lot of smaller models and
    use the crowd intelligence for making the final prediction rather than trying
    to fit all the data patterns in one model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility:** Gradient boosting is a model type that can be applied to regression
    or classification problem using different weak learners (not necessarily decision
    trees), loss functions, and data types (ordinal, continuous, categorical etc)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Easily accessible:** There are a number of modules from which you can apply
    gradient boosting onto your data problem whether you are using R ([gbm](https://www.rdocumentation.org/packages/gbm/versions/2.1.8.1),
    [xgboost](https://www.rdocumentation.org/packages/xgboost/versions/1.7.3.1), [lightgbm](https://www.rdocumentation.org/packages/lightgbm/versions/3.3.5)),
    Julia ([GradientBoost](https://juliapackages.com/p/gradientboost)), or Python
    ([sklearn](https://pypi.org/project/sklearn/), [xgboost](https://pypi.org/project/xgboost/),
    [lightgbm](https://pypi.org/project/lightgbm/), [catboost](https://pypi.org/project/catboost/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretability:** Gradient boosting machines are arguably one of the model
    topologies that has a better balance between complexity and interpretability compared
    against likes of neural networks. If you want to read more about interpreting
    machine learning models in general, here is a piece of blog where I have dived
    deep into how SHAP works.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](/shap-explain-any-machine-learning-model-in-python-72f0bea35f7c?source=post_page-----f5e0e013f441--------------------------------)
    [## SHAP: Explain Any Machine Learning Model in Python'
  prefs: []
  type: TYPE_NORMAL
- en: Your Comprehensive Guide to SHAP, TreeSHAP, and DeepSHAP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/shap-explain-any-machine-learning-model-in-python-72f0bea35f7c?source=post_page-----f5e0e013f441--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Cons of Gradient Boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Computational complexity:** Fitting/training a gradient boosting machine
    involves fitting oftern over 1,000 smaller weak learners. While keeping a weak
    learner small would reduce the training time, that would still aggregate very
    quickly as we start scaling the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting:** Another possible drawback of having so many smaller weak learners
    is the risk of overfitting. One way of mitigating this would be to set aside a
    validation set or use cross validation for evaluating the model’s performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyperparameter tuning:** This is perhaps one of the least mentioned cons
    of, in fact, any machine learning models. While models comes with some predefined
    hyperparameters e.g. maximum depth of tree = 3, these assumptions were made under
    certain statistical observations. Whether these assumptions are representative
    of our data problem, that is a completely different story. Hence it is critical
    that we look into not just the what’s of hyperparameters, but also the why’s.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There you go, a step-by-step tutorial data science guide on how gradient boosting
    works.
  prefs: []
  type: TYPE_NORMAL
- en: '*Don’t stop there*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Like any domain, data science is a rabbit hole in which we need to constantly
    polish our thinking and acquire new knowledge to stand out from the crowd. If
    you would like to read more about my thoughts on different data science related
    topics, feel free to choose from the list below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[5 Mistakes Every Data Scientist Should Avoid](https://medium.com/towards-data-science/5-mistakes-every-data-scientist-should-avoid-7e3523f6a9ec)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How To Call a Python Function With A String?](https://medium.com/towards-artificial-intelligence/python-trick-how-to-call-a-function-by-its-name-f35309469c66)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python Tricks: How to Check Table Merging with Pandas](https://medium.com/towards-data-science/python-tricks-how-to-check-table-merging-with-pandas-cae6b9b1d540)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ANN Recommendation System for Stock Selection](https://medium.com/towards-artificial-intelligence/ann-recommendation-system-for-stock-selection-c9751a3a0520)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Genetic Algorithm for Trading Strategy Optimization in Python](https://medium.com/towards-artificial-intelligence/genetic-algorithm-for-trading-strategy-optimization-in-python-614eb660990d)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Efficient Conditional Logic on Pandas DataFrames](https://medium.com/towards-data-science/efficient-implementation-of-conditional-logic-on-pandas-dataframes-4afa61eb7fce)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3 Common Problems with Neural Network Initialisation](https://medium.com/towards-data-science/3-common-problems-with-neural-network-initialisation-5e6cacfcd8e6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Last but definitely not least, if I have missed/mistaken anything critical,
    please feel free to drop a comment or send me a DM through LinkedIn. Let’s keep
    the knowledge flowing and get better at this domain together!
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.linkedin.com/in/louis-chan-b55b9287?source=post_page-----f5e0e013f441--------------------------------)
    [## Louis Chan — Lead GCP Data & ML Engineer — Associate Director — KPMG UK |
    LinkedIn'
  prefs: []
  type: TYPE_NORMAL
- en: Ambitious, curious and creative individual with a strong belief in inter-connectivity
    between branches of knowledge and a…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.linkedin.com](https://www.linkedin.com/in/louis-chan-b55b9287?source=post_page-----f5e0e013f441--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: “A Gentle Introduction to Gradient Boosting” by Brownlee (2018)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Gradient Boosting and XGBoost with Python” by Raschka (2017)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “An Introduction to Statistical Learning” by James et al. (2013)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Ensemble Learning” by Alpaydin (2010)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Introduction to Boosting” by Schapire (2003)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
