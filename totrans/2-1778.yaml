- en: 'Regularization: Avoiding Overfitting in Machine Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化：避免机器学习中的过拟合
- en: 原文：[https://towardsdatascience.com/regularization-avoiding-overfitting-in-machine-learning-bb65d993e9cc](https://towardsdatascience.com/regularization-avoiding-overfitting-in-machine-learning-bb65d993e9cc)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/regularization-avoiding-overfitting-in-machine-learning-bb65d993e9cc](https://towardsdatascience.com/regularization-avoiding-overfitting-in-machine-learning-bb65d993e9cc)
- en: How regularization works and when to use it
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化如何工作以及何时使用
- en: '[](https://medium.com/@riandolphin?source=post_page-----bb65d993e9cc--------------------------------)[![Rian
    Dolphin](../Images/86716dde8ad84b693d071e3face37f40.png)](https://medium.com/@riandolphin?source=post_page-----bb65d993e9cc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bb65d993e9cc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bb65d993e9cc--------------------------------)
    [Rian Dolphin](https://medium.com/@riandolphin?source=post_page-----bb65d993e9cc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@riandolphin?source=post_page-----bb65d993e9cc--------------------------------)[![Rian
    Dolphin](../Images/86716dde8ad84b693d071e3face37f40.png)](https://medium.com/@riandolphin?source=post_page-----bb65d993e9cc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bb65d993e9cc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bb65d993e9cc--------------------------------)
    [Rian Dolphin](https://medium.com/@riandolphin?source=post_page-----bb65d993e9cc--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bb65d993e9cc--------------------------------)
    ·7 min read·Jan 16, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bb65d993e9cc--------------------------------)
    ·阅读时间 7 分钟·2023年1月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: What is regularization?
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是正则化？
- en: Regularization is a technique used in machine learning to help fix a problem
    we all face in this space; when a model performs well on training data but poorly
    on new, unseen data — a problem known as [overfitting](https://medium.com/towards-data-science/overfitting-in-ml-avoiding-the-pitfalls-d5225b7118d).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是机器学习中用于解决我们都面临的问题的技术；当模型在训练数据上表现良好，但在新数据上表现差 — 这就是 [过拟合](https://medium.com/towards-data-science/overfitting-in-ml-avoiding-the-pitfalls-d5225b7118d)
    的问题。
- en: One of the telltale signs I have fallen into the trap of overfitting (and thus
    needing regularization) is when the model performs great on the training data
    but terribly on the test data. The reason this happens is that the model learns
    all the intricacies of the training data in too much detail, which means that
    it can’t generalize to unseen data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我陷入过拟合陷阱的一个明显迹象是模型在训练数据上表现很好，但在测试数据上表现极差。发生这种情况的原因是模型过于详细地学习了训练数据的所有复杂性，这意味着它不能对未见过的数据进行概括。
- en: Regularization is one way to solve this problem and works by penalising a model
    for having too many parameters with large values. Using a penalty term like this
    means that *the model is encouraged to learn only the most important patterns*
    in the data, and avoid getting bogged down in the noise specific to the training
    set.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是解决此问题的一种方法，通过对具有大值的过多参数进行惩罚来实现。使用这样的惩罚项意味着 *模型被鼓励只学习数据中最重要的模式*，而避免陷入特定于训练集的噪声中。
- en: Or, at least, that's the idea 👀 Let’s dig a bit further to see how it works.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，至少，这就是其理念 👀 让我们深入探讨一下它的工作原理。
- en: How Does Regularization Work?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化是如何工作的？
- en: '![](../Images/1e6c93cb41b4d3d97a5270d005e2d6d6.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e6c93cb41b4d3d97a5270d005e2d6d6.png)'
- en: Photo by [Ugur Akdemir](https://unsplash.com/ja/@ugur?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/lights?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Ugur Akdemir](https://unsplash.com/ja/@ugur?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，[Unsplash](https://unsplash.com/s/photos/lights?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: Generally speaking, in machine learning, we are trying to learn a model (function)
    that takes some input features and outputs a number (or vector of numbers in a
    multi-class classification scenario, for example). And the way we know if the
    model is doing a good job or not, is by calculating some type of error, which
    is a function of our model output and *y*. So, if we pass in some input *x* and
    get an output *y*, we can calculate the error/cost associated with that input.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，在机器学习中，我们尝试学习一个模型（函数），该模型接受一些输入特征并输出一个数字（或者在多分类场景下是一个数字向量）。我们知道模型是否表现良好的一种方法是计算某种类型的误差，这个误差是模型输出和
    *y* 的函数。因此，如果我们传入某个输入 *x* 并得到输出 *y*，我们可以计算与该输入相关的误差/成本。
- en: However, if we also want to penalise a model that is overly complex, we can
    add another element to the cost function, a penalty term that adds to the cost
    function when the model has many large weights. As a result, our cost function
    is now a function of our *model output*, *y*, and *the parameters of the model*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们还希望对过于复杂的模型进行惩罚，我们可以在成本函数中添加另一个元素，即当模型具有许多大权重时，增加到成本函数中的惩罚项。因此，我们的成本函数现在是模型输出、*y*
    和 *模型参数* 的函数。
- en: The penalty term is generally calculated based on the magnitude of the model’s
    parameters, and it increases the cost as the parameters get larger. This means
    that the model has to choose what features to give weight to wisely, and to reduce
    or eliminate the weight on less important features. By doing this, regularization
    helps to prevent [overfitting](https://medium.com/towards-data-science/overfitting-in-ml-avoiding-the-pitfalls-d5225b7118d),
    and can lead to better performance on new, unseen data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 惩罚项通常基于模型参数的大小进行计算，并且随着参数的增大，成本也会增加。这意味着模型必须明智地选择要赋予权重的特征，并减少或消除对不重要特征的权重。通过这样做，正则化有助于防止[过拟合](https://medium.com/towards-data-science/overfitting-in-ml-avoiding-the-pitfalls-d5225b7118d)，并且可以提高在新的、未见过的数据上的表现。
- en: '**An Example**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**一个例子**'
- en: An example of a regularization term is the L2 regularization term, which adds
    a penalty based on the sum of the squares of the model’s parameters. We’ll talk
    more about this later but for now, let’s just see how it is implemented in the
    cost function.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个正则化项的例子是L2正则化项，它通过对模型参数的平方和施加惩罚。我们稍后会详细讨论这个问题，但现在我们先看看它是如何在成本函数中实现的。
- en: Consider a generic [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error)
    (MSE) cost function *J(θ)*, which looks like this
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个通用的[均方误差](https://en.wikipedia.org/wiki/Mean_squared_error)（MSE）成本函数 *J(θ)*，它的形式如下：
- en: '![](../Images/65cbb3945647000d022667bf3b6a2608.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65cbb3945647000d022667bf3b6a2608.png)'
- en: Image by Author
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: where *m* denotes the number of training samples, *h(x;θ)* denotes our model
    output for an input *x* forsome model *h* with parameters *θ,* and *y* is the
    true value. Here we see the two things (*h(x;θ)* and *y*) I mentioned earlier
    that the cost function (before regularization) takes as inputs. Based on the output
    of this cost function, we would then update the model parameters *θ* to minimize
    the cost and could do this using an algorithm like stochastic gradient descent.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *m* 表示训练样本的数量，*h(x;θ)* 表示我们对输入 *x* 的模型输出，对于某个具有参数 *θ* 的模型 *h*，*y* 是真实值。在这里，我们看到成本函数（在正则化之前）所接受的两个输入
    (*h(x;θ)* 和 *y*)。基于这个成本函数的输出，我们将更新模型参数 *θ* 以最小化成本，并可以使用像随机梯度下降这样的算法来实现。
- en: '*As an aside on mean squared error:* h(x;θ)-y *tells us how far away our models
    prediction was from the truth* y*, and we square it because we want to penalise
    both predicting too high and too low (and squaring something makes everything
    positive* *➕)*.'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*关于均方误差的补充说明：* h(x;θ)-y *告诉我们模型预测与真实值* y *之间的距离，我们对其进行平方是因为我们希望对预测过高和过低的情况都进行惩罚（平方操作使得所有值都变为正值*
    *➕)*。'
- en: 'So, now we want to penalise large (in terms of magnitude, which is why we square
    *θ*) parameters in the model. We can do this by adding a term to the loss function.
    Remember the vector of model parameters is denoted by *θ.* The cost function with
    an L2 regularization term looks like this:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在我们希望对模型中的大（就其大小而言，这就是为什么我们对 *θ* 进行平方）参数进行惩罚。我们可以通过在损失函数中添加一个项来实现。请记住，模型参数的向量用
    *θ* 表示。包含L2正则化项的成本函数形式如下：
- en: '![](../Images/a8b4c6215d2b413d4e82723086f50f4e.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8b4c6215d2b413d4e82723086f50f4e.png)'
- en: Image by Author
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: where λ is the regularization parameter that controls how harsh the regularization
    is and needs to be chosen by you. Adding this term makes the loss *J(θ)* larger
    when the model parameter weights are larger. And so, in the optimization of *J(θ)*,
    smaller parameter values are encouraged.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 λ 是控制正则化强度的参数，需要由你选择。添加这个项会使当模型参数权重较大时，损失 *J(θ)* 变大。因此，在 *J(θ)* 的优化过程中，较小的参数值被鼓励使用。
- en: Tuning λ is important here. If we choose a value of λ too high, then we could
    make the regularization part of the cost function have a higher influence than
    the original MSE portion. This would be a big problem since it essentially amounts
    to sacrificing model performance just to have smaller model weights.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 调整 λ 是这里的关键。如果我们选择一个过高的 λ 值，则可能使正则化部分在成本函数中的影响大于原始的均方误差部分。这将是一个大问题，因为这实际上相当于牺牲模型性能仅仅为了获得较小的模型权重。
- en: When to use L1 & L2 Regularization?
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时使用 L1 和 L2 正则化？
- en: 'There are two main types of regularization: L1 regularization and L2 regularization.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化主要有两种类型：L1 正则化和 L2 正则化。
- en: '**L1** regularization, also known as LASSO (Least Absolute Shrinkage and Selection
    Operator), adds a penalty term to the cost function that is proportional to the
    *absolute value* of the model’s parameters (the example we discussed above used
    the square of the model''s parameters). This **encourages the model to use only
    a subset of the available parameters and can result in some parameters being set
    to zero**, effectively removing them from the model (think feature selection here
    💭).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**L1** 正则化，也称为 LASSO（最小绝对收缩和选择算子），在成本函数中添加了一个与模型参数的*绝对值*成正比的惩罚项（我们讨论的示例使用的是模型参数的平方）。这**鼓励模型只使用可用参数的一个子集，并且可能导致一些参数被设置为零**，从而有效地将其从模型中移除（这里可以考虑特征选择💭）。'
- en: '**L2** regularization, also known as Ridge Regression, adds a penalty term
    proportional to the *square* of the model’s parameters. This **encourages the
    model to use all of the parameters but to reduce their values**, resulting in
    a model that is less complex and less prone to overfitting.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**L2** 正则化，也称为岭回归，在模型参数的*平方*上添加了一个与之成正比的惩罚项。这**鼓励模型使用所有参数但减少其值**，从而生成一个较不复杂且较不容易过拟合的模型。'
- en: When is Regularization Useful?
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化什么时候有用？
- en: In general, regularization is most effective when the *training data is limited*
    or when the *model has a high complexity*, such as a deep neural network with
    many parameters. In these cases, the model is more likely to overfit, and regularization
    can help to prevent this by encouraging the model to learn only the most important
    patterns in the data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，当*训练数据有限*或*模型复杂度较高*时，例如具有许多参数的深度神经网络，正则化效果最佳。在这些情况下，模型更容易过拟合，而正则化可以通过鼓励模型仅学习数据中最重要的模式来帮助防止这一点。
- en: Also, since regularization encourages a model to try and use only a subset of
    features, it can also improve interpretability and lead to interesting insights.
    For example, applying regularization in the context of linear regression can have
    the added benefit of highlighting the most important predictor variables as those
    remaining with the largest weights in the model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于正则化鼓励模型尝试仅使用部分特征，它还可以提高模型的可解释性并带来有趣的见解。例如，在线性回归中应用正则化可以突出显示那些在模型中权重最大的最重要预测变量。
- en: '*To learn more about overfitting in ML, check out this article:*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*要了解更多关于 ML 中过拟合的信息，请查看这篇文章：*'
- en: '[](/overfitting-in-ml-avoiding-the-pitfalls-d5225b7118d?source=post_page-----bb65d993e9cc--------------------------------)
    [## Overfitting in ML: Avoiding the Pitfalls'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/overfitting-in-ml-avoiding-the-pitfalls-d5225b7118d?source=post_page-----bb65d993e9cc--------------------------------)
    [## 机器学习中的过拟合：避免陷阱'
- en: Exploring the Causes and Solutions for Overfitting in Machine Learning Models
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索机器学习模型中过拟合的原因和解决方案
- en: towardsdatascience.com](/overfitting-in-ml-avoiding-the-pitfalls-d5225b7118d?source=post_page-----bb65d993e9cc--------------------------------)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/overfitting-in-ml-avoiding-the-pitfalls-d5225b7118d?source=post_page-----bb65d993e9cc--------------------------------)'
- en: Challenges with Regularization
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化的挑战
- en: '![](../Images/927cd61ec19e18b7a8342092bf960334.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/927cd61ec19e18b7a8342092bf960334.png)'
- en: Photo by [Olav Ahrens Røtne](https://unsplash.com/@olav_ahrens?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/challenge?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Olav Ahrens Røtne](https://unsplash.com/@olav_ahrens?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    在 [Unsplash](https://unsplash.com/s/photos/challenge?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: One challenge with regularization is choosing the right regularization parameter,
    usually denoted λ. This parameter controls the strength of the regularization,
    and as we mentioned before, it needs to be set carefully in order to achieve the
    right balance and make sure that the regularization component is weighted enough
    so as to be useful but not too much so as to overpower the actual error part of
    your cost function. Finding the right value for lambda can be challenging, and
    it requires experimentation using a [validation set](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets).
    ⚖️
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化的一大挑战是选择正确的正则化参数，通常表示为 λ。这个参数控制正则化的强度，如前所述，需要谨慎设置，以实现正确的平衡，并确保正则化组件的权重足够，以便有用但又不会过于强大，从而压倒成本函数的实际误差部分。找到合适的
    λ 值可能很具挑战性，并且需要使用 [验证集](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets)
    进行实验。⚖️
- en: Another challenge with regularization is that it can be computationally expensive,
    especially for large models with many parameters. This is because the regularization
    term needs to be calculated and added to the cost function for *each iteration
    of training.* This can significantly slow down the training process and can be
    a particular problem for L2 regularization, which involves computing the square
    of the parameters.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化的另一个挑战是它可能计算开销很大，特别是对于参数众多的大型模型。这是因为正则化项需要计算并添加到 *每次训练迭代* 的成本函数中。这可能显著减慢训练过程，对于
    L2 正则化尤为突出，因为它涉及到参数的平方计算。
- en: Implementation Tips
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施技巧
- en: 'Despite these challenges, regularization can be a powerful tool for improving
    the performance of machine-learning models and preventing overfitting. The following
    are a few key takeaways to keep in mind when implementing regularization:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些挑战，正则化仍然是提升机器学习模型性能和防止过拟合的强大工具。以下是实施正则化时需要记住的一些关键点：
- en: Choose the right type of regularization. For example, L1 regularization is more
    effective for feature selection, while L2 regularization is more effective for
    preventing overfitting.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择正确类型的正则化。例如，L1 正则化在特征选择方面更有效，而 L2 正则化在防止过拟合方面更有效。
- en: Set the regularization parameter, λ. This parameter controls the strength of
    the regularization, and it needs to be set carefully in order to achieve the desired
    balance between model complexity and overfitting. It may be necessary to experiment
    with different values of lambda in order to find the best value for your model.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置正则化参数 λ。这个参数控制正则化的强度，需要谨慎设置，以实现模型复杂性和过拟合之间的理想平衡。可能需要尝试不同的 λ 值，以找到最适合你模型的最佳值。
- en: Incorporate regularization into your cost function. In order to use regularization,
    the regularization term needs to be added to the cost function that the model
    is optimizing. This can be done by simply adding the regularization term to the
    existing cost function or by using a pre-built regularization function provided
    by a machine learning library. There is often no need to reinvent the wheel!
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将正则化纳入你的成本函数中。为了使用正则化，需要将正则化项添加到模型优化的成本函数中。这可以通过简单地将正则化项添加到现有成本函数中，或者使用机器学习库提供的预构建正则化函数来实现。通常不需要重新发明轮子！
- en: Conclusion
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: I hope that this article has given you a better understanding of how regularization
    can be a valuable tool to have in your machine learning toolbox. It is by no means
    a silver bullet, and it may not work in all situations, but if a model is experiencing
    overfitting, it can often be a good place to start.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章能让你更好地理解正则化如何成为你机器学习工具箱中的宝贵工具。它绝不是万能的，可能并不适用于所有情况，但如果模型出现过拟合，通常可以作为一个良好的起点。
