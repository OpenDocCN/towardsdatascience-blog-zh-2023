- en: 'Regularization: Avoiding Overfitting in Machine Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–ï¼šé¿å…æœºå™¨å­¦ä¹ ä¸­çš„è¿‡æ‹Ÿåˆ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/regularization-avoiding-overfitting-in-machine-learning-bb65d993e9cc](https://towardsdatascience.com/regularization-avoiding-overfitting-in-machine-learning-bb65d993e9cc)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/regularization-avoiding-overfitting-in-machine-learning-bb65d993e9cc](https://towardsdatascience.com/regularization-avoiding-overfitting-in-machine-learning-bb65d993e9cc)
- en: How regularization works and when to use it
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–å¦‚ä½•å·¥ä½œä»¥åŠä½•æ—¶ä½¿ç”¨
- en: '[](https://medium.com/@riandolphin?source=post_page-----bb65d993e9cc--------------------------------)[![Rian
    Dolphin](../Images/86716dde8ad84b693d071e3face37f40.png)](https://medium.com/@riandolphin?source=post_page-----bb65d993e9cc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bb65d993e9cc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bb65d993e9cc--------------------------------)
    [Rian Dolphin](https://medium.com/@riandolphin?source=post_page-----bb65d993e9cc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@riandolphin?source=post_page-----bb65d993e9cc--------------------------------)[![Rian
    Dolphin](../Images/86716dde8ad84b693d071e3face37f40.png)](https://medium.com/@riandolphin?source=post_page-----bb65d993e9cc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bb65d993e9cc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bb65d993e9cc--------------------------------)
    [Rian Dolphin](https://medium.com/@riandolphin?source=post_page-----bb65d993e9cc--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bb65d993e9cc--------------------------------)
    Â·7 min readÂ·Jan 16, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bb65d993e9cc--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 7 åˆ†é’ŸÂ·2023å¹´1æœˆ16æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: What is regularization?
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æ­£åˆ™åŒ–ï¼Ÿ
- en: Regularization is a technique used in machine learning to help fix a problem
    we all face in this space; when a model performs well on training data but poorly
    on new, unseen data â€” a problem known as [overfitting](https://medium.com/towards-data-science/overfitting-in-ml-avoiding-the-pitfalls-d5225b7118d).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–æ˜¯æœºå™¨å­¦ä¹ ä¸­ç”¨äºè§£å†³æˆ‘ä»¬éƒ½é¢ä¸´çš„é—®é¢˜çš„æŠ€æœ¯ï¼›å½“æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°å·® â€” è¿™å°±æ˜¯ [è¿‡æ‹Ÿåˆ](https://medium.com/towards-data-science/overfitting-in-ml-avoiding-the-pitfalls-d5225b7118d)
    çš„é—®é¢˜ã€‚
- en: One of the telltale signs I have fallen into the trap of overfitting (and thus
    needing regularization) is when the model performs great on the training data
    but terribly on the test data. The reason this happens is that the model learns
    all the intricacies of the training data in too much detail, which means that
    it canâ€™t generalize to unseen data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘é™·å…¥è¿‡æ‹Ÿåˆé™·é˜±çš„ä¸€ä¸ªæ˜æ˜¾è¿¹è±¡æ˜¯æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨æµ‹è¯•æ•°æ®ä¸Šè¡¨ç°æå·®ã€‚å‘ç”Ÿè¿™ç§æƒ…å†µçš„åŸå› æ˜¯æ¨¡å‹è¿‡äºè¯¦ç»†åœ°å­¦ä¹ äº†è®­ç»ƒæ•°æ®çš„æ‰€æœ‰å¤æ‚æ€§ï¼Œè¿™æ„å‘³ç€å®ƒä¸èƒ½å¯¹æœªè§è¿‡çš„æ•°æ®è¿›è¡Œæ¦‚æ‹¬ã€‚
- en: Regularization is one way to solve this problem and works by penalising a model
    for having too many parameters with large values. Using a penalty term like this
    means that *the model is encouraged to learn only the most important patterns*
    in the data, and avoid getting bogged down in the noise specific to the training
    set.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–æ˜¯è§£å†³æ­¤é—®é¢˜çš„ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡å¯¹å…·æœ‰å¤§å€¼çš„è¿‡å¤šå‚æ•°è¿›è¡Œæƒ©ç½šæ¥å®ç°ã€‚ä½¿ç”¨è¿™æ ·çš„æƒ©ç½šé¡¹æ„å‘³ç€ *æ¨¡å‹è¢«é¼“åŠ±åªå­¦ä¹ æ•°æ®ä¸­æœ€é‡è¦çš„æ¨¡å¼*ï¼Œè€Œé¿å…é™·å…¥ç‰¹å®šäºè®­ç»ƒé›†çš„å™ªå£°ä¸­ã€‚
- en: Or, at least, that's the idea ğŸ‘€ Letâ€™s dig a bit further to see how it works.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œè‡³å°‘ï¼Œè¿™å°±æ˜¯å…¶ç†å¿µ ğŸ‘€ è®©æˆ‘ä»¬æ·±å…¥æ¢è®¨ä¸€ä¸‹å®ƒçš„å·¥ä½œåŸç†ã€‚
- en: How Does Regularization Work?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ
- en: '![](../Images/1e6c93cb41b4d3d97a5270d005e2d6d6.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e6c93cb41b4d3d97a5270d005e2d6d6.png)'
- en: Photo by [Ugur Akdemir](https://unsplash.com/ja/@ugur?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/lights?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Ugur Akdemir](https://unsplash.com/ja/@ugur?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    æä¾›ï¼Œ[Unsplash](https://unsplash.com/s/photos/lights?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: Generally speaking, in machine learning, we are trying to learn a model (function)
    that takes some input features and outputs a number (or vector of numbers in a
    multi-class classification scenario, for example). And the way we know if the
    model is doing a good job or not, is by calculating some type of error, which
    is a function of our model output and *y*. So, if we pass in some input *x* and
    get an output *y*, we can calculate the error/cost associated with that input.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼Œåœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å°è¯•å­¦ä¹ ä¸€ä¸ªæ¨¡å‹ï¼ˆå‡½æ•°ï¼‰ï¼Œè¯¥æ¨¡å‹æ¥å—ä¸€äº›è¾“å…¥ç‰¹å¾å¹¶è¾“å‡ºä¸€ä¸ªæ•°å­—ï¼ˆæˆ–è€…åœ¨å¤šåˆ†ç±»åœºæ™¯ä¸‹æ˜¯ä¸€ä¸ªæ•°å­—å‘é‡ï¼‰ã€‚æˆ‘ä»¬çŸ¥é“æ¨¡å‹æ˜¯å¦è¡¨ç°è‰¯å¥½çš„ä¸€ç§æ–¹æ³•æ˜¯è®¡ç®—æŸç§ç±»å‹çš„è¯¯å·®ï¼Œè¿™ä¸ªè¯¯å·®æ˜¯æ¨¡å‹è¾“å‡ºå’Œ
    *y* çš„å‡½æ•°ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬ä¼ å…¥æŸä¸ªè¾“å…¥ *x* å¹¶å¾—åˆ°è¾“å‡º *y*ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—ä¸è¯¥è¾“å…¥ç›¸å…³çš„è¯¯å·®/æˆæœ¬ã€‚
- en: However, if we also want to penalise a model that is overly complex, we can
    add another element to the cost function, a penalty term that adds to the cost
    function when the model has many large weights. As a result, our cost function
    is now a function of our *model output*, *y*, and *the parameters of the model*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬è¿˜å¸Œæœ›å¯¹è¿‡äºå¤æ‚çš„æ¨¡å‹è¿›è¡Œæƒ©ç½šï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æˆæœ¬å‡½æ•°ä¸­æ·»åŠ å¦ä¸€ä¸ªå…ƒç´ ï¼Œå³å½“æ¨¡å‹å…·æœ‰è®¸å¤šå¤§æƒé‡æ—¶ï¼Œå¢åŠ åˆ°æˆæœ¬å‡½æ•°ä¸­çš„æƒ©ç½šé¡¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æˆæœ¬å‡½æ•°ç°åœ¨æ˜¯æ¨¡å‹è¾“å‡ºã€*y*
    å’Œ *æ¨¡å‹å‚æ•°* çš„å‡½æ•°ã€‚
- en: The penalty term is generally calculated based on the magnitude of the modelâ€™s
    parameters, and it increases the cost as the parameters get larger. This means
    that the model has to choose what features to give weight to wisely, and to reduce
    or eliminate the weight on less important features. By doing this, regularization
    helps to prevent [overfitting](https://medium.com/towards-data-science/overfitting-in-ml-avoiding-the-pitfalls-d5225b7118d),
    and can lead to better performance on new, unseen data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ©ç½šé¡¹é€šå¸¸åŸºäºæ¨¡å‹å‚æ•°çš„å¤§å°è¿›è¡Œè®¡ç®—ï¼Œå¹¶ä¸”éšç€å‚æ•°çš„å¢å¤§ï¼Œæˆæœ¬ä¹Ÿä¼šå¢åŠ ã€‚è¿™æ„å‘³ç€æ¨¡å‹å¿…é¡»æ˜æ™ºåœ°é€‰æ‹©è¦èµ‹äºˆæƒé‡çš„ç‰¹å¾ï¼Œå¹¶å‡å°‘æˆ–æ¶ˆé™¤å¯¹ä¸é‡è¦ç‰¹å¾çš„æƒé‡ã€‚é€šè¿‡è¿™æ ·åšï¼Œæ­£åˆ™åŒ–æœ‰åŠ©äºé˜²æ­¢[è¿‡æ‹Ÿåˆ](https://medium.com/towards-data-science/overfitting-in-ml-avoiding-the-pitfalls-d5225b7118d)ï¼Œå¹¶ä¸”å¯ä»¥æé«˜åœ¨æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ä¸Šçš„è¡¨ç°ã€‚
- en: '**An Example**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸€ä¸ªä¾‹å­**'
- en: An example of a regularization term is the L2 regularization term, which adds
    a penalty based on the sum of the squares of the modelâ€™s parameters. Weâ€™ll talk
    more about this later but for now, letâ€™s just see how it is implemented in the
    cost function.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹çš„ä¾‹å­æ˜¯L2æ­£åˆ™åŒ–é¡¹ï¼Œå®ƒé€šè¿‡å¯¹æ¨¡å‹å‚æ•°çš„å¹³æ–¹å’Œæ–½åŠ æƒ©ç½šã€‚æˆ‘ä»¬ç¨åä¼šè¯¦ç»†è®¨è®ºè¿™ä¸ªé—®é¢˜ï¼Œä½†ç°åœ¨æˆ‘ä»¬å…ˆçœ‹çœ‹å®ƒæ˜¯å¦‚ä½•åœ¨æˆæœ¬å‡½æ•°ä¸­å®ç°çš„ã€‚
- en: Consider a generic [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error)
    (MSE) cost function *J(Î¸)*, which looks like this
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä¸€ä¸ªé€šç”¨çš„[å‡æ–¹è¯¯å·®](https://en.wikipedia.org/wiki/Mean_squared_error)ï¼ˆMSEï¼‰æˆæœ¬å‡½æ•° *J(Î¸)*ï¼Œå®ƒçš„å½¢å¼å¦‚ä¸‹ï¼š
- en: '![](../Images/65cbb3945647000d022667bf3b6a2608.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65cbb3945647000d022667bf3b6a2608.png)'
- en: Image by Author
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ
- en: where *m* denotes the number of training samples, *h(x;Î¸)* denotes our model
    output for an input *x* forsome model *h* with parameters *Î¸,* and *y* is the
    true value. Here we see the two things (*h(x;Î¸)* and *y*) I mentioned earlier
    that the cost function (before regularization) takes as inputs. Based on the output
    of this cost function, we would then update the model parameters *Î¸* to minimize
    the cost and could do this using an algorithm like stochastic gradient descent.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *m* è¡¨ç¤ºè®­ç»ƒæ ·æœ¬çš„æ•°é‡ï¼Œ*h(x;Î¸)* è¡¨ç¤ºæˆ‘ä»¬å¯¹è¾“å…¥ *x* çš„æ¨¡å‹è¾“å‡ºï¼Œå¯¹äºæŸä¸ªå…·æœ‰å‚æ•° *Î¸* çš„æ¨¡å‹ *h*ï¼Œ*y* æ˜¯çœŸå®å€¼ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°æˆæœ¬å‡½æ•°ï¼ˆåœ¨æ­£åˆ™åŒ–ä¹‹å‰ï¼‰æ‰€æ¥å—çš„ä¸¤ä¸ªè¾“å…¥
    (*h(x;Î¸)* å’Œ *y*)ã€‚åŸºäºè¿™ä¸ªæˆæœ¬å‡½æ•°çš„è¾“å‡ºï¼Œæˆ‘ä»¬å°†æ›´æ–°æ¨¡å‹å‚æ•° *Î¸* ä»¥æœ€å°åŒ–æˆæœ¬ï¼Œå¹¶å¯ä»¥ä½¿ç”¨åƒéšæœºæ¢¯åº¦ä¸‹é™è¿™æ ·çš„ç®—æ³•æ¥å®ç°ã€‚
- en: '*As an aside on mean squared error:* h(x;Î¸)-y *tells us how far away our models
    prediction was from the truth* y*, and we square it because we want to penalise
    both predicting too high and too low (and squaring something makes everything
    positive* *â•)*.'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*å…³äºå‡æ–¹è¯¯å·®çš„è¡¥å……è¯´æ˜ï¼š* h(x;Î¸)-y *å‘Šè¯‰æˆ‘ä»¬æ¨¡å‹é¢„æµ‹ä¸çœŸå®å€¼* y *ä¹‹é—´çš„è·ç¦»ï¼Œæˆ‘ä»¬å¯¹å…¶è¿›è¡Œå¹³æ–¹æ˜¯å› ä¸ºæˆ‘ä»¬å¸Œæœ›å¯¹é¢„æµ‹è¿‡é«˜å’Œè¿‡ä½çš„æƒ…å†µéƒ½è¿›è¡Œæƒ©ç½šï¼ˆå¹³æ–¹æ“ä½œä½¿å¾—æ‰€æœ‰å€¼éƒ½å˜ä¸ºæ­£å€¼*
    *â•)*ã€‚'
- en: 'So, now we want to penalise large (in terms of magnitude, which is why we square
    *Î¸*) parameters in the model. We can do this by adding a term to the loss function.
    Remember the vector of model parameters is denoted by *Î¸.* The cost function with
    an L2 regularization term looks like this:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œç°åœ¨æˆ‘ä»¬å¸Œæœ›å¯¹æ¨¡å‹ä¸­çš„å¤§ï¼ˆå°±å…¶å¤§å°è€Œè¨€ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å¯¹ *Î¸* è¿›è¡Œå¹³æ–¹ï¼‰å‚æ•°è¿›è¡Œæƒ©ç½šã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ ä¸€ä¸ªé¡¹æ¥å®ç°ã€‚è¯·è®°ä½ï¼Œæ¨¡å‹å‚æ•°çš„å‘é‡ç”¨
    *Î¸* è¡¨ç¤ºã€‚åŒ…å«L2æ­£åˆ™åŒ–é¡¹çš„æˆæœ¬å‡½æ•°å½¢å¼å¦‚ä¸‹ï¼š
- en: '![](../Images/a8b4c6215d2b413d4e82723086f50f4e.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8b4c6215d2b413d4e82723086f50f4e.png)'
- en: Image by Author
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ
- en: where Î» is the regularization parameter that controls how harsh the regularization
    is and needs to be chosen by you. Adding this term makes the loss *J(Î¸)* larger
    when the model parameter weights are larger. And so, in the optimization of *J(Î¸)*,
    smaller parameter values are encouraged.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ Î» æ˜¯æ§åˆ¶æ­£åˆ™åŒ–å¼ºåº¦çš„å‚æ•°ï¼Œéœ€è¦ç”±ä½ é€‰æ‹©ã€‚æ·»åŠ è¿™ä¸ªé¡¹ä¼šä½¿å½“æ¨¡å‹å‚æ•°æƒé‡è¾ƒå¤§æ—¶ï¼ŒæŸå¤± *J(Î¸)* å˜å¤§ã€‚å› æ­¤ï¼Œåœ¨ *J(Î¸)* çš„ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œè¾ƒå°çš„å‚æ•°å€¼è¢«é¼“åŠ±ä½¿ç”¨ã€‚
- en: Tuning Î» is important here. If we choose a value of Î» too high, then we could
    make the regularization part of the cost function have a higher influence than
    the original MSE portion. This would be a big problem since it essentially amounts
    to sacrificing model performance just to have smaller model weights.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒæ•´ Î» æ˜¯è¿™é‡Œçš„å…³é”®ã€‚å¦‚æœæˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªè¿‡é«˜çš„ Î» å€¼ï¼Œåˆ™å¯èƒ½ä½¿æ­£åˆ™åŒ–éƒ¨åˆ†åœ¨æˆæœ¬å‡½æ•°ä¸­çš„å½±å“å¤§äºåŸå§‹çš„å‡æ–¹è¯¯å·®éƒ¨åˆ†ã€‚è¿™å°†æ˜¯ä¸€ä¸ªå¤§é—®é¢˜ï¼Œå› ä¸ºè¿™å®é™…ä¸Šç›¸å½“äºç‰ºç‰²æ¨¡å‹æ€§èƒ½ä»…ä»…ä¸ºäº†è·å¾—è¾ƒå°çš„æ¨¡å‹æƒé‡ã€‚
- en: When to use L1 & L2 Regularization?
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½•æ—¶ä½¿ç”¨ L1 å’Œ L2 æ­£åˆ™åŒ–ï¼Ÿ
- en: 'There are two main types of regularization: L1 regularization and L2 regularization.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–ä¸»è¦æœ‰ä¸¤ç§ç±»å‹ï¼šL1 æ­£åˆ™åŒ–å’Œ L2 æ­£åˆ™åŒ–ã€‚
- en: '**L1** regularization, also known as LASSO (Least Absolute Shrinkage and Selection
    Operator), adds a penalty term to the cost function that is proportional to the
    *absolute value* of the modelâ€™s parameters (the example we discussed above used
    the square of the model''s parameters). This **encourages the model to use only
    a subset of the available parameters and can result in some parameters being set
    to zero**, effectively removing them from the model (think feature selection here
    ğŸ’­).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**L1** æ­£åˆ™åŒ–ï¼Œä¹Ÿç§°ä¸º LASSOï¼ˆæœ€å°ç»å¯¹æ”¶ç¼©å’Œé€‰æ‹©ç®—å­ï¼‰ï¼Œåœ¨æˆæœ¬å‡½æ•°ä¸­æ·»åŠ äº†ä¸€ä¸ªä¸æ¨¡å‹å‚æ•°çš„*ç»å¯¹å€¼*æˆæ­£æ¯”çš„æƒ©ç½šé¡¹ï¼ˆæˆ‘ä»¬è®¨è®ºçš„ç¤ºä¾‹ä½¿ç”¨çš„æ˜¯æ¨¡å‹å‚æ•°çš„å¹³æ–¹ï¼‰ã€‚è¿™**é¼“åŠ±æ¨¡å‹åªä½¿ç”¨å¯ç”¨å‚æ•°çš„ä¸€ä¸ªå­é›†ï¼Œå¹¶ä¸”å¯èƒ½å¯¼è‡´ä¸€äº›å‚æ•°è¢«è®¾ç½®ä¸ºé›¶**ï¼Œä»è€Œæœ‰æ•ˆåœ°å°†å…¶ä»æ¨¡å‹ä¸­ç§»é™¤ï¼ˆè¿™é‡Œå¯ä»¥è€ƒè™‘ç‰¹å¾é€‰æ‹©ğŸ’­ï¼‰ã€‚'
- en: '**L2** regularization, also known as Ridge Regression, adds a penalty term
    proportional to the *square* of the modelâ€™s parameters. This **encourages the
    model to use all of the parameters but to reduce their values**, resulting in
    a model that is less complex and less prone to overfitting.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**L2** æ­£åˆ™åŒ–ï¼Œä¹Ÿç§°ä¸ºå²­å›å½’ï¼Œåœ¨æ¨¡å‹å‚æ•°çš„*å¹³æ–¹*ä¸Šæ·»åŠ äº†ä¸€ä¸ªä¸ä¹‹æˆæ­£æ¯”çš„æƒ©ç½šé¡¹ã€‚è¿™**é¼“åŠ±æ¨¡å‹ä½¿ç”¨æ‰€æœ‰å‚æ•°ä½†å‡å°‘å…¶å€¼**ï¼Œä»è€Œç”Ÿæˆä¸€ä¸ªè¾ƒä¸å¤æ‚ä¸”è¾ƒä¸å®¹æ˜“è¿‡æ‹Ÿåˆçš„æ¨¡å‹ã€‚'
- en: When is Regularization Useful?
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–ä»€ä¹ˆæ—¶å€™æœ‰ç”¨ï¼Ÿ
- en: In general, regularization is most effective when the *training data is limited*
    or when the *model has a high complexity*, such as a deep neural network with
    many parameters. In these cases, the model is more likely to overfit, and regularization
    can help to prevent this by encouraging the model to learn only the most important
    patterns in the data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼Œå½“*è®­ç»ƒæ•°æ®æœ‰é™*æˆ–*æ¨¡å‹å¤æ‚åº¦è¾ƒé«˜*æ—¶ï¼Œä¾‹å¦‚å…·æœ‰è®¸å¤šå‚æ•°çš„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œæ­£åˆ™åŒ–æ•ˆæœæœ€ä½³ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œæ¨¡å‹æ›´å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œè€Œæ­£åˆ™åŒ–å¯ä»¥é€šè¿‡é¼“åŠ±æ¨¡å‹ä»…å­¦ä¹ æ•°æ®ä¸­æœ€é‡è¦çš„æ¨¡å¼æ¥å¸®åŠ©é˜²æ­¢è¿™ä¸€ç‚¹ã€‚
- en: Also, since regularization encourages a model to try and use only a subset of
    features, it can also improve interpretability and lead to interesting insights.
    For example, applying regularization in the context of linear regression can have
    the added benefit of highlighting the most important predictor variables as those
    remaining with the largest weights in the model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œç”±äºæ­£åˆ™åŒ–é¼“åŠ±æ¨¡å‹å°è¯•ä»…ä½¿ç”¨éƒ¨åˆ†ç‰¹å¾ï¼Œå®ƒè¿˜å¯ä»¥æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§å¹¶å¸¦æ¥æœ‰è¶£çš„è§è§£ã€‚ä¾‹å¦‚ï¼Œåœ¨çº¿æ€§å›å½’ä¸­åº”ç”¨æ­£åˆ™åŒ–å¯ä»¥çªå‡ºæ˜¾ç¤ºé‚£äº›åœ¨æ¨¡å‹ä¸­æƒé‡æœ€å¤§çš„æœ€é‡è¦é¢„æµ‹å˜é‡ã€‚
- en: '*To learn more about overfitting in ML, check out this article:*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¦äº†è§£æ›´å¤šå…³äº ML ä¸­è¿‡æ‹Ÿåˆçš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿™ç¯‡æ–‡ç« ï¼š*'
- en: '[](/overfitting-in-ml-avoiding-the-pitfalls-d5225b7118d?source=post_page-----bb65d993e9cc--------------------------------)
    [## Overfitting in ML: Avoiding the Pitfalls'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/overfitting-in-ml-avoiding-the-pitfalls-d5225b7118d?source=post_page-----bb65d993e9cc--------------------------------)
    [## æœºå™¨å­¦ä¹ ä¸­çš„è¿‡æ‹Ÿåˆï¼šé¿å…é™·é˜±'
- en: Exploring the Causes and Solutions for Overfitting in Machine Learning Models
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¢ç´¢æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­è¿‡æ‹Ÿåˆçš„åŸå› å’Œè§£å†³æ–¹æ¡ˆ
- en: towardsdatascience.com](/overfitting-in-ml-avoiding-the-pitfalls-d5225b7118d?source=post_page-----bb65d993e9cc--------------------------------)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/overfitting-in-ml-avoiding-the-pitfalls-d5225b7118d?source=post_page-----bb65d993e9cc--------------------------------)'
- en: Challenges with Regularization
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–çš„æŒ‘æˆ˜
- en: '![](../Images/927cd61ec19e18b7a8342092bf960334.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/927cd61ec19e18b7a8342092bf960334.png)'
- en: Photo by [Olav Ahrens RÃ¸tne](https://unsplash.com/@olav_ahrens?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/challenge?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼š[Olav Ahrens RÃ¸tne](https://unsplash.com/@olav_ahrens?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    åœ¨ [Unsplash](https://unsplash.com/s/photos/challenge?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: One challenge with regularization is choosing the right regularization parameter,
    usually denoted Î». This parameter controls the strength of the regularization,
    and as we mentioned before, it needs to be set carefully in order to achieve the
    right balance and make sure that the regularization component is weighted enough
    so as to be useful but not too much so as to overpower the actual error part of
    your cost function. Finding the right value for lambda can be challenging, and
    it requires experimentation using a [validation set](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets).
    âš–ï¸
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–çš„ä¸€å¤§æŒ‘æˆ˜æ˜¯é€‰æ‹©æ­£ç¡®çš„æ­£åˆ™åŒ–å‚æ•°ï¼Œé€šå¸¸è¡¨ç¤ºä¸º Î»ã€‚è¿™ä¸ªå‚æ•°æ§åˆ¶æ­£åˆ™åŒ–çš„å¼ºåº¦ï¼Œå¦‚å‰æ‰€è¿°ï¼Œéœ€è¦è°¨æ…è®¾ç½®ï¼Œä»¥å®ç°æ­£ç¡®çš„å¹³è¡¡ï¼Œå¹¶ç¡®ä¿æ­£åˆ™åŒ–ç»„ä»¶çš„æƒé‡è¶³å¤Ÿï¼Œä»¥ä¾¿æœ‰ç”¨ä½†åˆä¸ä¼šè¿‡äºå¼ºå¤§ï¼Œä»è€Œå‹å€’æˆæœ¬å‡½æ•°çš„å®é™…è¯¯å·®éƒ¨åˆ†ã€‚æ‰¾åˆ°åˆé€‚çš„
    Î» å€¼å¯èƒ½å¾ˆå…·æŒ‘æˆ˜æ€§ï¼Œå¹¶ä¸”éœ€è¦ä½¿ç”¨ [éªŒè¯é›†](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets)
    è¿›è¡Œå®éªŒã€‚âš–ï¸
- en: Another challenge with regularization is that it can be computationally expensive,
    especially for large models with many parameters. This is because the regularization
    term needs to be calculated and added to the cost function for *each iteration
    of training.* This can significantly slow down the training process and can be
    a particular problem for L2 regularization, which involves computing the square
    of the parameters.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–çš„å¦ä¸€ä¸ªæŒ‘æˆ˜æ˜¯å®ƒå¯èƒ½è®¡ç®—å¼€é”€å¾ˆå¤§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå‚æ•°ä¼—å¤šçš„å¤§å‹æ¨¡å‹ã€‚è¿™æ˜¯å› ä¸ºæ­£åˆ™åŒ–é¡¹éœ€è¦è®¡ç®—å¹¶æ·»åŠ åˆ° *æ¯æ¬¡è®­ç»ƒè¿­ä»£* çš„æˆæœ¬å‡½æ•°ä¸­ã€‚è¿™å¯èƒ½æ˜¾è‘—å‡æ…¢è®­ç»ƒè¿‡ç¨‹ï¼Œå¯¹äº
    L2 æ­£åˆ™åŒ–å°¤ä¸ºçªå‡ºï¼Œå› ä¸ºå®ƒæ¶‰åŠåˆ°å‚æ•°çš„å¹³æ–¹è®¡ç®—ã€‚
- en: Implementation Tips
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®æ–½æŠ€å·§
- en: 'Despite these challenges, regularization can be a powerful tool for improving
    the performance of machine-learning models and preventing overfitting. The following
    are a few key takeaways to keep in mind when implementing regularization:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å­˜åœ¨è¿™äº›æŒ‘æˆ˜ï¼Œæ­£åˆ™åŒ–ä»ç„¶æ˜¯æå‡æœºå™¨å­¦ä¹ æ¨¡å‹æ€§èƒ½å’Œé˜²æ­¢è¿‡æ‹Ÿåˆçš„å¼ºå¤§å·¥å…·ã€‚ä»¥ä¸‹æ˜¯å®æ–½æ­£åˆ™åŒ–æ—¶éœ€è¦è®°ä½çš„ä¸€äº›å…³é”®ç‚¹ï¼š
- en: Choose the right type of regularization. For example, L1 regularization is more
    effective for feature selection, while L2 regularization is more effective for
    preventing overfitting.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹©æ­£ç¡®ç±»å‹çš„æ­£åˆ™åŒ–ã€‚ä¾‹å¦‚ï¼ŒL1 æ­£åˆ™åŒ–åœ¨ç‰¹å¾é€‰æ‹©æ–¹é¢æ›´æœ‰æ•ˆï¼Œè€Œ L2 æ­£åˆ™åŒ–åœ¨é˜²æ­¢è¿‡æ‹Ÿåˆæ–¹é¢æ›´æœ‰æ•ˆã€‚
- en: Set the regularization parameter, Î». This parameter controls the strength of
    the regularization, and it needs to be set carefully in order to achieve the desired
    balance between model complexity and overfitting. It may be necessary to experiment
    with different values of lambda in order to find the best value for your model.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¾ç½®æ­£åˆ™åŒ–å‚æ•° Î»ã€‚è¿™ä¸ªå‚æ•°æ§åˆ¶æ­£åˆ™åŒ–çš„å¼ºåº¦ï¼Œéœ€è¦è°¨æ…è®¾ç½®ï¼Œä»¥å®ç°æ¨¡å‹å¤æ‚æ€§å’Œè¿‡æ‹Ÿåˆä¹‹é—´çš„ç†æƒ³å¹³è¡¡ã€‚å¯èƒ½éœ€è¦å°è¯•ä¸åŒçš„ Î» å€¼ï¼Œä»¥æ‰¾åˆ°æœ€é€‚åˆä½ æ¨¡å‹çš„æœ€ä½³å€¼ã€‚
- en: Incorporate regularization into your cost function. In order to use regularization,
    the regularization term needs to be added to the cost function that the model
    is optimizing. This can be done by simply adding the regularization term to the
    existing cost function or by using a pre-built regularization function provided
    by a machine learning library. There is often no need to reinvent the wheel!
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ­£åˆ™åŒ–çº³å…¥ä½ çš„æˆæœ¬å‡½æ•°ä¸­ã€‚ä¸ºäº†ä½¿ç”¨æ­£åˆ™åŒ–ï¼Œéœ€è¦å°†æ­£åˆ™åŒ–é¡¹æ·»åŠ åˆ°æ¨¡å‹ä¼˜åŒ–çš„æˆæœ¬å‡½æ•°ä¸­ã€‚è¿™å¯ä»¥é€šè¿‡ç®€å•åœ°å°†æ­£åˆ™åŒ–é¡¹æ·»åŠ åˆ°ç°æœ‰æˆæœ¬å‡½æ•°ä¸­ï¼Œæˆ–è€…ä½¿ç”¨æœºå™¨å­¦ä¹ åº“æä¾›çš„é¢„æ„å»ºæ­£åˆ™åŒ–å‡½æ•°æ¥å®ç°ã€‚é€šå¸¸ä¸éœ€è¦é‡æ–°å‘æ˜è½®å­ï¼
- en: Conclusion
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: I hope that this article has given you a better understanding of how regularization
    can be a valuable tool to have in your machine learning toolbox. It is by no means
    a silver bullet, and it may not work in all situations, but if a model is experiencing
    overfitting, it can often be a good place to start.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½è®©ä½ æ›´å¥½åœ°ç†è§£æ­£åˆ™åŒ–å¦‚ä½•æˆä¸ºä½ æœºå™¨å­¦ä¹ å·¥å…·ç®±ä¸­çš„å®è´µå·¥å…·ã€‚å®ƒç»ä¸æ˜¯ä¸‡èƒ½çš„ï¼Œå¯èƒ½å¹¶ä¸é€‚ç”¨äºæ‰€æœ‰æƒ…å†µï¼Œä½†å¦‚æœæ¨¡å‹å‡ºç°è¿‡æ‹Ÿåˆï¼Œé€šå¸¸å¯ä»¥ä½œä¸ºä¸€ä¸ªè‰¯å¥½çš„èµ·ç‚¹ã€‚
