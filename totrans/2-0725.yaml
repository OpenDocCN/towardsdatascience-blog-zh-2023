- en: Detecting Cancer Growth Using AI and Computer Vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/detecting-cancer-growth-using-ai-and-computer-vision-c88e985f450e](https://towardsdatascience.com/detecting-cancer-growth-using-ai-and-computer-vision-c88e985f450e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'AI for social good: applications in medical imaging'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@saranggupta94?source=post_page-----c88e985f450e--------------------------------)[![Sarang
    Gupta](../Images/23f5c6984019ddc098f198e2c6c55b5f.png)](https://medium.com/@saranggupta94?source=post_page-----c88e985f450e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c88e985f450e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c88e985f450e--------------------------------)
    [Sarang Gupta](https://medium.com/@saranggupta94?source=post_page-----c88e985f450e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c88e985f450e--------------------------------)
    ·9 min read·Jun 15, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/15f850a7ab92e533f3e3712165b1cb54.png)'
  prefs: []
  type: TYPE_IMG
- en: Cover image from [unsplash.com](https://unsplash.com/photos/pwcKF7L4-no)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Breast cancer is one of the deadliest forms of cancer in women. As per the [World
    Health Organization (WHO)](https://www.who.int/news-room/fact-sheets/detail/breast-cancer),
    in 2020 alone, around 2.3 million new cases of invasive breast cancer were diagnosed
    worldwide that resulted in 685,000 deaths.
  prefs: []
  type: TYPE_NORMAL
- en: Even though developing countries represent one-half of all breast cancer cases,
    they represent 62% of all deaths caused by breast cancer. Survival of breast cancer
    for at least 5 years after diagnosis ranges from more than [90% in high-income
    countries, to 66% in India and 40% in South Africa](https://www.who.int/news-room/fact-sheets/detail/breast-cancer).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e68a613d03dd048885efa5adc07cd125.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Various steps in breast cancer metastases detection as performed
    by pathologists | Top Left: Image from [Camelyon17 challenge](https://camelyon17.grand-challenge.org/Background/)
    | Top Right: Image from [unsplash.com](https://unsplash.com/photos/tGYrlchfObE?utm_source=unsplash&utm_medium=referral&utm_content=creditShareLink)
    | Center: Image from [unsplash.com](https://unsplash.com/photos/bHNUueWud9c) |
    Bottom Left and Bottom Right: Images by the Author'
  prefs: []
  type: TYPE_NORMAL
- en: A key step in determining what stage the cancer is in is by microscopic examination
    of lymph nodes adjacent to the breast to understand whether cancer has [metastasised](https://en.wikipedia.org/wiki/Metastasis)
    (a medical term meaning spread to other sites in the body). This step is not only
    sensitive, time intensive and laborious but also requires highly-skilled medical
    pathologists. It impacts decisions related to treatment, which includes considerations
    about radiation therapy, chemotherapy, and the potential surgical removal of more
    lymph nodes
  prefs: []
  type: TYPE_NORMAL
- en: With the advent and advancement of AI and computer vision techniques, particularly
    Convolutional Neural Networks (CNNs), we have been able to improve accuracy on
    a wide range of computer vision tasks such as image recognition, object detection,
    and segmentation. These have been beneficial in solving some of the most challenging
    healthcare problems, especially in regions with limited access to advanced medical
    facilities.
  prefs: []
  type: TYPE_NORMAL
- en: Building on that, in this article I will present a framework leveraging a state-of-art
    CNNs and computer vision technologies to aid the detection of metastases in lymph
    nodes. A successful solution holds great promise to reduce the workload of pathologies,
    while at the same time reduce the subjectivity in diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: Methodology and Approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given a [whole slide image](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7522141/)
    of lymph node sections, our objective is to generate a *mask* that indicates potential
    cancerous regions (cells with tumors) in the section. An example is depicted in
    Figure 2, which shows an image of a tissue on the slide alongside a mask where
    the yellow region depicts areas in the tissue that are cancerous.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c369572287e8fc869000899fa93c5581.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Left: A WSI from the dataset | Right: Binary Mask with yellow regions
    indicating cancerous regions — Images by the Author'
  prefs: []
  type: TYPE_NORMAL
- en: '[Image segmentation](https://en.wikipedia.org/wiki/Image_segmentation) is one
    of the classic computer vision tasks, where the objective is to train a neural
    network to output a pixel-wise mask of the image (something similar to the mask
    in Figure 2). There are several deep-learning techniques available for image segmentation
    that are elaborately described in [this](https://arxiv.org/pdf/1907.06119.pdf)
    paper. [TensorFlow](https://www.tensorflow.org/) by Google also has a great [tutorial](https://arxiv.org/pdf/1907.06119.pdf)
    that uses an encoder-decoder approach to image segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using an encoder-decoder which is commonly used in image segmentation
    problems, we will treat this as a [binary classification](https://en.wikipedia.org/wiki/Binary_classification#:~:text=Binary%20classification%20is%20the%20task,basis%20of%20a%20classification%20rule.)
    problem where each custom defined region on the slide is classified as healthy
    or tumorous using a neural network. These individual regions of a who slide image
    can be stitched together to desired mask.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the standard ML process for building the CV model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Collection → Preprocessing → Train-Test Split → Model Selection → Fine-tuning
    & Training → Evaluation**'
  prefs: []
  type: TYPE_NORMAL
- en: Data Collection and Preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset is sourced from the [CAMELYON16 Challenge](https://camelyon16.grand-challenge.org/Data/)
    which as per the [challenge website](https://camelyon16.grand-challenge.org/Data/)
    *contains a total of 400 whole-slide images (WSIs) of sentinel lymph nodes collected
    in* [*Radboud University Medical Center*](https://www.radboudumc.nl/en/patient-care)
    *(Nijmegen, the Netherlands), and the* [*University Medical Center Utrech*](https://www.umcutrecht.nl/en/)*t
    (Utrecht, the Netherlands)”.*
  prefs: []
  type: TYPE_NORMAL
- en: Whole-slide images are stored in a multi-resolution pyramid structure and each
    image file contains multiple downsampled versions of the original image. Each
    image in the pyramid is stored as a series of tiles to facilitate rapid retrieval
    of subregions of the image (see Figure 3 for illustration).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: More information about Whole Slide Imaging can be found [here](https://www.mbfbioscience.com/whole-slide-imaging).
  prefs: []
  type: TYPE_NORMAL
- en: The ground truth for the slides is provided as WSI binary masks indicating the
    regions in the slides that contain cells with tumors (see figure 2 above as an
    example).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d5bd534689fe6d8beb125e87a7dfde3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Illustration of different magnification levels in whole-slide images
    (WSI). Image sourced from [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)'
  prefs: []
  type: TYPE_NORMAL
- en: WSI's in our dataset have 8 zoom levels that allow us to zoom the images from
    1x all the way to 40x. Level 0 is considered the highest resolution (40x) and
    level 7 is the lowest (1x).
  prefs: []
  type: TYPE_NORMAL
- en: Due to their enormous size (each WSI in our dataset ranges well over 2GB), standard
    image tools are incapable of reading and compressing them into system RAM. We
    used the [OpenSlide](https://openslide.org/) library’s implementation in Python
    to efficiently read the images in our dataset and also provide an interface to
    navigate across different zoom levels.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7d1efe9df2449fc5ef4d74b15e66cda.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4- Images by the Author
  prefs: []
  type: TYPE_NORMAL
- en: Training a CNN on a whole dataset of 400 WSIs is computationally very expensive
    (imagine training on 2 x 400 = 800GB dataset). We had access to the free tier
    of [Google Collab](https://colab.research.google.com/) which has limited GPU support
    available. Therefore, we randomly subsampled 22 WSIs from the dataset. At first,
    a set of 22 images might seem like a tiny dataset to accurately train a Convolutional
    Neural network but, as I previously mentioned, we extract small patches from each
    of these enormous WSIs and treat each patch as an independent image that can be
    used to train our model, as depicted in Figure 5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa6cce965e5d887a0d6cd965cbc960a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Each WSI is further split into smaller patches to augment the dataset
    — Images by the Author'
  prefs: []
  type: TYPE_NORMAL
- en: At the highest zoom level (level 0 = 40x zoom), each image is approximately
    62000 x 54000 pixels — extracting 299 x 299 size patches a would give us about
    35,000 individual images from each WSI. We extracted patches from each zoom level.
    As the zoom level increases, the resolution decreases and the number of patches
    we can extract from the WSI also decreases. At level 7, we can extract less than
    200 patches per image.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, every WSI has a lot of empty area where the tissue cells were not
    present. To maintain data sanity, we avoided patches that had less than 30% of
    tissue cells, which was calculate programmatically using intensive of the grey
    area.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset was **balanced** to have approximately the same number of patches
    that contained healthy and tumorous cells. An **80–20 train-test split** was done
    on this final dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Model Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We built **multiple CNN models** which were trained on the image patches generated
    using the mechanism described in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: Objective Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our primary optimization objectives were [sensitivity and recall](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall#:~:text=Recall%20attempts%20to%20answer%20the,has%20a%20recall%20of%201.0.),
    but we also closely monitored the [Area Under the Curve (AUC) of the Receiver
    Operating Characteristic (ROC)](https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/#:~:text=The%20Area%20Under%20the%20Curve,the%20positive%20and%20negative%20classes.)
    to ensure that we were not producing an excessive number of false positives.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of cancer detection, it’s crucial that we minimize the number
    of false negatives, i.e., instances where the model incorrectly classifies a cancerous
    sample as non-cancerous. A high number of false negatives could delay the diagnosis
    and treatment for patients who indeed have cancer. Sensitivity (or recall) measures
    the proportion of actual positives that are correctly identified, and by optimizing
    for high recall, we aim to correctly identify as many actual positive cases as
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: However, focusing on sensitivity alone could lead the model to predict most
    samples as positive, thereby increasing the number of false positives (cases where
    a non-cancerous sample is classified as cancerous). This is undesirable as it
    could lead to unnecessary medical interventions and cause undue anxiety for patients.
    This is where monitoring the AUC-ROC becomes extremely important.
  prefs: []
  type: TYPE_NORMAL
- en: Model Building
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We started-off by building **baseline** was a very simple architecture that
    comprised 2 [convolutional layers](https://www.databricks.com/glossary/convolutional-layer)
    with [max pooling](https://medium.com/codex/what-is-max-polling-in-cnn-is-it-useful-to-use-6f2d6ff44c6)
    and [dropout for regularization](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/).
    To improve over the baseline, we fine-tuned state-of-art image recognition models
    such as as [VGG16](https://arxiv.org/abs/1409.1556) and [Inception v3](https://www.mathworks.com/help/deeplearning/ref/inceptionv3.html)
    on our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: As we had images available at different zoom levels, we **trained multiple models**
    each of which consumed images from one zoom level to see if viewing images at
    a particular zoom level enhances performance of the network. Due to a limited
    number of extracted patches available at lower zoom levels — 3,4,5 images in these
    zoom levels were combined into a single training set. Separate models were built
    for images at 0, 1 and 2 zoom levels.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0f72c2cdaae99b9189870f34b304ed1f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Standard Inception v3 model appended with a Global Max Pool Layer
    and Sigmoid Activation. Inception v3 image sourced from: [https://cloud.google.com/tpu/docs/inception-v3-advanced](https://cloud.google.com/tpu/docs/inception-v3-advanced)'
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, the best performing model was the Inception v3 model pre-trained
    on [ImageNet](http://www.image-net.org/) weights with an additional [Global Max
    Pooling](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/)
    layer (see figure 6). The [sigmoid activation](https://en.wikipedia.org/wiki/Sigmoid_function)
    function takes any range real number and squashes it into a range between 0 and
    1\. This is particularly useful in our scenarios, where we’d like to map predictions
    to probabilities of two classes (0 and 1).
  prefs: []
  type: TYPE_NORMAL
- en: Model Configurations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We did cross-validation to learn the the best hyperparameters for the model.
    The below shows the final configurations of our augmented ImageNet v3, including
    the optimizer, learning rate, rho, epochs and batch size used. By using class
    weights, we enhanced the model’s focus on the minority class (tumorous cases),
    improving its ability to correctly identify and diagnose cancer cases, an essential
    requirement in this critical health context.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0f5faabea27bcfe0794f0bbd54daa0ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Model configurations and hyperparameters — Image by the Author'
  prefs: []
  type: TYPE_NORMAL
- en: Model Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We looked at the loss, AUC and Recall for training runs with different hyperparameters
    and sampled image patches at different zoom levels.
  prefs: []
  type: TYPE_NORMAL
- en: As aforementioned, images at 3,4,5 zoom levels were combined into a single training
    set and separate models were built for images at 0, 1 and 2 zoom levels. Below
    charts show the performance for different zoom levels on the validations set.
    The performance was best at zoom level 1 in terms of the AUC and recall, on the
    modified Imagenet v3.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab05fd357c10e9d288ed7d896af6b367.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Configurations and performance of the final fine-tuned model — Image
    by the Author'
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the model has been fine-tuned, we can use it to generate ‘masks’ for any
    new whole-slide image. To do this, we would first need to generate 299 x 299 resolution
    (the input size for standard Imagenet v3 architecture) patches from the image
    at the zoom level that we are interested in (either level 1 or level 2).
  prefs: []
  type: TYPE_NORMAL
- en: The individual images are then passed through the fine-tuned model to classify
    each of them as containing tumorous or non-tumorous cells. The images are then
    stitched together to generate the mask.
  prefs: []
  type: TYPE_NORMAL
- en: Here are the outputs and the actual masks for two whole-slide images in our
    test set. As you can see, the masks output by our model decently resembles that
    actual mask.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c45ee9bd8a8ac60e3144521e1b734bd9.png)![](../Images/07da7e670c236896702e44ff1fdd6c53.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Model results on a couple of images in the test set — Image by the
    Author'
  prefs: []
  type: TYPE_NORMAL
- en: Concluding remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, we explored how computer vision models can be fine-tune to detect
    cancer metastases on gigapixel pathology images. The below image summarizes the
    workflow for training of the model and the inference process to classify new images.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c2c53b21d2d9564d0f33a57a23899f5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Summary of the training and inference workflow — Image by the Author'
  prefs: []
  type: TYPE_NORMAL
- en: This model integrated in existing workflow of pathologists can act an assistive
    tool, and can be of high clinical relevance especially in organizations with limited
    resource capabilities, and can also be used as the first line of defence to diagnose
    the underlying disease in a timely manner.
  prefs: []
  type: TYPE_NORMAL
- en: Further work needs to be done to assess the impact on real clinical workflows
    and patient outcomes. Nonetheless, we maintain a positive outlook that meticulously
    verified deep learning technologies, alongside thoughtfully crafted clinical instruments,
    have the potential to enhance the precision and accessibility of pathological
    diagnoses globally.
  prefs: []
  type: TYPE_NORMAL
- en: '*Do check out the source code on my Github:* [*https://github.com/saranggupta94/detecting_cancer_metastasis*](https://github.com/saranggupta94/detecting_cancer_metastasis/tree/master)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*You can find the final results to the CAMELYON competition here:* [*https://jamanetwork.com/journals/jama/article-abstract/2665774*](https://jamanetwork.com/journals/jama/article-abstract/2665774)'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you are interested in collaborating on a project or would like to connect,
    feel free to connect with me on* [*LinkedIn*](https://www.linkedin.com/in/sarang-gupta/)
    *or drop me a message at saranggupta94@gmail.com.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Thanks to* [*Niti Jain*](https://www.linkedin.com/in/niti--jain/) *for her
    contributions to this article.*'
  prefs: []
  type: TYPE_NORMAL
