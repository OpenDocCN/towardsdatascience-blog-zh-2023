- en: Detecting Cancer Growth Using AI and Computer Vision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AI和计算机视觉检测癌症增长
- en: 原文：[https://towardsdatascience.com/detecting-cancer-growth-using-ai-and-computer-vision-c88e985f450e](https://towardsdatascience.com/detecting-cancer-growth-using-ai-and-computer-vision-c88e985f450e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/detecting-cancer-growth-using-ai-and-computer-vision-c88e985f450e](https://towardsdatascience.com/detecting-cancer-growth-using-ai-and-computer-vision-c88e985f450e)
- en: 'AI for social good: applications in medical imaging'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI为社会带来的好处：医学影像中的应用
- en: '[](https://medium.com/@saranggupta94?source=post_page-----c88e985f450e--------------------------------)[![Sarang
    Gupta](../Images/23f5c6984019ddc098f198e2c6c55b5f.png)](https://medium.com/@saranggupta94?source=post_page-----c88e985f450e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c88e985f450e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c88e985f450e--------------------------------)
    [Sarang Gupta](https://medium.com/@saranggupta94?source=post_page-----c88e985f450e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@saranggupta94?source=post_page-----c88e985f450e--------------------------------)[![Sarang
    Gupta](../Images/23f5c6984019ddc098f198e2c6c55b5f.png)](https://medium.com/@saranggupta94?source=post_page-----c88e985f450e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c88e985f450e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c88e985f450e--------------------------------)
    [Sarang Gupta](https://medium.com/@saranggupta94?source=post_page-----c88e985f450e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c88e985f450e--------------------------------)
    ·9 min read·Jun 15, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c88e985f450e--------------------------------)
    ·9 分钟阅读·2023年6月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/15f850a7ab92e533f3e3712165b1cb54.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15f850a7ab92e533f3e3712165b1cb54.png)'
- en: Cover image from [unsplash.com](https://unsplash.com/photos/pwcKF7L4-no)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 封面图片来自 [unsplash.com](https://unsplash.com/photos/pwcKF7L4-no)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Breast cancer is one of the deadliest forms of cancer in women. As per the [World
    Health Organization (WHO)](https://www.who.int/news-room/fact-sheets/detail/breast-cancer),
    in 2020 alone, around 2.3 million new cases of invasive breast cancer were diagnosed
    worldwide that resulted in 685,000 deaths.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 乳腺癌是女性中最致命的癌症之一。根据[世界卫生组织 (WHO)](https://www.who.int/news-room/fact-sheets/detail/breast-cancer)，仅在2020年，全球就诊断出约230万例侵袭性乳腺癌，导致68.5万人死亡。
- en: Even though developing countries represent one-half of all breast cancer cases,
    they represent 62% of all deaths caused by breast cancer. Survival of breast cancer
    for at least 5 years after diagnosis ranges from more than [90% in high-income
    countries, to 66% in India and 40% in South Africa](https://www.who.int/news-room/fact-sheets/detail/breast-cancer).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管发展中国家占所有乳腺癌病例的一半，但它们占乳腺癌死亡人数的62%。乳腺癌在诊断后至少存活5年的几率从高收入国家的[90% 到印度的66% 和南非的40%](https://www.who.int/news-room/fact-sheets/detail/breast-cancer)。
- en: '![](../Images/e68a613d03dd048885efa5adc07cd125.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e68a613d03dd048885efa5adc07cd125.png)'
- en: 'Figure 1: Various steps in breast cancer metastases detection as performed
    by pathologists | Top Left: Image from [Camelyon17 challenge](https://camelyon17.grand-challenge.org/Background/)
    | Top Right: Image from [unsplash.com](https://unsplash.com/photos/tGYrlchfObE?utm_source=unsplash&utm_medium=referral&utm_content=creditShareLink)
    | Center: Image from [unsplash.com](https://unsplash.com/photos/bHNUueWud9c) |
    Bottom Left and Bottom Right: Images by the Author'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：病理学家进行乳腺癌转移检测的各个步骤 | 左上：来自 [Camelyon17 challenge](https://camelyon17.grand-challenge.org/Background/)
    的图像 | 右上：来自 [unsplash.com](https://unsplash.com/photos/tGYrlchfObE?utm_source=unsplash&utm_medium=referral&utm_content=creditShareLink)
    的图像 | 中央：来自 [unsplash.com](https://unsplash.com/photos/bHNUueWud9c) 的图像 | 左下和右下：作者提供的图像
- en: A key step in determining what stage the cancer is in is by microscopic examination
    of lymph nodes adjacent to the breast to understand whether cancer has [metastasised](https://en.wikipedia.org/wiki/Metastasis)
    (a medical term meaning spread to other sites in the body). This step is not only
    sensitive, time intensive and laborious but also requires highly-skilled medical
    pathologists. It impacts decisions related to treatment, which includes considerations
    about radiation therapy, chemotherapy, and the potential surgical removal of more
    lymph nodes
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 确定癌症处于哪个阶段的一个关键步骤是通过显微镜检查邻近乳腺的淋巴结，以了解癌症是否[转移](https://en.wikipedia.org/wiki/Metastasis)（医学术语，意为扩散到身体的其他部位）。这个步骤不仅敏感、耗时且劳动密集，还需要高技能的医学病理学家。它影响与治疗相关的决策，包括放疗、化疗和可能的更多淋巴结切除。
- en: With the advent and advancement of AI and computer vision techniques, particularly
    Convolutional Neural Networks (CNNs), we have been able to improve accuracy on
    a wide range of computer vision tasks such as image recognition, object detection,
    and segmentation. These have been beneficial in solving some of the most challenging
    healthcare problems, especially in regions with limited access to advanced medical
    facilities.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着AI和计算机视觉技术的发展，特别是卷积神经网络（CNNs）的进步，我们已经能够提高在图像识别、目标检测和分割等各种计算机视觉任务中的准确性。这些进步在解决一些最具挑战性的医疗问题方面非常有帮助，尤其是在医疗设施有限的地区。
- en: Building on that, in this article I will present a framework leveraging a state-of-art
    CNNs and computer vision technologies to aid the detection of metastases in lymph
    nodes. A successful solution holds great promise to reduce the workload of pathologies,
    while at the same time reduce the subjectivity in diagnosis.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，本文将介绍一个利用最先进的CNNs和计算机视觉技术的框架，帮助检测淋巴结中的转移。一个成功的解决方案有望减轻病理学工作的负担，同时减少诊断中的主观性。
- en: Methodology and Approach
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法论和方法
- en: Given a [whole slide image](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7522141/)
    of lymph node sections, our objective is to generate a *mask* that indicates potential
    cancerous regions (cells with tumors) in the section. An example is depicted in
    Figure 2, which shows an image of a tissue on the slide alongside a mask where
    the yellow region depicts areas in the tissue that are cancerous.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一张[全切片图像](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7522141/)的淋巴结切片，我们的目标是生成一个*掩模*，标识切片中潜在的癌变区域（肿瘤细胞）。图2展示了一个例子，图中显示了切片上的组织图像以及一个掩模，其中黄色区域表示组织中癌变的区域。
- en: '![](../Images/c369572287e8fc869000899fa93c5581.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c369572287e8fc869000899fa93c5581.png)'
- en: 'Figure 2: Left: A WSI from the dataset | Right: Binary Mask with yellow regions
    indicating cancerous regions — Images by the Author'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：左：数据集中的WSI | 右：二值掩模，黄色区域表示癌变区域 — 图片由作者提供
- en: '[Image segmentation](https://en.wikipedia.org/wiki/Image_segmentation) is one
    of the classic computer vision tasks, where the objective is to train a neural
    network to output a pixel-wise mask of the image (something similar to the mask
    in Figure 2). There are several deep-learning techniques available for image segmentation
    that are elaborately described in [this](https://arxiv.org/pdf/1907.06119.pdf)
    paper. [TensorFlow](https://www.tensorflow.org/) by Google also has a great [tutorial](https://arxiv.org/pdf/1907.06119.pdf)
    that uses an encoder-decoder approach to image segmentation.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[图像分割](https://en.wikipedia.org/wiki/Image_segmentation)是经典的计算机视觉任务之一，其目标是训练神经网络输出图像的像素级掩模（类似于图2中的掩模）。有多种深度学习技术可用于图像分割，这些技术在[这篇](https://arxiv.org/pdf/1907.06119.pdf)论文中有详细描述。谷歌的[TensorFlow](https://www.tensorflow.org/)也提供了一个很好的[教程](https://arxiv.org/pdf/1907.06119.pdf)，使用编码器-解码器方法进行图像分割。'
- en: Instead of using an encoder-decoder which is commonly used in image segmentation
    problems, we will treat this as a [binary classification](https://en.wikipedia.org/wiki/Binary_classification#:~:text=Binary%20classification%20is%20the%20task,basis%20of%20a%20classification%20rule.)
    problem where each custom defined region on the slide is classified as healthy
    or tumorous using a neural network. These individual regions of a who slide image
    can be stitched together to desired mask.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这个问题视为一个[二分类](https://en.wikipedia.org/wiki/Binary_classification#:~:text=Binary%20classification%20is%20the%20task,basis%20of%20a%20classification%20rule.)问题，而不是使用常用于图像分割问题的编码器-解码器方法，其中每个自定义定义的区域都通过神经网络分类为健康或肿瘤区域。这些全切片图像的单独区域可以拼接在一起形成所需的掩模。
- en: 'We will use the standard ML process for building the CV model:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用标准的 ML 过程来构建 CV 模型：
- en: '**Data Collection → Preprocessing → Train-Test Split → Model Selection → Fine-tuning
    & Training → Evaluation**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据收集 → 预处理 → 训练-测试拆分 → 模型选择 → 微调与训练 → 评估**'
- en: Data Collection and Preprocessing
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据收集与预处理
- en: The dataset is sourced from the [CAMELYON16 Challenge](https://camelyon16.grand-challenge.org/Data/)
    which as per the [challenge website](https://camelyon16.grand-challenge.org/Data/)
    *contains a total of 400 whole-slide images (WSIs) of sentinel lymph nodes collected
    in* [*Radboud University Medical Center*](https://www.radboudumc.nl/en/patient-care)
    *(Nijmegen, the Netherlands), and the* [*University Medical Center Utrech*](https://www.umcutrecht.nl/en/)*t
    (Utrecht, the Netherlands)”.*
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集来源于 [CAMELYON16 Challenge](https://camelyon16.grand-challenge.org/Data/)，根据
    [挑战网站](https://camelyon16.grand-challenge.org/Data/) *包含了来自* [*Radboud University
    Medical Center*](https://www.radboudumc.nl/en/patient-care) *(荷兰奈梅亨) 和* [*University
    Medical Center Utrecht*](https://www.umcutrecht.nl/en/)*（荷兰乌特勒支）的 400 张全切片图像（WSI）。*
- en: Whole-slide images are stored in a multi-resolution pyramid structure and each
    image file contains multiple downsampled versions of the original image. Each
    image in the pyramid is stored as a series of tiles to facilitate rapid retrieval
    of subregions of the image (see Figure 3 for illustration).
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 整张幻灯片图像以多分辨率金字塔结构存储，每个图像文件包含多个下采样版本的原始图像。金字塔中的每个图像都以一系列瓷砖的形式存储，以便快速检索图像的子区域（参见图
    3 以获取说明）。
- en: More information about Whole Slide Imaging can be found [here](https://www.mbfbioscience.com/whole-slide-imaging).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有关全切片成像的更多信息，请参见 [这里](https://www.mbfbioscience.com/whole-slide-imaging)。
- en: The ground truth for the slides is provided as WSI binary masks indicating the
    regions in the slides that contain cells with tumors (see figure 2 above as an
    example).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 幻灯片的真实标签提供为 WSI 二进制掩模，指示幻灯片中含有肿瘤细胞的区域（参见上图 2 作为示例）。
- en: '![](../Images/9d5bd534689fe6d8beb125e87a7dfde3.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d5bd534689fe6d8beb125e87a7dfde3.png)'
- en: 'Figure 3: Illustration of different magnification levels in whole-slide images
    (WSI). Image sourced from [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：全切片图像（WSI）中不同放大级别的说明。图像来源于 [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)
- en: WSI's in our dataset have 8 zoom levels that allow us to zoom the images from
    1x all the way to 40x. Level 0 is considered the highest resolution (40x) and
    level 7 is the lowest (1x).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集中 WSIs 有 8 个缩放级别，允许我们将图像从 1x 放大到 40x。级别 0 被认为是最高分辨率（40x），级别 7 是最低分辨率（1x）。
- en: Due to their enormous size (each WSI in our dataset ranges well over 2GB), standard
    image tools are incapable of reading and compressing them into system RAM. We
    used the [OpenSlide](https://openslide.org/) library’s implementation in Python
    to efficiently read the images in our dataset and also provide an interface to
    navigate across different zoom levels.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其庞大的尺寸（我们数据集中每个 WSI 的大小都超过 2GB），标准的图像工具无法将其读取并压缩到系统 RAM 中。我们使用了 [OpenSlide](https://openslide.org/)
    库在 Python 中的实现来高效地读取数据集中的图像，并提供了跨不同缩放级别导航的接口。
- en: '![](../Images/c7d1efe9df2449fc5ef4d74b15e66cda.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7d1efe9df2449fc5ef4d74b15e66cda.png)'
- en: Figure 4- Images by the Author
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4 - 作者提供的图像
- en: Training a CNN on a whole dataset of 400 WSIs is computationally very expensive
    (imagine training on 2 x 400 = 800GB dataset). We had access to the free tier
    of [Google Collab](https://colab.research.google.com/) which has limited GPU support
    available. Therefore, we randomly subsampled 22 WSIs from the dataset. At first,
    a set of 22 images might seem like a tiny dataset to accurately train a Convolutional
    Neural network but, as I previously mentioned, we extract small patches from each
    of these enormous WSIs and treat each patch as an independent image that can be
    used to train our model, as depicted in Figure 5.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在包含 400 张 WSI 的整个数据集上训练 CNN 计算开销非常大（想象一下在 2 x 400 = 800GB 数据集上训练）。我们使用了 [Google
    Collab](https://colab.research.google.com/) 的免费版，但 GPU 支持有限。因此，我们从数据集中随机抽取了 22
    张 WSI。起初，22 张图像可能看起来像是一个较小的数据集，难以准确训练卷积神经网络，但正如我之前提到的，我们从这些庞大的 WSI 中提取小补丁，并将每个补丁视为可以用于训练我们模型的独立图像，如图
    5 所示。
- en: '![](../Images/aa6cce965e5d887a0d6cd965cbc960a7.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa6cce965e5d887a0d6cd965cbc960a7.png)'
- en: 'Figure 5: Each WSI is further split into smaller patches to augment the dataset
    — Images by the Author'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：每个 WSI 进一步拆分为更小的补丁以增强数据集 — 作者提供的图像
- en: At the highest zoom level (level 0 = 40x zoom), each image is approximately
    62000 x 54000 pixels — extracting 299 x 299 size patches a would give us about
    35,000 individual images from each WSI. We extracted patches from each zoom level.
    As the zoom level increases, the resolution decreases and the number of patches
    we can extract from the WSI also decreases. At level 7, we can extract less than
    200 patches per image.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在最高的缩放级别（级别0 = 40倍缩放）下，每张图像的尺寸大约为62000 x 54000像素——提取299 x 299大小的补丁将从每个WSI中提供约35,000张单独的图像。我们从每个缩放级别提取补丁。随着缩放级别的增加，分辨率降低，我们可以从WSI中提取的补丁数量也减少。在级别7时，我们每张图像只能提取少于200个补丁。
- en: Furthermore, every WSI has a lot of empty area where the tissue cells were not
    present. To maintain data sanity, we avoided patches that had less than 30% of
    tissue cells, which was calculate programmatically using intensive of the grey
    area.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，每个WSI有很多空白区域，其中没有组织细胞。为了保持数据的准确性，我们避免了组织细胞不足30%的补丁，这些是通过对灰色区域的密集计算得到的。
- en: The dataset was **balanced** to have approximately the same number of patches
    that contained healthy and tumorous cells. An **80–20 train-test split** was done
    on this final dataset.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集被**平衡**为大约包含相同数量的健康和肿瘤细胞的补丁。在这个最终数据集上进行了**80–20的训练-测试分割**。
- en: Model Training
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练
- en: We built **multiple CNN models** which were trained on the image patches generated
    using the mechanism described in the previous section.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了**多个CNN模型**，这些模型在使用前一节中描述的机制生成的图像补丁上进行了训练。
- en: Objective Function
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标函数
- en: Our primary optimization objectives were [sensitivity and recall](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall#:~:text=Recall%20attempts%20to%20answer%20the,has%20a%20recall%20of%201.0.),
    but we also closely monitored the [Area Under the Curve (AUC) of the Receiver
    Operating Characteristic (ROC)](https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/#:~:text=The%20Area%20Under%20the%20Curve,the%20positive%20and%20negative%20classes.)
    to ensure that we were not producing an excessive number of false positives.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要优化目标是[sensitivity and recall](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall#:~:text=Recall%20attempts%20to%20answer%20the,has%20a%20recall%20of%201.0.)，但我们也密切监控[接收器工作特征曲线（ROC）的曲线下面积（AUC）](https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/#:~:text=The%20Area%20Under%20the%20Curve,the%20positive%20and%20negative%20classes.)，以确保我们不会产生过多的假阳性。
- en: In the context of cancer detection, it’s crucial that we minimize the number
    of false negatives, i.e., instances where the model incorrectly classifies a cancerous
    sample as non-cancerous. A high number of false negatives could delay the diagnosis
    and treatment for patients who indeed have cancer. Sensitivity (or recall) measures
    the proportion of actual positives that are correctly identified, and by optimizing
    for high recall, we aim to correctly identify as many actual positive cases as
    possible.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在癌症检测的背景下，最重要的是最小化假阴性的数量，即模型错误地将癌症样本分类为非癌症样本的实例。假阴性的数量过高可能会延迟真正患有癌症患者的诊断和治疗。灵敏度（或召回率）衡量的是实际阳性样本被正确识别的比例，通过优化高召回率，我们旨在正确识别尽可能多的实际阳性病例。
- en: However, focusing on sensitivity alone could lead the model to predict most
    samples as positive, thereby increasing the number of false positives (cases where
    a non-cancerous sample is classified as cancerous). This is undesirable as it
    could lead to unnecessary medical interventions and cause undue anxiety for patients.
    This is where monitoring the AUC-ROC becomes extremely important.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，单纯关注灵敏度可能导致模型将大多数样本预测为阳性，从而增加假阳性的数量（将非癌症样本错误分类为癌症样本）。这是不理想的，因为它可能导致不必要的医疗干预，并引起患者的不必要焦虑。这就是监控AUC-ROC变得极其重要的地方。
- en: Model Building
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型构建
- en: We started-off by building **baseline** was a very simple architecture that
    comprised 2 [convolutional layers](https://www.databricks.com/glossary/convolutional-layer)
    with [max pooling](https://medium.com/codex/what-is-max-polling-in-cnn-is-it-useful-to-use-6f2d6ff44c6)
    and [dropout for regularization](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/).
    To improve over the baseline, we fine-tuned state-of-art image recognition models
    such as as [VGG16](https://arxiv.org/abs/1409.1556) and [Inception v3](https://www.mathworks.com/help/deeplearning/ref/inceptionv3.html)
    on our dataset.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从构建**基线**模型开始，该模型是一个非常简单的架构，由 2 个 [卷积层](https://www.databricks.com/glossary/convolutional-layer)
    和 [最大池化](https://medium.com/codex/what-is-max-polling-in-cnn-is-it-useful-to-use-6f2d6ff44c6)
    以及 [dropout 正则化](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)
    组成。为了改进基线，我们在我们的数据集上微调了先进的图像识别模型，如 [VGG16](https://arxiv.org/abs/1409.1556) 和
    [Inception v3](https://www.mathworks.com/help/deeplearning/ref/inceptionv3.html)。
- en: As we had images available at different zoom levels, we **trained multiple models**
    each of which consumed images from one zoom level to see if viewing images at
    a particular zoom level enhances performance of the network. Due to a limited
    number of extracted patches available at lower zoom levels — 3,4,5 images in these
    zoom levels were combined into a single training set. Separate models were built
    for images at 0, 1 and 2 zoom levels.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有不同缩放级别的图像，我们**训练了多个模型**，每个模型使用来自一个缩放级别的图像，以查看在特定缩放级别查看图像是否能提升网络性能。由于在较低缩放级别提取的补丁数量有限——这些缩放级别的
    3、4、5 张图像被合并为一个训练集。针对 0、1 和 2 缩放级别的图像分别构建了单独的模型。
- en: '![](../Images/0f72c2cdaae99b9189870f34b304ed1f.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f72c2cdaae99b9189870f34b304ed1f.png)'
- en: 'Figure 6: Standard Inception v3 model appended with a Global Max Pool Layer
    and Sigmoid Activation. Inception v3 image sourced from: [https://cloud.google.com/tpu/docs/inception-v3-advanced](https://cloud.google.com/tpu/docs/inception-v3-advanced)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：标准 Inception v3 模型附加了全局最大池化层和 Sigmoid 激活。Inception v3 图像来源：[https://cloud.google.com/tpu/docs/inception-v3-advanced](https://cloud.google.com/tpu/docs/inception-v3-advanced)
- en: Interestingly, the best performing model was the Inception v3 model pre-trained
    on [ImageNet](http://www.image-net.org/) weights with an additional [Global Max
    Pooling](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/)
    layer (see figure 6). The [sigmoid activation](https://en.wikipedia.org/wiki/Sigmoid_function)
    function takes any range real number and squashes it into a range between 0 and
    1\. This is particularly useful in our scenarios, where we’d like to map predictions
    to probabilities of two classes (0 and 1).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，表现最佳的模型是预训练的 Inception v3 模型，使用了 [ImageNet](http://www.image-net.org/)
    权重，并添加了一个 [全局最大池化](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/)
    层（见图 6）。[Sigmoid 激活](https://en.wikipedia.org/wiki/Sigmoid_function) 函数将任何范围的实数压缩到
    0 和 1 之间。这在我们的场景中特别有用，因为我们希望将预测映射到两个类别（0 和 1）的概率上。
- en: Model Configurations
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型配置
- en: We did cross-validation to learn the the best hyperparameters for the model.
    The below shows the final configurations of our augmented ImageNet v3, including
    the optimizer, learning rate, rho, epochs and batch size used. By using class
    weights, we enhanced the model’s focus on the minority class (tumorous cases),
    improving its ability to correctly identify and diagnose cancer cases, an essential
    requirement in this critical health context.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了交叉验证，以确定模型的最佳超参数。下面展示了我们增强版 ImageNet v3 的最终配置，包括所用的优化器、学习率、rho、训练轮次和批量大小。通过使用类别权重，我们增强了模型对少数类（肿瘤病例）的关注，提高了其正确识别和诊断癌症病例的能力，这是在这个关键健康背景下的基本要求。
- en: '![](../Images/0f5faabea27bcfe0794f0bbd54daa0ca.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f5faabea27bcfe0794f0bbd54daa0ca.png)'
- en: 'Figure 7: Model configurations and hyperparameters — Image by the Author'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：模型配置和超参数——图像由作者提供
- en: Model Evaluation
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估
- en: We looked at the loss, AUC and Recall for training runs with different hyperparameters
    and sampled image patches at different zoom levels.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们查看了不同超参数的训练运行的损失、AUC 和召回率，并在不同缩放级别上取样了图像补丁。
- en: As aforementioned, images at 3,4,5 zoom levels were combined into a single training
    set and separate models were built for images at 0, 1 and 2 zoom levels. Below
    charts show the performance for different zoom levels on the validations set.
    The performance was best at zoom level 1 in terms of the AUC and recall, on the
    modified Imagenet v3.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在 3、4、5 个缩放级别下的图像被合并为一个训练集，并为 0、1 和 2 个缩放级别的图像建立了不同的模型。下面的图表显示了不同缩放级别在验证集上的表现。根据修改后的
    Imagenet v3，缩放级别 1 的 AUC 和召回率表现最佳。
- en: '![](../Images/ab05fd357c10e9d288ed7d896af6b367.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab05fd357c10e9d288ed7d896af6b367.png)'
- en: 'Figure 8: Configurations and performance of the final fine-tuned model — Image
    by the Author'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：最终微调模型的配置和性能 — 作者提供的图片
- en: Inference
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理
- en: Once the model has been fine-tuned, we can use it to generate ‘masks’ for any
    new whole-slide image. To do this, we would first need to generate 299 x 299 resolution
    (the input size for standard Imagenet v3 architecture) patches from the image
    at the zoom level that we are interested in (either level 1 or level 2).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型经过微调，我们可以使用它为任何新的全切片图像生成“掩模”。为此，我们首先需要从感兴趣的缩放级别（第 1 级或第 2 级）生成 299 x 299
    分辨率（标准 Imagenet v3 架构的输入大小）的图像块。
- en: The individual images are then passed through the fine-tuned model to classify
    each of them as containing tumorous or non-tumorous cells. The images are then
    stitched together to generate the mask.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这些单独的图像随后会通过经过微调的模型进行分类，判断每张图像是否包含肿瘤细胞或非肿瘤细胞。然后将这些图像拼接在一起以生成掩模。
- en: Here are the outputs and the actual masks for two whole-slide images in our
    test set. As you can see, the masks output by our model decently resembles that
    actual mask.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了我们测试集中的两个全切片图像的输出结果和实际掩模。正如你所见，我们模型输出的掩模与实际掩模相当相似。
- en: '![](../Images/c45ee9bd8a8ac60e3144521e1b734bd9.png)![](../Images/07da7e670c236896702e44ff1fdd6c53.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c45ee9bd8a8ac60e3144521e1b734bd9.png)![](../Images/07da7e670c236896702e44ff1fdd6c53.png)'
- en: 'Figure 9: Model results on a couple of images in the test set — Image by the
    Author'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：模型在测试集中的几张图像上的结果 — 作者提供的图片
- en: Concluding remarks
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this post, we explored how computer vision models can be fine-tune to detect
    cancer metastases on gigapixel pathology images. The below image summarizes the
    workflow for training of the model and the inference process to classify new images.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们探讨了计算机视觉模型如何通过微调来检测 gigapixel 病理图像中的癌症转移。下面的图像总结了模型训练的工作流程和分类新图像的推理过程。
- en: '![](../Images/8c2c53b21d2d9564d0f33a57a23899f5.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c2c53b21d2d9564d0f33a57a23899f5.png)'
- en: 'Figure 9: Summary of the training and inference workflow — Image by the Author'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：训练和推理工作流程的总结 — 作者提供的图片
- en: This model integrated in existing workflow of pathologists can act an assistive
    tool, and can be of high clinical relevance especially in organizations with limited
    resource capabilities, and can also be used as the first line of defence to diagnose
    the underlying disease in a timely manner.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 将该模型整合到现有的病理学家工作流程中，可以作为辅助工具，特别是在资源有限的机构中具有很高的临床相关性，也可以作为及时诊断潜在疾病的第一道防线。
- en: Further work needs to be done to assess the impact on real clinical workflows
    and patient outcomes. Nonetheless, we maintain a positive outlook that meticulously
    verified deep learning technologies, alongside thoughtfully crafted clinical instruments,
    have the potential to enhance the precision and accessibility of pathological
    diagnoses globally.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 需要进一步工作来评估对实际临床工作流程和患者结果的影响。尽管如此，我们对经过细致验证的深度学习技术与精心设计的临床工具的积极前景保持乐观，这些技术和工具有可能提升全球病理诊断的精准度和可及性。
- en: '*Do check out the source code on my Github:* [*https://github.com/saranggupta94/detecting_cancer_metastasis*](https://github.com/saranggupta94/detecting_cancer_metastasis/tree/master)*.*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*请查看我在 Github 上的源代码：* [*https://github.com/saranggupta94/detecting_cancer_metastasis*](https://github.com/saranggupta94/detecting_cancer_metastasis/tree/master)*.*'
- en: '*You can find the final results to the CAMELYON competition here:* [*https://jamanetwork.com/journals/jama/article-abstract/2665774*](https://jamanetwork.com/journals/jama/article-abstract/2665774)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以在这里找到 CAMELYON 竞赛的最终结果：* [*https://jamanetwork.com/journals/jama/article-abstract/2665774*](https://jamanetwork.com/journals/jama/article-abstract/2665774)'
- en: '*If you are interested in collaborating on a project or would like to connect,
    feel free to connect with me on* [*LinkedIn*](https://www.linkedin.com/in/sarang-gupta/)
    *or drop me a message at saranggupta94@gmail.com.*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你对合作项目感兴趣或想要联系我，可以随时通过* [*LinkedIn*](https://www.linkedin.com/in/sarang-gupta/)
    *与我联系，或者发邮件至 saranggupta94@gmail.com。*'
- en: '*Thanks to* [*Niti Jain*](https://www.linkedin.com/in/niti--jain/) *for her
    contributions to this article.*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢* [*Niti Jain*](https://www.linkedin.com/in/niti--jain/) *对本文的贡献。*'
