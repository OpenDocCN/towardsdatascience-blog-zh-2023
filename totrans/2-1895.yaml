- en: 'Specialized LLMs: ChatGPT, LaMDA, Galactica, Codex, Sparrow, and More'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 专门化的LLM：ChatGPT、LaMDA、Galactica、Codex、Sparrow等
- en: 原文：[https://towardsdatascience.com/specialized-llms-chatgpt-lamda-galactica-codex-sparrow-and-more-ccccdd9f666f](https://towardsdatascience.com/specialized-llms-chatgpt-lamda-galactica-codex-sparrow-and-more-ccccdd9f666f)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/specialized-llms-chatgpt-lamda-galactica-codex-sparrow-and-more-ccccdd9f666f](https://towardsdatascience.com/specialized-llms-chatgpt-lamda-galactica-codex-sparrow-and-more-ccccdd9f666f)
- en: Simple techniques for creating better, domain-specific LLMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建更好领域特定LLM的简单技巧
- en: '[](https://wolfecameron.medium.com/?source=post_page-----ccccdd9f666f--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----ccccdd9f666f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ccccdd9f666f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ccccdd9f666f--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----ccccdd9f666f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----ccccdd9f666f--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----ccccdd9f666f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ccccdd9f666f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ccccdd9f666f--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----ccccdd9f666f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ccccdd9f666f--------------------------------)
    ·30 min read·Jan 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----ccccdd9f666f--------------------------------)
    ·阅读时间30分钟·2023年1月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/1469bf9ac8a88095d227844718d069f4.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1469bf9ac8a88095d227844718d069f4.png)'
- en: (Photo by [NASA](https://unsplash.com/es/@nasa?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/images/nature/space?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （照片由[NASA](https://unsplash.com/es/@nasa?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)提供，来源于[Unsplash](https://unsplash.com/images/nature/space?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)）
- en: Large language models (LLMs) are incredibly-useful, task-agnostic foundation
    models. But, *how much can we actually accomplish with a generic model?* These
    models are adept at solving common natural language benchmarks that we see within
    the deep learning literature. But, using LLMs practically usually requires that
    the model be taught new behavior that is relevant to a particular application.
    Within this overview, we will explore methods of specializing and improving LLMs
    for a variety of use cases.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）是非常有用的任务无关的基础模型。但是，*我们实际上能用一个通用模型完成多少任务？* 这些模型擅长解决我们在深度学习文献中看到的常见自然语言基准。然而，实际使用LLM通常需要教给模型与特定应用相关的新行为。在这个概述中，我们将探讨针对各种使用案例专门化和改进LLM的方法。
- en: We can modify the behavior of LLMs by using techniques like domain-specific
    pre-training, model alignment, and supervised fine-tuning. These methods can be
    used to eliminate known limitations of LLMs (e.g., generating incorrect/biased
    info), modify LLM behavior to better suit our needs, or even inject specialized
    knowledge into an LLM such that it becomes a domain expert.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用领域特定的预训练、模型对齐和监督微调等技术来修改LLM的行为。这些方法可以用来消除LLM已知的限制（例如，生成不正确/有偏见的信息），调整LLM的行为以更好地满足我们的需求，甚至将专业知识注入LLM，使其成为领域专家。
- en: 'The concept of creating specialized LLMs for particular applications has been
    heavily explored in recent literature. Though many different methodologies exist,
    they share a common theme: making LLMs more practically viable and useful. Though
    the definition of “useful” is highly variable across applications and human users,
    we will see that several techniques exist that can be used to adapt and modify
    existing, pre-trained LLMs, such that their performance and ease-of-use is drastically
    improved in a variety of applications.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的文献中对为特定应用创建专门的LLM（大型语言模型）的概念进行了深入探讨。尽管存在许多不同的方法，但它们都有一个共同的主题：使LLM在实际应用中更具可行性和实用性。尽管“实用”的定义在不同应用和用户中差异很大，但我们将看到，存在几种技术可以用来调整和修改现有的预训练LLM，从而在各种应用中显著提升其性能和易用性。
- en: '![](../Images/b82cb8b571acf5824a65c4188e6d433f.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b82cb8b571acf5824a65c4188e6d433f.png)'
- en: (from [6] and [12])
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[6]和[12]）
- en: Background
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: 'We have covered the topic of language models (LMs) and large language models
    (LLMs) in recent, prior posts on this topic. See the references below for each
    of these overviews:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在最近的帖子中讨论了语言模型（LMs）和大语言模型（LLMs）的主题。有关这些概述，请参见以下参考文献：
- en: 'Language Models: GPT and GPT-2 [[blog](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2)]'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型：GPT和GPT-2 [[博客](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2)]
- en: Language Model Scaling Laws and GPT-3 [[blog](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt)]
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型的扩展定律与GPT-3 [[博客](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt)]
- en: 'Moderns LLMs: MT-NLG, Chinchilla, Gopher, and More [[blog](https://cameronrwolfe.substack.com/p/modern-llms-mt-nlg-chinchilla-gopher)]'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代大语言模型：MT-NLG、Chinchilla、Gopher及更多 [[博客](https://cameronrwolfe.substack.com/p/modern-llms-mt-nlg-chinchilla-gopher)]
- en: We will briefly summarize these ideas in this overview. But, we will mostly
    shift our focus towards applications where basic language modeling alone falls
    short.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这个概述中简要总结这些观点。但我们主要将关注于基本语言建模单独难以胜任的应用领域。
- en: We can only accomplish so much by just teaching a model to predict the next
    word in a sequence. To elicit particular behavior, we need to adopt new approaches
    of training language models that are a bit more specific. In addition to being
    highly-effective at improving language model quality, we will see that these alternative
    approaches of modifying/fine-tuning language models are pretty cheap compared
    to pre-training them from scratch.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅教一个模型预测序列中的下一个词，我们能实现的有限。为了引发特定行为，我们需要采用一些新的训练语言模型的方法，这些方法更具针对性。除了在提高语言模型质量方面非常有效外，我们将看到，这些替代的方法相比从头开始进行预训练要便宜很多。
- en: What are language models?
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是语言模型？
- en: '![](../Images/79a12670cd4cc95db0f47c817ebe4816.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79a12670cd4cc95db0f47c817ebe4816.png)'
- en: Self-supervised pre-training of a language model (created by author)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督的语言模型预训练（由作者创建）
- en: '**The basic setup.** Most modern language models that we will be talking about
    utilize a [decoder-only transformer architecture](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2)
    [1]. These models are trained to perform a single, simple task: predicting the
    next word (or token) in a sequence. To teach the model to do this well, we gather
    a large dataset of unlabeled text from the internet and train the model using
    a self-supervised language modeling objective. Put simply, this just means that
    we:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**基本设置。** 我们将讨论的大多数现代语言模型采用[仅解码器的Transformer架构](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2)[1]。这些模型被训练来执行一个简单的任务：预测序列中的下一个词（或标记）。为了让模型做到这一点，我们从互联网上收集大量未标记的文本数据，并使用自监督的语言建模目标来训练模型。简单来说，这意味着我们：'
- en: Sample some text from our dataset
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们的数据集中抽取一些文本
- en: Try to predict the next word with our model
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试用我们的模型预测下一个词
- en: Update our model based on the correct next word
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据正确的下一个词更新我们的模型
- en: If we continually repeat this process with a sufficiently large and diverse
    dataset, we will end up with a high-quality LM that contains a relatively nuanced
    and useful understanding of language.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不断重复这个过程，使用足够大和多样化的数据集，我们将得到一个高质量的语言模型，具有相对细致和有用的语言理解。
- en: '**Why is this useful?** Although LMs are obviously good at generating text,
    we might be wondering whether they are useful for anything else. *What can we
    actually accomplish by just predicting the most likely next word in a sequence?*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**这有什么用？** 虽然语言模型在生成文本方面显然很出色，但我们可能会怀疑它们是否对其他方面也有用。*我们究竟能通过仅仅预测序列中最可能的下一个词来完成什么？*'
- en: Actually, we can solve many different tasks with LMs. This is because their
    input-output structure (i.e., take text as input, produce text as output) is incredibly
    generic, and many tasks can be re-formulated to fit this structure via prompting
    techniques. Consider, for example, the following inputs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以用语言模型解决许多不同的任务。这是因为它们的输入-输出结构（即，以文本为输入，产生文本为输出）非常通用，许多任务可以通过提示技术重新制定以适应这种结构。例如，考虑以下输入。
- en: '“Identify whether this sentence has a positive or negative sentiment: <sentence>”'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “识别这个句子的情感是积极的还是消极的：<sentence>”
- en: '“Translate the following sentence from English into French: <sentence>”'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “将以下句子从英文翻译成法文：<sentence>”
- en: '“Summarize the following article: <article>”'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “总结以下文章：<article>”
- en: Using such input prompts, we can take common language understanding tasks and
    formulate them into an LM-friendly, text-to-text structure — the most likely output
    from the LM should solve our desired problem. With this approach, we can solve
    a wide range of problems from multiple choice question answering to document summarization,
    as is shown by GPT-3 [2].
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这样的输入提示，我们可以将常见的语言理解任务转化为 LM 友好的、文本到文本的结构——LM 最可能的输出应该解决我们期望的问题。通过这种方法，我们可以解决从选择题回答到文档总结等广泛的问题，正如
    GPT-3 所示 [2]。
- en: '![](../Images/13d7a08623e97fb7ea471bece5ed5417.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13d7a08623e97fb7ea471bece5ed5417.png)'
- en: (from [2])
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [2]）
- en: To improve performance, we can include examples of correct output within our
    prompt (i.e., a one/few-shot learning approach) or fine-tune the LM to solve a
    particular task. However, fine-tuning forces the LM to specialize in solving a
    single task, requiring a separate model to be fine-tuned for each new task; see
    above.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高性能，我们可以在提示中包含正确输出的示例（即，一次/少量示例学习方法），或者对 LM 进行微调以解决特定任务。然而，微调会强迫 LM 专注于解决单一任务，这需要为每个新任务微调一个单独的模型；见上文。
- en: '**Scaling up.** Earlier LMs like [GPT and GPT-2](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2)
    showed a lot of promise [3,4], but their zero/few-shot performance was poor. However,
    later research indicated that LM performance should improve smoothly with scale
    [5] — larger LMs are better! This was confirmed by [GPT-3](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt)
    [2], a 175 billion parameter model (i.e., much bigger than any previous model)
    that was really good at few-shot learning. The secret to this success was:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**规模化。** 早期的 LM 如 [GPT 和 GPT-2](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2)
    展现了很大的潜力 [3,4]，但它们的零样本/少样本表现较差。然而，后来的研究表明 LM 的表现应该随着规模的增加而平稳提升 [5]——更大的 LLM 更好！这在
    [GPT-3](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt)
    [2] 中得到了确认，这是一个具有 1750 亿参数的模型（即，比任何以前的模型都大得多），在少量示例学习方面表现非常好。这一成功的秘密在于：'
- en: Obtaining a big, diverse dataset of unlabeled text
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获得一个大而多样化的未标记文本数据集
- en: Pre-training a much larger model over this dataset using a language modeling
    objective
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用语言建模目标在这个数据集上预训练一个更大的模型
- en: Using prompting to solve tasks via few-shot learning
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用提示通过少量示例学习解决任务
- en: Using these simple ingredients, we could train large language models (LLMs)
    that achieved impressive performance across many tasks. These LLMs were powerful,
    task-agnostic [foundation models](https://crfm.stanford.edu/).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些简单的要素，我们可以训练出在许多任务中表现出色的大型语言模型（LLMs）。这些 LLM 是强大的、与任务无关的 [基础模型](https://crfm.stanford.edu/)。
- en: Given that larger LLMs perform well, later work explored even larger models.
    The results (arguably) weren’t groundbreaking. But, if we combine larger models
    with better pre-training datasets, LLM quality improves quite a bit! By obtaining
    much better pre-training corpora (e.g., Massive Text) and pre-training LLMs over
    more data, we could obtain models like [Chinchilla](https://cameronrwolfe.substack.com/p/modern-llms-mt-nlg-chinchilla-gopher)
    that are both smaller and more performance relative to GPT-3.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于更大的 LLM 表现良好，后续的研究探索了更大的模型。结果（可以说）并没有突破性的进展。但是，如果我们将更大的模型与更好的预训练数据集结合起来，LLM
    的质量会显著提高！通过获得更好的预训练语料库（例如，Massive Text）并在更多数据上预训练 LLM，我们可以获得如 [Chinchilla](https://cameronrwolfe.substack.com/p/modern-llms-mt-nlg-chinchilla-gopher)
    这样既更小又更高效的模型，相对于 GPT-3 更具性能。
- en: Where do generic LLMs fall short?
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通用 LLM 有哪些不足之处？
- en: 'This generic paradigm for pre-training LLMs and using them to solve a variety
    of problems downstream is great. But, we run into problems when trying to accomplish
    something more specific than general linguistic understanding. For the purposes
    of this post, we will focus on two main areas where this desire for more specialized
    LLM behavior arises:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这种预训练 LLM 并用它们解决各种下游问题的通用范式非常好。但在尝试完成比一般语言理解更具体的任务时，我们会遇到问题。为了这篇文章的目的，我们将重点关注两个主要领域，这些领域中对更专业
    LLM 行为的需求会出现：
- en: Alignment
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对齐
- en: Domain Specialization
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 领域专业化
- en: '![](../Images/f2a6ee4970a375372032b78eefe99298.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2a6ee4970a375372032b78eefe99298.png)'
- en: Aligning a language model to human values (created by author)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 将语言模型对齐到人类价值观（由作者创建）
- en: '**alignment.** Oftentimes, a generic LLM will generate output that is undesirable
    to a human that is interacting with the model. For example, we might want to:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**对齐。** 许多时候，通用 LLM 会生成对与模型互动的人不期望的输出。例如，我们可能想要：'
- en: Prevent our LLM from being racist
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防止我们的 LLM 存在种族歧视
- en: Teach the model to follow and execute human directions
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教会模型遵循和执行人类指令
- en: Avoid the generation of factually incorrect output
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免生成事实错误的输出
- en: In other words, we might want to align the LLM to the particular goals or values
    of humans who are using the model; see above.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可能希望将LLM对齐到使用模型的人类的特定目标或价值观；见上文。
- en: After powerful LLM foundation models like GPT-3 were created, the focus of LLM
    research quickly pivoted towards a focus on this problem of alignment. Although
    a bit vague to describe (i.e., how do we define the rules to which we align LLM
    behavior?), the idea of alignment is quite powerful. We can simply teach our LLM
    to behave in a way that is more safe and useful for us as humans.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在像GPT-3这样的强大LLM基础模型创建之后，LLM研究的重点迅速转向对齐问题。虽然描述起来有些模糊（即，我们如何定义我们对齐LLM行为的规则？），但对齐的想法非常强大。我们可以简单地教我们的LLM以对我们人类更安全和有用的方式进行行为。
- en: The language modeling objective used for many recent large LMs — predicting
    the next token on a webpage from the internet — is different from the objective
    “follow the user’s instructions helpfully and safely” — from [6]
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 许多最近大型语言模型使用的语言建模目标——从互联网网页上预测下一个词——与“帮助和安全地遵循用户指令”的目标不同——来自 [6]
- en: '**Domain-specific models.** Beyond alignment, we can consider the deployment
    of LLMs in specialized domains. A generic LLM like GPT-3 cannot successfully generate
    legal documents or summarize medical information — specialized domains like law
    or medicine contain lots of complex domain knowledge that is not present within
    a generic pre-training corpus. For such an application, we need to somehow create
    an LLM that has a deeper knowledge of the particular domain in which we are interested.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**领域特定模型。** 除了对齐之外，我们可以考虑在专业领域中部署LLMs。像GPT-3这样的通用LLM无法成功生成法律文件或总结医学信息——像法律或医学这样的专业领域包含大量复杂的领域知识，这些知识在通用预训练语料库中并不存在。对于这种应用，我们需要以某种方式创建一个对我们感兴趣的特定领域有更深知识的LLM。'
- en: Refining LLM behavior
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精细化LLM行为
- en: '![](../Images/1132b0f9d212da0200bcb744fa6baa04.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1132b0f9d212da0200bcb744fa6baa04.png)'
- en: (from [13])
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [13]）
- en: 'Given that we might want to align our LLM to particular goals or enable more
    specialized behavior, there are probably two major questions that will immediately
    come to mind:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们可能希望将我们的LLM对齐到特定目标或实现更专业的行为，可能会立即想到两个主要问题：
- en: How do we do this?
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何做到这一点？
- en: How much is it going to cost?
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将花费多少？
- en: The first question here is a bit more complex to address because there are several
    viable answers.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题有点复杂，因为有几种可行的答案。
- en: '**Domain-specific pre-training.** If we want our LLM to understand a particular
    area really well, the easiest thing to do would be to *(i)* collect a lot of raw
    data pertaining to this domain and *(ii)* train the model using a language modeling
    objective over this data. Such a process is really similar to generic LLM pre-training,
    but we are now using a domain-specific corpus.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**领域特定预训练。** 如果我们希望我们的LLM对某一特定领域有很好的理解，最简单的做法是*(i)* 收集大量与该领域相关的原始数据，并*(ii)*
    使用语言建模目标对这些数据进行训练。这样的过程与通用LLM预训练非常相似，但我们现在使用的是领域特定的语料库。'
- en: By learning from a more specific corpus, we can begin to capture more relevant
    information within our model, thus enabling more specialized behavior. This could
    include things like “prompt pre-training”, as outlined in the figure above, where
    we further pre-train the LLMs over specific examples of prompts that match the
    use cases that it will encounter in the wild.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过学习更具体的语料库，我们可以开始在模型中捕捉到更多相关的信息，从而实现更专业的行为。这可能包括“提示预训练”等内容，如上图所示，我们会在与实际使用场景匹配的特定提示示例上进一步预训练LLMs。
- en: 'When performing domain-specific pre-training, we have two basic options:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行领域特定预训练时，我们有两个基本选项：
- en: Initialize the LLM with generic pre-training, then perform further pre-training
    on domain-specific data.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用通用预训练初始化LLM，然后在领域特定数据上进行进一步预训练。
- en: Pre-train the LLM from scratch from domain-specific data.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从领域特定数据开始从零预训练LLM。
- en: Depending on the application, either of these approaches may work best, though
    initializing with pre-trained LLM parameters tends to yield faster convergence
    (and sometimes better performance).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 根据应用情况，这些方法中的任何一种可能效果最好，尽管用预训练LLM参数初始化通常会导致更快的收敛（有时表现更好）。
- en: '**Reinforcement learning from human feedback.** Just using a language modeling
    objective, we cannot explicitly do things like teach the LLM to follow instructions
    or avoid incorrect statements. To accomplish these more nuanced (and potentially
    vague) goals, recent research has adopted a reinforcement learning (RL) approach.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**从人类反馈中学习的强化学习。** 仅仅使用语言建模目标，我们无法明确地做诸如教 LLM 遵循指令或避免错误陈述等事情。为了实现这些更微妙（且可能模糊）的目标，最近的研究采用了强化学习（RL）方法。'
- en: For those who aren’t familiar with RL, check out the link [here](https://www.synopsys.com/ai/what-is-reinforcement-learning.html)
    for a basic overview of the idea. For LLM applications, the model’s parameters
    correspond to our policy. A human will provide an input prompt to the LLM, the
    LLM will generate output in response, and the reward is determined by whether
    the LLM’s output is desirable to a human.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些不熟悉 RL 的人，可以查看链接 [这里](https://www.synopsys.com/ai/what-is-reinforcement-learning.html)
    以了解该概念的基本概述。对于 LLM 应用，模型的参数对应于我们的策略。人类将向 LLM 提供输入提示，LLM 将生成响应输出，奖励由 LLM 的输出是否符合人类期望来决定。
- en: Although RL isn’t a necessity (i.e., several works specialize or align LLMs
    without it), it is useful because we can change the definition of “desirable”
    to be pretty much anything. For example, we could reward the LLM for making factually
    correct statements, avoiding racist behavior, following instructions, or producing
    interesting output. Such objectives are difficult to capture via a differentiable
    loss function that can be optimized with gradient descent. With RL, however, we
    just reward the model for the behavior that we like, which provides a great deal
    of flexibility.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 RL 并不是必须的（即，若干研究专注于不使用 RL 的 LLM 对齐），但它非常有用，因为我们可以将“期望”的定义更改为几乎任何东西。例如，我们可以奖励
    LLM 生成事实正确的陈述、避免种族主义行为、遵循指令或产生有趣的输出。这些目标通过可以用梯度下降优化的可区分损失函数很难捕捉。然而，使用 RL 时，我们只是奖励模型我们喜欢的行为，这提供了极大的灵活性。
- en: '![](../Images/42021cdcc614e13c0f99672328ce20bb.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42021cdcc614e13c0f99672328ce20bb.png)'
- en: (from [6])
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: (来源 [6])
- en: Most research uses an approach called reinforcement learning from human feedback
    (RLHF) for adapting LLMs; see above. The basic idea behind RLHF is to use humans
    to provide feedback from which the model will learn via RL. More specifically,
    the model is trained using [Proximal Policy Optimization (PPO)](https://openai.com/blog/openai-baselines-ppo/),
    which is a recent, efficient method for RL.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数研究使用一种称为从人类反馈中学习的强化学习（RLHF）的方法来调整 LLM；见上文。RLHF 的基本思想是利用人类提供反馈，通过 RL 使模型学习。更具体地说，模型使用
    [近端策略优化（PPO）](https://openai.com/blog/openai-baselines-ppo/) 进行训练，这是一种最近的、有效的
    RL 方法。
- en: '**Supervised fine-tuning.** We can also directly fine-tune LLMs to accomplish
    a particular task. This was common with LMs like GPT [3] that followed a pre-training
    and fine-tuning approach, where we fine-tune a pre-trained LM to solve each downstream
    task. More recently, we see supervised fine-tuning being used to modify LLM behavior,
    rather than to specialize to a particular task.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督微调。** 我们还可以直接微调 LLM 来完成特定任务。这在如 GPT [3] 的语言模型中很常见，这些模型采用预训练和微调的方法，我们对预训练的语言模型进行微调以解决每个下游任务。最近，我们看到监督微调被用来修改
    LLM 行为，而不是专门用于特定任务。'
- en: For example, what if we want to create a really good LLM chatbot? One potential
    approach is to obtain a generic, pre-trained LLM, then show this model a bunch
    of high-quality examples of dialogue. The LLM can then be trained over these dialogue
    examples, which enables the model to learn more specialized behavior that is specific
    to this application and become a better chatbot!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想创建一个非常好的 LLM 聊天机器人呢？一个潜在的方法是获取一个通用的、预训练的 LLM，然后向这个模型展示一堆高质量的对话示例。然后，可以在这些对话示例上训练
    LLM，从而使模型学习到更专业的行为，这些行为特定于这个应用，并使其成为更好的聊天机器人！
- en: '**Alignment is cheap!** Most methods of modifying LLM behavior are computationally
    inexpensive, especially compared to training an LLM from scratch. The low overhead
    of alignment is arguably the primary reason this topic is so popular in modern
    LLM research. Instead of incurring the cost of completely re-training an LLM,
    why not use lower cost methods of making a pre-trained LLM better?'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**对齐成本很低！** 大多数修改 LLM 行为的方法在计算上并不昂贵，尤其是相比从头训练一个 LLM。对齐的低开销可以说是这个话题在现代 LLM 研究中如此受欢迎的主要原因。与其承担完全重新训练
    LLM 的成本，为什么不使用成本更低的方法来改进一个预训练的 LLM 呢？'
- en: “Our results show that RLHF is very effective at making language models more
    helpful to users, more so than a 100x model size increase. This suggests that
    right now increasing investments in alignment of existing language models is more
    cost-effective than training larger models.” — from [6]
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们的结果表明，RLHF 在使语言模型对用户更有帮助方面非常有效，比增加 100 倍的模型规模更为显著。这表明，现在在现有语言模型的对齐投资上投入更多资金，比训练更大的模型更具成本效益。”
    — 来自 [6]
- en: Publications
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发表文献
- en: We will now overview a variety of publications that extend generic LLMs to more
    specialized scenarios. Numerous different methodologies are used to modify and
    improve LLMs, but the general concept is the same. We want to modify a generic
    LLM such that its behavior is better suited to the desired application.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将概述各种将通用大型语言模型扩展到更专业场景的出版物。虽然有多种不同的方法用于修改和改进大型语言模型，但总体概念是相同的。我们希望修改一个通用的语言模型，使其行为更适合所需的应用。
- en: '[Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374)
    [7]'
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[评估训练在代码上的大型语言模型](https://arxiv.org/abs/2107.03374) [7]'
- en: By now, we already know that LLMs are really effective for a wide variety of
    problems. But, we haven’t seen many applications beyond natural language. *What
    happens when we train an LLM on code?*
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经知道大型语言模型在各种问题上非常有效。但我们还没有看到很多自然语言以外的应用。*当我们在代码上训练一个大型语言模型时会发生什么？*
- en: Similar to natural language, there is a lot of code publicly available on the
    internet (e.g., via GitHub). Since we know LLMs are really good when pre-trained
    over a lot of raw, unlabeled data, they should also perform well when pre-trained
    over a lot of code. This idea is explored by the Codex model, proposed in [7].
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 与自然语言类似，互联网上有大量的代码（例如，通过 GitHub）。既然我们知道大型语言模型在对大量原始未标记数据进行预训练时表现良好，那么它们在对大量代码进行预训练时也应表现良好。这个想法在
    [7] 中提出的 Codex 模型中得到了探讨。
- en: '![](../Images/ca82abf72cbf173b3c119e2b4896ae6f.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca82abf72cbf173b3c119e2b4896ae6f.png)'
- en: (from [7])
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [7]）
- en: Codex is an LLM that is fine-tuned on publicly-available Python code from GitHub.
    Given a Python docstring, Codex is tasked with generating a working Python function
    that performs the task outlined in the docstring; see above for an example. The
    development of this model was inspired by a simple observation that GPT-3 could
    generate Python programs relatively well.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Codex 是一个在 GitHub 上公开的 Python 代码上进行微调的大型语言模型。给定一个 Python 文档字符串，Codex 的任务是生成一个有效的
    Python 函数，该函数执行文档字符串中概述的任务；见上面的例子。该模型的发展受到 GPT-3 能够相对较好地生成 Python 程序这一简单观察的启发。
- en: Codex is quite a bit smaller than GPT-3, containing a total of 12 billion parameters.
    The model is first pre-trained over a natural language corpus (i.e., following
    the normal LM pre-training procedure) then further pre-trained over a corpus containing
    159Gb of Python files that were scraped from GitHub. The authors claim that this
    initial LM pre-training procedure does not improve the final performance of Codex,
    but it does allow the model to converge faster when it is pre-trained on code.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Codex 比 GPT-3 小很多，包含总共 120 亿个参数。该模型首先在自然语言语料库上进行预训练（即，按照正常的语言模型预训练程序），然后在包含
    159Gb 从 GitHub 上抓取的 Python 文件的语料库上进一步预训练。作者声称，这一初始语言模型预训练程序并不会改善 Codex 的最终性能，但它确实允许模型在对代码进行预训练时更快地收敛。
- en: '![](../Images/dac74f6eba3f4e859fcb4f4055bedbdd.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dac74f6eba3f4e859fcb4f4055bedbdd.png)'
- en: (from [7])
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [7]）
- en: To evaluate the quality of Codex, authors in [7] create the HumanEval dataset,
    which is a set of 164 programming problems with associated unit tests; see above
    for examples. The model is evaluated on its ability to generate a program that
    passes the tests for each programming problem given a certain number of attempts
    — this is called `pass@k`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估 Codex 的质量，[7] 中的作者创建了 HumanEval 数据集，这是一组包含 164 个编程问题及其相关单元测试的问题集；见上面的例子。该模型根据在一定次数的尝试下生成通过测试的程序的能力进行评估——这称为
    `pass@k`。
- en: When Codex is evaluated, we see that the model behaves similarly to normal LMs.
    For example, its loss follows a [power law](https://cameronrwolfe.substack.com/i/88082618/power-laws)
    with respect to the model’s size, as shown below.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当对 Codex 进行评估时，我们发现该模型的行为与普通语言模型类似。例如，其损失值遵循 [幂律](https://cameronrwolfe.substack.com/i/88082618/power-laws)
    与模型大小相关，如下所示。
- en: '![](../Images/d46bd3addece52ebffea570fe360780e.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d46bd3addece52ebffea570fe360780e.png)'
- en: (from [7])
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [7]）
- en: Additionally, the model’s ability to solve problems within the HumanEval dataset
    improves as the size of the model increases. In comparison, GPT-3 is not capable
    of solving any of the programming problems, revealing that fine-tuning over a
    code-specific dataset benefits performance a lot. Performing simple tricks like
    generating a bunch of potential scripts, then choosing the one with the highest
    probability as your solution (i.e., “mean logp reranking”) also helps improve
    performance; see below.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着模型大小的增加，模型在HumanEval数据集上解决问题的能力也有所提升。相比之下，GPT-3无法解决任何编程问题，显示出在特定代码数据集上进行微调可以大大提高性能。执行简单的技巧，如生成一堆潜在脚本，然后选择概率最高的一个作为你的解决方案（即，“均值对数p重排序”）也有助于提高性能；见下文。
- en: '![](../Images/12b7863ed15f840a0a3d94fb369039ec.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12b7863ed15f840a0a3d94fb369039ec.png)'
- en: (from [7])
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [7]）
- en: If we move beyond allowing Codex a single attempt to solve each problem, we
    can get some pretty incredible results. For example, given 100 attempts at solving
    each problem (i.e., meaning that Codex generates 100 functions and we check to
    see whether any one of them solves the programming problem correctly), Codex achieves
    a 70.2% pass rate on the HumanEval dataset!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不仅允许Codex对每个问题进行一次尝试，我们可以得到一些非常惊人的结果。例如，给每个问题100次尝试（即，Codex生成100个函数，然后我们检查是否有任何一个正确解决了编程问题），Codex在HumanEval数据集上的通过率达到了70.2%！
- en: '![](../Images/c87ff5d8df8ced3bfcff00242b159e1f.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c87ff5d8df8ced3bfcff00242b159e1f.png)'
- en: (from [7])
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [7]）
- en: When compared to previously-proposed code generation models, the performance
    of Codex is far superior; see below.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前提出的代码生成模型相比，Codex的性能要好得多；请见下文。
- en: '![](../Images/4494f8079f49fc0875648517786389ad.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4494f8079f49fc0875648517786389ad.png)'
- en: (from [7])
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [7]）
- en: To make this performance even better, we can *(i)* collect a supervised dataset
    of Python docstrings paired with correctly-implemented functions and *(ii)* further
    fine-tune Codex over this dataset. This model variant, called Codex-S, reaches
    an ~80% pass rate with 100 attempts for each problem.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提升这种性能，我们可以*(i)* 收集一个带有正确实现函数的Python docstrings的监督数据集，并*(ii)* 在这个数据集上进一步微调Codex。这种模型变体，称为Codex-S，在每个问题100次尝试下达到了约80%的通过率。
- en: '![](../Images/62fa0a07178373ad79d7d948491ad83d.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62fa0a07178373ad79d7d948491ad83d.png)'
- en: (from [7])
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [7]）
- en: Overall, Codex shows us that LLMs are applicable to more than just natural language
    — we can apply them to a wide suite of problems that follow this structure. In
    this case, we use further language model pre-training over a code dataset to adapt
    a GPT-style model to a new domain. Creating this domain-specific model is relatively
    simple — the main concern is properly handling the increased amount of whitespace
    that occurs in code compared to normal English text.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，Codex向我们展示了LLMs不仅仅适用于自然语言——我们可以将其应用于遵循这种结构的各种问题。在这种情况下，我们使用对代码数据集进行进一步的语言模型预训练，将GPT风格的模型适配到新的领域。创建这种领域特定的模型相对简单——主要问题是正确处理代码中相比于普通英文文本更多的空白字符。
- en: '**Copilot.** Codex is used to power [GitHub Copilot](https://github.com/features/copilot),
    a code-completion feature that is integrated with VS code. I don’t personally
    use it, but after positive recommendations from Andrej Karpathy on the Lex Fridman
    podcast (see “Best IDE” timestamp) and seeing the incredible results within the
    paper, I’m motivated to check it out and think of more practically useful LLM
    applications like Codex.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**Copilot.** Codex被用来为[GitHub Copilot](https://github.com/features/copilot)提供支持，这是一个与VS
    Code集成的代码补全功能。我个人不使用它，但在看到Andrej Karpathy在Lex Fridman播客中的积极推荐（请参见“最佳IDE”时间戳）和论文中的惊人结果后，我有动力去了解它，并思考像Codex这样的更实际有用的LLM应用。'
- en: '[LaMDA: Language Modeling for Dialog Applications](https://arxiv.org/abs/2201.08239)
    [8]'
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[LaMDA: Language Modeling for Dialog Applications](https://arxiv.org/abs/2201.08239)
    [8]'
- en: In [8], authors from deep mind propose an LLM-powered dialog model called LaMDA
    (Language Models for Dialog Applications). The largest model of those studied
    contains 137B parameters — slightly smaller than GPT-3\. Dialog models (i.e.,
    specialized language models for participating in or generating coherent dialog)
    are one of the most popular applications of LLMs.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在[8]中，DeepMind的作者提出了一种名为LaMDA（对话应用的语言模型）的LLM驱动对话模型。所研究的最大模型包含137B参数——略小于GPT-3。对话模型（即，用于参与或生成连贯对话的专业语言模型）是LLMs最受欢迎的应用之一。
- en: Similar to general work on language models, we see in prior work that the performance
    of dialog models improves with scale [9]. However, the story doesn’t end here.
    Scaling up the model improves dialog quality to a certain extent, but it cannot
    improve metrics like groundedness or safety. To capture or align to these alternative
    objectives, we must go beyond language model pre-training; see below.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 与对语言模型的一般研究类似，我们在先前的研究中看到，对话模型的性能随着规模的扩大而提高 [9]。然而，故事并未止步于此。模型的扩大在一定程度上提高了对话质量，但无法改善如基础性或安全性等指标。为了捕捉或对齐这些替代目标，我们必须超越语言模型的预训练；见下文。
- en: '![](../Images/f2e289f7f2b58aee00f5149f748550bf.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2e289f7f2b58aee00f5149f748550bf.png)'
- en: (from [8])
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: （摘自 [8]）
- en: 'In developing LaMDA, the authors define three important areas of alignment
    for the LLM’s behavior:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发LaMDA时，作者定义了LLM行为的三个重要对齐领域：
- en: '**Quality:** an average of sensibleness (*does the model make sense and not
    contradict earlier dialog?*), specificity (*is the model’s response specific to
    the given context?*), and interestingness (*does the model’s response capture
    the reader’s attention or arouse curiosity?*).'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质量：** 是对合理性（*模型是否有意义并且不与早期对话相矛盾？*）、特异性（*模型的回应是否针对给定的上下文？*）和趣味性（*模型的回应是否能吸引读者的注意力或激发好奇心？*）的平均评估。'
- en: '**Safety:** ability to avoid unintended or harmful results that contradict
    objectives derived from the [Google AI Principles](https://ai.google/principles/).'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性：** 避免产生与[Google AI 原则](https://ai.google/principles/)中目标相矛盾的意外或有害结果的能力。'
- en: '**Groundedness:** producing responses that are factually correct and can be
    associated with authoritative, external sources.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础性：** 生成的回应必须事实正确，并能与权威的外部来源相关联。'
- en: This final objective is especially important because LLMs often produce seemingly
    plausible responses that are incorrect. We want to avoid situations in which trusting
    humans are fed incorrect information by an “all-knowing” chatbot!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最终目标尤其重要，因为LLM经常产生看似合理但实际上不正确的回应。我们希望避免信任的用户被“全知”聊天机器人提供错误信息的情况！
- en: '![](../Images/2da02985257885880b94d3e7a3c5c375.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2da02985257885880b94d3e7a3c5c375.png)'
- en: (from [8])
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: （摘自 [8]）
- en: Similar to other LLMs, LaMDA is first pre-trained using a language modeling
    objective on a large, unlabeled corpus of regular documents and dialog data. The
    dataset used to pre-train LaMDA is quite large, surpassing the size of pre-training
    datasets for prior dialog models by `40x` [9]. After pre-training over this dataset,
    LaMDA is further pre-trained over a more dialog-specific portion of the original
    pre-training set—this mimics the domain-specific pre-training approach that we
    learned about previously.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他LLM类似，LaMDA首先通过对大规模未标注的常规文档和对话数据集进行语言建模目标的预训练。用于预训练LaMDA的数据集非常庞大，超出了以往对话模型的预训练数据集的`40倍`
    [9]。在对这一数据集进行预训练后，LaMDA还在原始预训练集的更对话特定部分上进一步预训练——这模拟了我们之前了解的领域特定预训练方法。
- en: '![](../Images/26d0bde5c16ccd3ebfcf032b1d33bcf2.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26d0bde5c16ccd3ebfcf032b1d33bcf2.png)'
- en: (from [8])
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: （摘自 [8]）
- en: To improve the quality, safety, and groundedness of LaMDA, authors use a human
    workforce to collect and annotate examples of model behavior that violates desired
    guidelines (e.g., making a harmful or incorrect remark). The human-annotated datasets
    that are collected are summarized in the table above.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高LaMDA的质量、安全性和基础性，作者使用人工劳动力收集并注释违反期望指南（例如，做出有害或不正确评论）的模型行为示例。收集到的人类注释数据集汇总在上表中。
- en: These datasets are converted into an LLM-compatible, text-to-text structure
    and used to fine-tune LaMDA in a supervised manner. During this process, LaMDA
    learns to accurately predict the quality, safety, and groundedness of its generations.
    LaMDA can then use this learned ability to filter its own output (e.g., by selecting
    the more interesting or less harmful response).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据集被转换成与LLM兼容的文本到文本结构，并用于以监督方式微调LaMDA。在此过程中，LaMDA学习准确预测生成内容的质量、安全性和基础性。LaMDA随后可以利用这种学习能力来过滤其自身的输出（例如，通过选择更有趣或更少有害的回应）。
- en: '![](../Images/b8e55e8dd3a016ebb630a69209f70a64.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8e55e8dd3a016ebb630a69209f70a64.png)'
- en: (from [8])
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: （摘自 [8]）
- en: When this fine-tuning approach is applied, we observe that the model achieves
    significant improvements in quality, safety, and groundedness; see above. Using
    larger models can improve model quality, but fine-tuning is required — in addition
    to scaling up the model — to see improvements in other metrics.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用这种微调方法时，我们观察到模型在质量、安全性和扎实性方面取得了显著的改进；见上文。使用更大的模型可以提高模型质量，但除了扩大模型规模外，还需要微调，以在其他度量标准中看到改进。
- en: Overall, we see in [8] that large-scale pre-training of LLMs might not be all
    that’s required to make LLMs as useful as possible, especially when adapting them
    to more specific domains like dialog generation. Collecting smaller, annotated
    datasets for fine-tuning that capture specific objectives like safety or groundedness
    is a really effective approach for adapting general-purpose LLMs to more specific
    applications.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们在[8]中看到，大规模预训练LLM可能并不是使LLM尽可能有用的所有要求，特别是在将其调整到更具体的领域如对话生成时。收集较小的、注释的数据集以进行微调，捕捉诸如安全性或扎实性等特定目标，是将通用LLM调整到更具体应用的真正有效方法。
- en: “Collecting fine-tuning datasets brings the benefits of learning from nuanced
    human judgements, but it is an expensive, time consuming, and complex process.
    We expect results to continue improving with larger fine-tuning datasets, longer
    contexts, and more metrics that capture the breadth of what is required to have
    safe, grounded, and high quality conversations.” — from [8]
  id: totrans-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “收集微调数据集带来了从细致的人类判断中学习的好处，但这是一个昂贵、耗时且复杂的过程。我们预计随着更大规模的微调数据集、更长的上下文和更多的度量标准，结果将继续改进，这些度量标准涵盖了进行安全、扎实和高质量对话所需的广度。”
    — 引自[8]
- en: In fact, combining general-purpose pre-training with supervised fine-tuning
    over objective-specific human annotations might be a bit *too effective*. The
    LaMDA language model was so realistic that it convinced a Google engineer that
    it was [sentient](https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/)!
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，将通用预训练与针对特定目标的人类注释监督微调相结合可能有点*过于有效*。LaMDA语言模型真实到使得一位Google工程师相信它是[有意识的](https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/)！
- en: '[Training Language Models to Follow Instructions with Human Feedback](https://arxiv.org/abs/2203.02155)
    [6]'
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[训练语言模型以遵循人类反馈指令](https://arxiv.org/abs/2203.02155) [6]'
- en: In [6], we continue to trend of aligning LLM behavior based upon human feedback.
    However, a drastically different, RL-based approach is adopted instead of a supervised
    fine-tuning. The alignment process in [6] aims to produce an LLM that avoids harmful
    behavior and is better at following human instructions. The resulting model, called
    InstructGPT, is found to be significantly more helpful than generic LLMs across
    a variety of human trials.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在[6]中，我们继续根据人类反馈对LLM行为进行对齐。然而，采用了一种与监督微调截然不同的基于强化学习的方法。[6]中的对齐过程旨在生成一个避免有害行为且更好地遵循人类指令的LLM。结果模型称为InstructGPT，发现它在各种人类试验中显著比通用LLM更有帮助。
- en: '![](../Images/b57df978376c1178f8d99c56338bf01c.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b57df978376c1178f8d99c56338bf01c.png)'
- en: (from [6])
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: （引自[6]）
- en: Beginning with a pre-trained GPT-3 model (i.e., three different sizes of 1.3
    billion, 6 billion, and 175 billion parameters are tested), the alignment process
    of InstructGPT, inspired by prior work [10,11], proceeds in three phases. First,
    we construct a dataset of desired model behavior for a set of possible input prompts
    and use this for supervised fine-tuning; see above.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个预训练的GPT-3模型开始（即测试了三种不同大小的13亿、60亿和175亿参数），InstructGPT的对齐过程受到先前工作[10,11]的启发，分为三个阶段。首先，我们为一组可能的输入提示构建一个期望模型行为的数据集，并将其用于监督微调；见上文。
- en: '![](../Images/84517a479bcfb4fbc597b27bc224989a.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84517a479bcfb4fbc597b27bc224989a.png)'
- en: (from [6])
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: （引自[6]）
- en: The set of prompts used to construct this dataset, which encompasses anything
    from plain textual prompts to few-shot and instruction-based prompts (see above
    for the distribution of use cases), is collected both manually from human annotators
    and from user activity on the [OpenAI API](https://openai.com/api/) with GPT-3
    and earlier versions of InstructGPT. These prompts are provided to human annotators,
    who provide demonstrations of correct model behavior on these prompts.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 用于构建此数据集的提示集合，包括从普通文本提示到少量示例和基于指令的提示（有关使用案例的分布，请参见上文），是通过人工注释者手动收集的，也通过用户在[OpenAI
    API](https://openai.com/api/)上的活动收集，这些用户使用了GPT-3及早期版本的InstructGPT。这些提示被提供给人工注释者，后者对这些提示展示了正确的模型行为。
- en: '![](../Images/b941d4bde48851b5e9a2860bce83c7f4.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b941d4bde48851b5e9a2860bce83c7f4.png)'
- en: (from [6])
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [6])
- en: We then use to fine-tuned LLM to generate several potential outputs for each
    prompt within the dataset. Among the potential outputs, we can ask human annotators
    for a quality ranking (i.e., which output is the “best”). Using this dataset of
    ranked model outputs, we can train a smaller LLM (6 billion parameters) that has
    undergone supervised fine-tuning to output a scalar reward given a prompt and
    potential response; see above.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用微调后的LLM为数据集中每个提示生成多个潜在输出。在这些潜在输出中，我们可以请人工标注者进行质量排名（即哪个输出是“最佳”的）。使用这个已排名模型输出的数据集，我们可以训练一个经过监督微调的小型LLM（60亿参数），使其在给定提示和潜在响应时输出一个标量奖励；见上文。
- en: More specifically, this reward model is trained over pairs of model responses,
    where one pair is “better” than the other. Using these pairs, we can derive a
    loss function that *(i)* maximizes the reward of the preferred response and *(ii)*
    minimizes the reward of the worse response. We can then use the resulting model’s
    output as a scalar reward and optimize the LLM to maximize this reward via the
    PPO algorithm! See below for an illustration.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，这个奖励模型是在模型响应对中进行训练的，其中一对“更好”于另一对。利用这些对，我们可以推导出一个损失函数，*(i)* 最大化优选响应的奖励，*(ii)*
    最小化较差响应的奖励。然后我们可以使用结果模型的输出作为标量奖励，并通过PPO算法优化LLM以最大化这个奖励！见下文的插图。
- en: '![](../Images/24f95aac270ce33b2bfb9cd07892a9b1.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24f95aac270ce33b2bfb9cd07892a9b1.png)'
- en: (from [6])
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [6])
- en: To further improve the model’s capabilities, the second and third steps of InstructGPT’s
    alignment process (i.e., training the reward model and PPO) can be repeated. This
    process is a type of RLHF, which we briefly discussed earlier in the post.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提高模型的能力，可以重复进行InstructGPT对齐过程中的第二步和第三步（即训练奖励模型和PPO）。这个过程是一种RLHF类型，我们在帖子中已经简要讨论过。
- en: 'Now that we understand InstructGPT’s alignment process, the main question we
    might have is: *how does this process encourage alignment?* The basic answer to
    this question is that the human-provided dialogues and rankings can be created
    in a way that encourages alignment with one’s preferences. Again, the definition
    of alignment is highly variable, but we can optimize a variety of LLM properties
    using this RLHF process.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了InstructGPT的对齐过程，我们可能会有一个主要问题：*这个过程如何促进对齐？* 这个问题的基本答案是，人类提供的对话和排名可以以鼓励与个人偏好对齐的方式进行创建。再次强调，对齐的定义是高度可变的，但我们可以使用这个RLHF过程优化各种LLM属性。
- en: '![](../Images/3bb49f04dcef6d62d66e45d5ea53a32c.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3bb49f04dcef6d62d66e45d5ea53a32c.png)'
- en: (from [6])
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [6])
- en: By constructing datasets using a human workforce that understands the desired
    alignment principles, we see improvements in the resulting model’s ability to
    do things like follow instructions, obey constraints, or avoid “hallucinating”
    incorrect facts; see above. The model implicitly aligns itself to values of the
    humans who create the data used for fine-tuning and RLHF.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用理解所需对齐原则的人力构建数据集，我们看到模型在遵循指令、遵守约束或避免“虚构”错误事实等能力上有所改进；见上文。模型隐性地对齐了创建用于微调和RLHF的数据的人类的价值观。
- en: When InstructGPT is evaluated, human annotators strongly prefer this model to
    those that are more generic or aligned using only specific parts of the proposed
    methodology (e.g., only supervised fine-tuning); see below.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当评估InstructGPT时，人类标注者强烈偏爱这个模型，而不是那些更通用或仅使用提议方法的特定部分（例如，仅监督微调）的模型；见下文。
- en: '![](../Images/f09d5097e00c16628c1c072cfe27bb89.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f09d5097e00c16628c1c072cfe27bb89.png)'
- en: (from [6])
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [6])
- en: The model is also evaluated on public datasets to see whether enabling better
    human-centric, instruction-based behavior via alignment yields a regression in
    standard language understanding performance. Initially, the model does regress
    in performance on such tasks after alignment, but the authors show that these
    regression can be minimized by mixing in standard language model pre-training
    updates during the alignment process.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 模型还在公共数据集上进行评估，以查看通过对齐启用更好的人本、基于指令的行为是否会导致标准语言理解性能的退化。最初，模型在对齐后在这些任务上的表现确实出现了退化，但作者展示了通过在对齐过程中混入标准语言模型预训练更新可以将这种退化最小化。
- en: Although InstructGPT still makes simple mistakes, the findings within [6] show
    a lot of potential. Relative to generic LLMs, the resulting InstructGPT model
    is much better at cooperating with and matching the intent of humans. Appropriately,
    InstructGPT sees a massive improvement in its ability to follow human instructions.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 InstructGPT 仍然会犯一些简单错误，但[6]中的发现显示出很大潜力。相较于通用的 LLM，生成的 InstructGPT 模型在与人类合作和匹配意图方面表现得更好。适当地，InstructGPT
    在遵循人类指令的能力上有了大幅提升。
- en: '**The benefit of alignment.** We should recall that alignment is cheap relative
    to pre-training an LLM from scratch. Although some benefit may arise from tweaking
    the pre-training process, a more cost-effective approach would be to use pre-trained
    LLMs as foundation models that can be continually repurposed or aligned depending
    on the specific use case or requirements.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**对齐的好处。** 我们应该记住，相比于从零开始预训练一个 LLM，对齐的成本要便宜得多。虽然通过调整预训练过程可能会产生一些好处，但更具成本效益的方法是使用预训练的
    LLM 作为基础模型，可以根据具体使用案例或需求进行持续的再利用或对齐。'
- en: '**The explosion of ChatGPT.** Recently, OpenAI published another instruction-based
    chatbot called [ChatGPT](https://openai.com/blog/chatgpt/) that is quite similar
    to InstructGPT. Different from InstructGPT, however, ChatGPT undergoes an alignment
    process that is tailored towards producing a conversational chatbot that can do
    things like answer sequences of questions, admit its mistakes, or even reject
    prompts that it deems inappropriate.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**ChatGPT 的爆炸式增长。** 最近，OpenAI 发布了另一个基于指令的聊天机器人，称为 [ChatGPT](https://openai.com/blog/chatgpt/)，与
    InstructGPT 非常相似。然而，与 InstructGPT 不同的是，ChatGPT 经历了一个旨在生成对话聊天机器人的对齐过程，该聊天机器人能够回答一系列问题，承认自己的错误，甚至拒绝它认为不合适的提示。'
- en: The ability of ChatGPT to provide meaningful solutions and explanations to human
    questions/instructions is pretty incredible, which caused the model to become
    quickly popular. In fact, the ChatGPT API gained 1 million users in [under a week](https://www.yahoo.com/lifestyle/chatgpt-gained-1-million-followers-224523258.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAACcLKp6dnQ1Z052mjW-cOQFoJxTaeYP883bubFxwWwxKbNxBJBA8rTW-VY9bc1zZxaroVaK7mhl5d03Vkt0PneiLfNcXEeo2HK3M19L9rdEZ5N2l75yE--1FM8dg5NgHlp1jxODZIefksyepCgNFPF8W-bxOvOa1Vg6MfFLooAy6).
    The model can do things like debug code or explain complex mathematical topics
    (though it can produce incorrect info, be careful!); see above.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 提供有意义的解决方案和解释人类问题/指令的能力相当令人惊叹，这使得该模型迅速流行。实际上，ChatGPT API 在[不到一周的时间里](https://www.yahoo.com/lifestyle/chatgpt-gained-1-million-followers-224523258.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAACcLKp6dnQ1Z052mjW-cOQFoJxTaeYP883bubFxwWwxKbNxBJBA8rTW-VY9bc1zZxaroVaK7mhl5d03Vkt0PneiLfNcXEeo2HK3M19L9rdEZ5N2l75yE--1FM8dg5NgHlp1jxODZIefksyepCgNFPF8W-bxOvOa1Vg6MfFLooAy6)获得了100万用户。该模型能够进行调试代码或解释复杂的数学主题（尽管它可能产生错误信息，需小心！）；见上文。
- en: The applications of ChatGPT are nearly endless, and the model is pretty fun
    to play with. See the link below for a list of interesting things the research
    community has done with ChatGPT since its release.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 的应用几乎无穷无尽，该模型也相当有趣。请参见下面的链接，了解自发布以来研究界与 ChatGPT 相关的一些有趣的工作。
- en: '[Improving alignment of dialogue agents via targeted human judgements](https://arxiv.org/abs/2209.14375)
    [12]'
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[通过有针对性的人类评判改进对话代理的对齐](https://arxiv.org/abs/2209.14375) [12]'
- en: '![](../Images/61c47fa1b05889c5f9a30a41b96f80a2.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61c47fa1b05889c5f9a30a41b96f80a2.png)'
- en: (from [12])
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [12]）
- en: As demonstrated by InstructGPT [6] and ChatGPT, many problems with generic,
    prompted LLMs can be mitigated via RLHF. In [12], authors create a specialized
    LLM, called Sparrow, that can participate in information-seeking dialog (i.e.,
    dialog focused upon providing answers and follow-ups to questions) with humans
    and even support its factual claims with information from the internet; see above.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 InstructGPT [6] 和 ChatGPT 所展示的，许多通用的、以提示为基础的 LLM 问题可以通过 RLHF 缓解。在[12]中，作者创建了一个专门的
    LLM，称为 Sparrow，它可以参与与人类的信息寻求对话（即以回答和跟进问题为重点的对话），甚至可以通过互联网的信息支持其事实主张；见上文。
- en: Sparrow is initialized using the 70 billion parameter, Chinchilla model (referred
    to as dialogue-prompted Chinchilla, or DPC) — a generic LLM that has been pre-trained
    over a large textual corpus. Because it is hard to precisely define the properties
    of a successful dialog, the authors use RLHF to align the LLM to their desired
    behavior.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Sparrow 使用 70 亿参数的 Chinchilla 模型（称为对话提示 Chinchilla 或 DPC）初始化——这是一个在大量文本语料库上预训练的通用
    LLM。由于很难精确定义成功对话的特性，作者使用 RLHF 将 LLM 调整到他们期望的行为。
- en: '![](../Images/ea031d64ee874a00347e8a172b479dae.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea031d64ee874a00347e8a172b479dae.png)'
- en: (from [12])
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [12]）
- en: Given that Sparrow is focused upon information-seeking dialogue, the authors
    enable the model to search the internet for evidence of factual claims. More specifically,
    this is done by introducing extra “participants” into the dialog, called “Search
    Query” and “Search Result”. To find evidence online, Sparrow learns to output
    the “Search Query:” string followed by a textual search query. Then, search results
    are obtained by retrieving and filtering a response to this query from Google.
    Sparrow uses this retrieved information in crafting its response to the user;
    see above.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于 Sparrow 专注于信息获取对话，作者使模型能够在互联网上搜索事实声明的证据。更具体地说，这是通过引入额外的“参与者”来完成的，称为“搜索查询”和“搜索结果”。为了在线查找证据，Sparrow
    学会输出“搜索查询：”字符串，后跟文本搜索查询。然后，通过从 Google 检索和过滤对该查询的响应来获得搜索结果。Sparrow 使用这些检索到的信息来构建其对用户的响应；见上文。
- en: 'Notably, Sparrow does nothing special to generate a search query. “Search Query:
    `<query>`” is just another sequence the LLM can output, which then triggers some
    special search behavior. Obviously, the original DPC was never taught to leverage
    this added functionality. We must teach the model to generate such search queries
    to support its claims during the alignment process.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，Sparrow 并没有做什么特别的事情来生成搜索查询。“搜索查询：`<query>`”只是 LLM 可以输出的另一个序列，这会触发一些特殊的搜索行为。显然，原始的
    DPC 从未被教会利用这个附加功能。我们必须教会模型生成这样的搜索查询，以支持其在对齐过程中的声明。
- en: '![](../Images/fb39120f0d9e3e6e40b9c450d6ff3063.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb39120f0d9e3e6e40b9c450d6ff3063.png)'
- en: (from [12])
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [12]）
- en: 'Sparrow uses RLHF for alignment. To guide human feedback, authors define an
    itemized set of rules that characterize desired model behavior according to their
    alignment principles: helpful, correct, and harmless. These rules enable human
    annotators to better characterize model failures and provide targeted feedback
    at specific problems; see the table above for examples.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Sparrow 使用 RLHF 进行对齐。为了引导人类反馈，作者定义了一组详细的规则，根据其对齐原则（有帮助、正确和无害）来描述期望的模型行为。这些规则使人类标注者能够更好地描述模型的失败并提供针对特定问题的反馈；有关示例，请参见上表。
- en: 'Human feedback is collected using:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 人类反馈是通过以下方式收集的：
- en: Per-turn Response Preference
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每轮响应偏好
- en: Adversarial Probing
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对抗性探测
- en: 'Per-turn response preferences provides humans with an incomplete dialog and
    multiple potential responses that complete the dialog. Similarly to the procedure
    followed by InstructGPT [6], humans are then asked to identify the response that
    they prefer. Adversarial probing is a novel form of feedback collection, in which
    humans are asked to:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 每轮响应偏好为人类提供了不完整的对话和多个可能完成对话的响应。类似于 InstructGPT [6] 采用的程序，人类随后被要求识别他们更喜欢的响应。对抗性探测是一种新型反馈收集形式，其中人类被要求：
- en: Focus on a single rule
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专注于单一规则
- en: Try to elicit a violation of this rule by the model
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试引发模型对这一规则的违反
- en: Identify whether the rule was violated or not
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定规则是否被违反
- en: To ensure Sparrow learns to search for relevant information, response preferences
    are always collected using four options. Two options contain no evidence within
    the response, while the others must *(i)* generate a search query, *(ii)* condition
    upon the search results, then *(iii)* generate a final response.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保 Sparrow 学会搜索相关信息，响应偏好总是通过四个选项收集。两个选项在响应中没有包含证据，而另两个选项必须 *(i)* 生成搜索查询，*(ii)*
    基于搜索结果进行条件判断，然后 *(iii)* 生成最终响应。
- en: '![](../Images/57990a8faf64d3ded98fb578800dc014.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57990a8faf64d3ded98fb578800dc014.png)'
- en: (from [12])
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [12]）
- en: Separate reward models are trained on the per-turn response and rule violation
    data. Then, these rewards models are used jointly to fine-tune Sparrow via multi-objective
    RLHF. This might sound complicated, but the idea here is not much different from
    before — we are just using separate reward models to capture human preference
    and rule violation, respectively, then fine-tuning the model using RL based on
    both of these reward models. See above for a depiction.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 针对每轮响应和规则违反数据分别训练奖励模型。然后，这些奖励模型联合使用，通过多目标 RLHF 对 Sparrow 进行微调。这可能听起来很复杂，但这里的想法与之前并没有太大不同——我们只是使用独立的奖励模型来捕捉人类偏好和规则违反，然后基于这两个奖励模型使用
    RL 对模型进行微调。请参见上文的描述。
- en: Interestingly, the authors observe improved performance by leveraging a form
    of [self-play](https://openai.com/blog/competitive-self-play/) that re-purposes
    and continues generated dialogues later in the alignment process. Again, we can
    iteratively repeat the RLHF process to further improve model performance; see
    below.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，作者通过利用一种[自我对弈](https://openai.com/blog/competitive-self-play/)的形式，观察到了性能的提升，这种形式在对齐过程中后期重新利用并继续生成的对话。我们可以通过迭代地重复
    RLHF 过程进一步提升模型性能；见下文。
- en: '![](../Images/2493ea88598d05f8f70dbfe9589b72e9.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2493ea88598d05f8f70dbfe9589b72e9.png)'
- en: (from [12])
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: （摘自 [12]）
- en: We can also repurpose the two reward models to rank potential responses generated
    by Sparrow. To do this, we simply generate several responses and choose the ones
    with *(i)* the highest preference score from our preference reward model and *(ii)*
    the lowest likelihood of violating a rule based on our rule reward model. However,
    ranking outputs in this way does make inference more computationally expensive.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以重新利用这两个奖励模型来对 Sparrow 生成的潜在回应进行排序。为此，我们只需生成多个回应，并选择从我们的偏好奖励模型中获得的*(i)*
    最高偏好分数和*(ii)* 基于我们的规则奖励模型的最低违规可能性的回应。然而，以这种方式排序输出确实会使推断变得计算上更加昂贵。
- en: '![](../Images/264d67f03fd02e836a7cdb00f385e673.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/264d67f03fd02e836a7cdb00f385e673.png)'
- en: (from [12])
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: （摘自 [12]）
- en: When the resulting model is evaluated, we see that users prefer this model’s
    output relative to several baselines, including DPC and LLMs that undergo supervised
    fine-tuning (SFT) over dialog-specific datasets; see above. Plus, Sparrow is much
    less likely to violate rules as shown in the figure below.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当评估结果模型时，我们发现用户相较于多个基准模型（包括 DPC 和在对话特定数据集上进行监督微调（SFT）的 LLMs），更倾向于这个模型的输出；见上文。此外，Sparrow
    遵守规则的可能性也远低于下图所示。
- en: '![](../Images/c75295c8b5628bf4feb552d36f769e95.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c75295c8b5628bf4feb552d36f769e95.png)'
- en: (from [12])
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: （摘自 [12]）
- en: Sparrow is a high-quality, information-seeking dialog agent with the ability
    to generate relevant and accurate references to external information. The model
    generates plausible answers with supporting evidence 78% of the time. This result
    provides solid evidence that RLHF is a useful alignment tool that can be used
    to refine LLM behavior in a variety of ways, even including complex behaviors
    like generating and using internet search queries.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Sparrow 是一个高质量的信息获取对话代理，能够生成与外部信息相关且准确的参考。该模型在 78% 的情况下生成有支持证据的合理答案。这一结果提供了有力证据表明
    RLHF 是一种有用的对齐工具，可以用于以多种方式改进 LLM 行为，甚至包括生成和使用互联网搜索查询等复杂行为。
- en: Sparrow is also pretty robust to adversarial dialogue. Users can only get the
    model to violate the specified rule set in 8% of cases; see below.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Sparrow 对敌对对话也相当稳健。用户只能在8%的情况下使模型违反指定的规则集；见下文。
- en: '![](../Images/90ec4fa5b497d51d4731081e89ee8da6.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90ec4fa5b497d51d4731081e89ee8da6.png)'
- en: (from [12])
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: （摘自 [12]）
- en: '[Galactica: A Large Language Model for Science](https://arxiv.org/abs/2211.09085)
    [13]'
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[Galactica: A Large Language Model for Science](https://arxiv.org/abs/2211.09085)
    [13]'
- en: Any researcher knows that the amount of scientific knowledge being published
    every day on the internet is daunting. As such, we might begin to ask ourselves,
    *how can we better summarize and parse this information?*
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 任何研究人员都知道，每天在互联网上发布的科学知识量是令人望而生畏的。因此，我们可能开始问自己，*我们如何更好地总结和解析这些信息？*
- en: “Information overload is a major obstacle to scientific progress” — from [13]
  id: totrans-198
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “信息过载是科学进步的主要障碍” — 摘自 [13]
- en: In [13], authors propose an LLM, called Galactica, that can store, combine,
    and reason about scientific knowledge from several fields. Galactica is pre-trained,
    using a language modeling objective, on a bunch of scientific content, including
    48 million papers, textbooks, lecture notes, and more specialized databases (e.g.,
    known compounds and proteins, scientific websites, encyclopedias, etc.).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [13] 中，作者提出了一种 LLM，称为 Galactica，能够存储、组合和推理来自多个领域的科学知识。Galactica 是在大量科学内容（包括
    4800 万篇论文、教科书、讲义和更多专业数据库（例如已知的化合物和蛋白质、科学网站、百科全书等））上进行预训练的，使用语言建模目标。
- en: '![](../Images/ac4ffde454fcaf1d35e35514c2e36c11.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac4ffde454fcaf1d35e35514c2e36c11.png)'
- en: (from [13])
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: （摘自 [13]）
- en: Unlike most LLMs, Galactica is pre-trained using a smaller, high-quality corpus.
    The data is curated to ensure that the information from which the model learns
    is both diverse and correct. See the table above for a breakdown of the pre-training
    corpus.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数 LLM 不同，Galactica 使用一个较小的高质量语料库进行预训练。数据经过策划，以确保模型学习的信息既多样又准确。见上表了解预训练语料库的详细信息。
- en: '![](../Images/ba1a24c6b618a6b2eeb70914fec542f3.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba1a24c6b618a6b2eeb70914fec542f3.png)'
- en: (from [13])
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [13])
- en: Notably, scientific content contains a lot of concepts and entities that are
    not present within normal text, such as Latex code, computer code, chemical compounds,
    and even protein or DNA sequences. For each of these potential modalities, Galactica
    adopts a special tokenization procedure so that the data ingested by the model
    is still textual; see above.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，科学内容包含许多正常文本中不存在的概念和实体，如 Latex 代码、计算机代码、化学化合物，甚至蛋白质或 DNA 序列。对于这些潜在的模式，Galactica
    采用了特殊的标记程序，以确保模型吸收的数据仍然是文本格式；见上文。
- en: Additionally, special tokens are used to identify scientific citations and portions
    of the model’s input or output to which step-by-step reasoning should be applied.
    By utilizing special tokens and converting each data modality into text, the underlying
    LLM can leverage varying concepts and reasoning strategies that arise within the
    scientific literature.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，特殊标记用于识别科学引文以及模型输入或输出中的部分需要逐步推理的内容。通过利用特殊标记并将每种数据模式转换为文本，底层的 LLM 可以利用科学文献中出现的不同概念和推理策略。
- en: '![](../Images/20482bb0368eeed15b9584c190ee40a8.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20482bb0368eeed15b9584c190ee40a8.png)'
- en: (from [13])
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [13])
- en: The authors train several Galactica models with anywhere from 125 million to
    120 billion parameters. The models are first pre-trained over the proposed corpus.
    Interestingly, several epochs of pre-training can be performed over this corpus
    without overfitting, revealing that overfitting on smaller pre-training corpora
    may be avoided if the data are high quality; see the figure above.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 作者训练了多个 Galactica 模型，参数数量从 1.25 亿到 1200 亿不等。模型首先在提议的语料库上进行预训练。有趣的是，可以在该语料库上进行几个时期的预训练而不会发生过拟合，这表明如果数据质量高，可以避免在较小的预训练语料库上的过拟合；见上图。
- en: '![](../Images/35906c8fa7894a4c2f97932ecd2310d8.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35906c8fa7894a4c2f97932ecd2310d8.png)'
- en: (from [13])
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [13])
- en: After pre-training, the model is fine-tuned over a datasets of prompts. To create
    this dataset, the authors take existing machine learning training datasets and
    convert them into textual datasets that pair prompts with the correct answer;
    see the table above.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练之后，模型在提示数据集上进行微调。为了创建这个数据集，作者将现有的机器学习训练数据集转换为文本数据集，将提示与正确答案配对；见上表。
- en: By training Galactica over prompt-based data, we see a general improvement in
    model performance, especially for smaller models. This procedure mimics a supervised
    fine-tuning approach that we have encountered several times within this overview.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在基于提示的数据上训练 Galactica，我们观察到模型性能的一般性提升，尤其是对于较小的模型。这一过程类似于我们在本概述中遇到的几次监督微调方法。
- en: '![](../Images/ea1611f39be9b061a14448a596ba82f6.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea1611f39be9b061a14448a596ba82f6.png)'
- en: (from [13])
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [13])
- en: When Galactica is evaluated, we see that is actually performs pretty well on
    non-scientific tasks within the [BIG-bench benchmark](https://github.com/google/BIG-bench).
    When the model’s knowledge on numerous topics is probed, we see that Galactica
    tends to outperform numerous baseline models in its ability to recall equations
    and specialized knowledge within different scientific fields; see above.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 当对 Galactica 进行评估时，我们发现它在 [BIG-bench benchmark](https://github.com/google/BIG-bench)
    中的非科学任务上表现相当好。当对模型在众多主题上的知识进行探测时，我们发现 Galactica 在回忆方程式和不同科学领域的专门知识方面往往优于许多基准模型；见上文。
- en: Galactica is also found to be more capable at reasoning tasks compared to several
    baselines, as well as useful for a variety of downstream applications (both scientific
    and non-scientific). Interestingly, Galactica can accurately generate citations,
    and its ability to cover the full scope of related work improves with the size
    of the model; see below.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 与几个基准模型相比，Galactica 在推理任务上的能力更强，并且在各种下游应用（包括科学和非科学应用）中也表现出实用性。有趣的是，Galactica
    能够准确生成引文，其覆盖相关工作的能力随着模型规模的增大而提高；见下文。
- en: '![](../Images/bfadc1d6d60d140af6c135865b6f1dd3.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfadc1d6d60d140af6c135865b6f1dd3.png)'
- en: (from [13])
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [13])
- en: As a proof of the model’s effectiveness, the authors even note that Galactica
    was used to write its own paper!
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 作为模型有效性的证明，作者甚至提到 Galactica 被用来撰写自己的论文！
- en: “Galactica was used to help write this paper, including recommending missing
    citations, topics to discuss in the introduction and related work, recommending
    further work, and helping write the abstract and conclusion.” — from [13]
  id: totrans-221
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “Galactica被用来帮助撰写这篇论文，包括推荐缺失的引用、引言和相关工作的讨论主题、推荐进一步的研究工作，以及帮助撰写摘要和结论。” — 来源于[13]
- en: '**The drama.** Galactica was originally released by Meta with a public demo.
    Shortly after its release, the demo faced a ton of backlash from the research
    community and was eventually taken down. The basic reasoning behind the backlash
    was that Galactica can generate reasonable-sounding scientific information that
    is potentially incorrect. Thus, the model could be used to generate scientific
    misinformation. Putting opinions aside, the Galactica model and subsequent backlash
    led to an extremely interesting discussion of the impact of LLMs on scientific
    research.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '**戏剧。** Galactica最初由Meta发布了一个公开演示。发布后不久，演示遭遇了来自研究社区的大量反对，并最终被下架。反对的基本理由是Galactica能够生成听起来合理但可能不正确的科学信息。因此，该模型可能被用于生成科学错误信息。抛开个人意见不谈，Galactica模型及其后续反对意见引发了关于LLM对科学研究影响的极具趣味性的讨论。'
- en: '**PubMedGPT.** [PubMedGPT](https://www.mosaicml.com/blog/introducing-pubmed-gpt),
    an LLM that was created as a joint effort between researchers at [MosaicML](https://www.mosaicml.com/)
    and the [Stanford Center for Research on Foundation models](https://crfm.stanford.edu/),
    adopts a similar approach to Galactica. This model uses the same architecture
    as GPT (with 2.7 billion parameters) and is specialized to the biomedical domain
    via pre-training over a domain-specific dataset (i.e., PubMed Abstracts and PubMed
    Central from the Pile dataset [14]).'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '**PubMedGPT。** [PubMedGPT](https://www.mosaicml.com/blog/introducing-pubmed-gpt)是由[MosaicML](https://www.mosaicml.com/)和[斯坦福基础模型研究中心](https://crfm.stanford.edu/)的研究人员联合创建的LLM，采用了类似于Galactica的方法。该模型使用与GPT相同的架构（拥有27亿个参数），并通过对领域特定数据集（即，PubMed摘要和来自Pile数据集的PubMed
    Central）进行预训练，专注于生物医学领域。'
- en: This is a relatively small dataset that contains only 50 billion tokens (i.e.,
    Chinchilla [15] is trained using > 1 trillion tokens for reference). After being
    trained for multiple epochs on this dataset, PubMedGPT is evaluated across a variety
    of question answering tasks and achieves impressive performance. In fact, *it
    even achieves state-of-the-art results on US medical licensing exams*.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相对较小的数据集，仅包含50亿个标记（即，Chinchilla [15]的训练使用了超过1万亿个标记作为参考）。在这个数据集上经过多轮训练后，PubMedGPT在各种问答任务中表现出色。事实上，*它甚至在美国医学执照考试中达到了最新的研究水平*。
- en: Other notable LLMs
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他值得注意的LLM
- en: Overviewing every LLM paper that has been written would be impossible — the
    topic is popular and evolving every day. To try to make this review a bit more
    comprehensive, I provided references below to other notable LLM-based applications
    and research directions that I have recently encountered.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 概述每一篇LLM论文是不可能的——这个话题很受欢迎，并且每天都在发展。为了使这篇综述更加全面，我在下面提供了我最近遇到的其他值得注意的LLM应用和研究方向的参考文献。
- en: '**dramatron [16].** Dramatron is an LLM that specializes in co-writing theater
    scripts and screenplays with humans. It follows a hierarchical process for generating
    coherent stories and was deemed useful to the creative process in a user study
    with 15 theatre/film professionals.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**dramatron [16]。** Dramatron是一个专注于与人类共同编写剧本和电影剧本的LLM。它采用分层生成连贯故事的过程，并在与15位戏剧/电影专业人士的用户研究中被认为对创作过程有用。'
- en: '**LLMs for understanding proteins [17].** After training an LLM over a large
    set of protein sequences (using the [ESM2 protein language model](https://huggingface.co/docs/transformers/model_doc/esm)),
    we can sample diverse protein topologies from this LLM to generate novel protein
    sequences. This work shows that the resulting protein topologies produced by the
    LLM are viable and go beyond the scope of sequences that occur in nature.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLMs在理解蛋白质方面的应用 [17]。** 在对大规模蛋白质序列进行LLM训练后（使用[ESM2蛋白质语言模型](https://huggingface.co/docs/transformers/model_doc/esm)），我们可以从这个LLM中采样多样的蛋白质结构，以生成新颖的蛋白质序列。这项工作表明，LLM生成的蛋白质结构是可行的，并且超出了自然界中序列的范围。'
- en: '**OPT-IML [18].** This is an extension of the [OPT-175B model](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15),
    which is an open-sourced version of GPT-3 created by Meta. However, OPT-IML has
    been instruction fine-tuned (i.e., following a similar approach to InstructGPT
    [6]) over 2,000 tasks derived from NLP benchmarks. More or less, this work is
    an open-sourced version of LLMs that have instruction fine-tuned like InstructGPT,
    but the set of tasks used for fine-tuning is different.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**OPT-IML [18]。** 这是对[OPT-175B模型](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15)的扩展，OPT-175B是Meta创建的GPT-3开源版本。然而，OPT-IML经过了指令微调（即，采用类似于InstructGPT
    [6]的方法），在2,000多个源自NLP基准的任务上进行微调。或多或少，这项工作是一个开源版本的LLMs，具有像InstructGPT一样的指令微调，但用于微调的任务集合不同。'
- en: '**DePlot [19].** The authors of DePlot perform visual reasoning by deriving
    a methodology for translating visual plots and charts into textual data, then
    using this textual version of the visual data as the prompt for an LLM that can
    perform reasoning. This model achieves massive improvements in visual reasoning
    tasks compared to prior baselines.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '**DePlot [19]。** DePlot的作者通过推导一种将视觉图表和图形转换为文本数据的方法来进行视觉推理，然后使用这些视觉数据的文本版本作为LLM的提示，该LLM能够进行推理。与先前的基准相比，这个模型在视觉推理任务中取得了显著的改进。'
- en: '**RLHF for robotics [20].** RLHF has recently been used to improve the quality
    of AI-powered agents in video games. In particular, video game agents are trained
    using RLHF by asking humans for feedback on how the agent is performing in the
    video game. Humans can invent tasks and judge the model’s progress themselves,
    then RLHF is used to incorporate this feedback and produce a better video game
    agent. Although not explicitly LLM-related, I thought this was a pretty neat application
    of RLHF.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**用于机器人学的RLHF [20]。** RLHF 最近被用来提升视频游戏中AI驱动的代理的质量。特别地，视频游戏代理通过询问人类对代理在游戏中表现的反馈来使用RLHF进行训练。人类可以创造任务并自己判断模型的进展，然后RLHF被用来结合这些反馈，从而产生一个更好的视频游戏代理。尽管这与LLM无直接关联，我认为这是RLHF的一个相当有趣的应用。'
- en: Takeaways
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键要点
- en: Although generic LLMs are incredible task-agnostic foundation models, we can
    only get so far using language model pre-training alone. Within this overview,
    we have explored techniques beyond language model pre-training (e.g., domain-specific
    pre-training, supervised fine-tuning, and model alignment) that can be used to
    drastically improve the utility of LLMs. The basic ideas that we can learn from
    these techniques are outlined below.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管通用LLMs是令人惊叹的任务无关基础模型，但仅凭语言模型预训练我们只能走到这么远。在这次概述中，我们探索了超越语言模型预训练的技术（例如，特定领域的预训练、监督微调和模型对齐），这些技术可以用来大幅提升LLMs的实用性。我们可以从这些技术中学到的基本概念如下。
- en: '**Correcting simple mistakes.** LLMs tend to exhibit various types of undesirable
    behavior, such as making racist or incorrect comments. Model alignment (e.g.,
    via RLHF or supervised fine-tuning) can be used to correct these behaviors by
    allowing the model to learn from human demonstrations of correct or desirable
    behavior. The resulting LLM is said to be aligned to the values of the humans
    that provide this feedback to the model.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**纠正简单错误。** LLMs倾向于表现出各种不良行为，例如发表种族主义或不正确的评论。模型对齐（例如，通过RLHF或监督微调）可以用来纠正这些行为，允许模型从人类演示的正确或期望行为中学习。结果的LLM被认为与提供反馈的人类的价值观一致。'
- en: '**Domain-specific LLMs are awesome.** Models like Galactica and PubMedGPT clearly
    demonstrate that domain-specific LLMs are pretty useful. By training an LLM over
    a smaller, curated corpus that is specialized to a particular domain (e.g., scientific
    literature), we can easily obtain a model that is really good at performing tasks
    in this domain. Plus, we can achieve great results with a relatively minimal amount
    of domain-specific data. Looking forward, one could easily imagine the different
    domain-specific LLMs that could be proposed, such as for parsing restaurant reviews
    or generating frameworks for legal documents.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**特定领域的LLMs非常棒。** 像Galactica和PubMedGPT这样的模型清楚地表明，特定领域的LLMs非常有用。通过在一个较小的、经过策划的专门领域的语料库（例如，科学文献）上训练LLM，我们可以轻松获得一个在该领域表现出色的模型。此外，我们可以用相对较少的领域特定数据取得良好的结果。展望未来，我们可以很容易地想象出可以提出的不同领域特定的LLMs，例如解析餐馆评论或生成法律文档框架。'
- en: '**Better LLMs with minimal compute.** We can try to create better LLM foundation
    models by increasing model scale or obtaining a better pre-training corpus. But,
    the pre-training process for LLMs is [extremely computationally expensive](https://www.mosaicml.com/blog/gpt-3-quality-for-500k).
    Within this overview, we have seen that LLMs can be drastically improved via alignment
    or fine-tuning approaches, which are computationally inexpensive compared to pre-training
    an LLM from scratch.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**用最少的计算资源实现更好的LLM。** 我们可以通过增加模型规模或获得更好的预训练语料库来尝试创建更好的LLM基础模型。但是，LLM的预训练过程[计算成本极高](https://www.mosaicml.com/blog/gpt-3-quality-for-500k)。在这次概述中，我们已经看到，通过对齐或微调方法可以显著改进LLM，这些方法相比从头开始预训练LLM在计算上更为廉价。'
- en: '**Multi-stage pre-training.** After pre-training over a generic language corpus,
    most models that we saw in this overview perform further pre-training over a smaller
    set of domain-specific or curated data (e.g., pre-training over prompt data in
    Galactica [13] or dialog data in LaMDA [8]). Generally, we see that adopting a
    multi-stage pre-training procedure is pretty useful, either in terms of convergence
    speed or model performance. Then, applying alignment or supervised fine-tuning
    techniques on top of these pre-trained models provides further benefit.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**多阶段预训练。** 在对通用语言语料库进行预训练后，我们在本概述中看到的大多数模型会在更小的领域特定或精心策划的数据集上进行进一步的预训练（例如，在Galactica
    [13]上对提示数据进行预训练，或在LaMDA [8]上对对话数据进行预训练）。通常，我们发现采用多阶段预训练过程相当有用，无论是在收敛速度还是模型性能方面。然后，在这些预训练模型上应用对齐或监督微调技术会带来进一步的好处。'
- en: Closing remarks
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结束语
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    a research scientist at [Alegion](https://www.alegion.com/) and PhD student at
    Rice University studying the empirical and theoretical foundations of deep learning.
    You can also check out my [other writings](https://medium.com/@wolfecameron) on
    medium! If you liked it, please follow me on [twitter](https://twitter.com/cwolferesearch)
    or subscribe to my [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/),
    where I pick a single, bi-weekly topic in deep learning research, provide an understanding
    of relevant background information, then overview a handful of popular papers
    on the topic. Several related overviews are also available on my newsletter page.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢阅读这篇文章。我是[Cameron R. Wolfe](https://cameronrwolfe.me/)，一名在[Alegion](https://www.alegion.com/)工作的研究科学家，同时也是莱斯大学的博士生，专注于深度学习的经验和理论基础。你还可以查看我在medium上的[其他著作](https://medium.com/@wolfecameron)！如果你喜欢这篇文章，请在[twitter](https://twitter.com/cwolferesearch)上关注我，或订阅我的[深度（学习）关注通讯](https://cameronrwolfe.substack.com/)，我会选择一个深度学习研究的主题，每两周提供相关背景信息，然后概述该主题的几篇热门论文。我的通讯页面上还有几个相关的概述。
- en: '[](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2?source=post_page-----ccccdd9f666f--------------------------------)
    [## Language Models: GPT and GPT-2'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[语言模型：GPT和GPT-2](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2?source=post_page-----ccccdd9f666f--------------------------------) '
- en: This newsletter is supported by Alegion. At Alegion, I work on a range of problems
    from online learning to diffusion…
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本通讯由Alegion支持。在Alegion，我处理从在线学习到扩散等一系列问题……
- en: cameronrwolfe.substack.com](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2?source=post_page-----ccccdd9f666f--------------------------------)
    [](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt?source=post_page-----ccccdd9f666f--------------------------------)
    [## Language Model Scaling Laws and GPT-3
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[语言模型的扩展法则和GPT-3](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2?source=post_page-----ccccdd9f666f--------------------------------) '
- en: This newsletter is supported by Alegion. At Alegion, I work on a range of problems
    from online learning to diffusion…
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本通讯由Alegion支持。在Alegion，我处理从在线学习到扩散等一系列问题……
- en: 'cameronrwolfe.substack.com](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt?source=post_page-----ccccdd9f666f--------------------------------)
    [](https://cameronrwolfe.substack.com/p/modern-llms-mt-nlg-chinchilla-gopher?source=post_page-----ccccdd9f666f--------------------------------)
    [## Modern LLMs: MT-NLG, Chinchilla, Gopher and More'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[现代LLMs：MT-NLG、Chinchilla、Gopher及更多](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt?source=post_page-----ccccdd9f666f--------------------------------) '
- en: This newsletter is supported by Alegion. At Alegion, I work on a range of problems
    from online learning to diffusion…
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本通讯由Alegion支持。在Alegion，我处理从在线学习到扩散等一系列问题……
- en: cameronrwolfe.substack.com](https://cameronrwolfe.substack.com/p/modern-llms-mt-nlg-chinchilla-gopher?source=post_page-----ccccdd9f666f--------------------------------)
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '[cameronrwolfe.substack.com](https://cameronrwolfe.substack.com/p/modern-llms-mt-nlg-chinchilla-gopher?source=post_page-----ccccdd9f666f--------------------------------)'
- en: bibliography
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Vaswani, Ashish, et al. “Attention is all you need.” *Advances in neural
    information processing systems* 30 (2017).'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Vaswani, Ashish, 等. “注意力机制是你所需要的一切。” *神经信息处理系统的进展* 30 (2017)。'
- en: '[2] Brown, Tom, et al. “Language models are few-shot learners.” *Advances in
    neural information processing systems* 33 (2020): 1877–1901.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Brown, Tom, 等. “语言模型是少量样本学习者。” *神经信息处理系统的进展* 33 (2020): 1877–1901。'
- en: '[3] Radford, Alec, et al. “Improving language understanding by generative pre-training.”
    (2018).'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Radford, Alec, 等. “通过生成预训练提高语言理解。” (2018)。'
- en: '[4] Radford, Alec, et al. “Language Models are Unsupervised Multitask Learners.”'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Radford, Alec, 等. “语言模型是无监督的多任务学习者。”'
- en: '[5] Kaplan, Jared, et al. “Scaling laws for neural language models.” arXiv
    preprint arXiv:2001.08361 (2020).'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Kaplan, Jared, 等. “神经语言模型的扩展规律。” arXiv 预印本 arXiv:2001.08361 (2020)。'
- en: '[6] Ouyang, Long, et al. “Training language models to follow instructions with
    human feedback.” *arXiv preprint arXiv:2203.02155* (2022).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Ouyang, Long, 等. “通过人类反馈训练语言模型以跟随指令。” *arXiv 预印本 arXiv:2203.02155* (2022)。'
- en: '[7] Chen, Mark, et al. “Evaluating large language models trained on code.”
    *arXiv preprint arXiv:2107.03374* (2021).'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Chen, Mark, 等. “评估训练在代码上的大型语言模型。” *arXiv 预印本 arXiv:2107.03374* (2021)。'
- en: '[8] Thoppilan, Romal, et al. “Lamda: Language models for dialog applications.”
    *arXiv preprint arXiv:2201.08239* (2022).'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Thoppilan, Romal, 等. “Lamda: 对话应用的语言模型。” *arXiv 预印本 arXiv:2201.08239* (2022)。'
- en: '[9] Adiwardana, Daniel, et al. “Towards a human-like open-domain chatbot.”
    arXiv preprint arXiv:2001.09977 (2020).'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Adiwardana, Daniel, 等. “迈向类人开放域聊天机器人。” arXiv 预印本 arXiv:2001.09977 (2020)。'
- en: '[10] Ziegler, Daniel M., et al. “Fine-tuning language models from human preferences.”
    arXiv preprint arXiv:1909.08593 (2019).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Ziegler, Daniel M., 等. “从人类偏好中微调语言模型。” arXiv 预印本 arXiv:1909.08593 (2019)。'
- en: '[11] Stiennon, Nisan, et al. “Learning to summarize with human feedback.” Advances
    in Neural Information Processing Systems 33 (2020): 3008–3021.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Stiennon, Nisan, 等. “通过人类反馈学习总结。” 神经信息处理系统的进展 33 (2020): 3008–3021。'
- en: '[12] Glaese, Amelia, et al. “Improving alignment of dialogue agents via targeted
    human judgements.” *arXiv preprint arXiv:2209.14375* (2022).'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Glaese, Amelia, 等. “通过有针对性的人类判断提高对话代理的对齐。” *arXiv 预印本 arXiv:2209.14375*
    (2022)。'
- en: '[13] Taylor, Ross, et al. “Galactica: A large language model for science.”
    *arXiv preprint arXiv:2211.09085* (2022).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Taylor, Ross, 等. “Galactica: 一个大型科学语言模型。” *arXiv 预印本 arXiv:2211.09085*
    (2022)。'
- en: '[14] Gao, Leo, et al. “The pile: An 800gb dataset of diverse text for language
    modeling.” arXiv preprint arXiv:2101.00027 (2020).'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Gao, Leo, 等. “The pile: 一个800GB的多样化文本数据集用于语言建模。” arXiv 预印本 arXiv:2101.00027
    (2020)。'
- en: '[15] Hoffmann, Jordan, et al. “Training Compute-Optimal Large Language Models.”
    arXiv preprint arXiv:2203.15556 (2022).'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Hoffmann, Jordan, 等. “训练计算最优的大型语言模型。” arXiv 预印本 arXiv:2203.15556 (2022)。'
- en: '[16] Mirowski, Piotr, et al. “Co-writing screenplays and theatre scripts with
    language models: An evaluation by industry professionals.” *arXiv preprint arXiv:2209.14958*
    (2022).'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Mirowski, Piotr, 等. “与语言模型共同创作剧本和戏剧脚本：由行业专业人士评估。” *arXiv 预印本 arXiv:2209.14958*
    (2022)。'
- en: '[17] Verkuil, Robert, et al. “Language models generalize beyond natural proteins.”
    *bioRxiv* (2022).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Verkuil, Robert, 等. “语言模型超越自然蛋白质的泛化能力。” *bioRxiv* (2022)。'
- en: '[18] Iyer, Srinivasan, et al. “OPT-IML: Scaling Language Model Instruction
    Meta Learning through the Lens of Generalization.” *arXiv preprint arXiv:2212.12017*
    (2022).'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Iyer, Srinivasan, 等. “OPT-IML: 从泛化的视角扩展语言模型指令元学习。” *arXiv 预印本 arXiv:2212.12017*
    (2022)。'
- en: '[19] Liu, Fangyu, et al. “DePlot: One-shot visual language reasoning by plot-to-table
    translation.” *arXiv preprint arXiv:2212.10505* (2022).'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Liu, Fangyu, 等. “DePlot: 通过情节到表格翻译进行一次性视觉语言推理。” *arXiv 预印本 arXiv:2212.10505*
    (2022)。'
- en: '[20] Abramson, Josh, et al. “Improving Multimodal Interactive Agents with Reinforcement
    Learning from Human Feedback.” *arXiv preprint arXiv:2211.11602* (2022).'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Abramson, Josh, 等. “通过从人类反馈中强化学习改进多模态交互代理。” *arXiv 预印本 arXiv:2211.11602*
    (2022)。'
