- en: Apple M2 Max GPU vs Nvidia V100, P100 and T4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/apple-m2-max-gpu-vs-nvidia-v100-p100-and-t4-8b0d18d08894](https://towardsdatascience.com/apple-m2-max-gpu-vs-nvidia-v100-p100-and-t4-8b0d18d08894)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Compare Apple Silicon M2 Max GPU performances to Nvidia V100, P100, and T4 for
    training MLP, CNN, and LSTM models with TensorFlow.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://fabrice-daniel.medium.com/?source=post_page-----8b0d18d08894--------------------------------)[![Fabrice
    Daniel](../Images/c13598211944934b5565fad97f152700.png)](https://fabrice-daniel.medium.com/?source=post_page-----8b0d18d08894--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8b0d18d08894--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8b0d18d08894--------------------------------)
    [Fabrice Daniel](https://fabrice-daniel.medium.com/?source=post_page-----8b0d18d08894--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8b0d18d08894--------------------------------)
    ·6 min read·Nov 2, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9c4460ec6c7c0b9a501cae279c02a10f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Launched in November 2020, [**Apple M1**](https://en.wikipedia.org/wiki/Apple_M1)
    was a revolution in the world of computers dominated by Intel. These new M1 Macs
    showed impressive performance in many benchmarks as M1 was faster than most high-end
    desktop computers for only a fraction of their energy consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are my previous benchmarks for the M1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Benchmark M1 vs Xeon® vs Core i5 vs K80 and T4**](https://medium.com/towards-data-science/benchmark-m1-vs-xeon-vs-core-i5-vs-k80-and-t4-e3802f27421c)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**M1 competes with 20 cores Xeon® on TensorFlow training**](https://medium.com/towards-data-science/benchmark-m1-part-2-vs-20-cores-xeon-vs-amd-epyc-16-and-32-cores-8e394d56003d)'
  prefs: []
  type: TYPE_NORMAL
- en: In January 2023, Apple announced the new [**M2 Pro and M2 Max**](https://en.wikipedia.org/wiki/Apple_M2).
    Their specs let us expect good performance increases, especially regarding the
    GPU.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e66a1ec49c63c10d2ba6765b0f8cefc6.png)'
  prefs: []
  type: TYPE_IMG
- en: Compared M2 Max with M1 specs
  prefs: []
  type: TYPE_NORMAL
- en: '*This M2 Max has 30 GPU cores, so we estimated the 10.7 TFLOPS from the 13.6
    TFLOPS of the 38 GPU cores version.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a1c7ee55fcac13bdff32d65895caf633.png)'
  prefs: []
  type: TYPE_IMG
- en: GPU Compared
  prefs: []
  type: TYPE_NORMAL
- en: By comparison, the M2 Max 38 Cores GPU reaches 13.6 TFlops. My test below will
    show that the TFlops alone cannot be used to estimate the actual performances
    of these GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: To get comparable results, I run every test with the default TensorFlow FP32
    floating point precision.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can verify this precision by running :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Setup**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, I benchmark the [**M2 Max**](https://en.wikipedia.org/wiki/Apple_M2)
    GPU against Nvidia [**V100**](https://www.nvidia.com/en-us/data-center/v100/),
    [**P100**](https://www.nvidia.com/en-us/data-center/tesla-p100/), and [**T4**](https://www.nvidia.com/en-us/data-center/tesla-t4/)
    for MLP, CNN, and LSTM [**TensorFlow**](https://www.tensorflow.org) models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3ecb7d91531c5b327914f809fbca04f.png)'
  prefs: []
  type: TYPE_IMG
- en: Benchmark setup
  prefs: []
  type: TYPE_NORMAL
- en: 'On M1 and M2 Max computers, the environment was created under [**miniforge**](https://github.com/conda-forge/miniforge).
    Only the following packages were installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Unlike in my previous articles, TensorFlow is now directly working with Apple
    Silicon, no matter if you install it from pip, anaconda, or miniforge.
  prefs: []
  type: TYPE_NORMAL
- en: To enable GPU usage, install the [**tensorflow-metal**](https://developer.apple.com/metal/tensorflow-plugin/)
    package distributed by Apple using [**TensorFlow PluggableDevices**](https://blog.tensorflow.org/2021/06/pluggabledevice-device-plugins-for-TensorFlow.html).
    Note that [**Metal**](https://developer.apple.com/metal/) acceleration is also
    available for [**PyTorch**](https://developer.apple.com/metal/pytorch/) and [**JAX**](https://developer.apple.com/metal/jax/).
  prefs: []
  type: TYPE_NORMAL
- en: Apple says
  prefs: []
  type: TYPE_NORMAL
- en: With updates to Metal backend support, you can train a wider set of networks
    faster with new features like custom kernels and mixed-precision training.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This benchmark consists of a Python program running a sequence of [**MLP**](https://en.wikipedia.org/wiki/Multilayer_perceptron),
    [**CNN**](https://en.wikipedia.org/wiki/Convolutional_neural_network), and [**LSTM**](https://en.wikipedia.org/wiki/Long_short-term_memory)
    models training on [**Fashion MNIST**](https://github.com/zalandoresearch/fashion-mnist)**¹**
    for batch sizes of 32, 128, 512, and 1024 samples.
  prefs: []
  type: TYPE_NORMAL
- en: It also uses a validation set to be consistent with the way most model training
    is performed in real-life applications. Then, a test set is used to evaluate the
    model after the training to make sure everything works well. So, the training,
    validation, and test set sizes are respectively 50000, 10000, and 10000.
  prefs: []
  type: TYPE_NORMAL
- en: I use the same test program as described in this [previous article](https://medium.com/towards-data-science/benchmark-m1-vs-xeon-vs-core-i5-vs-k80-and-t4-e3802f27421c).
    As a reminder, the three models are simple and summarized below.
  prefs: []
  type: TYPE_NORMAL
- en: '**MLP**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**CNN**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**LSTM**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: They are all using the following optimizer and loss function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note that, unlike in the previous benchmark articles, the eager mode is now
    working for GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s first compare M2 Max with M1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/633645acf4ea7899e96db3182146af59.png)'
  prefs: []
  type: TYPE_IMG
- en: M2 Max vs M1
  prefs: []
  type: TYPE_NORMAL
- en: As expected, the M2 Max is always faster than the M1\. The following shows how
    many times.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a72117e77c3d3ce486b11e1fcd9d6d3e.png)'
  prefs: []
  type: TYPE_IMG
- en: How many times M2 Max is faster than M1 for GPU Training
  prefs: []
  type: TYPE_NORMAL
- en: Here are the average, minimum, and maximum increases in speed. We found that
    Convnet training with M2 Max can be up to 4.38 times faster than with M1 GPU.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf56c6b28fb1b238f344010927a7a520.png)'
  prefs: []
  type: TYPE_IMG
- en: How many times M2 Max is faster than M1 for GPU Training
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s compare M2 Max with the other Nvidia High-End GPUs found on Google
    Cloud, AWS, Google Colaboratory, or Kaggle platforms.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de94672cc997005e5fe3ba7975a16a9f.png)'
  prefs: []
  type: TYPE_IMG
- en: M2 Max vs Nvidia T4, V100 and P100
  prefs: []
  type: TYPE_NORMAL
- en: While training performances look quite similar for batch sizes 32 and 128, M2
    Max is showing the best performances over all the GPUs for batch sizes 512 and
    1024.
  prefs: []
  type: TYPE_NORMAL
- en: The P100 is the fastest of the other GPUs while when looking at the specifications
    V100 was expected. This can be due to the instance itself as the characteristics
    of the three different versions of each card cannot explain it. When checking
    the details, Kaggle P100 instance has 4 vCPU while Colab V100 proposes 2 vCPU
    only. Maybe this can impact global performances and should be checked further.
  prefs: []
  type: TYPE_NORMAL
- en: Here we compare how many times M2 Max is faster than Nvidia P100.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/efa2db05d34bf6c8f42c6a78008f7635.png)'
  prefs: []
  type: TYPE_IMG
- en: How many times M2 Max is faster than P100 for GPU Training
  prefs: []
  type: TYPE_NORMAL
- en: Here are the average, minimum, and maximum increases in speed. We found that
    on average the M2 Max is identical to a P100 and that while it’s slower for a
    batch size of 32 and 128 it can be faster up to 1.77 times for MLP, 1.43 times
    for LSTM or 1.24 times for CNN with a batch size of 1024.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26a1ac1af363faec738f73f479eac797.png)'
  prefs: []
  type: TYPE_IMG
- en: How many times M2 Max is faster than P100 for GPU Training
  prefs: []
  type: TYPE_NORMAL
- en: Benefit of GPU vs CPU on M2 Max
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the past, with the previous version of TensorFlow, it was often observed
    that MLP and LSTM were more efficiently trained on the CPU than GPU. Only CNN
    benefits from GPU. But now, with the recent version of TensorFlow things have
    changed for LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: So a last interesting test is to check when models training benefit from the
    GPU compared to the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97b57ea5b1725f773b9fd58c2c84afa7.png)'
  prefs: []
  type: TYPE_IMG
- en: How many times training on M2 Max GPU is faster than CPU
  prefs: []
  type: TYPE_NORMAL
- en: MLP does not benefit from GPU acceleration. As expected CNN benefits for every
    case but from a batch size of 128 the difference is huge and goes up to about
    5 times faster.
  prefs: []
  type: TYPE_NORMAL
- en: But the really good surprise comes from LSTM which shows by far the biggest
    benefit from training on GPU which is up to 6.77 times faster for a batch size
    of 1024
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From these tests, it appears that
  prefs: []
  type: TYPE_NORMAL
- en: M2 Max is by far faster than M1, so Mac users can benefit from such an upgrade
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compared to T4, P100, and V100 M2 Max is always faster for a batch size of 512
    and 1024
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance differences are not only a TFlops concern. M2 Max is theoretically
    15% faster than P100 but in the true test for a batch size of 1024 it shows performances
    higher by 24% for CNN, 43% for LSTM, and 77% for MLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, these metrics can only be considered for similar neural network types
    and depths as used in this test.
  prefs: []
  type: TYPE_NORMAL
- en: A new test is needed with bigger models and datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Now TensorFlow is easy to install and run efficiently on Apple Silicon. The
    GPU can now be used for any model and increase a lot the training performances
    for any type of model. That’s especially true for LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: We can conclude that M2 Max is a very good platform for machine learning engineers.
    It enables training models on GPU with very good performances, even better than
    a T4, P100, or V100 commonly found on cloud instances. It also includes 12 CPU
    cores, making it more flexible than these GPU instances, and all of this for only
    a fraction of their energy consumption.
  prefs: []
  type: TYPE_NORMAL
- en: '**Update (12/11/2023)**: The last versions of tensorflow-metal (> 0.8.0) has
    a bug making some models unable to converge. To avoid this issue, I’ve updated
    the installation instructions in the article by setting the TensorFlow and tensorflow-metal
    versions. Then, I replayed the test with this new package combination to ensure
    the benchmark was unchanged. I confirm that it does not change anything to the
    performance results.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Images and Code** : All images and codes in this work are by the author unless
    explicitly stated otherwise.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Datasets Licences** : [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist)
    is licenced under the [MIT Licence](https://github.com/zalandoresearch/fashion-mnist/blob/master/LICENSE)'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Han Xiao and Kashif Rasul and Roland Vollgraf, [Fashion-MNIST: a Novel
    Image Dataset for Benchmarking Machine Learning Algorithms](https://arxiv.org/abs/1708.07747)
    (2017)'
  prefs: []
  type: TYPE_NORMAL
