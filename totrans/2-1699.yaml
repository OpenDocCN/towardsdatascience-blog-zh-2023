- en: Prompt Engineering Guide
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/prompt-engineering-guide-for-data-analysts-54f480ba4d98](https://towardsdatascience.com/prompt-engineering-guide-for-data-analysts-54f480ba4d98)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/7eb4d92c8f54e60146f7dd4ba68cb20b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Emiliano Vittoriosi](https://unsplash.com/@emilianovittoriosi?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Principles, Techniques, and Applications to Harness the Power of Prompts in
    LLMs as a Data Analyst
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://tanuwidjajaolivia.medium.com/?source=post_page-----54f480ba4d98--------------------------------)[![Olivia
    Tanuwidjaja](../Images/52a56de28da9b782b57f1c3928655cfb.png)](https://tanuwidjajaolivia.medium.com/?source=post_page-----54f480ba4d98--------------------------------)[](https://towardsdatascience.com/?source=post_page-----54f480ba4d98--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----54f480ba4d98--------------------------------)
    [Olivia Tanuwidjaja](https://tanuwidjajaolivia.medium.com/?source=post_page-----54f480ba4d98--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----54f480ba4d98--------------------------------)
    ·7 min read·May 7, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[Large Language Model (LLM)](https://www.youtube.com/watch?v=iR2O2GPbB0E) is
    on the rise, driven by the popularity of [ChatGPT by OpenAI](https://openai.com/blog/chatgpt)
    which took the internet by storm. As a practitioner in the data field, I look
    for ways to best utilize this technology in my work, especially for insightful-yet-practical
    work as a Data Analyst.'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs can solve tasks without additional model training via “prompting” techniques,
    in which the **problem is presented to the model as a text prompt**. Getting to
    “**the right prompts**” are important to ensure the model is providing high-quality
    and accurate results for the tasks assigned.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will be sharing the principles of prompting, techniques to
    build prompts, and the roles Data Analysts can play in this “prompting era”.
  prefs: []
  type: TYPE_NORMAL
- en: What is prompt engineering?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quoting [Ben Lorica from Gradient Flow](https://gradientflow.com/the-future-of-prompt-engineering-getting-the-most-out-of-llms/),
    “**prompt engineering is the art of crafting effective input prompts to elicit
    the desired output from foundation models**.” It’s the iterative process of developing
    prompts that can effectively leverage the capabilities of existing generative
    AI models to accomplish specific objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering skills can help us understand the capabilities and limitations
    of a large language model. **The prompt itself acts as an input to the model,
    which signifies the impact on the model output.** A good prompt will get the model
    to produce desirable output, whereas working iteratively from a bad prompt will
    help us understand the limitations of the model and how to work with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Isa Fulford and Andrew Ng in the [ChatGPT Prompt Engineering for Developers
    course](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)
    mentioned two main principles of prompting:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 1: **Write clear and specific instructions**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Principle 2: **Give the model time to “think”**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I think prompting is *like giving instructions to a naive “machine kid”*.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The child is very intelligent, but you need to be **clear about what you need**
    from it (by providing explanations, examples, specified output format, etc) and
    **give it some space to digest and process it** (specify the problem-solving steps,
    ask it to slowly process it). The child, given its exposure, can also be *very
    creative and imaginary* in providing answers — which we call a [hallucination
    of the LLM](https://thestack.technology/the-big-hallucination-large-language-models-consciousness/).
    Understanding the context and providing the right prompt might help in avoiding
    this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Engineering Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prompt engineering is a growing field, with research on this topic rapidly increasing
    from 2022 onwards. Some of the state-of-the-art prompting techniques commonly
    used include n-shot prompting, chain-of-thought (CoT) prompting, and generated
    knowledge prompting.
  prefs: []
  type: TYPE_NORMAL
- en: '*A sample Python notebook demonstrating these techniques* *is shared under*
    [*this GitHub project.*](https://github.com/oliviatan29/prompt-engineering)'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. N-shot prompting (Zero-shot prompting, Few-shot prompting)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Known for its variation like Zero-shot prompting and Few-shot prompting, the
    N in N-shot prompting represents the number of “training” or clues given to the
    model to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Zero-shot prompting** is where a model makes predictions **without any additional
    training.** This works for common straightforward problems like classification
    (i.e. sentiment analysis, spam classification), text transformation (i.e. translation,
    summarizing, expanding), and simple text generation on which the LLM has been
    largely trained.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/506a7bd75f299d281a79c711e7f63a67.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Zero-shot prompting: Straightforwardly ask the model on sentiment (Image by
    Author)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Few-shot prompting** uses a **small amount of data** (typically between two
    and five) **to adapt its output** based on these small examples. These examples
    are meant to steer the model to better performance for a more context-specific
    problem.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9d3dfc9576322375ef78cec97c31c4f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Few-shot prompting: Give examples of how we expect the model output to be'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Chain-of-Thought (CoT) prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Chain-of-Thought prompting](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)
    was introduced by Google researchers in 2022\. In the *Chain-of-Thought prompting*,
    the model is prompted to **produce intermediate reasoning steps before giving
    the final answer** to a multi-step problem. The idea is that a model-generated
    chain of thought would mimic an intuitive thought process when working through
    a multi-step reasoning problem.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03c679acfed8e015cadabbfde18c7164.png)'
  prefs: []
  type: TYPE_IMG
- en: Chain-of-Thought prompting helps in driving the model to break down problems
    accordingly
  prefs: []
  type: TYPE_NORMAL
- en: This method enables models to decompose multi-step problems into intermediate
    steps, enabling them to solve complex reasoning problems that are not solvable
    with standard prompting methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some further variations of Chain-of Thought prompting include:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Self-consistency prompting](https://arxiv.org/pdf/2203.11171.pdf): Sample
    multiple diverse reasoning paths and **select the most consistent answers**. By
    utilizing a majority voting system, the model can arrive at more accurate and
    reliable answers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Least-to-Most prompting (LtM)](https://arxiv.org/abs/2205.10625): Specify
    the chain of thought to first break a problem into a series of simpler subproblems
    and then **solve them in sequence**. Solving each subproblem is facilitated by
    the answers to previously solved subproblems. This technique is inspired by real-world
    educational strategies for children.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Active Prompting](https://arxiv.org/abs/2302.12246): **Scaling the CoT approach**
    by determining which questions are the most important and helpful ones for human
    annotation. It first calculates the uncertainty among the LLM’s predictions, then
    select the most uncertain questions, and these questions are selected for human
    annotation before being put into a CoT prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3\. Generated knowledge prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The idea behind the [generated knowledge prompting](https://arxiv.org/pdf/2110.08387.pdf)
    is to ask the LLM to **generate potentially useful information** about a given
    question/prompt, and then **leverage that provided knowledge** as additional input
    for generating a final response.
  prefs: []
  type: TYPE_NORMAL
- en: For example, say you want to write an article about cybersecurity, particularly
    cookie theft. Before asking the LLM to write the article, you can ask it to generate
    some danger and protection against cookie theft. This will help the LLM write
    a more informative blog post.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/63b735ddbcebc6a4f71b74db8974c0a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Generated knowledge prompting: (1) Ask the model to generate some content'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ffc1680c472203be05e038dde1a2beb5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Generated knowledge prompting: (2) Use the generated content as input to the
    model'
  prefs: []
  type: TYPE_NORMAL
- en: Additional tactics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On top of the above-specified techniques, you can also use these tactics below
    to make the prompting more effective
  prefs: []
  type: TYPE_NORMAL
- en: '**Use delimiters** like triple backticks (```), angle brackets (<>), or tags
    (<tag> </tag>) to indicate distinct parts of the input, making it cleaner for
    debugging and avoiding prompt injection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ask for structured output** (i.e. HTML/JSON format), this is useful for using
    the model output for another machine processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specify the **intended tone of the text** to get the tonality, format, and length
    of model output that you need. For example, you can instruct the model to formalize
    the language, generate not more than 50 words, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modify the model’s **temperature parameter** to play around the model’s degree
    of randomness. The higher the temperature, the model’s output would be random
    than accurate, and even hallucinate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A sample Python notebook demonstrating these techniques* *is shared under*
    [*this GitHub project.*](https://github.com/oliviatan29/prompt-engineering)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e6afbdbdd508428b09ef83b6f52eb01d.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Camylla Battani](https://unsplash.com/@camylla93?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The Role of Analysts in Prompt Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can possibly infer from the examples above, prompt engineering requires
    a very specific technical communication craft. While you still require business
    context and problem-solving skills, it is still a new kind of craft that is not
    entirely covered as part of a [conventional data analytics skillset](https://medium.com/towards-data-science/how-does-a-remarkable-data-analyst-look-like-dd0e4326c670).
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Analysts can leverage their context knowledge, problem-solving skills,
    and statistical/technical capabilities, with the addition of effective communication
    for prompt engineering. These are the key tasks related to prompt engineering
    (and LLMs) which potentially be done by Analysts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Specifying LLM problems to be solved**. With an understanding of the LLM
    concepts, we can define the actions to be executed by the model (i.e. whether
    it is text classification, generation, or transformation problem) and the right
    question with reference points to be put as the prompts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative prompting**. In developing a data model, oftentimes we go through
    an iterative process. After building the initial model, we evaluate the result,
    refine it, and retry it along the way. Similarly for a prompt, we analyze where
    the result does not give what you want, and refine it with clearer instructions,
    additional examples, or specified steps. This requires critical reasoning which
    most Data Analysts are already good at.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt versioning and management**. With iterative prompting, you will end
    up with numerous prompt attempts, and the identified model capabilities and/or
    limitations. It is important to keep track of and document these findings for
    team learning and continuous improvement, as with any other existing data analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Designing for safe-prompting**. Although it has shown impressive capabilities,
    LLM is still in a very early stage and is prone to loopholes and limitations.
    There is this [hallucination problem](https://thestack.technology/the-big-hallucination-large-language-models-consciousness/)
    where models provide highly misleading information, and also [prompt injection](https://simonwillison.net/2023/Apr/14/worst-that-can-happen/)
    risk of getting untrusted text is used as part of the prompt. Depending on the
    use case of the model and prompting, Analysts can advise programmatic safeguards
    to limit the prompt usage and analysis of problematic prompting detection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On top of leveraging the existing skills, Analysts need to **hone their communication
    skills and the ability to break down problems** to provide better prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large Language Models have shown promising results in performing numerous types
    of language tasks, and **prompt engineering is the key to unlocking these capabilities**.
    Prompt engineering is about communicating effectively with an AI to achieve desired
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Several techniques can be used to do prompt engineering, but the foundational
    principle is consistent. It is about providing clear instructions to the model
    and helping it in digesting and processing these instructions. **Data Analysts
    can leverage their context knowledge and problem-solving skills to frame the right
    prompts and leverage their technical capabilities for designing prompt safeguards**.
  prefs: []
  type: TYPE_NORMAL
- en: 'For further resources on prompt engineering, check out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ChatGPT Prompt Engineering for Developers](https://learn.deeplearning.ai/chatgpt-prompt-eng)
    (DeepLearning.AI course)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LearnPrompting.org](https://learnprompting.org/docs/intro)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PromptingGuide.ai](https://www.promptingguide.ai/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenAI applications documentation](https://platform.openai.com/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Future of Prompt Engineering: Getting The Most Out of LLMs](https://gradientflow.com/the-future-of-prompt-engineering-getting-the-most-out-of-llms/)
    (Gradient Flow article)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I believe this area will grow even further in the next few years, and I’m excited
    to see and take part in the evolution.
  prefs: []
  type: TYPE_NORMAL
