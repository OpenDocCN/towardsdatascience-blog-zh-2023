- en: Your Features Are Important? It Doesn’t Mean They Are Good
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你的特征重要吗？这并不意味着它们是好的
- en: 原文：[https://towardsdatascience.com/your-features-are-important-it-doesnt-mean-they-are-good-ff468ae2e3d4](https://towardsdatascience.com/your-features-are-important-it-doesnt-mean-they-are-good-ff468ae2e3d4)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/your-features-are-important-it-doesnt-mean-they-are-good-ff468ae2e3d4](https://towardsdatascience.com/your-features-are-important-it-doesnt-mean-they-are-good-ff468ae2e3d4)
- en: “Feature Importance” is not enough. You also need to look at “Error Contribution”
    if you want to know which features are beneficial for your model.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “特征重要性”是不够的。如果你想知道哪些特征对模型有益，你还需要关注“错误贡献”。
- en: '[](https://medium.com/@mazzanti.sam?source=post_page-----ff468ae2e3d4--------------------------------)[![Samuele
    Mazzanti](../Images/432477d6418a3f79bf25dec42755d364.png)](https://medium.com/@mazzanti.sam?source=post_page-----ff468ae2e3d4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ff468ae2e3d4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ff468ae2e3d4--------------------------------)
    [Samuele Mazzanti](https://medium.com/@mazzanti.sam?source=post_page-----ff468ae2e3d4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mazzanti.sam?source=post_page-----ff468ae2e3d4--------------------------------)[![Samuele
    Mazzanti](../Images/432477d6418a3f79bf25dec42755d364.png)](https://medium.com/@mazzanti.sam?source=post_page-----ff468ae2e3d4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ff468ae2e3d4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ff468ae2e3d4--------------------------------)
    [Samuele Mazzanti](https://medium.com/@mazzanti.sam?source=post_page-----ff468ae2e3d4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ff468ae2e3d4--------------------------------)
    ·10 min read·Aug 21, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ff468ae2e3d4--------------------------------)
    ·10分钟阅读·2023年8月21日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/38f178e94336a144567ffa647733e5b7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38f178e94336a144567ffa647733e5b7.png)'
- en: '[Image by Author]'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[作者提供的图片]'
- en: “Important” and “Good” Are Not Synonyms
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “重要”和“好”并不是同义词
- en: The concept of “feature importance” is widely used in machine learning as the
    most basic type of model explainability. For example, it is used in Recursive
    Feature Elimination (RFE), to iteratively drop the least important feature of
    the model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: “特征重要性”这一概念在机器学习中被广泛使用，是最基本的模型可解释性类型。例如，它在递归特征消除（RFE）中被使用，用于迭代地删除模型中最不重要的特征。
- en: However, there is a misconception about it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于这一点存在一种误解。
- en: '**The fact that a feature is important doesn’t imply that it is beneficial
    for the model!**'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**一个特征重要并不意味着它对模型有益！**'
- en: Indeed, when we say that a feature is important, this simply means that the
    feature brings a high contribution to the predictions made by the model. But we
    should consider that **such contribution may be wrong**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，当我们说一个特征重要时，这仅仅意味着该特征对模型的预测贡献很高。但我们应考虑到**这种贡献可能是错误的**。
- en: 'Take a simple example: a data scientist accidentally forgets the Customer ID
    between its model’s features. The model uses Customer ID as a highly predictive
    feature. As a consequence, this feature will have a high feature importance even
    if it is actually worsening the model, because it cannot work well on unseen data.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 举个简单的例子：一个数据科学家不小心忘记了模型特征中的客户ID。模型将客户ID作为一个高度预测性特征。因此，即使这个特征实际上在降低模型性能，因为它无法在未见数据上良好运行，这个特征的重要性也会很高。
- en: 'To make things clearer, we will need to make a distinction between two concepts:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让事情更清楚，我们需要区分两个概念：
- en: '**Prediction Contribution**: what part of the predictions is due to the feature;
    this is equivalent to feature importance.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测贡献**：预测中有多少部分是由于特征；这等同于特征重要性。'
- en: '**Error Contribution**: what part of the prediction errors is due to the presence
    of the feature in the model.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**错误贡献**：预测错误中有多少部分是由于模型中存在该特征。'
- en: In this article, we will see how to calculate these quantities and how to use
    them to get valuable insights about a predictive model (and to improve it).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨如何计算这些量，并如何利用它们获得有关预测模型的有价值的见解（并加以改进）。
- en: 'Note: this article is focused on the regression case. If you are more interested
    in the classification case, you can read [“Which features are harmful for your
    classification model?”](/which-features-are-harmful-for-your-classification-model-6227859a44a6)'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：本文专注于回归案例。如果你对分类案例更感兴趣，可以阅读[“哪些特征对你的分类模型有害？”](/which-features-are-harmful-for-your-classification-model-6227859a44a6)
- en: Starting from a Toy Example
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从一个玩具示例开始
- en: Suppose we built a model to predict the income of people based on their job,
    age, and nationality. Now we use the model to make predictions on three people.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们建立了一个模型，以根据人们的工作、年龄和国籍来预测收入。现在我们使用该模型对三个人进行预测。
- en: 'Thus, we have the ground truth, the model prediction, and the resulting error:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有实际值、模型预测和结果误差：
- en: '![](../Images/581833f2d1e1c5e5485badfaec244606.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/581833f2d1e1c5e5485badfaec244606.png)'
- en: Ground truth, model prediction, and absolute error (in thousands of $). [Image
    by Author]
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 实际值、模型预测和绝对误差（以千美元计）。[作者提供的图片]
- en: Computing “Prediction Contribution”
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算“预测贡献”
- en: 'When we have a predictive model, we can always decompose the model predictions
    into the contributions brought by the single features. This can be done through
    SHAP values (if you don’t know about how SHAP values work, you can read my article:
    [SHAP Values Explained Exactly How You Wished Someone Explained to You](/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30)).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有一个预测模型时，我们可以始终将模型预测分解为各个特征带来的贡献。这可以通过 SHAP 值来完成（如果你不知道 SHAP 值的工作原理，可以阅读我的文章：[SHAP
    值解释：正如你希望有人向你解释的那样](/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30)）。
- en: So, let’s say these are the SHAP values relative to our model for the three
    individuals.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，假设这些是我们模型对三个人的 SHAP 值。
- en: '![](../Images/0e1426785a530186b67cf5aeaf064a85.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e1426785a530186b67cf5aeaf064a85.png)'
- en: SHAP values for our model’s predictions (in thousands of $). [Image by Author]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型预测的 SHAP 值（以千美元计）。[作者提供的图片]
- en: 'The main property of SHAP values is that they are additive. This means that
    — by taking the sum of each row — we will obtain our model’s prediction for that
    individual. For instance, if we take the second row: 72k $ +3k $ -22k $ = 53k
    $, which is exactly the model’s prediction for the second individual.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP 值的主要特性是它们是可加的。这意味着——通过对每一行求和——我们将得到该个体的模型预测。例如，如果我们取第二行：72k $ +3k $ -22k
    $ = 53k $，这正是模型对第二个体的预测。
- en: 'Now, SHAP values are a good indicator of how important a feature is for our
    predictions. Indeed, the higher the (absolute) SHAP value, the more influential
    the feature for the prediction about that specific individual. Note that I am
    talking about absolute SHAP values because the sign here doesn’t matter: a feature
    is equally important if it pushes the prediction up or down.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，SHAP 值是特征对我们预测重要性的良好指标。确实，SHAP 值的绝对值越高，特征对该特定个体预测的影响越大。注意，我在这里讨论的是绝对 SHAP
    值，因为符号并不重要：一个特征无论是使预测值上升还是下降，其重要性是相同的。
- en: 'Therefore, **the Prediction Contribution of a feature is equal to the mean
    of the absolute SHAP values of that feature**. If you have the SHAP values stored
    in a Pandas dataframe, this is as simple as:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，**特征的预测贡献等于该特征绝对 SHAP 值的均值**。如果你在 Pandas 数据框中存储了 SHAP 值，这非常简单：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In our example, this is the result:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，结果如下：
- en: '![](../Images/0934a4e19a2cdbd69a913c958df98775.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0934a4e19a2cdbd69a913c958df98775.png)'
- en: Prediction Contribution. [Image by Author]
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 预测贡献。[作者提供的图片]
- en: As you can see, job is clearly the most important feature since, on average,
    it accounts for 71.67k $ of the final prediction. Nationality and age are respectively
    the second and the third most relevant feature.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，工作（job）显然是最重要的特征，因为它在平均情况下占据了最终预测的71.67k $。国籍（nationality）和年龄（age）分别是第二和第三重要的特征。
- en: However, the fact that a given feature accounts for a relevant part of the final
    prediction doesn’t tell anything about the feature’s performance. To consider
    also this aspect, we will need to compute the “Error Contribution”.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，某个特征对最终预测的重要性并不能说明该特征的表现。为了考虑这一方面，我们需要计算“误差贡献”。
- en: Computing “Error Contribution”
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算“误差贡献”
- en: 'Let’s say that we want to answer the following question: “What predictions
    would the model make if it didn’t have the feature *job*?” SHAP values allow us
    to answer this question. In fact, since they are additive, it’s enough to subtract
    the SHAP values relative to the feature *job* from the predictions made by the
    model.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想回答以下问题：“如果模型没有特征*工作*，会做出什么预测？”SHAP 值允许我们回答这个问题。事实上，由于它们是可加的，我们只需从模型做出的预测中减去与特征*工作*相关的
    SHAP 值即可。
- en: 'Of course, we can repeat this procedure for each feature. In Pandas:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们可以对每个特征重复这一过程。在 Pandas 中：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This is the outcome:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![](../Images/c9c89f145e4f2405408868f49e7f2a02.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9c89f145e4f2405408868f49e7f2a02.png)'
- en: Predictions that we would obtain if we removed the respective feature. [Image
    by Author]
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果去掉相应特征后得到的预测值。[图片由作者提供]
- en: This means that, if we didn’t have the feature *job*, then the model would predict
    20k $ for the first individual, -19k $ for the second one, and -8k $ for the third
    one. Instead, if we didn’t have the feature *age*, the model would predict 73k
    $ for the first individual, 50k $ for the second one, and so on.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，如果我们没有*工作*这个特征，那么模型会预测第一位个体20k美元，第二位个体-19k美元，第三位个体-8k美元。相反，如果我们没有*年龄*这个特征，模型会预测第一位个体73k美元，第二位个体50k美元，以此类推。
- en: 'As you can see, the predictions for each individual vary a lot if we removed
    different features. As a consequence, also the prediction errors would be very
    different. We can easily compute them:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，如果我们去掉不同的特征，每个个体的预测变化很大。因此，预测误差也会非常不同。我们可以轻松地计算它们：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The result is the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![](../Images/805670ab127b17f740977f67ed31092c.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/805670ab127b17f740977f67ed31092c.png)'
- en: Absolute errors that we would obtain if we removed the respective feature. [Image
    by Author]
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果去掉相应特征后得到的绝对误差。[图片由作者提供]
- en: These are the errors that we would obtain if we removed the respective feature.
    Intuitively, if the error is small, then removing the feature is not a problem
    — or it’s even beneficial — for the model. If the error is high, then removing
    the feature is not a good idea.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们如果去掉相应特征后会得到的错误。直观地说，如果误差很小，那么去掉该特征对模型没有问题——甚至是有益的。如果误差很高，那么去掉特征则不是一个好主意。
- en: 'But we can do more than this. Indeed, we can compute the difference between
    the errors of the full model and the errors we would obtain without the feature:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们还可以做更多。事实上，我们可以计算完整模型的错误与去掉特征后得到的错误之间的差异：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Which is:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是：
- en: '![](../Images/4e85e2e22ecb7b3b97ac00d0cf583d03.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e85e2e22ecb7b3b97ac00d0cf583d03.png)'
- en: Difference between the errors of the model and the errors we would have without
    the feature. [Image by Author]
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的错误与去掉特征后我们会得到的错误之间的差异。[图片由作者提供]
- en: 'If this number is:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个数字是：
- en: negative, then the presence of the feature leads to a reduction in the prediction
    error, so the feature works well for that observation!
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是负的，那么特征的存在会减少预测误差，因此该特征对该观察结果很有效！
- en: positive, then the presence of the feature leads to an increase in the prediction
    error, so the feature is bad for that observation.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是正的，那么特征的存在会导致预测误差增加，因此该特征对该观察结果是不利的。
- en: '**We can compute “Error Contribution” as the mean of these values, for each
    feature**. In Pandas:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们可以计算“误差贡献”，作为每个特征这些值的均值**。在Pandas中：'
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This is the outcome:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![](../Images/1d980a045fc6a6071ec204c003d44d71.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d980a045fc6a6071ec204c003d44d71.png)'
- en: Error Contribution. [Image by Author]
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 误差贡献。[图片由作者提供]
- en: If this value is positive, then it means that, on average, the presence of the
    feature in the model leads to a higher error. Thus, without that feature, the
    prediction would have been generally better. In other words, the feature is making
    more harm than good!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个值是正的，那么这意味着，平均而言，特征的存在导致模型的错误增加。因此，没有这个特征，预测会更好。换句话说，这个特征的负面影响大于正面影响！
- en: On the contrary, the more negative this value, the more beneficial the feature
    is for the predictions since its presence leads to smaller errors.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，这个值越负，特征对预测的益处越大，因为其存在导致更小的误差。
- en: Let’s try to use these concepts on a real dataset.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试在实际数据集上使用这些概念。
- en: Predicting Gold Returns
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测黄金回报
- en: Hereafter, I will use a dataset taken from [Pycaret](https://github.com/pycaret/pycaret)
    (a Python library under [MIT license](https://github.com/pycaret/pycaret/blob/master/LICENSE)).
    The dataset is called “Gold” and it contains time series of financial data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在起，我将使用来自[Pycaret](https://github.com/pycaret/pycaret)（一个[MIT许可](https://github.com/pycaret/pycaret/blob/master/LICENSE)的Python库）的数据集。该数据集名为“Gold”，包含财务数据的时间序列。
- en: '![](../Images/85c99fabd81256f5c9b1686d0cc517d1.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85c99fabd81256f5c9b1686d0cc517d1.png)'
- en: Dataset sample. The features are all expressed in percentage, so -4.07 means
    a return of -4.07%. [Image by Author]
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集样本。特征都以百分比表示，因此 -4.07 意味着 -4.07% 的回报。[图片由作者提供]
- en: 'The features consist in the returns of financial assets respectively 22, 14,
    7, and 1 days before the observation moment (“T-22”, “T-14”, “T-7”, “T-1”). Here
    is the exhaustive list of all the financial assets used as predictive features:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 特征由观察时刻前22、14、7和1天的金融资产回报组成（“T-22”、“T-14”、“T-7”、“T-1”）。这是所有用作预测特征的金融资产的详尽列表：
- en: '![](../Images/259858b70548feba804837baa9e4b9b0.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/259858b70548feba804837baa9e4b9b0.png)'
- en: List of the available assets. Each asset is observed at time -22, -14, -7, and
    -1\. [Image by Author]
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 可用资产列表。每个资产在时间 -22、-14、-7 和 -1 被观察到。[作者提供的图片]
- en: In total, we have 120 features.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有120个特征。
- en: The goal is to predict the Gold price (return) 22 days ahead in time (“Gold_T+22”).
    Let’s take a look at the target variable.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是预测22天后的黄金价格（回报） (“Gold_T+22”)。让我们来看看目标变量。
- en: '![](../Images/d07a8d376c2fe6dc13c31094d3f5317d.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d07a8d376c2fe6dc13c31094d3f5317d.png)'
- en: Histogram of the variable. [Image by Author]
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 变量的直方图。[作者提供的图片]
- en: 'Once I loaded the dataset, these are the steps I carried out:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦加载了数据集，这些是我进行的步骤：
- en: 'Split the full dataset randomly: 33% of the rows in the training dataset, another
    33% in the validation dataset, and the remaining 33% in the test dataset.'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机拆分完整数据集：33%分配给训练数据集，另33%分配给验证数据集，其余33%分配给测试数据集。
- en: Train a LightGBM Regressor on the training dataset.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据集上训练一个LightGBM回归模型。
- en: Make predictions on training, validation, and test datasets, using the model
    trained at the previous step.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用前一步训练的模型对训练、验证和测试数据集进行预测。
- en: Compute SHAP values of training, validation, and test datasets, using the Python
    library “shap”.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算训练、验证和测试数据集的SHAP值，使用Python库“shap”。
- en: Compute the Prediction Contribution and the Error Contribution of each feature
    on each dataset (training, validation, and test), using the code we have seen
    in the previous paragraph.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个特征在每个数据集上的预测贡献和错误贡献（训练集、验证集和测试集），使用我们在前一段看到的代码。
- en: Comparing Prediction Contribution and Error Contribution
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较预测贡献和错误贡献
- en: Let’s compare the Error Contribution and the Prediction Contribution in the
    training dataset. We will use a scatter plot, so the dots identify the 120 features
    of the model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 比较训练数据集中错误贡献和预测贡献。我们将使用散点图，其中点表示模型的120个特征。
- en: '![](../Images/51b06d42a016d31570fed57179f1a6ca.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51b06d42a016d31570fed57179f1a6ca.png)'
- en: Prediction Contribution vs. Error Contribution (on the Training dataset). [Image
    by Author]
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 预测贡献与错误贡献（在训练数据集上）。[作者提供的图片]
- en: There is a highly negative correlation between Prediction Contribution and Error
    Contribution in the training set.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练集中，预测贡献与错误贡献之间存在高度负相关。
- en: 'And this makes sense: **since the model learns on the training dataset, it
    tends to attribute high importance (i.e. high Prediction Contribution) to those
    features that lead to a great reduction in the prediction error (i.e. highly negative
    Error Contribution)**.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有意义：**因为模型在训练数据集上学习，它倾向于将高重要性（即高预测贡献）分配给那些导致预测错误大幅减少的特征（即高度负的错误贡献）**。
- en: But this doesn’t add much to our knowledge, right?
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并没有增加我们对知识的了解，对吧？
- en: Indeed, what really matters to us is the validation dataset. The validation
    dataset is in fact the best proxy we can have about how our features will behave
    on new data. So, let’s make the same comparison on the validation set.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，我们真正关心的是验证数据集。验证数据集实际上是我们可以用来了解特征在新数据上表现的最佳代理。因此，让我们在验证集上做相同的比较。
- en: '![](../Images/73807cbd88b313b089ec734f6718506b.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73807cbd88b313b089ec734f6718506b.png)'
- en: Prediction Contribution vs. Error Contribution (on the Validation dataset).
    [Image by Author]
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 预测贡献与错误贡献（在验证数据集上）。[作者提供的图片]
- en: From this plot, we can extract some much more interesting information.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个图中，我们可以提取出一些更有趣的信息。
- en: The features in the lower right part of the plot are those to which our model
    is correctly assigning high importance since they actually bring a reduction in
    the prediction error.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图表右下角的特征是模型正确地赋予高重要性的特征，因为这些特征实际上降低了预测错误。
- en: Also, note that “Gold_T-22” (the return of gold 22 days before the observation
    period) is working really well compared to the importance that the model is attributing
    to it. This means that **this feature is possibly underfitting**. And this piece
    of information is particularly interesting since gold is the asset we are trying
    to predict (“Gold_T+22”).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，请注意“Gold_T-22”（观察期前22天的黄金回报）与模型赋予的权重相比表现非常好。这意味着**这个特征可能存在欠拟合**。这一点特别有趣，因为黄金是我们试图预测的资产（“Gold_T+22”）。
- en: On the other hand, **the features that have an Error Contribution above 0 are
    making our predictions worse**. For instance, “US Bond ETF_T-1” on average changes
    the model prediction by 0.092% (Prediction Contribution), but it leads the model
    to make a prediction on average 0.013% (Error Contribution) worse than it would
    have been without that feature.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**误差贡献高于0的特征使我们的预测变得更差**。例如，“US Bond ETF_T-1”平均改变了模型预测0.092%（预测贡献），但使模型预测比没有该特征时平均差0.013%（误差贡献）。
- en: We may suppose that **all the features with a high Error Contribution (compared
    to their Prediction Contribution) are probably overfitting** or, in general, they
    have different behavior in the training set and in the validation set.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以假设**所有具有高误差贡献（相对于其预测贡献）的特征可能存在过拟合**，或者通常它们在训练集和验证集中的表现不同。
- en: Let’s see which features have the largest Error Contribution.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看哪些特征的误差贡献最大。
- en: '![](../Images/b38068dfea07c17febd3c0862a5a88ee.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b38068dfea07c17febd3c0862a5a88ee.png)'
- en: Features sorted by decreasing Error Contribution. [Image by Author]
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 按误差贡献递减排序的特征。[图片来源]
- en: 'And now the features with the lowest Error Contribution:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是误差贡献最低的特征：
- en: '![](../Images/779c5eef6326438a7aa8aaf68e92e543.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/779c5eef6326438a7aa8aaf68e92e543.png)'
- en: Features sorted by increasing Error Contribution. [Image by Author]
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 按误差贡献递增排序的特征。[图片来源]
- en: Interestingly, we may observe that all the features with higher Error Contribution
    are relative to T-1 (1 day before the observation moment), whereas almost all
    the features with smaller Error Contribution are relative to T-22 (22 days before
    the observation moment).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，我们可以观察到所有高误差贡献的特征与T-1（观察时刻前1天）相关，而几乎所有低误差贡献的特征与T-22（观察时刻前22天）相关。
- en: This seems to indicate that **the most recent features are prone to overfitting,
    whereas the features more distant in time tend to generalize better**.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎表明**最新的特征容易过拟合，而时间更久远的特征往往更容易泛化**。
- en: Note that, without Error Contribution, we would never have known this insight.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果没有误差贡献，我们永远不会知道这个洞察。
- en: RFE Using Error Contribution
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用误差贡献的RFE
- en: Traditional Recursive Feature Elimination (RFE) methods are based on the removal
    of unimportant features. This is equivalent to removing the features with a small
    Prediction Contribution first.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 传统递归特征消除（RFE）方法基于移除不重要的特征。这相当于首先移除预测贡献小的特征。
- en: However, based on what we said in the previous paragraph, it would make more
    sense to remove the features with the highest Error Contribution first.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，根据我们在上一段所说的，首先移除误差贡献最大的特征会更有意义。
- en: 'To check whether our intuition is verified, let’s compare the two approaches:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们的直觉，让我们比较这两种方法：
- en: '**Traditional RFE: removing useless features first** (lowest Prediction Contribution).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**传统RFE：首先移除无用特征**（预测贡献最低）。'
- en: '**Our RFE: removing harmful features** **first** (highest Error Contribution).'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我们的RFE：首先移除有害特征**（误差贡献最高）。'
- en: 'Let’s see the results on the validation set:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看验证集上的结果：
- en: '![](../Images/00f4e577bc1262cb522917527047d9bf.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00f4e577bc1262cb522917527047d9bf.png)'
- en: Mean Absolute Error of the two strategies on the validation set. [Image by Author]
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 两种策略在验证集上的平均绝对误差。[图片来源]
- en: 'The best iteration for each method has been circled: it’s the model with 19
    features for the traditional RFE (blue line) and the model with 17 features for
    our RFE (orange line).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 每种方法的最佳迭代已被圈出：传统RFE（蓝线）的模型有19个特征，而我们的RFE（橙线）的模型有17个特征。
- en: 'In general, it seems that our method works well: removing the feature with
    the highest Error Contribution leads to a consistently smaller MAE compared to
    removing the feature with the highest Prediction Contribution.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，我们的方法似乎效果良好：移除误差贡献最大的特征比移除预测贡献最大的特征会导致一致较小的MAE。
- en: However, you may think that this works well just because we are overfitting
    the validation set. After all, we are interested in the result that we will obtain
    on the test set.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你可能会认为这只因为我们对验证集进行了过拟合。毕竟，我们关心的是在测试集上获得的结果。
- en: So let’s see the same comparison on the test set.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们看看在测试集上的相同比较。
- en: '![](../Images/99febd1bda43843ed30aa221e2c10f19.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99febd1bda43843ed30aa221e2c10f19.png)'
- en: Mean Absolute Error of the two strategies on the test set. [Image by Author]
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集上两种策略的均绝对误差。[作者提供的图片]
- en: The result is similar to the previous one. Even if there is less distance between
    the two lines, the MAE obtained by removing the highest Error Contributor is clearly
    better than the MAE by obtained removing the lowest Prediction Contributor.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与之前的相似。即使两条线之间的距离较小，但通过移除最高误差贡献者获得的MAE显然优于通过移除最低预测贡献者获得的MAE。
- en: 'Since we selected the models leading to the smallest MAE on the validation
    set, let’s see their outcome on the test set:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们选择了在验证集上导致最小MAE的模型，让我们看看它们在测试集上的结果：
- en: 'RFE-Prediction Contribution (19 features). MAE on test set: 2.04.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RFE-预测贡献（19个特征）。测试集上的MAE：2.04。
- en: 'RFE-Error Contribution (17 features). MAE on test set: 1.94.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RFE-误差贡献（17个特征）。测试集上的MAE：1.94。
- en: So the best MAE using our method is 5% better compared to traditional RFE!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用我们的方法的最佳MAE比传统RFE提高了5%！
- en: Conclusions
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The concept of feature importance plays a fundamental role in machine learning.
    However, the notion of “importance” is often mistaken for “goodness”.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性概念在机器学习中起着基础性的作用。然而，“重要性”这一概念常常被误解为“优越性”。
- en: 'In order to distinguish between these two aspects we have introduced two concepts:
    Prediction Contribution and Error Contribution. Both concepts are based on the
    SHAP values of the validation dataset, and in the article we have seen the Python
    code to compute them.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了区分这两个方面，我们引入了两个概念：预测贡献和误差贡献。这两个概念都基于验证数据集的SHAP值，文章中我们展示了计算这些值的Python代码。
- en: We have also tried them on a real financial dataset (in which the task is predicting
    the price of Gold) and proved that Recursive Feature Elimination based on Error
    Contribution leads to a 5% better Mean Absolute Error compared to traditional
    RFE based on Prediction Contribution.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在一个真实的金融数据集上进行了尝试（该任务是预测黄金价格），并证明基于误差贡献的递归特征消除方法相比于基于预测贡献的传统RFE，能使均绝对误差提高5%。
- en: '*All the code used for this article can be found in* [*this notebook*](https://github.com/smazzanti/tds_features_important_doesnt_mean_good/blob/main/regression.ipynb)*.*'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有用于本文的代码可以在* [*这个笔记本*](https://github.com/smazzanti/tds_features_important_doesnt_mean_good/blob/main/regression.ipynb)*找到。*'
- en: '*Thank you for reading!*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢阅读！*'
- en: '*If you find my work useful, you can subscribe to* [***get an email every time
    that I publish a new article***](https://medium.com/@mazzanti.sam/subscribe) *(usually
    once a month).*'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你觉得我的工作有用，你可以* [***每次我发布新文章时收到邮件***](https://medium.com/@mazzanti.sam/subscribe)
    *(通常是每月一次)。*'
- en: '*If you want to support my work, you can* [***buy me a coffee***](https://ko-fi.com/samuelemazzanti)*.*'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你想支持我的工作，你可以* [***请我喝咖啡***](https://ko-fi.com/samuelemazzanti)*。*'
- en: '*If you’d like,* [***add me on Linkedin***](https://www.linkedin.com/in/samuelemazzanti/)*!*'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你愿意，* [***可以在Linkedin加我***](https://www.linkedin.com/in/samuelemazzanti/)*！*'
