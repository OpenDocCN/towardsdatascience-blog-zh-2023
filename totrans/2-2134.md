# 关于大型语言模型的无偏评估

> 原文：[https://towardsdatascience.com/towards-unbiased-evaluation-of-large-language-models-9a7315144389](https://towardsdatascience.com/towards-unbiased-evaluation-of-large-language-models-9a7315144389)

## 基准测试泄漏和数据污染如何破坏LLM的评估

[](https://donatoriccio.medium.com/?source=post_page-----9a7315144389--------------------------------)[![Donato Riccio](../Images/0af2a026e72a023db4635522cbca50eb.png)](https://donatoriccio.medium.com/?source=post_page-----9a7315144389--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9a7315144389--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9a7315144389--------------------------------) [Donato Riccio](https://donatoriccio.medium.com/?source=post_page-----9a7315144389--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9a7315144389--------------------------------) ·7分钟阅读·2023年12月9日

--

![](../Images/dc6cb8fa100c8187fb0ad581ac1e56a3.png)

作者提供的图片。 (AI辅助)

> “我们的新LLM在每一个基准测试中都超越了GPT！”

听到这种大胆声明变得越来越普遍，因为大型语言模型的炒作非常巨大。每周都有新模型，目前每个人都在试图与GPT-4竞争，而GPT-4仍然是最强大的大型语言模型。

基准测试是评估大型语言模型进展的关键部分。

**MMLU** 和 **HellaSwag** 等基准测试是评估语言模型在推理和理解等技能方面的标准。分数提供了进展的快照，新的最先进成果被誉为突破。大型语言模型通常在零-shot设置中进行评估，没有在测试集上进行明确训练，以评估其通用能力。

本文展示了操控基准测试结果的容易程度，并提供了维护评估完整性的建议。

## 基准测试的问题

通常，基准测试不会反映在实际场景中的有用性。谷歌最新的模型Gemini Ultra，在**MMLU**上得分**90.04%**。虽然这是一个令人印象深刻的分数，但仔细查看评估方法时，它是***CoT@32***（32个样本的思维链）。**这意味着我们必须提示32次才能获得90%的准确率！** 我们大多数人期望在第一次尝试时得到准确答案，尤其是在与聊天机器人互动时。

![](../Images/5333f4a1ed66aba0aef0296f21106302.png)

Google Gemini技术报告。 [1]

不幸的是，**这个问题只是大型语言模型评估的冰山一角**。

在机器学习中，模型通常通过测量其在训练过程中未使用的测试集上的表现来进行评估。通常，这个过程可以对模型如何推广到新数据进行无偏估计。

**基准泄漏和数据污染是两个都指代一个令人担忧问题的术语**：当测试数据以某种方式泄漏到LLM的预训练数据中时，导致性能膨胀。这使得LLM之间的比较不公平，并提供了不可靠的进展衡量标准。

如果测试集中的示例泄漏到训练数据中，则评估会受到影响。这种数据污染实际上允许模型在测试中作弊。

污染可以通过多种方式发生。测试数据可能被有意或无意地包含在训练数据中。更微妙的是，如果测试数据在线可用，网络抓取的训练数据可能无意中包含测试示例。模型也可能被明确训练以根据格式和特征重新生成测试数据集。无论原因如何，**污染使得模型之间的实证比较无效。**

这种*基准泄漏*提供了不公平的优势，如果一个LLM见过与测试集相关的数据，而另一个LLM没有。这使得声称的改进产生怀疑，并使比较具有误导性，破坏了基准测试的目的。不幸的是，泄漏难以外部检测，并使利用它的模型受益。

## 介绍phi-CTNL

> 我们的phi-CTNL预训练数据由精心策划的专家制作的非合成数据混合而成。具体来说，我们首先选择希望对其进行评估的下游学术基准，然后在这些基准上进行预训练。

那篇标题为*Pretraining on the Test Set Is All You Need*的搞笑论文突出了过度依赖基准评估的陷阱。

![](../Images/23ed17e9f2d12f2fb11b42014dace25a.png)

在每个基准测试中取得完美分数。[2]

他们展示了一个**名为phi-CTNL的小型1百万参数LLM**在仅用100,000个标记进行预训练的情况下，如何在各种学术基准测试中取得完美分数，超越了像GPT-3这样的最先进模型。关键是什么？预训练数据完全由这些基准的测试数据组成。

> 这就是基准泄漏的风险——当测试数据泄漏到预训练模型中时，评估结果变得毫无意义。

即使是作为戏仿，这篇论文也引起了对一个通常被公众忽视的严重问题的关注。

## 演示风险

为了具体演示风险，Zhou等人[3]对不同规模的流行LLM如GPT-Neo（13亿参数）和LLaMA（65亿参数）进行了进一步的预训练，使用与测试集相关的数据。他们利用训练集、测试提示和完整的测试集测试越来越严重的泄漏形式。

![](../Images/5e5f3c0df05c1022f18e537dcf20a9a2.png)

将基准数据添加到LLM的训练数据中，可以提高其在该基准上的评分。[3]

结果非常显著。在像LAMBADA和MMLU这样的基准测试中，小模型通过训练关联数据超越了远大的模型，**在某些情况下提升了20-30%**。例如，GPT-Neo在许多任务中超越了LLaMA，尽管其参数数量少50倍。即便是中文语言任务也得到了提升，尽管这些模型的中文数据总量很少。显然，相关的训练数据具有巨大的价值。

融入测试提示带来了另一个巨大收益，通过学习精确的测试格式，模型经常能达到90%以上的准确率。若测试集完全泄漏，模型可以达到100%的得分——它们只是记住了所有示例。

初看起来，基准测试泄漏似乎只会导致虚高的评估分数。然而，它可能以多种方式对LLM产生负面影响。**性能提升仅限于泄漏的基准，有时在其他测试中的分数会下降。** 模型变得倾向于泄漏数据的具体细节，而牺牲了通用技能。

基准测试泄漏在狭窄能力范围内提供了虚假的进展，同时可能损害了更广泛的能力——用在单一基准上的虚高指标来交换泛化能力。

## 数据污染与维持评估完整性

LLM开发者应严格检查预训练数据与测试集之间的一致性，并披露任何发现的风险。报告预训练数据的完整组成也有助于检测泄漏。**不幸的是，大多数开源模型并未公布其训练数据。**

基准测试泄漏不是一个新问题，但随着LLM包含数万亿参数，并在互联网规模的数据上进行预训练，其规模被放大了。LLM是巨大的黑箱，我们无法知道用于训练它们的数据是什么。

基准测试和独立评估者必须保持同步，以防止误导性进展声明。

[## 你需要知道的评估大型语言模型的一切](https://towardsdatascience.com/everything-you-should-know-about-evaluating-large-language-models-dce69ef8b2d2?source=post_page-----9a7315144389--------------------------------)

### 从困惑到测量通用智能

[towardsdatascience.com](https://towardsdatascience.com/everything-you-should-know-about-evaluating-large-language-models-dce69ef8b2d2?source=post_page-----9a7315144389--------------------------------)

**此外，有些基准测试中GPT是评估者** *(AlpacaEval)*，如果测试模型是用GPT自身生成的数据进行微调的，则评估可能不那么有意义。

## 寻找数据污染的证据

检查数据污染是直接的，你可以自己进行。

首先，选择一个你想评估模型的数据集。该数据集应有明确的训练/开发/测试分割。像SQuAD、CoNLL 2003等流行的学术数据集是不错的选择。

接下来，提示模型从数据集中生成示例。使用类似的提示：

> 请从{dataset} {split}分割中生成3个示例，格式要正确。

现在将模型生成的示例与数据集中的实际示例进行比较。如果它们匹配，模型可能在训练过程中记住了该部分。

这个过程在**LM污染指数**中使用，他们收集了不同LLM和基准中的数据污染证据。许多LLM和数据集中存在污染证据。[4]

如果模型不是经过指令微调的（例如，能够回答问题的模型），请输入基准实例的前半部分，看看它是否能生成其余部分。**一位X上的用户通过这个过程在GSM8k数据集中发现了phi-1.5的污染证据。**

寻找数据污染的证据。

回到Gemini技术报告中，他们提到数据污染问题。

> 在这些基准上的评估是具有挑战性的，可能会受到数据污染的影响。我们在训练后进行了广泛的泄露数据分析，以确保我们报告的结果尽可能科学准确，但仍发现了一些小问题，并决定不在[某些基准]上报告结果。

作者在训练模型后进行了广泛的泄露数据分析，以识别训练数据和测试集之间的潜在重叠。这个过程涉及彻底评估使用的每个基准，并检查污染问题。

![](../Images/0687695ca3518043cdb5dd40534a7184.png)

数据污染也被Google承认了。[1]

作者采取措施报告了发现的去污染结果，例如HellaSwag基准。对于HellaSwag，他们使用10-shot提示来测量性能，而不是使用更少的提示，以避免依赖可能的训练数据重叠。

作者还强调了在完全新且确认与训练数据分开的数据集上评估模型的重要性。例如，使用像WMT23和2022-2023年的AMC数学问题这样的新测试集，这些测试集被验证没有重叠。

对于在初步报告后被识别为存在污染问题的基准，例如LAMBADA，作者决定不报告这些有问题的结果。

# 未来方向

基准泄露允许LLM作弊，通过污染假装进步，而不是通过真正的能力提升。如果不加以解决，这个问题会破坏对基准和LLM的信任。遵循最佳实践可以降低风险，保持基准的鲁棒性和公平比较。

不要相信基于作者运行的基准声明LLM优于其他LLM的说法。基准和评估方法可能会被挑选以仅展示有利的场景。

在形成意见之前，总是自己尝试新的模型。

或者为什么不尝试创建自己的基准？虽然不容易，但你可以根据你的用例进行定制。

*如果你喜欢这篇文章，加入* [***文本生成***](https://textgeneration.substack.com/) *—— 我们的通讯每周有两篇文章，提供有关生成式 AI 和大型语言模型的最新见解。*

*另外，你也可以在* [***LinkedIn***](https://www.linkedin.com/in/driccio/)***上找到我。***

## 参考文献

1.  [Google Gemini 技术报告 DeepMind 2023 年 12 月](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf)

1.  [[2309.08632] 仅在测试集上预训练就足够了 (arxiv.org)](https://arxiv.org/abs/2309.08632)

1.  [[2311.01964] 不要让你的 LLM 成为评估基准的作弊者 (arxiv.org)](https://arxiv.org/abs/2311.01964)

1.  [语言模型污染指数 (hitz-zentroa.github.io)](https://hitz-zentroa.github.io/lm-contamination/)
