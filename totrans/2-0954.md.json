["```py\nfrom scipy.stats import norm\n\ndef plot_mixture(mean1, std1, mean2, std2, w1, w2):\n    # Generate points for the x-axis\n    x = np.linspace(-5, 10, 1000)\n\n    # Calculate the individual nomral distributions\n    normal1 = norm.pdf(x, mean1, std1)\n    normal2 = norm.pdf(x, mean2, std2)\n\n    # Calculate the mixture\n    mixture = w1 * normal1 + w2 * normal2\n\n    # Plot the results\n    plt.plot(x, normal1, label='Normal distribution 1', linestyle='--')\n    plt.plot(x, normal2, label='Normal distribution 2', linestyle='--')\n    plt.plot(x, mixture, label='Mixture model', color='black')\n    plt.xlabel('$x$')\n    plt.ylabel('$p(x)$')\n    plt.legend()\n```", "```py\n# Parameters for the two univariate normal distributions\nmean1, std1 = -1, 1\nmean2, std2 = 4, 1.5\nw1, w2 = 0.7, 0.3\n\nplot_mixture(mean1, std1, mean2, std2, w1, w2)\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import norm\n\nnp.random.seed(0)  # for reproducibility\n```", "```py\ndef init_params(x):    \n    \"\"\"Initialize the parameters for the GMM\n    \"\"\"    \n    # Randomly initialize the means to points from the dataset\n    mean1, mean2 = np.random.choice(x, 2, replace=False)\n\n    # Initialize the standard deviations to 1\n    std1, std2 = 1, 1\n\n    # Initialize the mixing weights uniformly\n    w1, w2 = 0.5, 0.5\n\n    return mean1, mean2, std1, std2, w1, w2\n```", "```py\ndef e_step(x, mean1, std1, mean2, std2, w1, w2):\n    \"\"\"E-Step: Compute the responsibilities\n    \"\"\"    \n    # Compute the densities of the points under the two normal distributions  \n    prob1 = norm(mean1, std1).pdf(x) * w1\n    prob2 = norm(mean2, std2).pdf(x) * w2\n\n    # Normalize the probabilities\n    prob_sum = prob1 + prob2 \n    prob1 /= prob_sum\n    prob2 /= prob_sum\n\n    return prob1, prob2\n```", "```py\ndef m_step(x, prob1, prob2):\n    \"\"\"M-Step: Update the GMM parameters\n    \"\"\"    \n    # Update means\n    mean1 = np.dot(prob1, x) / np.sum(prob1)\n    mean2 = np.dot(prob2, x) / np.sum(prob2)\n\n    # Update standard deviations\n    std1 = np.sqrt(np.dot(prob1, (x - mean1)**2) / np.sum(prob1))\n    std2 = np.sqrt(np.dot(prob2, (x - mean2)**2) / np.sum(prob2))\n\n    # Update mixing weights\n    w1 = np.sum(prob1) / len(x)\n    w2 = 1 - w1\n\n    return mean1, std1, mean2, std2, w1, w2\n```", "```py\ndef gmm_em(x, max_iter=100):\n    \"\"\"Gaussian mixture model estimation using Expectation-Maximization\n    \"\"\"    \n    mean1, mean2, std1, std2, w1, w2 = init_params(x)\n\n    for i in range(max_iter):\n        print(f'Iteration {i}: μ1 = {mean1:.3f}, σ1 = {std1:.3f}, μ2 = {mean2:.3f}, σ2 = {std2:.3f}, ' \n              f'w1 = {w1:.3f}, w2 = {w2:.3f}')\n\n        prob1, prob2 = e_step(x, mean1, std1, mean2, std2, w1, w2)\n        mean1, std1, mean2, std2, w1, w2 = m_step(x, prob1, prob2)     \n\n    return mean1, std1, mean2, std2, w1, w2\n```", "```py\ndef sample_data(mean1, std1, mean2, std2, w1, w2, n_samples):    \n    \"\"\"Sample random data from a mixture of two Gaussian distribution.\n    \"\"\"\n    x = np.zeros(n_samples)\n    for i in range(n_samples):\n        # Choose distribution based on mixing weights\n        if np.random.rand() < w1:\n            # Sample from the first distribution\n            x[i] = np.random.normal(mean1, std1)\n        else:\n            # Sample from the second distribution\n            x[i] = np.random.normal(mean2, std2)\n\n    return x\n```", "```py\n# Parameters for the two univariate normal distributions\nmean1, std1 = -1, 1\nmean2, std2 = 4, 1.5\nw1, w2 = 0.7, 0.3\n\nx = sample_data(mean1, std1, mean2, std2, w1, w2, n_samples=1000)\n```", "```py\nfinal_dist_params = gmm_em(x, max_iter=30)\n```", "```py\nIteration 0: μ1 = -1.311, σ1 = 1.000, μ2 = 0.239, σ2 = 1.000, w1 = 0.500, w2 = 0.500\nIteration 1: μ1 = -1.442, σ1 = 0.898, μ2 = 2.232, σ2 = 2.521, w1 = 0.427, w2 = 0.573\nIteration 2: μ1 = -1.306, σ1 = 0.837, μ2 = 2.410, σ2 = 2.577, w1 = 0.470, w2 = 0.530\nIteration 3: μ1 = -1.254, σ1 = 0.835, μ2 = 2.572, σ2 = 2.559, w1 = 0.499, w2 = 0.501\n...\nIteration 27: μ1 = -1.031, σ1 = 1.033, μ2 = 4.180, σ2 = 1.371, w1 = 0.675, w2 = 0.325\nIteration 28: μ1 = -1.031, σ1 = 1.033, μ2 = 4.181, σ2 = 1.370, w1 = 0.675, w2 = 0.325\nIteration 29: μ1 = -1.031, σ1 = 1.033, μ2 = 4.181, σ2 = 1.370, w1 = 0.675, w2 = 0.325\n```", "```py\ndef plot_mixture(x, mean1, std1, mean2, std2, w1, w2):\n    # Plot an histogram of the input data\n    sns.histplot(x, bins=20, kde=True, stat='density', linewidth=0.5, color='gray')\n\n    # Generate points for the x-axis\n    x_ = np.linspace(-5, 10, 1000)\n\n    # Calculate the individual nomral distributions\n    normal1 = norm.pdf(x_, mean1, std1)\n    normal2 = norm.pdf(x_, mean2, std2)\n\n    # Calculate the mixture\n    mixture = w1 * normal1 + w2 * normal2\n\n    # Plot the results\n    plt.plot(x_, normal1, label='Normal distribution 1', linestyle='--')\n    plt.plot(x_, normal2, label='Normal distribution 2', linestyle='--')\n    plt.plot(x_, mixture, label='Mixture model', color='black')\n    plt.xlabel('$x$')\n    plt.ylabel('$p(x)$')\n    plt.legend()\n```", "```py\nplot_mixture(x, *final_dist_params)\n```", "```py\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=500, centers=[(0, 0), (4, 4)], random_state=0)\n\n# Apply a linear transformation to make the blobs elliptical\ntransformation = [[0.6, -0.6], [-0.2, 0.8]]\nX = np.dot(X, transformation) \n\n# Add another spherical blob\nX2, y2 = make_blobs(n_samples=150, centers=[(-2, -2)], cluster_std=0.5, random_state=0)\nX = np.vstack((X, X2))\n```", "```py\ndef plot_data(X):\n    sns.scatterplot(x=X[:, 0], y=X[:, 1], edgecolor='k', legend=False)\n    plt.xlabel('$x_1$')\n    plt.ylabel('$x_2$')\n\nplot_data(X)\n```", "```py\nfrom sklearn.mixture import GaussianMixture\n\ngmm = GaussianMixture(n_components=3)\nlabels = gmm.fit_predict(X)\n```", "```py\nprint(gmm.n_iter_)\n```", "```py\n2\n```", "```py\nprint('Weights:', gmm.weights_)\nprint('Means:\\n', gmm.means_)\nprint('Covariances:\\n', gmm.covariances_)\n```", "```py\nWeights: [0.23077331 0.38468283 0.38454386]\nMeans:\n [[-2.01578902 -1.95662033]\n [-0.03230299  0.03527593]\n [ 1.56421574  0.80307925]]\nCovariances:\n [[[ 0.254315   -0.01588303]\n  [-0.01588303  0.24474151]]\n\n [[ 0.41202765 -0.53078979]\n  [-0.53078979  0.99966631]]\n\n [[ 0.35577946 -0.48222654]\n  [-0.48222654  0.98318187]]]\n```", "```py\ndef plot_clusters(X, labels):    \n    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=labels, palette='tab10', edgecolor='k', legend=False)\n    plt.xlabel('$x_1$')\n    plt.ylabel('$x_2$')\n\nplot_clusters(X, labels)\n```", "```py\nprob = gmm.predict_proba(X)\n```", "```py\nprint('x =', X[0])\nprint('prob =', prob[0])\n```", "```py\nx = [ 2.41692591 -0.07769481]\nprob = [3.11052582e-21 8.85973054e-10 9.99999999e-01]\n```", "```py\nsizes = prob.max(axis=1)\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=labels, size=sizes, palette='tab10', edgecolor='k', legend=False)\nplt.xlabel('$x_1$')\nplt.ylabel('$x_2$')\nplt.savefig('figures/elliptical_blobs_gmm_prob.pdf')\n```", "```py\nprint(f'AIC = {gmm.aic(X):.3f}')\nprint(f'BIC = {gmm.bic(X):.3f}')\n```", "```py\nAIC = 4061.318\nBIC = 4110.565\n```"]