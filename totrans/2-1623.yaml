- en: 'PaLM: Efficiently Training Massive Language Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PaLM：高效训练大型语言模型
- en: 原文：[https://towardsdatascience.com/palm-efficiently-training-massive-language-models-b82d6cc1582](https://towardsdatascience.com/palm-efficiently-training-massive-language-models-b82d6cc1582)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/palm-efficiently-training-massive-language-models-b82d6cc1582](https://towardsdatascience.com/palm-efficiently-training-massive-language-models-b82d6cc1582)
- en: Unprecedented size, efficiency, and performance for LLMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 史无前例的LLM规模、效率和性能
- en: '[](https://wolfecameron.medium.com/?source=post_page-----b82d6cc1582--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----b82d6cc1582--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b82d6cc1582--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b82d6cc1582--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----b82d6cc1582--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----b82d6cc1582--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----b82d6cc1582--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b82d6cc1582--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b82d6cc1582--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----b82d6cc1582--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b82d6cc1582--------------------------------)
    ·17 min read·Jun 19, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----b82d6cc1582--------------------------------)
    ·阅读时间17分钟·2023年6月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/451e0862c498cfad7a6e861d29efbd84.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/451e0862c498cfad7a6e861d29efbd84.png)'
- en: (Photo by [Corey Agopian](https://unsplash.com/@corey_lyfe?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/images/nature/palm-tree?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: (照片由[Corey Agopian](https://unsplash.com/@corey_lyfe?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)提供，来自[Unsplash](https://unsplash.com/images/nature/palm-tree?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
- en: In recent years, large, deep neural networks have become the definitive architecture
    of choice for solving most language understanding and generation tasks. Initially,
    models were proposed, such as [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)
    [2] and T5 [3], that used a [two-part training methodology](https://cameronrwolfe.substack.com/i/76273144/training-bert)
    of pre-training (with [self-supervised “infilling” objectives](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning))
    over a large corpus of text, then fine-tuning on a target dataset; see below.
    Despite the utility of these techniques, recent work on large language models
    (LLMs) has shown that large, autoregressive (decoder-only) transformer models
    are incredibly capable at few-shot learning, achieving impressive performance
    with minimal adaptation to downstream tasks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型深度神经网络已成为解决大多数语言理解和生成任务的**终极架构**。最初，提出了诸如[BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)
    [2]和T5 [3]的模型，这些模型使用了[两阶段训练方法](https://cameronrwolfe.substack.com/i/76273144/training-bert)，即在大规模文本语料库上进行预训练（使用[自监督“填充”目标](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning)），然后在目标数据集上进行微调；见下文。尽管这些技术很有用，但对大型语言模型（LLMs）的最新研究表明，大型自回归（仅解码器）变换器模型在少样本学习中表现出色，能够以最小的适应性在下游任务中取得令人印象深刻的性能。
- en: '![](../Images/dc4d51598c9de0703ecde8cfa28d78cd.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc4d51598c9de0703ecde8cfa28d78cd.png)'
- en: (from [4])
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: (来自[4])
- en: The few-shot learning capabilities of LLMs were first demonstrated by [GPT-3](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt)
    [4], a 175 billion parameter LLM. To perform few-shot prediction, the model is
    pre-trained (using a basic [language modeling objective](https://cameronrwolfe.substack.com/i/85568430/language-modeling))
    over a massive corpus of text, then provided task descriptions and a handful of
    examples of how a task should be solved; see above. Further analysis of LLMs indicated
    that model performance improves smoothly with scale (according to a [power law](https://cameronrwolfe.substack.com/i/88082618/power-laws))
    [5, 6]. As such, various LLMs were proposed following GPT-3 that attempt to “scale
    up” the model and training, oftentimes achieving improved results via a combination
    of larger models and more/better pre-training data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的少样本学习能力最初由[GPT-3](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt)
    [4]展示，这是一种拥有1750亿参数的LLM。为了进行少样本预测，该模型在一个庞大的文本语料库上进行预训练（使用基本的[语言建模目标](https://cameronrwolfe.substack.com/i/85568430/language-modeling)），然后提供任务描述和若干个任务解决示例；见上文。对LLM的进一步分析表明，模型性能随着规模的增加而平滑提升（根据[幂律](https://cameronrwolfe.substack.com/i/88082618/power-laws)）[5,
    6]。因此，在GPT-3之后提出了各种LLM，试图通过“扩大”模型和训练来提高结果，通常通过更大的模型和更多/更好的预训练数据组合来实现改进。
- en: Training larger LLMs is beneficial but difficult to do efficiently. Typically,
    we distribute training across many machines, each with several accelerators (i.e.,
    GPUs or [TPUs](https://cloud.google.com/tpu/docs/tpus)). This has been done successfully
    before (e.g., MT-NLG trains a 530 billion parameter LLM across a system with 2240
    A100 GPUs), but the results were not that impressive. The model, although large,
    was not trained over enough data. However, given a higher training throughput,
    we could (in theory) pre-train such large models more extensively on larger datasets,
    yielding much better results.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 训练更大的LLM是有益的，但要高效地做到这一点却非常困难。通常，我们将训练分布到许多机器上，每台机器配备多个加速器（即GPU或[TPUs](https://cloud.google.com/tpu/docs/tpus)）。这一方法以前已成功实施（例如，MT-NLG在一个拥有2240个A100
    GPU的系统上训练了一个5300亿参数的LLM），但结果并不特别令人印象深刻。尽管模型很大，但训练的数据量不足。然而，鉴于更高的训练吞吐量，我们可以（理论上）在更大的数据集上更广泛地预训练这样的巨大模型，从而获得更好的结果。
- en: In this overview, we will explore the Pathways Language Model (PaLM), a 540
    billion parameter LLM trained using Google’s [Pathways](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/)
    framework. By eliminating [pipeline parallelism](https://cameronrwolfe.substack.com/i/88082618/other-useful-details),
    this architecture achieves impressive training throughput, allowing PaLM to be
    pre-trained over a more extensive dataset. The few-shot performance of the resulting
    model is state-of-the-art. Plus, PaLM is somewhat capable of solving difficult
    reasoning tasks. Put simply, PaLM is a clear reminder that LLM performance has
    not yet reached a plateau with respect to scale. Given a sufficiently efficient
    training infrastructure that permits pre-training larger models over more data,
    we continue to see improvements in performance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本概述中，我们将探讨**Pathways语言模型（PaLM）**，这是一种拥有5400亿参数的LLM，使用Google的[Pathways](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/)框架进行训练。通过消除[管道并行性](https://cameronrwolfe.substack.com/i/88082618/other-useful-details)，这一架构实现了令人印象深刻的训练吞吐量，使得PaLM能够在更广泛的数据集上进行预训练。最终模型的少样本性能处于行业领先水平。此外，PaLM在解决复杂推理任务方面也有一定能力。简单来说，PaLM清楚地提醒我们，LLM的性能在规模上尚未达到瓶颈。只要有足够高效的训练基础设施来支持在更多数据上预训练更大的模型，我们仍将继续看到性能的提升。
- en: '![](../Images/19db6121b1dfffc050cb58ad62d2b9a7.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19db6121b1dfffc050cb58ad62d2b9a7.png)'
- en: (from [1, 16])
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于 [1, 16]）
- en: Background
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: 'We have explored the topic of language modeling extensively in this newsletter
    and overviewed several notable (large) language models in prior posts:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本新闻通讯中广泛探讨了语言建模主题，并在之前的帖子中回顾了几种显著的（大规模）语言模型：
- en: GPT and GPT-2 [[link](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2)]
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT和GPT-2 [[链接](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2)]
- en: Scaling Laws and GPT-3 [[link](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt#%C2%A7other-useful-details)]
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规模定律和GPT-3 [[链接](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt#%C2%A7other-useful-details)]
- en: Modern LLMs [[link](https://cameronrwolfe.substack.com/p/modern-llms-mt-nlg-chinchilla-gopher)]
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代LLMs [[链接](https://cameronrwolfe.substack.com/p/modern-llms-mt-nlg-chinchilla-gopher)]
- en: Specialized LLMs [[link](https://cameronrwolfe.substack.com/p/specialized-llms-chatgpt-lamda-galactica)]
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专业化LLM [[link](https://cameronrwolfe.substack.com/p/specialized-llms-chatgpt-lamda-galactica)]
- en: Nonetheless, we will briefly go over prior work on LLMs here to provide some
    important context for understanding PaLM.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们将在这里简要回顾有关LLM的先前工作，以提供理解PaLM的重要背景。
- en: Language Modeling Recap
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言建模回顾
- en: '![](../Images/ad1a8eef982b5323861c9035b089e7e5.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad1a8eef982b5323861c9035b089e7e5.png)'
- en: Decoder-only transformer architecture
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 仅解码器转换器架构
- en: Modern language models are simply [decoder-only transformer models](https://cameronrwolfe.substack.com/i/85568430/decoder-only-transformers)
    (shown above) that are pre-trained using a self-supervised [language modeling
    objective](https://cameronrwolfe.substack.com/i/85568430/language-modeling) over
    unlabeled text. This objective samples a sequence of text and trains the language
    model to accurately predict the next word/token. After performing extensive pre-training,
    LLMs such as [GPT-3](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt#%C2%A7other-useful-details)
    were found to perform really well in the few-shot learning regime.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现代语言模型仅仅是 [仅解码器转换器模型](https://cameronrwolfe.substack.com/i/85568430/decoder-only-transformers)（如上图所示），这些模型通过自监督
    [语言建模目标](https://cameronrwolfe.substack.com/i/85568430/language-modeling) 在无标签文本上进行预训练。该目标对文本序列进行采样，并训练语言模型准确预测下一个词/标记。在进行广泛的预训练后，LLM如
    [GPT-3](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt#%C2%A7other-useful-details)
    被发现能在少量示例学习模式下表现非常好。
- en: '**why is this useful?** Put simply, the generic, text-to-text format of LLMs
    allows them to easily generalize to solving a variety of tasks with minimal adaptation.
    Instead of fine-tuning models or adding task-specific layers, we can just pre-train
    a single model extensively and solve a variety of tasks with the same model using
    few-shot learning. Despite the fact that pre-training such foundation models is
    incredibly expensive, these approaches hold incredible potential, as a single
    model can be re-purposed for many applications. This process is referred to as
    in-context learning; see below.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**这有什么用？** 简而言之，LLM的通用文本到文本格式使其能够轻松地推广到解决各种任务，只需少量调整。我们可以仅仅通过广泛地预训练一个单一模型，并利用少量示例学习来解决各种任务，而不必对模型进行微调或添加任务特定的层。尽管预训练这样的基础模型是非常昂贵的，但这些方法具有巨大的潜力，因为一个模型可以被重新用于许多应用。这一过程称为上下文学习；见下文。'
- en: '![](../Images/f31b84c2d4ba5f0101819c9583065433.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f31b84c2d4ba5f0101819c9583065433.png)'
- en: (from [4])
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于 [4]）
- en: '**what goes into a good LLM?** Early work on this topic indicated that language
    model performance should improved smoothly (according to a [power law](https://cameronrwolfe.substack.com/i/88082618/power-laws))
    with model scale (i.e., big models perform better). This finding led to the proposal
    of GPT-3, an LLM of unprecedented scale (175 billion parameters) that achieved
    breakthrough few-shot learning performance. Subsequent work tried to explore [even
    larger LLMs](https://cameronrwolfe.substack.com/i/91134599/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-b-a-large-scale-generative-language-model),
    but these larger models did not lead to further breakthroughs in performance.
    Rather, we eventually discovered that producing high-performing LLMs requires
    a combination of larger models with larger pre-training datasets [6].'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么构成一个好的LLM？** 早期的研究表明，语言模型的性能应该随着模型规模的增加而平滑提高（根据 [幂律](https://cameronrwolfe.substack.com/i/88082618/power-laws)）（即，大模型表现更好）。这一发现导致了GPT-3的提出，这是一个规模空前的LLM（1750亿参数），实现了突破性的少量示例学习性能。后续工作尝试探索
    [更大的LLM](https://cameronrwolfe.substack.com/i/91134599/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-b-a-large-scale-generative-language-model)，但这些更大的模型并没有带来进一步的性能突破。相反，我们最终发现，生产高性能LLM需要将更大的模型与更大的预训练数据集相结合
    [6]。'
- en: '*“The amount of training data that is projected to be needed is far beyond
    what is currently used to train large models, and underscores the importance of
    dataset collection in addition to engineering improvements that allow for model
    scale.” —* from [6]'
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“预计所需的训练数据量远超目前用于训练大型模型的数据量，这突显了数据集收集的重要性，除了允许模型规模扩展的工程改进。”* — 来源于 [6]'
- en: Architectural Modifications
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构修改
- en: Beyond using an improved training framework, PaLM modifies the underlying, decoder-only
    architecture quite a bit. Most of these changes are adopted from prior work that
    reveals best practices for maximizing LLM training efficiency and performance.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用改进的训练框架，PaLM 还对基础的仅解码器架构进行了相当大的修改。这些更改大多数采纳了先前的工作，这些工作揭示了最大化 LLM 训练效率和性能的最佳实践。
- en: '**SwiGLU activations.** Most LLMs share a similar structure for the [feed-forward
    neural network](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)
    used within each of their layers. Namely, this network performs two feed-forward
    transformations (using no bias and applied individually to each token vector in
    the sequence) with a [Rectified Linear Unit (ReLU) activation](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)
    function in between. However, subsequent work [13] revealed that other choices
    of the activation function may actually be better.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**SwiGLU 激活函数。** 大多数大语言模型（LLMs）在每一层内使用的[前馈神经网络](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)具有类似的结构。也就是说，这个网络进行两次前馈变换（不使用偏置，并单独应用于序列中的每个令牌向量），中间使用[修正线性单元（ReLU）激活函数](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)。然而，后续的工作[13]揭示了其他激活函数的选择可能实际上更好。'
- en: In particular, PaLM uses the SwiGLU activation function, a combination of Swish
    [14] and GLU [15] activations. This activation function is given by the equation
    below.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，PaLM 使用了 SwiGLU 激活函数，它是 Swish [14] 和 GLU [15] 激活函数的组合。这个激活函数由下面的方程给出。
- en: '![](../Images/3674693963fb4d65a4d10c669b2f1e06.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3674693963fb4d65a4d10c669b2f1e06.png)'
- en: (created by author)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: （作者创作）
- en: where we define the Swish activation function as
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 Swish 激活函数定义为
- en: '![](../Images/5bca0924d4c415d014e3f259fec3bb31.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5bca0924d4c415d014e3f259fec3bb31.png)'
- en: (created by author)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: （作者创作）
- en: In other words, SwiGLU is an element-wise product of two [linear transformations](https://mathworld.wolfram.com/LinearTransformation.html)
    of the input, one of which has had a Swish activation applied to it. Although
    this activation function requires three matrix multiplications, recent work has
    found that it provides a performance benefit given a fixed amount of computation.
    Compared to vanilla activations like ReLU, SwiGLU seems to provide a non-negligible
    performance improvement [13].
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，SwiGLU 是对输入的两个[线性变换](https://mathworld.wolfram.com/LinearTransformation.html)的逐元素乘积，其中一个变换应用了
    Swish 激活函数。尽管这个激活函数需要进行三次矩阵乘法，但最近的工作发现，在固定的计算量下，它能带来性能提升。与像 ReLU 这样的普通激活函数相比，SwiGLU
    似乎提供了不可忽视的性能提升 [13]。
- en: '**parallel transformer blocks.** PaLM also uses parallel versions of the [transformer
    block](https://cameronrwolfe.substack.com/i/85568430/decoder-only-transformers),
    rather than the normal (serialized) variant. The difference between these two
    formulations is demonstrated within the illustration below.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**并行变压器块。** PaLM 还使用了[变压器块](https://cameronrwolfe.substack.com/i/85568430/decoder-only-transformers)的并行版本，而不是常规（串行）版本。这两种形式之间的差异在下图中展示。'
- en: '![](../Images/8a0018932a1a6d510c63cda67cec562b.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a0018932a1a6d510c63cda67cec562b.png)'
- en: Parallel vs. serialized transformer blocks (created by author)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 并行与串行变压器块（作者创作）
- en: Given a sufficiently large model, using parallel transformer blocks can speed
    up the training process by 15%. This speedup comes at the cost of slightly degraded
    performance for smaller LLMs (e.g., an 8 billion parameter model), but full-sized
    LLMs tend to perform similarly with parallel blocks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型足够大的情况下，使用并行变压器块可以将训练过程的速度提高 15%。这种加速会以较小 LLM（例如，80 亿参数模型）性能略有下降为代价，但全尺寸
    LLM 使用并行块时往往表现相似。
- en: '![](../Images/922f9455fb787b2955029e12c191c096.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/922f9455fb787b2955029e12c191c096.png)'
- en: (from [16])
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: （来源 [16]）
- en: '**rotary positional embeddings.** Instead of [absolute](https://cameronrwolfe.substack.com/i/76273144/berts-architecture)
    or [relative positional embeddings](https://jaketae.github.io/study/relative-positional-encoding/),
    PaLM utilizes rotary positional embeddings (RoPE), as proposed in [16]. RoPE embeddings
    incorporate both absolute and relative positioning by:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**旋转位置嵌入。** PaLM 使用了旋转位置嵌入（RoPE），而不是[绝对位置嵌入](https://cameronrwolfe.substack.com/i/76273144/berts-architecture)或[相对位置嵌入](https://jaketae.github.io/study/relative-positional-encoding/)，如[16]中所提议。RoPE
    嵌入通过以下方式结合了绝对和相对位置：'
- en: Encoding the absolute position with a rotation matrix
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用旋转矩阵编码绝对位置
- en: Incorporating relative position directly into self-attention
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将相对位置直接纳入自注意力机制
- en: Intuitively, RoPE finds a middle ground between absolute and relative positional
    embeddings. Illustrated in the figure above, RoPE consistently outperforms alternative
    embedding strategies. Plus, it is implemented and [easily accessible](https://huggingface.co/docs/transformers/model_doc/roformer)
    in common libraries such as HuggingFace.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，RoPE 找到了绝对和相对位置嵌入之间的折中。上图所示，RoPE 始终优于其他嵌入策略。而且，它在HuggingFace等常见库中[易于访问](https://huggingface.co/docs/transformers/model_doc/roformer)。
- en: '**multi-query attention.** Finally, PaLM replaces the typical, multi-headed
    self-attention mechanism with an alternative structure called multi-query attention.
    Multi-query attention just shares key and value vectors (highlighted in red below)
    between each of the attention heads, instead of performing a separate projection
    for each head. This change does not make training any faster, but it does significantly
    improve the [auto-regressive decoding](https://cameronrwolfe.substack.com/i/85568430/decoder-only-transformers)
    (i.e., used to perform inference or generation) efficiency of LLMs.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**多查询注意力。** 最后，PaLM 用一种称为多查询注意力的替代结构替换了典型的多头自注意力机制。多查询注意力在每个注意力头之间共享键和值向量（下面用红色突出显示），而不是为每个头执行单独的投影。这一变化并没有使训练速度更快，但确实显著提高了LLMs的[自回归解码](https://cameronrwolfe.substack.com/i/85568430/decoder-only-transformers)（即用于执行推理或生成）的效率。'
- en: '![](../Images/885765124ac35d0fb1b32e2eea51c7f0.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/885765124ac35d0fb1b32e2eea51c7f0.png)'
- en: Multi-query attention shares key and value projections between attention heads
    (from [17])
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 多查询注意力在注意力头之间共享键和值投影（来源 [17]）
- en: other useful concepts
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他有用的概念
- en: Foundation models and zero/few-shot learning [[link](https://cameronrwolfe.substack.com/i/85568430/creating-foundation-models)]
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础模型和零/少样本学习 [[link](https://cameronrwolfe.substack.com/i/85568430/creating-foundation-models)]
- en: LLM alignment [[link](https://cameronrwolfe.substack.com/i/93578656/where-do-generic-llms-fall-short)]
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 对齐 [[link](https://cameronrwolfe.substack.com/i/93578656/where-do-generic-llms-fall-short)]
- en: Adaptation strategies for LLMs [[link](https://cameronrwolfe.substack.com/i/93578656/refining-llm-behavior)]
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM的适应策略 [[link](https://cameronrwolfe.substack.com/i/93578656/refining-llm-behavior)]
- en: A brief progression of LLMs [[link](https://cameronrwolfe.substack.com/i/93578656/what-are-language-models)]
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM的简要发展 [[link](https://cameronrwolfe.substack.com/i/93578656/what-are-language-models)]
- en: '[PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311)
    [1]'
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[PaLM: 使用Pathways扩展语言建模](https://arxiv.org/abs/2204.02311) [1]'
- en: Now, we will overview PaLM, a 540 billion parameter dense language model that
    is efficiently trained using the [Pathways framework](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/).
    PaLM is one of the largest dense LLMs that has been trained to date, and its efficient
    training strategy allows its pre-training process to be performed over a large
    dataset (>700 billion tokens). This combination of a massive language model with
    an extensive pre-training corpus leads to some interesting results that we will
    explore within this section.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将概述PaLM，一个5400亿参数的密集语言模型，它通过使用[Pathways框架](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/)进行了高效训练。PaLM
    是迄今为止训练过的最大密集型LLM之一，其高效的训练策略使得其预训练过程可以在大数据集（>7000亿个标记）上进行。这种大型语言模型与广泛的预训练语料库的结合，导致了一些有趣的结果，我们将在本节中探讨。
- en: How does PaLM work?
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PaLM 是如何工作的？
- en: PaLM is a massive LLM that achieves impressive few-shot learning performance
    via a combination of extensive pre-training (enabled by the efficient Pathways
    architecture) and some modifications to the underlying model architecture. We
    will now overview the details of PaLM’s architecture and training regime.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: PaLM 是一个大型的LLM，通过广泛的预训练（得益于高效的Pathways架构）和对基础模型架构的一些修改，达到了令人印象深刻的少样本学习性能。我们将概述PaLM架构和训练模式的细节。
- en: '**the model.** PaLM uses a decoder-only transformer with 540 billion parameters.
    However, this model goes beyond the typical, decoder-only architecture by making
    a few modifications:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型。** PaLM 使用了一个仅解码器的变换器，具有5400亿参数。然而，这个模型超越了典型的仅解码器架构，通过进行一些修改：'
- en: SwiGLU activations (instead of [ReLU](https://deepai.org/machine-learning-glossary-and-terms/relu))
    are used in MLP layers.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在MLP层中使用SwiGLU激活（而不是[ReLU](https://deepai.org/machine-learning-glossary-and-terms/relu)）。
- en: Multi-query attention is used in attention layers.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在注意力层中使用多查询注意力。
- en: Only parallel transformer blocks are used.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅使用并行变换器块。
- en: Absolute or relative positional embeddings are replaced with ROPE embeddings.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绝对或相对位置嵌入被 ROPE 嵌入替代。
- en: To understand the impact of model scale, three different sizes of PaLM are tested
    within [1]; see below.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解模型规模的影响，[1] 中测试了三种不同规模的 PaLM；见下文。
- en: '![](../Images/5396394b02e07bb43c008e57abd5bd34.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5396394b02e07bb43c008e57abd5bd34.png)'
- en: (from [1])
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Although power laws suggest that performance should improve smoothly between
    the models shown above, analysis in [1] finds that *we often see a disproportionate
    performance improvement when using the largest (540 billion parameter) model*.
    Larger LLMs provide a surprisingly large benefit when combined with a more extensive
    pre-training process.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管幂律法则表明性能在上述模型之间应平滑提升，[1] 的分析发现 *我们通常会看到使用最大（5400亿参数）模型时，性能有不成比例的提升*。较大的 LLM
    在结合更广泛的预训练过程时提供了意外的大收益。
- en: '*“For certain tasks, we observe discontinuous improvements, where scaling from
    62B to 540B results in a drastic jump in accuracy compared to scaling from 8B
    to 62B… This suggests that new capabilities of large LMs can emerge when the model
    achieves sufficient scale, and that these capabilities continue to emerge beyond
    previously studied scales.”* — from [1]'
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“对于某些任务，我们观察到不连续的改进，其中从 62B 扩展到 540B 在准确性上有显著跃升，相较于从 8B 扩展到 62B… 这表明，当模型达到足够的规模时，大型语言模型的新能力可能会出现，并且这些能力会在先前研究的规模之外继续出现。”*
    — 来自 [1]'
- en: '**dataset.** PaLM’s pre-training corpus is comprised of 780B tokens. This is
    somewhat smaller than the dataset used to train [Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms)
    [6] but still larger than that of most prior LLMs; see below.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据集。** PaLM 的预训练语料库包含 780B 个标记。这个数据集比用于训练 [Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms)
    [6] 的数据集略小，但仍然大于大多数之前的 LLM；见下文。'
- en: '![](../Images/1b0e2a69bc8ef8a32c0438962c24c332.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b0e2a69bc8ef8a32c0438962c24c332.png)'
- en: (from [3])
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [3]）
- en: Creating high-performing LLMs is not just about making the model larger. Recent
    work on scaling laws for LLMs [6] indicates that performance will increase as
    a factor of both model size and pre-training corpus size. As such, PaLM has the
    opportunity to significantly outperform models like MT-NLG (despite being only
    slightly larger) by using a much larger pre-training corpus.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 创建高性能的 LLM 不仅仅是让模型变得更大。对 LLM 的缩放规律的最新研究 [6] 表明，性能将随着模型大小和预训练语料库大小的增加而提高。因此，PaLM
    有机会显著超越 MT-NLG 等模型（尽管其仅略大），通过使用更大的预训练语料库。
- en: The pre-training corpus used for PaLM is derived from high-quality webpages,
    books, wikipedia, news, articles, code, and social media conversations. It contains
    22% non-English data (see below) and is inspired by the corpora used to train
    LaMDA and GLaM [8, 9]. All models are trained for exactly one epoch over this
    dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: PaLM 使用的预训练语料库来源于高质量的网页、书籍、维基百科、新闻、文章、代码和社交媒体对话。它包含 22% 的非英语数据（见下文），并受到用于训练
    LaMDA 和 GLaM [8, 9] 的语料库的启发。所有模型都在这个数据集上训练了一个完整的周期。
- en: '![](../Images/c3d797f04ac759c73bfb2b3d097fa0c1.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3d797f04ac759c73bfb2b3d097fa0c1.png)'
- en: (from [1])
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: '**using a large vocabulary.** Given that a non-negligible portion of the pre-training
    corpus is non-English, the authors also adopt a [SentencePiece tokenizer](https://github.com/google/sentencepiece)
    with a vocabulary size of 256K. The tokenizer simply takes raw textual input and
    extracts tokens (i.e., words or sub-words) from the text. This tokenization process
    is based upon an underlying vocabulary (i.e., set of known tokens), and all tokens
    extracted from text must be a member of the vocabulary. If a token is not part
    of the underlying vocabulary, it will be broken into smaller chunks (possibly
    even characters) until it has been decomposed into valid tokens, or replaced with
    the generic “`[UNK]`” out of vocabulary token.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用大词汇量。** 由于预训练语料库中有相当一部分是非英语的，作者还采用了 [SentencePiece tokenizer](https://github.com/google/sentencepiece)
    ，其词汇量为 256K。该分词器直接处理原始文本输入，并从文本中提取标记（即词或子词）。此分词过程基于一个基础词汇表（即已知标记的集合），从文本中提取的所有标记必须是词汇表的成员。如果一个标记不在基础词汇表中，它将被拆分成更小的块（可能是字符），直到被分解成有效的标记，或者用通用的“`[UNK]`”（未知词汇标记）替代。'
- en: 'Using a small vocabulary would mean that a lot of important tokens would fail
    to be properly captured, which can damage the LLM’s performance. For multi-lingual
    models, we typically see that the size of the underlying vocabulary is increased
    a lot to avoid this effect, as data from multiple languages will utilize a wider
    range of tokens. PaLM is no different: the authors adopt a larger-than-usual vocabulary
    size to avoid improperly tokenizing the data and allow more effective learning
    across multiple languages. To learn more about language models that are trained
    over many languages, check out the link [here](https://cameronrwolfe.substack.com/p/many-languages-one-deep-learning).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用小词汇表意味着许多重要的标记可能无法被正确捕捉，这可能会影响LLM的性能。对于多语言模型，我们通常会看到底层词汇表的大小大幅增加，以避免这种情况，因为来自多种语言的数据会利用更广泛的标记范围。PaLM
    也不例外：作者采用了比平常更大的词汇表，以避免错误标记数据，并允许在多种语言间更有效的学习。要了解更多关于在多种语言上训练的语言模型的信息，请查看 [这里](https://cameronrwolfe.substack.com/p/many-languages-one-deep-learning)
    的链接。
- en: '**training system.** Prior to overviewing the training framework used for PaLM,
    we need to understand a few concepts related to distributed training. Most importantly,
    we need to understand the differences between model, data, and pipeline parallelism.
    Although I’ve explained these concepts [before](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt#%C2%A7other-useful-details),
    the tweet [here](https://twitter.com/rasbt/status/1625494398778892292?s=20) has
    a much better (and more concise) description.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练系统。** 在概述用于PaLM的训练框架之前，我们需要了解一些与分布式训练相关的概念。最重要的是，我们需要理解模型、数据和流水线并行之间的区别。虽然我已经[之前](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt#%C2%A7other-useful-details)解释了这些概念，但
    [这里](https://twitter.com/rasbt/status/1625494398778892292?s=20) 的推文提供了更好（且更简明）的描述。'
- en: PaLM is trained on a collection of 6144 TPU chips that are distributed across
    two [TPU pods](https://cloud.google.com/tpu/docs/training-on-tpu-pods) (i.e.,
    groups of TPUs connect with high-speed network interfaces). At the time of publication,
    this system was the largest configuration yet described; see below.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: PaLM 在一组6144个TPU芯片上进行训练，这些芯片分布在两个 [TPU集群](https://cloud.google.com/tpu/docs/training-on-tpu-pods)（即，通过高速网络接口连接的TPU组）中。在发布时，这个系统是描述的最大配置；见下图。
- en: '![](../Images/c510fbb84015aa991062b47a0e2870e4.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c510fbb84015aa991062b47a0e2870e4.png)'
- en: (from [1])
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: 'Within a pod, communication is very fast between TPUs. But, communication between
    pods is much slower. Typically, model and data parallelism have bandwidth requirements
    that are too large for efficient training across TPU pods. Most prior work has
    dealt with this by either:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个集群内部，TPU之间的通信非常快速。但集群之间的通信则要慢得多。通常，模型和数据并行的带宽需求过大，无法在TPU集群之间高效训练。大多数先前的工作通过以下方式处理这个问题：
- en: Limiting training to a single TPU pod [8, 9].
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练限制在单个TPU集群上 [8, 9]。
- en: Using pipeline parallelism, which has lower bandwidth requirements, between
    pods [7, 10].
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在集群之间使用带宽要求较低的流水线并行 [7, 10]。
- en: However, pipelining has many notable drawbacks, such as leaving accelerators
    idle while emptying or filling the pipeline and having high memory requirements.
    Using the Pathways system, PaLM is efficiently trained across TPU pods with a
    combination of model and data parallelism (i.e., no pipeline parallelism). This
    novel training paradigm enables significant improvements in efficiency.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，流水线有许多显著的缺点，比如在清空或填充流水线时使加速器处于空闲状态，以及高内存需求。使用Pathways系统，PaLM在TPU集群之间通过模型和数据并行（即无流水线并行）进行高效训练。这种新颖的训练范式显著提高了效率。
- en: '![](../Images/1587a60f1ea895454c697671a0ddbaca.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1587a60f1ea895454c697671a0ddbaca.png)'
- en: (from [1])
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: For example, PaLM achieves a model [FLOPs](https://stackoverflow.com/questions/58498651/what-is-flops-in-field-of-deep-learning)
    utilization (i.e., throughput in tokens-per-second divided by theoretical maximum
    throughput of a system) of 46.2%, while prior systems struggle to surpass utilization
    of 30%; see above. For more information on the Pathways system and how it achieves
    such a massive improvement in LLM training efficiency, check out the article [here](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，PaLM 实现了一个模型 [FLOPs](https://stackoverflow.com/questions/58498651/what-is-flops-in-field-of-deep-learning)
    利用率（即每秒令牌吞吐量与系统理论最大吞吐量的比值）为 46.2%，而之前的系统难以超过 30% 的利用率；详见上文。有关 Pathways 系统及其如何在
    LLM 训练效率上实现如此巨大的改进的信息，请查看 [这里](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/)
    的文章。
- en: How does PaLM perform?
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PaLM 的表现如何？
- en: The analysis provided in [1] goes beyond achieving superior few-shot learning
    performance. PaLM is shown to effectively handle multiple languages, have improved
    reasoning capabilities, perform significantly better than smaller models, and
    even surpass human-level language understanding on certain tasks.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [1] 中提供的分析超越了实现卓越的少样本学习表现。PaLM 被证明能够有效处理多种语言，具备改进的推理能力，性能显著优于较小的模型，甚至在某些任务上超越了人类水平的语言理解。
- en: '**multi-lingual LLMs.** Prior LLMs (e.g., GPT-3 [4]) had been shown somewhat
    capable of performing machine translation, especially when translating other languages
    into English. Across English-centric data pairs and settings, we see that PaLM
    improves translation performance relative to prior LLMs; see below.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**多语言 LLM。** 之前的 LLM（例如，GPT-3 [4]）已经表现出一定的机器翻译能力，特别是在将其他语言翻译成英语时。在以英语为中心的数据对和设置中，我们看到
    PaLM 相对于之前的 LLM 改进了翻译性能；详见下文。'
- en: '![](../Images/98efc51027350013ece94284ae4336e7.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98efc51027350013ece94284ae4336e7.png)'
- en: (from [1])
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: On low resource and non-English centric data, PaLM still performs relatively
    well, but it is outperformed by existing supervised translation approaches; see
    above. However, given that non-English settings are not widely considered by prior
    work, PaLM’s ability to perform relatively well in this setting is impressive.
    Overall, this analysis shows us that PaLM has improved language translation abilities
    but still falls short of supervised techniques.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在资源稀缺和非英语中心的数据上，PaLM 仍表现相对良好，但不及现有的监督翻译方法；详见上文。然而，鉴于之前的工作并未广泛考虑非英语设置，PaLM 在这种设置下的相对良好表现令人印象深刻。总体而言，这项分析显示
    PaLM 语言翻译能力有所提升，但仍不及监督技术。
- en: Beyond language translation, we also see that PaLM performs well on multilingual
    generation tasks. As expected, PaLM’s language generation abilities are best in
    English, but the model still outperforms prior LLMs on non-English generation.
    Overall, these results shows us that an LLM’s multilingual capabilities can be
    improved significantly by making small modifications (i.e., more non-English pre-training
    data and using a larger vocabulary for our tokenizer).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 除了语言翻译之外，我们还看到 PaLM 在多语言生成任务上表现出色。正如预期的那样，PaLM 在英语语言生成能力方面表现最佳，但该模型在非英语生成任务上仍优于之前的语言模型。总体而言，这些结果表明，通过进行小幅调整（例如，增加非英语预训练数据并为分词器使用更大的词汇表），可以显著提高
    LLM 的多语言能力。
- en: '**surpassing human performance.** The [BIG-bench dataset](https://github.com/google/BIG-bench)
    contains a collection of 150 tasks with topics including logical reasoning, translation,
    question answering, mathematics, and more. Relative to prior LLMs, we see that
    PaLM achieves improved performance on a majority of these tasks; see below.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**超越人类表现。** [BIG-bench 数据集](https://github.com/google/BIG-bench)包含 150 个任务，涵盖逻辑推理、翻译、问答、数学等主题。相对于之前的
    LLM，我们看到 PaLM 在大多数任务上表现有所提升；详见下文。'
- en: '![](../Images/b76d87a10708fe92e9c70219521afe90.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b76d87a10708fe92e9c70219521afe90.png)'
- en: (from [1])
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Somewhat more impressively than outperforming prior LLMs, PaLM also surpasses
    the average performance of humans on most BIG-bench tasks; see below. For some
    of these tasks, outperforming humans simply indicates that PaLM is capable of
    memorizing data or reasoning across multiple languages. However, this is not always
    the case! On other tasks (e.g., cause and effect identification), we see that
    PaLM seems to have improved language understanding.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 比起超越之前的LLMs，PaLM在大多数BIG-bench任务上的表现也超越了人类的平均水平；见下文。对于这些任务中的某些任务，超越人类简单地表明PaLM能够记忆数据或在多种语言之间进行推理。然而，这并不总是如此！在其他任务中（例如，因果关系识别），我们看到PaLM似乎在语言理解上有所改善。
- en: '![](../Images/cac19f428c176dc42a507f3db9a6921c.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cac19f428c176dc42a507f3db9a6921c.png)'
- en: (from [1])
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[1]）
- en: '**do power laws always hold?** When we break down the performance of PaLM into
    specific task categories, we see that model scale is especially helpful for certain
    tasks. For example, on logical sequence tasks (i.e., putting a set of words into
    a logical order), the largest PaLM model sees a massive improvement in performance
    relative to smaller models. For other tasks (e.g., mathematical induction), model
    scale makes little difference.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**幂律是否总是成立？** 当我们将PaLM的表现细分到特定任务类别时，我们发现模型规模对某些任务特别有帮助。例如，在逻辑序列任务（即将一组词语排列成逻辑顺序）中，最大的PaLM模型在相对于较小模型的性能上有了巨大的提升。对于其他任务（例如，数学归纳），模型规模几乎没有影响。'
- en: '![](../Images/34a5c208470a2a1409cc944c24688ad5.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34a5c208470a2a1409cc944c24688ad5.png)'
- en: (from [1])
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[1]）
- en: Overall, PaLM’s performance does not always follow a power law with respect
    to model scale. In some cases, using a larger model causes a massive, unexpected
    spike in performance, while in others the largest model only performs marginally
    better than smaller variants; see above.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，PaLM的表现并不总是遵循与模型规模相关的幂律。在某些情况下，使用更大的模型会导致性能的巨大意外提升，而在其他情况下，最大的模型仅比较小的变体表现稍好；见上文。
- en: '**learning to reason.** Although language models perform well on many tasks,
    they notoriously struggle to solve basic reasoning tasks. Many researchers cite
    this limitation of LLMs as proof of their “shallow” linguistic understanding.
    However, recent publications have used *chain-of-thought prompting* (i.e., generating
    several reasoning “steps” within the LLM before the final output) to improve the
    reasoning capabilities of LLMs [11, 12]; see below.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习推理。** 尽管语言模型在许多任务上表现良好，但它们在解决基本推理任务时常常遇到困难。许多研究人员引用了LLMs这一局限性作为其“浅薄”语言理解的证明。然而，最近的出版物已经使用*链式思维提示*（即在LLM生成最终输出之前生成几个推理“步骤”）来提高LLMs的推理能力[11,
    12]；见下文。'
- en: '![](../Images/a444eaef4b971dfa265795f5ed8e9c43.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a444eaef4b971dfa265795f5ed8e9c43.png)'
- en: (from [1])
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[1]）
- en: When evaluating PaLM, authors in [1] find that combining a model of this scale
    with chain-of-thought prompting is enough to achieve state-of-the-art accuracy
    on arithmetic and commonsense reasoning tasks. Prior methods leverage domain-specific
    architectures, fine-tuning, and even task-specific verification modules to solve
    such reasoning tasks. In comparison, PaLM simply solves these tasks using few-shot,
    chain-of-thought prompting (and an external calculator module for arithmetic reasoning
    tasks); see below.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估PaLM时，[1]中的作者发现，将这种规模的模型与链式思维提示相结合，足以在算术和常识推理任务上实现最先进的准确度。之前的方法利用了特定领域的架构、微调，甚至任务特定的验证模块来解决这些推理任务。相比之下，PaLM只需使用少量示例的链式思维提示（以及用于算术推理任务的外部计算器模块）即可解决这些任务；见下文。
- en: '![](../Images/bc16d3c7dc7d00ef6bdcd6226a2c1abb.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc16d3c7dc7d00ef6bdcd6226a2c1abb.png)'
- en: (from [1])
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[1]）
- en: Interestingly, we see that the largest PaLM model has much better reasoning
    abilities compared to smaller variants. Such a finding is interesting given that
    prior work has observed a mixed (oftentimes negative) impact of scale on reasoning
    performance. Results in PaLM indicate that model (and data) scale can seemingly
    benefit reasoning performance given the correct prompting approach.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，我们发现最大的PaLM模型在推理能力上远优于较小的变体。考虑到之前的研究发现规模对推理性能的影响通常是混合的（有时是负面的），这一发现很有趣。PaLM的结果表明，只要采用正确的提示方法，模型（和数据）规模似乎可以有益于推理性能。
- en: '![](../Images/556743ea5e168f06eee15c2105e4c42c.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/556743ea5e168f06eee15c2105e4c42c.png)'
- en: (from [1])
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[1]）
- en: The PaLM API
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PaLM API
- en: If you’re interested in testing out PaLM, then you’re in luck! The API for PaLM
    was released to select developers within the last few weeks. Many in the AI community
    saw this release of the PaLM API by Google as a response to the public release
    of the [ChatGPT API](https://openai.com/blog/introducing-chatgpt-and-whisper-apis)
    by OpenAI roughly a week before. Read more about the PaLM API release in the article
    [here](https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html).
    Although training and hosting LLMs is difficult, we are currently seeing a huge
    shift towards these tool being made available to developers via APIs. As such,
    practitioners can get easy access to these incredible models without the hassle
    or cost of training and hosting them. This lowers the barrier of entry for building
    applications with these powerful models, which unlocks a world of possibilities!
    For examples of applications that can be built, I recommend checking out the [OpenAI
    cookbook](https://github.com/openai/openai-cookbook).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣试用PaLM，那你很幸运！PaLM的API在过去几周内已向部分开发者发布。许多AI社区成员将Google发布的PaLM API视为对OpenAI在一周前公开发布的[ChatGPT
    API](https://openai.com/blog/introducing-chatgpt-and-whisper-apis)的回应。有关PaLM API发布的更多信息，请阅读[这里](https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html)的文章。尽管训练和托管LLMs很困难，但我们目前看到这些工具通过API向开发者开放的巨大转变。因此，实践者可以轻松访问这些令人惊叹的模型，无需培训和托管的麻烦或费用。这降低了使用这些强大模型构建应用的门槛，开启了无限的可能性！关于可以构建的应用实例，我推荐查看[OpenAI
    cookbook](https://github.com/openai/openai-cookbook)。
- en: Takeaways
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重点总结
- en: Although initial attempts to train LLMs beyond the scale of GPT-3 were somewhat
    unsuccessful, we see with PaLM that all we need is an efficient training framework
    that allows for more extensive pre-training. By using the Pathways framework,
    PaLM can be trained over a much larger dataset compared to prior models of its
    scale, such as MT-NLG [7]. The resulting LLM has impressive multi-lingual understanding
    and reasoning capabilities, and we see that increasing the size of the model can
    oftentimes provide a major benefit. Some important takeaways from PaLM are listed
    below.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最初尝试训练超越GPT-3规模的LLMs并不完全成功，但我们从PaLM中看到，我们所需的只是一个高效的训练框架，允许更广泛的预训练。通过使用Pathways框架，PaLM可以在比之前模型（如MT-NLG
    [7]）更大的数据集上进行训练。得到的LLM具有令人印象深刻的多语言理解和推理能力，我们看到模型规模的增加往往能带来显著的好处。以下是从PaLM中得到的一些重要启示。
- en: '**do power laws always hold?** Numerous publications on the topic of LLMs have
    shown that a power law exists between LLM performance and various quantities,
    such as (non-embedding) model parameters, dataset size, amount of training compute
    and more. Although this trend holds in terms of aggregate performance, the story
    is a bit more complicated when we examine performance separately with respect
    to each task. Certain tasks benefit disproportionately from scale, while others
    don’t see much of a benefit. Thus, scale is generally helpful for LLMs, but the
    results vary significantly depending on the downstream task being solved.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**幂律定律是否总是成立？** 关于LLMs的众多出版物表明，LLM性能与各种数量（如（非嵌入式）模型参数、数据集规模、训练计算量等）之间存在幂律关系。虽然这种趋势在整体性能方面是成立的，但当我们单独检查每个任务的性能时，情况则更为复杂。某些任务从规模中受益不成比例，而其他任务则未见太多好处。因此，规模通常对LLMs有帮助，但结果会根据下游任务的不同而有显著差异。'
- en: '**should we avoid pipeline parallelism?** One of the main selling points of
    PaLM is the efficient Pathways training framework with which it is trained. Typically,
    training over multiple TPU pods or compute nodes requires the use of pipeline
    parallelism due to limited memory bandwidth. However, by removing pipeline parallelism
    and allowing training across TPU pods to be performed solely with data and model
    parallelism, we see that PaLM achieves groundbreaking training efficiency and
    throughput. These gains to the training framework allow PaLM to be trained over
    much more data, enabling the model’s impressive performance.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们是否应该避免管道并行性？** PaLM的一个主要卖点是其高效的Pathways训练框架。通常，在多个TPU集群或计算节点上训练需要使用管道并行性，因为内存带宽有限。然而，通过去除管道并行性并仅使用数据和模型并行性进行TPU集群训练，我们发现PaLM实现了突破性的训练效率和吞吐量。这些对训练框架的提升使得PaLM可以在更多的数据上进行训练，从而展现出模型令人印象深刻的性能。'
- en: '**LLM scale and reasoning.** Prior work on LLMs has oftentimes pointed out
    their poor reasoning capabilities. In fact, it seemed that the ability of LLMs
    to perform reasoning tasks degraded with scale. However, we see with PaLM that
    this is not always the case. If we combine larger LLMs with more pre-training
    data and the correct prompting approach (i.e., chain-of-thought prompting), we
    see pretty noticeable improvements in LLM reasoning abilities!'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLM 的规模与推理。** 之前关于 LLM 的研究经常指出其推理能力较差。实际上，LLM 在进行推理任务时，其能力似乎会随着规模的增大而下降。然而，我们看到在
    PaLM 的情况下并非总是如此。如果我们将更大的 LLM 与更多的预训练数据和正确的提示方法（即链式思维提示）相结合，我们可以看到 LLM 推理能力有相当明显的提升！'
- en: Closing Remarks
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结束语
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/). I study the empirical
    and theoretical foundations of deep learning. You can also check out my [other
    writings](https://medium.com/@wolfecameron) on medium! If you liked it, please
    follow me on [twitter](https://twitter.com/cwolferesearch) or subscribe to my
    [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/), where
    I help readers build a deeper understanding of topics in deep learning research
    via understandable overviews of popular papers.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢你阅读这篇文章。我是 [Cameron R. Wolfe](https://cameronrwolfe.me/)，[Rebuy](https://www.rebuyengine.com/)
    的 AI 主管。我研究深度学习的经验和理论基础。你也可以查看我在 medium 上的 [其他文章](https://medium.com/@wolfecameron)！如果你喜欢这篇文章，请关注我的
    [twitter](https://twitter.com/cwolferesearch) 或订阅我的 [Deep (Learning) Focus 新闻通讯](https://cameronrwolfe.substack.com/)，我在其中帮助读者通过对热门论文的易懂概述，深入理解深度学习研究中的主题。
- en: Bibliography
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Chowdhery, Aakanksha, et al. “Palm: Scaling language modeling with pathways.”
    *arXiv preprint arXiv:2204.02311* (2022).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Chowdhery, Aakanksha, 等人。“Palm：通过路径扩展语言建模。” *arXiv 预印本 arXiv:2204.02311*（2022）。'
- en: '[2] Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers
    for language understanding.” *arXiv preprint arXiv:1810.04805* (2018).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Devlin, Jacob, 等人。“Bert：用于语言理解的深度双向 transformer 预训练。” *arXiv 预印本 arXiv:1810.04805*（2018）。'
- en: '[3] Raffel, Colin, et al. “Exploring the limits of transfer learning with a
    unified text-to-text transformer.” *The Journal of Machine Learning Research*
    21.1 (2020): 5485–5551.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Raffel, Colin, 等人。“利用统一的文本到文本 transformer 探索迁移学习的极限。” *机器学习研究杂志* 21.1（2020）：5485–5551。'
- en: '[4] Brown, Tom, et al. “Language models are few-shot learners.” *Advances in
    neural information processing systems* 33 (2020): 1877–1901.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Brown, Tom, 等人。“语言模型是少样本学习者。” *神经信息处理系统进展* 33（2020）：1877–1901。'
- en: '[5] Kaplan, Jared, et al. “Scaling laws for neural language models.” *arXiv
    preprint arXiv:2001.08361* (2020).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Kaplan, Jared, 等人。“神经语言模型的规模定律。” *arXiv 预印本 arXiv:2001.08361*（2020）。'
- en: '[6] Hoffmann, Jordan, et al. “Training compute-optimal large language models.”
    *arXiv preprint arXiv:2203.15556* (2022).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Hoffmann, Jordan, 等人。“训练计算最优的大型语言模型。” *arXiv 预印本 arXiv:2203.15556*（2022）。'
- en: '[7] Smith, Shaden, et al. “Using deepspeed and megatron to train megatron-turing
    nlg 530b, a large-scale generative language model.” *arXiv preprint arXiv:2201.11990*
    (2022).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Smith, Shaden, 等人。“使用 deepspeed 和 megatron 训练 megatron-turing nlg 530b，一个大规模生成语言模型。”
    *arXiv 预印本 arXiv:2201.11990*（2022）。'
- en: '[8] Thoppilan, Romal, et al. “Lamda: Language models for dialog applications.”
    *arXiv preprint arXiv:2201.08239* (2022).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Thoppilan, Romal, 等人。“Lamda：对话应用的语言模型。” *arXiv 预印本 arXiv:2201.08239*（2022）。'
- en: '[9] Du, Nan, et al. “Glam: Efficient scaling of language models with mixture-of-experts.”
    *International Conference on Machine Learning*. PMLR, 2022.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Du, Nan, 等人。“Glam：使用专家混合进行语言模型的高效扩展。” *国际机器学习会议*。PMLR，2022。'
- en: '[10] Rae, Jack W., et al. “Scaling language models: Methods, analysis & insights
    from training gopher.” *arXiv preprint arXiv:2112.11446* (2021).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Rae, Jack W., 等人。“扩展语言模型：方法、分析与训练 gopher 的见解。” *arXiv 预印本 arXiv:2112.11446*（2021）。'
- en: '[11] Nye, Maxwell, et al. “Show your work: Scratchpads for intermediate computation
    with language models.” *arXiv preprint arXiv:2112.00114* (2021).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Nye, Maxwell, 等人。“展示你的工作：语言模型的中间计算记事本。” *arXiv 预印本 arXiv:2112.00114*（2021）。'
- en: '[12] Cobbe, Karl, et al. “Training verifiers to solve math word problems.”
    *arXiv preprint arXiv:2110.14168* (2021).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Cobbe, Karl, 等人。“训练验证者解决数学词题。” *arXiv 预印本 arXiv:2110.14168*（2021）。'
- en: '[13] Shazeer, Noam. “Glu variants improve transformer.” *arXiv preprint arXiv:2002.05202*
    (2020).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Shazeer, Noam。“Glu 变体改进了 transformer。” *arXiv 预印本 arXiv:2002.05202*（2020）。'
- en: '[14] Ramachandran, Prajit, Barret Zoph, and Quoc V. Le. “Searching for activation
    functions.” *arXiv preprint arXiv:1710.05941* (2017).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] 拉马钱德兰，普拉吉特，巴雷特·佐普和阮光伟。“寻找激活函数。” *arXiv 预印本 arXiv:1710.05941* (2017)。'
- en: '[15] Dauphin, Yann N., et al. “Language modeling with gated convolutional networks.”
    *International conference on machine learning*. PMLR, 2017.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] 多芬，扬·N. 等人。“使用门控卷积网络进行语言建模。” *国际机器学习会议*。PMLR，2017。'
- en: '[16] Su, Jianlin, et al. “Roformer: Enhanced transformer with rotary position
    embedding.” *arXiv preprint arXiv:2104.09864* (2021).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] 苏建林等人。“Roformer: 增强型变换器与旋转位置嵌入。” *arXiv 预印本 arXiv:2104.09864* (2021)。'
- en: '[17] Vaswani, Ashish, et al. “Attention is all you need.” *Advances in neural
    information processing systems* 30 (2017).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] 瓦斯瓦尼，阿希什等人。“注意力机制即你所需的一切。” *神经信息处理系统进展* 30 (2017)。'
