- en: Training XGBoost with MLflow Experiments and HyperOpt Tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 MLflow 实验和 HyperOpt 调整训练 XGBoost
- en: 原文：[https://towardsdatascience.com/training-xgboost-with-mlflow-experiments-and-hyperopt-c0d3a4994ea6](https://towardsdatascience.com/training-xgboost-with-mlflow-experiments-and-hyperopt-c0d3a4994ea6)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/training-xgboost-with-mlflow-experiments-and-hyperopt-c0d3a4994ea6](https://towardsdatascience.com/training-xgboost-with-mlflow-experiments-and-hyperopt-c0d3a4994ea6)
- en: A starting point on your MLOps Journey
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你 MLOps 旅程的起点
- en: '[](https://animadurkar.medium.com/?source=post_page-----c0d3a4994ea6--------------------------------)[![Ani
    Madurkar](../Images/ad54a9e110c56ba1f4c7f5ce0bc7d7e4.png)](https://animadurkar.medium.com/?source=post_page-----c0d3a4994ea6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c0d3a4994ea6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c0d3a4994ea6--------------------------------)
    [Ani Madurkar](https://animadurkar.medium.com/?source=post_page-----c0d3a4994ea6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://animadurkar.medium.com/?source=post_page-----c0d3a4994ea6--------------------------------)[![Ani
    Madurkar](../Images/ad54a9e110c56ba1f4c7f5ce0bc7d7e4.png)](https://animadurkar.medium.com/?source=post_page-----c0d3a4994ea6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c0d3a4994ea6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c0d3a4994ea6--------------------------------)
    [Ani Madurkar](https://animadurkar.medium.com/?source=post_page-----c0d3a4994ea6--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c0d3a4994ea6--------------------------------)
    ·10 min read·Jan 9, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c0d3a4994ea6--------------------------------)
    ·10 分钟阅读·2023年1月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/2a9a554a515ef56cb3a1694c3f067a09.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a9a554a515ef56cb3a1694c3f067a09.png)'
- en: Colors of the Adirondacks. Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 亚德irondack 山脉的色彩。图像来源于作者
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: As you evolve in your journey in Machine Learning, you’ll soon find yourself
    gravitating closer and closer to MLOps whether you like it or not. Building efficient,
    scalable, and resilient machine learning systems is a challenge and the real job
    of a Data Scientist (in my opinion) as opposed to just doing modeling.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你在机器学习领域的进步，你会发现自己不管愿不愿意，都会越来越接近 MLOps。构建高效、可扩展和可靠的机器学习系统是一项挑战，并且是数据科学家的真正工作（在我看来），而不仅仅是进行建模。
- en: The modeling part has been largely figured out for most use cases. Unless you’re
    trying to be at the bleeding edge of the craft, you’re likely dealing with structured,
    tabular datasets. The choice of model can vary depending on the dataset size,
    assumptions, and technical restrictions, but for the most part, it is fairly repeatable.
    My workflow for supervised learning ML during the experimentation phase has converged
    to using XGBoost with HyperOpt and MLflow. XGBoost for the model of choice, HyperOpt
    for the hyperparameter tuning, and MLflow for the experimentation and tracking.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数用例，建模部分已经被大致解决了。除非你试图站在技术最前沿，否则你很可能处理的是结构化的表格数据集。模型的选择可能会根据数据集的大小、假设和技术限制而有所不同，但在大多数情况下，这个过程是相当可重复的。在实验阶段，我的监督学习
    ML 工作流程已经收敛于使用 XGBoost 结合 HyperOpt 和 MLflow。选择 XGBoost 作为模型，HyperOpt 进行超参数调整，以及使用
    MLflow 进行实验和跟踪。
- en: This also represents a phenomenal step 1 as you embark on the MLOps journey
    because I think it’s easiest to start doing more MLOps work during the experimentation
    phase (model tracking, versioning, registry, etc.). It’s lightweight and highly
    configurable which makes it easy to scale up and down as you may need.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这也代表了你开始 MLOps 旅程的一个重要第一步，因为我认为在实验阶段（模型跟踪、版本管理、注册等）开始做更多 MLOps 工作是最容易的。它轻量且高度可配置，使得根据需要进行扩展和收缩变得容易。
- en: Although I briefly discuss XGBoost, MLflow, and HyperOpt, this isn’t a deep
    walkthrough of each. Initial hands-on familiarity with each would be really helpful
    to understand how some pieces here are working in more depth. I’ll be working
    with the [UCI ML Breast Cancer Wisconsin (Diagnostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))
    dataset (CC BY 4.0).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我简要讨论了 XGBoost、MLflow 和 HyperOpt，但这并不是对每个工具的深度讲解。对每个工具的初步动手熟悉会非常有帮助，以更深入地理解这里的一些内容。我将使用[UCI
    ML 乳腺癌威斯康星州（诊断）](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))
    数据集（CC BY 4.0）。
- en: 'To start, we can start an MLflow server (I discuss what’s happening here a
    bit later):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，我们可以启动一个 MLflow 服务器（我稍后会讨论这里发生了什么）：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/32d6510bee78e8781f1a1cca2df01d53.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32d6510bee78e8781f1a1cca2df01d53.png)'
- en: PandasProfiler is a fantastic open-source library to run a quick exploratory
    data analysis report on a dataset. Tracking descriptive statistics, finding nulls,
    anomaly detection, distribution analysis, and more are all shown in the report.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: PandasProfiler 是一个极好的开源库，用于对数据集进行快速的探索性数据分析报告。报告中显示了描述性统计、空值检测、异常检测、分布分析等内容。
- en: '![](../Images/445ae919f9967e0e325a84959a0a6fa4.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/445ae919f9967e0e325a84959a0a6fa4.png)'
- en: Sample view of the HTML Profiler output
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: HTML Profiler 输出的示例视图
- en: 'I save the file as an HTML to interact with the analysis on a webpage instead
    of in a Jupyter Notebook which could yield memory errors depending on dataset
    size. See the Quickstart guide here for how PandasProfiler works: [https://pandas-profiling.ydata.ai/docs/master/pages/getting_started/quickstart.html](https://pandas-profiling.ydata.ai/docs/master/pages/getting_started/quickstart.html)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我将文件保存为 HTML 格式，以便在网页上与分析进行交互，而不是在可能因数据集大小而导致内存错误的 Jupyter Notebook 中。有关 PandasProfiler
    的工作原理，请参阅快速入门指南：[https://pandas-profiling.ydata.ai/docs/master/pages/getting_started/quickstart.html](https://pandas-profiling.ydata.ai/docs/master/pages/getting_started/quickstart.html)
- en: Lastly, we can create training, validation, and testing datasets.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以创建训练集、验证集和测试集。
- en: XGBoost
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost
- en: XGBoost, eXtreme Gradient Boosted Decision Trees, has become the de facto model
    of choice for a large number of tabular modeling tasks. It’s still highly recommended
    to try out simpler models like Linear/Logistic Regression first but almost all
    structured tabular modeling projects with >50–100K rows have resulted in these
    models winning out by a significant margin.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost，即极限梯度提升决策树，已成为大量表格建模任务的首选模型。尽管仍然强烈建议首先尝试更简单的模型，如线性/逻辑回归，但几乎所有结构化表格建模项目（超过
    50-100K 行）的结果都显示，这些模型以显著的优势胜出。
- en: '*How do they work?*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*它们是如何工作的？*'
- en: 'XGBoost is an open-source, gradient-boosting algorithm that uses decision trees
    as weak learners to build up a stronger model — considered an ensemble model due
    to its nature to combine multiple models together. There are two common ensemble
    methods: Bagging and Boosting.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 是一个开源的梯度提升算法，使用决策树作为弱学习器来构建更强的模型——由于其结合多个模型的特性，它被认为是一种集成模型。常见的集成方法有两种：Bagging
    和 Boosting。
- en: Bagging, or bootstrap aggregating, typically is low variance but can be high
    bias. It can lead to better training stability, stronger ML accuracy, and a lower
    tendency to overfit. Random Forest models leverage bagging by combining decision
    trees where each tree can pick only from a random subset of features to use.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging，或称为自助聚合，通常具有较低的方差，但可能会有较高的偏差。它可以带来更好的训练稳定性、更强的机器学习准确性，并且更不容易过拟合。随机森林模型通过结合决策树来利用
    bagging，每棵树只能从随机特征子集中选择特征进行使用。
- en: '![](../Images/ca92faa8e5ad1f15492301360bf873db.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca92faa8e5ad1f15492301360bf873db.png)'
- en: An illustration of the concept of bootstrap aggregating. Open Domain, [Wikipedia](https://en.wikipedia.org/wiki/Bootstrap_aggregating)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 自助聚合概念的示意图。开放领域，[维基百科](https://en.wikipedia.org/wiki/Bootstrap_aggregating)
- en: Boosting, in contrast, works to convert weak learners to strong ones. Each learner,
    or model, is trained on the same set of samples but each sample is weighted differently
    in each iteration. This results in weak learners getting better at learning the
    right weights and parameters for strong model performance over time.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相对，Boosting 的目标是将弱学习器转化为强学习器。每个学习器或模型都在相同的样本集上进行训练，但每个样本在每次迭代中都有不同的权重。这使得弱学习器能够随着时间的推移，更好地学习适合强模型性能的权重和参数。
- en: '![](../Images/1b88b1428e42970815abcfa43f2472d4.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b88b1428e42970815abcfa43f2472d4.png)'
- en: An illustration of the concept of boosting. Open Domain, [Wikipedia](https://commons.wikimedia.org/wiki/File:Ensemble_Boosting.svg)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 提升概念的示意图。开放领域，[维基百科](https://commons.wikimedia.org/wiki/File:Ensemble_Boosting.svg)
- en: The gradient boosting part of this algorithm refers to the fact it uses [Gradient
    Descent](https://en.wikipedia.org/wiki/Gradient_descent) to minimize the loss
    function. The loss function for XGBoost is a regularised (L1 and L2) objective
    function that incorporates a function of convex loss and a model complexity penalty
    term.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的梯度提升部分指的是它使用[梯度下降](https://en.wikipedia.org/wiki/Gradient_descent)来最小化损失函数。XGBoost
    的损失函数是一个正则化的（L1 和 L2）目标函数，它结合了凸损失函数和模型复杂性惩罚项。
- en: XGBoost took fame as it became the standard for winning a multitude of tournaments
    on Kaggle, but lately, people have also used Microsoft’s LightGBM as it can be
    faster for large datasets. I’ve typically found phenomenal performance using both
    — can just be dependent on your needs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 因其成为 Kaggle 多项比赛的获胜标准而广受欢迎，但最近，人们也开始使用微软的 LightGBM，因为它在处理大型数据集时可能更快。我通常发现两者都能提供卓越的性能——这通常取决于你的需求。
- en: MLflow
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLflow
- en: Mlflow is an open-source machine learning experiment tracking software. It makes
    it incredibly easy to spin up a local web interface to monitor your machine learning
    models, compare them, and stage them.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow 是一个开源的机器学习实验跟踪软件。它使得启动一个本地 web 界面以监控你的机器学习模型、对比它们并进行管理变得非常简单。
- en: As you’re experimenting between the right modeling algorithm and architecture,
    it can be a nightmare to efficiently evaluate the best one especially when you’re
    running hundreds of experiments at once. Ideally, you want a way to store each
    model run, its hyperparameters, its evaluation criteria, and more. MLflow makes
    this all possible with minimal code around your training code.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在你实验不同的建模算法和架构时，尤其是当你同时运行数百个实验时，效率地评估最佳方案可能会非常困难。理想情况下，你希望有一种方法来存储每次模型运行、其超参数、评估标准等。MLflow
    使得这一切成为可能，并且对你的训练代码的干扰最小。
- en: 'As you experiment with different modeling architectures you can add new experiments
    and compare each one with the same criteria. Logging artifacts is extremely easy
    and fast. There are two main components that it looks to track and store: entities
    and artifacts.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当你尝试不同的建模架构时，你可以添加新的实验，并用相同的标准对每个实验进行比较。记录工件非常简单和快速。它主要追踪和存储两个组成部分：实体和工件。
- en: 'Entities: runs, parameters, metrics, tags, notes, metadata, etc. These are
    stored in the backend store.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 实体：运行、参数、指标、标签、备注、元数据等。这些都存储在后台存储中。
- en: 'Artifacts: files, models, images, in-memory objects, or model summaries, etc.
    These are stored in the artifact store.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 工件：文件、模型、图像、内存对象或模型摘要等。这些都存储在工件存储中。
- en: 'The default storage location for these are local files, but you can specify
    the backend store to instead be an SQLite database (this is minimally needed if
    you want to stage models to Staging or Production), a PostgreSQL database (this
    enables user authentication to access), or an S3 database. This page clarifies
    the different store configurations really well: [https://mlflow.org/docs/latest/tracking.html#how-runs-and-artifacts-are-recorded](https://mlflow.org/docs/latest/tracking.html#how-runs-and-artifacts-are-recorded)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些的默认存储位置是本地文件，但你可以指定后台存储为 SQLite 数据库（如果你想将模型阶段设为 Staging 或 Production，这通常是最低要求），PostgreSQL
    数据库（这使得用户身份验证访问成为可能），或者 S3 数据库。这个页面很好地阐明了不同的存储配置：[https://mlflow.org/docs/latest/tracking.html#how-runs-and-artifacts-are-recorded](https://mlflow.org/docs/latest/tracking.html#how-runs-and-artifacts-are-recorded)
- en: 'I won’t be going into the basics of how to use MLflow because their documentation
    covers a lot of what you’d need already. In case you need a quick tutorial for
    how to spin up MLflow, I highly recommend starting here: [https://mlflow.org/docs/latest/tutorials-and-examples/tutorial.html](https://mlflow.org/docs/latest/tutorials-and-examples/tutorial.html)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会详细讲解如何使用 MLflow，因为他们的文档已经涵盖了你需要的很多内容。如果你需要一个关于如何启动 MLflow 的快速教程，我强烈推荐从这里开始：[https://mlflow.org/docs/latest/tutorials-and-examples/tutorial.html](https://mlflow.org/docs/latest/tutorials-and-examples/tutorial.html)
- en: I will be leveraging a lightweight architecture that is independent of the cloud
    but know that as long as you have read/write access to an S3 bucket it’s as easy
    as signing into your AWS credentials and then changing the bucket store path.
    I’ll also be enabling a TrackingServer that connects to the runs and artifacts
    via REST API so you’re able to see the results on a website.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我将利用一种轻量级架构，它不依赖于云，但要知道，只要你对 S3 存储桶有读写权限，就像登录到你的 AWS 凭证然后更改存储桶路径一样简单。我还会启用一个
    TrackingServer，通过 REST API 连接到运行和工件，以便你可以在网站上查看结果。
- en: '![](../Images/5916b4278eabc07004f6039365305ac2.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5916b4278eabc07004f6039365305ac2.png)'
- en: MLflow on localhost with Tracking Server architecture. Adapted from [MLflow
    documentation](https://mlflow.org/docs/latest/tracking.html#scenario-3-mlflow-on-localhost-with-tracking-server).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本地运行的 MLflow，采用 Tracking Server 架构。改编自[MLflow 文档](https://mlflow.org/docs/latest/tracking.html#scenario-3-mlflow-on-localhost-with-tracking-server)。
- en: HyperOpt
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HyperOpt
- en: HyperOpt is an open source for Bayesian optimization to find the right model
    architecture. It is designed for large-scale optimization for models with hundreds
    of parameters and allows the optimization procedure to be scaled across multiple
    cores and multiple machines.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: HyperOpt 是一个用于贝叶斯优化的开源工具，旨在找到合适的模型架构。它专为大规模优化设计，可以处理具有数百个参数的模型，并允许在多个核心和多台机器上进行优化过程的扩展。
- en: 'The different algorithms it can leverage are:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以利用的不同算法包括：
- en: Random Search
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机搜索
- en: Tree of Parzen Estimators
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tree of Parzen Estimators
- en: Annealing
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 退火
- en: Tree
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树
- en: Gaussian Process Tree
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯过程树
- en: Most hyperparameter tuning I see done in practice is either manual or Grid Search.
    This exhaustive process can sometimes yield good results but often times it’s
    highly expensive (computing & time) and unneeded. What’s more, is that Grid Search
    doesn’t selectively use hyperparameters that are better or worse to find the global
    minima for your loss function. It simply searches your entire search space.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我看到的大多数超参数调整在实践中要么是手动的，要么是网格搜索。这种详尽的过程有时会产生良好的结果，但往往成本高昂（计算和时间）且不必要。更重要的是，网格搜索不会选择更好或更差的超参数来找到损失函数的全局最小值。它只是搜索整个搜索空间。
- en: Random Search is often better as a baseline as it can be faster and still provide
    good enough starting points to filter your search space down. A more optimal method
    is Bayesian Optimization though. This is because Bayesian Optimization takes into
    account the prior runs in iteration to guide future selections. Bayesian Optimization
    creates a probabilistic distribution of our hyperparameter space to traverse and
    attempts to find the hyperparameters that minimize our loss function the best.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索通常作为基线效果更好，因为它可以更快，并且仍然提供足够好的起始点来缩小搜索空间。然而，更优化的方法是贝叶斯优化。这是因为贝叶斯优化在迭代中考虑了先前的运行情况，以指导未来的选择。贝叶斯优化创建了超参数空间的概率分布进行遍历，并尝试找到最小化损失函数的超参数。
- en: 'The Tree of Parzen Estimators is a great default I’ve found, but here’s a paper
    that dives deeper into each of the algorithms: [Algorithms for Hyper-Parameter
    Optimization](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)
    (Bergstra, et al.). It typically outperforms basic Bayesian Optimization as it
    leverages a tree structure to better traverse complex search spaces which includes
    categorical hyperparameters whereas Bayesian Optimization requires numerical values.
    Tree of Parzen Estimators has been found to be extremely robust and efficient
    for large-scale hyperparameter optimization.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现 Tree of Parzen Estimators 是一个很好的默认选择，但这里有一篇论文深入探讨了每种算法：[超参数优化算法](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)（Bergstra
    等）。它通常优于基础的贝叶斯优化，因为它利用树结构更好地遍历复杂的搜索空间，其中包括分类超参数，而贝叶斯优化需要数值值。Tree of Parzen Estimators
    被发现对大规模超参数优化非常鲁棒和高效。
- en: 'Michael Berk has written a phenomenal writeup on HyperOpt and its algorithms
    if you’re interested in a deeper dive: [HyperOpt Demystified](/hyperopt-demystified-3e14006eb6fa).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对深入了解感兴趣，Michael Berk 写了一篇关于 HyperOpt 及其算法的精彩文章：[HyperOpt 解密](/hyperopt-demystified-3e14006eb6fa)。
- en: All Together
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 综合来看
- en: 'HyperOpt’s fmin function takes in the key components of putting all of this
    together. Here are some key parameters of fmin:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: HyperOpt 的 fmin 函数将所有这些组件整合在一起。以下是 fmin 的一些关键参数：
- en: 'fn: training model function'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'fn: 训练模型函数'
- en: 'space: hyperparameter search space'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '空间: 超参数搜索空间'
- en: 'algo: optimization algorithm'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'algo: 优化算法'
- en: 'trials: an object can be saved, passed on to the built-in plotting routines,
    or analyzed with your own custom code.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'trials: 一个对象可以被保存、传递给内置绘图例程，或用你自己的自定义代码进行分析。'
- en: 'max_evals: number of modeling experiments to run'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'max_evals: 要运行的建模实验的数量'
- en: HyperOpt’s trials object helps us see a bit more of why you’d return a dictionary
    from the training function.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: HyperOpt 的 trials 对象帮助我们更好地理解为何你会从训练函数中返回一个字典。
- en: '`trials.trials` - a list of dictionaries representing everything about the
    search'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trials.trials` - 代表搜索所有内容的字典列表'
- en: '`trials.results` - a list of dictionaries returned by ''objective'' during
    the search'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trials.results` - 搜索期间由“目标”返回的字典列表'
- en: '`trials.losses()` - a list of losses (float for each ''ok'' trial)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trials.losses()` - 损失的列表（每个“ok”试验的浮点数）'
- en: '`trials.statuses()` - a list of status strings'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trials.statuses()` - 状态字符串的列表'
- en: Another thing to note in the return of the training function is that is where
    you define the loss function (can be custom if you choose). It defaults to minimize
    that metric so if you want to maximize it, as we do with ROC AUC Score, then multiply
    it by -1.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 另外需要注意的是，在训练函数的返回值中定义了损失函数（如果选择的话，可以自定义）。它默认最小化该指标，所以如果你想最大化它，就像我们用ROC AUC分数那样，将其乘以-1。
- en: '![](../Images/b93e0f4eca4d2959bd89e6ff816f2d64.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b93e0f4eca4d2959bd89e6ff816f2d64.png)'
- en: MLflow UI for analyzing ML experiments
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow UI用于分析机器学习实验
- en: We can see all the 50 modeling experiments ran with trying to find the XGBoost
    model with the highest validation ROC AUC score as it searches the hyperparameter
    space. We can easily filter the columns to see different parameters or metrics
    and change how we sort to rapidly analyze the results.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到所有50个建模实验运行的情况，以寻找具有最高验证ROC AUC分数的XGBoost模型，因为它搜索超参数空间。我们可以轻松地筛选列以查看不同的参数或指标，并更改排序方式以快速分析结果。
- en: Model Evaluation & Registry
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估与注册
- en: Let’s compare the top two modeling results.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较前两个建模结果。
- en: '![](../Images/8936dee96706e80ffd79b728dfb72386.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8936dee96706e80ffd79b728dfb72386.png)'
- en: Comparing the top two modeling results
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 比较前两个建模结果
- en: We can choose different parameters and metrics to analyze via a Parallel Coordinates
    Plot, Scatter Plot, Box Plot, and Contour Plot to gauge how changes in parameters
    affect the metrics.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择不同的参数和指标，通过平行坐标图、散点图、箱线图和等高线图来分析，以评估参数变化对指标的影响。
- en: Furthermore, we can click on one run to see all the artifacts of the model saved
    of it and some code snippets to make predictions from this run.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以点击一个运行，查看该模型保存的所有工件以及一些代码片段，以从这个运行中进行预测。
- en: '![](../Images/c4c5f3f397cf79670ae44eabca777d38.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4c5f3f397cf79670ae44eabca777d38.png)'
- en: Details of best model run
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳模型运行的详细信息
- en: We can now load in the best model to evaluate the model on the test set before
    we register it.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以加载最佳模型，在测试集上评估模型，然后再进行注册。
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Looks great! We can now register the model into the Model Registry like so:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来很棒！现在我们可以像这样将模型注册到模型注册表中：
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now let’s update some information such as the description and the version information
    of the model.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更新一些信息，比如模型的描述和版本信息。
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Once we know we want to push the model to production, we can do this step easily
    in MLflow as well. One [very] important thing to note here is that MLflow doesn’t
    have great proxy controls for this which is very much not the recommended approach
    — you don’t want just anyone to be able to push the model up to production. I’ve
    typically found the Staging and Production environments for modeling live in different
    areas (ie. Cloud) so you can manage permissions and access better.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定要将模型推送到生产环境，我们也可以在MLflow中轻松完成这一步。这里有一个[非常]重要的事情需要注意的是，MLflow对此的代理控制并不好，这不是推荐的方法——你不希望任何人都能将模型推送到生产环境。我通常发现建模的Staging和Production环境位于不同的区域（例如：云端），这样可以更好地管理权限和访问。
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: What’s the value of pushing to Production here? Keep in mind this is version
    1 of our experimentation phase, which happened to perform great. Over time we
    may acquire more data or better knowledge of what modeling techniques could beat
    our version 1 model. When that happens and we run a second experiment, we can
    evaluate the second modeling experiment results with our model in production and
    then gauge whether to replace it or not. This is when we would want to leverage
    the “Staging” region between “None” and “Production”.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 推送到生产环境的价值是什么？请记住，这是我们实验阶段的第1版，表现得非常好。随着时间的推移，我们可能会获得更多的数据或更好的建模技术来超越第1版模型。当这种情况发生并且我们进行第二次实验时，我们可以将第二次建模实验的结果与生产环境中的模型进行评估，然后判断是否替换它。这时我们可以利用“Staging”区域在“None”和“Production”之间。
- en: Conclusion
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this story we started with UCI ML Breast Cancer Wisconsin (Diagnostic) dataset
    and showed a standard structured, tabular supervised learning machine learning
    workflow (for a simple dataset) that leveraged:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个故事中，我们从UCI ML乳腺癌威斯康星（诊断）数据集开始，展示了一个标准结构化的、表格化的监督学习机器学习工作流（针对一个简单的数据集），利用了：
- en: PandasProfiler for creating an Exploratory Data Analysis report
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PandasProfiler用于创建探索性数据分析报告
- en: Sci-kit Learn for preprocessing
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sci-kit Learn用于预处理
- en: XGBoost for model training
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost用于模型训练
- en: HyperOpt for hyperparameter tuning
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HyperOpt用于超参数调优
- en: MLflow for experiment tracking, model evaluation, model logging/versioning,
    and model registry
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLflow用于实验跟踪、模型评估、模型记录/版本控制和模型注册
- en: Hope this helps you jumpstart your journey into the MLOps world and levels up
    your machine learning workflows!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这能帮助你启动MLOps之旅，并提升你的机器学习工作流程！
- en: References
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] [Breast Cancer Wisconsin (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic))
    (Wolberg, Street, Mangasarian)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [乳腺癌威斯康星（诊断）数据集](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic))
    (Wolberg, Street, Mangasarian)'
- en: '[2] [Designing Machine Learning Systems](https://learning.oreilly.com/library/view/designing-machine-learning/9781098107956/)
    (Huyen)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [设计机器学习系统](https://learning.oreilly.com/library/view/designing-machine-learning/9781098107956/)
    (Huyen)'
- en: '[3] [MLflow](https://mlflow.org/)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [MLflow](https://mlflow.org/)'
- en: '[4] [HyperOpt](http://hyperopt.github.io/hyperopt/)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [HyperOpt](http://hyperopt.github.io/hyperopt/)'
- en: '[5] [HyperOpt Demystified](/hyperopt-demystified-3e14006eb6fa) (Berk)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [HyperOpt 揭秘](/hyperopt-demystified-3e14006eb6fa) (Berk)'
- en: '[6] [Algorithms for Hyper-Parameter Optimization](https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf)
    (Bergstra et al.)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [超参数优化算法](https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf)
    (Bergstra 等)'
- en: All images unless otherwise noted are by the author.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，否则所有图片均为作者所用。
- en: Become a Medium Member with my Referral Link
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用我的推荐链接成为Medium会员
- en: 'Medium is a large repository of where I do my daily reading, and if you’re
    in the data space, this platform is a gold mine. If you wish to subscribe, here’s
    my referral link to sign up. Full disclosure: if you use this link to subscribe
    to Medium, a portion of your subscription fee will go directly to me. Would love
    to have you be a part of our community.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Medium是我每天阅读的主要平台，如果你在数据领域，这个平台是一个宝藏。如果你希望订阅，这里是我的推荐链接。完全透明：如果你使用此链接订阅Medium，你的订阅费的一部分将直接给我。希望你能成为我们社区的一部分。
- en: '[](https://animadurkar.medium.com/membership?source=post_page-----c0d3a4994ea6--------------------------------)
    [## Join Medium with my referral link - Ani Madurkar'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://animadurkar.medium.com/membership?source=post_page-----c0d3a4994ea6--------------------------------)
    [## 通过我的推荐链接加入 Medium - Ani Madurkar'
- en: Read every story from Ani Madurkar (and thousands of other writers on Medium).
    Your membership fee directly supports…
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读Ani Madurkar（以及Medium上成千上万其他作者）的每一篇故事。你的会员费直接支持…
- en: animadurkar.medium.com](https://animadurkar.medium.com/membership?source=post_page-----c0d3a4994ea6--------------------------------)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: animadurkar.medium.com](https://animadurkar.medium.com/membership?source=post_page-----c0d3a4994ea6--------------------------------)
