- en: Everything You Should Know About Evaluating Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/everything-you-should-know-about-evaluating-large-language-models-dce69ef8b2d2](https://towardsdatascience.com/everything-you-should-know-about-evaluating-large-language-models-dce69ef8b2d2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Open Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From perplexity to measuring general intelligence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://donatoriccio.medium.com/?source=post_page-----dce69ef8b2d2--------------------------------)[![Donato
    Riccio](../Images/0af2a026e72a023db4635522cbca50eb.png)](https://donatoriccio.medium.com/?source=post_page-----dce69ef8b2d2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dce69ef8b2d2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dce69ef8b2d2--------------------------------)
    [Donato Riccio](https://donatoriccio.medium.com/?source=post_page-----dce69ef8b2d2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dce69ef8b2d2--------------------------------)
    ·10 min read·Aug 28, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1557949e969c2f9d2a1f22fa5916c168.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by the author using Stable Diffusion.
  prefs: []
  type: TYPE_NORMAL
- en: As open source language models become more readily available, getting lost in
    all the options is easy.
  prefs: []
  type: TYPE_NORMAL
- en: How do we determine their performance and compare them? And how can we confidently
    say that one model is better than another?
  prefs: []
  type: TYPE_NORMAL
- en: This article provides some answers by presenting training and evaluation metrics,
    and general and specific benchmarks to have a clear picture of your model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you missed it, take a look at the first article in the Open Language Models
    series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/a-gentle-introduction-to-open-source-large-language-models-3643f5ca774?source=post_page-----dce69ef8b2d2--------------------------------)
    [## A Gentle Introduction to Open Source Large Language Models'
  prefs: []
  type: TYPE_NORMAL
- en: Why everyone is talking about Llamas, Alpacas, Falcons and other animals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-gentle-introduction-to-open-source-large-language-models-3643f5ca774?source=post_page-----dce69ef8b2d2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Perplexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Language models define a probability distribution over a vocabulary of words
    to select the most likely next word in a sequence. Given a text, a language model
    assigns a probability to each word in the language, and the most likely is selected.
  prefs: []
  type: TYPE_NORMAL
- en: '**Perplexity** measures how well a language model can predict the next word
    in a given sequence. As a training metric, it shows how well the models learned
    its training set.'
  prefs: []
  type: TYPE_NORMAL
- en: We won’t go into the mathematical details but intuitively, **minimizing perplexity
    means maximizing the predicted probability.**
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the best model is the one that is not *surprised* when it sees
    the new text because it’s expecting it — meaning it already predicted well what
    words are coming next in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: While perplexity is helpful, it doesn’t consider the meaning behind the words
    or the context in which they are used, and it’s influenced by how we tokenize
    our data — different language models with varying vocabularies and tokenization
    techniques can produce varying perplexity scores, making direct comparisons less
    meaningful.
  prefs: []
  type: TYPE_NORMAL
- en: '**Perplexity is a useful but limited metric**. We use it primarily to track
    progress during a model’s training or to compare different versions of the same
    model. For instance, after applying quantization — a technique that reduces a
    model’s computational demands — we often use perplexity to assess any changes
    in the model’s quality.'
  prefs: []
  type: TYPE_NORMAL
- en: Perplexity is just one part of the equation — it offers valuable insights but
    doesn’t tell the whole story. ¹
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8db6afe466fbec575a24d6a365f89c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Some of the tasks for Large Language Models. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: BLEU and ROUGE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’re into Natural Language Processing, you may have heard about the **ROUGE
    and BLEU scores.**
  prefs: []
  type: TYPE_NORMAL
- en: Introduced in the early 2000s for machine translation, they quantify how close
    the machine text is to a human reference.
  prefs: []
  type: TYPE_NORMAL
- en: The **BLEU** score is the number of words in the human reference text divided
    by the total words. Similarly to the precision score, it takes values between
    zero and one, where values closer to one represent more similar texts.
  prefs: []
  type: TYPE_NORMAL
- en: '**ROUGE** works on similar principles but is a bit more complex since it analyzes
    overlap through several aspects, such as n-grams (ROUGE-N), longest common subsequences
    (ROUGE-L) and skip bigrams. (ROUGE-S)'
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to large language models, BLEU and ROUGE are used to evaluate
    how close the output is aligned to the human solution, considered correct. But
    they are not enough for every generative task. As you can imagine,producing the
    reference textcan be expensive and time-consuming and not even feasible for some
    domains or languages.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes there isn’t just one correct way to summarize or translate a text.
    These scores can only account for a few valid options.
  prefs: []
  type: TYPE_NORMAL
- en: Also, **they don’t take into consideration the context** — a text that works
    for a news article might not be the best fit for a social media post, and what’s
    suitable for a formal setting might not be appropriate for a casual one.
  prefs: []
  type: TYPE_NORMAL
- en: The need for benchmarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Open source models are usually smaller and fine-tuned to be more specialized
    for a particular task.
  prefs: []
  type: TYPE_NORMAL
- en: Meta’s founder, **Mark Zuckerberg**, thinks we’ll interact with different AI
    entities for different needs instead of relying on a general-purpose AI assistant.²
    To really understand which model best suits a particular task, we need a way to
    compare them.
  prefs: []
  type: TYPE_NORMAL
- en: '**Specific benchmarks** assess a particular aspect of a language model. For
    example, if you want to evaluate how truthful your model answers are or quantify
    how well it does on a task after fine-tuning, use a specific benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: Four of them are used in [*Hugging’s Face OpenLLM Leaderboard*](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
  prefs: []
  type: TYPE_NORMAL
- en: '**The Abstraction and Reasoning Corpus (ARC)** is an **abstract reasoning test**.
    It applies to humans and AIs and tries to measure a *human-like form of fluid
    intelligence.* Given an input grid, the user needs to choose the correct output.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/14d0a1ee6042b9276a4dc0697f9b298e.png)'
  prefs: []
  type: TYPE_IMG
- en: The ARC test interface. Language models can interact with it through JSON files.³
  prefs: []
  type: TYPE_NORMAL
- en: '**HellaSwag** is a test where the user needs to pick the best ending to a given
    context, a task called **commonsense inference**. While easy for humans, many
    LLMs struggle with this test. The only one able to reach almost human-level performance
    is GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d60eefdd92c36f9dbc64f2eeaa7b4a3.png)'
  prefs: []
  type: TYPE_IMG
- en: An example test in HellaSwag.⁴
  prefs: []
  type: TYPE_NORMAL
- en: '**Massive Multitask Language Understanding (MMLU)** measures a text model’s
    multitask accuracy on 57 tasks, including mathematics, US history, computer science,
    law, and more. The test looks like multiple choice questions on different problems
    and assesses **understanding of the world and general knowledge.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2eea9088510f8fe88d7605b22647c90d.png)'
  prefs: []
  type: TYPE_IMG
- en: Measuring Massive Multitask Language Understanding. ⁵
  prefs: []
  type: TYPE_NORMAL
- en: '**TruthfulQA** consists of two tasks: generation and multiple-choice. The generation
    task requires models to produce authentic and informative answers to questions,
    while the multiple-choice one requires models to select or assign probabilities
    to true and false answer choices. The benchmark covers 57 topics and uses various
    metrics to measure the models’ **ability to recognize false information**. Interestingly,
    the paper shows that larger models are less truthful. ⁶'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f0f66f040f86a5563b17fec762908a3b.png)'
  prefs: []
  type: TYPE_IMG
- en: The models are measured ontheir ability to recognize false information.⁶
  prefs: []
  type: TYPE_NORMAL
- en: Measuring code generation abilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When ChatGPT came out, asking it to write some code is probably the first thing
    we’ve all tried. The ability to code is one of the most useful and time-saving
    skills that LLMs can offer to us.
  prefs: []
  type: TYPE_NORMAL
- en: In the open source landscape there are many models specialized in code generation,
    like **Wizard Coder** or the most recent **Code LLama.**
  prefs: []
  type: TYPE_NORMAL
- en: 'To show the impressive coding abilities of their new **Code Llama** model,
    they chose two code-specific benchmarks: [**HumanEval**](https://github.com/openai/human-eval)and
    **Mostly Basic Python Programming (**[**MBPP**](https://github.com/google-research/google-research/tree/master/mbpp)**)**,
    complemented by a human evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: In the first, models need to generate a code starting from a docstring, while
    in the second they start from a text prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Every prompt comes with one or more unit tests to evaluate the correctness of
    the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/66183ff25f6b5e4d7f8aeeb2daf4674f.png)'
  prefs: []
  type: TYPE_IMG
- en: An MBPP entry. [Source.](https://github.com/google-research/google-research/blob/master/mbpp/sanitized-mbpp.json)
  prefs: []
  type: TYPE_NORMAL
- en: After collecting a sample of **k** entries generated by the model, the **pass@k
    metric** is computed. If at least one entry passes the unit tests, the solution
    is considered correct. For example, **a pass@1 score of 67.0 means the model can
    solve 67% of the problems at the first try.**
  prefs: []
  type: TYPE_NORMAL
- en: When computing this metric, you can use any value of **k**. But in practice,
    we are interested in the **pass@1**. If you have to keep trying to get a correct
    solution, how can you trust that model?
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation results for **Code LLama** are the following.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1041feff23bbd2b453ee4f3ef6d97865.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Llama evaluation. ⁷
  prefs: []
  type: TYPE_NORMAL
- en: Their results show that GPT-4 is the best model, able to solve 67% of the tasks
    in **HumanEval** at the first try. However, **Code Llama is the best open source
    code-specific model, with just 34B parameters.**
  prefs: []
  type: TYPE_NORMAL
- en: Measuring general intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluation systems must cover numerous scenarios, especially for larger Language
    Models designed to be general purpose because of their impressive generalization
    ability to diverse tasks.
  prefs: []
  type: TYPE_NORMAL
- en: While for classic machine learning models, you are used to evaluating the model
    using a *test set*, LLMs enable *zero-shot learning* and *few-shot learning —*
    an LLM can learn to perform a task that hasn’t been explicitly trained for. Under
    these circumstances, using a test set or a single metric to benchmark LLM’s capabilities
    is insufficient.
  prefs: []
  type: TYPE_NORMAL
- en: '**General benchmarks** are extensive collections of tests in diverse scenarios
    and tasks. They’re like the ultimate test for your model, aiming to gauge every
    aspect of intelligence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of them are the **Holistic Evaluation of Language Models (HELM), built
    to evaluate** models based on seven key metrics: accuracy, calibration and uncertainty,
    robustness, fairness, bias and stereotypes, toxicity, and efficiency, calculated
    in 16 scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/48761bd94d5f661f7a1a6fee5d8953c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Holistic Evaluation of Language Models.⁸
  prefs: []
  type: TYPE_NORMAL
- en: '**SuperGLUE**, introduced in 2019, is an advanced version of the General Language
    Understanding Evaluation (GLUE) test. The GLUE benchmark comprises nine tasks
    related to sentence or sentence-pair language understanding, all built on pre-existing
    datasets. **SuperGLUE** offers a more challenging set of tasks and a public leaderboard.'
  prefs: []
  type: TYPE_NORMAL
- en: '**BIG-bench,** from Google, expands GLUE and SuperGLUE with a more extensive
    collection of natural language understanding tasks. It is a massive collaborative
    project with contributions from 444 authors from 132 institutions worldwide. It
    assesses LLMs based on their accuracy, fluency, creativity, and generalization
    abilities on [over 200 tasks](https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/keywords_to_tasks.md#summary-table)!'
  prefs: []
  type: TYPE_NORMAL
- en: Since running BIG-bench can be very time-consuming, the authors also provide
    a lite version with a subset of 24 tasks called BIG-bench lite. Their GitHub repo
    is open for contributions and new ideas.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe51824a9ddcda3ce58ea568cafbf8a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Diversity and scale of BIG-bench tasks.⁹
  prefs: []
  type: TYPE_NORMAL
- en: Another way of evaluating language models is a manual **human evaluation**.
    As the name suggests, it measures the quality and performance of large language
    models by asking human judges to rate or compare the outputs of LLMs, like in
    **Chatbot Arena**. It’s a platform for benchmarking large language models (LLMs)
    using the Elo rating system — like chess — where users chat with two anonymized
    LLMs side-by-side and vote for the one they think is best. The votes are then
    used to calculate the ELO ratings and rank the LLMs on a leaderboard.
  prefs: []
  type: TYPE_NORMAL
- en: You can visit [their website](https://chat.lmsys.org/) and chat with different
    LLMs yourself.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5416b49cc003e19491c9faffc0905bc4.png)'
  prefs: []
  type: TYPE_IMG
- en: Chatbot Arena [leaderboard.](https://chat.lmsys.org/)
  prefs: []
  type: TYPE_NORMAL
- en: 'A case from research: Llama 2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Llama 2** is the successor to Llama. It was released in July 2023 in 7B,
    13B, 34B and 70B sizes, including a fine-tuned versions called **Llama 2 Chat.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the paper, we can find two main evaluation procedures: a **general** and
    a **safety** evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94b3490d50e78389ac1abdfb88e555e0.png)'
  prefs: []
  type: TYPE_IMG
- en: General evaluation. LLama 2 is an improvement of LLama, and scores better than
    MPT and Falcon.¹⁰
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation criteria in the authors’ work suggest that they prioritized two
    main objectives.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, **compare Llama 2 to the first version and the open source competitors.**
    To achieve that, they used a comprehensive general evaluation, where the models
    are evaluated on five dimensions: **Code, Commonsense Reasoning, World Knowledge,
    Reading Comprehension** and **Math**. Each dimension is an average of multiple
    benchmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: The results are complemented by the **MMLU, BBH (BigBench Hard), and AGI Eval
    benchmarks,** shown in separate columns.
  prefs: []
  type: TYPE_NORMAL
- en: The second objective evident in the authors’ work was to **show that their fine-tuning
    method led to a more truthful and less toxic model.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dfa33942088c45fa0d30d7d2f2ac30b6.png)![](../Images/4f049abf17b90b2e6a3eabecc334c478.png)'
  prefs: []
  type: TYPE_IMG
- en: LLMs before fine tuning (left) and after. (right) ¹⁰
  prefs: []
  type: TYPE_NORMAL
- en: The safety evaluation is aimed to assess **Truthfulness** and **Toxicity** using
    the **TruthfulQA** and **ToxiGen** benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: It shows that thanks to the fine-tuning process, **Llama 2 is less toxic than
    other models but less truthful than ChatGPT.**
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Language models have a multifaceted and flexible nature.
  prefs: []
  type: TYPE_NORMAL
- en: Open-source models offer tailored solutions, and specialization might be the
    way forward.
  prefs: []
  type: TYPE_NORMAL
- en: When comparing models, look for benchmarks relevant to your needs.
  prefs: []
  type: TYPE_NORMAL
- en: The best one isn’t necessarily the one with the lowest perplexity or highest
    BLEU score, but the one that truly adds value to your life.
  prefs: []
  type: TYPE_NORMAL
- en: '*If you enjoyed this article, join* [***Text Generation***](https://textgeneration.substack.com/)
    *— our newsletter has two weekly posts with the latest insights on Generative
    AI and Large Language Models.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Also, you can find me on* [***LinkedIn***](https://www.linkedin.com/in/driccio/)***.***'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ¹ M. Shoeybi and R. Caruana, [Language Model Evaluation Beyond Perplexity](https://arxiv.org/abs/2106.00085)
    (2023), arXiv.org
  prefs: []
  type: TYPE_NORMAL
- en: '² Lex Friedman Podcast, [Mark Zuckerberg: The Future of AI](https://www.youtube.com/watch?v=Ff4fRgnuFgQ&t=2723s)
    (2023), YouTube.'
  prefs: []
  type: TYPE_NORMAL
- en: '³ Xu, Y., Li, W., Vaezipoor, P., Sanner, S., & Khalil, E. B., [LLMs and the
    Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based
    Representations.](https://arxiv.org/abs/2305.18354) (2023), arXiv.org'
  prefs: []
  type: TYPE_NORMAL
- en: '⁴ Zellers, R. et al, [HellaSwag: Can a Machine Really Finish Your Sentence?](https://arxiv.org/abs/1905.07830)
    (2022), arXiv.org'
  prefs: []
  type: TYPE_NORMAL
- en: ⁵ Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., & Steinhardt,
    J. [Measuring Massive Multitask Language Understanding.](https://arxiv.org/abs/2009.03300)
    (2021), arXiv.org
  prefs: []
  type: TYPE_NORMAL
- en: '⁶ Lin, S., Hilton, J., & Evans, O. (2021). [TruthfulQA: Measuring How Models
    Mimic Human Falsehoods.](https://arxiv.org/abs/2109.07958) (2022), arXiv.org'
  prefs: []
  type: TYPE_NORMAL
- en: ⁷ [Introducing Code Llama, a state-of-the-art large language model for coding](https://ai.meta.com/blog/code-llama-large-language-model-coding/).
    (2023), meta.com
  prefs: []
  type: TYPE_NORMAL
- en: ⁸ Liang, P. et al (2022). [Holistic Evaluation of Language Models](https://arxiv.org/abs/2211.09110).
    (2022), arXiv.org
  prefs: []
  type: TYPE_NORMAL
- en: '⁹ Srivastava, A. et al. [Beyond the Imitation Game: Quantifying and extrapolating
    the capabilities of language models.](https://arxiv.org/abs/2206.04615) (2022),
    arXiv.org'
  prefs: []
  type: TYPE_NORMAL
- en: '¹⁰ Touvron, H. et al. (2023). [Llama 2: Open Foundation and Fine-Tuned Chat
    Models](https://arxiv.org/abs/2307.09288). (2023), arXiv.org'
  prefs: []
  type: TYPE_NORMAL
