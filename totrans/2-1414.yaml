- en: 'Leveraging LLMs with Information Retrieval: A Simple Demo'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/leveraging-llms-with-information-retrieval-a-simple-demo-600825d3cb4c](https://towardsdatascience.com/leveraging-llms-with-information-retrieval-a-simple-demo-600825d3cb4c)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A demo of integrating a Question-Answering LLM with retrieval components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@vuphuongthao9611?source=post_page-----600825d3cb4c--------------------------------)[![Thao
    Vu](../Images/9d44a2f199cdc9c29da72d9dc4971561.png)](https://medium.com/@vuphuongthao9611?source=post_page-----600825d3cb4c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----600825d3cb4c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----600825d3cb4c--------------------------------)
    [Thao Vu](https://medium.com/@vuphuongthao9611?source=post_page-----600825d3cb4c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----600825d3cb4c--------------------------------)
    ·7 min read·Aug 14, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/309014edda2c597a2486ed5abab55028.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by the author using Stable Diffusion
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLM) can store an impressive amount of factual data,
    but their capabilities are limited by the number of parameters. Furthermore, frequently
    updating LLM is expensive, while old training data can make LLM produce out-of-date
    responses.
  prefs: []
  type: TYPE_NORMAL
- en: To tackle the problem above, we can augment LLM with external tools. In this
    article, I will share how to integrate LLM with retrieval components to enhance
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval-Augmented (RA)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A retrieval component can provide the LLM with more up-to-date and precise knowledge.
    Given input ***x***, we want to predict output ***p(y|x)***. From an external
    data source ***R***, we retrieve a list of contexts ***z***=(***z_1, z_2,..,z_n)***
    relevant to ***x***. We can join **x** and ***z*** together and make full use
    of ***z***’s rich information to predict ***p(y|x,z)****.* Besides, maintaining
    **R** up-to-date is also much cheaper.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3fe2cc3cad76ee8963d0c05e7be2de8f.png)'
  prefs: []
  type: TYPE_IMG
- en: Retrieval Augmented pipeline (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: QA Demo Using Wikipedia data + ChatGPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this demo, for a given question, we do the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve Wikipedia documents related to the question.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide both the question and the Wikipedia to ChatGPT.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to compare and see how the extra context affects ChatGPT’s responses.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the Wikipedia dataset, we can extract it from [here](https://huggingface.co/datasets/wikipedia).
    I use “20220301.simple” subset with more than 200k documents. Due to the context
    length limit, I only use the title and abstract parts. For each document, I also
    add a doc id for the retrieval purpose later. So the data examples look like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We combine the title and the abstract passage and prepare them for encoding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Afterwards, we need a reliable embedding model to build our retrieval index.
    In this demo, I use the pre-trained [multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large)
    with dim=1024 to encode the docs. For faster indexing and storage efficiency,
    you can choose other small-dimension embedding models.
  prefs: []
  type: TYPE_NORMAL
- en: My first embedding model choice was the pre-trained [ALBERT](https://huggingface.co/docs/transformers/model_doc/albert),
    but the results’ quality was poor. You should do a few test cases to make sure
    your index works reasonably well before moving to the next step. To pick a good
    embedding for retrieval, you can check out [this leaderboard](https://huggingface.co/spaces/mteb/leaderboard).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ANN Index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have got our documents’ embeddings and ids list ready. The next step is to
    index them nicely for retrieval. I use the [HNSW](https://arxiv.org/pdf/1603.09320.pdf)
    index with cosine distance measurement.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Given a question, we can first go to the retrieval index and look for some
    relevant information. To avoid the wrong context, you can set a threshold for
    distance here. In this way, only relevant documents will be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ChatGPT API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So now our retrieval pipeline is ready! For the next step, let’s prepare a prompt
    to ask ChatGPT. We prepare 2 prompt formats as below, one only has the question
    and one with both the question and relevant Wikipedia text.
  prefs: []
  type: TYPE_NORMAL
- en: The **‘question’** placeholder here is the target question we want to ask ChatGPT,
    while **‘info’** is the Wikipedia document retrieved from our HNSW index.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]{question}[PRE6]{question}[PRE7]{info}[PRE8]'
  prefs: []
  type: TYPE_NORMAL
- en: Filling the placeholder with the question and corresponding Wikipedia doc, we
    can send out the request to [openai ChatCompletion API](https://platform.openai.com/docs/guides/gpt/chat-completions-api).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: I have tried sending a few requests with and without Wikipedia documents and
    see the difference.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Question**: what does the cost of living include?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**ChatGPT**: The cost of living typically includes expenses such as housing,
    food, transportation, healthcare, utilities, and other basic necessities required
    to maintain a certain standard of living in a particular location. It may also
    include taxes, insurance, and other miscellaneous expenses.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Wiki info**: [[‘Cost of living</s>Cost of living is the amount of money it
    costs just to live in a certain place. It includes food, housing, etc.’]]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**ChatGPT + wiki info**: The cost of living includes the amount of money it
    costs to live in a certain place, which encompasses expenses such as food, housing,
    and other necessary expenses.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Example 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Question**: how to make cheese?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**ChatGPT:** Cheese can be made by curdling milk with the help of an acid or
    rennet, separating the curds from the whey, and then pressing and aging the curds
    to develop flavor and texture. The specific process and ingredients used can vary
    depending on the type of cheese being made.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Wiki info:** [[‘Cheese</s>Cheese is a dairy product that is made from milk.
    There are many types of cheese, such as cheddar, Swiss, and provolone. ‘]]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**ChatGPT + wiki info:** To make cheese, you need milk as the main ingredient.
    Cheese is a dairy product that is made from milk. There are many types of cheese,
    such as cheddar, Swiss, and provolone.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Example 3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Question:** can you explain the seq2seq model?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**ChatGPT:** The seq2seq (sequence-to-sequence) model is a type of neural network
    architecture used for tasks that involve generating a sequence of output values
    from a sequence of input values. It consists of two main components: an encoder
    and a decoder. The encoder takes in the input sequence and generates a fixed-length
    vector representation of it, which is then passed to the decoder. The decoder
    uses this vector to generate the output sequence, one element at a time. The model
    is commonly used in natural language processing tasks such as machine translation,
    text summarization, and speech recognition.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Wiki info:** [[‘Computability theory</s>Computability theory is part of computer
    science. Scientists want to know what can be computed, and what can not. ‘]]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**ChatGPT + wiki info:** The seq2seq model, or sequence-to-sequence model,
    is a type of neural network architecture used for natural language processing
    tasks such as machine translation, text summarization, and conversational modeling.
    It consists of two recurrent neural networks (RNNs), an encoder and a decoder,
    that work together to transform an input sequence into an output sequence. The
    encoder processes the input sequence and produces a fixed-length vector representation,
    which is then fed into the decoder to generate the output sequence. This model
    has been successful in many NLP tasks and has become a popular choice for language
    generation. The provided additional information about computability theory is
    not directly related to the question about seq2seq model.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: At first sight, we can see that the ANN index works quite well, but our dataset
    coverage is quite small. A better method is to expand your retrieval using Google
    Search results. You can also update your retrieval index with relevant documents
    when doing QA on a specific domain.
  prefs: []
  type: TYPE_NORMAL
- en: In examples 1 and 2, when the Wikipedia page seems related, ChatGPT's answer
    seems to heavily depend on additional information. But the model may not be able
    to distinguish if the additional information is actually helpful (example 2).
    In order to avoid bad cases like this, you can add a few examples in the prompt
    to let the model know when and when not to use the extra information.
  prefs: []
  type: TYPE_NORMAL
- en: Another case is example 3, where the Wikipedia text is completely irrelevant.
    Fortunately, the answer seems not to be affected by the extra context.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code [here](https://github.com/thao9611/chatgpt_and_retrieval).
    Hope you enjoy the reading :-)
  prefs: []
  type: TYPE_NORMAL
