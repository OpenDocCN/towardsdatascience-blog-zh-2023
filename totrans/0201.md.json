["```py\nimport pandas as pd\n\nSTART_DATE = '2022-01-01'\nURL = f'https://erddap.marine.ie/erddap/tabledap/IWaveBNetwork.csv?time%2CSignificantWaveHeight&time%3E={START_DATE}T00%3A00%3A00Z&station_id=%22AMETS%20Berth%20B%20Wave%20Buoy%22'\n\ndef reading_data(url: str) -> pd.Series:\n    \"\"\"\n    Reading ERDAP data\n\n    :param url: ERDAP url as string\n    :return: hourly wave height time series as pd.Series\n    \"\"\"\n\n    # reading data directly from erdap\n    data = pd.read_csv(url, skiprows=[1], parse_dates=['time'])\n\n    # setting time to index and getting the target series\n    series = data.set_index('time')['SignificantWaveHeight']\n\n    # transforming data to hourly and from centimeters to meters\n    series_hourly = series.resample('H').mean() / 100\n\n    return series_hourly\n\nseries = reading_data(URL)\n```", "```py\nimport numpy as np\n\nclass LogTransformation:\n    \"\"\"\n    Log transformation and inverse transformation\n\n    Taking the log helps stabilize the variance\n    \"\"\"\n\n    @staticmethod\n    def transform(x):\n        xt = np.sign(x) * np.log(np.abs(x) + 1)\n\n        return xt\n\n    @staticmethod\n    def inverse_transform(xt):\n        x = np.sign(xt) * (np.exp(np.abs(xt)) - 1)\n\n        return x\n```", "```py\ndef feature_engineering(X: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    param X: lagged observations (explanatory variables)\n\n    :return: new features\n    \"\"\"\n\n    summary_stats = {'mean': np.mean, 'sdev': np.std}\n\n    features = {}\n    for f in summary_stats:\n        features[f] = X.apply(lambda x: summary_stats[f](x), axis=1)\n\n    features_df = pd.concat(features, axis=1)\n    X_feats = pd.concat([X, features_df], axis=1)\n\n    return X_feats\n```", "```py\nfrom sklearn.model_selection import train_test_split\n# https://github.com/vcerqueira/blog\nfrom src.tde import time_delay_embedding\n\n# using last 24 observations as lags, \n# and next 24 observations as the forecasting horizon\nN_LAGS, HORIZON = 24, 24\n\ntrain, test = train_test_split(series, test_size=0.2, shuffle=False)\n\nX_train, Y_train = time_delay_embedding(train, n_lags=N_LAGS, horizon=HORIZON, return_Xy=True)\nX_test, Y_test = time_delay_embedding(test, n_lags=N_LAGS, horizon=HORIZON, return_Xy=True)\n```", "```py\nfrom sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n\n## apply preprocessing steps\n# log transformation\nX_train = LogTransformation.transform(X_train)\nY_train = LogTransformation.transform(Y_train)\n# feature engineering\nX_train_ext = feature_engineering(X_train)\n\n# time series cv procedure\ntscv = TimeSeriesSplit(n_splits=5, gap=50)\n\n# defining the search space\n# a simple optimization of the number of trees of a RF\nmodel = RandomForestRegressor()\nparam_search = {'n_estimators': [10, 50, 100, 200],\n                'criterion': ['squared_error', 'absolute_error'],\n                'max_depth': [None, 2, 5, 10],\n                'max_features': ['log2', 'sqrt']}\n\n# applying CV with random search on the training data\ngs = RandomizedSearchCV(estimator=model,\n                        cv=tscv,\n                        refit=True,\n                        param_distributions=param_search,\n                        n_iter=10, n_jobs=1)\n\ngs.fit(X_train_ext, Y_train)\n```", "```py\n# applying preprocessing steps to test data\nX_test = LogTransformation.transform(X_test)\nX_test_ext = feature_engineering(X_test)\n\n# inference on test set and evaluation\npreds = gs.predict(X_test_ext)\n\n# log forecasts\npreds_log = gs.predict(X_test_ext)\n\n# reverting the log transformation\npreds = LogTransformation.inverse_transform(preds_log)\n\n# estimating performance using r-squared\nestimated_performance = r2_score(Y_test, preds)\n```", "```py\nfrom joblib import dump\n\n# preparing all available data for auto-regression\nX, Y = time_delay_embedding(series, n_lags=N_LAGS, horizon=HORIZON, return_Xy=True)\n\n# applying preprocessing steps\nX = LogTransformation.transform(X)\nY = LogTransformation.transform(Y)\nX_ext = feature_engineering(X)\n\n# model fitting\nfinal_model = RandomForestRegressor(**gs.best_params_)\nfinal_model.fit(X_ext, Y)\n\ndump(final_model, 'random_forest_v1.joblib')\n```", "```py\nimport datetime\n\n# setting the max history to yesterday\nyesterday = datetime.date.today() - datetime.timedelta(days=1)\nyesterday = yesterday.strftime('%Y-%m-%d')\n\nLIVE_URL = f'https://erddap.marine.ie/erddap/tabledap/IWaveBNetwork.csv?time%2CSignificantWaveHeight&time%3E={yesterday}T00%3A00%3A00Z&station_id=%22AMETS%20Berth%20B%20Wave%20Buoy%22'\n\n# reading the data from the ERDAP server\nnew_series = reading_data(LIVE_URL)\n\n# getting the last 24 observations needed for the model\nlags = new_series.tail(N_LAGS)\n```", "```py\nfrom joblib import load\n\n# structuring the lags as a DataFrame\nlags_df = pd.DataFrame(lags).T.reset_index(drop=True)\nlags_df.columns = X.columns\n\n# applying preprocessing steps\nlags_df = LogTransformation.transform(lags_df)\nlags_feats = feature_engineering(lags_df)\n\n# loading the model from disk\nfinal_model = load('random_forest_v1.joblib')\n\n# applying the model\nlog_forecasts = final_model.predict(lags_feats)\n\n# reverting the log transformation\nforecasts = LogTransformation.inverse_transform(log_forecasts)\n```"]