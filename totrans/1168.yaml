- en: How to Do Data Validation on Your Data on Pandas with pytest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-do-data-validation-on-your-data-on-pandas-with-pytest-d5dda51ad0e4](https://towardsdatascience.com/how-to-do-data-validation-on-your-data-on-pandas-with-pytest-d5dda51ad0e4)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: DATA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Implementing basic data validation on your processed DataFrames with Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://byrondolon.medium.com/?source=post_page-----d5dda51ad0e4--------------------------------)[![Byron
    Dolon](../Images/9ff32138c7b1913be24cc7ab971752b0.png)](https://byrondolon.medium.com/?source=post_page-----d5dda51ad0e4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d5dda51ad0e4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d5dda51ad0e4--------------------------------)
    [Byron Dolon](https://byrondolon.medium.com/?source=post_page-----d5dda51ad0e4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d5dda51ad0e4--------------------------------)
    ·10 min read·May 26, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0ef97c2f7836a1f5e61f73ce2556434.png)'
  prefs: []
  type: TYPE_IMG
- en: Image used with permission by my talented sister [ohmintyartz](https://www.instagram.com/ohmintyartz/)
  prefs: []
  type: TYPE_NORMAL
- en: 'Working with data at scale for machine learning is exciting, but there’s an
    important step you shouldn’t forget before you even begin thinking about training
    a model: **data validation**.'
  prefs: []
  type: TYPE_NORMAL
- en: Data validation refers to verifying if the data you collect and transform is
    correct and usable. It would help if you always made sure that the data you end
    up using in a machine learning model or data analysis is ending up as you expect.
  prefs: []
  type: TYPE_NORMAL
- en: This process can involve starting by checking to see if your raw data fits your
    expectations (like how the source you’re collecting from defines the output) to
    checking that your data processing functions are working as expected.
  prefs: []
  type: TYPE_NORMAL
- en: We can take a look at how you can implement basic testing and data validation
    in Python using **pytest** on a data processing pipeline with Pandas. We’ll start
    by taking a look at an initial set of data processing functions and then how we
    can implement some tests to ensure that our processing functions and data are
    behaving as expected.
  prefs: []
  type: TYPE_NORMAL
- en: You can follow along in a notebook or IDE of your own. You can download the
    dataset from Kaggle [here](https://www.kaggle.com/datasets/deepanshuverma0154/sales-dataset-of-ecommerce-electronic-products?resource=download),
    available free for use under the CC0 1.0 Universal (CC0 1.0) Public Domain Dedication
    license.
  prefs: []
  type: TYPE_NORMAL
- en: Intro — An initial data processing and simple data validation with Pytest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code we’ll be working with in this piece is this set of Python functions
    that use Pandas to read in and process data. It includes a function to read the
    raw data in chunks, then a few functions that perform some transformations on
    the raw data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can get started with implementing our first data validation test.
    If you’re going to follow along in a notebook or IDE, you should import the following
    in a new file (or in another cell in your notebook):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can read more on how to actually run pytest (naming conventions for files
    and how tests are discovered [here](https://docs.pytest.org/en/7.1.x/explanation/goodpractices.html#test-discovery),
    but for our case, all you need to do is create a new file called `test_data_processing.py`
    and in your IDE as you add to the file you just can run `pytest` and optionally
    with “- -verbose”.
  prefs: []
  type: TYPE_NORMAL
- en: Quick Introduction to pytest and Simple Data Validation Check
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pytest is a testing framework in Python that makes it easy for you to write
    tests for your data pipelines. You can primarily make use of the assert statement,
    which essentially checks if a condition you place after `assert` evaluates to
    True or False. If it evaluates to False, it will raise an exception `AssertionError`
    (and when used with pytest will cause the test to fail).
  prefs: []
  type: TYPE_NORMAL
- en: So first, let’s test something simple. All we’re going to do is check if the
    output of one of our functions (the first one to read the raw data) returns a
    DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: As a quick aside, you’ll notice in the original function we write the arrow
    `->` syntax to add type hints to the function where we say that the function should
    return a DataFrame. This means that if you write in your function to return something
    other than a DataFrame, your IDE will flag it as returning an invalid output (but
    this won’t technically break your code or prevent it from running).
  prefs: []
  type: TYPE_NORMAL
- en: To actually check if the function returns a DataFrame, we’ll implement a function
    to test the `read_raw_data` function and just call it `test_read_raw_data`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this function, we add a one-line docstring to explain that our test function
    is just checking if the output is a DataFrame. Then, we assign the output of the
    existing `read_raw_data` function to a variable and use `isinstance` to return
    True or False if the specified object is of the type you put in. In this case,
    we check if the `test_df` is a `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can similarly do this for the rest of our functions that just take a DataFrame
    as input and are expected to return a DataFrame as output. Implementing it can
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that you can also use the `assert` statement in a for loop, so we just
    go through each of the functions, passing in a DataFrame as input and checking
    to see if the output is also a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing fixtures in pytest for more efficient testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can see above that we had to write the exact same line twice in our two
    different test functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This is because for both test functions, we needed a DataFrame as input for
    our test to check if the output of our data processing functions resulted in a
    DataFrame. So you can avoid copying the same code in all your test functions,
    you can use fixtures, which let you write some code that **pytest will let you
    reuse in your different tests.** Doing so looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We define the `test_df` in a function this time that returns the raw DataFrame.
    Then, in our test functions, we just include `test_df` as a parameter and we can
    use it just as we did before.
  prefs: []
  type: TYPE_NORMAL
- en: Testing if a function transforms a DataFrame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, let’s get into checking our `split_purchase_address` function, which
    essentially outputs the same DataFrame passed as input but with additional address
    columns. Our test function will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we’ll check two main things:'
  prefs: []
  type: TYPE_NORMAL
- en: Does the output DataFrame have more columns than the original DataFrame?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the output DataFrame have a different index than the original DataFrame?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, we run the `split_purchase_address` function, passing the `test_df` as
    input and assigning the result to a new variable. This gives us the output of
    the original function that we can then test.
  prefs: []
  type: TYPE_NORMAL
- en: To actually do the test, we could check if a specific column exists in the output
    DataFrame, but a simpler (not necessarily better) way of doing it is just checking
    if the output DataFrame has more columns than the original with the `assert` statement.
    Similarly, we can `assert` if the length of the index for each of the DataFrames
    is the same.
  prefs: []
  type: TYPE_NORMAL
- en: You can also check the [Pandas testing documentation](https://pandas.pydata.org/docs/reference/testing.html)
    for some built-in testing functions, but there are only a few functions that essentially
    just check if two of a DataFrame, index, or Series are equal. We use the `assert_index_equal`
    function to do the same thing that we do with the `index.__len__()`.
  prefs: []
  type: TYPE_NORMAL
- en: Testing if a DataFrame has a specific column in the output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned before, we can also check if a DataFrame contains a specific column.
    We’ll move on to the next function `extract_product_pack_information` which should
    always output the original DataFrame with an additional column called “Pack Information”.
    Our test function will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Here, all we do is call `columns` again on the output of the original function,
    but this time check specifically if the “Pack Information” column is in the list
    of columns. If for some reason we edited our original `extract_product_pack_information`
    function to return additional columns or renamed the output column, this test
    would fail. This would be a good reminder to check if what whatever we used the
    final data for (like a machine learning model) also took that into account.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could then make do two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Make changes downstream in our code pipeline (like code that refers to the “Pack
    Information” column);
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Edit our tests to reflect the changes in our processing function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Testing to see if a DataFrame’s columns are the correct data types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another thing we should be doing is checking to see if the DataFrame returned
    by our functions has columns of our desired data types. For example, if we’re
    doing calculations on numerical columns, we should see if the columns are returned
    as an `int` or `float`, depending on what we need.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s test data types on our `one_hot_encode_product_column` function, where
    we do a common step in feature engineering on one of the categorical columns in
    the original DataFrame. We expect all the columns to be of the `uint8` DataType
    (what the `get_dummies` function in Pandas returns by default), so we can test
    that like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The output of the `get_dummies` function also returns columns that have an underscore
    (this, of course, could be done better by checking the actual column names- like
    in the previous test function we check for specific columns).
  prefs: []
  type: TYPE_NORMAL
- en: Here, all we’re doing is in a for loop of target columns checking if all of
    them are of the `np.dtype("uint8")` data type. I checked this previously by just
    in a notebook checking the data type of one of the output columns like `column.dtype`.
  prefs: []
  type: TYPE_NORMAL
- en: Final additional data validation on the final output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another good practice in addition to testing the individual functions you have
    that make up your data processing and transformation pipelines is testing the
    final output of your pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: To do so, we’ll simulate running our entire pipeline in the test, and then check
    the resulting DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Our final `test_process_raw_data` will check for two final things:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Checking if the original columns are still present in the final DataFrame
    —** this isn’t always a requirement, but it might be that you want all the raw
    data to still be available (and not transformed) in your output. Doing so is simple-
    we just need to check if the column in the `test_df` is still present in the `processed_df`.
    Finally, we can this time raise an `AssertionError` (similarly to just using an
    `assert` statement) if the column is not present. This is a nice example of how
    you can output a specific message in your tests when needed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Checking if the final DataFrame doesn’t have any duplicates** — there are
    a lot of different ways you can do this- in this case, we’re just using the “Order
    ID” (which we expect to be like an index) and the `assert_series_equal` to see
    if the output DataFrame didn’t generate any duplicate rows.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Checking the pytest output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For a quick look at what running pytest looks like, in your IDE just run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Pytest will check the new test file with all the test functions and run them!
    This is a simple implementation of having a series of data validation and testing
    checks on your data processing pipeline. If you run the above, the output should
    look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc15c89fd7a8569aad10610a272e65fe.png)'
  prefs: []
  type: TYPE_IMG
- en: The output of pytest 1 — image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9683f3c7842a24fec75c9ff210e4bf82.png)'
  prefs: []
  type: TYPE_IMG
- en: The output of pytest 2— image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4329338c6ab7af6d989a113c829a207c.png)'
  prefs: []
  type: TYPE_IMG
- en: The output of pytest 3— image by Author
  prefs: []
  type: TYPE_NORMAL
- en: You can see that our final test failed, specifically the part of the test where
    we check if all of the columns from the initial DataFrame are present in the final.
    Also that our custom error message in the `AssertionError` we defined earlier
    is populating correctly—that the “Product” column from our original DataFrame
    is not showing up in the final DataFrame (see if you can find why based on our
    initial data processing functions).
  prefs: []
  type: TYPE_NORMAL
- en: There’s a lot more room to improve on this testing—we just have a really simple
    implementation with basic testing and data validation cases. For more complex
    pipelines, you may want to have a lot more testing both for your individual data
    processing functions, as well as on your raw and final output DataFrames to ensure
    that the data you end up using is data you can trust.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for taking the time to read this piece! If you enjoy my content I’d love
    it if you sign up for Medium using my referral link below. This’ll let me get
    a portion of your monthly subscription AND you’ll get access to some exclusive
    features only for Medium members. And if you’re already following me, thanks a
    bunch for your support!
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://byrondolon.medium.com/membership?source=post_page-----d5dda51ad0e4--------------------------------)
    [## Join Medium with my referral link — Byron Dolon'
  prefs: []
  type: TYPE_NORMAL
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: byrondolon.medium.com](https://byrondolon.medium.com/membership?source=post_page-----d5dda51ad0e4--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: M**ore by me:** *-* [*5 Practical Tips for Aspiring Data Analysts*](https://byrondolon.medium.com/5-practical-tips-for-aspiring-data-analysts-9917006d4dae?sk=019edbddaca4d313665caafe4b747d26)
    *-* [*Mastering Ecommerce Data Analysis*](https://python.plainenglish.io/mastering-analysis-for-e-commerce-with-pandas-e4219a87b10c?sk=9aa8fd1024b89e89e4b0904c8d00d242)
    *-* [*Check for a Substring in a Pandas DataFrame*](/check-for-a-substring-in-a-pandas-dataframe-column-4b949f64852?sk=bfb5bbab11ae45c47bfb316d931c3b56)
    *-* [*7 Best Repositories on Github to Learn Python*](https://medium.com/towards-data-science/top-7-repositories-on-github-to-learn-python-44a3a7accb44)
    *-* [*5 (and a half) Lines of Code for Understanding Your Data with Pandas*](/5-and-a-half-lines-of-code-for-understanding-your-data-with-pandas-aedd3bec4c89?sk=7007a1ae248cf7ea4ef5fcd4af7ae72b)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
