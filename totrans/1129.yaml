- en: How to Combine the Forecasts of an Ensemble
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-combine-the-forecasts-of-an-ensemble-11022e5cac25](https://towardsdatascience.com/how-to-combine-the-forecasts-of-an-ensemble-11022e5cac25)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Using dynamic forecasting ensembles to cope with changes in the time series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://vcerq.medium.com/?source=post_page-----11022e5cac25--------------------------------)[![Vitor
    Cerqueira](../Images/9e52f462c6bc20453d3ea273eb52114b.png)](https://vcerq.medium.com/?source=post_page-----11022e5cac25--------------------------------)[](https://towardsdatascience.com/?source=post_page-----11022e5cac25--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----11022e5cac25--------------------------------)
    [Vitor Cerqueira](https://vcerq.medium.com/?source=post_page-----11022e5cac25--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----11022e5cac25--------------------------------)
    ·6 min read·Jan 19, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67181e5ede0e2d3fce12e999390d23fe.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Chris Lawton](https://unsplash.com/@chrislawton?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In a [previous article](https://medium.com/towards-data-science/introduction-to-forecasting-ensembles-f63877a2498),
    we explored the main steps for building ensembles.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this post:'
  prefs: []
  type: TYPE_NORMAL
- en: we dive deeper into forecasting ensembles;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we discuss how ensembles combine many forecasts;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we explore dynamic weighted averages for forecast combination;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we apply a dynamic ensemble to a case study using Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Combining the predictions of many models [improves forecasting performance](https://medium.com/towards-data-science/introduction-to-forecasting-ensembles-f63877a2498).
    These approaches can be further improved with dynamic combination rules.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to build a forecasting ensemble. Yet, standard approaches
    do not consider the dynamic nature of time series.
  prefs: []
  type: TYPE_NORMAL
- en: In time series, things evolve over time due to the non-stationarities at play.
    For instance, different regimes or seasonal effects. These cause variability in
    relative performance. Different forecasting models perform better in different
    periods.
  prefs: []
  type: TYPE_NORMAL
- en: This variability in performance should be reflected when combining forecasts.
    At each time step, each model should be weighted according to how you expect them
    to perform.
  prefs: []
  type: TYPE_NORMAL
- en: Ensembles that adapt their weights over time are called dynamic.
  prefs: []
  type: TYPE_NORMAL
- en: From Static to Dynamic Ensembles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/1edf7cc5fb60759baec0bc9479694c58.png)'
  prefs: []
  type: TYPE_IMG
- en: A time series and the forecasts of several models. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Constant weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bagging, boosting, chaining, or stacking are a few examples of ensembles.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging or boosting ensembles combine individual forecasts using a simple average.
    Thus, all models in the ensemble have equal weights. In stacking or chaining,
    the weights are not equal. The best combination rule is learned during training.
  prefs: []
  type: TYPE_NORMAL
- en: In any of these cases, the combination rule is static. The weight of each model
    is the same at all times.
  prefs: []
  type: TYPE_NORMAL
- en: But, constant weights fail to adapt to changes in the time series. This issue
    is solved by dynamic combination rules.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/44a91be05899b8dabcc5509fedf48fb7.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jan Huber](https://unsplash.com/@jan_huber?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Ensembles with dynamic weights can cope with changes in time series.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge becomes estimating which models are stronger in a given instant.
    Turns out, this is a thorny task. But, many approaches are available to solve
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dynamic ensembles fall into three possible categories:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Windowing: the weights are computed based on model performance in a window
    of past recent data. For example, you can compute the rolling squared error. Then,
    at each instant, you get the weights by normalizing the error scores. You can
    find [an implementation of Windowing in my Github](https://github.com/vcerqueira/blog/blob/main/src/ensembles/windowing.py);'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regret minimization: Some methods attempt to minimize a metric called regret.
    Examples include the exponentially weighted average, the polynomially weighted
    average, or the fixed share aggregation. The second chapter of the book in reference
    [2] describes these methods in detail. You can also find their implementation
    in the R package *opera***.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meta-learning: Other techniques learn and predict the weights of each model
    for a given instant. [Arbitrating is an example of such a method.](https://github.com/vcerqueira/blog/blob/main/src/ensembles/ade.py)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case Study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the rest of this article, we’ll create a dynamic forecasting ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use a time series related to energy demand. Here’s how it looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/58f5548e5e820f13acd9d9ddd48210fa.png)'
  prefs: []
  type: TYPE_IMG
- en: Half-hourly electricity demand time series. Source in reference [3]. Image by
    author.
  prefs: []
  type: TYPE_NORMAL
- en: Building the Ensemble
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We start by transforming the time series for supervised learning. This is done
    using time delay embedding. You can check [my previous post](https://medium.com/towards-data-science/machine-learning-for-forecasting-transformations-and-feature-extraction-bbbea9de0ac2)
    to learn more about this approach.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we build the ensemble. Here’s the script to do this. Check the comments
    in the code for more context.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ve built an ensemble composed of five models: a Random Forest, a KNN, a
    LASSO, a Ridge, and an Elastic-net.'
  prefs: []
  type: TYPE_NORMAL
- en: In the script above, we also store the training predictions of each model in
    a pd.DataFrame. In the first few instances, there’s little information for weighing
    the different models. So, the training predictions are useful for warm-starting
    the weight estimates.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic Combination Rule
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After building the ensemble, we’re ready to make a dynamic combination rule.
    We focus on two methods: windowing, and arbitrating. Besides these, we also include
    a static approach based on the simple average of forecasts.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The final forecast is a weighted average. This is done by multiplying the weights
    with the individual forecasts, followed by a sum operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the evolution of the weights of each model according to the windowing
    approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb8f4119b81b917e59ce6819b8e9e475.png)'
  prefs: []
  type: TYPE_IMG
- en: The weights of each model along the time series. A higher weight means greater
    importance in the final forecast. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Early on, the Random Forest (RF) is the most relevant model. But, Ridge becomes
    more important in the final part of the series.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the final error of each approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4776ef9dbd38594397356bbb34ec6c1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Mean absolute error of each method. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The ensembles perform better than the individual models. Especially the dynamic
    ones, which are better than taking a simple average of forecasts.
  prefs: []
  type: TYPE_NORMAL
- en: Take Aways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Different forecasting models perform better in distinct periods;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a dynamic ensemble, the weights of different models change over time. So,
    these approaches can cope with changes in the time series;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In each time step, the weights should reflect how well we expect models to perform;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are several ways to build dynamic ensembles. Often, these can be categorized
    into either windowing, regret minimization, or meta-learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hope you found this one useful! Thanks for reading, and see you in the next
    story!
  prefs: []
  type: TYPE_NORMAL
- en: Related Article
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Introduction to Forecasting Ensembles](https://medium.com/towards-data-science/introduction-to-forecasting-ensembles-f63877a2498)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Opera R package: [https://pbil.univ-lyon1.fr/CRAN/web/packages/opera/opera.pdf](https://pbil.univ-lyon1.fr/CRAN/web/packages/opera/opera.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Cesa-Bianchi, N., & Lugosi, G. (2006). Prediction, Learning, and Games.
    New York, NY, USA: Cambridge University Press.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [taylor time series: Half-hourly electricity demand](https://www.rdocumentation.org/packages/forecast/versions/8.9/topics/taylor)
    (License: GPL-3)'
  prefs: []
  type: TYPE_NORMAL
