# 从损失函数的角度来看，决策树如何分裂节点

> 原文：[https://towardsdatascience.com/how-decision-trees-split-nodes-from-loss-function-perspective-60a2f2124b4e](https://towardsdatascience.com/how-decision-trees-split-nodes-from-loss-function-perspective-60a2f2124b4e)

## 了解决策树如何通过分裂节点来最小化其损失函数

[](https://jasonweiyi.medium.com/?source=post_page-----60a2f2124b4e--------------------------------)[![魏毅](../Images/24b7a438912082519f24d18e11ac9638.png)](https://jasonweiyi.medium.com/?source=post_page-----60a2f2124b4e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----60a2f2124b4e--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----60a2f2124b4e--------------------------------) [魏毅](https://jasonweiyi.medium.com/?source=post_page-----60a2f2124b4e--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----60a2f2124b4e--------------------------------) ·阅读时间 12 分钟 ·2023年5月15日

--

![](../Images/de317baa764ffb421a758d3453dbd4f5.png)

图片由 [Ernest Brillo](https://unsplash.com/@ernest_brillo?utm_source=medium&utm_medium=referral) 提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

在谈论决策树节点分裂时，我经常听到诸如“方差减少”和“信息增益最大化”的术语。我总是对这些术语感到恐惧。这些并不是我日常用词中熟悉的内容，直到我意识到这些术语是“最小化决策树损失函数”的同义词。

哪种损失函数？嗯，每个机器学习模型都需要一个损失函数。损失函数量化了模型预测与实际值之间的*差异*。另一种说法是模型预测中的*误差*。决策树模型也不例外。

对我来说，理解损失函数是理解机器学习模型的最重要一步。这是因为损失函数通过一个单一的数字编码了我们希望模型在各个方面表现良好的标准。就像我们用一个单一的数字来量化一个人的优秀程度；我记得那是某种长度的度量，对吧，眨眼，眨眼？

由于损失函数衡量的是树的预测与实际值之间的差异，为了理解损失函数，我们首先需要了解决策树是如何进行预测的。我们从回归树开始，然后再转到分类树。

# 回归树如何进行预测

回归决策树的预测形式是一个连续值。例如，一个回归树根据一个人的年龄和性别来预测其薪资。这里有一个小数据集：

![](../Images/649ffed19c626688ad7efad936a08972.png)

这是一个用于回归树的示例数据集，以薪资作为目标变量，作者提供

我们希望我们的回归树预测*目标变量*薪资，它是一个连续数量，因此得名“回归树”，由两个*特征*组成，年龄是连续的，性别是分类的。对于薪资，我使用*y₁*到*y₇*而不是实际数字，以突出我们不应知道其他人的薪资。让*sₒ={y₁, y₂, y₃, y₄, y₅, y₆, y₇}*表示所有薪资数据条目。

回归树使用一些条件元组（特征，值，比较）将数据分成两部分。请注意，决策树总是进行二分拆分。

以下树使用拆分条件元组（年龄，41，<）将完整数据集*s₀*拆分成两个子集*s1={y₁, y₃, y₅}*和*s2={y₂, y₄, y₆, y₇}*。元组（年龄，41，<）表示树使用条件“年龄 < 41”将当前根节点（目前是完整数据集）中的所有数据点拆分成两个不重叠的子集。

我*可视化*了这个拆分，将子集*s1*放在左支，将*s2*放在右支。注意，拆分条件元组（年龄，41，<）仅为说明目的，可能并非示例数据集的最佳选择。

![](../Images/a99c3756d4a82ae0ad3f80d8e490c79d.png)

作者提供的回归树，拆分条件为年龄 < 41

## 回归树通过取平均值来进行预测

这个小的回归树通过一个拆分为一个数据点的薪资预测如下：

如果该人年龄小于41岁，他将进入左支，即子集*s1*；模型随后预测的数量是*ŷₛ₁*，即子集*s1*中薪资的平均值：

![](../Images/13d5134b826e8011bec819644e396971.png)

如果该人年龄大于或等于41岁，他将进入右支，即子集*s2*；模型预测的数量是*ŷₛ₂*，即子集*s2*中薪资的平均值：

![](../Images/463e80026d9b07a4976f9b5108d7b5fa.png)

换句话说，对于一个数据点，回归树的预测方法是：

1.  首先让数据点通过树，直到到达叶节点

1.  收集目标变量值，以便用于位于该叶节点的训练数据子集

1.  使用这些目标值的平均值作为预测。

# 回归树的损失函数

由于回归树的预测是一个连续数量，最明显的损失函数选择是测量该预测与实际值之间误差的平方损失，或等效的均方误差（MSE）。由于有两个分支，我们分别定义它们的损失项。

对于子集*s1*：

![](../Images/3d483275c0143a369d527432d6912afb.png)

对于子集*s2*：

![](../Images/b36bcbd419fdd7e184553bb8647209b0.png)

其中 *|s1|* 表示子集 *s1* 的大小，即3，*|s2|* 表示子集 *s2* 的大小，即4。⅓和¼的标准化项将两个子集的大小考虑到损失函数中。因此，*Lₛ₁* 和 *Lₛ₂* 是 *每数据点* 的平均损失。

整棵树的总体损失是这两个损失项的总和：

![](../Images/0e620014efe55ab107c7839e2355233f.png)

*Lₛ₁* 和 *Lₛ₂* 每数据点损失的事实意味着我们不需要在总损失 *L* 前面加上系数来考虑 *s1* 和 *s2* 的大小差异。子集大小差异已由标准化常数⅓和¼考虑。

# 为决策树寻找分裂条件

为了从数据中学习模型，或者说，优化模型，我们改变模型参数，使得模型的预测误差尽可能小。换句话说，模型的预测应该最小化上述损失函数 *L*。

在回归树中，每次分支时，模型参数是分裂条件元组，如（年龄，40，<）。决定分裂树的元组的算法包括以下三个步骤：

## 1\. 确定适合的分裂条件元组子集

观察数据中的特征，并在所有可能的分裂条件中找到候选分裂条件：

+   如果特征是连续的，比如年龄，则查看此特征的范围并决定一个短列表的分裂值。例如，数据中的年龄范围从20到70，则短列表的分裂值可以是30、40、50、60。这给出以下四个元组（年龄，30，<）、（年龄，40，<）、（年龄，50，<）和（年龄，60，<）。对于连续特征，可以有无限多个分裂值，算法只能使用启发式方法选择一个*小子集*。这些子集可能不是最优的，但这是模型精度和计算之间的折衷。

+   如果特征是类别型的，比如性别，则使用它的值作为分裂值。这给出两个元组（性别，男，=），（性别，女，=），对于这个二元特征，简化为一个元组（性别，男，=）。

所以我们总共有五个分裂条件元组。

## 2\. 使用分裂条件元组对数据进行分区并评估损失

根据每个分裂条件元组对数据集进行分区，并评估损失函数 *L*。在这个例子中，我们有五个元组，因此我们根据每个元组将数据分为五次，评估五次损失函数，每次基于数据的不同分区。

## 3\. 选择最小损失的分裂条件元组

现在，算法选择能产生最小损失的分裂条件元组，并将这个条件下的两个分区数据子集放入由分裂创建的两个叶节点中。

然后它将每个叶节点视为新树的根，并对每棵新树应用相同的三个步骤进行进一步分裂。

# 量化模型预测的改进

这就是回归树如何拆分节点——它拆分节点以最小化其损失函数，即均方损失。拆分后，模型可以在预测产生更小误差的意义上进行更好的预测。模型之所以能做到这一点，是因为它从数据集中提取了信息。为了量化拆分前后提取的信息量，请计算整个数据集 s₀ 上的损失，公式为

![](../Images/c02aedc33c091b54b6e8af3cf511bca9.png)

然后对两者进行差分：

![](../Images/3e5c46e4f03eeff12b03379ec00d76e3.png)

# 方差减少

你有时会在回归树拆分的讨论中听到“方差减少”这个短语。这是因为如果我们查看树的预测和相应的损失，如下所示，仅为左分支：

预测：

![](../Images/13d5134b826e8011bec819644e396971.png)

损失：

![](../Images/3d483275c0143a369d527432d6912afb.png)

你会发现，对于一系列数字 *y₁, y₃, y₅*，回归树的预测 *ŷₛ₁* 是这些数字的均值，通常记作 *ȳ*。均方误差 *Lₛ₁* 具有与计算该数字列表的[方差](https://en.wikipedia.org/wiki/Variance)相同的结构：

![](../Images/eac424ac0616db88959295d9a3ee6236.png)

在我们的例子中，*n=3*。因此，上述拆分算法通过最小化损失，或者说，减少方差来实现目标。这就是为什么我们说回归树通过减少方差来拆分节点。

现在转到分类树，它以分类值的形式进行预测，而不是连续值。你知道怎么做的。首先我们展示分类树如何进行预测，然后定义损失函数，最后看看上述拆分算法如何像应用于回归树一样应用于分类树。

# 分类树如何进行预测

首先通过交换之前数据集中性别和薪资列来创建一个分类任务的数据集：

![](../Images/bc3f7344feeaed1084dfabcb3eaf7ecf.png)

用于分类树的示例数据集，目标变量为性别，作者

在这里，目标变量是性别（Gender），它是一个分类变量，因此我们需要一个分类树。两个特征是年龄（Age）和薪资（Salary）。

假设我们有以下分类树：

![](../Images/577a8fb1f2f3356b294d3122b0d7d017.png)

拥有拆分条件 Age<41 的分类树，作者

这个树需要将一个数据点分类到女性类别（Female）或男性类别（Male）。假设这个树仍然使用条件 Age < 41 来将数据集 *s₀* 拆分成 *s1={y₁F, y₃F, y₅M}* 和 *s2={y₂M, y₄F, y₆M, y₇F}*。我在每个数据点旁边附上性别符号“F”或“M”，例如“*y1F”*，以使每个数据点的性别在达到叶节点时更加清晰。

所以在分裂后，到达左叶节点的⅔数据点是女性，用*P(F)=⅔*表示，这说明该节点中女性数据点的*发生概率*是⅔。类似地，对于男性，它是*P(M)=⅓*。

## 分类树通过多数投票来做出预测

我们需要一种不同于平均的方法（如回归树中的情况），因为⅔的女性加上*⅓*的男性不是一个实际的情况，尽管近年来LGBT运动积极推进。

更好的选择是*多数投票*——预测发生概率最高的类别。对于左节点，由于女性数据点多于男性数据点，模型将预测女性类别。

那右侧叶节点呢，其中女性和男性数据点的概率相等，都是½？在这种情况下，当两个类别的概率相等时，模型可以随机打破平局，例如预测男性类别。实际上，分裂算法会防止平局情况的发生，正如我们稍后将看到的那样。

# 分类树的损失函数

让我们来处理量化树的错误的损失函数。直观地说，在一个叶节点中，如果主要类别的发生概率更大，则模型对该节点的预测更好：

1.  我们的树对左节点预测为女性，这一预测在⅔的情况下是正确的，而在*⅓*的情况下是错误的。换句话说，如果一个节点中不同类别的分布**不够均匀**，预测会更好。

1.  它对右节点预测为男性，这一预测仅在½的情况下是正确的。换句话说，如果节点中不同类别的分布**更均匀**，预测效果会更差。

所以我们需要一个衡量数据点类别标签均匀性的指标。存在许多度量方法。常见的选择是熵：

![](../Images/6da67e07df847c987a8bca41fbcdc20b.png)

其中求和是对所有类别*c*，在我们的案例中是女性和男性。*P(c)* 是类别*c*的发生概率，而*log(P(c))* 是该发生概率的对数。

请注意，熵是衡量均匀性的一个指标；还有其他常见的衡量方法，如卡方和基尼 impurity。回想一下，在经济学中，基尼指数用于量化社会中财富在不同人群之间分配的不均等性——它是一个不均匀性的度量。

如果你想知道*为什么*熵可以衡量均匀性，请参阅其他[资源](https://stats.stackexchange.com/questions/66935/measure-for-the-uniformity-of-a-distribution)。在本文中，只需知道熵可以衡量均匀性就足够了。为了好玩，我们可以插入一些数字来看看熵是否告诉我们左分支比右分支更均匀。

对于左侧叶节点：

![](../Images/c1831178dade93e44d71cee03b637f82.png)

对于右侧叶节点：

![](../Images/9daf06f993310e002d1c3cadf860b354.png)

我们可以看到，左叶节点的损失*Hₛ₁=0.637*确实小于右叶节点的损失*Hₛ*₂=0.693，这表明模型对左节点的数据点的预测更好。

现在，就像回归树的情况一样，我们需要将*Hₛ₁*和*Hₛ*₂结合起来，以表示整个树的总体损失。

在回归树的情况下，我们将*Lₛ₁*和*Lₛ*₂相加，因为*Lₛ₁*和*Lₛ*₂代表每个数据点的损失，并且不同子集的大小*s1*和*s2*经过标准化。但是在分类树的情况下，上述基于熵的损失项并非如此。如果你查看*Hₛ₁*和*Hₛ*₂的定义，子集s1和s2的大小并没有提到。

从直观上看，当模型对大量数据子集的预测产生错误时，即使这个错误很小，对整体模型性能指标的影响也可能很大，因为数据子集的规模很大。

为了考虑数据子集的大小，我们将总损失H定义为加权和，权重根据数据子集的比例确定：

![](../Images/5bac29b5ba8f09b9a75a22db5a77c1f7.png)

# 寻找分类树的分裂条件

使用为分类树定义的损失函数*H*，我们可以应用用于回归树的相同算法来分裂节点。我们只需要使用分类的新损失*H*，而不是回归的损失*L*。

这很有意义，因为加权熵*H*表示我们分类树的错误——越均匀，错误越大；越不均匀，错误越小。上述分裂算法会选择最小化这种错误的分裂条件，从而鼓励每个叶节点的数据点变得更少均匀。因此，树可以减少错误。同时，通过在分裂叶节点中鼓励较少均匀的数据子集，不太可能出现两个类别之间的平局。

太神奇了！

# 量化模型的预测改进

就像回归树的情况一样，你可以通过计算分裂前的根节点*Hₛ₀*上的损失来衡量数据在分裂前后的信息提取，然后计算*H-Hₛ₀*。

# 信息增益

在结束之前，我想提到信息增益，因为你会在讨论分类树时到处看到它。

![](../Images/62a8cde34c07890200b246f66b772a3b.png)

你可以看到信息增益实际上是熵的负值。因此，我们可以等效地找到一个最大化信息增益的分裂条件，而不是最小化熵损失的分裂条件。信息增益公式中的常数1在分裂算法中没有影响。如果算法找到一个最大化*-熵*的分裂条件，那么这个条件也会最大化信息增益，即*1-熵*。

# 结论

本文解释了决策树，包括回归树和分类树，从损失函数的角度如何进行节点分裂。这种视角使得如方差减少、信息增益等概念变得更加直观易懂。
