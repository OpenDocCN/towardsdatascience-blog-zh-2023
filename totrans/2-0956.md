# 强化学习中的广义优势估计

> 原文：[https://towardsdatascience.com/generalized-advantage-estimation-in-reinforcement-learning-bf4a957f7975](https://towardsdatascience.com/generalized-advantage-estimation-in-reinforcement-learning-bf4a957f7975)

## 策略梯度中的偏差与方差权衡

[](https://siwei-causevic.medium.com/?source=post_page-----bf4a957f7975--------------------------------)[![Siwei Causevic](../Images/bb8e4ec911272a8e381e94129eabe166.png)](https://siwei-causevic.medium.com/?source=post_page-----bf4a957f7975--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bf4a957f7975--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bf4a957f7975--------------------------------) [Siwei Causevic](https://siwei-causevic.medium.com/?source=post_page-----bf4a957f7975--------------------------------)

·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----bf4a957f7975--------------------------------) ·6分钟阅读·2023年3月27日

--

![](../Images/f6ba99d09255dd7c15926e13695c4125.png)

作者提供的照片

策略梯度方法是强化学习中最广泛使用的学习算法之一。它们旨在优化参数化策略，并使用价值函数来帮助估计如何改进策略。

强化学习中的一个主要问题，特别是对于策略梯度方法，是行动与其对奖励的正面或负面影响之间的长时间延迟，这使得奖励估计极为困难。也就是说，强化学习研究者通常用从回合中获得的引导奖励或价值函数来估计长期奖励（回报），有时两者都会用。然而，这两种方法都有其缺点。前者的问题在于样本的高方差，后者则是在估计的价值函数中的高偏差。

在这篇文章中，我们将讨论广义优势估计（GAE），这是一类策略梯度估计器，它在保持可接受的偏差水平的同时，显著减少了方差。

以下内容假设你对策略梯度方法有基本了解。如果你是强化学习的新手，请查看我之前关于[强化学习基础和算法概述](/an-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af)的文章，以及[对普通策略梯度的深入分析](https://medium.com/towards-data-science/policy-gradient-reinforce-algorithm-with-baseline-e95ace11c1c4)。

# 偏差与方差权衡

回忆一下我们在[普通策略梯度](https://medium.com/towards-data-science/policy-gradient-reinforce-algorithm-with-baseline-e95ace11c1c4)中讨论的策略梯度的一般形式：

![](../Images/79d444a751fc692dff9a1b470a12455e.png)

目标是找到使策略最大化*V(θ)*的参数*θ*。为此，我们通过沿着策略的梯度上升来寻找*V(θ)*的最大值，关于参数θ。

![](../Images/a9021ed8605d1743d9651e3c97f3be6d.png)

上述函数是普通策略梯度，完全依赖于回报R(τ)，即轨迹τ的奖励总和：

![](../Images/bbe90f8b7eddc6cd83e56781f6ef7a0b.png)

由于R(τ)是从许多采样轨迹中估计得到的，普通的策略梯度方法容易受到高方差的影响，为了解决这个问题，研究人员发现了几种以更稳定的方式估计奖励的方法。

让我们将上述值函数扩展为逐步形式：

![](../Images/115e5db734c525465df60a1e3ffc3c28.png)

Ψt是奖励的通用表示，可以是以下之一：

![](../Images/0a235de60f591aa182c7f49b912b6ae2.png)

选项1和2完全依赖于采样奖励，而选项3则从奖励中减去基线。然而，它们在学习过程中仍然会受到高方差的困扰。实际上，选择Ψt = Aπ (st , at )已被证明能够产生最低的方差。这里，优势函数定义为：

![](../Images/994002b6fb52321a308b1d72bdaa3b3b.png)

它衡量动作是否比策略的默认行为更好或更差。注意，选择5和6在学习是在线策略时是等效的。在实践中，优势函数是不知道的，必须进行估计，这使得估计器有偏。GAE进一步通过引入额外参数γ来折扣优势函数。我们将在下一节中深入讨论。

# 什么是广义优势估计（GAE）

在优势估计器的基础上，GAE引入了一个参数γ，使我们可以通过降低对应于延迟效应的奖励的权重来减少方差。当然，这会引入偏差。这与贝尔曼方程中的折扣因子的思想类似，折扣因子降低了远期奖励的优先级。使用折扣后，优势函数表示为：

![](../Images/bb79b7b22b2a25ec223aa77da8f61cad.png)

在实践中，我们需要估计值函数，这通常使用一个神经网络来预测特定状态的值（如果我们想估计Q值，则包括动作）。我们定义折扣γ的TD残差δt。注意，δt可以视为优势Ψt的估计：

![](../Images/3cef11a0258c45f1bea391bbfe0d6ce1.png)

回顾我们之前讨论的时序差分（TD）方法[在这里](/an-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af)。在这里，我们将Q值替换为值函数（更多细节请参阅贝尔曼方程）。折扣优势可以表示为：

![](../Images/d3c4edd7155307faf9b5be89f8ab1570.png)

在 TD 中，我们通过决定采样步骤的数量来权衡偏差和方差。采样的步骤越多，我们对带有偏差的价值函数估计器的依赖就越小，但方差也会增加。一个棘手的问题是找到 TD 中带来理想偏差-方差折衷的甜蜜点。这就是 GAE 发挥作用的地方——我们不再通过经验测试不同的步骤大小，而是直接使用这些 k 步估计器的指数加权平均。下面的方程展示了不同步骤大小 (k) 下的折扣优势函数。

![](../Images/c224f5dd68334e41deb577ce056a1a4e.png)

采用指数加权平均，我们得到了**GAE 的最终形式**：

![](../Images/a4630df2a6027da15cd7128ddbaebc75.png)

请注意，我们在这里引入了另一个参数 λ。当 λ = 0 时，GAE 本质上与 TD(0) 相同，但应用于策略优化的背景下。由于 heavily 依赖于估计的价值函数，因此具有较高的偏差。当 λ = 1 时，这就是带有基线的原始策略梯度，由于项的和，具有较高的方差。

![](../Images/2637df51d54142760925bc8ca4e1f651.png)

实际应用中，我们设置 0 < λ < 1 来控制偏差和方差之间的折衷，就像 TD λ 中的 lambda 参数一样。

# 奖励塑形——GAE 的另一种解释

另一种解读 GAE 的方式是将环境视为奖励重塑的 MDP。假设我们有一个转换的奖励函数：

![](../Images/e678fe9daf4876e7824d39f90120e588.png)

转换奖励的折扣和（即回报）正是我们上述讨论的 TD 残差：

![](../Images/5eccb8b1084d6d0c1bfd1d34536f0672.png)

本质上，策略梯度和最优策略保持不变，但我们的目标是最大化折扣奖励的总和。我们还可以增加一个“更陡”的折扣 λ，其中 0 ≤ λ ≤ 1，我们可以得到最终的 GAE 形式：

![](../Images/0fddf47a1b65500ebf0a3da8221837fb.png)

总结一下，我们可以将 GAE 视为奖励重塑 MDP 中相同的策略梯度，用一个陡峭的折扣 γλ 来削减来自长延迟的噪声。

# 结论

在本文中，我们讨论了 RL 中的偏差和方差折衷，特别是在策略梯度方法中。然后我们介绍了 GAE，一种通过以较小的偏差代价来降低方差的技术。GAE 是一个非常重要的估计器，并在许多高级算法中得到广泛应用，包括 VPG、TRPO 和 PPO。

在此，我想感谢你阅读本文，任何反馈都将不胜感激。
