- en: Decision Science Meets Design
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/decision-science-meets-design-fb30eaa0ded9](https://towardsdatascience.com/decision-science-meets-design-fb30eaa0ded9)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Dive into Solving Generative Design Problems Through Deep Reinforcement Learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://houssameeddinehsain.medium.com/?source=post_page-----fb30eaa0ded9--------------------------------)[![Houssame
    E. Hsain](../Images/2f7295dfb473f66e419fabd32ab12ffa.png)](https://houssameeddinehsain.medium.com/?source=post_page-----fb30eaa0ded9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fb30eaa0ded9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fb30eaa0ded9--------------------------------)
    [Houssame E. Hsain](https://houssameeddinehsain.medium.com/?source=post_page-----fb30eaa0ded9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fb30eaa0ded9--------------------------------)
    ·9 min read·Oct 27, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d61041140cffb1f009f720dbad3a024.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Photo by [Igor Omilaev](https://unsplash.com/@omilaev?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: The design process has changed dramatically over the last few decades. What
    was once a domain driven by human intuition, judgment, and aesthetic preferences
    is now augmented by computational methods and data-driven processes. This transition
    is exemplified by the intersection of data science and design, a crossroads where
    precision meets creativity.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: The utility of data-driven techniques in design is well demonstrated within
    its subdomain, generative design, a methodology that uses computational algorithms
    to produce multiple design variations based on predefined criteria. However, as
    these design problems become more intricate and multidimensional, there’s a need
    for more sophisticated techniques to find satisfactory solutions. This is where
    decision science, specifically reinforcement learning, comes into play.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Applying Decision Science to Generative Design
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The core of design is not so much creation as it is the series of purposeful
    decisions that lead to that creation.
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Decision science is fundamentally about making informed choices by assessing
    the options available within a context based on their predicted or known consequences.
    It entails quantitative statistical approaches combined with optimization processes.
    When applied to generative design, decision science can help determine the design
    decisions or sequences of decisions that would improve a certain configuration
    or design instance. This process would require three components:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluating Designs:** assessing the performance or quality of each variation
    to understand the contribution of each design choice towards the desired outcome'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization:** synthesizing the sequence of design choices that would yield
    feasible and satisfactory design variants'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scenario Analysis:** exploring various design possibilities by making design
    decisions in different contexts and constraints'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Framing Generative Design Problems as Markov Decision Processes (MDPs)
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/82d388b8a96fd57365c57bfd3041e40e.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Simple Markov decision process (Illustration by author)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Before delving deeper into deep reinforcement learning (DRL) for generative
    design, framing these design problems in the context of Markov Decision Processes
    (MDPs) is vital. But what is an MDP?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'MDPs are a mathematical framework for modeling decision-making in settings
    where outcomes are partially determined by probabilistic dynamics and partly by
    the actions of the decision-maker. It is comprised of the following major components:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '**States (S):** represent different scenarios or conditions.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actions (A):** represent choices available in each state.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transitions (P):** represent the probability of moving from one state to
    another after taking an action.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rewards (R):** represent feedback or outcome from taking an action in a state.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the context of generative design, we can think of states as design configurations,
    actions as design modifications, and transitions as the likelihood of a design
    modification leading from one initial design configuration to another. The reward,
    on the other hand, is the feedback that conveys the performance measure of the
    design instance to the designer and guides the whole design process.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Tackling Generative Design with Deep Reinforcement Learning (DRL)
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/fafd35d14026f963d15ef5f8b192abd6.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: Reinforcement learning training closed-loop (Illustration by author)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning (RL) aims to learn the best strategy of action to perform
    a task through a trial-and-error process. The agent, which in our context is an
    adaptive design policy, learns from the environment by taking actions (modifying
    designs) and receiving rewards or penalties based on the outcomes (efficiency
    or performance).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: The challenge arises when dealing with large state and action spaces in design
    problems. This is where deep learning, specifically deep reinforcement learning
    (DRL), becomes invaluable. DRL combines the decision-making capabilities of RL
    with the powerful function approximation of deep learning. In simpler terms, it
    uses neural networks to predict the best actions to take in large and complex
    design scenarios.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'DRL in Action: Optimizing Building Placement on Topography'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/93fcbaaf2d5ae4ac2dc9a7ba36078ebe.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: Placing building masses on topography and calculating the required cut-and-fill
    volume (Animation by author)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Consider the challenge of constructing a building on uneven terrain. One of
    the considerations that a designer might have is to situate the building so that
    the earthmoving (cutting and filling) is minimized. Every possible position within
    the terrain represents an action, and the resulting cut-and-fill volume represents
    the reward (or a penalty in this case).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑在不平坦地形上建造建筑物的挑战。设计师可能需要考虑将建筑物放置在减少土方（挖掘和填充）量的地方。地形中的每一个可能位置代表一个动作，产生的挖填量代表奖励（或在这种情况下的惩罚）。
- en: We’ll go through a workflow to showcase how DRL agents can be trained to place
    buildings on topography while minimizing the cut and fill volume required.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个工作流程展示如何训练DRL代理在地形上放置建筑物，同时最小化所需的挖填量。
- en: '***Defining the Observation and Action Spaces***'
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***定义观察和动作空间***'
- en: 'We first define the DRL agent''s action space. The agent controls three parameters
    of the building mass: its x and y coordinates and its rotation angle (theta).
    We use a 3-dimensional discrete action space representation. As for the observation
    space, an image frame of the terrain with the position of the building on it is
    used to represent the state of our environment.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义了DRL代理的动作空间。代理控制建筑质量的三个参数：其x和y坐标以及旋转角度（theta）。我们使用3维离散动作空间表示。至于观察空间，使用包含建筑位置的地形图像帧来表示我们环境的状态。
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***Reward Function***'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***奖励函数***'
- en: The agent’s primary objective is to minimize the earthmoving required to construct
    the building. For this, the reward function penalizes the agent according to the
    cut and fill volume.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的主要目标是最小化建造建筑物所需的土方。为此，奖励函数根据挖填量对代理进行惩罚。
- en: 'The agent receives a penalty value equivalent to the cut and fill volume necessary
    to place the building at each step. In the multi-building setting, there is an
    additional -5 penalty if the building mass intersects with any previously positioned
    building masses during the training episode. The reward signal is computed at
    each step in the [Rhinoceros 3D Grasshopper](https://en.wikipedia.org/wiki/Grasshopper_3D)
    environment according to the following code:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 代理在每一步都会收到一个等同于放置建筑所需的挖填量的惩罚值。在多建筑设置中，如果建筑质量在训练过程中与任何之前定位的建筑质量相交，还会有额外的-5惩罚。奖励信号在[Rhinoceros
    3D Grasshopper](https://en.wikipedia.org/wiki/Grasshopper_3D)环境中根据以下代码计算：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***Connection between Agent and Environment***'
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***代理与环境之间的连接***'
- en: With the observation and action spaces and reward function established, it is
    necessary to facilitate the interaction between the DRL agent and the Grasshopper
    simulation environment. This is orchestrated using sockets, a popular method for
    inter-process communication.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立了观察和动作空间以及奖励函数之后，有必要促进DRL代理与Grasshopper模拟环境之间的互动。这是通过使用套接字进行协调的，这是一种流行的进程间通信方法。
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '***DRL Actor Critic Model Definition***'
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***DRL演员评论家模型定义***'
- en: 'After defining various utility functions for communication, we define and then
    initialize the DRL model and the ADAM optimizer for training:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了各种通信实用函数后，我们定义并初始化DRL模型和ADAM优化器用于训练：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The agent architecture is defined within the GRUpolicy class. It is an actor-critic
    architecture. The actor provides a probability distribution over actions given
    a state, and the critic estimates the value of that state, which is the expected
    return starting from that state and following the agent’s policy.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 代理架构在GRUpolicy类中定义。它是一种演员-评论家架构。演员在给定状态下提供动作的概率分布，而评论家估计该状态的价值，即从该状态开始并遵循代理策略的期望回报。
- en: '***Agent Training***'
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***代理训练***'
- en: Once the socket connection between the agent and the environment is defined,
    the model architecture is implemented, and an instance of the model is initialized,
    we are ready to train the DRL agent to place building masses on topography properly.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了代理与环境之间的套接字连接，模型架构得以实现，并且模型的一个实例被初始化，我们就准备好训练DRL代理以正确地在地形上放置建筑物质量。
- en: 'The core of the experiment is the training loop. Here, the agent interacts
    with the environment iteratively over several training episodes, following these
    steps:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 实验的核心是训练循环。在这里，代理在多个训练回合中反复与环境互动，遵循以下步骤：
- en: Once the agent receives the current state observation, it selects an action
    based on its current policy.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦代理收到当前状态观察，它会根据其当前策略选择一个动作。
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The agent then sends the action to Grasshopper and gets the resulting reward
    and new state.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体随后将动作发送到Grasshopper，并获得结果奖励和新状态。
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Then, it updates its policy based on the episodic reward it receives.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，它会根据接收到的阶段性奖励更新其策略。
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This process continues for several training iterations until the agent converges
    on an optimal placement of the building mass that minimizes the terrain cut and
    fill volume. This iterative trial-and-error learning process, guided by reward
    feedback, results in optimized design configurations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程会持续进行几个训练迭代，直到智能体收敛到一个最优的建筑质量布置，以最小化地形切割和填充体积。这个由奖励反馈指导的迭代试错学习过程，结果产生了优化的设计配置。
- en: '![](../Images/464ffcb7261b0fa82b7ddcabbd266cfe.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/464ffcb7261b0fa82b7ddcabbd266cfe.png)'
- en: DRL agent converges to the building position that minimizes the necessary cut-and-fill
    volume. The gradient colouring in the terrain represents its slope (Animation
    by Author)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: DRL智能体收敛到一个最小化必要切割和填充体积的建筑位置。地形中的渐变着色代表了其坡度（动画作者提供）
- en: Through this experiment, you can see how DRL agents can be tailored to address
    real-world design challenges. This approach to generative design, enabled by DRL,
    presents promising avenues for future exploration at the intersection of data
    science and design.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个实验，你可以看到DRL智能体如何被调整以应对现实世界中的设计挑战。这种由DRL驱动的生成设计方法，展示了数据科学与设计交汇处未来探索的有希望的途径。
- en: '*All implementations and details on this experiment, including implementations
    of the environment in Grasshopper and its associated Rhinoceros 3D files, are
    available in the* [*CutnFill_DeepRL*](https://github.com/houssameehsain/CutnFill_DeepRL)
    *repository on GitHub.*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有关于这个实验的实现和细节，包括在Grasshopper中的环境实现以及相关的Rhinoceros 3D文件，都可以在GitHub上的* [*CutnFill_DeepRL*](https://github.com/houssameehsain/CutnFill_DeepRL)
    *库中找到。*'
- en: Conclusion
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: By framing generative design problems as MDPs and harnessing the power of deep
    reinforcement learning, we can explore design spaces more extensively, evaluate
    designs more objectively, and optimize them more efficiently. Computational techniques
    like DRL are becoming more common in design practice. The convergence of data
    science and design promises a future in which aesthetically pleasing and rigorously
    informed design proposals are autonomously generated and evaluated, leading to
    a quick and effective iterative design process where humans and machines collaborate
    to produce intelligent design solutions.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将生成设计问题框架设为MDP，并利用深度强化学习的力量，我们可以更广泛地探索设计空间，更客观地评估设计，并更有效地优化它们。像DRL这样的计算技术在设计实践中变得越来越普遍。数据科学与设计的融合预示着一个未来，在这个未来中，美观且经过严谨信息化的设计方案将被自动生成和评估，从而实现一个快速有效的迭代设计过程，其中人类和机器协作，产生智能设计解决方案。
