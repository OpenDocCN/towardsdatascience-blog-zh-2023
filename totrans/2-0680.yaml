- en: Decision Science Meets Design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/decision-science-meets-design-fb30eaa0ded9](https://towardsdatascience.com/decision-science-meets-design-fb30eaa0ded9)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Dive into Solving Generative Design Problems Through Deep Reinforcement Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://houssameeddinehsain.medium.com/?source=post_page-----fb30eaa0ded9--------------------------------)[![Houssame
    E. Hsain](../Images/2f7295dfb473f66e419fabd32ab12ffa.png)](https://houssameeddinehsain.medium.com/?source=post_page-----fb30eaa0ded9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fb30eaa0ded9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fb30eaa0ded9--------------------------------)
    [Houssame E. Hsain](https://houssameeddinehsain.medium.com/?source=post_page-----fb30eaa0ded9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fb30eaa0ded9--------------------------------)
    ·9 min read·Oct 27, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d61041140cffb1f009f720dbad3a024.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Igor Omilaev](https://unsplash.com/@omilaev?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The design process has changed dramatically over the last few decades. What
    was once a domain driven by human intuition, judgment, and aesthetic preferences
    is now augmented by computational methods and data-driven processes. This transition
    is exemplified by the intersection of data science and design, a crossroads where
    precision meets creativity.
  prefs: []
  type: TYPE_NORMAL
- en: The utility of data-driven techniques in design is well demonstrated within
    its subdomain, generative design, a methodology that uses computational algorithms
    to produce multiple design variations based on predefined criteria. However, as
    these design problems become more intricate and multidimensional, there’s a need
    for more sophisticated techniques to find satisfactory solutions. This is where
    decision science, specifically reinforcement learning, comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: Applying Decision Science to Generative Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The core of design is not so much creation as it is the series of purposeful
    decisions that lead to that creation.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Decision science is fundamentally about making informed choices by assessing
    the options available within a context based on their predicted or known consequences.
    It entails quantitative statistical approaches combined with optimization processes.
    When applied to generative design, decision science can help determine the design
    decisions or sequences of decisions that would improve a certain configuration
    or design instance. This process would require three components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluating Designs:** assessing the performance or quality of each variation
    to understand the contribution of each design choice towards the desired outcome'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization:** synthesizing the sequence of design choices that would yield
    feasible and satisfactory design variants'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scenario Analysis:** exploring various design possibilities by making design
    decisions in different contexts and constraints'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Framing Generative Design Problems as Markov Decision Processes (MDPs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/82d388b8a96fd57365c57bfd3041e40e.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple Markov decision process (Illustration by author)
  prefs: []
  type: TYPE_NORMAL
- en: Before delving deeper into deep reinforcement learning (DRL) for generative
    design, framing these design problems in the context of Markov Decision Processes
    (MDPs) is vital. But what is an MDP?
  prefs: []
  type: TYPE_NORMAL
- en: 'MDPs are a mathematical framework for modeling decision-making in settings
    where outcomes are partially determined by probabilistic dynamics and partly by
    the actions of the decision-maker. It is comprised of the following major components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**States (S):** represent different scenarios or conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actions (A):** represent choices available in each state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transitions (P):** represent the probability of moving from one state to
    another after taking an action.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rewards (R):** represent feedback or outcome from taking an action in a state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the context of generative design, we can think of states as design configurations,
    actions as design modifications, and transitions as the likelihood of a design
    modification leading from one initial design configuration to another. The reward,
    on the other hand, is the feedback that conveys the performance measure of the
    design instance to the designer and guides the whole design process.
  prefs: []
  type: TYPE_NORMAL
- en: Tackling Generative Design with Deep Reinforcement Learning (DRL)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/fafd35d14026f963d15ef5f8b192abd6.png)'
  prefs: []
  type: TYPE_IMG
- en: Reinforcement learning training closed-loop (Illustration by author)
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning (RL) aims to learn the best strategy of action to perform
    a task through a trial-and-error process. The agent, which in our context is an
    adaptive design policy, learns from the environment by taking actions (modifying
    designs) and receiving rewards or penalties based on the outcomes (efficiency
    or performance).
  prefs: []
  type: TYPE_NORMAL
- en: The challenge arises when dealing with large state and action spaces in design
    problems. This is where deep learning, specifically deep reinforcement learning
    (DRL), becomes invaluable. DRL combines the decision-making capabilities of RL
    with the powerful function approximation of deep learning. In simpler terms, it
    uses neural networks to predict the best actions to take in large and complex
    design scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'DRL in Action: Optimizing Building Placement on Topography'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/93fcbaaf2d5ae4ac2dc9a7ba36078ebe.png)'
  prefs: []
  type: TYPE_IMG
- en: Placing building masses on topography and calculating the required cut-and-fill
    volume (Animation by author)
  prefs: []
  type: TYPE_NORMAL
- en: Consider the challenge of constructing a building on uneven terrain. One of
    the considerations that a designer might have is to situate the building so that
    the earthmoving (cutting and filling) is minimized. Every possible position within
    the terrain represents an action, and the resulting cut-and-fill volume represents
    the reward (or a penalty in this case).
  prefs: []
  type: TYPE_NORMAL
- en: We’ll go through a workflow to showcase how DRL agents can be trained to place
    buildings on topography while minimizing the cut and fill volume required.
  prefs: []
  type: TYPE_NORMAL
- en: '***Defining the Observation and Action Spaces***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We first define the DRL agent''s action space. The agent controls three parameters
    of the building mass: its x and y coordinates and its rotation angle (theta).
    We use a 3-dimensional discrete action space representation. As for the observation
    space, an image frame of the terrain with the position of the building on it is
    used to represent the state of our environment.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '***Reward Function***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The agent’s primary objective is to minimize the earthmoving required to construct
    the building. For this, the reward function penalizes the agent according to the
    cut and fill volume.
  prefs: []
  type: TYPE_NORMAL
- en: 'The agent receives a penalty value equivalent to the cut and fill volume necessary
    to place the building at each step. In the multi-building setting, there is an
    additional -5 penalty if the building mass intersects with any previously positioned
    building masses during the training episode. The reward signal is computed at
    each step in the [Rhinoceros 3D Grasshopper](https://en.wikipedia.org/wiki/Grasshopper_3D)
    environment according to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '***Connection between Agent and Environment***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: With the observation and action spaces and reward function established, it is
    necessary to facilitate the interaction between the DRL agent and the Grasshopper
    simulation environment. This is orchestrated using sockets, a popular method for
    inter-process communication.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '***DRL Actor Critic Model Definition***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'After defining various utility functions for communication, we define and then
    initialize the DRL model and the ADAM optimizer for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The agent architecture is defined within the GRUpolicy class. It is an actor-critic
    architecture. The actor provides a probability distribution over actions given
    a state, and the critic estimates the value of that state, which is the expected
    return starting from that state and following the agent’s policy.
  prefs: []
  type: TYPE_NORMAL
- en: '***Agent Training***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Once the socket connection between the agent and the environment is defined,
    the model architecture is implemented, and an instance of the model is initialized,
    we are ready to train the DRL agent to place building masses on topography properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The core of the experiment is the training loop. Here, the agent interacts
    with the environment iteratively over several training episodes, following these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Once the agent receives the current state observation, it selects an action
    based on its current policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The agent then sends the action to Grasshopper and gets the resulting reward
    and new state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Then, it updates its policy based on the episodic reward it receives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This process continues for several training iterations until the agent converges
    on an optimal placement of the building mass that minimizes the terrain cut and
    fill volume. This iterative trial-and-error learning process, guided by reward
    feedback, results in optimized design configurations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/464ffcb7261b0fa82b7ddcabbd266cfe.png)'
  prefs: []
  type: TYPE_IMG
- en: DRL agent converges to the building position that minimizes the necessary cut-and-fill
    volume. The gradient colouring in the terrain represents its slope (Animation
    by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Through this experiment, you can see how DRL agents can be tailored to address
    real-world design challenges. This approach to generative design, enabled by DRL,
    presents promising avenues for future exploration at the intersection of data
    science and design.
  prefs: []
  type: TYPE_NORMAL
- en: '*All implementations and details on this experiment, including implementations
    of the environment in Grasshopper and its associated Rhinoceros 3D files, are
    available in the* [*CutnFill_DeepRL*](https://github.com/houssameehsain/CutnFill_DeepRL)
    *repository on GitHub.*'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By framing generative design problems as MDPs and harnessing the power of deep
    reinforcement learning, we can explore design spaces more extensively, evaluate
    designs more objectively, and optimize them more efficiently. Computational techniques
    like DRL are becoming more common in design practice. The convergence of data
    science and design promises a future in which aesthetically pleasing and rigorously
    informed design proposals are autonomously generated and evaluated, leading to
    a quick and effective iterative design process where humans and machines collaborate
    to produce intelligent design solutions.
  prefs: []
  type: TYPE_NORMAL
