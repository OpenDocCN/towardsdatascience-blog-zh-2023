["```py\npip install transformers\n```", "```py\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('roberta-base')\n```", "```py\ntext = \"hello world\"\n\ntokenizer.encode_plus(\n    text,\n    truncation=True,\n    add_special_tokens=True,\n    max_length=10,\n    padding='max_length'\n    )\n```", "```py\n{\n  'input_ids': [0, 42891, 232, 2, 1, 1, 1, 1, 1, 1], \n  'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n}\n```", "```py\nfrom transformers import AutoModel\n\nrobertaBase = AutoModel.from_pretrained(\"roberta-base\")\n```", "```py\nimport pytorch.nn as torch\n\nisinstance(robertaBase, nn.Module)\n>> True\n```", "```py\nprint(robertaBase)\n\n>> RobertaModel(\n  (embeddings): RobertaEmbeddings(\n    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n    (position_embeddings): Embedding(514, 768, padding_idx=1)\n    (token_type_embeddings): Embedding(1, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): RobertaEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): RobertaPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)\n```", "```py\nmodel_name = \"roberta-base\"\nlast_hidden_layer_size = 768\nfinal_node_size = 1\n\nclass ToxicRankModel(nn.Module):\n\n    def __init__(self, model_name, last_hidden_layer_size):\n        super(ToxicRankModel, self).__init__()\n        self.robertaBase = AutoModel.from_pretrained(model_name)\n        self.dropout = nn.Dropout(p=0.1)\n        self.rank_head = nn.Linear(last_hidden_layer_size, 1)\n\n    def forward(self, ids, mask):        \n        output = self.robertaBase(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        output = self.dropout(output[1])\n        score= self.fc(output)\n        return score\n\n#This line check if the GPU is available, else it goes with the CPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n#After initiation, we send the model to the device\ntoxicRankModel = ToxicRankModel(model_name, last_hidden_layer_size)\ntoxicRankModel = toxicRankModel.to(device)\n```", "```py\nclass CustomDataset(Dataset):\n    def __init__(self, train_df, tokenizer, max_length):\n\n        #token list standard size\n        self.length = max_length\n\n        #Here the tokenizer will be an instance of the tokenizer\n        #shown previously\n        self.tokenizer = tokenizer\n\n        #df is the training df shown in the beginning of the article\n        self.more_toxic = train_df['more_toxic'].values\n        self.less_toxic = train_df['less_toxic'].values\n\n    def __len__(self):\n        return len(self.more_toxic)\n\n    def __getitem__(self, i):\n        # get both messages at index i\n        message_more_toxic = self.more_toxic[i]\n        message_less_toxic = self.less_toxic[i]\n\n        #tokenize the messages\n        dic_more_toxic = self.tokenizer.encode_plus(\n                                message_more_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.length,\n                                padding='max_length'\n                            )\n        dic_less_toxic = self.tokenizer.encode_plus(\n                                message_less_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.length,\n                                padding='max_length'\n                            )\n\n        #extract tokens and masks\n        tokens_more_toxic = dic_more_toxic['input_ids']\n        mask_more_toxic = dic_more_toxic['attention_mask']\n\n        tokens_less_toxic = dic_less_toxic['input_ids']\n        mask_less_toxic = dic_less_toxic['attention_mask']\n\n        #return a dictionnary of tensors\n        return {\n            'tokens_more_toxic': torch.tensor(tokens_more_toxic, dtype=torch.long),\n            'mask_more_toxic': torch.tensor(mask_more_toxic, dtype=torch.long),\n            'tokens_less_toxic': torch.tensor(tokens_less_toxic, dtype=torch.long),\n            'mask_less_toxic': torch.tensor(mask_less_toxic, dtype=torch.long),\n        }\n```", "```py\ndef get_loader(df, tokenizer, max_length, batch_size):\n\n    dataset = CustomDataset(\n        df, \n        tokenizer=tokenizer, \n        max_length=max_length\n    )\n\n    return DataLoader(\n        dataset, \n        batch_size=batch_size, \n        shuffle=True,\n        drop_last=True)\n\nmax_length = 128\nbatch_size = 32\ntrain_loader = get_loader(train_df, tokenizer, max_length, batch_size=batch_size)\n```", "```py\nfrom torch.nn import MarginRankingLoss\n\n#Custom implementation of the MarginRankingLoss with y = 1\nclass CustomMarginRankingLoss(nn.Module):\n    def __init__(self, margin=0):\n        super(CustomMarginRankingLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, x1, x2):\n        #with y=1 this is how looks the loss\n        loss = torch.relu(x2 - x1 + self.margin)\n        return loss.mean()\n\ndef criterion(x1, x2):\n    return CustomMarginRankingLoss()(x1, x2)\n```", "```py\noptimizer_lr = 1e-4\noptimizer_weight_decay = 1e-6\noptimizer = AdamW(toxicRankModel.parameters(), \n                  lr=optimizer_lr, \n                  weight_decay=optimizer_weight_decay)\n```", "```py\nscheduler_T_max = 500\nscheduler_eta_min = 1e-6\nscheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=scheduler_T_max, eta_min=scheduler_eta_min)\n```", "```py\ndef train_one_epoch(model, optimizer, scheduler, dataloader, device):\n\n    #Setup train mode, this is important as some layers behave differently\n    # during train and inference (like batch norm)\n    model.train()\n\n    #Initialisation of some loss\n    dataset_size = 0\n    running_loss = 0.0\n    running_accuracy = 0.0\n\n    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Training\")\n\n    for i, data in progress_bar:\n        more_toxic_ids = data['tokens_more_toxic'].to(device, dtype = torch.long)\n        more_toxic_mask = data['mask_more_toxic'].to(device, dtype = torch.long)\n        less_toxic_ids = data['tokens_less_toxic'].to(device, dtype = torch.long)\n        less_toxic_mask = data['mask_less_toxic'].to(device, dtype = torch.long)\n\n        batch_size = more_toxic_ids.size(0)\n\n        #Forward pass both inputs in the model\n        x1 = model(more_toxic_ids, more_toxic_mask)\n        x2 = model(less_toxic_ids, less_toxic_mask)\n\n        #Compute margin ranking loss\n        loss = criterion(x1, x2)\n        accuracy_measure = (x1 > x2).float().mean().item()\n\n        #apply backpropagation, increment optimizer\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        optimizer.zero_grad()\n\n        #Update cumulative loss for monitoring\n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n\n        epoch_loss = running_loss / dataset_size\n\n        running_accuracy += (accuracy_measure * batch_size)\n        epoch_accuracy = running_accuracy / dataset_size\n\n        progress_bar.set_postfix({'loss': epoch_loss, 'accuracy': epoch_accuracy}, refresh=True)        \n\n    #Garbage collector\n    gc.collect()\n\n    return epoch_loss\n```", "```py\nclass CustomInferenceDataset(Dataset):\n    def __init__(self, messages, tokenizer, max_length):\n\n        #token list standard size\n        self.length = max_length\n\n        #Here the tokenizer will be an instance of the tokenizer\n        #shown previously\n        self.tokenizer = tokenizer\n\n        #df is the training df shown in the beginning of the article\n        self.messages = messages\n\n    def __len__(self):\n        return len(self.messages)\n\n    def __getitem__(self, i):\n        # get a message at index i\n        message = self.messages[i]\n\n        #tokenize the message\n        dic_messages = self.tokenizer.encode_plus(\n                                message,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.length,\n                                padding='max_length'\n                            )\n\n        #extract tokens and masks\n        tokens_message = dic_messages['input_ids']\n        mask_message = dic_messages['attention_mask']\n\n        #return a dictionnary of tensors\n        return {\n            'tokens_message': torch.tensor(tokens_message, dtype=torch.long),\n            'mask_message': torch.tensor(mask_message, dtype=torch.long),\n        }\n\ndef get_loader_inference(messages, tokenizer, max_length, batch_size):\n\n    dataset = CustomInferenceDataset(\n        messages, \n        tokenizer=tokenizer, \n        max_length=max_length\n    )\n\n    return DataLoader(\n        dataset, \n        batch_size=batch_size, \n        shuffle=False,\n        drop_last=False)\n```", "```py\n@torch.no_grad()\ndef get_scores(model, test_loader, device):\n    model.eval()  # Set the model to evaluation mode\n    ranks = []  # List to store the rank scores\n\n    progress_bar = tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Scoring\")\n\n    for i, data in progress_bar:\n        tokens_message = data['tokens_message'].to(device, dtype=torch.long)\n        mask_message = data['mask_message'].to(device, dtype=torch.long)\n\n        # Forward pass to get the rank scores\n        rank = model(tokens_message, mask_message)\n        # Convert tensor to NumPy and add to the list\n        ranks+=list(rank.cpu().numpy().flatten())\n\n    return ranks\n```"]