- en: Building PCA from the Ground Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/building-pca-from-the-ground-up-434ac88b03ef](https://towardsdatascience.com/building-pca-from-the-ground-up-434ac88b03ef)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Supercharge your understanding of Principal Component Analysis with a step-by-step
    derivation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://harrisonfhoffman.medium.com/?source=post_page-----434ac88b03ef--------------------------------)[![Harrison
    Hoffman](../Images/5eaa3e2bd0507297eb6c4a7efcf06324.png)](https://harrisonfhoffman.medium.com/?source=post_page-----434ac88b03ef--------------------------------)[](https://towardsdatascience.com/?source=post_page-----434ac88b03ef--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----434ac88b03ef--------------------------------)
    [Harrison Hoffman](https://harrisonfhoffman.medium.com/?source=post_page-----434ac88b03ef--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----434ac88b03ef--------------------------------)
    ·12 min read·Aug 7, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09f3176416e889b15a24ea18524b299a.png)'
  prefs: []
  type: TYPE_IMG
- en: Hot air balloons. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis (PCA) is an [old](https://en.wikipedia.org/wiki/Principal_component_analysis#History:~:text=PCA%20was%20invented%20in%201901)
    technique commonly used for dimensionality reduction. Despite being a well-known
    topic among data scientists, the derivation of PCA is often overlooked, leaving
    behind valuable insights about the nature of data and the relationship between
    calculus, statistics, and linear algebra.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will derive PCA through a thought experiment, beginning
    with two dimensions and extending to arbitrary dimensions. As we progress through
    each derivation, we will see the harmonious interplay of seemingly distinct branches
    of mathematics, culminating in an elegant coordinate transformation. This derivation
    will unravel the mechanics of PCA and reveal the captivating interconnectedness
    of mathematical concepts. Let’s embark on this enlightening exploration of PCA
    and its beauty.
  prefs: []
  type: TYPE_NORMAL
- en: Warming Up in Two Dimensions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As humans living in a three-dimensional world, we generally grasp two-dimensional
    concepts, and this is where we will begin in this article. Starting in two dimensions
    will simplify our first thought experiment and allow us to better understand the
    nature of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: Theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have a dataset that looks something like this (note that each feature should
    be scaled to have a mean of 0 and variance of 1):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/364f66e80e7272963610ae918440bb91.png)'
  prefs: []
  type: TYPE_IMG
- en: (1) Correlated Data. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: We immediately notice this data lies in a coordinate system described by ***x1***
    and ***x2***, and these variables are correlated. *Our goal is to find a new coordinate
    system informed by the covariance structure of the data.* In particular, the first
    basis vector in the coordinate system should explain the majority of the variance
    when projecting the original data onto it.
  prefs: []
  type: TYPE_NORMAL
- en: Our first order of business is to find a vector such that when we project the
    original data onto the vector, the maximum amount of variance is preserved. In
    other words, the ideal vector points in the direction of maximal variance, as
    defined by the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This vector can be defined by the angle it makes with the x-axis in the counter-clockwise
    direction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3dfa6fba3e6d1cb3026078a984be9ac7.png)'
  prefs: []
  type: TYPE_IMG
- en: (2) Searching for the direction of maximal variance by rotating a vector. Image
    by Author.
  prefs: []
  type: TYPE_NORMAL
- en: In the animation above, we define a vector by the angle it makes with the x-
    axis, and we can see the direction the vector points at each angle between 0 and
    180 degrees. Visually, we can see that a ***θ*** value near 45 degrees points
    in the direction of maximal variability in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To restate our objective, we want to find the angle the causes the vector to
    point in the direction of maximal variance. Mathematically, we want to find ***θ***
    that maximizes this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/803d660009c0ee2e4cdb6c686edbedab.png)'
  prefs: []
  type: TYPE_IMG
- en: (3) The best θ will maximize this objective function. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: Here, ***N*** is the number of observations in the data. We project each observation
    onto the axis defined by ***[cos(θ) sin(θ)]***, which is a unit vector defined
    by the angle ***θ***, and square the result.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we vary ***θ***, this equation gives of the variance of the data when projected
    onto the axis defined by ***θ***. Let’s compute the dot product inside the square
    and rewrite this expression to make it easier to work with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d922b2bb66f5a070baa481bae0416302.png)'
  prefs: []
  type: TYPE_IMG
- en: (4) The variance equation rewritten. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily for us, this is a convex function, and we can maximize it by computing
    it’s derivative and setting it equal to 0\. We will also need to compute the second
    derivative of the variance equation to know whether we’ve found a minima or maxima.
    The first and second derivatives of ***var(θ)*** look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/64a2408a12fe159b516ab05bff0fd001.png)'
  prefs: []
  type: TYPE_IMG
- en: (5) The first and second derivative of var(θ). Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can set the first derivative equal to 0, and rearrange the equation
    to isolate ***θ***:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/28b55012957951891efdc1b5fd90a4dc.png)'
  prefs: []
  type: TYPE_IMG
- en: (6) The first derivative of var(θ) set equal to 0 and rearranged. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, using a common trig identity and some algebra, we get a closed form
    solution for the ***θ*** that minimizes or maximizes ***var(θ)***:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9643601112402e5343b2cdaac7ca86b5.png)'
  prefs: []
  type: TYPE_IMG
- en: (7) The equation to compute θ that finds the direction of maximal or minimal
    variance. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fascinatingly, this equation is all we need to perform PCA in two dimensions.
    The second derivative will tell us whether ***θ*** corresponds to a local minima
    or maxima. Because there is only one other principal component, it must be defined
    by shifting ***θ*** by 90 degrees. Therefore, the two principal component angles
    are given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad642bb49557fa023e74a1fe79fd6874.png)'
  prefs: []
  type: TYPE_IMG
- en: (8) The angles that define the principal components in two dimensions. Image
    by Author.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, we can use the second derivative of ***var(θ)*** to figure
    out which ***θ*** belongs to principal component 1 (the direction of maximal variance)
    and which ***θ*** belongs to principal component 2 (the direction of minimal variance).
    Alternatively, we can plug both ***θs*** into ***var(θ)*** and see which one results
    in a higher value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we know which ***θ*** corresponds to each principal component, we plug
    each one into the trigonometric equation for a unit vector in two dimensions (***[cos(θ)
    sin(θ)])***. Explicitly, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ca1eafaf1bf4f3dd1282647a2286fe02.png)'
  prefs: []
  type: TYPE_IMG
- en: (9) Determining the two principal components from the θs that maximize or minimize
    var(θ). Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: That’s it — ***pc1*** and ***pc2*** are the principal component vectors. By
    thinking through the objective of principal component analysis, we were able to
    derive the principal components from scratch in two dimensions. To convince ourselves
    this is correct, let’s write some Python code to implement our strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first function finds one of the principal angles derived from the derivative
    of the variance equation in figure (7). Because this is a closed-form equation,
    the implementation is straight forward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Depending on the nature of the variance equation, `find_principal_angle()`
    recovers one of the principal angles, and the other principal angle is 90 degrees
    away. To determine which principal angle `find_principal_angle()` returns, we
    can use the second derivative, or hessian, of the variance equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The logic for this function comes directly from figure (5). The last function
    we need determines the two principal components from `find_principal_angle()`
    and `compute_pca_cost_hessian()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In `find_principal_components_2d()`, `theta0`is one of the principal angles
    and `theta0_hessian`is the second derivative of the variance equation. Because
    `theta0`is an extrema of the variance equation, `theta0_hessian`tells us whether
    `theta0`is a minimum or maximum. In particular, if `theta0_hessian`is positive,
    `theta0`must be a minimum and correspond to the second principal component. Otherwise,
    `theta0_hessian`is negative and `theta0`is a maximum corresponding to the first
    principal component.
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify `find_principal_components_2d()`does what we want, let’s find the
    principal components for a two dimensional dataset and compare them to the results
    of the [PCA implementation in scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).
    Here’s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This code creates two-dimensional correlated data, normalizes it, and finds
    the principal components using our custom logic and the [scikit-learn](https://scikit-learn.org/stable/)
    PCA implementation. We can see that we’ve achieved the exact same results as scikit-learn.
    The resulting principal components look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/22c10018692acb9165469ef7a78b0667.png)'
  prefs: []
  type: TYPE_IMG
- en: (10) Our two-dimensional PCA method and the scikit-learn implementation are
    exact matches. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: As expected, the two sets of principal components perfectly overlap. However,
    the issue with our method is that it doesn’t generalize beyond two dimensions.
    In the next section, we will derive an analogous results for arbitrary dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Deriving a General Result
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In two dimensions, we derived a closed form equation to find the principal components
    using the definition of variance, calculus, and a bit of trigonometry. Unfortunately,
    this approach will quickly become infeasible for datasets beyond two dimensions.
    For this, we will have to rely on the most powerful branch of mathematics for
    computation — linear algebra.
  prefs: []
  type: TYPE_NORMAL
- en: Theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The derivation of PCA in higher dimensions is analogous to the two-dimensional
    case in many ways. The fundamental difference is that we have to leverage some
    tools from linear algebra to help us account for all possible angles that the
    principal components can make with the origin of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, assume our data set has ***d*** features and ***n*** observations.
    As before, each feature should be scaled to have a mean of 0 and variance of 1\.
    We will arrange the data in a matrix as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2d635700dde60aea98bdce43f6c04407.png)'
  prefs: []
  type: TYPE_IMG
- en: (11) The data matrix for PCA. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, our goal is to find a unit vector, ***p***, that points in the direction
    of maximum variance of the data. To do this we first need to state two useful
    facts. The first is that the [covariance matrix](https://en.wikipedia.org/wiki/Covariance_matrix)
    of our data, the analog of variance in higher dimensions, can be found my a multiplying
    X with its transpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be86b90f50486e6f8c0081705f66ba3a.png)'
  prefs: []
  type: TYPE_IMG
- en: (12) The equation to compute the covariance matrix of X. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'While this is useful, we’re not interested in the variance of the data itself.
    **We want to know the variance of the data when projected onto a new axis.** To
    do this, we recall a result about the variance of a scalar quantity. Namely, when
    a scalar is multiplied by a constant, the resulting variance is that constant
    squared multiplied by the original variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e81fad8387521e3c4a282cc49468c30.png)'
  prefs: []
  type: TYPE_IMG
- en: (13) The variance of a scalar multiplied by a constant. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Analogously, when we multiply a vector with our data matrix (i.e. when we project
    a vector onto the matrix), the resulting variance can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f88925c03278849f16cf1af146fa5b0e.png)'
  prefs: []
  type: TYPE_IMG
- en: (14) The variance of a vector projected onto the data matrix. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'This gives us everything we need to define our problem for PCA. That is, we
    want to find the vector ***p*** (the principal component) that maximizes the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b68008d831be70a929522701e5ce312e.png)'
  prefs: []
  type: TYPE_IMG
- en: (15) The PCA problem statement in **d** dimensions is to find the vector, **p**,
    that maximizes the variance when projected onto the data. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: The PCA problem statement in ***d*** dimensions is to find the vector, ***p***,
    that maximizes the variance when projected onto the data. We also stipulate that
    ***p*** is a unit vector, making this a constrained optimization problem. Therefore,
    just as in the two-dimensional case, we need calculus.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve this, we can leverage [lagrange multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier),
    which allow us to minimize the objective while still meeting the specified constraints.
    The Lagrangian expression for this problem is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5f51d128d65caaed839e0a5674ac90d2.png)'
  prefs: []
  type: TYPE_IMG
- en: (16) The PCA Lagrangian expression. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'To optimize this, we take the derivative and set it equal to 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/48c13934f459ab096b90863c3e67ce64.png)'
  prefs: []
  type: TYPE_IMG
- en: (17) The derivative of the PCA Lagrangian expression. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some rearranging gives us the following expression — a very familiar result
    from linear algebra:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b1ab5575ce78f5e2cdd9af1d9b772c1.png)'
  prefs: []
  type: TYPE_IMG
- en: (18) The PCA Eigen Equation. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'What does this tell us? Astonishingly, the optimal principal component (***p***),
    when multiplied by the covariance matrix of data (***S***), is simply that same
    ***p*** multiplied by a scalar (***λ***). In other words, ***p* must be an** [**eigenvector**](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors)
    **of the covariance matrix,** and we can find ***p*** by computing the eigenvalue
    decomposition of ***S.*** We also observe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4dbf6605177b0bd6d092dedf00490c8.png)'
  prefs: []
  type: TYPE_IMG
- en: (19) The variance of the data when projected onto **p** is equal to the eigenvalue
    of **p.** Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'The variance of the data when projected onto ***p*** is equal to the eigenvalue
    corresponding to ***p.*** We also know thatthere are at most ***d*** eigenvalues/eigenvectors
    of ***S,*** the eigenvectors are orthogonal, and the eigenvalues are sorted from
    largest to smallest. This means, by taking the eigenvalue decomposition of ***S,***
    we recover all of the principal components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b62129059503c568dfef653e25879cfc.png)'
  prefs: []
  type: TYPE_IMG
- en: (20) All **d** principal components come from the eigenvalue decomposition of
    **S**. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: This is quite a beautiful intersection of statistics, calculus, and linear algebra,
    giving us an elegant way to find the principal components of high dimensional
    datasets. What a beautiful result!
  prefs: []
  type: TYPE_NORMAL
- en: Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we use NumPy, the general form of PCA is actually easier to code than the
    two-dimensional version we derived. Here’s an example of PCA using a three-dimensional
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the principal components given by the scikit-learn PCA implementation
    are exactly the eigenvectors of the covariance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Principal Component Analysis (PCA) is a powerful technique used in data science
    and machine learning to reduce the dimensionality of high-dimensional datasets
    while preserving the most important information. In this article, we explored
    the foundational principles of PCA in two dimensions and extended it to arbitrary
    dimensions using linear algebra.
  prefs: []
  type: TYPE_NORMAL
- en: In the two-dimensional case, we derived a closed-form solution for finding the
    principal components by maximizing the variance when the data is projected onto
    a new axis. We used trigonometry and calculus to find the angles that define the
    principal components. We then implemented our strategy in Python and verified
    its correctness by comparing the results to the scikit-learn PCA implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Moving to arbitrary dimensions, we leveraged linear algebra to find a generalized
    solution. By expressing PCA as an eigenvalue problem, we showed that the principal
    components are the eigenvectors of the covariance matrix. The corresponding eigenvalues
    represent the variance of the data when projected onto each principal component.
  prefs: []
  type: TYPE_NORMAL
- en: As we often see in mathematics, seemingly unrelated concepts that were developed
    by different mathematicians over varying time periods end up culminating into
    a beautiful result. PCA is a prime example of this perplexing intersection, and
    there are many more to be explored!
  prefs: []
  type: TYPE_NORMAL
- en: '*Become a Member:* [*https://harrisonfhoffman.medium.com/membership*](https://harrisonfhoffman.medium.com/membership)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Buy me a coffee:* [*https://www.buymeacoffee.com/HarrisonfhU*](https://www.buymeacoffee.com/HarrisonfhU)'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Ali Ghodsi, Lec 1: Principal Component Analysis —* [https://www.youtube.com/watch?v=L-pQtGm3VS8&t=3129s](https://www.youtube.com/watch?v=L-pQtGm3VS8&t=3129s)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Independent Component Analysis 2* — [https://www.youtube.com/watch?v=olKgmOuAvrc](https://www.youtube.com/watch?v=olKgmOuAvrc)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
