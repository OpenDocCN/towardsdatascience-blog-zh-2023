- en: Easily Estimate Your OpenAI API Costs with Tiktoken
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/easily-estimate-your-openai-api-costs-with-tiktoken-c17caf6d015e](https://towardsdatascience.com/easily-estimate-your-openai-api-costs-with-tiktoken-c17caf6d015e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Count your tokens and avoid going bankrupt from using the OpenAI API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@iamleonie?source=post_page-----c17caf6d015e--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----c17caf6d015e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c17caf6d015e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c17caf6d015e--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----c17caf6d015e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c17caf6d015e--------------------------------)
    ·6 min read·Aug 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/286da1233aab3f57c2e2d49f77d8d96e.png)'
  prefs: []
  type: TYPE_IMG
- en: Fresh tokens! $0.0015 per kilo!
  prefs: []
  type: TYPE_NORMAL
- en: Many people I know are interested in playing with OpenAI’s large language models
    (LLMs). But hosting LLMs is expensive, and thus, inference services like OpenAI’s
    application programming interface (API) are not free. But entering your payment
    information without knowing what inferencing costs will add up to can be a little
    intimidating.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, I like to include a little indicator of the API costs of a walkthrough
    of my articles, so my readers know what to expect and can get a feeling for inferencing
    costs.
  prefs: []
  type: TYPE_NORMAL
- en: This article introduces you to the `tiktoken` library I use to estimate inferencing
    costs for OpenAI foundation models.
  prefs: []
  type: TYPE_NORMAL
- en: What is tiktoken?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`tiktoken`is an open-source byte pair encoding (BPE) tokenizer developed by
    OpenAI that is used for tokenizing text in their LLMs. It allows developers to
    count how many tokens are in a text before making calls to the OpenAI endpoint.'
  prefs: []
  type: TYPE_NORMAL
- en: It thus helps with estimating the associated costs of using the OpenAI API because
    its costs are **billed in units of 1,000 tokens** according to the [OpenAI’s pricing
    page](https://openai.com/pricing) [1].
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/openai/tiktoken?source=post_page-----c17caf6d015e--------------------------------)
    [## GitHub — openai/tiktoken: tiktoken is a fast BPE tokeniser for use with OpenAI’s
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'tiktoken is a fast BPE tokeniser for use with OpenAI’s models. — GitHub — openai/tiktoken:
    tiktoken is a fast BPE…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/openai/tiktoken?source=post_page-----c17caf6d015e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Tokens and Tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Tokens** are common sequences of characters in a text, and **tokenization**
    is when you split a text string into a list of tokens. A token can be equal to
    a word but usually a word consists of multiple tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: Natural language processing (NLP) models are trained on tokens and understand
    the relationships between them. Thus, the input text is tokenized before an NLP
    model processes it.
  prefs: []
  type: TYPE_NORMAL
- en: But how words are tokenized exactly depends on the used tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: Below you can see an example of how the text
  prefs: []
  type: TYPE_NORMAL
- en: “Alice has a parrot.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What animal is Alice’s pet?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Alice’s pet is a parrot.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: can be tokenized.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/359f5ca1e40f9c09b26964db2d24217c.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that the text is split into chunks of characters, including spaces
    and punctuation, and even line breaks. Then each token is encoded as an integer.
  prefs: []
  type: TYPE_NORMAL
- en: 'While some shorter words are equivalent to one token, longer words, e.g., the
    word “parrot”, are separated into multiple tokens, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/15e5eb17092e773a3cf7c6b8bdd780fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Depending on the tokenizer, the same word may not be encoded as the same token.
    E.g., in this example, the word Alice is once tokenized as “Alice” and once as
    “ Alice” (with a leading space), depending on where in the text this word appeared.
    Thus, the tokens “Alice” and “ Alice” (with a leading space) have different token
    identifiers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/392cfbd72426e0c8ad030a03997b4fb2.png)'
  prefs: []
  type: TYPE_IMG
- en: OpenAI uses a tokenization technique called byte pair encoding (BPE), which
    replaces the most frequent pairs of bytes in a text with a single byte and thus
    helps NLP models understand grammar better [4].
  prefs: []
  type: TYPE_NORMAL
- en: E.g., “ing” is a frequent substring of characters in the English language. Thus,
    BPE will split words ending in “ing” as, e.g., “walking” into “walk” and “ing”.
  prefs: []
  type: TYPE_NORMAL
- en: On average, each token corresponds to about 4 bytes or 4 characters for common
    English text in BPE, which roughly translates to 100 tokens for 75 words [2, 4].
  prefs: []
  type: TYPE_NORMAL
- en: 'For an in-depth explanation of BPE, I recommend this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0?source=post_page-----c17caf6d015e--------------------------------)
    [## Byte-Pair Encoding: Subword-based tokenization algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: Understand subword-based tokenization algorithm used by state-of-the-art NLP
    models — Byte-Pair Encoding (BPE)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0?source=post_page-----c17caf6d015e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: How To Use tiktoken To Estimate OpenAI API Costs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Estimating the OpenAI API costs with `tiktoken` consist of the following four
    simple steps, which we will discuss in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Installation and setup](#6216)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Define encoding](#39cb)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Tokenize text](#3fc9)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Estimate OpenAI API costs](#8a89)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Step 1: Installation and setup'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, you need to install `tiktoken` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you import the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: Define encoding'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, you need to define which encoding to use for tokenization because OpenAI
    models use different encodings [3]:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cl100k_base`: for `gpt-4`, `gpt-3.5-turbo`, and `text-embedding-ada-002`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`p50k_base`: for codex models, `text-davinci-002`, `text-davinci-003`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gpt2` (or `r50k_base`): for GPT-3 models like `davinci`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you know the encoding of your model, you can define the encoding as shown
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can define the encoding according to the used model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: Tokenize the text'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, you can tokenize any body of text with the `.encode()` method, which
    will return a list of integers that represent the tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 4: Estimate OpenAI API costs'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To estimate the OpenAI API costs, you can now count the number of tokens in
    your text and estimate the associated costs of the inferencing service according
    to the model you are using.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If you are using an **embedding model**, you are only charged for the number
    of tokens of the input text to embed. E.g., `text-embedding-ada-002` cost $0.0001
    for 1,000 tokens at the time of writing [1].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/626974f1844ef6d37e9b5348d37ac295.png)'
  prefs: []
  type: TYPE_IMG
- en: But note, if you are using a **conversational model**, you are charged both
    for the number of input tokens (number of tokens of your prompt) as well as for
    the number of output tokens (number of tokens of the returned completion). E.g.,
    the `gpt-3.5-turbo` (4K context) model costs $0.0015 for 1,000 input tokens and
    $0.002 for 1,000 output tokens at the time of writing [1].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/382101a4f1e6f8778dbdbc1960c3f56d.png)'
  prefs: []
  type: TYPE_IMG
- en: That’s why you need to control the number of output tokens in addition to managing
    the length of your input text to avoid unexpected costs. You can control the number
    of output tokens via the optional but highly recommended `max_tokens` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Optional step: Decode tokens'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another advantage of BPE is that it is reversible. If you want to decode a
    list of tokens, you can use the `.decode_sigle_token_bytes()` method shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article showed how you can easily calculate the number of tokens in your
    input text (text to embed or prompt) with the `tiktoken` library before calling
    the OpenAI API endpoint. By including this step in your coding practice, you will
    get a feeling for the resulting API costs. Additionally, we discussed that you
    should also use the `max_tokens` parameter when using an LLM that will output
    a completion to avoid unexpected costs.
  prefs: []
  type: TYPE_NORMAL
- en: Enjoyed This Story?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Subscribe for free*](https://medium.com/subscribe/@iamleonie) *to get notified
    when I publish a new story.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----c17caf6d015e--------------------------------)
    [## Get an email whenever Leonie Monigatti publishes.'
  prefs: []
  type: TYPE_NORMAL
- en: Get an email whenever Leonie Monigatti publishes. By signing up, you will create
    a Medium account if you don’t already…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----c17caf6d015e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*Find me on* [*LinkedIn*](https://www.linkedin.com/in/804250ab/),[*Twitter*](https://twitter.com/helloiamleonie)*,
    and* [*Kaggle*](https://www.kaggle.com/iamleonie)*!*'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If not otherwise stated, all images are created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Web & Literature
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] OpenAI (2023). [Pricing](https://openai.com/pricing) (accessed July 31st,
    2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] OpenAI (2023). [Tokenizer](https://platform.openai.com/tokenizer) (accessed
    July 31st, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] OpenAI on GitHub(2023). [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
    (accessed July 31st, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] OpenAI on GitHub(2023). [tiktoken](https://github.com/openai/tiktoken)
    (accessed July 31st, 2023)'
  prefs: []
  type: TYPE_NORMAL
