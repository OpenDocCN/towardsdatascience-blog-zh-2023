- en: All You Need to Know to Build Your First LLM App
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/all-you-need-to-know-to-build-your-first-llm-app-eb982c78ffac](https://towardsdatascience.com/all-you-need-to-know-to-build-your-first-llm-app-eb982c78ffac)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Step-by-Step Tutorial to Document Loaders, Embeddings, Vector Stores and Prompt
    Templates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dmnkplzr.medium.com/?source=post_page-----eb982c78ffac--------------------------------)[![Dominik
    Polzer](../Images/7e48cd15df31a0ab961391c0d57521de.png)](https://dmnkplzr.medium.com/?source=post_page-----eb982c78ffac--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eb982c78ffac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----eb982c78ffac--------------------------------)
    [Dominik Polzer](https://dmnkplzr.medium.com/?source=post_page-----eb982c78ffac--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----eb982c78ffac--------------------------------)
    ·26 min read·Jun 22, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/215dfd8fde2517ee28b96285604db80a.png)'
  prefs: []
  type: TYPE_IMG
- en: Build your own chatbot with context injection — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are just looking for a short tutorial that explains how to build a simple
    LLM application, you can skip to section “6\. Creating a Vector store”, there
    you have all the code snippets you need to build up a minimalistic LLM app with
    vector store, prompt template and LLM call.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Intro**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Why we need LLMs](#d5e4)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Fine-Tuning vs. Context Injection](#f0e7)'
  prefs: []
  type: TYPE_NORMAL
- en: '[What is LangChain?](#b9f0)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step-by-Step Tutorial**'
  prefs: []
  type: TYPE_NORMAL
- en: '[1\. Load documents using LangChain](#dcec)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2\. Split our Documents into Text Chunks](#2e88)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3\. From Text Chunks to Embeddings](#335e)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4\. Define the LLM you want to use](#b5ab)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5\. Define our Prompt Template](#5e34)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6\. Creating a Vector Store](#4cff)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/abb5a66e45437849d1939f663604994d.png)'
  prefs: []
  type: TYPE_IMG
- en: Table of contents
  prefs: []
  type: TYPE_NORMAL
- en: Why we need LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The evolution of language has brought us humans incredibly far to this day.
    It enables us to efficiently share knowledge and collaborate in the form we know
    today. Consequently, most of our collective knowledge continues to be preserved
    and communicated through unorganized written texts.
  prefs: []
  type: TYPE_NORMAL
- en: Initiatives undertaken over the past two decades to digitize information and
    processes have often focused on accumulating more and more data in relational
    databases. This approach enables traditional analytical machine learning algorithms
    to process and understand our data.
  prefs: []
  type: TYPE_NORMAL
- en: However, despite our extensive efforts to store an increasing amount of data
    in a structured manner, we are still unable to capture and process the entirety
    of our knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: '**About 80% of all data in companies is unstructured, like work descriptions,
    resumes, emails, text documents, power point slides, voice recordings, videos
    and social media**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/161315a8f6daffba557fdcc5d9fb0d5a.png)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of data in companies — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: The development and advancement leading to GPT3.5 signify a major milestone
    as it empowers us to effectively interpret and analyze diverse datasets, regardless
    of their structure or lack thereof. Nowadays, we have models that can comprehend
    and generate various forms of content, including text, images, and audio files.
  prefs: []
  type: TYPE_NORMAL
- en: '**So how can we leverage their capabilities for our needs and data?**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Fine-Tuning vs. Context Injection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In general, we have two fundamentally different approaches to enable large
    language models to answer questions that the LLM cannot know: **Model fine-tuning**
    and **context injection**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fine-Tuning**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fine-tuning refers to training an existing language model with additional data
    to optimise it for a specific task.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of training a language model from scratch, a pre-trained model such
    as BERT or LLama is used and then adapted to the needs of a specific task by adding
    use case specific training data.
  prefs: []
  type: TYPE_NORMAL
- en: A team from Stanford University used the LLM Llama and fine-tuned it by using
    50,000 examples of how a user/model interaction could look like. The result is
    a Chat Bot that interacts with a user and answers queries. This fine-tuning step
    changed the way the model is interacting with the end user.
  prefs: []
  type: TYPE_NORMAL
- en: '**→ Misconceptions around fine-tuning**'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning of PLLMs (Pre-trained Language Models) is a way to adjust the model
    for a specific task, but it doesn’t really allow you to inject your own domain
    knowledge into the model. This is because the model has already been trained on
    a massive amount of general language data, and your specific domain data is usually
    not enough to override what the model has already learned.
  prefs: []
  type: TYPE_NORMAL
- en: So, when you fine-tune the model, it might occasionally provide correct answers,
    but it will often fail because it heavily relies on the information it learned
    during pre-training, which might not be accurate or relevant to your specific
    task. In other words, fine-tuning helps the model adapt to HOW it communicates,
    but not necessarily WHAT it communicates. (Porsche AG, 2023)
  prefs: []
  type: TYPE_NORMAL
- en: This is where context injection comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: '**In-context learning / Context Injection**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When using context injection, we are not modifying the LLM, we focus on the
    prompt itself and inject relevant context into the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: So we need to think about how to provide the prompt with the right information.
    In the figure below, you can see schematically how the whole thing works. We need
    a process that is able to identify the most relevant data. To do this, we need
    to enable our computer to compare text snippets with each other.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e88d7155e335397440e55e00b5b0ea6.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarity search in our unstructured data — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: This can be done with embeddings. With embeddings, we translate text into vectors,
    allowing us to represent text in a multidimensional embedding space. Points that
    are closer to each other in space are often used in the same context. To prevent
    this similarity search from taking forever, we store our vectors in a vector database
    and index them.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft is showing us how this could work with Bing Chat. Bing combines the
    ability of LLMs to understand language and context with the efficiency of traditional
    web search.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The objective of the article is to demonstrate the process of creating a straightforward
    solution that allows us to analyse our own texts and documents, and then incorporate
    the insights gained from them into the answers our solution returns to the user.
    I will describe all steps and components you need to implement an end-to-end solution.
  prefs: []
  type: TYPE_NORMAL
- en: So how can we use the capabilities of LLMs to meet our needs? Let’s go through
    it step by step.
  prefs: []
  type: TYPE_NORMAL
- en: Step by Step tutorial — Your first LLM App
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the following, we want to utilize LLMs to respond to inquiries about our
    personal data. To accomplish this, I begin by transferring the content of our
    personal data into a vector database. This step is crucial as it enables us to
    efficiently search for relevant sections within the text. We will use this information
    from our data and the LLMs capabilities to interpret text to answer the user’s
    question.
  prefs: []
  type: TYPE_NORMAL
- en: We can also guide the chatbot to exclusively answer questions based on the data
    we provide. This way, we can ensure that the chatbot remains focused on the data
    at hand and provides accurate and relevant responses.
  prefs: []
  type: TYPE_NORMAL
- en: To implement our use case, we will rely heavily on LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: What is LangChain?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**“LangChain is a framework for developing applications powered by language
    models.” (Langchain, 2023)**'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, LangChain is a Python framework that was designed to support the creation
    of various LLM applications such as chatbots, summary tools, and basically any
    tool you want to create to leverage the power of LLMs. The library combines various
    components we will need. We can connect these components in so-called chains.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important modules of Langchain are (Langchain, 2023):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Models:** Interfaces to various model types'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prompts:** Prompt management, prompt optimization, and prompt serialization'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Indexes:** Document loaders, text splitters, vector stores — Enable faster
    and more efficient access to the data'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Chains:** Chains go beyond a single LLM call, they allow us to set up sequences
    of calls'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the image below, you can see where these components come into play. We load
    and process our own unstructured data using the document loaders and text splitters
    from the indexes module. The prompts module allows us to inject the found content
    into our prompt template, and finally, we are sending the prompt to our model
    using the model's module.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ef1c2c72aff354412158ff38565e9aa3.png)'
  prefs: []
  type: TYPE_IMG
- en: Components you need for your LLM app — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '**5\. Agents:** Agents are entities that use LLMs to make choices regarding
    which actions to take. After taking an action, they observe the outcome of that
    action and repeat the process until their task is completed.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e660c81a051ece8e5fb10fce61226cfa.png)'
  prefs: []
  type: TYPE_IMG
- en: Agents decide autonomously how to perform a particular task — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: We use Langchain in the first step to load documents, analyse them and make
    them efficiently searchable. After we have indexed the text, it should become
    much more efficient to recognize text snippets that are relevant for answering
    the user’s questions.
  prefs: []
  type: TYPE_NORMAL
- en: What we need for our simple application is of course an LLM. We will use GPT3.5
    via the OpenAI API. Then we need a vector store that allows us to feed the LLM
    with our own data. And if we want to perform different actions for different queries,
    we need an agent that decides what should happen for each query.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start from the beginning. We first need to import our own documents.
  prefs: []
  type: TYPE_NORMAL
- en: The following section describes what modules are included in LangChain’s Loader
    Module to load different types of documents from different sources.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Load documents using Langchain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LangChain is able to load a number of documents from a wide variety of sources.
    You can find a list of possible document loaders in the LangChain [documentation](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html).
    Among them are loaders for HTML pages, S3 buckets, PDFs, Notion, Google Drive
    and many more.
  prefs: []
  type: TYPE_NORMAL
- en: For our simple example, we use data that was probably not included in the training
    data of GPT3.5\. I use the Wikipedia article about GPT4 because I assume that
    GPT3.5 has limited knowledge about GPT4.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this minimal example, I’m not using any of the LangChain loaders, I’m just
    scraping the text directly from Wikipedia [License: CC BY-SA 3.0]using ***BeautifulSoup.***'
  prefs: []
  type: TYPE_NORMAL
- en: '**Please note that scraping websites should only be done in accordance with
    the website’s terms of use and the copyright/license status of the text and data
    you wish to use.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0ecc028506e2ab4fad286d986afcda92.png)'
  prefs: []
  type: TYPE_IMG
- en: 2\. Split our document into text fragments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we must divide the text into smaller sections called text chunks. Each
    text chunk represents a data point in the embedding space, allowing the computer
    to determine the similarity between these chunks.
  prefs: []
  type: TYPE_NORMAL
- en: The following text snippet is utilizing the text splitter module from langchain.
    In this particular case, we specify a chunk size of 100 and a chunk overlap of
    20\. It’s common to use larger text chunks, but you can experiment a bit to find
    the optimal size for your use case. You just need to remember that every LLM has
    a token limit (4000 tokes for GPT 3.5). Since we are inserting the text blocks
    into our prompt, we need to make sure that the entire prompt is no larger than
    4000 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5f30c3e19308d6a95e82e8dbbd2be48a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This splits our entire text as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9883b42387e6a80ef31e153685590e63.png)'
  prefs: []
  type: TYPE_IMG
- en: Langchain text spliter — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 3\. From Text Chunks to Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we need to make the textual components understandable and comparable to
    our algorithms. We must find a way to convert human language into digital form,
    represented by bits and bytes.
  prefs: []
  type: TYPE_NORMAL
- en: The image provides a simple example that may seem obvious to most humans. However,
    we need to find a way to make the computer understand that the name “Charles”
    is associated with men rather than women, and if Charles is a man, he is the king
    and not the queen.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e983ecc380357da8438928f5f94f251.png)'
  prefs: []
  type: TYPE_IMG
- en: Making language understandable for our computer — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Over the last few years, new methods and models have emerged that do just that.
    What we want is a way to be able to translate the meaning of words into an n-dimensional
    space, so we are able to compare text chunks with each other and even calculate
    a measure for the similarity of them.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding models attempt to learn exactly that by analyzing the context in which
    words are typically used. Since tea, coffee, and breakfast are often used in the
    same context, they are closer to each other in the n-dimensional space than, for
    example, tea and pea. Tea and pea sound similar but are rarely used together.
    (AssemblyAI, 2022)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b82fbabaf51da10acdee62bd36353824.png)'
  prefs: []
  type: TYPE_IMG
- en: Embeddings analyze the context in which words are used, not the word itself
    — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: The embedding models provide us with a vector for each word in the embedding
    space. Finally, by representing them using vectors, we are able to perform mathematical
    calculations, such as calculating similarities between words as the distance between
    data points.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e52927d7c78f5f6d5235f1d06ef8a3e9.png)'
  prefs: []
  type: TYPE_IMG
- en: Random english words in a two dimensional embeddings space — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: To convert text into embeddings, there are several ways, e.g. Word2Vec, GloVe,
    fastText or ELMo.
  prefs: []
  type: TYPE_NORMAL
- en: '**Embedding Models**'
  prefs: []
  type: TYPE_NORMAL
- en: To capture similarities between words in embeddings, Word2Vec uses a simple
    neural network. We train this model with large amounts of text data and want to
    create a model that is able to assign a point in the n-dimensional embedding space
    to each word and thus describe its meaning in the form of a vector.
  prefs: []
  type: TYPE_NORMAL
- en: For the training, we assign a neuron in the input layer to each unique word
    in our data set. In the image below, you can see a simple example. In this case,
    the hidden layer contains only two neurons. Two, because we want to map the words
    in a two dimensional embedding space. (The existing models are in reality much
    larger and thus represent the words in higher dimensional spaces — OpenAI’s Ada
    Embedding Model for example, is using 1536 dimensions) After the training process
    the individual weights describe the position in the embedding space.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, our dataset consists of a single sentence: “Google is a tech
    company.” Each word in the sentence serves as an input for the neural network
    (NN). Consequently, our network has five input neurons, one for each word.'
  prefs: []
  type: TYPE_NORMAL
- en: During the training process, we focus on predicting the next word for each input
    word. When we begin at the start of the sentence, the input neuron corresponding
    to the word “Google” receives a value of 1, while the remaining neurons receive
    a value of 0\. We aim to train the network to predict the word “is” in this particular
    scenario.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/207cc5bc488b08fbaa216e98e65841dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Word2Vec: Learning word embeddings — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: In reality, there are multiple approaches to learn embedding models, each with
    its own unique way of predicting outputs during the training process. Two commonly
    used methods are CBOW (Continuous Bag of Words) and Skip-gram.
  prefs: []
  type: TYPE_NORMAL
- en: In CBOW, we take the surrounding words as input and aim to predict the middle
    word. Conversely, in Skip-gram, we take the middle word as input and attempt to
    predict the words occurring on its left and right sides. However, I won’t delve
    into the intricacies of these methods. Let’s just say that these approaches provide
    us with embeddings, which are representations that capture the relationships between
    words by analysing the context of huge amounts of text data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72da8315043be5325c287969edce53ab.png)'
  prefs: []
  type: TYPE_IMG
- en: CBOW vs. Skip-gram — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: If you want to know more about embeddings*, there is a wealth of information
    available on the internet. However, if you prefer a visual and step-by-step guide,
    you might find it helpful to watch Josh* [*Starmer’s StatQuest on Word Embedding
    and Word2Vec*](https://www.youtube.com/watch?v=viZrOnJclY0&t=204s)*.*
  prefs: []
  type: TYPE_NORMAL
- en: '**Back to embedding models**'
  prefs: []
  type: TYPE_NORMAL
- en: What I just tried to explain using a simple example in a 2-dimensional embedding
    space also applies to larger models. For instance, the standard Word2Vec vectors
    have 300 dimensions, while OpenAI’s Ada model has 1536 dimensions. These pretrained
    vectors allow us to capture the relationships between words and their meanings
    with such precision that we can perform calculations with them. For example, using
    these vectors, we can find that France + Berlin — Germany = Paris, and also faster
    + warm — fast = warmer. (Tazzyman, n.d.)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00d72db50ce28b008b94bcdc04011b8b.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculate with embeddings — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: In the following we want to use the OpenAI API not only to use OpenAI’s LLMs,
    but also to leverage their Embedding Models.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: The difference between Embedding Models and LLMs is that Embedding Models
    focus on creating vector representations of words or phrases to capture their
    meanings and relationships, while LLMs are versatile models trained to generate
    coherent and contextually relevant text based on provided prompts or queries.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenAI Embedding Models**'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the various LLMs from OpenAI, you can also choose between a variety
    of embedding models, such as Ada, Davinci, Curie, and Babbage. Among them, Ada-002
    is currently the fastest and most cost-effective model, while Davinci generally
    provides the highest accuracy and performance. However, you need to try them out
    yourself and find the optimal model for your use case. If you’re interested in
    a detailed understanding of OpenAI Embeddings, you can refer to the [OpenAI documentation](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings).
  prefs: []
  type: TYPE_NORMAL
- en: Our goal with the Embedding Models is to convert our text chunks into vectors.
    In the case of the second generation of Ada, these vectors have 1536 output dimensions,
    which means they represent a specific position or orientation within a 1536-dimensional
    space.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI describes these embedding vector in their documentation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: “Embeddings that are numerically similar are also semantically similar. For
    example, the embedding vector of “canine companions say” will be more similar
    to the embedding vector of “woof” than that of “meow.” (OpenAI, 2022)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s give it a try. We use OpenAI’s API to translate our text snippets into
    embeddings as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0f1bafc97daecdcd048e21ad373303d3.png)'
  prefs: []
  type: TYPE_IMG
- en: We convert our text, such as the first text chunk containing “2023 text-generating
    language model,” into a vector with 1536 dimensions. By doing this for each text
    chunk, we can observe in a 1536-dimensional space which text chunks are closer
    and more similar to each other.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s give it a try. We aim to compare the users’ questions with the text chunks
    by generating embeddings for the question and then comparing it with other data
    points in the space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/65eb5bd6420dcbd472c3f86a5dd22b4b.png)'
  prefs: []
  type: TYPE_IMG
- en: Which text segment is semantically closer to the user’s question? — Image by
    the author
  prefs: []
  type: TYPE_NORMAL
- en: When we represent the text chunks and the user’s question as vectors, we gain
    the ability to explore various mathematical possibilities. In order to determine
    the similarity between two data points, we need to calculate their proximity in
    the multidimensional space, which is achieved using distance metrics. There are
    several methods available to compute the distance between points. [Maarten Grootendorst
    has summarized nine of them in one of his Medium posts.](/9-distance-measures-in-data-science-918109d069fa)
  prefs: []
  type: TYPE_NORMAL
- en: 'A commonly used distance metric is cosine similarity. So let’s try to calculate
    the cosine similarity between our question and the text chunks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/239863d9e0969e84b130659926f610c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we have the option to choose the number of text chunks we want to provide
    to our LLM in order to answer the question.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to determine which LLM we would like to use.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Define the model you want to use
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Langchain provides a variety of models and integrations, including OpenAI’s
    GPT and Huggingface, among others. If we decide to use OpenAI’s GPT as our Large
    Language Model, the first step is to define our API Key. Currently, OpenAI offers
    some free usage capacity, but once we exceed a certain number of tokens per month,
    we will need to switch to a paid account.
  prefs: []
  type: TYPE_NORMAL
- en: If we use GPT to answer short questions similar to how we would use Google,
    the costs remain relatively low. However, if we use GPT to answer questions that
    require providing extensive context, such as personal data, the query can quickly
    accumulate thousands of tokens. That increases the cost significantly. But don’t
    worry, you can set a cost limit.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is a token?**'
  prefs: []
  type: TYPE_NORMAL
- en: In simpler terms, a token is basically a word or a group of words. However,
    in English, words can have different forms, such as verb tenses, plurals, or compound
    words. To handle this, we can use sub-word tokenization, which breaks down a word
    into smaller parts like its root, prefix, suffix, and other linguistic elements.
    For example, the word “tiresome” can be split into “tire” and “some,” while “tired”
    can be divided into “tire” and “d.” By doing this, we can recognize that “tiresome”
    and “tired” share the same root and have a similar derivation. (Wang, 2023)
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI offers a tokenizer on its website to get a feel for what a token is.
    According to OpenAI one token generally corresponds to ~4 characters of text for
    common English text. This translates to roughly ¾ of a word (so 100 tokens ~=
    75 words). You can find a [Tokenizer App on OpenAI’s website](https://platform.openai.com/tokenizer)
    that gives you an idea of what actually counts as a token.
  prefs: []
  type: TYPE_NORMAL
- en: '**Set a usage limit**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you are concerned about the cost, you can find an option in your OpenAI user
    portal to limit the monthly costs.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You can find the API key in your user account at OpenAI. The simplest way is
    to search in Google for “OpenAI API key”. This brings you directly to the settings
    page, to create a new key.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use in Python, you have to save the key as a new environment variable with
    the name “OPENAI_API_KEY”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: When you select the LLM you want to use, you can preset a few parameters. The
    [OpenAI Playground](https://platform.openai.com/playground) gives you the possibility
    to play around a bit with the different parameters before you decide what settings
    you want to use.
  prefs: []
  type: TYPE_NORMAL
- en: On the right side in the Playground WebUI, you will find several parameters
    provided by OpenAI that allow us to influence the output of the LLM. Two parameters
    worth exploring are the model selection and the temperature.
  prefs: []
  type: TYPE_NORMAL
- en: You have the option to choose from a variety of different models. The Text-davinci-003
    model is currently the largest and most powerful. On the other hand, models like
    Text-ada-001 are smaller, faster, and more cost-effective.
  prefs: []
  type: TYPE_NORMAL
- en: Below, you can see a summary of [OpenAI’s pricing](https://openai.com/pricing)
    list. Ada is cheaper compared to the most powerful model, Davinci. So if Ada’s
    performance meets our needs, we can not only save money, but also achieve shorter
    response times.
  prefs: []
  type: TYPE_NORMAL
- en: You could begin by using Davinci and then evaluate whether we can achieve good
    enough results with Ada as well.
  prefs: []
  type: TYPE_NORMAL
- en: So let’s try it out in Jupyter Notebook. We are using langchain to connect to
    GPT.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to see a list with all attributes, use __dict__:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a684aa79cd7cce2dbd32c1d847c4ea65.png)'
  prefs: []
  type: TYPE_IMG
- en: If we don’t specify a particular model, the langchain connector defaults to
    using “text-davinci-003”.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can directly invoke the model in Python. Simply call the llm function
    and provide your prompt as input.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3b12ecf96f1efa3c6f40b112aa2b5636.png)'
  prefs: []
  type: TYPE_IMG
- en: You can now ask GPT anything about common human knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1e05a7a00e093c974d44fd9f31b6716.png)'
  prefs: []
  type: TYPE_IMG
- en: GPT can only provide limited information on topics that are not included in
    its training data. This includes specific details that are not publicly available
    or events that occurred after the training data was last updated.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0fcfb9a7848e3a8be48b599bae9d827e.png)'
  prefs: []
  type: TYPE_IMG
- en: '**So, how can we make sure that the models are able to respond to questions
    about current events?**'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, there’s a way to do this. We need to give the model the
    necessary information within the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'To answer the question about the current Prime Minister of the UK, I feed the
    prompt with information from the Wikipedia article “Prime Ministers of the UK”.
    To summarize the process, we are:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading the article
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split the text into text chunks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the embeddings for the text chunks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the similarity between all text chunks and the user’s question
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we try to find the text chunks with the highest similarity to the user’s
    question:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ca95025e46092c6aac7cbbb536db9a81.png)'
  prefs: []
  type: TYPE_IMG
- en: The text chunks look quite messy, but let’s give it a shot and see if GPT is
    clever enough to handle it.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve identified the text segments that potentially hold the relevant
    information, we can test whether our model is capable of answering the question.
    To achieve this, we must construct our prompt in a way that clearly conveys our
    desired task to the model.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Define our Prompt Template
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we have the text snippets that contain the information we are looking for,
    we need to build a prompt. Within the prompt we also specify the desired mode
    for the model to answer questions. When we define the mode we are specifying the
    desired behavior style in which we want the LLM to generate answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The LLM can be utilized for various tasks, and here are a few examples of the
    wide range of possibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Summarization:** “Summarize the following text into 3 paragraphs for executives:
    [TEXT]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Knowledge extraction:** “Based on this article: [TEXT], what should people
    consider before purchasing a home?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Writing content (e.g. mails, messages, code):** Write an email to Jane asking
    for an update on the document for our project. Use an informal, friendly tone.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Grammar and style improvements:** “Correct this to standard English and change
    the tone to a friendlier one: [TEXT]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification:** “Classify each message as a type of support ticket: [TEXT]”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For our example, we want to implement a solution that extracts data from Wikipedia
    and interacts with the user like a chatbot. We want it to answer questions like
    a motivated, helpful help desk expert.
  prefs: []
  type: TYPE_NORMAL
- en: 'To guide the LLM in the right direction, I am adding the following instruction
    to the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '**“You are a chatbot that loves to help people! Answer the following question
    using only the context provided. If you’re unsure and the answer isn’t explicitly
    in the context, say “Sorry, I don’t know how to help you.”**'
  prefs: []
  type: TYPE_NORMAL
- en: By doing this, I set a limitation that only allows GPT to utilize the information
    stored in our database. This restriction enables us to provide the sources our
    chatbot relied upon to generate the response, which is crucial for traceability
    and establishing trust. Additionally, it helps us address the issue of generating
    unreliable information and allows us to provide answers that can be utilized in
    a corporate setting for decision-making purposes.
  prefs: []
  type: TYPE_NORMAL
- en: As the context, I am simply using the top 50 text chunks with the highest similarity
    to the question. A larger size of the text chunks would probably have been better
    since we can usually answer most questions with one or two text passages. But
    I’ll leave it to you to figure out the best size for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'By using that specific template, I am incorporating both the context and the
    user’s question into our prompt. The resulting response is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/90849344b5dd20ee37b89e46d4e84319.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Surprisingly, even this simple implementation seems to have produced some satisfactory
    results. Let’s proceed by asking the system a few more questions regarding British
    prime ministers. I will keep everything unchanged and solely replace the user’s
    question:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/daf31fbecc5113e76cc4f0649d162f63.png)'
  prefs: []
  type: TYPE_IMG
- en: It appears to be functioning to some extent. However, our objective now is to
    transform this slow process into a robust and efficient one. To achieve this,
    we introduce an indexing step where we store our embeddings and indexes in a vector
    store. This will enhance the overall performance and decrease the response time.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Creating a vector store (vector database)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A vector store is a type of data store that is optimized for storing and retrieving
    large quantities of data that can be represented as vectors. These types of databases
    allow for efficient querying and retrieval of subsets of the data based on various
    criteria, such as similarity measures or other mathematical operations.
  prefs: []
  type: TYPE_NORMAL
- en: Converting our text data into vectors is the first step, but it is not enough
    for our needs. If we were to store the vectors in a data frame and search step-by-step
    the similarities between words every time we get a query, the whole process would
    be incredibly slow.
  prefs: []
  type: TYPE_NORMAL
- en: In order to efficiently search our embeddings, we need to index them. Indexing
    is the second important component of a vector database. The index provides a way
    to map queries to the most relevant documents or items in the vector store without
    having to compute similarities between every query and every document.
  prefs: []
  type: TYPE_NORMAL
- en: 'In recent years, a number of vector stores have been released. Especially in
    the field of LLMs, the attention around vector stores has exploded:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/105814f00b193fc06addfb2a85f368b1.png)'
  prefs: []
  type: TYPE_IMG
- en: Release Vector Stores in the past years — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s just pick one and try it out for our use case. Similar to what we
    did in the previous sections, we are again calculating the embeddings and storing
    them in a vector store. To do this, we are using suitable modules from LangChain
    and chroma as a vector store.
  prefs: []
  type: TYPE_NORMAL
- en: '**Collect data that we want to use to answer the users’ questions:**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/f90d9f156966a818815b8c5b4533d1a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**2\. Load the data and define how you want to split the data into text chunks**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d8ae68a1b459338281e1f039d3ce5ebc.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**3\. Define the Embeddings Model you want to use to calculate the embeddings
    for your text chunks and store them in a vector store (here: Chroma)**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/637ae1d82b4e950d0aeb6f96769eb065.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '**4\. Calculate the embeddings for the user’s question, find similar text chunks
    in our vector store and use them to build our prompt**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/730fb905ee3a9f4c5af1ae6d7b5f89cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b31e0fadb9e5b7644a8725ae0a7ccc30.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To enable our LLM to analyze and answer questions about our data, we usually
    don’t fine-tune the model. Instead, during the fine-tuning process, the objective
    is to improve the model’s ability to effectively respond to a specific task, rather
    than teaching it new information.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of Alpaca 7B, the LLM (LLaMA) was fine-tuned to behave and interact
    like a chatbot. The focus was on refining the model’s responses, rather than teaching
    it completely new information.
  prefs: []
  type: TYPE_NORMAL
- en: So to be able to answer questions about our own data, we use the Context Injection
    approach. Creating an LLM app with Context Injection is a relatively simple process.
    The main challenge lies in organizing and formatting the data to be stored in
    a vector database. This step is crucial for efficiently retrieving contextually
    similar information and ensuring reliable results.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the article was to demonstrate a minimalist approach to using embedding
    models, vector stores, and LLMs to process user queries. It shows how these technologies
    can work together to provide relevant and accurate answers, even to constantly
    changing facts.
  prefs: []
  type: TYPE_NORMAL
- en: '*Enjoyed the story?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[*Subscribe for free*](https://dmnkplzr.medium.com/subscribe) *to get notified
    when I publish a new story.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Want to read more than 3 free stories a month? — Become a Medium member for
    5$/month. You can support me by using my* [*referral link*](https://dmnkplzr.medium.com/membership)
    *when you sign up. I’ll receive a commission at no extra cost to you.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Feel free to reach out to me on* [*LinkedIn*](https://www.linkedin.com/in/polzerdo/)
    *!*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AssemblyAI (Director). (2022, January 5). A Complete Overview of Word Embeddings.
    [https://www.youtube.com/watch?v=5MaWmXwxFNQ](https://www.youtube.com/watch?v=5MaWmXwxFNQ)
  prefs: []
  type: TYPE_NORMAL
- en: Grootendorst, M. (2021, December 7). 9 Distance Measures in Data Science. Medium.
    [https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa](/9-distance-measures-in-data-science-918109d069fa)
  prefs: []
  type: TYPE_NORMAL
- en: Langchain. (2023). Welcome to LangChain — 🦜🔗 LangChain 0.0.189\. [https://python.langchain.com/en/latest/index.html](https://python.langchain.com/en/latest/index.html)
  prefs: []
  type: TYPE_NORMAL
- en: Nelson, P. (2023). Search and Unstructured Data Analytics Trends |
  prefs: []
  type: TYPE_NORMAL
- en: Accenture. Search and Content Analytics Blog. [https://www.accenture.com/us-en/blogs/search-and-content-analytics-blog/search-unstructured-data-analytics-trends](https://www.accenture.com/us-en/blogs/search-and-content-analytics-blog/search-unstructured-data-analytics-trends)
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI. (2022). Introducing text and code embeddings. [https://openai.com/blog/introducing-text-and-code-embeddings](https://openai.com/blog/introducing-text-and-code-embeddings)
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI (Director). (2023, March 14). What can you do with GPT-4? [https://www.youtube.com/watch?v=oc6RV5c1yd0](https://www.youtube.com/watch?v=oc6RV5c1yd0)
  prefs: []
  type: TYPE_NORMAL
- en: 'Porsche AG. (2023, May 17). ChatGPT & enterprise knowledge: “How can I create
    a chatbot for my business unit?” #NextLevelGermanEngineering. [https://medium.com/next-level-german-engineering/chatgpt-enterprise-knowledge-how-can-i-create-a-chatbot-for-my-business-unit-4380f7b3d4c0](https://medium.com/next-level-german-engineering/chatgpt-enterprise-knowledge-how-can-i-create-a-chatbot-for-my-business-unit-4380f7b3d4c0)'
  prefs: []
  type: TYPE_NORMAL
- en: Tazzyman, S. (2023). Neural Network models. NLP-Guidance. [https://moj-analytical-services.github.io/NLP-guidance/NNmodels.html](https://moj-analytical-services.github.io/NLP-guidance/NNmodels.html)
  prefs: []
  type: TYPE_NORMAL
- en: Wang, W. (2023, April 12). An In-Depth Look at the Transformer Based Models.
    Medium.
  prefs: []
  type: TYPE_NORMAL
