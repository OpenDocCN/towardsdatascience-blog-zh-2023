# 专家模型的机器学习：入门指南

> 原文：[https://towardsdatascience.com/machine-learning-with-expert-models-a-primer-6c74585f223f](https://towardsdatascience.com/machine-learning-with-expert-models-a-primer-6c74585f223f)

## 数十年前的理念如何使得今天训练极其庞大的神经网络成为可能

[](https://medium.com/@samuel.flender?source=post_page-----6c74585f223f--------------------------------)[![Samuel Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----6c74585f223f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6c74585f223f--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6c74585f223f--------------------------------) [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----6c74585f223f--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6c74585f223f--------------------------------) ·9分钟阅读·2023年9月5日

--

![](../Images/6589b3c96e9f917a19170e43a71a9d9a.png)

（[Pexels](https://www.pexels.com/photo/set-of-tool-wrench-162553/)）

专家模型是机器学习中最有用的发明之一，但它们往往未能获得应有的关注。事实上，专家建模不仅使我们能够训练出“极其庞大”的神经网络（稍后会详细讲述），它们还使我们能够构建出更像人脑的模型，即不同区域专门处理不同类型的输入。

在这篇文章中，我们将探讨专家建模的关键创新，这些创新最终导致了如Switch Transformer和Expert Choice Routing算法这样的最新突破。但首先让我们回顾一下所有这一切的起点： “专家混合模型”。

## 专家混合模型（1991）

![](../Images/ee84508a45288fb4a059d3aeeee05161.png)

1991年的原始MoE模型。图片来源：[Jabocs et al 1991, Adaptive Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)。

专家混合模型（MoE）的理念可以追溯到三十多年前，源于1991年由人工智能奠基人**Geoffrey Hinton**共同撰写的[论文](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)。MoE的核心思想是通过结合多个“专家”E来建模输出“y”，每个专家的权重由“门控网络”G控制：

![](../Images/22bac53e59837ac67d6b84d8f758bb9f.png)

在这个上下文中，专家可以是任何类型的模型，但通常选择的是多层神经网络，门控网络是

![](../Images/f1d03c869186766e071ffdf2da9fe361.png)

其中W是一个可学习的矩阵，用于将训练样本分配给专家。因此，训练MoE模型的学习目标有两个方面：

1.  专家将学习处理他们所得到的输出，生成最佳输出（即预测），并且

1.  门控网络将学习“路由”正确的训练样本到正确的专家，通过联合学习路由矩阵W。

为什么要这样做？为什么有效？从高层次来看，使用这种方法有三个主要动机：

首先，MoE 由于模型的稀疏性允许将神经网络扩展到非常大的规模，即使整体模型很大，由于存在高度专业化的专家，对于任何给定的训练样本，只需执行少量计算。这种方法与标准的“密集型”神经网络形成对比，在标准模型中，每个输入样本都需要每一个神经元。

其次，MoE 允许更好的模型泛化，降低过拟合风险，因为每个单独的专家可以是一个相对较小的神经网络，但通过添加更多专家，我们仍然能实现强大的整体模型性能。类似于提升方法，这是一种将大量相对较弱的学习器结合成一个强大的学习器的方法。

第三，MoE 更接近我们大脑的工作方式：任何给定的输入只会激活我们大脑中的某些区域，不同区域处理触觉、视觉、听觉、嗅觉、味觉、方向感等。这些区域都可以看作是“专家”。

简而言之，MoE 使我们不再需要对每个输入激活每一个神经元。MoE 模型稀疏、高度灵活且极其强大。

## **极其庞大的神经网络**（2017）

快进 26 年到影响深远的论文 “[极其庞大的神经网络](https://arxiv.org/pdf/1701.06538.pdf)”，同样来自 Hinton 的团队（这次在 Google Brain）。在这项工作中，作者将 MoE 推向极限，展示了一个具有千名专家的 60 亿参数 MoE 模型。为了构建如此庞大的 MoE 模型，作者引入了几种建模技巧，包括：

**噪声 top-k 门控。** Top-k 门控意味着对于每个训练样本，我们只计算 top k 专家的输出（由门控确定），忽略其他所有专家。主要动机是节省计算成本：例如，如果我们有20个专家并应用 k=5 的 top-k 门控，我们可以将模型训练的总计算成本减少一个数量级的4倍！

“噪声”意味着我们在门控值中添加可调的高斯随机噪声。作者发现这有助于负载均衡，即确保整批数据不会全部发送到单一专家。

**辅助损失。** 作者将两个辅助（正则化）损失项添加到模型的损失函数中，即“负载均衡损失”和“专家多样性损失”，每个都有其自身可调的正则化参数：

+   负载均衡损失与每个专家接收的训练样本数量的变异系数成正比。增加这个损失可以提高计算性能，因为它防止了“专家瓶颈”的出现，即所有训练样本必须通过一个专家。（一个细节是每个专家的训练样本数量不可微分——因此作者使用了这个数量的平滑近似值。）

+   专家多样性损失与专家重要性的变异系数成正比，其中专家重要性定义为该专家的门值之和。增加这个损失可以促使模型平等地利用所有专家，而不是简单地将所有训练样本发送给一个学习所有内容的专家，这在训练MoE模型中是一个常见问题，也是一个局部最小值。

尽管这两个损失项相似，作者发现当同时添加负载均衡损失和专家多样性损失，并且两个损失项的正则化参数为0.1时，整体性能最佳。

**定制化并行性。** 作者展示了大型MoE模型如何从[数据并行和模型并行的定制组合](/distributed-learning-a-primer-790812b817f1)中受益：特别是，我们可以允许专家分布在不同的机器上，因为我们不需要它们之间的通信，同时使用数据并行性来增加批处理大小。这种并行形式使我们能够大幅扩展MoE模型：在他们的实验中，作者将模型扩展到60亿参数，配备数千个专家。

使用他们的大规模MoE模型，作者在Billion-words语言建模基准上建立了新的SOTA（最先进技术）。

## Switch Transformers (2022)

![](../Images/b3d1e3e8781ff387be9ab7914d8cb929.png)

使用Switch Transformer进行7倍更快的训练。图片来源：[Fedus et al 2022](https://arxiv.org/pdf/2101.03961.pdf)

虽然“极大规模神经网络”展示了top-k门控在MoE模型中的有效性，[Switch Transformers](https://arxiv.org/pdf/2101.03961.pdf)同样由Google提出，通过构建k=1的MoE Transformer模型将其推向了极限，即每个训练样本仅被发送到一个专家。作者称之为“硬路由”，与标准MoE模型中的“软路由”形成对比，在标准MoE模型中，来自多个专家的输出被汇聚。

实际上，硬路由（k=1）意味着我们可以拥有多个专家，数量N可以为*任何*数值，并且计算复杂度保持不变：模型容量按O(1)扩展！唯一的折衷是我们需要大量的内存来存储所有专家。然而，由于我们不要求专家之间的通信，这些内存可以分布在一个大型机器集群中。

此外，作者还介绍了许多实用的建模技巧，包括：

+   **精确投射**：这意味着我们在机器之间传送权重时使用BF16（16位[brain float](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus)），但在计算门控值时将其转换为FP32（32位浮点精度）。这一技巧减少了通信开销，因为我们只需传输16位而非32位，同时确保了softmax计算的稳定性（16位下不稳定）。

+   **激进的专家丢弃**：作者发现，通过在专家模块中应用0.4的激进丢弃，同时在模型架构的其他部分保持0.1的丢弃率，可以提高模型性能。原因是专家容易过拟合，因为它们仅看到一部分数据，因此需要更强的正则化。

+   **缩放的专家初始化**：作者发现，当将专家层的初始化缩小10倍时，训练稳定性显著提高。

最终，作者基于Google的T5语言模型构建了一个Switch Transformer，并在相同计算资源下实现了7倍的预训练速度提升，这强有力地展示了通过将基于硬路由的MoE与Transformer模型结合所能带来的建模改进。

## 专家选择路由（2022）

![](../Images/45da8d3241e0d84578938532e41ce163.png)

标准的令牌选择路由（左）与新的专家选择路由算法（右）相比。请注意，在专家选择路由中，一个令牌可以同时路由到多个专家。图像来源：[Zhou et al 2022](https://arxiv.org/abs/2202.09368)

近期在专家建模领域的突破之一是“[专家选择路由](https://arxiv.org/abs/2202.09368)”，这是Google的又一创新。

训练MoE模型的问题在于专家通常由于“赢家通吃”效应而未被充分利用，其中一个随机接收到前几个训练样本的专家迅速变得比其他专家更优秀并主导门控。到目前为止，标准做法是添加辅助损失，以促使模型平等利用所有专家（如Hinton工作中的专家多样性损失）。然而，添加辅助损失带来了调整其正则化参数相对于实际损失的难题：太小，则模型行为无变化；太大，则风险使所有专家相同（另一个局部最小值）。

“专家选择路由”的关键思想简单却强大：让专家在批次中选择其最优的k个训练样本，而不是让训练样本选择其最优的k个专家。这有几个优点：

+   它保证了负载均衡和专家利用率的完美一致（每个专家*总是*在每个批次中接收相同数量的训练样本），

+   并非每个训练样本都有相同数量的专家，有些可能被分配给1位专家，有些则分配给所有专家。这是一种期望的特性：不同的训练样本对模型而言难度不同，因此可能需要不同数量的专家。

+   不需要调整的额外辅助损失。

从数学上讲，专家选择路由替代了标准的门控函数。

![](../Images/629e1928f81ac1ab4fe6518224904ef0.png)

与

![](../Images/2f0640c537b330ab652b38037fe5b352.png)

也就是说，与e维向量不同，门控G现在是一个维度为e（专家数量）乘以n（令牌数量）的矩阵。为什么是令牌？作者将专家选择路由考虑在NLP问题的背景下，因此每个训练样本只是n个可能令牌的序列。

到目前为止，这些理论（实际上就是将G从向量转换为矩阵），但它在实践中表现如何？作者展示了他们可以在一半的训练时间内达到与**Switch Transformer**相同的性能，并且在相同的计算成本下，在GLUE和SuperGLUE基准测试的11项任务上超越了**Switch Transformer**。

**专家选择路由**击败了**Switch Transformer**，这证明了至少对于NLP问题而言，“专家选择”优于“令牌选择”。

## 摘要

简要回顾：

+   MoE模型中的关键思想是将输出y建模为专家的加权和，其中每个专家本身就是一个小型神经网络，权重由G(x) = softmax(Wx)决定，其中W是一个可学习的矩阵。

+   top-k门控意味着我们忽略所有专家，只关注前k个。这节省了大量计算成本，并且是专家建模中的一个重要突破。

+   MoE模型经常陷入局部最小值，即所有训练样本都被送往单个专家。解决方法是向模型的损失函数中添加“专家多样性损失”。

+   **Switch Transformers**将MoE的概念与**Transformer**模型结合，并展示了这种组合可以将T5语言模型的训练速度提高7倍。这里的一个关键创新是“硬路由”，即k=1的top-k路由。

+   **专家选择路由**用专家选择训练样本的概念取代了训练样本选择专家的概念。这允许更好的训练稳定性而无需引入额外的辅助损失。

这仅仅是冰山一角。事实上，受到**Hinton**“极大规模”MoE模型、**Switch Transformer**和新专家选择路由算法成功的启发，专家建模领域涌现了大量新的论文。

![](../Images/6bc1c533d38fc68d148ffdc1efbeb8ed.png)

关于MoE模型的研究论文集，其中“专家选择”论文用红色突出显示，来源：[ConnectedPapers](https://www.connectedpapers.com/main/bbc57e1b3cf90e09b64377f13de455793bc81ad5/Mixture%20of%20Experts-with-Expert-Choice-Routing/graph)。

专家建模是一个令人兴奋的领域，经过数十年的发展，我们才刚刚开始看到它对现代机器学习应用的影响。请关注这个领域——新的突破无疑在即！
