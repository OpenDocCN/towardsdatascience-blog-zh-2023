- en: 'A Birds-Eye View of Linear Algebra: Why Is Matrix Multiplication Like That?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-birds-eye-view-of-linear-algebra-why-is-matrix-multiplication-like-that-a4d94067651e](https://towardsdatascience.com/a-birds-eye-view-of-linear-algebra-why-is-matrix-multiplication-like-that-a4d94067651e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why should the columns of the first matrix match the rows of the second? Why
    not have the rows of both match?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@rohitpandey576?source=post_page-----a4d94067651e--------------------------------)[![Rohit
    Pandey](../Images/af817d8f68f2984058f0afb8fd7ecbe9.png)](https://medium.com/@rohitpandey576?source=post_page-----a4d94067651e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a4d94067651e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a4d94067651e--------------------------------)
    [Rohit Pandey](https://medium.com/@rohitpandey576?source=post_page-----a4d94067651e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a4d94067651e--------------------------------)
    ·18 min read·Nov 23, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2512b5389933ce8260d05fe8308ab411.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created with midjourney
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the third chapter of the in-progress book on linear algebra, “A birds
    eye view of linear algebra”. The table of contents so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter-1: The basics](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-the-basics-29ad2122d98f)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chapter-2: [The measure of a map — determinants](https://medium.com/p/1e5fd752a3be)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Chapter-3:** (Current) Why is matrix multiplication the way it is?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chapter-4: [Systems of equations, linear regression and neural networks](https://medium.com/p/fe5b88a57f66)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chapter-5: [Rank nullity and why row rank == col rank](/a-birds-eye-view-of-linear-algebra-rank-nullity-and-why-row-rank-equals-column-rank-bc084e0e1075)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, we will describe operations we can do with two matrices, but keeping in
    mind they are just representations of linear maps.
  prefs: []
  type: TYPE_NORMAL
- en: I) Why care about matrix multiplication?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Almost any information can be embedded in a vector space. Images, video, language,
    speech, biometric information and whatever else you can imagine. And all the applications
    of machine learning and artificial intelligence (like the recent chat-bots, text
    to image, etc.) work on top of these vector embeddings. Since linear algebra is
    the science of dealing with high dimensional vector spaces, it is an indispensable
    building block.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/359f7de8bc4e3ed8bbd719f387d18415.png)'
  prefs: []
  type: TYPE_IMG
- en: Complex concepts from our real world like images, text, speech, etc. can be
    embedded in high dimensional vector spaces. The higher the dimensionality of the
    vector space, the more complex information it can encode. Image created with midjourney.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of the techniques involve taking some input vectors from one space and
    mapping them to other vectors from some other space.
  prefs: []
  type: TYPE_NORMAL
- en: But why the focus on “linear” when most interesting functions are non-linear?
    It’s because the problem of making our models high dimensional and that of making
    them non-linear (general enough to capture all kinds of complex relationships)
    turn out to be orthogonal to each other. Many neural network architectures work
    by using linear layers with simple one dimensional non-linearities in between
    them. And there is a theorem that says this kind of architecture can model any
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Since the way we manipulate high dimensional vectors is primarily matrix multiplication,
    it isn’t a stretch to say it is the bedrock of the modern AI revolution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1eb2d34de16b7a676591669ffcf655b0.png)'
  prefs: []
  type: TYPE_IMG
- en: Deep neural networks have layers with vectors at each layer and connections
    between successive layers encoded as matrices. Translation between layers happens
    with linear algebra and matrix multiplication. Image created with midjourney.
  prefs: []
  type: TYPE_NORMAL
- en: II) Algebra on maps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/57446621a614cbd30acff5168fb9b656.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created with midjourney
  prefs: []
  type: TYPE_NORMAL
- en: In [chapter 2](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-the-measure-of-a-map-determinant-1e5fd752a3be),
    we learnt how to quantify linear maps with determinants. Now, let’s do some algebra
    with them. We’ll need two linear maps and a basis.
  prefs: []
  type: TYPE_NORMAL
- en: II-A) Addition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we can add matrices, we can add linear maps since matrices are the representations
    of linear maps. And matrix addition is not very interesting if you know scalar
    addition. Just as with vectors, it’s only defined if the two matrices are the
    same size (same rows and columns) and involves lining them up and adding element
    by element.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e9e135280d6c0614afaa336d2d0858de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Animation-0: Addition of two matrices. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: So, we’re just doing a bunch of scalar additions. Which means that the properties
    of scalar addition logically extend.
  prefs: []
  type: TYPE_NORMAL
- en: '**Commutative: if you switch, the result won’t twitch**'
  prefs: []
  type: TYPE_NORMAL
- en: '*A+B = B+A*'
  prefs: []
  type: TYPE_NORMAL
- en: '*But commuting to work might not be commutative since going from A to B might
    take longer than B to A.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Associative: in a chain, don’t refrain, take any 2 and continue**'
  prefs: []
  type: TYPE_NORMAL
- en: '***A+(B+C) = (A+B)+C***'
  prefs: []
  type: TYPE_NORMAL
- en: '**Identity: And here I am where I began! That’s no way to treat a man!**'
  prefs: []
  type: TYPE_NORMAL
- en: The presence of a special element that when added to anything results in the
    same thing. In the case of scalars, it is the number *0*. In the case of matrices,
    it is a matrix full of zeros.
  prefs: []
  type: TYPE_NORMAL
- en: '***A + 0 = A or 0 + A = A***'
  prefs: []
  type: TYPE_NORMAL
- en: Also, it is possible to start at any element and end up at any other via addition.
    So it must be possible to start at *A* and end up at the additive identity, *0*.
    The thing that must be added to *A* to achieve this is the additive inverse of
    *A* and it’s called *-A.*
  prefs: []
  type: TYPE_NORMAL
- en: '*A + (-A) = 0*'
  prefs: []
  type: TYPE_NORMAL
- en: For matrices, you just go to each scalar element in the matrix and replace with
    the additive inverse of each one (switching the signs if the scalars are numbers)
    to get the additive inverse of the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: II-B) Subtraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Subtraction is just addition with the additive inverse of the second matrix
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: '*A-B = A+(-B)*'
  prefs: []
  type: TYPE_NORMAL
- en: II-C) Multiplication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We could have defined matrix multiplication just as we defined matrix addition.
    Just take two matrices that are the same size (rows and columns) and then multiply
    the scalars element by element. There is a name for that kinds of operation, the
    [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)).
  prefs: []
  type: TYPE_NORMAL
- en: But no, we defined matrix multiplication as a far more convoluted operation,
    more “exotic” than addition. And it isn’t complex just for the sake of it. It
    is the most important operation in linear algebra by far.
  prefs: []
  type: TYPE_NORMAL
- en: It enjoys this special status because it’s the means by which linear maps are
    applied to vectors, building on top of dot products.
  prefs: []
  type: TYPE_NORMAL
- en: The way it actually works requires a dedicated section, so we’ll cover that
    in section III. Here, let’s list some of its properties.
  prefs: []
  type: TYPE_NORMAL
- en: '**Commutative**'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike addition, matrix multiplication is not always commutative. Which means
    that the order in which you apply linear maps to your input vector matters.
  prefs: []
  type: TYPE_NORMAL
- en: '*A.B != B.A*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Associative**'
  prefs: []
  type: TYPE_NORMAL
- en: It is still associative
  prefs: []
  type: TYPE_NORMAL
- en: '*A.B.C = A.(B.C) = (A.B).C*'
  prefs: []
  type: TYPE_NORMAL
- en: And there is a lot of depth to this property, as we will see in section IV.
  prefs: []
  type: TYPE_NORMAL
- en: '**Identity**'
  prefs: []
  type: TYPE_NORMAL
- en: Just like addition, matrix multiplication also has an identity element, *I,*
    an element that when any matrix is multiplied to results in the same matrix. The
    big caveat being that this element only exists for square matrices and is itself
    square.
  prefs: []
  type: TYPE_NORMAL
- en: Now, because of the importance of matrix multiplication, “the identity matrix”
    in general is defined as the identity element of matrix multiplication (not that
    of addition or the Hadamard product for example).
  prefs: []
  type: TYPE_NORMAL
- en: 'The identity element for addition is a matrix composed of *0*’s and that of
    the Hadamard product is a matrix composed of *1*’s. The identity element of matrix
    multiplication is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/66b7e53fa0d6d41acf5f6a8968fa78a4.png)'
  prefs: []
  type: TYPE_IMG
- en: The identity matrix of matrix multiplication and also linear algebra. Image
    by author.
  prefs: []
  type: TYPE_NORMAL
- en: So, *1*’s on the main diagonal and *0*’s everywhere else. What kind of definition
    for matrix multiplication would lead to an identity element like this? We’ll need
    to describe how it works to see, but first let’s go to the final operation.
  prefs: []
  type: TYPE_NORMAL
- en: II-D) Division
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just as with addition, the presence of an identity matrix suggests any matrix,
    *A* can be multiplied with another matrix, *A^-1* and taken to the identity. This
    is called the inverse. Since matrix multiplication isn’t commutative, there are
    two ways to this. Thankfully, both lead to the identity matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '*A.(A^-1) = (A^-1).A = I*'
  prefs: []
  type: TYPE_NORMAL
- en: So, “dividing” a matrix by another is simply multiplication with the second
    ones inverse, *A.B^-1*. If matrix multiplication is very important, then this
    operation is as well since it’s the inverse. It is also related to how we historically
    developed (or maybe stumbled upon) linear algebra. But more on that in the next
    chapter (4).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another property we’ll be using that is a combined property of addition and
    multiplication is the distributive property. It applies to all kinds of matrix
    multiplication from the traditional one to the Hadamard product:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A.(B+C) = A.B + A.C*'
  prefs: []
  type: TYPE_NORMAL
- en: III) Why is matrix multiplication defined this way?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have arrived at last to the section where we will answer the question in
    the title, the meat of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix multiplication is the way linear maps act on vectors. So, we get to motivate
    it that way.
  prefs: []
  type: TYPE_NORMAL
- en: III-A) How are linear maps applied in practice?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider a linear map that takes *m* dimensional vectors (from *R^m*) as input
    and maps them to *n* dimensional vectors (in *R^n*). Let’s call the *m* dimensional
    input vector, *v*.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, it might be helpful to think of yourself actually coding up this
    linear map in some programming language. It should be a function that takes the
    m-dimensional vector, *v* as input and returns the *n* dimensional vector, *u*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The linear map has to take this vector and turn it into an *n* dimensional vector
    somehow. In the function above, you’ll notice we just generated some vector at
    random. But this completely ignored the input vector, *v*. That’s unreasonable,
    *v* should have some say. Now, *v* is just an ordered list of *m* scalars *v =
    [v1, v2, v3, …, vm]*. What do scalars do? They scale vectors. And the output vector
    we need should be *n* dimensional. How about we take some (fixed) *m* vectors
    (pulled out of thin air, each *n* dimensional), *w1, w2, …, wm*. Then, scale *w1*
    by v*1*, *w2* by v*2* and so on and add them all up. This leads to an equation
    for our linear map (with the output on the left).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff8453f94f83af00cc41ad37db8b27c9.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq (1) A linear map motivated as a linear combination of m vectors. Image by
    author.
  prefs: []
  type: TYPE_NORMAL
- en: Make note of the equation (1) above since we’ll be using it again.
  prefs: []
  type: TYPE_NORMAL
- en: Since the *w1*, *w2,…* are all *n* dimensional, so is *u.* And all the elements
    of *v=[v1, v2, …, vm]* have an influence on the output, *u.* The idea in equation
    (1) is implemented below. We take some randomly generated vectors for the *w*’s
    but with fixed seeds (ensuring that the vectors are the same across every call
    of the function).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We have a way now to “map” *m* dimensional vectors (*v)* to *n* dimensional
    vectors (*u)*. But does this “map” satisfy the properties of a linear map? Recall
    from chapter-1, section II the properties of a linear map, *f* (here, *a* and
    *b* are vectors and *c* is a scalar):'
  prefs: []
  type: TYPE_NORMAL
- en: '*f(a+b) = f(a) + f(b)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*f(c.a) = c.f(a)*'
  prefs: []
  type: TYPE_NORMAL
- en: It’s clear that the map specified by equation (1) satisfies the above two properties
    of a linear map.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf3004024206b300d57137c118695129.png)'
  prefs: []
  type: TYPE_IMG
- en: The function defined by equation (1) satisfies the property that the function
    of sum is the sum of functions. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/019f8e8cc74f75ac4b3844d59b696145.png)'
  prefs: []
  type: TYPE_IMG
- en: The function defined in equation (1) satisfies the property that a scalar times
    a vector passed to the function is equivalent to the scalar times the vector passed
    to the function. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The *m* vectors, *w1, w2, …, wm* are arbitrary and no matter what we choose
    for them, the function, *f* defined in equation (1) is a linear map. So, different
    choices for those *w* vectors results in different linear maps. Moreover, for
    any linear map you can imagine, there will be some vectors *w1, w2,…* that can
    be applied in conjunction with equation (1) to represent it.
  prefs: []
  type: TYPE_NORMAL
- en: Now, for a given linear map, we can collect the vectors *w1, w2,…* into the
    columns of a matrix. Such a matrix will have *n* rows and *m* columns. This matrix
    represents the linear map, *f* and its multiplication with an input vector, *v*
    represents the application of the linear map, *f* to *v*. And this application
    is where the definition of matrix multiplication comes from.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7cfaab8913283d2eef05d37e5b65326.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Animation (1): Matrix vector multiplication. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now see why the identity element for matrix multiplication is the way
    it is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8f795682bef71d6ae09a2baea562089.png)'
  prefs: []
  type: TYPE_IMG
- en: Animation (2) Why is the identity matrix of multiplication what it is? Image
    by author.
  prefs: []
  type: TYPE_NORMAL
- en: We start with a column vector, *v* and end with a column vector, *u* (so just
    one column for each of them). And since the elements of *v* must align with the
    column vectors of the matrix representing the linear map, the number of columns
    of the matrix must equal the number of elements in *v*. More on this in section
    III-C.
  prefs: []
  type: TYPE_NORMAL
- en: III-B) Matrix multiplication as a composition of linear maps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we described how a matrix is multiplied to a vector, we can move on
    to multiplying a matrix with another matrix.
  prefs: []
  type: TYPE_NORMAL
- en: The definition of matrix multiplication is much more natural when we consider
    the matrices as representations of linear maps.
  prefs: []
  type: TYPE_NORMAL
- en: Linear maps are functions that take a vector as input and produce a vector as
    output. Let’s say the linear maps corresponding to two matrices are *f* and *g*.
    How would you think of adding these maps *(f+g)*?
  prefs: []
  type: TYPE_NORMAL
- en: '*(f+g)(v) = f(v)+g(v)*'
  prefs: []
  type: TYPE_NORMAL
- en: This is reminiscent of the distributive property of addition where the argument
    goes inside the bracket to both the functions and we add the results. And if we
    fix a basis, this corresponds to applying both linear maps to the input vector
    and adding the result. By the distributive property of matrix and vector multiplication,
    this is the same as adding the matrices corresponding to the linear maps and applying
    the result to the vector.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s think of multiplication *(f.g)*.
  prefs: []
  type: TYPE_NORMAL
- en: '*(f.g)(v) = f(g(v))*'
  prefs: []
  type: TYPE_NORMAL
- en: Since linear maps are functions, the most natural interpretation of multiplication
    is to compose them (apply them one at a time, in sequence to the input vector).
  prefs: []
  type: TYPE_NORMAL
- en: When two matrices are multiplied, the resulting matrix represents the composition
    of the corresponding linear maps. Consider matrices A and B; the product *AB*
    embodies the transformation achieved by applying the linear map represented by
    *B* to the input vector first and then applying the linear map represented by
    *A*.
  prefs: []
  type: TYPE_NORMAL
- en: So we have a linear map corresponding to the matrix, *A* and a linear map corresponding
    to the matrix, *B*. We’d like to know the matrix, *C* corresponding to the composition
    of the two linear maps. So, applying *B* to any vector first and then applying
    *A* to the result should be equivalent to just applying *C.*
  prefs: []
  type: TYPE_NORMAL
- en: '*A.(B.v) = C.v = (A.B).v*'
  prefs: []
  type: TYPE_NORMAL
- en: In the last section, we learnt how to multiply a matrix and a vector. Let’s
    do that twice for *A.(B.v).* Say the columns of *B* are the column vectors, *b1,
    b2, …, bm.* From equation (1) in the previous section,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55f3db899766ddec60fad8d62cea67a3.png)'
  prefs: []
  type: TYPE_IMG
- en: Proving that matrix multiplication is just linear maps applied in sequence.
    Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: And what if we applied the linear map corresponding to *C=A.B* directly to the
    vector, *v.* The column vectors of the matrix *C* are *c1, c2, …, ck.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8f851974344b774f823d6e290325a22e.png)'
  prefs: []
  type: TYPE_IMG
- en: The same result as animation (2), multiplying a matrix with a vector. Image
    by author.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the two equations above we get,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3717d381958068a05e19f6d392d5168d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Eq (2): the column vectors of the matrix C = AB where b1, b2,… are the column
    vectors of B. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: So, the columns of the product matrix, *C=AB* are obtained by applying the linear
    map corresponding to matrix *A* to each of the columns of the matrix *B.* And
    collecting those resulting vectors into a matrix gives us *C*.
  prefs: []
  type: TYPE_NORMAL
- en: We have just extended our matrix-vector multiplication result from the previous
    section to the multiplication of two matrices. We just break the second matrix
    into a collection of vectors, multiply the first matrix to all of them and collect
    the resulting vectors into the columns of the result matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/81ed7b47ad5c845fcecf966b514563be.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq (3) The column vectors of the C matrix (C=A.B). Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: So the first row and first column of the result matrix, *C* is the dot product
    of the first column of *B* and the first row of *A.* And in general the *i*-th
    row and *j*-th column of *C* is the dot product of the *i*-th row of *A* and the
    *j*-th column of *B.* This is the definition of matrix multiplication most of
    us first learn.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52d4de8e1be991023858062a4b73c900.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Animation (3): Matrix multiplication as dot products. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Associative proof**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also show that matrix multiplication is associative now. Instead of
    the single vector, *v*, let’s apply the product C=*AB* individuallyto a group
    of vectors, *w1, w2, …, wl*. Let’s say the matrix that has these as column vectors
    is *W*. We can use the exact same trick as above to show:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(A.B).W = A.(B.W)*'
  prefs: []
  type: TYPE_NORMAL
- en: It’s because *(A.B).w1 = A.(B.w1)* and the same for all the other *w* vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sum of outer products**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we’re multiplying two matrices *A* and *B*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/829020840b10a380d034fe52c71c8eab.png)'
  prefs: []
  type: TYPE_IMG
- en: Multiplication of two matrices. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Equation (3) can be generalized to show that the *i*,*j* element of the resulting
    matrix, *C* is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ca37125b6f2da37bcf5ffae9aae53412.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Eq (4): Generalization of equation (3). Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have a sum over *k* terms. What if we took each of those terms and created
    *k* individual matrices out of them. For example, the first matrix will have as
    its *i*,*j-*th entry: *b_{i,1}. a_{1,j}*. The *k* matrices and their relationship
    to *C:*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e2b65fa5b4b005b938f3164a290daa5.png)'
  prefs: []
  type: TYPE_IMG
- en: Matrix multiplication is the sum of k sub-matrices. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process of summing over *k* matrices can be visualized as follows (reminiscent
    of the animation in section III-A that visualized a matrix multiplied to a vector):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38c2f82ac398fcfa5d6ed1b09133f768.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Animation (4): Matrix multiplication by expanding to 3-d. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: We see here the sum over *k* matrices all of the same size (*n*x*m*) which is
    the same size as the result matrix, *C*. Notice in equation (4) how for the first
    matrix, *A*, the column index stays the same while for the second matrix, *B*,
    the row index stays the same. So the *k* matrices we’re getting are the matrix
    products of the *i*-th column of *A* and the *i*-th **row** of *B*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3148dbd9e5da635bb3ba467c4ba0997.png)'
  prefs: []
  type: TYPE_IMG
- en: Matrix multiplication as a sum of outer products. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the summation, two vectors are multiplied to produce matrices. It’s
    a special case of matrix multiplication when applied to vectors (special cases
    of matrices) and called “outer product”. Here is yet another animation to show
    this sum of outer products process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/64038aec7fcf352594bed9a59b424235.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Animation (5): Matrix multiplication as a sum of outer products. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: This tells us why the number of row vectors in *B* should be the same as the
    number of column vectors in *A*. Because they have to be mapped together to get
    the individual matrices.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve seen a lot of visualizations and some math, now let’s see the same thing
    via code for the special case where *A* and *B* are square matrices. This is based
    on section 4.2 of the book Introduction to Algorithms, [2].
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'III-C) Matrix multiplication: the structural choices'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/d317ff7f16031fd1f6b778bc4788e2eb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created with midjourney
  prefs: []
  type: TYPE_NORMAL
- en: Matrix multiplication seems to be structured in a weird way. It’s clear that
    we need to take a bunch of dot products. So, one of the dimensions has to match.
    But why make the columns of the first matrix be equal to the number of rows of
    the second one?
  prefs: []
  type: TYPE_NORMAL
- en: Won’t it make things more straightforward if we redefine it in a way that the
    number of rows of the two matrices should be the same (or the number of columns)?
    This would make it much easier to identify when two matrices can be multiplied.
  prefs: []
  type: TYPE_NORMAL
- en: The traditional definition where we require the rows of the first matrix to
    align with the columns of the second one has more than one advantage. Let’s go
    first to matrix-vector multiplication. Animation (1) in section III-A showed us
    how the traditional version works. Let’s visualize what it if we required the
    rows of the matrix to align with the number of elements in the vector instead.
    Now, the *n* rows of the matrix will need to align with the *n* elements of the
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a5d51746d8fad51a27f5dba51300a5bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Animation (6): what would an alternate setup for matrix multiplication look
    like? Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: We see that we’d have to start with a column vector, *v* with *n* rows and one
    column and end up with a row vector, *u* with *1* row and *m* columns. This is
    awkward and makes defining an identity element for matrix multiplication challenging
    since the input and output vectors can never have the same shape. With the traditional
    definition, this isn’t an issue since the input is a column vector and the output
    is also a column vector (see animation (1)).
  prefs: []
  type: TYPE_NORMAL
- en: Another consideration is multiplying a chain of matrices. In the traditional
    method, it is so easy to see first of all that the chain of matrices below can
    be multiplied together based on their dimensionalities.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/034ee3517b1dbb116eeb38dd902efad5.png)'
  prefs: []
  type: TYPE_IMG
- en: What matrix chain multiplication looks like with the traditional accepted method.
    Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Further, we can tell that the output matrix will have *l* rows and *p* columns.
  prefs: []
  type: TYPE_NORMAL
- en: In the framework where the rows of the two matrices should line up, this quickly
    becomes a mess. For the first two matrices, we can tell that the rows should align
    and that the result will have *n* rows and *l* columns. But visualizing how many
    rows and columns the result will have and then reasoning about weather it’ll be
    compatible with *C*, etc. becomes a nightmare.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb16626a5b91d35097a894ec7282d6c4.png)'
  prefs: []
  type: TYPE_IMG
- en: What matrix chain multiplication might look like if we modified the definition
    of matrix multiplication. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: And that is why we require the rows of the first matrix to align with the columns
    of the second matrix. But maybe I missed something. Maybe there is an alternate
    definition that is “cleaner” and manager to side-step these two challenges. Would
    love to hear ideas in the comments :)
  prefs: []
  type: TYPE_NORMAL
- en: III-D) Matrix multiplication as a change of basis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we’ve thought of matrix multiplication with vectors as a linear map
    that takes a vector as input and returns some other vector as output. But there
    is another way to think of matrix multiplication — as a way to change perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider two-dimensional space, *R².* We represent any vector in this
    space with two numbers. What do those numbers represent? The coordinates along
    the x-axis and y-axis. A unit vector that points just along the x-axis is *[1,0]*
    and one that points along the y-axis is *[0,1]*. These are our basis for the space.
    Every vector now has an address. For example, the vector *[2,3]* means we scale
    the first basis vector by *2* and the second one by *3*.
  prefs: []
  type: TYPE_NORMAL
- en: But this isn’t the only basis for the space. Someone else (say, he who shall
    not be named) might want to use two other vectors as their basis. For example,
    the vectors *e1=[3,2]* and *e2=[1,1]*. Any vector in the space *R²* can also be
    expressed in their basis. The same vector would have different representations
    in our basis and their basis. Like different addresses for the same house (perhaps
    based on different postal systems).
  prefs: []
  type: TYPE_NORMAL
- en: When we’re in the basis of he who shall not be named, the vector *e1 = [1,0]*
    and the vector *e2 = [0,1] (*which are the basis vectors from his perspective
    by definition of basis vectors). And the functions that translates vectors from
    our basis system to that of he who shall not be named and vise-versa are linear
    maps. And so the translations can be represented as matrix multiplications. Let’s
    call the matrix that takes vectors from us to the vectors to he who shall not
    be named, *M1* and the matrix that does the opposite, *M2\.* How do we find the
    matrices for these matrices?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bdd89151aa58d32c4ad7d74f701b8d01.png)'
  prefs: []
  type: TYPE_IMG
- en: Shifting your perspective in seeing the world. Image by midjourney. Image by
    author.
  prefs: []
  type: TYPE_NORMAL
- en: We know that the vectors we call *e1=[3,2]* and *e2=[1,1],* he who shall not
    be named calls *e1=[1,0]* and *e2=[0,1].* Let’s collect our version of the vectors
    into the columns of a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c36906403917474a9f09bae0a2441edf.png)'
  prefs: []
  type: TYPE_IMG
- en: A basic 2 by 2 matrix composed of two column vectors. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: And also collect the vectors, *e1* and *e2* of he who shall not be named into
    the columns of another matrix. This is just the identity matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e5ced9ba006de4b7d3bdc4f8a9abd423.png)'
  prefs: []
  type: TYPE_IMG
- en: We’d like to transform the 2 by 2 matrix into the identity matrix to change
    basis. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Since matrix multiplication operates independently on the columns of the second
    matrix,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bbe606253de58d7921b2c29a9cacae9c.png)'
  prefs: []
  type: TYPE_IMG
- en: The equation that moves the matrix to the identity. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-multiplying by an appropriate matrix on both sides gives us *M1:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/58516bf67ec981508e7bb87a71fa9c8e.png)'
  prefs: []
  type: TYPE_IMG
- en: Changing basis with a matrix. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Doing the same thing in reverse gives us *M2:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d3b9a7e810226d19f4bfe489a5072f25.png)'
  prefs: []
  type: TYPE_IMG
- en: Reverse mapping to basis. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can all be generalized into the following statement: A matrix with column
    vectors; *w1, w2, …, wn* translates vectors expressed in a basis where *w1, w2,
    …, wn* are the basis vectors to our basis.'
  prefs: []
  type: TYPE_NORMAL
- en: And the inverse of that matrix translates vectors from our basis to the one
    where *w1, w2, …, wn* are the basis.
  prefs: []
  type: TYPE_NORMAL
- en: All square matrices can hence be thought of as “basis changers”.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: In the special case of an orthonormal matrix (where every column is a
    unit vector and orthogonal to every other column), the inverse becomes the same
    as the transpose. So, changing to the basis of the columns of such a matrix becomes
    equivalent to taking the dot product of a vector with each of the rows.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For more on this, see the [3B1B video, [1]](https://www.youtube.com/watch?v=P2LTAUO1TdA&t=2s).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Matrix multiplication is arguably one of the most important operations in modern
    computing and also with almost any data science field. Understanding deeply how
    it works is important for any data scientist. Most linear algebra textbooks describe
    the “what” but not why its structured the way it is. Hopefully this blog filled
    that gap.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] 3B1B video on change of basis: [https://www.youtube.com/watch?v=P2LTAUO1TdA&t=2s](https://www.youtube.com/watch?v=P2LTAUO1TdA&t=2s)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Introduction to Algorithms by Cormen et.al. Third edition'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Matrix multiplication as sum of outer products: [https://math.stackexchange.com/questions/2335457/matrix-at-a-as-sum-of-outer-products](https://math.stackexchange.com/questions/2335457/matrix-at-a-as-sum-of-outer-products)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Catalan numbers wikipedia article [https://en.wikipedia.org/wiki/Catalan_number](https://en.wikipedia.org/wiki/Catalan_number)'
  prefs: []
  type: TYPE_NORMAL
