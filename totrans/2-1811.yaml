- en: Scene Representation Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 场景表示网络
- en: 原文：[https://towardsdatascience.com/scene-representation-networks-bae6186d00d9](https://towardsdatascience.com/scene-representation-networks-bae6186d00d9)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/scene-representation-networks-bae6186d00d9](https://towardsdatascience.com/scene-representation-networks-bae6186d00d9)
- en: Modeling complex 3D scenes at infinite resolution
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在无限分辨率下建模复杂的3D场景
- en: '[](https://wolfecameron.medium.com/?source=post_page-----bae6186d00d9--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----bae6186d00d9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bae6186d00d9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bae6186d00d9--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----bae6186d00d9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----bae6186d00d9--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----bae6186d00d9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bae6186d00d9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bae6186d00d9--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----bae6186d00d9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bae6186d00d9--------------------------------)
    ·12 min read·Feb 22, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bae6186d00d9--------------------------------)
    ·阅读时间12分钟·2023年2月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b9e69450b3fa7f9dc92eb4c50cf7c7bc.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9e69450b3fa7f9dc92eb4c50cf7c7bc.png)'
- en: (Photo by [Alexandra Gorn](https://unsplash.com/@alexagorn?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/living-room?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （照片由 [Alexandra Gorn](https://unsplash.com/@alexagorn?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/s/photos/living-room?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)）
- en: We have seen recently (e.g., with [DeepSDF](https://cameronrwolfe.substack.com/p/3d-generative-modeling-with-deepsdf)
    and [ONets](https://cameronrwolfe.substack.com/p/shape-reconstruction-with-onets)
    [3, 5]) how 3D geometries can be represented with a neural net. But, these approaches
    have some limitations, such as requiring access to ground truth, 3D geometries
    for training and inference. Plus, *what if we want to represent an entire scene
    and not just an object or geometry*? This requires modeling both geometry and
    appearance; see below for an example. Luckily, neural nets are more than capable
    of modeling 3D scenes in this way given the correct approach.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最近已经看到（例如，通过[DeepSDF](https://cameronrwolfe.substack.com/p/3d-generative-modeling-with-deepsdf)和[ONets](https://cameronrwolfe.substack.com/p/shape-reconstruction-with-onets)
    [3, 5]）如何用神经网络表示3D几何体。但是，这些方法有一些限制，比如需要访问真实数据、3D几何体进行训练和推断。而且，*如果我们想表示整个场景而不仅仅是一个物体或几何体*呢？这需要同时建模几何体和外观；下面有一个示例。幸运的是，神经网络在正确的方法下完全能够以这种方式建模3D场景。
- en: '![](../Images/0c7b3bc733de45910ed93862eab0bc9f.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c7b3bc733de45910ed93862eab0bc9f.png)'
- en: (from [1] and [3])
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于 [1] 和 [3]）
- en: One approach for modeling 3D scenes is via Scene Representation Networks (SRNs)
    [1]. SRNs model scenes as a continuous function that maps each 3D coordinate to
    a representation that describes the object’s shape and appearance at that location.
    This function is learned with a feed-forward neural network. Then, SRNs use a
    learnable rendering algorithm to produce novel viewpoints (i.e., just 2D images)
    of the underlying 3D scene. Both the feed-forward network and the rendering algorithm
    can be trained end-to-end using only 2D images of scenes for supervision.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一种建模3D场景的方法是通过场景表示网络（SRNs）[1]。SRNs将场景建模为一个连续函数，该函数将每个3D坐标映射到描述该位置物体形状和外观的表示。这个函数通过前馈神经网络进行学习。然后，SRNs使用可学习的渲染算法生成底层3D场景的新视角（即，仅2D图像）。前馈网络和渲染算法都可以通过仅使用场景的2D图像进行端到端的训练。
- en: 'Compared to prior work, SRNs are very useful because they:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前的工作相比，SRNs非常有用，因为它们：
- en: Directly enforce 3D structure, which encourages different viewpoints of a scene
    to be consistent.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接强制3D结构，这鼓励场景的不同视角保持一致。
- en: Only require 2D images of a scene for training.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅需场景的2D图像进行训练。
- en: Produce better results given limited training data.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在有限的训练数据下产生更好的结果。
- en: Model a continuous (as opposed to discrete) representation of a scene, which
    can be rendered in arbitrary resolutions.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建模场景的连续（而非离散）表示，这种表示可以以任意分辨率渲染。
- en: We can use SRNs to generate accurate representations of 3D scenes, which has
    major implications for applications like robotic manipulation and rendering of
    complex scenes for virtual reality.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用SRNs生成准确的3D场景表示，这对机器人操作和虚拟现实中复杂场景的渲染等应用具有重大影响。
- en: '![](../Images/f4835333ec6649baaf8389c6e182bf55.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4835333ec6649baaf8389c6e182bf55.png)'
- en: (from [1])
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [1])
- en: Background
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: So far, we have reviewed [DeepSDF](https://cameronrwolfe.substack.com/p/3d-generative-modeling-with-deepsdf)
    and [ONet](https://cameronrwolfe.substack.com/p/shape-reconstruction-with-onets)
    models for representing 3D geometries. These approaches never consider modeling
    both geometry and appearance like SRNs. However, there are a few useful background
    concepts from these models that will be useful here.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经回顾了[DeepSDF](https://cameronrwolfe.substack.com/p/3d-generative-modeling-with-deepsdf)和[ONet](https://cameronrwolfe.substack.com/p/shape-reconstruction-with-onets)模型来表示3D几何。这些方法从未考虑像SRNs那样建模几何和外观。然而，从这些模型中有一些有用的背景概念在这里会很有用。
- en: Feed-forward neural networks [[link](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)]
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈神经网络 [[link](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)]
- en: 3D shape representations [[link](https://cameronrwolfe.substack.com/i/94634004/representing-d-shapes)]
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3D形状表示 [[link](https://cameronrwolfe.substack.com/i/94634004/representing-d-shapes)]
- en: Signed distance functions [[link](https://cameronrwolfe.substack.com/i/94634004/signed-distance-functions)]
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 签名距离函数 [[link](https://cameronrwolfe.substack.com/i/94634004/signed-distance-functions)]
- en: But, this isn’t all we need to know. Let’s cover some more background concepts
    that will be useful for building an understanding of how SRNs work.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并不是我们需要了解的全部内容。让我们深入一些背景概念，这些概念对于建立对SRNs工作原理的理解将非常有用。
- en: '**hypernetworks.** SRNs are complex models that contain several different neural
    network-based modules. One of these modules is a particular type of neural network
    called a hypernetwork. This may sound fancy, but *hypernetworks are just neural
    networks that generate the weights of another neural network as output*.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**超网络。** SRNs是包含多个不同神经网络模块的复杂模型。其中一个模块是特定类型的神经网络，称为超网络。这可能听起来很复杂，但*超网络只是生成另一个神经网络权重的神经网络*。'
- en: '![](../Images/94b31c7eaf6058003da570c016754b9d.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94b31c7eaf6058003da570c016754b9d.png)'
- en: Hypernetworks are neural networks that are used to output the weights of another
    neural network (created by author).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 超网络是用于输出另一个神经网络权重的神经网络（由作者创建）。
- en: In the case of SRNs, our hypernetwork is a feed-forward neural network that
    takes a latent vector (i.e., a unique vector corresponding to a particular scene)
    as input, and produces a vector as output. Instead of directly using this vector,
    we take the values within it and use them as the weights of another feed-forward
    neural network. The output of our hypernetwork is used as the weights of another
    neural network within the SRN; see above.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在SRNs的情况下，我们的超网络是一个前馈神经网络，它接受一个潜在向量（即对应特定场景的唯一向量）作为输入，并产生一个向量作为输出。我们不直接使用这个向量，而是将其值作为另一个前馈神经网络的权重。我们的超网络的输出被用作SRN中另一个神经网络的权重；见上文。
- en: '![](../Images/0e52de3ada0d2dde088ab5a513413604.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e52de3ada0d2dde088ab5a513413604.png)'
- en: RNNs process ordered sequences with feed-forward transformations while maintaining
    a memory of the sequence (from [4])
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: RNN处理有序序列，通过前馈变换的同时保持对序列的记忆（来自 [4]）
- en: '**long short-term memory networks.** Another component of the SRN is based
    on a type of recurrent neural network (RNN), called a long short-term memory (LSTM)
    network. RNNs are similar to feed-forward networks. They take a vector as input,
    produce a vector as output, and have several linear transformations and non-linearities
    in between. The main difference is that RNNs operate over sequences of vectors
    instead of just a single vector; see above.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**长短期记忆网络。** SRN的另一个组件基于一种称为长短期记忆（LSTM）网络的递归神经网络（RNN）。RNN类似于前馈网络。它们接收一个向量作为输入，产生一个向量作为输出，并在中间进行几个线性变换和非线性变换。主要区别在于，RNN处理的是向量序列，而不仅仅是单个向量；见上文。'
- en: Given an ordered sequence of vectors as input, the RNN will use the first vector
    as input, produce an output, then pass its hidden state as an additional input
    for processing the second vector. This process will repeat for each vector, until
    we reach the end of the sequence. So, RNNs are similar to feed-forward networks,
    but we are processing a sequence of inputs over time and maintaining a hidden/memory
    state that is *(i)* updated each time we see a new input and *(ii)* used as an
    additional input to generate our output at each time step or sequence position.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个有序的向量序列作为输入，RNN 会使用第一个向量作为输入，产生一个输出，然后将其隐藏状态作为附加输入传递给第二个向量进行处理。这个过程会对每个向量重复，直到我们到达序列的末尾。因此，RNN
    类似于前馈网络，但我们在处理一个时间序列的输入，并且维护一个每次看到新输入时都会*(i)* 更新的隐藏/记忆状态，并且*(ii)* 这个状态作为附加输入用于生成每个时间步或序列位置的输出。
- en: '![](../Images/416e64923e50f4fa3aaa8d0c0c0225c5.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/416e64923e50f4fa3aaa8d0c0c0225c5.png)'
- en: LSTMs use gating mechanisms to better manage the memory that is maintained for
    a sequence (from [4])
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 使用门控机制来更好地管理用于序列的记忆（来自 [4]）
- en: One major problem with RNNs is that they struggle to cope with very long sequences.
    We are trying to store everything about this sequence in a single, fixed-size
    hidden/memory state that is passed along to each sequence position! To solve this
    issue, LSTMs introduce gating mechanisms to the RNN’s forward pass, allowing the
    RNN to cope with longer sequences and better manage the information that is stored
    within its memory; see above. To learn more about LSTMs, check out the deep dive
    [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的一个主要问题是它们难以处理非常长的序列。我们试图将有关该序列的所有信息存储在一个固定大小的隐藏/记忆状态中，并传递到每个序列位置！为了解决这个问题，LSTM
    引入了门控机制到 RNN 的前向传递中，使 RNN 能够处理更长的序列并更好地管理存储在其记忆中的信息；见上文。要了解更多关于 LSTM 的信息，请查看深入了解
    [这里](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)。
- en: '**Camera viewpoints.** To understand SRNs, there are a few general computer
    vision concepts that we should briefly explore. In particular, we need to understand
    camera viewpoints and parameters. Given an underlying scene, the purpose of a
    scene representation model is to accurately generate 2D views or images of the
    ground truth, 3D scene from different camera perspectives.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**相机视角。** 要理解 SRNs，我们需要简要探讨一些通用的计算机视觉概念。特别是，我们需要了解相机视角和参数。给定一个基础场景，场景表示模型的目的是准确地从不同的相机视角生成地面真实场景的
    2D 视图或图像。'
- en: Intuitively, we can think of this as being similar to a person taking a picture
    of a 3D scene in real life. We use cameras every day to project the real world
    into a 2D image; see below.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，我们可以将其视为类似于一个人在现实生活中拍摄一个 3D 场景。我们每天使用相机将现实世界投射到 2D 图像中；见下文。
- en: '![](../Images/a9a4ea4971d53142f7a5ca04d2db070a.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9a4ea4971d53142f7a5ca04d2db070a.png)'
- en: (from [2])
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [2]）
- en: 'This view of the scene that we generate depends on several factors, including
    the location, orientation, and properties of the camera being used. In computer
    vision, we divide all of the parameters needed to compute an accurate 2D image
    of the outside world into two groups: extrinsic and intrinsic.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成的场景视图依赖于多个因素，包括相机的位置、方向和属性。在计算机视觉中，我们将计算外部世界准确的 2D 图像所需的所有参数分为两组：外部参数和内部参数。
- en: Extrinsic parameters include properties like camera location and orientation,
    while intrinsic parameters capture the internal properties of the camera (e.g.,
    focal length, resolution, etc.). See the article [here](/what-are-intrinsic-and-extrinsic-camera-parameters-in-computer-vision-7071b72fb8ec)
    for more details.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 外部参数包括相机位置和方向等属性，而内部参数则捕捉相机的内部属性（例如，焦距、分辨率等）。有关更多详细信息，请参见文章 [这里](/what-are-intrinsic-and-extrinsic-camera-parameters-in-computer-vision-7071b72fb8ec)。
- en: For SRNs, we actually need more than just images to train our model — we also
    need the associated camera parameters. This extra information allows us to understand
    the location, orientation, and camera properties that were used to construct this
    image, which is relevant to developing an understanding of the underlying scene.
    *How would we know how to generate an image of a scene without knowing the viewpoint
    from which the scene is being observed?*
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 SRNs，我们实际上需要的不仅仅是图像来训练我们的模型——我们还需要相关的相机参数。这些额外的信息使我们能够了解用于构建图像的位置、方向和相机属性，这对于理解基础场景至关重要。*如果不知道观察场景的视角，我们怎么知道如何生成场景的图像呢？*
- en: How do SRNs work?
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SRNs 如何工作？
- en: 'The SRN has two basic components: the scene representation model and the rendering
    function. We will first build an understanding of the data needed to train an
    SRN, then we will overview how each of these components work and come together,
    forming a robust system for representing and rendering scenes.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: SRN 具有两个基本组件：场景表示模型和渲染函数。我们将首先建立对训练 SRN 所需数据的理解，然后概述这些组件如何工作及其如何结合，形成一个强大的场景表示和渲染系统。
- en: '**the data.** We will first consider training an SRN over images corresponding
    to different viewpoints of a single scene. Given access to these images, we can
    train an SRN to generate images corresponding to arbitrary viewpoints of this
    scene at a high resolution. However, the images by themselves are not enough.
    We also need the matrices corresponding to [extrinsic and intrinsic camera parameters](/what-are-intrinsic-and-extrinsic-camera-parameters-in-computer-vision-7071b72fb8ec)
    for each viewpoint. Thus, our training dataset will take the following form:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据。** 我们首先考虑在与单一场景的不同视角相对应的图像上训练 SRN。获得这些图像后，我们可以训练 SRN 生成与场景任意视角对应的高分辨率图像。然而，单凭图像是不够的。我们还需要每个视角对应的
    [外部和内部相机参数](/what-are-intrinsic-and-extrinsic-camera-parameters-in-computer-vision-7071b72fb8ec)。因此，我们的训练数据集将如下所示：'
- en: '![](../Images/aaee0af6a0d6531d909545d434e8c1be.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aaee0af6a0d6531d909545d434e8c1be.png)'
- en: (from [1])
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: where each I is an image, E is an extrinsic parameter matrix, and K is an intrinsic
    parameter matrix. Intrinsic parameters are specific to the camera, while extrinsic
    parameters are specific to the camera’s viewpoint of the scene.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中每个 I 是一幅图像，E 是外部参数矩阵，K 是内部参数矩阵。内部参数是相机特定的，而外部参数是相机对场景视角的特定参数。
- en: '![](../Images/d661b9b0c046c75e3fe4af65ed87b218.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d661b9b0c046c75e3fe4af65ed87b218.png)'
- en: (from [1])
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: '**representing the scene.** SRNs represent scenes as functions that map a spatial
    location (i.e., `[x, y, z]` coordinate) to a feature vector. These features contain
    information about both the geometry and appearance of the scene at the specified
    location (e.g., surface color, [signed distance](https://cameronrwolfe.substack.com/i/94634004/signed-distance-functions),
    etc.). To model this scene function, we use a [feed-forward neural network](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)
    (i.e., a simple architecture!) that takes an `[x, y, z]` coordinate as input and
    produces this feature vector as output; see above.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**场景表示。** SRNs 将场景表示为将空间位置（即 `[x, y, z]` 坐标）映射到特征向量的函数。这些特征包含关于指定位置的场景几何和外观的信息（例如，表面颜色，[符号距离](https://cameronrwolfe.substack.com/i/94634004/signed-distance-functions)
    等）。为了建模这个场景函数，我们使用一个 [前馈神经网络](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)（即简单架构！），它以
    `[x, y, z]` 坐标作为输入，并产生这个特征向量作为输出；见上文。'
- en: '**neural rendering.** Once we have generated relevant scene features for several
    different spatial locations in a scene, *how do we use these features to render
    a new viewpoint of the scene?* Our goal here is to take our scene representation
    — produced by the neural network described above, evaluated at a bunch of different
    spatial locations in the scene — and camera parameter matrices as input, then
    use this information to produce a novel viewpoint of the scene. SRNs use two separate
    modules to do this.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经渲染。** 一旦我们为场景中的几个不同空间位置生成了相关的场景特征，*我们如何利用这些特征来渲染场景的一个新视角？* 我们的目标是以由上述神经网络生成的场景表示——在场景中的多个不同空间位置评估——和相机参数矩阵作为输入，然后利用这些信息生成场景的新视角。
    SRNs 使用两个独立的模块来完成这个任务。'
- en: 'First, we must deduce the scene’s geometry. To do this, we use a modified version
    of the [Ray Marching algorithm](https://michaelwalczyk.com/blog-ray-marching.html).
    I won’t describe this algorithm in detail, but the basic idea is to:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须推断场景的几何。为此，我们使用 [Ray Marching 算法](https://michaelwalczyk.com/blog-ray-marching.html)
    的修改版。我不会详细描述这个算法，但基本思想是：
- en: Consider different camera viewpoints that are available
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑不同的相机视角
- en: Send “camera rays” from the camera’s position into the scene
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从相机的位置向场景发射“相机光线”
- en: Deduce geometry by finding where rays intersect with objects
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过寻找光线与物体的交点来推断几何
- en: Using this process, we can deduce the underlying objects and geometries that
    exist within a scene.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个过程，我们可以推断出场景中存在的底层物体和几何。
- en: '![](../Images/68d9bcdc4ae0f87bed5c8cd60833edd2.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68d9bcdc4ae0f87bed5c8cd60833edd2.png)'
- en: (from [1])
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: In [1], the authors make the Ray Marching process learnable by using an LSTM
    to perform each of these steps, as shown in the figure above. In this way, the
    Ray Marching process can be updated using our training data to become faster and
    more accurate! For more details on Ray Marching, check out the overview [here](https://michaelwalczyk.com/blog-ray-marching.html).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [1] 中，作者通过使用 LSTM 执行每一个步骤来使光线行进过程可学习，如上图所示。这样，光线行进过程可以使用我们的训练数据进行更新，从而变得更快更准确！有关光线行进的更多细节，请查看
    [这里](https://michaelwalczyk.com/blog-ray-marching.html)。
- en: Next, we must uncover the appearance of the scene by mapping the feature vector
    at each spatial coordinate to an associated color. In [1], this transformation
    is again modeled using a feed-forward neural network that *(i)* takes the per-coordinate
    feature vector as input and *(ii)* produces a per-coordinate RGB pixel value.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须通过将每个空间坐标的特征向量映射到相关颜色来揭示场景的外观。在 [1] 中，这种转换再次通过前馈神经网络建模，该网络 *(i)* 以每个坐标的特征向量为输入，并
    *(ii)* 生成每个坐标的 RGB 像素值。
- en: '**handling many scenes.** So far, we have only considered training an SRN over
    a set of images corresponding to the same scene. But, *what if we want to model
    multiple different scenes?*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**处理多个场景。** 到目前为止，我们只考虑过在与同一场景对应的一组图像上训练 SRN。但，*如果我们想建模多个不同的场景呢？*'
- en: '![](../Images/29a2e10e6a8a7b8051f2d7011338203d.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29a2e10e6a8a7b8051f2d7011338203d.png)'
- en: (from [3])
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [3]）
- en: In [1], we associate each scene with a unique latent vector that is learned
    via an auto-decoder architecture (i.e., similar to the architecture used by DeepSDFs
    [3]!); see above. Using this latent vector as input, we can use a hypernetwork
    (i.e., a neural network that outputs the weights of another neural network) to
    generate the weights of the feed-forward feature representation network. This
    approach allows us to craft unique feature vectors for each scene, while also
    learning patterns that generalize across scenes.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [1] 中，我们将每个场景与一个唯一的潜在向量关联，该向量通过自动编码器架构学习（即，类似于 DeepSDFs [3] 使用的架构！）；见上文。使用这个潜在向量作为输入，我们可以使用一个超网络（即，输出另一个神经网络权重的神经网络）来生成前馈特征表示网络的权重。这种方法使我们能够为每个场景创建独特的特征向量，同时学习能够跨场景泛化的模式。
- en: '![](../Images/cc1b5448546b4ea1b3f2b8be58373770.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc1b5448546b4ea1b3f2b8be58373770.png)'
- en: (from [1])
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: '**a global view.** The full SRN model is depicted above. This model has several
    components, including:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**全局视角。** 上面展示了完整的 SRN 模型。该模型包含多个组件，包括：'
- en: A feed-forward network that generates feature representations for each spatial
    location in a given scene.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个前馈网络，用于生成给定场景中每个空间位置的特征表示。
- en: A separate (feed-forward) hypernetwork that generates the weights of the above
    network for a particular scene.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个单独的（前馈）超网络，用于为特定场景生成上述网络的权重。
- en: An LSTM-based, learnable Ray Marching module for rendering 3D geometries in
    the scene.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于 LSTM 的可学习光线行进模块，用于渲染场景中的 3D 几何形状。
- en: A feed-forward network that generates RGB pixel values for modeling the scene’s
    appearance.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个前馈网络，用于生成建模场景外观的 RGB 像素值。
- en: All of these components combine together into an end-to-end trainable SRN that
    can learn to output arbitrary viewpoints of an underlying scene from a dataset
    of images with associated camera viewpoint information.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些组件结合在一起形成一个端到端可训练的 SRN，能够从带有相机视角信息的图像数据集中学习输出任意的场景视角。
- en: Do they perform well?
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它们表现得好吗？
- en: 'In [1], SRNs are evaluated on large-scale 3D datasets such as [ShapeNet](https://shapenet.org/)
    and [DeepVoxels](https://www.vincentsitzmann.com/deepvoxels/), as well as smaller,
    synthetic datasets of 3D objects. Performance is measured in two ways:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [1] 中，SRN 在大规模的 3D 数据集上进行评估，如 [ShapeNet](https://shapenet.org/) 和 [DeepVoxels](https://www.vincentsitzmann.com/deepvoxels/)，以及较小的合成
    3D 对象数据集。性能通过两种方式进行测量：
- en: Accuracy of SRN-generated viewpoints of training objects
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SRN 生成的训练对象视角的准确性
- en: Accuracy of few-shot generations of hold-out test objects
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 少样本生成的保留测试对象的准确性
- en: To generate viewpoints of test objects (i.e., never observed during training),
    we need to first observe a few sample viewpoints of the object so that we can
    solve for the scene’s optimal latent vector. Then, we use the SRN to generate
    a viewpoint with this latent vector (i.e., hence “few shot” generation!).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成测试对象的视角（即，在训练过程中从未观察过的视角），我们需要首先观察对象的几个样本视角，以便我们可以求解场景的最佳潜在向量。然后，我们使用 SRN
    生成带有该潜在向量的视角（即，“少量样本”生成！）。
- en: '![](../Images/5db6700e5f439de42028e801f8e4211f.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5db6700e5f439de42028e801f8e4211f.png)'
- en: (from [1])
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于 [1]）
- en: On smaller synthetic datasets, SRNs clearly outperform baseline techniques,
    which we can see in a qualitative examination of model outputs; see above. SRN
    results on these smaller datasets are near pixel-perfect, indicating that the
    approach works well given limited training data. In fact, we can see in these
    experiments that SRNs are usually capable of recovering any component of the underlying
    scene that has been observed in the data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在较小的合成数据集上，SRNs 明显优于基线技术，这可以通过对模型输出的定性检查来看出；见上文。这些较小数据集上的 SRN 结果几乎达到像素级完美，表明该方法在训练数据有限的情况下表现良好。实际上，我们可以在这些实验中看到，SRNs
    通常能够恢复在数据中观察到的底层场景的任何组件。
- en: '![](../Images/151133c51c205df9642ecdfc9b425937.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/151133c51c205df9642ecdfc9b425937.png)'
- en: (from [1])
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于 [1]）
- en: On larger datasets, SRNs continue to perform well relative to baselines both
    in generating novel viewpoints of known objects and performing few-shot generation;
    see above. Notably, SRNs really shine in the few-shot case, where the model can
    leverage learned patterns from scenes that it has observed during training (i.e.,
    referred to as prior information) to render reasonable viewpoints of new scenes
    with limited data. Overall, we can see that SRN output *(i)* is sharper and more
    accurate relative to prior models and *(ii)* improves when given more samples/information;
    see below.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在更大的数据集上，SRNs 在生成已知对象的新视角和进行少样本生成方面，相对于基线方法表现良好；见上文。特别是，SRNs 在少样本情况下表现出色，在这种情况下，模型可以利用训练期间观察到的场景中的学习模式（即，称为先验信息）来呈现新场景的合理视角，数据有限。总体而言，我们可以看到
    SRN 输出 *(i)* 相对于先前的模型更清晰、更准确，并且 *(ii)* 在提供更多样本/信息时会有所改善；见下文。
- en: '![](../Images/1434e3b95bd46cc1ebb11d81e699cb03.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1434e3b95bd46cc1ebb11d81e699cb03.png)'
- en: (from [1])
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于 [1]）
- en: Similar to prior work, we can interpolate embeddings of the SRN’s latent space
    to produce new scenes. These results, illustrated below, indicate that the SRN’s
    embedding space contains useful, structured information about the underlying scene.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于先前的工作，我们可以插值 SRN 的潜在空间中的嵌入来生成新场景。这些结果，如下所示，表明 SRN 的嵌入空间包含有关底层场景的有用、结构化的信息。
- en: '![](../Images/2f119039030c669634b2f6b6c67ab725.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f119039030c669634b2f6b6c67ab725.png)'
- en: (from [1])
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于 [1]）
- en: Takeaways
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要收获
- en: Relative to work that we have seen in prior overviews, SRNs are useful because
    they *(i)* allow us to model more than just geometry (i.e., both geometry and
    appearance) and *(ii)* can be trained using only images. Using SRNs, we can generate
    novel viewpoints of 3D scenes at arbitrary resolution in an end-to-end, learnable
    manner. The main takeaways of this approach are listed below.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于我们在之前综述中看到的工作，SRNs 很有用，因为它们 *(i)* 允许我们建模的不仅仅是几何信息（即，既包括几何信息也包括外观），而且 *(ii)*
    可以仅使用图像进行训练。使用 SRNs，我们可以以端到端的、可学习的方式生成任意分辨率的 3D 场景的新视角。此方法的主要收获列在下方。
- en: '**directly encoding 3D info.** SRNs directly inject 3D information into the
    viewpoint generation process because the feature representation network (and the
    pixel generation network) take `[x, y, z]` coordinates as input. This approach
    is not always followed by prior approaches. In the case of SRNs, however, this
    direct use of 3D spatial information allows the model to generate more consistent
    results between different viewpoints of the same scene.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**直接编码 3D 信息。** SRNs 直接将 3D 信息注入到视角生成过程中，因为特征表示网络（和像素生成网络）以 `[x, y, z]` 坐标作为输入。此方法并非总是被先前的方法所采用。然而，对于
    SRNs，这种直接使用 3D 空间信息的方法使模型能够在同一场景的不同视角之间生成更一致的结果。'
- en: '**no ground truth geometry.** SRNs can be trained only using images and some
    associated camera parameters that reveal relevant viewpoint information for each
    image. Compared to prior approaches (e.g., [DeepSDF](https://cameronrwolfe.substack.com/p/3d-generative-modeling-with-deepsdf)
    [3]) that can only perform inference when given direct access to portions of an
    underlying 3D geometry, SRNs are pretty flexible. At the very least, modeling
    scenes from images only is a step in the right direction.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**没有地面真实几何信息。** SRNs 仅使用图像和一些相关的相机参数进行训练，这些参数揭示了每张图像的相关视角信息。与先前的方法（例如，[DeepSDF](https://cameronrwolfe.substack.com/p/3d-generative-modeling-with-deepsdf)
    [3]），这些方法只能在直接访问底层 3D 几何部分时进行推断不同，SRNs 非常灵活。至少，从图像中建模场景是朝着正确方向迈出的一步。'
- en: '**simple networks aren’t always cheap.** One of the great things about 3D deep
    learning approaches (at least the ones we have seen so far) is that they mostly
    use simple, feed-forward networks. Compared to research topics that require massive
    deep learning models (e.g., [language modeling](https://cameronrwolfe.substack.com/p/modern-llms-mt-nlg-chinchilla-gopher)),
    these simple networks are pretty nice! However, the training process might still
    be expensive even if the underlying network is simple. In [1], *authors claim
    that most SRNs take around a week to train on a single GPU*.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**简单网络并不总是便宜的。** 3D 深度学习方法的一个优点（至少是我们迄今见过的那些）是它们大多使用简单的前馈网络。与需要大规模深度学习模型的研究主题相比（例如，[语言建模](https://cameronrwolfe.substack.com/p/modern-llms-mt-nlg-chinchilla-gopher)），这些简单的网络相当不错！然而，即使底层网络很简单，训练过程仍然可能很昂贵。在
    [1] 中，*作者声称大多数 SRN 需要大约一周的时间在单个 GPU 上进行训练*。'
- en: '![](../Images/7ecf6f646a19e969386a8707f23e342c.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ecf6f646a19e969386a8707f23e342c.png)'
- en: (from [1])
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: (来源于 [1])
- en: '**limitations.** SRNs fall short in a lot of areas. Notably, they often fail
    to capture fine-grained details, such as holes within objects (e.g., see the chair
    holes in the above figure). They also struggle with objects that are not similar
    to the training data distribution and cannot capture effects due to varying lighting
    conditions or translucency. Future works like NeRF [2] improve upon the SRN by
    mitigating these issues and pushing towards simpler models that are capable of
    representing complex scenes and effects; e.g., entire rooms with many objects
    and variable lighting and reflection conditions.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**限制。** SRN 在许多方面表现不佳。特别是，它们往往无法捕捉细微的细节，例如物体中的孔洞（例如，见上图中的椅子孔洞）。它们还难以处理与训练数据分布不相似的物体，无法捕捉由于光照条件或半透明度变化带来的效果。未来的工作如
    NeRF [2] 通过缓解这些问题，推动向更简单的模型发展，这些模型能够表示复杂的场景和效果；例如，包含许多物体和可变光照及反射条件的整个房间。'
- en: Closing remarks
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结束语
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    a research scientist at [Alegion](https://www.alegion.com/) and PhD student at
    Rice University studying the empirical and theoretical foundations of deep learning.
    You can also check out my [other writings](https://medium.com/@wolfecameron) on
    medium! If you liked it, please follow me on [twitter](https://twitter.com/cwolferesearch)
    or subscribe to my [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/),
    where I write series of understandable overviews on important deep learning topics.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢你阅读这篇文章。我是 [Cameron R. Wolfe](https://cameronrwolfe.me/)，一名在 [Alegion](https://www.alegion.com/)
    的研究科学家以及在莱斯大学攻读深度学习的实证和理论基础的博士生。你也可以查看我在 medium 上的 [其他文章](https://medium.com/@wolfecameron)！如果你喜欢这篇文章，请在
    [twitter](https://twitter.com/cwolferesearch) 上关注我，或者订阅我的 [Deep (Learning) Focus
    通讯](https://cameronrwolfe.substack.com/)，我在其中撰写关于重要深度学习主题的易懂概述。
- en: Bibliography
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Sitzmann, Vincent, Michael Zollhöfer, and Gordon Wetzstein. “Scene representation
    networks: Continuous 3d-structure-aware neural scene representations.” *Advances
    in Neural Information Processing Systems* 32 (2019).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Sitzmann, Vincent, Michael Zollhöfer, 和 Gordon Wetzstein. “场景表示网络：连续的 3D
    结构感知神经场景表示。” *神经信息处理系统进展* 32 (2019)。'
- en: '[2] Mildenhall, Ben, et al. “Nerf: Representing scenes as neural radiance fields
    for view synthesis.” *Communications of the ACM* 65.1 (2021): 99–106.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Mildenhall, Ben, 等. “Nerf：将场景表示为神经辐射场以进行视图合成。” *ACM 通讯* 65.1 (2021): 99–106。'
- en: '[3] Park, Jeong Joon, et al. “Deepsdf: Learning continuous signed distance
    functions for shape representation.” *Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition*. 2019.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Park, Jeong Joon, 等. “Deepsdf：学习连续签名距离函数以进行形状表示。” *IEEE/CVF 计算机视觉与模式识别会议论文集*。2019。'
- en: '[4] Zaremba, Wojciech, Ilya Sutskever, and Oriol Vinyals. “Recurrent neural
    network regularization.” *arXiv preprint arXiv:1409.2329* (2014).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Zaremba, Wojciech, Ilya Sutskever, 和 Oriol Vinyals. “递归神经网络正则化。” *arXiv
    预印本 arXiv:1409.2329* (2014).'
- en: '[5] Mescheder, Lars, et al. “Occupancy networks: Learning 3d reconstruction
    in function space.” *Proceedings of the IEEE/CVF conference on computer vision
    and pattern recognition*. 2019.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Mescheder, Lars, 等. “占据网络：在函数空间中学习 3D 重建。” *IEEE/CVF 计算机视觉与模式识别会议论文集*。2019。'
