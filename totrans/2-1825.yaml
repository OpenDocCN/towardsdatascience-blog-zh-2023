- en: 'Segment Anything: Promptable Segmentation of Arbitrary Objects'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Segment Anything: 可提示的任意对象分割'
- en: 原文：[https://towardsdatascience.com/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d](https://towardsdatascience.com/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d](https://towardsdatascience.com/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d)
- en: '[🚀Sascha’s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[🚀Sascha的论文俱乐部](https://towardsdatascience.com/tagged/saschas-paper-club)'
- en: Segment Anything by A. Krillov et. al.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Segment Anything 由 A. Krillov 等人
- en: '[](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f28958c5612d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f28958c5612d--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f28958c5612d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f28958c5612d--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f28958c5612d--------------------------------)
    ·12 min read·Sep 14, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f28958c5612d--------------------------------)
    ·12 分钟阅读·2023年9月14日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Today’s paper walkthrough it is going to be visual! We will analyze S*egment
    Anything*, a paper by Meta’s AI research team that made headlines not only in
    the research community but also by all sorts of deep learning practitioners and
    advocates.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的论文讲解将是视觉化的！我们将分析 *Segment Anything*，这是Meta AI研究团队的一篇论文，它不仅在研究界引起了关注，也在各种深度学习从业者和支持者中引起了广泛关注。
- en: Segment Anything introduces the task of promptable segmentation, it introduces
    the segment anything model (SAM), and it details the generation of a new publicly
    available dataset of 11 million images containing more than 1 billion masks. SAM
    has been widely adopted by the community and resulted in some new state-of-the-art
    foundation models such as [Grounded-SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything)
    that combines [Grounding DINO](https://arxiv.org/abs/2303.05499) with SAM.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything 引入了可提示的分割任务，介绍了 segment anything 模型（SAM），并详细描述了生成一个包含超过10亿个掩膜的1100万张图片的新公开数据集。SAM
    已被广泛采纳，并产生了一些新的最先进基础模型，如 [Grounded-SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything)，它将
    [Grounding DINO](https://arxiv.org/abs/2303.05499) 与 SAM 结合起来。
- en: '![](../Images/3655fd5fedea80f78ac89299c8bd7b30.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3655fd5fedea80f78ac89299c8bd7b30.png)'
- en: Image created from [publication](https://arxiv.org/abs/2304.02643) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [出版物](https://arxiv.org/abs/2304.02643) 由 [Sascha Kirch](https://medium.com/@SaschaKirch)
- en: '**Paper:** [Segment Anything](https://arxiv.org/abs/2304.02643) by [Alexander
    Kirillov](https://arxiv.org/search/cs?searchtype=author&query=Kirillov%2C+A) et.
    al., 5 Apr. 2023'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**论文:** [Segment Anything](https://arxiv.org/abs/2304.02643) 由 [Alexander Kirillov](https://arxiv.org/search/cs?searchtype=author&query=Kirillov%2C+A)
    等人，2023年4月5日'
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/facebookresearch/segment-anything)
    — [Demo](https://segment-anything.com/demo) — [Project Page](https://segment-anything.com/)
    — [Dataset](https://segment-anything.com/dataset/index.html) — [HuggingFace](https://huggingface.co/docs/transformers/main/model_doc/sam)'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**资源:** [GitHub](https://github.com/facebookresearch/segment-anything) — [演示](https://segment-anything.com/demo)
    — [项目页面](https://segment-anything.com/) — [数据集](https://segment-anything.com/dataset/index.html)
    — [HuggingFace](https://huggingface.co/docs/transformers/main/model_doc/sam)'
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** segmentation, zero-shot prediction, computer vison, prompting,
    large-scale'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**类别:** 分割、零-shot预测、计算机视觉、提示、大规模'
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[**其他教程**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
- en: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    — [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    — [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    — [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    — [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    — [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    — [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    — [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    — [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
- en: Outline
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大纲
- en: Context & Background
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 背景与背景
- en: SAM — Segment Anything Model
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SAM — Segment Anything Model
- en: SA-1B — Dataset with 1 Billion Masks
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SA-1B — 具有10亿个掩码的数据集
- en: Experiments and Ablations
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实验与消融
- en: Conclusion
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结论
- en: Further Readings & Resources
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进一步阅读与资源
- en: Context & Background
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景与背景
- en: 'The authors of Segment Anything made a clear statement: “*[…] our goal is to
    build a foundation model for image segmentation.*” Foundation models originated
    from the great success of Natural Language Processing (NLP). Models have been
    trained on a laaaarge scale in a self-supervised fashion. These models usually
    perform very well at zero-shot tasks, meaning they can solve tasks different to
    those they were trained on and perform reasonable well or even better as their
    supervised competitors. In recent years, many researchers worked on bringing the
    success of NLP foundation models to other domains such as computer vision.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 《Segment Anything》的作者明确声明：“*我们目标是建立一个图像分割的基础模型。*” 基础模型源于自然语言处理（NLP）的巨大成功。这些模型在自监督的方式下经过了大规模的训练。通常，这些模型在零-shot
    任务中表现非常好，即它们可以解决与训练时不同的任务，并表现得相当不错，甚至比其监督型对手更优秀。近年来，许多研究人员致力于将NLP基础模型的成功带到计算机视觉等其他领域。
- en: Models such as [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    and [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)
    made it possible to condition an image classification or object detection task
    on text prompts, rather than a fixed set of classes. Other models, such as [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)
    or DINO, came up with different techniques to learn semantically rich representations
    of input images, which is one of the key requirements for many computer vision
    applications.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 模型如 [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    和 [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)
    使得可以根据文本提示对图像分类或对象检测任务进行条件限制，而不是固定的类别集合。其他模型，如 [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)
    或 DINO，提出了不同的技术来学习输入图像的语义丰富表示，这也是许多计算机视觉应用的关键要求。
- en: 'The Segment Anything paper aims to:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 《Segment Anything》论文旨在：
- en: Enable zero-shot segmentation by prompting
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过提示启用零-shot 分割
- en: Train a large-scale model (SAM) as demonstrator
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个大规模模型（SAM）作为演示模型
- en: Collect and release the largest publicly available dataset for segmentation.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集并发布最大的公开可用分割数据集。
- en: '***But why is zero-shot performance so important?*** — The answer is two-fold.
    First, initially computer vision models have been trained in a supervised fashion
    requiring not only data, but also a lot of ground truth labels. Collection these
    data is extremely time-consuming and costly. Second, the classes a model can predict
    are limited to a fixed set of classes used for training. If you would like to
    add a new class to your model, you would need to first collect the data and retrain
    the model.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '***但为什么零-shot 性能如此重要？*** — 答案有两个方面。首先，最初计算机视觉模型是以监督方式训练的，这不仅需要数据，还需要大量的真实标签。收集这些数据是极其耗时和昂贵的。其次，模型可以预测的类别仅限于训练时使用的固定类别集合。如果你想向模型中添加一个新类别，你需要首先收集数据并重新训练模型。'
- en: '***How is it possible to prompt a segmentation model?*** — You might be familiar
    with text prompting from models like ChatGPT, [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    or [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05).
    While SAM in principle also was tested with text prompts, it is mainly prompted
    with either masks, points, boxes or point grids as shown in the image bellow.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '***如何对分割模型进行提示？*** — 你可能对来自ChatGPT、[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)或[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)等模型的文本提示比较熟悉。虽然SAM原则上也经过了文本提示的测试，但它主要通过掩码、点、框或点网格来进行提示，如下图所示。'
- en: '![](../Images/78dbec949be2553ac6a297eed84a8ee7.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78dbec949be2553ac6a297eed84a8ee7.png)'
- en: 'Fig.1: Different input prompts and resulting masks. Photo by [Terence Burke](https://unsplash.com/@ancientwanderer?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) + masks
    generated by [Sascha Kirch](https://medium.com/@SaschaKirch) with [SAM](https://segment-anything.com/demo)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：不同输入提示和生成的掩码。照片由[Terence Burke](https://unsplash.com/@ancientwanderer?utm_source=medium&utm_medium=referral)拍摄，发布于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
    + 掩码由[Sascha Kirch](https://medium.com/@SaschaKirch)用[SAM](https://segment-anything.com/demo)生成
- en: Having put SAM into context let’s now switch gears and have a closer look onto
    the Segment Anything Model, aka SAM.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 了解了SAM的背景后，让我们转到重点，详细了解Segment Anything Model，即SAM。
- en: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
- en: '[Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)'
- en: Paper Walkthroughs by Sascha Kirch
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sascha Kirch的论文讲解
- en: '[View list](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2?source=post_page-----f28958c5612d--------------------------------)7
    stories![“DDPM — Denoising Diffusion Probabilistic Models “ paper illustration
    by Sascha Kirch](../Images/6e785c0a911386676abebe0fa646f483.png)![“Depth Anything”
    paper illustration by Sascha Kirch](../Images/bd8cd71a02e42cf64d0afd39f41f48e0.png)![](../Images/8708d91a4a1902cef889ced95d46fc39.png)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2?source=post_page-----f28958c5612d--------------------------------)7个故事![“DDPM — 去噪扩散概率模型”
    论文插图，Sascha Kirch](../Images/6e785c0a911386676abebe0fa646f483.png)![“Depth Anything”
    论文插图，Sascha Kirch](../Images/bd8cd71a02e42cf64d0afd39f41f48e0.png)![](../Images/8708d91a4a1902cef889ced95d46fc39.png)'
- en: SAM — Segment Anything Model
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SAM — Segment Anything Model
- en: 'The Segment Anything Model (SAM) is a multi-modal model that inputs an image
    and one or more prompts and outputs a valid segmentation mask. The model consists
    of three main modules: image encoder, prompt encoder and mask decoder.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything Model（SAM）是一个多模态模型，它输入一张图像和一个或多个提示，并输出一个有效的分割掩码。该模型由三个主要模块组成：图像编码器、提示编码器和掩码解码器。
- en: SAM is prompted with either a mask, a set of points, a bounding box or text
    or any combination of those.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: SAM 可以通过掩码、一组点、边界框或文本，或这些的任何组合来进行提示。
- en: 'NOTE: Even though the paper mentions and experiments with text as prompt, it
    is not yet released (as of September 2023) in the [official implementation](https://github.com/facebookresearch/segment-anything)
    nor in the [SAM demo](https://segment-anything.com/demo).'
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：尽管论文提到并实验了文本作为提示，但截至2023年9月，文本提示尚未在[官方实现](https://github.com/facebookresearch/segment-anything)或[SAM演示](https://segment-anything.com/demo)中发布。
- en: '![](../Images/61ae8b49ffb731555c8a774e41126b9c.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61ae8b49ffb731555c8a774e41126b9c.png)'
- en: 'Fig.2: SAM architecture. [Image Source](https://arxiv.org/abs/2304.02643) +
    Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：SAM架构。[图片来源](https://arxiv.org/abs/2304.02643) + 注释由[Sascha Kirch](https://medium.com/@SaschaKirch)
- en: '**Image Encoder** — Outputs an image embedding for a given input image. SAM
    implements and adapts a pre-trained ViT-H/16 masked auto-encoder. This is a relatively
    large model with strong performance.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**图像编码器** — 为给定的输入图像输出图像嵌入。SAM 实现并适配了一个预训练的ViT-H/16掩码自编码器。这是一个相对较大的模型，性能强劲。'
- en: '**Prompt Encoder** — Sparse prompts (i.e. points, boxes and text) are translated
    into embedding vectors. Text prompts are converted into text embeddings using
    [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    before feeding it into the prompt encoder. Dense prompts (i.e. masks) are simply
    downsampled with strided convolutions and added with the image embeddings. All
    embeddings are then fed into the final stage: the mask decoder.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示编码器** — 稀疏提示（例如点、框和文本）被转换为嵌入向量。文本提示在输入提示编码器之前，使用 [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    转换为文本嵌入。密集提示（例如掩码）则简单地通过步幅卷积下采样，并与图像嵌入相加。所有嵌入随后被送入最终阶段：掩码解码器。'
- en: '**Mask Decoder** — Takes a set of image embeddings (optionally containing the
    dense mask embeddings) and a set of prompt embeddings and outputs a valid segmentation
    mask.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**掩码解码器** — 接受一组图像嵌入（可选地包含密集掩码嵌入）和一组提示嵌入，并输出有效的分割掩码。'
- en: 'There are two more details we should address: ambiguities of prompts and performance.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 还有两个细节我们应该讨论：提示的歧义性和性能。
- en: In a nutshell, the less context a prompt contains, the more ambiguous it is
    and the more difficult it is for the model the provide the correct output. For
    text-prompts we have seen this connection between the specificness of the input
    text and the model’s performance in [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    and [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05).
    Similar, providing a single point as input might result in a variety of possible
    masks. For that reason, SAM outputs a set of three output masks corresponding
    to the object level, the part level and the sub-part level of a valid mask as
    indicated in the image bellow.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，提示包含的上下文越少，就越模糊，对模型提供正确输出的难度也越大。对于文本提示，我们已经在 [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    和 [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)
    中看到了输入文本的具体性与模型性能之间的这种联系。同样，提供一个单点作为输入可能会产生多种可能的掩码。因此，SAM 输出一组三种掩码，分别对应于有效掩码的对象级别、部件级别和子部件级别，如下图所示。
- en: '![](../Images/e2ca2d9f5176ee540cfdd88fc1d259ae.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2ca2d9f5176ee540cfdd88fc1d259ae.png)'
- en: 'Fig.3: Ambiguity for single-point prompts. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：单点提示的歧义性。[图片来源](https://arxiv.org/abs/2304.02643) + 由 [Sascha Kirch](https://medium.com/@SaschaKirch)
    注释
- en: 'The second detail I want to mention is performance in terms of inference speed.
    Did you notice that the image encoder is by far the largest sub-module in SAM?
    Well, that’s an unfair question because I did not tell you so far, but SAM is
    designed in a way to have semantically rich image embeddings (which often requires
    a large model) to subsequently act upon these embeddings applying a light-weight
    prompt encoder and a light-weight mask decoder. The good thing: one must only
    run the image encoder once per image and can then prompt the model multiple times
    using the same image embedding. This allows SAM to be executed in a browser taking
    only ~50ms to predict a mask for a given prompt (after the image embeddings have
    been calculated).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我想提到的第二个细节是推理速度方面的性能。你是否注意到图像编码器是SAM中最大的一部分？好吧，这个问题有点不公平，因为我之前没有告诉你，但SAM的设计目的是拥有语义丰富的图像嵌入（通常需要一个大型模型），然后通过一个轻量级的提示编码器和轻量级的掩码解码器来处理这些嵌入。好的一点是：每张图像只需运行一次图像编码器，然后可以使用相同的图像嵌入多次提示模型。这使得SAM可以在浏览器中运行，仅需
    ~50ms 来预测给定提示的掩码（在图像嵌入计算后）。
- en: Let’s have a closer look on the light-weight mask decoder. It inputs the image
    embeddings and prompt embeddings and outputs a set of masks with corresponding
    scores. Internally, two consecutive decoder blocks perform a combination of self-attention
    and cross-attention to generate a strong dependence between the image and the
    prompts. A simple up-sampling network in combination with another cross-attention
    block generates the masks and the scores.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看轻量级掩码解码器。它输入图像嵌入和提示嵌入，并输出一组带有相应分数的掩码。在内部，两个连续的解码器块通过自注意力和交叉注意力的组合生成图像与提示之间的强依赖关系。一个简单的上采样网络结合另一个交叉注意力块生成掩码和分数。
- en: '![](../Images/4404071c556ef28c5df3a42f22b0ce7e.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4404071c556ef28c5df3a42f22b0ce7e.png)'
- en: 'Fig.4: Detailed architecture of the mask decoder. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：掩码解码器的详细架构。 [图片来源](https://arxiv.org/abs/2304.02643) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    的注释
- en: SA-1B — Dataset with 1 Billion Masks
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SA-1B — 具有 10 亿掩码的数据集
- en: The second great deal of *Segment Anything* was the creation and release of
    a large-scale dataset for segmentation. It contains 11 million high-resolution
    and licensed images with roughly 1.1 billion masks. While the original version
    of the dataset have 3300x4950 pixels on average, the released version is downsampled
    to have 1500 pixels at the shortest edge. It is diverse in terms of different
    scenes and number of masks per image ranging from less than 50 to more than 500.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*Segment Anything* 的第二个重大成果是创建和发布了一个大规模的分割数据集。它包含 1100 万张高分辨率和许可的图像，大约有 11
    亿个掩码。虽然数据集的原始版本平均有 3300x4950 像素，但发布版本经过下采样，使最短边为 1500 像素。它在不同场景和每张图像掩码数量上都具有多样性，范围从不到
    50 个到超过 500 个。'
- en: '![](../Images/07221a70160f677f1590ec2479714fae.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07221a70160f677f1590ec2479714fae.png)'
- en: 'Fig.5: Different masks from SA-1B. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：来自 SA-1B 的不同掩码。 [图片来源](https://arxiv.org/abs/2304.02643) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    的注释
- en: The dataset has been created in a three-stage data engine which combines manual
    labels annotated by humans with automatic labels generated by SAM.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集是在一个三阶段数据引擎中创建的，该引擎结合了人工标注和 SAM 生成的自动标注。
- en: '**Stage 1: Assisted-manual Stage —** A team of professional labelers labeled
    images assisted by an early version of SAM trained on common segmentation datasets.
    They were asked to label the most prominence objects and were encouraged to proceed
    after 30 seconds. At the end of this stage, SAM was retrained with the new labels
    (total 120k images with 4.3M masks).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**阶段 1：辅助手动阶段** — 一组专业标注员在 SAM 的早期版本的帮助下对图像进行了标注，SAM 在常见的分割数据集上进行了训练。他们被要求标注最显著的对象，并被鼓励在
    30 秒后继续。在此阶段结束时，SAM 通过新的标签进行重新训练（总计 12 万张图像和 430 万个掩码）。'
- en: '**Stage 2: Semi-automatic Stage** — In this stage the goal was to increase
    the diversity of the masks by first letting SAM predict some masks and let the
    labelers annotate the missing less prominence objects. At the end of this stage,
    SAM was retrained again including the new samples (total 300k images with 10.2M
    masks).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**阶段 2：半自动阶段** — 在这一阶段的目标是通过首先让SAM预测一些掩码，然后让标注员标注缺少的、不太显著的对象，以增加掩码的多样性。在此阶段结束时，SAM再次进行重新训练，包括新的样本（总计
    30 万张图像和 1020 万个掩码）。'
- en: '**Stage 3: Fully automatic Stage** — In this stage, annotation was fully automatic.
    SAM was prompted with a 32x32 grid of points to generate masks and applied some
    post-processing.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**阶段 3：完全自动阶段** — 在这一阶段，注释完全自动化。SAM 通过 32x32 的网格点生成掩码，并应用一些后处理。'
- en: Dataset Analysis
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集分析
- en: Now let’s have a closer look on some analysis concerning the SA-1B dataset presented
    in the paper.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们仔细看一下论文中关于 SA-1B 数据集的一些分析。
- en: In a first evaluation, the authors created a normalized distribution of the
    masks’ center point. Interestingly, these distributions are subject to a photographer’s
    bias, meaning, most photos center the object of interest in the images center
    and the main axis.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次评估中，作者创建了掩码中心点的标准化分布。有趣的是，这些分布会受到摄影师的偏差，即大多数照片将感兴趣的对象置于图像的中心和主轴上。
- en: '![](../Images/416ddd1903ba104bce16bd225eb01e48.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/416ddd1903ba104bce16bd225eb01e48.png)'
- en: 'Fig.6: Distribution of the objects’ center point location on the image. [Image
    Source](https://arxiv.org/abs/2304.02643) + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：图像中对象中心点位置的分布。 [图片来源](https://arxiv.org/abs/2304.02643) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    的注释
- en: One of SA-1B’s major strengths is the high number of masks per image compared
    to other datasets (Fig.7 left). This also implies that SA-1B has many small masks
    (Fig.7 center). Comparing the masks’ concavity. which is a measure of complexity,
    SA-1B is very similar to other datasets that have been manually labeled (Fig.7
    right).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: SA-1B 的一个主要优点是每张图像的掩码数量相比其他数据集更高（见图 7 左）。这也意味着 SA-1B 有许多小掩码（见图 7 中）。比较掩码的凹凸度，作为复杂性的衡量标准，SA-1B
    与其他手动标注的数据集非常相似（见图 7 右）。
- en: '![](../Images/a12065841d0f5361c0dac6bdb32ba1ce.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a12065841d0f5361c0dac6bdb32ba1ce.png)'
- en: 'Fig.7: Mask properties of SA-1B compared to other datasets. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：SA-1B 的掩码属性与其他数据集的比较。 [图片来源](https://arxiv.org/abs/2304.02643) + [Sascha
    Kirch](https://medium.com/@SaschaKirch) 注释
- en: A high focus is put on responsible AI (RAI), where biases towards certain groups
    of people is not only analyzed but also tried to be mitigated. As Fig.8 shows,
    most of the world’s countries have more than 1000 images and the top 3 countries
    are from different parts of the world. While low-income countries are still under
    represented relatively speaking (0.9% of all samples), on an absolute scale these
    are still over 9M masks and more than other segmentation datasets.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 高度关注负责任的人工智能（RAI），在这里，不仅分析对某些人群的偏见，还尝试减轻这些偏见。如图8所示，世界上大多数国家的图像数量超过1000张，前3名国家来自不同地区。虽然低收入国家的样本相对较少（占所有样本的0.9%），但绝对数量仍超过900万张，比其他分割数据集更多。
- en: '![](../Images/2be5329ed6df5f35d27532e88b3df605.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2be5329ed6df5f35d27532e88b3df605.png)'
- en: 'Fig.8: Estimated geographic distribution of SA-1B images. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：SA-1B 图像的估计地理分布。 [图片来源](https://arxiv.org/abs/2304.02643) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    注释
- en: The authors further investigated the performance discrepancy between perceived
    gender presentation, perceived age group and perceived skin tone. They provided
    the mean IoU (Intersection over Union), between the predicted masks and the ground
    truth masks and a 95% confidence interval. SAM is prompted with either a single
    point or three points. The key message is, that results are very similar (and
    overlapping confidence intervals) within a group which shows that no member of
    the group is favored. The only exception are older people in the perceived age
    group.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 作者进一步研究了感知性别展示、感知年龄组和感知肤色之间的性能差异。他们提供了预测掩码与真实掩码之间的平均IoU（交并比）以及95%的置信区间。SAM的提示可以是单个点或三个点。主要信息是，在一个组内，结果非常相似（且置信区间重叠），这表明该组的任何成员都没有被偏袒。唯一的例外是感知年龄组中的老年人。
- en: '![](../Images/9fcf400cf4b61a13baed1ededa12244e.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9fcf400cf4b61a13baed1ededa12244e.png)'
- en: 'Fig.9: SAM’s performance segmenting people across perceived gender presentation,
    age group, and skin tone. [Image Source](https://arxiv.org/abs/2304.02643) + Annotations
    by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：SAM 在感知性别展示、年龄组和肤色方面的分割性能。 [图片来源](https://arxiv.org/abs/2304.02643) + [Sascha
    Kirch](https://medium.com/@SaschaKirch) 注释
- en: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----f28958c5612d--------------------------------)
    [## Get an email whenever Sascha Kirch publishes 🚀'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 每当 Sascha Kirch 发布新内容时获取电子邮件 🚀](https://medium.com/@SaschaKirch/subscribe?source=post_page-----f28958c5612d--------------------------------)'
- en: Get an email whenever Sascha Kirch publishes 🚀 Looking to learn more about deep
    learning or simply stay up to date…
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[每当 Sascha Kirch 发布新内容时获取电子邮件 🚀 想了解更多关于深度学习的知识或只是保持更新…](https://medium.com/@SaschaKirch/subscribe?source=post_page-----f28958c5612d--------------------------------)'
- en: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----f28958c5612d--------------------------------)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----f28958c5612d--------------------------------)'
- en: Experiments and Ablations
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验和消融研究
- en: 'Segment Anything did provide us with a bunch of experiments mainly focused
    on its zero-shot performance, since this was the main target of the authors: to
    find a promptable zero-shot segmentation model. Also we know from other models
    such as [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    and [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05),
    that prompt tuning is nearly as effective as fine-tuning a model in terms of performance.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything 确实为我们提供了一系列实验，主要集中在其零-shot 性能上，因为这是作者的主要目标：找到一个可提示的零-shot 分割模型。同时，我们也知道其他模型如
    [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    和 [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)
    的表现，提示调整几乎与模型微调在性能上等效。
- en: To perform the experiments, a suite of 23 diverse datasets was compiled. It
    contains samples from a wide variety of data distributions as shown in Fig. 10.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行实验，编制了一套包含23个多样化数据集的集合。它包含了来自各种数据分布的样本，如图10所示。
- en: '![](../Images/abb58c4363f9c280c03085773ffcb60b.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/abb58c4363f9c280c03085773ffcb60b.png)'
- en: 'Fig.10: Samples from the suite of 23 datasets. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：来自23个数据集的样本。[图片来源](https://arxiv.org/abs/2304.02643) + 注释由 [Sascha Kirch](https://medium.com/@SaschaKirch)
- en: Zero-Shot Single Point Valid Mask Evaluation
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零-Shot单点有效掩码评估
- en: Recall that zero-shot means the model was never trained on the data it is exposed
    to during the evaluation. Also recall that single point prompting is quite a difficult
    task due to its ambiguity as depicted in Fig.3.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，零-Shot意味着模型从未在评估过程中接触过的数据上进行训练。同样，单点提示由于其模糊性，如图3所示，是一项相当困难的任务。
- en: In this first experiment, the authors compared SAM against [RITM](https://arxiv.org/abs/2102.06583),
    a strong interactive segmenter which they said performed best on their benchmarks.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个第一次实验中，作者将SAM与 [RITM](https://arxiv.org/abs/2102.06583)进行了比较，RITM是一种强大的交互式分割器，作者表示其在他们的基准测试中表现最佳。
- en: Remember that SAM outputs 3 different masks with an associated score when prompted
    with a single point. In this experiment, the mask with the highest score is selected
    for the evaluation. Since this is sometimes wrong, the authors also evaluate on
    the best mask, which is determined by comparing the predictions to the ground
    truth masks and select those with the highest overlap. These are the “oracle”
    predictions.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，当用单个点进行提示时，SAM会输出3个不同的掩码及其相关分数。在这个实验中，选择分数最高的掩码进行评估。由于这种情况有时会出现错误，作者还对最佳掩码进行了评估，通过将预测结果与真实掩码进行比较，选择重叠度最高的掩码。这些是“oracle”预测。
- en: SAM outperforms RITM in 16 of the 23 datasets in zero-shot single point valid
    mask prediction. When performing oracle predictions, it outperforms RITM in all
    23 datasets.
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在23个数据集中，SAM在16个数据集中中的零-Shot单点有效掩码预测中表现优于RITM。在进行oracle预测时，它在所有23个数据集中都优于RITM。
- en: '![](../Images/88f2550688b089b34fc4d3c34c770708.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88f2550688b089b34fc4d3c34c770708.png)'
- en: 'Fig.11: SAM vs. RITM on 23 datasets. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：在23个数据集上的SAM与RITM对比。[图片来源](https://arxiv.org/abs/2304.02643) + 注释由 [Sascha
    Kirch](https://medium.com/@SaschaKirch)
- en: Zero-Shot Text-to-Mask
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零-Shot文本到掩码
- en: In this experiment SAM was prompted with text. The authors refer to this feature
    as a proof of concept and hence neither perform extensive experiments nor release
    this feature in their official code implementation.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，SAM通过文本进行提示。作者将此功能称为概念验证，因此既没有进行广泛的实验，也没有在其官方代码实现中发布此功能。
- en: Looking at Fig.12, you can see that SAM is able to return correct masks even
    for complex objects like the “beaver tooth grille”. In some other cases, the model
    fails with inserting only text prompts and they show that when providing context
    in form of a point, SAM is able correctly predict either a single or multiple
    wipers, showing that not only the point is considered for prediction but also
    the text.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 看图12，你可以看到SAM能够为像“海狸牙齿格栅”这样的复杂对象返回正确的掩码。在其他一些情况下，模型仅通过文本提示失败，他们展示了在提供点的上下文时，SAM能够正确预测单个或多个擦拭器，显示出不仅点被用于预测，文本也被考虑在内。
- en: '![](../Images/e3b30e8179c463ea42e0487641bcd45f.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3b30e8179c463ea42e0487641bcd45f.png)'
- en: 'Fig.12: Zero-shot text to mask. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：零-shot文本到掩码。[图片来源](https://arxiv.org/abs/2304.02643) + 注释由 [Sascha Kirch](https://medium.com/@SaschaKirch)
- en: Zero-Shot Edge Detection
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零-Shot边缘检测
- en: Interestingly, SAM can also be used for edge detection, a task it was not considered
    to do nor did it had access to such data during training.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，SAM也可以用于边缘检测，这是一项它在训练过程中未被考虑的任务，也没有访问相关数据。
- en: To predict maps, SAM is first prompted with a grid of 16x16 points resulting
    in 768 predicted masks (object, part and sub-part for each of the 256 points).
    The resulting masks are then filtered and post-processed to obtain the edge masks.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预测图像，SAM首先使用16x16点的网格进行提示，生成768个预测的掩码（每个256个点的对象、部分和子部分）。然后对生成的掩码进行筛选和后处理，以获取边缘掩码。
- en: As shown in Fig. 13, compared to the ground truth, SAM predicts much more details.
    But to be fair, if the GT is not complete or covers a different layer of abstraction,
    this comparison seems not fair to me. But still, the performance is quite good!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如图13所示，与真实数据相比，SAM预测了更多的细节。但为了公平起见，如果真实数据不完整或覆盖了不同的抽象层次，这种比较对我来说似乎不太公平。但总的来说，性能还是相当不错的！
- en: '![](../Images/9ae9bb4def9477efc2149ca5e3b975bf.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ae9bb4def9477efc2149ca5e3b975bf.png)'
- en: 'Fig.13: SAM’s zero-shot edge prediction. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：SAM 的零样本边缘预测。 [图片来源](https://arxiv.org/abs/2304.02643) + 注释由 [Sascha Kirch](https://medium.com/@SaschaKirch)
- en: Zero-Shot Instance Segmentation
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零样本实例分割
- en: For this experiment, SAM is prompted with a bounding box output of a fully supervised
    ViTDet-H trained on [COCO](https://cocodataset.org/#home) and [LVIS](https://www.lvisdataset.org/).
    The resulting mask is then fed again into SAM together with the initial bounding
    box to refine the result. A comparison between ViTDet and SAM is shown in Fig.14.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实验，SAM 以 [COCO](https://cocodataset.org/#home) 和 [LVIS](https://www.lvisdataset.org/)
    上训练的完全监督的 ViTDet-H 的边界框输出作为提示。然后将生成的掩码连同初始边界框一起输入到 SAM 中，以精细化结果。图14 显示了 ViTDet
    和 SAM 的比较。
- en: '![](../Images/3d740a4bf3e6040227b562f9d86727be.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d740a4bf3e6040227b562f9d86727be.png)'
- en: 'Fig.14: Zero-shot instance segmentation on LVIS v1\. SAM. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：在 LVIS v1 上的零样本实例分割。 [图片来源](https://arxiv.org/abs/2304.02643) + 注释由 [Sascha
    Kirch](https://medium.com/@SaschaKirch)
- en: 'Two things to note here: If you have a look into [COCO](https://cocodataset.org/#home)
    and [LVIS](https://www.lvisdataset.org/), you will find that the masks are not
    pixel-aligned with the objects. This bias is present in ViTDet, so that’s why
    the quality of SAM seems to be better. How much better is hard to tell with computed
    metrics, since the ground truth has the same bias and compared to a bad GT, SAM
    would perform worse. Hence, they asked humans to visually inspect those. Second,
    why does this elephant only has 3 legs 😅. No matter how hard I try I can’t see
    the fourth one…'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两件事需要注意：如果你查看 [COCO](https://cocodataset.org/#home) 和 [LVIS](https://www.lvisdataset.org/)，你会发现掩码与对象的像素对齐并不完全。这种偏差在
    ViTDet 中也存在，这就是为什么 SAM 的质量似乎更好的原因。由于基准真实值具有相同的偏差，而与差的 GT 相比，SAM 的表现可能更差。因此，他们要求人工进行视觉检查。其次，为什么这只大象只有
    3 条腿 😅。无论我怎么努力，我都看不到第四条腿…
- en: Ablations
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消融实验
- en: In the ablation section the authors were mainly concerned with scaling either
    the dataset, the number of points for prompting and the size of the image encoder
    (see Fig.13). Performance is reported in mean IoU.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在消融实验部分，作者主要关注于扩展数据集、提示点数量和图像编码器的大小（见图13）。性能以平均 IoU 报告。
- en: '![](../Images/96239e65aa812035d0b012ce9fb13b38.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96239e65aa812035d0b012ce9fb13b38.png)'
- en: 'Fig.15: Ablation studies. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：消融研究。 [图片来源](https://arxiv.org/abs/2304.02643) + 注释由 [Sascha Kirch](https://medium.com/@SaschaKirch)
- en: Interestingly, even though scaling data and scaling model size influences the
    mIoU performance, it saturates. This might either indicate that the model is so
    good there is not much room for improvement or probably it’s the limitation of
    their approach.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，尽管数据扩展和模型规模扩展影响了 mIoU 性能，但它达到饱和状态。这可能表明模型已经足够好，没有太多改进的空间，或者可能是他们方法的局限性。
- en: Conclusion
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Segment Anything introduced the promptable Segment Anything Model (SAM) as well
    as a large-scale dataset for segmentation containing over 1 billion masks in over
    11 million images. Being able to prompt a segmentation model brings a lot of flexibility
    like adapting a trained model to unseen tasks or to be able to detect unknown
    classes. While some debate if SAM is considered a foundation model since it has
    ben trained in a supervised manner, it still has shown remarkable results and
    has been widely adopted.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything 引入了可提示的 Segment Anything Model (SAM) 以及一个包含超过 10 亿个掩码的分割大规模数据集，涵盖超过
    1100 万张图像。能够提示分割模型带来了很多灵活性，比如将训练好的模型适应于未见过的任务或检测未知类别。虽然有些人讨论 SAM 是否可以被视为基础模型，因为它是以监督方式训练的，但它仍然显示出了显著的成果，并已被广泛采用。
- en: Further Readings & Resources
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读与资源
- en: 'As you probably know yourself: the field of deep learning is evolving in an
    unbelievably fast pace. Hence it is no surprise that right after the release of
    SAM, many new projects build upon its success, further improving either the quality
    of predictions, decreasing inference time or make the model suitable for on the
    edge applications.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你自己可能知道的那样：深度学习领域正在以令人难以置信的速度发展。因此，SAM 发布后，许多新项目在其成功的基础上进一步改进了预测质量、减少了推理时间，或者使模型适用于边缘应用，这也就不足为奇了。
- en: 'Following a list of interesting resources building upon SAM:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些有趣的资源，它们在 SAM 的基础上进行扩展：
- en: '[Grounded Segment Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything)'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[基础分割任何内容](https://github.com/IDEA-Research/Grounded-Segment-Anything)'
- en: '[Segment Anything in High Quality](https://arxiv.org/abs/2306.01567)'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[高质量分割任何内容](https://arxiv.org/abs/2306.01567)'
- en: '[Fast Segment Anything](https://arxiv.org/abs/2306.12156)'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[快速分割任何内容](https://arxiv.org/abs/2306.12156)'
- en: '[Faster Segment Anything: Towards Lightweight SAM for Mobile Applications](https://arxiv.org/abs/2306.14289)'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[更快的分割任何内容：朝着适用于移动应用的轻量级 SAM](https://arxiv.org/abs/2306.14289)'
- en: 'Here I share some links if you want to have a hands-on experience with SAM
    and SA-1B:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我分享一些链接，如果你想亲自体验 SAM 和 SA-1B：
- en: '[SA-1B dataset download](https://ai.meta.com/datasets/segment-anything-downloads/)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SA-1B 数据集下载](https://ai.meta.com/datasets/segment-anything-downloads/)'
- en: '[Segment Anything Demo](https://segment-anything.com/demo)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分割任何内容演示](https://segment-anything.com/demo)'
- en: '[Segment Anything GitHub](https://github.com/facebookresearch/segment-anything)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分割任何内容 GitHub](https://github.com/facebookresearch/segment-anything)'
- en: '[Python Notebook to experiment with SAM](https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 笔记本用于实验 SAM](https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb)'
- en: 'Here are some of my articles that walk you through some related foundation
    models:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我一些文章的链接，带你了解一些相关的基础模型：
- en: '[](/the-clip-foundation-model-7770858b487d?source=post_page-----f28958c5612d--------------------------------)
    [## The CLIP Foundation Model'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/the-clip-foundation-model-7770858b487d?source=post_page-----f28958c5612d--------------------------------)
    [## CLIP 基础模型'
- en: Paper Summary— Learning Transferable Visual Models From Natural Language Supervision
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 论文总结— 从自然语言监督中学习可迁移的视觉模型
- en: 'towardsdatascience.com](/the-clip-foundation-model-7770858b487d?source=post_page-----f28958c5612d--------------------------------)
    [](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----f28958c5612d--------------------------------)
    [## GLIP: Introducing Language-Image Pre-Training to Object Detection'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 'towardsdatascience.com](/the-clip-foundation-model-7770858b487d?source=post_page-----f28958c5612d--------------------------------)
    [](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----f28958c5612d--------------------------------)
    [## GLIP: 将语言-图像预训练引入物体检测'
- en: 'Paper Summary: Grounded Language-Image Pre-training'
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 论文总结：基础语言-图像预训练
- en: towardsdatascience.com](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----f28958c5612d--------------------------------)
    [](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----f28958c5612d--------------------------------)
    [## BYOL -The Alternative to Contrastive Self-Supervised Learning
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----f28958c5612d--------------------------------)
    [](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----f28958c5612d--------------------------------)
    [## BYOL - 对比自监督学习的替代方案
- en: 'Paper Analysis— Bootstrap Your Own Latent: A New Approach to Self-Supervised
    Learning'
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '论文分析—Bootstrap Your Own Latent: 自监督学习的新方法'
- en: towardsdatascience.com](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----f28958c5612d--------------------------------)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----f28958c5612d--------------------------------)
