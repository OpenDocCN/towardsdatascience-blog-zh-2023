- en: 'Segment Anything: Promptable Segmentation of Arbitrary Objects'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Segment Anything: å¯æç¤ºçš„ä»»æ„å¯¹è±¡åˆ†å‰²'
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d](https://towardsdatascience.com/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d](https://towardsdatascience.com/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d)
- en: '[ğŸš€Saschaâ€™s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[ğŸš€Saschaçš„è®ºæ–‡ä¿±ä¹éƒ¨](https://towardsdatascience.com/tagged/saschas-paper-club)'
- en: Segment Anything by A. Krillov et. al.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Segment Anything ç”± A. Krillov ç­‰äºº
- en: '[](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f28958c5612d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f28958c5612d--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f28958c5612d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f28958c5612d--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f28958c5612d--------------------------------)
    Â·12 min readÂ·Sep 14, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f28958c5612d--------------------------------)
    Â·12 åˆ†é’Ÿé˜…è¯»Â·2023å¹´9æœˆ14æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Todayâ€™s paper walkthrough it is going to be visual! We will analyze S*egment
    Anything*, a paper by Metaâ€™s AI research team that made headlines not only in
    the research community but also by all sorts of deep learning practitioners and
    advocates.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Šå¤©çš„è®ºæ–‡è®²è§£å°†æ˜¯è§†è§‰åŒ–çš„ï¼æˆ‘ä»¬å°†åˆ†æ *Segment Anything*ï¼Œè¿™æ˜¯Meta AIç ”ç©¶å›¢é˜Ÿçš„ä¸€ç¯‡è®ºæ–‡ï¼Œå®ƒä¸ä»…åœ¨ç ”ç©¶ç•Œå¼•èµ·äº†å…³æ³¨ï¼Œä¹Ÿåœ¨å„ç§æ·±åº¦å­¦ä¹ ä»ä¸šè€…å’Œæ”¯æŒè€…ä¸­å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚
- en: Segment Anything introduces the task of promptable segmentation, it introduces
    the segment anything model (SAM), and it details the generation of a new publicly
    available dataset of 11 million images containing more than 1 billion masks. SAM
    has been widely adopted by the community and resulted in some new state-of-the-art
    foundation models such as [Grounded-SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything)
    that combines [Grounding DINO](https://arxiv.org/abs/2303.05499) with SAM.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything å¼•å…¥äº†å¯æç¤ºçš„åˆ†å‰²ä»»åŠ¡ï¼Œä»‹ç»äº† segment anything æ¨¡å‹ï¼ˆSAMï¼‰ï¼Œå¹¶è¯¦ç»†æè¿°äº†ç”Ÿæˆä¸€ä¸ªåŒ…å«è¶…è¿‡10äº¿ä¸ªæ©è†œçš„1100ä¸‡å¼ å›¾ç‰‡çš„æ–°å…¬å¼€æ•°æ®é›†ã€‚SAM
    å·²è¢«å¹¿æ³›é‡‡çº³ï¼Œå¹¶äº§ç”Ÿäº†ä¸€äº›æ–°çš„æœ€å…ˆè¿›åŸºç¡€æ¨¡å‹ï¼Œå¦‚ [Grounded-SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything)ï¼Œå®ƒå°†
    [Grounding DINO](https://arxiv.org/abs/2303.05499) ä¸ SAM ç»“åˆèµ·æ¥ã€‚
- en: '![](../Images/3655fd5fedea80f78ac89299c8bd7b30.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3655fd5fedea80f78ac89299c8bd7b30.png)'
- en: Image created from [publication](https://arxiv.org/abs/2304.02643) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºäº [å‡ºç‰ˆç‰©](https://arxiv.org/abs/2304.02643) ç”± [Sascha Kirch](https://medium.com/@SaschaKirch)
- en: '**Paper:** [Segment Anything](https://arxiv.org/abs/2304.02643) by [Alexander
    Kirillov](https://arxiv.org/search/cs?searchtype=author&query=Kirillov%2C+A) et.
    al., 5 Apr. 2023'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**è®ºæ–‡:** [Segment Anything](https://arxiv.org/abs/2304.02643) ç”± [Alexander Kirillov](https://arxiv.org/search/cs?searchtype=author&query=Kirillov%2C+A)
    ç­‰äººï¼Œ2023å¹´4æœˆ5æ—¥'
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/facebookresearch/segment-anything)
    â€” [Demo](https://segment-anything.com/demo) â€” [Project Page](https://segment-anything.com/)
    â€” [Dataset](https://segment-anything.com/dataset/index.html) â€” [HuggingFace](https://huggingface.co/docs/transformers/main/model_doc/sam)'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**èµ„æº:** [GitHub](https://github.com/facebookresearch/segment-anything) â€” [æ¼”ç¤º](https://segment-anything.com/demo)
    â€” [é¡¹ç›®é¡µé¢](https://segment-anything.com/) â€” [æ•°æ®é›†](https://segment-anything.com/dataset/index.html)
    â€” [HuggingFace](https://huggingface.co/docs/transformers/main/model_doc/sam)'
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** segmentation, zero-shot prediction, computer vison, prompting,
    large-scale'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ç±»åˆ«:** åˆ†å‰²ã€é›¶-shoté¢„æµ‹ã€è®¡ç®—æœºè§†è§‰ã€æç¤ºã€å¤§è§„æ¨¡'
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[**å…¶ä»–æ•™ç¨‹**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
- en: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    â€” [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    â€” [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    â€” [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    â€” [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    â€” [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    â€” [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    â€” [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    â€” [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
- en: Outline
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤§çº²
- en: Context & Background
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: èƒŒæ™¯ä¸èƒŒæ™¯
- en: SAM â€” Segment Anything Model
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SAM â€” Segment Anything Model
- en: SA-1B â€” Dataset with 1 Billion Masks
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SA-1B â€” å…·æœ‰10äº¿ä¸ªæ©ç çš„æ•°æ®é›†
- en: Experiments and Ablations
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®éªŒä¸æ¶ˆè
- en: Conclusion
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Further Readings & Resources
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»ä¸èµ„æº
- en: Context & Background
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: èƒŒæ™¯ä¸èƒŒæ™¯
- en: 'The authors of Segment Anything made a clear statement: â€œ*[â€¦] our goal is to
    build a foundation model for image segmentation.*â€ Foundation models originated
    from the great success of Natural Language Processing (NLP). Models have been
    trained on a laaaarge scale in a self-supervised fashion. These models usually
    perform very well at zero-shot tasks, meaning they can solve tasks different to
    those they were trained on and perform reasonable well or even better as their
    supervised competitors. In recent years, many researchers worked on bringing the
    success of NLP foundation models to other domains such as computer vision.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ã€ŠSegment Anythingã€‹çš„ä½œè€…æ˜ç¡®å£°æ˜ï¼šâ€œ*æˆ‘ä»¬ç›®æ ‡æ˜¯å»ºç«‹ä¸€ä¸ªå›¾åƒåˆ†å‰²çš„åŸºç¡€æ¨¡å‹ã€‚*â€ åŸºç¡€æ¨¡å‹æºäºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„å·¨å¤§æˆåŠŸã€‚è¿™äº›æ¨¡å‹åœ¨è‡ªç›‘ç£çš„æ–¹å¼ä¸‹ç»è¿‡äº†å¤§è§„æ¨¡çš„è®­ç»ƒã€‚é€šå¸¸ï¼Œè¿™äº›æ¨¡å‹åœ¨é›¶-shot
    ä»»åŠ¡ä¸­è¡¨ç°éå¸¸å¥½ï¼Œå³å®ƒä»¬å¯ä»¥è§£å†³ä¸è®­ç»ƒæ—¶ä¸åŒçš„ä»»åŠ¡ï¼Œå¹¶è¡¨ç°å¾—ç›¸å½“ä¸é”™ï¼Œç”šè‡³æ¯”å…¶ç›‘ç£å‹å¯¹æ‰‹æ›´ä¼˜ç§€ã€‚è¿‘å¹´æ¥ï¼Œè®¸å¤šç ”ç©¶äººå‘˜è‡´åŠ›äºå°†NLPåŸºç¡€æ¨¡å‹çš„æˆåŠŸå¸¦åˆ°è®¡ç®—æœºè§†è§‰ç­‰å…¶ä»–é¢†åŸŸã€‚
- en: Models such as [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    and [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)
    made it possible to condition an image classification or object detection task
    on text prompts, rather than a fixed set of classes. Other models, such as [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)
    or DINO, came up with different techniques to learn semantically rich representations
    of input images, which is one of the key requirements for many computer vision
    applications.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¦‚ [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    å’Œ [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)
    ä½¿å¾—å¯ä»¥æ ¹æ®æ–‡æœ¬æç¤ºå¯¹å›¾åƒåˆ†ç±»æˆ–å¯¹è±¡æ£€æµ‹ä»»åŠ¡è¿›è¡Œæ¡ä»¶é™åˆ¶ï¼Œè€Œä¸æ˜¯å›ºå®šçš„ç±»åˆ«é›†åˆã€‚å…¶ä»–æ¨¡å‹ï¼Œå¦‚ [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)
    æˆ– DINOï¼Œæå‡ºäº†ä¸åŒçš„æŠ€æœ¯æ¥å­¦ä¹ è¾“å…¥å›¾åƒçš„è¯­ä¹‰ä¸°å¯Œè¡¨ç¤ºï¼Œè¿™ä¹Ÿæ˜¯è®¸å¤šè®¡ç®—æœºè§†è§‰åº”ç”¨çš„å…³é”®è¦æ±‚ã€‚
- en: 'The Segment Anything paper aims to:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ã€ŠSegment Anythingã€‹è®ºæ–‡æ—¨åœ¨ï¼š
- en: Enable zero-shot segmentation by prompting
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡æç¤ºå¯ç”¨é›¶-shot åˆ†å‰²
- en: Train a large-scale model (SAM) as demonstrator
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®­ç»ƒä¸€ä¸ªå¤§è§„æ¨¡æ¨¡å‹ï¼ˆSAMï¼‰ä½œä¸ºæ¼”ç¤ºæ¨¡å‹
- en: Collect and release the largest publicly available dataset for segmentation.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ”¶é›†å¹¶å‘å¸ƒæœ€å¤§çš„å…¬å¼€å¯ç”¨åˆ†å‰²æ•°æ®é›†ã€‚
- en: '***But why is zero-shot performance so important?*** â€” The answer is two-fold.
    First, initially computer vision models have been trained in a supervised fashion
    requiring not only data, but also a lot of ground truth labels. Collection these
    data is extremely time-consuming and costly. Second, the classes a model can predict
    are limited to a fixed set of classes used for training. If you would like to
    add a new class to your model, you would need to first collect the data and retrain
    the model.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä½†ä¸ºä»€ä¹ˆé›¶-shot æ€§èƒ½å¦‚æ­¤é‡è¦ï¼Ÿ*** â€” ç­”æ¡ˆæœ‰ä¸¤ä¸ªæ–¹é¢ã€‚é¦–å…ˆï¼Œæœ€åˆè®¡ç®—æœºè§†è§‰æ¨¡å‹æ˜¯ä»¥ç›‘ç£æ–¹å¼è®­ç»ƒçš„ï¼Œè¿™ä¸ä»…éœ€è¦æ•°æ®ï¼Œè¿˜éœ€è¦å¤§é‡çš„çœŸå®æ ‡ç­¾ã€‚æ”¶é›†è¿™äº›æ•°æ®æ˜¯æå…¶è€—æ—¶å’Œæ˜‚è´µçš„ã€‚å…¶æ¬¡ï¼Œæ¨¡å‹å¯ä»¥é¢„æµ‹çš„ç±»åˆ«ä»…é™äºè®­ç»ƒæ—¶ä½¿ç”¨çš„å›ºå®šç±»åˆ«é›†åˆã€‚å¦‚æœä½ æƒ³å‘æ¨¡å‹ä¸­æ·»åŠ ä¸€ä¸ªæ–°ç±»åˆ«ï¼Œä½ éœ€è¦é¦–å…ˆæ”¶é›†æ•°æ®å¹¶é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚'
- en: '***How is it possible to prompt a segmentation model?*** â€” You might be familiar
    with text prompting from models like ChatGPT, [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    or [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05).
    While SAM in principle also was tested with text prompts, it is mainly prompted
    with either masks, points, boxes or point grids as shown in the image bellow.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '***å¦‚ä½•å¯¹åˆ†å‰²æ¨¡å‹è¿›è¡Œæç¤ºï¼Ÿ*** â€” ä½ å¯èƒ½å¯¹æ¥è‡ªChatGPTã€[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)æˆ–[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)ç­‰æ¨¡å‹çš„æ–‡æœ¬æç¤ºæ¯”è¾ƒç†Ÿæ‚‰ã€‚è™½ç„¶SAMåŸåˆ™ä¸Šä¹Ÿç»è¿‡äº†æ–‡æœ¬æç¤ºçš„æµ‹è¯•ï¼Œä½†å®ƒä¸»è¦é€šè¿‡æ©ç ã€ç‚¹ã€æ¡†æˆ–ç‚¹ç½‘æ ¼æ¥è¿›è¡Œæç¤ºï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚'
- en: '![](../Images/78dbec949be2553ac6a297eed84a8ee7.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78dbec949be2553ac6a297eed84a8ee7.png)'
- en: 'Fig.1: Different input prompts and resulting masks. Photo by [Terence Burke](https://unsplash.com/@ancientwanderer?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) + masks
    generated by [Sascha Kirch](https://medium.com/@SaschaKirch) with [SAM](https://segment-anything.com/demo)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1ï¼šä¸åŒè¾“å…¥æç¤ºå’Œç”Ÿæˆçš„æ©ç ã€‚ç…§ç‰‡ç”±[Terence Burke](https://unsplash.com/@ancientwanderer?utm_source=medium&utm_medium=referral)æ‹æ‘„ï¼Œå‘å¸ƒäº[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
    + æ©ç ç”±[Sascha Kirch](https://medium.com/@SaschaKirch)ç”¨[SAM](https://segment-anything.com/demo)ç”Ÿæˆ
- en: Having put SAM into context letâ€™s now switch gears and have a closer look onto
    the Segment Anything Model, aka SAM.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: äº†è§£äº†SAMçš„èƒŒæ™¯åï¼Œè®©æˆ‘ä»¬è½¬åˆ°é‡ç‚¹ï¼Œè¯¦ç»†äº†è§£Segment Anything Modelï¼Œå³SAMã€‚
- en: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
- en: '[Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)'
- en: Paper Walkthroughs by Sascha Kirch
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sascha Kirchçš„è®ºæ–‡è®²è§£
- en: '[View list](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2?source=post_page-----f28958c5612d--------------------------------)7
    stories![â€œDDPMâ€Šâ€”â€ŠDenoising Diffusion Probabilistic Models â€œ paper illustration
    by Sascha Kirch](../Images/6e785c0a911386676abebe0fa646f483.png)![â€œDepth Anythingâ€
    paper illustration by Sascha Kirch](../Images/bd8cd71a02e42cf64d0afd39f41f48e0.png)![](../Images/8708d91a4a1902cef889ced95d46fc39.png)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2?source=post_page-----f28958c5612d--------------------------------)7ä¸ªæ•…äº‹![â€œDDPMâ€Šâ€”â€Šå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹â€
    è®ºæ–‡æ’å›¾ï¼ŒSascha Kirch](../Images/6e785c0a911386676abebe0fa646f483.png)![â€œDepth Anythingâ€
    è®ºæ–‡æ’å›¾ï¼ŒSascha Kirch](../Images/bd8cd71a02e42cf64d0afd39f41f48e0.png)![](../Images/8708d91a4a1902cef889ced95d46fc39.png)'
- en: SAM â€” Segment Anything Model
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SAM â€” Segment Anything Model
- en: 'The Segment Anything Model (SAM) is a multi-modal model that inputs an image
    and one or more prompts and outputs a valid segmentation mask. The model consists
    of three main modules: image encoder, prompt encoder and mask decoder.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything Modelï¼ˆSAMï¼‰æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¨¡å‹ï¼Œå®ƒè¾“å…¥ä¸€å¼ å›¾åƒå’Œä¸€ä¸ªæˆ–å¤šä¸ªæç¤ºï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæœ‰æ•ˆçš„åˆ†å‰²æ©ç ã€‚è¯¥æ¨¡å‹ç”±ä¸‰ä¸ªä¸»è¦æ¨¡å—ç»„æˆï¼šå›¾åƒç¼–ç å™¨ã€æç¤ºç¼–ç å™¨å’Œæ©ç è§£ç å™¨ã€‚
- en: SAM is prompted with either a mask, a set of points, a bounding box or text
    or any combination of those.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: SAM å¯ä»¥é€šè¿‡æ©ç ã€ä¸€ç»„ç‚¹ã€è¾¹ç•Œæ¡†æˆ–æ–‡æœ¬ï¼Œæˆ–è¿™äº›çš„ä»»ä½•ç»„åˆæ¥è¿›è¡Œæç¤ºã€‚
- en: 'NOTE: Even though the paper mentions and experiments with text as prompt, it
    is not yet released (as of September 2023) in the [official implementation](https://github.com/facebookresearch/segment-anything)
    nor in the [SAM demo](https://segment-anything.com/demo).'
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šå°½ç®¡è®ºæ–‡æåˆ°å¹¶å®éªŒäº†æ–‡æœ¬ä½œä¸ºæç¤ºï¼Œä½†æˆªè‡³2023å¹´9æœˆï¼Œæ–‡æœ¬æç¤ºå°šæœªåœ¨[å®˜æ–¹å®ç°](https://github.com/facebookresearch/segment-anything)æˆ–[SAMæ¼”ç¤º](https://segment-anything.com/demo)ä¸­å‘å¸ƒã€‚
- en: '![](../Images/61ae8b49ffb731555c8a774e41126b9c.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61ae8b49ffb731555c8a774e41126b9c.png)'
- en: 'Fig.2: SAM architecture. [Image Source](https://arxiv.org/abs/2304.02643) +
    Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2ï¼šSAMæ¶æ„ã€‚[å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2304.02643) + æ³¨é‡Šç”±[Sascha Kirch](https://medium.com/@SaschaKirch)
- en: '**Image Encoder** â€” Outputs an image embedding for a given input image. SAM
    implements and adapts a pre-trained ViT-H/16 masked auto-encoder. This is a relatively
    large model with strong performance.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾åƒç¼–ç å™¨** â€” ä¸ºç»™å®šçš„è¾“å…¥å›¾åƒè¾“å‡ºå›¾åƒåµŒå…¥ã€‚SAM å®ç°å¹¶é€‚é…äº†ä¸€ä¸ªé¢„è®­ç»ƒçš„ViT-H/16æ©ç è‡ªç¼–ç å™¨ã€‚è¿™æ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒå¤§çš„æ¨¡å‹ï¼Œæ€§èƒ½å¼ºåŠ²ã€‚'
- en: '**Prompt Encoder** â€” Sparse prompts (i.e. points, boxes and text) are translated
    into embedding vectors. Text prompts are converted into text embeddings using
    [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    before feeding it into the prompt encoder. Dense prompts (i.e. masks) are simply
    downsampled with strided convolutions and added with the image embeddings. All
    embeddings are then fed into the final stage: the mask decoder.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**æç¤ºç¼–ç å™¨** â€” ç¨€ç–æç¤ºï¼ˆä¾‹å¦‚ç‚¹ã€æ¡†å’Œæ–‡æœ¬ï¼‰è¢«è½¬æ¢ä¸ºåµŒå…¥å‘é‡ã€‚æ–‡æœ¬æç¤ºåœ¨è¾“å…¥æç¤ºç¼–ç å™¨ä¹‹å‰ï¼Œä½¿ç”¨ [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    è½¬æ¢ä¸ºæ–‡æœ¬åµŒå…¥ã€‚å¯†é›†æç¤ºï¼ˆä¾‹å¦‚æ©ç ï¼‰åˆ™ç®€å•åœ°é€šè¿‡æ­¥å¹…å·ç§¯ä¸‹é‡‡æ ·ï¼Œå¹¶ä¸å›¾åƒåµŒå…¥ç›¸åŠ ã€‚æ‰€æœ‰åµŒå…¥éšåè¢«é€å…¥æœ€ç»ˆé˜¶æ®µï¼šæ©ç è§£ç å™¨ã€‚'
- en: '**Mask Decoder** â€” Takes a set of image embeddings (optionally containing the
    dense mask embeddings) and a set of prompt embeddings and outputs a valid segmentation
    mask.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ©ç è§£ç å™¨** â€” æ¥å—ä¸€ç»„å›¾åƒåµŒå…¥ï¼ˆå¯é€‰åœ°åŒ…å«å¯†é›†æ©ç åµŒå…¥ï¼‰å’Œä¸€ç»„æç¤ºåµŒå…¥ï¼Œå¹¶è¾“å‡ºæœ‰æ•ˆçš„åˆ†å‰²æ©ç ã€‚'
- en: 'There are two more details we should address: ambiguities of prompts and performance.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰ä¸¤ä¸ªç»†èŠ‚æˆ‘ä»¬åº”è¯¥è®¨è®ºï¼šæç¤ºçš„æ­§ä¹‰æ€§å’Œæ€§èƒ½ã€‚
- en: In a nutshell, the less context a prompt contains, the more ambiguous it is
    and the more difficult it is for the model the provide the correct output. For
    text-prompts we have seen this connection between the specificness of the input
    text and the modelâ€™s performance in [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    and [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05).
    Similar, providing a single point as input might result in a variety of possible
    masks. For that reason, SAM outputs a set of three output masks corresponding
    to the object level, the part level and the sub-part level of a valid mask as
    indicated in the image bellow.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼Œæç¤ºåŒ…å«çš„ä¸Šä¸‹æ–‡è¶Šå°‘ï¼Œå°±è¶Šæ¨¡ç³Šï¼Œå¯¹æ¨¡å‹æä¾›æ­£ç¡®è¾“å‡ºçš„éš¾åº¦ä¹Ÿè¶Šå¤§ã€‚å¯¹äºæ–‡æœ¬æç¤ºï¼Œæˆ‘ä»¬å·²ç»åœ¨ [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    å’Œ [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)
    ä¸­çœ‹åˆ°äº†è¾“å…¥æ–‡æœ¬çš„å…·ä½“æ€§ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„è¿™ç§è”ç³»ã€‚åŒæ ·ï¼Œæä¾›ä¸€ä¸ªå•ç‚¹ä½œä¸ºè¾“å…¥å¯èƒ½ä¼šäº§ç”Ÿå¤šç§å¯èƒ½çš„æ©ç ã€‚å› æ­¤ï¼ŒSAM è¾“å‡ºä¸€ç»„ä¸‰ç§æ©ç ï¼Œåˆ†åˆ«å¯¹åº”äºæœ‰æ•ˆæ©ç çš„å¯¹è±¡çº§åˆ«ã€éƒ¨ä»¶çº§åˆ«å’Œå­éƒ¨ä»¶çº§åˆ«ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚
- en: '![](../Images/e2ca2d9f5176ee540cfdd88fc1d259ae.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2ca2d9f5176ee540cfdd88fc1d259ae.png)'
- en: 'Fig.3: Ambiguity for single-point prompts. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3ï¼šå•ç‚¹æç¤ºçš„æ­§ä¹‰æ€§ã€‚[å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2304.02643) + ç”± [Sascha Kirch](https://medium.com/@SaschaKirch)
    æ³¨é‡Š
- en: 'The second detail I want to mention is performance in terms of inference speed.
    Did you notice that the image encoder is by far the largest sub-module in SAM?
    Well, thatâ€™s an unfair question because I did not tell you so far, but SAM is
    designed in a way to have semantically rich image embeddings (which often requires
    a large model) to subsequently act upon these embeddings applying a light-weight
    prompt encoder and a light-weight mask decoder. The good thing: one must only
    run the image encoder once per image and can then prompt the model multiple times
    using the same image embedding. This allows SAM to be executed in a browser taking
    only ~50ms to predict a mask for a given prompt (after the image embeddings have
    been calculated).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³æåˆ°çš„ç¬¬äºŒä¸ªç»†èŠ‚æ˜¯æ¨ç†é€Ÿåº¦æ–¹é¢çš„æ€§èƒ½ã€‚ä½ æ˜¯å¦æ³¨æ„åˆ°å›¾åƒç¼–ç å™¨æ˜¯SAMä¸­æœ€å¤§çš„ä¸€éƒ¨åˆ†ï¼Ÿå¥½å§ï¼Œè¿™ä¸ªé—®é¢˜æœ‰ç‚¹ä¸å…¬å¹³ï¼Œå› ä¸ºæˆ‘ä¹‹å‰æ²¡æœ‰å‘Šè¯‰ä½ ï¼Œä½†SAMçš„è®¾è®¡ç›®çš„æ˜¯æ‹¥æœ‰è¯­ä¹‰ä¸°å¯Œçš„å›¾åƒåµŒå…¥ï¼ˆé€šå¸¸éœ€è¦ä¸€ä¸ªå¤§å‹æ¨¡å‹ï¼‰ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªè½»é‡çº§çš„æç¤ºç¼–ç å™¨å’Œè½»é‡çº§çš„æ©ç è§£ç å™¨æ¥å¤„ç†è¿™äº›åµŒå…¥ã€‚å¥½çš„ä¸€ç‚¹æ˜¯ï¼šæ¯å¼ å›¾åƒåªéœ€è¿è¡Œä¸€æ¬¡å›¾åƒç¼–ç å™¨ï¼Œç„¶åå¯ä»¥ä½¿ç”¨ç›¸åŒçš„å›¾åƒåµŒå…¥å¤šæ¬¡æç¤ºæ¨¡å‹ã€‚è¿™ä½¿å¾—SAMå¯ä»¥åœ¨æµè§ˆå™¨ä¸­è¿è¡Œï¼Œä»…éœ€
    ~50ms æ¥é¢„æµ‹ç»™å®šæç¤ºçš„æ©ç ï¼ˆåœ¨å›¾åƒåµŒå…¥è®¡ç®—åï¼‰ã€‚
- en: Letâ€™s have a closer look on the light-weight mask decoder. It inputs the image
    embeddings and prompt embeddings and outputs a set of masks with corresponding
    scores. Internally, two consecutive decoder blocks perform a combination of self-attention
    and cross-attention to generate a strong dependence between the image and the
    prompts. A simple up-sampling network in combination with another cross-attention
    block generates the masks and the scores.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ›´è¯¦ç»†åœ°çœ‹çœ‹è½»é‡çº§æ©ç è§£ç å™¨ã€‚å®ƒè¾“å…¥å›¾åƒåµŒå…¥å’Œæç¤ºåµŒå…¥ï¼Œå¹¶è¾“å‡ºä¸€ç»„å¸¦æœ‰ç›¸åº”åˆ†æ•°çš„æ©ç ã€‚åœ¨å†…éƒ¨ï¼Œä¸¤ä¸ªè¿ç»­çš„è§£ç å™¨å—é€šè¿‡è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›çš„ç»„åˆç”Ÿæˆå›¾åƒä¸æç¤ºä¹‹é—´çš„å¼ºä¾èµ–å…³ç³»ã€‚ä¸€ä¸ªç®€å•çš„ä¸Šé‡‡æ ·ç½‘ç»œç»“åˆå¦ä¸€ä¸ªäº¤å‰æ³¨æ„åŠ›å—ç”Ÿæˆæ©ç å’Œåˆ†æ•°ã€‚
- en: '![](../Images/4404071c556ef28c5df3a42f22b0ce7e.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4404071c556ef28c5df3a42f22b0ce7e.png)'
- en: 'Fig.4: Detailed architecture of the mask decoder. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4ï¼šæ©ç è§£ç å™¨çš„è¯¦ç»†æ¶æ„ã€‚ [å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2304.02643) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    çš„æ³¨é‡Š
- en: SA-1B â€” Dataset with 1 Billion Masks
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SA-1B â€” å…·æœ‰ 10 äº¿æ©ç çš„æ•°æ®é›†
- en: The second great deal of *Segment Anything* was the creation and release of
    a large-scale dataset for segmentation. It contains 11 million high-resolution
    and licensed images with roughly 1.1 billion masks. While the original version
    of the dataset have 3300x4950 pixels on average, the released version is downsampled
    to have 1500 pixels at the shortest edge. It is diverse in terms of different
    scenes and number of masks per image ranging from less than 50 to more than 500.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*Segment Anything* çš„ç¬¬äºŒä¸ªé‡å¤§æˆæœæ˜¯åˆ›å»ºå’Œå‘å¸ƒäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åˆ†å‰²æ•°æ®é›†ã€‚å®ƒåŒ…å« 1100 ä¸‡å¼ é«˜åˆ†è¾¨ç‡å’Œè®¸å¯çš„å›¾åƒï¼Œå¤§çº¦æœ‰ 11
    äº¿ä¸ªæ©ç ã€‚è™½ç„¶æ•°æ®é›†çš„åŸå§‹ç‰ˆæœ¬å¹³å‡æœ‰ 3300x4950 åƒç´ ï¼Œä½†å‘å¸ƒç‰ˆæœ¬ç»è¿‡ä¸‹é‡‡æ ·ï¼Œä½¿æœ€çŸ­è¾¹ä¸º 1500 åƒç´ ã€‚å®ƒåœ¨ä¸åŒåœºæ™¯å’Œæ¯å¼ å›¾åƒæ©ç æ•°é‡ä¸Šéƒ½å…·æœ‰å¤šæ ·æ€§ï¼ŒèŒƒå›´ä»ä¸åˆ°
    50 ä¸ªåˆ°è¶…è¿‡ 500 ä¸ªã€‚'
- en: '![](../Images/07221a70160f677f1590ec2479714fae.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07221a70160f677f1590ec2479714fae.png)'
- en: 'Fig.5: Different masks from SA-1B. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5ï¼šæ¥è‡ª SA-1B çš„ä¸åŒæ©ç ã€‚ [å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2304.02643) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    çš„æ³¨é‡Š
- en: The dataset has been created in a three-stage data engine which combines manual
    labels annotated by humans with automatic labels generated by SAM.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ•°æ®é›†æ˜¯åœ¨ä¸€ä¸ªä¸‰é˜¶æ®µæ•°æ®å¼•æ“ä¸­åˆ›å»ºçš„ï¼Œè¯¥å¼•æ“ç»“åˆäº†äººå·¥æ ‡æ³¨å’Œ SAM ç”Ÿæˆçš„è‡ªåŠ¨æ ‡æ³¨ã€‚
- en: '**Stage 1: Assisted-manual Stage â€”** A team of professional labelers labeled
    images assisted by an early version of SAM trained on common segmentation datasets.
    They were asked to label the most prominence objects and were encouraged to proceed
    after 30 seconds. At the end of this stage, SAM was retrained with the new labels
    (total 120k images with 4.3M masks).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**é˜¶æ®µ 1ï¼šè¾…åŠ©æ‰‹åŠ¨é˜¶æ®µ** â€” ä¸€ç»„ä¸“ä¸šæ ‡æ³¨å‘˜åœ¨ SAM çš„æ—©æœŸç‰ˆæœ¬çš„å¸®åŠ©ä¸‹å¯¹å›¾åƒè¿›è¡Œäº†æ ‡æ³¨ï¼ŒSAM åœ¨å¸¸è§çš„åˆ†å‰²æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚ä»–ä»¬è¢«è¦æ±‚æ ‡æ³¨æœ€æ˜¾è‘—çš„å¯¹è±¡ï¼Œå¹¶è¢«é¼“åŠ±åœ¨
    30 ç§’åç»§ç»­ã€‚åœ¨æ­¤é˜¶æ®µç»“æŸæ—¶ï¼ŒSAM é€šè¿‡æ–°çš„æ ‡ç­¾è¿›è¡Œé‡æ–°è®­ç»ƒï¼ˆæ€»è®¡ 12 ä¸‡å¼ å›¾åƒå’Œ 430 ä¸‡ä¸ªæ©ç ï¼‰ã€‚'
- en: '**Stage 2: Semi-automatic Stage** â€” In this stage the goal was to increase
    the diversity of the masks by first letting SAM predict some masks and let the
    labelers annotate the missing less prominence objects. At the end of this stage,
    SAM was retrained again including the new samples (total 300k images with 10.2M
    masks).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**é˜¶æ®µ 2ï¼šåŠè‡ªåŠ¨é˜¶æ®µ** â€” åœ¨è¿™ä¸€é˜¶æ®µçš„ç›®æ ‡æ˜¯é€šè¿‡é¦–å…ˆè®©SAMé¢„æµ‹ä¸€äº›æ©ç ï¼Œç„¶åè®©æ ‡æ³¨å‘˜æ ‡æ³¨ç¼ºå°‘çš„ã€ä¸å¤ªæ˜¾è‘—çš„å¯¹è±¡ï¼Œä»¥å¢åŠ æ©ç çš„å¤šæ ·æ€§ã€‚åœ¨æ­¤é˜¶æ®µç»“æŸæ—¶ï¼ŒSAMå†æ¬¡è¿›è¡Œé‡æ–°è®­ç»ƒï¼ŒåŒ…æ‹¬æ–°çš„æ ·æœ¬ï¼ˆæ€»è®¡
    30 ä¸‡å¼ å›¾åƒå’Œ 1020 ä¸‡ä¸ªæ©ç ï¼‰ã€‚'
- en: '**Stage 3: Fully automatic Stage** â€” In this stage, annotation was fully automatic.
    SAM was prompted with a 32x32 grid of points to generate masks and applied some
    post-processing.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**é˜¶æ®µ 3ï¼šå®Œå…¨è‡ªåŠ¨é˜¶æ®µ** â€” åœ¨è¿™ä¸€é˜¶æ®µï¼Œæ³¨é‡Šå®Œå…¨è‡ªåŠ¨åŒ–ã€‚SAM é€šè¿‡ 32x32 çš„ç½‘æ ¼ç‚¹ç”Ÿæˆæ©ç ï¼Œå¹¶åº”ç”¨ä¸€äº›åå¤„ç†ã€‚'
- en: Dataset Analysis
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®é›†åˆ†æ
- en: Now letâ€™s have a closer look on some analysis concerning the SA-1B dataset presented
    in the paper.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬ä»”ç»†çœ‹ä¸€ä¸‹è®ºæ–‡ä¸­å…³äº SA-1B æ•°æ®é›†çš„ä¸€äº›åˆ†æã€‚
- en: In a first evaluation, the authors created a normalized distribution of the
    masksâ€™ center point. Interestingly, these distributions are subject to a photographerâ€™s
    bias, meaning, most photos center the object of interest in the images center
    and the main axis.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ä¸€æ¬¡è¯„ä¼°ä¸­ï¼Œä½œè€…åˆ›å»ºäº†æ©ç ä¸­å¿ƒç‚¹çš„æ ‡å‡†åŒ–åˆ†å¸ƒã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¿™äº›åˆ†å¸ƒä¼šå—åˆ°æ‘„å½±å¸ˆçš„åå·®ï¼Œå³å¤§å¤šæ•°ç…§ç‰‡å°†æ„Ÿå…´è¶£çš„å¯¹è±¡ç½®äºå›¾åƒçš„ä¸­å¿ƒå’Œä¸»è½´ä¸Šã€‚
- en: '![](../Images/416ddd1903ba104bce16bd225eb01e48.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/416ddd1903ba104bce16bd225eb01e48.png)'
- en: 'Fig.6: Distribution of the objectsâ€™ center point location on the image. [Image
    Source](https://arxiv.org/abs/2304.02643) + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 6ï¼šå›¾åƒä¸­å¯¹è±¡ä¸­å¿ƒç‚¹ä½ç½®çš„åˆ†å¸ƒã€‚ [å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2304.02643) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    çš„æ³¨é‡Š
- en: One of SA-1Bâ€™s major strengths is the high number of masks per image compared
    to other datasets (Fig.7 left). This also implies that SA-1B has many small masks
    (Fig.7 center). Comparing the masksâ€™ concavity. which is a measure of complexity,
    SA-1B is very similar to other datasets that have been manually labeled (Fig.7
    right).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: SA-1B çš„ä¸€ä¸ªä¸»è¦ä¼˜ç‚¹æ˜¯æ¯å¼ å›¾åƒçš„æ©ç æ•°é‡ç›¸æ¯”å…¶ä»–æ•°æ®é›†æ›´é«˜ï¼ˆè§å›¾ 7 å·¦ï¼‰ã€‚è¿™ä¹Ÿæ„å‘³ç€ SA-1B æœ‰è®¸å¤šå°æ©ç ï¼ˆè§å›¾ 7 ä¸­ï¼‰ã€‚æ¯”è¾ƒæ©ç çš„å‡¹å‡¸åº¦ï¼Œä½œä¸ºå¤æ‚æ€§çš„è¡¡é‡æ ‡å‡†ï¼ŒSA-1B
    ä¸å…¶ä»–æ‰‹åŠ¨æ ‡æ³¨çš„æ•°æ®é›†éå¸¸ç›¸ä¼¼ï¼ˆè§å›¾ 7 å³ï¼‰ã€‚
- en: '![](../Images/a12065841d0f5361c0dac6bdb32ba1ce.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a12065841d0f5361c0dac6bdb32ba1ce.png)'
- en: 'Fig.7: Mask properties of SA-1B compared to other datasets. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾7ï¼šSA-1B çš„æ©ç å±æ€§ä¸å…¶ä»–æ•°æ®é›†çš„æ¯”è¾ƒã€‚ [å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2304.02643) + [Sascha
    Kirch](https://medium.com/@SaschaKirch) æ³¨é‡Š
- en: A high focus is put on responsible AI (RAI), where biases towards certain groups
    of people is not only analyzed but also tried to be mitigated. As Fig.8 shows,
    most of the worldâ€™s countries have more than 1000 images and the top 3 countries
    are from different parts of the world. While low-income countries are still under
    represented relatively speaking (0.9% of all samples), on an absolute scale these
    are still over 9M masks and more than other segmentation datasets.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜åº¦å…³æ³¨è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½ï¼ˆRAIï¼‰ï¼Œåœ¨è¿™é‡Œï¼Œä¸ä»…åˆ†æå¯¹æŸäº›äººç¾¤çš„åè§ï¼Œè¿˜å°è¯•å‡è½»è¿™äº›åè§ã€‚å¦‚å›¾8æ‰€ç¤ºï¼Œä¸–ç•Œä¸Šå¤§å¤šæ•°å›½å®¶çš„å›¾åƒæ•°é‡è¶…è¿‡1000å¼ ï¼Œå‰3åå›½å®¶æ¥è‡ªä¸åŒåœ°åŒºã€‚è™½ç„¶ä½æ”¶å…¥å›½å®¶çš„æ ·æœ¬ç›¸å¯¹è¾ƒå°‘ï¼ˆå æ‰€æœ‰æ ·æœ¬çš„0.9%ï¼‰ï¼Œä½†ç»å¯¹æ•°é‡ä»è¶…è¿‡900ä¸‡å¼ ï¼Œæ¯”å…¶ä»–åˆ†å‰²æ•°æ®é›†æ›´å¤šã€‚
- en: '![](../Images/2be5329ed6df5f35d27532e88b3df605.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2be5329ed6df5f35d27532e88b3df605.png)'
- en: 'Fig.8: Estimated geographic distribution of SA-1B images. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8ï¼šSA-1B å›¾åƒçš„ä¼°è®¡åœ°ç†åˆ†å¸ƒã€‚ [å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2304.02643) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    æ³¨é‡Š
- en: The authors further investigated the performance discrepancy between perceived
    gender presentation, perceived age group and perceived skin tone. They provided
    the mean IoU (Intersection over Union), between the predicted masks and the ground
    truth masks and a 95% confidence interval. SAM is prompted with either a single
    point or three points. The key message is, that results are very similar (and
    overlapping confidence intervals) within a group which shows that no member of
    the group is favored. The only exception are older people in the perceived age
    group.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…è¿›ä¸€æ­¥ç ”ç©¶äº†æ„ŸçŸ¥æ€§åˆ«å±•ç¤ºã€æ„ŸçŸ¥å¹´é¾„ç»„å’Œæ„ŸçŸ¥è‚¤è‰²ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ã€‚ä»–ä»¬æä¾›äº†é¢„æµ‹æ©ç ä¸çœŸå®æ©ç ä¹‹é—´çš„å¹³å‡IoUï¼ˆäº¤å¹¶æ¯”ï¼‰ä»¥åŠ95%çš„ç½®ä¿¡åŒºé—´ã€‚SAMçš„æç¤ºå¯ä»¥æ˜¯å•ä¸ªç‚¹æˆ–ä¸‰ä¸ªç‚¹ã€‚ä¸»è¦ä¿¡æ¯æ˜¯ï¼Œåœ¨ä¸€ä¸ªç»„å†…ï¼Œç»“æœéå¸¸ç›¸ä¼¼ï¼ˆä¸”ç½®ä¿¡åŒºé—´é‡å ï¼‰ï¼Œè¿™è¡¨æ˜è¯¥ç»„çš„ä»»ä½•æˆå‘˜éƒ½æ²¡æœ‰è¢«åè¢’ã€‚å”¯ä¸€çš„ä¾‹å¤–æ˜¯æ„ŸçŸ¥å¹´é¾„ç»„ä¸­çš„è€å¹´äººã€‚
- en: '![](../Images/9fcf400cf4b61a13baed1ededa12244e.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9fcf400cf4b61a13baed1ededa12244e.png)'
- en: 'Fig.9: SAMâ€™s performance segmenting people across perceived gender presentation,
    age group, and skin tone. [Image Source](https://arxiv.org/abs/2304.02643) + Annotations
    by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9ï¼šSAM åœ¨æ„ŸçŸ¥æ€§åˆ«å±•ç¤ºã€å¹´é¾„ç»„å’Œè‚¤è‰²æ–¹é¢çš„åˆ†å‰²æ€§èƒ½ã€‚ [å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2304.02643) + [Sascha
    Kirch](https://medium.com/@SaschaKirch) æ³¨é‡Š
- en: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----f28958c5612d--------------------------------)
    [## Get an email whenever Sascha Kirch publishes ğŸš€'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[## æ¯å½“ Sascha Kirch å‘å¸ƒæ–°å†…å®¹æ—¶è·å–ç”µå­é‚®ä»¶ ğŸš€](https://medium.com/@SaschaKirch/subscribe?source=post_page-----f28958c5612d--------------------------------)'
- en: Get an email whenever Sascha Kirch publishes ğŸš€ Looking to learn more about deep
    learning or simply stay up to dateâ€¦
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[æ¯å½“ Sascha Kirch å‘å¸ƒæ–°å†…å®¹æ—¶è·å–ç”µå­é‚®ä»¶ ğŸš€ æƒ³äº†è§£æ›´å¤šå…³äºæ·±åº¦å­¦ä¹ çš„çŸ¥è¯†æˆ–åªæ˜¯ä¿æŒæ›´æ–°â€¦](https://medium.com/@SaschaKirch/subscribe?source=post_page-----f28958c5612d--------------------------------)'
- en: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----f28958c5612d--------------------------------)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----f28958c5612d--------------------------------)'
- en: Experiments and Ablations
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®éªŒå’Œæ¶ˆèç ”ç©¶
- en: 'Segment Anything did provide us with a bunch of experiments mainly focused
    on its zero-shot performance, since this was the main target of the authors: to
    find a promptable zero-shot segmentation model. Also we know from other models
    such as [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    and [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05),
    that prompt tuning is nearly as effective as fine-tuning a model in terms of performance.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything ç¡®å®ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç³»åˆ—å®éªŒï¼Œä¸»è¦é›†ä¸­åœ¨å…¶é›¶-shot æ€§èƒ½ä¸Šï¼Œå› ä¸ºè¿™æ˜¯ä½œè€…çš„ä¸»è¦ç›®æ ‡ï¼šæ‰¾åˆ°ä¸€ä¸ªå¯æç¤ºçš„é›¶-shot åˆ†å‰²æ¨¡å‹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¹ŸçŸ¥é“å…¶ä»–æ¨¡å‹å¦‚
    [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    å’Œ [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)
    çš„è¡¨ç°ï¼Œæç¤ºè°ƒæ•´å‡ ä¹ä¸æ¨¡å‹å¾®è°ƒåœ¨æ€§èƒ½ä¸Šç­‰æ•ˆã€‚
- en: To perform the experiments, a suite of 23 diverse datasets was compiled. It
    contains samples from a wide variety of data distributions as shown in Fig. 10.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¿›è¡Œå®éªŒï¼Œç¼–åˆ¶äº†ä¸€å¥—åŒ…å«23ä¸ªå¤šæ ·åŒ–æ•°æ®é›†çš„é›†åˆã€‚å®ƒåŒ…å«äº†æ¥è‡ªå„ç§æ•°æ®åˆ†å¸ƒçš„æ ·æœ¬ï¼Œå¦‚å›¾10æ‰€ç¤ºã€‚
- en: '![](../Images/abb58c4363f9c280c03085773ffcb60b.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/abb58c4363f9c280c03085773ffcb60b.png)'
- en: 'Fig.10: Samples from the suite of 23 datasets. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10ï¼šæ¥è‡ª23ä¸ªæ•°æ®é›†çš„æ ·æœ¬ã€‚[å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2304.02643) + æ³¨é‡Šç”± [Sascha Kirch](https://medium.com/@SaschaKirch)
- en: Zero-Shot Single Point Valid Mask Evaluation
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é›¶-Shotå•ç‚¹æœ‰æ•ˆæ©ç è¯„ä¼°
- en: Recall that zero-shot means the model was never trained on the data it is exposed
    to during the evaluation. Also recall that single point prompting is quite a difficult
    task due to its ambiguity as depicted in Fig.3.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è®°ä½ï¼Œé›¶-Shotæ„å‘³ç€æ¨¡å‹ä»æœªåœ¨è¯„ä¼°è¿‡ç¨‹ä¸­æ¥è§¦è¿‡çš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚åŒæ ·ï¼Œå•ç‚¹æç¤ºç”±äºå…¶æ¨¡ç³Šæ€§ï¼Œå¦‚å›¾3æ‰€ç¤ºï¼Œæ˜¯ä¸€é¡¹ç›¸å½“å›°éš¾çš„ä»»åŠ¡ã€‚
- en: In this first experiment, the authors compared SAM against [RITM](https://arxiv.org/abs/2102.06583),
    a strong interactive segmenter which they said performed best on their benchmarks.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç¬¬ä¸€æ¬¡å®éªŒä¸­ï¼Œä½œè€…å°†SAMä¸ [RITM](https://arxiv.org/abs/2102.06583)è¿›è¡Œäº†æ¯”è¾ƒï¼ŒRITMæ˜¯ä¸€ç§å¼ºå¤§çš„äº¤äº’å¼åˆ†å‰²å™¨ï¼Œä½œè€…è¡¨ç¤ºå…¶åœ¨ä»–ä»¬çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ã€‚
- en: Remember that SAM outputs 3 different masks with an associated score when prompted
    with a single point. In this experiment, the mask with the highest score is selected
    for the evaluation. Since this is sometimes wrong, the authors also evaluate on
    the best mask, which is determined by comparing the predictions to the ground
    truth masks and select those with the highest overlap. These are the â€œoracleâ€
    predictions.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: è®°ä½ï¼Œå½“ç”¨å•ä¸ªç‚¹è¿›è¡Œæç¤ºæ—¶ï¼ŒSAMä¼šè¾“å‡º3ä¸ªä¸åŒçš„æ©ç åŠå…¶ç›¸å…³åˆ†æ•°ã€‚åœ¨è¿™ä¸ªå®éªŒä¸­ï¼Œé€‰æ‹©åˆ†æ•°æœ€é«˜çš„æ©ç è¿›è¡Œè¯„ä¼°ã€‚ç”±äºè¿™ç§æƒ…å†µæœ‰æ—¶ä¼šå‡ºç°é”™è¯¯ï¼Œä½œè€…è¿˜å¯¹æœ€ä½³æ©ç è¿›è¡Œäº†è¯„ä¼°ï¼Œé€šè¿‡å°†é¢„æµ‹ç»“æœä¸çœŸå®æ©ç è¿›è¡Œæ¯”è¾ƒï¼Œé€‰æ‹©é‡å åº¦æœ€é«˜çš„æ©ç ã€‚è¿™äº›æ˜¯â€œoracleâ€é¢„æµ‹ã€‚
- en: SAM outperforms RITM in 16 of the 23 datasets in zero-shot single point valid
    mask prediction. When performing oracle predictions, it outperforms RITM in all
    23 datasets.
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ¨23ä¸ªæ•°æ®é›†ä¸­ï¼ŒSAMåœ¨16ä¸ªæ•°æ®é›†ä¸­ä¸­çš„é›¶-Shotå•ç‚¹æœ‰æ•ˆæ©ç é¢„æµ‹ä¸­è¡¨ç°ä¼˜äºRITMã€‚åœ¨è¿›è¡Œoracleé¢„æµ‹æ—¶ï¼Œå®ƒåœ¨æ‰€æœ‰23ä¸ªæ•°æ®é›†ä¸­éƒ½ä¼˜äºRITMã€‚
- en: '![](../Images/88f2550688b089b34fc4d3c34c770708.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88f2550688b089b34fc4d3c34c770708.png)'
- en: 'Fig.11: SAM vs. RITM on 23 datasets. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾11ï¼šåœ¨23ä¸ªæ•°æ®é›†ä¸Šçš„SAMä¸RITMå¯¹æ¯”ã€‚[å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2304.02643) + æ³¨é‡Šç”± [Sascha
    Kirch](https://medium.com/@SaschaKirch)
- en: Zero-Shot Text-to-Mask
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é›¶-Shotæ–‡æœ¬åˆ°æ©ç 
- en: In this experiment SAM was prompted with text. The authors refer to this feature
    as a proof of concept and hence neither perform extensive experiments nor release
    this feature in their official code implementation.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªå®éªŒä¸­ï¼ŒSAMé€šè¿‡æ–‡æœ¬è¿›è¡Œæç¤ºã€‚ä½œè€…å°†æ­¤åŠŸèƒ½ç§°ä¸ºæ¦‚å¿µéªŒè¯ï¼Œå› æ­¤æ—¢æ²¡æœ‰è¿›è¡Œå¹¿æ³›çš„å®éªŒï¼Œä¹Ÿæ²¡æœ‰åœ¨å…¶å®˜æ–¹ä»£ç å®ç°ä¸­å‘å¸ƒæ­¤åŠŸèƒ½ã€‚
- en: Looking at Fig.12, you can see that SAM is able to return correct masks even
    for complex objects like the â€œbeaver tooth grilleâ€. In some other cases, the model
    fails with inserting only text prompts and they show that when providing context
    in form of a point, SAM is able correctly predict either a single or multiple
    wipers, showing that not only the point is considered for prediction but also
    the text.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹å›¾12ï¼Œä½ å¯ä»¥çœ‹åˆ°SAMèƒ½å¤Ÿä¸ºåƒâ€œæµ·ç‹¸ç‰™é½¿æ ¼æ …â€è¿™æ ·çš„å¤æ‚å¯¹è±¡è¿”å›æ­£ç¡®çš„æ©ç ã€‚åœ¨å…¶ä»–ä¸€äº›æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä»…é€šè¿‡æ–‡æœ¬æç¤ºå¤±è´¥ï¼Œä»–ä»¬å±•ç¤ºäº†åœ¨æä¾›ç‚¹çš„ä¸Šä¸‹æ–‡æ—¶ï¼ŒSAMèƒ½å¤Ÿæ­£ç¡®é¢„æµ‹å•ä¸ªæˆ–å¤šä¸ªæ“¦æ‹­å™¨ï¼Œæ˜¾ç¤ºå‡ºä¸ä»…ç‚¹è¢«ç”¨äºé¢„æµ‹ï¼Œæ–‡æœ¬ä¹Ÿè¢«è€ƒè™‘åœ¨å†…ã€‚
- en: '![](../Images/e3b30e8179c463ea42e0487641bcd45f.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3b30e8179c463ea42e0487641bcd45f.png)'
- en: 'Fig.12: Zero-shot text to mask. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾12ï¼šé›¶-shotæ–‡æœ¬åˆ°æ©ç ã€‚[å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2304.02643) + æ³¨é‡Šç”± [Sascha Kirch](https://medium.com/@SaschaKirch)
- en: Zero-Shot Edge Detection
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é›¶-Shotè¾¹ç¼˜æ£€æµ‹
- en: Interestingly, SAM can also be used for edge detection, a task it was not considered
    to do nor did it had access to such data during training.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è¶£çš„æ˜¯ï¼ŒSAMä¹Ÿå¯ä»¥ç”¨äºè¾¹ç¼˜æ£€æµ‹ï¼Œè¿™æ˜¯ä¸€é¡¹å®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœªè¢«è€ƒè™‘çš„ä»»åŠ¡ï¼Œä¹Ÿæ²¡æœ‰è®¿é—®ç›¸å…³æ•°æ®ã€‚
- en: To predict maps, SAM is first prompted with a grid of 16x16 points resulting
    in 768 predicted masks (object, part and sub-part for each of the 256 points).
    The resulting masks are then filtered and post-processed to obtain the edge masks.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†é¢„æµ‹å›¾åƒï¼ŒSAMé¦–å…ˆä½¿ç”¨16x16ç‚¹çš„ç½‘æ ¼è¿›è¡Œæç¤ºï¼Œç”Ÿæˆ768ä¸ªé¢„æµ‹çš„æ©ç ï¼ˆæ¯ä¸ª256ä¸ªç‚¹çš„å¯¹è±¡ã€éƒ¨åˆ†å’Œå­éƒ¨åˆ†ï¼‰ã€‚ç„¶åå¯¹ç”Ÿæˆçš„æ©ç è¿›è¡Œç­›é€‰å’Œåå¤„ç†ï¼Œä»¥è·å–è¾¹ç¼˜æ©ç ã€‚
- en: As shown in Fig. 13, compared to the ground truth, SAM predicts much more details.
    But to be fair, if the GT is not complete or covers a different layer of abstraction,
    this comparison seems not fair to me. But still, the performance is quite good!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å›¾13æ‰€ç¤ºï¼Œä¸çœŸå®æ•°æ®ç›¸æ¯”ï¼ŒSAMé¢„æµ‹äº†æ›´å¤šçš„ç»†èŠ‚ã€‚ä½†ä¸ºäº†å…¬å¹³èµ·è§ï¼Œå¦‚æœçœŸå®æ•°æ®ä¸å®Œæ•´æˆ–è¦†ç›–äº†ä¸åŒçš„æŠ½è±¡å±‚æ¬¡ï¼Œè¿™ç§æ¯”è¾ƒå¯¹æˆ‘æ¥è¯´ä¼¼ä¹ä¸å¤ªå…¬å¹³ã€‚ä½†æ€»çš„æ¥è¯´ï¼Œæ€§èƒ½è¿˜æ˜¯ç›¸å½“ä¸é”™çš„ï¼
- en: '![](../Images/9ae9bb4def9477efc2149ca5e3b975bf.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ae9bb4def9477efc2149ca5e3b975bf.png)'
- en: 'Fig.13: SAMâ€™s zero-shot edge prediction. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾13ï¼šSAM çš„é›¶æ ·æœ¬è¾¹ç¼˜é¢„æµ‹ã€‚ [å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2304.02643) + æ³¨é‡Šç”± [Sascha Kirch](https://medium.com/@SaschaKirch)
- en: Zero-Shot Instance Segmentation
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é›¶æ ·æœ¬å®ä¾‹åˆ†å‰²
- en: For this experiment, SAM is prompted with a bounding box output of a fully supervised
    ViTDet-H trained on [COCO](https://cocodataset.org/#home) and [LVIS](https://www.lvisdataset.org/).
    The resulting mask is then fed again into SAM together with the initial bounding
    box to refine the result. A comparison between ViTDet and SAM is shown in Fig.14.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªå®éªŒï¼ŒSAM ä»¥ [COCO](https://cocodataset.org/#home) å’Œ [LVIS](https://www.lvisdataset.org/)
    ä¸Šè®­ç»ƒçš„å®Œå…¨ç›‘ç£çš„ ViTDet-H çš„è¾¹ç•Œæ¡†è¾“å‡ºä½œä¸ºæç¤ºã€‚ç„¶åå°†ç”Ÿæˆçš„æ©ç è¿åŒåˆå§‹è¾¹ç•Œæ¡†ä¸€èµ·è¾“å…¥åˆ° SAM ä¸­ï¼Œä»¥ç²¾ç»†åŒ–ç»“æœã€‚å›¾14 æ˜¾ç¤ºäº† ViTDet
    å’Œ SAM çš„æ¯”è¾ƒã€‚
- en: '![](../Images/3d740a4bf3e6040227b562f9d86727be.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d740a4bf3e6040227b562f9d86727be.png)'
- en: 'Fig.14: Zero-shot instance segmentation on LVIS v1\. SAM. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾14ï¼šåœ¨ LVIS v1 ä¸Šçš„é›¶æ ·æœ¬å®ä¾‹åˆ†å‰²ã€‚ [å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2304.02643) + æ³¨é‡Šç”± [Sascha
    Kirch](https://medium.com/@SaschaKirch)
- en: 'Two things to note here: If you have a look into [COCO](https://cocodataset.org/#home)
    and [LVIS](https://www.lvisdataset.org/), you will find that the masks are not
    pixel-aligned with the objects. This bias is present in ViTDet, so thatâ€™s why
    the quality of SAM seems to be better. How much better is hard to tell with computed
    metrics, since the ground truth has the same bias and compared to a bad GT, SAM
    would perform worse. Hence, they asked humans to visually inspect those. Second,
    why does this elephant only has 3 legs ğŸ˜…. No matter how hard I try I canâ€™t see
    the fourth oneâ€¦'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸¤ä»¶äº‹éœ€è¦æ³¨æ„ï¼šå¦‚æœä½ æŸ¥çœ‹ [COCO](https://cocodataset.org/#home) å’Œ [LVIS](https://www.lvisdataset.org/)ï¼Œä½ ä¼šå‘ç°æ©ç ä¸å¯¹è±¡çš„åƒç´ å¯¹é½å¹¶ä¸å®Œå…¨ã€‚è¿™ç§åå·®åœ¨
    ViTDet ä¸­ä¹Ÿå­˜åœ¨ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆ SAM çš„è´¨é‡ä¼¼ä¹æ›´å¥½çš„åŸå› ã€‚ç”±äºåŸºå‡†çœŸå®å€¼å…·æœ‰ç›¸åŒçš„åå·®ï¼Œè€Œä¸å·®çš„ GT ç›¸æ¯”ï¼ŒSAM çš„è¡¨ç°å¯èƒ½æ›´å·®ã€‚å› æ­¤ï¼Œä»–ä»¬è¦æ±‚äººå·¥è¿›è¡Œè§†è§‰æ£€æŸ¥ã€‚å…¶æ¬¡ï¼Œä¸ºä»€ä¹ˆè¿™åªå¤§è±¡åªæœ‰
    3 æ¡è…¿ ğŸ˜…ã€‚æ— è®ºæˆ‘æ€ä¹ˆåŠªåŠ›ï¼Œæˆ‘éƒ½çœ‹ä¸åˆ°ç¬¬å››æ¡è…¿â€¦
- en: Ablations
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¶ˆèå®éªŒ
- en: In the ablation section the authors were mainly concerned with scaling either
    the dataset, the number of points for prompting and the size of the image encoder
    (see Fig.13). Performance is reported in mean IoU.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¶ˆèå®éªŒéƒ¨åˆ†ï¼Œä½œè€…ä¸»è¦å…³æ³¨äºæ‰©å±•æ•°æ®é›†ã€æç¤ºç‚¹æ•°é‡å’Œå›¾åƒç¼–ç å™¨çš„å¤§å°ï¼ˆè§å›¾13ï¼‰ã€‚æ€§èƒ½ä»¥å¹³å‡ IoU æŠ¥å‘Šã€‚
- en: '![](../Images/96239e65aa812035d0b012ce9fb13b38.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96239e65aa812035d0b012ce9fb13b38.png)'
- en: 'Fig.15: Ablation studies. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾15ï¼šæ¶ˆèç ”ç©¶ã€‚ [å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2304.02643) + æ³¨é‡Šç”± [Sascha Kirch](https://medium.com/@SaschaKirch)
- en: Interestingly, even though scaling data and scaling model size influences the
    mIoU performance, it saturates. This might either indicate that the model is so
    good there is not much room for improvement or probably itâ€™s the limitation of
    their approach.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è¶£çš„æ˜¯ï¼Œå°½ç®¡æ•°æ®æ‰©å±•å’Œæ¨¡å‹è§„æ¨¡æ‰©å±•å½±å“äº† mIoU æ€§èƒ½ï¼Œä½†å®ƒè¾¾åˆ°é¥±å’ŒçŠ¶æ€ã€‚è¿™å¯èƒ½è¡¨æ˜æ¨¡å‹å·²ç»è¶³å¤Ÿå¥½ï¼Œæ²¡æœ‰å¤ªå¤šæ”¹è¿›çš„ç©ºé—´ï¼Œæˆ–è€…å¯èƒ½æ˜¯ä»–ä»¬æ–¹æ³•çš„å±€é™æ€§ã€‚
- en: Conclusion
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Segment Anything introduced the promptable Segment Anything Model (SAM) as well
    as a large-scale dataset for segmentation containing over 1 billion masks in over
    11 million images. Being able to prompt a segmentation model brings a lot of flexibility
    like adapting a trained model to unseen tasks or to be able to detect unknown
    classes. While some debate if SAM is considered a foundation model since it has
    ben trained in a supervised manner, it still has shown remarkable results and
    has been widely adopted.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything å¼•å…¥äº†å¯æç¤ºçš„ Segment Anything Model (SAM) ä»¥åŠä¸€ä¸ªåŒ…å«è¶…è¿‡ 10 äº¿ä¸ªæ©ç çš„åˆ†å‰²å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ¶µç›–è¶…è¿‡
    1100 ä¸‡å¼ å›¾åƒã€‚èƒ½å¤Ÿæç¤ºåˆ†å‰²æ¨¡å‹å¸¦æ¥äº†å¾ˆå¤šçµæ´»æ€§ï¼Œæ¯”å¦‚å°†è®­ç»ƒå¥½çš„æ¨¡å‹é€‚åº”äºæœªè§è¿‡çš„ä»»åŠ¡æˆ–æ£€æµ‹æœªçŸ¥ç±»åˆ«ã€‚è™½ç„¶æœ‰äº›äººè®¨è®º SAM æ˜¯å¦å¯ä»¥è¢«è§†ä¸ºåŸºç¡€æ¨¡å‹ï¼Œå› ä¸ºå®ƒæ˜¯ä»¥ç›‘ç£æ–¹å¼è®­ç»ƒçš„ï¼Œä½†å®ƒä»ç„¶æ˜¾ç¤ºå‡ºäº†æ˜¾è‘—çš„æˆæœï¼Œå¹¶å·²è¢«å¹¿æ³›é‡‡ç”¨ã€‚
- en: Further Readings & Resources
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»ä¸èµ„æº
- en: 'As you probably know yourself: the field of deep learning is evolving in an
    unbelievably fast pace. Hence it is no surprise that right after the release of
    SAM, many new projects build upon its success, further improving either the quality
    of predictions, decreasing inference time or make the model suitable for on the
    edge applications.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ è‡ªå·±å¯èƒ½çŸ¥é“çš„é‚£æ ·ï¼šæ·±åº¦å­¦ä¹ é¢†åŸŸæ­£åœ¨ä»¥ä»¤äººéš¾ä»¥ç½®ä¿¡çš„é€Ÿåº¦å‘å±•ã€‚å› æ­¤ï¼ŒSAM å‘å¸ƒåï¼Œè®¸å¤šæ–°é¡¹ç›®åœ¨å…¶æˆåŠŸçš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥æ”¹è¿›äº†é¢„æµ‹è´¨é‡ã€å‡å°‘äº†æ¨ç†æ—¶é—´ï¼Œæˆ–è€…ä½¿æ¨¡å‹é€‚ç”¨äºè¾¹ç¼˜åº”ç”¨ï¼Œè¿™ä¹Ÿå°±ä¸è¶³ä¸ºå¥‡äº†ã€‚
- en: 'Following a list of interesting resources building upon SAM:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸€äº›æœ‰è¶£çš„èµ„æºï¼Œå®ƒä»¬åœ¨ SAM çš„åŸºç¡€ä¸Šè¿›è¡Œæ‰©å±•ï¼š
- en: '[Grounded Segment Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything)'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[åŸºç¡€åˆ†å‰²ä»»ä½•å†…å®¹](https://github.com/IDEA-Research/Grounded-Segment-Anything)'
- en: '[Segment Anything in High Quality](https://arxiv.org/abs/2306.01567)'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[é«˜è´¨é‡åˆ†å‰²ä»»ä½•å†…å®¹](https://arxiv.org/abs/2306.01567)'
- en: '[Fast Segment Anything](https://arxiv.org/abs/2306.12156)'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[å¿«é€Ÿåˆ†å‰²ä»»ä½•å†…å®¹](https://arxiv.org/abs/2306.12156)'
- en: '[Faster Segment Anything: Towards Lightweight SAM for Mobile Applications](https://arxiv.org/abs/2306.14289)'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[æ›´å¿«çš„åˆ†å‰²ä»»ä½•å†…å®¹ï¼šæœç€é€‚ç”¨äºç§»åŠ¨åº”ç”¨çš„è½»é‡çº§ SAM](https://arxiv.org/abs/2306.14289)'
- en: 'Here I share some links if you want to have a hands-on experience with SAM
    and SA-1B:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘åˆ†äº«ä¸€äº›é“¾æ¥ï¼Œå¦‚æœä½ æƒ³äº²è‡ªä½“éªŒ SAM å’Œ SA-1Bï¼š
- en: '[SA-1B dataset download](https://ai.meta.com/datasets/segment-anything-downloads/)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SA-1B æ•°æ®é›†ä¸‹è½½](https://ai.meta.com/datasets/segment-anything-downloads/)'
- en: '[Segment Anything Demo](https://segment-anything.com/demo)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åˆ†å‰²ä»»ä½•å†…å®¹æ¼”ç¤º](https://segment-anything.com/demo)'
- en: '[Segment Anything GitHub](https://github.com/facebookresearch/segment-anything)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åˆ†å‰²ä»»ä½•å†…å®¹ GitHub](https://github.com/facebookresearch/segment-anything)'
- en: '[Python Notebook to experiment with SAM](https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python ç¬”è®°æœ¬ç”¨äºå®éªŒ SAM](https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb)'
- en: 'Here are some of my articles that walk you through some related foundation
    models:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯æˆ‘ä¸€äº›æ–‡ç« çš„é“¾æ¥ï¼Œå¸¦ä½ äº†è§£ä¸€äº›ç›¸å…³çš„åŸºç¡€æ¨¡å‹ï¼š
- en: '[](/the-clip-foundation-model-7770858b487d?source=post_page-----f28958c5612d--------------------------------)
    [## The CLIP Foundation Model'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/the-clip-foundation-model-7770858b487d?source=post_page-----f28958c5612d--------------------------------)
    [## CLIP åŸºç¡€æ¨¡å‹'
- en: Paper Summaryâ€” Learning Transferable Visual Models From Natural Language Supervision
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ€»ç»“â€” ä»è‡ªç„¶è¯­è¨€ç›‘ç£ä¸­å­¦ä¹ å¯è¿ç§»çš„è§†è§‰æ¨¡å‹
- en: 'towardsdatascience.com](/the-clip-foundation-model-7770858b487d?source=post_page-----f28958c5612d--------------------------------)
    [](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----f28958c5612d--------------------------------)
    [## GLIP: Introducing Language-Image Pre-Training to Object Detection'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 'towardsdatascience.com](/the-clip-foundation-model-7770858b487d?source=post_page-----f28958c5612d--------------------------------)
    [](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----f28958c5612d--------------------------------)
    [## GLIP: å°†è¯­è¨€-å›¾åƒé¢„è®­ç»ƒå¼•å…¥ç‰©ä½“æ£€æµ‹'
- en: 'Paper Summary: Grounded Language-Image Pre-training'
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ€»ç»“ï¼šåŸºç¡€è¯­è¨€-å›¾åƒé¢„è®­ç»ƒ
- en: towardsdatascience.com](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----f28958c5612d--------------------------------)
    [](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----f28958c5612d--------------------------------)
    [## BYOL -The Alternative to Contrastive Self-Supervised Learning
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----f28958c5612d--------------------------------)
    [](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----f28958c5612d--------------------------------)
    [## BYOL - å¯¹æ¯”è‡ªç›‘ç£å­¦ä¹ çš„æ›¿ä»£æ–¹æ¡ˆ
- en: 'Paper Analysisâ€” Bootstrap Your Own Latent: A New Approach to Self-Supervised
    Learning'
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'è®ºæ–‡åˆ†æâ€”Bootstrap Your Own Latent: è‡ªç›‘ç£å­¦ä¹ çš„æ–°æ–¹æ³•'
- en: towardsdatascience.com](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----f28958c5612d--------------------------------)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----f28958c5612d--------------------------------)
