- en: 'Segment Anything: Promptable Segmentation of Arbitrary Objects'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d](https://towardsdatascience.com/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[üöÄSascha‚Äôs Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Segment Anything by A. Krillov et. al.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f28958c5612d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f28958c5612d--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f28958c5612d--------------------------------)
    ¬∑12 min read¬∑Sep 14, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Today‚Äôs paper walkthrough it is going to be visual! We will analyze S*egment
    Anything*, a paper by Meta‚Äôs AI research team that made headlines not only in
    the research community but also by all sorts of deep learning practitioners and
    advocates.
  prefs: []
  type: TYPE_NORMAL
- en: Segment Anything introduces the task of promptable segmentation, it introduces
    the segment anything model (SAM), and it details the generation of a new publicly
    available dataset of 11 million images containing more than 1 billion masks. SAM
    has been widely adopted by the community and resulted in some new state-of-the-art
    foundation models such as [Grounded-SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything)
    that combines [Grounding DINO](https://arxiv.org/abs/2303.05499) with SAM.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3655fd5fedea80f78ac89299c8bd7b30.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created from [publication](https://arxiv.org/abs/2304.02643) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  prefs: []
  type: TYPE_NORMAL
- en: '**Paper:** [Segment Anything](https://arxiv.org/abs/2304.02643) by [Alexander
    Kirillov](https://arxiv.org/search/cs?searchtype=author&query=Kirillov%2C+A) et.
    al., 5 Apr. 2023'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/facebookresearch/segment-anything)
    ‚Äî [Demo](https://segment-anything.com/demo) ‚Äî [Project Page](https://segment-anything.com/)
    ‚Äî [Dataset](https://segment-anything.com/dataset/index.html) ‚Äî [HuggingFace](https://huggingface.co/docs/transformers/main/model_doc/sam)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** segmentation, zero-shot prediction, computer vison, prompting,
    large-scale'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    ‚Äî [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    ‚Äî [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    ‚Äî [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    ‚Äî [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    ‚Äî [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Outline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Context & Background
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SAM ‚Äî Segment Anything Model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SA-1B ‚Äî Dataset with 1 Billion Masks
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiments and Ablations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further Readings & Resources
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Context & Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The authors of Segment Anything made a clear statement: ‚Äú*[‚Ä¶] our goal is to
    build a foundation model for image segmentation.*‚Äù Foundation models originated
    from the great success of Natural Language Processing (NLP). Models have been
    trained on a laaaarge scale in a self-supervised fashion. These models usually
    perform very well at zero-shot tasks, meaning they can solve tasks different to
    those they were trained on and perform reasonable well or even better as their
    supervised competitors. In recent years, many researchers worked on bringing the
    success of NLP foundation models to other domains such as computer vision.'
  prefs: []
  type: TYPE_NORMAL
- en: Models such as [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    and [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)
    made it possible to condition an image classification or object detection task
    on text prompts, rather than a fixed set of classes. Other models, such as [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)
    or DINO, came up with different techniques to learn semantically rich representations
    of input images, which is one of the key requirements for many computer vision
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Segment Anything paper aims to:'
  prefs: []
  type: TYPE_NORMAL
- en: Enable zero-shot segmentation by prompting
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a large-scale model (SAM) as demonstrator
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect and release the largest publicly available dataset for segmentation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '***But why is zero-shot performance so important?*** ‚Äî The answer is two-fold.
    First, initially computer vision models have been trained in a supervised fashion
    requiring not only data, but also a lot of ground truth labels. Collection these
    data is extremely time-consuming and costly. Second, the classes a model can predict
    are limited to a fixed set of classes used for training. If you would like to
    add a new class to your model, you would need to first collect the data and retrain
    the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '***How is it possible to prompt a segmentation model?*** ‚Äî You might be familiar
    with text prompting from models like ChatGPT, [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    or [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05).
    While SAM in principle also was tested with text prompts, it is mainly prompted
    with either masks, points, boxes or point grids as shown in the image bellow.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/78dbec949be2553ac6a297eed84a8ee7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig.1: Different input prompts and resulting masks. Photo by [Terence Burke](https://unsplash.com/@ancientwanderer?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) + masks
    generated by [Sascha Kirch](https://medium.com/@SaschaKirch) with [SAM](https://segment-anything.com/demo)'
  prefs: []
  type: TYPE_NORMAL
- en: Having put SAM into context let‚Äôs now switch gears and have a closer look onto
    the Segment Anything Model, aka SAM.
  prefs: []
  type: TYPE_NORMAL
- en: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Paper Walkthroughs by Sascha Kirch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2?source=post_page-----f28958c5612d--------------------------------)7
    stories![‚ÄúDDPM‚Ää‚Äî‚ÄäDenoising Diffusion Probabilistic Models ‚Äú paper illustration
    by Sascha Kirch](../Images/6e785c0a911386676abebe0fa646f483.png)![‚ÄúDepth Anything‚Äù
    paper illustration by Sascha Kirch](../Images/bd8cd71a02e42cf64d0afd39f41f48e0.png)![](../Images/8708d91a4a1902cef889ced95d46fc39.png)'
  prefs: []
  type: TYPE_NORMAL
- en: SAM ‚Äî Segment Anything Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Segment Anything Model (SAM) is a multi-modal model that inputs an image
    and one or more prompts and outputs a valid segmentation mask. The model consists
    of three main modules: image encoder, prompt encoder and mask decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: SAM is prompted with either a mask, a set of points, a bounding box or text
    or any combination of those.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE: Even though the paper mentions and experiments with text as prompt, it
    is not yet released (as of September 2023) in the [official implementation](https://github.com/facebookresearch/segment-anything)
    nor in the [SAM demo](https://segment-anything.com/demo).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/61ae8b49ffb731555c8a774e41126b9c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig.2: SAM architecture. [Image Source](https://arxiv.org/abs/2304.02643) +
    Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image Encoder** ‚Äî Outputs an image embedding for a given input image. SAM
    implements and adapts a pre-trained ViT-H/16 masked auto-encoder. This is a relatively
    large model with strong performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt Encoder** ‚Äî Sparse prompts (i.e. points, boxes and text) are translated
    into embedding vectors. Text prompts are converted into text embeddings using
    [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    before feeding it into the prompt encoder. Dense prompts (i.e. masks) are simply
    downsampled with strided convolutions and added with the image embeddings. All
    embeddings are then fed into the final stage: the mask decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mask Decoder** ‚Äî Takes a set of image embeddings (optionally containing the
    dense mask embeddings) and a set of prompt embeddings and outputs a valid segmentation
    mask.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two more details we should address: ambiguities of prompts and performance.'
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, the less context a prompt contains, the more ambiguous it is
    and the more difficult it is for the model the provide the correct output. For
    text-prompts we have seen this connection between the specificness of the input
    text and the model‚Äôs performance in [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    and [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05).
    Similar, providing a single point as input might result in a variety of possible
    masks. For that reason, SAM outputs a set of three output masks corresponding
    to the object level, the part level and the sub-part level of a valid mask as
    indicated in the image bellow.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e2ca2d9f5176ee540cfdd88fc1d259ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig.3: Ambiguity for single-point prompts. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second detail I want to mention is performance in terms of inference speed.
    Did you notice that the image encoder is by far the largest sub-module in SAM?
    Well, that‚Äôs an unfair question because I did not tell you so far, but SAM is
    designed in a way to have semantically rich image embeddings (which often requires
    a large model) to subsequently act upon these embeddings applying a light-weight
    prompt encoder and a light-weight mask decoder. The good thing: one must only
    run the image encoder once per image and can then prompt the model multiple times
    using the same image embedding. This allows SAM to be executed in a browser taking
    only ~50ms to predict a mask for a given prompt (after the image embeddings have
    been calculated).'
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs have a closer look on the light-weight mask decoder. It inputs the image
    embeddings and prompt embeddings and outputs a set of masks with corresponding
    scores. Internally, two consecutive decoder blocks perform a combination of self-attention
    and cross-attention to generate a strong dependence between the image and the
    prompts. A simple up-sampling network in combination with another cross-attention
    block generates the masks and the scores.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4404071c556ef28c5df3a42f22b0ce7e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig.4: Detailed architecture of the mask decoder. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: SA-1B ‚Äî Dataset with 1 Billion Masks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The second great deal of *Segment Anything* was the creation and release of
    a large-scale dataset for segmentation. It contains 11 million high-resolution
    and licensed images with roughly 1.1 billion masks. While the original version
    of the dataset have 3300x4950 pixels on average, the released version is downsampled
    to have 1500 pixels at the shortest edge. It is diverse in terms of different
    scenes and number of masks per image ranging from less than 50 to more than 500.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07221a70160f677f1590ec2479714fae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig.5: Different masks from SA-1B. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset has been created in a three-stage data engine which combines manual
    labels annotated by humans with automatic labels generated by SAM.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stage 1: Assisted-manual Stage ‚Äî** A team of professional labelers labeled
    images assisted by an early version of SAM trained on common segmentation datasets.
    They were asked to label the most prominence objects and were encouraged to proceed
    after 30 seconds. At the end of this stage, SAM was retrained with the new labels
    (total 120k images with 4.3M masks).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stage 2: Semi-automatic Stage** ‚Äî In this stage the goal was to increase
    the diversity of the masks by first letting SAM predict some masks and let the
    labelers annotate the missing less prominence objects. At the end of this stage,
    SAM was retrained again including the new samples (total 300k images with 10.2M
    masks).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stage 3: Fully automatic Stage** ‚Äî In this stage, annotation was fully automatic.
    SAM was prompted with a 32x32 grid of points to generate masks and applied some
    post-processing.'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let‚Äôs have a closer look on some analysis concerning the SA-1B dataset presented
    in the paper.
  prefs: []
  type: TYPE_NORMAL
- en: In a first evaluation, the authors created a normalized distribution of the
    masks‚Äô center point. Interestingly, these distributions are subject to a photographer‚Äôs
    bias, meaning, most photos center the object of interest in the images center
    and the main axis.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/416ddd1903ba104bce16bd225eb01e48.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig.6: Distribution of the objects‚Äô center point location on the image. [Image
    Source](https://arxiv.org/abs/2304.02643) + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: One of SA-1B‚Äôs major strengths is the high number of masks per image compared
    to other datasets (Fig.7 left). This also implies that SA-1B has many small masks
    (Fig.7 center). Comparing the masks‚Äô concavity. which is a measure of complexity,
    SA-1B is very similar to other datasets that have been manually labeled (Fig.7
    right).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a12065841d0f5361c0dac6bdb32ba1ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig.7: Mask properties of SA-1B compared to other datasets. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: A high focus is put on responsible AI (RAI), where biases towards certain groups
    of people is not only analyzed but also tried to be mitigated. As Fig.8 shows,
    most of the world‚Äôs countries have more than 1000 images and the top 3 countries
    are from different parts of the world. While low-income countries are still under
    represented relatively speaking (0.9% of all samples), on an absolute scale these
    are still over 9M masks and more than other segmentation datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2be5329ed6df5f35d27532e88b3df605.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig.8: Estimated geographic distribution of SA-1B images. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: The authors further investigated the performance discrepancy between perceived
    gender presentation, perceived age group and perceived skin tone. They provided
    the mean IoU (Intersection over Union), between the predicted masks and the ground
    truth masks and a 95% confidence interval. SAM is prompted with either a single
    point or three points. The key message is, that results are very similar (and
    overlapping confidence intervals) within a group which shows that no member of
    the group is favored. The only exception are older people in the perceived age
    group.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9fcf400cf4b61a13baed1ededa12244e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig.9: SAM‚Äôs performance segmenting people across perceived gender presentation,
    age group, and skin tone. [Image Source](https://arxiv.org/abs/2304.02643) + Annotations
    by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----f28958c5612d--------------------------------)
    [## Get an email whenever Sascha Kirch publishes üöÄ'
  prefs: []
  type: TYPE_NORMAL
- en: Get an email whenever Sascha Kirch publishes üöÄ Looking to learn more about deep
    learning or simply stay up to date‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----f28958c5612d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Experiments and Ablations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Segment Anything did provide us with a bunch of experiments mainly focused
    on its zero-shot performance, since this was the main target of the authors: to
    find a promptable zero-shot segmentation model. Also we know from other models
    such as [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    and [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05),
    that prompt tuning is nearly as effective as fine-tuning a model in terms of performance.'
  prefs: []
  type: TYPE_NORMAL
- en: To perform the experiments, a suite of 23 diverse datasets was compiled. It
    contains samples from a wide variety of data distributions as shown in Fig. 10.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/abb58c4363f9c280c03085773ffcb60b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig.10: Samples from the suite of 23 datasets. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Shot Single Point Valid Mask Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall that zero-shot means the model was never trained on the data it is exposed
    to during the evaluation. Also recall that single point prompting is quite a difficult
    task due to its ambiguity as depicted in Fig.3.
  prefs: []
  type: TYPE_NORMAL
- en: In this first experiment, the authors compared SAM against [RITM](https://arxiv.org/abs/2102.06583),
    a strong interactive segmenter which they said performed best on their benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that SAM outputs 3 different masks with an associated score when prompted
    with a single point. In this experiment, the mask with the highest score is selected
    for the evaluation. Since this is sometimes wrong, the authors also evaluate on
    the best mask, which is determined by comparing the predictions to the ground
    truth masks and select those with the highest overlap. These are the ‚Äúoracle‚Äù
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: SAM outperforms RITM in 16 of the 23 datasets in zero-shot single point valid
    mask prediction. When performing oracle predictions, it outperforms RITM in all
    23 datasets.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/88f2550688b089b34fc4d3c34c770708.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig.11: SAM vs. RITM on 23 datasets. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Shot Text-to-Mask
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this experiment SAM was prompted with text. The authors refer to this feature
    as a proof of concept and hence neither perform extensive experiments nor release
    this feature in their official code implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at Fig.12, you can see that SAM is able to return correct masks even
    for complex objects like the ‚Äúbeaver tooth grille‚Äù. In some other cases, the model
    fails with inserting only text prompts and they show that when providing context
    in form of a point, SAM is able correctly predict either a single or multiple
    wipers, showing that not only the point is considered for prediction but also
    the text.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e3b30e8179c463ea42e0487641bcd45f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig.12: Zero-shot text to mask. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Shot Edge Detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Interestingly, SAM can also be used for edge detection, a task it was not considered
    to do nor did it had access to such data during training.
  prefs: []
  type: TYPE_NORMAL
- en: To predict maps, SAM is first prompted with a grid of 16x16 points resulting
    in 768 predicted masks (object, part and sub-part for each of the 256 points).
    The resulting masks are then filtered and post-processed to obtain the edge masks.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in Fig. 13, compared to the ground truth, SAM predicts much more details.
    But to be fair, if the GT is not complete or covers a different layer of abstraction,
    this comparison seems not fair to me. But still, the performance is quite good!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ae9bb4def9477efc2149ca5e3b975bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig.13: SAM‚Äôs zero-shot edge prediction. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Shot Instance Segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this experiment, SAM is prompted with a bounding box output of a fully supervised
    ViTDet-H trained on [COCO](https://cocodataset.org/#home) and [LVIS](https://www.lvisdataset.org/).
    The resulting mask is then fed again into SAM together with the initial bounding
    box to refine the result. A comparison between ViTDet and SAM is shown in Fig.14.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d740a4bf3e6040227b562f9d86727be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig.14: Zero-shot instance segmentation on LVIS v1\. SAM. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two things to note here: If you have a look into [COCO](https://cocodataset.org/#home)
    and [LVIS](https://www.lvisdataset.org/), you will find that the masks are not
    pixel-aligned with the objects. This bias is present in ViTDet, so that‚Äôs why
    the quality of SAM seems to be better. How much better is hard to tell with computed
    metrics, since the ground truth has the same bias and compared to a bad GT, SAM
    would perform worse. Hence, they asked humans to visually inspect those. Second,
    why does this elephant only has 3 legs üòÖ. No matter how hard I try I can‚Äôt see
    the fourth one‚Ä¶'
  prefs: []
  type: TYPE_NORMAL
- en: Ablations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the ablation section the authors were mainly concerned with scaling either
    the dataset, the number of points for prompting and the size of the image encoder
    (see Fig.13). Performance is reported in mean IoU.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96239e65aa812035d0b012ce9fb13b38.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig.15: Ablation studies. [Image Source](https://arxiv.org/abs/2304.02643)
    + Annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, even though scaling data and scaling model size influences the
    mIoU performance, it saturates. This might either indicate that the model is so
    good there is not much room for improvement or probably it‚Äôs the limitation of
    their approach.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Segment Anything introduced the promptable Segment Anything Model (SAM) as well
    as a large-scale dataset for segmentation containing over 1 billion masks in over
    11 million images. Being able to prompt a segmentation model brings a lot of flexibility
    like adapting a trained model to unseen tasks or to be able to detect unknown
    classes. While some debate if SAM is considered a foundation model since it has
    ben trained in a supervised manner, it still has shown remarkable results and
    has been widely adopted.
  prefs: []
  type: TYPE_NORMAL
- en: Further Readings & Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you probably know yourself: the field of deep learning is evolving in an
    unbelievably fast pace. Hence it is no surprise that right after the release of
    SAM, many new projects build upon its success, further improving either the quality
    of predictions, decreasing inference time or make the model suitable for on the
    edge applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following a list of interesting resources building upon SAM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Grounded Segment Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Segment Anything in High Quality](https://arxiv.org/abs/2306.01567)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Fast Segment Anything](https://arxiv.org/abs/2306.12156)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Faster Segment Anything: Towards Lightweight SAM for Mobile Applications](https://arxiv.org/abs/2306.14289)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here I share some links if you want to have a hands-on experience with SAM
    and SA-1B:'
  prefs: []
  type: TYPE_NORMAL
- en: '[SA-1B dataset download](https://ai.meta.com/datasets/segment-anything-downloads/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Segment Anything Demo](https://segment-anything.com/demo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Segment Anything GitHub](https://github.com/facebookresearch/segment-anything)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python Notebook to experiment with SAM](https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some of my articles that walk you through some related foundation
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/the-clip-foundation-model-7770858b487d?source=post_page-----f28958c5612d--------------------------------)
    [## The CLIP Foundation Model'
  prefs: []
  type: TYPE_NORMAL
- en: Paper Summary‚Äî Learning Transferable Visual Models From Natural Language Supervision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/the-clip-foundation-model-7770858b487d?source=post_page-----f28958c5612d--------------------------------)
    [](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----f28958c5612d--------------------------------)
    [## GLIP: Introducing Language-Image Pre-Training to Object Detection'
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper Summary: Grounded Language-Image Pre-training'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----f28958c5612d--------------------------------)
    [](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----f28958c5612d--------------------------------)
    [## BYOL -The Alternative to Contrastive Self-Supervised Learning
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper Analysis‚Äî Bootstrap Your Own Latent: A New Approach to Self-Supervised
    Learning'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----f28958c5612d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
