- en: How we think about Data Pipelines is changing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-we-think-about-data-pipelines-is-changing-51c3bf6f34dc](https://towardsdatascience.com/how-we-think-about-data-pipelines-is-changing-51c3bf6f34dc)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/7d79fb389159e7b29999cbc1ea81691a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Ali Kazal](https://unsplash.com/@lureofadventure?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/a-mountain-range-with-trees-in-the-foreground-and-a-field-in-the-foreground-walahB6h_sU?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to reliably and efficiently release data into production
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@hugolu87?source=post_page-----51c3bf6f34dc--------------------------------)[![Hugo
    Lu](../Images/045de11463bb16ea70a816ba89118a9e.png)](https://medium.com/@hugolu87?source=post_page-----51c3bf6f34dc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----51c3bf6f34dc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----51c3bf6f34dc--------------------------------)
    [Hugo Lu](https://medium.com/@hugolu87?source=post_page-----51c3bf6f34dc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----51c3bf6f34dc--------------------------------)
    ·6 min read·Nov 8, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Data Pipelines are series of tasks organised in a [directed acyclic graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph)
    or “DAG”. Historically, these are run on open-source workflow orchestration packages
    like [Airflow](https://airflow.apache.org/) or [Prefect](https://www.prefect.io/?gclid=Cj0KCQjwqP2pBhDMARIsAJQ0CzoV5DrzqjyDqDJonPcBPT5lE2ih47H2LMSKBst2jh6mR6h3azCcRnwaAhOJEALw_wcB),
    and require i[nfrastructure](https://www.bhavaniravi.com/apache-airflow/deploying-airflow-on-kubernetes)
    managed by data engineers or platform teams. These data pipelines typically run
    on a [schedule](https://airflow.apache.org/docs/apache-airflow/1.10.1/scheduler.html),
    and allow data engineers to update data in locations such as data warehouses or
    data lakes.
  prefs: []
  type: TYPE_NORMAL
- en: This is now changing. There is a [great shift in mentality](/what-data-engineers-can-learn-from-software-engineers-and-vice-versa-643cade3ef23)
    happening. As the data engineering industry matures, mindsets are shifting from
    a “move data to serve the business at all costs” mindset to “reliability and efficiency”
    / “software engineering” mindset.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Data Integration and Delivery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I’ve written before about how [Data Teams ship *data*](https://medium.com/orchestras-data-release-pipeline-blog/a-new-paradigm-for-data-continuous-data-integration-and-delivery-miniseries-part-5-a3338b3ffd03)
    whereas software teams ship *code.*
  prefs: []
  type: TYPE_NORMAL
- en: This is a process called “Continuous Data Integration and Delivery”, and is
    the process of reliably and efficiently releasing data into production. There
    are subtle differences with the definition of “[CI/CD](https://aws.amazon.com/solutions/app-development/ci-cd/#:~:text=An%20integral%20part%20of%20development,with%20collaborative%20and%20automated%20processes.)”
    as used in Software Engineer, illustrated below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5a9b3efdc44da8184627cefa728133c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image the author’s
  prefs: []
  type: TYPE_NORMAL
- en: In software engineering, Continuous Delivery is non-trivial because of the importance
    of having a [near exact replica](https://www.techtarget.com/searchsoftwarequality/definition/staging-environment#:~:text=A%20staging%20environment%20(stage)%20is,like%20environment%20before%20application%20deployment.)
    for code to operate in a staging environment.
  prefs: []
  type: TYPE_NORMAL
- en: Within Data Engineering, this is not necessary because the good we ship is *data*.
    If there is a table of data, and we *know* that as long as a few conditions are
    satisfied, the data *is* of a sufficient quality to be used, then that is sufficient
    for it to be “released” into production, so to speak.
  prefs: []
  type: TYPE_NORMAL
- en: The process of releasing data into production — the analog for Continuous Delivery
    — is very simple, as it simply relates to copying or [cloning](https://docs.snowflake.com/en/sql-reference/sql/create-clone)
    a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, a *key pillar* of data engineering is *reacting* to new data *as
    it arrives* or checking to see if new data exists. There is no analog for this
    in software engineering — software applications do not need to poll APIs for the
    existence of new code, whereas data applications do.
  prefs: []
  type: TYPE_NORMAL
- en: Given the analog of Continuous Delivery in data is so trivial, we can loosely
    define Continuous Data Integration as the process of reliably and efficiently
    releasing data into production in response to code changes. Code changes that
    govern the state of the data are “continuously integrated” via a process of cloning,
    materialising views, and running tests.
  prefs: []
  type: TYPE_NORMAL
- en: We can also loosely define Continuous Data Delivery as the process of reliably
    and efficiently releasing *new data into production*. This covers the invocations
    of pipelines or operations in response to the existence of new data.
  prefs: []
  type: TYPE_NORMAL
- en: Thinking about these two processes as the same type of operation but in a different
    context is a fairly radical departure from how most data teams think about data
    pipelines, or *data release pipelines*.
  prefs: []
  type: TYPE_NORMAL
- en: Additional considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are lots of additional considerations to think about here, and that’s
    because there is *so much to consider* beyond merely releasing data in production.
  prefs: []
  type: TYPE_NORMAL
- en: Data isn’t static. It doesn’t simply exist in a place where it can be manipulated.
    It arrives in locations sparsely distributed across an organisation. It gets moved
    between tools. It arrives at different frequencies and only after the laborious
    process of “ELT” does it finally arrive in a data lake or data warehouse.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, Github Actions isn’t a sufficient infrastructure for doing all
    of this work. Perhaps as an orchestration layer, but certainly not for doing heavy-compute
    and doing data management.
  prefs: []
  type: TYPE_NORMAL
- en: These factors lead to many additional considerations for how to design a system
    that’s capable of delivering Continuous Data Integration and Delivery, which I
    discuss here
  prefs: []
  type: TYPE_NORMAL
- en: User Interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having a single User Interface to view Data deployments is key. Data teams who
    just use the UIs from multiple cloud data providers for DataOps will be at a loss
    when it comes to aggregating metadata to do effective DataOps, but also [BizFinOps](https://aws.amazon.com/blogs/enterprise-strategy/introducing-finops-excuse-me-devsecfinbizops/).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also data deployments to aggregate, which typically arise due to:'
  prefs: []
  type: TYPE_NORMAL
- en: New data arriving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A change in the logic for how to materialise the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are currently handled using a workflow orchestration tool and GitHub actions
    or something similar. This creates a disjoint — Data Teams need to inspect multiple
    tools to understand when data tables have been updated, what their definitions
    are, and so on. You could, of course, buy an observability tool. However this
    is *yet another UI, another tool, and another cost.*
  prefs: []
  type: TYPE_NORMAL
- en: Having a genuine single pane of glass for Orchestration, Observability, and
    some kind of Ops would be a killer feature and one I would have loved to use at
    Codat, where we’d stitched together a whole host of open-sourced and closed-source
    vendor SAAS tools.
  prefs: []
  type: TYPE_NORMAL
- en: Observability or Metadata gathering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I alluded to this in the previous section, but observability and metadata gathering
    is fundamental to a strong Data Pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The important thing here, which I believe Observability Platforms miss, is to
    place the Observation in the Pipeline itself. Otherwise, presenting data engineers
    with metadata, pieces of information like “this failed” or “that table is stale”
    is undesirable, since it’s A) ex-post (after it’s too late) and B) unrelated to
    pipeline runs. Sure — a table is broken. But is it broken because of a change
    someone just pushed or because of some new data that arrived?
  prefs: []
  type: TYPE_NORMAL
- en: There was another good talk from [Andrew Jones](https://andrew-jones.com/categories/data-contracts/)
    I attended recently where he spoke about the 1, 10, 100 pyramid.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c9e90125d2e5e686ec0628f83a9afc09.png)'
  prefs: []
  type: TYPE_IMG
- en: It costs $1 to prevent, $10 to mitigate and $100 once it’s too late. Image credit
    to Andrew Jones, posted with permission.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prevention Cost** — Preventing an error in data at the point of extraction
    will cost you a $1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correction Cost** — Having someone correct an error post extraction will
    cost you $10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failure Cost** — Letting bad data run through a process to its end resting
    place will cost you $100.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observability tools are in the yellow to red section. If they’re setup on your
    prod databases, the likelihood is you’re in a race against time to fix the issue
    before someone realises.
  prefs: []
  type: TYPE_NORMAL
- en: Having analysts patch bad data with hacky SQL sits in the yellow section.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestration with observability combined is between the green and the yellow.
    If all your data pipelines have access to all your metadata, and can implement
    data quality tests as you materialise and update tables and views, then you can
    pause a pipeline *any time* these tests fail. This means [*no bad data ever gets
    into production*](https://medium.com/snowflake/avoid-bad-data-completely-continuous-delivery-architectures-in-the-modern-data-stack-part-1-22a0d48935f6)*.*
  prefs: []
  type: TYPE_NORMAL
- en: This is extremely powerful, which is why I believe having an Orchestration tool
    that executes data pipelines with access to granular metadata is the way forward
    (disclosure my start-up is doing just that).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are moving away from data-agnostic workflow orchestration tools plus janky
    or non-existent Continuous Integration to unified Continuous *DATA* Integration
    and Delivery.
  prefs: []
  type: TYPE_NORMAL
- en: There will be platforms that enable data teams to get full, reliable, and efficient
    version-controlling of datasets and rock-solid data pipelines. These will have
    observability capabilities built-in, and while many are not end-to-end yet, it
    certainly feels like this is the way things are heading.
  prefs: []
  type: TYPE_NORMAL
- en: There are many mature data tools that are doing this. For example, for data
    warehousing CI and CD, Y42 have the [concept](https://www.y42.com/blog/virtual-data-builds-one-data-warehouse-environment-for-every-git-commit/)
    of “Virtual Data Builds” which is basically the same thing as part 1 of this article.
    For the Data Lake environment, Einat Orr over at Lake FS / Treeverse posted on
    this [recently](https://www.linkedin.com/feed/update/urn:li:activity:7126949272050106369/?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7126949272050106369%2C7126969193651924992%29&dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287126969193651924992%2Curn%3Ali%3Aactivity%3A7126949272050106369%29&dashReplyUrn=urn%3Ali%3Afsd_comment%3A%287126970713164439552%2Curn%3Ali%3Aactivity%3A7126949272050106369%29&replyUrn=urn%3Ali%3Acomment%3A%28activity%3A7126949272050106369%2C7126970713164439552%29)
    — what they do is functionally pretty similar and easily implementable in Snowflake
    ([I wrote an article about that here](https://medium.com/snowflake/why-snowflakes-clone-command-changes-the-game-for-ci-cd-in-data-ccb6fb9955ba)).
    SQLMesh’ “enterprise” version (don’t believe the open source [happy clappyness](https://medium.com/@hugolu87/y-combinator-had-282-companies-this-winter-and-c-50-were-open-source-sunday-scaries-128e53318454),
    this *is* a venture backed business. They *will* try to make money, like all of
    us) has observability and version control built into it, and it’s pretty cool.
  prefs: []
  type: TYPE_NORMAL
- en: You can, of course, still do *all of this* using something like Airflow. Hell,
    you could brew your morning coffee with Airflow if you wanted to. I guess the
    question is — do you have the time, the patience, and the expertise to write all
    of that code? Or are you like me, and do you just want to get shit done? 🐠
  prefs: []
  type: TYPE_NORMAL
