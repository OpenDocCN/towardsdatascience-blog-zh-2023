- en: 'The Ultimate Guide to Training BERT from Scratch: Introduction'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Demystifying BERT: The definition and various applications of the model that
    changed the NLP landscape.'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dpoulopoulos.medium.com/?source=post_page-----b048682c795f--------------------------------)[![Dimitris
    Poulopoulos](../Images/ce535a1679779f5a2ec8b024e6691e50.png)](https://dpoulopoulos.medium.com/?source=post_page-----b048682c795f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b048682c795f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b048682c795f--------------------------------)
    [Dimitris Poulopoulos](https://dpoulopoulos.medium.com/?source=post_page-----b048682c795f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b048682c795f--------------------------------)
    ·10 min read·Sep 2, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f9d976bd1bc99fc0737ffec178ea1be4.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Ryan Wallace](https://unsplash.com/@accrualbowtie?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[Part II](/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822)
    and [Part III](/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5)
    of this story are now live.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A few weeks ago, I trained and deployed my very own question-answering system
    using Retrieval Augmented Generation (RAG). The goal was to introduce such a system
    over my study notes and create an agent to help me connect the dots. LangChain
    truly shines in these specific types of applications:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As the system''s quality blew me away, I couldn’t help but dig deeper to understand
    the wizardry under the hood. One of the features of the RAG pipeline is its ability
    to sift through mountains of information and find the context most relevant to
    a user’s query. It sounds complex but starts with a simple yet powerful process:
    encoding sentences into information-dense vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most popular way to create these sentence embeddings for free is none other
    than SBERT, a [sentence transformer](https://www.sbert.net/) built upon the legendary
    BERT encoder. And finally, that brings us to the main object of this series: understanding
    the fascinating world of BERT. What is it? What can you do with it? And the million-dollar
    question: How can you train your very own BERT model from scratch?'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll kick things off by demystifying what BERT actually is, delve into its
    objectives and wide-ranging applications, and then move on to the nitty-gritty
    — like preparing datasets, mastering tokenization, understanding key metrics,
    and, finally, the ins and outs of training and evaluating your model.
  prefs: []
  type: TYPE_NORMAL
- en: This series will be highly detailed and technical, featuring code snippets as
    well as links to GitHub repositories. By the end, I’m confident you’ll gain a
    deeper understanding of why BERT is regarded as a legendary model in the field
    of NLP. So, if you share my excitement, grab a colab Notebook, and let’s dive
    in!
  prefs: []
  type: TYPE_NORMAL
- en: '[Learning Rate](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-intro)
    is a newsletter for those who are curious about the world of ML and MLOps. If
    you want to learn more about topics like this subscribe [here](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-intro).
    You’ll hear from me on the last Sunday of every month with updates and thoughts
    on the latest MLOps news and articles!'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BERT, which stands for Bidirectional Encoder Representations from Transformers,
    is a revolutionary Natural Language Processing (NLP) model developed by Google
    in 2018 (Michael Rupe, [How the Google BERT Update Changed Keyword Research](https://www.t3seo.com/how-the-google-bert-update-changed-keyword-research/)).
    Its introduction marked a significant advancement in the field, setting new state-of-the-art
    benchmarks across various NLP tasks. For many, this is regarded as the ImageNet
    moment for the field.
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT is pre-trained on a massive amount of data, with one goal: to understand
    what language is and what’s the meaning of context in a document. As a result,
    this pre-trained model can be fine-tuned for specific tasks such as question-answering
    or sentiment analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: So far, so good, but too much theory won’t get us anywhere. Thus, let’s briefly
    go through the architecture of BERT over the next section and then take a closer
    look at how you pre-train such a model and, more importantly, how you can put
    it to good use, using the question-answering use case as a concrete example.
  prefs: []
  type: TYPE_NORMAL
- en: The Scaffold
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'BERT’s architecture is based on the Transformer model, which has been particularly
    influential in the realm of deep learning for NLP tasks. The Transformer model
    itself is made of an encoder-decoder stack, but BERT only uses the encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6befc9916c917599b5e2f58ceefb7500.png)'
  prefs: []
  type: TYPE_IMG
- en: Bert Architecture — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what each colored block in the architecture signifies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input Embeddings:** Input tokens (words or subwords) are transformed into
    embeddings, which are then fed into the model. BERT combines both token and positional
    embeddings as input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Positional Encodings:** Since BERT and the underlying Transformer architecture
    do not have any built-in sense of word order (as recurrent models like LSTMs do),
    they incorporate positional encodings to give the model information about the
    position of words in a sequence. In the original Transformer paper, the positional
    encodings were fixed beforehand; however, nowadays, it’s much more common to learn
    these encodings during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attention Mechanism:** One of the primary innovations in the Transformer
    architecture is the “self-attention mechanism”, which allows the model to weigh
    the importance of different words in a sentence relative to a given word, thereby
    capturing context. This is crucial for BERT’s bidirectionality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feed-Forward Neural Network:** Each Transformer block contains a feed-forward
    neural network that operates independently on each position.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer Normalization & Residual Connections:** Each sub-layer (like self-attention
    or feed-forward neural network) in the model includes a residual connection around
    it followed by layer normalization. This helps in training deep networks by mitigating
    the vanishing gradient problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiple Stacks:** BERT’s depth is one of its defining characteristics. BERT''s
    “base” version uses 12 stacked Transformer encoders, while the “large” version
    uses 24.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have a mental image in our cache let’s proceed to examine more closely
    how we can train such a model.
  prefs: []
  type: TYPE_NORMAL
- en: Going to School
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We usually split the training of BERT into two phases: in the first phase —
    called pre-training — the goal is to teach the model what language is and how
    context changes the meaning of words. In the second phase — called fine-tuning
    — we make it do something actually useful.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine the two phases separately, using concrete examples and visuals.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In pre-training, BERT tries to solve two tasks simultaneously: i) masked language
    model (MLM or “cloze” test) and ii) next-sentence prediction (NSP).'
  prefs: []
  type: TYPE_NORMAL
- en: The word [cloze](https://en.wiktionary.org/wiki/cloze) is derived from closure
    in [Gestalt theory](https://en.wikipedia.org/wiki/Gestalt_psychology) ( “Gestalt
    psychology”).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the first paradigm, random words in a sentence are replaced with a `[MASK]`
    token, and BERT tries to predict the original word from the context. This differs
    from traditional language models, which predict words in a sequence. This is crucial
    for BERT since it is trying to encode every word in a sequence using information
    from both directions, left and right, hence “Bidirectional”.
  prefs: []
  type: TYPE_NORMAL
- en: For the second task, BERT takes in two sentences, determining if the second
    sentence follows the first. This helps BERT understand context across sentences.
    Also, this is where segment embeddings become critical, as they allow the model
    to differentiate between the two sentences. A pair of sentences fed into BERT
    for NSP will be assigned different segment embeddings to indicate to the model
    which sentence each token belongs to.
  prefs: []
  type: TYPE_NORMAL
- en: 'How does BERT learn these two tasks simultaneously? The following figure will
    clarify everything:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1942614f4740405f9162a1f4b02f43d.png)'
  prefs: []
  type: TYPE_IMG
- en: BERT pre-training — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we add two special tokens to our sequence: the `[CLS]` token — for classification
    — and the `[SEP]` token to separate the two sentences. Next, we pass the sequence
    through BERT and obtain one contextualized representation (i.e., embedding) for
    each token. If you’re paying attention, you’ll see two new embeddings: `E_A` and
    `E_B`. These are two new embeddings we learn during training to inform BERT that
    there are two separate sequences in the example. We’ll see the role of these embeddings
    later in the fine-tuning phase.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here comes the final part: First, we take the embedding of the `[CLS]` token
    and pass it through a new linear layer with two output units to classify it either
    as a positive label (the sentences are related) or a negative one. Take a look
    at the HuggingFace `transformers` library source code on [GitHub](https://github.com/huggingface/transformers/blob/07998ef39926b76d3f6667025535d0859eed61c3/src/transformers/models/bert/modeling_bert.py#L716):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s first examine the model’s prediction shape for the Masked ML task. After
    some transformations, the predictions for each token provide a score for every
    word in the vocabulary. So, the first output will be of shape `[1, 50,000]` if
    we assume that we have 50,000 words in our vocabulary. Let’s see that in the [code](https://github.com/huggingface/transformers/blob/07998ef39926b76d3f6667025535d0859eed61c3/src/transformers/models/bert/modeling_bert.py#L706):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, to compute the total loss, we use cross entropy to compute the loss
    of each task separately, and then we add them [together](https://github.com/huggingface/transformers/blob/07998ef39926b76d3f6667025535d0859eed61c3/src/transformers/models/bert/modeling_bert.py#L1142):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the final section, let’s see how we can fine-tune BERT to produce a question-answering
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Graduation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once pre-trained, we can specialize BERT on a specific task using a smaller
    labeled dataset. For example, let’s take a closer look at the Q&A task:'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning BERT for question-answering (Q&A) tasks, such as the [Stanford Question
    Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/Super_Bowl_50.html),
    involves adjusting the model to predict the start and end positions of the answer
    in a given passage for a provided question.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through the steps to fine-tune BERT for such tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each item in the dataset will typically have a question, a passage (or reference),
    and the start and end positions of the answer within the passage as the label.
  prefs: []
  type: TYPE_NORMAL
- en: We tokenize the question and the passage into subwords using BERT’s tokenizer,
    separate the question from the passage using the `[SEP]` special token, and start
    the input sequence using the `[CLS]` special token.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we create a new array, marking the question as segment `A` and the
    reference as segment `B`. We will use this information to add the learned segment
    embeddings later on.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0084e2efd13b31a45c05215b77d84f0f.png)'
  prefs: []
  type: TYPE_IMG
- en: BERT QnA Dataset Preparation — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Model modification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the pre-trained BERT model can output contextualized embeddings for
    each token in a sequence, for Q&A tasks, you’ll need to derive start and end position
    predictions from these embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'To this end, we add a dense (fully connected) layer on top of BERT, with two
    output nodes: one for predicting the start position and one for predicting the
    end position of the answer in the [passage](https://github.com/huggingface/transformers/blob/c385de24414e4ec6125ee14c46c128bfe70ecb66/src/transformers/models/bert/modeling_bert.py#L1803).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For each token in the passage, the model will output a score indicating how
    likely that token is the starting point of the answer and another score for how
    likely it is the ending point of the answer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5347c5000f5e9812dc29493591b7d994.png)'
  prefs: []
  type: TYPE_IMG
- en: BERT QnA Prediction — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: In the figure above, it seems that the model predicts that the answer starts
    with token `15`, because the model assigned the highest probability to its output
    and ends with token `18`, for the same reason.
  prefs: []
  type: TYPE_NORMAL
- en: We use a SoftMax function over the entire sequence to get a probability distribution
    for the start and end positions, and the [loss](https://github.com/huggingface/transformers/blob/c385de24414e4ec6125ee14c46c128bfe70ecb66/src/transformers/models/bert/modeling_bert.py#L1875)
    is calculated using the cross entropy between the predictions and the correct
    start and end positions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we start with the pre-trained BERT weights and use a smaller learning
    rate (e.g., `2e-5` or `3e-5`) since BERT is already pre-trained. Too large a learning
    rate might cause the model to diverge. We fine-tune the model on the Q&A dataset
    for several epochs until the validation performance plateaus or starts decreasing.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve covered a lot of ground in this comprehensive guide on training BERT from
    scratch. Starting from understanding what BERT is, we delved into its architectural
    intricacies, the logic behind its pre-training and fine-tuning, and even explored
    how to adapt it for a question-answering task. Along the way, we touched on the
    key metrics, the importance of tokenization, and the code snippets essential for
    anyone looking to get their hands dirty in the field of Natural Language Processing.
  prefs: []
  type: TYPE_NORMAL
- en: The story of BERT, however, doesn’t end here. In the next story, we’ll take
    things from the start and dive deeper into the BERT tokenizer and how it learns
    to split words into subwords. Stay tuned!
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: My name is [Dimitris Poulopoulos](https://www.dimpo.me/?utm_source=medium&utm_medium=article&utm_campaign=bert-intro),
    and I’m a machine learning engineer working for [HPE](https://www.hpe.com/us/en/home.html).
    I have designed and implemented AI and software solutions for major clients such
    as the European Commission, IMF, the European Central Bank, IKEA, Roblox and others.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in reading more posts about Machine Learning, Deep Learning,
    Data Science, and DataOps, follow me on [Medium](https://towardsdatascience.com/medium.com/@dpoulopoulos/follow),
    [LinkedIn](https://www.linkedin.com/in/dpoulopoulos/), or [@james2pl](https://twitter.com/james2pl)
    on Twitter.
  prefs: []
  type: TYPE_NORMAL
- en: Opinions expressed are solely my own and do not express the views or opinions
    of my employer.
  prefs: []
  type: TYPE_NORMAL
