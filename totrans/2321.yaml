- en: What are Query, Key, and Value in the Transformer Architecture and Why Are They
    Used?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/what-are-query-key-and-value-in-the-transformer-architecture-and-why-are-they-used-acbe73f731f2](https://towardsdatascience.com/what-are-query-key-and-value-in-the-transformer-architecture-and-why-are-they-used-acbe73f731f2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An analysis of the intuition behind the notion of Key, Query, and Value in Transformer
    architecture and why is it used.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ebrahimpichka.medium.com/?source=post_page-----acbe73f731f2--------------------------------)[![Ebrahim
    Pichka](../Images/8add6e8e875d9e921caf7f5eaa77d545.png)](https://ebrahimpichka.medium.com/?source=post_page-----acbe73f731f2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----acbe73f731f2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----acbe73f731f2--------------------------------)
    [Ebrahim Pichka](https://ebrahimpichka.medium.com/?source=post_page-----acbe73f731f2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----acbe73f731f2--------------------------------)
    ·10 min read·Oct 5, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36bd08a0784d8f29ebfaca4463809896.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author — generated by [**Midjourney**](https://www.midjourney.com/)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recent years have seen the Transformer architecture make waves in the field
    of natural language processing (NLP), achieving state-of-the-art results in a
    variety of tasks including machine translation, language modeling, and text summarization,
    as well as other domains of AI i.e. Vision, Speech, RL, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Vaswani et al. (2017), first introduced the transformer in their paper *“Attention
    Is All You Need”*, in which they used the self-attention mechanism without incorporating
    recurrent connections while the model can focus selectively on specific portions
    of input sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c8d9226a835a3ef8723261c98b8ff73.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Transformer model architecture — Image from the Vaswani et al. (2017) paper
    (Source: [**arXiv:1706.03762v7**](https://arxiv.org/abs/1706.03762v7))'
  prefs: []
  type: TYPE_NORMAL
- en: In particular, previous sequence models, such as recurrent encoder-decoder models,
    were limited in their ability to capture long-term dependencies and parallel computations.
    In fact, right before the Transformers paper came out in 2017, state-of-the-art
    performance in most NLP tasks was obtained by using RNNs with an attention mechanism
    on top, so attention kind of existed before transformers. By introducing the multi-head
    attention mechanism on its own, and dropping the RNN part, the transformer architecture
    resolves these issues by allowing multiple independent attention mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we will go over one of the details of this architecture, namely
    the Query, Key, and Values, and try to make sense of the intuition used behind
    this part.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this post assumes you are already familiar with some basic concepts
    in NLP and deep learning such as **embeddings**, **Linear (dense) layers**, and
    in general how a simple neural network works.
  prefs: []
  type: TYPE_NORMAL
- en: Attention!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, let’s start understanding what the attention mechanism is trying to achieve.
    And for the sake of simplicity, let’s start with **a simple case** of sequential
    data to understand what problem exactly we are going to solve, without going through
    all the jargon of the *attention mechanism*.
  prefs: []
  type: TYPE_NORMAL
- en: Context Matters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider the case of **smoothing time-series data**. Time series are known to
    be one of the most basic kinds of sequential data due to the fact that it is already
    in a numerical and structured form, and is usually in low-dimensional space. So
    it would be suitable to lay out a good starting example.
  prefs: []
  type: TYPE_NORMAL
- en: To smooth a highly variant time series, a common technique is to calculate a
    “**weighted average”** of the proximate timesteps for each timestep, as shown
    in image 1, the weights are usually chosen based on how close the proximate timesteps
    are to our desired timestep. For instance, in Gaussian Smoothing, these weights
    are drawn from a Gaussian function that is centered at our current step.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16b29fa0cd09627df288a00c2d248be8.png)'
  prefs: []
  type: TYPE_IMG
- en: time-series smoothing example — Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we have done here, in a sense, is that:'
  prefs: []
  type: TYPE_NORMAL
- en: We took **a sequence of values**,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And for each step of this sequence, we **added (a weighted) context** from its
    proximate values, while the proportion of added context (**the weight)** is only
    related to their **proximity** to the target value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And finally, we attained a new **contextualized** sequence, which we can understand
    and analyze more easily.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**There are two key points/issues in this example:**'
  prefs: []
  type: TYPE_NORMAL
- en: It only uses the **proximity** and **ordinal position** of the values to obtain
    the weights of the context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weights are calculated by fixed arbitrary rules for all points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Case of Language
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In machine learning, textual data always have to be represented by vectors
    of real-valued numbers AKA **Embeddings**. So we assume that the primary meanings
    of tokens (or words) are encoded in these vectors. Now in the case of textual
    sequence data, if we would like to apply the same kind of technique to **contextualize**
    each token of the sequence as the above example so that each token’s new embedding
    would contain more information about its context, we would encounter some issues
    which we will discuss now:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, in the example above, we **only** used the **proximity of tokens**
    to determine the importance (weights) of the context to be added, while **words
    do not work like that.**
  prefs: []
  type: TYPE_NORMAL
- en: In language, the context of a word in a sentence is not based only on the ordinal
    distance and proximity. We can’t just blindly use proximity to incorporate context
    from other words.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, adding the context only by taking the (weighted) average of the embeddings
    of the context tokens itself may not be entirely intuitive. A token’s embedding
    may contain information about different syntactical, semantical, or lexical aspects
    of that token. All of this information may not be relevant to the target token
    to be added. So it’s better not to add all the information as a whole as context.
  prefs: []
  type: TYPE_NORMAL
- en: So if we have some (vector) representation of words in a sequence, **how** do
    we **obtain the weights** and **the relevant context** to re-weight and contextualize
    each token of the sequence?
  prefs: []
  type: TYPE_NORMAL
- en: The answer, in a broad sense, is that we have to “**search”** for it, based
    on some specific aspects of the tokens meaning (could be semantic, syntactic,
    or anything). And during this **search**, assign the **weights** and the **context
    information** based on relevance or importance.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It means that for each of the tokens in a sequence, we have to **go through
    all other tokens** in the sequence, and assign them **weights** and the **context
    information,** based on a similarity metric that we use to compare our target
    token with others. **The more similar they are in terms of the desired context,
    the larger the weight it gets**.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So, in general, we could say that the attention mechanism is basically (1) **assigning
    weights** toand (2) extracting **relevant context** from other tokens of a sequence
    based on their relevance or importance to a target token (i.e. attending to them).
  prefs: []
  type: TYPE_NORMAL
- en: And we said that in order to find this relevance/importance we need to **search
    through our sequence** and compare tokens one-to-one.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55d8c5d3622b879762f626cf777d50a0.png)'
  prefs: []
  type: TYPE_IMG
- en: Searching through the sequence for relevant context to a token for adding —
    Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: '**This is where the *Query*, *Key*, and *Values* find meaning.**'
  prefs: []
  type: TYPE_NORMAL
- en: '***Query*, *Key*, and *Value***'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To make more sense, think of when you search for something on YouTube, for example.
    Assume YouTube stores all its videos as a pair of “*video title*” and the “*video
    file*” itself. Which we call a **Key-Value pair**, with the Key being the video
    title and the Value being the video itself.
  prefs: []
  type: TYPE_NORMAL
- en: The text you put in the search box is called a **Query** in search terms. So
    in a sense, when you search for something, **YouTube compares your search Query
    with the Keys of all its videos**, then measures the similarity between them,
    and ranks their **Values** from the highest similarity down.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0387c1a513e915155c6bb6fdefeb750d.png)'
  prefs: []
  type: TYPE_IMG
- en: A basic search procedure illustrating the notion of Key, Quey, and Value — Image
    by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our problem, we have a sequence of token vectors, and we want to search
    for **the weights** to re-weight and contextualize each token (word) embedding
    of the sequence, we can think in terms of:'
  prefs: []
  type: TYPE_NORMAL
- en: What you want to look for is the **Query**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What you are searching among is **Key-Value** pairs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The query is compared to all the Keys to measure the **relevance/importance/similarity**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Values** are utilized based on the assigned similarity measure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Another helpful relevant analogy is a dictionary (or hashmap) data structure.
    A dictionary stores data in key-value pairs and it maps keys to their respective
    value pairs. When you try to **get a specific** value from the dictionary, you
    have to provide a **query** to match its corresponding **key**, then it **searches
    among those keys**, **compares them with the query**, and **if matched**, the
    desired value will be returned.
  prefs: []
  type: TYPE_NORMAL
- en: However, the difference here is that this is a “hard-matching” case, where the
    Query either **exactly matches** the Key or it doesn’t and an in-between similarity
    is not measured between them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/311e0dba10b97d888a7104f7029b7847.png)'
  prefs: []
  type: TYPE_IMG
- en: Dictionary Query matching with Key-Value pairs — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: We earlier mentioned that we are only working with real-valued vectors (token
    embeddings). So the Query, Key, and Value also need to be vectors. However, so
    far we only have **one vector for each token** which is its embedding vector.
    So, how should we obtain the Query, Key, and Value vectors?
  prefs: []
  type: TYPE_NORMAL
- en: 'We **Construct them** using linear projections (linear transformations aka
    single dense layer with separate sets of weights: Wq, Wₖ, Wᵥ) of the embedding
    vector of each token. This means we use a **learnable** vector of weights for
    each of the Query, Key, and Value to do a linear transformation on the word embedding
    to **obtain the corresponding Query, Key, and Value vectors**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/466cfc4ec0ca8ce6bf781237a111f9c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear transformation of the word embedding to obtain Query, Key, and Value
    vectors — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'An embedding of a token may represent different contextual, structural, and
    syntactical, aspects or meanings of that token. By using **learnable linear transformation
    layers** to construct these vectors from the token’s embedding, we allow the network
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extract** and pass a limited specific part of that information into the ***Q***,
    ***K***, and ***V*** vectors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine a narrower context in which the search and match is going to be done.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Learn** what information in an embedding is more important to attend to.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, having the ***Q***, ***K***, and ***V*** vectors in hand, we are able to
    perform the “**search and compare”** procedure that was discussed before, with
    these vectors. This results in the final derivation of the attention mechanism
    proposed in the proposed in (Vaswani et al 2017).
  prefs: []
  type: TYPE_NORMAL
- en: 'For each token:'
  prefs: []
  type: TYPE_NORMAL
- en: We compare its **Query vector** to all other tokens’ **Key vectors**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate a **vector similarity** score between each two (i.e. the dot-product
    similarity in the original paper)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transform these similarity **scores** into **weights** by scaling them into
    [0,1] (i.e. Softmax)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And add the weighted context by weighting their corresponding **value vectors**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/d292c611f2a18909aba5f8e76835fca3.png)'
  prefs: []
  type: TYPE_IMG
- en: Dot-product attention procedure —Image by author
  prefs: []
  type: TYPE_NORMAL
- en: So the whole notion of the ***Q***, ***K***, and ***V*** vectors is like a **soft**
    dictionary to mimic a ***search-and-match procedure*** from which we learn how
    much two tokens in a sequence are relevant (the weights), and **what should be
    added** as the context (the values). Also, note that this process does not have
    to happen sequentially (one token at a time). This all happens in parallel by
    using matrix operations.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the illustration below, the matrix dimensions are switched compared
    to that of the original paper (***n_tokens*** by ***dim*** instead of ***dim***
    by ***n_tokens***). Later in this post, you will see the original and complete
    formulation of the attention mechanism which is the other way around.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eae164765934b254aeedd32b94c5f5cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Matrix form of the dot-product attention — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'This results in a more context-aware embedding of each token, where the added
    context is based on the relevance of the tokens to each other and it is learned
    through ***Q***, ***K***, ***V*** vector transformation. Hence, the dot-product
    attention mechanism. The original attention mechanism in (Vaswani et al, 2017)
    also scales the dot-product of ***K*** and ***Q*** vectors, meaning it divides
    the resulting vector by ***sqrt(d)***, where ***d*** is the dimension of the Query
    vector. Hence the name, ***“scaled dot-product attention”***. This scaling helps
    with reducing the variance of the dot-product before being passed to the Softmax
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e602bdf1db623fd467c9c072ed50f509.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Attention mechanism formula — Vaswani et al. (2017) (Source: [**arXiv:1706.03762v7**](https://arxiv.org/abs/1706.03762v7))'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we mentioned that the linear layers that transform the embedding into
    ***Q***, ***K***, ***V,*** may extract only a specific pattern in the embedding
    for finding the attention weights. To enable the model to learn different complex
    relations between the sequence tokens, create and use multiple different versions
    of these ***Q***, ***K***, ***V***, so that each will focus on different patterns
    existing in our embeddings. These multiple versions are called **attention heads**
    resulting in the name “multi-head attention”. These heads can also be vectorized
    and computed in parallel using current popular deep learning frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2cdd64a6737faef6b55fa00cf48e24e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Multi-head scaled dot-product attention — Image from the Vaswani et al. (2017)
    paper (Source: [**arXiv:1706.03762v7**](https://arxiv.org/abs/1706.03762v7))'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So to wrap up, in this post I tried to picture and analyze the intuition behind
    the use of Query, Key, and Value which are key components in the attention mechanism
    and may be a little difficult to make sense of, at first encounters.
  prefs: []
  type: TYPE_NORMAL
- en: The attention mechanism discussed in this post was proposed in the transformer
    architecture that is introduced in the (Vaswani et al, 2017) paper “Attention
    is all you need” and has been one of the top-performing architectures since, in
    several different tasks and benchmarks in deep learning. With its vast use cases
    and applicability, it would be helpful to have an understanding of the intuition
    behind the nuts and bolts used in this architecture and know why we use it.
  prefs: []
  type: TYPE_NORMAL
- en: I attempted to be as clear and as basic as possible while explaining this topic
    by laying down examples and illustrations wherever possible.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “*Attention Is All You Need.*”
    arXiv, August 1, 2023\. [https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762.).'
  prefs: []
  type: TYPE_NORMAL
