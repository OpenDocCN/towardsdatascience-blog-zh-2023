- en: 'BERTopic: What Is So Special About v0.16?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/bertopic-what-is-so-special-about-v0-16-64d5eb3783d9](https://towardsdatascience.com/bertopic-what-is-so-special-about-v0-16-64d5eb3783d9)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exploring Zero-Shot Topic Modeling, Model Merging, and LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@maartengrootendorst?source=post_page-----64d5eb3783d9--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----64d5eb3783d9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----64d5eb3783d9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----64d5eb3783d9--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----64d5eb3783d9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----64d5eb3783d9--------------------------------)
    ·8 min read·Dec 13, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dfb10d9cf8adc4cc4209ef052d1984d2.png)'
  prefs: []
  type: TYPE_IMG
- en: My ambition for [BERTopic](https://github.com/MaartenGr/BERTopic) is to make
    it the **one-stop shop** for topic modeling by allowing for significant flexibility
    and modularity.
  prefs: []
  type: TYPE_NORMAL
- en: That has been the goal for the last few years and with the [release of v0.16](https://github.com/MaartenGr/BERTopic/releases/tag/v0.16.0),
    I believe we are a BIG step closer to achieving that.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s take a small step back. *What is BERTopic?*
  prefs: []
  type: TYPE_NORMAL
- en: Well, BERTopic is a topic modeling framework that allows users to essentially
    create their version of a topic model. With many variations of topic modeling
    implemented, the idea is that it should support almost any use case.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/49c2eac172607192b89273d7ba3c958f.png)'
  prefs: []
  type: TYPE_IMG
- en: The modular nature of BERTopic allows you to build your topic model however
    you want. Switching components allows BERTopic to grow with the latest developments
    in Language AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'With [v0.16](https://github.com/MaartenGr/BERTopic/releases/tag/v0.16.0), several
    features were implemented that I believe will take BERTopic to the next level,
    namely:'
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Shot Topic Modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model Merging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More Large Language Model (LLM) Support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/e3dc2396fcba0eabdc3089c841e69f3f.png)'
  prefs: []
  type: TYPE_IMG
- en: Just a few of BERTopic’s capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we will go through what these features are and for which use
    cases they could be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, you can install BERTopic (with HF datasets) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can also follow along with the [Google Colab Notebook](https://colab.research.google.com/drive/113Eg-cq9wUuOuNwXO40Zo3AlMgaW2Go1?usp=sharing)
    to make sure everything works as intended.
  prefs: []
  type: TYPE_NORMAL
- en: '**UPDATE**: I uploaded a video version to YouTube that goes more in-depth into
    how to use these new features:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Zero-Shot Topic Modeling: A Flexible Technique'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Zero-shot techniques generally refer to having no examples to train your data
    on. Although you know the target, it is not assigned to your data.
  prefs: []
  type: TYPE_NORMAL
- en: In BERTopic, we use Zero-shot Topic Modeling to find pre-defined topics in large
    amounts of documents.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you have ArXiv abstracts about Machine Learning and you know that the
    topic “*Large Language Models*” is in there. With Zero-shot Topic Modeling, you
    can ask BERTopic to find all documents related to “Large Language Models”.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, it is nothing more than semantic search! But… there is a neat trick
    ;-)
  prefs: []
  type: TYPE_NORMAL
- en: When you try to find those documents related to “Large Language Models”, there
    will be many left not about those topics. So, what do you do with those topics?
    You use BERTopic to find all topics that were left!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/27e6574d50cce294aefaf97e289314a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As a result, you will have three scenarios of Zero-shot Topic Modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '**No zero-shot topics were detected**. This means that none of the documents
    would fit with the predefined topics and a regular BERTopic would be run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Only zero-shot topics were detected**. Here, we would not need to find additional
    topics since all original documents were assigned to one of the predefined topics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Both zero-shot topics and clustered topics were detected**. This means that
    some documents would fit with the predefined topics whereas others would not.
    For the latter, new topics were found.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using Zero-shot BERTopic is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can view the three pre-defined topics along with several newly discovered
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/46b38e80f4d38334c92b4d6b2fd3925d.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that although we have pre-defined names for the topics, we allow BERTopic
    for additional representations.
  prefs: []
  type: TYPE_NORMAL
- en: This gives exciting new insight into pre-defined topics!
  prefs: []
  type: TYPE_NORMAL
- en: So… when do you use Zero-shot Topic Modeling?
  prefs: []
  type: TYPE_NORMAL
- en: If you already know some of the topics in your data, this is a great solution
    for finding them! Since it can discover both pre-defined and new topics, is an
    incredibly flexible technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model Merging: Federated and Incremental Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a fun new feature, *model merging*!
  prefs: []
  type: TYPE_NORMAL
- en: Model merging refers to BERTopic’s capability to combine multiple pre-trained
    BERTopic models to create one large topic model. It explores which topics should
    be merged and which should remain separate.
  prefs: []
  type: TYPE_NORMAL
- en: It works as follows. When we pass a list of models to this new feature, `.merge_models`,
    the first model in the list is chosen as the baseline. This baseline is used to
    check whether all other models contain new topics based on the similarity between
    their topic embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Dissimilar topics are added to the baseline model whereas similar topics are
    assigned to the topic of the baseline. This means that we need the embedding models
    to be the same.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e86b0503a7c4538ca331cd911112f58c.png)'
  prefs: []
  type: TYPE_IMG
- en: When merging BERTopic models, duplicate topics will be merged and all other
    topics will be kept the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Merging pre-trained BERTopic models is straightforward and only requires a
    few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: And that is it! With a single function, `.merge_models`, you can merge pre-trained
    BERTopic models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The benefit of merging pre-trained models is that it allows for a variety of
    creative and useful use cases. For instance, we could use it for:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Incremental Learning* — We can continuously discover new topics by iteratively
    merging models. This can be used for issue tickets to quickly uncover pressing
    bugs/issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Batched Learning* — Compute and memory problems can arise with large datasets
    or when you simply do not have the hardware for it. By splitting the training
    process up into smaller models, we can get similar performance whilst reducing
    the necessary compute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Federated Learning* —Merging models allow for the training to be distributed
    among different clients who do not wish to share their data. This increases privacy
    and security with respect to their data especially if a non-keyword-based method
    is used for generating the representations, such as using a [Large Language Model](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html#zephyr-mistral-7b).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Federated Learning is rather straightforward, simply run `.merge_models` on
    your central server.
  prefs: []
  type: TYPE_NORMAL
- en: The other two, incremental and batched learning, might require a bit of an example!
  prefs: []
  type: TYPE_NORMAL
- en: Incremental and Batched Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To perform both *incremental* and *batched* learning, we are going to mimic
    a typical `.partial_fit` pipeline. Here, we will train a base model first and
    then iteratively add a small newly trained model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In each iteration, we can check any topics that were added to the base model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To illustrate, this will give back newly found topics such as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics are newly found:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '['
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ‘50_forecasting_predicting_prediction_stocks’,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ‘51_activity_activities_accelerometer_accelerometers’,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ‘57_rnns_deepcare_neural_imputation’
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ']'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It retains everything from the original model, including
  prefs: []
  type: TYPE_NORMAL
- en: Not only do we reduce the compute by splitting the training up into chunks,
    but we can monitor any new topics that were added to the model.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, you can train a new model with a frequency that fits your use case.
    You might check for new topics monthly, weekly, or even daily if you have enough
    data.
  prefs: []
  type: TYPE_NORMAL
- en: More Large Language Model Support
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we could use Large Language Models (LLMs) for a while now in BERTopic,
    the v0.16 release has several smaller additions that make working with LLMs a
    nicer experience!
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum up, the following were added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*llama-cpp-python*](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html#llamacpp):
    Load any GGUF-compatible LLM with llama.cpp'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Truncate documents*](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html#truncating-documents):
    Use a variety of techniques to truncate documents when passing them to any LLM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LangChain*: Support for LCEL Runnables by [@joshuasundance-swca](https://github.com/joshuasundance-swca)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s explore a short example of the first two features, *llama.cpp* and *document
    truncation*.
  prefs: []
  type: TYPE_NORMAL
- en: When you pass documents to any LLM module, they might exceed its token limit.
    Instead, we can truncate the documents passed to the LLM by defining a `tokenizer`
    and a `doc_length`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5a2bd99fada7d93ba07dbbaf4f854106.png)'
  prefs: []
  type: TYPE_IMG
- en: The different methods for tokenization when truncating documents.
  prefs: []
  type: TYPE_NORMAL
- en: The definition of a `doc_length` depends on the tokenizer you use. For example,
    a value of 100 can refer to truncating by the number of tokens or even characters.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a1b08a2ba4009bd08970a4be14fe09f.png)'
  prefs: []
  type: TYPE_IMG
- en: Before documents are added to the prompt, they can be truncated first based
    on the tokenization strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use this together with `llama-cpp-python` , let’s consider the following
    example. First, we install the necessary packages, prepare the environment, and
    download a small but capable model ([Zephyr-7B](https://huggingface.co/TheBloke/zephyr-7B-alpha-GGUF)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Loading a GGUF model with `llama-cpp-python` in BERTopic is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: And that is it! We created a model that truncates input documents and creates
    interesting topic representations without being constrained by its token limit.
  prefs: []
  type: TYPE_NORMAL
- en: Thank You For Reading!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are, like me, passionate about *AI* and/or *Psychology*, please feel
    free to add me on [**LinkedIn**](https://www.linkedin.com/in/mgrootendorst/)and
    [**Twitter**](https://twitter.com/MaartenGr), or subscribe to my [**Newsletter**](http://maartengrootendorst.substack.com/).
    You can also find some of my content on my [**Personal Website**](https://maartengrootendorst.com/).
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://maartengrootendorst.substack.com/?source=post_page-----64d5eb3783d9--------------------------------)
    [## Exploring Language Models | Substack'
  prefs: []
  type: TYPE_NORMAL
- en: Writing about the intersection of AI, Language Models, and Psychology.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: maartengrootendorst.substack.com](https://maartengrootendorst.substack.com/?source=post_page-----64d5eb3783d9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*All images without a source credit were created by the author — Which means
    all of them, I like creating my own images ;)*'
  prefs: []
  type: TYPE_NORMAL
