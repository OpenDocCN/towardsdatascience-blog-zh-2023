- en: 'Muybridge Derby: Bringing Animal Locomotion Photographs to Life with AI'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/muybridge-derby-bringing-animal-locomotion-photographs-to-life-with-ai-b1918e6622ec](https://towardsdatascience.com/muybridge-derby-bringing-animal-locomotion-photographs-to-life-with-ai-b1918e6622ec)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How I used Midjourney and RunwayML to transform Eadweard Muybridge’s photo sequences
    into high-resolution videos
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://robgon.medium.com/?source=post_page-----b1918e6622ec--------------------------------)[![Robert
    A. Gonsalves](../Images/96b4da0f602a1cd9d1e1d2917868cbee.png)](https://robgon.medium.com/?source=post_page-----b1918e6622ec--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b1918e6622ec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b1918e6622ec--------------------------------)
    [Robert A. Gonsalves](https://robgon.medium.com/?source=post_page-----b1918e6622ec--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b1918e6622ec--------------------------------)
    ·16 min read·Jul 25, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/13a194ce4c3d4ddc03b88feb11f5e9fd.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Frame 2 of** [**Plate Number 626, Gallop**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)by
    Eadweard Muybridge(left), **Transformation using RunwayML’s Gen-1 Video Generator
    Based on a Midjourney Reference Image** (center and right), I*mages created using
    an AI image creation programs*'
  prefs: []
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I’m sure you’ve seen the series of images of a galloping horse by 19th-century
    English photographer Eadweard Muybridge. As a refresher, here is a GIF animation
    that shows one of his more famous photo series.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75eacf68bff18bc34a17bcac5652d108.png)'
  prefs: []
  type: TYPE_IMG
- en: '[**Plate Number 626, Gallop**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)by
    Eadweard Muybridge, Animated GIF by Author'
  prefs: []
  type: TYPE_NORMAL
- en: And here’s a portrait of Muybridge with an illustration of the apparatus he
    built to capture the photo series.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c9ec33691e714d02c8070506ca18cdf6.png)![](../Images/4a6856cb5f8cf5662885e37a3cd321b2.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Portrait of Eadweard Muybridge** (left) Image from [Wikimedia](https://commons.wikimedia.org/wiki/File:Optic_Projection_fig_411.jpg),
    **Muybridge’s Apparatus** (right), Image from [Wikimedia](https://commons.wikimedia.org/wiki/File:Portret_van_Eadweard_Muybridge_Eadweard_Muybridge_(1830,_%2B1904.)_(titel_op_object),_RP-F-2001-7-509-65.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Eadweard Muybridge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Muybridge was a nature photographer commissioned by the Governor of California,
    Leland Stanford, to document his mansion and possessions. Stanford posed an exciting
    challenge to Muybridge: could he take clear pictures of a galloping horse?'
  prefs: []
  type: TYPE_NORMAL
- en: 1872 was the year that Muybridge began his zealous involvement with motion photography.
    He was commissioned by Governor Leland Stanford to photograph the moving gait
    of his racehorse, Occident. Until this time the gait of a moving horse had been
    a mystery. When did the feet touch the ground? Did all four feet ever leave the
    ground at the same time? Painting the feet of the galloping horse had been an
    unsolved problem for artists. ... [He used] 12 cameras, each hooked to an electrical
    apparatus that would trip the shutters as the horse galloped past. … Muybridge
    invented the zoopraxiscope in 1879, a machine that allowed him to project up to
    two hundred single images on a screen. In 1880 he gave his first presentation
    of projected moving pictures on a screen to a group at the California School of
    Fine Arts, thus becoming the father of motion pictures.- Vi Whitmire [1]
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And Muybridge didn’t just take pictures of moving horses. He created similar
    sequences of moving cats, dogs, buffaloes, ostriches, people, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Muybridge Derby
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this project, I wanted to see if I could use AI systems to transform Muybridge’s
    animal locomotion photographs into high-resolution, full-color videos. After experimenting
    with various techniques, I changed the original sequences to be more realistic
    using a combination of [Midjourney](https://www.midjourney.com/home/) to create
    reference frames from text prompts and RunwayML’s Gen-1 Video Generator. For fun,
    I made a short animation, “Muybridge Derby,” showcasing the work. Here it is.
  prefs: []
  type: TYPE_NORMAL
- en: '**“Muybridge Derby,” based on Eadweard Muybridge’s Animal Locomotion Photographs,**
    Video by Author'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, I will describe how I transformed the locomotion
    sequences, generated the background scroll, and combined the elements to create
    the animation.
  prefs: []
  type: TYPE_NORMAL
- en: Using Midjourney to Generate Reference Frames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a prerequisite for transforming a Muybridge photo series into a high-def
    video, I generated a high-resolution reference frame using one of the original
    series’s photos and a text prompt in Midjourney.
  prefs: []
  type: TYPE_NORMAL
- en: For example, here is the prompt I used for generating the reference frame of
    the horse and jockey, “**a man wearing a blue cap, blue jacket, white pants, and
    black boots riding a brown horse with a white background -**- **ar 4:3**.”Note
    that the --ar 4:3 parameter indicates the aspect ratio of 4:3.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/33a83553394d80d2ab60b90f232c191f.png)![](../Images/e5f012484b2b5eacccbbe3ab47ec1534.png)![](../Images/a671a89c6fc9d7d946f9b4e8d42c7580.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Frame 2 of** [**Plate Number 626, Gallop**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)
    by Eadweard Muybridge(left), **Midjourney Thumbnails** by Author (center), **Selected
    Image, Retouched**, Image by Author (right)'
  prefs: []
  type: TYPE_NORMAL
- en: I pasted in a link to the image of Muybridge’s frame number 2 along with the
    prompt into Midjourney, and it produced four thumbnail images. All four generated
    images were pretty good. I liked the details and texture of the images, including
    the look of the jockey’s clothes and the shininess of the horse’s coat. None of
    them matched the original pose of the horse exactly, but I found out that it doesn’t
    matter when stylizing a video. The video stylizer in RunwayML only picks up on
    the general look of the image. I chose the thumbnail image at the lower right
    (outlined in green) and made some edits in Photoshop; I flipped the image horizontally,
    changed the hue of the horse to brown, and changed the style of the jockey’s cap.
  prefs: []
  type: TYPE_NORMAL
- en: I repeated this process for the other four animals in the animation, a cat,
    a buffalo, an elephant, and an ostrich. Here are the results. You can see an image
    from one of Muybridge’s photo series in the left column below. The middle column
    shows the results from Midjourney using the Muybridge image and the text, like
    “a full-color photo of a cat running on a dirt track, side view, -- ar 4:3.” The
    selected thumbnail is outlined in green. The right column shows the selected image,
    cleaned up a bit and flipped horizontally if needed, in Photoshop.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3b5a750615a94dd5cbe0198fe3b63491.png)![](../Images/be8fad3b7a7be282e4757abb0b9989cb.png)![](../Images/4a181c22443ee60521ce968f821085a2.png)![](../Images/0219b349907f7463307102218bcd0ae1.png)![](../Images/a96f2dc22ce8a0a13201fc7bf0c30005.png)![](../Images/f9ecc180c482ff7a4e51e487b09ec75f.png)![](../Images/3e1da164cfdc63d1624e724362c34262.png)![](../Images/142b5ee90f63e4a8cac47c91a0d11b3a.png)![](../Images/072b2fd8e72885a3990b97d4f3023e02.png)![](../Images/82f423f40f9d14b668ece23047c610a2.png)![](../Images/97e409525596b24f65ebdd9733792e9d.png)![](../Images/398b372ee6514ac8f491c8a7bc99c3b8.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Locomotion Photos of a** [**Cat**](https://www.royalacademy.org.uk/art-artists/work-of-art/cat-trotting-change-to-galloping)**,**
    [**Buffalo**](https://www.royalacademy.org.uk/art-artists/work-of-art/buffalo-galloping)**,**
    [**Elephant**](https://www.royalacademy.org.uk/art-artists/work-of-art/elephant-walking)**,
    and** [**Ostrich**](https://www.royalacademy.org.uk/art-artists/work-of-art/ostrich-running)**,**
    by Eadweard Muybridge(left), **Midjourney Thumbnails** by Author (center), **Chosen
    Midjourney Image,** by Author'
  prefs: []
  type: TYPE_NORMAL
- en: The Midjourney system did a great job generating the reference images. The details
    of the animals are amazing. You can click on any of the images to zoom in and
    see. Again, it didn’t precisely match the pose in the reference image, but the
    overall quality of the renderings was excellent. For more information on Midjourney,
    you can check out my earlier article [here](/exploring-midjourney-v4-for-creating-digital-art-4d20980a96f7).
  prefs: []
  type: TYPE_NORMAL
- en: Next, I will show you how I used reference images to transform the photo series
    with RunwayML.
  prefs: []
  type: TYPE_NORMAL
- en: RunwayML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Runway is a start-up company in New York City that researches and provides
    media creation and editing services that use Machine Learning (ML.) They are known
    as RunwayML because of the URL for their website, [runwayml.com](https://runwayml.com/).
    They offer a range of [subscription tiers](https://runwayml.com/pricing/) for
    their services at various price points: free, $12 per month, $28 per month, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a list of some of the services they provide:'
  prefs: []
  type: TYPE_NORMAL
- en: Super-Slow Motion - Transform video to have super smooth motion
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Video-to-Video Editing - Change the style of a video with text or images
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove Background - Remove, blur, or replace the video background
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Text-to-Video Generation - Generate videos with text prompts
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Image-to-Image Editing - Transform images with text prompts
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I used the first three to stylize the Muybridge sequences in my video.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a8a8f3f6824bfb049a0d3fdf4ac7a3ec.png)'
  prefs: []
  type: TYPE_IMG
- en: '[**Plate Number 626, Gallop**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)**,**
    by Eadweard Muybridge, **Stylized by RunwayML**, Animation by Author'
  prefs: []
  type: TYPE_NORMAL
- en: RunwayML’s Video-to-video Editing Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RunwayML’s video-to-video editing service allows users to upload an input video
    and provide either text or a reference image as a prompt. The ML model will then
    “edit” the footage by imposing the style specified in the prompt while keeping
    the primary elements of the input video intact. The process is written up in their
    paper, *Structure and Content-Guided Video Synthesis with Diffusion Models* [2].
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we present a structure and content-guided video diffusion model
    that edits videos based on visual or textual descriptions of the desired output.
    Conflicts between user-provided content edits and structure representations occur
    due to insufficient disentanglement between the two aspects. As a solution, we
    show that training on monocular depth estimates with varying levels of detail
    provides control over structure and content fidelity. … We find that depth estimates
    extracted from input video frames provide the desired properties as they encode
    significantly less content information compared to simpler structure representations.
    — P. Esser et al, RunwayML
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note that “monocular depth estimates” refers to a depth map, where the values
    of the pixels indicate the distance from the camera to the surface of the objects
    depicted in the scene. To get the depth estimates, they used another ML model
    from a group of European researchers [3]. The model is called MiDaS (which, I
    guess, is a backronym for **M**onocular **D**epth e**S**timator?) The MiDaS system
    was trained on a dataset of 3D movie scenes, like the shots below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd71874f5aaabadebd55c2f6f582ba6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Sample Images from the 3D Movies Dataset**, Image from the [MiDaS Paper](https://arxiv.org/pdf/1907.01341.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: You can see how the light yellow colors in the depth map show the closer points
    in the scene, and the dark blue colors show the more distant points in the background.
    The trained MiDaS model can estimate depth maps from any input model. Here are
    some results from the paper.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/78e6690fdbbc805168fe1cfeb8afb342.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Predicted Depth Maps using MiDaS**, Image from the [MiDaS Paper](https://arxiv.org/pdf/1907.01341.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: You can see how the MiDaS model does an excellent job of estimating depths.
    For example, you can see how the dog’s tail is well-defined as it stands out from
    the stream behind it.
  prefs: []
  type: TYPE_NORMAL
- en: RunwayML’s video-to-video model uses the predicted depth maps of the input video
    to condition a diffusion video generation model directed by a text prompt or reference
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Our latent video diffusion model synthesizes new videos given structure and
    content information. We ensure structural consistency by conditioning on depth
    estimates while content is controlled with images or natural language. Temporally
    stable results are achieved with additional temporal connections in the model
    and joint image and video training. — P. Esser et al, RunwayML
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You can see some video editing results with text prompts from the paper below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/312e621313470a1d5ba79849fcf506a7.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Results for RunwayML’s Image-to-Video-Editing with Text**, Image from [RunwayML’s
    Paper](https://arxiv.org/pdf/2302.03011.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The various styles, pencil sketch, anime, and low-poly render, transform the
    input video to create the output. And you can see the consistency of applying
    the style from frame to frame. Below are some examples from the paper that use
    image prompts to stylize video.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/19e1263184b7e0c3050c3a06a193280b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Results for RunwayML’s Image-to-Video-Editing with Reference Images**, Image
    from [RunwayML’s Paper](https://arxiv.org/pdf/2302.03011.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Again, you can see how the color palette and the look of the prompt image transform
    the video to the indicated style. And details from the generated frames are rendered
    consistently for the final video.
  prefs: []
  type: TYPE_NORMAL
- en: Using RunwayML’s Video-to-video Editing Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To use the service, I [created an account](https://app.runwayml.com/login) and
    logged in. As mentioned above, you can use the free version, which has limitations,
    like generating videos up to only four seconds. I opted to pay US$12 per month,
    which allows me to create videos up to 15 seconds and [other benefits](https://runwayml.com/pricing/).
  prefs: []
  type: TYPE_NORMAL
- en: 'I shot a brief shadow puppet clip of a rabbit to test the system, cleaned it
    up with an editing system, and uploaded it to RunwayML. Then I chose the Gen-1:
    Video to Video tool. I loaded the clip, typed in the prompt, “photorealistic rabbit
    with floppy ears in a field,” and hit the **Preview Styles** button. The system
    thought about it a bit and rendered four thumbnails. You can see them at the bottom
    of the screenshot below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b11962e741f5e333a01e196d3204089.png)'
  prefs: []
  type: TYPE_IMG
- en: '**RunwayML Gen-1: Video to Video Screen**, Image by Author'
  prefs: []
  type: TYPE_NORMAL
- en: All four thumbnails looked good. They all followed the form of the shadow puppet
    but with a realistic rabbit entering the frame. I chose the third one and hit
    **Generate Video**. It took about 20 minutes to render the video. I also created
    one with the prompt, “2D animated rabbit with floppy ears in a field.” You can
    see the results below, the original shadow puppet video, and my cleaned-up version
    for reference.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4edc4a191c9e8324a707991350088ab7.png)![](../Images/25f4375ebc7c6c32c049df2d560ad9ae.png)![](../Images/629fece34a73dbe140667f8eaf03f796.png)![](../Images/5dc768435388f93b2f82f72e9d40d4d6.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Original Shadow Puppet Video** (upper left), **Cleaned-up Shadow Puppet Video**
    (upper right), **Video Stylized with RunwayML using the prompt “photorealistic
    …”** (lower left), and **Video Stylized with RunwayML using the prompt “2D animation
    …”**, (lower right), Videos by Author'
  prefs: []
  type: TYPE_NORMAL
- en: The generated videos came out nice! The photorealistic one in the lower left
    looks the best, with lovely details in the rabbit’s eyes, ears, and nose. The
    2D animation render is a bit off. The system seems confused about which ear is
    which, and the background is less interesting. Next, I tried the same experiment
    with two reference images generated in Midjourney.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b091e18161db3b91bd6d305f81fbed81.png)![](../Images/4df4310a1769d85687ac041f4d0ad6e7.png)![](../Images/6b85cc56b398faee5770bba60c4163d1.png)![](../Images/809f1515cb4259eb711fbcf52d32ea5d.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Midjourney Image using the prompt “photorealistic …”** (upper left), **Midjourney
    Image using the prompt “2D animation …”** (upper right), **Video Stylized with
    RunwayML using the photorealistic reference,** (lower left), and **Video Stylized
    with RunwayML using the animation reference,** (lower right), Videos by Author'
  prefs: []
  type: TYPE_NORMAL
- en: These came out nice, too. They both picked up the style from the reference image
    while following the shapes and motion in the original shadow puppet video. The
    one on the right has a strange effect coming in from the right, however. It almost
    looks like a sun flare. Notice how both generated animations show details from
    the background in the reference frames, like the nice clouds on the right. But
    the foreground forms from the reference frames are missing, like the wheat grains
    on the left and the tree on the right. This is probably due to the use of depth
    images in the training data that RunwayML used. It shows my hand movements transformed
    into bunnies as the foreground imagery but kept elements of the reference image
    for the background, like the field and sky.
  prefs: []
  type: TYPE_NORMAL
- en: Bringing Muybridge’s Photos to Life
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I used the RunwayML technique described above with a minor variation to transform
    the original image sequences from Muybridge to high-res versions.
  prefs: []
  type: TYPE_NORMAL
- en: Super Slomo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because the animals ran quickly in Muybridge’s experiments, there is a lot of
    motion between frames. For example, here are three frames of the horse sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/60779274859ba1ecc20b6ce18561dca1.png)![](../Images/d865c1f5d3407587f3c09e4b2d67563b.png)![](../Images/36c66e53bb7d36ef3aff636f527a8dd1.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Frames 2, 3, and 4 of** [**Plate Number 626, Gallop**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)
    by Eadweard Muybridge'
  prefs: []
  type: TYPE_NORMAL
- en: Notice how much motion is seen in the horse’s legs between frames. The results
    were not very good when I experimented with the video-to-video stylization with
    fast-moving animation. My solution was first to slow down the motion by a factor
    of 2 using RunwayML’s **Super-Slow Motion** feature, then apply the transformation,
    and finally speed the resultant video up by a factor of two.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s what the slowed-down video looks like.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/60779274859ba1ecc20b6ce18561dca1.png)![](../Images/813e00ad13b7f2ae2c694bb213b5608f.png)![](../Images/d865c1f5d3407587f3c09e4b2d67563b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Frames 2** (left) **and 3** (right) **of** [**Plate Number 626, Gallop**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)
    by Eadweard Muybridge, **RunwayML Frame Interpolation** (center) by Author'
  prefs: []
  type: TYPE_NORMAL
- en: You can see less motion between frames, especially with the horse’s legs. Here’s
    what the original horse sequence looks like compared to the 50% slow-motion version.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75eacf68bff18bc34a17bcac5652d108.png)![](../Images/fad48e4ace87d9e47314f5aa6f2233a5.png)'
  prefs: []
  type: TYPE_IMG
- en: '[**Plate Number 626, Gallop**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)by
    Eadweard Muybridge animated by Author (left), **RunwayML’s 50% Super-Slow Motion,**
    animation by Author'
  prefs: []
  type: TYPE_NORMAL
- en: The system did an excellent job with motion interpolation. In general, the motion
    is smoother with the RunwayML’s super-slow motion. There is a little hiccup in
    the action when the sequence resets, but it will get masked when I speed the transformed
    videos up by a factor of two.
  prefs: []
  type: TYPE_NORMAL
- en: '**Video-to-Video Transformations**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I first uploaded the slowed-down horse animation into RunwayML to create the
    transformed video and then chose the **Gen-1: Video to Video** tool. I selected
    the **Image Style** **reference** and uploaded my reference frame for the horse
    created with Midjourney. Several settings are available for the transformation,
    including the following.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Style: Structural consistency** - Higher values make the output more structurally
    different from the input video.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Style: Weight** - Higher values emphasize matching the style rather than
    the input video.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Frame Consistency** - Values below 1 give decreased consistency across time;
    values above 1 increase how closely frames relate to prior frames.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can see some examples of varying these settings on RunwayML’s [help page](https://help.runwayml.com/hc/en-us/articles/15161225169171-Gen-1-Advanced-Settings).
    I experimented with these settings but used the defaults, Structural Consistency
    of 2, Weight of 8.5, and Frame Consistency of 1.
  prefs: []
  type: TYPE_NORMAL
- en: I then clicked **Preview styles,** and it displayed four options at the bottom.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f527b973d7f1a88fc86c4f606ab8d9e1.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Screenshot of RunwayML’s Gen-1: Video to Video Feature**, Image by Author'
  prefs: []
  type: TYPE_NORMAL
- en: I chose the third preview and hit the **Generate video** button. Here is the
    reference image, the original horse sequence, and the stylized animation sped
    up by a factor of two to match the initial speed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57c42c2174be01d41b7a2db17e7b55bc.png)![](../Images/b761027e0eaab13a844c639c6d76106f.png)![](../Images/30b9351e38f1889a940737c6ce288314.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Midjoruney Reference Image** by Author (left), [**Plate Number 626, Gallop**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)by
    Eadweard Muybridge animated by Author (center), **Animation Stylized with RunwayML**
    by Author (right)'
  prefs: []
  type: TYPE_NORMAL
- en: This came out well! You can see how the style from the reference image got imposed
    on the original Muybridge animation while keeping the motion of the horse and
    jockey intact. The system also performed an ML-based video resize to bring the
    final video up to 640x480, which brought in some nice details. Note that the system
    has an Upscale setting, which would double the resolution horizontally and vertically.
  prefs: []
  type: TYPE_NORMAL
- en: I performed the same operations on the other four image sequences. You can see
    the results below, with the reference frames from Midjourney, Muybridge's original
    animal photo sequences, and the stylized videos by RunwayML.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4a181c22443ee60521ce968f821085a2.png)![](../Images/fb301fc8d7f5af37276b3e47d25d2736.png)![](../Images/01a8769a7800547ce3be7cd932b0799b.png)![](../Images/0ae712c5397cfc01b8a5e4e92d2355bb.png)![](../Images/3ee35d9bc47c82ea89b038263a020f57.png)![](../Images/e1aa014505828df4f357beeb80240dcc.png)![](../Images/fcfc2b4e4ce180c64836e5ffd521d10e.png)![](../Images/bab698fe2d0daf00bf15ce4985d9e52a.png)![](../Images/a50fac88f71945520144b9995ed8b9ff.png)![](../Images/96214d9a39a53cc289044e804c95dd2a.png)![](../Images/65334658f4100f2a64ed9b7b8afd5fe4.png)![](../Images/48aae389c3f9ad66e2551e456b9acd01.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Midjoruney Reference Images** by Author (left), **Animal Locomotion Studies**
    by Eadweard Muybridge animated by Author (center), **Animations Stylized with
    RunwayML** by Author (right)'
  prefs: []
  type: TYPE_NORMAL
- en: These look great, too! Like the horse animation, the RunwayML model picked up
    the textures and coloring from the reference image and applied them to the original
    animations while keeping the motion intact. The backgrounds in the new animations
    didn’t scroll right to the left, however. But this was not a problem. You can
    see in the next section how I created an “alpha mask” to keep just the foreground
    imagery and composite the animals over a new background.
  prefs: []
  type: TYPE_NORMAL
- en: Removing Background Imagery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I used RunwayML’s **Remove Background** feature to replace the running animal
    clips' backgrounds. I loaded the original video clip from Muybridge’s photos and
    used the cursor to select two points, the horse and the jockey’s leg. The system
    thought a bit, then showed the chosen area in green, as you can see in the screenshot
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad4a2d6bb6ea38908c0eddb41192c348.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Screenshot of RunwayML’s Remove Background Feature**, Image by Author'
  prefs: []
  type: TYPE_NORMAL
- en: The system shows how it selected the foreground for all of the frames in the
    video, and I could play it as a preview. It did an excellent job that didn’t require
    much work on my part. I then saved the alpha matte as a video for my compositing
    app.
  prefs: []
  type: TYPE_NORMAL
- en: I created a still image of a derby stadium in Midjourney and used it as a scrolling
    background for my animation. Wherever the matte is black, it will show the background
    (the stadium); wherever it is white, it will show the foreground (the horse and
    jockey.) Here is the stylized clip of the horse, the alpha matte, and the final
    result.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/959918e566271716b28bd1f1cc94b504.png)![](../Images/7a7b0ef1ce36b97ca34f351145930d9b.png)![](../Images/8bb6fee237df623e45aac9a1d569f5fb.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Animation Stylized with RunwayML** (left), **Alpha Matte** (center), and
    **Final Results** (right), Animations by Author'
  prefs: []
  type: TYPE_NORMAL
- en: In my compositing program, I had to clean up the alpha matte a bit. For example,
    I blurred the tail to make it look more like hair and not a solid object. You
    can see how the scrolling background helps sell the effect that the horse is running
    forward, which you don’t see in the original stylized animation.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the final animation again, this time a bit bigger, so you can check
    out the details.
  prefs: []
  type: TYPE_NORMAL
- en: '**“Muybridge Derby,” based on Eadweard Muybridge’s Animal Locomotion Photographs,**
    Video by Author'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to see the animation on a large screen, it will be shown at [*The
    Next Big Thing*](https://studiochannelislands.org/nbt23/), August 5 to September
    30, 2023, at the Studio Channel Islands Art Center in Camarillo, CA.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I enjoyed working with the Muybridge images and using Midjourney and the tools
    at RunwayML to generate and modify media for this project. If you are familiar
    with my writing on Medium, you know I like to try new production methods, but
    I don’t always create a finished piece. So it was satisfying for me to bring multiple
    elements together. As a “deep cut,” I used a song I generated with AI for a previous
    article as the music played over the credits. It’s called “I’ll Get There When
    I Get There,” which is kinda appropriate for a derby race. 😄
  prefs: []
  type: TYPE_NORMAL
- en: '[](/ai-tunes-creating-new-songs-with-artificial-intelligence-4fb383218146?source=post_page-----b1918e6622ec--------------------------------)
    [## AI-Tunes: Creating New Songs with Artificial Intelligence'
  prefs: []
  type: TYPE_NORMAL
- en: How I fine-tuned OpenAI’s GPT-3 to generate music with a global structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/ai-tunes-creating-new-songs-with-artificial-intelligence-4fb383218146?source=post_page-----b1918e6622ec--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Ownership of Inputs and Generated Media
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Midjourney and RunwayML have different policies regarding the imagery and text
    used for prompts and the resulting generated images. Midjourney distinguishes
    between paid and free users, whereas RunwayML uses the same policy for both types
    of users.
  prefs: []
  type: TYPE_NORMAL
- en: Midjourney Terms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: According to Midjourney’s [terms of service](https://docs.midjourney.com/docs/terms-of-service),
    users of the free service tier do not own their generated images. Midjourney owns
    them. The images are licensed back to the non-paying users for noncommercial purposes
    under the Creative Commons Noncommercial 4.0 Attribution International License.
    Users of the paid service own their generated images, which can be used commercially.
  prefs: []
  type: TYPE_NORMAL
- en: If you are working for a big company, there are additional limitations.
  prefs: []
  type: TYPE_NORMAL
- en: If You are an employee or owner of a company with more than $1,000,000 USD a
    year in gross revenue and You are using the Services on behalf of Your employer,
    You must purchase a “Pro” or “Mega” membership for every individual accessing
    the Services on Your behalf in order to own Assets You create. If You are not
    sure whether Your use qualifies as on behalf of Your employer, please assume it
    does. — Midjourney Terms of Service
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: FYI, the Pro plan costs US$60 per person per month, and the Mega monthly charge
    is US$120.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, under the terms, all users grant a license to Midjourney to use
    any text prompts, uploaded images used as prompts, and generated images for any
    purpose, including for training future versions of their models.
  prefs: []
  type: TYPE_NORMAL
- en: RunwayML Terms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: According to RunwayML’s [terms of use](https://runwayml.com/terms-of-use/),
    all users own and can use their generated content commercially. However, all users
    grant a license to RunwayML to use their inputs and outputs for any purpose, including
    training future model versions.
  prefs: []
  type: TYPE_NORMAL
- en: License Terms for Images and Animations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I am releasing the images and animations for this project under the Creative
    Commons Attribution Sharealike license.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be9434bf14bc4eab9a454950cbe4a9c5.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Creative Commons Attribution Sharealike**'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I want to thank Jennifer Lim for reviewing the article and providing feedback.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] V. Whitmire, International Photography Hall of Fame: Eadweard Muybridge
    *(2017)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] P. Esser et al. [Structure and Content-Guided Video Synthesis with Diffusion
    Models](https://arxiv.org/pdf/2302.03011.pdf) (2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] R. Ranftl et al. [Towards Robust Monocular Depth Estimation: Mixing Datasets
    for Zero-shot Cross-dataset Transfer](https://arxiv.org/pdf/1907.01341.pdf) (2020)'
  prefs: []
  type: TYPE_NORMAL
