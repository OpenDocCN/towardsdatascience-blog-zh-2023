- en: 'Muybridge Derby: Bringing Animal Locomotion Photographs to Life with AI'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/muybridge-derby-bringing-animal-locomotion-photographs-to-life-with-ai-b1918e6622ec](https://towardsdatascience.com/muybridge-derby-bringing-animal-locomotion-photographs-to-life-with-ai-b1918e6622ec)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How I used Midjourney and RunwayML to transform Eadweard Muybridge’s photo sequences
    into high-resolution videos
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://robgon.medium.com/?source=post_page-----b1918e6622ec--------------------------------)[![Robert
    A. Gonsalves](../Images/96b4da0f602a1cd9d1e1d2917868cbee.png)](https://robgon.medium.com/?source=post_page-----b1918e6622ec--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b1918e6622ec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b1918e6622ec--------------------------------)
    [Robert A. Gonsalves](https://robgon.medium.com/?source=post_page-----b1918e6622ec--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b1918e6622ec--------------------------------)
    ·16 min read·Jul 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/13a194ce4c3d4ddc03b88feb11f5e9fd.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: '**Frame 2 of** [**Plate Number 626, Gallop**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)by
    Eadweard Muybridge(left), **Transformation using RunwayML’s Gen-1 Video Generator
    Based on a Midjourney Reference Image** (center and right), I*mages created using
    an AI image creation programs*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Background
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I’m sure you’ve seen the series of images of a galloping horse by 19th-century
    English photographer Eadweard Muybridge. As a refresher, here is a GIF animation
    that shows one of his more famous photo series.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75eacf68bff18bc34a17bcac5652d108.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: '[**Plate Number 626, Gallop**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)by
    Eadweard Muybridge, Animated GIF by Author'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: And here’s a portrait of Muybridge with an illustration of the apparatus he
    built to capture the photo series.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c9ec33691e714d02c8070506ca18cdf6.png)![](../Images/4a6856cb5f8cf5662885e37a3cd321b2.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: '**Portrait of Eadweard Muybridge** (left) Image from [Wikimedia](https://commons.wikimedia.org/wiki/File:Optic_Projection_fig_411.jpg),
    **Muybridge’s Apparatus** (right), Image from [Wikimedia](https://commons.wikimedia.org/wiki/File:Portret_van_Eadweard_Muybridge_Eadweard_Muybridge_(1830,_%2B1904.)_(titel_op_object),_RP-F-2001-7-509-65.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Eadweard Muybridge
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Muybridge was a nature photographer commissioned by the Governor of California,
    Leland Stanford, to document his mansion and possessions. Stanford posed an exciting
    challenge to Muybridge: could he take clear pictures of a galloping horse?'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 1872 was the year that Muybridge began his zealous involvement with motion photography.
    He was commissioned by Governor Leland Stanford to photograph the moving gait
    of his racehorse, Occident. Until this time the gait of a moving horse had been
    a mystery. When did the feet touch the ground? Did all four feet ever leave the
    ground at the same time? Painting the feet of the galloping horse had been an
    unsolved problem for artists. ... [He used] 12 cameras, each hooked to an electrical
    apparatus that would trip the shutters as the horse galloped past. … Muybridge
    invented the zoopraxiscope in 1879, a machine that allowed him to project up to
    two hundred single images on a screen. In 1880 he gave his first presentation
    of projected moving pictures on a screen to a group at the California School of
    Fine Arts, thus becoming the father of motion pictures.- Vi Whitmire [1]
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 1872年是马依布里奇开始热衷于运动摄影的年份。他受加州州长利兰·斯坦福委托，拍摄了他的赛马“西方”的运动步态。在此之前，马的运动步态仍是个谜。马蹄何时触地？四只脚是否曾同时离开地面？绘制奔跑中的马蹄一直是艺术家的难题。...
    [他使用] 12台相机，每台相机连接到一个电器装置，当马奔跑经过时，装置会触发快门。 … 马依布里奇于1879年发明了zoopraxiscope，这是一台可以将多达两百张单独图像投影到屏幕上的机器。1880年，他首次向加州美术学院的一群人展示了投影移动图像，从而成为电影之父。-
    维·惠特迈尔 [1]
- en: And Muybridge didn’t just take pictures of moving horses. He created similar
    sequences of moving cats, dogs, buffaloes, ostriches, people, etc.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 马依布里奇不仅拍摄了移动的马。他还拍摄了移动的猫、狗、水牛、鸵鸟、人物等类似序列。
- en: Muybridge Derby
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马依布里奇德比
- en: For this project, I wanted to see if I could use AI systems to transform Muybridge’s
    animal locomotion photographs into high-resolution, full-color videos. After experimenting
    with various techniques, I changed the original sequences to be more realistic
    using a combination of [Midjourney](https://www.midjourney.com/home/) to create
    reference frames from text prompts and RunwayML’s Gen-1 Video Generator. For fun,
    I made a short animation, “Muybridge Derby,” showcasing the work. Here it is.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我想看看是否可以使用AI系统将马依布里奇的动物运动照片转换为高清晰度全彩视频。在尝试了各种技术后，我使用[Midjourney](https://www.midjourney.com/home/)通过文本提示生成参考帧，并结合RunwayML的Gen-1视频生成器，使原始序列变得更为逼真。为了好玩，我制作了一个短动画，“马依布里奇德比”，展示了这一工作。这里就是。
- en: '**“Muybridge Derby,” based on Eadweard Muybridge’s Animal Locomotion Photographs,**
    Video by Author'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**“马依布里奇德比，”基于爱德华·马依布里奇的动物运动照片，** 作者制作的视频'
- en: In the following sections, I will describe how I transformed the locomotion
    sequences, generated the background scroll, and combined the elements to create
    the animation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我将描述如何转换运动序列、生成背景滚动以及将这些元素结合起来创建动画。
- en: Using Midjourney to Generate Reference Frames
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Midjourney生成参考帧
- en: As a prerequisite for transforming a Muybridge photo series into a high-def
    video, I generated a high-resolution reference frame using one of the original
    series’s photos and a text prompt in Midjourney.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作为将马依布里奇照片系列转换为高清晰度视频的前提，我使用了原始系列的一张照片和Midjourney中的文本提示生成了高分辨率参考帧。
- en: For example, here is the prompt I used for generating the reference frame of
    the horse and jockey, “**a man wearing a blue cap, blue jacket, white pants, and
    black boots riding a brown horse with a white background -**- **ar 4:3**.”Note
    that the --ar 4:3 parameter indicates the aspect ratio of 4:3.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这里是我用于生成马和骑手参考帧的提示，“**一个戴着蓝色帽子、穿着蓝色夹克、白色裤子和黑色靴子的男人骑着一匹棕色马，背景为白色 -**- **ar
    4:3**。”请注意，--ar 4:3参数表示4:3的宽高比。
- en: '![](../Images/33a83553394d80d2ab60b90f232c191f.png)![](../Images/e5f012484b2b5eacccbbe3ab47ec1534.png)![](../Images/a671a89c6fc9d7d946f9b4e8d42c7580.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33a83553394d80d2ab60b90f232c191f.png)![](../Images/e5f012484b2b5eacccbbe3ab47ec1534.png)![](../Images/a671a89c6fc9d7d946f9b4e8d42c7580.png)'
- en: '**Frame 2 of** [**Plate Number 626, Gallop**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)
    by Eadweard Muybridge(left), **Midjourney Thumbnails** by Author (center), **Selected
    Image, Retouched**, Image by Author (right)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**[**第626号板画的**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)**
    **第二帧**（左），**Midjourney 缩略图**（中），**精选图像，修饰后**，作者提供的图像（右）'
- en: I pasted in a link to the image of Muybridge’s frame number 2 along with the
    prompt into Midjourney, and it produced four thumbnail images. All four generated
    images were pretty good. I liked the details and texture of the images, including
    the look of the jockey’s clothes and the shininess of the horse’s coat. None of
    them matched the original pose of the horse exactly, but I found out that it doesn’t
    matter when stylizing a video. The video stylizer in RunwayML only picks up on
    the general look of the image. I chose the thumbnail image at the lower right
    (outlined in green) and made some edits in Photoshop; I flipped the image horizontally,
    changed the hue of the horse to brown, and changed the style of the jockey’s cap.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我将穆布里奇第2帧的图像链接和提示粘贴到 Midjourney 中，它生成了四个缩略图。所有四张生成的图像都很不错。我喜欢这些图像的细节和质感，包括骑师的衣服和马的毛发光泽。虽然它们都没有完全匹配原始马的姿势，但我发现当对视频进行风格化时，这并不重要。RunwayML
    中的视频风格化器仅捕捉图像的一般外观。我选择了右下角（以绿色轮廓标出）的缩略图，并在 Photoshop 中进行了一些编辑；我将图像水平翻转，将马的颜色更改为棕色，并改变了骑师帽子的风格。
- en: I repeated this process for the other four animals in the animation, a cat,
    a buffalo, an elephant, and an ostrich. Here are the results. You can see an image
    from one of Muybridge’s photo series in the left column below. The middle column
    shows the results from Midjourney using the Muybridge image and the text, like
    “a full-color photo of a cat running on a dirt track, side view, -- ar 4:3.” The
    selected thumbnail is outlined in green. The right column shows the selected image,
    cleaned up a bit and flipped horizontally if needed, in Photoshop.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我对动画中的其他四种动物重复了这个过程，包括一只猫、一只水牛、一只大象和一只鸵鸟。以下是结果。你可以在下面的左列看到穆布里奇照片系列中的一张图片。中间列显示了使用穆布里奇图像和文本（如“跑步中的猫的全彩照片，侧面视图，--
    ar 4:3”）从 Midjourney 得到的结果。所选缩略图以绿色轮廓标出。右列展示了经过稍微清理和在 Photoshop 中水平翻转（如有需要）的所选图像。
- en: '![](../Images/3b5a750615a94dd5cbe0198fe3b63491.png)![](../Images/be8fad3b7a7be282e4757abb0b9989cb.png)![](../Images/4a181c22443ee60521ce968f821085a2.png)![](../Images/0219b349907f7463307102218bcd0ae1.png)![](../Images/a96f2dc22ce8a0a13201fc7bf0c30005.png)![](../Images/f9ecc180c482ff7a4e51e487b09ec75f.png)![](../Images/3e1da164cfdc63d1624e724362c34262.png)![](../Images/142b5ee90f63e4a8cac47c91a0d11b3a.png)![](../Images/072b2fd8e72885a3990b97d4f3023e02.png)![](../Images/82f423f40f9d14b668ece23047c610a2.png)![](../Images/97e409525596b24f65ebdd9733792e9d.png)![](../Images/398b372ee6514ac8f491c8a7bc99c3b8.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b5a750615a94dd5cbe0198fe3b63491.png)![](../Images/be8fad3b7a7be282e4757abb0b9989cb.png)![](../Images/4a181c22443ee60521ce968f821085a2.png)![](../Images/0219b349907f7463307102218bcd0ae1.png)![](../Images/a96f2dc22ce8a0a13201fc7bf0c30005.png)![](../Images/f9ecc180c482ff7a4e51e487b09ec75f.png)![](../Images/3e1da164cfdc63d1624e724362c34262.png)![](../Images/142b5ee90f63e4a8cac47c91a0d11b3a.png)![](../Images/072b2fd8e72885a3990b97d4f3023e02.png)![](../Images/82f423f40f9d14b668ece23047c610a2.png)![](../Images/97e409525596b24f65ebdd9733792e9d.png)![](../Images/398b372ee6514ac8f491c8a7bc99c3b8.png)'
- en: '**Locomotion Photos of a** [**Cat**](https://www.royalacademy.org.uk/art-artists/work-of-art/cat-trotting-change-to-galloping)**,**
    [**Buffalo**](https://www.royalacademy.org.uk/art-artists/work-of-art/buffalo-galloping)**,**
    [**Elephant**](https://www.royalacademy.org.uk/art-artists/work-of-art/elephant-walking)**,
    and** [**Ostrich**](https://www.royalacademy.org.uk/art-artists/work-of-art/ostrich-running)**,**
    by Eadweard Muybridge(left), **Midjourney Thumbnails** by Author (center), **Chosen
    Midjourney Image,** by Author'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**运动照片的** [**猫**](https://www.royalacademy.org.uk/art-artists/work-of-art/cat-trotting-change-to-galloping)**、**
    [**水牛**](https://www.royalacademy.org.uk/art-artists/work-of-art/buffalo-galloping)**、**
    [**大象**](https://www.royalacademy.org.uk/art-artists/work-of-art/elephant-walking)**
    和** [**鸵鸟**](https://www.royalacademy.org.uk/art-artists/work-of-art/ostrich-running)**，**由伊德华德·穆布里奇（左），**Midjourney
    缩略图**（中），**所选 Midjourney 图像，**（作者）'
- en: The Midjourney system did a great job generating the reference images. The details
    of the animals are amazing. You can click on any of the images to zoom in and
    see. Again, it didn’t precisely match the pose in the reference image, but the
    overall quality of the renderings was excellent. For more information on Midjourney,
    you can check out my earlier article [here](/exploring-midjourney-v4-for-creating-digital-art-4d20980a96f7).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Midjourney 系统在生成参考图像方面表现出色。动物的细节令人惊叹。你可以点击任何一张图片来放大查看。虽然它没有完全匹配参考图像中的姿势，但整体渲染质量非常优秀。有关
    Midjourney 的更多信息，你可以查看我之前的文章[这里](/exploring-midjourney-v4-for-creating-digital-art-4d20980a96f7)。
- en: Next, I will show you how I used reference images to transform the photo series
    with RunwayML.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将展示如何利用参考图像使用 RunwayML 转换照片系列。
- en: RunwayML
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RunwayML
- en: 'Runway is a start-up company in New York City that researches and provides
    media creation and editing services that use Machine Learning (ML.) They are known
    as RunwayML because of the URL for their website, [runwayml.com](https://runwayml.com/).
    They offer a range of [subscription tiers](https://runwayml.com/pricing/) for
    their services at various price points: free, $12 per month, $28 per month, etc.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 'Runway是一家位于纽约市的初创公司，研究并提供使用机器学习（ML）的媒体创建和编辑服务。他们因其网站的URL[runwayml.com](https://runwayml.com/)而被称为RunwayML。他们提供不同价格点的[订阅层级](https://runwayml.com/pricing/):
    免费、每月$12、每月$28等。'
- en: 'Here is a list of some of the services they provide:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这是他们提供的一些服务的列表：
- en: Super-Slow Motion - Transform video to have super smooth motion
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 超慢动作 - 将视频转换为超平滑的运动
- en: Video-to-Video Editing - Change the style of a video with text or images
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 视频对视频编辑 - 使用文本或图像更改视频风格
- en: Remove Background - Remove, blur, or replace the video background
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除背景 - 移除、模糊或替换视频背景
- en: Text-to-Video Generation - Generate videos with text prompts
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本对视频生成 - 使用文本提示生成视频
- en: Image-to-Image Editing - Transform images with text prompts
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像对图像编辑 - 使用文本提示转换图像
- en: I used the first three to stylize the Muybridge sequences in my video.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了前三个来为我的视频中的Muybridge序列进行风格化。
- en: '![](../Images/a8a8f3f6824bfb049a0d3fdf4ac7a3ec.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8a8f3f6824bfb049a0d3fdf4ac7a3ec.png)'
- en: '[**Plate Number 626, Gallop**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)**,**
    by Eadweard Muybridge, **Stylized by RunwayML**, Animation by Author'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[**626号牌，奔马**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)**，**由Eadweard
    Muybridge创作，**由RunwayML风格化**，作者动画'
- en: RunwayML’s Video-to-video Editing Model
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RunwayML的视频对视频编辑模型
- en: RunwayML’s video-to-video editing service allows users to upload an input video
    and provide either text or a reference image as a prompt. The ML model will then
    “edit” the footage by imposing the style specified in the prompt while keeping
    the primary elements of the input video intact. The process is written up in their
    paper, *Structure and Content-Guided Video Synthesis with Diffusion Models* [2].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: RunwayML的视频对视频编辑服务允许用户上传输入视频并提供文本或参考图像作为提示。然后，机器学习模型将通过施加提示中指定的风格来“编辑”镜头，同时保持输入视频的主要元素完整。该过程在他们的论文中有详细描述，*结构和内容引导的视频合成与扩散模型*
    [2]。
- en: In this work, we present a structure and content-guided video diffusion model
    that edits videos based on visual or textual descriptions of the desired output.
    Conflicts between user-provided content edits and structure representations occur
    due to insufficient disentanglement between the two aspects. As a solution, we
    show that training on monocular depth estimates with varying levels of detail
    provides control over structure and content fidelity. … We find that depth estimates
    extracted from input video frames provide the desired properties as they encode
    significantly less content information compared to simpler structure representations.
    — P. Esser et al, RunwayML
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这项工作中，我们展示了一种结构和内容引导的视频扩散模型，该模型根据对期望输出的视觉或文本描述编辑视频。由于两个方面之间的解耦不足，用户提供的内容编辑与结构表示之间会发生冲突。作为解决方案，我们展示了在不同细节水平的单眼深度估计上进行训练提供了对结构和内容保真的控制。……我们发现从输入视频帧提取的深度估计提供了所需的属性，因为它们编码的内容信息显著少于更简单的结构表示。——
    P. Esser 等人，RunwayML
- en: Note that “monocular depth estimates” refers to a depth map, where the values
    of the pixels indicate the distance from the camera to the surface of the objects
    depicted in the scene. To get the depth estimates, they used another ML model
    from a group of European researchers [3]. The model is called MiDaS (which, I
    guess, is a backronym for **M**onocular **D**epth e**S**timator?) The MiDaS system
    was trained on a dataset of 3D movie scenes, like the shots below.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 注意“单眼深度估计”指的是深度图，其中像素的值表示从相机到场景中物体表面的距离。为了获得深度估计，他们使用了一组欧洲研究人员的另一个机器学习模型[3]。这个模型叫做MiDaS（我猜，这是**单眼深度估计器**的倒编词？）MiDaS系统在3D电影场景的数据集上进行了训练，如下图所示。
- en: '![](../Images/bd71874f5aaabadebd55c2f6f582ba6c.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd71874f5aaabadebd55c2f6f582ba6c.png)'
- en: '**Sample Images from the 3D Movies Dataset**, Image from the [MiDaS Paper](https://arxiv.org/pdf/1907.01341.pdf)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**来自3D电影数据集的样本图像**，来自[MiDaS论文](https://arxiv.org/pdf/1907.01341.pdf)的图像'
- en: You can see how the light yellow colors in the depth map show the closer points
    in the scene, and the dark blue colors show the more distant points in the background.
    The trained MiDaS model can estimate depth maps from any input model. Here are
    some results from the paper.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到深度图中浅黄色显示了场景中较近的点，而深蓝色则显示了背景中较远的点。训练过的MiDaS模型可以从任何输入模型估计深度图。以下是论文中的一些结果。
- en: '![](../Images/78e6690fdbbc805168fe1cfeb8afb342.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78e6690fdbbc805168fe1cfeb8afb342.png)'
- en: '**Predicted Depth Maps using MiDaS**, Image from the [MiDaS Paper](https://arxiv.org/pdf/1907.01341.pdf)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用MiDaS的预测深度图**，图片来自[MiDaS论文](https://arxiv.org/pdf/1907.01341.pdf)'
- en: You can see how the MiDaS model does an excellent job of estimating depths.
    For example, you can see how the dog’s tail is well-defined as it stands out from
    the stream behind it.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到MiDaS模型在深度估计方面表现出色。例如，你可以看到狗的尾巴非常清晰地突显出它在后面的水流中。
- en: RunwayML’s video-to-video model uses the predicted depth maps of the input video
    to condition a diffusion video generation model directed by a text prompt or reference
    image.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: RunwayML的视频到视频模型使用输入视频的预测深度图来条件化一个由文本提示或参考图像指导的扩散视频生成模型。
- en: Our latent video diffusion model synthesizes new videos given structure and
    content information. We ensure structural consistency by conditioning on depth
    estimates while content is controlled with images or natural language. Temporally
    stable results are achieved with additional temporal connections in the model
    and joint image and video training. — P. Esser et al, RunwayML
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们的潜在视频扩散模型根据结构和内容信息合成新的视频。我们通过根据深度估计进行条件处理来确保结构一致性，同时内容由图像或自然语言控制。通过模型中的额外时间连接和图像与视频的联合训练，实现了时间稳定的结果。—
    P. Esser 等人，RunwayML
- en: You can see some video editing results with text prompts from the paper below.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到一些来自论文的文本提示的视频编辑结果。
- en: '![](../Images/312e621313470a1d5ba79849fcf506a7.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/312e621313470a1d5ba79849fcf506a7.png)'
- en: '**Results for RunwayML’s Image-to-Video-Editing with Text**, Image from [RunwayML’s
    Paper](https://arxiv.org/pdf/2302.03011.pdf)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**RunwayML的图像到视频编辑与文本的结果**，图片来自[RunwayML的论文](https://arxiv.org/pdf/2302.03011.pdf)'
- en: The various styles, pencil sketch, anime, and low-poly render, transform the
    input video to create the output. And you can see the consistency of applying
    the style from frame to frame. Below are some examples from the paper that use
    image prompts to stylize video.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 各种风格，包括铅笔素描、动漫和低多边形渲染，转化了输入视频以创建输出。你可以看到应用风格在每一帧中的一致性。以下是一些论文中的例子，使用图像提示来美化视频。
- en: '![](../Images/19e1263184b7e0c3050c3a06a193280b.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19e1263184b7e0c3050c3a06a193280b.png)'
- en: '**Results for RunwayML’s Image-to-Video-Editing with Reference Images**, Image
    from [RunwayML’s Paper](https://arxiv.org/pdf/2302.03011.pdf)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**RunwayML的图像到视频编辑与参考图像的结果**，图片来自[RunwayML的论文](https://arxiv.org/pdf/2302.03011.pdf)'
- en: Again, you can see how the color palette and the look of the prompt image transform
    the video to the indicated style. And details from the generated frames are rendered
    consistently for the final video.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，你可以看到颜色调色板和提示图像的外观如何将视频转变为指定的风格。生成帧的细节也在最终视频中保持一致。
- en: Using RunwayML’s Video-to-video Editing Service
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RunwayML的视频到视频编辑服务
- en: To use the service, I [created an account](https://app.runwayml.com/login) and
    logged in. As mentioned above, you can use the free version, which has limitations,
    like generating videos up to only four seconds. I opted to pay US$12 per month,
    which allows me to create videos up to 15 seconds and [other benefits](https://runwayml.com/pricing/).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用该服务，我[创建了一个账户](https://app.runwayml.com/login)并登录了。如上所述，你可以使用有限制的免费版本，比如生成的视频最长只能为四秒。我选择每月支付12美元，这允许我创建最长15秒的视频并享受[其他好处](https://runwayml.com/pricing/)。
- en: 'I shot a brief shadow puppet clip of a rabbit to test the system, cleaned it
    up with an editing system, and uploaded it to RunwayML. Then I chose the Gen-1:
    Video to Video tool. I loaded the clip, typed in the prompt, “photorealistic rabbit
    with floppy ears in a field,” and hit the **Preview Styles** button. The system
    thought about it a bit and rendered four thumbnails. You can see them at the bottom
    of the screenshot below.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '我拍摄了一段关于兔子的简短影像以测试系统，用编辑系统进行了清理，然后上传到RunwayML。我选择了Gen-1: Video to Video工具。我加载了剪辑，输入了提示语“在田野中的逼真兔子，耳朵下垂”，然后点击了**预览风格**按钮。系统思考了一会儿，渲染了四个缩略图。你可以在下面的截图底部看到它们。'
- en: '![](../Images/8b11962e741f5e333a01e196d3204089.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b11962e741f5e333a01e196d3204089.png)'
- en: '**RunwayML Gen-1: Video to Video Screen**, Image by Author'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**RunwayML Gen-1: 视频到视频屏幕**，图像由作者提供'
- en: All four thumbnails looked good. They all followed the form of the shadow puppet
    but with a realistic rabbit entering the frame. I chose the third one and hit
    **Generate Video**. It took about 20 minutes to render the video. I also created
    one with the prompt, “2D animated rabbit with floppy ears in a field.” You can
    see the results below, the original shadow puppet video, and my cleaned-up version
    for reference.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 四个缩略图看起来都不错。它们都遵循了影子木偶的形式，但画面中出现了一只逼真的兔子。我选择了第三个，然后点击 **生成视频**。视频渲染大约花了 20 分钟。我还用提示“在田野里有垂耳的
    2D 动画兔子”制作了一个视频。你可以在下面看到结果，原始影子木偶视频，以及我的清理版本供参考。
- en: '![](../Images/4edc4a191c9e8324a707991350088ab7.png)![](../Images/25f4375ebc7c6c32c049df2d560ad9ae.png)![](../Images/629fece34a73dbe140667f8eaf03f796.png)![](../Images/5dc768435388f93b2f82f72e9d40d4d6.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4edc4a191c9e8324a707991350088ab7.png)![](../Images/25f4375ebc7c6c32c049df2d560ad9ae.png)![](../Images/629fece34a73dbe140667f8eaf03f796.png)![](../Images/5dc768435388f93b2f82f72e9d40d4d6.png)'
- en: '**Original Shadow Puppet Video** (upper left), **Cleaned-up Shadow Puppet Video**
    (upper right), **Video Stylized with RunwayML using the prompt “photorealistic
    …”** (lower left), and **Video Stylized with RunwayML using the prompt “2D animation
    …”**, (lower right), Videos by Author'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**原始影子木偶视频**（左上），**清理后的影子木偶视频**（右上），**使用提示“逼真 …”的 RunwayML 风格化视频**（左下），和 **使用提示“2D
    动画 …”的 RunwayML 风格化视频**（右下），视频由作者提供'
- en: The generated videos came out nice! The photorealistic one in the lower left
    looks the best, with lovely details in the rabbit’s eyes, ears, and nose. The
    2D animation render is a bit off. The system seems confused about which ear is
    which, and the background is less interesting. Next, I tried the same experiment
    with two reference images generated in Midjourney.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的视频效果很好！左下角的逼真视频效果最好，兔子的眼睛、耳朵和鼻子细节非常漂亮。2D 动画渲染效果有些偏差。系统似乎对耳朵的识别有些困惑，背景也不太有趣。接下来，我用
    Midjourney 生成的两个参考图像尝试了同样的实验。
- en: '![](../Images/b091e18161db3b91bd6d305f81fbed81.png)![](../Images/4df4310a1769d85687ac041f4d0ad6e7.png)![](../Images/6b85cc56b398faee5770bba60c4163d1.png)![](../Images/809f1515cb4259eb711fbcf52d32ea5d.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b091e18161db3b91bd6d305f81fbed81.png)![](../Images/4df4310a1769d85687ac041f4d0ad6e7.png)![](../Images/6b85cc56b398faee5770bba60c4163d1.png)![](../Images/809f1515cb4259eb711fbcf52d32ea5d.png)'
- en: '**Midjourney Image using the prompt “photorealistic …”** (upper left), **Midjourney
    Image using the prompt “2D animation …”** (upper right), **Video Stylized with
    RunwayML using the photorealistic reference,** (lower left), and **Video Stylized
    with RunwayML using the animation reference,** (lower right), Videos by Author'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用提示“逼真 …”的 Midjourney 图像**（左上），**使用提示“2D 动画 …”的 Midjourney 图像**（右上），**使用逼真参考的
    RunwayML 风格化视频**（左下），和 **使用动画参考的 RunwayML 风格化视频**（右下），视频由作者提供'
- en: These came out nice, too. They both picked up the style from the reference image
    while following the shapes and motion in the original shadow puppet video. The
    one on the right has a strange effect coming in from the right, however. It almost
    looks like a sun flare. Notice how both generated animations show details from
    the background in the reference frames, like the nice clouds on the right. But
    the foreground forms from the reference frames are missing, like the wheat grains
    on the left and the tree on the right. This is probably due to the use of depth
    images in the training data that RunwayML used. It shows my hand movements transformed
    into bunnies as the foreground imagery but kept elements of the reference image
    for the background, like the field and sky.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些也做得很好。它们都从参考图像中拾取了风格，同时遵循了原始影子木偶视频中的形状和动作。然而，右边的那个视频有一个奇怪的效果从右侧出现，几乎看起来像是太阳光晕。注意到两个生成的动画都显示了参考帧中的背景细节，比如右侧漂亮的云朵。但参考帧中的前景形状却缺失了，比如左侧的麦粒和右侧的树。这可能是由于
    RunwayML 使用的训练数据中包含了深度图像。它展示了我的手部动作被转换成兔子作为前景图像，但保留了参考图像中的背景元素，比如田野和天空。
- en: Bringing Muybridge’s Photos to Life
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使穆伊布里奇的照片栩栩如生
- en: I used the RunwayML technique described above with a minor variation to transform
    the original image sequences from Muybridge to high-res versions.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了上述描述的 RunwayML 技术，并进行了小幅变更，将穆伊布里奇的原始图像序列转换为高分辨率版本。
- en: Super Slomo
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超慢动作
- en: Because the animals ran quickly in Muybridge’s experiments, there is a lot of
    motion between frames. For example, here are three frames of the horse sequence.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 由于穆伊布里奇的实验中动物动作很快，帧间存在大量的运动。例如，这里是马匹序列的三个帧。
- en: '![](../Images/60779274859ba1ecc20b6ce18561dca1.png)![](../Images/d865c1f5d3407587f3c09e4b2d67563b.png)![](../Images/36c66e53bb7d36ef3aff636f527a8dd1.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60779274859ba1ecc20b6ce18561dca1.png)![](../Images/d865c1f5d3407587f3c09e4b2d67563b.png)![](../Images/36c66e53bb7d36ef3aff636f527a8dd1.png)'
- en: '**Frames 2, 3, and 4 of** [**Plate Number 626, Gallop**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)
    by Eadweard Muybridge'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**帧 2、3 和 4 的** [**编号626，疾驰**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)
    由 Eadweard Muybridge 制作'
- en: Notice how much motion is seen in the horse’s legs between frames. The results
    were not very good when I experimented with the video-to-video stylization with
    fast-moving animation. My solution was first to slow down the motion by a factor
    of 2 using RunwayML’s **Super-Slow Motion** feature, then apply the transformation,
    and finally speed the resultant video up by a factor of two.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 注意马腿间帧的运动量。我在尝试快速移动动画的视频到视频风格化时，结果并不理想。我的解决方案是先使用 RunwayML 的**超慢动作**功能将运动减慢两倍，然后应用转换，最后将结果视频的速度提高两倍。
- en: Here’s what the slowed-down video looks like.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这是减速视频的效果。
- en: '![](../Images/60779274859ba1ecc20b6ce18561dca1.png)![](../Images/813e00ad13b7f2ae2c694bb213b5608f.png)![](../Images/d865c1f5d3407587f3c09e4b2d67563b.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60779274859ba1ecc20b6ce18561dca1.png)![](../Images/813e00ad13b7f2ae2c694bb213b5608f.png)![](../Images/d865c1f5d3407587f3c09e4b2d67563b.png)'
- en: '**Frames 2** (left) **and 3** (right) **of** [**Plate Number 626, Gallop**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)
    by Eadweard Muybridge, **RunwayML Frame Interpolation** (center) by Author'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**帧 2**（左）**和 3**（右）**的** [**编号626，疾驰**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)
    由 Eadweard Muybridge 制作，**RunwayML 帧插值**（中）由作者'
- en: You can see less motion between frames, especially with the horse’s legs. Here’s
    what the original horse sequence looks like compared to the 50% slow-motion version.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到帧间的运动减少，尤其是马的腿部。这是原始马匹序列与 50% 慢动作版本的对比。
- en: '![](../Images/75eacf68bff18bc34a17bcac5652d108.png)![](../Images/fad48e4ace87d9e47314f5aa6f2233a5.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75eacf68bff18bc34a17bcac5652d108.png)![](../Images/fad48e4ace87d9e47314f5aa6f2233a5.png)'
- en: '[**Plate Number 626, Gallop**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)by
    Eadweard Muybridge animated by Author (left), **RunwayML’s 50% Super-Slow Motion,**
    animation by Author'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[**编号626，疾驰**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)由
    Eadweard Muybridge 制作，作者动画（左），**RunwayML 的 50% 超慢动作**，作者动画'
- en: The system did an excellent job with motion interpolation. In general, the motion
    is smoother with the RunwayML’s super-slow motion. There is a little hiccup in
    the action when the sequence resets, but it will get masked when I speed the transformed
    videos up by a factor of two.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 系统在运动插值方面表现出色。总体而言，使用 RunwayML 的超慢动作时运动更为流畅。当序列重置时，动作有一点小的卡顿，但当我将转换后的视频速度提高两倍时，这种情况会被掩盖。
- en: '**Video-to-Video Transformations**'
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**视频到视频转换**'
- en: 'I first uploaded the slowed-down horse animation into RunwayML to create the
    transformed video and then chose the **Gen-1: Video to Video** tool. I selected
    the **Image Style** **reference** and uploaded my reference frame for the horse
    created with Midjourney. Several settings are available for the transformation,
    including the following.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '我首先将减速的马匹动画上传到 RunwayML 来创建转换后的视频，然后选择了**Gen-1: 视频到视频**工具。我选择了**图像风格** **参考**，并上传了我使用
    Midjourney 创建的马匹参考帧。转换有多种设置，包括以下内容。'
- en: '**Style: Structural consistency** - Higher values make the output more structurally
    different from the input video.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**风格：结构一致性** - 更高的值使输出与输入视频在结构上更有差异。'
- en: '**Style: Weight** - Higher values emphasize matching the style rather than
    the input video.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**风格：权重** - 更高的值强调匹配风格而非输入视频。'
- en: '**Frame Consistency** - Values below 1 give decreased consistency across time;
    values above 1 increase how closely frames relate to prior frames.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**帧一致性** - 值低于 1 会减少时间上的一致性；值高于 1 会增加帧与前一帧的相关性。'
- en: You can see some examples of varying these settings on RunwayML’s [help page](https://help.runwayml.com/hc/en-us/articles/15161225169171-Gen-1-Advanced-Settings).
    I experimented with these settings but used the defaults, Structural Consistency
    of 2, Weight of 8.5, and Frame Consistency of 1.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 RunwayML 的 [帮助页面](https://help.runwayml.com/hc/en-us/articles/15161225169171-Gen-1-Advanced-Settings)
    上看到这些设置的变体示例。我试验了这些设置，但使用了默认值，即结构一致性 2、权重 8.5 和帧一致性 1。
- en: I then clicked **Preview styles,** and it displayed four options at the bottom.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我点击了**预览样式**，它在底部显示了四个选项。
- en: '![](../Images/f527b973d7f1a88fc86c4f606ab8d9e1.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f527b973d7f1a88fc86c4f606ab8d9e1.png)'
- en: '**Screenshot of RunwayML’s Gen-1: Video to Video Feature**, Image by Author'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**RunwayML 的 Gen-1：视频到视频功能的屏幕截图**，图片由作者提供'
- en: I chose the third preview and hit the **Generate video** button. Here is the
    reference image, the original horse sequence, and the stylized animation sped
    up by a factor of two to match the initial speed.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择了第三个预览，并点击了**生成视频**按钮。这里是参考图像、原始马匹序列和经过风格化的动画，速度加快了两倍以匹配初始速度。
- en: '![](../Images/57c42c2174be01d41b7a2db17e7b55bc.png)![](../Images/b761027e0eaab13a844c639c6d76106f.png)![](../Images/30b9351e38f1889a940737c6ce288314.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57c42c2174be01d41b7a2db17e7b55bc.png)![](../Images/b761027e0eaab13a844c639c6d76106f.png)![](../Images/30b9351e38f1889a940737c6ce288314.png)'
- en: '**Midjoruney Reference Image** by Author (left), [**Plate Number 626, Gallop**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)by
    Eadweard Muybridge animated by Author (center), **Animation Stylized with RunwayML**
    by Author (right)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**Midjoruney 参考图像**（左），[**626 号版图，飞跑**](https://www.royalacademy.org.uk/art-artists/work-of-art/horses-gallop-thoroughbred-bay-mare-annie-g-with-male-rider)（中），**使用
    RunwayML 风格化的动画**（右）'
- en: This came out well! You can see how the style from the reference image got imposed
    on the original Muybridge animation while keeping the motion of the horse and
    jockey intact. The system also performed an ML-based video resize to bring the
    final video up to 640x480, which brought in some nice details. Note that the system
    has an Upscale setting, which would double the resolution horizontally and vertically.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这做得很好！你可以看到参考图像的风格如何被施加到原始的梅布里奇动画上，同时保持了马匹和骑手的动作完整。系统还进行了基于机器学习的视频缩放，将最终视频调整到
    640x480，这带来了不少细节。请注意，系统有一个“放大”设置，它会将分辨率水平和垂直方向都翻倍。
- en: I performed the same operations on the other four image sequences. You can see
    the results below, with the reference frames from Midjourney, Muybridge's original
    animal photo sequences, and the stylized videos by RunwayML.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我对另外四个图像序列执行了相同的操作。你可以在下面看到结果，包括 Midjourney 的参考帧、梅布里奇的原始动物照片序列和 RunwayML 的风格化视频。
- en: '![](../Images/4a181c22443ee60521ce968f821085a2.png)![](../Images/fb301fc8d7f5af37276b3e47d25d2736.png)![](../Images/01a8769a7800547ce3be7cd932b0799b.png)![](../Images/0ae712c5397cfc01b8a5e4e92d2355bb.png)![](../Images/3ee35d9bc47c82ea89b038263a020f57.png)![](../Images/e1aa014505828df4f357beeb80240dcc.png)![](../Images/fcfc2b4e4ce180c64836e5ffd521d10e.png)![](../Images/bab698fe2d0daf00bf15ce4985d9e52a.png)![](../Images/a50fac88f71945520144b9995ed8b9ff.png)![](../Images/96214d9a39a53cc289044e804c95dd2a.png)![](../Images/65334658f4100f2a64ed9b7b8afd5fe4.png)![](../Images/48aae389c3f9ad66e2551e456b9acd01.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a181c22443ee60521ce968f821085a2.png)![](../Images/fb301fc8d7f5af37276b3e47d25d2736.png)![](../Images/01a8769a7800547ce3be7cd932b0799b.png)![](../Images/0ae712c5397cfc01b8a5e4e92d2355bb.png)![](../Images/3ee35d9bc47c82ea89b038263a020f57.png)![](../Images/e1aa014505828df4f357beeb80240dcc.png)![](../Images/fcfc2b4e4ce180c64836e5ffd521d10e.png)![](../Images/bab698fe2d0daf00bf15ce4985d9e52a.png)![](../Images/a50fac88f71945520144b9995ed8b9ff.png)![](../Images/96214d9a39a53cc289044e804c95dd2a.png)![](../Images/65334658f4100f2a64ed9b7b8afd5fe4.png)![](../Images/48aae389c3f9ad66e2551e456b9acd01.png)'
- en: '**Midjoruney Reference Images** by Author (left), **Animal Locomotion Studies**
    by Eadweard Muybridge animated by Author (center), **Animations Stylized with
    RunwayML** by Author (right)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**Midjoruney 参考图像**（左），**伊德华·梅布里奇的动物运动研究**（中），**使用 RunwayML 风格化的动画**（右）'
- en: These look great, too! Like the horse animation, the RunwayML model picked up
    the textures and coloring from the reference image and applied them to the original
    animations while keeping the motion intact. The backgrounds in the new animations
    didn’t scroll right to the left, however. But this was not a problem. You can
    see in the next section how I created an “alpha mask” to keep just the foreground
    imagery and composite the animals over a new background.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这些看起来也很棒！就像马匹动画一样，RunwayML 模型从参考图像中提取了纹理和颜色，并将其应用于原始动画，同时保持运动的完整性。然而，新动画中的背景并没有从右到左滚动。但这不是问题。你可以在下一部分看到我如何创建一个“alpha
    mask”来保留前景图像，并将动物合成到新的背景上。
- en: Removing Background Imagery
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 移除背景图像
- en: I used RunwayML’s **Remove Background** feature to replace the running animal
    clips' backgrounds. I loaded the original video clip from Muybridge’s photos and
    used the cursor to select two points, the horse and the jockey’s leg. The system
    thought a bit, then showed the chosen area in green, as you can see in the screenshot
    below.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了 RunwayML 的 **Remove Background** 功能来替换奔跑动物片段的背景。我加载了来自 Muybridge 照片的原始视频片段，并用光标选择了两个点，即马匹和骑师的腿。系统思考了一会儿，然后显示了所选区域为绿色，如下面的截图所示。
- en: '![](../Images/ad4a2d6bb6ea38908c0eddb41192c348.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad4a2d6bb6ea38908c0eddb41192c348.png)'
- en: '**Screenshot of RunwayML’s Remove Background Feature**, Image by Author'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**RunwayML 背景移除功能截图**，图像由作者提供'
- en: The system shows how it selected the foreground for all of the frames in the
    video, and I could play it as a preview. It did an excellent job that didn’t require
    much work on my part. I then saved the alpha matte as a video for my compositing
    app.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 系统展示了它如何选择视频中所有帧的前景，我可以将其作为预览播放。它做得非常出色，没有花费我太多的工作。然后，我将 alpha matte 保存为视频以供合成应用程序使用。
- en: I created a still image of a derby stadium in Midjourney and used it as a scrolling
    background for my animation. Wherever the matte is black, it will show the background
    (the stadium); wherever it is white, it will show the foreground (the horse and
    jockey.) Here is the stylized clip of the horse, the alpha matte, and the final
    result.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 Midjourney 中创建了一个赛马场的静态图像，并将其用作动画的滚动背景。凡是 matte 为黑色的地方，会显示背景（赛马场）；凡是 matte
    为白色的地方，会显示前景（马匹和骑师）。这里是马匹的风格化剪辑、alpha matte 和最终结果。
- en: '![](../Images/959918e566271716b28bd1f1cc94b504.png)![](../Images/7a7b0ef1ce36b97ca34f351145930d9b.png)![](../Images/8bb6fee237df623e45aac9a1d569f5fb.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/959918e566271716b28bd1f1cc94b504.png)![](../Images/7a7b0ef1ce36b97ca34f351145930d9b.png)![](../Images/8bb6fee237df623e45aac9a1d569f5fb.png)'
- en: '**Animation Stylized with RunwayML** (left), **Alpha Matte** (center), and
    **Final Results** (right), Animations by Author'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 RunwayML 风格化的动画**（左），**Alpha Matte**（中），和 **最终结果**（右），动画由作者提供'
- en: In my compositing program, I had to clean up the alpha matte a bit. For example,
    I blurred the tail to make it look more like hair and not a solid object. You
    can see how the scrolling background helps sell the effect that the horse is running
    forward, which you don’t see in the original stylized animation.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的合成程序中，我需要稍微清理一下 alpha matte。例如，我模糊了尾部，使其看起来更像头发而不是固体物体。你可以看到滚动背景如何帮助展示马匹向前奔跑的效果，而在原始风格化动画中并没有这个效果。
- en: Here is the final animation again, this time a bit bigger, so you can check
    out the details.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是最终的动画，这次稍微大一点，以便你查看细节。
- en: '**“Muybridge Derby,” based on Eadweard Muybridge’s Animal Locomotion Photographs,**
    Video by Author'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**“Muybridge Derby”，基于 Eadweard Muybridge 的动物运动照片**，视频由作者提供'
- en: If you want to see the animation on a large screen, it will be shown at [*The
    Next Big Thing*](https://studiochannelislands.org/nbt23/), August 5 to September
    30, 2023, at the Studio Channel Islands Art Center in Camarillo, CA.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在大屏幕上观看动画，它将于 2023 年 8 月 5 日至 9 月 30 日在加州卡马里奥的 Studio Channel Islands Art
    Center 展出，地址是 [*The Next Big Thing*](https://studiochannelislands.org/nbt23/)。
- en: Final Thoughts
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最终想法
- en: I enjoyed working with the Muybridge images and using Midjourney and the tools
    at RunwayML to generate and modify media for this project. If you are familiar
    with my writing on Medium, you know I like to try new production methods, but
    I don’t always create a finished piece. So it was satisfying for me to bring multiple
    elements together. As a “deep cut,” I used a song I generated with AI for a previous
    article as the music played over the credits. It’s called “I’ll Get There When
    I Get There,” which is kinda appropriate for a derby race. 😄
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[](/ai-tunes-creating-new-songs-with-artificial-intelligence-4fb383218146?source=post_page-----b1918e6622ec--------------------------------)
    [## AI-Tunes: Creating New Songs with Artificial Intelligence'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: How I fine-tuned OpenAI’s GPT-3 to generate music with a global structure
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/ai-tunes-creating-new-songs-with-artificial-intelligence-4fb383218146?source=post_page-----b1918e6622ec--------------------------------)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Ownership of Inputs and Generated Media
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Midjourney and RunwayML have different policies regarding the imagery and text
    used for prompts and the resulting generated images. Midjourney distinguishes
    between paid and free users, whereas RunwayML uses the same policy for both types
    of users.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Midjourney Terms
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: According to Midjourney’s [terms of service](https://docs.midjourney.com/docs/terms-of-service),
    users of the free service tier do not own their generated images. Midjourney owns
    them. The images are licensed back to the non-paying users for noncommercial purposes
    under the Creative Commons Noncommercial 4.0 Attribution International License.
    Users of the paid service own their generated images, which can be used commercially.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: If you are working for a big company, there are additional limitations.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: If You are an employee or owner of a company with more than $1,000,000 USD a
    year in gross revenue and You are using the Services on behalf of Your employer,
    You must purchase a “Pro” or “Mega” membership for every individual accessing
    the Services on Your behalf in order to own Assets You create. If You are not
    sure whether Your use qualifies as on behalf of Your employer, please assume it
    does. — Midjourney Terms of Service
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: FYI, the Pro plan costs US$60 per person per month, and the Mega monthly charge
    is US$120.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, under the terms, all users grant a license to Midjourney to use
    any text prompts, uploaded images used as prompts, and generated images for any
    purpose, including for training future versions of their models.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: RunwayML Terms
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: According to RunwayML’s [terms of use](https://runwayml.com/terms-of-use/),
    all users own and can use their generated content commercially. However, all users
    grant a license to RunwayML to use their inputs and outputs for any purpose, including
    training future model versions.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: License Terms for Images and Animations
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I am releasing the images and animations for this project under the Creative
    Commons Attribution Sharealike license.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be9434bf14bc4eab9a454950cbe4a9c5.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: '**Creative Commons Attribution Sharealike**'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I want to thank Jennifer Lim for reviewing the article and providing feedback.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] V. Whitmire, International Photography Hall of Fame: Eadweard Muybridge
    *(2017)*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[2] P. Esser et al. [Structure and Content-Guided Video Synthesis with Diffusion
    Models](https://arxiv.org/pdf/2302.03011.pdf) (2023)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[3] R. Ranftl et al. [Towards Robust Monocular Depth Estimation: Mixing Datasets
    for Zero-shot Cross-dataset Transfer](https://arxiv.org/pdf/1907.01341.pdf) (2020)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
