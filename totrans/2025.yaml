- en: 'The History of Open-Source LLMs: Imitation and Alignment (Part Three)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-history-of-open-source-llms-imitation-and-alignment-part-three-603d709c7aa5](https://towardsdatascience.com/the-history-of-open-source-llms-imitation-and-alignment-part-three-603d709c7aa5)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Open-source LLMs need alignment to become truly remarkable…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----603d709c7aa5--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----603d709c7aa5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----603d709c7aa5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----603d709c7aa5--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----603d709c7aa5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----603d709c7aa5--------------------------------)
    ·20 min read·Nov 28, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09529330477dcea0f682d4764a7fe0fc.png)'
  prefs: []
  type: TYPE_IMG
- en: (Photo by [Joanna Kosinska](https://unsplash.com/@joannakosinska?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/brown-paper-and-black-pen-B6yDtYs2IgY?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  prefs: []
  type: TYPE_NORMAL
- en: A majority of prior research on open-source large language models (LLMs) focused
    heavily upon creating pre-trained base models. However, these models have not
    undergone any fine-tuning, so they fail to match the quality of top closed-source
    LLMs (e.g., ChatGPT or Claude) due to their lack of alignment. Paid models are
    aligned extensively using techniques like SFT and RLHF, which greatly enhances
    their usability. In comparison, open-source models are typically fine-tuned to
    a lesser extent using smaller, public datasets. Within this overview, however,
    we will take a look at recent research that aims to improve the quality of open-source
    LLMs via more extensive fine-tuning and alignment.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a77e11ba9dd423a60dcfd943e2a772e.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [1, 2, 12])
  prefs: []
  type: TYPE_NORMAL
- en: This overview is the third (and final) part of my series on the history of open-source
    LLMs. In the [first part](/the-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8)
    of the series, we looked at initial attempts at creating open-source language
    models. Although these initial pre-trained LLMs performed poorly, they were quickly
    followed up by much better open-source base models, which we covered in [part
    two](https://medium.com/towards-data-science/the-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe)
    of this series. Now, we will cover how these better open-source models can be
    fine-tuned/aligned to improve their quality and close the gap in performance between
    open-source and proprietary LLMs, completing the journey from initial models like
    OPT to the incredibly high-performing open-source LLMs that we have today (e.g.,
    LLaMA-2-Chat).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/83a43fe06b6c5fd20d6c2670a174813a.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [17, 18])
  prefs: []
  type: TYPE_NORMAL
- en: '**The alignment process.** This overview will study the fine-tuning and alignment
    process for open-source LLMs. Prior to studying research in this area, however,
    we need to understand what alignment is and how it is accomplished. We should
    recall that the training process for language models proceeds in several parts.
    As shown above, we begin with pre-training, which is followed by several fine-tuning
    steps. After pre-training, the LLM can accurately perform next token prediction,
    but its output may be repetitive and uninteresting. Thus, the model needs to be
    fine-tuned to improve its *alignment*, or its ability to generate text that aligns
    with the desires of a human user (e.g., follow instructions, avoid harmful output,
    avoid lying, produce interesting or creative output, etc.).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac091974fab99dc79bc6c2c9734df861.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [17])
  prefs: []
  type: TYPE_NORMAL
- en: '**SFT.** Alignment is accomplished via two fine-tuning techniques: supervised
    fine-tuning (SFT) and reinforcement learning from human feedback (RLHF); see above
    for a depiction. SFT simply fine-tunes the model, using a standard language modeling
    objective, over examples of high-quality prompt and response pairs. The LLM is
    allowed to see examples of how it should respond and learn from these responses!
    SFT is incredibly simple and effective, but it requires carefully curating a dataset
    that captures “correct” behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: '**RLHF** trains the LLM directly on feedback from human annotators — *humans
    identify outputs that they like, and the LLM learns how to produce more outputs
    like this*. To do this, we first obtain a set of prompts and generate several
    different outputs from the LLM on each prompt. Using a group of human annotators,
    we score each of these responses based on their quality. These scores can then
    be used to train a reward model (i.e., just a fine-tuned version of our LLM with
    an added regression head) to predict the score of a response. From here, RLHF
    fine-tunes the model to maximize this score using a reinforcement learning algorithm
    called PPO. Typically, the highest-performing LLMs are aligned by performing both
    SFT and RLHF (with lots of human feedback) in sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: Imitation Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/e8b7bce991020cec584a6ebd5e46eaec.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [16])
  prefs: []
  type: TYPE_NORMAL
- en: With the release of LLaMA [3], the open-source research community finally had
    access to powerful base LLMs that could be fine-tuned or aligned for a variety
    of different applications. As such, LLaMa catalyzed an explosion of open-source
    LLM research, as practitioners rushed to fine-tune LLaMA models on their task
    of choice. Interestingly, one of the most common directions of research during
    this time was *imitation learning*. Imitation learning, which is (arguably) a
    form of alignment, fine-tunes an LLM over outputs from another, more powerful
    LLM. Such an approach is inspired by the idea of knowledge distillation; see above.
  prefs: []
  type: TYPE_NORMAL
- en: “The premise of model imitation is that once a proprietary LM is made available
    via API, one can collect a dataset of API outputs and use it to fine-tune an open-source
    LM.” *— from [6]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The question posed by open-source imitation learning research was simple: *can
    we create a model that is as powerful as ChatGPT or GPT-4 by just fine-tuning
    on responses from these models?* To test this out, we can follow a simple approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect dialogue examples from these models (e.g., using the OpenAI API).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform (supervised) fine-tuning on this data (i.e., using a normal language
    modeling objective).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we will see, the research community hotly debated whether imitation learning
    was a valuable approach for quite some time! In the end, we found out that the
    approach is practically useful, but it only works well under certain conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Initial Efforts in Imitation Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/97cfd5f7a37fc8e420426e833bd4c0ae.png)'
  prefs: []
  type: TYPE_IMG
- en: LLaMA catalyzed the creation of numerous imitation models (from [7, 8, 9, 10])
  prefs: []
  type: TYPE_NORMAL
- en: After the release of LLaMA, researchers quickly began to release a variety of
    imitation models using dialogue derived from ChatGPT. Typically, the data used
    for training — *which prohibits the resulting model from being used commercially*
    — is obtained either from the OpenAI API or sources like [ShareGPT](https://sharegpt.com/).
    A few of the most widely-known imitation models are outlined below (in chronological
    order).
  prefs: []
  type: TYPE_NORMAL
- en: '**Alpaca [7]** fine-tunes LLaMA-7B by using the self-instruct [11] framework
    to automatically collect a fine-tuning dataset from [GPT-3.5](https://platform.openai.com/docs/models/gpt-3-5)
    (i.e., `text-davinci-003`). Collecting data and fine-tuning Alpaca costs only
    $600.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vicuna [8]** fine-tunes LLaMA-13B over 70K dialogue examples from ChatGPT
    (i.e., derived from ShareGPT). Interestingly, the entire fine-tuning process for
    Vicuna costs only $300.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Koala [9]** fine-tunes LLaMA-13B on a large dataset of dialogue examples
    from both the Alpaca fine-tuning set and a variety of other sources like [ShareGPT](https://sharegpt.com/),
    [HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3), [OIG](https://laion.ai/blog/oig-dataset/),
    [Anthropic HH](https://huggingface.co/datasets/Anthropic/hh-rlhf), and OpenAI
    [WebGPT](https://huggingface.co/datasets/openai/webgpt_comparisons)/[Summarization](https://huggingface.co/datasets/openai/summarize_from_feedback).
    Compared to prior imitation models, Koala is fine-tuned over a larger dataset
    and evaluated more extensively.'
  prefs: []
  type: TYPE_NORMAL
- en: '**GPT4ALL [16]** fine-tunes LLaMA-7B on over 800K chat completions from `GPT-3.5-turbo`.
    Along with the model, authors release both training/inference code and quantized
    model weights that can be used to perform inference with minimal compute resources
    (e.g., a laptop).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b4be15e264d09efec80cf2c456aac282.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [8\. 9])
  prefs: []
  type: TYPE_NORMAL
- en: '**The impact of imitation.** These models were published in close succession
    and claimed to achieve comparably quality to top proprietary models like ChatGPT
    and GPT-4\. For example, Vicuna is found to maintain 92% of the quality of GPT-4,
    while Koala is found to match or exceed the quality of ChatGPT in many cases;
    see above. Such findings seemed to indicate that model imitation could be used
    to distill the capabilities of any proprietary model into a smaller, open-source
    LLM. If this were true, the quality of even the best proprietary LLMs could be
    easily replicated and these models would be left with [no true advantage](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither).'
  prefs: []
  type: TYPE_NORMAL
- en: “Open-source models are faster, more customizable, more private, and … more
    capable. They are doing things with $100 and 13B params that [Google] struggles
    with at $10M and 540B. And they are doing so in weeks, not months.” *— from [9]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The explosion of imitation models was one of the first instances in which open-source
    models were truly seen as a potential alternative to the closed-source LLMs that
    had dominated the LLM landscape [since the proposal of GPT-3](https://openai.com/blog/openai-api).
    Despite the use of paid APIs becoming standard, the impressive performance of
    imitation models fostered a feeling of promise for open-source LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Are imitation models a false promise?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/6061f398d45ebc75a6b1a2fb047f72bc.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [6])
  prefs: []
  type: TYPE_NORMAL
- en: Despite the promise of imitation models’ impressive performance, we see in [6]
    that we are missing something important. Namely, more targeted evaluations of
    these models reveal that they do not perform nearly as well as top proprietary
    LLMs like ChatGPT and GPT-4\. In fact, we see that fine-tuning a base model via
    imitation actually does very little to close the gap in performance between open-source
    and proprietary models in most cases. Rather, the resulting model tends to only
    improve in performance on tasks that are heavily represented in the fine-tuning
    set and may even have a more pronounced tendency for hallucination.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/206639a640feaaf12da8f2a229df06bd.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [6])
  prefs: []
  type: TYPE_NORMAL
- en: '**Experimental setup.** To determine the utility of imitation learning, authors
    in [6] curate a dataset of ~130K diverse dialogue examples from ChatGPT. Then,
    several different sizes of language models are fine-tuned over various amounts
    of imitation data before having their performance measured. As shown above, there
    are a few interesting observations that we can make from these experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: The amount of imitation data used for fine-tuning does not improve model quality
    in human evaluation trials.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imitation models’ performance on standardized benchmarks is often worse than
    that of the base model (and deteriorates as more imitation data is used).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing the size of the base model consistently improves the quality of the
    resulting imitation models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**What is going on here?** When imitation models are evaluated across a wider
    variety of natural language benchmarks, we see that their performance is comparable
    to or below that of the corresponding base LLM. In other words, *imitation models
    do not actually match the quality of models like ChatGPT*. Compared to proprietary
    LLMs, these models have a less extensive knowledge base, as revealed by the performance
    improvement observed with larger base models.'
  prefs: []
  type: TYPE_NORMAL
- en: “We argue that the highest leverage action for improving open-source models
    is to tackle the difficult challenge of developing better base LMs, rather than
    taking the shortcut of imitating proprietary systems.” *— from [6]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'With this in mind, the first question we might have is: *why did it seem like
    these models performed so well?* We see in [6] that imitation models learn to
    mimic the style of a model like ChatGPT. As such, human workers can be tricked
    into perceiving the model as high-quality even if it generates factually incorrect
    information more frequently (i.e., this is harder to easily check or verify).'
  prefs: []
  type: TYPE_NORMAL
- en: Is imitation learning actually useful?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: “Our research indicates that learning from step-by-step explanations, whether
    these are generated by humans or more advanced AI models, is a promising direction
    to improve model capabilities and skills.” *— from [1]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'After work in [6] revealed that imitation models did not perform nearly as
    well as initially thought, the research community was unclear whether imitation
    models actually had any value. Notably, analysis in [6] indicates that local imitation
    — *or learning to imitate a model’s behavior on a specific task, instead of imitating
    its behavior as a whole —* is quite effective. However, this does not mean the
    imitation model matches the quality of proprietary models more generally. To make
    imitation models better in general, authors in [6] pose two paths forward:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating a much bigger and more comprehensive imitation dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a better base model to use for imitation learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interestingly, both of these recommendations were explored extensively by subsequent
    research and found to yield positive results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/47c1dc31d49b7f02ce1e25a3703ec4d6.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [12])
  prefs: []
  type: TYPE_NORMAL
- en: '**Orca [12]** is an imitation model based upon LLaMA-13B. Compared to prior
    work on imitation learning, however, Orca is trained over a higher-quality, more
    detailed, and more comprehensive dataset collected from ChatGPT and GPT-4\. In
    particular, prior datasets collected for imitation learning can be considered
    “shallow” — they are simply examples of prompt and response pairs generated by
    a model like ChatGPT; see above.'
  prefs: []
  type: TYPE_NORMAL
- en: “We conclude that broadly matching ChatGPT using purely imitation would require
    a concerted effort to collect enormous imitation datasets and far more diverse
    and higher quality imitation data than is currently available.” *— from [6]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Improving upon shallow imitation, Orca attempts to augment imitation datasets
    generated by models like ChatGPT or GPT-4 with:'
  prefs: []
  type: TYPE_NORMAL
- en: Explanation traces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step-by-step thought processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complex instructions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To do this, the model being imitated is prompted to provide detailed explanations
    of its response via an instruction or system message. Such an approach goes beyond
    simple prompt-response pairs by adding extra, useful information to the data seen
    by an imitation model. When learning from powerful LLMs like ChatGPT, Orca sees
    more than just the model’s response. Namely, it can learn from detailed explanations
    and thought processes generated along with the model’s response on complex prompts!
    See below for an illustration.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fcfe4674c54960734d383ec64f2eaf87.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [12])
  prefs: []
  type: TYPE_NORMAL
- en: After being fine-tuned over a massive dataset of such detailed imitation data
    (i.e., 5M examples from ChatGPT and 1M examples from GPT-4), we see that Orca
    performs incredibly well compared to prior imitation models; see below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/84a8f7eec9fe68743ed2e4a23f5dfc24.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [12])
  prefs: []
  type: TYPE_NORMAL
- en: Although Orca significantly narrows the gap between open-source imitation models
    and proprietary LLMs, we still see in the table below that the model is outperformed
    consistently by GPT-4\. Unfortunately, even an improved imitation approach is
    not enough to fully match the quality of top proprietary models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b0baf2a72f3ebbbe331a4f92c4a2c430.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [12])
  prefs: []
  type: TYPE_NORMAL
- en: 'Nonetheless, Orca’s impressive performance reveals that imitation learning
    is a valuable fine-tuning strategy that can drastically improve the performance
    of any high-quality base LLM. Going further, we learn in [12] that leveraging
    imitation learning successfully has two main requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: A large, comprehensive imitation dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detailed explanation traces within each response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Better base LLMs.** Although authors in [6] argue that collecting a sufficiently
    large and diverse imitation learning dataset is incredibly difficult, we see with
    Orca that such a feat is at least possible. Additionally, later work extensively
    explores the alternative suggestion in [6]: *creating more powerful (open-source)
    base models*. Although open-source pre-trained LLMs performed poorly at first,
    we have recently seen the proposal of a variety of powerful pre-trained LLMs;
    e.g., LLaMA [3], MPT [14, 15], and Falcon [13]. Given that model pre-training
    is a starting point for any fine-tuning that follows (e.g., imitation learning,
    SFT, RLHF, etc.), starting with a better base model improves the downstream imitation
    model as well! Luckily, we covered all of the best open-source, pre-trained language
    models in part two of this series. See [here](https://medium.com/towards-data-science/the-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe)
    for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: Aligning Open-Source LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/aa181a9101c5231ac41a5af68680489a.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [5])
  prefs: []
  type: TYPE_NORMAL
- en: Imitation learning attempted to improve the quality of open-source base models
    by training over the responses (and explanation traces) of proprietary LLMs. Although
    this approach is successful in some cases, this is (obviously) not the manner
    in which the top proprietary models are trained — *imitation is a short cut for
    creating powerful open-source models*. If we want open-source LLMs that rival
    the quality of proprietary models, we need to invest significantly into alignment.
  prefs: []
  type: TYPE_NORMAL
- en: “These closed product LLMs are heavily fine-tuned to align with human preferences,
    which greatly enhances their usability and safety. This step can require significant
    costs in compute and human annotation, and is often not transparent or easily
    reproducible.” *— from [1]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**What’s the hold up?** The idea of aligning open-source imitation models seems
    easy enough. We have really great base models, *why not just replicate the alignment
    process used by models like GPT-4?* The alignment process requires extensive compute
    and human annotation resources. Plus, it is heavily dependent upon proprietary
    data, which limits transparency and makes reproducing results quite difficult.
    As such, open-source models have lagged behind their proprietary counterparts
    in alignment research for quite some time. Within this section, however, we will
    explore two recent works — LIMA [2] and LLaMA-2 [1] — that drastically improve
    the quality of open-source LLMs via better alignment.'
  prefs: []
  type: TYPE_NORMAL
- en: Prior Work on Open-Source Alignment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before covering LIMA and LLaMA-2, it is important to note that the open-source
    research community has not avoided aligning pre-trained models altogether. For
    example, Falcon-40B-Instruct [13] undergoes SFT over 150M token of data from [Baize](https://github.com/project-baize/baize-chatbot).
    Similarly, numerous fine-tuned variants of MPT-7B [14] and MPT-30B [15] have been
    released, including both chat/instruct variants that undergo SFT on public datasets
    and a StoryWriter variant that is fine-tuned over data with a much longer context
    length.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97dde27c493c4444d9b4f753e8b9adb5.png)'
  prefs: []
  type: TYPE_IMG
- en: (from the OpenLLM Leaderboard)
  prefs: []
  type: TYPE_NORMAL
- en: Plus, if we take a simple look at the Open LLM Leaderboard (see above), we see
    a variety of different models that have underwent fine-tuning via SFT on all types
    of different datasets. Open-source LLMs have not avoided alignment altogether.
    However, top proprietary models undergo both SFT and RLHF over massive datasets
    of high-quality dialogue and human feedback. In comparison, most open-source models
    have been aligned using solely SFT over public datasets that lack in quality and
    diversity. To truly match the quality of proprietary models, *open-source LLMs
    needed to make an attempt at replicating their alignment process.*
  prefs: []
  type: TYPE_NORMAL
- en: 'LIMA: Data-Efficient Alignment [2]'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: “A model’s knowledge and capabilities are learnt almost entirely during pretraining,
    while alignment teaches it which subdistribution of formats should be used when
    interacting with users.” *— from [2]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As mentioned above, open-source LLMs — for quite some time — mostly performed
    alignment via SFT on public datasets. Given this heavy emphasis upon SFT, authors
    in [2] studied extensively the impact impact of SFT on pre-trained LLMs. The goal
    of this analysis was to uncover the relative importance of pre-training and alignment
    via SFT in creating a high-performing LLM, as well as to reveal best practices
    for maximizing a model’s performance after undergoing SFT.
  prefs: []
  type: TYPE_NORMAL
- en: '**The dataset.** To do this, authors in [2] construct a small dataset of 1,000
    dialogue examples to use for SFT. Although this might not seem like enough data,
    the examples included in this dataset are curated to ensure quality by using diverse
    prompts and a uniform output style or tone; see below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/710bc1acada4be78166c5200050186ec.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [2])
  prefs: []
  type: TYPE_NORMAL
- en: The SFT dataset used to train LIMA is small but of incredibly high quality.
    Interestingly, we see in [2] that LIMA performs surprisingly well when fine-tuned
    over this dataset, even approaching the performance of state-of-the-art LLMs like
    GPT-4 or Claude; see below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77b7023d4c4ad933838db14d34dac813.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [2])
  prefs: []
  type: TYPE_NORMAL
- en: Such a result reveals that language models can be effectively aligned via a
    small number of carefully chosen examples. Although LIMA still falls short of
    GPT-4’s performance, the ability to perform such high-quality alignment with such
    little data is both unexpected and impressive. Such a result shows us that data
    quality is seemingly the most important factor in performing alignment via SFT.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da2ad328c87f2649eb949f91e5887c8e.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [2])
  prefs: []
  type: TYPE_NORMAL
- en: '**What do we learn?** We learn a variety of useful lessons from LIMA. First,
    the quality of data is incredibly important for SFT. Just using more data is not
    enough — *the data also needs to be of high quality*; see above. Additionally,
    the results in [2] lead to the proposal of the “Superficial Alignment Hypothesis”,
    which offers a new and unique perspective of alignment. Put simply, this hypothesis
    posits that most of an LLM’s core knowledge is [learned during pre-training](https://twitter.com/cwolferesearch/status/1660744247123890179?s=20),
    while alignment searches for the proper format or style for surfacing this knowledge.
    As such, alignment can be learned in a data efficient manner.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLaMA-2: Improving Transparency in Alignment Research [1]'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: “Llama 2-Chat is the result of several months of research and iterative applications
    of alignment techniques, including both instruction tuning and RLHF, requiring
    significant computational and annotation resources.” *— from [1]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The recently-released LLaMA-2 [1] suite of LLMs is comprised of several open-source
    models with sizes ranging from 7–70 billion parameters. Compared to their predecessors
    (i.e., LLaMA-1 [3]), LLaMA-2 models differentiate themselves by pre-training over
    40% more data (i.e., 2 trillion tokens instead of 1.4 trillion), having a longer
    context length, and using an architecture that is optimized for fast inference
    (i.e., by using [grouped query attention](https://twitter.com/_philschmid/status/1673335690912825347?s=20)
    [4]).
  prefs: []
  type: TYPE_NORMAL
- en: LLaMA-2 achieves state-of-the-art performance among open-source models. However,
    the LLaMA-2 suite contains more than just pre-trained LLMs. Authors invest heavily
    into the alignment process by fine-tuning each model — using both SFT and RLHF
    — over a massive amount of dialogue data and human feedback; see below. The resulting
    models are referred to as the LLaMA-2-Chat models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a6512d59a0d2d6d6409ddfaf5eacbc20.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [5])
  prefs: []
  type: TYPE_NORMAL
- en: 'These refined versions of LLaMA-2 perform incredibly well and take a major
    step towards closing the gap in alignment between open-source and proprietary
    LLMs. LLaMA-2’s alignment process emphasizes two key behavioral properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Helpfulness*: the model fulfills users’ requests and provides requested information.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Safety: the model avoids responses that are “unsafe”'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To ensure that the aligned model is both helpful and safe, data curated for
    both SFT and RLHF is filtered, collected, and annotated according to these principles.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/759625144742a35309e36d163de6e316.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  prefs: []
  type: TYPE_NORMAL
- en: '**SFT.** The first step in LLaMA-2’s alignment process is fine-tuning with
    SFT. Similar to other open-source LLMs, LLaMA-2 is first fine-tuned over publicly-available
    instruction tuning data. However, such data tends to lack in diversity and quality,
    which — *as demonstrated by LIMA [2]* — massively impacts performance. As such,
    authors in [1] focus upon collecting a smaller set of high-quality data for SFT.
    This data comes from a variety of sources, including both manually created or
    annotated examples and data from public sources that is filtered for quality.
    Ultimately, LLaMA-2 undergoes a second stage of fine-tuning with 27,540 high-quality
    dialogue examples; see above for samples.'
  prefs: []
  type: TYPE_NORMAL
- en: “Surprisingly, we found that the outputs sampled from the resulting SFT model
    were often competitive with SFT data handwritten by human annotators, suggesting
    that we could reprioritize and devote more annotation effort to preference-based
    annotation for RLHF.” *— from [1]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Interestingly, authors in [1] observe that collecting more data (i.e., beyond
    the 27K high-quality examples) for SFT provides diminishing benefits. These findings
    align with the empirical analysis from LIMA [2]. We don’t need a ton of data for
    SFT, but the data should be of high-quality! Interestingly, authors in [1] also
    note that LLaMA-2 models that have underwent SFT seem to be capable of generating
    their own data for SFT anyways.
  prefs: []
  type: TYPE_NORMAL
- en: '**RLHF.** LLaMA-2 is further fine-tuned using RLHF over a dataset of >1M examples
    of human feedback. To collect this feedback, a binary protocol is adopted, in
    which human annotators are asked to write a prompt and choose the better of two
    generated responses from the LLM. Here, human preference data is collected according
    to both helpfulness and safety standards. For example, human preference annotations
    focused upon safety may encourage the annotator to craft an adversarial prompt
    that is likely to elicit an unsafe response. Then, the human annotator can label
    which of the responses — if any — is preferable and safe.'
  prefs: []
  type: TYPE_NORMAL
- en: “Everything else being equal, an improvement of the reward model can be directly
    translated into an improvement for Llama 2-Chat.” *— from [1]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Human feedback data is collected in batches, and LLaMA-2 is fine-tuned via RLHF
    between each batch. As such, several versions of each LLaMA-2-Chat model — five
    in total — are iteratively created after each trial of RLHF. In [1], we see that
    a new reward model is trained for use in RLHF each time fresh human preference
    data is collected, ensuring the reward model accurately captures human preferences
    of the latest model. Additionally, we see that the quality of the resulting reward
    model is surprisingly predictive of LLaMA-2-Chat model quality overall. In total,
    LLaMA-2 is fine-tuned on over 1M instances of human feedback throughout the entirety
    of the iterative RLHF process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e1be3b210426fae3e8054e32371fcc3f.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the figure above, the quality of LLaMA-2-Chat — in terms of both
    helpfulness and safety — improves smoothly throughout the several iterations of
    alignment with both SFT and RLHF. This visualization clearly depicts the level
    of impact of each technique on the resulting model’s quality. Namely, performing
    SFT alone only gets us so far! The model’s alignment improves drastically with
    each phase of RLHF that is performed even after SFT is applied.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1165788d2c200dd51e5a9e35f00c25ec.png)'
  prefs: []
  type: TYPE_IMG
- en: All top-5 models on the Open LLM leaderboard are based upon LLaMA-2 (from Open
    LLM leaderboard)
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance.** The LLaMA-2-Chat models are currently state-of-the-art for
    open-source LLMs, as shown by the [Open LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    above. When LLaMA-2-Chat models are compared to other popular LLMs in [1], we
    see that they far exceed other open-source models in terms of helpfulness and
    safety; see below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6018d1404960abc1e8cab15a06fbc5e6.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, LLaMA-2 is even found to perform comparably to top proprietary
    models like ChatGPT when evaluated in terms of helpfulness and safety. Put simply,
    these results heavily indicate that the quality of alignment performed for the
    LLaMA-2-Chat models is high. The resulting models tend to accurately capture and
    adhere to desired helpfulness and safety standards.
  prefs: []
  type: TYPE_NORMAL
- en: “[Alignment] can require significant costs in compute and human annotation,
    and is often not transparent or easily reproducible, limiting progress within
    the community to advance AI alignment research.” *— from [1]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**The importance of LLaMA-2.** The impact of LLaMA-2 on open-source LLM research
    goes beyond simply setting a new state-of-the-art in terms of performance. *Why?*
    We see in [2] that LLaMA-2 adopts a fundamentally different approached compared
    to prior work. Due to the fact that closed-source LLMs are typically aligned with
    extensive amount of proprietary, human-annotated data, this process has been more
    difficult to replicate within open-source research. Although prior open-source
    models mostly leverage SFT and public sources of dialogue data, LLaMA-2 is one
    of the first open-source LLMs to invest extensively into the alignment process,
    curating a great deal of high-quality dialogues and human preferences for both
    SFT and RLHF.'
  prefs: []
  type: TYPE_NORMAL
- en: Closing Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have now studied the entire journey of open-source language models from OPT
    to LLAMA-2\. Despite the incredible amount of research that occurred between these
    two models, their proposal was only a year apart! The open-source AI research
    community moves very quickly, and keeping up with research in this area is incredibly
    exciting, interesting, and rewarding. Having access to powerful models like LLaMA-2-Chat
    is humbling. As both practitioners and researchers, we have the ability to use
    these models, learn from them, and truly gain a deeper understanding of how they
    work. Such an opportunity is unique and should not be taken for granted. Especially
    for LLMs, open-source is pretty cool!
  prefs: []
  type: TYPE_NORMAL
- en: Connect with me!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/). I study the empirical
    and theoretical foundations of deep learning. If you liked this overview, subscribe
    to my [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/),
    where I help readers understand AI research via overviews of relevant topics from
    the ground up. You can also follow me on [X](https://twitter.com/cwolferesearch)
    and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/), or
    check out my [other writings](https://medium.com/@wolfecameron) on medium!
  prefs: []
  type: TYPE_NORMAL
- en: Bibliography
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Touvron, Hugo, et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.”
    *arXiv preprint arXiv:2307.09288* (2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Zhou, Chunting, et al. “Lima: Less is more for alignment.” *arXiv preprint
    arXiv:2305.11206* (2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Touvron, Hugo, et al. “Llama: Open and efficient foundation language models.”
    *arXiv preprint arXiv:2302.13971* (2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Ainslie, Joshua, et al. “GQA: Training Generalized Multi-Query Transformer
    Models from Multi-Head Checkpoints.” *arXiv preprint arXiv:2305.13245* (2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] “Introducing Llama2: The next generation of our open source large language
    model”, *Meta*, [https://ai.meta.com/llama/.](https://ai.meta.com/llama/.)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Gudibande, Arnav, et al. “The false promise of imitating proprietary llms.”
    *arXiv preprint arXiv:2305.15717* (2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Taori, Rohan et al. “Stanford Alpaca: An Instruction-following LLaMA model.”
    (2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Chiang, Wei-Lin et al. “Vicuna: An Open-Source Chatbot Impressing GPT-4
    with 90%* ChatGPT Quality.” (2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Geng, Xinyang et al. “Koala: A Dialogue Model for Academic Research.” (2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and
    Andriy Mulyar. GPT4All: Training an assistant-style chatbot with large scale data
    distillation from GPT-3.5-Turbo, 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] Wang, Yizhong, et al. “Self-instruct: Aligning language model with self
    generated instructions.” *arXiv preprint arXiv:2212.10560* (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] Mukherjee, Subhabrata, et al. “Orca: Progressive Learning from Complex
    Explanation Traces of GPT-4.” *arXiv preprint arXiv:2306.02707* (2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[13] “Introducing Falcon LLM”, *Technology Innovation Institute*, [https://falconllm.tii.ae/.](https://falconllm.tii.ae/.)'
  prefs: []
  type: TYPE_NORMAL
- en: '[14] “Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable
    Llms.” *MosaicML*, [www.mosaicml.com/blog/mpt-7b.](http://www.mosaicml.com/blog/mpt-7b.)'
  prefs: []
  type: TYPE_NORMAL
- en: '[15] “MPT-30B: Raising the Bar for Open-Source Foundation Models.” *MosaicML*,
    [www.mosaicml.com/blog/mpt-30b.](http://www.mosaicml.com/blog/mpt-30b.)'
  prefs: []
  type: TYPE_NORMAL
- en: '[16] Gou, Jianping, et al. “Knowledge distillation: A survey.” *International
    Journal of Computer Vision* 129 (2021): 1789–1819.'
  prefs: []
  type: TYPE_NORMAL
- en: '[17] Ouyang, Long, et al. “Training language models to follow instructions
    with human feedback.” *Advances in Neural Information Processing Systems* 35 (2022):
    27730–27744.'
  prefs: []
  type: TYPE_NORMAL
- en: '[18] Glaese, Amelia, et al. “Improving alignment of dialogue agents via targeted
    human judgements.” *arXiv preprint arXiv:2209.14375* (2022).'
  prefs: []
  type: TYPE_NORMAL
