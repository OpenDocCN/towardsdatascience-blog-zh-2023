- en: Data Pipeline Orchestration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://towardsdatascience.com/data-pipeline-orchestration-9887e1b5eb7a](https://towardsdatascience.com/data-pipeline-orchestration-9887e1b5eb7a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Data pipeline management done right simplifies deployment and increases the
    availability and accessibility of data for analytics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----9887e1b5eb7a--------------------------------)[![ðŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----9887e1b5eb7a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9887e1b5eb7a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9887e1b5eb7a--------------------------------)
    [ðŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----9887e1b5eb7a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9887e1b5eb7a--------------------------------)
    Â·7 min readÂ·Apr 3, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ab99cb7a0920a4d1f69a66bee5f00b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Manuel NÃ¤geli](https://unsplash.com/@gwundrig?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: What is data orchestration?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DataOps teams use **Data Pipeline Orchestration** as a solution to centralize
    administration and oversight of end-to-end data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: The process of automating the data pipeline is known as data orchestration.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is important to manage data pipelines right as it affects almost everything,
    i.e. data quality, process speed and data governance.
  prefs: []
  type: TYPE_NORMAL
- en: What makes an effective data pipeline management?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- **Transparency and visibility.** It is important that everyone in the team
    know how exactly the data is being transformed, where it comes from and where
    the process of data transformation ends.'
  prefs: []
  type: TYPE_NORMAL
- en: '- **Faster deployments.** It is crucial to be able to reproduce the elements
    of the data pipeline continuously. Consider these elements as building blocks
    of the data pipeline. So, when there is a requirement to create a new data pipeline,
    it is important to replicate building blocks with ease for each new data process
    instead of creating them from scratch.'
  prefs: []
  type: TYPE_NORMAL
- en: '- **Efficient data governance.** By creating a structured data flow, processes
    are managed and source controlled by relevant teams and data is accessible and
    manageable with ease.'
  prefs: []
  type: TYPE_NORMAL
- en: Common problems while handling data workflows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ETL workflow complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'My experience suggests that the most typical issue while managing a data pipeline
    is the ETL process complexity. The data pipeline is usually defined by a set of
    data transformation steps moving the data from its source to its destination.
    There are numerous tools, even frameworks, approaches and techniques to do it
    and I previously wrote about it here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----9887e1b5eb7a--------------------------------)
    [## Data pipeline design patterns'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right architecture with examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----9887e1b5eb7a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: So it does make sense to source control data pipelines and the steps they consist
    of. Documenting everything is very important so the rest of the team knows exactly
    what the data solution does.
  prefs: []
  type: TYPE_NORMAL
- en: It is always nice to have some visual representation of the data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: If you deploy pipelines with Airflow, DBT, Dataform, Jinja or AWS Step functions,
    it is great and usually these tools provide a great dependency graph functionality.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9aea09abe9e5292450751d0bcf44658f.png)'
  prefs: []
  type: TYPE_IMG
- en: Example dependency graph. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Hard to replicate and deploy changes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Very often data pipelines are complex and it might take a lot of time to change
    the associated resources and deploy them. In other words, it might become a very
    time-consuming task for data and machine learning engineers to do so.
  prefs: []
  type: TYPE_NORMAL
- en: It is also essential to cover all parts of the data pipeline and not only the
    data transformation part. For example, Airflow is a great tool to orchestrate
    the data pipeline and we might want to use an S3 bucket connector for the data
    lake there but you also might want to describe the S3 resource and keep it in
    Github.
  prefs: []
  type: TYPE_NORMAL
- en: This is where Infrastructure as Code (IAC) becomes useful.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: With IAC tools like Terraform and AWS Cloudformaion, we can describe all resources
    we might need for our data pipeline and not only the ones that actually perform
    the data transformation. For example, we can describe not just the pipeline resources
    that transform the data, i.e. AWS Lambda functions and other services, but also
    data storage resources, notification settings and alarms for those microservices.
    Some IAC solutions are platform agnostic (Terraform), some are not (AWS Cloudformation)
    and all have their pros and cons **depending on the data stack in hand and DataOps
    team skills.**
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----9887e1b5eb7a--------------------------------)
    [## Infrastructure as Code for Beginners'
  prefs: []
  type: TYPE_NORMAL
- en: Deploy Data Pipelines like a pro with these templates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----9887e1b5eb7a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Data pipeline solutions that can be continuously reproduced and deployed in
    different environments are great because they are source controlled and have great
    **CI/CD features**. All these things help to **avoid any potential human errors
    and to decrease data engineering time and costs.**
  prefs: []
  type: TYPE_NORMAL
- en: So the modern approach for data pipeline management and orchestration is the
    one to reduce all potential issues mentioned above, i.e.
  prefs: []
  type: TYPE_NORMAL
- en: To reduce human errors, be able to easily replicate data pipeline resources,
    visualize the dependencies and improve data quality.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Data orchestration done right increases the availability and accessibility
    of data for analytics.**'
  prefs: []
  type: TYPE_NORMAL
- en: Typical data flow that needs to be managed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Typically by data pipeline we mean a collection of data-related resources that
    help us to deliver and transform the data from point A to point B.
  prefs: []
  type: TYPE_NORMAL
- en: 'By resources we mean tools and at the conceptual level we have three main data
    processing tasks that must be able to perform effectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '**- Data Storage**'
  prefs: []
  type: TYPE_NORMAL
- en: This can be data lakes in Google Cloud Storage, AWS S3 data lakes, etc. or any
    Relational and non-relational databases or even third-party resources available
    via APIs. they all serve one purpose to store the data. And in the majority of
    cases, this is where the data will be coming from on our data pipeline design
    diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '**- Data loading or ingestion.**'
  prefs: []
  type: TYPE_NORMAL
- en: This can be managed tools (Fivetran, Stitch, etc.) or something bespoke like
    serverless microservices and various AWS Lambda or GCP Cloud Functions. They all
    serve one purpose to perform an ETL and to load data into the destination, i.e.
    data warehouse or another data lake depending on our data platform architecture
    type. Sometimes it might be more efficient to use tools that scale well, i.e.
    Spark.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----9887e1b5eb7a--------------------------------)
    [## Data Platform Architecture Types'
  prefs: []
  type: TYPE_NORMAL
- en: How well does it answer your business needs? Dilemma of a choice.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----9887e1b5eb7a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**- Data transformation tools**'
  prefs: []
  type: TYPE_NORMAL
- en: Historically the most natural way to transform data is SQL. This is a common
    data manipulation language recognised by all teams, Business Intelligence (BI)
    and data analysts, software engineers and data scientists. So there is a variety
    of tools at the moment that can offer reliable source-controlled data transformation
    with SQL queries, i.e. Jinja, DBT, Dataform, AWS Step Functions etc.
  prefs: []
  type: TYPE_NORMAL
- en: '**- Business intelligence**'
  prefs: []
  type: TYPE_NORMAL
- en: These tools help to deliver analyses and insights. Some of them a free community
    tools, i.e. Looker Studio and others are subscription-based paid only. All have
    pros and cons and it might be wise to choose one based on the company size. For
    example, SMEs donâ€™t usually need to pay extra for BI OLAP cube features where
    data is being additionally analyzed and transformed. All they need is to email
    the daily dashboard with the main KPIs.
  prefs: []
  type: TYPE_NORMAL
- en: The list of tools is massive and some great BI solutions are also available
    with free features.
  prefs: []
  type: TYPE_NORMAL
- en: '**Not an extensive list:**'
  prefs: []
  type: TYPE_NORMAL
- en: AWS Quicksight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sisense
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metabase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PowerBI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The problem is that data tools with fully integrated features donâ€™t exist.**'
  prefs: []
  type: TYPE_NORMAL
- en: How do we link and connect data services together?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Wrapping our data processes **into one solution** can also be achieved with
    a little bit of programming and Infrastructure as Code techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conceptually we might want to deploy a data pipeline like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b999ac569c1399f8eacbfa0d66c53a15.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: How do we make this solution robust and cost-effective?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: With no doubt, the most cost-effective way would be to use **serverless architecture**.
    Consider this data pipeline below, for example. It can be deployed with IAC (AWS
    Cloudformation or Terraform) and all main parts are integrated into one complete
    data solution.
  prefs: []
  type: TYPE_NORMAL
- en: We have AWS Lambda functions, AWS Step functions, a relational database, data
    lake storage buckets and a data lake house solution (AWS Athena).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d6fa66e126398bfe0bd267f57bf7059.png)'
  prefs: []
  type: TYPE_IMG
- en: Example DAG. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Consider this data pipeline example which consists of an AWS Lambda function.
    Standalone, microservice like this is just a lambda function but there is so much
    more we can do with it, i.e. extract data from APIs, export data from relational
    databases, invoke and trigger other services, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can deploy the pipeline with just one AWS CLI command using infrastructure
    as code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a53bea5cc9239238a4bb2fce87a101e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Deploy result. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we should be able to create a simple Step Function and can use it
    as a template to extend and improve our ETL pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Should we choose to **be platform agnostic** we can deploy the complete pipeline
    with **Terraform** and use various data services across different cloud platforms.
    Often I use this data pipeline design pattern with AWS Lambda functions to run
    SQL queries in my BigQuery data warehouse solution or to perform any other ETL
    task. Very useful.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Wrapping our data processes into one solution might be challenging and would
    require some **imagination**.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure as code sounds like the right way to go. Indeed, it simplifies
    data solution deployments and replication, eliminates human errors and helps to
    perform data engineering and MLOps tasks faster. Although it might look complex
    and sophisticated, it all comes with experience and requires time to learn. Donâ€™t
    hesitate to invest a bit of yours in learning it. This is a very rewarding skill.
  prefs: []
  type: TYPE_NORMAL
- en: It can offer multitudes of options for data pipeline orchestration.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: With just a little bit of coding and knowing how APIs work opportunities for
    data pipeline design and management become endless.
  prefs: []
  type: TYPE_NORMAL
- en: Recommended read
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a?source=post_page-----9887e1b5eb7a--------------------------------)
    [## Create MySQL and Postgres instances using AWS Cloudformation'
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure as Code for database practitioners
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a?source=post_page-----9887e1b5eb7a--------------------------------)
    [](https://aws.amazon.com/cloudformation/?source=post_page-----9887e1b5eb7a--------------------------------)
    [## Provision Infrastructure as Code - AWS CloudFormation - AWS
  prefs: []
  type: TYPE_NORMAL
- en: AWS CloudFormation Scale your infrastructure worldwide and manage resources
    across all AWS accounts and regions throughâ€¦
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: aws.amazon.com](https://aws.amazon.com/cloudformation/?source=post_page-----9887e1b5eb7a--------------------------------)
    [](https://aws.amazon.com/step-functions/?source=post_page-----9887e1b5eb7a--------------------------------)
    [## Serverless Workflow Orchestration - AWS Step Functions - Amazon Web Services
  prefs: []
  type: TYPE_NORMAL
- en: AWS Step Functions 4,000 state transitions per month Use code to process data
    on demand with large-scale parallelâ€¦
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: aws.amazon.com](https://aws.amazon.com/step-functions/?source=post_page-----9887e1b5eb7a--------------------------------)
    [](https://www.terraform.io/?source=post_page-----9887e1b5eb7a--------------------------------)
    [## Terraform by HashiCorp
  prefs: []
  type: TYPE_NORMAL
- en: Terraform is an open-source infrastructure as code software tool that enables
    you to safely and predictably createâ€¦
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.terraform.io](https://www.terraform.io/?source=post_page-----9887e1b5eb7a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
