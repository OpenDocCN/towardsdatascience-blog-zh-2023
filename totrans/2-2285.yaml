- en: Exploding & Vanishing Gradient Problem in Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的梯度爆炸与消失问题
- en: 原文：[https://towardsdatascience.com/vanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b](https://towardsdatascience.com/vanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/vanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b](https://towardsdatascience.com/vanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b)
- en: How to ensure your neural network doesn’t “die” or “blow-up”
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何确保你的神经网络不会“死掉”或“爆炸”
- en: '[](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)
    ·9 min read·Dec 8, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)
    ·阅读时间 9 分钟·2023年12月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8ec02d8429b5e34801dd4ccb016d5e74.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ec02d8429b5e34801dd4ccb016d5e74.png)'
- en: '[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network).
    title=”neural network icons.” Neural network icons created by Paul J. — Flaticon.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network)。标题=”神经网络图标。”
    神经网络图标由 Paul J. 创建 — Flaticon。'
- en: What are Vanishing & Exploding Gradients?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是梯度消失与梯度爆炸？
- en: 'In one of my previous posts, we explained neural networks learn through the
    backpropagation algorithm. The main idea is that we start on the output layer
    and move or “propagate” the error all the way to the input layer updating the
    weights with respect to the loss function as we go. If you are unfamiliar with
    this, then I highly recommend you check that post:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前的一篇文章中，我们解释了神经网络如何通过反向传播算法进行学习。主要思路是我们从输出层开始，将误差“传播”到输入层，在这个过程中根据损失函数更新权重。如果你对此不熟悉，我强烈建议你查看那篇文章：
- en: '[](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----c8f48ec6a80b--------------------------------)
    [## Forward Pass & Backpropagation: Neural Networks 101'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----c8f48ec6a80b--------------------------------)
    [## 前向传播与反向传播：神经网络 101'
- en: Explaining how neural networks “train” and “learn” patterns in data by hand
    and in code using PyTorch
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释神经网络如何通过手动和使用 PyTorch 的代码“训练”和“学习”数据中的模式
- en: towardsdatascience.com](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----c8f48ec6a80b--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----c8f48ec6a80b--------------------------------)'
- en: The weights are updated using their partial derivative with respect to the loss
    function. The problem is that these gradients get smaller and smaller as we approach
    the lower layers of the network. This leads to the lower layers’ weights barely
    changing when training the network. This is known as the *vanishing gradient problem.*
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 权重通过其相对于损失函数的偏导数进行更新。问题在于，当我们接近网络的下层时，这些梯度变得越来越小。这导致下层的权重在训练网络时几乎没有变化。这被称为*梯度消失问题*。
- en: The opposite can be true where gradients continue getting larger through the
    layers. This is the *exploding gradient problem* whichis mainly an issue in [***recurrent
    neural networks***](https://en.wikipedia.org/wiki/Recurrent_neural_network).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，梯度也可能在各层中持续增大。这就是*梯度爆炸问题*，它主要是[***递归神经网络***](https://en.wikipedia.org/wiki/Recurrent_neural_network)中的一个问题。
- en: However, a [***paper***](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
    published by [***Xavier Glorot***](https://www.linkedin.com/in/xglorot/?originalSubdomain=ca)
    and [***Yoshua Bengio***](https://en.wikipedia.org/wiki/Yoshua_Bengio) in 2010
    diagnosed several reasons why this is happening to the gradients. The main culprits
    were the [***sigmoid activation function***](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)and
    how weights are initialised (typically from the standard normal distribution).
    This combination leads to the variances changing between layers until they *saturate*
    at the extreme edges of the sigmoid function.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，2010年由[***Xavier Glorot***](https://www.linkedin.com/in/xglorot/?originalSubdomain=ca)和[***Yoshua
    Bengio***](https://en.wikipedia.org/wiki/Yoshua_Bengio)发布的[***论文***](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)诊断了导致梯度出现问题的几个原因。主要的罪魁祸首是[***sigmoid激活函数***](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)和权重的初始化方式（通常来自标准正态分布）。这种组合导致了层间方差的变化，直到它们在sigmoid函数的极端边缘*饱和*。
- en: Below is the mathematical equation and plot of the sigmoid function. Notice
    that in its extremes, the gradient becomes zero. Therefore, no “learning” is done
    at these saturation points.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是sigmoid函数的数学方程和图示。请注意，在其极端情况下，梯度变为零。因此，在这些饱和点没有“学习”发生。
- en: '![](../Images/29fff662bec33adab7258d011d2e9db6.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29fff662bec33adab7258d011d2e9db6.png)'
- en: Sigmoid function. Equation by author in LaTeX.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数。方程由作者使用LaTeX编写。
- en: '![](../Images/b71b01559c120a8967c3bf80bc04c16f.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b71b01559c120a8967c3bf80bc04c16f.png)'
- en: Sigmoid function. Plot generated by author in Python.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数。由作者在Python中生成的图示。
- en: We will now go through some techniques that can reduce the chance of our gradients
    vanishing or exploding during training.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将介绍一些可以减少训练过程中梯度消失或爆炸的技术。
- en: 'If you want to learn more about activation functions along with their pros
    and cons, check my previous post on the subject:'
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于激活函数及其优缺点的信息，请查看我之前的帖子：
- en: '[](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----c8f48ec6a80b--------------------------------)
    [## Activation Functions & Non-Linearity: Neural Networks 101'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----c8f48ec6a80b--------------------------------)
    [## 激活函数与非线性：神经网络101'
- en: Explaining why neural networks can learn (nearly) anything and everything
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释神经网络为何能学习（几乎）任何和一切
- en: towardsdatascience.com](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----c8f48ec6a80b--------------------------------)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----c8f48ec6a80b--------------------------------)
- en: Glorot/Xavier Initialisation
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Glorot/Xavier初始化
- en: Fortunately, the authors of the paper suggested methods to neutralise the above
    problem. They suggested a new initialisation method for the weights, named Glorot
    initialisation after the author, that ensures the variance between layers remains
    constant.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，论文的作者提出了中和上述问题的方法。他们建议了一种新的权重初始化方法，命名为Glorot初始化，以作者命名，确保层间的方差保持不变。
- en: 'The paper linked above has the full mathematical details, but their proposed
    initialisation strategy was:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 上述论文中包含了完整的数学细节，但他们提出的初始化策略是：
- en: '**Normal Distribution**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**正态分布**'
- en: For a normal distribution*,* ***X ~ N(0, 𝜎²)****,* the weights will be initialised
    as follows*:*
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于正态分布***X ~ N(0, 𝜎²)***，权重将如下初始化：
- en: '![](../Images/5e33362e36072d25bec235c65713679a.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e33362e36072d25bec235c65713679a.png)'
- en: Mean and variance for Glorot normal distribution initialisation. Equation by
    author in LaTeX.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Glorot正态分布初始化的均值和方差。方程由作者使用LaTeX编写。
- en: If ***n_in*** ***=*** ***n_out*** then we have [***LeCun initialisation***](https://wandb.ai/sauravmaheshkar/initialization/reports/A-Gentle-Introduction-To-Weight-Initialization-for-Neural-Networks--Vmlldzo2ODExMTg)named
    after the computer scientist [***Yann LeCun***](https://en.wikipedia.org/wiki/Yann_LeCun).
    This initialisation was proposed in the 1990s, a decade before Glorot’s paper.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果***n_in*** ***=*** ***n_out***，则我们有[***LeCun初始化***](https://wandb.ai/sauravmaheshkar/initialization/reports/A-Gentle-Introduction-To-Weight-Initialization-for-Neural-Networks--Vmlldzo2ODExMTg)以计算机科学家[***Yann
    LeCun***](https://en.wikipedia.org/wiki/Yann_LeCun)命名。这个初始化方法是在Glorot的论文发表前十年，即1990年代提出的。
- en: '![](../Images/742f40143e3cdca52d66f0ba0d4d427b.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/742f40143e3cdca52d66f0ba0d4d427b.png)'
- en: LeCun initialisation. Equation by author in LaTeX.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: LeCun初始化。方程由作者使用LaTeX编写。
- en: '**Uniform Distribution**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**均匀分布**'
- en: For a uniform distribution ***X ~ U(-a, a)****:*
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于均匀分布***X ~ U(-a, a)***：
- en: '![](../Images/a93c6c22f95680d5b3463ea4a9cbd8d7.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a93c6c22f95680d5b3463ea4a9cbd8d7.png)'
- en: Mean and variance for Glorot uniform distribution initialisation. Equation by
    author in LaTeX.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Glorot均匀分布初始化的均值和方差。方程由作者在LaTeX中编写。
- en: Setting up our neural networks using these initialisations leads them to converge
    faster since the weights are not too small or large at the beginning of training.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些初始化设置我们的神经网络可以更快地收敛，因为在训练开始时权重不会过小或过大。
- en: The above expressions are only valid for the sigmoid and [***tanh***](https://medium.com/towards-data-science/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)
    activation functions. For example, for the [***ReLU***](/breaking-linearity-with-relu-d2cfa7ebf264)(rectified
    linear unit) activation function, the normal distribution variance needs to be
    initialised using ***1/n_in.***
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以上表达式仅适用于sigmoid和[***tanh***](https://medium.com/towards-data-science/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)激活函数。例如，对于[***ReLU***](/breaking-linearity-with-relu-d2cfa7ebf264)（修正线性单元）激活函数，正常分布方差需要使用***1/n_in***进行初始化。
- en: A full list of activation functions and their corresponding initialisations
    can be found [***here***](/weight-initialization-and-activation-functions-in-deep-learning-50aac05c3533)for
    the interested reader.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣的读者可以在[***这里***](/weight-initialization-and-activation-functions-in-deep-learning-50aac05c3533)找到完整的激活函数及其对应的初始化列表。
- en: Better Activation Functions
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更好的激活函数
- en: ReLU
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReLU
- en: 'For most industry standard neural networks, the sigmoid activation function
    has largely been abandoned and replaced with the ReLU as it doesn’t saturate for
    large positive values (it’s also more compute efficient):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数行业标准的神经网络，sigmoid激活函数已被大量弃用，取而代之的是ReLU，因为它对大正值不会饱和（它也更具计算效率）：
- en: '![](../Images/ad1cf21ebe2a1666034bee8102b78d29.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad1cf21ebe2a1666034bee8102b78d29.png)'
- en: ReLU function. Equation by author in LaTeX.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU函数。方程由作者在LaTeX中编写。
- en: '![](../Images/cf6409fc078754bcab71802d9bbe477e.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf6409fc078754bcab71802d9bbe477e.png)'
- en: ReLU activation function. Plot generated by author in Python.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU激活函数。图由作者在Python中生成。
- en: However, ReLU is not perfect and suffers from the [***dying ReLU problem***](https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks)***.***
    This is where neurons start “dying” as they only output zero because their inputted
    weighted sum is always negative. This leads to a zero gradient, so the network
    stops “learning” anything. This is particularly bad for a large learning rate.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，ReLU并不完美，且受到[***dying ReLU问题***](https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks)的影响***。***
    这是指神经元开始“死亡”，因为它们只输出零，因为它们的输入加权和总是负的。这导致零梯度，从而网络停止“学习”任何东西。这对较大的学习率特别糟糕。
- en: 'ReLU is easily applied in [***PyTorch***](https://pytorch.org/) as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU可以很容易地在[***PyTorch***](https://pytorch.org/)中应用，如下所示：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Leaky ReLU
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Leaky ReLU
- en: 'You can get around this by using variants of the classic ReLU function such
    as [***‘Leaky’ ReLU***](https://www.educative.io/answers/what-is-leaky-relu#)
    where the negative inputs are not zero, but rather have some shallow slope with
    gradient ***α***:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用经典ReLU函数的变体来解决这个问题，例如[***‘Leaky’ ReLU***](https://www.educative.io/answers/what-is-leaky_relu#)，其中负输入不是零，而是具有一些浅坡度的梯度***α***：
- en: '![](../Images/d5dbcbc68a24c8ab953e5b19704fabfa.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5dbcbc68a24c8ab953e5b19704fabfa.png)'
- en: Leaky ReLU function. Equation by author in LaTeX.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU函数。方程由作者在LaTeX中编写。
- en: '![](../Images/4d44ec1c36473a7f01361080256878af.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d44ec1c36473a7f01361080256878af.png)'
- en: Leaky ReLU, notice the small gradient for negative x values. Plot generated
    by author in Python.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU，注意负x值的小梯度。图由作者在Python中生成。
- en: The Leaky ReLU often [outperforms](https://ai.stackexchange.com/questions/40576/why-use-relu-over-leaky-relu)
    the classic ReLU as it reduces the chance of this dying neuron problem, hence
    more robust learning. It is often found that a larger “leak” is better but within
    reason.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU通常[超越](https://ai.stackexchange.com/questions/40576/why-use-relu-over-leaky-relu)经典ReLU，因为它减少了这种“死亡神经元”问题的可能性，从而实现更稳健的学习。通常发现更大的“泄漏”更好，但要适度。
- en: 'There are other variants that also frequently improve performance over the
    basic ReLU :'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他变体也常常改善基本ReLU的性能：
- en: '[**Randomised Leaky ReLU (RReLU)**](https://paperswithcode.com/method/rrelu):
    During training the hyperparameter ***α*** is randomly initialised.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**随机Leaky ReLU (RReLU)**](https://paperswithcode.com/method/rrelu)：在训练过程中，超参数***α***被随机初始化。'
- en: '[**Parametric Leaky ReLU (PReLU):**](https://www.educative.io/answers/what-is-parametric-relu)During
    training the hyperparameter ***α*** is learned.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**参数化 Leaky ReLU (PReLU):**](https://www.educative.io/answers/what-is-parametric-relu)
    在训练过程中超参数 ***α*** 会被学习。'
- en: 'You can apply Leaky ReLU in PyTorch as:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 PyTorch 中应用 Leaky ReLU，如下所示：
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Exponential Linear Unit
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指数线性单元
- en: The last activation function we will consider is the [***exponential linear
    unit***](https://paperswithcode.com/method/elu) (ELU).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将考虑的最后一个激活函数是 [***指数线性单元***](https://paperswithcode.com/method/elu)（ELU）。
- en: '![](../Images/e0667299960e38ea18c78404225f18e0.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0667299960e38ea18c78404225f18e0.png)'
- en: ELU function. Equation by author in LaTeX.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ELU 函数。作者使用 LaTeX 表示的方程。
- en: '![](../Images/1acad18951fc3a113560ef45efc7a38d.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1acad18951fc3a113560ef45efc7a38d.png)'
- en: ELU, notice the exponential value for negative values. Plot generated by author
    in Python.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ELU，注意负值的指数值。图像由作者在 Python 中生成。
- en: 'The key differences between ELU and ReLU are:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ELU 和 ReLU 之间的主要区别是：
- en: '*For negative values, the ELU function is not zero, so it alleviates the dying
    neuron problem with the vanilla ReLU.*'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对于负值，ELU 函数不为零，因此缓解了普通 ReLU 的神经元死亡问题。*'
- en: '*Its gradient is non-zero for negative inputs.*'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*它的梯度对于负输入是非零的。*'
- en: '*ELU has a slower compute speed due to the exponential than ReLU and its variants.*'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*由于指数计算，ELU 的计算速度比 ReLU 及其变体要慢。*'
- en: '*The ELU generally leads to better performance than the ReLU*'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ELU 通常比 ReLU 性能更好*'
- en: 'ELU in PyTorch:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 中的 ELU：
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Which one to use?
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择哪个？
- en: With so many activation functions, it’s hard to know which one to choose. The
    general rule is ***ELU > Leaky ReLU > ReLU > tanh > sigmoid***. However, compute
    speed may play a factor in your model, so you may have to reconsider which activation
    you go for. Always best to consider your problem at hand and also play around
    with a few to see which one is best.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有这么多激活函数，很难知道选择哪个。一般规则是 ***ELU > Leaky ReLU > ReLU > tanh > sigmoid***。然而，计算速度可能会影响你的模型，所以你可能需要重新考虑选择哪个激活函数。最好考虑一下你的问题并尝试几种，看看哪个最适合。
- en: 'The code used to generate these activation function plots is available at my
    GitHub here:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成这些激活函数图的代码可以在我的 GitHub 上找到：
- en: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/activation_functions.py?source=post_page-----c8f48ec6a80b--------------------------------)
    [## Medium-Articles/Data Science Basics/activation_functions.py at main · egorhowell/Medium-Articles'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/activation_functions.py?source=post_page-----c8f48ec6a80b--------------------------------)
    [## Medium-Articles/Data Science Basics/activation_functions.py at main · egorhowell/Medium-Articles'
- en: Code I use in my medium blog/articles. Contribute to egorhowell/Medium-Articles
    development by creating an account on…
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我在我的 Medium 博客/文章中使用的代码。通过创建一个账户为 egorhowell/Medium-Articles 开发做贡献……
- en: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/activation_functions.py?source=post_page-----c8f48ec6a80b--------------------------------)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/activation_functions.py?source=post_page-----c8f48ec6a80b--------------------------------)
- en: Batch Normalisation
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量归一化
- en: Using Glorot initialisation and the ReLU (and variants) activation function
    helps curtail the chance of vanishing/exploding gradients at the beginning of
    the algorithm, but doesn’t help during training.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Glorot 初始化和 ReLU（及其变体）激活函数有助于减少算法开始时梯度消失/爆炸的可能性，但在训练过程中没有帮助。
- en: One way to prevent the vanishing/exploding gradients during training is [***Batch
    Normalisation***](https://en.wikipedia.org/wiki/Batch_normalization#:~:text=Batch%20normalization%20%28also%20known%20as,%2Dcentering%20and%20re%2Dscaling.)
    (BN). This process zero centers and re-normalising the outputs or inputs just
    before or after the activation function is applied. Then, it shifts and scales
    this result allowing each layer to have its own “learned” mean and variance.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 防止训练过程中梯度消失/爆炸的一种方法是 [***批量归一化***](https://en.wikipedia.org/wiki/Batch_normalization#:~:text=Batch%20normalization%20%28also%20known%20as,%2Dcentering%20and%20re%2Dscaling.)（BN）。这个过程在激活函数应用之前或之后零中心化和重新归一化输出或输入。然后，它会对结果进行平移和缩放，使每一层都有自己“学习”的均值和方差。
- en: This is analogous to why we scale and normalise our features before training.
    It ensures everything is on an even playing field and large valued features won’t
    drown out the smaller ones. Batch Norm is applying this process after the output
    of each layer as they are inputs into the next!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于我们在训练前对特征进行缩放和归一化的原因。它确保所有特征处于同一水平，并且较大值的特征不会淹没较小值的特征。批量归一化是在每一层的输出之后应用这一过程，因为它们是下一层的输入！
- en: Great [explanation here](/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739)
    about why we need to normalise our features.
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 关于为什么我们需要归一化特征的[详细解释在这里](/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739)。
- en: 'The algorithm looks like as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 算法如下所示：
- en: '![](../Images/ba0f4224ca3afadcc5641717efdf783f.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba0f4224ca3afadcc5641717efdf783f.png)'
- en: Batch Normalisation algorithm. Equation generated in LaTeX by author.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化算法。方程由作者在LaTeX中生成。
- en: 'Where:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '***x_i***​ is the input to a batch normalisation layer.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***x_i*** 是批量归一化层的输入。'
- en: '***μ_B***​ is the mean of the batch.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***μ_B*** 是批次的均值。'
- en: '***σ²_B***​ is the variance of the batch.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***σ²_B*** 是批次的方差。'
- en: '***ϵ*** is a small constant added for numerical stability (to avoid division
    by zero).'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***ϵ*** 是为数值稳定性（避免除以零）添加的小常数。'
- en: '***γ*** is the scale parameter that is learned during training'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***γ*** 是在训练过程中学习的缩放参数。'
- en: '***β*** is the shift parameter that is learned during training'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***β*** 是在训练过程中学习的偏移参数。'
- en: We can walk through it to make it clearer. The first part is just calculating
    the mean and variance of the inputs or outputs of each layer in the network. These
    outputs are then normalised using the mean and variance. The final part is to
    scale, using ***γ*** hyperparameter, and shift, using ***β*** hyperparameter***.***
    These hyperparameters are learned by the network, which is what makes BN so powerful.
    Each layer will have its custom transformation!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过它使其更清晰。第一部分只是计算网络中每层输入或输出的均值和方差。这些输出随后使用均值和方差进行归一化。最后一部分是使用 ***γ*** 超参数进行缩放，使用
    ***β*** 超参数进行偏移。这些超参数由网络学习，这使得BN如此强大。每一层都会有其自定义的变换！
- en: One other important thing to note is that whilst training BN keeps track of
    an [***exponential moving average***](https://en.wikipedia.org/wiki/Moving_average#:~:text=An%20exponential%20moving%20average%20%28EMA,decreases%20exponentially%2C%20never%20reaching%20zero.)
    (EMA) for both the mean and variance. This is used when predicting because you
    can’t really apply BN to a single prediction row, which is what happens during
    inference.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 另一件重要的事情是，在训练期间，BN 会跟踪[***指数移动平均***](https://en.wikipedia.org/wiki/Moving_average#:~:text=An%20exponential%20moving%20average%20%28EMA,decreases%20exponentially%2C%20never%20reaching%20zero.)（EMA）的均值和方差。这在预测时使用，因为你不能真的将BN应用于单个预测行，这在推理过程中发生。
- en: 'You can apply BN in PyTorch as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在PyTorch中按如下方式应用BN：
- en: '[PRE3]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see, we insert a batch norm layer in between all the hidden layers.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们在所有隐藏层之间插入一个批量归一化层。
- en: Batch normalisation has been shown to improve the training of deep neural networks
    and reduce the impact of the vanishing gradients problem. However, training each
    epoch is slower as we need to pass hidden layers outputs through batch normalisation
    layers that increase the total number of parameters in the network.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化已被证明能改善深度神经网络的训练，并减少梯度消失问题的影响。然而，每个epoch的训练速度较慢，因为我们需要将隐藏层的输出通过批量归一化层，这增加了网络中的总参数数量。
- en: Batch norm can also act like regulariser!
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 批量归一化也可以作为正则化器！
- en: Gradient Clipping
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度裁剪
- en: The final method to reduce the chance of vanishing/exploding gradient is to
    clip the gradient at some maximum threshold. For example, we can clip the gradient
    at a maximum value of 1\. So, any gradient that is greater than that will be “clipped”
    down to 1\. This technique is often applied to recurrent neural networks (RNN)
    as it is hard to apply batch norm to RNNs.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 减少梯度消失/爆炸几率的最终方法是将梯度裁剪到某个最大阈值。例如，我们可以将梯度裁剪到最大值1。这样，任何大于1的梯度都会被“裁剪”到1。这种技术通常应用于递归神经网络（RNN），因为很难对RNN应用批量归一化（batch
    norm）。
- en: 'This is an example of how gradient clipping can be applied in PyTorch:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个如何在PyTorch中应用梯度裁剪的示例：
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here `max_norm` is the threshold the gradients are clipped at.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 `max_norm` 是梯度被裁剪的阈值。
- en: Summary & Further Thoughts
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结与进一步思考
- en: 'Vanishing and exploding gradients occur due to the variance change between
    neural network layers and gradients decreasing due to the multiplication effect
    as they are backpropagated. In this post, we discussed three methods to reduce
    the chance of this effect: better activation functions, batch normalisation, and
    gradient clipping. In my opinion, batch normalisation is probably the best option
    combined with a ReLU activation function. Batch normalisation ensures the variance
    across each layer is constant by normalising and scaling the inputs.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 消失和爆炸梯度的发生是由于神经网络层之间的方差变化以及由于反向传播时的乘法效应导致梯度减少。在这篇文章中，我们讨论了三种减少这种效应的方法：更好的激活函数、批量归一化和梯度裁剪。在我看来，批量归一化与
    ReLU 激活函数结合使用可能是最好的选择。批量归一化通过规范化和缩放输入来确保每层之间的方差保持恒定。
- en: Another Thing!
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另一个信息！
- en: I have a free newsletter, [**Dishing the Data**](https://dishingthedata.substack.com/),
    where I share weekly tips for becoming a better Data Scientist. There is no “fluff”
    or “clickbait,” just pure actionable insights from a practicing Data Scientist.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一个免费的通讯，[**Dishing the Data**](https://dishingthedata.substack.com/)，每周分享成为更好的数据科学家的技巧。没有“空洞的内容”或“点击诱饵”，只有来自实践数据科学家的纯粹可操作见解。
- en: '[](https://newsletter.egorhowell.com/?source=post_page-----c8f48ec6a80b--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://newsletter.egorhowell.com/?source=post_page-----c8f48ec6a80b--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
- en: How To Become A Better Data Scientist. Click to read Dishing The Data, by Egor
    Howell, a Substack publication with…
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何成为更好的数据科学家。点击阅读《Dishing The Data》，由 Egor Howell 发表的 Substack 出版物…
- en: newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----c8f48ec6a80b--------------------------------)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----c8f48ec6a80b--------------------------------)'
- en: Connect With Me!
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联系我！
- en: '[**YouTube**](https://www.youtube.com/@egorhowell)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**YouTube**](https://www.youtube.com/@egorhowell)'
- en: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
- en: '[**Twitter**](https://twitter.com/EgorHowell)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Twitter**](https://twitter.com/EgorHowell)'
- en: '[**GitHub**](https://github.com/egorhowell)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**GitHub**](https://github.com/egorhowell)'
- en: References & Further Reading
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料与进一步阅读
- en: '[*Andrej Karpathy Neural Network Course*](https://www.youtube.com/watch?v=i94OvYb6noo)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*Andrej Karpathy 神经网络课程*](https://www.youtube.com/watch?v=i94OvYb6noo)'
- en: '[*PyTorch site*](https://pytorch.org/)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*PyTorch 网站*](https://pytorch.org/)'
- en: '[*Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition.
    Aurélien Géron. September 2019\. Publisher(s): O’Reilly Media, Inc. ISBN: 9781492032649*](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)*.*'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*《动手学习机器学习：使用 Scikit-Learn、Keras 和 TensorFlow（第2版）》，奥雷利安·热龙，2019年9月。出版商：O''Reilly
    媒体公司，ISBN: 9781492032649*](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)*.*'
- en: '*Paper on vanishing gradients study:* [*https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf*](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关于消失梯度研究的论文：* [*https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf*](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)'
- en: '[*Great visual explanation of batch norm*](/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*批量归一化的精彩视觉解释*](/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739)'
- en: '[*Great video on the vanishing gradient problem*](https://www.youtube.com/watch?v=8z3DFk4VxRo)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*关于消失梯度问题的精彩视频*](https://www.youtube.com/watch?v=8z3DFk4VxRo)'
