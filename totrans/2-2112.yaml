- en: 'TimeGPT: The First Foundation Model for Time Series Forecasting'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/timegpt-the-first-foundation-model-for-time-series-forecasting-bf0a75e63b3a](https://towardsdatascience.com/timegpt-the-first-foundation-model-for-time-series-forecasting-bf0a75e63b3a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explore the first generative pre-trained forecasting model and apply it in a
    project with Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcopeixeiro?source=post_page-----bf0a75e63b3a--------------------------------)[![Marco
    Peixeiro](../Images/7cf0a81d87281d35ff47f51e3026a3e9.png)](https://medium.com/@marcopeixeiro?source=post_page-----bf0a75e63b3a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bf0a75e63b3a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bf0a75e63b3a--------------------------------)
    [Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page-----bf0a75e63b3a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bf0a75e63b3a--------------------------------)
    ·12 min read·Oct 24, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69c006f4feeba3f9fe392aff3b2a35f0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Photo by [Boris Smokrovic](https://unsplash.com/@borisworkshop?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: The field of time series forecasting is going through a very exciting period.
    In only the last three years, we have seen many important contributions, like
    [N-BEATS](https://medium.com/towards-data-science/the-easiest-way-to-forecast-time-series-using-n-beats-d778fcc2ba60),
    [N-HiTS](https://medium.com/towards-data-science/all-about-n-hits-the-latest-breakthrough-in-time-series-forecasting-a8ddcb27b0d5),
    [PatchTST](https://medium.com/towards-data-science/patchtst-a-breakthrough-in-time-series-forecasting-e02d48869ccc)
    and [TimesNet](https://medium.com/towards-data-science/timesnet-the-latest-advance-in-time-series-forecasting-745b69068c9c).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, [large language models (LLMs)](https://medium.com/towards-data-science/catch-up-on-large-language-models-8daf784f46f8)
    have gained a lot of popularity lately, with applications like ChatGPT, as they
    can adapt to a wide variety of tasks without further training.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Which leads to the question: can foundation models exist for time series like
    they exist for natural language processing? Is it possible that a large model
    pre-trained on massive amounts of time series data can then produce accurate predictions
    on unseen data?'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: With [TimeGPT-1](https://arxiv.org/pdf/2310.03589.pdf), proposed by Azul Garza
    and Max Mergenthaler-Canseco, the authors adapt the techniques and architecture
    behind LLMs to the field of forecasting, successfully building the first time
    series foundation model capable of zero-shot inference.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we first explore the architecture behind TimeGPT and how the
    model was trained. Then, we apply it in a forecasting project to evaluate its
    performance against other state-of-the-art methods, like N-BEATS, N-HiTS and PatchTST.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: For more details, make sure to read the [original paper](https://arxiv.org/pdf/2310.03589.pdf).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '***Learn the latest time series analysis techniques with my*** [***free time
    series cheat sheet***](https://www.datasciencewithmarco.com/pl/2147608294) ***in
    Python! Get the implementation of statistical and deep learning techniques, all
    in Python and TensorFlow!***'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s get started!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Explore TimeGPT
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, TimeGPT is a first attempt at creating a foundation model
    for time series forecasting.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5a18b137f0b737e2e4df634cbc8111b2.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: Illustration of how TimeGPT was trained to make inference on unseen data. Image
    by Azul Garza and Max Mergenthaler-Canseco from [TimeGPT-1](https://arxiv.org/pdf/2310.03589.pdf)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: From the figure above, we can see that the general idea behind TimeGPT is to
    train a model on massive amounts of data from different domains to then produce
    zero-shot inference on unseen data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this method relies on **transfer learning**, which is the capacity
    of a model to solve a new task using its knowledge gained during training.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Now, this only works if the model is large enough, and if it is trained on lots
    of data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Training TimeGPT
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To that end, the authors trained TimeGPT on more than 100 billion data points
    all coming from open-source time series data. The dataset spans a wide array of
    domains, from finance, economics and weather, to web traffic, energy and sales.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Note that the authors do not disclose the sources of public data used to curate
    100 billion data points.
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This variety is critical for the success of a foundation model, as it can learn
    different temporal patterns and therefore generalize better.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can expect weather data to have a daily (hotter during the day
    than at night) and yearly seasonality, while car traffic data can have a daily
    seasonality (more cars on the road during the day than at night) and a weekly
    seasonality (more cars on the road during the week than on weekends).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: To ensure the robustness and generalization capabilities of the model, preprocessing
    was kept to a minimum. In fact, only missing values were filled, and the rest
    was kept in its raw form. While the authors do not specify the method for data
    imputation, I suspect that some kind of interpolation technique was used, like
    linear, spline or moving average interpolation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: The model was then trained over multiple days, during which hyperparameters
    and learning rates were optimized. While the authors do not disclose how many
    days and GPUs were required for training, we do know that the model is implemented
    in PyTorch, and it uses the Adam optimizer and a learning rate decay strategy.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Architecture of TimeGPT
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TimeGPT leverages the Transformer architecture with self-attention mechanism
    based on the seminal work of Google and the University of Toronto in 2017.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: TimeGPT 利用基于 Google 和多伦多大学 2017 年开创性工作的自注意力机制的 Transformer 架构。
- en: '![](../Images/cf13280001ad8725c402657bb8fe31eb.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf13280001ad8725c402657bb8fe31eb.png)'
- en: Architecture of TimeGPT. The input series, along with exogenous variables, is
    fed to the encoder of the Transfomer, and the decoder then generates forecasts.
    Image by Azul Garza and Max Mergenthaler-Canseco from [TimeGPT-1](https://arxiv.org/pdf/2310.03589.pdf).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: TimeGPT 的架构。输入序列以及外生变量被送入 Transformer 的编码器，解码器随后生成预测。图像来自 [TimeGPT-1](https://arxiv.org/pdf/2310.03589.pdf)
    的 Azul Garza 和 Max Mergenthaler-Canseco。
- en: From the figure above, we can see that TimeGPT uses the full encoder-decoder
    Transformer architecture.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图中，我们可以看到 TimeGPT 使用了完整的编码器-解码器 Transformer 架构。
- en: The inputs can consist of a window of historical data, as well as exogenous
    data, like punctual events or another series.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 输入可以包括一段历史数据窗口以及外生数据，如特定事件或其他序列。
- en: The inputs are fed to the encoder portion of the model. The attention mechanism
    inside the encoder then learns different properties from the inputs. This is then
    fed to the decoder, which uses the learned information to produce forecasts. Of
    course, the sequence of predictions ends when it reaches the length of the forecast
    horizon set by the user.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 输入被送入模型的编码器部分。编码器内部的注意力机制然后学习来自输入的不同特性。这些信息随后被送入解码器，解码器利用学到的信息来生成预测。当然，预测序列会在达到用户设置的预测范围长度时结束。
- en: It is important to note that the authors have implemented conformal predictions
    in TimeGPT, allowing the model to estimate prediction intervals based on historic
    errors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，作者在 TimeGPT 中实现了符合预测功能，使模型能够根据历史误差估计预测区间。
- en: The capabilities of TimeGPT
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TimeGPT 的能力
- en: TimeGPT comes with a wide array of capabilities considering that it is a first
    attempt at building a foundation model for time series.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 TimeGPT 是首次尝试构建时间序列的基础模型，它具备了广泛的功能。
- en: First, with TimeGPT being a pre-trained model, it means that we can generate
    predictions without training it on our data specifically. Still, it is possible
    to fine-tune the model to our data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，由于 TimeGPT 是一个预训练模型，这意味着我们可以在不专门在我们的数据上训练它的情况下生成预测。不过，也可以对模型进行微调以适应我们的数据。
- en: Second, the model supports exogenous variables to forecast our target, and it
    can handle multivariate forecasting tasks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，该模型支持外生变量来预测我们的目标，并且可以处理多变量预测任务。
- en: Finally, with the use of conformal prediction, TimeGPT can estimate prediction
    intervals. This in turn allows the model to perform anomaly detection. Basically,
    if a data point falls outside of a 99% confidence interval, then the model labels
    it as an anomaly.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，通过使用符合预测，TimeGPT 可以估计预测区间。这反过来允许模型执行异常检测。基本上，如果一个数据点落在 99% 置信区间之外，模型就会将其标记为异常。
- en: Keep in mind that all those tasks are possible with zero-shot inference, or
    with some fine-tuning, which is a radical shift in paradigm for the field of time
    series forecasting.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，所有这些任务都可以通过零样本推理或一些微调来实现，这对时间序列预测领域来说是一次根本性的范式转变。
- en: Now that we have a more solid understanding of TimeGPT, how it works and how
    it was trained, let’s see the model in action.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 TimeGPT 有了更扎实的理解，了解了它的工作原理和训练方式，让我们看看模型的实际表现。
- en: Forecast with TimeGPT
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TimeGPT 进行预测
- en: Let’s now apply TimeGPT on a forecasting task and compare its performance to
    other models.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将 TimeGPT 应用于一个预测任务，并将其表现与其他模型进行比较。
- en: Note that at the time of writing this article, TimeGPT is only accessible by
    API, and it is in closed beta. I submitted a request and was granted free access
    to the model for two weeks. To get a token and access the model, you have to visit
    their [website](https://www.nixtla.io/).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在撰写本文时，TimeGPT 仅通过 API 访问，并处于封闭测试阶段。我提交了申请，并获得了为期两周的免费访问模型权限。要获取令牌并访问模型，你必须访问他们的
    [网站](https://www.nixtla.io/)。
- en: As mentioned earlier, the model was trained on 100 billion data points coming
    from publicly available data. Since the authors do not specify the actual datasets
    used, I think it is unreasonable to test the model on known benchmark datasets,
    like [ETT](https://paperswithcode.com/dataset/ett) or [weather](https://www.kaggle.com/datasets/mnassrib/jena-climate),
    as the model has likely seen this data during training.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，该模型在来自公开数据的 1000 亿数据点上进行训练。由于作者没有指定实际使用的数据集，我认为在已知的基准数据集上测试模型是不合理的，例如 [ETT](https://paperswithcode.com/dataset/ett)
    或 [weather](https://www.kaggle.com/datasets/mnassrib/jena-climate)，因为模型在训练过程中可能已经见过这些数据。
- en: Therefore, I compiled and open-sourced my own dataset for this article.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我编制并开源了我自己的数据集用于本文。
- en: 'Specifically, I curated the daily views on my blog from January 1st 2020, to
    October 12th 2023\. I also added two exogenous variables: one to signal a day
    where a new article was published, and the other to flag a day where it is a holiday
    in the United States, as the majority of my audience lives there.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我整理了从 2020 年 1 月 1 日到 2023 年 10 月 12 日我博客的每日浏览量。我还添加了两个外生变量：一个是标记新文章发布的日子，另一个是标记美国的假期日子，因为我的大多数观众都在美国。
- en: The dataset is now publicly available on [GitHub](https://github.com/marcopeix/time-series-analysis/blob/master/data/medium_views_published_holidays.csv),
    and most importantly, we are sure that TimeGPT did not train on this data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集现已在 [GitHub](https://github.com/marcopeix/time-series-analysis/blob/master/data/medium_views_published_holidays.csv)
    上公开，并且最重要的是，我们确定 TimeGPT 没有在此数据上进行训练。
- en: As always, you can access the full notebook on [GitHub](https://github.com/marcopeix/time-series-analysis/blob/master/TimeGPT.ipynb).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，你可以在 [GitHub](https://github.com/marcopeix/time-series-analysis/blob/master/TimeGPT.ipynb)
    上访问完整的笔记本。
- en: Import libraries and read the data
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入库并读取数据
- en: The natural first step is to import the libraries for this experiment.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 自然的第一步是导入实验所需的库。
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then, to access the TimeGPT model, we read the API key from a file. Note that
    I did not assign the API key to an environment variable, because the access was
    limited to two weeks only.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了访问 TimeGPT 模型，我们从文件中读取 API 密钥。请注意，我没有将 API 密钥分配给环境变量，因为访问仅限于两周。
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then, we can read the data.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以读取数据。
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/dfd146f8cdfc8f777aac50ca4a8472e6.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfd146f8cdfc8f777aac50ca4a8472e6.png)'
- en: The first five rows of our dataset. Image by the author.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集的前五行。图片由作者提供。
- en: From the figure above, we can see that the format of the dataset is the same
    as when we work with other open-source libraries from Nixtla.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图中，我们可以看到数据集的格式与我们使用其他来自 Nixtla 的开源库时相同。
- en: We have a *unique_id* column to label different time series, but in our case,
    we only have one series.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个 *unique_id* 列来标记不同的时间序列，但在我们的情况下，我们只有一个序列。
- en: The column *y* represents the daily views on my blog, and *published* is a simple
    flag to label a day where a new article was published (1) or no article was published
    (0). Intuitively, we know that when new content is released, the views usually
    increase for a period of time.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 列 *y* 代表我博客上的每日浏览量，*published* 是一个简单的标志，用于标记一天是否有新文章发布（1）或没有文章发布（0）。直观上，我们知道当发布新内容时，浏览量通常会在一段时间内增加。
- en: Finally, the column *is_holiday* indicates if there is a holiday or not in the
    United States. The intuition is that on holidays, fewer people will visit my blog.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，列 *is_holiday* 指示是否为美国假期。直观上，假期期间访问我博客的人较少。
- en: Now, let’s visualize our data and look for discerning patterns.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们可视化数据并寻找明显的模式。
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/2bcfd0a41d8a036f517aecf0be949731.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bcfd0a41d8a036f517aecf0be949731.png)'
- en: Daily views on my blog. Image by the author.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我博客上的每日浏览量。图片由作者提供。
- en: From the figure above, we can already see some interesting behaviour. First,
    notice that the red dots indicate a new published article, and they are almost
    immediately followed by peaks in visits.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图中，我们已经可以看到一些有趣的行为。首先，注意红点表示新发布的文章，它们几乎立即伴随着访问量的峰值。
- en: We also notice less activity in 2021 which is reflected in fewer daily views
    on my blog. Finally, in 2023, we notice some anomalous peaks in visits after an
    article is published.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还注意到 2021 年的活动较少，这在我博客的每日浏览量减少中体现出来。最后，在 2023 年，我们注意到在文章发布后访问量出现一些异常峰值。
- en: Zooming in on the data, we also uncover a clear weekly seasonality.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 放大数据后，我们还发现了明显的每周季节性。
- en: '![](../Images/2a9cb296a60b240f8c5df3f584f588dc.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a9cb296a60b240f8c5df3f584f588dc.png)'
- en: Daily views on my blog. Here, we see a clear weekly seasonality with less people
    visiting on the weekend. Image by the author.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我的博客每日浏览量。在这里，我们看到明显的每周季节性，周末访问人数较少。图片由作者提供。
- en: From the figure above, we can now see that fewer visitors come to the blog during
    the weekend than during the week.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图中，我们现在可以看到周末来博客的访客比工作日少。
- en: With all of that in mind, let’s see how we can work with TimeGPT to make predictions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 了解了这些之后，让我们看看如何利用TimeGPT进行预测。
- en: Predict with TimeGPT
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TimeGPT进行预测
- en: First, let’s split the dataset into a training set and a test set. Here, I will
    keep 168 time steps for the test set, which corresponds to 24 weeks of daily data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将数据集拆分为训练集和测试集。在这里，我会保留168个时间步作为测试集，这对应于24周的每日数据。
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Then, we work with a forecast horizon of seven days, as I am interested in predicting
    the daily views for a full week.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用七天的预测范围，因为我有兴趣预测一整周的每日浏览量。
- en: Now, the API does not come with an implementation of cross-validation. Therefore,
    we create our own loop to generate seven predictions at a time, until we have
    predictions for the entire test set.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，API没有交叉验证的实现。因此，我们创建了自己的循环，每次生成七个预测，直到我们对整个测试集都有预测。
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the code block above, notice that we have to pass the future values of our
    exogenous variables. This is fine, because they are static variables. We know
    the future dates of holidays, and the blog author personally knows when he plans
    on publishing an article.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码块中，请注意我们必须传递外生变量的未来值。这是可以的，因为它们是静态变量。我们知道假期的未来日期，博客作者也知道他计划发布文章的时间。
- en: Also note that we fine-tune TimeGPT to our data using the *finetune_steps* parameter.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 同样注意，我们使用*finetune_steps*参数对TimeGPT进行微调以适应我们的数据。
- en: Once the loop is done, we can add the predictions to the test set. Again, TimeGPT
    generated seven predictions at a time until 168 predictions were obtained, so
    that we can evaluate its capacity in forecasting the daily views for next week.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦循环完成，我们可以将预测添加到测试集中。再次，TimeGPT每次生成七个预测，直到获得168个预测，以便我们评估其预测下周每日浏览量的能力。
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/7568508f0602da67a14bc017d7b8893c.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7568508f0602da67a14bc017d7b8893c.png)'
- en: Predictions from TimeGPT. Image by the author.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: TimeGPT的预测。图片由作者提供。
- en: Forecasting with N-BEATS, N-HiTS and PatchTST
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用N-BEATS、N-HiTS和PatchTST进行预测
- en: Now, let’s apply other methods to see if training these models specifically
    on our dataset can produce better predictions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们应用其他方法，看看是否专门在我们的数据集上训练这些模型可以产生更好的预测。
- en: For this experiment, as mentioned before, we use N-BEATS, N-HiTS and PatchTST.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实验，如前所述，我们使用了N-BEATS、N-HiTS和PatchTST。
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Then, we initialize the *NeuralForecast* object and specify the frequency of
    our data, which is daily in this case.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们初始化*NeuralForecast*对象，并指定数据的频率，在这种情况下是每日。
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Then, we run perform cross-validation over 24 windows of 7 time steps to have
    predictions that align with the test set used for TimeGPT.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们在24个7时间步的窗口上进行交叉验证，以获得与TimeGPT使用的测试集对齐的预测。
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Then, we can simply add the predictions from TimeGPT to this new *preds_df*
    DataFrame to have a single DataFrame with the predictions from all models.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将TimeGPT的预测简单地添加到这个新的*preds_df* DataFrame中，以便拥有一个包含所有模型预测的单一DataFrame。
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/88ac6201d63fccff2ba447acfcdfb5e3.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88ac6201d63fccff2ba447acfcdfb5e3.png)'
- en: DataFrame with predictions from all models. Image by the author.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 包含所有模型预测的DataFrame。图片由作者提供。
- en: Great! We are now ready to evaluate the performance of each model.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们现在准备评估每个模型的性能。
- en: Evaluation
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估
- en: Before measuring performance metrics, let’s visualize the predictions of each
    model on our test set.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在测量性能指标之前，让我们可视化每个模型在测试集上的预测。
- en: '![](../Images/1dae3bf9da71a92143bbb581ee80e438.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1dae3bf9da71a92143bbb581ee80e438.png)'
- en: Visualizing the predictions from each model. Image by the author.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化每个模型的预测。图片由作者提供。
- en: First, we see a lot of overlapping between each model. However, we do notice
    that N-HiTS predicted two peaks that were not realized in real life. Also, it
    seems that PatchTST is often under-forecasting. However, TimeGPT seem to generally
    overlap the actual data quite well.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们看到每个模型之间有很多重叠。然而，我们确实注意到N-HiTS预测了两个在现实中未实现的峰值。此外，PatchTST似乎经常低估预测。然而，TimeGPT似乎总体上与实际数据重叠得相当好。
- en: Of course, the only way to assess each model’s performance is to measure performance
    metrics. Here, we use the mean absolute error (MAE) and mean squared error (MSE).
    Also, we round the predictions to whole numbers, as a decimal number makes no
    sense in the context of daily visitors to a blog.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，评估每个模型性能的唯一方法是测量性能指标。在这里，我们使用均值绝对误差（MAE）和均方误差（MSE）。此外，我们将预测结果四舍五入为整数，因为在博客的日常访问者上下文中，小数没有意义。
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/b5f5589624a057c73f2c8b3685b85464.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5f5589624a057c73f2c8b3685b85464.png)'
- en: Performance metrics of each model. Here, TimeGPT is the champion model as it
    achieves the lowest MAE and MSE. Image by the author.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型的性能指标。在这里，TimeGPT 是冠军模型，因为它实现了最低的 MAE 和 MSE。图片由作者提供。
- en: From the figure above, we see that TimeGPT is the champion model as it achieves
    the lowest MAE and MSE, followed by N-BEATS, PatchTST and N-HiTS.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图中，我们可以看到 TimeGPT 是冠军模型，因为它实现了最低的 MAE 和 MSE，紧随其后的是 N-BEATS、PatchTST 和 N-HiTS。
- en: This is an exciting result, as TimeGPT has never seen this dataset and was only
    fine-tuned for a few steps. While this is not an exhaustive experiment, I believe
    it does show a glimpse of the potential foundational models can have in the field
    of forecasting.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个令人兴奋的结果，因为 TimeGPT 从未见过这个数据集，只进行了少量的微调。虽然这不是一个详尽的实验，但我相信它确实展示了基础模型在预测领域的潜力。
- en: My personal opinion on TimeGPT
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我对 TimeGPT 的个人看法
- en: While my short experiment with TimeGPT proved to be exciting, I must point out
    that the [original paper](https://arxiv.org/pdf/2310.03589.pdf) remains vague
    in many important areas.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我对 TimeGPT 的短期实验令人兴奋，但我必须指出，[原始论文](https://arxiv.org/pdf/2310.03589.pdf) 在许多重要领域仍然模糊。
- en: Again, we do not know what datasets were used to train and test the model, so
    we cannot really verify the performance results of TimeGPT, as shown below.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们不知道用于训练和测试模型的数据集，因此无法真正验证 TimeGPT 的性能结果，如下所示。
- en: '![](../Images/3d0b4e177853479b015c216be4d57815.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d0b4e177853479b015c216be4d57815.png)'
- en: Performance result of TimeGPT as reported in the [original paper](https://arxiv.org/pdf/2310.03589.pdf)
    by Azul Garza and Max Mergenthaler-Canseco
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: TimeGPT 的性能结果如 [原始论文](https://arxiv.org/pdf/2310.03589.pdf) 所述，由 Azul Garza
    和 Max Mergenthaler-Canseco 提供。
- en: From the table above, we can see that TimeGPT performs best for the monthly
    and weekly frequencies, with N-HiTS and Temporal Fusion Transformer (TFT) usually
    ranking 2nd or 3rd. Then again, because we do not know what data was used, we
    cannot verify these metrics.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从上表中可以看出，TimeGPT 在月度和每周频率下表现最佳，N-HiTS 和 Temporal Fusion Transformer (TFT) 通常排名第2或第3。再者，由于我们不知道使用了什么数据，我们无法验证这些指标。
- en: There is also a lack of transparency when it comes to how the model was trained
    and how it was adapted to handle time series data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 关于模型的训练方式以及如何适应时间序列数据，也缺乏透明度。
- en: I believe that the model is intended for commercial use, which explains why
    the paper lacks the details to reproduce TimeGPT. There is nothing wrong with
    that, but the lack of reproducibility of the paper is a concern for the scientific
    community.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信该模型旨在商业使用，这解释了为什么论文缺乏重现 TimeGPT 的细节。这并没有错，但论文缺乏可重现性对科学界来说是个问题。
- en: Still, I hope that this sparks new work and research in foundation models for
    time series, and that we eventually see an open-source version of these models,
    much like we see it happening for LLMs.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我希望这能激发在时间序列领域基础模型的新工作和研究，并且我们最终能看到这些模型的开源版本，就像我们看到 LLM 的情况一样。
- en: Conclusion
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: TimeGPT is the first foundation model for time series forecasting.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: TimeGPT 是第一个用于时间序列预测的基础模型。
- en: It leverages the Transformer architecture and was pre-trained on 100 billion
    data points to make zero-shot inference on new unseen data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 它利用了 Transformer 架构，并在 1000 亿数据点上进行了预训练，以对新的未见数据进行零样本推理。
- en: Combined with the technique of conformal prediction, the model can generate
    prediction intervals and perform anomaly detection without being trained on a
    specific dataset.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 结合符合预测技术，该模型可以生成预测区间并执行异常检测，而无需在特定数据集上进行训练。
- en: I still believe that each forecasting problem requires a unique approach, so
    make sure to test out TimeGPT as well as other models.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我仍然相信每个预测问题需要独特的方法，因此请确保测试 TimeGPT 以及其他模型。
- en: Thanks for reading! I hope that you enjoyed it and that you learned something
    new!
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！希望你喜欢这篇文章，并学到了新东西！
- en: Looking to master time series forecasting? Then check out my course [Applied
    Time Series Forecasting in Python](https://www.datasciencewithmarco.com/offers/zTAs2hi6/checkout?coupon_code=ATSFP10).
    This is the only course that uses Python to implement statistical, deep learning
    and state-of-the-art models in 16 guided hands-on projects.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 想要掌握时间序列预测吗？那就来看看我的课程[Python中的应用时间序列预测](https://www.datasciencewithmarco.com/offers/zTAs2hi6/checkout?coupon_code=ATSFP10)。这是唯一一个通过Python实现统计学、深度学习和最先进模型的课程，包含16个指导性的实践项目。
- en: Cheers 🍻
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 干杯 🍻
- en: Support me
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持我
- en: Enjoying my work? Show your support with [Buy me a coffee](http://buymeacoffee.com/dswm),
    a simple way for you to encourage me, and I get to enjoy a cup of coffee! If you
    feel like it, just click the button below 👇
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 喜欢我的工作吗？通过[请我喝咖啡](http://buymeacoffee.com/dswm)来支持我，这是一种简单的鼓励方式，我可以享受一杯咖啡！如果你愿意，只需点击下面的按钮
    👇
- en: '![](../Images/a90a701107c4ea11414ef27bd59465af.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a90a701107c4ea11414ef27bd59465af.png)'
- en: References
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[TimeGPT-1](https://arxiv.org/pdf/2310.03589.pdf) by Azul Garza and Max Mergenthaler-Canseco'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[TimeGPT-1](https://arxiv.org/pdf/2310.03589.pdf) 由Azul Garza和Max Mergenthaler-Canseco编写'
