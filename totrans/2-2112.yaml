- en: 'TimeGPT: The First Foundation Model for Time Series Forecasting'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/timegpt-the-first-foundation-model-for-time-series-forecasting-bf0a75e63b3a](https://towardsdatascience.com/timegpt-the-first-foundation-model-for-time-series-forecasting-bf0a75e63b3a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explore the first generative pre-trained forecasting model and apply it in a
    project with Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcopeixeiro?source=post_page-----bf0a75e63b3a--------------------------------)[![Marco
    Peixeiro](../Images/7cf0a81d87281d35ff47f51e3026a3e9.png)](https://medium.com/@marcopeixeiro?source=post_page-----bf0a75e63b3a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bf0a75e63b3a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bf0a75e63b3a--------------------------------)
    [Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page-----bf0a75e63b3a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bf0a75e63b3a--------------------------------)
    Â·12 min readÂ·Oct 24, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69c006f4feeba3f9fe392aff3b2a35f0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Photo by [Boris Smokrovic](https://unsplash.com/@borisworkshop?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: The field of time series forecasting is going through a very exciting period.
    In only the last three years, we have seen many important contributions, like
    [N-BEATS](https://medium.com/towards-data-science/the-easiest-way-to-forecast-time-series-using-n-beats-d778fcc2ba60),
    [N-HiTS](https://medium.com/towards-data-science/all-about-n-hits-the-latest-breakthrough-in-time-series-forecasting-a8ddcb27b0d5),
    [PatchTST](https://medium.com/towards-data-science/patchtst-a-breakthrough-in-time-series-forecasting-e02d48869ccc)
    and [TimesNet](https://medium.com/towards-data-science/timesnet-the-latest-advance-in-time-series-forecasting-745b69068c9c).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, [large language models (LLMs)](https://medium.com/towards-data-science/catch-up-on-large-language-models-8daf784f46f8)
    have gained a lot of popularity lately, with applications like ChatGPT, as they
    can adapt to a wide variety of tasks without further training.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Which leads to the question: can foundation models exist for time series like
    they exist for natural language processing? Is it possible that a large model
    pre-trained on massive amounts of time series data can then produce accurate predictions
    on unseen data?'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: With [TimeGPT-1](https://arxiv.org/pdf/2310.03589.pdf), proposed by Azul Garza
    and Max Mergenthaler-Canseco, the authors adapt the techniques and architecture
    behind LLMs to the field of forecasting, successfully building the first time
    series foundation model capable of zero-shot inference.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we first explore the architecture behind TimeGPT and how the
    model was trained. Then, we apply it in a forecasting project to evaluate its
    performance against other state-of-the-art methods, like N-BEATS, N-HiTS and PatchTST.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: For more details, make sure to read the [original paper](https://arxiv.org/pdf/2310.03589.pdf).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '***Learn the latest time series analysis techniques with my*** [***free time
    series cheat sheet***](https://www.datasciencewithmarco.com/pl/2147608294) ***in
    Python! Get the implementation of statistical and deep learning techniques, all
    in Python and TensorFlow!***'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Letâ€™s get started!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Explore TimeGPT
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, TimeGPT is a first attempt at creating a foundation model
    for time series forecasting.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5a18b137f0b737e2e4df634cbc8111b2.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: Illustration of how TimeGPT was trained to make inference on unseen data. Image
    by Azul Garza and Max Mergenthaler-Canseco from [TimeGPT-1](https://arxiv.org/pdf/2310.03589.pdf)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: From the figure above, we can see that the general idea behind TimeGPT is to
    train a model on massive amounts of data from different domains to then produce
    zero-shot inference on unseen data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this method relies on **transfer learning**, which is the capacity
    of a model to solve a new task using its knowledge gained during training.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Now, this only works if the model is large enough, and if it is trained on lots
    of data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Training TimeGPT
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To that end, the authors trained TimeGPT on more than 100 billion data points
    all coming from open-source time series data. The dataset spans a wide array of
    domains, from finance, economics and weather, to web traffic, energy and sales.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Note that the authors do not disclose the sources of public data used to curate
    100 billion data points.
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This variety is critical for the success of a foundation model, as it can learn
    different temporal patterns and therefore generalize better.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can expect weather data to have a daily (hotter during the day
    than at night) and yearly seasonality, while car traffic data can have a daily
    seasonality (more cars on the road during the day than at night) and a weekly
    seasonality (more cars on the road during the week than on weekends).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: To ensure the robustness and generalization capabilities of the model, preprocessing
    was kept to a minimum. In fact, only missing values were filled, and the rest
    was kept in its raw form. While the authors do not specify the method for data
    imputation, I suspect that some kind of interpolation technique was used, like
    linear, spline or moving average interpolation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: The model was then trained over multiple days, during which hyperparameters
    and learning rates were optimized. While the authors do not disclose how many
    days and GPUs were required for training, we do know that the model is implemented
    in PyTorch, and it uses the Adam optimizer and a learning rate decay strategy.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Architecture of TimeGPT
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TimeGPT leverages the Transformer architecture with self-attention mechanism
    based on the seminal work of Google and the University of Toronto in 2017.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: TimeGPT åˆ©ç”¨åŸºäº Google å’Œå¤šä¼¦å¤šå¤§å­¦ 2017 å¹´å¼€åˆ›æ€§å·¥ä½œçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ Transformer æ¶æ„ã€‚
- en: '![](../Images/cf13280001ad8725c402657bb8fe31eb.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf13280001ad8725c402657bb8fe31eb.png)'
- en: Architecture of TimeGPT. The input series, along with exogenous variables, is
    fed to the encoder of the Transfomer, and the decoder then generates forecasts.
    Image by Azul Garza and Max Mergenthaler-Canseco from [TimeGPT-1](https://arxiv.org/pdf/2310.03589.pdf).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: TimeGPT çš„æ¶æ„ã€‚è¾“å…¥åºåˆ—ä»¥åŠå¤–ç”Ÿå˜é‡è¢«é€å…¥ Transformer çš„ç¼–ç å™¨ï¼Œè§£ç å™¨éšåç”Ÿæˆé¢„æµ‹ã€‚å›¾åƒæ¥è‡ª [TimeGPT-1](https://arxiv.org/pdf/2310.03589.pdf)
    çš„ Azul Garza å’Œ Max Mergenthaler-Cansecoã€‚
- en: From the figure above, we can see that TimeGPT uses the full encoder-decoder
    Transformer architecture.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° TimeGPT ä½¿ç”¨äº†å®Œæ•´çš„ç¼–ç å™¨-è§£ç å™¨ Transformer æ¶æ„ã€‚
- en: The inputs can consist of a window of historical data, as well as exogenous
    data, like punctual events or another series.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥å¯ä»¥åŒ…æ‹¬ä¸€æ®µå†å²æ•°æ®çª—å£ä»¥åŠå¤–ç”Ÿæ•°æ®ï¼Œå¦‚ç‰¹å®šäº‹ä»¶æˆ–å…¶ä»–åºåˆ—ã€‚
- en: The inputs are fed to the encoder portion of the model. The attention mechanism
    inside the encoder then learns different properties from the inputs. This is then
    fed to the decoder, which uses the learned information to produce forecasts. Of
    course, the sequence of predictions ends when it reaches the length of the forecast
    horizon set by the user.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥è¢«é€å…¥æ¨¡å‹çš„ç¼–ç å™¨éƒ¨åˆ†ã€‚ç¼–ç å™¨å†…éƒ¨çš„æ³¨æ„åŠ›æœºåˆ¶ç„¶åå­¦ä¹ æ¥è‡ªè¾“å…¥çš„ä¸åŒç‰¹æ€§ã€‚è¿™äº›ä¿¡æ¯éšåè¢«é€å…¥è§£ç å™¨ï¼Œè§£ç å™¨åˆ©ç”¨å­¦åˆ°çš„ä¿¡æ¯æ¥ç”Ÿæˆé¢„æµ‹ã€‚å½“ç„¶ï¼Œé¢„æµ‹åºåˆ—ä¼šåœ¨è¾¾åˆ°ç”¨æˆ·è®¾ç½®çš„é¢„æµ‹èŒƒå›´é•¿åº¦æ—¶ç»“æŸã€‚
- en: It is important to note that the authors have implemented conformal predictions
    in TimeGPT, allowing the model to estimate prediction intervals based on historic
    errors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦çš„æ˜¯è¦æ³¨æ„ï¼Œä½œè€…åœ¨ TimeGPT ä¸­å®ç°äº†ç¬¦åˆé¢„æµ‹åŠŸèƒ½ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®å†å²è¯¯å·®ä¼°è®¡é¢„æµ‹åŒºé—´ã€‚
- en: The capabilities of TimeGPT
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TimeGPT çš„èƒ½åŠ›
- en: TimeGPT comes with a wide array of capabilities considering that it is a first
    attempt at building a foundation model for time series.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘åˆ° TimeGPT æ˜¯é¦–æ¬¡å°è¯•æ„å»ºæ—¶é—´åºåˆ—çš„åŸºç¡€æ¨¡å‹ï¼Œå®ƒå…·å¤‡äº†å¹¿æ³›çš„åŠŸèƒ½ã€‚
- en: First, with TimeGPT being a pre-trained model, it means that we can generate
    predictions without training it on our data specifically. Still, it is possible
    to fine-tune the model to our data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œç”±äº TimeGPT æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥åœ¨ä¸ä¸“é—¨åœ¨æˆ‘ä»¬çš„æ•°æ®ä¸Šè®­ç»ƒå®ƒçš„æƒ…å†µä¸‹ç”Ÿæˆé¢„æµ‹ã€‚ä¸è¿‡ï¼Œä¹Ÿå¯ä»¥å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥é€‚åº”æˆ‘ä»¬çš„æ•°æ®ã€‚
- en: Second, the model supports exogenous variables to forecast our target, and it
    can handle multivariate forecasting tasks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶æ¬¡ï¼Œè¯¥æ¨¡å‹æ”¯æŒå¤–ç”Ÿå˜é‡æ¥é¢„æµ‹æˆ‘ä»¬çš„ç›®æ ‡ï¼Œå¹¶ä¸”å¯ä»¥å¤„ç†å¤šå˜é‡é¢„æµ‹ä»»åŠ¡ã€‚
- en: Finally, with the use of conformal prediction, TimeGPT can estimate prediction
    intervals. This in turn allows the model to perform anomaly detection. Basically,
    if a data point falls outside of a 99% confidence interval, then the model labels
    it as an anomaly.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆï¼Œé€šè¿‡ä½¿ç”¨ç¬¦åˆé¢„æµ‹ï¼ŒTimeGPT å¯ä»¥ä¼°è®¡é¢„æµ‹åŒºé—´ã€‚è¿™åè¿‡æ¥å…è®¸æ¨¡å‹æ‰§è¡Œå¼‚å¸¸æ£€æµ‹ã€‚åŸºæœ¬ä¸Šï¼Œå¦‚æœä¸€ä¸ªæ•°æ®ç‚¹è½åœ¨ 99% ç½®ä¿¡åŒºé—´ä¹‹å¤–ï¼Œæ¨¡å‹å°±ä¼šå°†å…¶æ ‡è®°ä¸ºå¼‚å¸¸ã€‚
- en: Keep in mind that all those tasks are possible with zero-shot inference, or
    with some fine-tuning, which is a radical shift in paradigm for the field of time
    series forecasting.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·è®°ä½ï¼Œæ‰€æœ‰è¿™äº›ä»»åŠ¡éƒ½å¯ä»¥é€šè¿‡é›¶æ ·æœ¬æ¨ç†æˆ–ä¸€äº›å¾®è°ƒæ¥å®ç°ï¼Œè¿™å¯¹æ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸæ¥è¯´æ˜¯ä¸€æ¬¡æ ¹æœ¬æ€§çš„èŒƒå¼è½¬å˜ã€‚
- en: Now that we have a more solid understanding of TimeGPT, how it works and how
    it was trained, letâ€™s see the model in action.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯¹ TimeGPT æœ‰äº†æ›´æ‰å®çš„ç†è§£ï¼Œäº†è§£äº†å®ƒçš„å·¥ä½œåŸç†å’Œè®­ç»ƒæ–¹å¼ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ¨¡å‹çš„å®é™…è¡¨ç°ã€‚
- en: Forecast with TimeGPT
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ TimeGPT è¿›è¡Œé¢„æµ‹
- en: Letâ€™s now apply TimeGPT on a forecasting task and compare its performance to
    other models.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°† TimeGPT åº”ç”¨äºä¸€ä¸ªé¢„æµ‹ä»»åŠ¡ï¼Œå¹¶å°†å…¶è¡¨ç°ä¸å…¶ä»–æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚
- en: Note that at the time of writing this article, TimeGPT is only accessible by
    API, and it is in closed beta. I submitted a request and was granted free access
    to the model for two weeks. To get a token and access the model, you have to visit
    their [website](https://www.nixtla.io/).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œåœ¨æ’°å†™æœ¬æ–‡æ—¶ï¼ŒTimeGPT ä»…é€šè¿‡ API è®¿é—®ï¼Œå¹¶å¤„äºå°é—­æµ‹è¯•é˜¶æ®µã€‚æˆ‘æäº¤äº†ç”³è¯·ï¼Œå¹¶è·å¾—äº†ä¸ºæœŸä¸¤å‘¨çš„å…è´¹è®¿é—®æ¨¡å‹æƒé™ã€‚è¦è·å–ä»¤ç‰Œå¹¶è®¿é—®æ¨¡å‹ï¼Œä½ å¿…é¡»è®¿é—®ä»–ä»¬çš„
    [ç½‘ç«™](https://www.nixtla.io/)ã€‚
- en: As mentioned earlier, the model was trained on 100 billion data points coming
    from publicly available data. Since the authors do not specify the actual datasets
    used, I think it is unreasonable to test the model on known benchmark datasets,
    like [ETT](https://paperswithcode.com/dataset/ett) or [weather](https://www.kaggle.com/datasets/mnassrib/jena-climate),
    as the model has likely seen this data during training.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œè¯¥æ¨¡å‹åœ¨æ¥è‡ªå…¬å¼€æ•°æ®çš„ 1000 äº¿æ•°æ®ç‚¹ä¸Šè¿›è¡Œè®­ç»ƒã€‚ç”±äºä½œè€…æ²¡æœ‰æŒ‡å®šå®é™…ä½¿ç”¨çš„æ•°æ®é›†ï¼Œæˆ‘è®¤ä¸ºåœ¨å·²çŸ¥çš„åŸºå‡†æ•°æ®é›†ä¸Šæµ‹è¯•æ¨¡å‹æ˜¯ä¸åˆç†çš„ï¼Œä¾‹å¦‚ [ETT](https://paperswithcode.com/dataset/ett)
    æˆ– [weather](https://www.kaggle.com/datasets/mnassrib/jena-climate)ï¼Œå› ä¸ºæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½å·²ç»è§è¿‡è¿™äº›æ•°æ®ã€‚
- en: Therefore, I compiled and open-sourced my own dataset for this article.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ç¼–åˆ¶å¹¶å¼€æºäº†æˆ‘è‡ªå·±çš„æ•°æ®é›†ç”¨äºæœ¬æ–‡ã€‚
- en: 'Specifically, I curated the daily views on my blog from January 1st 2020, to
    October 12th 2023\. I also added two exogenous variables: one to signal a day
    where a new article was published, and the other to flag a day where it is a holiday
    in the United States, as the majority of my audience lives there.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“æ¥è¯´ï¼Œæˆ‘æ•´ç†äº†ä» 2020 å¹´ 1 æœˆ 1 æ—¥åˆ° 2023 å¹´ 10 æœˆ 12 æ—¥æˆ‘åšå®¢çš„æ¯æ—¥æµè§ˆé‡ã€‚æˆ‘è¿˜æ·»åŠ äº†ä¸¤ä¸ªå¤–ç”Ÿå˜é‡ï¼šä¸€ä¸ªæ˜¯æ ‡è®°æ–°æ–‡ç« å‘å¸ƒçš„æ—¥å­ï¼Œå¦ä¸€ä¸ªæ˜¯æ ‡è®°ç¾å›½çš„å‡æœŸæ—¥å­ï¼Œå› ä¸ºæˆ‘çš„å¤§å¤šæ•°è§‚ä¼—éƒ½åœ¨ç¾å›½ã€‚
- en: The dataset is now publicly available on [GitHub](https://github.com/marcopeix/time-series-analysis/blob/master/data/medium_views_published_holidays.csv),
    and most importantly, we are sure that TimeGPT did not train on this data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†ç°å·²åœ¨ [GitHub](https://github.com/marcopeix/time-series-analysis/blob/master/data/medium_views_published_holidays.csv)
    ä¸Šå…¬å¼€ï¼Œå¹¶ä¸”æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬ç¡®å®š TimeGPT æ²¡æœ‰åœ¨æ­¤æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚
- en: As always, you can access the full notebook on [GitHub](https://github.com/marcopeix/time-series-analysis/blob/master/TimeGPT.ipynb).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åƒå¾€å¸¸ä¸€æ ·ï¼Œä½ å¯ä»¥åœ¨ [GitHub](https://github.com/marcopeix/time-series-analysis/blob/master/TimeGPT.ipynb)
    ä¸Šè®¿é—®å®Œæ•´çš„ç¬”è®°æœ¬ã€‚
- en: Import libraries and read the data
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¼å…¥åº“å¹¶è¯»å–æ•°æ®
- en: The natural first step is to import the libraries for this experiment.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç„¶çš„ç¬¬ä¸€æ­¥æ˜¯å¯¼å…¥å®éªŒæ‰€éœ€çš„åº“ã€‚
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then, to access the TimeGPT model, we read the API key from a file. Note that
    I did not assign the API key to an environment variable, because the access was
    limited to two weeks only.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œä¸ºäº†è®¿é—® TimeGPT æ¨¡å‹ï¼Œæˆ‘ä»¬ä»æ–‡ä»¶ä¸­è¯»å– API å¯†é’¥ã€‚è¯·æ³¨æ„ï¼Œæˆ‘æ²¡æœ‰å°† API å¯†é’¥åˆ†é…ç»™ç¯å¢ƒå˜é‡ï¼Œå› ä¸ºè®¿é—®ä»…é™äºä¸¤å‘¨ã€‚
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then, we can read the data.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥è¯»å–æ•°æ®ã€‚
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/dfd146f8cdfc8f777aac50ca4a8472e6.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfd146f8cdfc8f777aac50ca4a8472e6.png)'
- en: The first five rows of our dataset. Image by the author.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ•°æ®é›†çš„å‰äº”è¡Œã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: From the figure above, we can see that the format of the dataset is the same
    as when we work with other open-source libraries from Nixtla.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ•°æ®é›†çš„æ ¼å¼ä¸æˆ‘ä»¬ä½¿ç”¨å…¶ä»–æ¥è‡ª Nixtla çš„å¼€æºåº“æ—¶ç›¸åŒã€‚
- en: We have a *unique_id* column to label different time series, but in our case,
    we only have one series.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰ä¸€ä¸ª *unique_id* åˆ—æ¥æ ‡è®°ä¸åŒçš„æ—¶é—´åºåˆ—ï¼Œä½†åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªæœ‰ä¸€ä¸ªåºåˆ—ã€‚
- en: The column *y* represents the daily views on my blog, and *published* is a simple
    flag to label a day where a new article was published (1) or no article was published
    (0). Intuitively, we know that when new content is released, the views usually
    increase for a period of time.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ— *y* ä»£è¡¨æˆ‘åšå®¢ä¸Šçš„æ¯æ—¥æµè§ˆé‡ï¼Œ*published* æ˜¯ä¸€ä¸ªç®€å•çš„æ ‡å¿—ï¼Œç”¨äºæ ‡è®°ä¸€å¤©æ˜¯å¦æœ‰æ–°æ–‡ç« å‘å¸ƒï¼ˆ1ï¼‰æˆ–æ²¡æœ‰æ–‡ç« å‘å¸ƒï¼ˆ0ï¼‰ã€‚ç›´è§‚ä¸Šï¼Œæˆ‘ä»¬çŸ¥é“å½“å‘å¸ƒæ–°å†…å®¹æ—¶ï¼Œæµè§ˆé‡é€šå¸¸ä¼šåœ¨ä¸€æ®µæ—¶é—´å†…å¢åŠ ã€‚
- en: Finally, the column *is_holiday* indicates if there is a holiday or not in the
    United States. The intuition is that on holidays, fewer people will visit my blog.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œåˆ— *is_holiday* æŒ‡ç¤ºæ˜¯å¦ä¸ºç¾å›½å‡æœŸã€‚ç›´è§‚ä¸Šï¼Œå‡æœŸæœŸé—´è®¿é—®æˆ‘åšå®¢çš„äººè¾ƒå°‘ã€‚
- en: Now, letâ€™s visualize our data and look for discerning patterns.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬å¯è§†åŒ–æ•°æ®å¹¶å¯»æ‰¾æ˜æ˜¾çš„æ¨¡å¼ã€‚
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/2bcfd0a41d8a036f517aecf0be949731.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bcfd0a41d8a036f517aecf0be949731.png)'
- en: Daily views on my blog. Image by the author.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åšå®¢ä¸Šçš„æ¯æ—¥æµè§ˆé‡ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: From the figure above, we can already see some interesting behaviour. First,
    notice that the red dots indicate a new published article, and they are almost
    immediately followed by peaks in visits.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬å·²ç»å¯ä»¥çœ‹åˆ°ä¸€äº›æœ‰è¶£çš„è¡Œä¸ºã€‚é¦–å…ˆï¼Œæ³¨æ„çº¢ç‚¹è¡¨ç¤ºæ–°å‘å¸ƒçš„æ–‡ç« ï¼Œå®ƒä»¬å‡ ä¹ç«‹å³ä¼´éšç€è®¿é—®é‡çš„å³°å€¼ã€‚
- en: We also notice less activity in 2021 which is reflected in fewer daily views
    on my blog. Finally, in 2023, we notice some anomalous peaks in visits after an
    article is published.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜æ³¨æ„åˆ° 2021 å¹´çš„æ´»åŠ¨è¾ƒå°‘ï¼Œè¿™åœ¨æˆ‘åšå®¢çš„æ¯æ—¥æµè§ˆé‡å‡å°‘ä¸­ä½“ç°å‡ºæ¥ã€‚æœ€åï¼Œåœ¨ 2023 å¹´ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°åœ¨æ–‡ç« å‘å¸ƒåè®¿é—®é‡å‡ºç°ä¸€äº›å¼‚å¸¸å³°å€¼ã€‚
- en: Zooming in on the data, we also uncover a clear weekly seasonality.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¾å¤§æ•°æ®åï¼Œæˆ‘ä»¬è¿˜å‘ç°äº†æ˜æ˜¾çš„æ¯å‘¨å­£èŠ‚æ€§ã€‚
- en: '![](../Images/2a9cb296a60b240f8c5df3f584f588dc.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a9cb296a60b240f8c5df3f584f588dc.png)'
- en: Daily views on my blog. Here, we see a clear weekly seasonality with less people
    visiting on the weekend. Image by the author.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„åšå®¢æ¯æ—¥æµè§ˆé‡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°æ˜æ˜¾çš„æ¯å‘¨å­£èŠ‚æ€§ï¼Œå‘¨æœ«è®¿é—®äººæ•°è¾ƒå°‘ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: From the figure above, we can now see that fewer visitors come to the blog during
    the weekend than during the week.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥çœ‹åˆ°å‘¨æœ«æ¥åšå®¢çš„è®¿å®¢æ¯”å·¥ä½œæ—¥å°‘ã€‚
- en: With all of that in mind, letâ€™s see how we can work with TimeGPT to make predictions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: äº†è§£äº†è¿™äº›ä¹‹åï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åˆ©ç”¨TimeGPTè¿›è¡Œé¢„æµ‹ã€‚
- en: Predict with TimeGPT
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨TimeGPTè¿›è¡Œé¢„æµ‹
- en: First, letâ€™s split the dataset into a training set and a test set. Here, I will
    keep 168 time steps for the test set, which corresponds to 24 weeks of daily data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œå°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä¼šä¿ç•™168ä¸ªæ—¶é—´æ­¥ä½œä¸ºæµ‹è¯•é›†ï¼Œè¿™å¯¹åº”äº24å‘¨çš„æ¯æ—¥æ•°æ®ã€‚
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Then, we work with a forecast horizon of seven days, as I am interested in predicting
    the daily views for a full week.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸ƒå¤©çš„é¢„æµ‹èŒƒå›´ï¼Œå› ä¸ºæˆ‘æœ‰å…´è¶£é¢„æµ‹ä¸€æ•´å‘¨çš„æ¯æ—¥æµè§ˆé‡ã€‚
- en: Now, the API does not come with an implementation of cross-validation. Therefore,
    we create our own loop to generate seven predictions at a time, until we have
    predictions for the entire test set.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼ŒAPIæ²¡æœ‰äº¤å‰éªŒè¯çš„å®ç°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºäº†è‡ªå·±çš„å¾ªç¯ï¼Œæ¯æ¬¡ç”Ÿæˆä¸ƒä¸ªé¢„æµ‹ï¼Œç›´åˆ°æˆ‘ä»¬å¯¹æ•´ä¸ªæµ‹è¯•é›†éƒ½æœ‰é¢„æµ‹ã€‚
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the code block above, notice that we have to pass the future values of our
    exogenous variables. This is fine, because they are static variables. We know
    the future dates of holidays, and the blog author personally knows when he plans
    on publishing an article.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šé¢çš„ä»£ç å—ä¸­ï¼Œè¯·æ³¨æ„æˆ‘ä»¬å¿…é¡»ä¼ é€’å¤–ç”Ÿå˜é‡çš„æœªæ¥å€¼ã€‚è¿™æ˜¯å¯ä»¥çš„ï¼Œå› ä¸ºå®ƒä»¬æ˜¯é™æ€å˜é‡ã€‚æˆ‘ä»¬çŸ¥é“å‡æœŸçš„æœªæ¥æ—¥æœŸï¼Œåšå®¢ä½œè€…ä¹ŸçŸ¥é“ä»–è®¡åˆ’å‘å¸ƒæ–‡ç« çš„æ—¶é—´ã€‚
- en: Also note that we fine-tune TimeGPT to our data using the *finetune_steps* parameter.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨*finetune_steps*å‚æ•°å¯¹TimeGPTè¿›è¡Œå¾®è°ƒä»¥é€‚åº”æˆ‘ä»¬çš„æ•°æ®ã€‚
- en: Once the loop is done, we can add the predictions to the test set. Again, TimeGPT
    generated seven predictions at a time until 168 predictions were obtained, so
    that we can evaluate its capacity in forecasting the daily views for next week.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å¾ªç¯å®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥å°†é¢„æµ‹æ·»åŠ åˆ°æµ‹è¯•é›†ä¸­ã€‚å†æ¬¡ï¼ŒTimeGPTæ¯æ¬¡ç”Ÿæˆä¸ƒä¸ªé¢„æµ‹ï¼Œç›´åˆ°è·å¾—168ä¸ªé¢„æµ‹ï¼Œä»¥ä¾¿æˆ‘ä»¬è¯„ä¼°å…¶é¢„æµ‹ä¸‹å‘¨æ¯æ—¥æµè§ˆé‡çš„èƒ½åŠ›ã€‚
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/7568508f0602da67a14bc017d7b8893c.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7568508f0602da67a14bc017d7b8893c.png)'
- en: Predictions from TimeGPT. Image by the author.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: TimeGPTçš„é¢„æµ‹ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Forecasting with N-BEATS, N-HiTS and PatchTST
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨N-BEATSã€N-HiTSå’ŒPatchTSTè¿›è¡Œé¢„æµ‹
- en: Now, letâ€™s apply other methods to see if training these models specifically
    on our dataset can produce better predictions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬åº”ç”¨å…¶ä»–æ–¹æ³•ï¼Œçœ‹çœ‹æ˜¯å¦ä¸“é—¨åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šè®­ç»ƒè¿™äº›æ¨¡å‹å¯ä»¥äº§ç”Ÿæ›´å¥½çš„é¢„æµ‹ã€‚
- en: For this experiment, as mentioned before, we use N-BEATS, N-HiTS and PatchTST.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªå®éªŒï¼Œå¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†N-BEATSã€N-HiTSå’ŒPatchTSTã€‚
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Then, we initialize the *NeuralForecast* object and specify the frequency of
    our data, which is daily in this case.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬åˆå§‹åŒ–*NeuralForecast*å¯¹è±¡ï¼Œå¹¶æŒ‡å®šæ•°æ®çš„é¢‘ç‡ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯æ¯æ—¥ã€‚
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Then, we run perform cross-validation over 24 windows of 7 time steps to have
    predictions that align with the test set used for TimeGPT.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ç€ï¼Œæˆ‘ä»¬åœ¨24ä¸ª7æ—¶é—´æ­¥çš„çª—å£ä¸Šè¿›è¡Œäº¤å‰éªŒè¯ï¼Œä»¥è·å¾—ä¸TimeGPTä½¿ç”¨çš„æµ‹è¯•é›†å¯¹é½çš„é¢„æµ‹ã€‚
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Then, we can simply add the predictions from TimeGPT to this new *preds_df*
    DataFrame to have a single DataFrame with the predictions from all models.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å°†TimeGPTçš„é¢„æµ‹ç®€å•åœ°æ·»åŠ åˆ°è¿™ä¸ªæ–°çš„*preds_df* DataFrameä¸­ï¼Œä»¥ä¾¿æ‹¥æœ‰ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ¨¡å‹é¢„æµ‹çš„å•ä¸€DataFrameã€‚
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/88ac6201d63fccff2ba447acfcdfb5e3.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88ac6201d63fccff2ba447acfcdfb5e3.png)'
- en: DataFrame with predictions from all models. Image by the author.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: åŒ…å«æ‰€æœ‰æ¨¡å‹é¢„æµ‹çš„DataFrameã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Great! We are now ready to evaluate the performance of each model.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: å¤ªå¥½äº†ï¼æˆ‘ä»¬ç°åœ¨å‡†å¤‡è¯„ä¼°æ¯ä¸ªæ¨¡å‹çš„æ€§èƒ½ã€‚
- en: Evaluation
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°
- en: Before measuring performance metrics, letâ€™s visualize the predictions of each
    model on our test set.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æµ‹é‡æ€§èƒ½æŒ‡æ ‡ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å¯è§†åŒ–æ¯ä¸ªæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„é¢„æµ‹ã€‚
- en: '![](../Images/1dae3bf9da71a92143bbb581ee80e438.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1dae3bf9da71a92143bbb581ee80e438.png)'
- en: Visualizing the predictions from each model. Image by the author.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å¯è§†åŒ–æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: First, we see a lot of overlapping between each model. However, we do notice
    that N-HiTS predicted two peaks that were not realized in real life. Also, it
    seems that PatchTST is often under-forecasting. However, TimeGPT seem to generally
    overlap the actual data quite well.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬çœ‹åˆ°æ¯ä¸ªæ¨¡å‹ä¹‹é—´æœ‰å¾ˆå¤šé‡å ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ç¡®å®æ³¨æ„åˆ°N-HiTSé¢„æµ‹äº†ä¸¤ä¸ªåœ¨ç°å®ä¸­æœªå®ç°çš„å³°å€¼ã€‚æ­¤å¤–ï¼ŒPatchTSTä¼¼ä¹ç»å¸¸ä½ä¼°é¢„æµ‹ã€‚ç„¶è€Œï¼ŒTimeGPTä¼¼ä¹æ€»ä½“ä¸Šä¸å®é™…æ•°æ®é‡å å¾—ç›¸å½“å¥½ã€‚
- en: Of course, the only way to assess each modelâ€™s performance is to measure performance
    metrics. Here, we use the mean absolute error (MAE) and mean squared error (MSE).
    Also, we round the predictions to whole numbers, as a decimal number makes no
    sense in the context of daily visitors to a blog.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œè¯„ä¼°æ¯ä¸ªæ¨¡å‹æ€§èƒ½çš„å”¯ä¸€æ–¹æ³•æ˜¯æµ‹é‡æ€§èƒ½æŒ‡æ ‡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨å‡å€¼ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰å’Œå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†é¢„æµ‹ç»“æœå››èˆäº”å…¥ä¸ºæ•´æ•°ï¼Œå› ä¸ºåœ¨åšå®¢çš„æ—¥å¸¸è®¿é—®è€…ä¸Šä¸‹æ–‡ä¸­ï¼Œå°æ•°æ²¡æœ‰æ„ä¹‰ã€‚
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/b5f5589624a057c73f2c8b3685b85464.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5f5589624a057c73f2c8b3685b85464.png)'
- en: Performance metrics of each model. Here, TimeGPT is the champion model as it
    achieves the lowest MAE and MSE. Image by the author.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡ã€‚åœ¨è¿™é‡Œï¼ŒTimeGPT æ˜¯å† å†›æ¨¡å‹ï¼Œå› ä¸ºå®ƒå®ç°äº†æœ€ä½çš„ MAE å’Œ MSEã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: From the figure above, we see that TimeGPT is the champion model as it achieves
    the lowest MAE and MSE, followed by N-BEATS, PatchTST and N-HiTS.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° TimeGPT æ˜¯å† å†›æ¨¡å‹ï¼Œå› ä¸ºå®ƒå®ç°äº†æœ€ä½çš„ MAE å’Œ MSEï¼Œç´§éšå…¶åçš„æ˜¯ N-BEATSã€PatchTST å’Œ N-HiTSã€‚
- en: This is an exciting result, as TimeGPT has never seen this dataset and was only
    fine-tuned for a few steps. While this is not an exhaustive experiment, I believe
    it does show a glimpse of the potential foundational models can have in the field
    of forecasting.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªä»¤äººå…´å¥‹çš„ç»“æœï¼Œå› ä¸º TimeGPT ä»æœªè§è¿‡è¿™ä¸ªæ•°æ®é›†ï¼Œåªè¿›è¡Œäº†å°‘é‡çš„å¾®è°ƒã€‚è™½ç„¶è¿™ä¸æ˜¯ä¸€ä¸ªè¯¦å°½çš„å®éªŒï¼Œä½†æˆ‘ç›¸ä¿¡å®ƒç¡®å®å±•ç¤ºäº†åŸºç¡€æ¨¡å‹åœ¨é¢„æµ‹é¢†åŸŸçš„æ½œåŠ›ã€‚
- en: My personal opinion on TimeGPT
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æˆ‘å¯¹ TimeGPT çš„ä¸ªäººçœ‹æ³•
- en: While my short experiment with TimeGPT proved to be exciting, I must point out
    that the [original paper](https://arxiv.org/pdf/2310.03589.pdf) remains vague
    in many important areas.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æˆ‘å¯¹ TimeGPT çš„çŸ­æœŸå®éªŒä»¤äººå…´å¥‹ï¼Œä½†æˆ‘å¿…é¡»æŒ‡å‡ºï¼Œ[åŸå§‹è®ºæ–‡](https://arxiv.org/pdf/2310.03589.pdf) åœ¨è®¸å¤šé‡è¦é¢†åŸŸä»ç„¶æ¨¡ç³Šã€‚
- en: Again, we do not know what datasets were used to train and test the model, so
    we cannot really verify the performance results of TimeGPT, as shown below.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡å¼ºè°ƒï¼Œæˆ‘ä»¬ä¸çŸ¥é“ç”¨äºè®­ç»ƒå’Œæµ‹è¯•æ¨¡å‹çš„æ•°æ®é›†ï¼Œå› æ­¤æ— æ³•çœŸæ­£éªŒè¯ TimeGPT çš„æ€§èƒ½ç»“æœï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/3d0b4e177853479b015c216be4d57815.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d0b4e177853479b015c216be4d57815.png)'
- en: Performance result of TimeGPT as reported in the [original paper](https://arxiv.org/pdf/2310.03589.pdf)
    by Azul Garza and Max Mergenthaler-Canseco
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: TimeGPT çš„æ€§èƒ½ç»“æœå¦‚ [åŸå§‹è®ºæ–‡](https://arxiv.org/pdf/2310.03589.pdf) æ‰€è¿°ï¼Œç”± Azul Garza
    å’Œ Max Mergenthaler-Canseco æä¾›ã€‚
- en: From the table above, we can see that TimeGPT performs best for the monthly
    and weekly frequencies, with N-HiTS and Temporal Fusion Transformer (TFT) usually
    ranking 2nd or 3rd. Then again, because we do not know what data was used, we
    cannot verify these metrics.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šè¡¨ä¸­å¯ä»¥çœ‹å‡ºï¼ŒTimeGPT åœ¨æœˆåº¦å’Œæ¯å‘¨é¢‘ç‡ä¸‹è¡¨ç°æœ€ä½³ï¼ŒN-HiTS å’Œ Temporal Fusion Transformer (TFT) é€šå¸¸æ’åç¬¬2æˆ–ç¬¬3ã€‚å†è€…ï¼Œç”±äºæˆ‘ä»¬ä¸çŸ¥é“ä½¿ç”¨äº†ä»€ä¹ˆæ•°æ®ï¼Œæˆ‘ä»¬æ— æ³•éªŒè¯è¿™äº›æŒ‡æ ‡ã€‚
- en: There is also a lack of transparency when it comes to how the model was trained
    and how it was adapted to handle time series data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºæ¨¡å‹çš„è®­ç»ƒæ–¹å¼ä»¥åŠå¦‚ä½•é€‚åº”æ—¶é—´åºåˆ—æ•°æ®ï¼Œä¹Ÿç¼ºä¹é€æ˜åº¦ã€‚
- en: I believe that the model is intended for commercial use, which explains why
    the paper lacks the details to reproduce TimeGPT. There is nothing wrong with
    that, but the lack of reproducibility of the paper is a concern for the scientific
    community.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç›¸ä¿¡è¯¥æ¨¡å‹æ—¨åœ¨å•†ä¸šä½¿ç”¨ï¼Œè¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆè®ºæ–‡ç¼ºä¹é‡ç° TimeGPT çš„ç»†èŠ‚ã€‚è¿™å¹¶æ²¡æœ‰é”™ï¼Œä½†è®ºæ–‡ç¼ºä¹å¯é‡ç°æ€§å¯¹ç§‘å­¦ç•Œæ¥è¯´æ˜¯ä¸ªé—®é¢˜ã€‚
- en: Still, I hope that this sparks new work and research in foundation models for
    time series, and that we eventually see an open-source version of these models,
    much like we see it happening for LLMs.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘å¸Œæœ›è¿™èƒ½æ¿€å‘åœ¨æ—¶é—´åºåˆ—é¢†åŸŸåŸºç¡€æ¨¡å‹çš„æ–°å·¥ä½œå’Œç ”ç©¶ï¼Œå¹¶ä¸”æˆ‘ä»¬æœ€ç»ˆèƒ½çœ‹åˆ°è¿™äº›æ¨¡å‹çš„å¼€æºç‰ˆæœ¬ï¼Œå°±åƒæˆ‘ä»¬çœ‹åˆ° LLM çš„æƒ…å†µä¸€æ ·ã€‚
- en: Conclusion
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: TimeGPT is the first foundation model for time series forecasting.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: TimeGPT æ˜¯ç¬¬ä¸€ä¸ªç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹çš„åŸºç¡€æ¨¡å‹ã€‚
- en: It leverages the Transformer architecture and was pre-trained on 100 billion
    data points to make zero-shot inference on new unseen data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒåˆ©ç”¨äº† Transformer æ¶æ„ï¼Œå¹¶åœ¨ 1000 äº¿æ•°æ®ç‚¹ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œä»¥å¯¹æ–°çš„æœªè§æ•°æ®è¿›è¡Œé›¶æ ·æœ¬æ¨ç†ã€‚
- en: Combined with the technique of conformal prediction, the model can generate
    prediction intervals and perform anomaly detection without being trained on a
    specific dataset.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“åˆç¬¦åˆé¢„æµ‹æŠ€æœ¯ï¼Œè¯¥æ¨¡å‹å¯ä»¥ç”Ÿæˆé¢„æµ‹åŒºé—´å¹¶æ‰§è¡Œå¼‚å¸¸æ£€æµ‹ï¼Œè€Œæ— éœ€åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚
- en: I still believe that each forecasting problem requires a unique approach, so
    make sure to test out TimeGPT as well as other models.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»ç„¶ç›¸ä¿¡æ¯ä¸ªé¢„æµ‹é—®é¢˜éœ€è¦ç‹¬ç‰¹çš„æ–¹æ³•ï¼Œå› æ­¤è¯·ç¡®ä¿æµ‹è¯• TimeGPT ä»¥åŠå…¶ä»–æ¨¡å‹ã€‚
- en: Thanks for reading! I hope that you enjoyed it and that you learned something
    new!
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼å¸Œæœ›ä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œå¹¶å­¦åˆ°äº†æ–°ä¸œè¥¿ï¼
- en: Looking to master time series forecasting? Then check out my course [Applied
    Time Series Forecasting in Python](https://www.datasciencewithmarco.com/offers/zTAs2hi6/checkout?coupon_code=ATSFP10).
    This is the only course that uses Python to implement statistical, deep learning
    and state-of-the-art models in 16 guided hands-on projects.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è¦æŒæ¡æ—¶é—´åºåˆ—é¢„æµ‹å—ï¼Ÿé‚£å°±æ¥çœ‹çœ‹æˆ‘çš„è¯¾ç¨‹[Pythonä¸­çš„åº”ç”¨æ—¶é—´åºåˆ—é¢„æµ‹](https://www.datasciencewithmarco.com/offers/zTAs2hi6/checkout?coupon_code=ATSFP10)ã€‚è¿™æ˜¯å”¯ä¸€ä¸€ä¸ªé€šè¿‡Pythonå®ç°ç»Ÿè®¡å­¦ã€æ·±åº¦å­¦ä¹ å’Œæœ€å…ˆè¿›æ¨¡å‹çš„è¯¾ç¨‹ï¼ŒåŒ…å«16ä¸ªæŒ‡å¯¼æ€§çš„å®è·µé¡¹ç›®ã€‚
- en: Cheers ğŸ»
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å¹²æ¯ ğŸ»
- en: Support me
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ”¯æŒæˆ‘
- en: Enjoying my work? Show your support with [Buy me a coffee](http://buymeacoffee.com/dswm),
    a simple way for you to encourage me, and I get to enjoy a cup of coffee! If you
    feel like it, just click the button below ğŸ‘‡
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: å–œæ¬¢æˆ‘çš„å·¥ä½œå—ï¼Ÿé€šè¿‡[è¯·æˆ‘å–å’–å•¡](http://buymeacoffee.com/dswm)æ¥æ”¯æŒæˆ‘ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„é¼“åŠ±æ–¹å¼ï¼Œæˆ‘å¯ä»¥äº«å—ä¸€æ¯å’–å•¡ï¼å¦‚æœä½ æ„¿æ„ï¼Œåªéœ€ç‚¹å‡»ä¸‹é¢çš„æŒ‰é’®
    ğŸ‘‡
- en: '![](../Images/a90a701107c4ea11414ef27bd59465af.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a90a701107c4ea11414ef27bd59465af.png)'
- en: References
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[TimeGPT-1](https://arxiv.org/pdf/2310.03589.pdf) by Azul Garza and Max Mergenthaler-Canseco'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[TimeGPT-1](https://arxiv.org/pdf/2310.03589.pdf) ç”±Azul Garzaå’ŒMax Mergenthaler-Cansecoç¼–å†™'
