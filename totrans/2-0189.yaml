- en: A Deep Dive into Autoencoders and Their Relationship to PCA and SVD
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨自编码器及其与 PCA 和 SVD 的关系
- en: 原文：[https://towardsdatascience.com/a-deep-dive-into-autoencoders-and-their-relationship-to-pca-and-svd-97e37c81898a](https://towardsdatascience.com/a-deep-dive-into-autoencoders-and-their-relationship-to-pca-and-svd-97e37c81898a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-deep-dive-into-autoencoders-and-their-relationship-to-pca-and-svd-97e37c81898a](https://towardsdatascience.com/a-deep-dive-into-autoencoders-and-their-relationship-to-pca-and-svd-97e37c81898a)
- en: An in-depth exploration of autoencoders and dimensionality reduction
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对自编码器和降维的深入探索
- en: '[](https://reza-bagheri79.medium.com/?source=post_page-----97e37c81898a--------------------------------)[![Reza
    Bagheri](../Images/7c5a7dc9e6e31048ce31c8d49055987c.png)](https://reza-bagheri79.medium.com/?source=post_page-----97e37c81898a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----97e37c81898a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----97e37c81898a--------------------------------)
    [Reza Bagheri](https://reza-bagheri79.medium.com/?source=post_page-----97e37c81898a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://reza-bagheri79.medium.com/?source=post_page-----97e37c81898a--------------------------------)[![Reza
    Bagheri](../Images/7c5a7dc9e6e31048ce31c8d49055987c.png)](https://reza-bagheri79.medium.com/?source=post_page-----97e37c81898a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----97e37c81898a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----97e37c81898a--------------------------------)
    [Reza Bagheri](https://reza-bagheri79.medium.com/?source=post_page-----97e37c81898a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----97e37c81898a--------------------------------)
    ·46 min read·Jun 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----97e37c81898a--------------------------------)
    ·阅读时间 46 分钟·2023年6月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e2dacc0a1ecbcd18de0a94b4ba484e25.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2dacc0a1ecbcd18de0a94b4ba484e25.png)'
- en: Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: An autoencoder is a type of neural network that learns to reconstruct its input.
    It consists of an encoder network that compresses the input data into a low-dimensional
    space and a decoder network that reconstructs the input data from that space.
    The encoder and decoder are trained jointly to minimize the reconstruction error
    between the input data and its reconstruction.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一种学习重构输入的神经网络。它由一个编码器网络和一个解码器网络组成，其中编码器将输入数据压缩到一个低维空间中，而解码器则从该空间重构输入数据。编码器和解码器被联合训练，以最小化输入数据及其重构之间的重构误差。
- en: Autoencoders can be used for various tasks such as data compression, denoising,
    feature extraction, anomaly detection, and generative modeling. They have applications
    in a wide range of fields such as computer vision, natural language processing,
    and speech recognition. Autoencoders can be also used for dimensionality reduction.
    In fact, one of the main purposes of autoencoders is to learn a compressed representation
    of the input data, which can be used as a form of dimensionality reduction.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器可以用于各种任务，例如数据压缩、去噪、特征提取、异常检测和生成建模。它们在计算机视觉、自然语言处理和语音识别等众多领域都有应用。自编码器还可以用于降维。实际上，自编码器的主要目的之一是学习输入数据的压缩表示，这可以作为一种降维形式。
- en: In this article, we will discuss the underlying math behind autoencoders and
    will see how they can do dimensionality reduction. We also look at the relationship
    between an autoencoder, principal component analysis (PCA), and singular value
    decomposition (SVD). We will also show how to implement both linear and non-linear
    autoencoders in Pytorch.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将讨论自编码器背后的数学原理，并了解它们如何进行降维。我们还将研究自编码器、主成分分析（PCA）和奇异值分解（SVD）之间的关系。我们还将展示如何在
    Pytorch 中实现线性和非线性自编码器。
- en: '**Autoencoder architecture**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**自编码器架构**'
- en: Figure 1 shows the architecture of an autoencoder. As mentioned before an autoencoder
    learns to reconstruct its input data, hence the size of the input and output layers
    is always the same (*n*). Since the autoencoder learns its own input, it does
    not require labeled data for training. Hence it is an unsupervised learning algorithm.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1 显示了自编码器的架构。如前所述，自编码器学习重构其输入数据，因此输入层和输出层的大小总是相同的 (*n*)。由于自编码器学习自己的输入，因此不需要标记数据进行训练。因此，它是一种无监督学习算法。
- en: But what is the point of learning the same input data? As you see, the hidden
    layers in this architecture are shaped in the form of a double-sided funnel in
    which the number of neurons in each layer decreases as we move from the first
    hidden layer to a layer that is referred to as the *bottleneck layer*. This layer
    has the minimum number of neurons. After the bottleneck layer, the number of neurons
    increases once more until reaching the output layer, where it equals the number
    of neurons in the input layer. It is important to note that the number of neurons
    in the bottleneck layer is less than *n*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，学习相同的输入数据有什么意义呢？如你所见，这种架构中的隐藏层呈双侧漏斗状，层间的神经元数量从第一个隐藏层开始逐渐减少，到达称为*瓶颈层*的层时达到最小值。在瓶颈层之后，神经元数量再次增加，直到到达输出层，与输入层的神经元数量相等。值得注意的是，瓶颈层的神经元数量少于*n*。
- en: '![](../Images/4907f5017e45106718d028137958095e.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4907f5017e45106718d028137958095e.png)'
- en: Figure 1 (image was generated using [https://alexlenail.me/NN-SVG/](https://alexlenail.me/NN-SVG/))
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1（图像由[https://alexlenail.me/NN-SVG/](https://alexlenail.me/NN-SVG/)生成）
- en: In a neural network, each layer learns an abstract representation of the input
    space, so the bottleneck layer is indeed a bottleneck for the information that
    transfers between the input and output layers. This layer learns the most compact
    representation of the input data compared to other layers and also learns to extract
    the most important features of the input data. These new features (also called
    *latent variables*) are the result of the transformation of the input data points
    into a continuous lower-dimensional space. In fact, the latent variables can describe
    or explain the input data in a simpler way. The output of the neurons in the bottleneck
    layer represents the values of these latent variables.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，每一层都学习输入空间的抽象表示，因此瓶颈层确实是信息在输入层和输出层之间传递的瓶颈。这一层学习输入数据的最紧凑表示，并且还学习提取输入数据的最重要特征。这些新特征（也称为*潜变量*）是将输入数据点转换为连续低维空间的结果。实际上，潜变量可以以更简单的方式描述或解释输入数据。瓶颈层中神经元的输出表示这些潜变量的值。
- en: The presence of a bottleneck layer is the key feature of this architecture.
    If all the layers in the network had the same number of neurons, the network could
    easily learn to memorize the input data values by passing them all along the network.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 瓶颈层的存在是这种架构的关键特征。如果网络中的所有层都具有相同数量的神经元，网络可以通过将所有输入数据值传递通过网络轻松地记住这些值。
- en: 'An autoencoder can be divided into two networks:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器可以分为两个网络：
- en: 'Encoder network: It starts from the input layer and ends at the bottleneck
    layer. It transforms the high-dimensional input data into the low-dimensional
    space formed by the latent variables. The output of the neurons in the bottleneck
    layer represents the values of these latent variables.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码网络：它从输入层开始，到达瓶颈层结束。它将高维输入数据转换为由潜变量形成的低维空间。瓶颈层中神经元的输出表示这些潜变量的值。
- en: 'Decode network: It starts after the bottleneck layer and ends at the output
    layer. It receives the values of the low dimensional latent variables from the
    bottleneck layer and reconstructs the original high dimensional input data from
    them.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码网络：它从瓶颈层开始，到达输出层结束。它接收来自瓶颈层的低维潜变量的值，并从这些值中重建原始的高维输入数据。
- en: In this article, we are going to discuss the similarity between autoencoders
    and PCA. In order to comprehend PCA, we need some concepts from linear algebra.
    So, we first review linear algebra.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将讨论自编码器与PCA之间的相似性。为了理解PCA，我们需要一些线性代数的概念。因此，我们首先回顾一下线性代数。
- en: '**Linear algebra review: Basis, dimension, and rank**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性代数复习：基，维度和秩**'
- en: 'A set of vectors {***v***₁, ***v***₂, …, ***v_****n*} form a *basis* for the
    vector space *V*, if they are linearly independent and span *V*. If a set of vectors
    is linearly independent, then no vector in the set can be written as a linear
    combination of the other vectors. A set of vectors {***v***₁, ***v***₂, …, ***v_****n*}
    *span* a vector space if every other vector in that space can be written as a
    linear combination of this set. So, any vector ***x*** in *V* can be written as:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一组向量 {***v***₁, ***v***₂, …, ***v_****n*} 形成一个 *基*，如果它们是线性无关的并且张成 *V*。如果一组向量是线性无关的，那么该组中的任何向量都不能表示为其他向量的线性组合。一组向量
    {***v***₁, ***v***₂, …, ***v_****n*} *张成* 一个向量空间，如果该空间中的每个其他向量都可以表示为这一组向量的线性组合。因此，任何
    *V* 中的向量 ***x*** 可以写作：
- en: '![](../Images/db318fb0276021f90b866bc053913712.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db318fb0276021f90b866bc053913712.png)'
- en: 'where *a*₁, *a*₂, …, *a_n* are some constants. The vector space *V* can have
    many different vector bases, but each basis always has the same number of vectors.
    The number of vectors in a basis of a vector space is called the *dimension* of
    that vector space. A basis {***v***₁, ***v***₂, …, ***v_****n*} is *orthonormal*
    when all the vectors are normalized (the length of a normalized vector is 1) and
    orthogonal (mutually perpendicular). In Euclidean space R², the vectors:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *a*₁, *a*₂, …, *a_n* 是一些常数。向量空间 *V* 可以有许多不同的向量基，但每个基总是有相同数量的向量。一个向量空间的基中的向量数量被称为该向量空间的
    *维度*。当所有向量都是标准化的（标准化向量的长度为 1）并且正交（相互垂直）时，基 {***v***₁, ***v***₂, …, ***v_****n*}
    是 *正交标准* 的。在欧几里得空间 R² 中，向量：
- en: '![](../Images/aba4a7dc633c5c17110785ce2f863767.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aba4a7dc633c5c17110785ce2f863767.png)'
- en: form an orthonormal basis which is called the *standard basis*. They are linearly
    independent and span any vectors in R². Since the basis has only two vectors,
    the dimension of R² is 2\. If we have another pair of vectors that are linearly
    independent and span R², that pair can also be a basis for R². For example
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 形成一个被称为 *标准基* 的正交标准基。它们是线性无关的，并且张成 R² 中的任意向量。由于基中只有两个向量，所以 R² 的维度是 2。如果我们有另一对线性无关且张成
    R² 的向量，这对向量也可以作为 R² 的基。例如
- en: '![](../Images/15b0927abd9a5b6c792696ae2e0b57d9.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15b0927abd9a5b6c792696ae2e0b57d9.png)'
- en: 'is also a basis but is not an orthonormal basis since the vectors are not orthogonal.
    More generally we can define the standard basis for R^*n* as:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 也是一个基，但不是一个正交标准基，因为这些向量不正交。更一般地，我们可以将 R^*n* 的标准基定义为：
- en: '![](../Images/5011ff5e783449a9fbeba3f9dbc0ea6b.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5011ff5e783449a9fbeba3f9dbc0ea6b.png)'
- en: where in ***e****ᵢ* the *i*th element is one and all the other elements are
    zero.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中在 ***e****ᵢ* 中，第 *i* 个元素是 1，其他所有元素都是 0。
- en: 'Let the set of vectors *B*={***v***₁, ***v***₂, …, ***v_****n*} form a basis
    for a vector space, then we can write any vector ***x*** in that space in terms
    of the basis vectors:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让向量集 *B*={***v***₁, ***v***₂, …, ***v_****n*} 形成一个向量空间的基，那么我们可以用基向量表示该空间中的任何向量
    ***x***：
- en: '![](../Images/cf613598f4dcc27482a05a11e50d7c0c.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf613598f4dcc27482a05a11e50d7c0c.png)'
- en: 'Hence the coordinates of ***x*** relative to this basis *B* can be written
    as:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，***x*** 相对于这个基 *B* 的坐标可以写作：
- en: '![](../Images/4af591b9909d5d6388181efcfe4b2b5b.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4af591b9909d5d6388181efcfe4b2b5b.png)'
- en: In fact, when we define a vector in R² like
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，当我们像这样定义 R² 中的一个向量
- en: '![](../Images/c34ab85e5a4a3a9b4bd00d1fef939b37.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c34ab85e5a4a3a9b4bd00d1fef939b37.png)'
- en: 'the elements of this vector are its coordinate relative to the standard basis:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个向量的元素就是它相对于标准基的坐标：
- en: '![](../Images/d6feba6ba63ed5ead1d5eae1a14a57c2.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6feba6ba63ed5ead1d5eae1a14a57c2.png)'
- en: 'We can easily find the coordinates of a vector relative to another basis. Suppose
    that we have the vector:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松找到一个向量相对于另一个基的坐标。假设我们有一个向量：
- en: '![](../Images/db318fb0276021f90b866bc053913712.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db318fb0276021f90b866bc053913712.png)'
- en: 'where *B*={***v***₁, ***v***₂, …, ***v_****n*} is a basis. Now we can write:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *B*={***v***₁, ***v***₂, …, ***v_****n*} 是一个基。现在我们可以写作：
- en: '![](../Images/9a6f82324a4c29dfedfd1fff32421c02.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a6f82324a4c29dfedfd1fff32421c02.png)'
- en: 'Here *P_****B*** is called the *change-of-coordinate matrix*, and its columns
    are the vectors in basis *B*. Hence if we have the coordinates of ***x*** relative
    to the basis *B*, we can calculate its coordinates relative to the standard basis
    using Equation 1\. Figure 2 shows an example. Here the *B*={***v***₁, ***v***₂}
    is a basis for R². The vector ***x*** is defined as:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 *P_****B*** 被称为 *坐标变换矩阵*，它的列是基 *B* 中的向量。因此，如果我们知道 ***x*** 相对于基 *B* 的坐标，我们可以使用方程
    1 计算它相对于标准基的坐标。图 2 显示了一个示例。这里的 *B*={***v***₁, ***v***₂} 是 R² 的一个基。向量 ***x*** 被定义为：
- en: '![](../Images/532801f6aef630fb00b44fcaa8a66c86.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: 'And the coordinates of ***x*** relative to *B* is:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23faa3c12fc93d93f8eac85e09dfb754.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: 'So, we have:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b0e0376e15b3434662492f52dc0bd565.png)![](../Images/e162172c918a6b69b0bd05fd42c8d37c.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: Figure 2 (Image by author)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'The *column space* of matrix ***A*** (also written as *Col* ***A***) is the
    set of all linear combinations of the columns of ***A.*** Suppose that we denote
    the columns of the matrix ***A*** by vectors ***a***₁*,* ***a***₂*, …* ***a_****n*.Now
    for any vector like ***u***, ***Au*** can be written as:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/90092a0420df92cf83227923f87e8451.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: Hence, ***Au*** is a linear combination of the columns of ***A***, and the column
    space of ***A*** is the set of vectors that can be written as ***Au***.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'The *row space* of a matrix ***A*** is the set of all linear combinations of
    the rows of ***A****.* Suppose that we denote the rows of matrix ***A*** by vectors
    ***a***₁*ᵀ,* ***a***₂*ᵀ, …* ***a_m***ᵀ:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/511b37c1e3dd944d0b35cda3431a7215.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: The row space of ***A*** is the set of all vectors that can be written as
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3dadba71b02602c9ff99a4a89f08a76e.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: The number of basis vectors of *Col* ***A*** or the dimension of *Col* ***A***
    is called the *rank* of ***A***. The rank of ***A*** is also the maximum number
    of linearly independent columns of ***A***. It can be also shown that the rank
    of a matrix ***A*** is equal to the dimension of its row space, and similarly,
    it is equal to the maximum number of linearly independent rows of ***A***. Hence,
    the rank of a matrix cannot exceed the number of its rows or columns. For example,
    for an *m*×*n* matrix,thentherank cannot be greater than *min*(*m*, *n*).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '**PCA: a review**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis (PCA) is a linear technique. It finds the directions
    in the data that capture the most variation and then projects the data onto a
    lower-dimensional subspace spanned by these directions. PCA is a widely used method
    for reducing the dimensionality of data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: PCA transforms the data into a new orthogonal coordinate system. This coordinate
    system is chosen such that the variance of the projected data points onto the
    first coordinate axis (called the *first principal component*) is maximized. The
    variance of the projected data points onto the second coordinate axis (called
    the *second principal component*) is maximized amongst all possible directions
    orthogonal to the first principal component, and more generally, the variance
    of the projected data points onto each coordinate axis is maximized amongst all
    possible directions orthogonal to the previous principal components.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that we have a dataset with *n* features and *m* data points or observations.
    We can use the *m*×*n* matrix
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cc1fb90e7351c571c80cb2c965c7bbc0.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: to represent this dataset, and we call it the *design matrix*. Hence each row
    of ***X*** represents a data point, and each column represents a feature. We can
    also write ***X*** as
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表示这个数据集，我们称之为*设计矩阵*。因此，***X***的每一行代表一个数据点，每一列代表一个特征。我们还可以将***X***写作
- en: '![](../Images/fde5b00c82eb7f34ca0f0dcdf5784cf9.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fde5b00c82eb7f34ca0f0dcdf5784cf9.png)'
- en: where each column vector
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中每列向量
- en: '![](../Images/a72a2e1746940e875e7ac28760e9db48.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a72a2e1746940e875e7ac28760e9db48.png)'
- en: represents an observation (or data point) in this dataset. Hence, we can think
    of our dataset as a set of *m* vectors in R^*n*. Figure 3 shows an example for
    *n*=2\. Here we can plot each observation as a vector (or simply a point) in R².
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 表示这个数据集中的一个观察值（或数据点）。因此，我们可以将数据集视为 R^*n* 中的 *m* 个向量集合。图 3 显示了 *n*=2 的示例。在这里，我们可以将每个观察值绘制为
    R² 中的一个向量（或简单的点）。
- en: '![](../Images/540fe5a0703eceb257a050e9b05f84e6.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/540fe5a0703eceb257a050e9b05f84e6.png)'
- en: Figure 3 (Image by author)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3（作者提供的图像）
- en: 'Let ***u*** be a unit vector, so we have:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 设 ***u*** 为单位向量，则我们有：
- en: '![](../Images/8722f77d7f569a2133cb4f2c2cb4ffbb.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8722f77d7f569a2133cb4f2c2cb4ffbb.png)'
- en: 'The scalar projection of each data point ***x****ᵢ* onto the vector ***u***
    is:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据点 ***x****ᵢ* 在向量 ***u*** 上的标量投影是：
- en: '![](../Images/1654984871834e088c00938210f0f952.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1654984871834e088c00938210f0f952.png)'
- en: Figure 4 shows an example for *n*=2.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4 显示了 *n*=2 的示例。
- en: '![](../Images/027ece092c781babe4bc9a0817829d18.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/027ece092c781babe4bc9a0817829d18.png)'
- en: Figure 4 (Image by author)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4（作者提供的图像）
- en: We denote the mean of each column of ***X*** by
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来表示 ***X*** 每列的均值
- en: '![](../Images/25eb7d0a8b84a7ff9a9330bc5c8b0063.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25eb7d0a8b84a7ff9a9330bc5c8b0063.png)'
- en: 'Then the mean of the dataset can is defined as:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 那么数据集的均值定义为：
- en: '![](../Images/422418c453d73783aa4d2d885870fde6.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/422418c453d73783aa4d2d885870fde6.png)'
- en: 'And we can also write it as:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将其写作：
- en: '![](../Images/9411fd8c91233df7d47dba76e21021e6.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9411fd8c91233df7d47dba76e21021e6.png)'
- en: 'Now the variance of these projected data points is defined as:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这些投影数据点的方差定义为：
- en: '![](../Images/d93b0dc5f5774ad6402d7df482912cca.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d93b0dc5f5774ad6402d7df482912cca.png)'
- en: Equation 1 can be simplified further. The term
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 1 可以进一步简化。术语
- en: '![](../Images/f85decc86bbf3fc7fff666709bdcd059.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f85decc86bbf3fc7fff666709bdcd059.png)'
- en: is a scalar (since the result of a is a scalar quantity). Besides, we know that
    the transpose of a scalar quantity is equal to itself. So, we can have
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个标量（因为 a 的结果是一个标量量）。此外，我们知道标量量的转置等于其自身。因此，我们可以得到
- en: '![](../Images/6341d8b4cfedea591b32884141ca8350.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6341d8b4cfedea591b32884141ca8350.png)'
- en: Hence the variance of the scalar projection of data points in ***X*** onto the
    vector ***u*** can be written as
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将数据点在 ***X*** 上投影到向量 ***u*** 的方差可以写作
- en: '![](../Images/1a88c83520a48d9dc6260ddc81d7363c.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a88c83520a48d9dc6260ddc81d7363c.png)'
- en: where
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![](../Images/887cc3c1ec0efd9c12837d65b16b6c4a.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/887cc3c1ec0efd9c12837d65b16b6c4a.png)'
- en: is called the *covariance matrix* (Figure 5).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 被称为*协方差矩阵*（图 5）。
- en: '![](../Images/f171668a215c6391e2fb65e940551868.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f171668a215c6391e2fb65e940551868.png)'
- en: Figure 5 (Image by author)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5（作者提供的图像）
- en: 'By simplifying Equation 5, it can be shown that the covariance matrix can be
    written as:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通过简化方程 5，可以显示协方差矩阵可以写作：
- en: '![](../Images/b76dc95b2513e57ce0a0698969700db8.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b76dc95b2513e57ce0a0698969700db8.png)'
- en: where
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![](../Images/3248a08844ca898b822eaba23cb6674a.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3248a08844ca898b822eaba23cb6674a.png)'
- en: Here *xᵢ*,*ₖ* is the (*i*, *k*) element of the design matrix ***X*** (or simply
    the *k*th element of the vector ***x****ᵢ*).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *xᵢ*,*ₖ* 是设计矩阵 ***X*** 的 (*i*, *k*) 元素（或简单地说是向量 ***x****ᵢ* 的 *k*th 元素）。
- en: 'For a dataset with *n* features, the covariance matrix is an *n*×*n* matrix.
    In addition based on the definition of *Sᵢ*,*ⱼ* in Equation 6 we have:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有 *n* 个特征的数据集，协方差矩阵是一个 *n*×*n* 矩阵。此外，基于方程 6 中 *Sᵢ*,*ⱼ* 的定义，我们有：
- en: '![](../Images/fd83b0d5fd5802a8ce940981b60dded5.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd83b0d5fd5802a8ce940981b60dded5.png)'
- en: 'So, its (*i*, *j*) element is equal to its (*j*, *i*) element which means that
    the covariance matrix is a symmetric matrix and is equal to its transpose:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，其 (*i*, *j*) 元素等于其 (*j*, *i*) 元素，这意味着协方差矩阵是对称矩阵，并且等于其转置：
- en: '![](../Images/3b87633cc808fb26dfe5e3b711a7b13c.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b87633cc808fb26dfe5e3b711a7b13c.png)'
- en: Now we find the vector ***u***₁ that maximizes
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们找到向量 ***u***₁ 使其最大化
- en: '![](../Images/bd08d75370f655c15fcd079acd0d3ff2.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd08d75370f655c15fcd079acd0d3ff2.png)'
- en: 'Since ***u***₁ is a normalized vector, we add this constraint to the optimization
    problem:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 ***u***₁ 是一个标准化向量，我们将此约束添加到优化问题中：
- en: '![](../Images/f321e5ecb2216372fcff949fee64f154.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f321e5ecb2216372fcff949fee64f154.png)'
- en: We can solve this optimization problem by adding the Lagrange multiplier *λ*₁
    and maximize
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过添加拉格朗日乘数*λ*₁并最大化来解决这个优化问题
- en: '![](../Images/766d1bb6ffa1658bee11a6f5c045403f.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/766d1bb6ffa1658bee11a6f5c045403f.png)'
- en: 'To do that, we set the derivative of this term with respect to ***u***₁ equal
    to zero:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将该项对***u***₁的导数设为零：
- en: '![](../Images/f99e5827faa23cec7a2e0f0588de77fe.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f99e5827faa23cec7a2e0f0588de77fe.png)'
- en: 'And we get:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到：
- en: '![](../Images/d29b71d631f11881e01bfa08d8480ac4.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d29b71d631f11881e01bfa08d8480ac4.png)'
- en: 'This means that ***u***₁ is an eigenvector of the covariance matrix ***S***,
    and *λ*₁ is its corresponding eigenvalue. We call the eigenvector ***u***₁ the
    first *principal component*. Next, we want to find the unit vector ***u***₂ that
    maximizes ***u***₂ᵀ***Su***₂ amongst all possible directions orthogonal to the
    first principal component. So, we need to find the vector ***u***₂ that maximizes
    ***u***₂*ᵀ****Su***₂ with these constraints:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着***u***₁是协方差矩阵***S***的特征向量，*λ*₁是其对应的特征值。我们称特征向量***u***₁为第一个*主成分*。接下来，我们要找到单位向量***u***₂，使其在所有与第一个主成分正交的方向中最大化***u***₂ᵀ***Su***₂。因此，我们需要找到在这些约束条件下使***u***₂*ᵀ****Su***₂最大化的向量***u***₂：
- en: '![](../Images/d3039d4fa2822fd4d3b7b1bb0c8c3e00.png)![](../Images/a4032df1626261724645ff5f3e98e062.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3039d4fa2822fd4d3b7b1bb0c8c3e00.png)![](../Images/a4032df1626261724645ff5f3e98e062.png)'
- en: 'It can be shown that ***u***₂ is the solution of this equation:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明***u***₂ 是这个方程的解：
- en: '![](../Images/e587a287535f41ea34777ccdfd6430a4.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e587a287535f41ea34777ccdfd6430a4.png)'
- en: So we conclude that ***u***₂ is also an eigenvector of ***S***, and *λ*₂ is
    its corresponding eigenvalue (proof is given in the appendix). More generally,
    we want to find the unit vector ***u****ᵢ* that maximizes ***u****ᵢᵀ****Su****ᵢ*
    amongst all possible directions orthogonal to the previous principal components
    ***u***₁…***u_****i*-1\. Hence, we need to find the vector ***u****ᵢ* that maximizes
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们得出结论，***u***₂也是***S***的特征向量，*λ*₂ 是其对应的特征值（证明见附录）。更一般地，我们希望找到单位向量***u****ᵢ*，使其在所有与先前主成分***u***₁…***u****i*-1正交的方向中最大化***u****ᵢᵀ****Su****ᵢ*。因此，我们需要找到使得
- en: '![](../Images/278b15c978542f6ee4700b08847045c1.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/278b15c978542f6ee4700b08847045c1.png)'
- en: 'with these constraints:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些约束条件下：
- en: '![](../Images/c53a1113e808d0bd90ad22f90a4bece1.png)![](../Images/6560b5efee8d1c3c3d95b35f0eef3164.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c53a1113e808d0bd90ad22f90a4bece1.png)![](../Images/6560b5efee8d1c3c3d95b35f0eef3164.png)'
- en: Again it can be shown that ***u****ᵢ* is the solution to this equation
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 再次可以证明，***u****ᵢ* 是这个方程的解
- en: '![](../Images/4b494b69d5107c9b88a7a642cd40fe0a.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b494b69d5107c9b88a7a642cd40fe0a.png)'
- en: 'Hence ***u****ᵢ* is an eigenvector of ***S***, and *λᵢ* is its corresponding
    eigenvalue (proof is given in the appendix). The vector ***u****ᵢ* is called the
    *i*th principal component. If we multiply the previous equation by ***u****ⱼ*ᵀwe
    get:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，***u****ᵢ* 是***S***的特征向量，*λᵢ* 是其对应的特征值（证明见附录）。向量***u****ᵢ* 称为第*i*个主成分。如果我们将前面的方程乘以***u****ⱼ*ᵀ，得到：
- en: '![](../Images/c0d5bf7cbc1f6b08e842935eadb4fd5c.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0d5bf7cbc1f6b08e842935eadb4fd5c.png)'
- en: Hence, we conclude that the variance of the scalar projection of the data points
    in ***X*** onto the eigenvector ***u****ᵢ* is equal to its corresponding eigenvalue.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得出结论，数据点在***X***中沿着特征向量***u****ᵢ*的标量投影的方差等于其对应的特征值。
- en: If we have a dataset with *n* features, the covariance matrix will be an *n*×*n*
    symmetric matrix. Here each data point can be represented by a vector in R^*n*
    (***x****ᵢ*). As mentioned before, the elements of a vector in R^*n* give its
    coordinates relative to the standard basis.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个具有*n*特征的数据集，那么协方差矩阵将是一个*n*×*n*的对称矩阵。这里每个数据点可以表示为R^*n*中的一个向量（***x****ᵢ*）。如前所述，R^*n*中向量的元素给出了相对于标准基的坐标。
- en: It can be shown that an *n*×*n* symmetric matrix has *n* real eigenvalues, and
    *n* linearly independent and orthogonal corresponding eigenvectors (spectral theorem).
    These *n* orthogonal eigenvectors are the principal components of this dataset.
    It can be shown that a set of *n* orthogonal vectors can form a basis for *R^n*.
    So, these principal components form an orthogonal basis and can be used to define
    a new coordinate system for the data points (Figure 6).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明一个*n*×*n*的对称矩阵有*n*个实特征值，以及*n*个线性无关且正交的对应特征向量（谱定理）。这*n*个正交特征向量是这个数据集的主成分。可以证明一组*n*个正交向量可以形成R^n*的一个基。因此，这些主成分形成了一个正交基，可以用来为数据点定义一个新的坐标系统（图6）。
- en: 'We can easily calculate the coordinates of each data point ***x****ᵢ* relative
    to this new coordinate system. Let *B*={***v***₁, ***v***₂, …, ***v_****n*} be
    the set of the principal components. We first write ***x****ᵢ* in terms of the
    basis vectors:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以很容易地计算每个数据点***x****ᵢ*相对于这个新坐标系统的坐标。令*B*={***v***₁, ***v***₂, …, ***v_****n*}为主成分的集合。我们首先将***x****ᵢ*用基向量表示：
- en: '![](../Images/58877e06237f1b6ba7fa58406fb555c0.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58877e06237f1b6ba7fa58406fb555c0.png)'
- en: 'Now if we multiply both sides of this equation by ***v****ᵢᵀ* we have:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果我们将这个方程的两边都乘以***v****ᵢᵀ*，我们有：
- en: '![](../Images/6c184280a7f4314af45ee3b0c942e440.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c184280a7f4314af45ee3b0c942e440.png)'
- en: 'Since we have an orthogonal basis:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有一个正交基：
- en: '![](../Images/23fd9936ef40e5d49304bff8a44316d6.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23fd9936ef40e5d49304bff8a44316d6.png)'
- en: So, it follows that
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，得出：
- en: '![](../Images/d53da74f67ce7a9f40edd6c3cb25dadc.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d53da74f67ce7a9f40edd6c3cb25dadc.png)'
- en: 'Since the dot product is commutative, we can also write:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由于点积是交换律的，我们也可以写作：
- en: '![](../Images/4b34900c0ff768f400ee756513f39c41.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b34900c0ff768f400ee756513f39c41.png)'
- en: 'Hence, the coordinates of ***x****ᵢ* relative to *B* are:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，***x****ᵢ*相对于*B*的坐标为：
- en: '![](../Images/8c8a81a42068a2cf2ad932bc17687112.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c8a81a42068a2cf2ad932bc17687112.png)'
- en: and the design matrix can be written as
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 设计矩阵可以写作
- en: '![](../Images/292866b1f22920a345aa921f44ea684e.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/292866b1f22920a345aa921f44ea684e.png)'
- en: in the new coordinate system. Here each row represents a data point (observation)
    in the new coordinate system. Figure 6 shows an example for *n*=2.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在新的坐标系统中。这里每行表示新坐标系统中的一个数据点（观察）。图6展示了*n*=2的一个示例。
- en: '![](../Images/4dd6cac6f24eb2d68468cbf0690050f3.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4dd6cac6f24eb2d68468cbf0690050f3.png)'
- en: Figure 6 (Image by author)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图6（作者提供的图片）
- en: The variance of the scalar projection of data points onto each eigenvector (principal
    component) is equal to its corresponding eigenvalue. The first principal component
    has the greatest eigenvalue (variance). The second principal component has the
    second greatest eigenvalue and so on. Now we can choose the first *d* principal
    components and project the original data points on the subspace spanned by them.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 数据点在每个特征向量（主成分）上的标量投影的方差等于其对应的特征值。第一个主成分具有最大的特征值（方差）。第二个主成分具有第二大的特征值，以此类推。现在我们可以选择前*d*个主成分，并将原始数据点投影到由它们张成的子空间中。
- en: 'So, we transform the original data points (with *n* features) to these projected
    data points that belong to a *d*-dimensional subspace. In this way, we reduce
    the dimensionality of the original dataset from *n* to *d* while maximizing the
    variance of the projected data. Now the first *d* columns of the matrix in Equation
    9 give the coordinates of the projected data points:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将原始数据点（具有*n*特征）转换为这些投影的数据点，这些数据点属于一个*d*维子空间。通过这种方式，我们将原始数据集的维度从*n*减少到*d*，同时最大化投影数据的方差。现在，方程9中的前*d*列给出了投影数据点的坐标：
- en: '![](../Images/378ef9b400c36f6f6987fcbd0d6d0deb.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/378ef9b400c36f6f6987fcbd0d6d0deb.png)'
- en: 'Figure 7 gives an example of this transformation. The original dataset has
    3 features (*n*=3) and we reduce its dimensionality to *d*=2 by projecting the
    data points on the plane formed by the first two principal components (***v***₁,
    ***v***₂). The coordinates of each data point ***x****ᵢ* in the subspace spanned
    by ***v***₁ and ***v***₂ are:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图7给出了这种转换的一个示例。原始数据集具有3个特征（*n*=3），我们通过将数据点投影到由前两个主成分（***v***₁, ***v***₂）形成的平面上，将其维度减少到*d*=2。子空间中每个数据点***x****ᵢ*的坐标为：
- en: '![](../Images/188092dd8e69f772385f8e4391487248.png)![](../Images/8f8509c9df30f70be8dde9cd08953bad.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/188092dd8e69f772385f8e4391487248.png)![](../Images/8f8509c9df30f70be8dde9cd08953bad.png)'
- en: Figure 7 (Image by author)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图7（作者提供的图片）
- en: It is usual to *center* the dataset around zero before the PCA analysis. To
    do that we first create the design matrix ***X*** that represents our dataset
    (Equation 2). Then we create a new matrix ***Y*** by subtracting the mean of each
    column from the elements in that column
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行PCA分析之前，通常会将数据集以零为中心。为此，我们首先创建表示我们数据集的设计矩阵***X***（方程2）。然后通过从每列的元素中减去每列的均值来创建一个新矩阵***Y***。
- en: '![](../Images/24e96abb3bbcca8d75d8878acebc6497.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24e96abb3bbcca8d75d8878acebc6497.png)'
- en: 'The matrix ***Y*** represents the centered dataset. In this new matrix, the
    mean of each column is zero:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵***Y***代表中心化的数据集。在这个新矩阵中，每列的均值为零：
- en: '![](../Images/022b51899c9497d6e7fa1aafb522612e.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/022b51899c9497d6e7fa1aafb522612e.png)'
- en: 'So, the mean of the dataset is also zero:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数据集的均值也为零：
- en: '![](../Images/806a53877eb163ea345cb632f785cd39.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: 'Now suppose that we start with a centered design matrix ***X*** and want to
    calculate its covariance matrix. Hence, the mean of each column of ***X*** is
    zero. From Equation 6 we have:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f91326b63332fb300aa99087bb2951cd.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
- en: where [***X***]_*k*,*j* denotes the (*k*, *j*) element of the matrix ***X***.
    By using the definition of matrix multiplication, we get
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b11bfdeeaafd7ee26e5c9dcaf91f5744.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: Please note that this equation is only valid when the design matrix (***X***)
    is centered.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '**The relation between PCA and singular value decomposition (SVD)**'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that ***A*** is an *m*×*n* matrix. Then ***A****ᵀ****A*** will be a
    square *n*×*n* matrix, and it can be easily shown that it is a symmetric matrix.
    Since ***A****ᵀ****A*** is symmetric, it has *n* real eigenvalues and *n* linear
    independent and orthogonal eigenvectors (spectral theorem). We call these eigenvectors
    ***v***₁, ***v***₂, …, ***v_****n* and we assume they are normalized. It can be
    shown the eigenvalues of ***A****ᵀ****A*** are all positive.Now assume that we
    label them in decreasing order, so:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b653a829442c2fe625796d745402f2a2.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: Let ***v***₁, ***v***₂, …, ***v_****n* be the eigenvectors of ***A****ᵀ****A***
    corresponding to these eigenvalues. We define the *singular value* of the matrix
    ***A*** (denoted by *σᵢ*) as the square root of *λᵢ*. So we have
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff02620b0ea15a6a74fb51d8587c2a97.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: Now suppose that the rank of ***A*** is *r*.Thenit can be shown thatthe number
    of the nonzero eigenvalues of ***A****ᵀ****A*** or the number of nonzero singular
    values of ***A*** is *r:*
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/143d9c3615dfd4d0585e04fcdff0c9be.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: Now the singular value decomposition (SVD) of ***A*** can be written as
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fbd608f5637d92e23176f9ddb0660c50.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: 'Here ***V*** is an *n×n* matrix and its columns are ***v****ᵢ*:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ba305e512e8d72055941aac9e8a6a4d.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: '***Σ*** is an *m*×*n* diagonal matrix, and all the elements of ***Σ*** are
    zero except the first *r* diagonal elements which are equal to the singular values
    of ***A***. We define the matrix ***U*** as'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d3e04330a9b43747a5052a8c263f43a9.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: We define ***u***₁ *to* ***u_****r* as
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/725763b1db2caf57b49bc415976051bd.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: 'We can easily show that these vectors are orthogonal:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3978641faa43dfb1eb106c4bb938252b.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: Here we used the fact that ***v_****j* is an eigenvector of ***A****ᵀ****A***
    and these eigenvectors are orthogonal. Since these vectors are orthogonal, they
    are also linearly independent. The other ***u****ᵢ* vectors (*r<i≤m*) are defined
    in a way that ***u***₁, ***u***₂, *…****u****_m* form a basis for an *m*-dimensional
    vector space (***R^****m)*.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'Let ***X*** be a centered design matrix, and its SVD decomposition is as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/45893a365216f8687be4a61093e2df13.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: As mentioned before, ***v***₁, ***v***₂, …, ***v_****n* arethe eigenvectors
    of ***X****ᵀ****X*** and the singular values are the square root of their corresponding
    eigenvalues. Hence, we have
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，***v***₁, ***v***₂, …, ***v_****n* 是 ***X****ᵀ****X*** 的特征向量，而奇异值是其对应特征值的平方根。因此，我们有：
- en: '![](../Images/48ff2947d2be0fb1975b5b1bfd2cccf0.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48ff2947d2be0fb1975b5b1bfd2cccf0.png)'
- en: Now we can divide both sides of the previous equation by *m* (where *m* is the
    number of data points) and use Equation 10, to get
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将前一个方程的两边除以 *m*（其中 *m* 是数据点的数量），并使用方程 10，得到：
- en: '![](../Images/c15bc382793ef750e30828f1b43f4d9b.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c15bc382793ef750e30828f1b43f4d9b.png)'
- en: Hence, it follows that ***v****ᵢ* is the eigenvector of the covariance matrix
    and its corresponding eigenvalue is the square of its corresponding singular value
    divided by *m*. So, the matrix ***V*** in the SVD equation gives the principal
    components of ***X,*** and using the singular values in ***Σ***, we can easily
    calculate the eigenvalues. In summary, we can use SVD to do PCA.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，***v****ᵢ* 是协方差矩阵的特征向量，其对应的特征值是其对应奇异值的平方除以 *m*。所以，SVD 方程中的矩阵 ***V*** 给出了
    ***X*** 的主要成分，并且使用 ***Σ*** 中的奇异值，我们可以轻松计算特征值。总之，我们可以使用 SVD 来进行 PCA。
- en: 'Let’s see what else we can get from the SVD equation. We can simplify ***UΣ***
    in Equation 12 using Equations 3 and 11:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看从 SVD 方程中还能得到什么。我们可以使用方程 3 和方程 11 来简化方程 12 中的 ***UΣ***：
- en: '![](../Images/d6e5bee059cbe2770e7184e384ef8511.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6e5bee059cbe2770e7184e384ef8511.png)'
- en: Comparing with Equation 9, we conclude that the *i*th row of ***UΣ*** gives
    the coordinates of the data point ***x****ᵢ* relative to the basis defined by
    the principal components.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 与方程 9 相比，我们可以得出结论，***UΣ*** 的第 *i* 行给出了数据点 ***x****ᵢ* 相对于主要成分定义的基的坐标。
- en: 'Now suppose that in Equation 12, we only keep the first *k* columns of ***U***,
    the first *k* rows of ***V***, and the first *k* rows and columns of ***Σ***.
    If we multiply them together, we get:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设在方程 12 中，我们只保留 ***U*** 的前 *k* 列、***V*** 的前 *k* 行以及 ***Σ*** 的前 *k* 行和列。如果我们将它们相乘，我们得到：
- en: '![](../Images/4dbed1e1d397ef36fee631b4a2e0afe4.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4dbed1e1d397ef36fee631b4a2e0afe4.png)'
- en: 'Please note that ***X****ₖ* is still an *m*×*n* matrix. If we multiply ***X****ₖ*
    by the vector *b* which has *n* elements, we get:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，***X****ₖ* 仍然是一个 *m*×*n* 矩阵。如果我们将 ***X****ₖ* 乘以具有 *n* 个元素的向量 *b*，我们得到：
- en: '![](../Images/7e21e89f86125daf2b55e20688ee43d9.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e21e89f86125daf2b55e20688ee43d9.png)'
- en: where [***Cb***]*ᵢ* is the *i*th element of the vector ***Cb***. Since ***u***₁,
    ***u***₂, …, ***u****ₖ* are linearly independent vectors (remember that they form
    a basis, so they should be linearly independent) and they span ***X****ₖ****b***,we
    conclude that they form a basis for ***X****ₖ****b***. This basis has *k* vectors,
    so the dimension of the column space of ***X****ₖ* is k. Hence ***X****ₖ* is a
    rank-*k* matrix.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 [***Cb***]*ᵢ* 是向量 ***Cb*** 的 *i* 项元素。由于 ***u***₁, ***u***₂, …, ***u****ₖ*
    是线性无关的向量（记住它们构成一个基，因此应该是线性无关的），并且它们生成了 ***X****ₖ****b***，我们可以得出结论，它们构成了 ***X****ₖ****b***
    的基。这一基有 *k* 个向量，所以 ***X****ₖ* 的列空间维度是 k。因此，***X****ₖ* 是一个秩为 *k* 的矩阵。
- en: 'But what does ***X****ₖ* represent? Using Equation 13 we can write:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 但 ***X****ₖ* 代表什么呢？使用方程 13，我们可以写成：
- en: '![](../Images/e2e3b0efa44486b437999d136176e6c5.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2e3b0efa44486b437999d136176e6c5.png)'
- en: 'So, the *i*th row of ***X****ₖ* is the transpose of:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，***X****ₖ* 的第 *i* 行是以下的转置：
- en: '![](../Images/dd908a370b8d3585568465ace026a906.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd908a370b8d3585568465ace026a906.png)'
- en: 'which is the vector projection of the data point ***x****ᵢ* on the subspace
    spanned by the principal components ***v***₁, ***v***₂, … ***v****ₖ*. Remember
    that ***v***₁, ***v***₂, … ***v_****n* is a basis for our original dataset. In
    addition, the coordinates of ***x****ᵢ* relative to this basis are:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这是数据点 ***x****ᵢ* 在由主要成分 ***v***₁, ***v***₂, … ***v****ₖ* 张成的子空间上的向量投影。记住，***v***₁,
    ***v***₂, … ***v_****n* 是我们原始数据集的基。此外，***x****ᵢ* 相对于这一基的坐标为：
- en: '![](../Images/83a7881c292c9664ba48d9a5a68b9498.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83a7881c292c9664ba48d9a5a68b9498.png)'
- en: 'Hence, using Equation 1, we can write ***x****ᵢ* as:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用方程 1，我们可以将 ***x****ᵢ* 写成：
- en: '![](../Images/9ec4d7f941048378697084d99b930b04.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ec4d7f941048378697084d99b930b04.png)'
- en: Now we can decompose ***x****ᵢ* into two vectors. One is in the subspace defined
    by vectors defined by ***v***₁, ***v***₂, … ***v****ₖ,* and the in the subspace
    defined by the remaining vectors.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将 ***x****ᵢ* 分解为两个向量。一个在由 ***v***₁, ***v***₂, … ***v****ₖ* 定义的子空间中，另一个在由剩余向量定义的子空间中。
- en: '![](../Images/1a717a39ffb75adaf876cce9dbdeb5ab.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a717a39ffb75adaf876cce9dbdeb5ab.png)'
- en: The first vector is the result of the projection of ***x****ᵢ* onto the subspace
    defined by vectors defined by ***v***₁, ***v***₂, … ***v****ₖ* and is equal to***x̃****ᵢ.*
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Remember that each row of the design matrix ***X*** represents one of the original
    data points. Similarly, each row of ***X****ₖ* represents the same data point
    projected on the subspace spanned by the principal components ***v***₁, ***v***₂,
    … ***v****ₖ* (Figure 8).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/611c8e23b710a72bc5afd26eec12fc31.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: Figure 8 (Image by author)
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can calculate the distance between the original data point (***x****ᵢ*)
    and the projected data point (***x̃****ᵢ*). The square of the distance between
    the vectors ***x****ᵢ* and *x̃ᵢ* is:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/905c89fec77def9e1672fdc68c8b2811.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: 'And if we add the square of the distances for all the data points, we get:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/81d5352479dd7bafd9075b0536cbff71.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
- en: 'The Frobenius norm of an *m*×*n* matrix ***C*** is defined as:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b6cad328dbf3f941728bd5567e308bf1.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: 'Since vectors ***x****ᵢ* and ***x̃****ᵢ* are the transpose of the rows of matrices
    ***X*** and ***X****ₖ*, we can write:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09324c163ac9fd6f16a27fab93350e3c.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: Hence the Frobenius norm of ***X***-***X****ₖ* is proportional to the sum of
    the square of the distances between the original data points and the projected
    data points (Figure 9), and as the projected data points get closer to the original
    data points || ***X***-***X****ₖ* ||_*F* decreases.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/12d5b906b1d56e135a990293b0bb12bc.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: Figure 9 (Image by author)
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: We want the projected points to be a good approximation of the original data
    points, so we want ***X****ₖ* to give the lowest value for ***X***-***X****ₖ*
    among all the rank-*k* matrices.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that we have the *m*×*n* matrix ***X*** with rank =*r* and the singular
    values of ***X*** are sorted, so we have:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a3ec7a72e61d91be3bbfd91796432947.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
- en: 'It can be shown that ***X****ₖ* minimizes the Frobenius norm of ***X***-***A***among
    all the *m*×*n* matrices ***A*** that have a rank of *k*. Mathematically:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8a35344f08fbc667dc89e8e9250e38d.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: '***X****ₖ* is the closest matrix to ***X*** among all the rank-*k* matrices
    and can be considered as the best rank-*k* approximation of the design matrix
    ***X***. This also means that the projected data points represented by ***X****ₖ*
    are the rank-*k* best approximation (in terms of the total error) for the original
    data points represented by ***X***.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Now we can try to write the previous equation in a different format. Suppose
    that ***Z*** is an *m*×*k* and ***W*** is a *k*×*n* matrix. We can show that
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e3d088cd09f112d82971634ff31071e.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
- en: So finding a rank-*k* matrix ***A*** that minimizes ||***X***-***A***||_*F*
    is equivalent to finding the matrices ***Z*** and ***W*** that minimize ||***X***-***ZW***||_*F*
    (proof is given in the appendix). Therefore, we can write
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/68062afc6044b6b2b97b0d59a57cd8be.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
- en: where ***Z******(**an*m*×*k* matrix**)**and ***W**** (a *k*×*n* matrix) are
    the solutions to the minimization problem and we have
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ***Z****（*an*m*×*k* 矩阵**）和 ***W****（*k*×*n* 矩阵）是最小化问题的解，我们有
- en: '![](../Images/fe87575101ce799d0a2a4670fc20d711.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe87575101ce799d0a2a4670fc20d711.png)'
- en: 'Now based on Equations 13 and 14 we can write:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，根据方程 13 和 14，我们可以写出：
- en: '![](../Images/7a81721d1659060c1046f7bbe16074ef.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a81721d1659060c1046f7bbe16074ef.png)'
- en: 'So, if we solve the minimization problem in Equation 18 using SVD, we get the
    following values for ***Z**** and ***W****:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们使用 SVD 求解方程 18 中的最小化问题，我们得到以下 ***Z**** 和 ***W**** 的值：
- en: '![](../Images/0d4732d15e6efd52c2d5193abb75cdd9.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d4732d15e6efd52c2d5193abb75cdd9.png)'
- en: and
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![](../Images/737d7836d4dd78fb820bc97e83b1ef9a.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/737d7836d4dd78fb820bc97e83b1ef9a.png)'
- en: The rows of ***W**** give the transpose of the principal components and the
    rows of ***Z**** give the transpose of the coordinates of each projected data
    point relative to the basis formed by these principal components. It is important
    to note that the principal components form an orthonormal basis (so the principal
    components are both normalized and orthogonal). In fact, we can assume that PCA
    only looks for a matrix ***W*** in which the rows form an orthonormal set. We
    know that when two vectors are orthogonal, their inner product is zero, so we
    can say that PCA (or SVD) solves the minimization problem
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '***W**** 的行给出了主成分的转置，***Z**** 的行给出了相对于这些主成分形成的基的每个投影数据点的坐标的转置。重要的是，主成分形成了一个正交归一基（因此主成分既是归一化的，又是正交的）。实际上，我们可以假设
    PCA 仅仅是寻找一个矩阵 ***W***，其中行形成一个正交归一集合。我们知道当两个向量正交时，它们的内积为零，因此我们可以说 PCA（或 SVD）解决了最小化问题'
- en: '![](../Images/ed824011dba0506e4e91eb96bc23170a.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed824011dba0506e4e91eb96bc23170a.png)'
- en: 'with this constraint:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 具有以下约束：
- en: '![](../Images/2010d212356df33524845684a4065c3c.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2010d212356df33524845684a4065c3c.png)'
- en: where ***Z*** and ***W*** are *m*×*k* and *k*×*n* matrices. In addition, if
    ***Z****are ***W**** are the solutions to the minimization problem then we have
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ***Z*** 和 ***W*** 是 *m*×*k* 和 *k*×*n* 矩阵。此外，如果 ***Z**** 和 ***W**** 是最小化问题的解，那么我们有
- en: '![](../Images/8fa2fe8a2c73fb62a2e832d2f4496c97.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fa2fe8a2c73fb62a2e832d2f4496c97.png)'
- en: This formulation is very important since it allows us to establish a connection
    between PCA and autoencoders.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式非常重要，因为它使我们能够建立 PCA 和自编码器之间的联系。
- en: '**The relation between PCA and autoencoders**'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**PCA 与自编码器之间的关系**'
- en: We start with an autoencoder which only has three layers and is shown in Figure
    10\. This network has *n* input features denoted by *x*₁…*x_n* and *n* neurons
    at the output layer. The outputs of the network are denoted by *x^*₁…*x^_n*. The
    hidden layer has *k* neurons (where *k*<*n*) and the outputs of the hidden layer
    are denoted by *z*₁…*zₖ*. The matrices ***W^***[1] and ***W^***[2] contain the
    weights of the hidden layer and output layer respectively.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个只有三层的自编码器开始，如图 10 所示。这个网络有 *n* 个输入特征，表示为 *x*₁…*x_n*，以及 *n* 个输出层神经元。网络的输出表示为
    *x^*₁…*x^_n*。隐藏层有 *k* 个神经元（其中 *k*<*n*），隐藏层的输出表示为 *z*₁…*zₖ*。矩阵 ***W^***[1] 和 ***W^***[2]
    分别包含隐藏层和输出层的权重。
- en: '![](../Images/eb4da49a74bf4d860aade53b7a261851.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb4da49a74bf4d860aade53b7a261851.png)'
- en: Figure 10 (Image by author)
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10（作者提供的图片）
- en: Here
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里
- en: '![](../Images/8d5705a4a14d59a9895717a0ebf61b53.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d5705a4a14d59a9895717a0ebf61b53.png)'
- en: represents the weight for the input *j*th input (coming from the *j*th neuron
    in layer *l*-1) of the *i*th neuron in layer *l* (Figure 11). Here we assume that
    for the hidden *l*=1, and for the output layer *l*=2.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 表示第 *i* 个神经元在第 *l* 层的输入 *j* 的权重（来自第 *j* 个神经元在第 *l*-1 层）（图 11）。在这里我们假设隐藏层的 *l*=1，输出层的
    *l*=2。
- en: '![](../Images/65947050c4fea7c4ce5d13f9d325969f.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65947050c4fea7c4ce5d13f9d325969f.png)'
- en: Figure 11 (Image by author)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11（作者提供的图片）
- en: 'Hence the weights of the hidden layer are given by:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，隐藏层的权重由下式给出：
- en: '![](../Images/83dfe42f7a7be7e122be9c825b95e777.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83dfe42f7a7be7e122be9c825b95e777.png)'
- en: 'And the *i*th row of this matrix gives all the weights of the *i*th neuron
    in the hidden layer. Similarly, the weights of the output layer are given by:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 该矩阵的第 *i* 行给出了隐藏层中第 *i* 个神经元的所有权重。同样，输出层的权重由下式给出：
- en: '![](../Images/56d04d8a1865ed2a982c479f9fc47190.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56d04d8a1865ed2a982c479f9fc47190.png)'
- en: 'Each neuron has an activation function. We can calculate the output of a neuron
    in the hidden layer (activation of that neuron) using the weight matrix ***W^***[1]
    and input features:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元都有一个激活函数。我们可以使用权重矩阵 ***W^***[1] 和输入特征计算隐藏层中神经元的输出（该神经元的激活）：
- en: '![](../Images/be59678a64fe58bbd5932904528473d4.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be59678a64fe58bbd5932904528473d4.png)'
- en: 'Where *bᵢ*^[1] is the bias for the *i*th neuron, and *g^*[1] is the activation
    function of the neurons in layer 1\. We can write this equation in vectorized
    form as:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*bᵢ*^[1]是第*i*个神经元的偏置，*g^*[1]是第1层神经元的激活函数。我们可以将这个方程写成向量化形式：
- en: '![](../Images/49e5878cdf4fde3b10e9af434a218edd.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49e5878cdf4fde3b10e9af434a218edd.png)'
- en: 'where ***b*** is the vector of biases:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 其中***b***是偏置向量：
- en: '![](../Images/856de7aeb3d175f99f9f0a0cad8df56a.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/856de7aeb3d175f99f9f0a0cad8df56a.png)'
- en: 'and ***x*** is the vector of input features:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 而***x***是输入特征的向量：
- en: '![](../Images/7e9d70a3cde486a3c677d0d63e293a26.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e9d70a3cde486a3c677d0d63e293a26.png)'
- en: 'Similarly, we can write the output of the *i*th neuron in the output layer
    as:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们可以将输出层的第*i*个神经元的输出写成：
- en: '![](../Images/caba3c1cc8acffa518f1562bdc2ca89a.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/caba3c1cc8acffa518f1562bdc2ca89a.png)'
- en: 'And in the vectorized form, it becomes:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在向量化形式中，它变为：
- en: '![](../Images/3f3e1ca2f0386c6ae1417cc6b69e236b.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f3e1ca2f0386c6ae1417cc6b69e236b.png)'
- en: 'Now suppose that we use the following design matrix as a training dataset to
    train this network:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们使用以下设计矩阵作为训练数据集来训练这个网络：
- en: '![](../Images/102d9c1989fffb688deed2146a811664.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/102d9c1989fffb688deed2146a811664.png)'
- en: Hence the training dataset has *m* observations (examples) and *n* features.
    Remember that the *i*th observation is represented by the vector
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，训练数据集有*m*个观察值（样本）和*n*个特征。记住，第*i*个观察值由向量表示
- en: '![](../Images/a72a2e1746940e875e7ac28760e9db48.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a72a2e1746940e875e7ac28760e9db48.png)'
- en: 'If we feed this vector into the network, the output of the network is denoted
    by this vector:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这个向量输入到网络中，网络的输出由这个向量表示：
- en: '![](../Images/9f89d6b7d3a2633ca974cef6928096aa.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f89d6b7d3a2633ca974cef6928096aa.png)'
- en: 'We also need to make the following assumptions to make sure that the autoencoder
    mimics PCA:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要做以下假设，以确保自编码器模拟PCA：
- en: '1-The training dataset is centered, so the mean of each column of ***X*** is
    zero:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 1-训练数据集是中心化的，因此***X***的每列均值为零：
- en: '![](../Images/58a30a9a00cf4eca4e2c7c8cf5e4a89a.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58a30a9a00cf4eca4e2c7c8cf5e4a89a.png)'
- en: '2-The activation of functions of the hidden and output layer is linear and
    the bias of all neurons is zero. This means that we are using a linear encoder
    and decoder in this network. Hence, we have:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 2-隐藏层和输出层的激活函数是线性的，所有神经元的偏置为零。这意味着我们在这个网络中使用的是线性编码器和解码器。因此，我们有：
- en: '![](../Images/d7a85cf23b8490b901dca850ba990b23.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7a85cf23b8490b901dca850ba990b23.png)'
- en: '3-We use the quadratic loss function to train this network. Hence the cost
    function is the mean squared error (MSE) and is defined as:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 3-我们使用二次损失函数来训练这个网络。因此，成本函数是均方误差（MSE），定义为：
- en: '![](../Images/01e8dd9cb447ce0c92498f999b8ec1f1.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01e8dd9cb447ce0c92498f999b8ec1f1.png)'
- en: 'Now we can show that:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以证明：
- en: '![](../Images/63545cbdfcbc8291f5d52a47c4b7a9c3.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63545cbdfcbc8291f5d52a47c4b7a9c3.png)'
- en: 'where ***Z*** is defined as:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 其中***Z***定义为：
- en: '![](../Images/8050e399e609f7ab6f1ed1c4f24b4c75.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8050e399e609f7ab6f1ed1c4f24b4c75.png)'
- en: 'The proof is given in the appendix. Here the *i*th row of ***Z*** gives the
    output of the hidden layer when the *i*th observation is fed into the network.
    Hence minimizing the cost function of this network is the same as minimizing:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 证明在附录中给出。这里***Z***的第*i*行表示当第*i*个观察值输入网络时隐藏层的输出。因此，最小化这个网络的成本函数等同于最小化：
- en: '![](../Images/24ec93b6748632ed86cadeb764d4caa4.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24ec93b6748632ed86cadeb764d4caa4.png)'
- en: where we define the matrix ***W*** as
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们定义矩阵***W***为
- en: '![](../Images/b59158741e371db665d931bf3cff17cc.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b59158741e371db665d931bf3cff17cc.png)'
- en: Please note that each row of ***W***^[2]is a column of ***W***.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，***W***^[2]的每一行是***W***的一列。
- en: 'We know that if we multiply a function with a positive multiplier, its minimum
    does not change. So, we can remove the multiplier 1/(2*m*) when we minimize the
    cost function. Hence by training this network, we are solving this minimization
    problem:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，如果用一个正的乘数乘以一个函数，其最小值不会改变。因此，我们可以在最小化成本函数时去掉乘数1/(2*m*)。因此，通过训练这个网络，我们解决了这个最小化问题：
- en: '![](../Images/ccd9f3f42aede7a1eb82f57de3006ff2.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ccd9f3f42aede7a1eb82f57de3006ff2.png)'
- en: 'where ***Z*** and ***W*** are *m*×*k* and *k*×*n* matrices. If we compare this
    equation with Equation 20, we see that it is the same minimization problem of
    PCA. Hence the solution should be the same as that of Equation 20:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 其中***Z***和***W***分别是*m*×*k*和*k*×*n*矩阵。如果我们将这个方程与方程20进行比较，我们会发现它是PCA的相同最小化问题。因此，解决方案应该与方程20的结果相同：
- en: '![](../Images/e5ebe99fe7fd2781001281dbe68c25d3.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e5ebe99fe7fd2781001281dbe68c25d3.png)'
- en: However, we have an important difference here. The constraint of Equation 21
    is not applied here. So here is the question. Are the optimum values of ***Z***
    and ***W*** found by the autoencoder the same as those of PCA? Should the rows
    of ***W**** always form an orthogonal set?
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里有一个重要的区别。方程21的约束没有在这里应用。那么问题来了。自动编码器找到的 ***Z*** 和 ***W*** 的最优值是否与 PCA 的相同？***W***
    的行是否总是应该形成正交集？
- en: First, let’s expand the previous equation.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们扩展之前的方程。
- en: '![](../Images/cc3ed899728203115fdd29dfa14b03d9.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc3ed899728203115fdd29dfa14b03d9.png)'
- en: 'We know that the *i*th row of ***X****ₖ* is the transpose of:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，***X***ₖ* 的 *i* 行是以下内容的转置：
- en: '![](../Images/dd908a370b8d3585568465ace026a906.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd908a370b8d3585568465ace026a906.png)'
- en: which is the vector projection of the data point ***x****ᵢ* on the subspace
    spanned by the principal components ***v***₁, ***v***₂, … ***v****ₖ*. Hence, ***x̃****ᵢ*
    belongs to a *k*-dimensional subspace. The vectors ***w***₁, ***w***₂, … ***w****ₖ*
    should be linearly independent. Otherwise, the rank of ***W**** will be less than
    *k* (remember that the rank of ***W**** is equal to the maximum number of linearly
    independent rows of ***W****), and based on Equation A.3 the rank of ***X****ₖ*
    will be less than *k*. It can be shown that a set of *k* linearly independent
    vectors form a basis for a *k*-dimensional subspace. Hence, we conclude that the
    vectors ***w***₁, ***w***₂, … ***w****ₖ* also form a basis for the same subspace
    spanned by the principal components. We can now use Equation 24 to write *i*th
    row of ***X****ₖ* in terms of the vectors ***w***₁, ***w***₂, … ***w****ₖ*.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这是数据点 ***x***ᵢ* 在由主成分 ***v***₁、***v***₂、… ***v***ₖ* 张成的子空间上的向量投影。因此，***x̃***ᵢ*
    属于 *k* 维子空间。向量 ***w***₁、***w***₂、… ***w***ₖ* 应该是线性无关的。否则，***W*** 的秩将小于 *k*（记住
    ***W*** 的秩等于 ***W*** 的最大线性无关行数），根据方程 A.3，***X***ₖ* 的秩也将小于 *k*。可以证明，一组 *k* 个线性无关的向量形成
    *k* 维子空间的基底。因此，我们得出结论，向量 ***w***₁、***w***₂、… ***w***ₖ* 也形成与主成分所张成的相同子空间的基底。我们现在可以使用方程24将
    ***X***ₖ* 的 *i* 行表示为向量 ***w***₁、***w***₂、… ***w***ₖ* 的线性组合。
- en: '![](../Images/40b2efbe3ad938905d36a9e1d0ba3365.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40b2efbe3ad938905d36a9e1d0ba3365.png)'
- en: This means that the *i*th row of ***Z**** simply gives the coordinates of ***x̃****ᵢ*
    relative to the basis formed by the vectors ***w***₁, ***w***₂, … ***w****ₖ*.
    Figure 12 shows an example of *k*=2.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 ***Z*** 的 *i* 行仅给出相对于由向量 ***w***₁、***w***₂、… ***w***ₖ* 形成的基底的 ***x̃***ᵢ*
    的坐标。图12 显示了 *k*=2 的示例。
- en: '![](../Images/53f5c77da04bb7960484954ff71792f5.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53f5c77da04bb7960484954ff71792f5.png)'
- en: Figure 12 (Image by author)
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图12（作者提供的图像）
- en: 'In summary, the matrices ***Z**** and ***W**** found by the autoencoder can
    generate the same subspace spanned by the principal components. We also get the
    same projected data points of PCA since:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，自动编码器找到的矩阵 ***Z*** 和 ***W*** 可以生成与主成分所张成的相同子空间。由于以下原因，我们也得到了 PCA 的相同投影数据点：
- en: '![](../Images/f4c6afe30370b94f94e96e5dae1a361a.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4c6afe30370b94f94e96e5dae1a361a.png)'
- en: However, these matrices define a new basis for that subspace. Unlike the principal
    components found by PCA, the vectors of this new basis are not necessarily orthogonal.
    The rows of ***W**** give the transpose of the vectors of the new basis and the
    rows of ***Z**** give the transpose of the coordinates of each data point relative
    to that basis.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些矩阵定义了该子空间的新基底。与 PCA 找到的主成分不同，这个新基底的向量不一定是正交的。***W*** 的行给出了新基底向量的转置，而 ***Z***
    的行给出了相对于该基底的每个数据点坐标的转置。
- en: So we conclude that a linear autoencoder cannot find the principal component,
    but it can find the subspace spanned by them using a different basis. There is
    one exception here.. Suppose that we only want to keep the first principal component
    ***v***₁. So we want to reduce the dimensionality of the original dataset from
    *n* to 1\. In this case, the sunspace is just a straight line spanned by the first
    principal component. A linear autoencoder will also find the same line with a
    different basis vector ***w***₁. This basis vector is not necessarily normalized
    and might have the opposite direction of ***v***₁, but it is still on the same
    line (subspace). This is demonstrated in Figure 13\. Now, if we normalize ***w***₁,
    we get the first principal component of the dataset. So in such a case, a linear
    autoencoder is able to the first principal component indirectly.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得出结论，线性自编码器不能找到主成分，但它可以使用不同的基找到由主成分张成的子空间。这里有一个例外……假设我们只想保留第一个主成分***v***₁。因此，我们希望将原始数据集的维度从*n*减少到1。在这种情况下，子空间只是由第一个主成分张成的直线。线性自编码器也会找到同一条直线，但使用不同的基向量***w***₁。这个基向量不一定是标准化的，可能具有与***v***₁相反的方向，但它仍然在同一条直线上（子空间）。这在图13中得到了证明。现在，如果我们将***w***₁标准化，我们将得到数据集的第一个主成分。因此，在这种情况下，线性自编码器能够间接地获得第一个主成分。
- en: '![](../Images/f85ad79cdd64767fceae0572aef5e0f0.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f85ad79cdd64767fceae0572aef5e0f0.png)'
- en: Figure 13
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图13
- en: So far, we have discussed the theory underlying autoencoders and PCA. Now let’s
    see an example in Python. In the next section, we will create an autoencoder using
    Pytorch and compare it with PCA.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了自编码器和PCA的基础理论。现在让我们看一个Python示例。在下一部分，我们将使用Pytorch创建一个自编码器，并与PCA进行比较。
- en: '**Case study: PCA vs autoencoder**'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例研究：PCA与自编码器**'
- en: We first need to create a dataset. Listing 1 creates a simple dataset with 3
    features. The first two features (*x*₁ and *x*₂) have a 2d multivariate normal
    distribution and the 3rd feature (*x*₃) is equal to half of *x*₂. This dataset
    is stored in the array `X` which plays the role of the design matrix. We also
    center the design matrix.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要创建一个数据集。清单1创建了一个具有3个特征的简单数据集。前两个特征（*x*₁和*x*₂）具有二维多元正态分布，第3个特征（*x*₃）等于*x*₂的一半。这个数据集存储在数组`X`中，充当设计矩阵。我们还对设计矩阵进行了中心化处理。
- en: '[PRE0]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Listing 2 creates a 3d plot of this dataset, and the result is shown in Figure
    14.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 清单2创建了这个数据集的3d图，结果如图14所示。
- en: '[PRE1]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/bb3727796e4dbb6040fdcc8418487e49.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb3727796e4dbb6040fdcc8418487e49.png)'
- en: Figure 14
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图14
- en: As you see this dataset is defined on the plane represented by *x*₃= *x*₁/2\.
    Now we start the PCA analysis.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个数据集定义在由*x*₃= *x*₁/2表示的平面上。现在我们开始PCA分析。
- en: '[PRE2]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can easily get the principal components (eigenvectors of `X`) using the `components_`
    field. It returns an array in which each row represents one of the principal components.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`components_`字段轻松获取主成分（`X`的特征向量）。它返回一个数组，每行表示一个主成分。
- en: '[PRE3]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can also see their corresponding eigenvalues using the `explained_variance_`
    field. Remember that the variance of the scalar projection of data points onto
    the eigenvector ***u****ᵢ* is equal to its corresponding eigenvalue.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过`explained_variance_`字段查看对应的特征值。记住，数据点在特征向量***u***ᵢ上的标量投影的方差等于其对应的特征值。
- en: '[PRE5]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Please note that the eigenvalues are sorted in descending order. So the first
    row of `pca.components_` gives the first principal component. Listing 3 plots
    the principal components besides the data points (Figure 15).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，特征值是按降序排列的。因此，`pca.components_`的第一行给出第一个主成分。清单3绘制了主成分与数据点的图（见图15）。
- en: '[PRE7]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/35703b15aeb523219b54ca9ca4d2dd3d.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35703b15aeb523219b54ca9ca4d2dd3d.png)'
- en: Figure 15
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图15
- en: Please also note that the 3rd eigenvalue is almost zero. That is because the
    dataset lies on a 2d plane (*x*₃= *x*₁/2), and as Figure 15 shows it has no variance
    along ***v***₃. We can use the `transform()` method to get the coordinates of
    each data point relative to the new coordinate system defined by the principal
    components. Each row of the array returned by `transform()` gives the coordinates
    of one of the data points.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，第3个特征值几乎为零。这是因为数据集位于二维平面上（*x*₃= *x*₁/2），如图15所示，它在***v***₃方向上没有方差。我们可以使用`transform()`方法获取每个数据点相对于主成分定义的新坐标系统的坐标。`transform()`返回的数组的每一行给出一个数据点的坐标。
- en: '[PRE8]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now we can choose the first 2 principal components and project the original
    data points on the subspace spanned by them. So, we transform the original data
    points (with *3* features) to these projected data points that belong to a 2-dimensional
    subspace. To do that we only need to drop the 3rd column of the array returned
    by `pca.transform(X)`. This means that we reduce the dimensionality of the original
    dataset from 3 to 2 while maximizing the variance of the projected data. Listing
    5 plots this 2d dataset, and the result is shown in Figure 16.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/c01c51bb119d4559c5986b30bad9652a.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
- en: Figure 16
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: We could also get the same results using SVD. Listing 6 uses the `svd()` function
    in `numpy` to do the singular value decomposition of `X`.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This function returns the matrices ***U*** and ***V****ᵀ* and the diagonal elements
    of ***Σ*** (remember that the other elements of ***Σ*** are zero). Please note
    that the rows of ***V****ᵀ* give the same principal componentsreturned by`pca.omponents_`.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 'Now to get ***X****ₖ* we only keep the first 2 columns of ***U*** and ***V***
    and the first 2 rows and columns of ***Σ*** (Equation 14). If we multiply them
    together, we get:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f12fe9fc2937245f6bbb379a1046fba5.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
- en: 'Listing 7 calculates this matrix:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Each row of ***Z****=***U***₂***Σ***₂ gives the coordinates of one of the projected
    data points relative to the basis formed by the first 2 principal components.
    Listing 8 calculates ***Z****=***U***₂***Σ***₂. Please note that it gives the
    first two columns of `pca.transform(X)` given in Listing 4\. So PCA and SVD both
    find the same subspace and the same projected data points.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now we create an autoencoder and train it with this data set to later compare
    it with PCA. Figure 17 shows the network architecture. The bottleneck layer has
    two neurons since we want to project the data points on a 2-dimensional subspace.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/39c23c90c07acfa7a7f19f410b812e9e.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
- en: Figure 17 (Image by author)
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9 defines this architecture in Pytorch. The neurons in all the layers
    have a linear activation function and a zero bias.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We use the MSE cost function and Adam optimizer.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We use the design matrix defined in Listing 1 to train this model.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then we train it for 3000 epochs:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The Pytorch tensor `encoded` stores the output of the hidden layer (*z*₁, *z*₂),
    and the tensor `decoded` stores the output of the autoencoder (*x^*₁, *x^*₂, *x^*₃).
    We first convert them into `numpy` arrays.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As mentioned before the linear autoencoder with a centered dataset and MSE
    cost function solves the following minimization problem:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ccd9f3f42aede7a1eb82f57de3006ff2.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
- en: where
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b59158741e371db665d931bf3cff17cc.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
- en: 'And ***Z*** contains the output of the bottleneck layer for all the examples
    in the training dataset. We also saw that the solution to this minimization is
    given by Equation 23\. So, in this case, we have:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1363e42e9de39bbe00d2eb313140c105.png)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
- en: 'Once we train the autoencoder, we can retrieve the matrices ***Z**** and ***W****.
    The array `encoded` gives the matrix ***Z****:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练了自编码器，我们可以检索矩阵 ***Z**** 和 ***W****。数组 `encoded` 给出矩阵 ***Z****：
- en: '[PRE23]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Listing 12 retrieves the matrix ***W^***[2]:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12 检索矩阵 ***W^***[2]：
- en: '[PRE25]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'And to get ***W**** we can write:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 要得到 ***W****，我们可以写出：
- en: '[PRE27]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Each row of ***W**** represents one of the basis vectors (***w****ᵢ*), and
    since the bottleneck layer has two neurons, we end up with two basis vectors (***w***₁,
    ***w***₂). We can easily see that ***w***₁ and ***w***₂ don’t form an orthogonal
    basis since their inner product is not zero:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '***W**** 的每一行表示一个基向量 (***w****ᵢ*)，由于瓶颈层有两个神经元，我们最终得到两个基向量 (***w***₁, ***w***₂)。我们可以很容易地看到
    ***w***₁ 和 ***w***₂ 不形成正交基，因为它们的内积不为零：'
- en: '[PRE29]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now we can easily calculate ***X***₂ using Equation 25:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用公式 25 轻松计算 ***X***₂：
- en: '[PRE31]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Please note that this array and the array `X2` which was calculated using SVD
    in Listing 7, are the same (there is a small difference between them due to numerical
    errors). As mentioned before, each row of ***Z**** gives the coordinates of the
    projected data points (***x̃****ᵢ*) relative to the basis formed by the vectors
    ***w***₁ and ***w***₂.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个数组和在列表 7 中使用 SVD 计算得到的数组 `X2` 是相同的（由于数值误差，它们之间有一个小的差异）。如前所述，***Z**** 的每一行给出了相对于由向量
    ***w***₁ 和 ***w***₂ 形成的基的投影数据点 (***x̃****ᵢ*) 的坐标。
- en: Listing 13 plots the dataset, its principal components ***v***₁ and ***v***₂,
    and the new basis vectors ***w***₁ and ***w***₂ in two different views. The result
    is shown in Figure 18\. Please note that the data points and basis vectors all
    lie on the same plane. Please note that training the autoencoder starts with the
    random initialization of weights, so if we don’t use a random seed in Listing
    9, the vectors ***w***₁ and ***w***₂ will be different, however, they always lie
    on the same plane of the principal components.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13 绘制数据集、其主成分 ***v***₁ 和 ***v***₂ 以及新的基向量 ***w***₁ 和 ***w***₂ 的两个不同视图。结果如图
    18 所示。请注意，数据点和基向量都位于同一平面上。请注意，训练自编码器从权重的随机初始化开始，因此如果我们在列表 9 中不使用随机种子，向量 ***w***₁
    和 ***w***₂ 将会不同，但它们始终位于主成分的同一平面上。
- en: '[PRE33]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '![](../Images/8a6d11464ab866194541fb7bba9f5f90.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a6d11464ab866194541fb7bba9f5f90.png)'
- en: Figure 18
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18
- en: Listing 14 plots the rows of ***Z**** and the result is shown in Figure 19\.
    These rows represent the encoded data points. It is important to note that if
    we compare this plot with that of Figure 16, they look different. We know that
    both the autoencoder and PCA give the same projected data points (same ***X***₂),
    but when we plot these projected data points in a 2d space, they look different.
    Why?
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14 绘制了 ***Z**** 的行，结果如图 19 所示。这些行表示编码后的数据点。需要注意的是，如果我们将这个图与图 16 进行比较，它们看起来不同。我们知道自编码器和
    PCA 给出的投影数据点（相同的 ***X***₂）是相同的，但当我们在二维空间中绘制这些投影数据点时，它们看起来不同。为什么？
- en: '[PRE34]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](../Images/1a770b9390508713ff7e9ca6228fc3a9.png)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a770b9390508713ff7e9ca6228fc3a9.png)'
- en: Figure 19
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19
- en: The reason is that we have a different basis for each plot. In Figure 16, we
    have the coordinates of the projected data points relative to the orthogonal basis
    formed by ***v***₁ and ***v***₂. However, in Figure 19, the coordinates of the
    projected data points are relative to the ***w***₁ and ***w***₂ which are not
    orthogonal. So if we try to plot them using an orthogonal coordinate system (like
    that of Figure 19), we get a distorted plot. This is also demonstrated in Figure
    20.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是我们为每个图有不同的基。在图 16 中，我们有相对于由 ***v***₁ 和 ***v***₂ 形成的正交基的投影数据点的坐标。然而，在图 19
    中，投影数据点的坐标是相对于 ***w***₁ 和 ***w***₂ 的，它们不是正交的。因此，如果我们尝试使用正交坐标系统（如图 19 的坐标系统）来绘制它们，我们会得到一个失真的图。这一点在图
    20 中也得到了演示。
- en: '![](../Images/c3b87b6cb798566db5bc8e33f7989684.png)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3b87b6cb798566db5bc8e33f7989684.png)'
- en: Figure 20 (Image by author)
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20（图像由作者提供）
- en: To have the correct plot of the rows ***Z****, we first need to find the coordinates
    of the vectors ***w***₁ and ***w***₂ relative to the orthogonal basis formed by
    *V*={***v***₁, ***v***₂}.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 要正确绘制 ***Z**** 的行，我们首先需要找到向量 ***w***₁ 和 ***w***₂ 相对于由 *V*={***v***₁, ***v***₂}
    形成的正交基的坐标。
- en: '![](../Images/aa056c59839d139f9b4ba80e437de752.png)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa056c59839d139f9b4ba80e437de752.png)'
- en: We know that the transpose of each row of ***Z**** gives the coordinates of
    a projected data point relative to the basis formed by *W*={***w***₁, ***w***₂}.
    So, we can use Equation 1 to get the coordinates of the same data point relative
    to the orthogonal basis *V*={***v***₁, ***v***₂}
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，***Z*** 的每一行的转置给出了相对于由 *W*={***w***₁, ***w***₂} 形成的基的投影数据点的坐标。因此，我们可以使用方程
    1 来获取相对于正交基 *V*={***v***₁, ***v***₂} 的相同数据点的坐标。
- en: '![](../Images/110809ceba18fe4ae932cbb1ff41e2ee.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/110809ceba18fe4ae932cbb1ff41e2ee.png)'
- en: where
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![](../Images/09cf621b1347be64ff1914f7ffeff0f1.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09cf621b1347be64ff1914f7ffeff0f1.png)'
- en: is the change-of-coordinate matrix. Listing 15 uses these equations to plot
    the rows of ***Z**** relative to the orthogonal basis *V*={***v***₁, ***v***₂}.
    The result is shown in Figure 21, and now it exactly looks like the plot of Figure
    15 which was generated using SVD.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 是坐标变换矩阵。列表 15 使用这些方程将 ***Z*** 的行相对于正交基 *V*={***v***₁, ***v***₂} 绘制出来。结果如图 21
    所示，现在它与图 15 使用 SVD 生成的图形完全一致。
- en: '[PRE35]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](../Images/bac576768ae1cf36b53e8f563fa5ae06.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bac576768ae1cf36b53e8f563fa5ae06.png)'
- en: Figure 21
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21
- en: Figure 22 demonstrates the different components of the linear autoencoder that
    was created in this case study and the geometrical interpretation of their values.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22 展示了在此案例研究中创建的线性自编码器的不同组件及其值的几何解释。
- en: '![](../Images/7ef3b78d6db73668380803a24adb3e72.png)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ef3b78d6db73668380803a24adb3e72.png)'
- en: Figure 22 (Image by author)
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22（作者提供的图片）
- en: '**Non-linear autoencoders**'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '**非线性自编码器**'
- en: Though an autoencoder is not able to find the principal components of a dataset,
    it is still a much more powerful tool for dimensionality reduction compared to
    PCA. In this section, we will discuss non-linear autoencoders, and we will see
    an example in which PCA fails, but a non-linear autoencoder can still do the dimensionality
    reduction. One problem with PCA is that assumes that the maximum variances of
    the projected data points are along the principal components. In other words,
    it assumes that they are all along straight lines, and in many real applications,
    this is not true.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自编码器不能找到数据集的主成分，但它仍然是比 PCA 更强大的降维工具。在本节中，我们将讨论非线性自编码器，并举一个 PCA 失败而非线性自编码器仍能进行降维的例子。PCA
    的一个问题是它假设投影数据点的最大方差沿主成分方向。换句话说，它假设这些方差都在直线上，而在许多实际应用中，这种假设并不成立。
- en: Let’s see an example. Listing 16 generates a random circular dataset called
    `X_circ` and plots it in Figure 23\. The dataset has 70 data points. `X_circ`
    is a 2d array and each row of that represents one of the data points (observations).
    We also assign a color to each data point. The color is not used for modeling
    and we only add it to keep the order of the data points.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。列表 16 生成了一个名为`X_circ`的随机圆形数据集，并在图 23中绘制了它。数据集包含 70 个数据点。`X_circ`是一个
    2d 数组，每一行代表一个数据点（观察值）。我们还为每个数据点分配了一个颜色。这个颜色在建模中并没有使用，我们只是为了保持数据点的顺序。
- en: '[PRE36]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![](../Images/219dc78e8859d1e9871c74043d178f09.png)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/219dc78e8859d1e9871c74043d178f09.png)'
- en: Figure 23
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23
- en: Next, we use PCA to find the principal components of this dataset. Listing 17
    finds the principal components and plots them in Figure 24.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 PCA 查找该数据集的主成分。列表 17 找到主成分并在图 24 中绘制它们。
- en: '[PRE37]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](../Images/39d64ba714ef6579fb254914868573f7.png)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39d64ba714ef6579fb254914868573f7.png)'
- en: Figure 24
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 图 24
- en: In this data set the maximum variance is along a circle not a straight line.
    However, PCA still assumes that the maximum variance of the projected data points
    is along the vector ***v***₁ (the first principal component). Listing 18 calculates
    the coordinates of the projected data points onto ***v***₁ and plots them in Figure
    25.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中，最大方差沿着圆圈而不是直线。然而，PCA 仍然假设投影数据点的最大方差沿向量 ***v***₁（第一个主成分）。列表 18 计算了投影数据点到
    ***v***₁ 的坐标，并在图 25 中绘制了它们。
- en: '[PRE38]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![](../Images/cd0aac564886e2796b0a02bc789183d9.png)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd0aac564886e2796b0a02bc789183d9.png)'
- en: Figure 25
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 图 25
- en: As you see the projected data points have lost their order and the colors are
    mixed. Now we train a non-linear autoencoder on this dataset. Figure 26 shows
    its architecture. The network has two input features and two neurons in the output
    layer. There are 5 hidden layers, and the number of neurons in the hidden layers
    is 64, 32, 1, 32, and 64 respectively. So, the bottleneck layer has only one neuron
    which means that we want to reduce the dimension of the training dataset from
    2 to 1.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，投影数据点已经丧失了顺序，颜色也混杂在一起。现在我们在这个数据集上训练一个非线性自编码器。图 26 展示了它的架构。网络有两个输入特征和两个神经元的输出层。共有
    5 个隐藏层，隐藏层中的神经元数量分别为 64、32、1、32 和 64。因此，瓶颈层只有一个神经元，这意味着我们希望将训练数据集的维度从 2 降至 1。
- en: '![](../Images/6bf3d46caca9032700030cf969bfff87.png)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bf3d46caca9032700030cf969bfff87.png)'
- en: Figure 26 (Image by author)
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 图 26（作者提供的图片）
- en: One thing that you may have noticed is that the number of neurons in the first
    hidden layer increases. Hence only the hidden layers have a double-sided funnel
    shape. That is because we only have two input features, so we need to add more
    neurons in the first hidden layer to have enough neurons for training the network.
    Listing 19 defines the autoencoder network in Pytorch.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到的一点是，第一个隐藏层中的神经元数量在增加。因此，只有隐藏层具有双面漏斗形状。这是因为我们只有两个输入特征，所以需要在第一个隐藏层中增加更多神经元，以便为训练网络提供足够的神经元。列表
    19 定义了 Pytorch 中的自编码器网络。
- en: '[PRE39]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: As you see all the hidden layers have a non-linear RELU activation function
    now. We still use the MSE cost function and the Adam optimizer.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，现在所有隐藏层都有一个非线性 RELU 激活函数。我们仍然使用 MSE 损失函数和 Adam 优化器。
- en: '[PRE40]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We use `X_circ` as the training dataset, but we use `MinMaxScaler()` to scale
    all the features into the range [0,1].
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `X_circ` 作为训练数据集，但我们使用 `MinMaxScaler()` 将所有特征缩放到 [0,1] 的范围内。
- en: '[PRE41]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Next, we train the model with 5000 epochs.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们用 5000 个周期训练模型。
- en: '[PRE42]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Finally, we plot the values of the single neuron in the bottleneck layer (encoded
    data) for all the observations in the training dataset. Remember that we assigned
    a color to each data point in the training dataset. Now we use the same color
    for the encoded data points. This plot is shown in Figure 27, and now compared
    to the projected data point generated by PCA (Figure 25), most of the projected
    data points have the right order.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们绘制了瓶颈层（编码数据）中单个神经元的值，用于训练数据集中的所有观测点。请记住，我们为训练数据集中的每个数据点分配了颜色。现在我们使用相同的颜色表示编码数据点。这个图在图
    27 中显示，现在与 PCA 生成的投影数据点（图 25）相比，大多数投影数据点的顺序是正确的。
- en: '[PRE44]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![](../Images/6611990f4a83bffc3c87bad4886073bf.png)'
  id: totrans-442
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6611990f4a83bffc3c87bad4886073bf.png)'
- en: Figure 27
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 图 27
- en: That is because the non-linear autoencoder doesn’t project the original data
    points on a straight line anymore. The autoencoder tries to find a curve (also
    called the non-linear manifold) along which the projected data points have the
    highest variance and projects the input data points on them (Figure 28). This
    example clearly shows the advantage of an autoencoder over PCA. PCA is a linear
    transformation, so it is not suitable for a dataset having non-linear correlations.
    On the other hand, we may employ non-linear activation functions in autoencoders.
    This enables us to do non-linear dimensionality reduction using an autoencoder.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为非线性自编码器不再将原始数据点投影到一条直线上。自编码器试图找到一条曲线（也称为非线性流形），沿着这条曲线，投影的数据点具有最大的方差，并将输入数据点投影到这些曲线上（图
    28）。这个例子清楚地展示了自编码器相对于 PCA 的优势。PCA 是线性变换，因此不适用于具有非线性相关的数据集。另一方面，我们可以在自编码器中使用非线性激活函数。这使我们能够使用自编码器进行非线性降维。
- en: '![](../Images/34a292b1dc7cad5b90725425df4b7fea.png)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34a292b1dc7cad5b90725425df4b7fea.png)'
- en: Figure 28 (Image by author)
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 图 28（作者提供的图片）
- en: In this article, we discussed the math behind PCA, SVD, and autoencoders. PCA
    finds a new orthogonal coordinate system for the data set. Each axis of this coordinate
    system is called a principal component. The principal components are chosen such
    that the variance of the projected data points onto each coordinate axis is maximized
    amongst all possible directions orthogonal to the principal components already
    considered.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们讨论了 PCA、SVD 和自编码器背后的数学。PCA 为数据集找到一个新的正交坐标系统。这个坐标系统的每个轴称为主成分。选择主成分的方式是使数据点在每个坐标轴上的方差在所有可能的方向上最大化，这些方向都与已经考虑过的主成分正交。
- en: A linear autoencoder and PCA have some similarities. We saw that an autoencoder
    with centered input features, linear activations, and an MSE cost function can
    find the same subspace spanned by the principal components. We also get the same
    projected data points of PCA. However, it cannot find the principal components
    themselves. In fact, the basis that it returns is not even necessarily orthogonal.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 线性自编码器和PCA有一些相似之处。我们看到一个具有中心化输入特征、线性激活和MSE成本函数的自编码器可以找到由主成分张成的相同子空间。我们也得到PCA的相同投影数据点。然而，它不能找到主成分本身。实际上，它返回的基底甚至不一定是正交的。
- en: On the other hand, autoencoders are more flexible than PCA. One problem with
    PCA is that it assumes that the maximum variance of the projected data points
    is along a line represented by the principal components. Hence, if the maximum
    variance is along a non-linear curve, PCA cannot find it. However, an autoencoder
    with non-linear activation functions can find a non-linear manifold and project
    the data point onto it.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，自编码器比PCA更灵活。PCA的一个问题是，它假设投影数据点的最大方差沿着由主成分表示的直线。因此，如果最大方差沿着非线性曲线，PCA无法找到它。然而，一个具有非线性激活函数的自编码器可以找到一个非线性流形，并将数据点投影到上面。
- en: 'I hope that you enjoyed reading this article. Please let me know if you have
    any questions or suggestions. All the Code Listings in this article are available
    for download as a Jupyter Notebook from GitHub at:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你喜欢阅读这篇文章。如果你有任何问题或建议，请告诉我。文章中的所有代码清单都可以从GitHub下载为Jupyter Notebook，链接为：
- en: '[https://github.com/reza-bagheri/autoencoders_pca](https://github.com/reza-bagheri/autoencoders_pca)'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/reza-bagheri/autoencoders_pca](https://github.com/reza-bagheri/autoencoders_pca)'
- en: '**Appendix**'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '**附录**'
- en: '**The equations for finding the principal components:**'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '**寻找主成分的方程：**'
- en: For the second principal component, we need to find the vector ***u***₂ that
    maximizes
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二主成分，我们需要找到最大化的向量***u***₂
- en: '![](../Images/bab1a7eedb631b5c728c8f54fca3ef1c.png)'
  id: totrans-455
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bab1a7eedb631b5c728c8f54fca3ef1c.png)'
- en: 'with these constraints:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些约束下：
- en: '![](../Images/a4032df1626261724645ff5f3e98e062.png)![](../Images/d3039d4fa2822fd4d3b7b1bb0c8c3e00.png)'
  id: totrans-457
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4032df1626261724645ff5f3e98e062.png)![](../Images/d3039d4fa2822fd4d3b7b1bb0c8c3e00.png)'
- en: This means that we need to maximize
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们需要最大化
- en: '![](../Images/5ce90bfaf336958a06c89c60d7ec6975.png)'
  id: totrans-459
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ce90bfaf336958a06c89c60d7ec6975.png)'
- en: 'with respect to ***u***₂. To find the maximum point we write:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 关于***u***₂。为了找到最大点，我们写道：
- en: '![](../Images/c4069bb91b9700131e7ef10ff11f7316.png)'
  id: totrans-461
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4069bb91b9700131e7ef10ff11f7316.png)'
- en: 'By multiplying this equation by ***u***₁*ᵀ* we get:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将这个方程乘以***u***₁*ᵀ*，我们得到：
- en: '![](../Images/bf7b3ce6aa46e648b541157d5e8ae603.png)'
  id: totrans-463
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf7b3ce6aa46e648b541157d5e8ae603.png)'
- en: 'Now using Equation 7 and the fact that the covariance matrix is a symmetric
    matrix we can write:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用方程7和协方差矩阵是对称矩阵的事实，我们可以写：
- en: '![](../Images/ce7f051bcc787a15d2e924b4085e3a05.png)'
  id: totrans-465
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce7f051bcc787a15d2e924b4085e3a05.png)'
- en: 'By substituting this equation into the previous one we get:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将这个方程代入前面的方程，我们得到：
- en: '![](../Images/149daa3593107f1e2d1894bf0a6e9bc2.png)'
  id: totrans-467
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/149daa3593107f1e2d1894bf0a6e9bc2.png)'
- en: 'Now using the orthogonality constraint and the fact that ***u***₁ is normalized
    we get:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用正交性约束和***u***₁被标准化的事实，我们得到：
- en: '![](../Images/56ee3a3ac5e248a3a55b66373d5c45ad.png)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56ee3a3ac5e248a3a55b66373d5c45ad.png)'
- en: Using this equation and Equation A.1, it follows that
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个方程和方程A.1，可以得出
- en: '![](../Images/85b5ee0e10bcb9284087af49b7f8baab.png)'
  id: totrans-471
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85b5ee0e10bcb9284087af49b7f8baab.png)'
- en: So, we have
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们有
- en: '![](../Images/e587a287535f41ea34777ccdfd6430a4.png)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e587a287535f41ea34777ccdfd6430a4.png)'
- en: We also need to find the vector ***u****ᵢ* that maximizes
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要找到最大化的向量***u***ᵢ*
- en: '![](../Images/278b15c978542f6ee4700b08847045c1.png)'
  id: totrans-475
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/278b15c978542f6ee4700b08847045c1.png)'
- en: 'with these constraints:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些约束下：
- en: '![](../Images/c53a1113e808d0bd90ad22f90a4bece1.png)![](../Images/6560b5efee8d1c3c3d95b35f0eef3164.png)'
  id: totrans-477
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c53a1113e808d0bd90ad22f90a4bece1.png)![](../Images/6560b5efee8d1c3c3d95b35f0eef3164.png)'
- en: This is equivalent to maximizing
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 这等同于最大化
- en: '![](../Images/d465b68f027230d8ba3bbd270cc579d0.png)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d465b68f027230d8ba3bbd270cc579d0.png)'
- en: 'with respect to ***u****ᵢ*. To find the maximum point, we set the derivative
    of this term with respect to ***u****ᵢ* equal to zero:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 关于***u***ᵢ*。为了找到最大点，我们将这个项对***u***ᵢ*的导数设为零：
- en: '![](../Images/7b327a4913bcf8541bb87d7566c6fd09.png)'
  id: totrans-481
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b327a4913bcf8541bb87d7566c6fd09.png)'
- en: 'By multiplying this equation by ***u****ₖᵀ* (where 1≤*k*≤*i*-1)we get:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将这个方程乘以***u***ₖᵀ*（其中1≤*k*≤*i*-1），我们得到：
- en: '![](../Images/99172139f7ce3363576a68b7c3c95ce3.png)'
  id: totrans-483
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99172139f7ce3363576a68b7c3c95ce3.png)'
- en: 'We know that the previous principal components are the eigenvectors of ***S***,
    and ***S*** is a symmetric matrix. So, we have:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7766332b79d69f429dfe68da240817e9.png)'
  id: totrans-485
  prefs: []
  type: TYPE_IMG
- en: 'And using the orthogonality constraints we conclude that:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75ba0689243cb07b3282018b7cba0d33.png)'
  id: totrans-487
  prefs: []
  type: TYPE_IMG
- en: By substituting this equation into equation A.2, it follows that
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c031dc5dbae280e6fb5027aca6393b5.png)'
  id: totrans-489
  prefs: []
  type: TYPE_IMG
- en: Hence ***u****ᵢ* is an eigenvector of ***S***, and *λᵢ* is its corresponding
    eigenvalue.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: '**Proof of Equation 17**:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that we have the *m*×*n* matrix ***X*** with rank =*r* and the singular
    values of ***X*** are sorted, so we have:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a3ec7a72e61d91be3bbfd91796432947.png)'
  id: totrans-493
  prefs: []
  type: TYPE_IMG
- en: We want to show that
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/141434731cbea5b51d5c2fa4dd2e1c74.png)'
  id: totrans-495
  prefs: []
  type: TYPE_IMG
- en: where ***A*** is an *m*×*n* matrix with rank=*k,* and ***Z*** and ***W*** are
    *m*×*k* and *k*×*n* matrices. We know that *k*<*n*. In addition, in a design matrix,
    we usually have *m*>*n*. Hence *k* is less than both *m* and *n*.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that the rank of a matrix cannot exceed the number of its rows or columns.
    So, we have:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/057732fb756e5c0ea14bb9c5d4984eaa.png)'
  id: totrans-498
  prefs: []
  type: TYPE_IMG
- en: It can be also shown that
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7d8e56808a99aa467c0cd40f65493371.png)'
  id: totrans-500
  prefs: []
  type: TYPE_IMG
- en: So, it follows that
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d2fb6cefcc9abf66e11e920f1058e25a.png)'
  id: totrans-502
  prefs: []
  type: TYPE_IMG
- en: 'Hence the rank of ***ZW*** cannot exceed *k*. In addition, we will show that
    the matrix ***ZW*** that minimizes ||***X***-***ZW***||_*F* cannot have a rank
    less than *k*. Suppose that ***Z****are ***W**** are two matrices so that ***Z*******W****is
    a rank-*k* matrix that minimizes ||***X***-***ZW***||_*F* among all rank-*k* matrices.
    Then according to Equation 16:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc21d93ce1cf6104db6fb24e64a47a77.png)'
  id: totrans-504
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, let ***Z*****and ***W***** be two matrices so that ***Z********W*****
    is a rank-*m* matrix (*m*<*k*) that minimizes ||***X***-***Z********W*****||_*F*
    among all rank-*m* matrices and. We have:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09c08fc7e8fc229f0784382661d6190a.png)'
  id: totrans-506
  prefs: []
  type: TYPE_IMG
- en: It is clear that
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7515e451ad7c54685ae9724368b24839.png)'
  id: totrans-508
  prefs: []
  type: TYPE_IMG
- en: Hence
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29ab8bb917b89d0ee0a94ceb6eea4ccd.png)'
  id: totrans-510
  prefs: []
  type: TYPE_IMG
- en: which means that a rank-*k* matrix always gives a lower value for ||***X***-***Z
    W***||_*F*. Hence we conclude that the rank of ***ZW*** cannot be less than *k*.
    Hence, the optimum value of ***ZW*** that minimizes ||***X***-***Z W***||_*F*
    should be a rank-*k* matrix.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: '**The cost function of linear autoencoder**:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: We want to show that
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/63545cbdfcbc8291f5d52a47c4b7a9c3.png)'
  id: totrans-514
  prefs: []
  type: TYPE_IMG
- en: 'To prove it, we first calculate the matrix ***X***-***Z*(*W^***[2])*ᵀ*:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/45f7ee04a58b8a4302f9fb4f1b11355a.png)'
  id: totrans-516
  prefs: []
  type: TYPE_IMG
- en: 'But according to Equation 22, we have:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b765e48d83add0e0232f67d5f36f8473.png)'
  id: totrans-518
  prefs: []
  type: TYPE_IMG
- en: 'Hence for the *k*th observation, we can write:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/937757446daa5f2aca063b6141dcb6cf.png)'
  id: totrans-520
  prefs: []
  type: TYPE_IMG
- en: 'By substituting this equation into Equation A.4, we have:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9522bcc450ee41fa0bb78340d5dbbfd2.png)'
  id: totrans-522
  prefs: []
  type: TYPE_IMG
- en: 'And finally using Equation 15 we get:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c2565eae516b2383d79b259ec15d42f0.png)'
  id: totrans-524
  prefs: []
  type: TYPE_IMG
