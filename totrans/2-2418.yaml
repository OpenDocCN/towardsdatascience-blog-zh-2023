- en: Your First Step Into Computer Vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/your-first-step-into-the-field-of-computer-vision-f9928ecb313f](https://towardsdatascience.com/your-first-step-into-the-field-of-computer-vision-f9928ecb313f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Knowing various Computer Vision tasks and their formats
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://sekharm.medium.com/?source=post_page-----f9928ecb313f--------------------------------)[![Sekhar
    M](../Images/7d3b78f65e579c27d68571d07fee8fa3.png)](https://sekharm.medium.com/?source=post_page-----f9928ecb313f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f9928ecb313f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f9928ecb313f--------------------------------)
    [Sekhar M](https://sekharm.medium.com/?source=post_page-----f9928ecb313f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f9928ecb313f--------------------------------)
    ·8 min read·Jan 30, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/65fda783746f38982c08d7e8a84b8b0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image Source: Wikimedia (Creative Commons)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Introduction:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Computer Vision(CV) is the field that makes computers extract information from
    images like how the Human Visual System can do.
  prefs: []
  type: TYPE_NORMAL
- en: There are various Computer Vision tasks, all of which typically take an image
    of dimension *W*x*H*x*3* as input and produce the output according to the task
    at hand. While *W* and *H* represent the image’s resolution (Width and Height),
    *3* is the number of channels (*R*, *G*, and *B*).
  prefs: []
  type: TYPE_NORMAL
- en: “The first step to solving a problem is to define it well.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This article covers various Computer Vision tasks and their formats, defined
    mathematically, highlighting what the corresponding inputs and outputs for each
    task are.
  prefs: []
  type: TYPE_NORMAL
- en: 'Various Computer Vision tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Image Classification](#be14)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '- Binary Classification'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Multi-class Classification'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Multi-label Classification'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Classification with Localization](#3ea9)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Object Detection](#f87c)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Image Segmentation](#a794)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '- Semantic Segmentation'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Instance Segmentation'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Panoptic Segmentation'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1\. Image Classification:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image Classification is the task of classifying or categorizing a given image
    into pre-defined classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Binary Classification:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Binary classification classifies a given image into either of the two pre-defined
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Input:** X — Dimension: *W* x *H* x *3*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output:** Y — Dimension: *1*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the ground truth value of an output Y will be either 1 or 0 (for positive
    and negative examples, respectively).
  prefs: []
  type: TYPE_NORMAL
- en: However, a binary classification model’s output will typically be a number between
    0 and 1, which will be treated as a probability of the input image belonging to
    the positive class.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/54bdc50168da698db03994ddbf227926.png)'
  prefs: []
  type: TYPE_IMG
- en: Binary Classification Model (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: The below example classifies an input image into either ‘Cat’ or ‘Not a Cat.’
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/017e70a08da9a86099a73b68f9459e53.png)'
  prefs: []
  type: TYPE_IMG
- en: Binary Classification Example (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-class Classification:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multi-class classification classifies a given image into one among the many(*n*)
    pre-defined classes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Input:** X — Dimension: *W* x *H* x *3*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output:** Y — Dimension: *n* x *1*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the ground truth value of an output Y will be a one-hot encoded vector
    (position of 1 indicating the class assigned to the input image).
  prefs: []
  type: TYPE_NORMAL
- en: However, a multi-class classification model’s output will typically be a vector
    of numbers between 0 and 1, all of which add up to 1\. These will be treated as
    probabilities of the input image belonging to the respective classes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/19306c367baaba9a35646d7c4cf60ec2.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi-class Classification Model (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: The below example classifies an input image into one among the n pre-defined
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/32718ed60e745c46a975abec7b49a4c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi-class Classification Example (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-label Classification:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multi-label classification is similar to multi-class classification except that
    the input image can be assigned with more than one pre-defined class among *n*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Input:** X — Dimension: *W* x *H* x *3*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output:** Y — Dimension: *n* x *1*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the ground truth value of an output Y can have 1 in more than one position.
    So, it can be a binary vector instead of a one-hot vector.
  prefs: []
  type: TYPE_NORMAL
- en: However, a multi-label classification model’s output will typically be a vector
    of numbers between 0 and 1\. Note that these numbers need not add up to 1 since
    an input image can belong to more than one pre-defined class at a time. Each number
    is treated as the probability of the input image belonging to the corresponding
    class.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/12ebb2d15a743a32afb89bb84c53e528.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi-label Classification Model (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: The below example assigns multiple classes to a given input image as applicable.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/785e6265e4dbdaa94fe7299fd62ddbde.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi-label Classification Example (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Classification With Localization:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification with localization typically means a multi-class classification
    along with localizing the portion of the input image that determined its classification
    output.
  prefs: []
  type: TYPE_NORMAL
- en: '**Input:** X — Dimension: *W* x *H* x *3*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output:** Y — Dimension: *(1+4+n)* x *1*'
  prefs: []
  type: TYPE_NORMAL
- en: In the ground truth value of Y, `*Pc*`is 1 if anyone among *n* classes is present
    in the input image and is 0 otherwise. `*bx*`, `*by*`, `*bw*`, and `*bh*` represent
    the bounding box around the portion that determined the classification output.
    And `*C1*` to `*Cn*` is the one-hot vector representing the classification.
  prefs: []
  type: TYPE_NORMAL
- en: Note that when none of the *n* classes is present in the input image, `*Pc*`
    is 0, and the rest of the values in the output are immaterial.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/915bdb5b158501fbfb69a9233edf4dc0.png)'
  prefs: []
  type: TYPE_IMG
- en: Classification with Localization Model (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: The below example classifies the input image as a Horse along with outputting
    the localization information.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8318b2d9fcd67ff6d8732de8b233ad0b.png)'
  prefs: []
  type: TYPE_IMG
- en: Classification with Localization Example (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: While `x`, `y`, `w`, and `h` are the absolute values in the image, `*bx*`, `*bw*`
    are expressed as a fractional value in `W`, and `*bx*`,`*bh*` in `H`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cc85e36ffab54d879647a4e04449fc0c.png)'
  prefs: []
  type: TYPE_IMG
- en: Input Image (left) and Localization information superimposed on Input Image
    (right). (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Note that the origin (0, 0) is always considered to be the top left corner of
    the image. And `x` is along the width (from left to right) of the image and `y`
    along the height (from top to bottom).
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Object Detection:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Object detection is typically the localization applied to multi-label classification.
  prefs: []
  type: TYPE_NORMAL
- en: YOLO is one example of an object detection model. It divides the input image
    into a `gxg` grid of cells and produces one classification-localization vector
    as output for each cell. YOLO Algorithm typically has a grid of size 19x19.
  prefs: []
  type: TYPE_NORMAL
- en: '**Input:** X — Dimension: *W* x *H* x *3*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output:** Y — Dimension: *g* x *g* x (*1*+*4*+*n)*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8cbedb7a31a424d8c4f753272aca4ef5.png)'
  prefs: []
  type: TYPE_IMG
- en: Object Detection Model (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: In output Y, for each of the `gxg` cells, the third dimension is the vector
    `[*Pc*, *bx*, *by*, *bw*, *bh*, *C1*,...,*Cn*]`. Here, `*bx*`, `*bw*` and `*bx*`,`*bh*`are
    expressed as a fractional value in `Width` and `Height` of the corresponding cell
    respectively (rather than of the Image).
  prefs: []
  type: TYPE_NORMAL
- en: Here, the values of `*bw*` and `*bh*` can exceed 1 since the size of the object
    can be more than that of the cell. However, the values of `*bx*` and `*by*` still
    remain between 0 and 1 since the centroid of an object can belong to any one cell
    alone.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d7330275e1f7df78d15caddeeb7f20a2.png)'
  prefs: []
  type: TYPE_IMG
- en: Input Image (left), 4x4 grid of cells (middle), Output superimposed on cells
    5 and 7 (right) (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The ground truth values of the output Y for the above image example with a
    *4*x*4* grid are as below:'
  prefs: []
  type: TYPE_NORMAL
- en: For cell positions 5 and 7, the classification-localization vectors are given
    by `[1, 0.90, 0.60, 1.30, 0.95, 1,...,0]` and `[1, 0.55, 0.80, 1.90, 1.20, 1,...,0]`
    respectively. (Assuming Car to be the label 1 among *n* labels)
  prefs: []
  type: TYPE_NORMAL
- en: And for all the other cell positions except 5 and 7, the Classification-Localization
    vector is given by `[0, ?, ?, ?, ?, ?,...,?]`
  prefs: []
  type: TYPE_NORMAL
- en: When `*Pc*` is 0, the rest of the values in the vector are immaterial. And ‘?’
    here indicates a *don’t care* condition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: In this algorithm, there can only be one object identified in a cell.
    However, there are other techniques, using anchor boxes, to be able to identify
    multiple objects of different sizes within a single cell.'
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Image Segmentation:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image segmentation is the task of partitioning an image into multiple image
    regions called segments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Semantic Segmentation:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Semantic segmentation is the task of multi-class classification applied at a
    pixel level, i.e., each pixel of the input image is classified into one of the
    *n* pre-defined classes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Input:** X — Dimension: *W* x *H* x *3*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output:** Y — Dimension: *W* x *H* x *n*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7cedb2dd371828073beea6e335df52ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Semantic Segmentation Model (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Each *1*x*n* dimensional vector, corresponding to each pixel position in the
    output, represents the label classification of that particular pixel in the input
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic segmentation does not distinguish between two instances of the same
    class present in the image. It only mentions the category of each pixel. For example,
    pixels from all the Horses present in an image are segmented into the same category
    — Horse pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instance Segmentation:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instance segmentation is object detection followed by per-pixel segmentation
    within the *Region of Interest(RoI)* of each detected object.
  prefs: []
  type: TYPE_NORMAL
- en: So, along with the classification label and bounding box, it also outputs a
    binary segmentation mask (of size equal to the corresponding ROI) for each detected
    object.
  prefs: []
  type: TYPE_NORMAL
- en: '**Input:** X — Dimension: *W* x *H* x *3*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output:** Y — Dimension: *N* x (*Label* + *Bounding Box + Segmentation Mask)*'
  prefs: []
  type: TYPE_NORMAL
- en: Where *N* is the number of objects detected, the label vector is a vector of
    dimension equal to the number of predetermined classes, the bounding box is represented
    by four values `[*bx,by,bw,bh*]` , and a segmentation mask is a binary mask of
    the size equal to the RoI of the corresponding object.
  prefs: []
  type: TYPE_NORMAL
- en: This binary segmentation mask highlights the pixels belonging to the object
    from the rest within the bounding box.
  prefs: []
  type: TYPE_NORMAL
- en: As the name suggests, Instance Segmentation distinguishes even between multiple
    instances of the same object class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Panoptic Segmentation:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Panoptic segmentation combines semantic segmentation and instance segmentation.
    For every pixel*,* it assigns a *Semantic Class Label*(*l*)and an *Instance ID*(*z*)*.*
  prefs: []
  type: TYPE_NORMAL
- en: '**Input:** X — Dimension: *W* x *H* x *3*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output:** Y — Dimension: *W* x *H* x one (*l, z)* pair per pixel'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a63a87aefcded72d8b8adf9a25e46f0b.png)'
  prefs: []
  type: TYPE_IMG
- en: Panoptic segmentation treats the contents of an image as two types — *Things*
    and *Stuff.*
  prefs: []
  type: TYPE_NORMAL
- en: A ‘Thing’ is any countable object, such as Car, Horse, and Person. And Stuff
    is an uncountable amorphous region of identical texture, such as Sky, Road, Water,
    and Ground.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/18b0883b4ad5a8b80e5e35424f517ad0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Semantic vs Instance vs Panoptic:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/f6db37d65be02a725272dd698e4c999a.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) Input Image, (b) Semantic, (c) Instance, (d) Panoptic (Source: [1])'
  prefs: []
  type: TYPE_NORMAL
- en: Note that, in semantic segmentation, there is no distinction between multiple
    instances of the same category, i.e., all Cars together are shown in blue and
    Persons in red, for example.
  prefs: []
  type: TYPE_NORMAL
- en: In instance segmentation, since the initial object detection step detects all
    the object instances present in the image, each ‘Thing’ — Car and Person — is
    segmented individually while all the ‘Stuff’ is ignored.
  prefs: []
  type: TYPE_NORMAL
- en: And in panoptic segmentation, there is a semantic segmentation of both ‘Things’
    as well as ‘Stuff’ while each instance of a ‘Thing’ is also segmented individually.
  prefs: []
  type: TYPE_NORMAL
- en: And in panoptic segmentation, there is a semantic segmentation of both ‘Things’
    as well as ‘Stuff’ while each instance of a ‘Thing’ is also segmented individually.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conclusion:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The field of Computer Vision has several applications ranging from Autonomous
    Vehicle Driving to Medical Image Analysis, which employ these various Computer
    Vision tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Since the first step to solving a problem is to define it well, I hope this
    article — demystifying various Computer Vision tasks — will be of good use to
    anyone breaking into the field of Computer Vision.
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Kirillov et al., Panoptic Segmentation, 2019.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] He et al., Mask R-CNN, 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Redmon et al., You Only Look Once, 2015.'
  prefs: []
  type: TYPE_NORMAL
