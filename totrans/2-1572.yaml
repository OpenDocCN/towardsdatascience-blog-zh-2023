- en: Nearest Neighbors Regressors — A Visual Guide
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/nearest-neighbors-regressors-a-visual-guide-78595b78072e](https://towardsdatascience.com/nearest-neighbors-regressors-a-visual-guide-78595b78072e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/bc24d842e277ced70e9a10b211985b90.png)'
  prefs: []
  type: TYPE_IMG
- en: Visual Understanding of the Models and the Impacts of Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@angela.shi?source=post_page-----78595b78072e--------------------------------)[![Angela
    and Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----78595b78072e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----78595b78072e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----78595b78072e--------------------------------)
    [Angela and Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----78595b78072e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----78595b78072e--------------------------------)
    ·8 min read·Mar 31, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: K Nearest Neighbors or KNN is one of the most simple models in machine learning.
    In fact, to some extent, there is no model, because, for the prediction of a new
    observation, it will use the entirety of the training dataset to find the “nearest
    neighbors” according to a distance (usually the euclidean distance). And then
    in the case of a regression task, the prediction value is calculated by averaging
    the target variable values of these neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: Since we use the notion of distance, only numerical features should be used.
    Well, you can always transform categorical using one-hot encoding or label encoding,
    the algorithm of distance calculation will work, but the distance is kind of meaningless.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth also noting that the value of the target variable is not used to
    find the neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will use some simple datasets to visualize how KNN Regressor
    works and how the hyperparameter k will impact the predictions. We also will discuss
    the impact of feature scaling. We will also explore a less known version of nearest
    neighbors which is radius nearest neighbors. In the end, we will finish with a
    discussion about the more customized versions of distance.
  prefs: []
  type: TYPE_NORMAL
- en: One continuous feature
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use a simple dataset with non-linear behavior since we know that KNN
    can handle it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eec6d8462f21b1ba92347d0846497300.png)'
  prefs: []
  type: TYPE_IMG
- en: Nearest Neighbors Regressors dataset — image by author
  prefs: []
  type: TYPE_NORMAL
- en: For those who read my article about the [Decision Tree Regressor](/decision-tree-regressor-a-visual-guide-with-scikit-learn-2aa9e01f5d7f)
    visualization, you can notice that it is the same data. We will do a quick comparison
    with the Decision Tree Regressor model.
  prefs: []
  type: TYPE_NORMAL
- en: We can create and fit a KNeighborsRegressor model with KNeighborsRegressor(n_neighbors
    = 3), then we “fit” the model with model.fit(X, y).
  prefs: []
  type: TYPE_NORMAL
- en: In order to make the process of fitting identical to all models, you can notice
    that the model is “fitted” with the classic fit method. But for KNeighborsRegressor,
    the fitting process is just saving the dataset X and y, and nothing else. Yes,
    that is the most rapid fitting ever! and the model is also the biggest ever!
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can test the “model” for one observation. In the following code, we
    will use one single point. The classic predict is to calculate the prediction,
    and the kneighbors method allows us to get the neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: Then we can plot the neighbors. I will show you the plots for x = 10 and x =
    20\. Please feel free to do more tests.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/50880de338c86b1a17dda9bb7b13881f.png)'
  prefs: []
  type: TYPE_IMG
- en: Nearest Neighbors Regressors with kneighbors— image by author
  prefs: []
  type: TYPE_NORMAL
- en: Now, we also can use a sequence of x values to get all the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the resulting plot from the previous code. For each point on the red
    segment, the y value represents the average value of the k nearest neighbors (here
    k = 3)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3206084794cb1460ccd1e9c9807f1bc4.png)'
  prefs: []
  type: TYPE_IMG
- en: Nearest Neighbors Regressors with predictions — image by author
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now create the model predictions for different values of k.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/849798f6874f8a94ecdf549bbafcde0f.png)'
  prefs: []
  type: TYPE_IMG
- en: Nearest Neighbors Regressors with different values of k — image by author
  prefs: []
  type: TYPE_NORMAL
- en: We can also compare with the [Decision Tree Regressor model](/decision-tree-regressor-a-visual-guide-with-scikit-learn-2aa9e01f5d7f)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d468b812a3f01418553983b8b32b4f40.png)'
  prefs: []
  type: TYPE_IMG
- en: Nearest Neighbors Regressors vs. Decision Tree Regressors — image by author
  prefs: []
  type: TYPE_NORMAL
- en: We can notice that the frontier is always clean-cut for decision tree regressors
    whereas it is more nuanced for k nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: Two continuous features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use the following dataset, with two continuous features, to create a
    KNN model. For the testing dataset, we will use meshgrid to generate a grid.
  prefs: []
  type: TYPE_NORMAL
- en: Then we can use plotly to create interactive 3D plots. In the image below, we
    can see the 3D plot with different values of k.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/68a995c02caa80cbfa149d6b7938d506.png)'
  prefs: []
  type: TYPE_IMG
- en: Nearest Neighbors Regressors with two features — image by author
  prefs: []
  type: TYPE_NORMAL
- en: Here again, we can compare them with Decision Tree Regressor model. We can SEE
    and FEEL the difference in the behavior of these two models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eac166e59bebc76c1a5409fb6ba41be4.png)'
  prefs: []
  type: TYPE_IMG
- en: Nearest Neighbors Regressors vs. Decision Tree Regressors — image by author
  prefs: []
  type: TYPE_NORMAL
- en: Impact of scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Contrary to Decision Trees, the scaling of the features has a direct impact
    on the model.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can do the following transformation for the two-feature case.
    It is worth noting that for one continuous feature, the scaling has no impact
    since the relative distance is not changed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c11172937cff92f876873a5f3762b18.png)'
  prefs: []
  type: TYPE_IMG
- en: Nearest Neighbors Regressors with different feature scales — image by author
  prefs: []
  type: TYPE_NORMAL
- en: We can visually conclude that the two series of models are very different. We
    can calculate the usual model performance to compare them. But here my approach
    is really to demonstrate visually how the model behaves differently. Can you feel
    it? The distances are changed, because the scales of the features are changed.
    And in the end, the neighbors change.
  prefs: []
  type: TYPE_NORMAL
- en: Some may say that we should use standardization or min-max scaling. But you
    can see that in the image above one case or another could have been a dataset
    with standardization (or min-max scaling). And you can not say in advance if the
    standardization helps the model to have a better performance.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, in order to take into account the relative importance of each feature
    in the distance calculation, we should give different weights for different features.
    But this would make the tuning process too complex.
  prefs: []
  type: TYPE_NORMAL
- en: Just imagine that in a linear regression model, the key is to find the coefficients
    for each feature. And in the distance calculation of k NN, all features are considered
    with the same importance. Intuitively, we can FEEL that this kNN model can not
    be that performant!
  prefs: []
  type: TYPE_NORMAL
- en: Radius Neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the scikit-learn `[neighbors](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors)`
    module, there is a lesser-known model called RadiusNeighborsRegressor and you
    can easily understand from its name that instead of taking the fixed number of
    neighbors (in the case of k nearest neighbors), we use a circle of a fixed radius
    around the new observation to find its neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: Now, when Radius Neighbors model could be more interesting? Let’s take an example
    of a dataset with an outlier. We can see how the two models behave differently.
    Since this outlier is “far” away, in the case of knn, the number of neighbors
    is fixed, so the neighbors are also far away from each other. But for radius neighbors,
    the effect of the outlier is more important.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26fc0073dc957a63b97111f4bbeae155.png)'
  prefs: []
  type: TYPE_IMG
- en: Nearest Neighbors Regressors KNN vs. Radius NN — image by author
  prefs: []
  type: TYPE_NORMAL
- en: Here please note that when I use the term “outlier”, it does not necessarily
    mean that we should get rid of this outlier. I just want to show the case of some
    data points are far away from others.
  prefs: []
  type: TYPE_NORMAL
- en: The impact is then also significant for non-outliers. Because we can not satisfy
    the two cases.
  prefs: []
  type: TYPE_NORMAL
- en: You may notice something strange in the image above when the radius is too small.
    Yes, in this case, for some points, there are no neighbors. Then, a huge negative
    number is assigned. The truth is, in this case, there is no solution. But I still
    think that it is an error that should be corrected in scikit-learn. Yielding an
    error is better than giving this value by default.
  prefs: []
  type: TYPE_NORMAL
- en: Before finishing with Radius Neighbors, when can it be really interesting? Imagine
    a case when you have lots of data in the area in a city, and in another area nearby,
    you don’t have much data, but you know that you could have collected more. Then
    radius neighbors can be more relevant.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if you have a value for one area (that will be assigned to all addresses
    of this area), then we can use the neighbors to smooth the value. Here again,
    the radius neighbors would be more relevant. Here below you have an illustration
    of the value smoothing using a nearest neighbors model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9fcb829d4253cf3374f3fe6fe7f49cae.png)'
  prefs: []
  type: TYPE_IMG
- en: Geographical Neighbors Smoothing — image by author
  prefs: []
  type: TYPE_NORMAL
- en: More about the Distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is one last interesting discussion around the notion of distance, you
    may already think about it when visualizing the previous cartography. The notion
    of distance can be very specific, because if you calculate the euclidean distance
    with longitude and latitude, then this distance may not correctly reflect the
    geographic neighborhood (which is the distance you may desire to use).
  prefs: []
  type: TYPE_NORMAL
- en: You may surely already know, but SEEING is always better. From the image below
    the red circle is the “true circle” for the central location, which is the red
    formed by the locations with an equal geographical distance from the central location.
    The blue “circle” is obtained by calculating a Euclidean distance of latitude
    and longitude. Around the equator area, the two circles are almost the same. But
    they are rather different when for locations far from the equator area. So next
    time you have latitude and longitude in your dataset, and you use Nearest Neighbors
    models, you have to think about it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eeba56d398e14d87d2b819dc20d564b2.png)'
  prefs: []
  type: TYPE_IMG
- en: True circle on Earth vs. “Circle” with lat lon — image by author
  prefs: []
  type: TYPE_NORMAL
- en: Now, you can imagine that in other cases, a more customized distance may be
    necessary. So this simple model can become more performant.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to weigh the neighbors. You can do it with the **weights**
    argument. And here is the description of this argument from the official documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**weights :** *{‘uniform’, ‘distance’}, callable or None, default=’uniform’*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Weight function used in prediction. Possible values:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- ‘uniform’ : uniform weights. All points in each neighborhood are weighted
    equally.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- ‘distance’ : weight points by the inverse of their distance. in this case,
    closer neighbors of a query point will have a greater influence than neighbors
    which are further away.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- [callable] : a user-defined function which accepts an array of distances,
    and returns an array of the same shape containing the weights.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Uniform weights are used by default.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/a9a91ce787c593f9eae8b33ed3f6d0e0.png)'
  prefs: []
  type: TYPE_IMG
- en: K Nearest Neighbors Regressors with weights = “distance” — image by author
  prefs: []
  type: TYPE_NORMAL
- en: However, the distance design can become so complex that it is rather easier
    to adopt another approach such as Decision Trees and Mathematical function-based
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I am writing a series of similar articles to demonstrate how visualization
    helps us to gain a better understanding of how machine learning models work without
    maths. Please follow me with the link below and get full access to my articles:
    [https://medium.com/@angela.shi/membership](https://medium.com/@angela.shi/membership)'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to get the code that produced the graphics in this article, you
    can support me here: [https://ko-fi.com/s/4cc6555852](https://ko-fi.com/s/4cc6555852)'
  prefs: []
  type: TYPE_NORMAL
- en: So in this article, we demonstrated that Nearest Neighbors “models” are quite
    intuitive to understand with visualization for simple datasets, because the notion
    of neighbors is intuitive and straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of the hyperparameter K or the radius has a significant impact on
    the performance of the Nearest neighbors models. If K (or radius) is too small,
    the model may overfit the noise in the data, while if K (or radius) is too large,
    the model may underfit and fail to capture the underlying patterns in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Changing the scale of the data is also important when using Nearest Neighbors
    models, as the algorithm is sensitive to the scale of the input features.
  prefs: []
  type: TYPE_NORMAL
