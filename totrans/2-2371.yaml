- en: Why I Signed the “Pause Giant AI Experiments” Petition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/why-i-signed-the-pause-giant-ai-experiments-petition-e9711f672d18](https://towardsdatascience.com/why-i-signed-the-pause-giant-ai-experiments-petition-e9711f672d18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://rafebrena.medium.com/?source=post_page-----e9711f672d18--------------------------------)[![Rafe
    Brena, Ph.D.](../Images/6bf622a8ce9b3d06d1cb989fd8d625c6.png)](https://rafebrena.medium.com/?source=post_page-----e9711f672d18--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e9711f672d18--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e9711f672d18--------------------------------)
    [Rafe Brena, Ph.D.](https://rafebrena.medium.com/?source=post_page-----e9711f672d18--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e9711f672d18--------------------------------)
    ·8 min read·Apr 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: The “spirit” is right; the body has many flaws
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/39703612df5c867141587e1adc2f3c36.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Álvaro Serrano](https://unsplash.com/@alvaroserrano?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Last Tuesday, I received from the Future of Life Institute an email asking me
    to sign a [petition to pause giant AI experiments](https://futureoflife.org/open-letter/pause-giant-ai-experiments/?utm_source=pocket_saves).
    When I signed the letter, the organizers asked us to keep it confidential until
    its publication moment arrived. At that time, I didn’t expect it to raise so much
    news, comments, articles, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Shortly after its publication, I was contacted by a couple of news outlets,
    one from Argentina and the other one from Mexico, to participate in their live
    programs and give my opinion there.
  prefs: []
  type: TYPE_NORMAL
- en: It was then that I realized the FLI’s letter was indeed a high-impact initiative.
  prefs: []
  type: TYPE_NORMAL
- en: Though in the end, I decided to sign it, I also found many statements in the
    letter I disagree with, so in this post, I want to make the record straight and
    give the reasons for and against the letter. I encourage you to read the letter
    itself as well; it’s not that long.
  prefs: []
  type: TYPE_NORMAL
- en: Why now?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s important to be aware that the sense of urgency in the open letter is not
    about Artificial Intelligence in general; it’s about the recent development and
    release of what’s been called “Generative AI” or GenAI for short.
  prefs: []
  type: TYPE_NORMAL
- en: Unless you’ve been hiding under a rock, you’ve heard about ChatGPT (released
    last November, gosh, it seems so far in the past), which is the most prominent
    example of GenAI, but there are many other ones, like [DALL-E](https://www.vox.com/future-perfect/23023538/ai-dalle-2-openai-bias-gpt-3-incentives),
    Claude, [Stable Diffusion](https://www.vox.com/recode/2023/1/5/23539055/generative-ai-chatgpt-stable-diffusion-lensa-dall-e),
    Poe, [You.com](http://You.com), [Copy.ai](http://Copy.ai), and more. AI capabilities
    are being incorporated as well into many products, like Notion, Microsoft Office,
    Google Workplace suite, GitHub, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Many of us [have recognized GenAI as a real game changer](https://medium.com/towards-data-science/how-generative-ia-will-disrupt-everything-in-the-current-decade-b4e8ce7dd4f1),
    as opposed to others who called it “a fad.” Bill Gates [writes](https://www.gatesnotes.com/The-Age-of-AI-Has-Begun)
    that he’s seen twice in his already long life transformational technologies and
    that GenAI is his second time (the first was when he saw a graphical user interface).
  prefs: []
  type: TYPE_NORMAL
- en: But it hasn’t been a smooth road.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from the notorious cases of “[evil personalities](https://medium.com/geekculture/chatgpts-doppelgänger-12d236743a92)”
    hijacking the chatbots, we have seen a lot of factual errors or even invented
    facts –called “hallucinations”– which are misleading to humans because the text
    looks as if it was written with the utmost assurance; we humans tend to show insecurity
    when we aren’t certain of what we are saying, but of course, machines don’t feel
    insecurities (nor assurance, actually).
  prefs: []
  type: TYPE_NORMAL
- en: Companies like OpenAI try to give the impression that mistakes are being ironed
    out, but some experts believe that mistakes and hallucinations are intrinsic to
    the technology and not minor details. I [proposed a way to minimize mistakes](https://medium.com/towards-artificial-intelligence/this-is-how-to-stop-chatgpt-bing-poe-and-you-from-hallucinating-42e8e80c2ef7)
    without pretending to eliminate them altogether.
  prefs: []
  type: TYPE_NORMAL
- en: While deficiencies are far from being corrected, the race between competing
    companies, in particular, OpenAI (with Microsoft behind) and Google (with its
    associates DeepMind, and Anthropic), is at full speed. Products are being released
    at a neck-breaking pace, just for the sake of a market share advantage, without
    really worrying about consequences in society.
  prefs: []
  type: TYPE_NORMAL
- en: We –the citizens– are left on our own to deal with the introduction of GenAI
    in our lives, with all the possibilities of misinformation, biases, fake news,
    fake audio, or even fake videos.
  prefs: []
  type: TYPE_NORMAL
- en: Governments do nothing about it. International organizations do nothing about
    it.
  prefs: []
  type: TYPE_NORMAL
- en: I understand that a text of image generation doesn’t look as critical as medical
    diagnosis or medication, but there are important consequences nonetheless. We
    had a first taste of how misinformation (leveraged by tech platforms like Twitter)
    played a role in the US 2016 and 2020 elections, and now we are suffering from
    polarization in society all around the world. But Twitter bots of some years ago
    are nothing compared to what is about to come with GenAI if we do nothing about
    its adoption.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now review what the letter gets right and later on what, in my opinion,
    it gets wrong.
  prefs: []
  type: TYPE_NORMAL
- en: What the open letter gets right
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GenAI systems are “*powerful digital minds that no one — not even their creators
    — can understand, predict, or reliably control*.” They are “*unpredictable black-box
    models with emergent capabilities*.” This explains why they are intrinsically
    dangerous systems. For instance, “*emergent capabilities*” means that when GenAI
    systems get large enough, new behaviors appear out of thin air –like hallucinations.
    Emergent behaviors are not engineered or programmed; they simply appear.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: “*AI labs [are] locked in an out-of-control race to develop and deploy ever
    more powerful digital minds*.” This non-stop race can be understood in terms of
    market share domination for the companies, but what about societal consequences?
    They say they care about it, but the relentless pace points otherwise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead of letting this reckless race continue, we should “*develop and implement
    a set of shared safety protocols for advanced AI design and development that are
    rigorously audited and overseen by independent outside experts*.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Another good point is not trying to stop AI research or innovation altogether:
    “*This does not mean a pause on AI development in general, merely a stepping back
    from the dangerous race to ever-larger unpredictable black-box models with emergent
    capabilities*.” Further, a reorientation of tech efforts is proposed: “*AI research
    and development should be refocused on making today’s powerful, state-of-the-art
    systems more accurate, safe, interpretable, transparent, robust, aligned, trustworthy,
    and loyal*.”'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, an emphasis on policymaking is proposed as the way to go: “*AI developers
    must work with policymakers to dramatically accelerate development of robust AI
    governance systems. These should, at a minimum, include: new and capable regulatory
    authorities dedicated to AI; oversight and tracking of highly capable AI systems
    and large pools of computational capability; provenance and watermarking systems
    to help distinguish real from synthetic and to track model leaks; a robust auditing
    and certification ecosystem; liability for AI-caused harm; robust public funding
    for technical AI safety research; and well-resourced institutions for coping with
    the dramatic economic and political disruptions (especially to democracy) that
    AI will cause*.”'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What the letter gets wrong
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most of what I think the letter doesn’t get right is at the beginning; later
    on, things improve a lot. I have the clear impression that the first and last
    parts of the letter were written by different people (I don’t suspect either to
    have been written by a bot). Let’s jump to the specifics:'
  prefs: []
  type: TYPE_NORMAL
- en: References are not authoritative enough. Oral declarations are not objective
    evidence. Even the [Bubeck et al. reference](https://arxiv.org/abs/2303.12712)
    is not really a scientific paper because it wasn’t even reviewed! You know, papers
    published in prestigious journals go through a review process with several anonymous
    reviewers. I review myself more than a dozen papers each year. If the Bubeck paper
    were sent to a reviewed journal, for sure, it wouldn’t be accepted as it is because
    it uses subjective language (what about “*Sparks of Artificial General Intelligence*”?).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Some claims in the letter are plain ridiculous: it starts with “*AI systems
    with human-competitive intelligence*…”, but as I explained in a [previous post](https://medium.com/@rafebrena/the-human-level-performance-of-gpt-4-5b840828f8d6),
    AI current systems are not at all human-competitive, and most human vs. GenAI
    comparisons are misleading. The [reference supporting machine competitiveness](https://arxiv.org/abs/2303.12712)
    is bogus, as I explained in the previous point.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The letter implies claims of Artificial General Intelligence (AGI), as in “*Contemporary
    AI systems are now becoming human-competitive at general tasks*,” but I’m in the
    camp of those who place AGI as a very distant future and don’t even see GPT-4
    as a substantial step to it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The dangers for the jobs market are not well put: “*Should we automate away
    all the jobs, including the fulfilling ones*?” Come on; AI is not coming for most
    of the jobs, but the way it’s taking some of them (like the graphic design capabilities
    made from scrapping thousands of images without giving any monetary compensation
    to their human authors) could be taken care of, not by a moratorium, but by imposing
    taxes to big tech and giving support to graphic designer communities.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sorry, but almost every single question the letter asks is ill-written: “*Should
    we develop nonhuman minds that might eventually outnumber, outsmart, obsolete
    and replace us*?” This is a “*human vs. machines*” scenario, which is not only
    ridiculous but also fuels the wrong hype about AI systems, as [Arvind Narayanan](https://twitter.com/random_walker/status/1641079347531448320)
    (@random_walker) points out on Twitter. Terminator-like scenarios are not the
    real danger here.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Just to conclude with nonsense questions in the letter, let’s check this one:
    “*Should we risk loss of control of our civilization*?” This is wrong at so many
    levels that it’s hard to comment on. For starters, do we currently have control
    of our civilization? Please tell me who has control of our civilization besides
    the rich people and the heads of state. Then, who is “*we*”? The humans? If this
    is the case, we are back to the human vs. machine mindset, which is basically
    wrong. The real danger is the use of AI tools by *some humans* to dominate *other
    humans*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The “remedy” proposed (the “pause” on the development of Large Language Models
    more capable than GPT-4) is both unrealistic and misplaced. It’s unrealistic because
    it’s addressed to AI labs, which are mostly under the control of big tech companies
    with specific financial interests –one of which is to *increase their market share*.
    What do you think they’ll do, what the FoL Institute proposes, or what their bosses
    want? You’re right. It’s also misplaced because the pause wouldn’t take care of
    the looting already taking place from human authors or the damage already being
    done with misinformation from human actors with tools that don’t need to be more
    powerful than GPT-4.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, some people signing the letter, and in particular Elon Musk, cannot
    be seen as an example of what would be AI ethical behavior: Musk has misled Tesla
    customers by naming “*Full Self-Driving*” the Tesla capabilities that not only
    fail to comply with Level 5 of the standard proposed by the Society of Automotive
    Engineers, but also fail to comply with level 4, and barely could fit into level
    3\. Not only that, but also Tesla has released to the public potentially deadly
    machines much before ensuring their safety, and Tesla cars in autonomous mode
    [have actually killed people](https://impakter.com/tesla-autopilot-crashes-with-at-least-a-dozen-dead-whos-fault-man-or-machine/#:~:text=After%20a%20Tesla%20car%20on,attempts%20to%20shift%20the%20blame).
    What is the moral authority of Elon Musk to ask for “*safe, interpretable, transparent,
    robust, aligned, trustworthy, and loyal*” AI systems that he hasn’t put into practice
    in his own company?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then again, why I signed at all?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After all the letter gets wrong, why I decided to sign?
  prefs: []
  type: TYPE_NORMAL
- en: 'I’m not alone in signing the letter and criticizing it as well. There is, for
    instance, @GaryMarcus, who said, as [reported by the NYT](https://www.nytimes.com/2023/03/29/technology/ai-artificial-intelligence-musk-risks.html):'
  prefs: []
  type: TYPE_NORMAL
- en: “The letter is not perfect, but the spirit is exactly right.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is a way to say that something needs to be done, and the letter can be
    seen as a first attempt at doing it. This is something I can agree on.
  prefs: []
  type: TYPE_NORMAL
- en: 'But if you want a more lucid take on the subject, read, for example, the [Yuval
    Harari op-ed in the NYT](https://www.nytimes.com/2023/03/24/opinion/yuval-harari-ai-chatgpt.html?utm_source=pocket_reader).
    Apart from some over-ambitious phrases as “In the beginning was the word,” I liked
    his critique of Terminator-like scenarios and his take on the real dangers:'
  prefs: []
  type: TYPE_NORMAL
- en: … Simply by gaining mastery of language, A.I. would have all it needs to contain
    us in a Matrix-like world of illusions, without shooting anyone or implanting
    any chips in our brains. If any shooting is necessary, A.I. could make humans
    pull the trigger, just by telling us the right story.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
