- en: 'End-to-End ML Pipelines with MLflow: Tracking, Projects & Serving'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/end-to-end-ml-pipelines-with-mlflow-tracking-projects-serving-1b491bcdc25f](https://towardsdatascience.com/end-to-end-ml-pipelines-with-mlflow-tracking-projects-serving-1b491bcdc25f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Definitive tutorial for advanced use of MLflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@antonsruberts?source=post_page-----1b491bcdc25f--------------------------------)[![Antons
    Tocilins-Ruberts](../Images/363a4f32aa793cca7a67dea68e76e3cf.png)](https://medium.com/@antonsruberts?source=post_page-----1b491bcdc25f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1b491bcdc25f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1b491bcdc25f--------------------------------)
    [Antons Tocilins-Ruberts](https://medium.com/@antonsruberts?source=post_page-----1b491bcdc25f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1b491bcdc25f--------------------------------)
    ·9 min read·Feb 16, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/537bead127eea662380418af3a0ce3b1.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jeswin Thomas](https://unsplash.com/@jeswinthomas?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLflow is a powerful tool that is often talked about for its experiment tracking
    capabilities. And it’s easy to see why — it’s a user-friendly platform for logging
    all the important details of your machine learning experiments, from hyper-parameters
    to models. But did you know that MLflow has more to offer than just experiment
    tracking? This versatile framework also includes features such as MLflow Projects,
    the Model Registry, and built-in deployment options. In this post, we’ll explore
    how to utilise all of these features to create a complete and efficient ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: For complete MLflow beginners, this tutorial might be too much so I highly encourage
    you to watch these two videos before diving into this one!
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this project we’ll be working locally, so make sure to properly setup your
    local environment. There are three main dependecnies that the project requires
    — `mlflow` ,`pyenv` , and `kaggle` . While MLflow can be installed simply using
    pip, you’ll need to follow separate instructions to setup [pyenv](https://github.com/pyenv/pyenv#installation)
    and [kaggle](https://github.com/Kaggle/kaggle-api).
  prefs: []
  type: TYPE_NORMAL
- en: Once you’re done with the installations, make sure to pull the latest version
    of [this repo](https://github.com/aruberts/tutorials). When you have the repo
    on your laptop, we’re finally ready to begin!
  prefs: []
  type: TYPE_NORMAL
- en: Project Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Move to the `mlflow_models` folder and you’ll see the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6bfda34601ead55847c0cae7c342b41a.png)'
  prefs: []
  type: TYPE_IMG
- en: mlflow_models folder structure. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a brief overview of each file in this project:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MLProject` — yaml-styled file describing the MLflow Project'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`python_env.yaml`— lists all the environment dependencies to run the project'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_hgbt.py` and `train_rf.py` — training scripts for HistGradientBoosterTree
    and RandomForest models using specific hyperparamaters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`search_params.py` — script to perform hyperparameter search.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`utils` — folder contains all the utility functions used in the project'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As stated before, this project is end-to-end, so we’re going to go from data
    download to model deployment. Approximate workflow is going to be like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Download data from Kaggle
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load and pre-process the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune Random Forest (RF) model for 10 iterations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Register the best RF model and put it into production bucket
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the models using in-built REST API
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After we’re done, you can repeat the steps 2 to 5 for HistGradientBoostedTrees
    model on your own. Before jumping into the project, let’s see how these steps
    can be supported by MLflow.
  prefs: []
  type: TYPE_NORMAL
- en: MLflow Components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally speaking, MLflow has 4 components — Tracking, Projects, Models, and
    Registry.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1bee01ad3f7a3c35ea36d74bac6fa558.png)'
  prefs: []
  type: TYPE_IMG
- en: MLflow components. Screenshot from mlflow.org
  prefs: []
  type: TYPE_NORMAL
- en: Thinking back to the project steps, here’s how we’re going to be using each
    of them. First of all, I’ve used **MLflow Projects to package up the code** so
    that you or any other data scientist/engineer can reproduce the results. Second,
    **MLflow Tracking serve is going to track your tuning experiments**. This way,
    you’ll be able to retrieve the best experiments in the next step, where **you’ll
    add your models to the Model Registry.** From the registry, deploying the models
    will literally be a one-liner because of the MLflow Models format that they’re
    saved in and their in-built REST API functionality.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1dbaeb97d6d723db175c5b1c7363cd83.png)'
  prefs: []
  type: TYPE_IMG
- en: Project steps overview. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data will be downloaded automatically when you run the pipeline. As an illustrative
    example, I’ll be using a [Loan Default](https://www.kaggle.com/datasets/hemanthsai7/loandefault)
    dataset (CC0: Public Domain license) but you can adjust this by re-writing `training_data`
    parameter and changing column names to the relevant ones.'
  prefs: []
  type: TYPE_NORMAL
- en: MLProject & Environment Files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`MLProject` file gives you a convenient way to manage and organise your machine
    learning projects by allowing you to specify important details such as the project
    name, location of your Python environment, and the entry points for your pipeline.
    Each entry point can be customized with a unique name, relevant parameters, and
    a specific command. The command serves as the executable shell line that will
    be executed whenever the entry point is activated, and it has the capability to
    utilize the parameters that have been previously defined.'
  prefs: []
  type: TYPE_NORMAL
- en: '`python_env.yaml` file outlines the precise version of Python necessary to
    execute the pipeline, along with a comprehensive list of all required packages.'
  prefs: []
  type: TYPE_NORMAL
- en: These two files were needed to create a necessary environment for running the
    project. Now, let’s see the actual scripts (entry points) that the pipeline will
    be executing.
  prefs: []
  type: TYPE_NORMAL
- en: Training and Experiment Tracking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training is done in `train_rf.py` and `train_hgbt.py` scripts. Both of them
    are largely the same, with exception of the hyper-parametrs that get passed and
    the pre-processing pipelines. Consider the function below which downloads the
    data and trains a Random Forest model.
  prefs: []
  type: TYPE_NORMAL
- en: The experiment starts when we define MLflow context using `with mlflow.start_run()`.
    Under this context, we use `mlflow.log_metrics` to save the PR AUC metrics (check
    out the `eval_and_log_metrics` function for more information) and `mlflow.sklearn.log_model`
    to save the preprocessing and modelling pipeline. This way, when we load the pipeline,
    it will do all the pre-processing together with the inference. Quite convenient
    if you ask me!
  prefs: []
  type: TYPE_NORMAL
- en: Hyper-parameter Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hyper-parameter tuning is done using Hyperopt package in `search_params.py`
    . A lot of the code is borrowed from the official [mlflow repo](https://github.com/mlflow/mlflow/tree/master/examples/hyperparam)
    but I’ve tried to simplify it quite a bit. The trickiest part of this script is
    to understand how to structure these tuning rounds, so that they appear connected
    to the “main” project run. Essentially, when we run `search_params.py` using MLflow,
    we want to make sure that the structure of experiments is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a03435cb340c618b037051fb8a491fff.png)'
  prefs: []
  type: TYPE_IMG
- en: Experiment structure visualised. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the `search_params` script does nothing else but specifies which
    parameters should `train_rf.py` use next (e.g. depths of 10, 2 and 5) and what
    should be its parent run ID (in the example above it’s 1234). When you explore
    the script, make sure to pay attention to the following details.
  prefs: []
  type: TYPE_NORMAL
- en: When we define `mlflow.start_run`context, we need to make sure that `nested`
    parameter is set to `True`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we run `train_rf.py` (or `train_hgbt.py` ), we explicitly pass the `run_id`
    and make it equal to the previously created `child_run` run
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also need to pass the correct`experiment_id`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please see the example below to understand how it all works in code. `eval`
    funtion is the one that will be optimised by the Hyperopt minimisation function.
  prefs: []
  type: TYPE_NORMAL
- en: The actual tuning function is relatively simple. All we do is initialise an
    MLflow experiment run (parent run of all the other runs) and optimise the objective
    function using provided search space.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that these functions are just to illustrate the main parts of the
    code. Refer to the [github repository](https://github.com/aruberts/tutorials/tree/main/mlflow_models)
    for the full versions of the code.
  prefs: []
  type: TYPE_NORMAL
- en: Run the RF Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By now you should have a general idea about how the scripts work! So, let’s
    run the pipeline for Random Forest using this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s decompose this command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mlflow run .` means that we want to run the Project in this folder'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-e search_params` specifies which of the entry points in MLProject file we
    want to run'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--experiment-name loan` makes the experiment name equal to “**loan”.** You
    can set it to whatever you want but write it down since you’ll need later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-P model_type=rf` sets the `model_type` parameter in `search_params` script
    to “**rf”** (aka Random Forest)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When we run this line, four things should happen:'
  prefs: []
  type: TYPE_NORMAL
- en: Python virtual environment will get created
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: New experiment called “**loan**” will get initialised
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kaggle data will get downloaded into a newly created folder `data`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hyperparameter search will begin
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the experiments are done, we can check the results in the MLflow UI . To
    access it, simply use the command `mlflow ui` in your command line. In the UI,
    select “loan” experiment (or whatever you’ve called it) and add your metric to
    the experiments view.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79bb66edb952c8cdde684cead0d3aeb2.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of MLflow UI. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The best RF model has achieved test PR AUC of 0.104 and took 1 minute to train.
    Overall, the hyper-parameter tuning took step too roughly 5 minutes to complete.
  prefs: []
  type: TYPE_NORMAL
- en: Register the Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, we have trained, evaluated and saved 10 Random Forest models. In theory,
    you can simply go to the UI to find the best model and manually register it in
    your Model Registry and promote it to production. However, a better way is to
    do it in code since then you can automate this step. This is exactly what `model_search.ipynb`
    notebook covers. Use it to follow along the sections below.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, we need to find the best model. To do it programatically you need
    to gather all the hyperparameter tuning experiments (10 of them) and sort them
    by the test metric.
  prefs: []
  type: TYPE_NORMAL
- en: Your results will be different but the main goal here is to end up with correct
    `best_run` parameter. Please note that if you’ve changed the experiment name,
    you’ll need to change it in this script as well. The parent run IDs can be looked
    up in the UI if you click on the parent experiment (in this case named **“capable-ray-599”)**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1bc70d87384fefc8a70426d83f30d99b.png)'
  prefs: []
  type: TYPE_IMG
- en: Run ID lookup in MLflow UI. Screenshot by author.
  prefs: []
  type: TYPE_NORMAL
- en: To test if your model is working as expected, we can easily load it into the
    notebook.
  prefs: []
  type: TYPE_NORMAL
- en: If you managed to get the prediction — congrats, you’ve done everything correctly!
    Finally, registering the model and promoting it to Production is a piece of cake
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: Running these 2 lines of code registers your model, and promotes it to “Production”
    bucket internally. All this does is changes the ways of how we can access the
    models and their metadata but it’s incredibly powerful in the context of model
    versioning. For example, at any point we can compare version 1 with version 2
    when it comes out.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1170edae42bc8d8f2edf4aa3abd34132.png)'
  prefs: []
  type: TYPE_IMG
- en: MLflow model registry. Screenshot by author.
  prefs: []
  type: TYPE_NORMAL
- en: If you go to the “Models” tab of the UI, you’ll indeed see that there is a model
    named `loan_model` and its Version 1 is currently in Production bucket. This means
    that we can now access the model by its name and stage which is very convenient.
  prefs: []
  type: TYPE_NORMAL
- en: Serve the Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The easiest way of serving the model is to do it locally. This is usually done
    to test the endpoint and to make sure that we get the expected outputs. Serving
    with MLflow is quite easy, especially when we’ve already registered the model.
    All you need to do is run this this command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This line will start a local server that will host your model (that’s called
    `loan_model` and is currently in `Production` stage) at the port 5001\. This means
    that you’ll be able to send the requests to `localhost:5001/invocations` endpoint
    and get the predictions back (given that the requests are properly formatted).
  prefs: []
  type: TYPE_NORMAL
- en: To test the endpoint locally, you can use a `requests` library to call it and
    get the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In the example above, we’re getting the same probability that we had before,
    but now these scores are produced by the local server and not your script. The
    inputs need needs to follow very specific guidelines, so that’s why we have 4
    lines of pre-processing. You can read more about the expected formats for MLflow
    serving [here](https://mlflow.org/docs/latest/models.html#deploy-mlflow-models)
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’ve managed to get this far and everything is working — give yourself
    a nice round of applause! I know it was a lot to take in, so let’s summarise everything
    you’ve achieved so far.
  prefs: []
  type: TYPE_NORMAL
- en: You saw and understand how to structure your project with MLflow Projects
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You understand where in the script we log our parameters, metrics and models,
    and how `search_params.py` invokes `train_rf.py`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can now run the MLflow Projects and see the results in MLflow UI
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You know how to find the best model, how to add it to the model registry, and
    how to promote it to Production bucket programmatically
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can serve the models from model registry locally and can call the endpoint
    to make a prediction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What Next?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I strongly recommend that you put your skills to the test by attempting to run
    the pipeline for the Gradient Boosted Trees model and then deploying the HGBT
    model. All the necessary scripts are available to you, so all that remains is
    for you to configure the pipeline and complete the deployment on your own. Give
    it a go and if you encounter any challenges or have any questions, don’t hesitate
    to leave them in the comments section.
  prefs: []
  type: TYPE_NORMAL
