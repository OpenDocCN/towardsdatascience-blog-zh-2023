- en: A Step By Step Guide to Selecting and Running Your Own Generative Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-step-by-step-guide-to-selecting-and-running-your-own-generative-model-4e52ecc28540](https://towardsdatascience.com/a-step-by-step-guide-to-selecting-and-running-your-own-generative-model-4e52ecc28540)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to navigate the sea of models being released every day?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@kevin.berlemont?source=post_page-----4e52ecc28540--------------------------------)[![Kevin
    Berlemont, PhD](../Images/18697f38b76f1fb04870f565cfb04b4c.png)](https://medium.com/@kevin.berlemont?source=post_page-----4e52ecc28540--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4e52ecc28540--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4e52ecc28540--------------------------------)
    [Kevin Berlemont, PhD](https://medium.com/@kevin.berlemont?source=post_page-----4e52ecc28540--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4e52ecc28540--------------------------------)
    ·5 min read·Oct 7, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57dcd30f489d1c312588752fbfe2e016.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [fabio](https://unsplash.com/@fabioha?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The past months have witnessed a drastic reduction in the parameters size of
    the various generative models, such as the new Mistral AI’s model that just came
    out. The reduction in size opens the door to getting your own personal assistant
    AI enabled that can be tied to you through your local computer. This type of local
    inference is very tempting to ensure confidential computation on your data. With
    all these new developments, deploying and managing an AI workload looks different
    than it did 6 months ago and is constantly evolving. How to use one of these models
    to play around with it or even to host it on your company’s infrastructure?
  prefs: []
  type: TYPE_NORMAL
- en: I think that before using any kind of API model that will be hosted by someone
    else, it is a good thing to experiment with different type of models to get a
    sense of how these different model families perform. So let’s assume you are not
    using an API model right away. How do you pull down a model and use it?
  prefs: []
  type: TYPE_NORMAL
- en: 'For this you have two type of models: proprietary and open access models. The
    proprietary models will be OpenAI, Cohere and so on and they all have their own
    API. The open access ones can be fully open or half-restricted models due to their
    license such as commercial, non-commercial, research purposes only …'
  prefs: []
  type: TYPE_NORMAL
- en: The best place to find these models is on [HuggingFace](https://huggingface.co/).
    On the models page, you can see that hey have over 350,000 models available across
    a very diverse set of tasks. So you do have a few to choose from!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4e3344bdbc5f8808d5e99cb5fdba746.png)'
  prefs: []
  type: TYPE_IMG
- en: Models Homepage on HuggingFace
  prefs: []
  type: TYPE_NORMAL
- en: Something to have in mind is that not all of these models are being/will be
    used. Some of them could just be somebody trying something out during an afternoon
    and then never updated it again. One of they key metrics to find the most useful
    models is to look at how many people downloaded the model and liked it. For example,
    you filter on the type of task you are looking for, such as Text Classification,
    and from there you can see which are the most downloaded and the trending models
    filtered by licenses and so on. This gives you a good overview of the landscape
    of the models for the task you have in mind and where to find them.
  prefs: []
  type: TYPE_NORMAL
- en: Going to the Text Generation task, which is the main topic of conversation in
    generative models at the moment, you can see that the trending model is the new
    Mistral model ([https://mistral.ai/news/announcing-mistral-7b)](https://mistral.ai/news/announcing-mistral-7b/)
    with 7 billions parameters. Now, with all of these models available on HuggingFace,
    how to know which model will be appropriate for your task?
  prefs: []
  type: TYPE_NORMAL
- en: The first thing to note, as you can see on the screenshot below, is that when
    clicking on a model you land on the model card and most of them already have a
    hosted interactive interface where you can get a hint of the output of the model.
    In addition of this hosted interactive interface, you can see under it what is
    called *Spaces*. Spaces are applications hosted on HuggingFace where people have
    integrated the model you are looking at. All of these interfaces are really handy
    at getting a sense of what all of these models do.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/260bd564f99ea9dd89c0caa8d470ace0.png)'
  prefs: []
  type: TYPE_IMG
- en: Mistral 7B model card on HuggingFace
  prefs: []
  type: TYPE_NORMAL
- en: Selecting and Running a Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of course the constraints during the model selection are going to depend on
    the infrastructure and hardware available to you. A good rule of thumb is that
    going above the 7 billions parameters for transformers models will make it hard
    to run it well on standard consumer GPUs. It is worth noting that you might find
    model optimization pieces to run such models on consumer hardwares that have been
    created. For example it could be a change in model size or even computing precision
    and that could be appropriate for your specific task and allow you to run the
    model on your hardware.
  prefs: []
  type: TYPE_NORMAL
- en: In any case, a good idea is to start with smaller problems and to then build
    up to something that solves your task. Once you figured out what model and customization
    you need, you can have a look at what it will mean for your hardware requirements
    and if it is something that can be done. For example, let’s consider that for
    the text generation task I have in mind, the Mistral 7B model fits my needs in
    term of hardware requirement. Then the model card on HuggingFace will give you
    instruction on how to download and run it. I usually use Google Collab ([https://colab.research.google.com/?utm_source=scs-index](https://colab.research.google.com/?utm_source=scs-index))
    to get an idea on the running time and resources usage but you could use any other
    platform too such as Kaggle.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/47ce30e42bbf4c93e85c5852bc7050ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Inference of Mistral 7B on a standard Google Collab notebook
  prefs: []
  type: TYPE_NORMAL
- en: In the case of Mistral 7B, the model runs on the basic Google Collab engine
    with 12GB of memory. So that gives you an idea of the resources needed for a simple
    inference of the model and if you have to go on the path of optimizing the model
    for it to run on fewer resources or if you do have the resources needed.
  prefs: []
  type: TYPE_NORMAL
- en: After having done your preliminary work on selecting the model and getting a
    sense of the resources needed, you might be constrained by your current hardware,
    leaving you no choice but to try to optimize your model. Lucky for you, there
    is a GitHub repo ([https://github.com/intel-analytics/BigDL](https://github.com/intel-analytics/BigDL))
    that goes through the different set of optimization for big deep learning that
    you would require. It allows, for example, to run a llama language model on a
    standard computer. On there you will be able to find if the model you are looking
    for is optimizing for CPU computing (or single GPU at least) and if it would fit
    your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, I described the first steps in selecting and running your
    model for a machine learning task. If you are interested in learning more about
    model modifications for a specific task and model deployment, HuggingFace has
    a great course ([https://huggingface.co/learn/nlp-course/chapter1/1](https://huggingface.co/learn/nlp-course/chapter1/1))
    available for free.
  prefs: []
  type: TYPE_NORMAL
- en: If you enjoyed this story, don’t hesitate to show your appreciation by clapping
    or leaving a comment! Follow me on [Medium](https://medium.com/@kevin.berlemont)
    for more content about Data Science! You can also connect with me on [LinkedIn](https://www.linkedin.com/in/kevinberlemont/).
  prefs: []
  type: TYPE_NORMAL
