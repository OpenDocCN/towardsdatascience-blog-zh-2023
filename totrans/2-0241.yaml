- en: A Step By Step Guide to Selecting and Running Your Own Generative Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择和运行自己生成模型的逐步指南
- en: 原文：[https://towardsdatascience.com/a-step-by-step-guide-to-selecting-and-running-your-own-generative-model-4e52ecc28540](https://towardsdatascience.com/a-step-by-step-guide-to-selecting-and-running-your-own-generative-model-4e52ecc28540)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-step-by-step-guide-to-selecting-and-running-your-own-generative-model-4e52ecc28540](https://towardsdatascience.com/a-step-by-step-guide-to-selecting-and-running-your-own-generative-model-4e52ecc28540)
- en: How to navigate the sea of models being released every day?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何在每天发布的模型海洋中进行导航？
- en: '[](https://medium.com/@kevin.berlemont?source=post_page-----4e52ecc28540--------------------------------)[![Kevin
    Berlemont, PhD](../Images/18697f38b76f1fb04870f565cfb04b4c.png)](https://medium.com/@kevin.berlemont?source=post_page-----4e52ecc28540--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4e52ecc28540--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4e52ecc28540--------------------------------)
    [Kevin Berlemont, PhD](https://medium.com/@kevin.berlemont?source=post_page-----4e52ecc28540--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@kevin.berlemont?source=post_page-----4e52ecc28540--------------------------------)[![Kevin
    Berlemont, PhD](../Images/18697f38b76f1fb04870f565cfb04b4c.png)](https://medium.com/@kevin.berlemont?source=post_page-----4e52ecc28540--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4e52ecc28540--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4e52ecc28540--------------------------------)
    [Kevin Berlemont, PhD](https://medium.com/@kevin.berlemont?source=post_page-----4e52ecc28540--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4e52ecc28540--------------------------------)
    ·5 min read·Oct 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4e52ecc28540--------------------------------)
    ·阅读时间5分钟·2023年10月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/57dcd30f489d1c312588752fbfe2e016.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57dcd30f489d1c312588752fbfe2e016.png)'
- en: Photo by [fabio](https://unsplash.com/@fabioha?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [fabio](https://unsplash.com/@fabioha?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The past months have witnessed a drastic reduction in the parameters size of
    the various generative models, such as the new Mistral AI’s model that just came
    out. The reduction in size opens the door to getting your own personal assistant
    AI enabled that can be tied to you through your local computer. This type of local
    inference is very tempting to ensure confidential computation on your data. With
    all these new developments, deploying and managing an AI workload looks different
    than it did 6 months ago and is constantly evolving. How to use one of these models
    to play around with it or even to host it on your company’s infrastructure?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 过去几个月，各种生成模型的参数规模大幅减少，比如刚刚发布的新 Mistral AI 模型。规模的减少为获取自己的个人助理 AI 提供了可能性，可以通过你的本地计算机与之绑定。这种本地推理非常诱人，以确保对你的数据进行机密计算。随着这些新发展，部署和管理
    AI 工作负载的方式与6个月前相比有所不同，并且不断演变。如何使用这些模型来试验，甚至在你公司的基础设施上托管它们？
- en: I think that before using any kind of API model that will be hosted by someone
    else, it is a good thing to experiment with different type of models to get a
    sense of how these different model families perform. So let’s assume you are not
    using an API model right away. How do you pull down a model and use it?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为，在使用任何由其他人托管的 API 模型之前，尝试不同类型的模型是一个好主意，以了解这些不同模型家族的表现。因此，让我们假设你不会立即使用 API
    模型。你如何下载并使用一个模型呢？
- en: 'For this you have two type of models: proprietary and open access models. The
    proprietary models will be OpenAI, Cohere and so on and they all have their own
    API. The open access ones can be fully open or half-restricted models due to their
    license such as commercial, non-commercial, research purposes only …'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此，你有两种类型的模型：专有模型和开放访问模型。专有模型如 OpenAI、Cohere 等都有各自的 API。开放访问模型可以是完全开放的，也可以是由于其许可证（如商业、非商业、仅研究目的等）而受限的。
- en: The best place to find these models is on [HuggingFace](https://huggingface.co/).
    On the models page, you can see that hey have over 350,000 models available across
    a very diverse set of tasks. So you do have a few to choose from!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 找到这些模型的最佳地方是[HuggingFace](https://huggingface.co/)。在模型页面上，你可以看到他们提供了超过350,000个模型，覆盖了非常多样的任务。因此，你确实有很多选择！
- en: '![](../Images/f4e3344bdbc5f8808d5e99cb5fdba746.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4e3344bdbc5f8808d5e99cb5fdba746.png)'
- en: Models Homepage on HuggingFace
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Something to have in mind is that not all of these models are being/will be
    used. Some of them could just be somebody trying something out during an afternoon
    and then never updated it again. One of they key metrics to find the most useful
    models is to look at how many people downloaded the model and liked it. For example,
    you filter on the type of task you are looking for, such as Text Classification,
    and from there you can see which are the most downloaded and the trending models
    filtered by licenses and so on. This gives you a good overview of the landscape
    of the models for the task you have in mind and where to find them.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Going to the Text Generation task, which is the main topic of conversation in
    generative models at the moment, you can see that the trending model is the new
    Mistral model ([https://mistral.ai/news/announcing-mistral-7b)](https://mistral.ai/news/announcing-mistral-7b/)
    with 7 billions parameters. Now, with all of these models available on HuggingFace,
    how to know which model will be appropriate for your task?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The first thing to note, as you can see on the screenshot below, is that when
    clicking on a model you land on the model card and most of them already have a
    hosted interactive interface where you can get a hint of the output of the model.
    In addition of this hosted interactive interface, you can see under it what is
    called *Spaces*. Spaces are applications hosted on HuggingFace where people have
    integrated the model you are looking at. All of these interfaces are really handy
    at getting a sense of what all of these models do.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/260bd564f99ea9dd89c0caa8d470ace0.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Mistral 7B model card on HuggingFace
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Selecting and Running a Model
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of course the constraints during the model selection are going to depend on
    the infrastructure and hardware available to you. A good rule of thumb is that
    going above the 7 billions parameters for transformers models will make it hard
    to run it well on standard consumer GPUs. It is worth noting that you might find
    model optimization pieces to run such models on consumer hardwares that have been
    created. For example it could be a change in model size or even computing precision
    and that could be appropriate for your specific task and allow you to run the
    model on your hardware.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: In any case, a good idea is to start with smaller problems and to then build
    up to something that solves your task. Once you figured out what model and customization
    you need, you can have a look at what it will mean for your hardware requirements
    and if it is something that can be done. For example, let’s consider that for
    the text generation task I have in mind, the Mistral 7B model fits my needs in
    term of hardware requirement. Then the model card on HuggingFace will give you
    instruction on how to download and run it. I usually use Google Collab ([https://colab.research.google.com/?utm_source=scs-index](https://colab.research.google.com/?utm_source=scs-index))
    to get an idea on the running time and resources usage but you could use any other
    platform too such as Kaggle.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，一个好的建议是从较小的问题开始，然后逐步构建到解决你的任务的方案。一旦你弄清楚了需要什么模型和定制化，你可以查看这将对你的硬件需求意味着什么，以及是否可以实现。例如，假设对于我考虑的文本生成任务，Mistral
    7B 模型在硬件需求方面符合我的需求。然后，HuggingFace 上的模型卡会给出如何下载和运行的说明。我通常使用 Google Collab ([https://colab.research.google.com/?utm_source=scs-index](https://colab.research.google.com/?utm_source=scs-index))
    来了解运行时间和资源使用情况，但你也可以使用其他平台，如 Kaggle。
- en: '![](../Images/47ce30e42bbf4c93e85c5852bc7050ba.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47ce30e42bbf4c93e85c5852bc7050ba.png)'
- en: Inference of Mistral 7B on a standard Google Collab notebook
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准 Google Collab 笔记本上推理 Mistral 7B
- en: In the case of Mistral 7B, the model runs on the basic Google Collab engine
    with 12GB of memory. So that gives you an idea of the resources needed for a simple
    inference of the model and if you have to go on the path of optimizing the model
    for it to run on fewer resources or if you do have the resources needed.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Mistral 7B 模型，它在基本的 Google Collab 引擎上运行，内存为 12GB。因此，这可以让你了解简单推理模型所需的资源，以及你是否需要优化模型以便在更少资源上运行，或者你是否已经具备所需的资源。
- en: After having done your preliminary work on selecting the model and getting a
    sense of the resources needed, you might be constrained by your current hardware,
    leaving you no choice but to try to optimize your model. Lucky for you, there
    is a GitHub repo ([https://github.com/intel-analytics/BigDL](https://github.com/intel-analytics/BigDL))
    that goes through the different set of optimization for big deep learning that
    you would require. It allows, for example, to run a llama language model on a
    standard computer. On there you will be able to find if the model you are looking
    for is optimizing for CPU computing (or single GPU at least) and if it would fit
    your requirements.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成选择模型和了解所需资源的初步工作后，你可能会受到当前硬件的限制，不得不尝试优化你的模型。幸运的是，有一个 GitHub 仓库 ([https://github.com/intel-analytics/BigDL](https://github.com/intel-analytics/BigDL))
    详细介绍了大规模深度学习所需的不同优化方法。例如，它允许在标准计算机上运行 llama 语言模型。在那里，你可以找到所寻求的模型是否针对 CPU 计算（或至少单个
    GPU）进行了优化，以及是否符合你的要求。
- en: In this blog post, I described the first steps in selecting and running your
    model for a machine learning task. If you are interested in learning more about
    model modifications for a specific task and model deployment, HuggingFace has
    a great course ([https://huggingface.co/learn/nlp-course/chapter1/1](https://huggingface.co/learn/nlp-course/chapter1/1))
    available for free.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我描述了选择和运行模型的第一步。如果你有兴趣了解更多关于特定任务的模型修改和模型部署的内容，HuggingFace 提供了一个很棒的课程
    ([https://huggingface.co/learn/nlp-course/chapter1/1](https://huggingface.co/learn/nlp-course/chapter1/1))，可以免费获取。
- en: If you enjoyed this story, don’t hesitate to show your appreciation by clapping
    or leaving a comment! Follow me on [Medium](https://medium.com/@kevin.berlemont)
    for more content about Data Science! You can also connect with me on [LinkedIn](https://www.linkedin.com/in/kevinberlemont/).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这个故事，别犹豫，通过鼓掌或留言来表达你的欣赏！关注我在 [Medium](https://medium.com/@kevin.berlemont)
    上的更多数据科学内容！你还可以通过 [LinkedIn](https://www.linkedin.com/in/kevinberlemont/) 与我联系。
