["```py\n**Table of Contents:**\n\nThe importance of prompt engineering today\nHow prompt engineering can affect Data Scientists\nExamples of effective prompts for Data Scientists\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_curve, auc\n\n# Generate synthetic data for binary classification\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=10,\n        n_redundant=10, random_state=42)\n\n# Standardize the data using StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                    random_state=42)\n\n# Fit the train set with classifiers\nrf_classifier = RandomForestClassifier(random_state=42)\nknn_classifier = KNeighborsClassifier()\nsvm_classifier = SVC(probability=True, random_state=42)\n\nrf_classifier.fit(X_train, y_train)\nknn_classifier.fit(X_train, y_train)\nsvm_classifier.fit(X_train, y_train)\n\n# Generate predictions for the test set\nrf_probs = rf_classifier.predict_proba(X_test)[:, 1]\nknn_probs = knn_classifier.predict_proba(X_test)[:, 1]\nsvm_probs = svm_classifier.predict_proba(X_test)[:, 1]\n\n# Calculate false positive rate, true positive rate,\n# and area under the curve for ROC curve\nrf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs)\nrf_auc = auc(rf_fpr, rf_tpr)\n\nknn_fpr, knn_tpr, _ = roc_curve(y_test, knn_probs)\nknn_auc = auc(knn_fpr, knn_tpr)\n\nsvm_fpr, svm_tpr, _ = roc_curve(y_test, svm_probs)\nsvm_auc = auc(svm_fpr, svm_tpr)\n\n# Plot the ROC curve\nplt.figure()\nplt.plot(rf_fpr, rf_tpr, label=f'Random Forest (AUC = {rf_auc:.2f})')\nplt.plot(knn_fpr, knn_tpr, label=f'KNN (AUC = {knn_auc:.2f})')\nplt.plot(svm_fpr, svm_tpr, label=f'SVM (AUC = {svm_auc:.2f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc='lower right')\nplt.show()\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Generate random data\nnp.random.seed(0)\nX = np.linspace(-3, 3, 100)\ny = 0.5 * X**2 + X + np.random.normal(0, 1, 100)\n\n# Reshape the input data\nX = X.reshape(-1, 1)\n\n# Plot the original data\nplt.scatter(X, y, color='b', label='Data')\n\n# Fit polynomial regression models of different degrees\ndegrees = [1, 4, 15]\ncolors = ['r', 'g', 'm']\nfor degree, color in zip(degrees, colors):\n    # Create polynomial features\n    polynomial_features = PolynomialFeatures(degree=degree)\n    X_poly = polynomial_features.fit_transform(X)\n\n    # Fit the polynomial regression model\n    model = LinearRegression()\n    model.fit(X_poly, y)\n\n    # Predict the values\n    y_pred = model.predict(X_poly)\n\n    # Plot the fitted curve\n    plt.plot(X, y_pred, color=color, linewidth=2, label=f'Degree {degree}')\n\n# Add labels and title to the plot\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Polynomial Regression - Overfitting Example')\nplt.legend(loc='upper left')\n\n# Display the plot\nplt.show()\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\n# Generate random data\nnp.random.seed(0)\nX = np.linspace(-3, 3, 100)\ny = 0.5 * X**2 + X + np.random.normal(0, 1, 100)\n\n# Reshape the input data\nX = X.reshape(-1, 1)\n\n# Normalize the data\nX_normalized = (X - np.mean(X)) / np.std(X)\ny_normalized = (y - np.mean(y)) / np.std(y)\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X_normalized, y_normalized, test_size=0.2, random_state=0\n)\n\n# Plot the original data\nplt.scatter(X_normalized, y_normalized, color='b', label='Data')\n\n# Fit polynomial regression models of different degrees\ndegrees = [1, 4, 15]\ncolors = ['r', 'g', 'm']\nfor degree, color in zip(degrees, colors):\n    # Create polynomial features\n    polynomial_features = PolynomialFeatures(degree=degree)\n    X_poly_train = polynomial_features.fit_transform(X_train)\n    X_poly_test = polynomial_features.transform(X_test)\n\n    # Fit the polynomial regression model\n    model = LinearRegression()\n    model.fit(X_poly_train, y_train)\n\n    # Predict the values for train and test sets\n    y_pred_train = model.predict(X_poly_train)\n    y_pred_test = model.predict(X_poly_test)\n\n    # Calculate R² scores\n    r2_train = r2_score(y_train, y_pred_train)\n    r2_test = r2_score(y_test, y_pred_test)\n\n    # Plot the fitted curve\n    plt.plot(X_normalized, model.predict(polynomial_features.transform(X_normalized)), color=color, linewidth=2,\n             label=f'Degree {degree} (Train R²={r2_train:.2f}, Test R²={r2_test:.2f})')\n\n# Add labels and title to the plot\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Polynomial Regression - Overfitting Example (Normalized)')\nplt.legend(loc='upper left')\n\n# Display the plot\nplt.show()\n```", "```py\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# Create the dataset\nX = np.random.rand(100, 1)  # Independent variable\ny = 3 * X**2 + np.random.randn(100, 1)  # Dependent variable (quadratic relationship)\n\n# Normalize the data\nX_normalized = (X - np.mean(X)) / np.std(X)\ny_normalized = (y - np.mean(y)) / np.std(y)\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_normalized, y_normalized, test_size=0.2, random_state=42)\n\n# Fit the linear regression model\nlinear_regression = LinearRegression()\nlinear_regression.fit(X_train, y_train)\n\n# Fit the 4-degree polynomial regression model\npoly_features = PolynomialFeatures(degree=4)\nX_poly_train = poly_features.fit_transform(X_train)\nX_poly_test = poly_features.transform(X_test)\npoly_regression = LinearRegression()\npoly_regression.fit(X_poly_train, y_train)\n\n# Fit the 15-degree polynomial regression model\npoly_features = PolynomialFeatures(degree=15)\nX_poly_train = poly_features.fit_transform(X_train)\nX_poly_test = poly_features.transform(X_test)\npoly_regression_15 = LinearRegression()\npoly_regression_15.fit(X_poly_train, y_train)\n\n# Calculate R-squared for train and test sets\nlinear_train_r2 = linear_regression.score(X_train, y_train)\nlinear_test_r2 = linear_regression.score(X_test, y_test)\n\npoly_train_r2 = poly_regression.score(X_poly_train, y_train)\npoly_test_r2 = poly_regression.score(X_poly_test, y_test)\n\npoly_15_train_r2 = poly_regression_15.score(X_poly_train, y_train)\npoly_15_test_r2 = poly_regression_15.score(X_poly_test, y_test)\n\n# Print the results\nprint(\"Linear Regression R-squared (Train):\", linear_train_r2)\nprint(\"Linear Regression R-squared (Test):\", linear_test_r2)\n\nprint(\"4-Degree Polynomial Regression R-squared (Train):\", poly_train_r2)\nprint(\"4-Degree Polynomial Regression R-squared (Test):\", poly_test_r2)\n\nprint(\"15-Degree Polynomial Regression R-squared (Train):\", poly_15_train_r2)\nprint(\"15-Degree Polynomial Regression R-squared (Test):\", poly_15_test_r2)\n```", "```py\nValueError: X has 16 features, but LinearRegression\nis expecting 5 features as input.\n```", "```py\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# Create the dataset\nX = np.random.rand(100, 1)  # Independent variable\ny = 3 * X**2 + np.random.randn(100, 1)  # Dependent variable (quadratic relationship)\n\n# Normalize the data\nX_normalized = (X - np.mean(X)) / np.std(X)\ny_normalized = (y - np.mean(y)) / np.std(y)\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_normalized, y_normalized, test_size=0.2, random_state=42)\n\n# Fit the linear regression model\nlinear_regression = LinearRegression()\nlinear_regression.fit(X_train, y_train)\n\n# Fit the 4-degree polynomial regression model\npoly_features = PolynomialFeatures(degree=4)\nX_poly_train = poly_features.fit_transform(X_train)\nX_poly_test = poly_features.transform(X_test)\npoly_regression = LinearRegression()\npoly_regression.fit(X_poly_train, y_train)\n\n# Fit the 15-degree polynomial regression model\npoly_features_15 = PolynomialFeatures(degree=15)\nX_poly_train_15 = poly_features_15.fit_transform(X_train)\nX_poly_test_15 = poly_features_15.transform(X_test)\npoly_regression_15 = LinearRegression()\npoly_regression_15.fit(X_poly_train_15, y_train)\n\n# Calculate R-squared for train and test sets\nlinear_train_r2 = linear_regression.score(X_train, y_train)\nlinear_test_r2 = linear_regression.score(X_test, y_test)\n\npoly_train_r2 = poly_regression.score(X_poly_train, y_train)\npoly_test_r2 = poly_regression.score(X_poly_test, y_test)\n\npoly_15_train_r2 = poly_regression_15.score(X_poly_train_15, y_train)\npoly_15_test_r2 = poly_regression_15.score(X_poly_test_15, y_test)\n\n# Print the results\nprint(\"Linear Regression R-squared (Train):\", linear_train_r2)\nprint(\"Linear Regression R-squared (Test):\", linear_test_r2)\n\nprint(\"4-Degree Polynomial Regression R-squared (Train):\", poly_train_r2)\nprint(\"4-Degree Polynomial Regression R-squared (Test):\", poly_test_r2)\n\nprint(\"15-Degree Polynomial Regression R-squared (Train):\", poly_15_train_r2)\nprint(\"15-Degree Polynomial Regression R-squared (Test):\", poly_15_test_r2)\n```", "```py\nLinear Regression R-squared (Train): 0.5287297254229243\nLinear Regression R-squared (Test): 0.4627146369316367\n\n4-Degree Polynomial Regression R-squared (Train): 0.5895312356444804\n4-Degree Polynomial Regression R-squared (Test): 0.4601670183046247\n\n15-Degree Polynomial Regression R-squared (Train): 0.6256705123172766\n15-Degree Polynomial Regression R-squared (Test): 0.26972190687190367\n```"]