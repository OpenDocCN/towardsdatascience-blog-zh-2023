- en: Maximum Likelihood Estimation for Beginners (with R code)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-laymans-guide-to-maximum-likelihood-estimation-with-r-code-9e992a10ecd9](https://towardsdatascience.com/a-laymans-guide-to-maximum-likelihood-estimation-with-r-code-9e992a10ecd9)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Intuitive explanation of maximum likelihood method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jaekim8080?source=post_page-----9e992a10ecd9--------------------------------)[![Jae
    Kim](../Images/34716958ecfe8c0540f5cf5c1640d587.png)](https://medium.com/@jaekim8080?source=post_page-----9e992a10ecd9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9e992a10ecd9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9e992a10ecd9--------------------------------)
    [Jae Kim](https://medium.com/@jaekim8080?source=post_page-----9e992a10ecd9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9e992a10ecd9--------------------------------)
    ·4 min read·Mar 14, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: The maximum likelihood principle is a fundamental method of estimation for a
    large number of models in data science, machine learning, and artificial intelligence.
    It is applicable to a range of methods from the logit model for classification
    to information theories in deep learning. This article is aimed to provide an
    intuitive introduction to the principle.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have three data points x = (1, 2, 3), and you believe that they
    are generated from a normal distribution with the unknown mean (μ) and standard
    deviation equal to 1, i.e., N(μ,1).
  prefs: []
  type: TYPE_NORMAL
- en: '*Given these data points you have, what is the most likely value of μ?*'
  prefs: []
  type: TYPE_NORMAL
- en: This is the question that the method of maximum likelihood estimation is designed
    to answer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8f59e21317a146d3fef8961a03fdf93f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by the author
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the researcher considers three possible values of μ = (0, 2, 6) as likely
    candidates. Which one of these values are most likely for the data observed?
  prefs: []
  type: TYPE_NORMAL
- en: The figure above plots three normal distribution probability (density) functions
    for N(0,1), N(2,1), and N(6,1), respectively in blue, black, and green. That is,
    they are respectively f(X|μ=0), f(X|μ=2), f(X|μ=6), where
  prefs: []
  type: TYPE_NORMAL
- en: Created by the author
  prefs: []
  type: TYPE_NORMAL
- en: The red square dots at the bottom indicate x=(1, 2, 3), which are the observed
    data points.
  prefs: []
  type: TYPE_NORMAL
- en: It is obvious from the above plots that the data points x is most likely to
    have generated from N(2,1). They are quite unlikely from N(0,1), and even more
    so from N(6,1). Hence, we can say that the value of μ = 2 is associated with the
    highest likelihood or compatibility with x = (1, 2, 3).
  prefs: []
  type: TYPE_NORMAL
- en: If we consider all other possible values of μ, and convinced that 2 is most
    likely to have generated x, then it is the maximum likelihood estimate for μ.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us define some mathematical details:'
  prefs: []
  type: TYPE_NORMAL
- en: 'f(X1,X2,X3|μ): **joint** **probability** **density function** of X, given μ.
    It shows the probability density of X, given the value of μ.'
  prefs: []
  type: TYPE_NORMAL
- en: 'L(μ|x1,x2,x3): **likelihood function** of μ, given x. It shows the likelihood
    of μ, given the observed data x=(x1, x2, x3).'
  prefs: []
  type: TYPE_NORMAL
- en: The difference is that the density function is indexed by random variable X,
    given a value of parameter such as μ; while the likelihood function is indexed
    by the parameter, given the observed data x.
  prefs: []
  type: TYPE_NORMAL
- en: The two functions are related as
  prefs: []
  type: TYPE_NORMAL
- en: L(μ|x1,x2,x3) = k f(x1,x2,x3|μ),
  prefs: []
  type: TYPE_NORMAL
- en: where k > 0 is any constant. Let us assume that k = 1 for simplicity. Then the
    two functions are almost the same, with the difference being their arguments and
    the conditioning values. If we assume for simplicity that X’s are independent,
    then we can write (since the joint probability is a product of individual probabilities
    under independence)
  prefs: []
  type: TYPE_NORMAL
- en: L(μ|x1,x2,x3) = f(x1|μ) × f(x2|μ) × f(x3|μ).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/769bb47ce63c0e666f870c421bb7194e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'The above table shows the values of the likelihood function L(μ|x1,x2,x3) when
    x=(1, 2, 3): the values listed in the last column as a product of those in the
    first three. The highest likelihood value achieved is at μ = 2.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we consider all possible values of μ, and plot the likelihood and log-of-likelihood
    functions as a function of μ. The log-of-likelihood function is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '*l*(μ|x1,x2,x3) = log[L(μ|x1,x2,x3)],'
  prefs: []
  type: TYPE_NORMAL
- en: where log() is the natural logarithmic function. The log-of-likelihood is a
    monotonic transformation of the likelihood function. It is widely used because
    it is analytically tractable, being additive and linear.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1fc02b374dc4c813f189845faf701ce4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by the author
  prefs: []
  type: TYPE_NORMAL
- en: The functions are plotted as above. It is clear from the above plots that the
    likelihood or log-of-likelihood is maximized at μ = 2, which is the maximum likelihood
    estimate for x = (1, 2, 3).
  prefs: []
  type: TYPE_NORMAL
- en: Analytically, it can be shown that the sample mean is the maximum likelihood
    estimator for a sample generated independently from N(μ,1), and the sample mean
    of x = (1, 2, 3) is indeed 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'R code for the calculations and plots are as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To conclude, the maximum likelihood estimation method is widely used to for
    many models and methods in data science. Its concept and principle are often not
    fully understood by the researchers and practitioners. This post is aimed to provide
    an intuitive explanation of the method without introducing the analytical details.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: Please follow me for more engaging stories!
  prefs: []
  type: TYPE_NORMAL
