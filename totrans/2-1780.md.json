["```py\ngit clone https://github.com/Guitaricet/peft_pretraining.git\n```", "```py\ncd peft_pretraining\npip install -r requirements.txt\n```", "```py\nif evaluated_on_tokens > target_eval_tokens:\n```", "```py\nif evaluated_on_tokens > target_eval_tokens or total_batches > 10:\n```", "```py\ntorchrun --nproc-per-node 1 torchrun_main.py \\\n    --model_config configs/llama_250m.json \\\n    --batch_size 4 \\\n    --total_batch_size 8 \\\n    --lr 5e-4 \\\n    --max_length 512 \\\n    --tags warm_start_250M \\\n    --save_every 10 \\\n    --num_training_steps 2 \\\n    --workers 1 \\\n    --eval_every 1\n```", "```py\nwandb: (1) Create a W&B account\nwandb: (2) Use an existing W&B account\nwandb: (3) Don't visualize my results\n```", "```py\ntorchrun --nproc-per-node 1 torchrun_main.py \\\n    --model_config configs/llama_250m.json \\\n    --batch_size 4 \\\n    --total_batch_size 8 \\\n    --lr 1e-3 \\\n    --max_length 512 \\\n    --use_peft \\\n    --relora 5 \\\n    --cycle_length 5 \\\n    --restart_warmup_steps 10 \\\n    --scheduler cosine_restarts \\\n    --warmup_steps 2 \\\n    --reset_optimizer_on_relora True \\\n    --num_training_steps 50 \\\n    --save_every 10 \\\n    --eval_every 10 \\\n    --continue_from checkpoints/llama_250m-2023-07-19-09-39-08/model_3 \\\n    --tags relora_250M\n```", "```py\ntorchrun --nproc-per-node 1 torchrun_main.py \\\n    --model_config configs/llama_250m.json \\\n    --batch_size 4 \\\n    --total_batch_size 8\\\n    --lr 5e-4 \\\n    --max_length 512 \\\n    --tags warm_start_250M \\\n    --save_every 1000 \\\n    --num_training_steps 10000\n\ntorchrun --nproc-per-node 1 torchrun_main.py \\\n    --model_config configs/llama_250m.json \\\n    --batch_size 4 \\\n    --total_batch_size 8 \\\n    --lr 1e-3 \\\n    --max_length 512 \\\n    --use_peft \\\n    --relora 5000 \\\n    --cycle_length 5000 \\\n    --restart_warmup_steps 100 \\\n    --scheduler cosine_restarts \\\n    --warmup_steps 500 \\\n    --reset_optimizer_on_relora True \\\n    --num_training_steps 10000 \\\n    --save_every 5000 \\\n    --eval_every 5000 \\\n    --continue_from <your checkpoint from step one>\\\n    --tags relora_250M \n```"]