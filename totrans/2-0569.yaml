- en: Convolutional vs Feedforward Autoencoders for Image Denoising
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/convolutional-vs-feedforward-autoencoders-for-image-denoising-2fe2e9aed71d](https://towardsdatascience.com/convolutional-vs-feedforward-autoencoders-for-image-denoising-2fe2e9aed71d)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: KavetiAPPLICATIONS OF AUTOENCODERS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cleaning corrupted images using convolutional and feedforward autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://rukshanpramoditha.medium.com/?source=post_page-----2fe2e9aed71d--------------------------------)[![Rukshan
    Pramoditha](../Images/b80426aff64ff186cb915795644590b1.png)](https://rukshanpramoditha.medium.com/?source=post_page-----2fe2e9aed71d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2fe2e9aed71d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2fe2e9aed71d--------------------------------)
    [Rukshan Pramoditha](https://rukshanpramoditha.medium.com/?source=post_page-----2fe2e9aed71d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2fe2e9aed71d--------------------------------)
    ·9 min read·Jan 24, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3dcd4cfa1a5be3ca4f767f62e4438c48.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Pierre Bamin](https://unsplash.com/@bamin?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/_EzTds6Fo44?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    (Slightly modified by the author)
  prefs: []
  type: TYPE_NORMAL
- en: '*Do you want to find out how convolutional autoencoders outperform feedforward
    autoencoders in image denoising?*'
  prefs: []
  type: TYPE_NORMAL
- en: If ‘*Yes’*, just keep reading this article.
  prefs: []
  type: TYPE_NORMAL
- en: Different types of autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many practical applications of autoencoders. Image denoising is one
    of them.
  prefs: []
  type: TYPE_NORMAL
- en: Image denoising refers to removing noise from corrupted images to get clean
    images.
  prefs: []
  type: TYPE_NORMAL
- en: The autoencoders used for image denoising are especially known as **denoising
    autoencoders**.
  prefs: []
  type: TYPE_NORMAL
- en: We have already covered the fundamentals of autoencoders in the following articles.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As a quick recap, an autoencoder is a type of neural network architecture that
    consists of three key elements: **Encoder**, **Decoder** and **Latent vector**.'
  prefs: []
  type: TYPE_NORMAL
- en: The relationship between these key elements is shown in the following diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ca0f75abe4d63c821ad0bd80e50d223.png)'
  prefs: []
  type: TYPE_IMG
- en: '**The structure of an autoencoder** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: The encoder (function ***f***) takes the input, ***x*** and transforms it into
    a latent vector denoted by ***z***. The decoder (function ***g***) takes ***z***
    as its input and recovers the input, ***x*** from the latent vector. The recovered
    input is approximately the same as ***x*** and therefore denoted as ***x̄***.
  prefs: []
  type: TYPE_NORMAL
- en: The latent vector can have the same or a higher dimension than the input (in
    case of an **overcomplete autoencoder**) or a much lower dimension than the input
    (in case of an **undercomplete autoencoder**).
  prefs: []
  type: TYPE_NORMAL
- en: However, for image-denoising applications, the latent vector should be lower-dimensional
    than the input.
  prefs: []
  type: TYPE_NORMAL
- en: Complex autoencoders have many hidden layers. They are known as **deep (multilayer)
    autoencoders**. In contrast, an autoencoder with a single hidden layer is called
    a **shallow (vanilla) autoencoder**.
  prefs: []
  type: TYPE_NORMAL
- en: We always use deep autoencoders in practical applications including image denoising
    as shallow autoencoders are not capable enough to capture the important relationships
    in the data.
  prefs: []
  type: TYPE_NORMAL
- en: An autoencoder can be created using dense and convolutional layers.
  prefs: []
  type: TYPE_NORMAL
- en: When both the encoder and decoder of an autoencoder consist of only dense (fully-connected
    or MLP) layers, such an autoencoder is known as a **feedforward autoencoder**.
  prefs: []
  type: TYPE_NORMAL
- en: When the encoder of an autoencoder consists of convolutional layers (downsampling
    layers) and the decoder consists of transposed convolutional layers (upsampling
    layers), such an autoencoder is known as a **convolutional autoencoder**.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will compare the outputs of both types.
  prefs: []
  type: TYPE_NORMAL
- en: How autoencoders remove noise from corrupted images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In image generation applications using autoencoders, we usually train the autoencoder
    model as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This is like telling the model “learn the relationships using the training data
    to generate new images that are almost identical to the original ones”. Then,
    we can call `autoencoder.predict(test_images)`to generate new images using the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In image-denoising applications using autoencoders, we usually train the autoencoder
    model as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This is like telling the model “learn the relationships using the noisy (corrupted)
    training data to remove the noise from the images of the same domain”. Then, we
    can call `autoencoder.predict(test_images)`to denoise new corrupted images of
    the same domain.
  prefs: []
  type: TYPE_NORMAL
- en: The noisy (corrupted) data is created by adding some stochastic (random) noise
    to the original data. The noise follows a Gaussian (normal) distribution centered
    (mean) at 0 with a standard deviation of 0.7.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: When we train a denoising autoencoder, the encoder part keeps the most important
    information and removes any unnecessary noise from the training data and creates
    the lower-dimensional latent (compressed) representation of the same data. Then,
    the decoder part recovers the clean images from that compressed representation.
    The trained model can be used to clean new corrupted images of the same domain
    — by author
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Building the models: Convolutional and feedforward autoencoders'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset we use
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the **MNIST dataset** (see [Citation](#54a3) at the end) to build
    the two autoencoder models here. This dataset comes preloaded with tf.keras.
  prefs: []
  type: TYPE_NORMAL
- en: To learn how to acquire and prepare the MNIST dataset for deep learning applications,
    read my article, [*Acquire, Understand and Prepare the MNIST Dataset*](https://rukshanpramoditha.medium.com/acquire-understand-and-prepare-the-mnist-dataset-3d71a84e07e7).
  prefs: []
  type: TYPE_NORMAL
- en: Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will build a convolutional and a feedforward autoencoder using the MNIST
    data and compare the outputs of both models. During the training, both modes have
    the same loss function, optimizer, batch size and number of epochs for comparison
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: However, the two architectures differ significantly as feedforward autoencoders
    have dense layers while convolutional autoencoders have convolutional and transposed
    convolutional layers.
  prefs: []
  type: TYPE_NORMAL
- en: Training the models and making predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After running the following code, you will get the trained autoencoder models
    with their training history and also the predictions made on new data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: It will take a long time to run this code if you use only the CPU. To speed
    up the process, you can use the [Colab free GPU](https://rukshanpramoditha.medium.com/how-to-use-google-colab-free-gpu-to-run-deep-learning-code-incredibly-faster-760604d26c7e)
    or a [GPU on your own laptop](https://rukshanpramoditha.medium.com/setting-up-a-deep-learning-workplace-with-an-nvidia-graphics-card-gpu-for-windows-os-b6bff06eeec7),
    if any.
  prefs: []
  type: TYPE_NORMAL
- en: I’m not going to explain the code line by line as I have already discussed all
    these things in my previous articles.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting the learning curves
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we can plot the learning curves for both models to analyze the training
    performance of both networks. For this, we can use the *history* objects of both
    models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ab23dceefcfdd30c129803dcc44810d7.png)'
  prefs: []
  type: TYPE_IMG
- en: '**The learning curves** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Left plot:** Shows the training performance of the feedforward autoencoder.
    The model seems to *slightly* overfit after the 35th epoch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Right plot:** Shows the training performance of the convolutional autoencoder.
    The model is in the just-right condition where the model is neither underfitting
    nor overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing the outputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we compare the outputs of both models. For comparison purposes, I will
    also add the original images and their corrupted (noisy) counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/993cae4901542d9ceedccf24d7544efa.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: '**1st row:** Represents the corrupted images after adding Gaussian noise centered
    (mean) at 0 with a standard deviation of 0.7\. We feed these images to autoencoders
    to get clean images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2nd row:** Represents the cleaned (denoised/predicted) images by the feedforward
    autoencoder. The contents of images can be identified and close enough to the
    original images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3rd row:** Represents the cleaned (denoised/predicted) images by the convolutional
    autoencoder. The contents of images can be easily identified and are almost identical
    to the original ones. The reason is that the convolutional layers can maintain
    the spatial structure between nearby pixels of the image. This is not possible
    with dense layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**4th row:** Represents the original images added for comparison purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolutional autoencoders outperform feedforward autoencoders in image denoising
    as the convolutional layers in convolutional autoencoders can maintain the spatial
    structure between nearby pixels of the images — by author
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is the end of today’s article.
  prefs: []
  type: TYPE_NORMAL
- en: '**Please let me know if you’ve any questions or feedback.**'
  prefs: []
  type: TYPE_NORMAL
- en: How about an AI course?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[![](../Images/25706837376c0842cfc45cb6237dd72d.png)](https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75)'
  prefs: []
  type: TYPE_NORMAL
- en: (Screenshot by author)
  prefs: []
  type: TYPE_NORMAL
- en: Support me as a writer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*I hope you enjoyed reading this article. If you’d like to support me as a
    writer, kindly consider* [***signing up for a membership***](https://rukshanpramoditha.medium.com/membership)
    *to get unlimited access to Medium. It only costs $5 per month and I will receive
    a portion of your membership fee.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://rukshanpramoditha.medium.com/membership?source=post_page-----2fe2e9aed71d--------------------------------)
    [## Join Medium with my referral link - Rukshan Pramoditha'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Rukshan Pramoditha (and thousands of other writers on
    Medium). Your membership fee directly…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: rukshanpramoditha.medium.com](https://rukshanpramoditha.medium.com/membership?source=post_page-----2fe2e9aed71d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Join my private list of emails
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Never miss a great story from me again. By* [***subscribing to my email list***](https://rukshanpramoditha.medium.com/subscribe)*,
    you will directly receive my stories as soon as I publish them.*'
  prefs: []
  type: TYPE_NORMAL
- en: Thank you so much for your continuous support! See you in the next article.
    Happy learning to everyone!
  prefs: []
  type: TYPE_NORMAL
- en: MNIST dataset info
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Citation:** Deng, L., 2012\. The mnist database of handwritten digit images
    for machine learning research. **IEEE Signal Processing Magazine**, 29(6), pp.
    141–142.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Source:** [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**License:** *Yann LeCun* (Courant Institute, NYU) and *Corinna Cortes* (Google
    Labs, New York) hold the copyright of the MNIST dataset which is available under
    the *Creative Commons Attribution-ShareAlike 4.0 International License* ([**CC
    BY-SA**](https://creativecommons.org/licenses/by-sa/4.0/)). You can learn more
    about different dataset license types [here](https://rukshanpramoditha.medium.com/dataset-and-software-license-types-you-need-to-consider-d20965ca43dc#6ade).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Rukshan Pramoditha](https://medium.com/u/f90a3bb1d400?source=post_page-----2fe2e9aed71d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: '**2023–01–24**'
  prefs: []
  type: TYPE_NORMAL
