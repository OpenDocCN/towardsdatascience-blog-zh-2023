- en: Beyond NeRFs (Part Two)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越 NeRFs（第二部分）
- en: 原文：[https://towardsdatascience.com/beyond-nerfs-part-two-83af2bd17d0](https://towardsdatascience.com/beyond-nerfs-part-two-83af2bd17d0)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/beyond-nerfs-part-two-83af2bd17d0](https://towardsdatascience.com/beyond-nerfs-part-two-83af2bd17d0)
- en: Tips and tricks for successfully using NeRFs in the wild
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成功使用 NeRFs 的技巧和窍门
- en: '[](https://wolfecameron.medium.com/?source=post_page-----83af2bd17d0--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----83af2bd17d0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----83af2bd17d0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----83af2bd17d0--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----83af2bd17d0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----83af2bd17d0--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----83af2bd17d0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----83af2bd17d0--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----83af2bd17d0--------------------------------)
    [Cameron R. Wolfe 博士](https://wolfecameron.medium.com/?source=post_page-----83af2bd17d0--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----83af2bd17d0--------------------------------)
    ·16 min read·Jun 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [数据科学前沿](https://towardsdatascience.com/?source=post_page-----83af2bd17d0--------------------------------)
    ·阅读时长 16 分钟·2023年6月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/84d3b26f3db9121dd04bc299b88a98e0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84d3b26f3db9121dd04bc299b88a98e0.png)'
- en: (Photo by [Ashim D’Silva](https://unsplash.com/@randomlies?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/wild-west?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （照片由 [Ashim D’Silva](https://unsplash.com/@randomlies?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/s/photos/wild-west?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)）
- en: In the domain of representing and rendering 3D scenes, neural radiance fields
    (NeRFs) provided a massive breakthrough in accuracy. Given several images of an
    underlying scene, NeRFs can reconstruct high-resolution, 2D renderings of this
    scene from arbitrary viewpoints. Compared to prior techniques like [local light
    field fusion (LLFF)](https://cameronrwolfe.substack.com/p/local-light-field-fusion)
    [5] and [scene representation networks (SRNs)](https://cameronrwolfe.substack.com/p/scene-representation-networks)
    [6], NeRFs are far more capable of capturing complex components of a scene’s appearance
    and geometry (e.g., view-dependent reflections and intricate materials).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在 3D 场景的表示和渲染领域，神经辐射场（NeRFs）在准确性上取得了巨大的突破。给定几个基础场景的图像，NeRFs 可以从任意视角重建高分辨率的 2D
    渲染图像。与先前的技术如 [局部光场融合（LLFF）](https://cameronrwolfe.substack.com/p/local-light-field-fusion)
    [5] 和 [场景表示网络（SRNs）](https://cameronrwolfe.substack.com/p/scene-representation-networks)
    [6] 相比，NeRFs 更能够捕捉场景外观和几何结构的复杂组件（例如，视角依赖的反射和复杂的材料）。
- en: NeRFs have the potential to revolutionize applications like virtual reality,
    computer graphics, and more. One could imagine, for example, using NeRFs to reconstruct
    3D renderings of a house that is for sale given images of the house that are available
    online, or even designing video game environments using NeRFs trained on real-world
    scenes. In their original formulation, however, NeRFs were mostly evaluated using
    images captured in simple, controlled environments. When trained over images of
    real-world scenes, NeRFs tend to not perform as well (see below), making them
    less useful in practical applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: NeRFs 具有颠覆虚拟现实、计算机图形学等应用的潜力。例如，可以设想使用 NeRFs 来重建正在出售的房子的 3D 渲染图像，前提是提供了该房子的在线图像，甚至可以使用基于现实场景训练的
    NeRFs 设计视频游戏环境。然而，在其原始形式中，NeRFs 大多是在简单、受控的环境中进行评估的。当对现实世界场景的图像进行训练时，NeRFs 的表现往往不如预期（见下文），这使得它们在实际应用中的实用性降低。
- en: '![](../Images/c8ab7a99a383caaa1cf72a5ae255c55a.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8ab7a99a383caaa1cf72a5ae255c55a.png)'
- en: (from [2])
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [2]）
- en: Within this overview, we will study NeRFs in depth to better understand why
    they perform poorly in the real world and how this problem can be solved. In particular,
    we will explore a few recent proposals, called NeRF-W [1] and def-NeRF [2], that
    modify NeRFs to better handle images that are captured in uncontrolled, noisy
    environments. Such techniques make NeRFs much more useful by enabling their application
    over images that more closely match data that will be encountered in most practical
    applications.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本概述中，我们将深入研究 NeRF，以更好地理解它们在现实世界中表现不佳的原因以及如何解决这个问题。特别是，我们将探讨一些最近的提议，如 NeRF-W
    [1] 和 def-NeRF [2]，这些提议修改了 NeRF，以更好地处理在不受控的噪声环境中拍摄的图像。这些技术通过使 NeRF 能够应用于与大多数实际应用中遇到的数据更接近的图像，从而使
    NeRF 更加有用。
- en: '![](../Images/6a47569552b85a66235aa90fde0ae29c.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a47569552b85a66235aa90fde0ae29c.png)'
- en: (from [1, 2])
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1, 2]）
- en: Background
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: This overview is part of our series on deep learning for 3D shapes and scenes.
    If you haven’t already, I recommend reading the [prior posts](https://cameronrwolfe.substack.com/i/100102968/related-posts)
    in this series, as they contain a lot of useful background information on NeRFs
    and related techniques. Here, we will briefly overview NeRFs and a few other relevant
    concepts (e.g., latent spaces, non-rigid deformations, positional encoding, etc.)
    that will arise in our discussion of NeRF-W [1] and def-NeRF [2].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本概述是我们关于 3D 形状和场景的深度学习系列的一部分。如果你还没有阅读过，我建议阅读这个系列中的 [先前帖子](https://cameronrwolfe.substack.com/i/100102968/related-posts)，因为它们包含了大量关于
    NeRF 和相关技术的有用背景信息。在这里，我们将简要概述 NeRF 和一些其他相关概念（例如，潜在空间、非刚性变形、位置编码等），这些概念将在我们讨论 NeRF-W
    [1] 和 def-NeRF [2] 时出现。
- en: A Brief Overview of NeRFs
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NeRF 的简要概述
- en: In a prior overview, we have already discussed the idea of neural radiance fields
    (NeRFS) [3] in depth. Given that this overview explores extending and modifying
    NeRFs for real-world applications, I recommend reading the overview of NeRFs [here](https://cameronrwolfe.substack.com/p/understanding-nerfs).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的概述中，我们已经深入讨论了神经辐射场（NeRFS）[3] 的概念。鉴于本概述探讨了扩展和修改 NeRF 以用于现实世界应用，我建议阅读 NeRF
    的概述 [这里](https://cameronrwolfe.substack.com/p/understanding-nerfs)。
- en: '**quick overview.** To re-hash the basic idea behind NeRFs, they are just [feed-forward
    neural networks](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)
    that take a 3D coordinate and viewing direction as input and produce a volume
    density and RGB color as output. By evaluating the NeRF at a variety of different
    points (and viewing directions) in 3D space, we can accumulate a lot of information
    about a scene’s geometry and appearance, which can be used to render an image
    (or view) of that scene; see below.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**快速概览。** 要重述 NeRF 的基本理念，它们只是 [前馈神经网络](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)，接受
    3D 坐标和视角方向作为输入，并生成体积密度和 RGB 颜色作为输出。通过在 3D 空间的不同点（和视角方向）上评估 NeRF，我们可以积累大量关于场景几何和外观的信息，这些信息可以用来渲染该场景的图像（或视图）；见下文。'
- en: '![](../Images/88182a1c4ca8fc497e64aea572d9496c.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88182a1c4ca8fc497e64aea572d9496c.png)'
- en: (from [3])
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [3]）
- en: To train a NeRF, we simply need to accumulate several images of a scene and
    relevant [camera pose information](https://cameronrwolfe.substack.com/i/97472888/background)
    for each image. Then, we can use these images as a target to train our NeRF! In
    particular, we repeatedly *i)* use the NeRF to render an image at a known viewpoint
    and *ii)* compare the NeRF’s output to the actual image using a photometric loss
    function (i.e., this just measures differences between RGB pixel values); see
    below.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练 NeRF，我们只需积累几个场景的图像和每张图像的相关 [相机姿态信息](https://cameronrwolfe.substack.com/i/97472888/background)。然后，我们可以使用这些图像作为目标来训练我们的
    NeRF！特别是，我们反复 *i)* 使用 NeRF 在已知视点处渲染图像，*ii)* 使用光度损失函数（即测量 RGB 像素值之间的差异）将 NeRF 的输出与实际图像进行比较；见下文。
- en: '![](../Images/6c82a1fb388d13c6afc9f4c656ef5a77.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c82a1fb388d13c6afc9f4c656ef5a77.png)'
- en: (from [3])
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [3]）
- en: '**problems with NeRFs.** NeRFs were a massive breakthrough in the field of
    3D scene representation, but they have some limitations. In a [prior overview](https://cameronrwolfe.substack.com/p/beyond-nerfs-part-one),
    we discussed the computational burden of training and rendering with NeRFs, as
    well as their need for many images of an underlying scene for training. However,
    techniques like InstantNGP [7] and PixelNeRF [8] drastically improve the computational
    and sample efficiency of NeRFs.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**NeRFs的问题**。NeRFs在3D场景表示领域是一个重大突破，但它们也有一些局限性。在一个[前期概述](https://cameronrwolfe.substack.com/p/beyond-nerfs-part-one)中，我们讨论了训练和渲染NeRFs的计算负担，以及它们对场景的多张图片的需求。然而，像InstantNGP
    [7]和PixelNeRF [8]这样的技术大幅提高了NeRFs的计算和样本效率。'
- en: Going further, NeRFs make the assumption that scenes are static. In practice,
    this assumption is oftentimes not true. Images may contain moving objects (e.g.,
    people) that occlude relevant portions of the scene or even be taken at different
    times of day (e.g., at night or in the morning). These are transient components
    of a scene that may be present in one image but not another.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 更进一步，NeRFs假设场景是静态的。实际上，这一假设往往不成立。图像可能包含移动的物体（例如人），这些物体会遮挡场景中的相关部分，甚至可能是在一天的不同时间拍摄的（例如在晚上或早晨）。这些是场景的瞬时成分，可能在一张图像中存在，而在另一张图像中不存在。
- en: “The central limitation of NeRF … is its assumption that the world is geometrically,
    materially, and photometrically static. NeRF requires that any two photographs
    taken at the same position and orientation must be identical. This assumption
    is violated in many real-world datasets.” *— from [1]*
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “NeRF的中心限制……是它假设世界在几何、材料和光度上都是静态的。NeRF要求任何在相同位置和方向下拍摄的两张照片必须是相同的。这一假设在许多现实世界的数据集中是被违背的。”
    *— 引自 [1]*
- en: This static assumption is a large factor that underlies the poor performance
    of NeRF in uncontrolled environments. Within this overview, we will explore how
    this assumption can be mitigated, allowing NeRFs to be trained over imperfect,
    real-world datasets that we encounter in practical applications.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这一静态假设是NeRF在不受控制的环境中表现不佳的一个重要因素。在本概述中，我们将探讨如何减轻这一假设，使NeRFs能够在我们在实际应用中遇到的不完美现实世界数据集上进行训练。
- en: Primer on Shape Deformation
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 形状变形入门
- en: To successfully train NeRFs on noisy smart phone images, recent techniques augment
    NeRFs with learnable deformation fields. To understand what this means, however,
    we need to learn about deformations in general. We will briefly cover this idea
    below.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了成功地在嘈杂的智能手机图像上训练NeRFs，最近的技术将NeRFs与可学习的变形场结合在一起。然而，要理解这意味着什么，我们需要了解一般的变形。我们将简要介绍这一概念。
- en: 'Put simply, a deformation describes a transformation of an initial geometry
    into a final geometry (e.g., by displacing, translating, or morphing points relative
    to some [reference frame](https://isaacphysics.org/concepts/cp_frame_reference?stage=all)).
    There are two basic types of deformations that we will typically encounter:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，变形描述了初始几何形状到最终几何形状的转变（例如，通过相对于某个[参考系](https://isaacphysics.org/concepts/cp_frame_reference?stage=all)的位移、平移或形态变换）。我们通常会遇到两种基本类型的变形：
- en: Rigid deformation
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 刚性变形
- en: Non-rigid deformation
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 非刚性变形
- en: For rigid deformations (e.g., rotations and translations), the object being
    deformed changes with respect to an external frame of reference but remains unchanged
    with respect to an internal frame of reference. Examples are provided in the image
    below.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于刚性变形（例如旋转和平移），变形的对象相对于外部参考系发生变化，但相对于内部参考系则保持不变。下面的图片中提供了相关示例。
- en: '![](../Images/5ad25b320d5edc3f1aa50a826c5d13ce.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ad25b320d5edc3f1aa50a826c5d13ce.png)'
- en: Examples of rigid deformations (from sci.sdsu.edu)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 刚性变形的示例（来自 sci.sdsu.edu）
- en: Non-rigid deformations are slightly different. Objects are changed with respect
    to both internal and external frames of reference. Thus, non-rigid deformations
    can capture transformations like dilation and shearing; see below.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 非刚性变形略有不同。物体相对于内部和外部参考系都会发生变化。因此，非刚性变形可以捕捉到像膨胀和剪切这样的变换；见下文。
- en: '![](../Images/ba37d08698e69bd16c1a871ed046cf91.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba37d08698e69bd16c1a871ed046cf91.png)'
- en: Examples of non-rigid deformations (from sci.sdsu.edu)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 非刚性变形的示例（来自 sci.sdsu.edu）
- en: '**deformation fields.** A deformation field is one way of representing deformations.
    It simply defines a transformation via a mapping of points in 3D space (i.e.,
    each point in space is mapped to a new point). By repositioning/transforming an
    object based on the mapping defined by this field, we can arbitrarily transform
    the the shape of an object, similar to the deformations shown above.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**变形场。** 变形场是一种表示变形的方法。它通过对3D空间中的点进行映射来定义一个变换（即，每个空间中的点被映射到一个新点）。通过根据该场定义的映射重新定位/变换物体，我们可以任意变换物体的形状，类似于上面显示的变形。'
- en: Other Resources
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他资源
- en: 'Beyond the discussion above, there are a few concepts that might provide a
    deeper understanding of the content in this post. Check out the links below to
    relevant resources:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述讨论，还有一些概念可能会提供对本文内容的更深刻理解。请查看下面的链接以获取相关资源：
- en: What is the latent space? [[link](https://www.baeldung.com/cs/dl-latent-space)]
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潜在空间是什么？ [[link](https://www.baeldung.com/cs/dl-latent-space)]
- en: Introduction to Positional Encoding [[link](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)]
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置编码简介 [[link](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)]
- en: Volume Rendering [[link](https://www.heavy.ai/technical-glossary/volume-rendering)]
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 体积渲染 [[link](https://www.heavy.ai/technical-glossary/volume-rendering)]
- en: Publications
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发表物
- en: Although NeRFs are effective in controlled environments, they struggle to render
    3D scenes from images captured in the real world. Here, we will overview two recently
    proposed methods, called NeRF-W [1] and def-NeRF [2], that try to solve this issue.
    These methods can render accurate 3D scenes from sets of photos that are imperfectly
    captured (e.g., on a mobile phone) and even contain drastic illumination changes
    or occluding objects!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管NeRF在受控环境中效果良好，但它们在渲染从现实世界中捕获的图像的3D场景时遇到困难。在这里，我们将概述两种最近提出的方法，称为NeRF-W [1]和def-NeRF
    [2]，它们试图解决这一问题。这些方法可以从一组捕捉不完美的照片（例如，手机拍摄）中渲染出准确的3D场景，甚至包含剧烈的光照变化或遮挡物体！
- en: '[NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections](https://arxiv.org/abs/2008.02268)
    [1]'
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections](https://arxiv.org/abs/2008.02268)
    [1]'
- en: '![](../Images/a971ab584fb7f06926f532d3fe104ebd.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a971ab584fb7f06926f532d3fe104ebd.png)'
- en: (from [1])
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: （来源 [1]）
- en: Real-world images oftentimes have a lot of undesirable properties that make
    the training of NeRFs quite difficult. Consider, for example, trying to train
    a NeRF over several images of a major landmark that were taken years apart from
    each other; see above. The images of this scene may be taken at different times
    of night or day and contain any number of moving people or objects that are not
    actually part of the scene’s geometry!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界中的图像往往具有许多不希望出现的特性，这使得训练NeRF变得相当困难。例如，考虑尝试训练一个NeRF，使用几年前拍摄的多个重要地标图像；见上图。这些场景的图像可能在不同的时间（夜晚或白天）拍摄，并且包含任何数量的移动人员或物体，这些人员或物体实际上并不是场景几何的一部分！
- en: In uncontrolled scenarios, NeRFs tend to fail due to their assumption that scenes
    are static, thus preventing their use in real-world applications. NeRF-W [1] —
    an extension to NeRF — mitigates these problems by relaxing the static assumption
    made by NeRF and allowing accurate modeling of 3D scenes under common, real-world
    problems (e.g., transient objects and illumination changes).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在不受控制的情况下，由于NeRF的假设是场景是静态的，这使得其在实际应用中往往失败。NeRF-W [1] —— NeRF的扩展 —— 通过放宽NeRF所做的静态假设来缓解这些问题，从而允许在常见的现实问题（例如瞬态对象和光照变化）下准确建模3D场景。
- en: '![](../Images/4191703746567fac7b8b3418b6f9532c.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4191703746567fac7b8b3418b6f9532c.png)'
- en: (from [1])
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: （来源 [1]）
- en: '**decomposing a scene.** Major problems encountered by NeRFs in the wild can
    be loosely categorized as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**分解场景。** NeRF在实际应用中遇到的主要问题可以大致分类如下：'
- en: '*Photometric changes:* time of day and atmospheric conditions impact the illumination/radiance
    of a scene.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*光度变化：* 一天中的时间和大气条件影响场景的光照/辐射。'
- en: '*Transient objects:* real-world scenes are rarely captured in isolation. There
    are usually people or objects that occlude and move through the scene as images
    are taken.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*瞬态对象：* 现实世界的场景很少被孤立地捕捉。通常会有一些人或物体在拍摄过程中遮挡或移动。'
- en: These problems, both violations to the static assumption, are illustrated in
    the figure above.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图中说明了这些问题，这些问题都是对静态假设的违反。
- en: '**photometric changes.** To address photometric changes, each image is assigned
    its own “appearance” vector, which is considered (as an extra input) by NeRF-W
    when predicting the output RGB color. However, the appearance embedding has no
    impact on the predicted volume density, which captures the 3D geometry of a scene.
    This change is made by just separating NeRF’s feed-forward network into a few
    components that take different inputs; see below.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**光度变化。** 为了解决光度变化问题，每张图片都会被分配一个“外观”向量，NeRF-W在预测输出RGB颜色时会将其视为（额外输入）。然而，外观嵌入对预测的体积密度没有影响，体积密度捕捉了场景的3D几何形状。这个改变仅通过将NeRF的前馈网络分离为几个接受不同输入的组件来实现；详见下文。'
- en: '![](../Images/cc109e5a629cbdeb3af944f0adf5712a.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc109e5a629cbdeb3af944f0adf5712a.png)'
- en: The appearance embedding impacts color but not volume density (from [1]).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 外观嵌入影响颜色但不影响体积密度（来自[1]）。
- en: By conditioning NeRF-W’s RGB output on this appearance embedding, the model
    can vary the appearance of a scene based on a particular image, while ensuring
    that the underlying geometry of the scene is appearance-invariant and shared across
    images. The unique appearance embeddings assigned to each training image are optimized
    alongside model parameters throughout training.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将NeRF-W的RGB输出与此外观嵌入条件化，模型可以基于特定图像改变场景的外观，同时确保场景的基本几何形状对外观是不变的，并且在图像之间共享。分配给每张训练图像的独特外观嵌入在训练过程中与模型参数一起优化。
- en: '**static vs. transient components.** To handle transient objects, we should
    notice that a scene contains two types of entities:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**静态与瞬态组件。** 为了处理瞬态对象，我们应注意场景包含两种类型的实体：'
- en: Image-dependent components (i.e., moving/transient objects)
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像依赖组件（即移动/瞬态对象）
- en: Shared components (i.e., the actual scene)
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享组件（即实际场景）
- en: NeRF-W uses separate feed-forward network components to model image-dependent
    (transient) and shared (static) scene components. Both transient and static portions
    of the network output their own color and density estimations, which allows NeRF-W
    to disentangle static and transient components of a scene; see below.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: NeRF-W使用独立的前馈网络组件来建模图像依赖（瞬态）和共享（静态）场景组件。网络的瞬态部分和静态部分分别输出它们自己的颜色和密度估计，这使得NeRF-W能够分离场景的静态和瞬态组件；详见下文。
- en: '![](../Images/b05768b6cdbb48d70989b5dc18e66737.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b05768b6cdbb48d70989b5dc18e66737.png)'
- en: Static and transient components of NeRF-W (from [1])
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: NeRF-W的静态和瞬态组件（来自[1]）
- en: The transient portion of NeRF-W emits an uncertainty field (using a Bayesian
    learning framework [4]) that allows occluded scene components to be ignored during
    training. To ensure that transient effects on the scene are image-dependent, each
    training image is associated with a “transient” embedding vector, which is given
    as input to the transient component of NeRF-W. Similar to appearance embeddings,
    transient embeddings are learned during training. See below for a full depiction
    of the NeRF-W architecture.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: NeRF-W的瞬态部分发出一个不确定性场（使用贝叶斯学习框架[4]），允许在训练过程中忽略被遮挡的场景组件。为了确保瞬态效果依赖于图像，每张训练图像都与一个“瞬态”嵌入向量相关联，该向量作为输入提供给NeRF-W的瞬态组件。与外观嵌入类似，瞬态嵌入在训练过程中被学习。见下文以获得NeRF-W架构的完整描述。
- en: '![](../Images/de0532dd247f1137f1dce665d9ad52c0.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de0532dd247f1137f1dce665d9ad52c0.png)'
- en: (from [1])
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[1]）
- en: All components of NeRF-W are jointly optimized using a procedure similar to
    NeRF [3], as described in the link [here](https://cameronrwolfe.substack.com/p/understanding-nerfs#%C2%A7modeling-nerfs).
    NeRF-W is evaluated using real-world photo collections of notable landmarks, selected
    from the [Photo Tourism dataset](http://phototour.cs.washington.edu/datasets/).
    When NeRF-W is trained to represent six landmarks, we see that NeRF-W outperforms
    baselines quantitatively in most cases; see below.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: NeRF-W的所有组件都通过类似于NeRF [3]的程序进行联合优化，具体描述可以在[此处](https://cameronrwolfe.substack.com/p/understanding-nerfs#%C2%A7modeling-nerfs)找到。NeRF-W使用真实世界中著名地标的照片集合进行评估，这些照片从[Photo
    Tourism dataset](http://phototour.cs.washington.edu/datasets/)中选取。当NeRF-W被训练以表示六个地标时，我们看到NeRF-W在大多数情况下在定量上优于基线；详见下文。
- en: '![](../Images/05e119f300b13901ef597ec64a871e49.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05e119f300b13901ef597ec64a871e49.png)'
- en: (from [1])
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[1]）
- en: 'We should recall that, to perform evaluation, we:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该回顾一下，为了进行评估，我们：
- en: Train a model over images corresponding to a single scene.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在对应于单个场景的图像上训练模型。
- en: Sample a hold-out test image (and corresponding camera pose).
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 采样一个保留测试图像（及其对应的相机姿态）。
- en: Render a viewpoint (with the trained model) using the camera pose information
    from the hold-out image.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用来自保留图像的相机姿态信息渲染一个视点（使用训练好的模型）。
- en: Compare the rendering to the ground truth image.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将渲染图像与真实图像进行比较。
- en: For NeRF-W, we do not have appearance or transient embeddings for test images.
    As a result, NeRF-W optimizes these embeddings based on one half of the test image
    and performs evaluation on the other half of the image; see below.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NeRF-W，我们没有测试图像的外观或瞬态嵌入。因此，NeRF-W基于测试图像的一半优化这些嵌入，并在另一半图像上进行评估；见下文。
- en: '![](../Images/a8188c9c47ad277928edd82795bbbe81.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8188c9c47ad277928edd82795bbbe81.png)'
- en: (from [1])
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[1]）
- en: When we examine the output of different NeRF variants, we see NeRF output tends
    to contain ghosting artifacts due to transient objects in the training images.
    In comparison, NeRF-W produces sharp and accurate renderings, indicating that
    is more capable of handling real-world variation in scene appearance; see below.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们检查不同NeRF变体的输出时，我们发现NeRF的输出往往包含由于训练图像中的瞬态物体而产生的鬼影伪影。相比之下，NeRF-W生成的渲染图像清晰准确，表明它更能处理现实世界中场景外观的变化；见下文。
- en: '![](../Images/877a1093e345c693798e829bde25e1c4.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/877a1093e345c693798e829bde25e1c4.png)'
- en: (from [1])
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[1]）
- en: Plus, NeRF-W can produce accurate scene renderings given training images with
    different lighting conditions. Given that NeRF-W can generate output given different
    appearance embeddings as input, we can tweak NeRF-W’s appearance embedding to
    modify the appearance of the final rendering; see below.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，NeRF-W可以根据不同光照条件的训练图像生成准确的场景渲染。考虑到NeRF-W能够根据不同的外观嵌入生成输出，我们可以调整NeRF-W的外观嵌入以修改最终渲染的外观；见下文。
- en: '![](../Images/a37f6b3eeed2bda8022f135660f8cbd9.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a37f6b3eeed2bda8022f135660f8cbd9.png)'
- en: (from [1])
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[1]）
- en: Taking this idea a step further, we can even interpolate between appearance
    embeddings of different training images, yielding a smooth change in rendered
    scene appearance; see below.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步推进这一理念，我们甚至可以在不同训练图像的外观嵌入之间进行插值，从而实现渲染场景外观的平滑变化；见下文。
- en: '![](../Images/5068204217d0160271cc5474393b961e.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5068204217d0160271cc5474393b961e.png)'
- en: (from [1])
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[1]）
- en: '[Nerfies: Deformable Neural Radiance Fields](https://arxiv.org/abs/2011.12948)
    [2]'
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[Nerfies: 变形神经辐射场](https://arxiv.org/abs/2011.12948) [2]'
- en: '![](../Images/381faf50b33a9ad8fd0d8b09484bda3c.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/381faf50b33a9ad8fd0d8b09484bda3c.png)'
- en: (from [2])
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[2]）
- en: 'Most computer vision data for modern applications is captured on a smart phone.
    With this in mind, one might wonder whether it’s possible to train a NeRF using
    this data. In [2], authors explore a specific application along these lines: *converting
    casually captured “selfie” images/videos into a NeRF that can generates photorealistic
    renderings of a subject/person*. The authors call these models “Nerfies” (i.e.,
    a NeRF-based selfie)!'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现代应用的大多数计算机视觉数据都是通过智能手机捕捉的。考虑到这一点，人们可能会想是否可以使用这些数据训练NeRF。在[2]中，作者探讨了沿这些思路的具体应用：*将随意捕捉的“自拍”图像/视频转换为能够生成真实感渲染的NeRF*。作者将这些模型称为“NeRFies”（即基于NeRF的自拍）！
- en: 'Initially, this application may seem pretty specific and useless. *Do we really
    care this much about tweaking the viewing angle of our selfie? How much more aesthetic
    can this make our instagram posts?* However, the methodology proposed in [2] is
    incredibly insightful for a few reasons:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，这个应用可能看起来相当具体且无用。*我们真的那么在意调整自拍角度吗？这能让我们的Instagram帖子更具美感吗？* 然而，[2]中提出的方法在几个方面都非常有洞察力：
- en: It gives us an idea of the feasibility of training NeRFs using images and videos
    from a smart phone.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它让我们了解了使用智能手机图像和视频训练NeRF的可行性。
- en: It improves the ability of NeRFs to handle challenging, detailed, or imperfectly
    captured materials in a scene.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它提高了NeRF处理场景中具有挑战性、细节丰富或捕捉不完美材料的能力。
- en: It is not just applicable to capturing self-portraits, but can also be applied
    to more general scene modeling applications.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它不仅适用于捕捉自画像，还可以应用于更一般的场景建模应用。
- en: Using techniques proposed in [2], we can produce high-quality scene representations
    given noisy, imperfect images captured on a mobile phone. As an example of how
    such a technique can be used, imagine generating a 3D model of yourself by just
    taking a quick video on your phone. Current approaches for this require entire,
    specialized labs with synchronized lights and cameras!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 [2] 中提出的技术，我们可以在给定噪声和不完美的手机拍摄图像的情况下生成高质量的场景表示。举个例子，想象一下仅通过在手机上拍摄一个快速视频来生成你自己的
    3D 模型。目前的相关方法需要整个专门的实验室，配备同步的灯光和相机！
- en: '**NeRFs + deformation fields.** When we think about using a hand-held camera
    to build a 3D model of a person, there are a few difficulties that might come
    to mind:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**NeRFs + 变形场**。当我们考虑使用手持相机来构建一个人的 3D 模型时，会想到一些可能的困难：'
- en: The camera will move (this violates the static assumption!).
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相机将会移动（这违反了静态假设！）。
- en: Humans contain a lot of complex geometries and materials that are difficult
    to model (e.g., hair, glasses, jewelry, etc.).
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类包含许多复杂的几何形状和材料，这些是难以建模的（例如，头发、眼镜、珠宝等）。
- en: In [2], authors address these challenges by augmenting NeRFs with a jointly
    optimized, non-rigid deformation field, which learns to transform the scene’s
    underlying geometry in 3D space.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [2] 中，作者通过用一个共同优化的非刚性变形场来增强 NeRF，解决了这些挑战，该变形场学习在 3D 空间中变换场景的底层几何形状。
- en: '![](../Images/43b46e16aba5b3137524ed0e71a3fac4.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43b46e16aba5b3137524ed0e71a3fac4.png)'
- en: Transforming coordinates with a deformation field (from [2])
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用变形场变换坐标（来自 [2]）
- en: This deformation field is modeled with a [feed-forward network](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)
    that takes a positionally-encoded, 3D coordinate and per-image latent deformation
    code as input, then produces a non-rigidly deformed 3D coordinate as output; see
    above.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这个变形场通过一个 [前馈网络](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)建模，该网络接受位置编码的
    3D 坐标和每张图像的潜在变形代码作为输入，然后生成一个非刚性变形的 3D 坐标作为输出；见上文。
- en: '**how def-NeRF works.** The methodology in [2], which we’ll call a Deformable
    Neural Radiance Field (def-NeRF), has two components:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**def-NeRF 的工作原理**。在 [2] 中的方法论，我们称之为可变形神经辐射场（def-NeRF），包含两个组件：'
- en: '*Deformation field*: models a non-rigid deformation of 3D coordinates using
    a feed-forward neural network.'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*变形场*：使用前馈神经网络建模 3D 坐标的非刚性变形。'
- en: '*NeRF*: uses a vanilla NeRF architecture to create a template of the underlying
    scene’s geometry and appearance.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*NeRF*：使用原始的 NeRF 架构来创建底层场景几何形状和外观的模板。'
- en: We associate each training image with learnable deformation and appearance vectors.
    These latent codes, which mimic the per-image embedding approach used by NeRF-W
    [1], allow deformation and appearance to be image-dependent and enable def-NeRF
    to handle variance in images of scene (e.g., illumination changes).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每张训练图像与可学习的变形和外观向量关联。这些潜在代码模拟了 NeRF-W [1] 中使用的每图像嵌入方法，使得变形和外观依赖于图像，从而使 def-NeRF
    能够处理场景图像中的变异（例如，光照变化）。
- en: '![](../Images/e7e47a6a32e3b7bb8fa4d99697e79f07.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7e47a6a32e3b7bb8fa4d99697e79f07.png)'
- en: (from [2])
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [2])
- en: def-NeRF takes a 3D coordinate as input. This coordinate is positionally-encoded
    and combined with the latent deformation code (via addition) before being passed
    into the feed-forward network that models def-NeRF’s deformation field. The output
    of this network is a transformed 3D coordinate; see above.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: def-NeRF 接受 3D 坐标作为输入。这个坐标经过位置编码，并与潜在的变形代码（通过加法）结合，然后传递给建模 def-NeRF 变形场的前馈网络。这个网络的输出是一个变换后的
    3D 坐标；见上文。
- en: '![](../Images/0b3ff1c04ef38a092a23103806589d1f.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b3ff1c04ef38a092a23103806589d1f.png)'
- en: (from [2])
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [2])
- en: This transformed coordinate is passed as input to the NeRF. Similar to NeRF-W
    [1], we augment this NeRF with a per-image, learnable appearance vector. Given
    the transformed coordinate, viewing direction, and an appearance vector as input,
    the NeRF produces a volume density and RGB color as output; see above.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这个变换后的坐标作为输入传递给 NeRF。类似于 NeRF-W [1]，我们用一个每张图像的、可学习的外观向量来增强这个 NeRF。给定变换后的坐标、观察方向和一个外观向量作为输入，NeRF
    输出一个体积密度和 RGB 颜色；见上文。
- en: '![](../Images/c3d8049d21e17a25908d61ba4e5d8c0b.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3d8049d21e17a25908d61ba4e5d8c0b.png)'
- en: (from [2])
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [2])
- en: 'The full def-NeRF architecture, illustrated above, shares a nearly identical
    [architecture and training strategy](https://cameronrwolfe.substack.com/i/97915766/modeling-nerfs)
    compared to vanilla NeRFs. The main differences are:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 上述的完整 def-NeRF 架构与原始 NeRF 的 [架构和训练策略](https://cameronrwolfe.substack.com/i/97915766/modeling-nerfs)
    几乎相同。主要区别是：
- en: The modeling of a deformation field.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变形场的建模。
- en: The use of per-image deformation and appearance vectors.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每图像变形和外观向量的使用。
- en: “When rendering, we simply cast rays and sample points in the observation frame
    and then use the deformation field to map the sampled points to the template.”
    *— from [2]*
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “在渲染时，我们简单地在观察帧中投射光线并采样点，然后使用变形场将采样点映射到模板。” *— 来自 [2]*
- en: '![](../Images/70ebd9f6d2ede2b208953ad5b62e43a1.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70ebd9f6d2ede2b208953ad5b62e43a1.png)'
- en: (from [2])
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [2]）
- en: '**why is this necessary?** def-NeRF simply adds a deformation field that non-rigidly
    deforms input coordinates to the primary NeRF architecture. As a result, this
    approach decomposes scene representations into two parts:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么这是必要的？** def-NeRF 仅在主要的 NeRF 架构上添加了一个变形场，该变形场以非刚性方式变形输入坐标。因此，这种方法将场景表示分解为两部分：'
- en: A geometric model of the scene.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 场景的几何模型。
- en: A deformation of this geometry into the desired viewpoint.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这种几何形状变形为期望的视角。
- en: As such, def-NeRFs relax the static assumption of NeRFs and allow the underlying
    scene geometry to be learned in a manner that is invariant to shifts, translations,
    viewpoint changes, and more.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，def-NeRF 解除 NeRF 的静态假设，允许在对位移、平移、视角变化等不变的情况下学习基础场景几何。
- en: '![](../Images/23cb63f6742ecee5901df78b76de65d2.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23cb63f6742ecee5901df78b76de65d2.png)'
- en: Added regularization improves reconstruction quality (from [2])
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 添加的正则化提高了重建质量（来自 [2]）
- en: '**regularization.** Authors in [2] observe that learned deformation fields
    are prone to local minima and overfitting. As a solution, we can add extra regularization
    to the optimization process of def-NeRF; see above. Several different regularization
    schemes are adopted, as described in Section 3.3–3.5 of [2].'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则化。** [2] 中的作者观察到学习到的变形场容易陷入局部最小值和过拟合。作为解决方案，我们可以在 def-NeRF 的优化过程中添加额外的正则化；详见上述。采用了几种不同的正则化方案，如
    [2] 第 3.3–3.5 节所述。'
- en: '**does it work well?** def-NeRF is primarily evaluated based on its ability
    to produce “Nerfies” (i.e., photorealistic renderings of a person/subject from
    arbitrary viewpoints). To create a Nerfie, a user films their face using a smart
    phone for about 20 seconds. Then, the def-NeRF methodology is trained over this
    data and used to render selfies from various, novel viewpoints.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**效果好吗？** def-NeRF 主要基于其生成 “Nerfies” （即从任意视角拍摄的逼真渲染图）的能力进行评估。为了创建 Nerfie，用户使用智能手机拍摄他们的脸大约
    20 秒。然后，def-NeRF 方法在这些数据上进行训练，并用于从各种新颖的视角渲染自拍。'
- en: '![](../Images/f3470aa24b0d545ac5f92eb492c416db.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3470aa24b0d545ac5f92eb492c416db.png)'
- en: (from [2])
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [2]）
- en: To evaluate the quality of these scene reconstructions from novel viewpoints,
    the authors construct a camera rig that simultaneously captures a subject from
    multiple viewpoints. This allows a validation set to be constructed using images
    that capture the same exact scene from two different viewpoints; see above.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估从新视角生成的这些场景重建的质量，作者构建了一个相机装置，同时从多个视角捕捉对象。这允许使用捕捉同一确切场景的两种不同视角的图像构建验证集；详见上述。
- en: '![](../Images/f19e6985a5a71db532ff9c80d6536515.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f19e6985a5a71db532ff9c80d6536515.png)'
- en: (from [2])
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [2]）
- en: When quantitatively compared to various baselines, def-NeRF is found to produce
    higher-quality reconstructions of subjects in most cases. Notably, def-NeRFs seems
    to struggle with the [PSNR metric](https://www.mathworks.com/help/vision/ref/psnr.html).
    However, authors claim that this metric favors blurry images and is not ideal
    for evaluating scene reconstructions.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当与各种基线进行定量比较时，def-NeRF 在大多数情况下能够生成更高质量的对象重建。值得注意的是，def-NeRF 似乎在处理 [PSNR metric](https://www.mathworks.com/help/vision/ref/psnr.html)
    时遇到困难。然而，作者声称该指标偏向模糊图像，不适合评估场景重建。
- en: '![](../Images/2a968d151f94faf7ffdac431d3e7129d.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a968d151f94faf7ffdac431d3e7129d.png)'
- en: (from [2])
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [2]）
- en: Qualitatively, we see that def-NeRF is more capable of capturing fine details
    within a scene (e.g., hair, shirt wrinkles, glasses, etc.) relative to baselines;
    see above. Additionally, the method works well for general scenes that go beyond
    reconstructing human subjects in a Nerfie. Overall, def-NeRF seems to provide
    high-quality scene reconstructions given images from a mobile phone!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 定性地，我们看到相较于基线，def-NeRF 更能够捕捉场景中的细节（例如头发、衬衫褶皱、眼镜等）；见上文。此外，该方法适用于一般场景，超越了在 NeRFie
    中重建人类主体的范围。总体而言，def-NeRF 在给定手机图像的情况下似乎能够提供高质量的场景重建！
- en: '![](../Images/f133741062ab23b6cc955ae6a5f3ded6.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f133741062ab23b6cc955ae6a5f3ded6.png)'
- en: (from [2])
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: （摘自 [2]）
- en: Takeaways
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要收获
- en: Although NeRFs produce impressive demos, they are not truly useful unless we
    can apply them to images encountered in the real world. In this overview, we highlight
    the main reasons that applying NeRFs in the wild is often difficult (i.e., the
    static assumption) and overview some recent research that aims to solve this problem.
    Some of the major takeaways are outlined below.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 NeRFs（神经辐射场）展示了令人印象深刻的演示效果，但除非我们能将它们应用于现实世界中的图像，否则它们的实际用途有限。在本综述中，我们强调了在实际应用中使用
    NeRFs 的主要困难（即静态假设），并概述了一些旨在解决这一问题的近期研究。以下是一些主要的收获。
- en: '**static assumption.** NeRFs, in their original form, assume that scenes are
    static, meaning that two images taken of a scene from the same position/direction
    must be identical. In practice, this assumption rarely holds! People or objects
    may be moving through the scene, and variable lighting conditions can significantly
    change the appearance of an image. Deploying NeRFs in the real world requires
    that this assumption be relaxed significantly.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**静态假设**。NeRFs 在其原始形式中假设场景是静态的，这意味着从相同位置/方向拍摄的两个场景图像必须是相同的。在实际操作中，这一假设很少成立！人或物体可能在场景中移动，而变化的光照条件可以显著改变图像的外观。在现实世界中部署
    NeRFs 需要显著放宽这一假设。'
- en: '**image-dependent embeddings.** Real-world scenes can be separated into image-independent
    and image-dependent components. If we want to learn the underlying geometry of
    a scene without overfitting to image-dependent components, we must customize a
    NeRF’s output on a per-image basis. For both NeRF-W and def-NeRF, this is largely
    accomplished via the addition of per-image embeddings vectors (i.e., appearance,
    transient, and deformation vectors). However, the fact that per-image embedding
    vectors are not available for unseen/test images may make deploying these models
    more difficult.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**图像依赖的嵌入**。现实世界中的场景可以分为图像独立和图像依赖的组件。如果我们希望学习场景的基础几何而不对图像依赖的组件进行过拟合，我们必须根据每张图像定制
    NeRF 的输出。对于 NeRF-W 和 def-NeRF，这主要通过添加每图像嵌入向量（即外观、瞬态和变形向量）来实现。然而，未见过/测试图像的每图像嵌入向量的缺乏可能使得这些模型的部署更加困难。'
- en: '**limitations.** Allowing NeRFs to be applied beyond controlled environments
    is important, but this is not the only limitation of NeRFs! These models still
    suffer from poor sample efficiency and computational complexity, as discussed
    in a [prior post](https://cameronrwolfe.substack.com/p/beyond-nerfs-part-one).
    Making NeRFs viable for real-time applications will require a combination of techniques
    that solve each individual issue faced by NeRFs.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**局限性**。允许 NeRFs 超越受控环境的应用是重要的，但这并不是 NeRFs 唯一的局限性！这些模型仍然面临着样本效率低和计算复杂性高的问题，正如
    [之前的文章](https://cameronrwolfe.substack.com/p/beyond-nerfs-part-one)中讨论的那样。使 NeRFs
    适用于实时应用将需要结合解决 NeRFs 每个个别问题的技术。'
- en: Closing Thoughts
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结语
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/). I study the empirical
    and theoretical foundations of deep learning. You can also check out my [other
    writings](https://medium.com/@wolfecameron) on medium! If you liked it, please
    follow me on [twitter](https://twitter.com/cwolferesearch) or subscribe to my
    [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/), where
    I help readers build a deeper understanding of topics in deep learning research
    via understandable overviews of popular papers.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢您阅读本文。我是 [Cameron R. Wolfe](https://cameronrwolfe.me/)，[Rebuy](https://www.rebuyengine.com/)
    的 AI 主管。我研究深度学习的经验和理论基础。您还可以查看我在 medium 上的 [其他文章](https://medium.com/@wolfecameron)！如果您喜欢这篇文章，请在
    [twitter](https://twitter.com/cwolferesearch) 上关注我或订阅我的 [Deep (Learning) Focus
    新闻通讯](https://cameronrwolfe.substack.com/)，我在其中帮助读者通过对热门论文的通俗概述建立对深度学习研究主题的更深理解。
- en: Bibliography
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Martin-Brualla, Ricardo, et al. “Nerf in the wild: Neural radiance fields
    for unconstrained photo collections.” *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*. 2021.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Martin-Brualla, Ricardo 等. “Nerf in the wild：用于无约束照片集合的神经辐射场。” *IEEE/CVF
    计算机视觉与模式识别会议论文集*。2021。'
- en: '[2] Park, Keunhong, et al. “Nerfies: Deformable neural radiance fields.” *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*. 2021.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Park, Keunhong 等. “Nerfies：可变形神经辐射场。” *IEEE/CVF 国际计算机视觉大会论文集*。2021。'
- en: '[3] Mildenhall, Ben, et al. “Nerf: Representing scenes as neural radiance fields
    for view synthesis.” *Communications of the ACM* 65.1 (2021): 99–106.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Mildenhall, Ben 等. “Nerf：将场景表示为神经辐射场以进行视图合成。” *ACM 通讯* 65.1 (2021): 99–106。'
- en: '[4] Kendall, Alex, and Yarin Gal. “What uncertainties do we need in bayesian
    deep learning for computer vision?.” *Advances in neural information processing
    systems* 30 (2017).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Kendall, Alex 和 Yarin Gal. “在计算机视觉的贝叶斯深度学习中，我们需要哪些不确定性？” *神经信息处理系统进展* 30
    (2017)。'
- en: '[5] Mildenhall, Ben, et al. “Local light field fusion: Practical view synthesis
    with prescriptive sampling guidelines.” *ACM Transactions on Graphics (TOG)* 38.4
    (2019): 1–14.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Mildenhall, Ben 等. “局部光场融合：具有规范化采样指南的实用视图合成。” *ACM 图形学会论文 (TOG)* 38.4 (2019):
    1–14。'
- en: '[6] Sitzmann, Vincent, Michael Zollhöfer, and Gordon Wetzstein. “Scene representation
    networks: Continuous 3d-structure-aware neural scene representations.” *Advances
    in Neural Information Processing Systems* 32 (2019).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Sitzmann, Vincent, Michael Zollhöfer 和 Gordon Wetzstein. “场景表示网络：连续 3D
    结构感知神经场景表示。” *神经信息处理系统进展* 32 (2019)。'
- en: '[7] Müller, Thomas, et al. “Instant neural graphics primitives with a multiresolution
    hash encoding.” *ACM Transactions on Graphics (ToG)* 41.4 (2022): 1–15.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Müller, Thomas 等. “瞬时神经图形原语与多分辨率哈希编码。” *ACM 图形学会论文 (ToG)* 41.4 (2022):
    1–15。'
- en: '[8] Yu, Alex, et al. “pixelnerf: Neural radiance fields from one or few images.”
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    2021.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Yu, Alex 等. “pixelnerf：来自一张或少量图像的神经辐射场。” *IEEE/CVF 计算机视觉与模式识别会议论文集*。2021。'
