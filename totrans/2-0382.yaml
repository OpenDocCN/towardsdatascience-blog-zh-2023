- en: Beyond NeRFs (Part Two)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/beyond-nerfs-part-two-83af2bd17d0](https://towardsdatascience.com/beyond-nerfs-part-two-83af2bd17d0)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tips and tricks for successfully using NeRFs in the wild
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----83af2bd17d0--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----83af2bd17d0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----83af2bd17d0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----83af2bd17d0--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----83af2bd17d0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----83af2bd17d0--------------------------------)
    ·16 min read·Jun 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/84d3b26f3db9121dd04bc299b88a98e0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: (Photo by [Ashim D’Silva](https://unsplash.com/@randomlies?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/wild-west?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: In the domain of representing and rendering 3D scenes, neural radiance fields
    (NeRFs) provided a massive breakthrough in accuracy. Given several images of an
    underlying scene, NeRFs can reconstruct high-resolution, 2D renderings of this
    scene from arbitrary viewpoints. Compared to prior techniques like [local light
    field fusion (LLFF)](https://cameronrwolfe.substack.com/p/local-light-field-fusion)
    [5] and [scene representation networks (SRNs)](https://cameronrwolfe.substack.com/p/scene-representation-networks)
    [6], NeRFs are far more capable of capturing complex components of a scene’s appearance
    and geometry (e.g., view-dependent reflections and intricate materials).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: NeRFs have the potential to revolutionize applications like virtual reality,
    computer graphics, and more. One could imagine, for example, using NeRFs to reconstruct
    3D renderings of a house that is for sale given images of the house that are available
    online, or even designing video game environments using NeRFs trained on real-world
    scenes. In their original formulation, however, NeRFs were mostly evaluated using
    images captured in simple, controlled environments. When trained over images of
    real-world scenes, NeRFs tend to not perform as well (see below), making them
    less useful in practical applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c8ab7a99a383caaa1cf72a5ae255c55a.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: (from [2])
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Within this overview, we will study NeRFs in depth to better understand why
    they perform poorly in the real world and how this problem can be solved. In particular,
    we will explore a few recent proposals, called NeRF-W [1] and def-NeRF [2], that
    modify NeRFs to better handle images that are captured in uncontrolled, noisy
    environments. Such techniques make NeRFs much more useful by enabling their application
    over images that more closely match data that will be encountered in most practical
    applications.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a47569552b85a66235aa90fde0ae29c.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: (from [1, 2])
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Background
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This overview is part of our series on deep learning for 3D shapes and scenes.
    If you haven’t already, I recommend reading the [prior posts](https://cameronrwolfe.substack.com/i/100102968/related-posts)
    in this series, as they contain a lot of useful background information on NeRFs
    and related techniques. Here, we will briefly overview NeRFs and a few other relevant
    concepts (e.g., latent spaces, non-rigid deformations, positional encoding, etc.)
    that will arise in our discussion of NeRF-W [1] and def-NeRF [2].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: A Brief Overview of NeRFs
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a prior overview, we have already discussed the idea of neural radiance fields
    (NeRFS) [3] in depth. Given that this overview explores extending and modifying
    NeRFs for real-world applications, I recommend reading the overview of NeRFs [here](https://cameronrwolfe.substack.com/p/understanding-nerfs).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '**quick overview.** To re-hash the basic idea behind NeRFs, they are just [feed-forward
    neural networks](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)
    that take a 3D coordinate and viewing direction as input and produce a volume
    density and RGB color as output. By evaluating the NeRF at a variety of different
    points (and viewing directions) in 3D space, we can accumulate a lot of information
    about a scene’s geometry and appearance, which can be used to render an image
    (or view) of that scene; see below.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88182a1c4ca8fc497e64aea572d9496c.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: (from [3])
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: To train a NeRF, we simply need to accumulate several images of a scene and
    relevant [camera pose information](https://cameronrwolfe.substack.com/i/97472888/background)
    for each image. Then, we can use these images as a target to train our NeRF! In
    particular, we repeatedly *i)* use the NeRF to render an image at a known viewpoint
    and *ii)* compare the NeRF’s output to the actual image using a photometric loss
    function (i.e., this just measures differences between RGB pixel values); see
    below.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c82a1fb388d13c6afc9f4c656ef5a77.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: (from [3])
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '**problems with NeRFs.** NeRFs were a massive breakthrough in the field of
    3D scene representation, but they have some limitations. In a [prior overview](https://cameronrwolfe.substack.com/p/beyond-nerfs-part-one),
    we discussed the computational burden of training and rendering with NeRFs, as
    well as their need for many images of an underlying scene for training. However,
    techniques like InstantNGP [7] and PixelNeRF [8] drastically improve the computational
    and sample efficiency of NeRFs.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Going further, NeRFs make the assumption that scenes are static. In practice,
    this assumption is oftentimes not true. Images may contain moving objects (e.g.,
    people) that occlude relevant portions of the scene or even be taken at different
    times of day (e.g., at night or in the morning). These are transient components
    of a scene that may be present in one image but not another.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: “The central limitation of NeRF … is its assumption that the world is geometrically,
    materially, and photometrically static. NeRF requires that any two photographs
    taken at the same position and orientation must be identical. This assumption
    is violated in many real-world datasets.” *— from [1]*
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This static assumption is a large factor that underlies the poor performance
    of NeRF in uncontrolled environments. Within this overview, we will explore how
    this assumption can be mitigated, allowing NeRFs to be trained over imperfect,
    real-world datasets that we encounter in practical applications.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Primer on Shape Deformation
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To successfully train NeRFs on noisy smart phone images, recent techniques augment
    NeRFs with learnable deformation fields. To understand what this means, however,
    we need to learn about deformations in general. We will briefly cover this idea
    below.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'Put simply, a deformation describes a transformation of an initial geometry
    into a final geometry (e.g., by displacing, translating, or morphing points relative
    to some [reference frame](https://isaacphysics.org/concepts/cp_frame_reference?stage=all)).
    There are two basic types of deformations that we will typically encounter:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Rigid deformation
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Non-rigid deformation
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For rigid deformations (e.g., rotations and translations), the object being
    deformed changes with respect to an external frame of reference but remains unchanged
    with respect to an internal frame of reference. Examples are provided in the image
    below.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ad25b320d5edc3f1aa50a826c5d13ce.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: Examples of rigid deformations (from sci.sdsu.edu)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Non-rigid deformations are slightly different. Objects are changed with respect
    to both internal and external frames of reference. Thus, non-rigid deformations
    can capture transformations like dilation and shearing; see below.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba37d08698e69bd16c1a871ed046cf91.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: Examples of non-rigid deformations (from sci.sdsu.edu)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '**deformation fields.** A deformation field is one way of representing deformations.
    It simply defines a transformation via a mapping of points in 3D space (i.e.,
    each point in space is mapped to a new point). By repositioning/transforming an
    object based on the mapping defined by this field, we can arbitrarily transform
    the the shape of an object, similar to the deformations shown above.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Other Resources
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Beyond the discussion above, there are a few concepts that might provide a
    deeper understanding of the content in this post. Check out the links below to
    relevant resources:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: What is the latent space? [[link](https://www.baeldung.com/cs/dl-latent-space)]
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Positional Encoding [[link](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)]
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Volume Rendering [[link](https://www.heavy.ai/technical-glossary/volume-rendering)]
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publications
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although NeRFs are effective in controlled environments, they struggle to render
    3D scenes from images captured in the real world. Here, we will overview two recently
    proposed methods, called NeRF-W [1] and def-NeRF [2], that try to solve this issue.
    These methods can render accurate 3D scenes from sets of photos that are imperfectly
    captured (e.g., on a mobile phone) and even contain drastic illumination changes
    or occluding objects!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections](https://arxiv.org/abs/2008.02268)
    [1]'
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/a971ab584fb7f06926f532d3fe104ebd.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Real-world images oftentimes have a lot of undesirable properties that make
    the training of NeRFs quite difficult. Consider, for example, trying to train
    a NeRF over several images of a major landmark that were taken years apart from
    each other; see above. The images of this scene may be taken at different times
    of night or day and contain any number of moving people or objects that are not
    actually part of the scene’s geometry!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: In uncontrolled scenarios, NeRFs tend to fail due to their assumption that scenes
    are static, thus preventing their use in real-world applications. NeRF-W [1] —
    an extension to NeRF — mitigates these problems by relaxing the static assumption
    made by NeRF and allowing accurate modeling of 3D scenes under common, real-world
    problems (e.g., transient objects and illumination changes).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4191703746567fac7b8b3418b6f9532c.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '**decomposing a scene.** Major problems encountered by NeRFs in the wild can
    be loosely categorized as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '*Photometric changes:* time of day and atmospheric conditions impact the illumination/radiance
    of a scene.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Transient objects:* real-world scenes are rarely captured in isolation. There
    are usually people or objects that occlude and move through the scene as images
    are taken.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These problems, both violations to the static assumption, are illustrated in
    the figure above.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '**photometric changes.** To address photometric changes, each image is assigned
    its own “appearance” vector, which is considered (as an extra input) by NeRF-W
    when predicting the output RGB color. However, the appearance embedding has no
    impact on the predicted volume density, which captures the 3D geometry of a scene.
    This change is made by just separating NeRF’s feed-forward network into a few
    components that take different inputs; see below.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cc109e5a629cbdeb3af944f0adf5712a.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: The appearance embedding impacts color but not volume density (from [1]).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: By conditioning NeRF-W’s RGB output on this appearance embedding, the model
    can vary the appearance of a scene based on a particular image, while ensuring
    that the underlying geometry of the scene is appearance-invariant and shared across
    images. The unique appearance embeddings assigned to each training image are optimized
    alongside model parameters throughout training.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '**static vs. transient components.** To handle transient objects, we should
    notice that a scene contains two types of entities:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Image-dependent components (i.e., moving/transient objects)
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shared components (i.e., the actual scene)
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NeRF-W uses separate feed-forward network components to model image-dependent
    (transient) and shared (static) scene components. Both transient and static portions
    of the network output their own color and density estimations, which allows NeRF-W
    to disentangle static and transient components of a scene; see below.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b05768b6cdbb48d70989b5dc18e66737.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: Static and transient components of NeRF-W (from [1])
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: The transient portion of NeRF-W emits an uncertainty field (using a Bayesian
    learning framework [4]) that allows occluded scene components to be ignored during
    training. To ensure that transient effects on the scene are image-dependent, each
    training image is associated with a “transient” embedding vector, which is given
    as input to the transient component of NeRF-W. Similar to appearance embeddings,
    transient embeddings are learned during training. See below for a full depiction
    of the NeRF-W architecture.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de0532dd247f1137f1dce665d9ad52c0.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: All components of NeRF-W are jointly optimized using a procedure similar to
    NeRF [3], as described in the link [here](https://cameronrwolfe.substack.com/p/understanding-nerfs#%C2%A7modeling-nerfs).
    NeRF-W is evaluated using real-world photo collections of notable landmarks, selected
    from the [Photo Tourism dataset](http://phototour.cs.washington.edu/datasets/).
    When NeRF-W is trained to represent six landmarks, we see that NeRF-W outperforms
    baselines quantitatively in most cases; see below.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05e119f300b13901ef597ec64a871e49.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'We should recall that, to perform evaluation, we:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Train a model over images corresponding to a single scene.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample a hold-out test image (and corresponding camera pose).
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Render a viewpoint (with the trained model) using the camera pose information
    from the hold-out image.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the rendering to the ground truth image.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For NeRF-W, we do not have appearance or transient embeddings for test images.
    As a result, NeRF-W optimizes these embeddings based on one half of the test image
    and performs evaluation on the other half of the image; see below.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a8188c9c47ad277928edd82795bbbe81.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: When we examine the output of different NeRF variants, we see NeRF output tends
    to contain ghosting artifacts due to transient objects in the training images.
    In comparison, NeRF-W produces sharp and accurate renderings, indicating that
    is more capable of handling real-world variation in scene appearance; see below.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/877a1093e345c693798e829bde25e1c4.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Plus, NeRF-W can produce accurate scene renderings given training images with
    different lighting conditions. Given that NeRF-W can generate output given different
    appearance embeddings as input, we can tweak NeRF-W’s appearance embedding to
    modify the appearance of the final rendering; see below.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a37f6b3eeed2bda8022f135660f8cbd9.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Taking this idea a step further, we can even interpolate between appearance
    embeddings of different training images, yielding a smooth change in rendered
    scene appearance; see below.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5068204217d0160271cc5474393b961e.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[Nerfies: Deformable Neural Radiance Fields](https://arxiv.org/abs/2011.12948)
    [2]'
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/381faf50b33a9ad8fd0d8b09484bda3c.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: (from [2])
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'Most computer vision data for modern applications is captured on a smart phone.
    With this in mind, one might wonder whether it’s possible to train a NeRF using
    this data. In [2], authors explore a specific application along these lines: *converting
    casually captured “selfie” images/videos into a NeRF that can generates photorealistic
    renderings of a subject/person*. The authors call these models “Nerfies” (i.e.,
    a NeRF-based selfie)!'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'Initially, this application may seem pretty specific and useless. *Do we really
    care this much about tweaking the viewing angle of our selfie? How much more aesthetic
    can this make our instagram posts?* However, the methodology proposed in [2] is
    incredibly insightful for a few reasons:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: It gives us an idea of the feasibility of training NeRFs using images and videos
    from a smart phone.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It improves the ability of NeRFs to handle challenging, detailed, or imperfectly
    captured materials in a scene.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is not just applicable to capturing self-portraits, but can also be applied
    to more general scene modeling applications.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using techniques proposed in [2], we can produce high-quality scene representations
    given noisy, imperfect images captured on a mobile phone. As an example of how
    such a technique can be used, imagine generating a 3D model of yourself by just
    taking a quick video on your phone. Current approaches for this require entire,
    specialized labs with synchronized lights and cameras!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '**NeRFs + deformation fields.** When we think about using a hand-held camera
    to build a 3D model of a person, there are a few difficulties that might come
    to mind:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: The camera will move (this violates the static assumption!).
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Humans contain a lot of complex geometries and materials that are difficult
    to model (e.g., hair, glasses, jewelry, etc.).
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [2], authors address these challenges by augmenting NeRFs with a jointly
    optimized, non-rigid deformation field, which learns to transform the scene’s
    underlying geometry in 3D space.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/43b46e16aba5b3137524ed0e71a3fac4.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: Transforming coordinates with a deformation field (from [2])
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: This deformation field is modeled with a [feed-forward network](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)
    that takes a positionally-encoded, 3D coordinate and per-image latent deformation
    code as input, then produces a non-rigidly deformed 3D coordinate as output; see
    above.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '**how def-NeRF works.** The methodology in [2], which we’ll call a Deformable
    Neural Radiance Field (def-NeRF), has two components:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '*Deformation field*: models a non-rigid deformation of 3D coordinates using
    a feed-forward neural network.'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*NeRF*: uses a vanilla NeRF architecture to create a template of the underlying
    scene’s geometry and appearance.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We associate each training image with learnable deformation and appearance vectors.
    These latent codes, which mimic the per-image embedding approach used by NeRF-W
    [1], allow deformation and appearance to be image-dependent and enable def-NeRF
    to handle variance in images of scene (e.g., illumination changes).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e7e47a6a32e3b7bb8fa4d99697e79f07.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: (from [2])
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: def-NeRF takes a 3D coordinate as input. This coordinate is positionally-encoded
    and combined with the latent deformation code (via addition) before being passed
    into the feed-forward network that models def-NeRF’s deformation field. The output
    of this network is a transformed 3D coordinate; see above.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0b3ff1c04ef38a092a23103806589d1f.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
- en: (from [2])
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: This transformed coordinate is passed as input to the NeRF. Similar to NeRF-W
    [1], we augment this NeRF with a per-image, learnable appearance vector. Given
    the transformed coordinate, viewing direction, and an appearance vector as input,
    the NeRF produces a volume density and RGB color as output; see above.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c3d8049d21e17a25908d61ba4e5d8c0b.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: (from [2])
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'The full def-NeRF architecture, illustrated above, shares a nearly identical
    [architecture and training strategy](https://cameronrwolfe.substack.com/i/97915766/modeling-nerfs)
    compared to vanilla NeRFs. The main differences are:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: The modeling of a deformation field.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use of per-image deformation and appearance vectors.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “When rendering, we simply cast rays and sample points in the observation frame
    and then use the deformation field to map the sampled points to the template.”
    *— from [2]*
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/70ebd9f6d2ede2b208953ad5b62e43a1.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: (from [2])
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '**why is this necessary?** def-NeRF simply adds a deformation field that non-rigidly
    deforms input coordinates to the primary NeRF architecture. As a result, this
    approach decomposes scene representations into two parts:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: A geometric model of the scene.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A deformation of this geometry into the desired viewpoint.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As such, def-NeRFs relax the static assumption of NeRFs and allow the underlying
    scene geometry to be learned in a manner that is invariant to shifts, translations,
    viewpoint changes, and more.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23cb63f6742ecee5901df78b76de65d2.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: Added regularization improves reconstruction quality (from [2])
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '**regularization.** Authors in [2] observe that learned deformation fields
    are prone to local minima and overfitting. As a solution, we can add extra regularization
    to the optimization process of def-NeRF; see above. Several different regularization
    schemes are adopted, as described in Section 3.3–3.5 of [2].'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '**does it work well?** def-NeRF is primarily evaluated based on its ability
    to produce “Nerfies” (i.e., photorealistic renderings of a person/subject from
    arbitrary viewpoints). To create a Nerfie, a user films their face using a smart
    phone for about 20 seconds. Then, the def-NeRF methodology is trained over this
    data and used to render selfies from various, novel viewpoints.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3470aa24b0d545ac5f92eb492c416db.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: (from [2])
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate the quality of these scene reconstructions from novel viewpoints,
    the authors construct a camera rig that simultaneously captures a subject from
    multiple viewpoints. This allows a validation set to be constructed using images
    that capture the same exact scene from two different viewpoints; see above.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f19e6985a5a71db532ff9c80d6536515.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: (from [2])
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: When quantitatively compared to various baselines, def-NeRF is found to produce
    higher-quality reconstructions of subjects in most cases. Notably, def-NeRFs seems
    to struggle with the [PSNR metric](https://www.mathworks.com/help/vision/ref/psnr.html).
    However, authors claim that this metric favors blurry images and is not ideal
    for evaluating scene reconstructions.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2a968d151f94faf7ffdac431d3e7129d.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: (from [2])
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Qualitatively, we see that def-NeRF is more capable of capturing fine details
    within a scene (e.g., hair, shirt wrinkles, glasses, etc.) relative to baselines;
    see above. Additionally, the method works well for general scenes that go beyond
    reconstructing human subjects in a Nerfie. Overall, def-NeRF seems to provide
    high-quality scene reconstructions given images from a mobile phone!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f133741062ab23b6cc955ae6a5f3ded6.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: (from [2])
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Takeaways
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although NeRFs produce impressive demos, they are not truly useful unless we
    can apply them to images encountered in the real world. In this overview, we highlight
    the main reasons that applying NeRFs in the wild is often difficult (i.e., the
    static assumption) and overview some recent research that aims to solve this problem.
    Some of the major takeaways are outlined below.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '**static assumption.** NeRFs, in their original form, assume that scenes are
    static, meaning that two images taken of a scene from the same position/direction
    must be identical. In practice, this assumption rarely holds! People or objects
    may be moving through the scene, and variable lighting conditions can significantly
    change the appearance of an image. Deploying NeRFs in the real world requires
    that this assumption be relaxed significantly.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '**image-dependent embeddings.** Real-world scenes can be separated into image-independent
    and image-dependent components. If we want to learn the underlying geometry of
    a scene without overfitting to image-dependent components, we must customize a
    NeRF’s output on a per-image basis. For both NeRF-W and def-NeRF, this is largely
    accomplished via the addition of per-image embeddings vectors (i.e., appearance,
    transient, and deformation vectors). However, the fact that per-image embedding
    vectors are not available for unseen/test images may make deploying these models
    more difficult.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '**limitations.** Allowing NeRFs to be applied beyond controlled environments
    is important, but this is not the only limitation of NeRFs! These models still
    suffer from poor sample efficiency and computational complexity, as discussed
    in a [prior post](https://cameronrwolfe.substack.com/p/beyond-nerfs-part-one).
    Making NeRFs viable for real-time applications will require a combination of techniques
    that solve each individual issue faced by NeRFs.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Closing Thoughts
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/). I study the empirical
    and theoretical foundations of deep learning. You can also check out my [other
    writings](https://medium.com/@wolfecameron) on medium! If you liked it, please
    follow me on [twitter](https://twitter.com/cwolferesearch) or subscribe to my
    [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/), where
    I help readers build a deeper understanding of topics in deep learning research
    via understandable overviews of popular papers.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Bibliography
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Martin-Brualla, Ricardo, et al. “Nerf in the wild: Neural radiance fields
    for unconstrained photo collections.” *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*. 2021.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Martin-Brualla, Ricardo 等. “Nerf in the wild：用于无约束照片集合的神经辐射场。” *IEEE/CVF
    计算机视觉与模式识别会议论文集*。2021。'
- en: '[2] Park, Keunhong, et al. “Nerfies: Deformable neural radiance fields.” *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*. 2021.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Park, Keunhong 等. “Nerfies：可变形神经辐射场。” *IEEE/CVF 国际计算机视觉大会论文集*。2021。'
- en: '[3] Mildenhall, Ben, et al. “Nerf: Representing scenes as neural radiance fields
    for view synthesis.” *Communications of the ACM* 65.1 (2021): 99–106.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Mildenhall, Ben 等. “Nerf：将场景表示为神经辐射场以进行视图合成。” *ACM 通讯* 65.1 (2021): 99–106。'
- en: '[4] Kendall, Alex, and Yarin Gal. “What uncertainties do we need in bayesian
    deep learning for computer vision?.” *Advances in neural information processing
    systems* 30 (2017).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Kendall, Alex 和 Yarin Gal. “在计算机视觉的贝叶斯深度学习中，我们需要哪些不确定性？” *神经信息处理系统进展* 30
    (2017)。'
- en: '[5] Mildenhall, Ben, et al. “Local light field fusion: Practical view synthesis
    with prescriptive sampling guidelines.” *ACM Transactions on Graphics (TOG)* 38.4
    (2019): 1–14.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Mildenhall, Ben 等. “局部光场融合：具有规范化采样指南的实用视图合成。” *ACM 图形学会论文 (TOG)* 38.4 (2019):
    1–14。'
- en: '[6] Sitzmann, Vincent, Michael Zollhöfer, and Gordon Wetzstein. “Scene representation
    networks: Continuous 3d-structure-aware neural scene representations.” *Advances
    in Neural Information Processing Systems* 32 (2019).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Sitzmann, Vincent, Michael Zollhöfer 和 Gordon Wetzstein. “场景表示网络：连续 3D
    结构感知神经场景表示。” *神经信息处理系统进展* 32 (2019)。'
- en: '[7] Müller, Thomas, et al. “Instant neural graphics primitives with a multiresolution
    hash encoding.” *ACM Transactions on Graphics (ToG)* 41.4 (2022): 1–15.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Müller, Thomas 等. “瞬时神经图形原语与多分辨率哈希编码。” *ACM 图形学会论文 (ToG)* 41.4 (2022):
    1–15。'
- en: '[8] Yu, Alex, et al. “pixelnerf: Neural radiance fields from one or few images.”
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    2021.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Yu, Alex 等. “pixelnerf：来自一张或少量图像的神经辐射场。” *IEEE/CVF 计算机视觉与模式识别会议论文集*。2021。'
