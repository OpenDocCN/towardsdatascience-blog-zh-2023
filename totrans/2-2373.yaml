- en: Why Is Feature Scaling Important in Machine Learning? Discussing 6 Feature Scaling
    Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/why-is-feature-scaling-important-in-machine-learning-discussing-6-feature-scaling-techniques-2773bda5be30](https://towardsdatascience.com/why-is-feature-scaling-important-in-machine-learning-discussing-6-feature-scaling-techniques-2773bda5be30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Standardization, Normalization, Robust Scaling, Mean Normalization, Maximum
    Absolute Scaling and Vector Unit Length Scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://rukshanpramoditha.medium.com/?source=post_page-----2773bda5be30--------------------------------)[![Rukshan
    Pramoditha](../Images/b80426aff64ff186cb915795644590b1.png)](https://rukshanpramoditha.medium.com/?source=post_page-----2773bda5be30--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2773bda5be30--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2773bda5be30--------------------------------)
    [Rukshan Pramoditha](https://rukshanpramoditha.medium.com/?source=post_page-----2773bda5be30--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2773bda5be30--------------------------------)
    ·13 min read·Aug 15, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd0c135d0ae75a274f8063f5a34b65b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Mediamodifier](https://unsplash.com/@mediamodifier?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/TuZAl7v4TCM?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Many machine-learning algorithms need to have features on the same scale.
  prefs: []
  type: TYPE_NORMAL
- en: There are diffident types of feature scaling methods that we can choose in various
    scenarios. They have different (technical) names. The term ***Feature Scaling***
    simply refers to any of those methods.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Feature scaling in different scenarios
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Feature scaling in PCA:** In principal component analysis, PCA components
    are highly sensitive to the relative ranges of the original features, if they
    are not measured on the same scale. PCA tries to choose the components that maximize
    the variance of the data. If the maximization of various occurs due to higher
    ranges of some features, those features may tend to dominate the PCA process.
    In this case, the true variance may not be captured by the components. To avoid
    this, we generally perform feature scaling before PCA. However, there are two
    exceptions. If there is no significant difference in the scale between the features,
    for example, one feature ranges between 0 and 1 and another ranges between 0 and
    1.2, we do not need to perform feature scaling although there will be no harm
    if we do! If you perform PCA by decomposing the correlation matrix instead of
    the covariance matrix, you do not need to do feature scaling even though the features
    are not measured on the same scale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature scaling in k-means clustering:** One of the main assumptions in k-means
    clustering is that all features are measured on the same scale. If not, we should
    perform feature scaling. The k-means algorithm calculates the distance between
    data points. Features with a higher range may dominate the calculations and those
    calculations may not be accurate. To avoid this, we need to perform feature scaling
    before k-means. Scaling features will also improve the training speed of k-means
    models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature scaling in KNN and SVM algorithms:** In general, algorithms that
    calculate the distance between data points are mostly affected by the relative
    ranges of features. KNN and SVM are not exceptions. Features with a higher range
    may contribute more because of the higher range, but not because of its importance.
    We do not want to algorithm to be biased in that way. Therefore, we need to scale
    features to contribute equally to distance calculations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature scaling in linear models:** The parameter values of linear models
    such as linear regression are highly dependent on the scale of the input features.
    Therefore, it is better to use the features measured on the same scale. That will
    also improve the training speed of linear models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature scaling in neural networks:** We usually apply feature scaling methods
    to input data. But, it is also possible to apply feature scaling to the activation
    values of hidden layers in a neural network! The scaled output values then become
    the inputs to the next layers. This is called *batch normalization* which can
    effectively eliminate the vanishing gradient problem and covariate shift problem
    and enhance the stability of the network during the training. It also speeds up
    the training process of neural network models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature scaling in the convergence of algorithms:** The learning rate is
    the main factor that decides the speed of the convergence of deep learning and
    machine learning algorithms. Feature scaling also has an effect on this! When
    the features are measured on the same scale, the calculations can be performed
    much faster and the algorithms converge faster!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature scaling in tree-based algorithms:** Feature scaling is not necessary
    for tree-based algorithms as they are not much sensitive to the relative scale
    of features. Popular tree-based algorithms are: Decision Tree, Random Forest,
    AdaBoost, Gradient Boosting, XGBoost, LightGBM, and CatBoot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature scaling in LDA:** Linear discriminant analysis (LDA) is a linear
    dimensionality reduction technique that performs dimensionality reduction by maximizing
    the class separability of classification datasets, not by maximizing the variance
    of the data. Therefore, LDA is not sensitive to the relative ranges of the features
    and feature scaling is not necessary for LDA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature scaling methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 1\. Standardization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Standardization is the most popular feature scaling method in which we center
    values at 0 and a unit standard deviation. The result is the z-score and therefore,
    this scaling method is also known as z-score standardization or normalization.
    After applying standardization to a feature, the data has a distribution with
    a mean of 0 and a standard deviation of 1\. That kind of distribution is called
    a standard normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: To apply standardization to a variable, first, we need to calculate the mean
    and the standard deviation of that variable. Then, we subtract the mean from each
    value and divide the result by the standard deviation. For a set of features,
    these calculations are performed feature-wise simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/20200f7ce28d119a83b9739b47a3ad48.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Standardization formula** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: The z-score or the output of standardization practically means how many standard
    deviations a value deviates from the mean!
  prefs: []
  type: TYPE_NORMAL
- en: The z-score values are not bounded to a certain range. The standardization process
    is not affected by the presence of outliers in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Standardization is particularly useful when the data follows a Gaussian or normal
    distribution or the distribution is unknown.
  prefs: []
  type: TYPE_NORMAL
- en: In Scikit-learn, standardization can be performed using the **StandardScaler()**
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: When calling the fit() method, the mean and standard deviation of each variable
    are calculated. If we need to apply the scaling process to data, we should also
    call the transform() method.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Min-Max Scaling (Normalization)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The min-max scaling or normalization is the process of scaling data into a specific
    range of your choice. The most commonly used range is (0, 1).
  prefs: []
  type: TYPE_NORMAL
- en: To apply min-max scaling to a variable, first, we need to find the minimum and
    maximum values of that variable. Then, we subtract the minimum from each data
    value and divide the result by the range (the difference between the maximum and
    minimum). For a set of features, these calculations are performed feature-wise
    simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad772b292f7677ce3ce25c495643af04.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Min-max scaling formula** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: The min-max scaling is particularly useful when the distribution of data is
    not known or does not follow the normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The min-max scaling process is highly sensitive to the outlier in the data.
  prefs: []
  type: TYPE_NORMAL
- en: In Scikit-learn, min-max scaling can be performed using the **MinMaxScaler()**
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: When calling the fit() method, the minimum and maximum values of each variable
    are found. If we need to apply the scaling process to data, we should also call
    the transform() method.
  prefs: []
  type: TYPE_NORMAL
- en: The **MinMaxScaler()** function also provides an option to change the range
    of your choice. The default is set to (0, 1).
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Robust Scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The robust scaling is also known as **quantile scaling** in which we scale data
    based on 1st, 2nd and 3rd quantiles. The 2nd quantile is the median as you already
    know.
  prefs: []
  type: TYPE_NORMAL
- en: To apply robust scaling to a variable, first, we need to find the quantiles
    of that variable. Then, we subtract the median (2nd quantile or Q2) from each
    data value and divide the result by the IQR (the difference between 3rd and 1st
    quantiles). For a set of features, these calculations are performed feature-wise
    simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/961dc3dee5397083a8f3eed848ee03fd.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Robust scaling formula** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/409816a59e781bb1f4c21765290dd059.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Robust scaling alternative formula** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: The robust scaling is particularly useful when there are outliers in the data.
    This is because quantiles are robust to outliers (hence the name, *robust scaling*!).
  prefs: []
  type: TYPE_NORMAL
- en: In Scikit-learn, robust scaling can be performed using the **RobustScaler()**
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: When calling the fit() method, the quantile values of each variable are found.
    If we need to apply the scaling process to data, we should also call the transform()
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Mean Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mean normalization is another popular feature scaling technique in which we
    subtract the mean from each data value and divide the result by the range (the
    difference between the maximum and minimum).
  prefs: []
  type: TYPE_NORMAL
- en: The formula is quite similar to the min-max scaling formula, except we subtract
    the mean from each data value, instead of subtracting the minimum from each data
    value.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e27c49cc6caa7118a2d28239cbf1db23.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Mean normalization formula** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: Mean normalization cannot be directly implemented in Scikit-learn as there is
    no dedicated function to do that. But, we can create a Scikit-learn pipeline by
    combing **StandardScaler()** and **RobustScaler()** transformers to perform mean
    normalization. A pipeline *sequentially* applies multiple transformers to data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the above code sample, I have customized both **StandardScaler()** and **RobustScaler()**
    transformers by changing their default hyperparameter values!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This code line does not perform typical standardization. Instead, it subtracts
    the mean from each value and divides the result by 1 because **with_std=False**.
    The output of this will become the input to the next transformer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: By setting **with_centering=False**, it does not subtract the median from the
    input values. In addition to that, it divides the input values by range (the difference
    between maximum and minimum) with **quantile_range=(0, 100)**. The 0 represents
    the minimum and 100 represents the maximum.
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline applies the above-customized transformers to data sequentially.
    When the data goes through the above pipeline, the mean normalization will be
    performed!
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Maximum Absolute Scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Maximum absolute scaling can be performed by dividing every data value by the
    maximum value of the feature. Its formula is very simple as compared to previous
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2defdd59966037f875b4aa8efdc4217d.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Maximum absolute scaling formula** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: The previous methods center the data by subtracting either mean, minimum or
    median from each data value. But, maximum absolute scaling does not center the
    data in that way. Therefore, this method works well with ***sparse data*** in
    whichmost of the values are zero.
  prefs: []
  type: TYPE_NORMAL
- en: In Scikit-learn, maximum absolute scaling can be performed using the **MaxAbsScaler()**
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: When calling the fit() method, the maximum values of each variable are found.
    If we need to apply the scaling process to data, we should also call the transform()
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Vector Unit-Length Scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A unit vector has a magnitude of 1\. To convert a non-zero vector to a unit
    vector, you need to divide that vector by its length which is calculated by using
    either Manhattan distance (L1 norm) or Euclidean distance (L2 norm) of that vector.
  prefs: []
  type: TYPE_NORMAL
- en: Now, consider the following non-zero vector.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0da2b1c33694d41973c2ccf48e9884c6.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Vector** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: The length (magnitude) of this vector can be calculated by using the Manhattan
    distance (L1 norm).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/702c37258c0ee787fb3327cad7a836a5.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Manhattan distance (L1 norm) of the vector** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: The length (magnitude) of the vector can also be calculated by using the Euclidean
    distance (L2 norm) which is the most commonly used method.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a87ec302d7ab1fe174439dccc6bc7a8.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Euclidean distance (L2 norm) of the vector** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: To convert ***x*** to a unit vector, we need to divide it by the length which
    is a real number as shown above. This is called *normalizing the vector*. After
    normalizing, the vector becomes a unit vector of magnitude (length) 1 and has
    the same direction as ***x***.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7859b9423798bb5c7496f161b151afbd.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Unit vector formula** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: The previous feature scaling methods performed calculations feature-wise, i.e.,
    by considering each feature across all observations. However, in unit length scaling,
    the calculations are performed observation-wise, i.e., by considering each observation
    across all features.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this, consider the following tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25e4c98ccac57707a62025af7341ff29.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: There are three observation vectors in the data. For example, the first observation
    can be represented by the following vector,
  prefs: []
  type: TYPE_NORMAL
- en: '**Ob1 = (2, 3, 5)**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: All other observations can be represented in a similar format.
  prefs: []
  type: TYPE_NORMAL
- en: The unit length scaling is applied to observation vectors in this way.
  prefs: []
  type: TYPE_NORMAL
- en: In Scikit-learn, unit length scaling is performed using the **Normalizer()**
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: When calling the fit() method, nothing happens this time. It only validates
    the model’s parameters. In previous cases, the parameters were learned. Calling
    the transform() method will divide each observation vector by its length, i.e.
    perform unit length scaling.
  prefs: []
  type: TYPE_NORMAL
- en: The **Normalizer()** function also provides an option to change the distance
    type, Manhattan (‘l1’ norm) or Euclidean (‘l2’ norm). By default, ‘l2’ is used.
  prefs: []
  type: TYPE_NORMAL
- en: Feature scaling and distribution of data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Any feature scaling method discussed above does not change the underlying distribution
    of data. Before and after applying feature scaling, the variable’s distribution
    remains unchanged! Only the range of values will be changed!
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this, I will create two histograms of the same feature before
    and after applying z-score standardization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f933e18ebb5a3067c1b8d46cf6bb9c01.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Histogram of unscaled variable** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6d1bf69f2c91d110a625e167f97ec6f9.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Histogram of scaled variable** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: The variable’s distribution remains unchanged! But, the range of values will
    be changed!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c0710f92da4be6bbdd0f16d7c4c170fd.png)'
  prefs: []
  type: TYPE_IMG
- en: '**The range before scaling** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/584b5f5e69e442239f46930515825899.png)'
  prefs: []
  type: TYPE_IMG
- en: '**The range after scaling** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: Data leakage when feature scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In data preprocessing, data leakage happens when some information in the train
    set leaks to the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'When training a model, the data used for training should not be used for testing.
    That’s why we split a dataset into two parts: train and test sets. The train and
    test sets should be independent and should not be mixed up.'
  prefs: []
  type: TYPE_NORMAL
- en: In feature scaling, data leakage easily happens in two ways.
  prefs: []
  type: TYPE_NORMAL
- en: '**When performing feature scaling before splitting data into train and test
    sets**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: When calling the fit() method of the scaler (sc), the parameters (means and
    standard deviations of each variable) are learned from the ***entire*** dataset.
    Some information in train and test sets may be mixed up because splitting is done
    after scaling the data.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this, you should do feature scaling after splitting data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '**When calling the fit() method of the scaler (sc) twice on both training and
    test sets**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: When calling the fit() method twice on both training and test sets, the parameters
    are learned twice. The parameters should only be learned on the training test,
    not on the test set. The learned parameters on the train set can also be applied
    to transform the test set too. In other words, you need to call the fit() method
    only once on the train set. In this way, we can avoid leaking data from the train
    set to the test set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Summary of feature scaling methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Standardization:** *StandardScaler()*, Usage — When the data follows a Gaussian
    or normal distribution or the distribution is unknown | Less sensitive to outliers
    in data'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Min-Max Scaling (Normalization):** *MinMaxScaler()*, Usage — When the distribution
    of data is not known or does not follow the normal distribution | Highly sensitive
    to the outlier in data'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Robust Scaling:** *RobustScaler()*, Usage — Robust to outliers in data'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Mean Normalization:** *StandardScaler()* and *RobustScaler()*, Usage — Sensitive
    to outliers in data'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Maximum Absolute Scaling:** *MaxAbsScaler(),* Usage — Does not center data
    unlike other methods | Works well with sparse data'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Vector Unit-Length Scaling:** *Normalizer()*, Usage — Performs calculations
    observation-wise, i.e., by considering each observation across all features (all
    other methods perform calculations feature-wise, i.e., by considering each feature
    across all observations)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is the end of today’s article.
  prefs: []
  type: TYPE_NORMAL
- en: '**Please let me know if you’ve any questions or feedback.**'
  prefs: []
  type: TYPE_NORMAL
- en: How about an AI course?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[**Neural Networks and Deep Learning Course**](https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join my private list of emails
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Never miss a great story from me again. By* [***subscribing to my email list***](https://rukshanpramoditha.medium.com/subscribe)*,
    you will directly receive my stories as soon as I publish them.*'
  prefs: []
  type: TYPE_NORMAL
- en: Thank you so much for your continuous support! See you in the next article.
    Happy learning to everyone!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Special credits go to the author of the following book which I read to get some
    knowledge on the feature scaling methods.
  prefs: []
  type: TYPE_NORMAL
- en: '*Python Feature Engineering Cookbook (2nd Edition 2022) by Soledad Galli*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iris dataset info
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Source:** You can download the original Iris dataset [here](https://archive.ics.uci.edu/dataset/53/iris).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Creator:** R. A. Fisher'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Citation:** Fisher,R. A.. (1988). Iris. UCI Machine Learning Repository.
    [https://doi.org/10.24432/C56C76](https://doi.org/10.24432/C56C76)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**License:** This dataset is licensed under a [Creative Commons Attribution
    4.0 International](https://creativecommons.org/licenses/by/4.0/) (CC BY 4.0) license.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This post was originally published on [Substack](https://datasciencemasterclass.substack.com/p/why-is-feature-scaling-important-in-ml)
    by me.
  prefs: []
  type: TYPE_NORMAL
- en: '**Designed and written by:** [**Rukshan Pramoditha**](https://medium.com/u/f90a3bb1d400?source=post_page-----2773bda5be30--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: '**2023–08–15**'
  prefs: []
  type: TYPE_NORMAL
