# 勇敢学习 ML：揭示 L1 和 L2 正则化（第4部分）

> 原文：[https://towardsdatascience.com/courage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9](https://towardsdatascience.com/courage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9)

## 探索 L1 和 L2 正则化作为贝叶斯先验

[](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)[![Amy Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------) [Amy Ma](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------) ·阅读时间 8 分钟·2023年12月11日

--

![](../Images/22bdc3089e02a827c9315e4de1e8cd0f.png)

照片由 [Dominik Jirovský](https://unsplash.com/@dominik_jirovsky?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 提供

欢迎回到‘[勇敢学习 ML](/towardsdatascience.com/tagged/courage-to-learn-ml)：揭示 L1 和 L2 正则化’，这是第四篇文章。上次，我们的导师-学习者配对通过 [拉格朗日乘子视角探讨了 L1 和 L2 正则化的性质](https://medium.com/p/ee27cd4b557a)。

在关于 L1 和 L2 正则化的总结部分，这对将从新的角度探讨这些话题——[贝叶斯先验](https://medium.com/p/65218b2c2b99)。我们还将总结 L1 和 L2 正则化如何在不同算法中应用。

在这篇文章中，我们将讨论几个引人入胜的问题。如果这些话题激起了你的好奇心，你来对地方了！

+   MAP 先验如何与 L1 和 L2 正则化相关

+   对使用拉普拉斯和正态分布作为先验的直观分析

+   理解 L1 正则化与拉普拉斯先验引起的稀疏性

+   与 L1 和 L2 正则化兼容的算法

+   为什么 L2 正则化在神经网络训练中通常被称为“权重衰减”

+   L1 范数在神经网络中使用较少的原因

# **所以，我们讨论了 MAP 如何不同于 MLE，主要因为 MAP 考虑了一个额外的信息：在看到数据之前的信念，即先验。这与 L1 和 L2 正则化有何关系？**

让我们深入探讨 MAP 公式中不同先验如何塑造我们对 L1 和 L2 正则化的理解（有关如何制定此方程式的详细讲解，请查看 [这篇文章](https://medium.com/p/65218b2c2b99)）。

在考虑权重的先验时，我们的初步直觉通常会使我们选择**正态分布**作为模型权重的先验。这样，我们通常会对每个权重wi使用均值为零的正态分布，并具有相同的标准差𝜎。将这种信念代入MAP中的先验项logp(w)（其中p(w)表示权重的先验）会自然得到**权重平方和**。这个项正是**L2范数**。这意味着使用正态分布作为我们的先验等同于应用L2正则化。

![](../Images/416c579fb6538b64246da161a2cd4b07.png)

相反，采用**拉普拉斯分布**作为我们的信念会导致**L1范数**的权重。因此，拉普拉斯先验本质上相当于L1正则化。

![](../Images/b01952bbb6dbbe648bdb8f04ba569cbd.png)

简而言之，L1正则化与拉普拉斯分布先验一致，而L2正则化对应于正态分布先验。

有趣的是，当在MAP框架中使用均匀先验时，它实际上在方程中“消失”了（你可以自己尝试一下！）。这使得似然项成为确定最佳权重值的唯一因素，实际上将MAP估计转变为最大似然估计（MLE）。

# **那么，当我们的先验是拉普拉斯分布与正态分布时，为什么会有不同的信念？我想更好地可视化这个问题。**

这是一个很好的问题。确实，不同的先验意味着你在收集任何数据之前对情况持有不同的初步假设。我们稍后将深入探讨不同分布的目的，但现在，让我们通过使用拉普拉斯分布和正态分布来考虑一个简单直观的例子。考虑一下我新发布的Medium帖子。两周前，作为一名没有粉丝的新作者，我预期没有查看次数。我的假设是，平均每日查看次数起初会很低，可能为零，但随着对类似主题感兴趣的读者发现我的作品，这一数字可能会增加。拉普拉斯先验很好地适应了这种情况。它建议了一系列可能的查看次数，但对接近零的数字赋予更高的概率，反映了我对初期查看次数很少但随时间增长的预期。

现在有55位观众（谢谢大家！），以及关注我帖子更新的粉丝，我的期望已经改变。我预期新帖子将表现类似于之前的帖子，平均查看次数接近我历史上的查看次数。这时，正态分布先验发挥作用，根据我已建立的记录预测未来的查看次数。

# **嗯……你能解释一下使用拉普拉斯先验的L1正则化的稀疏性吗？**

确实，通过比较拉普拉斯分布和正态分布，可以揭示 L1 正则化促进稀疏性的机制。**关键的区别在于它们在零附近的概率密度。** 拉普拉斯分布在零附近有一个尖锐的峰值，表明接近零的值的可能性较高。这一特征类似于 L1 正则化的效果，其中模型中的大多数权重被驱动到零，促进了稀疏性。相比之下，正态分布与 L2 正则化相关，在零附近的峰值较低，分布较广，表示对权重的更均匀分配有偏好。

![](../Images/49676c748501a234e00ec94b78953135.png)

来源: [https://austinrochford.com/posts/2013-09-02-prior-distributions-for-bayesian-regression-using-pymc.html](https://austinrochford.com/posts/2013-09-02-prior-distributions-for-bayesian-regression-using-pymc.html)

此外，拉普拉斯分布的**尾部更重**，意味着其延伸得更远。这一特性允许一些权重显著远离零，而其他权重则非常接近零。因此，通过选择拉普拉斯分布作为权重的先验（L1 正则化），我们鼓励模型学习解决方案，使大多数权重接近零，实现稀疏性而不牺牲潜在的相关特征。这就是为什么 L1 正则化可以作为特征选择方法。

# **所以，我认为 L1 和 L2 正则化对于避免过拟合和提升模型的泛化能力至关重要。你能告诉我这些方法可以应用于哪些算法吗？**

L1 和 L2 正则化可以通过向损失函数中添加惩罚项来应用于许多算法。以下是一些应用 L1 和 L2 正则化的具体算法示例：

+   **线性模型。** 这些技术在高维问题中特别有用。在线性模型中，它们被称为**lasso 和 ridge 回归**。需要注意的是，L1 正则化不仅有助于防止过拟合，还有助于特征选择，从而防止多重共线性。

+   **支持向量机。** 正则化方法是**SVM 的核心**。通过添加权重惩罚项，SVM 鼓励模型减少决策边界与最近支持向量之间的间距（L1 正则化）或平滑间距（L2 正则化），从而提高泛化能力。

+   **神经网络。** L2 正则化在神经网络中使用得更为普遍，通常被称为**权重衰减**。L1 正则化也可以在神经网络中使用，但由于其倾向于导致稀疏权重，因此较少见。

+   **集成算法。**像GBM和Xgboost这样的梯度提升机器使用L1和L2正则化来**限制集成中单个树的大小**。L1正则化通过缩小集成中弱学习器（每棵树）的权重来实现这一点，而L2正则化则惩罚每棵树中的总叶子数（叶子得分）。

# **为什么L2正则化也被称为神经网络训练中的‘权重衰减’？为什么L1范数在神经网络中使用较少？**

为了回答这两个问题，让我们引入一些数学来说明在L1和L2正则化下权重是如何更新的。

![](../Images/9901a688faa6f94f3cc1f00892ad1e5d.png)

带有L2正则化的损失函数；λ是惩罚系数，α代表学习率。

在L2正则化中，权重更新过程涉及对权重的轻微减少，根据其自身的大小缩放。这就是所谓的“权重衰减”。具体来说，**每个权重减少的量与其当前值直接成比例。**这种比例减少，由通常较小的惩罚系数（λ）和学习率（α）设置所控制，确保较大的权重比小的权重受到更高程度的惩罚。权重衰减的本质在于这种缩小权重的方式，鼓励模型保持较小的权重。这种行为在神经网络中是有利的，因为它趋向于产生更平滑的决策边界。

![](../Images/e43d75bcb2594d241d9ebd040600a708.png)

带有L1正则化的损失函数；λ是惩罚系数，α代表学习率。

相比之下，**L1正则化通过减去或添加一个由αλ和权重（w）符号确定的固定量来修改权重更新规则。**这种方法将权重推向零，无论它们是正的还是负的。在L1正则化下，所有权重，不论其大小，都以相同的固定量进行调整。这使得较大的权重保持相对较大，而较小的权重则更快地被驱动到零，从而促进网络的稀疏性。

![](../Images/93302c503c331685355b32d0adb09ee2.png)

让我们比较一下！

相比之下，L2对权重的修改方式是基于权重的现有值，使得较大的权重比小的权重衰减得更快。这种对所有权重的均匀衰减就是为什么它被称为‘权重衰减’。另一方面，L1的**固定调整量，不论权重大小**，可能导致一些问题，并在神经网络中变得不那么受欢迎：

+   它可能将一些权重归零，导致**‘死神经元’**，并可能破坏网络中的信息流，这可能影响模型性能。

+   **L1引入的零点处的不可微分点**使得像梯度下降这样的优化算法效果较差。

# **添加 L1 和 L2 正则化对我们的损失函数有什么影响？这些正则化的引入是否使我们远离原始的全局最小值？**

这是个很好的问题！简而言之，**一旦我们引入正则化，我们有意将关注点从原始的全局最小值转移开。** 这意味着向损失函数中添加惩罚项，从而根本改变其形状。理解这一变化是期望中的，而非偶然发生的，是至关重要的。

通过引入这些惩罚，我们旨在实现一个新的最优解，平衡两个关键目标：良好地拟合训练数据以最小化经验风险，同时减少模型复杂度并增强对未见数据的泛化。原始的全局最小值可能无法实现这种平衡，从而可能导致过拟合和在新数据上的性能下降。

如果你对测量原始最优解和正则化后最优解之间的距离的数学细节感兴趣，我强烈推荐 Ian Goodfellow 的《深度学习》第七章（第 224–229 页）。特别注意公式 7.7 和 7.13（L2）以及 7.22 和 7.23（L1）。这将提供对正则化项对权重影响的定量评估，深入了解 L1 和 L2 正则化。

我们现在已经结束了对 L1 和 L2 正则化的探讨。在下次讨论中，我很兴奋地将深入探讨损失函数的基础知识。**非常感谢所有喜欢本系列第一部分的读者。** 起初，我的目标是巩固我对基本 ML 概念的理解，但看到它引起了许多人的共鸣，我感到非常高兴😃。如果你对我们的下一个话题有建议，请随时留言！

本系列的其他文章：

+   [勇敢学习 ML: 揭开 L1 和 L2 正则化的面纱（第 1 部分）](/understanding-l1-l2-regularization-part-1-9c7affe6f920)

+   [勇敢学习 ML: 揭开 L1 和 L2 正则化的面纱（第 2 部分）](/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35)

+   [勇敢学习 ML: 揭开 L1 和 L2 正则化的面纱（第 3 部分）](/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a)

+   [勇敢学习 ML: 解码似然、MLE 和 MAP](/courage-to-learn-ml-decoding-likelihood-mle-and-map-65218b2c2b99)

***如果你喜欢这篇文章，你可以在*** [***LinkedIn***](https://www.linkedin.com/in/amyma101/)***找到我。***

## 参考

[## 正则化的概率解释

### 从概率的角度看正则化。

[bjlkeng.io](https://bjlkeng.io/posts/probabilistic-interpretation-of-regularization/?source=post_page-----27c13dc250f9--------------------------------)

[https://keras.io/api/layers/regularizers/](https://keras.io/api/layers/regularizers/)

[](https://arxiv.org/abs/2310.04415?source=post_page-----27c13dc250f9--------------------------------) [## 为什么现代深度学习中需要权重衰减？

### 权重衰减是一种广泛用于训练先进深度网络的技术，包括大型语言模型……

[arxiv.org](https://arxiv.org/abs/2310.04415?source=post_page-----27c13dc250f9--------------------------------)
