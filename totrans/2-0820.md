# 熵正则化强化学习解释

> 原文：[https://towardsdatascience.com/entropy-regularized-reinforcement-learning-explained-2ba959c92aad](https://towardsdatascience.com/entropy-regularized-reinforcement-learning-explained-2ba959c92aad)

## 通过为算法添加熵奖励来学习更可靠、稳健和可迁移的策略

[](https://wvheeswijk.medium.com/?source=post_page-----2ba959c92aad--------------------------------)[![Wouter van Heeswijk, PhD](../Images/9c996bccd6fdfb6d9aa8b50b93338eb9.png)](https://wvheeswijk.medium.com/?source=post_page-----2ba959c92aad--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2ba959c92aad--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2ba959c92aad--------------------------------) [Wouter van Heeswijk, PhD](https://wvheeswijk.medium.com/?source=post_page-----2ba959c92aad--------------------------------)

·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2ba959c92aad--------------------------------) ·8 min read·2023年10月26日

--

![](../Images/2fce76a87f64dcb27147a0868dc84b72.png)

图片由 [Jeremy Thomas](https://unsplash.com/@jeremythomasphoto?utm_source=medium&utm_medium=referral) 提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

*熵* 是与无序、随机或不确定状态相关的概念。它可以被视为**随机变量的信息度量**。传统上，它与热力学等领域相关，但这一术语也被引入了许多其他领域。

1948年，克劳德·香农在信息论中引入了熵的概念。在这种背景下，如果一个事件发生的概率较低，它被认为提供了更多的信息；**事件的信息与其发生的概率成反比**。直观地说：我们从稀有事件中学到的更多。

熵的概念可以被形式化为以下内容：

![](../Images/e73e66ad287c526dbcbf6c48ef1d9c81.png)

对一组事件 x 的熵定义。每个事件的信息与其发生的概率成反比。

在强化学习（RL）中，熵的概念也被应用，目的是鼓励探索。在这种背景下，熵是**由随机策略返回的动作的可预测性度量**。

具体来说，**RL 将策略的熵（即，动作的概率分布）作为奖励的一个组成部分来加以奖励**。这篇文章讨论了基本情况，但熵奖励是许多最先进的 RL 算法的一个重要组成部分。

# 什么是熵？

首先，让我们对熵的概念建立一些直觉。下图显示了低熵和高熵策略。低熵策略几乎是确定的；我们几乎总是选择相同的行动。在高熵策略中，我们选择的行动具有更多的随机性。

![](../Images/8249108e3727eaceb0769c8cdaf37b7d.png)

低熵策略（左）和高熵策略（右）的示例。在高熵策略中，行动选择中有更多的随机性。[作者提供的图片]

接下来，我们考虑硬币翻转的熵。

> 香农的熵利用以2为底的对数函数（在Numpy中为np.log2），相应的测量单位称为比特。其他形式的熵使用不同的底数。这些区别对于掌握主要思想并不是特别重要。

如果硬币是加重的会发生什么？下图显示了**熵将会减少，因为更有可能确定某一结果是否会发生**。

![](../Images/a1b3060c05698700a76957ef49716bf9.png)

硬币翻转的熵，随着头和尾的概率变化，以比特为单位测量。当硬币翻转的结果最不确定时，熵达到峰值。[图片来自[维基百科](https://en.wikipedia.org/wiki/Entropy_(information_theory)#/media/File:Binary_entropy_plot.svg)]

现在让我们计算一个公平骰子的熵：

注意，骰子的熵（2.58 bit）高于公平硬币（1 bit）。虽然两者都有均匀的结果概率，但骰子的个别概率较低。

现在，考虑一个加重骰子的概率，例如，[3/12, 1/12, 2/12, 2/12, 2/12, 2/12]。相应的熵为2.52，这反映了结果现在略微更可预测（我们更可能看到1点而不太可能看到2点）。最后，考虑一个更重的骰子，概率为[7/12, 1/12, 1/12, 1/12, 1/12, 1/12]。此时，我们得到的熵为1.95。结果的可预测性进一步提高，从熵的减少可以看出。

了解熵的概念后，让我们看看如何在强化学习中利用它。

# 熵正则化强化学习

我们在强化学习的背景下定义熵，其中行动概率源自随机策略π(⋅|s)。

![](../Images/17a5ae3a81c3bfe4fa2258dcfdff47d7.png)

在强化学习中的熵奖励，通过对所有行动概率及其对数的乘积求和来计算

我们将策略的熵用作**熵奖励**，将其添加到我们的奖励函数中。请注意，我们对每一个时间步骤都这样做，这意味着当前的行动也为最大化未来的熵做好了准备：

![](../Images/a324b2256c1dbd631125d52652a26565.png)

熵正则化强化学习。熵奖励——由α加权——被添加到我们寻求最大化的奖励中

这乍看起来可能有些违反直觉。毕竟，强化学习旨在优化决策过程，这涉及到常规地选择好的决策而非坏的决策。为什么我们要以鼓励*最大化*熵的方式来改变我们的奖励信号呢？这是引入**最大熵原则**的一个好时机：

> “***[策略]***最能代表关于系统***[环境]***当前知识状态的概率分布，是在精确陈述的先验数据***[观察奖励]***的背景下具有最大熵的分布” — [维基百科](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy)

如果策略的熵很大（例如，在初始化后的短时间内），我们对不同动作的影响了解得不多。高熵策略反映了我们尚未充分探索环境，还需要从各种动作中观察更多的奖励。

当然，*最终*我们确实希望采取好的行动，而不是无休止地探索，这意味着我们必须决定对熵奖励的重视程度。这是通过**熵正则化系数**α来实现的，这是一个可调参数。为了实际应用，权重熵奖励*αH(π(⋅|s)*可以简单地视为一种鼓励探索的奖励组件。

请注意，熵奖励总是计算在整个动作空间上，因此在评估每个动作时，我们都会添加相同的奖励。在典型的随机策略方法中，**动作概率与它们的期望奖励成正比，包括熵奖励**（例如，通过对它们应用softmax函数）。因此，如果熵相对于奖励非常大，则动作概率或多或少相等。如果熵非常小，则奖励在定义动作概率上占主导地位。

# Python实现

现在开始实现熵正则化的强化学习。为了本文的目的，我们在多臂老虎机背景下使用基本的离散策略梯度算法，但它可以很容易地扩展到更复杂的环境和算法。

> 记住，策略梯度算法具有内置的探索机制——对固有确定性策略（例如，Q-learning）应用的熵奖励具有更明显的效果。

引入熵正则化是相当直接的。我们只需将熵奖励添加到奖励中——因此它会被纳入损失函数——然后按常规步骤进行。根据算法和问题设置，您可能会在文献和代码库中遇到许多变体，但核心思想保持不变。下面的代码片段（离散策略梯度的TensorFlow实现）演示了它是如何工作的。

部分熵正则化强化学习的代码，在这种情况下扩展了离散策略梯度算法。

考虑一组老虎机，其平均奖励为*μ=[1.00,0.80,0.90,0.98]*，所有机器的标准差*σ=0.01*。显然，最佳策略是以概率1.0玩机器#1。然而，如果没有足够的探索，很容易将机器#4误认为是最佳机器。

为了说明这个想法，让我们首先看看没有熵奖励（α=0）的算法行为。我们绘制了玩机器#1的概率。尽管每台机器开始时的概率相等，但算法相当快地识别出机器#1提供了最高的期望奖励，并开始以更高的概率玩它。

![](../Images/221a756e901548799aa9e66e4542a5fe.png)

在这种情况下，没有熵正则化的算法收敛于主要玩最佳的机器#1。左图：每次试验玩机器#1的概率。右图：10k次试验后每台机器的概率[作者图片]

然而，这可能会有很大的不同……下图显示了使用相同算法的一个运行，只是这次它错误地收敛到次优的机器#4。

![](../Images/53a0aa195796d2e588c02c59ea15d1d5.png)

在这种情况下，没有熵正则化的算法收敛于主要玩次优的机器#4。左图：每次试验玩机器#1的概率。右图：10k次试验后每台机器的概率[作者图片]

*现在，我们设置α=1*。这产生了相对于奖励而言较大的熵奖励。尽管玩机器#1的概率逐渐增加，但正则化组件继续鼓励强烈的探索，即使在10k次迭代后。

![](../Images/2454757681a09e04e3db97422e0323c9.png)

通过熵正则化，我们看到即使在10k次试验后，算法仍然进行了大量探索，虽然逐渐认识到机器#1提供了更好的奖励。左图：每次试验玩机器#1的概率。右图：10k次试验后每台机器的概率[作者图片]

显然，在实际应用中，我们不知道真正的最佳解决方案，也不知道所需的探索量。通常，你会遇到*α=0.001*附近的值，但你可以想象，探索与利用之间的理想平衡强烈依赖于问题。因此，通常需要一些**试验和错误以找到合适的熵奖励**。系数*α*也可能是动态的，可能通过预定的衰减方案或者作为一个学习权重本身。

# 强化学习中的应用

熵正则化的原则可以应用于几乎任何RL算法。例如，你可以将熵奖励添加到Q值中，并通过softmax层将结果转换为动作概率（soft Q-learning）。最先进的算法，如近端策略优化（PPO）或软演员评论家（SAC），通常包括熵奖励，实证研究表明这通常能提升性能。具体来说，它提供了以下三种好处：

## I. 更好的解决方案质量

正如前面详细说明的，熵奖金鼓励探索。特别是在处理**稀疏奖励**时，这非常有用，因为我们很少收到关于动作的反馈，并且可能错误地重复那些过高估计奖励的次优动作。

## II. 更好的鲁棒性

由于熵奖金鼓励更多的探索，我们也会更频繁地遇到稀有或偏离的状态-动作对。因为我们遇到了**更丰富和更多样化的经验**，我们学习到的策略能够更好地处理各种情况。这种增加的鲁棒性提升了策略的质量。

## III. 促进迁移学习

增加探索也有助于将学习到的策略适应于新任务和环境。更多的多样化经验能够更好地适应新情况，因为我们已经从类似的情境中学到了东西。因此，熵正则化在迁移学习中通常很有用，它使得在应对变化环境时**重新训练或更新学习策略**变得容易。

# TL;DR

+   熵奖金鼓励对动作空间的探索，旨在避免过早收敛

+   奖励（利用）和奖金（探索）之间的平衡是通过一个需要微调的系数来控制的。

+   熵奖金在现代 RL 算法中如 PPO 和 SAC 中被广泛使用。

+   探索提高了质量、鲁棒性和对新实例变体的适应能力。

+   熵正则化在处理稀疏奖励时特别有用，当鲁棒性很重要和/或策略应该适用于相关问题设置时。

*如果你对 RL 中的熵正则化感兴趣，你可能还想查看以下文章：*

[](/a-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7?source=post_page-----2ba959c92aad--------------------------------) [## TensorFlow 2.0 中离散策略梯度的最小工作示例

### 用于训练离散演员网络的多臂老虎机示例。借助 GradientTape 功能，…

towardsdatascience.com](/a-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7?source=post_page-----2ba959c92aad--------------------------------) [](/policy-gradients-in-reinforcement-learning-explained-ecec7df94245?source=post_page-----2ba959c92aad--------------------------------) [## 强化学习中的策略梯度解释

### 了解基于似然比的策略梯度算法（REINFORCE）：直觉、推导、…

towardsdatascience.com](/policy-gradients-in-reinforcement-learning-explained-ecec7df94245?source=post_page-----2ba959c92aad--------------------------------) [](/proximal-policy-optimization-ppo-explained-abed1952457b?source=post_page-----2ba959c92aad--------------------------------) [## 近端策略优化（PPO）解释

### 从REINFORCE到连续控制中的首选算法的历程

[towardsdatascience.com](https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=post_page-----2ba959c92aad--------------------------------)

# 进一步阅读

Ahmed, Z., Le Roux, N., Norouzi, M., & Schuurmans, D. (2019). [理解熵对策略优化的影响](https://proceedings.mlr.press/v97/ahmed19a.html) 国际机器学习会议。

Eysenbach, B. & Levine, S. (2022). [最大熵强化学习（可证明地）解决一些鲁棒强化学习问题](https://arxiv.org/pdf/2103.06257.pdf) 国际学习表征会议。

Haarnoja, T., Tang, H., Abbeel, P., & Levine, S. (2017). [使用深度能量基策略的强化学习](https://proceedings.mlr.press/v70/haarnoja17a/haarnoja17a.pdf) 国际机器学习会议。

Reddy, A. (2021). [最大熵如何使强化学习更鲁棒](https://mlberkeley.substack.com/p/max-ent) 伯克利机器学习。

Schulman, J., Chen, X., & Abbeel, P. (2017). [策略梯度与软Q学习的等价性](https://arxiv.org/abs/1704.06440) *arXiv预印本arXiv:1704.06440*。

Tang, H. & Haarnoja, T. (2017). [通过最大熵深度强化学习学习多样化技能](https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/) 伯克利人工智能研究所。

Yu, H., Zhang, H., & Xu, W. (2022). [你实际需要熵奖励吗？](https://arxiv.org/abs/2201.12434) *arXiv预印本arXiv:2201.12434*。
