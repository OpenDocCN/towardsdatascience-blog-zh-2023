["```py\n# Import libraries\nfrom transformers import pipeline\n\n# Load the pre-trained model\nnlp = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n\n# Define the function to perform sentiment analysis\ndef sentiment_analyzer(input_text):\n    # Perform sentiment analysis\n    result = nlp(input_text)[0]\n\n    # Return results\n    return f\"'{input_text}' has a {result['label']} sentiment, with a score of {round(result['score'], 4)}!\\n\"\n\n# Define example sentences\nsentence_1 = \"I loved this movie!\"\nsentence_2 = \"I did not like this movie.\"\nsentence_list = [sentence_1, sentence_2]\n\n# Analyze the sentiment of each sentence\nfor sentence in sentence_list:\n    print(sentiment_analyzer(sentence))\n```", "```py\n# Import library\nfrom transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n\n# Load model and tokenizer\nmodel = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\ntokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n\ndef translator(source_sentence, source_language, target_language):\n    # Encode sentence\n    tokenizer.src_lang = source_language\n    input_ids = tokenizer(source_sentence, return_tensors=\"pt\").input_ids\n\n    # Translate sentence\n    output_ids = model.generate(input_ids, forced_bos_token_id=tokenizer.lang_code_to_id[target_language])\n    translation = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n\n    # return translation\n    return translation\n```", "```py\n# Define sentence to be translated\noriginal_sentence = 'Multilingual machine translation is impressive!'\n\n# Define source language\nenglish = \"en_XX\"\n\n# Define target languages\nfrench = \"fr_XX\"\nspanish = \"es_XX\"\nitalian = \"it_IT\"\ngerman = \"de_DE\"\nsimplified_chinese = \"zh_CN\"\njapanese = \"ja_XX\"\n\n# Create a list of target languages\ntarget_list = [french, spanish, italian, german, simplified_chinese, japanese]\n\n# Create a prompt list of lists\nprompt_list = []\n\nfor target in target_list:\n    prompt_list.append([original_sentence, english, target])\n\n# Create translations\nprint(f\"Generating machine translations for: \\n'{original_sentence}'\\n\")\n\nfor i in enumerate(prompt_list):\n    translation = translator(source_sentence=i[1][0], source_language=i[1][1], target_language=i[1][2])\n    print(f\"{i[1][2]}:\")\n    print(f\"{translation}\\n\")\n```", "```py\npip3 install spacy\npython -m spacy download en_core_web_sm\n```", "```py\n# Import library\nimport spacy\n\n# Load English tokenizer, tagger, parser and NER\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Define example sentence\nsentence = \"Farzad wrote this Medium article in March 2023, using an Apple laptop, on a Jupyter notebook!\"\n\n# Apply NER\ndoc = nlp(sentence)\n\n# Analyze syntax\nprint(f\"Noun phrases:{[chunk.text for chunk in doc.noun_chunks]}\\n\")\nprint(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\nprint(\"\")\n\n# Find named entities, phrases and concepts\nfor entity in doc.ents:\n    print(entity.text, entity.label_)\n```"]