- en: 'Back to Basics, Part Tres: Logistic Regression'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/back-to-basics-part-tres-logistic-regression-e309de76bd66](https://towardsdatascience.com/back-to-basics-part-tres-logistic-regression-e309de76bd66)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An illustrated guide on Logistic Regression, with code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@shreya.rao?source=post_page-----e309de76bd66--------------------------------)[![Shreya
    Rao](../Images/03f13be6f5f67783d32f0798f09a4f86.png)](https://medium.com/@shreya.rao?source=post_page-----e309de76bd66--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e309de76bd66--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e309de76bd66--------------------------------)
    [Shreya Rao](https://medium.com/@shreya.rao?source=post_page-----e309de76bd66--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e309de76bd66--------------------------------)
    ·8 min read·Mar 2, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'Welcome back to the final installment of our ***Back to Basics*** series, where
    we’ll delve into another fundamental machine learning algorithm: **Logistic Regression**.
    In the previous two articles, we helped our friend Mark determine the ideal selling
    price for his 2400 feet² house using [Linear Regression](https://medium.com/towards-data-science/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46)
    and [Gradient Descent](/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd).'
  prefs: []
  type: TYPE_NORMAL
- en: Today, Mark comes back to us again for help. He lives in a fancy neighborhood
    where he thinks houses below a certain size don’t sell, and he is worried that
    his house might not sell either. He asked us to help him determine how ***likely***
    it is that his house will sell.
  prefs: []
  type: TYPE_NORMAL
- en: This is where Logistic Regression comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic Regression is a type of algorithm that predicts the probability of
    a binary outcome, such as whether a house will sell or not. Unlike Linear Regression,
    Logistic Regression predicts probabilities using a range of 0% to 100%. Note the
    difference between predictions a linear regression model and logistic regression
    model make:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ae4ff646211b57f9cb04c3cfabc55b8f.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s delve deeper into how logistic regression works by determining the probability
    of selling houses with varying sizes.
  prefs: []
  type: TYPE_NORMAL
- en: We start our process again by collecting data about house sizes in Mark’s neighborhood
    and seeing if they sold or not.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e151a4188e3e07fe6225979f53ad35e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let’s plot these points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16baef721e5cb51e73af1a0c05df9177.png)'
  prefs: []
  type: TYPE_IMG
- en: Rather than representing the outcome of the plot as a binary output, it’ll be
    more informative to represent it using probabilities since that is the quantity
    we are trying to predict.
  prefs: []
  type: TYPE_NORMAL
- en: We represent 100% probability as 1 and 0% probability as 0
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/84e5644b20b6729ddaa6c55381ec8afe.png)'
  prefs: []
  type: TYPE_IMG
- en: In our [previous article](/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46),
    we learned about linear regression and its ability to fit a line to our data.
    But can it work for our problem where the desired output is a probability? Let’s
    find out by attempting to fit a line using linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that the formula for the best-fitting line is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d3b26ad9ff6630c464a8a179fe616311.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By following the steps outlined in linear regression, we can obtain optimal
    values for β₀ and β₁, which will result in the best-fitting line. Assuming we
    have done so, let’s take a look at the line that we have obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3d8e33858bc2079af667a28904ed58f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Based on this line, we can see that a house with a size just below 2700 feet²
    is predicted to have a 100% probability of being sold:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57c245fe0af0ab78f00c01b587e308e5.png)'
  prefs: []
  type: TYPE_IMG
- en: '…and a 2200 feet² house is predicted to have a 0% chance of being sold:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08a691a73ef23e6460863d15c9738d67.png)'
  prefs: []
  type: TYPE_IMG
- en: '…and a 2300 feet² house is predicted to have about a 20% probability of being
    sold:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/45bb577b7a475f937350adcf0ded0cdd.png)'
  prefs: []
  type: TYPE_IMG
- en: Alright, so far so good. But what if we have a house that is 2800 feet² in size?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0b440d8b839079aa49bdc3ee90de3793.png)'
  prefs: []
  type: TYPE_IMG
- en: Uh.. what does a probability above 100% mean? Would a house of this size be
    predicted to sell with a probability of 150%??
  prefs: []
  type: TYPE_NORMAL
- en: Weird. What about a house that’s 2100 feet²?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa1e4447b865ed673ba46d6a5af76115.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay, clearly we have run into a problem as the predicted probability for a
    house with a size of 2100 feet² appears to be negative. This definitely does not
    make sense, and it indicates an issue with using a standard linear regression
    line.
  prefs: []
  type: TYPE_NORMAL
- en: As we know, the range of probabilities is from 0 to 1, and we cannot exceed
    this range. So we need to find a way to constrain our predicted output to this
    range.
  prefs: []
  type: TYPE_NORMAL
- en: To solve this issue, we can pass our linear regression equation through a super
    cool machine called a **sigmoid function**. This machine transforms our predicted
    values to fall between 0 and 1\. We input our z value (where z = β₀ + β₁size)
    into the machine…
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a47a8ad759320703836158194e3e49e2.png)'
  prefs: []
  type: TYPE_IMG
- en: …and out comes a fancy-looking new equation that will fit our probability constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE: The **e** in the output is a constant value and is approximately equal
    to 2.718.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A math-ier way of representing the `sigmoid function:`
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/74cdf30f641d27bb80c161fe77e2ccd7.png)'
  prefs: []
  type: TYPE_IMG
- en: If we plot this, we see that the sigmoid function squeezes the straight line
    into an s-shaped curve confined between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b4fb44ee2d49cf45e2a1ec4d21be1e97.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Optional note for all my math-heads: You might be wondering why and how we
    used the sigmoid function to get our desired output. Let’s break it down.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We started with the incorrect assumption that using the linear regression formula
    will give us our desired probability.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/5f064f58093036f48cd276d15f4f5939.png)'
  prefs: []
  type: TYPE_IMG
- en: The issue with this assumption is that (β₀ + β₁size) has range (-∞,+∞) and p
    has a range of [0,1]. So we need to find a value that has a range that matches
    that of (β₀ + β₁size).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To overcome this issue, we can equate the line to“log odds” (watch [this video](https://www.youtube.com/watch?v=ARfXDSkQf1Y)
    to understand log odds better) because we know that the log odds has a range of
    (-∞,+∞).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/8f6692ca17ad1202fcc58b922b18844b.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we did that, it’s just a matter of rearranging this equation, so that
    we find what the **p** value should equal.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/5fbbd4598163dbbe98a40bc732fed798.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we know how to modify the linear regression line so that it fits our
    output constraints, we can return to our original problem.
  prefs: []
  type: TYPE_NORMAL
- en: We need to determine the optimal curve for our dataset. To achieve this, we
    need to identify the optimal values for β₀ and β₁ (because these are the only
    values in the predicted probability equation that will change the shape of the
    curve).
  prefs: []
  type: TYPE_NORMAL
- en: Similar to linear regression, we will leverage a cost function and the gradient
    descent algorithm to obtain suitable values for these coefficients. The key distinction,
    however, is that we will not be employing the [**MSE** cost function used in linear
    regression](https://medium.com/towards-data-science/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46#e9d3).
    Instead, we will be using a different cost function called **Log Loss**, which
    we will explore in greater detail shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we used gradient descent and the **Log Loss** cost ([using these steps](https://medium.com/towards-data-science/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd))
    to find that our optimal values are β₀ = -120.6 and β₁ = 0.051, then our predicted
    probability equation will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/232b8ea908ccd4ac1bae77d7410152d0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And the corresponding optimal curve is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/58fa4bdd05234712432a9ffb7c9a1236.png)'
  prefs: []
  type: TYPE_IMG
- en: With this new curve, we can now tackle Mark’s problem. By looking at it, we
    can see that a house with a size of 2400 feet²…
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ccae1eadc01887044c62c6383efe6f5d.png)'
  prefs: []
  type: TYPE_IMG
- en: …has a predicted probability of approximately 78%. Therefore, we can tell Mark
    not to worry because it looks like his house is pretty likely to sell.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can further enhance our approach by developing a **Classification Algorithm**.
    A classification algorithm is commonly used in machine learning to categorize
    data into categories. In our case, we have two categories: houses that will sell
    and houses that will not sell.'
  prefs: []
  type: TYPE_NORMAL
- en: To develop a classification algorithm, we need to define a threshold probability
    value. This threshold probability value separates the predicted probabilities
    into two categories, “yes, the house will sell” and “no, the house will not sell.”
    Typically, 50% (or 0.5) is used as the threshold value.
  prefs: []
  type: TYPE_NORMAL
- en: If the predicted probability for a house size is above 50%, it will be classified
    as “will sell,” and if it’s below 50%, it will be classified as “won’t sell.”
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc471fc2fa66dbd718d80da68d68ff7a.png)'
  prefs: []
  type: TYPE_IMG
- en: And that’s about it. That’s how we can use logistic regression to solve our
    problem. Now let’s understand the cost function we used to find optimal values
    for logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Cost Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In linear regression, the cost is based on how far off the line was from our
    data points. And, in logistic regression, the cost function depends on how far
    off our predictions are from our actual data, *given that we are dealing with
    probabilities*.
  prefs: []
  type: TYPE_NORMAL
- en: If we used the [**MSE** cost function](/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46#e9d3)
    (like we did in linear regression) in logistic regression, we would end up with
    a ***non-convex*** (fancy term for a *not-so-pretty-curve-that-can’t-be-used-effectively-in-gradient-decsent*)
    [cost function curve that can be difficult to optimize.](https://medium.com/towards-data-science/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd#6754)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67ffabe81bd7f9aac79c1914332dfacf.png)'
  prefs: []
  type: TYPE_IMG
- en: And as you may recall from our discussion on gradient descent, it is much easier
    to optimize a ***convex*** (aka *a curve with a distinct minimum point*) curve
    like this than a non-convex curve.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04170f51d7864ff48bece76cd841b004.png)'
  prefs: []
  type: TYPE_IMG
- en: To achieve a convex cost function curve, we use a cost function called **Log
    Loss**.
  prefs: []
  type: TYPE_NORMAL
- en: To break down the **Log Loss** cost function, we need to define separate costs
    for when the house actually sold (y=1) and when it did not (y=0).
  prefs: []
  type: TYPE_NORMAL
- en: If `y = 1` and we predicted 1 (i.e., 100% probability it sold), there is no
    penalty. However, if we predicted 0 (i.e., 0% probability it didn’t sell), then
    we get penalized heavily.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af708abbd0e008d4369c95bef4cbeaa2.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarly, if `y = 0` and we predicted a high probability of the house selling,
    we should be penalized heavily, and if we predicted a low probability of the house
    selling, there should be a lower penalty. The more off we are, the more it costs
    us.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00788ee7542440a2f13f22c6f10da771.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To compute the cost for all houses in our dataset, we can average the costs
    of all the individual predictions like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/811f835e3c26340d6b3ddaffd6463b18.png)'
  prefs: []
  type: TYPE_IMG
- en: By cleverly rewriting the two equations, we can combine them into one to give
    us our **Log Loss** cost function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8bb50be56791d1f4c9c62e61b0be49f6.png)'
  prefs: []
  type: TYPE_IMG
- en: This works because one of those two will always be zero, so only the other one
    will be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'And the combined cost graph looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/317d3fd6fb24d0d7ed7747b5c2bcee1d.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have a good understanding of the math and intuition behind logistic
    regression, let’s see how Mark’s house size problem can be implemented in Python.
  prefs: []
  type: TYPE_NORMAL
- en: And we’re done! You have everything you need to tackle a logistic regression
    problem of your own now.
  prefs: []
  type: TYPE_NORMAL
- en: And as always, please feel free to reach out to me on [LinkedIn](https://www.linkedin.com/in/shreyarao24/)
    or shoot me an email at *shreya.statistics@gmail.com*.
  prefs: []
  type: TYPE_NORMAL
