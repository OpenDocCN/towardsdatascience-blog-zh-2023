- en: 'Data Pipelines with Polars: Step-by-Step Guide'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/data-pipelines-with-polars-step-by-step-guide-f5474accacc4](https://towardsdatascience.com/data-pipelines-with-polars-step-by-step-guide-f5474accacc4)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Build scalable and fast data pipelines with Polars
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@antonsruberts?source=post_page-----f5474accacc4--------------------------------)[![Antons
    Tocilins-Ruberts](../Images/363a4f32aa793cca7a67dea68e76e3cf.png)](https://medium.com/@antonsruberts?source=post_page-----f5474accacc4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f5474accacc4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f5474accacc4--------------------------------)
    [Antons Tocilins-Ruberts](https://medium.com/@antonsruberts?source=post_page-----f5474accacc4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f5474accacc4--------------------------------)
    ·14 min read·Jul 24, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bfaae191cbbf33d6ba469a6bffdb83bf.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Filippo Vicini](https://unsplash.com/@filippo_vicini?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The aim of this post is to explain and show you how to build data pipelines
    with Polars. It puts together and uses all the knowledge you’ve got from the previous
    two parts of this series, so if you haven’t gone through them yet, I highly recommend
    you start there and come back here later.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/eda-with-polars-step-by-step-guide-for-pandas-users-part-1-b2ec500a1008?source=post_page-----f5474accacc4--------------------------------)
    [## EDA with Polars: Step-by-Step Guide for Pandas Users (Part 1)'
  prefs: []
  type: TYPE_NORMAL
- en: Level up your data analysis with Polars
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/eda-with-polars-step-by-step-guide-for-pandas-users-part-1-b2ec500a1008?source=post_page-----f5474accacc4--------------------------------)
    [](/eda-with-polars-step-by-step-guide-to-aggregate-and-analytic-functions-part-2-a22d986315aa?source=post_page-----f5474accacc4--------------------------------)
    [## EDA with Polars: Step-by-Step Guide to Aggregate and Analytic Functions (Part
    2)'
  prefs: []
  type: TYPE_NORMAL
- en: Advanced aggregates and rolling averages at lightning speed with Polars
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/eda-with-polars-step-by-step-guide-to-aggregate-and-analytic-functions-part-2-a22d986315aa?source=post_page-----f5474accacc4--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find all the code in this [repository](https://github.com/aruberts/tutorials/tree/main/polars),
    so don’t forget to clone/pull and star it. In particular, we’ll be exploring this
    [file](https://github.com/aruberts/tutorials/blob/main/polars/data_preparation_pipeline.py)
    which means that we’ll finally move away from notebooks into the real world!
  prefs: []
  type: TYPE_NORMAL
- en: 'Data used in this project can be downloaded from [Kaggle](https://www.kaggle.com/datasets/datasnaek/youtube-new?resource=download&sort=published)
    (CC0: Public Domain). It’s the same YouTube trending dataset that was used in
    the previous two parts. I assume that you already have Polars installed, so just
    make sure to update it to the latest version using `pip install -U polars` .'
  prefs: []
  type: TYPE_NORMAL
- en: Data Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Put simply, a data pipeline is an automated sequence of steps that pulls the
    data from one or multiple locations, applies processing steps and saves the processed
    data elsewhere making it available for further use.
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines in Polars
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Polars way of working with data lends itself quite nicely to building scalable
    data pipelines. First of all, the fact that we can chain the methods so easily
    allows for some fairly complicated pipelines to be written quite elegantly.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s say we want to find out ***which trending videos had the
    most views in each month of 2018***. Below you can see a full pipeline to calculate
    this metric and to save it as a parquet file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Quite neat, right? If you know SQL this is very easy to read and understand.
    But can we make it even better? Of course, with Polars `.pipe()` method. This
    method gives us a structured way to apply sequential functions to the Data Frame.
    For this to work, let’s refactor the code above into functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Notice that these functions take a Polars DataFrame as input (together with
    some other arguments) and output already altered Polars Data Frame. Chaining these
    methods together is a piece of cake with `.pipe()` .
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: First of all, the reformatted code is much easier to understand. Second, separation
    of concerns is in general a good programming principle to follow as it allows
    easier debugging and cleaner code. For this simple example, it might be an overkill
    to modularise the pipeline but you’ll see how useful it is in the larger example
    in the next section. Now, let’s make the whole thing faster using **lazy mode**.
  prefs: []
  type: TYPE_NORMAL
- en: Lazy mode allows us to write the queries and pipelines, put them all together,
    and let the backend engine do its optimisation magic. For example, the code written
    above is definitely not optimal. I’ve put the column selection as the last step
    which means that the size of processed data is unnecessarily large. Lucky for
    us, Polars is smart enough to realise this, so it will optimise the code. Also,
    we only need to change two small things in the code to get the speed up which
    is incredible. First of all, we change `pl.read_csv` to `pl.scan_csv` to read
    the data in lazy mode. Then, at the end of the query, we put `.collect()` to tell
    Polars that we want to execute the optimised query
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: On my machine, I’ve got the ~ 3x speedup which is impressive given we’ve made
    two very simple edits. Now that you’ve got the concept of piping and lazy evaluations,
    let’s move on to a more complicated example.
  prefs: []
  type: TYPE_NORMAL
- en: Data Pipeline for Machine Learning Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Warning: There’s a lot of text and a lot of code! Sections should be followed
    sequentially since they build up the pipeline.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Given the dataset that we have at hand (YouTube Trending Videos), let’s build
    features that can be used in predicting **how long a video will be in trending.**
    Even though this might sound simple, the pipeline to create them is going to be
    quite complex. The final format of dataset should have one row per video ID, features
    that are available as soon as the video gets into trending, and the actual number
    of days that the video stayed in trending (target).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6bf7ae837c8dac8debee9bcf95439939.png)'
  prefs: []
  type: TYPE_IMG
- en: Mock final dataset format. Created by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'The features that could be useful in our prediction task are:'
  prefs: []
  type: TYPE_NORMAL
- en: Video characteristics (e.g. category)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of views, like, comments, etc. at the moment of entry into Trending
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Past performance of the channel in Trending (e.g. number of trending videos
    in the last 7 days)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General Trending characteristics (e.g. average time in trending for all the
    videos in the last 30 days)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Below you can see a diagram representation of the required pipeline to create
    this dataset (make sure to zoom in).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/afa2b618abfc31e1c1b02b0c5ba2c9f9.png)'
  prefs: []
  type: TYPE_IMG
- en: Data Pipeline Flow. Created by author.
  prefs: []
  type: TYPE_NORMAL
- en: I know it’s a lot to take in, so let’s eat this elephant a bite at a time. Below
    you can find the descriptions and code for every pipeline step. In addition, this
    pipeline will be parametrised using a YAML configuration file, so you’ll find
    the configuration parameters for every steps as well. Here’s how you’d read a
    YAML file named `pipe_config.yaml` which you can find in the repo.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'So, for every step of the pipeline you’ll find:'
  prefs: []
  type: TYPE_NORMAL
- en: Description of the step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relevant functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relevant configuration parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code to run the pipeline up to this step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This way, we’ll slowly build up to the full pipeline and you’ll have an in-depth
    understanding of what’s going on and how to create something similar for your
    own data.
  prefs: []
  type: TYPE_NORMAL
- en: Read Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The aim of this step is self explanatory — to read in the dataset for further
    processing. We have two inputs — a csv file with the main data and a json with
    the data about category mappings. The parameters for this step are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: No need to write a function to read the csv data (since it alreday exists in
    Polars), so the only function we write is for reading in the category mappings.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: With this function, the code to read in the required files is quite simple.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s move on to a very unexciting yet crucial step — data cleaning.
  prefs: []
  type: TYPE_NORMAL
- en: Data Cleaning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This dataset is already quite clean but we need to do some additional pre-processing
    to the dates and the category columns.
  prefs: []
  type: TYPE_NORMAL
- en: '`trending_date` and `publish_time` need to be formatted as `pl.datetime`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`category_id` needs to be mapped from ID to the actual category name'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polars need to know in which format the dates will be provided, so it’s best
    to encode the data formats with corresponding date columns in the `pipe_config.yaml`
    file to make it clear and easy to change.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Since we want to use Polars pipelines to modularise the code, we need to create
    two functions — `parse_dates` and `map_dict_columns` which will perform two required
    transformations. However, here’s the catch — splitting these operations into two
    steps makes the code slower because Polars can’t use parallelisation efficiently.
    You can test this yourself by timing the execution of these two Polars expressions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'For me, the first expression was ~2x slower which is very significant. So what
    do we do then? Well, here’s the secret:'
  prefs: []
  type: TYPE_NORMAL
- en: We should build up the expressions before passing them through the `.with_columns`
    method.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hence, the functions `parse_dates` and `map_dict_columns` should return lists
    of expressions instead of the transformed Data Frames. These expressions can be
    combined and applied in the final cleaning function that we’ll call `clean_data`
    .
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, now we only have one `.with_columns` operation which makes the
    code more optimised. Note that all the arguments to the functions are provided
    as dictionaries. This is because YAML gets read in as a dictionary. Now, let’s
    add the cleaning step to the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Clean, modular, fast — what not to like? Let’s move on to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Basic Feature Engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This step does some basic feature engineering on top of the clean data, namely:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculates ratio features — likes to dislikes, likes to views and comments to
    views
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculates difference in days between publishing and trending
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracts weekdays from `trending_date` column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s parametrise the calculation of these features in the configuration file.
    We want to specify the name of a feature we want to create and the corresponding
    columns in the dataset that should be used for calculation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Logic for the functions is still the same — build up expressions and pass them
    to `.with_columns` method. Hence, the functions `ratio_features` , `diff_features`
    and `date_features` are all called within the main function named `basic_feature_engineering`
    .
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the previous step, we just pass the main function to the `pipe` and
    provide to it all the required configurations as arguments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Nice, we’re half way through the pipeline! Now, let’s transform the dataset
    into the right format and finally calculate our target — days in trending.
  prefs: []
  type: TYPE_NORMAL
- en: Data Transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just a reminder that the original dataset has multiple entries per video since
    it details information for every day in trending. If a video stayed five days
    in trending, this video would appear five times in this dataset. Our goal is to
    end up with the dataset that has just one entry per video (refer to the image
    below).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ce3629ab065fa04ef5204cfe10f3af84.png)'
  prefs: []
  type: TYPE_IMG
- en: Data transformation step mockup. Created by author.
  prefs: []
  type: TYPE_NORMAL
- en: We can achieve this using a combination of `.groupby` and `.agg` methods. The
    only configuration parameter to specify here is for filtering the videos that
    took too long to get into trending since these videos are the outliers identified
    in [the previous part](/eda-with-polars-step-by-step-guide-to-aggregate-and-analytic-functions-part-2-a22d986315aa)
    of this series. After we get the table with `video_ids` and the corresponding
    target (days in trending) we also need to not forget to join the features from
    the original dataset since they won’t be carried over during the `groupby` operation.
    Hence, we’ll also need to specify which features to join and which columns should
    be the joining keys.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: To perform the required step, we’ll design two functions — `join_original_features`
    and `create_target_df` .
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the `groupby` operation to create the target and the `join_original_features`
    function are both run in `create_target_df` function since they both use the original
    dataset as input. This means that even though we have an intermediate output (`target`
    variable), we can still run this function in a `pipe` method with no issues.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: For the final step, let’s generate more advanced features using dynamic and
    rolling aggregates (covered in-depth in the [last post](/eda-with-polars-step-by-step-guide-to-aggregate-and-analytic-functions-part-2-a22d986315aa)).
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Aggregates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This step is responsible to generating time-based aggregates. The only configuration
    that we need to provide are the windows for the aggregates.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '**Rolling Aggregates**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s starting with rolling features. Below you can see an example with two
    lagged rolling features for a channel `abc` for a window of two days.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e9a06003999e420e769cbdf361befe5a.png)'
  prefs: []
  type: TYPE_IMG
- en: Rolling features example. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rolling features are extremely easy in Polars, all you need is `.groupby_rolling()`
    method and some aggregates within the `.agg()` namespace. The aggregates that
    can be useful are:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of previous videos in trending
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The average number of days in trending for the previously trending videos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum number of days in trending for the previously trending videos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this in mind, let’s build a function named `build_channel_rolling` that
    can take the required period as input, this way we’ll be able to to easily create
    rolling features for any period that we want, and outputs these required aggregates.
    The `by` argument should be `channel_title` because we want to create aggregates
    by channel and the index column should be `first_day_in_trending` since this is
    our main date columns. These two columns will also be used to join these rolling
    aggregates to the original Data Frame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `add_rolling_features` is a wrapper function that can be passed to our pipeline.
    It generates and joins the aggregates for the periods specified in the config.
    Now, let’s move on to the final feature generation step.
  prefs: []
  type: TYPE_NORMAL
- en: Period Aggregates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These aggregates are similar to the rolling ones, but they aim to measure general
    behaviour in the Trending tab.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c8d82746db65b1ec3b36821024b3ae7.png)'
  prefs: []
  type: TYPE_IMG
- en: Period features example. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'If rolling aggregates aimed to capture the past behaviour of a channel, these
    aggregates will capture general trends. This might be useful since the algorithm
    that’s used to determine who gets into Trending and for how long changes constantly.
    Hence, the aggregates that we want to create are:'
  prefs: []
  type: TYPE_NORMAL
- en: Number videos in trending last period
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average days in trending in the last period
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum days in trending in the last period
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The logic for functions is the same — we’ll create a function that builds these
    aggregates and a wrapper function that will build and join the aggregates for
    all the periods. Notice that we don’t specify `by` parameter because we want to
    calculate these features for all the videos per day. Also notice that we need
    to use `shift` on the aggregates since we want to use the features for the last
    period and not the current one.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Finally, let’s put this all together into our pipeline!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: I hope you’re as excited as I am because we’re almost there! The final step
    — writing out data.
  prefs: []
  type: TYPE_NORMAL
- en: Write Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Saving transformed data is a piece of cake since we can use `.save_parquet()`
    right after the `collect()` operation. Below you can see full code that the file
    `data_preparation_pipeline.py` contains.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We can run this pipeline like any other Python file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: On my laptop these steps take less than a half a second which is impressive
    given how many operations we’ve chained together and how many features are generated.
    Most importantly, the pipeline looks clean, is very easy to debug, and can be
    extended/changed/cut in no time. Great job us!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you’ve followed through the steps and got here — amazing job! Here’s a brief
    summary of what you should’ve learned in this post:'
  prefs: []
  type: TYPE_NORMAL
- en: How to chain multiple operations together into a pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to make this pipeline efficient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to structure your pipeline project and parametrise it using YAML file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure to apply these learnings to your own data. I recommend starting small
    (2–3 steps) and then expanding the pipeline as your needs grow. Make sure to keep
    it modular, lazy, and group as many operations into `.with_columns()` as possible
    to ensure proper parallelisation.
  prefs: []
  type: TYPE_NORMAL
- en: Not a Medium Member yet?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@antonsruberts/membership?source=post_page-----f5474accacc4--------------------------------)
    [## Join Medium with my referral link — Antons Tocilins-Ruberts'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Antons Tocilins-Ruberts (and thousands of other writers
    on Medium). Your membership fee directly…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@antonsruberts/membership?source=post_page-----f5474accacc4--------------------------------)
  prefs: []
  type: TYPE_NORMAL
