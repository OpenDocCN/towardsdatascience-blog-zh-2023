# 嵌入技术：ChatGPT的秘密武器

> 原文：[https://towardsdatascience.com/embeddings-chatgpts-secret-weapon-1870e590f32c](https://towardsdatascience.com/embeddings-chatgpts-secret-weapon-1870e590f32c)

## 嵌入技术，以及它们如何帮助ChatGPT预测下一个词

[](https://emmaccode.medium.com/?source=post_page-----1870e590f32c--------------------------------)[![Emma Boudreau](../Images/f7201d012b733643d6e97957f73fd1fa.png)](https://emmaccode.medium.com/?source=post_page-----1870e590f32c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1870e590f32c--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1870e590f32c--------------------------------) [Emma Boudreau](https://emmaccode.medium.com/?source=post_page-----1870e590f32c--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1870e590f32c--------------------------------) ·阅读时间5分钟·2023年3月6日

--

![](../Images/b73140f2b26fd1fb712cffe14b2f354e.png)

（图像由作者提供）

## **变换器**和**注意力机制**

如果你最近经常浏览网页或阅读技术新闻，你可能在某个时候听说过或读到过ChatGPT。ChatGPT是OpenAI的新**语言变换器**模型，就这些模型而言，这个模型相当准确，产生了一些非常引人注目的——有时甚至是病毒式的——结果。在这个上下文中，**变换器**指的是一种采用**自注意力机制**的机器学习模型。**自注意力机制**是一个数据科学术语，简单来说就是这个模型试图模拟人类的认知功能或人类的认知注意力。这个模型的***语言***部分也很重要；它描述了变换器希望预测的内容，即人类语言。这通常被称为**自然语言处理**，或**NLP**，虽然NLP通常指的是对语言数据的处理，以将其转换为计算机或神经网络可以理解的数值权重。

变换器具有一些显著的特性，使得它们的定义在讨论语言变换器如 ChatGPT 时特别重要。变换器以及更广泛的注意力模型，拥有一些特征，这些特征并非每种机器学习模型所特有。在正常情况下，机器学习模型适配某些数据并生成权重；我们可以把这看作编程语言编译可执行文件的过程：一旦可执行文件编译完成，它变得**静态**和**不可变——无法更改**，我们无法从内部调整代码。另一方面，变换器具有**软权重**，这更像是使用动态类型编程语言的 REPL，模型权重**是可变的**，并且可以在运行时进行更改。这是许多不同且有用的模型类型的基础，如**长短期记忆**（**LSTM**）模型，以及我们今天讨论的变换器。

![](../Images/1a98179807beafbc730ef9ae844f079e.png)

这张图展示了变换器模型的架构。 ([图片来自维基共享资源](https://en.wikipedia.org/wiki/File:The-Transformer-model-architecture.png))

## embeddings

这引出了 embeddings，它们为这些软权重提供了更多的力量和能力。Embeddings 通过创建一个维度低于实际编码稀疏向量的新的维度层来工作。这可以被认为是对这些数据的一种分组，这在模型的最终计算中起到作用。实际上，embeddings 是一个低维空间，为更大的高维向量提供了节奏。我们几乎是在模型中加入了一个新的特征，我们的模型可能将其作为分类来推断数据中的更多细节——这在解释诸如人类语言这样复杂的事物时尤为重要。在这种情况下，维度指的是数据的维度或形状。理解 embeddings 的一种很好的方式是把它想象成一个飞镖靶。

在一个房间里，我们有一个飞镖板。我们希望利用这个飞镖板来预测给定房间内住客的技能水平。我们发现，非常差的飞镖手往往击中飞镖板的底部，而技能稍好的人往往击中顶部，非常有技巧的人则击中中间。实际击中位置将是我们的原始特征，我们将为其构建权重和概率。然而，如果我们给这些区域标记，并将它们与人的技能水平关联，以对这些数据做出更普遍的推断，这可能有助于我们在该上下文中做出更细致的预测，这就是嵌入的概念。嵌入成为一个点，数据本身，在确定其在这个低维空间中与其他嵌入的相似度的轴上。我们可以使用这来预测例如一个飞镖手已经玩了多少年。

![](../Images/77ea2fdc13d9657eab3e9b3cec011be7.png)

一个简化的示意图，展示了这在你的模型中的样子。（图片由作者提供）

从本质上讲，嵌入告诉我们关于一组数据的信息是，这组数据中的数据与该嵌入中的其他数据相似。嵌入本质上只是其他数据上的分类数据。另一个需要注意的点是，这些分类也可以是多维的，意味着可以有多个嵌入，并且权重可能绑定到相同的嵌入上。嵌入也可以从数据中学习，这意味着这可以是神经网络的一部分，而不会添加很多内容，使它们成为许多应用中的简单选择，例如 ChatGPT 等变压器。

OpenAI 有他们自己的嵌入端点，这使得执行自然语言任务、主题建模、分类，甚至聚类变得非常简单。如果你想了解更多关于 OpenAI 嵌入的内容，下面是一个详细讨论此主题的论文链接：

[](https://arxiv.org/abs/2201.10005?source=post_page-----1870e590f32c--------------------------------) [## 文本和代码嵌入的对比预训练

### 文本嵌入在许多应用中都是有用的特征，例如语义搜索和计算文本相似性……

arxiv.org](https://arxiv.org/abs/2201.10005?source=post_page-----1870e590f32c--------------------------------)

很容易想象这些嵌入在处理文本和创建准确的语言模型预测中所扮演的角色。词语确实可以被分类，当你考虑到英语词汇的宏观图景时，理解词语实际作用可能会非常困难。然而，如果我们将这些词语分解为类别：冠词、名词、动词，那么了解语法如何在我们的语言中实际运作就会容易得多。对于语言模型来说，词嵌入的目标是捕捉词语在向量表示中的意义。我们为词语的意义创建一个通用分类，然后利用我们对该类别的了解来对输出做出推断。此外，ChatGPT 用于表示文本的粒度是子词，而不是整个词。因此，ChatGPT 利用这些类型的嵌入来对词语的某些部分进行分类和描述。

## 结束

嵌入是许多不同类型机器学习模型的核心概念：推荐算法、语言转换器，甚至分类模型都是从拥有嵌入层中受益的例子。OpenAI 的嵌入实现帮助 ChatGPT 模型根据类别及其与这些类别的数值关系来解释词语，这比尝试从每个单独词语中找出见解要容易得多。如果你想了解更多关于嵌入的内容，并且如何将它们应用到自己的 Tensorflow 网络中，我强烈建议你查看 Google 几年前发布的嵌入教程，这里有一个链接：

[](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture?source=post_page-----1870e590f32c--------------------------------) [## 嵌入 | 机器学习 | Google 开发者

### 嵌入是一个相对低维的空间，你可以将高维向量转换到其中。嵌入…

developers.google.com](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture?source=post_page-----1870e590f32c--------------------------------)

感谢阅读！
