- en: 'The Ultimate Guide to Training BERT from Scratch: The Tokenizer'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始训练BERT的终极指南：分词器
- en: 原文：[https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822)
- en: 'From Text to Tokens: Your Step-by-Step Guide to BERT Tokenization'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从文本到词元：你的BERT分词逐步指南
- en: '[](https://dpoulopoulos.medium.com/?source=post_page-----ddf30f124822--------------------------------)[![Dimitris
    Poulopoulos](../Images/ce535a1679779f5a2ec8b024e6691e50.png)](https://dpoulopoulos.medium.com/?source=post_page-----ddf30f124822--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ddf30f124822--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ddf30f124822--------------------------------)
    [Dimitris Poulopoulos](https://dpoulopoulos.medium.com/?source=post_page-----ddf30f124822--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dpoulopoulos.medium.com/?source=post_page-----ddf30f124822--------------------------------)[![Dimitris
    Poulopoulos](../Images/ce535a1679779f5a2ec8b024e6691e50.png)](https://dpoulopoulos.medium.com/?source=post_page-----ddf30f124822--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ddf30f124822--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ddf30f124822--------------------------------)
    [Dimitris Poulopoulos](https://dpoulopoulos.medium.com/?source=post_page-----ddf30f124822--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ddf30f124822--------------------------------)
    ·13 min read·Sep 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ddf30f124822--------------------------------)
    ·阅读时间13分钟·2023年9月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5e6bf3a97ca45029e42154b1648e0172.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e6bf3a97ca45029e42154b1648e0172.png)'
- en: Photo by [Glen Carrie](https://unsplash.com/@glencarrie?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/oHoBIbDj7lo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Glen Carrie](https://unsplash.com/@glencarrie?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/photos/oHoBIbDj7lo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: '[Part I](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f),
    [Part III](/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5),
    and [Part IV](/the-ultimate-guide-to-training-bert-from-scratch-final-act-eab78b0657bb)
    of this story are now live.'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本文的 [第一部分](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f)、[第三部分](/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5)
    和 [第四部分](/the-ultimate-guide-to-training-bert-from-scratch-final-act-eab78b0657bb)
    现已上线。
- en: Did you know that the way you tokenize text can make or break your language
    model? Have you ever wanted to tokenize documents in a rare language or a specialized
    domain? Splitting text into tokens, it’s not a chore; *it’s a gateway to transforming
    language into actionable intelligence.* This story will teach you everything you
    need to know about tokenization, not only for BERT but for any LLM out there.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道吗？分词的方式可以决定你的语言模型的成败。你是否曾想过对罕见语言或专业领域的文档进行分词？将文本拆分成词元，这不仅仅是一项任务；*它是将语言转化为可操作智能的门户。*
    本文将教你关于分词的所有知识，不仅是针对BERT，还有任何大型语言模型。
- en: 'In my last story, we talked about BERT, explored its theoretical foundations
    and training mechanisms, and discussed how to fine-tune it and create a questing-answering
    system. Now, as we go further into the intricacies of this groundbreaking model,
    it’s time to spotlight one of the unsung heroes: *tokenization*.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我上一篇文章中，我们讨论了BERT，探讨了它的理论基础和训练机制，并讨论了如何微调它并创建问答系统。现在，随着我们深入了解这个开创性模型的复杂性，是时候关注一个被忽视的英雄：*分词*。
- en: '[](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----ddf30f124822--------------------------------)
    [## The Ultimate Guide to Training BERT from Scratch: Introduction'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----ddf30f124822--------------------------------)
    [## 从零开始训练BERT的终极指南：引言'
- en: 'Demystifying BERT: The definition and various applications of the model that
    changed the NLP landscape.'
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解密BERT：改变NLP领域的模型的定义和各种应用。
- en: towardsdatascience.com](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----ddf30f124822--------------------------------)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----ddf30f124822--------------------------------)
- en: '[Part III](/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5)
    of this story is now live.'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[第三部分](/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5)
    现在已上线。'
- en: I get it; tokenization might seem like the last boring obstacle between you
    and the thrilling process of training your model. Believe me, I used to think
    the same. But I’m here to tell you that tokenization is not just a “necessary
    evil”— *it’s an art form in its own right.*
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我明白了；分词可能看起来是你和训练模型这一激动人心过程之间的最后一个乏味障碍。相信我，我曾经也这样认为。但我在这里告诉你，分词不仅仅是一个“必要的邪恶”——*它本身就是一种艺术形式*。
- en: In this story, we’ll examine every part of the tokenization pipeline. Some steps
    are trivial (like normalization and pre-processing), while others, like the modeling
    part, are what make each tokenizer unique.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个故事中，我们将检查分词管道的每一个部分。有些步骤很简单（如规范化和预处理），而其他步骤，如建模部分，则是使每个分词器独特的部分。
- en: '![](../Images/3d93f5ab57125747b9d469ab16241500.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d93f5ab57125747b9d469ab16241500.png)'
- en: Tokenization pipeline — Image by Author
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 分词管道 — 作者图片
- en: '**By the time you finish reading this article, you’ll not only understand the
    ins and outs of the BERT tokenizer, but you’ll also be equipped to train it on
    your own data.** And if you’re feeling adventurous, you’ll even have the tools
    to customize this crucial step when training your very own BERT model from scratch.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**当你读完这篇文章时，你不仅会了解BERT分词器的方方面面，还会掌握如何在自己的数据上训练它**。如果你感到冒险，你甚至会拥有在从头开始训练自己的BERT模型时定制这个关键步骤的工具。'
- en: Splitting text into tokens, it’s not a chore; *it’s a gateway to transforming
    language into actionable intelligence.*
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将文本拆分成词元，这不是一项繁琐的工作；*它是将语言转化为可操作的智能的门户*。
- en: 'So, why is tokenization so critical? *At its essence, tokenization is a translator;*
    it takes in human language and translates it to the language machines can understand:
    numbers. But there’s a catch: During this translation process, the tokenizer must
    keep a crucial balance, finding the sweet spot between meaning and computational
    efficiency. So, you see, it’s not just about crunching numbers; *it’s about efficiently
    capturing the essence of language in a form that machines can comprehend.*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么分词如此关键？*本质上，分词是一个翻译器*；它接受人类语言并将其翻译成机器可以理解的语言：数字。但有一个难点：在这个翻译过程中，分词器必须保持一个重要的平衡，找到意义和计算效率之间的最佳点。所以，你看，这不仅仅是关于计算数字；*它是关于高效地捕捉语言本质，使机器能够理解*。
- en: Now that we have our premise let’s start by becoming acquainted with the different
    types of tokenizers. The first hurdle in this journey is settling on a definition
    for what constitutes a ‘word’ — and it might not be as straightforward as what
    you or I typically consider when we discuss language.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们有了前提，让我们开始熟悉不同类型的分词器。这个旅程的第一个障碍是确定什么构成一个“单词”——这可能不像你我在讨论语言时通常考虑的那样简单。
- en: '[Learning Rate](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-tokenization)
    is a newsletter for those who are curious about the world of ML and MLOps. If
    you want to learn more about topics like this subscribe [here](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-tokenization).
    You’ll hear from me on the last Sunday of every month with updates and thoughts
    on the latest MLOps news and articles!'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[学习率](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-tokenization)
    是一份面向对ML和MLOps世界感兴趣的人的通讯。如果你想了解更多类似的话题，可以在[这里](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-tokenization)订阅。每个月的最后一个星期天你将收到我关于最新MLOps新闻和文章的更新和想法！'
- en: Types of Tokenization
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词的类型
- en: Since the modeling step of the tokenization pipeline is the more demanding one,
    it’s crucial to approach it while the day is still young.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于分词管道的建模步骤是更具挑战性的，因此在白天还年轻时处理它至关重要。
- en: There are various approaches to building a translator between human and machine
    language. Let’s explore the three main types and weigh their pros and cons to
    understand why subword tokenization is often the go-to method in today’s NLP landscape.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 建立人类和机器语言之间的翻译器有多种方法。让我们探讨三种主要类型并权衡它们的利弊，以理解为什么子词分词在当今NLP领域中往往是首选方法。
- en: Word-based Tokenizers
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于单词的分词器
- en: The word-based approach is the simplest, essentially slicing up raw text into
    words based on whitespace or other delimiters like punctuation. Think of Python’s
    `split()` function as a classic example.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 基于词的方法是最简单的，本质上是根据空格或其他分隔符（如标点符号）将原始文本切割成单词。可以把Python的`split()`函数看作是一个经典例子。
- en: '![](../Images/000081618b093f1b79fca5182e0f7616.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/000081618b093f1b79fca5182e0f7616.png)'
- en: Word-based tokenization-Image by Author
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 基于词的分词 — 作者图像
- en: However, this method has its limitations. To truly grasp a language in all its
    complexity, you'd need to manage a vast token vocabulary—English alone boasts
    over half a million words. Moreover, this method struggles with inflected forms;
    words like "dog" and "dogs" are treated as completely separate entities. And if
    you choose to limit your vocabulary size, the "unknown" token, a placeholder for
    any word that's not in there, will be all over the place, polluting your data’s
    meaning.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法有其局限性。要真正掌握语言的复杂性，你需要管理一个庞大的词汇表——仅英语就有超过五十万单词。此外，这种方法对屈折形式的处理也存在困难；像“dog”和“dogs”这样的词被视为完全不同的实体。而且如果你选择限制词汇表的大小，"unknown"标记，即任何未包含的词的占位符，将会随处可见，污染数据的含义。
- en: Character-based Tokenizers
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于字符的分词器
- en: On the other end of the spectrum, we have character-based tokenizers, which
    break down text into individual characters. This drastically reduces the vocabulary
    size and nearly eliminates the problem of unknown tokens. For example, the English
    language has 26 characters. If you add punctuation and other symbols, you’re looking
    at a vocabulary of a size into the hundreds.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在光谱的另一端，我们有基于字符的分词器，它将文本拆分成单个字符。这大大减少了词汇表的大小，并几乎消除了未知标记的问题。例如，英语有26个字符。如果加上标点符号和其他符号，词汇表的大小会达到数百。
- en: '![](../Images/713c122e1b7e818d35b5662a8d60f028.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/713c122e1b7e818d35b5662a8d60f028.png)'
- en: Character-based tokenizers — Image by Author
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于字符的分词器 — 作者图像
- en: 'But it comes with a cost: characters in isolation often lack meaningful context,
    and this method can result in cumbersome sequences that are computationally expensive
    to process since the context the model has to keep in its cash grows really fast.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 但这也有代价：孤立的字符通常缺乏有意义的上下文，这种方法可能会导致冗长的序列，从而增加了计算处理的复杂性，因为模型必须在其缓存中保持的上下文增长得非常快。
- en: Subword Tokenization
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 子词分词
- en: Subword tokenization is the middle ground that often offers the best of both
    worlds. By dissecting rare or complex words into smaller, meaningful units— like
    breaking “annoyingly” into “annoy”, “ing”, and “ly” — this method strikes a balance
    between efficiency and expressiveness. It offers compact vocabularies without
    sacrificing the richness of language, making it particularly useful for languages
    where words can be naturally composed of smaller, meaningful pieces.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 子词分词是一种折衷方法，通常提供两全其美的解决方案。通过将稀有或复杂的词拆解成更小的有意义的单位——例如将“annoyingly”拆分为“annoy”，“ing”和“ly”——这种方法在效率和表现力之间取得了平衡。它提供了紧凑的词汇表而不牺牲语言的丰富性，特别适用于那些词汇可以自然组成更小的、有意义的部分的语言。
- en: '![](../Images/3ac7506d27e6b2368180e32f8265eb37.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ac7506d27e6b2368180e32f8265eb37.png)'
- en: Subword tokenizers — Image by Author
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 子词分词器 — 作者图像
- en: Note that in the figure, we skip most of the steps in the pipeline for the sake
    of simplicity. In a real-world scenario, most of the time the tokenizer has already
    transformed each word to lower case.
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请注意，在图中，为了简单起见，我们跳过了管道中的大部分步骤。在实际应用中，大多数情况下分词器已经将每个词转换为小写。
- en: Take the word “annoyingly” as a case in point for the power of subword tokenization.
    When split into three distinct subtokens — “annoy”, “ing”, and “ly”, each fragment
    serves as a mini-lesson in language comprehension for the model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以“annoyingly”这个词为例，展示了子词分词的强大。当将其拆分为三个独立的子标记——“annoy”，“ing”和“ly”时，每个片段都为模型提供了语言理解的小课程。
- en: The first subtoken, “annoy,” instructs the tokenizer about the root verb, offering
    a base layer of meaning. The second subtoken, “ing”, captures a common suffix
    that serves multiple grammatical functions. It could indicate an ongoing action
    (as in “running”), the process or result of an action (as in “building”), or even
    help form nouns (as in “painting”). Finally, the “ly” subtoken is a clue to the
    tokenizer about how adverbs or adjectives are often constructed in English, helping
    the model understand that the word modifies an action or describes a characteristic
    of a noun.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个子标记“annoy”指示分词器关于根动词的基本含义。第二个子标记“ing”捕捉了一个常见的后缀，该后缀具有多种语法功能。它可以表示正在进行的动作（如“running”），动作的过程或结果（如“building”），甚至有助于形成名词（如“painting”）。最后，“ly”子标记则向分词器提示副词或形容词在英语中的常见构造方式，帮助模型理解这个词修饰动作或描述名词的特征。
- en: So, why has subword tokenization become the gold standard? *It efficiently navigates
    the trade-offs of its word-based and character-based counterparts.* You get a
    vocabulary with almost zero unknown tokens and a system that’s robust enough to
    capture the nuances of human language, even in its most intricate forms.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么子词分词已经成为金标准？*它有效地解决了基于单词和基于字符的对比的权衡问题。* 你会得到一个几乎没有未知标记的词汇表，以及一个足够强大的系统，能够捕捉到人类语言中最复杂形式的细微差别。
- en: BERT uses a subword tokenization method known as “WordPiece”. The rest of this
    story explores how this method works and how you can train it yourself.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 使用一种称为“WordPiece”的子词分词方法。接下来的内容将探讨这种方法是如何工作的以及如何自行训练它。
- en: The WordPiece Tokenizer
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: WordPiece 分词器
- en: 'So, let’s build the WordPiece tokenizer from scratch to understand everything
    that’s going under the hood. **Our approach will be twofold: first, we’ll construct
    a mental framework by employing various illustrations to clarify the concepts.
    Then, we’ll put theory into practice by training our very own tokenizer on a custom
    corpus, utilizing the** `[**tokenizers**](https://huggingface.co/docs/tokenizers/index)`
    **library.**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们从头开始构建 WordPiece 分词器，以理解所有的内部工作原理。**我们的方法将是双管齐下：首先，通过使用各种插图构建一个心理框架，以澄清概念。然后，我们将通过在自定义语料库上训练我们自己的分词器，将理论付诸实践，利用**
    `[**tokenizers**](https://huggingface.co/docs/tokenizers/index)` **库。**
- en: 'To kick things off, we need a corpus — a dataset of text that our tokenizer
    will learn from. Let’s consider the following paragraph as our starting point:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要一个语料库——一个文本数据集，我们的分词器将从中学习。我们以以下段落作为起点：
- en: The WordPiece algorithm serves a crucial role in the architecture of BERT, a
    state-of-the-art language model. Specifically, WordPiece is responsible for the
    tokenization process, breaking down text into smaller, manageable units or tokens.
    This tokenization step is critical during the pre-training phase of BERT, allowing
    the model to effectively learn the relationships between words or sub-words. By
    using WordPiece for tokenization, BERT can be more flexible in handling various
    linguistic constructs and nuances.
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: WordPiece 算法在 BERT 这一前沿语言模型的架构中扮演了至关重要的角色。具体而言，WordPiece 负责分词过程，将文本分解成更小、更易管理的单位或标记。这个分词步骤在
    BERT 的预训练阶段至关重要，使模型能够有效地学习单词或子词之间的关系。通过使用 WordPiece 进行分词，BERT 在处理各种语言结构和细微差别时变得更加灵活。
- en: 'So, the first step is to count the appearances of each word in our corpus and
    split them into the smallest possible unit:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，第一步是统计我们语料库中每个单词的出现次数，并将它们拆分成最小的单位：
- en: '![](../Images/3b7254334a16a7d24aa4338cd8b02028.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b7254334a16a7d24aa4338cd8b02028.png)'
- en: Image by Author
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: 'In our hands-on exercise, you’ll notice that we initially split words into
    individual characters. But that’s not the whole story: we prepend the symbol `##`
    to characters that originally appeared in the middle of a word, setting them apart
    from those that were initial letters. You’ll see later why this is important.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实践练习中，你会注意到我们最初将单词拆分成单个字符。但这还不是全部：我们在原本出现在单词中间的字符前加上符号 `##`，将它们与那些作为单词开头字母的字符区分开。稍后你会看到这为什么很重要。
- en: 'If we collect all the splits, we create our initial tokenizer vocabulary. WordPiece
    starts from the smallest possible vocabulary you could have (that of single characters)
    and keeps expanding it to a limit that we set. How does it do that? I’m glad you
    asked! Let’s see; the next step is to identify each possible pair of characters
    in our vocabulary:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们收集所有拆分，我们将创建我们的初始分词器词汇表。WordPiece 从可能的最小词汇表（即单个字符的词汇表）开始，并将其扩展到我们设定的限制。它是如何做到的呢？很高兴你问了！让我们看看，下一步是识别我们词汇表中每一个可能的字符配对：
- en: '![](../Images/89c97b95fc626c28d2018327ff3bc78b.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89c97b95fc626c28d2018327ff3bc78b.png)'
- en: Image by Author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'Now, we need to assign a score to each pair. The formula that calculates the
    score goes as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要为每个配对分配一个得分。计算得分的公式如下：
- en: '![](../Images/fe50e99ce65ed76fc5baded27f1778b5.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe50e99ce65ed76fc5baded27f1778b5.png)'
- en: Image by Author
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'Thus, if we try to calculate the score of the first pair (`T, ##h`) we get
    the following:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，如果我们尝试计算第一个配对（`T, ##h`）的得分，我们得到以下结果：'
- en: '![](../Images/fe1e2fc19c13459df48f59cf0eb04fa9.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe1e2fc19c13459df48f59cf0eb04fa9.png)'
- en: Image by Author
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'Using the same approach, we calculate the score for every pair and identify
    the pair with the highest score. In our case, this is the pair (`##E, ##R`), so
    next, we can add this pair to our vocabulary and update the splits by merging
    these characters. Our new vocabulary has now one more token:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '使用相同的方法，我们计算每个配对的得分，并识别得分最高的配对。在我们的例子中，这个配对是（`##E, ##R`），所以接下来，我们可以将这个配对添加到我们的词汇表中，并通过合并这些字符更新拆分。我们的新词汇表现在多了一个标记：'
- en: '![](../Images/d73957a027ee55c51995a7219113ac8d.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d73957a027ee55c51995a7219113ac8d.png)'
- en: Image by Author
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Next, we go back to our splits again, identify every possible pair, score it,
    and append the pair with the highest score to our vocabulary. What we’re doing
    here is essentially a form of optimization. By identifying commonly appearing
    pairs and treating them as single units, we make the tokenizer more efficient.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们重新回到我们的拆分，识别每一个可能的配对，对其打分，并将得分最高的配对添加到我们的词汇表中。我们在这里做的本质上是一种优化。通过识别常见的配对并将其视为单一单位，我们使分词器更加高效。
- en: 'We continue this process of identifying, scoring, and appending high-scoring
    pairs until we hit the target number of tokens we wish to include in our vocabulary.
    So, by setting the desired number of tokens to `60`, we get the following vocabulary:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续这个识别、评分和添加高分配对的过程，直到我们达到希望在词汇表中包含的目标标记数量。因此，通过将期望的标记数量设置为`60`，我们得到了以下词汇表：
- en: ‘##E’, ‘##P’, ‘##R’, ‘##T’, ‘##a’, ‘##b’, ‘##c’, ‘##d’, ‘##e’, ‘##f’, ‘##g’,
    ‘##h’, ‘##i’, ‘##k’, ‘##l’, ‘##m’, ‘##n’, ‘##o’, ‘##p’, ‘##r’, ‘##s’, ‘##t’, ‘##u’,
    ‘##v’, ‘##w’, ‘##x’, ‘##y’, ‘##z’, ‘,’, ‘-’, ‘.’, ‘B’, ‘S’, ‘T’, ‘W’, ‘a’, ‘b’,
    ‘c’, ‘d’, ‘e’, ‘f’, ‘h’, ‘i’, ‘l’, ‘m’, ’n’, ‘o’, ‘p’, ‘r’, ‘s’, ‘t’, ‘u’, ‘v’,
    ‘w’, ‘##ERT’
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ‘##E’，‘##P’，‘##R’，‘##T’，‘##a’，‘##b’，‘##c’，‘##d’，‘##e’，‘##f’，‘##g’，‘##h’，‘##i’，‘##k’，‘##l’，‘##m’，‘##n’，‘##o’，‘##p’，‘##r’，‘##s’，‘##t’，‘##u’，‘##v’，‘##w’，‘##x’，‘##y’，‘##z’，‘，’，‘-’，‘。’，‘B’，‘S’，‘T’，‘W’，‘a’，‘b’，‘c’，‘d’，‘e’，‘f’，‘h’，‘i’，‘l’，‘m’，‘n’，‘o’，‘p’，‘r’，‘s’，‘t’，‘u’，‘v’，‘w’，‘##ERT’
- en: 'Finally, to calculate the subtokens of a word, we work from the beginning of
    the string, trying to identify the longest possible sequence that appears in our
    vocabulary. So, for example, the word `BERT` is split into the following subwords:
    `B`, `##ERT`:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，为了计算一个词的子标记，我们从字符串的开头开始，尝试识别出我们词汇表中出现的最长可能序列。例如，单词`BERT`被拆分为以下子词：`B`，`##ERT`：
- en: '![](../Images/02190ed0c0c2b836b2a91e4de77984c8.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02190ed0c0c2b836b2a91e4de77984c8.png)'
- en: Image by Author
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'As the finishing touch to our tokenizer’s vocabulary, we introduce a set of
    special tokens that serve specific functions within the BERT model and serve as
    linguistic markers. Generally, these special tokens include: `[[PAD], [UNK], [CLS],
    [SEP], [MASK]]`.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们分词器词汇表的最后润色，我们引入了一组特殊标记，这些标记在BERT模型中具有特定功能，并作为语言学标记。通常，这些特殊标记包括：`[[PAD],
    [UNK], [CLS], [SEP], [MASK]]`。
- en: 'Here’s a quick rundown of what each special token is designed to do:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是每个特殊标记设计用途的简要说明：
- en: '`[PAD]`: Padding token used to equalize the lengths of different input sequences.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[PAD]`：用于平衡不同输入序列长度的填充标记。'
- en: '`[UNK]`: Stands for “unknown” and is used to handle words that are not in the
    vocabulary.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[UNK]`：表示“未知”，用于处理不在词汇表中的词。'
- en: '`[CLS]`: Short for “classification”, this token is prepended to the input when
    the model is used for classification tasks.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[CLS]`：即“分类”的缩写，这个标记在模型用于分类任务时会被加在输入的前面。'
- en: '`[SEP]`: Separator token, often used to indicate the end of one sentence and
    the beginning of another in multi-sentence tasks.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[SEP]`：分隔符令牌，通常用于指示多句任务中一句话的结束和另一句话的开始。'
- en: '`[MASK]`: Used in the training process for the BERT model to indicate positions
    where the model should predict a missing word.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[MASK]`：在BERT模型的训练过程中使用，用于指示模型应预测缺失单词的位置。'
- en: That’s it! You now know how WordPiece, BERT’s tokenizer, learns to split each
    work into subwords. Next, let’s train a custom one on our corpus.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你现在知道了WordPiece，BERT的分词器，如何将每个单词拆分成子词。接下来，让我们在我们的语料库上训练一个自定义的分词器。
- en: Training WordPiece
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练WordPiece
- en: As a native Greek speaker, I’m particularly excited about the prospect of training
    BERT’s WordPiece tokenizer for the Greek language. And I would strongly encourage
    you to do the same if your native tongue isn’t English. *Localizing these powerful
    models not only contributes to the diversification of NLP technologies but also
    offers a chance to tailor them to the nuances of your own language or dialect.*
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名母语为希腊语的讲者，我对为希腊语训练BERT的WordPiece分词器的前景感到特别兴奋。如果你的母语不是英语，我强烈建议你也这么做。*本地化这些强大的模型不仅有助于自然语言处理技术的多样化，还提供了将它们调整到你自己语言或方言细微差别的机会。*
- en: But if you are an English speaker, consider honing your tokenizer for a specific
    domain or industry. For example, you could train it on legal documents to capture
    the idiosyncrasies of legal jargon. Or how about gearing it towards understanding
    Python code?
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你是英语讲者，可以考虑将你的分词器针对特定领域或行业进行优化。例如，你可以在法律文件上训练它，以捕捉法律术语的独特性。或者尝试让它理解Python代码？
- en: 'In my case, I need to find and download the dataset. These days getting the
    dataset you need is easier than ever, thanks to the Hugging Face Hub. I’ll work
    with the `greek_legal_code` dataset:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，我需要找到并下载数据集。如今，获取所需的数据集比以往任何时候都要容易，感谢Hugging Face Hub。我将使用`greek_legal_code`数据集：
- en: '[PRE0]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, let’s quickly create our training corpus. For this, we’ll use only the
    first `1000` examples of the `train` split:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们快速创建我们的训练语料库。为此，我们只使用`1000`个`train`拆分的示例：
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, let’s get a handle on the pre-trained BERT tokenizer:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们了解一下预训练的BERT分词器：
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To compare the before and after, let’s see how the original BERT tokenizer
    would split the Greek intro of Wikipedia on [Machine Learning](https://el.wikipedia.org/wiki/%CE%9C%CE%B7%CF%87%CE%B1%CE%BD%CE%B9%CE%BA%CE%AE_%CE%BC%CE%AC%CE%B8%CE%B7%CF%83%CE%B7):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较前后效果，让我们看看原始BERT分词器如何对希腊语维基百科的[机器学习](https://el.wikipedia.org/wiki/%CE%9C%CE%B7%CF%87%CE%B1%CE%BD%CE%B9%CE%BA%CE%AE_%CE%BC%CE%AC%CE%B8%CE%B7%CF%83%CE%B7)进行分词：
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The result is shown below. In total, we get 274 different tokens:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下所示。总共，我们得到274个不同的令牌：
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, let’s train the tokenizer to understand Greek better. We’ll use a vocabulary
    of `50000` words:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们训练分词器以更好地理解希腊语。我们将使用`50000`个单词的词汇表：
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, repeat the test:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，重复测试：
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The results are amazing; we reduced the number of tokens to `67`, while the
    tokenizer seems to understand the nuances of the Greek language better now:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 结果非常惊人；我们将令牌的数量减少到了`67`，而且分词器似乎更好地理解了希腊语的细微差别：
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: And there you have it! You’re now fully equipped to train a tokenizer tailored
    to your own corpus. But don’t stop there — feel free to experiment with different
    algorithms altogether! Want to switch lanes and explore another approach? You
    can easily load the GPT tokenizer, which employs the Byte Pair Encoding (BPE)
    algorithm instead of WordPiece. The sky really is the limit when it comes to fine-tuning
    and customizing your tokenizer.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你现在已经完全准备好训练一个符合你自己语料库的分词器。但不要止步于此——可以随意尝试完全不同的算法！想要切换方向，探索另一种方法？你可以轻松加载GPT分词器，它使用的是字节对编码（BPE）算法，而不是WordPiece。在微调和定制分词器方面，天空才是极限。
- en: Conclusion
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: What a journey it’s been! From diving into the intricate details of tokenization
    to building our very own WordPiece tokenizer, we’ve covered a lot of ground.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这真是一段旅程！从深入了解分词的复杂细节，到构建我们自己专属的WordPiece分词器，我们已经覆盖了很多内容。
- en: If you’ve followed along, you’re now equipped not just with theoretical knowledge
    but with the hands-on experience to create a tokenizer tailored to your needs.
    Maybe you’ll adapt it for your native language, perhaps for an industry-specific
    application, or even for something as intricate as source code analysis.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你跟随了整个过程，你现在不仅拥有理论知识，还有实际的经验来创建一个符合你需求的分词器。也许你会为你的母语适配它，或者为某个行业特定的应用，甚至是像源代码分析这样复杂的任务。
- en: Remember, the principles we’ve explored here extend beyond any single language
    or domain. They’re the foundations upon which all Natural Language Processing
    tasks are built. By understanding and mastering them, you’re setting yourself
    up for success in a field that’s only going to become more central to our lives.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们在这里探讨的原则超越了任何单一语言或领域。它们是所有自然语言处理任务的基础。通过理解和掌握这些原则，你为在一个只会变得更加重要的领域中取得成功奠定了基础。
- en: It’s time to gear up for the next exciting phase — preparing your dataset for
    model training. Trust me, if you found tokenization enthralling, dataset preparation
    will be equally eye-opening. This will be where all the pieces we’ve assembled
    — the tokenizer, the special tokens, and your domain or language-specific nuances
    — come together to fuel the BERT model’s learning process.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候为下一个激动人心的阶段做好准备了——为模型训练准备数据集。相信我，如果你觉得分词很吸引人，那么数据集准备将同样令人瞩目。这将是我们所组装的所有部分——分词器、特殊标记以及你的领域或语言特定的细节——汇聚在一起，为BERT模型的学习过程提供动力的地方。
- en: Stay tuned!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 敬请关注！
- en: About the Author
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于作者
- en: My name is [Dimitris Poulopoulos](https://www.dimpo.me/?utm_source=medium&utm_medium=article&utm_campaign=bert-tokenization),
    and I’m a machine learning engineer working for [HPE](https://www.hpe.com/us/en/home.html).
    I have designed and implemented AI and software solutions for major clients such
    as the European Commission, IMF, the European Central Bank, IKEA, Roblox and others.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我的名字是 [Dimitris Poulopoulos](https://www.dimpo.me/?utm_source=medium&utm_medium=article&utm_campaign=bert-tokenization)，我是为
    [HPE](https://www.hpe.com/us/en/home.html) 工作的机器学习工程师。我为欧洲委员会、国际货币基金组织、欧洲中央银行、宜家、Roblox
    等主要客户设计和实施了 AI 和软件解决方案。
- en: If you are interested in reading more posts about Machine Learning, Deep Learning,
    Data Science, and DataOps, follow me on [Medium](https://towardsdatascience.com/medium.com/@dpoulopoulos/follow),
    [LinkedIn](https://www.linkedin.com/in/dpoulopoulos/), or [@james2pl](https://twitter.com/james2pl)
    on Twitter.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣阅读更多关于机器学习、深度学习、数据科学和数据操作的文章，请关注我在 [Medium](https://towardsdatascience.com/medium.com/@dpoulopoulos/follow)、[LinkedIn](https://www.linkedin.com/in/dpoulopoulos/)
    或者 Twitter 上的 [@james2pl](https://twitter.com/james2pl)。
- en: Opinions expressed are solely my own and do not express the views or opinions
    of my employer.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 所表达的观点仅代表我个人，并不代表我的雇主的观点或意见。
