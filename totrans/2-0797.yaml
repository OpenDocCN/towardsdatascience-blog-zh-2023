- en: 'Elliot Activation Function: What Is It and Is It Effective?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Elliot 激活函数：它是什么，它有效吗？
- en: 原文：[https://towardsdatascience.com/elliot-activation-function-what-is-it-and-is-it-effective-59b63ec1fd8a](https://towardsdatascience.com/elliot-activation-function-what-is-it-and-is-it-effective-59b63ec1fd8a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/elliot-activation-function-what-is-it-and-is-it-effective-59b63ec1fd8a](https://towardsdatascience.com/elliot-activation-function-what-is-it-and-is-it-effective-59b63ec1fd8a)
- en: What is the Elliot Activation Function and is it a good alternative to the other
    activation functions used in neural networks?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是 Elliot 激活函数，它是否是神经网络中其他激活函数的良好替代方案？
- en: '[](https://ben-mccloskey20.medium.com/?source=post_page-----59b63ec1fd8a--------------------------------)[![Benjamin
    McCloskey](../Images/7118f5933f2affe2a7a4d3375452fa4c.png)](https://ben-mccloskey20.medium.com/?source=post_page-----59b63ec1fd8a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----59b63ec1fd8a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----59b63ec1fd8a--------------------------------)
    [Benjamin McCloskey](https://ben-mccloskey20.medium.com/?source=post_page-----59b63ec1fd8a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ben-mccloskey20.medium.com/?source=post_page-----59b63ec1fd8a--------------------------------)[![本杰明·麦克洛斯基](../Images/7118f5933f2affe2a7a4d3375452fa4c.png)](https://ben-mccloskey20.medium.com/?source=post_page-----59b63ec1fd8a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----59b63ec1fd8a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----59b63ec1fd8a--------------------------------)
    [本杰明·麦克洛斯基](https://ben-mccloskey20.medium.com/?source=post_page-----59b63ec1fd8a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----59b63ec1fd8a--------------------------------)
    ·7 min read·Feb 4, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----59b63ec1fd8a--------------------------------)
    ·阅读时间 7 分钟·2023年2月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/39a7ac7751ff21380e8ecc69c64edfb1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39a7ac7751ff21380e8ecc69c64edfb1.png)'
- en: Elliot Activation Function (Image from Author)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Elliot 激活函数（图片来源：作者）
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Are you in the middle of creating a new machine-learning model and unsure of
    what activation function you should be using?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否在创建新的机器学习模型时，不确定应该使用什么激活函数？
- en: '**But wait, *what is an activation function?***'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**但等一下，*什么是激活函数？***'
- en: Activation functions allow machine learning models to understand and solve *nonlinear*
    problems. Using an activation function in neural networks specifically helps with
    the passing of the most important information from each neuron to the next. Today,
    the ReLU Activation Function is generally used in the architecture of Neural Networks,
    however, that does not necessarily mean it is always the best choice. (Check out
    my post below on the ReLU and LReLU Activations).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数使机器学习模型能够理解和解决*非线性*问题。在神经网络中使用激活函数特别有助于将每个神经元传递给下一个神经元的最重要信息。今天，ReLU 激活函数通常用于神经网络的架构中，但这并不一定意味着它总是最佳选择。（请查看我下面关于
    ReLU 和 LReLU 激活函数的文章）。
- en: '[](/leaky-relu-vs-relu-activation-functions-which-is-better-1a1533d0a89f?source=post_page-----59b63ec1fd8a--------------------------------)
    [## Leaky ReLU vs. ReLU Activation Functions: Which is Better?'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/leaky-relu-vs-relu-activation-functions-which-is-better-1a1533d0a89f?source=post_page-----59b63ec1fd8a--------------------------------)
    [## Leaky ReLU 与 ReLU 激活函数：哪一个更好？'
- en: An experiment to investigate if there is a noticeable difference in a model’s
    performance when using a ReLU Activation…
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一项实验调查在使用 ReLU 激活函数时模型性能是否存在明显差异……
- en: towardsdatascience.com](/leaky-relu-vs-relu-activation-functions-which-is-better-1a1533d0a89f?source=post_page-----59b63ec1fd8a--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/leaky-relu-vs-relu-activation-functions-which-is-better-1a1533d0a89f?source=post_page-----59b63ec1fd8a--------------------------------)
- en: I recently came across the **Elliot Activation Function** which was praised
    as being a possible alternative to various activation functions, including the
    Sigmoid and Hyperbolic Tagenet. Today we will run an experiment to test the Elliot
    Activation Function’s performance.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我最近发现了**Elliot 激活函数**，它被赞誉为可能替代各种激活函数的选择，包括 Sigmoid 和双曲正切函数。今天我们将进行一个实验来测试 Elliot
    激活函数的性能。
- en: '**Experiment 1:** *Test the Elliot Activation Function’s performance against
    the Sigmoid Activation Function and Hyperbolic Tangent Activation Function.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**实验 1：** *测试 Elliot 激活函数与 Sigmoid 激活函数和双曲正切激活函数的性能。*'
- en: '**Experiment 2:** *Test the Elliot Activation Function’s performance against
    the ReLU Activation Function.*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is to answer the question: I***s the Elliot Activation Function effective
    or not?***'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Elliot Activation Function
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/77e52168c1320307627e828d342e28a2.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: The Elliot Activation Function will result in an approximation that is relatively
    close to the Sigmoid and Hyperbolic Tangent Activation Functions. Some have found
    that Elliot performs calculations **2x faster** than the Sigmoid Activation Function
    [3]. Just like the Sigmoid Activation function, the Elliot Activation Function
    is constrained between 0 and 1.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d6d284c49d6dfde0ab3cd3d7f19a2f7.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: Elliot Activation Function (Image from Author)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Experiment
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**PROBLEM:** Keras currently does not have the Elliot Activation Function in
    its repository.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '**SOLUTION:** We can use the Keras backend and create it ourselves!'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For this experiment, let's see how the Elliot Activation Function compares to
    its similar counterparts as well as the ReLU Activation, a fundamental activation
    function used in neural networks today.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Dataset and Setup
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step of all Python projects is to import your packages.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The dataset used today is the iris dataset, which can be found [here](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data).
    This dataset is publicly available and is allowed for public use (there is an
    option to load it into Python through *sklearn*).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, let’s create the four models. These will be fairly simple models. Each
    will have one layer of 8 neurons and an activation function. The final layer will
    have 3 neurons and use a Softmax Activation Function.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, simply train the models and analyze the results.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Results
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The results were… surprising, actually. As expected, the Elliot Activation Function
    produced models with similar performances to those adopting the Sigmoid And Hyperbolic
    Tangent Activation Functions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '**1 Epoch**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '*Sigmoid @ 1*: Accuracy: 0.3109 | Loss: 2.0030'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 1*: Accuracy: 0.3361 | Loss: 1.0866'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At 1 Epoch, the Elliot Activation Function model outperformed the Sigmoid Activation
    Function model with a 2.61% higher accuracy and an *almost 100% decrease in the
    amount of loss.*
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '**10 Epochs**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '*Sigmoid @ 10*: Accuracy: 0.3529 | Loss: 1.0932'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 10*: Accuracy: 0.6891 | Loss: 0.9434'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At 10 epochs, the model with the Elliot Activation Function had an almost 30%
    higher accuracy with a lower loss compared to the model using the Sigmoid Activation
    Function.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '**100 Epochs**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '*Sigmoid @ 100*: Accuracy: 0.9496 | Loss: 0.4596'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 100*: Accuracy: 0.9580 | Loss: 0.5485'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Sigmoid Model outperformed the Elliot Model, however, it should be noted
    that their performances were almost exactly the same.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '**1000 Epochs**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '*Sigmoid @ 1000*: Accuracy: 0.9832 | Loss: 0.0584'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 1000*: Accuracy: 0.9832 | loss: 0.0433'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At 1000 Epochs, the performances of the two different models were almost exactly
    the same.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '**Overall, the model using the Elliot Activation Function performed slightly
    better than the model using the Sigmoid Activation Function.**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Elliot versus Hyperbolic Tangent
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**1 Epoch**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '*tanh @ 1* : Accuracy: 0.3361 | loss: 1.1578'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 1*: Accuracy: 0.3361 | Loss: 1.0866'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At 1 Epoch, the Elliot Activation Function Model performs the same as the Hyperbolic
    Tangent Activation Function Model. I expected these functions to produce similar
    performing models since each similarly constrains the values passed on to the
    next layer of a neural network.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '**10 Epochs**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '*tanh @ 10*: Accuracy: 0.3277 | Loss: 0.9981'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 10*: Accuracy: 0.6891 | Loss: 0.9434'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model with the Elliot Activation Function greatly outperforms the model
    with the Hyperbolic Tangent Activation Function, just as the Elliot Model did
    at 10 Epochs when compared to the Sigmoid Model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '**100 Epochs**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '*tanh @ 100:* Accuracy: 0.9916 | Loss: 0.2325'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 100*: Accuracy: 0.9580 | Loss: 0.5485'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At 100 epochs, the Hyperbolic Tangent model performs much better than the Elliot
    model. At higher epochs, the Elliot Activation Function seems to underperform
    compared to the *tanh* activation function but let’s see how their performances
    differ at 1000 epochs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '**1000 Epochs**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '*tanh @ 1000:* Accuracy: 0.9748 | Loss: 0.0495'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 1000*: Accuracy: 0.9832 | Loss: 0.0433'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Well, at 1000 epochs, the Elliot Activation Function model slightly outperforms
    the Hyperbolic Tangent Activation Function model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Overall, I would say that the Hyperbolic Tangent and Elliot Activation Function
    Models work almost the same in the layers of a neural network. **There could be
    a difference in the time to train a model**, however, these models were very simple
    and time may become a bigger factor with the more data one has as well as the
    size of the network they are creating.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Elliot versus ReLU
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**1 Epoch**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '*ReLU @ 1:* Accuracy: 0.6639 | Loss: 1.0221'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 1*: Accuracy: 0.3361 | Loss: 1.0866'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At 1 epoch, the model with the ReLU Activation Function does *much* better which
    outlines one observation that the Elliot Activation Function is causing the model
    to train slower.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '**10 Epochs**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '*ReLU @ 10*: Accuracy: 0.6471 | Loss: 0.9413'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 10*: Accuracy: 0.6891 | Loss: 0.9434'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wow! The model containing the Elliot activation actually performed *better*
    than the model with the ReLU Activation Function with a 4.2% higher accuracy and
    0.21% lower loss.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '**100 Epochs**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '*ReLU @ 100* : Accuracy: 0.9160 | Loss: 0.4749'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 100*: Accuracy: 0.9580 | Loss: 0.5485'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though the model which adopted the Elliot Activation Function had a higher
    loss, it was able to achieve higher accuracy by 4.2%. *Again*, this shows the
    strength of the Elliot Activation Function when placed within neural networks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '**1000 Epochs**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '*ReLU @ 1000*: Accuracy: 0.9916 | Loss: 0.0494'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 1000*: Accuracy: 0.9832 | Loss: 0.0433'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the model with the Elliot Activation Function did not do better in terms
    of accuracy, the loss was lower and I was still happy with the results. As shown
    at 1000 epochs, the Elliot Activation Function was almost just as good as the
    ReLU Activation Function and with the right problem and hyperparameter tuning,
    the Elliot Activation Function could the more optimal choice.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Today, we looked at a lesser-known activation function: **The Elliot Activation**.
    To test its performance, it was compared against two activation functions that
    are similar in their shape: the Sigmoid and Hyperbolic Tangent Activation Functions.
    That trial resulted in the Elliot Function performing the same if not better than
    using either of those two functions within the body of a neural network. Next,
    we compared the performance of an Elliot Activation Function to the standard ReLU
    Activation Function used in Neural Networks today. Of the 4 trials, the model
    adopting the Elliot Activation Function performed better 50% of the time. For
    the other trials it underperformed, its performance was still almost exactly the
    same as the model which was deployed with the ReLU Activation Function. I recommend
    trying the Elliot Activation Function in your next Neural Network because there
    is a chance it may perform better!'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '**If you enjoyed today’s reading, PLEASE give me a follow and let me know if
    there is another topic you would like me to explore! If you do not have a Medium
    account, sign up through my link** [**here**](https://ben-mccloskey20.medium.com/membership)**!
    I will receive a small commission when you use my link. Additionally, add me on**
    [**LinkedIn**](https://www.linkedin.com/in/benjamin-mccloskey-169975a8/), **or
    feel free to reach out! Thanks for reading!**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Dubey, Shiv Ram, Satish Kumar Singh, and Bidyut Baran Chaudhuri. “A comprehensive
    survey and performance analysis of activation functions in deep learning.” *arXiv
    preprint arXiv:2109.14545* (2021).
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sharma, Sagar, Simone Sharma, and Anidhya Athaiya. “Activation functions in
    neural networks.” *towards data science* 6.12 (2017): 310–316.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.gallamine.com/2013/01/a-sigmoid-function-without-exponential_31.html](https://www.gallamine.com/2013/01/a-sigmoid-function-without-exponential_31.html)'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
