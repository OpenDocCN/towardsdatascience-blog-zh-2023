- en: V-Net, U-Net’s big brother in Image Segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/v-net-u-nets-big-brother-in-image-segmentation-906e393968f7](https://towardsdatascience.com/v-net-u-nets-big-brother-in-image-segmentation-906e393968f7)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Welcome to this guide about the V-Net, the cousin of the well known U-Net, for
    3D images segmentations. You will know it inside out!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@francoisporcher?source=post_page-----906e393968f7--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page-----906e393968f7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----906e393968f7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----906e393968f7--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page-----906e393968f7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----906e393968f7--------------------------------)
    ·8 min read·Jul 28, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to an exciting journey through the world of deep learning architectures!
    You may already be familiar with [U-Net](https://medium.com/@foporcher/cooking-your-first-u-net-for-image-segmentation-e812e37e9cd0),
    a game-changer in computer vision that has significantly reshaped the landscape
    of image segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Today, let’s turn the spotlight onto U-Net’s big brother, the V-Net.
  prefs: []
  type: TYPE_NORMAL
- en: 'Published by researchers Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi,
    the paper [“VNet: Fully Convolutional Neural Networks for Volumetric Medical Image
    Segmentation”](https://arxiv.org/abs/1606.04797) introduced a breakthrough methodology
    for 3D image analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: This article will take you on a tour of this groundbreaking paper, shedding
    light on its unique contributions and architectural advancements. Whether you’re
    a seasoned data scientist, a budding AI enthusiast, or just someone interested
    in the latest tech trends, there’s something here for you!
  prefs: []
  type: TYPE_NORMAL
- en: A short reminder about U-Net
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before diving into the heart of V-Net, let’s take a moment to appreciate its
    architectural inspiration — U-Net. Don’t worry if this is your first introduction
    to U-Net; I’ve got you covered with a [quick and easy tutorial on the U-Net architecture](https://medium.com/@foporcher/cooking-your-first-u-net-for-image-segmentation-e812e37e9cd0).
    It’s so concise that you’ll grasp the concept in no more than five minutes!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a brief overview of U-Net for a refresher:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b3d8c1b14e2b6d0b67b6eb9abc066de.png)'
  prefs: []
  type: TYPE_IMG
- en: U Net Architecture, from [U Net article](https://arxiv.org/abs/1505.04597)
  prefs: []
  type: TYPE_NORMAL
- en: 'U-Net is famed for its symmetrical structure, taking the form of a ‘U’. This
    architecture is composed of two distinct pathways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Contracting Pathway (Left):** Here, we progressively decrease the resolution
    of the image while increasing the number of filters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Expanding Pathway (Right):** This pathway acts as the mirror image of the
    contracting pathway. We gradually decrease the number of filters while increase
    the resolution until it aligns with the original image size.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The beauty of U-Net lies in its innovative use of **‘residual connections’**
    or **‘skip connections**’. These connect corresponding layers in the contracting
    and expanding paths, allowing the network to retain high-resolution details that
    are usually lost in the contracting process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eafd0fc04e3573f57b206526b2431a5c.png)'
  prefs: []
  type: TYPE_IMG
- en: Residual Connection, from [U Net paper](https://arxiv.org/abs/1505.04597)
  prefs: []
  type: TYPE_NORMAL
- en: 'Why does this matter? Because it eases the gradient flow during backpropagation,
    particularly in the early layers. In essence, we circumvent the risk of vanishing
    gradients — a common problem where gradients approach zero, hindering the learning
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ee0d62e5c4e2e5c95970aa5c22290d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from Author
  prefs: []
  type: TYPE_NORMAL
- en: Now, bearing this understanding of U-Net in mind, let’s transition into the
    world of V-Net. At its core, V-Net shares a similar encoder-decoder philosophy.
    But as you’ll soon discover, it comes with its own set of unique traits that set
    it apart from its sibling, U-Net.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c3382c5442ce9a2a60b468174e704bd.png)'
  prefs: []
  type: TYPE_IMG
- en: V-Net Architecture, from [VNet paper](https://arxiv.org/abs/1606.04797)
  prefs: []
  type: TYPE_NORMAL
- en: What sets V-Net apart from the U-Net?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: 'Difference 1: 3D Convolutions instead of 2D convolutions'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first difference is as clear as day. While U-Net was tailored for 2D image
    segmentation, medical images often require a 3D perspective (think of volumetric
    brain scans, CT scans, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: This is where V-Net comes into play. The ‘V’ in V-Net stands for ‘Volumetric,’
    and this dimensionality shift requires the replacement of 2D convolutions with
    3D convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Difference 2: Activation Functions, PreLU instead of ReLU'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The realm of deep learning has fallen in love with the ReLU function, owing
    to its simplicity and computational efficiency. Compared to other functions like
    sigmoid or tanh, ReLU is “non-saturating,” meaning it reduces the issue of vanishing
    gradients.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e731b05538d4cdfa61eac7c46fac7a0.png)'
  prefs: []
  type: TYPE_IMG
- en: (Left) ReLU, (Middle) LeakyReLU and (Last) PReLU, , from [PReLU paper](https://arxiv.org/abs/1502.01852v1)
  prefs: []
  type: TYPE_NORMAL
- en: But ReLU isn’t perfect. It’s notorious for a phenomenon known as the ‘Dying
    ReLU problem,’ where many neurons always output zero, becoming ‘dead neurons.’
    To counter this, LeakyReLU was introduced, which has a small but nonzero slope
    on the left side of zero.
  prefs: []
  type: TYPE_NORMAL
- en: Pushing the reasoning even further, V-Net leverages the Parametric ReLU (PReLU).
    Instead of hardcoding the slope of LeakyReLU, why not let the network learn it?
  prefs: []
  type: TYPE_NORMAL
- en: After all this is a core philosophy of Deep Learning, we want to put as little
    inductive bias as possible and let the model learn everything by itself, assuming
    we have enough data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Difference 3: Different loss function based on the Dice Score'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we arrive at perhaps the most impactful contribution of V-Net — a shift
    in the loss function. Unlike U-Net’s cross entropy loss function, V-Net uses the
    Dice loss function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/511b1b0a9a5e5ac72c65ac3a0294a130.png)'
  prefs: []
  type: TYPE_IMG
- en: Cross Entropy Function, Image from Author
  prefs: []
  type: TYPE_NORMAL
- en: But the main issue with this function is that it does not handle well unbalanced
    classes. And this issue is very frequent in medical images because most of the
    time the background is much more present than zone of interest.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example consider this picture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5fc6e6c4ee2532b20604eb50a6bd93ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Background is omnipresent, Image from Author
  prefs: []
  type: TYPE_NORMAL
- en: As a result some models can get “lazy” and predict background everywhere because
    they will still get a small loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'So the V-Net uses a loss function that is much more effective for this matter:
    the Dice coefficient.'
  prefs: []
  type: TYPE_NORMAL
- en: The reason why it is better is that it measures the overlap between the predicted
    zone and the ground truth as a **proportion**, so the size of the class is taken
    into account.
  prefs: []
  type: TYPE_NORMAL
- en: Even though the background is almost everywhere, the Dice score measures the
    overlap between the prediction and the ground truth, so we still get a number
    between 0 and 1 even though the class is preponderant.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/18b6cc1f26d27f943c4e57d07a0cb8fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Dice Coefficient, from [VNet paper](https://arxiv.org/abs/1606.04797)
  prefs: []
  type: TYPE_NORMAL
- en: I am saying that this is maybe the main contribution of the article because
    going from 2D to 3D Convolutions is a very natural idea to handle 3D images. However
    this loss function has been very widely adopted in the Image Segmentation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, a hybrid approach often proves effective, combining the Cross Entropy
    Loss and Dice Loss to leverage the strengths of both.
  prefs: []
  type: TYPE_NORMAL
- en: The Performance of the V-Net
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, we’ve journeyed through the unique aspects of V-Net, but you’re probably
    thinking, “All this theory is great, but does V-Net really deliver in practice?”
    Well, let’s put V-Net to the test!
  prefs: []
  type: TYPE_NORMAL
- en: The authors evaluated the V-Net performance on the PROMISE12 dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The **PROMISE12** dataset was made available for the MICCAI 2012 prostate segmentation
    challenge.
  prefs: []
  type: TYPE_NORMAL
- en: The V-Net was trained on 50 Magnetic Resonance (MR) images, this is not a lot!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a294d06408f469afb9697f4d6c800e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Segmentation of VNet on the PROMISE 2012 challenge dataset, [from VNet paper](https://arxiv.org/abs/1606.04797)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa5f0d97b7dab293d4751117aa31ed88.png)'
  prefs: []
  type: TYPE_IMG
- en: Quantitative Metrics on the PROMISE 2012 Challenge dataset, from [VNet paper](https://arxiv.org/abs/1606.04797)
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, even with few labels, the V-Net is able to produce good qualitative
    segmentations and obtain a very good Dice Score.
  prefs: []
  type: TYPE_NORMAL
- en: Main Limitations of the V-Net
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Indeed, V-Net has set a new benchmark in the realm of image segmentation, particularly
    in medical imaging. However, every innovation has room for growth. Here, we’ll
    discuss some of the prominent areas where V-Net could improve:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Limitation 1: Size of the model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Transitioning from 2D to 3D brings with it a significant increase in memory
    consumption. The ripple effects of this increase are multifold:'
  prefs: []
  type: TYPE_NORMAL
- en: The model demands substantial memory space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It severely restricts the batch size (as loading multiple 3D tensors into GPU
    memory becomes challenging).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Medical imaging data is sparse and expensive to label, making it harder to fit
    a model with so many parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Limitation 2: Does not use unsupervised learning or Self supervised learning'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: V-Net operates purely in a supervised learning context, neglecting the potential
    of unsupervised learning. In a field where unlabelled scans significantly outnumber
    the annotated ones, incorporating unsupervised learning could be a game-changer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Limitation 3: No uncertainty estimation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: V-Net doesn’t estimate uncertainties, meaning it cannot assess its own confidence
    in its predictions. This is an area where Bayesian Deep Learning shines. (Refer
    to this post for a [Gentle Introduction to Bayesian Deep Learning](https://medium.com/ai-mind-labs/a-gentle-introduction-to-bayesian-deep-learning-d298c7243fd6)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Limitation 4: Lack of Robustness'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Convolutional Neural Networks (CNNs) traditionally struggle with generalization.
    They are not robust against variations like contrast change, multimodal distributions,
    or different resolutions. This is another area where V-Net could improve.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: V-Net, the lesser-known yet powerful counterpart to U-Net, has revolutionized
    computer vision, especially image segmentation. Its transition from 2D to 3D images
    and the introduction of the Dice Coefficient, now a ubiquitous tool, set new standards
    in the field.
  prefs: []
  type: TYPE_NORMAL
- en: Despite its limitations, V-Net should be the go-to model for anyone embarking
    on a 3D image segmentation task. For further improvement, exploring unsupervised
    learning and integrating attention mechanisms seems like promising avenues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks for reading! Before you go:'
  prefs: []
  type: TYPE_NORMAL
- en: Check my [compilation of AI tutorials](https://github.com/FrancoisPorcher/awesome-ai-tutorials)
    on Github
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----906e393968f7--------------------------------)
    [## GitHub - FrancoisPorcher/awesome-ai-tutorials: The best collection of AI tutorials
    to make you a…'
  prefs: []
  type: TYPE_NORMAL
- en: The best collection of AI tutorials to make you a boss of Data Science! - GitHub
    …
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----906e393968f7--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Y*ou should get my articles in your inbox.* [***Subscribe here.***](https://medium.com/@francoisporcher/subscribe)
  prefs: []
  type: TYPE_NORMAL
- en: '*If you want to have access to premium articles on Medium, you only need a
    membership for $5 a month. If you sign up* [***with my link***](https://medium.com/@francoisporcher/membership)*,
    you support me with a part of your fee without additional costs.*'
  prefs: []
  type: TYPE_NORMAL
- en: If you found this article insightful and beneficial, please consider following
    me and leaving a clap for more in-depth content! Your support helps me continue
    producing content that aids our collective understanding.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Cooking your first U-Net for Image Segmentation](https://medium.com/@foporcher/cooking-your-first-u-net-for-image-segmentation-e812e37e9cd0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Gentle Introduction to Bayesian Deep Learning](https://medium.com/ai-mind-labs/a-gentle-introduction-to-bayesian-deep-learning-d298c7243fd6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Milletari, F., Navab, N., & Ahmadi, S. A. (2016). [V-Net: Fully Convolutional
    Neural Networks for Volumetric Medical Image Segmentation](https://arxiv.org/abs/1606.04797).
    In 3D Vision (3DV), 2016 Fourth International Conference on (pp. 565–571). IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger, O., Fischer, P., & Brox, T. (2015). [U-Net: Convolutional Networks
    for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597). In International
    Conference on Medical image computing and computer-assisted intervention (pp.
    234–241). Springer, Cham.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
