- en: Outlier Detection Using Principal Component Analysis and Hotelling’s T2 and
    SPE/DmodX Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/outlier-detection-using-principal-component-analysis-and-hotellings-t2-and-spe-dmodx-methods-625b3c90897](https://towardsdatascience.com/outlier-detection-using-principal-component-analysis-and-hotellings-t2-and-spe-dmodx-methods-625b3c90897)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Thanks to PCA’s sensitivity, it can be used to detect outliers in multivariate
    datasets.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://erdogant.medium.com/?source=post_page-----625b3c90897--------------------------------)[![Erdogan
    Taskesen](../Images/8e62cdae0502687710d8ae4bbcd8966e.png)](https://erdogant.medium.com/?source=post_page-----625b3c90897--------------------------------)[](https://towardsdatascience.com/?source=post_page-----625b3c90897--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----625b3c90897--------------------------------)
    [Erdogan Taskesen](https://erdogant.medium.com/?source=post_page-----625b3c90897--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----625b3c90897--------------------------------)
    ·11 min read·Mar 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c6d1216a283cf22c5bca9061dca4820b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Andrew Ridley](https://unsplash.com/@aridley88?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/jR4Zf-riEjI?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis (PCA) is a widely used technique for dimensionality
    reduction while preserving relevant information. Due to its sensitivity, it can
    also be used to detect outliers in multivariate datasets. Outlier detection can
    provide early warning signals for abnormal conditions, allowing experts to identify
    and address issues before they escalate. However, detecting outliers in multivariate
    datasets can be challenging due to the high dimensionality, and the lack of labels.
    PCA offers several advantages for outlier detection. ***I will describe the concepts
    of outlier detection using PCA. With a hands-on example, I will demonstrate how
    to create an unsupervised model for the detection of outliers for continuous and
    separately categorical data sets.***
  prefs: []
  type: TYPE_NORMAL
- en: Outlier Detection.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Outliers can be modeled in either a ***univariate*** or ***multivariate***
    approach (Figure 1). In the univariate approach, outliers are detected using one
    variable at a time for which data distribution analysis is a great manner. Read
    more details about univariate outlier detection in the following blog post [1]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/outlier-detection-using-distribution-fitting-in-univariate-data-sets-ac8b7a14d40e?source=post_page-----625b3c90897--------------------------------)
    [## Outlier Detection Using Distribution Fitting in Univariate Datasets'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to detect outliers using Probability Density Functions for fast and
    lightweight models and explainable results.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/outlier-detection-using-distribution-fitting-in-univariate-data-sets-ac8b7a14d40e?source=post_page-----625b3c90897--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The multivariate approach uses multiple features and can therefore detect outliers
    with (non-)linear relationships or skewed distributions. The scikit-learn library
    has multiple solutions for multivariate outlier detection, such as the one-class
    classifier, isolation forest, and local outlier factor [2]. ***In this blog, I
    will focus on multivariate outlier detection using Principal Component Analysis
    [3] which has its own advantages such as explainability; the outliers can be visualized
    as we rely on the dimensionality reduction of PCA itself.***
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc58358d39954cd5542e2f58f431be9f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Overview of univariate versus multivariate analysis for the detection
    of outliers. *Outlier detection for multivariate data sets will be described in
    this blog (*image by the author).
  prefs: []
  type: TYPE_NORMAL
- en: Anomalies vs. Novelties
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '***Anomalies and novelties*** are deviant observations from standard/expected
    behavior. Also referred to as outliers. There are some differences though: ***anomalies
    are deviations that have been seen before***, typically used for detecting fraud,
    intrusion, or malfunction. ***Novelties are deviations that have not been seen
    before*** or used to identify new patterns or events. In such cases, it is important
    to use domain knowledge. Both anomalies and novelties can be challenging to detect
    as the definition of what is normal or expected can be subjective and vary based
    on the application.'
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis for Outlier Detection.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Principal Component Analysis (PCA) is a linear transformation that reduces
    the dimensionality and searches for the direction in the data with the largest
    variance. Due to the nature of the method, it is sensitive to variables with different
    value ranges and, thus also outliers. An advantage is that it allows visualization
    of the data in a two or three-dimensional scatter plot, making it easier to visually
    confirm the detected outliers. Furthermore, it provides good interpretability
    of the response variables. Another great advantage of PCA is that it can be combined
    with other methods, such as different distance metrics, to improve the accuracy
    of the outlier detection. Here I will use the PCA library which contains two methods
    for the detection of outliers: Hotelling’s T2 and SPE/DmodX. For more details,
    read the blog post about Principal Component Analysis and `pca` library [3].'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/what-are-pca-loadings-and-biplots-9a7897f2e559?source=post_page-----625b3c90897--------------------------------)
    [## What are PCA loadings and how to effectively use Biplots?'
  prefs: []
  type: TYPE_NORMAL
- en: A practical guide for getting the most out of Principal Component Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/what-are-pca-loadings-and-biplots-9a7897f2e559?source=post_page-----625b3c90897--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*If you find this article about outlier detection helpful,* [*follow me*](http://erdogant.medium.com/)
    *to stay up-to-date with my latest content! Support this content using my* [*referral
    link*](https://medium.com/@erdogant/membership) *which will give you unlimited
    learning and reading with the Medium membership.*'
  prefs: []
  type: TYPE_NORMAL
- en: Outlier Detection for Continuous Random Variables.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s start with an example to demonstrate the working of outlier detection
    using [Hotelling’s T2](https://erdogant.github.io/pca/pages/html/Outlier%20detection.html#)
    and [SPE/DmodX](https://erdogant.github.io/pca/pages/html/Outlier%20detection.html#spe-dmodx)
    for continuous random variables. I will use the *wine dataset* from sklearn that
    contains 178 samples, with 13 features and 3 wine classes [4].
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We can see in the data frame that the value range per feature differs heavily
    and a normalization step is therefore important. The normalization step is a build-in
    functionality in the [*pca library*](https://erdogant.github.io/pca/)that can
    be set by `normalize=True.` During the initialization, we can specify the outlier
    detection methods separately, `ht2` for Hotelling’s T2 and `spe` for the SPE/DmodX
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: After running the fit function, the *pca* library will score sample-wise whether
    a sample is an outlier. For each sample, multiple statistics are collected as
    shown in the code section below. The first four columns in the data frame (`y_proba`,
    `p_raw`, `y_score`, and `y_bool`), are outliers detected using Hotelling’s T2
    method. The latter two columns (`y_bool_spe`, and `y_score_spe`) are based on
    the SPE/DmodX method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: H**otelling’s T2** computes the chi-square tests and P-values across the top
    `n_components` which allows the ranking of outliers from strong to weak using
    `y_proba`. Note that the search space for outliers is across the dimensions PC1
    to PC5 as it is expected that the highest variance (and thus the outliers) will
    be seen in the first few components. Note, the depth is optional in case the variance
    is poorly captured in the first five components. Let’s plot the outliers and mark
    them for the wine datasets (Figure 2).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5face2d00a99585174c9763e0781b358.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2\. Left panel: PC1 vs PC2 and the projected samples with 9 detected
    outliers using Hotelling’s T2 method. Right panel: Three-dimensional plot with
    the outliers. (image by the author)'
  prefs: []
  type: TYPE_NORMAL
- en: T**he SPE/DmodX** method is a measure of the distance between the actual observation
    and its projection, using the Principal Components. The distance to the center
    is expressed by Hotelling’s T2 values and, therefore, the ellipse in Figure 3
    represents the boundary beyond which a sample is outlying with respect to Hotelling’s
    T2\. A sample is flagged as an outlier based on the mean and covariance of the
    first two PCs (Figure 3). In other words, when it is outside the ellipse.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5e7d7da4b6d17b425f16dec0e34c7849.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3A. Outliers detected using the SPE/DmodX method are depicted with diamonds.
    Outliers detected using the Hotelling T2 method are depicted with crosses. (image
    by the author)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/facf901d760aad88afe6e75c4c8c98a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3B. Outliers are detected using the SPE/DmodX method and visualized in
    a 3D plot.
  prefs: []
  type: TYPE_NORMAL
- en: Using the results of both methods, we can now also compute the overlap. In this
    use case, there are 5 outliers that overlap *(see code section below)*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Outlier Detection for Categorical Variables.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the detection of outliers in categorical variables, we first need to discretize
    the categorical variables and make the distances comparable to each other. With
    the discretized data set (one-hot), we can proceed using the PCA approach and
    apply Hotelling’s T2 and SPE/DmodX methods. I will use the Student Performance
    data set [5] for demonstration purposes, which contains 649 samples and 33 variables.
    We will import the data set as shown in the *code section below*. More details
    about the column description can be found [here](https://archive.ics.uci.edu/ml/datasets/student+performance).
    I will not remove any columns but if there was an identifier column or variables
    with floating type, I would have removed it or categorized it into discrete bins.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The variables need to be one-hot encoded to make sure the distances between
    the variables become comparable to each other. This results in 177 columns for
    649 samples (see code section below).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We can now use the processed one-hot data frame as input for pca and detect
    outliers. During initialization, we can set `normalize=True` to normalize the
    data and we need to specify the outlier detection methods.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The Hotelling T2 test detected 85 outliers and the SPE/DmodX method detected
    6 outliers (Figure 4, see legend). The number of outliers that overlap between
    both methods is 5\. We can make a plot with the `biplot` functionality and color
    the samples in any category for further investigation (such as the `sex` label).
    The outliers are marked with `x` or `*` . This is now a good start for a deeper
    inspection; in our case, we can see in Figure 4 that the 5 outliers are drifting
    away from all other samples. We can rank the outliers, look at the loadings, and
    deeper investigate these students (*see previous code section*). To rank the outliers,
    we can use the `y_proba`(lower is better) for the Hotelling T2 method, and `y_score_spe`,
    for the SPE/DmodX method. The latter is the Euclidian distance of the sample to
    the center (thus larger is better).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/93736b498cc6ba780d1e90dad2f20c35.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Outliers detected using the SPE/DmodX method are depicted with diamonds.
    Outliers detected using the Hotelling T2 method are depicted with crosses. (image
    by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Final words.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I demonstrated how to use PCA for multivariate outlier detection for both continuous
    and categorical variables. With the *pca* library, we can use Hotelling’s T2 and/or
    the SPE/DmodX method to determine candidate outliers. The interpretation of the
    contribution of each variable to the principal components can be retrieved using
    the loadings and visualized with the biplot in the low-dimensional PC space. Such
    visual insights can help to provide intuition about the detection outliers and
    whether they require follow-up analysis. In general, the detection of outliers
    can be challenging because determining what is considered normal can be subjective
    and vary depending on the specific application.
  prefs: []
  type: TYPE_NORMAL
- en: '*Be Safe. Stay Frosty.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***Cheers E.***'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you find this article about outlier detection helpful,* [*follow me*](http://erdogant.medium.com/)
    *to stay up-to-date with my latest content! Support this content using my* [*referral
    link*](https://medium.com/@erdogant/membership) *which will give you unlimited
    learning and reading with the Medium membership.*'
  prefs: []
  type: TYPE_NORMAL
- en: Software
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PCA Github/Documentation](https://erdogant.github.io/pca/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PCA notebook examples](https://erdogant.github.io/pca/pages/html/Documentation.html#colab-notebook)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s connect!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Let’s connect on LinkedIn](https://www.linkedin.com/in/erdogant/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Follow me on Github](https://github.com/erdogant)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Follow me on Medium](https://erdogant.medium.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: E. Taskesen, [*What are PCA loadings and Biplots?*](/what-are-pca-loadings-and-biplots-9a7897f2e559),
    Medium, Towards Data Science, April 2022
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scikit-Learn, [*Outlier Detection*](https://scikit-learn.org/stable/modules/outlier_detection.html).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: E. Taskesen, [*How to Find the Best Theoretical Distribution for Your Data*](/how-to-find-the-best-theoretical-distribution-for-your-data-a26e5673b4bd),
    Febr. 2023 Medium.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wine Data set, [*https://archive-beta.ics.uci.edu/dataset/109/wine*](https://archive-beta.ics.uci.edu/dataset/109/wine)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: P. Cortez and A. Silva.[*Using Data Mining to Predict Secondary School Student
    Performance*](https://archive-beta.ics.uci.edu/dataset/320/student+performance)*,
    ISBN 978–9077381–39–7*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
