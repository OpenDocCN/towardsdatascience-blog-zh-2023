- en: 'Prompt Engineering 101: Zero, One, and Few-Shot Prompting'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示工程 101：零样本、单样本和少样本提示
- en: 原文：[https://towardsdatascience.com/prompt-engineering-101-zero-one-and-few-shot-prompting-1e8ced03d434](https://towardsdatascience.com/prompt-engineering-101-zero-one-and-few-shot-prompting-1e8ced03d434)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/prompt-engineering-101-zero-one-and-few-shot-prompting-1e8ced03d434](https://towardsdatascience.com/prompt-engineering-101-zero-one-and-few-shot-prompting-1e8ced03d434)
- en: An introduction to a basic prompt engineering strategy
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本提示工程策略的介绍
- en: '[](https://medium.com/@aashishnair?source=post_page-----1e8ced03d434--------------------------------)[![Aashish
    Nair](../Images/23f4b3839e464419332b690a4098d824.png)](https://medium.com/@aashishnair?source=post_page-----1e8ced03d434--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1e8ced03d434--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1e8ced03d434--------------------------------)
    [Aashish Nair](https://medium.com/@aashishnair?source=post_page-----1e8ced03d434--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@aashishnair?source=post_page-----1e8ced03d434--------------------------------)[![Aashish
    Nair](../Images/23f4b3839e464419332b690a4098d824.png)](https://medium.com/@aashishnair?source=post_page-----1e8ced03d434--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1e8ced03d434--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1e8ced03d434--------------------------------)
    [Aashish Nair](https://medium.com/@aashishnair?source=post_page-----1e8ced03d434--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e8ced03d434--------------------------------)
    ·5 min read·Sep 19, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e8ced03d434--------------------------------)
    ·5分钟阅读·2023年9月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/dc425a1abba413deb8fc810c761d1831.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc425a1abba413deb8fc810c761d1831.png)'
- en: Image by [Alexandra_Koch](https://pixabay.com/users/alexandra_koch-621802/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=7720802)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=7720802)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Alexandra_Koch](https://pixabay.com/users/alexandra_koch-621802/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=7720802)来自[Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=7720802)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: Despite their seemingly supernatural capabilities, LLMs are ultimately predictive
    models that simply predict the next word in the sequence of words based on the
    provided context.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型看似拥有超自然的能力，但它们最终还是预测模型，基于提供的上下文来预测下一个词。
- en: As such, their performances don’t just hinge on the vast volumes of data they
    are trained with; they also depend heavily on the context provided through the
    users’ inputs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它们的表现不仅依赖于训练数据的庞大体量，还高度依赖于用户输入中提供的上下文。
- en: Frequent users of LLM-powered chatbots are aware of the importance of context.
    Without sufficient context, chatbots, whether they are publicly available services
    (e.g., ChatGPT) or custom-made LLM products, will struggle to carry out the more
    complex instructions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 频繁使用 LLM 驱动聊天机器人的用户意识到上下文的重要性。如果没有足够的上下文，聊天机器人，无论是公共服务（例如 ChatGPT）还是定制的 LLM
    产品，都将难以执行更复杂的指令。
- en: 'Here, we delve into one of the most basic strategies used to guide LLMs to
    answer prompts correctly: providing context within user prompts. This is often
    carried out with 3 different methods: zero-shot prompting, one-shot prompting,
    and few-shot prompting.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们探讨了一种基本的策略，用于指导大型语言模型（LLMs）正确回答提示：在用户提示中提供上下文。这通常通过三种不同的方法进行：零样本提示、单样本提示和少样本提示。
- en: Zero-Shot Prompting
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零样本提示
- en: If you’ve interacted with an LLM-powered chatbot before, you’ve likely already
    used zero-shot prompting unwittingly. Zero-shot prompting entails relying solely
    on an LLM’s pre-trained information to answer a given user prompt.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你以前与一个由 LLM 驱动的聊天机器人互动过，你可能已经无意中使用了零样本提示。零样本提示意味着完全依赖 LLM 的预训练信息来回答给定的用户提示。
- en: 'For instance, let’s suppose that we used ChatGPT as a sentiment classifier.
    An input with zero-shot prompting would look like this:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，假设我们使用 ChatGPT 作为情感分类器。一个零样本提示的输入可能如下所示：
- en: '**User Prompt:** “Determine the sentiment of this sentence.'
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**用户提示：** “确定这个句子的情感。”'
- en: 'Sentence: ‘This basketball has a lot of weight’.”'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 句子：“这个篮球很重。”
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**ChatGPT:** “The sentiment of the sentence ‘This basketball has a lot of weight’
    is neutral. It does not convey a positive or negative sentiment but rather provides
    factual information about the weight of the basketball.”'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ChatGPT：** “句子‘这个篮球很重’的情感是中立的。它没有传达出正面或负面的情感，而是提供了关于篮球重量的实际信息。”'
- en: When provided the query, the LLM, with no additional context, evaluates this
    sentence to be neutral.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当查询被提供时，LLM在没有额外背景的情况下将此句子评估为中立。
- en: While this method can usually yield satisfactory responses, it may be insufficient
    when the complexity of the question is high or when the criteria for the answer
    are different from the norm.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种方法通常能产生令人满意的回应，但当问题的复杂性较高或答案的标准与常规不同的时候，它可能会不够充分。
- en: For such cases, users can consider providing additional context in their prompts.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种情况，用户可以考虑在提示中提供额外的背景信息。
- en: One-Shot Prompting
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一次性提示
- en: One-shot prompting entails providing an example in the user prompt to guide
    the LLM towards to desired generated response.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一次性提示涉及在用户提示中提供一个示例，以引导LLM生成所需的响应。
- en: In the previous example, ChatGPT deemed that the sentence, “This basketball
    has a lot of weight” is neutral.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，ChatGPT认为句子“这个篮球很重”是中立的。
- en: However, certain users (e.g., retailers selling basketballs) may deem heavy
    basketballs to be faulty products. Thus, they would need to ensure that their
    LLM gives the term “heavy” a negative connotation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，某些用户（例如销售篮球的零售商）可能会认为重的篮球是有缺陷的产品。因此，他们需要确保他们的LLM将“重”赋予负面的含义。
- en: A simple way to do so would be to provide an example in the query itself.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的方法是直接在查询中提供一个示例。
- en: '**Prompt:**'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**提示：**'
- en: ''
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '“Sentence: “This basketball is easy to carry”.'
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “句子：‘这个篮球容易携带’。”
- en: ''
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Answer: “The sentiment of this sentence is positive”.'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 答案：“这个句子的情感是积极的。”
- en: ''
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Determine the sentiment of the sentence.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 确定句子的情感。
- en: ''
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Sentence: “This basketball has a lot of weight”'
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 句子：“这个篮球很重”
- en: ''
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Answer:”
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 答案：
- en: ''
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**ChatGPT: ‘**The sentiment of the sentence “This basketball has a lot of weight”
    is negative. It expresses that the basketball is heavy, which is typically seen
    as a negative attribute when it comes to ease of use or play.’'
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ChatGPT：** “句子‘这个篮球很重’的情感是负面的。它表达了篮球很重，这通常被视为负面特征，尤其是在易用性或游戏性方面。”'
- en: This time, ChatGPT deems the same sentence to be negative, since it uses the
    provided example as a reference.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，ChatGPT认为同一句子是负面的，因为它使用了提供的示例作为参考。
- en: Few-Shot Prompting
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 少量示例提示
- en: Users will not always be able to provide all of the context that the LLM will
    need to function properly with just one example.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 用户并不总是能提供LLM正常工作所需的所有背景信息，仅凭一个示例是无法做到的。
- en: For such cases, they can provide more context with few-shot prompting, which
    entails providing multiple examples in the user prompt.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种情况，他们可以通过少量示例提示提供更多背景信息，这包括在用户提示中提供多个示例。
- en: In the previous examples, we explored the use of ChatGPT to classify sentence
    sentiments as “positive” and “negative”.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，我们探讨了使用ChatGPT将句子情感分类为“积极”和“消极”。
- en: However, what if we wanted a wider range of sentiment labels? For instance,
    we could use the labels “extremely positive”, “positive”, “somewhat positive”,
    “neutral”, “somewhat negative”, “negative”, and “extremely negative”.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，如果我们想要更广泛的情感标签范围怎么办？例如，我们可以使用“极其积极”、“积极”、“有些积极”、“中立”、“有些负面”、“负面”和“极其负面”这些标签。
- en: To enable ChatGPT (or another LLM product) to incorporate specific grading criteria,
    few-shot prompting can be an effective strategy.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使ChatGPT（或其他LLM产品）能够结合特定的评分标准，少量示例提示可以是一种有效的策略。
- en: '**User Prompt:**'
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**用户提示：**'
- en: ''
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '“Sentence: This basketball fits in my bag.'
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “句子：这个篮球能放进我的包里。”
- en: ''
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Sentiment: somewhat positive.'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 情感：有些积极。
- en: ''
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Sentence: This basketball deflates after just one game!'
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 句子：这个篮球在玩了一场比赛后就漏气了！
- en: ''
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Sentiment: extremely negative.'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 情感：极其负面。
- en: ''
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Sentence: This basketball is orange.'
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 句子：这个篮球是橙色的。
- en: ''
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Sentiment: neutral.'
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 情感：中立。
- en: ''
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Sentence: This basketball’s feel is a bit better than I am used to.'
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 句子：这个篮球的手感比我习惯的要好一点。
- en: ''
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sentiment:”
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 情感：
- en: ''
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**ChatGPT:** “Somewhat Positive”'
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ChatGPT：** “有些积极”'
- en: By introducing ChatGPT to the new range of sentiment labels, ChatGPT can be
    “taught” to classify sentences with those same labels.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通过向ChatGPT介绍新的情感标签范围，可以“教会”ChatGPT使用这些相同的标签来分类句子。
- en: Why We Can’t Use Infinite Examples
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么我们不能使用无限示例
- en: At this point, it’s tempting to consider including as many examples as possible
    in user prompts to guarantee a successful outcome.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，很容易考虑在用户提示中包含尽可能多的示例以确保成功结果。
- en: After all, the more context, the better, right?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 毕竟，更多的上下文，效果会更好，对吧？
- en: While that could be the case, LLMs are constrained by a limited number of useable
    tokens. Every LLM has a **context window**, the maximum number of tokens that
    the model can take as input.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可能是这样，但LLM受到可用令牌数量的限制。每个LLM都有一个**上下文窗口**，即模型可以作为输入的最大令牌数量。
- en: The number of tokens afforded to users depends on the underlying model itself.
    Oftentimes, the specific figures can be found in the documentation.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可用的令牌数量取决于底层模型本身。通常，具体的数字可以在文档中找到。
- en: For instance, the token limits of OpenAI’s GPT-3.5 models are detailed in the
    OpenAI API documentation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，OpenAI的GPT-3.5模型的令牌限制在OpenAI API文档中有详细说明。
- en: '![](../Images/66ded1f0ea457a88cc32d18d08553c9d.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66ded1f0ea457a88cc32d18d08553c9d.png)'
- en: 'Token Limits for GPT-3.5 Models (Source: [Models — OpenAI API](https://platform.openai.com/docs/models/gpt-3-5))'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3.5模型的令牌限制（来源：[Models — OpenAI API](https://platform.openai.com/docs/models/gpt-3-5)）
- en: Users exhaust tokens when writing user prompts and, as a result, don’t have
    the bandwidth to bombard LLMs with countless examples.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 用户在编写用户提示时会消耗令牌，因此没有足够的带宽向LLM提供大量示例。
- en: Instead, they have to figure out how much context an LLM will need to perform
    a given task adequately, and then determine how they can provide that context
    in as few tokens as possible.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，他们必须弄清楚LLM完成特定任务所需的上下文量，然后确定如何在尽可能少的令牌中提供这些上下文。
- en: Key Takeaways
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键要点
- en: '![](../Images/485eb33f1f57fd35c494dadcda69356b.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/485eb33f1f57fd35c494dadcda69356b.png)'
- en: Photo by [Prateek Katyal](https://unsplash.com/@prateekkatyal?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 摄影师：[Prateek Katyal](https://unsplash.com/@prateekkatyal?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)上的照片
- en: Overall, users can yield satisfactory results from their LLM *without* providing
    additional context within their prompts since these models are already trained
    with high volumes of data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，用户可以从他们的LLM中获得令人满意的结果，而*不需要*在提示中提供额外的上下文，因为这些模型已经用大量数据进行了训练。
- en: However, for cases where prompts are complex or where responses have to meet
    unique criteria, users can benefit from including one or more examples in their
    prompts (i.e., one-shot prompting, few-shot prompting), provided that they do
    so with the allotted number of tokens.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于提示复杂或响应需满足特定标准的情况，用户可以通过在提示中包含一个或多个示例（即单样本提示、少样本提示）来受益，前提是他们在分配的令牌数量范围内这样做。
- en: Whether you are using a ready-made chatbot or are building your own custom-made
    LLM product, using zero-shot, one-shot, and few-shot prompting is a simple yet
    effective way to obtain adequate results.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是使用现成的聊天机器人，还是构建自己的定制化大型语言模型（LLM）产品，使用零样本、单样本和少样本提示是一种简单而有效的获取足够结果的方法。
- en: Thank you for reading!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
