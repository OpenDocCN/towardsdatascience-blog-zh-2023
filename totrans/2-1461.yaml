- en: 'Machine Learning with Expert Models: A Primer'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/machine-learning-with-expert-models-a-primer-6c74585f223f](https://towardsdatascience.com/machine-learning-with-expert-models-a-primer-6c74585f223f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How a decades-old idea enables training outrageously large neural networks today
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samuel.flender?source=post_page-----6c74585f223f--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----6c74585f223f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6c74585f223f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6c74585f223f--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----6c74585f223f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6c74585f223f--------------------------------)
    ·9 min read·Sep 5, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6589b3c96e9f917a19170e43a71a9d9a.png)'
  prefs: []
  type: TYPE_IMG
- en: ([Pexels](https://www.pexels.com/photo/set-of-tool-wrench-162553/))
  prefs: []
  type: TYPE_NORMAL
- en: Expert models are one of the most useful inventions in Machine Learning, yet
    they hardly receive as much attention as they deserve. In fact, expert modeling
    does not only allow us to train neural networks that are “outrageously large”
    (more on that later), they also allow us to build models that learn more like
    the human brain, that is, different regions specialize in different types of input.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we’ll take a tour of the key innovations in expert modeling
    which ultimately lead to recent breakthroughs such as the Switch Transformer and
    the Expert Choice Routing algorithm. But let’s go back first to the paper that
    started it all: “Mixtures of Experts”.'
  prefs: []
  type: TYPE_NORMAL
- en: Mixtures of Experts (1991)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/ee84508a45288fb4a059d3aeeee05161.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The original MoE model from 1991\. Image credit: [Jabocs et al 1991, Adaptive
    Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea of mixtures of experts (MoE) traces back more than 3 decades ago,
    to a 1991 [paper](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf) co-authored
    by none other than the godfather of AI, Geoffrey Hinton. The key idea in MoE is
    to model an output “y” by combining a number of “experts” E, the weight of each
    is being controlled by a “gating network” G:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/22bac53e59837ac67d6b84d8f758bb9f.png)'
  prefs: []
  type: TYPE_IMG
- en: An expert in this context can be any kind of model, but is usually chosen to
    be a multi-layered neural network, and the gating network is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1d03c869186766e071ffdf2da9fe361.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where W is a learnable matrix that assigns training examples to experts. When
    training MoE models, the learning objective is therefore two-fold:'
  prefs: []
  type: TYPE_NORMAL
- en: the experts will learn to process the output they’re given into the best possible
    output (i.e., a prediction), and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the gating network will learn to “route” the right training examples to the
    right experts, by jointly learning the routing matrix W.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Why should one do this? And why does it work? At a high level, there are three
    main motivations for using such an approach:'
  prefs: []
  type: TYPE_NORMAL
- en: First, MoE allows scaling neural networks to very large sizes due to the sparsity
    of the resulting model, that is, even though the overall model is large, only
    a small amount of computation needs to be executed for any given training example
    because of the presence of highly specialized experts. This approach stands in
    contrast to the standard “dense” neural networks, in which every single neuron
    is needed for every single input example.
  prefs: []
  type: TYPE_NORMAL
- en: Second, MoE allows for better model generalization with less risk of overfitting
    because each individual expert can be a relatively small neural network, yet we
    still achieve strong overall model performance by adding more experts. Much like
    boosting, it’s a way to combine a large number of relatively weak learners into
    a single, strong learner.
  prefs: []
  type: TYPE_NORMAL
- en: 'And third, MoE mimics more closely how our brains work: any given input only
    activates certain regions in our brains, with distinct regions for touch, vision,
    sound, smell, taste, orientation, and so on. All of these are, well, “experts”.'
  prefs: []
  type: TYPE_NORMAL
- en: In short, MoE frees us from the requirement that every single neuron needs to
    be activated for every single input. MoE models are sparse, highly flexible, and
    extremely powerful.
  prefs: []
  type: TYPE_NORMAL
- en: Outrageously large neural networks (2017)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fast-forward 26 (!) years to the highly influential paper “[Outrageously large
    neural networks](https://arxiv.org/pdf/1701.06538.pdf)”, again from Hinton’s team
    (this time at Google Brain). In this work, the authors take MoE to its limits,
    presenting a 6 Billion parameter MoE model with thousands of experts. In order
    to build such an outrageously large MoE model, the authors introduce several modeling
    tricks, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Noisy top-k gating.** Top-k gating simply means that for each training example
    we only compute the expert output from the top k experts (which are determined
    by the gate), and ignore all of the other experts. The main motivation is saved
    computational cost: for example, if we have 20 experts and apply top-k gating
    with k=5, we’d cut down the total computational cost of training the model by
    a factor of 4!'
  prefs: []
  type: TYPE_NORMAL
- en: “Noisy” means that we’re adding tunable Gaussian random noise to the gating
    values. The authors find that this help with load balancing, that is, making sure
    an entire batch of data is not being sent to a single expert.
  prefs: []
  type: TYPE_NORMAL
- en: '**Auxiliary losses.** The authors add two auxiliary (regularizing) loss terms
    to the model’s loss function, “load balancing loss” and “expert diversity loss”,
    each with its own tunable regularization parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing loss is proportional to the coefficient of variation in the number
    of training examples received by each expert. Adding this loss improves computational
    performance because it prevents the introduction of “expert bottlenecks” where
    all training examples have to pass through one expert. (One subtlety is that the
    number of training examples per expert is not differentiable — so the authors
    use a smooth approximation of that number instead.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expert diversity loss is proportional to the coefficient of variation of expert
    importances, where expert importance is defined as the sum of the gate values
    for that expert. Adding this loss nudges the model to utilize all of the experts
    equally, instead of simply sending all training examples to a single expert that
    learns everything, which is a common problem and a local minimum in training MoE
    models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While these two loss terms are similar, the authors find the best overall performance
    when adding both load balancing loss and expert diversity loss, both with a regularization
    parameter of 0.1.
  prefs: []
  type: TYPE_NORMAL
- en: '**Customized parallelism.** The authors show that large MoE models benefit
    from a customized combination of [data parallelism and model parallelism](/distributed-learning-a-primer-790812b817f1):
    in particular, we can allow for experts to be distributed across machines because
    we don’t require communication between them, while at the same time using data
    parallelism in order to increase the batch size. This form of parallelism allows
    us to massively scale up the MoE model: in their experiments, the authors scale
    it up to 6 Billion parameters with thousands of experts.'
  prefs: []
  type: TYPE_NORMAL
- en: Using their massive MoE model, the authors establish a new SOTA on the Billion-words
    language modeling benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: Switch Transformers (2022)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/b3d1e3e8781ff387be9ab7914d8cb929.png)'
  prefs: []
  type: TYPE_IMG
- en: '7x faster training with the Switch Transformer. Image credit: [Fedus et al
    2022](https://arxiv.org/pdf/2101.03961.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: While “Outrageously large neural networks” demonstrated the usefulness of top-k
    gating in MoE models, [Switch Transformers](https://arxiv.org/pdf/2101.03961.pdf),
    also by Google, brought it to its extreme conclusion by building MoE Transformer
    models with k=1, that is, each training example is only being sent to a single
    expert. The authors call this “hard routing”, in contrast to the “soft routing”
    in standard MoE models where the output from multiple experts is being pooled.
  prefs: []
  type: TYPE_NORMAL
- en: 'Practically, hard routing (k=1) means that we can have N>1 experts, with *any*
    number N, and constant computational complexity: the model capacity scales as
    O(1)! The only trade-off is that we need a huge amount of memory to store all
    of the experts. However, since we require no communication between experts, that
    memory can be distributed across a large cluster of machines.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, the authors introduce a number of practical modeling tricks, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '**precision casting**: this means that we send weights in between machines
    in BF16 (16-bit [brain float](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus))
    but cast them to FP32 (32-bit floating point precision) when computing the gating
    values. This trick minimized communication overhead because we only need to communicate
    16 instead of 32 bits, while at the same time ensuring that the softmax computation
    is stable (which it isn’t with 16 bits).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**aggressive expert dropout**: the authors find that they can improve the performance
    of the model by applying aggressive dropout of 0.4 within the expert modules,
    while keeping dropout in the rest of the model architecture at a smaller rate
    of 0.1\. The reasoning is that experts overfit more easily because they only see
    a fraction of the data, and therefore need to be regularized more heavily.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scaled expert initialization**: the authors find much better training stability
    when scaling the initialization of the expert layers down by a factor of 10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ultimately, the authors build a Switch Transformer based off of Google’s T5
    language model and obtain a 7x increase in pre-training speed with the same computational
    resources, a powerful demonstration of the modeling improvements we can get by
    combining hard-routing based MoE with Transformer models.
  prefs: []
  type: TYPE_NORMAL
- en: Expert Choice Routing (2022)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/45da8d3241e0d84578938532e41ce163.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Standard token-choice routing (left) compared to the new expert-choice routing
    algorithm (right). Note that in expert choice routing, a single token can be routed
    to multiple experts at the same time. Image credit: [Zhou et al 2022](https://arxiv.org/abs/2202.09368)'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most recent breakthroughs in expert modeling is “[Expert choice routing](https://arxiv.org/abs/2202.09368)”,
    yet another innovation by Google.
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem with training MoE models is that experts often remain under-utilized
    because of the “winner-take-all” effect, where a single expert that randomly receives
    the first few training examples rapidly becomes much better than the other experts
    and ends up dominating the gate. So far, the standard practice has been to add
    auxiliary losses which nudge the model to utilize all of the experts equally (such
    as expert diversity loss in Hinton’s work). However, adding auxiliary losses introduces
    the problem of tuning their regularization parameters relative to the actual loss
    we want to minimize: too small, and there’s no change in the model behavior at
    all, too large, and we risk making all experts identical (another local minimum).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The key idea in “expert choice routing” is simple yet powerful: let expert
    choose their top k training examples within a batch, instead of having a training
    example choose its top k experts. This has several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: it guarantees perfect load balancing with equal expert utilization (each expert
    *always* gets the same number of training examples in each batch),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'not every training example ends up with the same number of experts, some of
    them may be routed to 1 experts, and some of them to all experts. This is a desired
    property: different training examples represent different degrees of difficulty
    for the model, and consequently may require a different number of experts,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: no need for adhoc auxiliary losses that need to be tuned.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mathematically, expert choice routing replaces the standard gating function
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/629e1928f81ac1ab4fe6518224904ef0.png)'
  prefs: []
  type: TYPE_IMG
- en: with
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f0640c537b330ab652b38037fe5b352.png)'
  prefs: []
  type: TYPE_IMG
- en: that is, instead of an e-dimensional vector, the gate G is now a matrix with
    dimensions e (the number of experts) times n (the number of tokens). Why tokens?
    The authors consider expert choice routing in the context of NLP problems, hence
    each training example is simply a sequence of n possible tokens.
  prefs: []
  type: TYPE_NORMAL
- en: So far for the theory (which really boils down to transforming G from a vector
    into a matrix), but how well does it work in practice? The authors show that they
    can achieve the same performance as the Switch Transformer in half the training
    time, and for the same computational cost, they beat the Switch Transformer on
    11 tasks from the GLUE and SuperGLUE benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: Expert Choice Routing beat the Switch Transformer, which proved that, at least
    for NLP problems, “expert choice” is superior to “token choice”.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A quick recap:'
  prefs: []
  type: TYPE_NORMAL
- en: the key idea in MoE models is to model the output y as a weighted sum of experts,
    where each expert is simply a small neural network itself, and the weights are
    determined by G(x) = softmax(Wx), where W is a learnable matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: top-k gating means that we ignore all experts except for the top k. This saves
    considerable computational cost, and was an important breakthrough in expert modeling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MoE models often get stuck in a local minimum where all training examples are
    simply being sent to a single expert. A remedy is to add “expert diversity loss”
    to the model’s loss function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Switch Transformers combined the idea of MoE with Transformer models, and showed
    that this combination allows training the T5 language model 7x faster. A key innovation
    here is that of “hard routing”, which is simply top-k routing with k=1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expert Choice Routing replaced the idea of training examples choosing their
    experts with experts choosing their training examples. This allows for better
    training stability without the introduction of adhoc auxiliary losses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And this is just the tip of the iceberg. In fact, there’s a wealth of new papers
    on expert modeling that has been sparked by the success of Hinton’s “outrageously
    large” MoE model, the Switch Transformer, and the new expert choice routing algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6bc1c533d38fc68d148ffdc1efbeb8ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Cluster of research papers on MoE models, with the “Expert Choice” paper highlighted
    in red, Source: [ConnectedPapers](https://www.connectedpapers.com/main/bbc57e1b3cf90e09b64377f13de455793bc81ad5/Mixture%20of%20Experts-with-Expert-Choice-Routing/graph).'
  prefs: []
  type: TYPE_NORMAL
- en: Expert modeling is an exciting domain that has been decades in the making, and
    we’re just starting to see its impact on modern Machine Learning applications.
    Watch this space — new breakthroughs are certainly on the horizon!
  prefs: []
  type: TYPE_NORMAL
