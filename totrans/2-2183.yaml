- en: Understanding Gradient Descent for Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解机器学习中的梯度下降
- en: 原文：[https://towardsdatascience.com/understanding-gradient-descent-for-machine-learning-246e324c229](https://towardsdatascience.com/understanding-gradient-descent-for-machine-learning-246e324c229)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-gradient-descent-for-machine-learning-246e324c229](https://towardsdatascience.com/understanding-gradient-descent-for-machine-learning-246e324c229)
- en: A deep dive into Batch, Stochastic, and Mini-Batch Gradient Descent algorithms
    using Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Python深入探讨批量、随机和小批量梯度下降算法
- en: '[](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)[![Idil
    Ismiguzel](../Images/6846628535770a9f3e13ebb555e82abd.png)](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)[](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)
    [Idil Ismiguzel](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)[![Idil
    Ismiguzel](../Images/6846628535770a9f3e13ebb555e82abd.png)](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)[](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)
    [Idil Ismiguzel](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)
    ·14 min read·May 21, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)
    ·阅读时间14分钟·2023年5月21日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5287b14c08ccf02afd428f2beb4f7204.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5287b14c08ccf02afd428f2beb4f7204.png)'
- en: Photo by [Lucas Clara](https://unsplash.com/ko/@lux17?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Lucas Clara](https://unsplash.com/ko/@lux17?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Gradient descent is a popular optimization algorithm that is used in machine
    learning and deep learning models such as linear regression, logistic regression,
    and neural networks. It uses first-order derivatives iteratively to minimize the
    cost function by updating model coefficients (for regression) and weights (for
    neural networks).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种流行的优化算法，广泛应用于机器学习和深度学习模型，如线性回归、逻辑回归和神经网络。它通过迭代使用一阶导数来最小化成本函数，通过更新模型系数（用于回归）和权重（用于神经网络）。
- en: In this article, we will delve into the mathematical theory of gradient descent
    and explore how to perform calculations using Python. We will examine various
    implementations including Batch Gradient Descent, Stochastic Gradient Descent,
    and Mini-Batch Gradient Descent, and assess their effectiveness on a range of
    test cases.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将深入探讨梯度下降的数学理论，并探索如何使用Python进行计算。我们将检查包括批量梯度下降、随机梯度下降和小批量梯度下降在内的各种实现，并评估它们在不同测试案例中的效果。
- en: While following the article, you can check out the [Jupyter Notebook](https://github.com/Idilismiguzel/Machine-Learning/blob/master/Gradient_Descent/gradient_descent_implementation.ipynb)
    on my GitHub for complete analysis and code.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读本文的同时，你可以查看我GitHub上的[Jupyter Notebook](https://github.com/Idilismiguzel/Machine-Learning/blob/master/Gradient_Descent/gradient_descent_implementation.ipynb)以获取完整的分析和代码。
- en: Before a deep dive into gradient descent, let’s first go through the loss function.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨梯度下降之前，让我们首先了解损失函数。
- en: What is Loss Function?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是损失函数？
- en: '**Loss** or **cost** are used interchangeably to describe the error in a prediction.
    A loss value indicates how different a prediction is from the actual value and
    the loss function aggregates all the loss values from multiple data points into
    a single number.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**损失**或**成本**这两个术语可以互换使用，用来描述预测中的误差。损失值表示预测值与实际值的差异，损失函数将来自多个数据点的所有损失值汇总为一个单一的数字。'
- en: You can see in the image below, the model on the left has high loss whereas
    the model on the right has low loss and fits the data better.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从下图中可以看出，左侧的模型具有高损失，而右侧的模型具有低损失，并且更好地拟合了数据。
- en: '![](../Images/eb7e40a2f95aa71254db5f4592880b6d.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb7e40a2f95aa71254db5f4592880b6d.png)'
- en: High loss vs low loss (blue lines) from the corresponding regression line in
    yellow.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 高损失与低损失（蓝线）相对于黄色回归线的对比。
- en: The loss function (J) is used as a performance measurement for prediction algorithms
    and the main goal of a predictive model is to minimize its loss function, which
    is determined by the values of the model parameters (i.e., θ0 and θ1).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数（J）用作预测算法的性能测量工具，预测模型的主要目标是最小化其损失函数，这由模型参数的值（即 θ0 和 θ1）决定。
- en: For example, linear regression models frequently use squared loss to compute
    the loss value and mean squared error is the loss function that averages all squared
    losses.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，线性回归模型通常使用平方损失来计算损失值，而均方误差是平均所有平方损失的损失函数。
- en: '![](../Images/d702a1a157eed6b1cb56125ef219ea27.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d702a1a157eed6b1cb56125ef219ea27.png)'
- en: Squared Loss value (L2 Loss) and Mean Squared Error (MSE)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 平方损失值（L2 损失）和均方误差（MSE）
- en: The linear regression model works behind the scenes by going through several
    iterations to optimize its coefficients and reach the lowest possible mean squared
    error.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型在后台通过多次迭代来优化其系数，以达到尽可能低的均方误差。
- en: What is Gradient Descent?
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是梯度下降？
- en: 'The gradient descent algorithm is usually described with a mountain analogy:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法通常用山的类比来描述：
- en: ⛰ Imagine yourself standing atop a mountain, with limited visibility, and you
    want to reach the ground. While descending, you'll encounter slopes and pass them
    using larger or smaller steps. Once you've reached a slope that is almost leveled,
    you'll know that you've arrived at the lowest point. ⛰
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⛰ 想象你站在山顶，视野有限，你想要到达地面。在下坡时，你会遇到斜坡，并通过较大或较小的步伐通过它们。一旦你到达几乎平坦的斜坡，你就会知道你已经到达最低点。
    ⛰
- en: In technical terms, **gradient** refers to these slopes. When the slope is zero,
    it may indicate that you’ve reached a function's minimum or maximum value.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，**梯度**指的就是这些斜率。当斜率为零时，可能表示你已经到达了函数的最小值或最大值。
- en: '![](../Images/2d3f12fa02efaed21b87ca718f61b10a.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d3f12fa02efaed21b87ca718f61b10a.png)'
- en: Like in the mountain analogy, GD minimizes the starting loss value by taking
    repeated steps in the opposite direction of the gradient to reduce the loss function.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在山的类比中一样，GD 通过在梯度的相反方向上重复迈步来最小化起始损失值，从而减少损失函数。
- en: At any given point on a curve, the steepness of the slope can be determined
    by a **tangent line** — a straight line that touches the point (red lines in the
    image above). Similar to the tangent line, the gradient of a point on the loss
    function is calculated with respect to the parameters, and a small step is taken
    in the opposite direction to reduce the loss.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在曲线上的任何一点，斜率的陡峭程度可以通过**切线**来确定——一条与该点相切的直线（如上图中的红线）。类似于切线，损失函数上某一点的梯度是相对于参数计算的，并且会朝相反方向迈出小步以减少损失。
- en: 'To summarize, the process of gradient descent can be broken down into the following
    steps:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，梯度下降的过程可以分解为以下步骤：
- en: Select a starting point for the model parameters.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择模型参数的起始点。
- en: Determine the gradient of the cost function with respect to the parameters and
    continually adjust the parameter values through iterative steps to minimize the
    cost function.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定成本函数相对于参数的梯度，并通过迭代步骤不断调整参数值以最小化成本函数。
- en: Repeat step 2 until the cost function no longer decreases or the maximum number
    of iterations is reached.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 2，直到成本函数不再减少或达到最大迭代次数。
- en: We can examine the gradient calculation for the previously defined cost (loss)
    function. Although we are utilizing linear regression with an intercept and coefficient,
    this reasoning can be extended to regression models incorporating several variables.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查之前定义的成本（损失）函数的梯度计算。虽然我们正在使用具有截距和系数的线性回归，但这种推理可以扩展到包含多个变量的回归模型。
- en: '![](../Images/b7efa05a2b319a6b2afaf3b0584f7bc4.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7efa05a2b319a6b2afaf3b0584f7bc4.png)'
- en: Linear regression function with 2 parameters, cost function, and objective function
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 具有 2 个参数的线性回归函数、成本函数和目标函数
- en: '![](../Images/abe1795d9a85eca87cc8383bef5dded3.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/abe1795d9a85eca87cc8383bef5dded3.png)'
- en: Partial derivatives calculated wrt model parameters
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于模型参数计算的偏导数
- en: 💡 Sometimes, the point that has been reached may only be a *local minimum* or
    a *plateau*. In such cases, the model needs to continue iterating until it reaches
    the global minimum. Reaching the global minimum is unfortunately not guaranteed
    but with a proper number of iterations and a learning rate we can increase the
    chances.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8df9feaf7bf49a9654c5b8e0f1944cac.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: When using gradient descent, it is important to be aware of the potential challenge
    of stopping at a local minimum or on a plateau. To avoid this, it is essential
    to choose the appropriate number of iterations and learning rate. We will discuss
    this further in the following sections.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '`Learning_rate` is the hyperparameter of gradient descent to define the size
    of the learning step. It can be tuned using [hyperparameter tuning techniques](https://medium.com/towards-data-science/hyperparameter-tuning-with-grid-search-and-random-search-6e1b5e175144).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: If the `learning_rate` is set too high it could result in a jump that produces
    a loss value greater than the starting point. A high `learning_rate` might cause
    gradient descent to **diverge**,leading it to continually obtain higher loss values
    and preventing it from finding the minimum.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/d1b33f2d63d0773bf5f91edff2582bf2.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: 'Example case: A high learning rate causes GD to diverge'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: If the `learning_rate` is set too low it can lead to a lengthy computation process
    where gradient descent iterates through numerous rounds of gradient calculations
    to reach **convergence** and discover the minimum loss value.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/50be59657a670b4f90cb4f14f649715d.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: 'Example case: A low learning rate causes GD to take too much time to converge'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: The value of the learning step is determined by the slope of the curve, which
    means that as we approach the minimum point, the learning steps become smaller.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: When using low learning rates, the progress made will be steady, whereas high
    learning rates may result in either exponential progress or being stuck at low
    points.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af1f3ef2e20a9932f0f6dfb4e0b4c078.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: Image adapted from [https://cs231n.github.io/neural-networks-3/](https://cs231n.github.io/neural-networks-3/)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: We will now cover three different implementations of the gradient descent algorithm.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Batch Gradient Descent
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The batch gradient descent is the most widely used method for implementing gradient
    descent. It involves computing gradients with respect to the model parameters
    (such as regression coefficients) at every iteration for the entire data set.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at an example 🔍
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: First, generate a dataset with an intercept of 5 and a coefficient of 4, along
    with a small amount of Gaussian noise. See the scatter plot of the generated data
    below.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/7644f9241b46b9de225680a0cfd89fa6.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: Scatter plot of data generated
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: The following function performs batch gradient descent by utilizing a specified
    learning rate and number of iterations. Initially, the model's coefficient (m)
    and intercept (b) are set to 0.5\. During each iteration, the error is calculated
    by taking the difference between predicted and actual values of y. The algorithm
    then updates m and b by extracting the gradient, which is also multiplied by the
    learning rate. The loop continues until the specified number of iterations is
    reached, and the resulting loss value and model parameters are stored in `params`
    and `loss`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数通过利用指定的学习率和迭代次数来执行批量梯度下降。最初，模型的系数（m）和截距（b）都设置为0.5。在每次迭代中，通过计算预测值和实际值之间的差异来确定误差。然后，算法通过提取梯度来更新m和b，梯度也会乘以学习率。循环持续进行，直到达到指定的迭代次数，结果的损失值和模型参数被存储在`params`和`loss`中。
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using the function above, we will now test different learning rates and evaluate
    the performance.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述函数，我们现在将测试不同的学习率并评估性能。
- en: 1️⃣ Let’s set `learning_rate=0.01` with 1000 iterations and plot the loss function.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 让我们设置 `learning_rate=0.01`，进行1000次迭代，并绘制损失函数图。
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/9b4bc468d425495f4c02fd781a6140d2.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b4bc468d425495f4c02fd781a6140d2.png)'
- en: Batch Gradient Descent with a learning rate of 0.01 and 1000 iterations
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用学习率为0.01和1000次迭代的批量梯度下降
- en: 'm: 4.27, b: 4.88, MSE: 0.95'
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'm: 4.27, b: 4.88, MSE: 0.95'
- en: ''
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 367 ms'
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 执行时间：367毫秒
- en: In the plots above, first, you can see the regression lines generated after
    each iteration (in purple) and notice how it gradually approaches the optimal
    after around 100 iterations. In the second plot, the loss function is displayed
    after every iteration, showing a significant decrease in the loss within the first
    50-100 iterations.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述图中，首先可以看到每次迭代生成的回归线（紫色），并注意它如何在大约100次迭代后逐渐接近最优。在第二个图中，损失函数在每次迭代后显示，在前50-100次迭代内损失显著减少。
- en: 2️⃣ Let’s now set `learning_rate=0.001` with the same number of iterations and
    plot the loss function.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 现在让我们设置 `learning_rate=0.001`，使用相同的迭代次数，并绘制损失函数图。
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'm: 4.67, b: 4.31, MSE: 1.05'
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'm: 4.67, b: 4.31, MSE: 1.05'
- en: ''
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 522 ms'
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 执行时间：522毫秒
- en: '![](../Images/e964bffeda41b26589b8f1124398fab2.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e964bffeda41b26589b8f1124398fab2.png)'
- en: Batch Gradient Descent with a learning rate of 0.001 and 1000 iterations
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用学习率为0.001和1000次迭代的批量梯度下降
- en: When the learning rate was reduced, the model needed more time to reach convergence
    and ended up with a higher final loss value. The first plot shows a darker shade
    of purple under the linear regression line, which means that there were various
    iterations that were far from the optimal line. Also, note that decreasing the
    learning rate from 0.01 to 0.001 resulted in an increase in execution time from
    367 ms to 522 ms.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当学习率降低时，模型需要更多时间才能收敛，并且最终损失值较高。第一个图显示了回归线下方更深的紫色阴影，这意味着在许多迭代中远离了最优线。此外，请注意将学习率从0.01减少到0.001导致执行时间从367毫秒增加到522毫秒。
- en: 3️⃣ Let’s test `learning_rate=0.1` using the same number of iterations and plot
    the loss function.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ 让我们使用相同的迭代次数设置 `learning_rate=0.1` 并绘制损失函数图。
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'm: 4.23, b: 4.93, MSE: 0.95'
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'm: 4.23, b: 4.93, MSE: 0.95'
- en: ''
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 214 ms'
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 执行时间：214毫秒
- en: '![](../Images/f93e84dee0e7b2db02ecae1807754fb5.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f93e84dee0e7b2db02ecae1807754fb5.png)'
- en: Batch Gradient Descent with a learning rate of 0.1 and 1000 iterations
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用学习率为0.1和1000次迭代的批量梯度下降
- en: The model displayed rapid convergence when a high learning rate was used. In
    the first plot, there were only a few iterations where the regression line was
    distant from the optimal linear regression line. In the second plot, the loss
    sharply decreased during the first few iterations and remained steady until all
    iterations were completed. Furthermore, the execution time was the lowest at 214ms,
    out of the three trials.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用高学习率时，模型表现出快速收敛。在第一个图中，只有少数几次迭代中回归线远离最优线。在第二个图中，损失在前几次迭代中急剧下降，并保持稳定直到所有迭代完成。此外，执行时间在三次试验中最低，为214毫秒。
- en: 🚨Calculating the gradient for the entire training data set for every parameter
    update can be computationally expensive on large data sets. Fortunately, stochastic
    gradient descent or mini-batch stochastic gradient descent can help solve this
    issue.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 🚨 对于每次参数更新计算整个训练数据集的梯度在大型数据集上可能计算开销较大。幸运的是，随机梯度下降或小批量随机梯度下降可以帮助解决这个问题。
- en: 2\. Stochastic Gradient Descent
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 随机梯度下降
- en: Another way to implement gradient descent is using stochastic gradient descent.
    It is especially preferred for large training data sets where the batch gradient
    descent might take too much time to compute.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic gradient descent updates model coefficients by calculating the loss
    value for only one **random** data point instead of calculating for every data
    point in the training data set and aggregating.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Random selection of the data point is essential to prevent getting stuck around
    similar values (such as clusters in the data). The learning rate and the number
    of iterations are also critical factors, similar to batch gradient descent.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: The following function performs stochastic gradient descent by utilizing a specified
    learning rate and number of iterations. Initially, the model’s coefficient (m)
    and intercept (b) are set to 0.5\. In each iteration, the error is computed for
    a randomly selected data point using `np.random`. The remaining steps of the function
    are identical to those in batch gradient descent.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Let's try using the same learning rate and number of iterations that we used
    for batch gradient descent.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 1️⃣ Let’s set `learning_rate=0.01` with 1000 iterations and plot the loss function.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'MSE: 0.97'
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 325 ms'
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/dabbc95a78881d732f085ce2cf4cf5d0.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: Stochastic Gradient Descent with a learning rate of 0.01 and 1000 iterations
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Notice that with the same learning rate, we were able to reduce the execution
    time from 367 ms to 325 ms when compared to batch gradient descent. However, the
    mean squared error (MSE) increased from 0.95 to 0.97.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 2️⃣ Let’s now set `learning_rate=0.001` with 1000 iterations and plot the loss
    function. I won't be including the code snippet since it's the same as in previous
    examples. However, feel free to refer to the [source code of the article](https://github.com/Idilismiguzel/Machine-Learning/blob/master/Gradient_Descent/gradient_descent_implementation.ipynb)
    if you need it.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'MSE: 1.03'
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 253 ms'
  id: totrans-106
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/374b35ec0c7c67e430e5558e260e27f4.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: Stochastic Gradient Descent with a learning rate of 0.001 and 1000 iterations
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Notice that using batch gradient descent with a learning rate of 0.001 resulted
    in a loss value of 1.05 in 522 milliseconds. However, using stochastic gradient
    descent with the same learning rate led to a loss value of 1.03 in only 253 milliseconds.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 3️⃣ Let’s now set `learning_rate=0.1` with 1000 iterations and plot the loss
    function.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'MSE: 1.27'
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-112
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 237 ms'
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/3297564ca9ad3cf2a892944218541bde.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: Stochastic Gradient Descent with a learning rate of 0.1 and 1000 iterations
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: When using a high learning rate with stochastic gradient descent, the computation
    introduced fluctuations in the loss values. As shown in the first plot, there
    were numerous iterations above and below the optimal regression line.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Overall, batch gradient descent outperforms stochastic gradient descent when
    it comes to converging to the minimum as stochastic gradient descent tends to
    wander around a vicinity near the global minimum. However, if the learning rate
    is carefully selected, stochastic gradient descent can also achieve similar loss
    values (sometimes even better) in a shorter amount of time.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，批量梯度下降在收敛到最小值方面优于随机梯度下降，因为随机梯度下降往往会在全局最小值附近徘徊。然而，如果仔细选择学习率，随机梯度下降也可以在较短的时间内达到类似的损失值（有时甚至更好）。
- en: 3\. Mini-Batch Gradient Descent
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 迷你批量梯度下降
- en: Mini-batch gradient descent is a useful intermediate option between batch and
    stochastic gradient descent. Rather than computing the gradient for the entire
    dataset or just one observation, it divides the training data into smaller batches
    and calculates the gradient for each one.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 迷你批量梯度下降是批量和随机梯度下降之间的一个有用的中间选项。它不是为整个数据集或单个观察值计算梯度，而是将训练数据分成较小的批次，并为每个批次计算梯度。
- en: By combining batch and stochastic methods, mini-batch GD enhances computation
    speed compared to the batch model and improves the accuracy of the stochastic
    model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合批量和随机方法，迷你批量GD在计算速度上优于批量模型，并且提高了随机模型的准确性。
- en: The function below executes mini-batch gradient descent using a given learning
    rate, a set number of iterations, and a chosen batch size. By specifying a batch
    size, we can determine the number of data points to include in each gradient calculation.
    For instance, in stochastic gradient descent, the batch size is just one and the
    gradient is computed for a single data point. However, if we set a batch size
    of 10, then the gradient will be computed for 10 data points and then combined.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数执行迷你批量梯度下降，使用给定的学习率、设定的迭代次数和选定的批量大小。通过指定批量大小，我们可以确定每次梯度计算中包含的数据点数量。例如，在随机梯度下降中，批量大小为1，梯度是针对单个数据点计算的。然而，如果我们设置批量大小为10，那么梯度将会针对10个数据点进行计算，然后合并。
- en: '[PRE7]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Let’s test the same 3 cases with `learning_rate=0.01` `learning_rate=0.001`
    and `learning_rate=0.1.` using 10 as batch size.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使用批量大小为10的情况下，测试相同的3种情况，`learning_rate=0.01`、`learning_rate=0.001`和`learning_rate=0.1`。
- en: 1️⃣ `learning_rate=0.01` with 1000 iterations.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ `learning_rate=0.01` 并进行1000次迭代。
- en: '[PRE8]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'MSE: 0.96'
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'MSE: 0.96'
- en: ''
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 224 ms'
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 执行时间：224毫秒
- en: '![](../Images/1d41acb0d5739dd227f564a8caed7ced.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d41acb0d5739dd227f564a8caed7ced.png)'
- en: Mini-Batch Gradient Descent with a learning rate of 0.01 and 1000 iterations
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用学习率为0.01和1000次迭代的迷你批量梯度下降
- en: Have you noticed that by using a 0.01 learning rate, we were able to achieve
    an MSE of 0.95 in 367 ms with batch GD, and an MSE of 0.97 in 325 ms with stochastic
    GD? Furthermore, with the implementation of mini-batch gradient descent, we were
    able to attain an MSE of 0.96 in just 224 ms.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否注意到，使用0.01的学习率时，我们能够在367毫秒内通过批量GD达到MSE为0.95，而使用随机GD则在325毫秒内得到MSE为0.97？此外，通过迷你批量梯度下降，我们能够在仅224毫秒内达到MSE为0.96。
- en: 2️⃣ `learning_rate=0.001` with 1000 iterations.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ `learning_rate=0.001` 并进行1000次迭代。
- en: 'MSE: 1.04'
  id: totrans-133
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'MSE: 1.04'
- en: ''
  id: totrans-134
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 485 ms'
  id: totrans-135
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 执行时间：485毫秒
- en: '![](../Images/8e1dcf849fba1281a37de8f1abdded16.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e1dcf849fba1281a37de8f1abdded16.png)'
- en: Mini-Batch Gradient Descent with a learning rate of 0.001 and 1000 iterations
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 使用学习率为0.001和1000次迭代的迷你批量梯度下降
- en: It's worth noting that by using the same learning rate, we managed to decrease
    the execution time from 522 ms to 485 ms compared to batch gradient descent. Additionally,
    the mean squared error (MSE) decreased from 1.05 to 1.04.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，通过使用相同的学习率，我们成功地将执行时间从522毫秒减少到485毫秒，相比批量梯度下降。此外，均方误差（MSE）从1.05降到了1.04。
- en: 3️⃣ `learning_rate=0.1` with 1000 iterations.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ `learning_rate=0.1` 并进行1000次迭代。
- en: 'MSE: 0.97'
  id: totrans-140
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'MSE: 0.97'
- en: ''
  id: totrans-141
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 239 ms'
  id: totrans-142
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 执行时间：239毫秒
- en: '![](../Images/e11819e75dd475a17f6c11f2467f4bdf.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e11819e75dd475a17f6c11f2467f4bdf.png)'
- en: Mini-Batch Gradient Descent with a learning rate of 0.1 and 1000 iterations
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 使用学习率为0.1和1000次迭代的迷你批量梯度下降
- en: By implementing mini-batches, the irregularities observed in the historical
    loss values of stochastic gradient descent have been eliminated, and model converged
    really fast.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实现迷你批量，消除了在随机梯度下降历史损失值中观察到的不规则性，并且模型收敛非常快。
- en: Conclusion
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we delved into the gradient descent algorithm and its three
    main implementations - batch, stochastic, and mini-batch. Through a series of
    experiments, we compared these three implementations using different learning
    rates and iterations. We found that batch gradient descent was able to achieve
    the most accurate model parameters, resulting in the lowest loss values, despite
    taking the longest time. On the other hand, stochastic gradient descent reached
    less accuracy but had the highest calculation speed. Lastly, we explored mini-batch
    and found that it achieved moderate accuracy in a reasonable execution time.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们深入探讨了梯度下降算法及其三种主要实现——批量、随机和小批量。通过一系列实验，我们比较了这三种实现的不同学习率和迭代次数。我们发现，批量梯度下降能够获得最准确的模型参数，导致最低的损失值，尽管需要的时间最长。另一方面，随机梯度下降虽然准确度较低，但计算速度最快。最后，我们探索了小批量梯度下降，发现它在合理的执行时间内达到了中等的准确度。
- en: I hope you enjoyed reading about the gradient descent and find the article useful!
    ✨
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你喜欢阅读有关梯度下降的内容，并觉得这篇文章有用！✨
- en: 🍓 If you enjoy reading articles like this and wish to support my writing, you
    may consider [becoming a Medium member](https://idilismiguzel.medium.com/membership)!
    Medium members get full access to articles from all writers and if you use [my
    referral link](https://idilismiguzel.medium.com/membership), you will be directly
    supporting my writing.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 🍓 如果你喜欢阅读这样的文章并希望支持我的写作，你可以考虑 [成为 Medium 会员](https://idilismiguzel.medium.com/membership)!
    Medium 会员可以全面访问所有作者的文章，如果你使用 [我的推荐链接](https://idilismiguzel.medium.com/membership)，你将直接支持我的写作。
- en: 🍓 If you are already a member and interested to read my articles, you can [subscribe
    to be notified](https://medium.com/subscribe/@idilismiguzel) or [follow me on
    Medium](https://idilismiguzel.medium.com). Let me know if you have any questions
    or suggestions.✨
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 🍓 如果你已经是会员并且有兴趣阅读我的文章，你可以 [订阅以获取通知](https://medium.com/subscribe/@idilismiguzel)
    或 [在 Medium 上关注我](https://idilismiguzel.medium.com)。如果你有任何问题或建议，请告诉我。✨
- en: 'Additional resources I recommend reading after this article:'
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我推荐在阅读完这篇文章后进一步阅读的附加资源：
- en: Understand the concept behind hyperparameter tuning with these two most common
    methods.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 了解通过这两种最常见的方法进行超参数调整的概念。
- en: '[](/hyperparameter-tuning-with-grid-search-and-random-search-6e1b5e175144?source=post_page-----246e324c229--------------------------------)
    [## Hyperparameter Tuning with Grid Search and Random Search'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/hyperparameter-tuning-with-grid-search-and-random-search-6e1b5e175144?source=post_page-----246e324c229--------------------------------)
    [## 网格搜索和随机搜索的超参数调整'
- en: And a deep dive into how to combine them
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 以及深入探讨如何将它们结合起来
- en: towardsdatascience.com](/hyperparameter-tuning-with-grid-search-and-random-search-6e1b5e175144?source=post_page-----246e324c229--------------------------------)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/hyperparameter-tuning-with-grid-search-and-random-search-6e1b5e175144?source=post_page-----246e324c229--------------------------------)'
- en: 2\. Understand the linear regression model and its assumptions.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 理解线性回归模型及其假设。
- en: '[](/linear-regression-model-with-python-481c89f0f05b?source=post_page-----246e324c229--------------------------------)
    [## Linear Regression Model with Python'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/linear-regression-model-with-python-481c89f0f05b?source=post_page-----246e324c229--------------------------------)
    [## 使用 Python 的线性回归模型'
- en: How you can build and check the quality of your regression model with graphical
    and numeric outputs
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何通过图形和数值输出构建和检查回归模型的质量
- en: towardsdatascience.com](/linear-regression-model-with-python-481c89f0f05b?source=post_page-----246e324c229--------------------------------)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/linear-regression-model-with-python-481c89f0f05b?source=post_page-----246e324c229--------------------------------)'
- en: References
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献
- en: Header Photo by [Lucas Clara](https://unsplash.com/ko/@lux17?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 头图由 [Lucas Clara](https://unsplash.com/ko/@lux17?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Learning rates plot adapted from [https://cs231n.github.io/neural-networks-3/](https://cs231n.github.io/neural-networks-3/)
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习率图表改编自 [https://cs231n.github.io/neural-networks-3/](https://cs231n.github.io/neural-networks-3/)
- en: All other images are by the Author
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有其他图片由作者提供
