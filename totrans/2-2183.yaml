- en: Understanding Gradient Descent for Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†è§£æœºå™¨å­¦ä¹ ä¸­çš„æ¢¯åº¦ä¸‹é™
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/understanding-gradient-descent-for-machine-learning-246e324c229](https://towardsdatascience.com/understanding-gradient-descent-for-machine-learning-246e324c229)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/understanding-gradient-descent-for-machine-learning-246e324c229](https://towardsdatascience.com/understanding-gradient-descent-for-machine-learning-246e324c229)
- en: A deep dive into Batch, Stochastic, and Mini-Batch Gradient Descent algorithms
    using Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Pythonæ·±å…¥æ¢è®¨æ‰¹é‡ã€éšæœºå’Œå°æ‰¹é‡æ¢¯åº¦ä¸‹é™ç®—æ³•
- en: '[](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)[![Idil
    Ismiguzel](../Images/6846628535770a9f3e13ebb555e82abd.png)](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)[](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)
    [Idil Ismiguzel](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)[![Idil
    Ismiguzel](../Images/6846628535770a9f3e13ebb555e82abd.png)](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)[](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)
    [Idil Ismiguzel](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)
    Â·14 min readÂ·May 21, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)
    Â·é˜…è¯»æ—¶é—´14åˆ†é’ŸÂ·2023å¹´5æœˆ21æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5287b14c08ccf02afd428f2beb4f7204.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5287b14c08ccf02afd428f2beb4f7204.png)'
- en: Photo by [Lucas Clara](https://unsplash.com/ko/@lux17?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±[Lucas Clara](https://unsplash.com/ko/@lux17?utm_source=medium&utm_medium=referral)æä¾›ï¼Œæ¥æºäº[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Gradient descent is a popular optimization algorithm that is used in machine
    learning and deep learning models such as linear regression, logistic regression,
    and neural networks. It uses first-order derivatives iteratively to minimize the
    cost function by updating model coefficients (for regression) and weights (for
    neural networks).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™æ˜¯ä¸€ç§æµè¡Œçš„ä¼˜åŒ–ç®—æ³•ï¼Œå¹¿æ³›åº”ç”¨äºæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¦‚çº¿æ€§å›å½’ã€é€»è¾‘å›å½’å’Œç¥ç»ç½‘ç»œã€‚å®ƒé€šè¿‡è¿­ä»£ä½¿ç”¨ä¸€é˜¶å¯¼æ•°æ¥æœ€å°åŒ–æˆæœ¬å‡½æ•°ï¼Œé€šè¿‡æ›´æ–°æ¨¡å‹ç³»æ•°ï¼ˆç”¨äºå›å½’ï¼‰å’Œæƒé‡ï¼ˆç”¨äºç¥ç»ç½‘ç»œï¼‰ã€‚
- en: In this article, we will delve into the mathematical theory of gradient descent
    and explore how to perform calculations using Python. We will examine various
    implementations including Batch Gradient Descent, Stochastic Gradient Descent,
    and Mini-Batch Gradient Descent, and assess their effectiveness on a range of
    test cases.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨æ¢¯åº¦ä¸‹é™çš„æ•°å­¦ç†è®ºï¼Œå¹¶æ¢ç´¢å¦‚ä½•ä½¿ç”¨Pythonè¿›è¡Œè®¡ç®—ã€‚æˆ‘ä»¬å°†æ£€æŸ¥åŒ…æ‹¬æ‰¹é‡æ¢¯åº¦ä¸‹é™ã€éšæœºæ¢¯åº¦ä¸‹é™å’Œå°æ‰¹é‡æ¢¯åº¦ä¸‹é™åœ¨å†…çš„å„ç§å®ç°ï¼Œå¹¶è¯„ä¼°å®ƒä»¬åœ¨ä¸åŒæµ‹è¯•æ¡ˆä¾‹ä¸­çš„æ•ˆæœã€‚
- en: While following the article, you can check out the [Jupyter Notebook](https://github.com/Idilismiguzel/Machine-Learning/blob/master/Gradient_Descent/gradient_descent_implementation.ipynb)
    on my GitHub for complete analysis and code.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é˜…è¯»æœ¬æ–‡çš„åŒæ—¶ï¼Œä½ å¯ä»¥æŸ¥çœ‹æˆ‘GitHubä¸Šçš„[Jupyter Notebook](https://github.com/Idilismiguzel/Machine-Learning/blob/master/Gradient_Descent/gradient_descent_implementation.ipynb)ä»¥è·å–å®Œæ•´çš„åˆ†æå’Œä»£ç ã€‚
- en: Before a deep dive into gradient descent, letâ€™s first go through the loss function.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±å…¥æ¢è®¨æ¢¯åº¦ä¸‹é™ä¹‹å‰ï¼Œè®©æˆ‘ä»¬é¦–å…ˆäº†è§£æŸå¤±å‡½æ•°ã€‚
- en: What is Loss Function?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æŸå¤±å‡½æ•°ï¼Ÿ
- en: '**Loss** or **cost** are used interchangeably to describe the error in a prediction.
    A loss value indicates how different a prediction is from the actual value and
    the loss function aggregates all the loss values from multiple data points into
    a single number.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**æŸå¤±**æˆ–**æˆæœ¬**è¿™ä¸¤ä¸ªæœ¯è¯­å¯ä»¥äº’æ¢ä½¿ç”¨ï¼Œç”¨æ¥æè¿°é¢„æµ‹ä¸­çš„è¯¯å·®ã€‚æŸå¤±å€¼è¡¨ç¤ºé¢„æµ‹å€¼ä¸å®é™…å€¼çš„å·®å¼‚ï¼ŒæŸå¤±å‡½æ•°å°†æ¥è‡ªå¤šä¸ªæ•°æ®ç‚¹çš„æ‰€æœ‰æŸå¤±å€¼æ±‡æ€»ä¸ºä¸€ä¸ªå•ä¸€çš„æ•°å­—ã€‚'
- en: You can see in the image below, the model on the left has high loss whereas
    the model on the right has low loss and fits the data better.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸‹å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œå·¦ä¾§çš„æ¨¡å‹å…·æœ‰é«˜æŸå¤±ï¼Œè€Œå³ä¾§çš„æ¨¡å‹å…·æœ‰ä½æŸå¤±ï¼Œå¹¶ä¸”æ›´å¥½åœ°æ‹Ÿåˆäº†æ•°æ®ã€‚
- en: '![](../Images/eb7e40a2f95aa71254db5f4592880b6d.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb7e40a2f95aa71254db5f4592880b6d.png)'
- en: High loss vs low loss (blue lines) from the corresponding regression line in
    yellow.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜æŸå¤±ä¸ä½æŸå¤±ï¼ˆè“çº¿ï¼‰ç›¸å¯¹äºé»„è‰²å›å½’çº¿çš„å¯¹æ¯”ã€‚
- en: The loss function (J) is used as a performance measurement for prediction algorithms
    and the main goal of a predictive model is to minimize its loss function, which
    is determined by the values of the model parameters (i.e., Î¸0 and Î¸1).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æŸå¤±å‡½æ•°ï¼ˆJï¼‰ç”¨ä½œé¢„æµ‹ç®—æ³•çš„æ€§èƒ½æµ‹é‡å·¥å…·ï¼Œé¢„æµ‹æ¨¡å‹çš„ä¸»è¦ç›®æ ‡æ˜¯æœ€å°åŒ–å…¶æŸå¤±å‡½æ•°ï¼Œè¿™ç”±æ¨¡å‹å‚æ•°çš„å€¼ï¼ˆå³ Î¸0 å’Œ Î¸1ï¼‰å†³å®šã€‚
- en: For example, linear regression models frequently use squared loss to compute
    the loss value and mean squared error is the loss function that averages all squared
    losses.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œçº¿æ€§å›å½’æ¨¡å‹é€šå¸¸ä½¿ç”¨å¹³æ–¹æŸå¤±æ¥è®¡ç®—æŸå¤±å€¼ï¼Œè€Œå‡æ–¹è¯¯å·®æ˜¯å¹³å‡æ‰€æœ‰å¹³æ–¹æŸå¤±çš„æŸå¤±å‡½æ•°ã€‚
- en: '![](../Images/d702a1a157eed6b1cb56125ef219ea27.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d702a1a157eed6b1cb56125ef219ea27.png)'
- en: Squared Loss value (L2 Loss) and Mean Squared Error (MSE)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å¹³æ–¹æŸå¤±å€¼ï¼ˆL2 æŸå¤±ï¼‰å’Œå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰
- en: The linear regression model works behind the scenes by going through several
    iterations to optimize its coefficients and reach the lowest possible mean squared
    error.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: çº¿æ€§å›å½’æ¨¡å‹åœ¨åå°é€šè¿‡å¤šæ¬¡è¿­ä»£æ¥ä¼˜åŒ–å…¶ç³»æ•°ï¼Œä»¥è¾¾åˆ°å°½å¯èƒ½ä½çš„å‡æ–¹è¯¯å·®ã€‚
- en: What is Gradient Descent?
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æ¢¯åº¦ä¸‹é™ï¼Ÿ
- en: 'The gradient descent algorithm is usually described with a mountain analogy:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™ç®—æ³•é€šå¸¸ç”¨å±±çš„ç±»æ¯”æ¥æè¿°ï¼š
- en: â›° Imagine yourself standing atop a mountain, with limited visibility, and you
    want to reach the ground. While descending, you'll encounter slopes and pass them
    using larger or smaller steps. Once you've reached a slope that is almost leveled,
    you'll know that you've arrived at the lowest point. â›°
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â›° æƒ³è±¡ä½ ç«™åœ¨å±±é¡¶ï¼Œè§†é‡æœ‰é™ï¼Œä½ æƒ³è¦åˆ°è¾¾åœ°é¢ã€‚åœ¨ä¸‹å¡æ—¶ï¼Œä½ ä¼šé‡åˆ°æ–œå¡ï¼Œå¹¶é€šè¿‡è¾ƒå¤§æˆ–è¾ƒå°çš„æ­¥ä¼é€šè¿‡å®ƒä»¬ã€‚ä¸€æ—¦ä½ åˆ°è¾¾å‡ ä¹å¹³å¦çš„æ–œå¡ï¼Œä½ å°±ä¼šçŸ¥é“ä½ å·²ç»åˆ°è¾¾æœ€ä½ç‚¹ã€‚
    â›°
- en: In technical terms, **gradient** refers to these slopes. When the slope is zero,
    it may indicate that youâ€™ve reached a function's minimum or maximum value.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æŠ€æœ¯ä¸Šè®²ï¼Œ**æ¢¯åº¦**æŒ‡çš„å°±æ˜¯è¿™äº›æ–œç‡ã€‚å½“æ–œç‡ä¸ºé›¶æ—¶ï¼Œå¯èƒ½è¡¨ç¤ºä½ å·²ç»åˆ°è¾¾äº†å‡½æ•°çš„æœ€å°å€¼æˆ–æœ€å¤§å€¼ã€‚
- en: '![](../Images/2d3f12fa02efaed21b87ca718f61b10a.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d3f12fa02efaed21b87ca718f61b10a.png)'
- en: Like in the mountain analogy, GD minimizes the starting loss value by taking
    repeated steps in the opposite direction of the gradient to reduce the loss function.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒåœ¨å±±çš„ç±»æ¯”ä¸­ä¸€æ ·ï¼ŒGD é€šè¿‡åœ¨æ¢¯åº¦çš„ç›¸åæ–¹å‘ä¸Šé‡å¤è¿ˆæ­¥æ¥æœ€å°åŒ–èµ·å§‹æŸå¤±å€¼ï¼Œä»è€Œå‡å°‘æŸå¤±å‡½æ•°ã€‚
- en: At any given point on a curve, the steepness of the slope can be determined
    by a **tangent line** â€” a straight line that touches the point (red lines in the
    image above). Similar to the tangent line, the gradient of a point on the loss
    function is calculated with respect to the parameters, and a small step is taken
    in the opposite direction to reduce the loss.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ›²çº¿ä¸Šçš„ä»»ä½•ä¸€ç‚¹ï¼Œæ–œç‡çš„é™¡å³­ç¨‹åº¦å¯ä»¥é€šè¿‡**åˆ‡çº¿**æ¥ç¡®å®šâ€”â€”ä¸€æ¡ä¸è¯¥ç‚¹ç›¸åˆ‡çš„ç›´çº¿ï¼ˆå¦‚ä¸Šå›¾ä¸­çš„çº¢çº¿ï¼‰ã€‚ç±»ä¼¼äºåˆ‡çº¿ï¼ŒæŸå¤±å‡½æ•°ä¸ŠæŸä¸€ç‚¹çš„æ¢¯åº¦æ˜¯ç›¸å¯¹äºå‚æ•°è®¡ç®—çš„ï¼Œå¹¶ä¸”ä¼šæœç›¸åæ–¹å‘è¿ˆå‡ºå°æ­¥ä»¥å‡å°‘æŸå¤±ã€‚
- en: 'To summarize, the process of gradient descent can be broken down into the following
    steps:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“ä¸€ä¸‹ï¼Œæ¢¯åº¦ä¸‹é™çš„è¿‡ç¨‹å¯ä»¥åˆ†è§£ä¸ºä»¥ä¸‹æ­¥éª¤ï¼š
- en: Select a starting point for the model parameters.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹©æ¨¡å‹å‚æ•°çš„èµ·å§‹ç‚¹ã€‚
- en: Determine the gradient of the cost function with respect to the parameters and
    continually adjust the parameter values through iterative steps to minimize the
    cost function.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç¡®å®šæˆæœ¬å‡½æ•°ç›¸å¯¹äºå‚æ•°çš„æ¢¯åº¦ï¼Œå¹¶é€šè¿‡è¿­ä»£æ­¥éª¤ä¸æ–­è°ƒæ•´å‚æ•°å€¼ä»¥æœ€å°åŒ–æˆæœ¬å‡½æ•°ã€‚
- en: Repeat step 2 until the cost function no longer decreases or the maximum number
    of iterations is reached.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é‡å¤æ­¥éª¤ 2ï¼Œç›´åˆ°æˆæœ¬å‡½æ•°ä¸å†å‡å°‘æˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚
- en: We can examine the gradient calculation for the previously defined cost (loss)
    function. Although we are utilizing linear regression with an intercept and coefficient,
    this reasoning can be extended to regression models incorporating several variables.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥æ£€æŸ¥ä¹‹å‰å®šä¹‰çš„æˆæœ¬ï¼ˆæŸå¤±ï¼‰å‡½æ•°çš„æ¢¯åº¦è®¡ç®—ã€‚è™½ç„¶æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨å…·æœ‰æˆªè·å’Œç³»æ•°çš„çº¿æ€§å›å½’ï¼Œä½†è¿™ç§æ¨ç†å¯ä»¥æ‰©å±•åˆ°åŒ…å«å¤šä¸ªå˜é‡çš„å›å½’æ¨¡å‹ã€‚
- en: '![](../Images/b7efa05a2b319a6b2afaf3b0584f7bc4.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7efa05a2b319a6b2afaf3b0584f7bc4.png)'
- en: Linear regression function with 2 parameters, cost function, and objective function
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å…·æœ‰ 2 ä¸ªå‚æ•°çš„çº¿æ€§å›å½’å‡½æ•°ã€æˆæœ¬å‡½æ•°å’Œç›®æ ‡å‡½æ•°
- en: '![](../Images/abe1795d9a85eca87cc8383bef5dded3.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/abe1795d9a85eca87cc8383bef5dded3.png)'
- en: Partial derivatives calculated wrt model parameters
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸å¯¹äºæ¨¡å‹å‚æ•°è®¡ç®—çš„åå¯¼æ•°
- en: ğŸ’¡ Sometimes, the point that has been reached may only be a *local minimum* or
    a *plateau*. In such cases, the model needs to continue iterating until it reaches
    the global minimum. Reaching the global minimum is unfortunately not guaranteed
    but with a proper number of iterations and a learning rate we can increase the
    chances.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8df9feaf7bf49a9654c5b8e0f1944cac.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: When using gradient descent, it is important to be aware of the potential challenge
    of stopping at a local minimum or on a plateau. To avoid this, it is essential
    to choose the appropriate number of iterations and learning rate. We will discuss
    this further in the following sections.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '`Learning_rate` is the hyperparameter of gradient descent to define the size
    of the learning step. It can be tuned using [hyperparameter tuning techniques](https://medium.com/towards-data-science/hyperparameter-tuning-with-grid-search-and-random-search-6e1b5e175144).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: If the `learning_rate` is set too high it could result in a jump that produces
    a loss value greater than the starting point. A high `learning_rate` might cause
    gradient descent to **diverge**,leading it to continually obtain higher loss values
    and preventing it from finding the minimum.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/d1b33f2d63d0773bf5f91edff2582bf2.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: 'Example case: A high learning rate causes GD to diverge'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: If the `learning_rate` is set too low it can lead to a lengthy computation process
    where gradient descent iterates through numerous rounds of gradient calculations
    to reach **convergence** and discover the minimum loss value.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/50be59657a670b4f90cb4f14f649715d.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: 'Example case: A low learning rate causes GD to take too much time to converge'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: The value of the learning step is determined by the slope of the curve, which
    means that as we approach the minimum point, the learning steps become smaller.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: When using low learning rates, the progress made will be steady, whereas high
    learning rates may result in either exponential progress or being stuck at low
    points.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af1f3ef2e20a9932f0f6dfb4e0b4c078.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: Image adapted from [https://cs231n.github.io/neural-networks-3/](https://cs231n.github.io/neural-networks-3/)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: We will now cover three different implementations of the gradient descent algorithm.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Batch Gradient Descent
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The batch gradient descent is the most widely used method for implementing gradient
    descent. It involves computing gradients with respect to the model parameters
    (such as regression coefficients) at every iteration for the entire data set.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s look at an example ğŸ”
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: First, generate a dataset with an intercept of 5 and a coefficient of 4, along
    with a small amount of Gaussian noise. See the scatter plot of the generated data
    below.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/7644f9241b46b9de225680a0cfd89fa6.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: Scatter plot of data generated
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: The following function performs batch gradient descent by utilizing a specified
    learning rate and number of iterations. Initially, the model's coefficient (m)
    and intercept (b) are set to 0.5\. During each iteration, the error is calculated
    by taking the difference between predicted and actual values of y. The algorithm
    then updates m and b by extracting the gradient, which is also multiplied by the
    learning rate. The loop continues until the specified number of iterations is
    reached, and the resulting loss value and model parameters are stored in `params`
    and `loss`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å‡½æ•°é€šè¿‡åˆ©ç”¨æŒ‡å®šçš„å­¦ä¹ ç‡å’Œè¿­ä»£æ¬¡æ•°æ¥æ‰§è¡Œæ‰¹é‡æ¢¯åº¦ä¸‹é™ã€‚æœ€åˆï¼Œæ¨¡å‹çš„ç³»æ•°ï¼ˆmï¼‰å’Œæˆªè·ï¼ˆbï¼‰éƒ½è®¾ç½®ä¸º0.5ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œé€šè¿‡è®¡ç®—é¢„æµ‹å€¼å’Œå®é™…å€¼ä¹‹é—´çš„å·®å¼‚æ¥ç¡®å®šè¯¯å·®ã€‚ç„¶åï¼Œç®—æ³•é€šè¿‡æå–æ¢¯åº¦æ¥æ›´æ–°må’Œbï¼Œæ¢¯åº¦ä¹Ÿä¼šä¹˜ä»¥å­¦ä¹ ç‡ã€‚å¾ªç¯æŒç»­è¿›è¡Œï¼Œç›´åˆ°è¾¾åˆ°æŒ‡å®šçš„è¿­ä»£æ¬¡æ•°ï¼Œç»“æœçš„æŸå¤±å€¼å’Œæ¨¡å‹å‚æ•°è¢«å­˜å‚¨åœ¨`params`å’Œ`loss`ä¸­ã€‚
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using the function above, we will now test different learning rates and evaluate
    the performance.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸Šè¿°å‡½æ•°ï¼Œæˆ‘ä»¬ç°åœ¨å°†æµ‹è¯•ä¸åŒçš„å­¦ä¹ ç‡å¹¶è¯„ä¼°æ€§èƒ½ã€‚
- en: 1ï¸âƒ£ Letâ€™s set `learning_rate=0.01` with 1000 iterations and plot the loss function.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ è®©æˆ‘ä»¬è®¾ç½® `learning_rate=0.01`ï¼Œè¿›è¡Œ1000æ¬¡è¿­ä»£ï¼Œå¹¶ç»˜åˆ¶æŸå¤±å‡½æ•°å›¾ã€‚
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/9b4bc468d425495f4c02fd781a6140d2.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b4bc468d425495f4c02fd781a6140d2.png)'
- en: Batch Gradient Descent with a learning rate of 0.01 and 1000 iterations
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å­¦ä¹ ç‡ä¸º0.01å’Œ1000æ¬¡è¿­ä»£çš„æ‰¹é‡æ¢¯åº¦ä¸‹é™
- en: 'm: 4.27, b: 4.88, MSE: 0.95'
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'm: 4.27, b: 4.88, MSE: 0.95'
- en: ''
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 367 ms'
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ‰§è¡Œæ—¶é—´ï¼š367æ¯«ç§’
- en: In the plots above, first, you can see the regression lines generated after
    each iteration (in purple) and notice how it gradually approaches the optimal
    after around 100 iterations. In the second plot, the loss function is displayed
    after every iteration, showing a significant decrease in the loss within the first
    50-100 iterations.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šè¿°å›¾ä¸­ï¼Œé¦–å…ˆå¯ä»¥çœ‹åˆ°æ¯æ¬¡è¿­ä»£ç”Ÿæˆçš„å›å½’çº¿ï¼ˆç´«è‰²ï¼‰ï¼Œå¹¶æ³¨æ„å®ƒå¦‚ä½•åœ¨å¤§çº¦100æ¬¡è¿­ä»£åé€æ¸æ¥è¿‘æœ€ä¼˜ã€‚åœ¨ç¬¬äºŒä¸ªå›¾ä¸­ï¼ŒæŸå¤±å‡½æ•°åœ¨æ¯æ¬¡è¿­ä»£åæ˜¾ç¤ºï¼Œåœ¨å‰50-100æ¬¡è¿­ä»£å†…æŸå¤±æ˜¾è‘—å‡å°‘ã€‚
- en: 2ï¸âƒ£ Letâ€™s now set `learning_rate=0.001` with the same number of iterations and
    plot the loss function.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ ç°åœ¨è®©æˆ‘ä»¬è®¾ç½® `learning_rate=0.001`ï¼Œä½¿ç”¨ç›¸åŒçš„è¿­ä»£æ¬¡æ•°ï¼Œå¹¶ç»˜åˆ¶æŸå¤±å‡½æ•°å›¾ã€‚
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'm: 4.67, b: 4.31, MSE: 1.05'
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'm: 4.67, b: 4.31, MSE: 1.05'
- en: ''
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 522 ms'
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ‰§è¡Œæ—¶é—´ï¼š522æ¯«ç§’
- en: '![](../Images/e964bffeda41b26589b8f1124398fab2.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e964bffeda41b26589b8f1124398fab2.png)'
- en: Batch Gradient Descent with a learning rate of 0.001 and 1000 iterations
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å­¦ä¹ ç‡ä¸º0.001å’Œ1000æ¬¡è¿­ä»£çš„æ‰¹é‡æ¢¯åº¦ä¸‹é™
- en: When the learning rate was reduced, the model needed more time to reach convergence
    and ended up with a higher final loss value. The first plot shows a darker shade
    of purple under the linear regression line, which means that there were various
    iterations that were far from the optimal line. Also, note that decreasing the
    learning rate from 0.01 to 0.001 resulted in an increase in execution time from
    367 ms to 522 ms.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å­¦ä¹ ç‡é™ä½æ—¶ï¼Œæ¨¡å‹éœ€è¦æ›´å¤šæ—¶é—´æ‰èƒ½æ”¶æ•›ï¼Œå¹¶ä¸”æœ€ç»ˆæŸå¤±å€¼è¾ƒé«˜ã€‚ç¬¬ä¸€ä¸ªå›¾æ˜¾ç¤ºäº†å›å½’çº¿ä¸‹æ–¹æ›´æ·±çš„ç´«è‰²é˜´å½±ï¼Œè¿™æ„å‘³ç€åœ¨è®¸å¤šè¿­ä»£ä¸­è¿œç¦»äº†æœ€ä¼˜çº¿ã€‚æ­¤å¤–ï¼Œè¯·æ³¨æ„å°†å­¦ä¹ ç‡ä»0.01å‡å°‘åˆ°0.001å¯¼è‡´æ‰§è¡Œæ—¶é—´ä»367æ¯«ç§’å¢åŠ åˆ°522æ¯«ç§’ã€‚
- en: 3ï¸âƒ£ Letâ€™s test `learning_rate=0.1` using the same number of iterations and plot
    the loss function.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ è®©æˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„è¿­ä»£æ¬¡æ•°è®¾ç½® `learning_rate=0.1` å¹¶ç»˜åˆ¶æŸå¤±å‡½æ•°å›¾ã€‚
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'm: 4.23, b: 4.93, MSE: 0.95'
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'm: 4.23, b: 4.93, MSE: 0.95'
- en: ''
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 214 ms'
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ‰§è¡Œæ—¶é—´ï¼š214æ¯«ç§’
- en: '![](../Images/f93e84dee0e7b2db02ecae1807754fb5.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f93e84dee0e7b2db02ecae1807754fb5.png)'
- en: Batch Gradient Descent with a learning rate of 0.1 and 1000 iterations
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å­¦ä¹ ç‡ä¸º0.1å’Œ1000æ¬¡è¿­ä»£çš„æ‰¹é‡æ¢¯åº¦ä¸‹é™
- en: The model displayed rapid convergence when a high learning rate was used. In
    the first plot, there were only a few iterations where the regression line was
    distant from the optimal linear regression line. In the second plot, the loss
    sharply decreased during the first few iterations and remained steady until all
    iterations were completed. Furthermore, the execution time was the lowest at 214ms,
    out of the three trials.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½¿ç”¨é«˜å­¦ä¹ ç‡æ—¶ï¼Œæ¨¡å‹è¡¨ç°å‡ºå¿«é€Ÿæ”¶æ•›ã€‚åœ¨ç¬¬ä¸€ä¸ªå›¾ä¸­ï¼Œåªæœ‰å°‘æ•°å‡ æ¬¡è¿­ä»£ä¸­å›å½’çº¿è¿œç¦»æœ€ä¼˜çº¿ã€‚åœ¨ç¬¬äºŒä¸ªå›¾ä¸­ï¼ŒæŸå¤±åœ¨å‰å‡ æ¬¡è¿­ä»£ä¸­æ€¥å‰§ä¸‹é™ï¼Œå¹¶ä¿æŒç¨³å®šç›´åˆ°æ‰€æœ‰è¿­ä»£å®Œæˆã€‚æ­¤å¤–ï¼Œæ‰§è¡Œæ—¶é—´åœ¨ä¸‰æ¬¡è¯•éªŒä¸­æœ€ä½ï¼Œä¸º214æ¯«ç§’ã€‚
- en: ğŸš¨Calculating the gradient for the entire training data set for every parameter
    update can be computationally expensive on large data sets. Fortunately, stochastic
    gradient descent or mini-batch stochastic gradient descent can help solve this
    issue.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš¨ å¯¹äºæ¯æ¬¡å‚æ•°æ›´æ–°è®¡ç®—æ•´ä¸ªè®­ç»ƒæ•°æ®é›†çš„æ¢¯åº¦åœ¨å¤§å‹æ•°æ®é›†ä¸Šå¯èƒ½è®¡ç®—å¼€é”€è¾ƒå¤§ã€‚å¹¸è¿çš„æ˜¯ï¼Œéšæœºæ¢¯åº¦ä¸‹é™æˆ–å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™å¯ä»¥å¸®åŠ©è§£å†³è¿™ä¸ªé—®é¢˜ã€‚
- en: 2\. Stochastic Gradient Descent
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. éšæœºæ¢¯åº¦ä¸‹é™
- en: Another way to implement gradient descent is using stochastic gradient descent.
    It is especially preferred for large training data sets where the batch gradient
    descent might take too much time to compute.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic gradient descent updates model coefficients by calculating the loss
    value for only one **random** data point instead of calculating for every data
    point in the training data set and aggregating.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Random selection of the data point is essential to prevent getting stuck around
    similar values (such as clusters in the data). The learning rate and the number
    of iterations are also critical factors, similar to batch gradient descent.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: The following function performs stochastic gradient descent by utilizing a specified
    learning rate and number of iterations. Initially, the modelâ€™s coefficient (m)
    and intercept (b) are set to 0.5\. In each iteration, the error is computed for
    a randomly selected data point using `np.random`. The remaining steps of the function
    are identical to those in batch gradient descent.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Let's try using the same learning rate and number of iterations that we used
    for batch gradient descent.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 1ï¸âƒ£ Letâ€™s set `learning_rate=0.01` with 1000 iterations and plot the loss function.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'MSE: 0.97'
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 325 ms'
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/dabbc95a78881d732f085ce2cf4cf5d0.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: Stochastic Gradient Descent with a learning rate of 0.01 and 1000 iterations
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Notice that with the same learning rate, we were able to reduce the execution
    time from 367 ms to 325 ms when compared to batch gradient descent. However, the
    mean squared error (MSE) increased from 0.95 to 0.97.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 2ï¸âƒ£ Letâ€™s now set `learning_rate=0.001` with 1000 iterations and plot the loss
    function. I won't be including the code snippet since it's the same as in previous
    examples. However, feel free to refer to the [source code of the article](https://github.com/Idilismiguzel/Machine-Learning/blob/master/Gradient_Descent/gradient_descent_implementation.ipynb)
    if you need it.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'MSE: 1.03'
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 253 ms'
  id: totrans-106
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/374b35ec0c7c67e430e5558e260e27f4.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: Stochastic Gradient Descent with a learning rate of 0.001 and 1000 iterations
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Notice that using batch gradient descent with a learning rate of 0.001 resulted
    in a loss value of 1.05 in 522 milliseconds. However, using stochastic gradient
    descent with the same learning rate led to a loss value of 1.03 in only 253 milliseconds.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 3ï¸âƒ£ Letâ€™s now set `learning_rate=0.1` with 1000 iterations and plot the loss
    function.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'MSE: 1.27'
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-112
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 237 ms'
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/3297564ca9ad3cf2a892944218541bde.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: Stochastic Gradient Descent with a learning rate of 0.1 and 1000 iterations
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: When using a high learning rate with stochastic gradient descent, the computation
    introduced fluctuations in the loss values. As shown in the first plot, there
    were numerous iterations above and below the optimal regression line.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Overall, batch gradient descent outperforms stochastic gradient descent when
    it comes to converging to the minimum as stochastic gradient descent tends to
    wander around a vicinity near the global minimum. However, if the learning rate
    is carefully selected, stochastic gradient descent can also achieve similar loss
    values (sometimes even better) in a shorter amount of time.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“è€Œè¨€ï¼Œæ‰¹é‡æ¢¯åº¦ä¸‹é™åœ¨æ”¶æ•›åˆ°æœ€å°å€¼æ–¹é¢ä¼˜äºéšæœºæ¢¯åº¦ä¸‹é™ï¼Œå› ä¸ºéšæœºæ¢¯åº¦ä¸‹é™å¾€å¾€ä¼šåœ¨å…¨å±€æœ€å°å€¼é™„è¿‘å¾˜å¾Šã€‚ç„¶è€Œï¼Œå¦‚æœä»”ç»†é€‰æ‹©å­¦ä¹ ç‡ï¼Œéšæœºæ¢¯åº¦ä¸‹é™ä¹Ÿå¯ä»¥åœ¨è¾ƒçŸ­çš„æ—¶é—´å†…è¾¾åˆ°ç±»ä¼¼çš„æŸå¤±å€¼ï¼ˆæœ‰æ—¶ç”šè‡³æ›´å¥½ï¼‰ã€‚
- en: 3\. Mini-Batch Gradient Descent
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. è¿·ä½ æ‰¹é‡æ¢¯åº¦ä¸‹é™
- en: Mini-batch gradient descent is a useful intermediate option between batch and
    stochastic gradient descent. Rather than computing the gradient for the entire
    dataset or just one observation, it divides the training data into smaller batches
    and calculates the gradient for each one.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: è¿·ä½ æ‰¹é‡æ¢¯åº¦ä¸‹é™æ˜¯æ‰¹é‡å’Œéšæœºæ¢¯åº¦ä¸‹é™ä¹‹é—´çš„ä¸€ä¸ªæœ‰ç”¨çš„ä¸­é—´é€‰é¡¹ã€‚å®ƒä¸æ˜¯ä¸ºæ•´ä¸ªæ•°æ®é›†æˆ–å•ä¸ªè§‚å¯Ÿå€¼è®¡ç®—æ¢¯åº¦ï¼Œè€Œæ˜¯å°†è®­ç»ƒæ•°æ®åˆ†æˆè¾ƒå°çš„æ‰¹æ¬¡ï¼Œå¹¶ä¸ºæ¯ä¸ªæ‰¹æ¬¡è®¡ç®—æ¢¯åº¦ã€‚
- en: By combining batch and stochastic methods, mini-batch GD enhances computation
    speed compared to the batch model and improves the accuracy of the stochastic
    model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ç»“åˆæ‰¹é‡å’Œéšæœºæ–¹æ³•ï¼Œè¿·ä½ æ‰¹é‡GDåœ¨è®¡ç®—é€Ÿåº¦ä¸Šä¼˜äºæ‰¹é‡æ¨¡å‹ï¼Œå¹¶ä¸”æé«˜äº†éšæœºæ¨¡å‹çš„å‡†ç¡®æ€§ã€‚
- en: The function below executes mini-batch gradient descent using a given learning
    rate, a set number of iterations, and a chosen batch size. By specifying a batch
    size, we can determine the number of data points to include in each gradient calculation.
    For instance, in stochastic gradient descent, the batch size is just one and the
    gradient is computed for a single data point. However, if we set a batch size
    of 10, then the gradient will be computed for 10 data points and then combined.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å‡½æ•°æ‰§è¡Œè¿·ä½ æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼Œä½¿ç”¨ç»™å®šçš„å­¦ä¹ ç‡ã€è®¾å®šçš„è¿­ä»£æ¬¡æ•°å’Œé€‰å®šçš„æ‰¹é‡å¤§å°ã€‚é€šè¿‡æŒ‡å®šæ‰¹é‡å¤§å°ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®å®šæ¯æ¬¡æ¢¯åº¦è®¡ç®—ä¸­åŒ…å«çš„æ•°æ®ç‚¹æ•°é‡ã€‚ä¾‹å¦‚ï¼Œåœ¨éšæœºæ¢¯åº¦ä¸‹é™ä¸­ï¼Œæ‰¹é‡å¤§å°ä¸º1ï¼Œæ¢¯åº¦æ˜¯é’ˆå¯¹å•ä¸ªæ•°æ®ç‚¹è®¡ç®—çš„ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬è®¾ç½®æ‰¹é‡å¤§å°ä¸º10ï¼Œé‚£ä¹ˆæ¢¯åº¦å°†ä¼šé’ˆå¯¹10ä¸ªæ•°æ®ç‚¹è¿›è¡Œè®¡ç®—ï¼Œç„¶ååˆå¹¶ã€‚
- en: '[PRE7]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Letâ€™s test the same 3 cases with `learning_rate=0.01` `learning_rate=0.001`
    and `learning_rate=0.1.` using 10 as batch size.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‰¹é‡å¤§å°ä¸º10çš„æƒ…å†µä¸‹ï¼Œæµ‹è¯•ç›¸åŒçš„3ç§æƒ…å†µï¼Œ`learning_rate=0.01`ã€`learning_rate=0.001`å’Œ`learning_rate=0.1`ã€‚
- en: 1ï¸âƒ£ `learning_rate=0.01` with 1000 iterations.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ `learning_rate=0.01` å¹¶è¿›è¡Œ1000æ¬¡è¿­ä»£ã€‚
- en: '[PRE8]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'MSE: 0.96'
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'MSE: 0.96'
- en: ''
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 224 ms'
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ‰§è¡Œæ—¶é—´ï¼š224æ¯«ç§’
- en: '![](../Images/1d41acb0d5739dd227f564a8caed7ced.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d41acb0d5739dd227f564a8caed7ced.png)'
- en: Mini-Batch Gradient Descent with a learning rate of 0.01 and 1000 iterations
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å­¦ä¹ ç‡ä¸º0.01å’Œ1000æ¬¡è¿­ä»£çš„è¿·ä½ æ‰¹é‡æ¢¯åº¦ä¸‹é™
- en: Have you noticed that by using a 0.01 learning rate, we were able to achieve
    an MSE of 0.95 in 367 ms with batch GD, and an MSE of 0.97 in 325 ms with stochastic
    GD? Furthermore, with the implementation of mini-batch gradient descent, we were
    able to attain an MSE of 0.96 in just 224 ms.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æ˜¯å¦æ³¨æ„åˆ°ï¼Œä½¿ç”¨0.01çš„å­¦ä¹ ç‡æ—¶ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨367æ¯«ç§’å†…é€šè¿‡æ‰¹é‡GDè¾¾åˆ°MSEä¸º0.95ï¼Œè€Œä½¿ç”¨éšæœºGDåˆ™åœ¨325æ¯«ç§’å†…å¾—åˆ°MSEä¸º0.97ï¼Ÿæ­¤å¤–ï¼Œé€šè¿‡è¿·ä½ æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨ä»…224æ¯«ç§’å†…è¾¾åˆ°MSEä¸º0.96ã€‚
- en: 2ï¸âƒ£ `learning_rate=0.001` with 1000 iterations.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ `learning_rate=0.001` å¹¶è¿›è¡Œ1000æ¬¡è¿­ä»£ã€‚
- en: 'MSE: 1.04'
  id: totrans-133
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'MSE: 1.04'
- en: ''
  id: totrans-134
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 485 ms'
  id: totrans-135
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ‰§è¡Œæ—¶é—´ï¼š485æ¯«ç§’
- en: '![](../Images/8e1dcf849fba1281a37de8f1abdded16.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e1dcf849fba1281a37de8f1abdded16.png)'
- en: Mini-Batch Gradient Descent with a learning rate of 0.001 and 1000 iterations
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å­¦ä¹ ç‡ä¸º0.001å’Œ1000æ¬¡è¿­ä»£çš„è¿·ä½ æ‰¹é‡æ¢¯åº¦ä¸‹é™
- en: It's worth noting that by using the same learning rate, we managed to decrease
    the execution time from 522 ms to 485 ms compared to batch gradient descent. Additionally,
    the mean squared error (MSE) decreased from 1.05 to 1.04.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé€šè¿‡ä½¿ç”¨ç›¸åŒçš„å­¦ä¹ ç‡ï¼Œæˆ‘ä»¬æˆåŠŸåœ°å°†æ‰§è¡Œæ—¶é—´ä»522æ¯«ç§’å‡å°‘åˆ°485æ¯«ç§’ï¼Œç›¸æ¯”æ‰¹é‡æ¢¯åº¦ä¸‹é™ã€‚æ­¤å¤–ï¼Œå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ä»1.05é™åˆ°äº†1.04ã€‚
- en: 3ï¸âƒ£ `learning_rate=0.1` with 1000 iterations.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ `learning_rate=0.1` å¹¶è¿›è¡Œ1000æ¬¡è¿­ä»£ã€‚
- en: 'MSE: 0.97'
  id: totrans-140
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'MSE: 0.97'
- en: ''
  id: totrans-141
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 239 ms'
  id: totrans-142
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ‰§è¡Œæ—¶é—´ï¼š239æ¯«ç§’
- en: '![](../Images/e11819e75dd475a17f6c11f2467f4bdf.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e11819e75dd475a17f6c11f2467f4bdf.png)'
- en: Mini-Batch Gradient Descent with a learning rate of 0.1 and 1000 iterations
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å­¦ä¹ ç‡ä¸º0.1å’Œ1000æ¬¡è¿­ä»£çš„è¿·ä½ æ‰¹é‡æ¢¯åº¦ä¸‹é™
- en: By implementing mini-batches, the irregularities observed in the historical
    loss values of stochastic gradient descent have been eliminated, and model converged
    really fast.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å®ç°è¿·ä½ æ‰¹é‡ï¼Œæ¶ˆé™¤äº†åœ¨éšæœºæ¢¯åº¦ä¸‹é™å†å²æŸå¤±å€¼ä¸­è§‚å¯Ÿåˆ°çš„ä¸è§„åˆ™æ€§ï¼Œå¹¶ä¸”æ¨¡å‹æ”¶æ•›éå¸¸å¿«ã€‚
- en: Conclusion
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this article, we delved into the gradient descent algorithm and its three
    main implementations - batch, stochastic, and mini-batch. Through a series of
    experiments, we compared these three implementations using different learning
    rates and iterations. We found that batch gradient descent was able to achieve
    the most accurate model parameters, resulting in the lowest loss values, despite
    taking the longest time. On the other hand, stochastic gradient descent reached
    less accuracy but had the highest calculation speed. Lastly, we explored mini-batch
    and found that it achieved moderate accuracy in a reasonable execution time.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†æ¢¯åº¦ä¸‹é™ç®—æ³•åŠå…¶ä¸‰ç§ä¸»è¦å®ç°â€”â€”æ‰¹é‡ã€éšæœºå’Œå°æ‰¹é‡ã€‚é€šè¿‡ä¸€ç³»åˆ—å®éªŒï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†è¿™ä¸‰ç§å®ç°çš„ä¸åŒå­¦ä¹ ç‡å’Œè¿­ä»£æ¬¡æ•°ã€‚æˆ‘ä»¬å‘ç°ï¼Œæ‰¹é‡æ¢¯åº¦ä¸‹é™èƒ½å¤Ÿè·å¾—æœ€å‡†ç¡®çš„æ¨¡å‹å‚æ•°ï¼Œå¯¼è‡´æœ€ä½çš„æŸå¤±å€¼ï¼Œå°½ç®¡éœ€è¦çš„æ—¶é—´æœ€é•¿ã€‚å¦ä¸€æ–¹é¢ï¼Œéšæœºæ¢¯åº¦ä¸‹é™è™½ç„¶å‡†ç¡®åº¦è¾ƒä½ï¼Œä½†è®¡ç®—é€Ÿåº¦æœ€å¿«ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢ç´¢äº†å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼Œå‘ç°å®ƒåœ¨åˆç†çš„æ‰§è¡Œæ—¶é—´å†…è¾¾åˆ°äº†ä¸­ç­‰çš„å‡†ç¡®åº¦ã€‚
- en: I hope you enjoyed reading about the gradient descent and find the article useful!
    âœ¨
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›ä½ å–œæ¬¢é˜…è¯»æœ‰å…³æ¢¯åº¦ä¸‹é™çš„å†…å®¹ï¼Œå¹¶è§‰å¾—è¿™ç¯‡æ–‡ç« æœ‰ç”¨ï¼âœ¨
- en: ğŸ“ If you enjoy reading articles like this and wish to support my writing, you
    may consider [becoming a Medium member](https://idilismiguzel.medium.com/membership)!
    Medium members get full access to articles from all writers and if you use [my
    referral link](https://idilismiguzel.medium.com/membership), you will be directly
    supporting my writing.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ“ å¦‚æœä½ å–œæ¬¢é˜…è¯»è¿™æ ·çš„æ–‡ç« å¹¶å¸Œæœ›æ”¯æŒæˆ‘çš„å†™ä½œï¼Œä½ å¯ä»¥è€ƒè™‘ [æˆä¸º Medium ä¼šå‘˜](https://idilismiguzel.medium.com/membership)!
    Medium ä¼šå‘˜å¯ä»¥å…¨é¢è®¿é—®æ‰€æœ‰ä½œè€…çš„æ–‡ç« ï¼Œå¦‚æœä½ ä½¿ç”¨ [æˆ‘çš„æ¨èé“¾æ¥](https://idilismiguzel.medium.com/membership)ï¼Œä½ å°†ç›´æ¥æ”¯æŒæˆ‘çš„å†™ä½œã€‚
- en: ğŸ“ If you are already a member and interested to read my articles, you can [subscribe
    to be notified](https://medium.com/subscribe/@idilismiguzel) or [follow me on
    Medium](https://idilismiguzel.medium.com). Let me know if you have any questions
    or suggestions.âœ¨
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ“ å¦‚æœä½ å·²ç»æ˜¯ä¼šå‘˜å¹¶ä¸”æœ‰å…´è¶£é˜…è¯»æˆ‘çš„æ–‡ç« ï¼Œä½ å¯ä»¥ [è®¢é˜…ä»¥è·å–é€šçŸ¥](https://medium.com/subscribe/@idilismiguzel)
    æˆ– [åœ¨ Medium ä¸Šå…³æ³¨æˆ‘](https://idilismiguzel.medium.com)ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚âœ¨
- en: 'Additional resources I recommend reading after this article:'
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æˆ‘æ¨èåœ¨é˜…è¯»å®Œè¿™ç¯‡æ–‡ç« åè¿›ä¸€æ­¥é˜…è¯»çš„é™„åŠ èµ„æºï¼š
- en: Understand the concept behind hyperparameter tuning with these two most common
    methods.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: äº†è§£é€šè¿‡è¿™ä¸¤ç§æœ€å¸¸è§çš„æ–¹æ³•è¿›è¡Œè¶…å‚æ•°è°ƒæ•´çš„æ¦‚å¿µã€‚
- en: '[](/hyperparameter-tuning-with-grid-search-and-random-search-6e1b5e175144?source=post_page-----246e324c229--------------------------------)
    [## Hyperparameter Tuning with Grid Search and Random Search'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/hyperparameter-tuning-with-grid-search-and-random-search-6e1b5e175144?source=post_page-----246e324c229--------------------------------)
    [## ç½‘æ ¼æœç´¢å’Œéšæœºæœç´¢çš„è¶…å‚æ•°è°ƒæ•´'
- en: And a deep dive into how to combine them
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä»¥åŠæ·±å…¥æ¢è®¨å¦‚ä½•å°†å®ƒä»¬ç»“åˆèµ·æ¥
- en: towardsdatascience.com](/hyperparameter-tuning-with-grid-search-and-random-search-6e1b5e175144?source=post_page-----246e324c229--------------------------------)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/hyperparameter-tuning-with-grid-search-and-random-search-6e1b5e175144?source=post_page-----246e324c229--------------------------------)'
- en: 2\. Understand the linear regression model and its assumptions.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. ç†è§£çº¿æ€§å›å½’æ¨¡å‹åŠå…¶å‡è®¾ã€‚
- en: '[](/linear-regression-model-with-python-481c89f0f05b?source=post_page-----246e324c229--------------------------------)
    [## Linear Regression Model with Python'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/linear-regression-model-with-python-481c89f0f05b?source=post_page-----246e324c229--------------------------------)
    [## ä½¿ç”¨ Python çš„çº¿æ€§å›å½’æ¨¡å‹'
- en: How you can build and check the quality of your regression model with graphical
    and numeric outputs
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¦‚ä½•é€šè¿‡å›¾å½¢å’Œæ•°å€¼è¾“å‡ºæ„å»ºå’Œæ£€æŸ¥å›å½’æ¨¡å‹çš„è´¨é‡
- en: towardsdatascience.com](/linear-regression-model-with-python-481c89f0f05b?source=post_page-----246e324c229--------------------------------)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/linear-regression-model-with-python-481c89f0f05b?source=post_page-----246e324c229--------------------------------)'
- en: References
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: Header Photo by [Lucas Clara](https://unsplash.com/ko/@lux17?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¤´å›¾ç”± [Lucas Clara](https://unsplash.com/ko/@lux17?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Learning rates plot adapted from [https://cs231n.github.io/neural-networks-3/](https://cs231n.github.io/neural-networks-3/)
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å­¦ä¹ ç‡å›¾è¡¨æ”¹ç¼–è‡ª [https://cs231n.github.io/neural-networks-3/](https://cs231n.github.io/neural-networks-3/)
- en: All other images are by the Author
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‰€æœ‰å…¶ä»–å›¾ç‰‡ç”±ä½œè€…æä¾›
