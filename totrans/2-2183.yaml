- en: Understanding Gradient Descent for Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/understanding-gradient-descent-for-machine-learning-246e324c229](https://towardsdatascience.com/understanding-gradient-descent-for-machine-learning-246e324c229)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A deep dive into Batch, Stochastic, and Mini-Batch Gradient Descent algorithms
    using Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)[![Idil
    Ismiguzel](../Images/6846628535770a9f3e13ebb555e82abd.png)](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)[](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)
    [Idil Ismiguzel](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)
    ¬∑14 min read¬∑May 21, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5287b14c08ccf02afd428f2beb4f7204.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Lucas Clara](https://unsplash.com/ko/@lux17?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent is a popular optimization algorithm that is used in machine
    learning and deep learning models such as linear regression, logistic regression,
    and neural networks. It uses first-order derivatives iteratively to minimize the
    cost function by updating model coefficients (for regression) and weights (for
    neural networks).
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will delve into the mathematical theory of gradient descent
    and explore how to perform calculations using Python. We will examine various
    implementations including Batch Gradient Descent, Stochastic Gradient Descent,
    and Mini-Batch Gradient Descent, and assess their effectiveness on a range of
    test cases.
  prefs: []
  type: TYPE_NORMAL
- en: While following the article, you can check out the [Jupyter Notebook](https://github.com/Idilismiguzel/Machine-Learning/blob/master/Gradient_Descent/gradient_descent_implementation.ipynb)
    on my GitHub for complete analysis and code.
  prefs: []
  type: TYPE_NORMAL
- en: Before a deep dive into gradient descent, let‚Äôs first go through the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: What is Loss Function?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Loss** or **cost** are used interchangeably to describe the error in a prediction.
    A loss value indicates how different a prediction is from the actual value and
    the loss function aggregates all the loss values from multiple data points into
    a single number.'
  prefs: []
  type: TYPE_NORMAL
- en: You can see in the image below, the model on the left has high loss whereas
    the model on the right has low loss and fits the data better.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb7e40a2f95aa71254db5f4592880b6d.png)'
  prefs: []
  type: TYPE_IMG
- en: High loss vs low loss (blue lines) from the corresponding regression line in
    yellow.
  prefs: []
  type: TYPE_NORMAL
- en: The loss function (J) is used as a performance measurement for prediction algorithms
    and the main goal of a predictive model is to minimize its loss function, which
    is determined by the values of the model parameters (i.e., Œ∏0 and Œ∏1).
  prefs: []
  type: TYPE_NORMAL
- en: For example, linear regression models frequently use squared loss to compute
    the loss value and mean squared error is the loss function that averages all squared
    losses.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d702a1a157eed6b1cb56125ef219ea27.png)'
  prefs: []
  type: TYPE_IMG
- en: Squared Loss value (L2 Loss) and Mean Squared Error (MSE)
  prefs: []
  type: TYPE_NORMAL
- en: The linear regression model works behind the scenes by going through several
    iterations to optimize its coefficients and reach the lowest possible mean squared
    error.
  prefs: []
  type: TYPE_NORMAL
- en: What is Gradient Descent?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The gradient descent algorithm is usually described with a mountain analogy:'
  prefs: []
  type: TYPE_NORMAL
- en: ‚õ∞ Imagine yourself standing atop a mountain, with limited visibility, and you
    want to reach the ground. While descending, you'll encounter slopes and pass them
    using larger or smaller steps. Once you've reached a slope that is almost leveled,
    you'll know that you've arrived at the lowest point. ‚õ∞
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In technical terms, **gradient** refers to these slopes. When the slope is zero,
    it may indicate that you‚Äôve reached a function's minimum or maximum value.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2d3f12fa02efaed21b87ca718f61b10a.png)'
  prefs: []
  type: TYPE_IMG
- en: Like in the mountain analogy, GD minimizes the starting loss value by taking
    repeated steps in the opposite direction of the gradient to reduce the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: At any given point on a curve, the steepness of the slope can be determined
    by a **tangent line** ‚Äî a straight line that touches the point (red lines in the
    image above). Similar to the tangent line, the gradient of a point on the loss
    function is calculated with respect to the parameters, and a small step is taken
    in the opposite direction to reduce the loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, the process of gradient descent can be broken down into the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Select a starting point for the model parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the gradient of the cost function with respect to the parameters and
    continually adjust the parameter values through iterative steps to minimize the
    cost function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step 2 until the cost function no longer decreases or the maximum number
    of iterations is reached.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can examine the gradient calculation for the previously defined cost (loss)
    function. Although we are utilizing linear regression with an intercept and coefficient,
    this reasoning can be extended to regression models incorporating several variables.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b7efa05a2b319a6b2afaf3b0584f7bc4.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear regression function with 2 parameters, cost function, and objective function
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/abe1795d9a85eca87cc8383bef5dded3.png)'
  prefs: []
  type: TYPE_IMG
- en: Partial derivatives calculated wrt model parameters
  prefs: []
  type: TYPE_NORMAL
- en: üí° Sometimes, the point that has been reached may only be a *local minimum* or
    a *plateau*. In such cases, the model needs to continue iterating until it reaches
    the global minimum. Reaching the global minimum is unfortunately not guaranteed
    but with a proper number of iterations and a learning rate we can increase the
    chances.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8df9feaf7bf49a9654c5b8e0f1944cac.png)'
  prefs: []
  type: TYPE_IMG
- en: When using gradient descent, it is important to be aware of the potential challenge
    of stopping at a local minimum or on a plateau. To avoid this, it is essential
    to choose the appropriate number of iterations and learning rate. We will discuss
    this further in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: '`Learning_rate` is the hyperparameter of gradient descent to define the size
    of the learning step. It can be tuned using [hyperparameter tuning techniques](https://medium.com/towards-data-science/hyperparameter-tuning-with-grid-search-and-random-search-6e1b5e175144).'
  prefs: []
  type: TYPE_NORMAL
- en: If the `learning_rate` is set too high it could result in a jump that produces
    a loss value greater than the starting point. A high `learning_rate` might cause
    gradient descent to **diverge**,leading it to continually obtain higher loss values
    and preventing it from finding the minimum.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/d1b33f2d63d0773bf5f91edff2582bf2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Example case: A high learning rate causes GD to diverge'
  prefs: []
  type: TYPE_NORMAL
- en: If the `learning_rate` is set too low it can lead to a lengthy computation process
    where gradient descent iterates through numerous rounds of gradient calculations
    to reach **convergence** and discover the minimum loss value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/50be59657a670b4f90cb4f14f649715d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Example case: A low learning rate causes GD to take too much time to converge'
  prefs: []
  type: TYPE_NORMAL
- en: The value of the learning step is determined by the slope of the curve, which
    means that as we approach the minimum point, the learning steps become smaller.
  prefs: []
  type: TYPE_NORMAL
- en: When using low learning rates, the progress made will be steady, whereas high
    learning rates may result in either exponential progress or being stuck at low
    points.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af1f3ef2e20a9932f0f6dfb4e0b4c078.png)'
  prefs: []
  type: TYPE_IMG
- en: Image adapted from [https://cs231n.github.io/neural-networks-3/](https://cs231n.github.io/neural-networks-3/)
  prefs: []
  type: TYPE_NORMAL
- en: We will now cover three different implementations of the gradient descent algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Batch Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The batch gradient descent is the most widely used method for implementing gradient
    descent. It involves computing gradients with respect to the model parameters
    (such as regression coefficients) at every iteration for the entire data set.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs look at an example üîç
  prefs: []
  type: TYPE_NORMAL
- en: First, generate a dataset with an intercept of 5 and a coefficient of 4, along
    with a small amount of Gaussian noise. See the scatter plot of the generated data
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7644f9241b46b9de225680a0cfd89fa6.png)'
  prefs: []
  type: TYPE_IMG
- en: Scatter plot of data generated
  prefs: []
  type: TYPE_NORMAL
- en: The following function performs batch gradient descent by utilizing a specified
    learning rate and number of iterations. Initially, the model's coefficient (m)
    and intercept (b) are set to 0.5\. During each iteration, the error is calculated
    by taking the difference between predicted and actual values of y. The algorithm
    then updates m and b by extracting the gradient, which is also multiplied by the
    learning rate. The loop continues until the specified number of iterations is
    reached, and the resulting loss value and model parameters are stored in `params`
    and `loss`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Using the function above, we will now test different learning rates and evaluate
    the performance.
  prefs: []
  type: TYPE_NORMAL
- en: 1Ô∏è‚É£ Let‚Äôs set `learning_rate=0.01` with 1000 iterations and plot the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9b4bc468d425495f4c02fd781a6140d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Batch Gradient Descent with a learning rate of 0.01 and 1000 iterations
  prefs: []
  type: TYPE_NORMAL
- en: 'm: 4.27, b: 4.88, MSE: 0.95'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 367 ms'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the plots above, first, you can see the regression lines generated after
    each iteration (in purple) and notice how it gradually approaches the optimal
    after around 100 iterations. In the second plot, the loss function is displayed
    after every iteration, showing a significant decrease in the loss within the first
    50-100 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 2Ô∏è‚É£ Let‚Äôs now set `learning_rate=0.001` with the same number of iterations and
    plot the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'm: 4.67, b: 4.31, MSE: 1.05'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 522 ms'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e964bffeda41b26589b8f1124398fab2.png)'
  prefs: []
  type: TYPE_IMG
- en: Batch Gradient Descent with a learning rate of 0.001 and 1000 iterations
  prefs: []
  type: TYPE_NORMAL
- en: When the learning rate was reduced, the model needed more time to reach convergence
    and ended up with a higher final loss value. The first plot shows a darker shade
    of purple under the linear regression line, which means that there were various
    iterations that were far from the optimal line. Also, note that decreasing the
    learning rate from 0.01 to 0.001 resulted in an increase in execution time from
    367 ms to 522 ms.
  prefs: []
  type: TYPE_NORMAL
- en: 3Ô∏è‚É£ Let‚Äôs test `learning_rate=0.1` using the same number of iterations and plot
    the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'm: 4.23, b: 4.93, MSE: 0.95'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 214 ms'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/f93e84dee0e7b2db02ecae1807754fb5.png)'
  prefs: []
  type: TYPE_IMG
- en: Batch Gradient Descent with a learning rate of 0.1 and 1000 iterations
  prefs: []
  type: TYPE_NORMAL
- en: The model displayed rapid convergence when a high learning rate was used. In
    the first plot, there were only a few iterations where the regression line was
    distant from the optimal linear regression line. In the second plot, the loss
    sharply decreased during the first few iterations and remained steady until all
    iterations were completed. Furthermore, the execution time was the lowest at 214ms,
    out of the three trials.
  prefs: []
  type: TYPE_NORMAL
- en: üö®Calculating the gradient for the entire training data set for every parameter
    update can be computationally expensive on large data sets. Fortunately, stochastic
    gradient descent or mini-batch stochastic gradient descent can help solve this
    issue.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Stochastic Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another way to implement gradient descent is using stochastic gradient descent.
    It is especially preferred for large training data sets where the batch gradient
    descent might take too much time to compute.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic gradient descent updates model coefficients by calculating the loss
    value for only one **random** data point instead of calculating for every data
    point in the training data set and aggregating.
  prefs: []
  type: TYPE_NORMAL
- en: Random selection of the data point is essential to prevent getting stuck around
    similar values (such as clusters in the data). The learning rate and the number
    of iterations are also critical factors, similar to batch gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: The following function performs stochastic gradient descent by utilizing a specified
    learning rate and number of iterations. Initially, the model‚Äôs coefficient (m)
    and intercept (b) are set to 0.5\. In each iteration, the error is computed for
    a randomly selected data point using `np.random`. The remaining steps of the function
    are identical to those in batch gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Let's try using the same learning rate and number of iterations that we used
    for batch gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: 1Ô∏è‚É£ Let‚Äôs set `learning_rate=0.01` with 1000 iterations and plot the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'MSE: 0.97'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 325 ms'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/dabbc95a78881d732f085ce2cf4cf5d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Stochastic Gradient Descent with a learning rate of 0.01 and 1000 iterations
  prefs: []
  type: TYPE_NORMAL
- en: Notice that with the same learning rate, we were able to reduce the execution
    time from 367 ms to 325 ms when compared to batch gradient descent. However, the
    mean squared error (MSE) increased from 0.95 to 0.97.
  prefs: []
  type: TYPE_NORMAL
- en: 2Ô∏è‚É£ Let‚Äôs now set `learning_rate=0.001` with 1000 iterations and plot the loss
    function. I won't be including the code snippet since it's the same as in previous
    examples. However, feel free to refer to the [source code of the article](https://github.com/Idilismiguzel/Machine-Learning/blob/master/Gradient_Descent/gradient_descent_implementation.ipynb)
    if you need it.
  prefs: []
  type: TYPE_NORMAL
- en: 'MSE: 1.03'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 253 ms'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/374b35ec0c7c67e430e5558e260e27f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Stochastic Gradient Descent with a learning rate of 0.001 and 1000 iterations
  prefs: []
  type: TYPE_NORMAL
- en: Notice that using batch gradient descent with a learning rate of 0.001 resulted
    in a loss value of 1.05 in 522 milliseconds. However, using stochastic gradient
    descent with the same learning rate led to a loss value of 1.03 in only 253 milliseconds.
  prefs: []
  type: TYPE_NORMAL
- en: 3Ô∏è‚É£ Let‚Äôs now set `learning_rate=0.1` with 1000 iterations and plot the loss
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'MSE: 1.27'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 237 ms'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/3297564ca9ad3cf2a892944218541bde.png)'
  prefs: []
  type: TYPE_IMG
- en: Stochastic Gradient Descent with a learning rate of 0.1 and 1000 iterations
  prefs: []
  type: TYPE_NORMAL
- en: When using a high learning rate with stochastic gradient descent, the computation
    introduced fluctuations in the loss values. As shown in the first plot, there
    were numerous iterations above and below the optimal regression line.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, batch gradient descent outperforms stochastic gradient descent when
    it comes to converging to the minimum as stochastic gradient descent tends to
    wander around a vicinity near the global minimum. However, if the learning rate
    is carefully selected, stochastic gradient descent can also achieve similar loss
    values (sometimes even better) in a shorter amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Mini-Batch Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mini-batch gradient descent is a useful intermediate option between batch and
    stochastic gradient descent. Rather than computing the gradient for the entire
    dataset or just one observation, it divides the training data into smaller batches
    and calculates the gradient for each one.
  prefs: []
  type: TYPE_NORMAL
- en: By combining batch and stochastic methods, mini-batch GD enhances computation
    speed compared to the batch model and improves the accuracy of the stochastic
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The function below executes mini-batch gradient descent using a given learning
    rate, a set number of iterations, and a chosen batch size. By specifying a batch
    size, we can determine the number of data points to include in each gradient calculation.
    For instance, in stochastic gradient descent, the batch size is just one and the
    gradient is computed for a single data point. However, if we set a batch size
    of 10, then the gradient will be computed for 10 data points and then combined.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs test the same 3 cases with `learning_rate=0.01` `learning_rate=0.001`
    and `learning_rate=0.1.` using 10 as batch size.
  prefs: []
  type: TYPE_NORMAL
- en: 1Ô∏è‚É£ `learning_rate=0.01` with 1000 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'MSE: 0.96'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 224 ms'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/1d41acb0d5739dd227f564a8caed7ced.png)'
  prefs: []
  type: TYPE_IMG
- en: Mini-Batch Gradient Descent with a learning rate of 0.01 and 1000 iterations
  prefs: []
  type: TYPE_NORMAL
- en: Have you noticed that by using a 0.01 learning rate, we were able to achieve
    an MSE of 0.95 in 367 ms with batch GD, and an MSE of 0.97 in 325 ms with stochastic
    GD? Furthermore, with the implementation of mini-batch gradient descent, we were
    able to attain an MSE of 0.96 in just 224 ms.
  prefs: []
  type: TYPE_NORMAL
- en: 2Ô∏è‚É£ `learning_rate=0.001` with 1000 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'MSE: 1.04'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 485 ms'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/8e1dcf849fba1281a37de8f1abdded16.png)'
  prefs: []
  type: TYPE_IMG
- en: Mini-Batch Gradient Descent with a learning rate of 0.001 and 1000 iterations
  prefs: []
  type: TYPE_NORMAL
- en: It's worth noting that by using the same learning rate, we managed to decrease
    the execution time from 522 ms to 485 ms compared to batch gradient descent. Additionally,
    the mean squared error (MSE) decreased from 1.05 to 1.04.
  prefs: []
  type: TYPE_NORMAL
- en: 3Ô∏è‚É£ `learning_rate=0.1` with 1000 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'MSE: 0.97'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'execution time: 239 ms'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e11819e75dd475a17f6c11f2467f4bdf.png)'
  prefs: []
  type: TYPE_IMG
- en: Mini-Batch Gradient Descent with a learning rate of 0.1 and 1000 iterations
  prefs: []
  type: TYPE_NORMAL
- en: By implementing mini-batches, the irregularities observed in the historical
    loss values of stochastic gradient descent have been eliminated, and model converged
    really fast.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we delved into the gradient descent algorithm and its three
    main implementations - batch, stochastic, and mini-batch. Through a series of
    experiments, we compared these three implementations using different learning
    rates and iterations. We found that batch gradient descent was able to achieve
    the most accurate model parameters, resulting in the lowest loss values, despite
    taking the longest time. On the other hand, stochastic gradient descent reached
    less accuracy but had the highest calculation speed. Lastly, we explored mini-batch
    and found that it achieved moderate accuracy in a reasonable execution time.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you enjoyed reading about the gradient descent and find the article useful!
    ‚ú®
  prefs: []
  type: TYPE_NORMAL
- en: üçì If you enjoy reading articles like this and wish to support my writing, you
    may consider [becoming a Medium member](https://idilismiguzel.medium.com/membership)!
    Medium members get full access to articles from all writers and if you use [my
    referral link](https://idilismiguzel.medium.com/membership), you will be directly
    supporting my writing.
  prefs: []
  type: TYPE_NORMAL
- en: üçì If you are already a member and interested to read my articles, you can [subscribe
    to be notified](https://medium.com/subscribe/@idilismiguzel) or [follow me on
    Medium](https://idilismiguzel.medium.com). Let me know if you have any questions
    or suggestions.‚ú®
  prefs: []
  type: TYPE_NORMAL
- en: 'Additional resources I recommend reading after this article:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understand the concept behind hyperparameter tuning with these two most common
    methods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[](/hyperparameter-tuning-with-grid-search-and-random-search-6e1b5e175144?source=post_page-----246e324c229--------------------------------)
    [## Hyperparameter Tuning with Grid Search and Random Search'
  prefs: []
  type: TYPE_NORMAL
- en: And a deep dive into how to combine them
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/hyperparameter-tuning-with-grid-search-and-random-search-6e1b5e175144?source=post_page-----246e324c229--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Understand the linear regression model and its assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/linear-regression-model-with-python-481c89f0f05b?source=post_page-----246e324c229--------------------------------)
    [## Linear Regression Model with Python'
  prefs: []
  type: TYPE_NORMAL
- en: How you can build and check the quality of your regression model with graphical
    and numeric outputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/linear-regression-model-with-python-481c89f0f05b?source=post_page-----246e324c229--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs: []
  type: TYPE_NORMAL
- en: Header Photo by [Lucas Clara](https://unsplash.com/ko/@lux17?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learning rates plot adapted from [https://cs231n.github.io/neural-networks-3/](https://cs231n.github.io/neural-networks-3/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All other images are by the Author
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
