- en: 'Machine Learning in Three Steps: How to Efficiently Learn It'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 三步掌握机器学习：如何高效学习
- en: 原文：[https://towardsdatascience.com/machine-learning-in-three-steps-how-to-efficiently-learn-it-aefcf423a9e1](https://towardsdatascience.com/machine-learning-in-three-steps-how-to-efficiently-learn-it-aefcf423a9e1)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/machine-learning-in-three-steps-how-to-efficiently-learn-it-aefcf423a9e1](https://towardsdatascience.com/machine-learning-in-three-steps-how-to-efficiently-learn-it-aefcf423a9e1)
- en: Prioritizing the Essentials for Predictive Modeling without Overwhelming Yourself
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优先考虑预测建模的核心要点，避免让自己不堪重负
- en: '[](https://medium.com/@angela.shi?source=post_page-----aefcf423a9e1--------------------------------)[![Angela
    and Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----aefcf423a9e1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----aefcf423a9e1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----aefcf423a9e1--------------------------------)
    [Angela and Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----aefcf423a9e1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@angela.shi?source=post_page-----aefcf423a9e1--------------------------------)[![Angela
    和 Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----aefcf423a9e1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----aefcf423a9e1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----aefcf423a9e1--------------------------------)
    [Angela 和 Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----aefcf423a9e1--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----aefcf423a9e1--------------------------------)
    ·21 min read·Mar 3, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----aefcf423a9e1--------------------------------)
    ·阅读时间 21 分钟·2023年3月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: I have observed two extreme approaches when it comes to aspiring data scientists
    attempting to learn machine learning algorithms. The first approach involves learning
    **all the intricacies** of the algorithms and implementing them from scratch to
    gain true mastery. The second approach, on the other hand, assumes that the computer
    will “learn” on its own, rendering the need for the individual to learn the algorithms
    unnecessary. This leads some to **only** rely on tools such as the package lazypredict.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我观察到两种极端的方法，数据科学家在学习机器学习算法时通常会采用其中一种。第一种方法涉及学习**所有的细节**，并从头开始实现算法，以获得真正的掌握。另一方面，第二种方法则认为计算机会“自动学习”，从而使个人学习算法变得不必要。这使得一些人**仅仅**依赖于如
    lazypredict 这样的工具。
- en: It is realistic to take an approach between the two extremes when learning machine
    learning algorithms. However, the question remains, where to start? In this article,
    I will categorize machine learning algorithms into three categories and provide
    my humble opinion on what to begin with and what can be skipped.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习机器学习算法时，采取两种极端方法之间的折中方式是现实的。然而，问题依然是，从哪里开始呢？在这篇文章中，我将把机器学习算法分为三类，并提供我个人的意见，告诉你应该从什么开始，以及哪些可以跳过。
- en: The complexity of machine learning algorithms
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习算法的复杂性
- en: Starting out in machine learning can be overwhelming due to the multitude of
    available algorithms. Linear regression, support vector machines (SVM), gradient
    descent, gradient boosting, decision trees, LASSO, ridge, grid search, and many
    more are some of the algorithms that come to mind when posed with the question.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 初学机器学习可能会因为众多可用的算法而感到不知所措。线性回归、支持向量机（SVM）、梯度下降、梯度提升、决策树、LASSO、岭回归、网格搜索等等，这些都是在提问时会想到的算法。
- en: In the field of supervised learning, these algorithms serve different purposes
    and have different objectives. In this article, we will only talk about supervised
    learning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习领域，这些算法服务于不同的目的，并具有不同的目标。本文将仅讨论监督学习。
- en: To gain a better understanding of the various techniques, it can be helpful
    to categorize them according to their objectives and levels of complexity. By
    organizing these algorithms into different categories and levels of complexity,
    one can simplify these concepts and make them easier to understand. This approach
    can greatly enhance one’s comprehension of machine learning and help to identify
    the most appropriate techniques to use for a particular task or objective.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解各种技术，按其目标和复杂性级别进行分类可能会有所帮助。通过将这些算法组织成不同的类别和复杂性级别，可以简化这些概念，使其更易于理解。这种方法可以大大提升对机器学习的理解，并帮助识别适合特定任务或目标的最佳技术。
- en: As students delve into the field of machine learning, they may become discouraged
    due to its complexity. However, it’s not necessary to learn or be familiar with
    all of the algorithms before putting them into practice. Different positions in
    the field of machine learning may demand diverse levels of proficiency, and it’s
    acceptable to lack knowledge of certain aspects. For instance, data scientists,
    data analysts, data engineers, and machine learning researchers have different
    requirements for their job roles.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当学生深入机器学习领域时，可能会因其复杂性而感到沮丧。然而，在实践之前并不需要学习或熟悉所有的算法。机器学习领域的不同职位可能需要不同水平的熟练度，缺乏某些方面的知识也是可以接受的。例如，数据科学家、数据分析师、数据工程师和机器学习研究人员对其工作角色的要求各不相同。
- en: Having a broad understanding of the overall process can enable machine learning
    practitioners to skip certain technical details if they are short on time, while
    still comprehending the process as a whole.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对整体过程有广泛的理解可以使机器学习从业者在时间紧迫的情况下跳过某些技术细节，同时仍能全面理解过程。
- en: '![](../Images/57f99cc62121b59e1011c724286b6b76.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57f99cc62121b59e1011c724286b6b76.png)'
- en: Photo by Julio Wolf on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由Julio Wolf提供，见[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 1\. Breaking Down Machine Learning Algorithms
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 机器学习算法的细分
- en: 1.1 Models, training, and tuning
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 模型、训练和调优
- en: 'The scope of “machine learning algorithms” is quite broad, and it can be categorized
    into three main types of algorithms:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: “机器学习算法”的范围相当广泛，可以分为三种主要类型的算法：
- en: (1) **machine learning models** which are designed to receive input data and
    subsequently generate predictions, such as linear regression, SVM, decision tree,
    KNN, etc. In the scikit-learn library, these are also referred to as “**estimators**”.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1) **机器学习模型**，旨在接收输入数据并随后生成预测，如线性回归、SVM、决策树、KNN等。在scikit-learn库中，这些也被称为“**估计器**”。
- en: (2) the **model training/fitting algorithms** that are used to create or optimize
    the models, aka find the parameters of the models for a specific dataset. And
    different machine learning models have their specific training algorithms. Although
    **gradient descent** is the most well-known method for training mathematical function-based
    models, other machine learning models can be trained using different techniques,
    which we will explore in more detail in the later sections of this article.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2) **模型训练/拟合算法**，用于创建或优化模型，也即为特定数据集寻找模型的参数。不同的机器学习模型有其特定的训练算法。虽然**梯度下降**是训练基于数学函数模型的最著名方法，但其他机器学习模型可以使用不同的技术进行训练，我们将在本文后续部分深入探讨这些技术。
- en: (3) **hyperparameter tuning** that consists of finding the optimal hyperparameters
    of the machine learning models. And contrary to the training process, the process
    of hyperparameter tuning usually does not depend on the machine learning models.
    **Grid search** is a popular and commonly used method for this task, although
    there are alternative approaches that we will delve into later in this article.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3) **超参数调优**，包括寻找机器学习模型的最佳超参数。与训练过程不同，超参数调优过程通常不依赖于机器学习模型。**网格搜索**是这个任务中一种流行且常用的方法，尽管还有其他替代方法，我们将在本文后续部分深入探讨。
- en: 1.2 Three Categories of ML models
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 机器学习模型的三种类别
- en: 'The first aspect deals with the models that are capable of taking in data and
    generating predictions based on that data. These models can be categorized into
    three families:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方面涉及能够接收数据并根据这些数据生成预测的模型。这些模型可以分为三类：
- en: '**Distance-based** models, which include K-nearest-neighbors, Linear Discriminant
    Analysis, and Quadratic Discriminant Analysis.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于距离**的模型，包括K近邻、线性判别分析和二次判别分析。'
- en: '**Decision tree-based** models, such as a single decision tree (used for classification
    or regression), Random Forest, and Gradient-Boosted Decision Trees.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于决策树**的模型，如单棵决策树（用于分类或回归）、随机森林和梯度提升决策树。'
- en: '**Mathematical functions-based** models: also known as parametric models, are
    models that assume a specific functional form for the relationship between the
    inputs and the outputs. They can be further divided into **linear models** like
    OLS regression, SVM (with a linear kernel), Ridge, and LASSO, and **non-linear
    models** like SVM with non-linear kernels and neural networks.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于数学函数**的模型，也称为参数模型，是假设输入和输出之间存在特定函数形式的模型。它们可以进一步分为**线性模型**，如OLS回归、SVM（具有线性内核）、岭回归和LASSO，以及**非线性模型**，如具有非线性内核的SVM和神经网络。'
- en: 1.3 Meta-models and ensemble methods
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 元模型和集成方法
- en: In machine learning, a meta-model is a model that combines the predictions of
    multiple individual models to make more accurate predictions. It is also known
    as a “stacked model” or “super learner”. The individual models that make up the
    meta-model can be of different types or use different algorithms, and their predictions
    are combined using a weighted average or other techniques.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，元模型是一个结合多个单独模型预测以进行更准确预测的模型。它也被称为“堆叠模型”或“超级学习器”。组成元模型的单独模型可以是不同类型的或使用不同算法的，它们的预测通过加权平均或其他技术进行组合。
- en: The goal of a meta-model is to improve the overall accuracy and robustness of
    predictions by reducing the variance and bias that may be present in individual
    models. It can also help to overcome the limitations of individual models by capturing
    more complex patterns in the data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 元模型的目标是通过减少单个模型中可能存在的方差和偏差，提高预测的整体准确性和鲁棒性。它还可以通过捕捉数据中的复杂模式来帮助克服单个模型的局限性。
- en: One common approach to creating a meta-model is to use ensemble methods such
    as bagging, boosting, or stacking.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 创建元模型的一个常见方法是使用集成方法，如自助采样、提升或堆叠。
- en: Bagging, or Bootstrap Aggregating, is a technique used in machine learning to
    reduce the variance of a model by combining multiple models trained on different
    samples of the dataset. The idea behind bagging is to generate several models,
    each with a subset of the data, and then combine them to create a more robust
    model that is less susceptible to overfitting.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自助采样（Bagging），或称自助聚合，是一种在机器学习中用于通过组合在数据集的不同样本上训练的多个模型来减少模型方差的技术。自助采样的想法是生成多个模型，每个模型使用数据的一个子集，然后将它们组合起来，创建一个更强健且不易过拟合的模型。
- en: 'Boosting: Boosting is another ensemble method that combines multiple weak models
    to create a strong model. In contrast to bagging, which trains each model independently,
    boosting trains models in a sequence. Each new model is trained on the data that
    was misclassified by the previous models, and the final prediction is made by
    aggregating the predictions of all the models.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升：提升是另一种集成方法，它结合多个弱模型来创建一个强模型。与自助采样不同，提升依次训练模型。每个新模型在由先前模型错误分类的数据上进行训练，最终预测通过聚合所有模型的预测来得出。
- en: 'Stacking: Stacking, or stacked generalization, is a meta-model ensemble method
    that involves training multiple base models and using their predictions as input
    to a higher-level model. The higher-level model learns how to combine the predictions
    of the base models to make a final prediction.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠：堆叠，或称堆叠泛化，是一种元模型集成方法，涉及训练多个基础模型，并使用它们的预测作为更高级模型的输入。高级模型学习如何组合基础模型的预测以进行最终预测。
- en: 'Random forests: it is an extension of bagging that adds an extra layer of randomness.
    In addition to randomly sampling the data, random forests also randomly select
    a subset of features for each split. This helps to reduce overfitting and increase
    the diversity of the models in the ensemble.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林：它是对自助采样（bagging）的扩展，增加了一层额外的随机性。除了随机抽样数据外，随机森林还会随机选择每次分裂时的特征子集。这有助于减少过拟合，并增加集成中模型的多样性。
- en: Ensemble methods are most commonly applied to decision trees rather than linear
    models such as linear regression. This is because decision trees are more prone
    to overfitting than linear models, and ensemble methods help to reduce overfitting
    by combining multiple models.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法最常应用于决策树，而不是像线性回归这样的线性模型。这是因为决策树比线性模型更容易过拟合，而集成方法通过结合多个模型来帮助减少过拟合。
- en: Decision trees have high variance and low bias, meaning that they are prone
    to overfitting the training data, resulting in poor performance on new, unseen
    data. Ensemble methods address this issue by aggregating the predictions of multiple
    decision trees, resulting in a more robust and accurate model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树具有高方差和低偏差，这意味着它们容易对训练数据进行过拟合，从而导致在新数据上的表现较差。集成方法通过聚合多个决策树的预测来解决这个问题，从而得到一个更强健和准确的模型。
- en: On the other hand, linear models such as linear regression have a low variance
    and high bias, meaning that they are less prone to overfitting but may underfit
    the training data. Ensemble methods are not as effective for linear models because
    the models already have a low variance and do not benefit as much from aggregation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，线性模型如线性回归具有低方差和高偏差，这意味着它们不容易过拟合，但可能会欠拟合训练数据。由于这些模型已经具有低方差，因此集成方法对线性模型的效果不如对决策树的效果明显，因为集成方法在这方面的收益不大。
- en: However, there are still some cases where ensemble methods can be applied to
    linear models. For example, the bootstrap aggregating technique used in bagging
    can be applied to any type of model, including linear regression. In this case,
    the bagging algorithm would sample the training data and fit multiple linear regression
    models on the bootstrapped samples, resulting in a more stable and robust model.
    However, it’s worth noting that the resulting model is still a linear regression
    model, not a meta-model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仍然有一些情况可以将集成方法应用于线性模型。例如，袋装技术中使用的自助聚合技术可以应用于任何类型的模型，包括线性回归。在这种情况下，袋装算法会对训练数据进行采样，并在自助样本上拟合多个线性回归模型，从而得到一个更稳定且更强健的模型。然而，值得注意的是，得到的模型仍然是线性回归模型，而不是一个元模型。
- en: Overall, while ensemble methods are most commonly applied to decision trees,
    there are some cases where they can be used with linear models. However, it’s
    important to keep in mind the strengths and limitations of each type of model
    and choose the appropriate method for the problem at hand.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，尽管集成方法最常用于决策树，但在某些情况下它们也可以与线性模型一起使用。然而，重要的是要记住每种模型的优缺点，并选择适合当前问题的方法。
- en: 1.4 Overview of Machine Learning Algorithms
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 机器学习算法概述
- en: The diagram below provides a summary of the various machine learning algorithms
    classified into the 3 categories. The subsequent sections of this article will
    delve deeper into each category.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 下图提供了分类为3类的各种机器学习算法的总结。本文的后续部分将更深入地探讨每一类。
- en: '![](../Images/54c901c9803cad52e3611de1cc5d8263.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54c901c9803cad52e3611de1cc5d8263.png)'
- en: Overview of machine learning algorithms — Image by the author
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法概述 — 图片由作者提供
- en: 2\. Machine learning models
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 机器学习模型
- en: In this section, we will have a closer look at the three families of machine
    learning models. here is a more detailed plan about the
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将更详细地观察三种机器学习模型的家族。以下是有关的更详细的计划。
- en: (1) Distance-based models
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 基于距离的模型
- en: 'Instance-based models: KNN'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于实例的模型：KNN
- en: 'Bayes classifiers: LDA, QDA'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯分类器：LDA，QDA
- en: (2) Decision trees-base models
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 基于决策树的模型
- en: (3) Matemathetical function-based models
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 基于数学函数的模型
- en: Linear models
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性模型
- en: Kernelized models such as kernel SVM or kernel ridge
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核方法模型，如核SVM或核岭回归
- en: Neural networks
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络
- en: Deep learning models
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型
- en: '**2.1 Instance-based models**'
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2.1 基于实例的模型**'
- en: The first family of machine learning models is distance-based models. These
    models use the distance between data points to make predictions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 第一类机器学习模型是基于距离的模型。这些模型使用数据点之间的距离来进行预测。
- en: The simplest and most representative model is K-Nearest Neighbors (KNN). It
    calculates the distance between the new data point and all the existing data points
    in the dataset. It then selects the K-nearest neighbors and assigns the new data
    point to the class that is most common among the K neighbors.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单且最具代表性的模型是K-最近邻（KNN）。它计算新数据点与数据集中所有现有数据点之间的距离。然后选择K个最近邻，并将新数据点分配给K个邻居中最常见的类别。
- en: When examining k-nearest neighbors (KNN) algorithm, it can be noted that there
    is no explicit model built during the training phase. In KNN, the prediction for
    a new observation is made by finding the k nearest neighbors to that observation
    in the training set and taking the average or majority vote of their target values.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other algorithms that fit a model to the data during training, KNN stores
    the entire training dataset and simply computes distances between new observations
    and the existing dataset to make predictions. Therefore, KNN can be considered
    a “lazy learning” algorithm, as it does not actively build a model during the
    training phase, but rather defers the decision-making process until inference
    time.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the inference/testing phase may be slow. It’s important to note
    that more efficient algorithms such as k-d tree can be used.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Bayes classifiers
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Linear Discriminant Analysis** (LDA) and **Quadratic Discriminant Analysis**
    (QDA) are distance-based models that use **Mahalanobis distance** to make predictions.
    Mahalanobis distance is a measure of the distance between a point and a distribution,
    taking into account the correlation between variables.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Linear Discriminant Analysis (LDA) assumes that the variance is the same across
    different classes, whereas Quadratic Discriminant Analysis (QDA) assumes that
    the variance is different for each class. This means that LDA assumes that the
    covariance matrix is the same for all classes, while QDA allows for each class
    to have its own covariance matrix.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Decision tree-based models
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second family of machine learning models is decision tree-based models.
    They also can be called rule-based models, this means that the model generates
    a set of rules that can be used to explain how it arrived at its decision or prediction.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Each branch of a decision tree represents a rule or condition, which is used
    to determine which subset of data to follow next. These rules are typically in
    the form of simple if-then statements, such as “if the value of variable X is
    greater than 5, then follow the left branch, otherwise follow the right branch.”
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: The final leaf nodes of the decision tree represent the predicted class or value
    of the target variable based on the values of the input variables and the rules
    that led to that prediction.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: One advantage of decision trees is that they are easy to interpret and understand
    since the rules can be visualized and explained in a clear and intuitive manner.
    This makes them useful for explaining the reasoning behind a prediction or decision
    to non-technical stakeholders.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: However, decision trees can also be prone to overfitting, which occurs when
    the model becomes too complex and fits the training data too closely, resulting
    in poor generalization to new data. To address this issue, ensemble methods are
    commonly applied to decision trees.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Mathematical functions-based models
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The third family of machine learning models is mathematical functions-based
    models. These models use mathematical functions to model the relationship between
    the input variables and the target variable. Linear models, such as Ordinary Least
    Squares (OLS) regression, and Support Vector Machines (SVM) with a linear kernel,
    Ridge, and LASSO, assume that the relationship between the input variables and
    the target variable is linear. Non-linear models, such as SVM with non-linear
    kernels and Neural Networks, can model more complex relationships between the
    input variables and the target variable.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 第三类机器学习模型是基于数学函数的模型。这些模型使用数学函数来建模输入变量和目标变量之间的关系。线性模型，例如普通最小二乘（OLS）回归和具有线性核的支持向量机（SVM）、岭回归和LASSO，假设输入变量和目标变量之间的关系是线性的。非线性模型，例如具有非线性核的SVM和神经网络，可以建模输入变量和目标变量之间更复杂的关系。
- en: For mathematical function-based models, such as linear regression or logistic
    regression, we have to define a loss function. The loss function measures how
    well the model’s predictions match the actual data. The goal is to minimize the
    loss function by adjusting the parameters of the model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于数学函数的模型，例如线性回归或逻辑回归，我们必须定义损失函数。损失函数衡量模型的预测与实际数据的匹配程度。目标是通过调整模型的参数来最小化损失函数。
- en: In contrast, for non-mathematical function-based models, such as KNN or decision
    trees, we don’t need to define a loss function. Instead, we use a different approach,
    such as finding the nearest neighbors in the case of KNN or recursively splitting
    the data based on the feature values in the case of decision trees.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 相对而言，对于基于非数学函数的模型，例如KNN或决策树，我们不需要定义损失函数。相反，我们使用不同的方法，比如在KNN的情况下寻找最近邻，或者在决策树的情况下根据特征值递归地划分数据。
- en: Defining an appropriate loss function is crucial in mathematical function-based
    models, as it determines the optimization problem that the model solves. Different
    loss functions can be used depending on the problem at hand, such as mean squared
    error for regression problems or cross-entropy for binary classification problems.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于数学函数的模型中，定义适当的损失函数至关重要，因为它决定了模型解决的优化问题。根据具体问题的不同，可以使用不同的损失函数，例如回归问题的均方误差或二分类问题的交叉熵。
- en: It is worth noting that all the linear models, such as Ordinary Least Squares
    (OLS), LASSO, Ridge, and Support Vector Machines (SVM) with linear kernel, can
    be written in the form of a linear equation y = wX + b. However, the difference
    between these models lies in the cost function used to estimate the optimal values
    of the model parameters w and b.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，所有线性模型，例如普通最小二乘（OLS）、LASSO、岭回归和具有线性核的支持向量机（SVM），都可以写成线性方程y = wX + b的形式。然而，这些模型之间的区别在于用于估计模型参数w和b的最优值的成本函数。
- en: So, while it is true that all these models can be written in the form of the
    same mathematical function, it is important to note that the choice of cost function
    determines the behavior and performance of the model. Hence, it would be more
    accurate to consider them as different models with different cost functions, rather
    than the same model with different cost functions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虽然所有这些模型可以写成相同数学函数的形式，但重要的是要注意成本函数的选择决定了模型的行为和性能。因此，更准确地说，这些模型应该被视为具有不同成本函数的不同模型，而不是相同模型的不同成本函数。
- en: 'Non-linear models are powerful tools for solving complex machine learning problems
    that cannot be adequately addressed by linear models. And there are essentially
    two approaches used in practice: the kernel trick and neural networks.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性模型是解决复杂机器学习问题的强大工具，而这些问题无法通过线性模型得到充分解决。在实践中，主要有两种方法：核技巧和神经网络。
- en: The kernel trick is a way of implementing feature mapping efficiently without
    explicitly computing the transformed features. Instead, it involves defining a
    kernel function that calculates the similarity between pairs of input samples
    in the transformed feature space. By using the kernel function, we can implicitly
    map the input data to a high-dimensional space, where it can be more easily separated
    and modeled.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 核技巧是一种有效地实现特征映射的方法，而无需显式计算变换后的特征。相反，它涉及定义一个核函数，用于计算变换特征空间中输入样本对之间的相似性。通过使用核函数，我们可以隐式地将输入数据映射到高维空间，在那里数据可以更容易地被分离和建模。
- en: In this sense, the kernel part can be seen as a form of feature engineering,
    where the model is able to create new features that are more suitable for the
    task at hand. This is in contrast to traditional feature engineering, where human
    experts manually create new features based on domain knowledge and intuition.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Another approach to creating non-linear models is through the use of neural
    networks. They consist of layers of interconnected nodes or “neurons,” each of
    which performs a simple mathematical operation on its inputs and passes the result
    to the next layer.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: The key to the power of neural networks is their ability to learn complex non-linear
    relationships between inputs and outputs. This is accomplished by adjusting the
    weights of the connections between neurons during training, based on the error
    between the predicted output and the actual output.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Deep learning models
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning is focused on learning representations of data through a hierarchy
    of multiple layers. It has become increasingly popular in recent years due to
    its success in a wide range of applications, including computer vision, natural
    language processing, and speech recognition. While deep learning models can be
    considered complex due to their large number of parameters and layers, it’s true
    that a significant part of deep learning is also about feature engineering.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: One example of a deep learning model is a convolutional neural network (CNN).
    At its core, a CNN applies a series of filters to an input image, with each filter
    looking for a specific feature such as edges or corners. The later layers of the
    network then use these extracted features to classify the input image.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: In this way, deep learning models like CNNs can be thought of as a combination
    of feature engineering and a trainable model. The feature engineering aspect of
    the model involves designing the architecture of the network to extract useful
    features from the input data, while the trainable model involves optimizing the
    network’s parameters to fit the data and make accurate predictions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Model training/fitting
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training a machine learning model is the process of teaching the model to make
    predictions or decisions by showing it a set of labeled examples. The labeled
    examples, also known as the training data, consist of pairs of input features
    and output labels.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: During the training process, the machine learning model learns to recognize
    patterns in the input features and their corresponding output labels. The model
    uses a specific algorithm to learn from the training data and adjust its internal
    parameters in order to improve its ability to predict or classify new data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Once the model has been trained on the labeled examples, it can be used to make
    predictions or decisions on new, unseen data. This process is known as inference
    or testing.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Different machine learning models have different training algorithms. Here are
    some examples of training algorithms used by different machine learning models.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Distance-based models training
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 基于距离的模型训练
- en: 'K-Nearest Neighbors (KNN): KNN is a non-parametric algorithm that does not
    require explicit training. Instead, it stores the entire training dataset and
    uses it to predict the label of a new instance by finding the K closest instances
    in the training dataset based on some distance metric. The prediction is then
    based on the majority vote of the K nearest neighbors.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: K-最近邻（KNN）：KNN是一种非参数算法，不需要显式的训练。相反，它存储整个训练数据集，并通过基于某种距离度量找到训练数据集中K个最接近的实例来预测新实例的标签。然后，预测基于K个最近邻的多数投票。
- en: Linear Discriminant Analysis (LDA) is a supervised learning algorithm used for
    classification tasks. LDA models the distribution of the input features for each
    class and uses this information to find a linear combination of the input features
    that maximizes the separation between classes. The resulting linear discriminants
    can then be used to classify new instances.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 线性判别分析（LDA）是一种用于分类任务的监督学习算法。LDA建模每个类别的输入特征分布，并利用这些信息找到一个线性组合的输入特征，最大化类别之间的分离。生成的线性判别函数随后可以用来对新实例进行分类。
- en: The training process for LDA involves estimating the mean and covariance matrix
    of the input features for each class. These estimates are then used to compute
    the within-class and between-class scatter matrices, which are used to derive
    the linear discriminants. The number of linear discriminants is equal to the number
    of classes minus one.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: LDA的训练过程涉及估计每个类别的输入特征的均值和协方差矩阵。这些估计值然后用来计算类内散布矩阵和类间散布矩阵，这些矩阵用于推导线性判别函数。线性判别函数的数量等于类别数减一。
- en: 3.2 Decision tree-based models training
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 基于决策树的模型训练
- en: As for decision trees, they are trained using a different approach called recursive
    partitioning.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 至于决策树，它们使用一种称为递归划分的不同方法进行训练。
- en: Recursive partitioning is a top-down process that starts with the entire dataset
    and splits it into subsets based on a set of rules or conditions. The splitting
    process is repeated recursively on each subset until a stopping criterion is met,
    typically when the subsets become too small or no further splitting improves the
    model’s performance.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 递归划分是一种自顶向下的过程，从整个数据集开始，根据一系列规则或条件将其划分为子集。这个划分过程在每个子集上递归重复，直到满足停止准则，通常是当子集变得过小或进一步划分无法改善模型性能时。
- en: The splitting rules are based on features or attributes of the dataset, and
    the algorithm selects the feature that provides the most significant improvement
    in model performance at each step. The splitting process results in a tree-like
    structure, where the internal nodes represent splitting conditions, and the leaf
    nodes represent the final predictions.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 划分规则基于数据集的特征或属性，算法在每一步选择提供模型性能最显著提升的特征。划分过程生成树状结构，其中内部节点代表划分条件，叶子节点代表最终预测结果。
- en: During the training process, decision trees can be evaluated using a variety
    of metrics, such as information gain or Gini impurity, to determine the best splitting
    criteria. Once the tree is trained, it can be used to make predictions on new,
    unseen data by following the path from the root node to the appropriate leaf node
    based on the input features.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，决策树可以使用各种度量标准进行评估，如信息增益或基尼不纯度，以确定最佳的划分标准。一旦树训练完成，可以通过根据输入特征从根节点到适当叶节点的路径来对新的、未见过的数据进行预测。
- en: 3.3 Mathematical functions-based models training
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 基于数学函数的模型训练
- en: Mathematical functions-based models, also known as parametric models, are models
    that assume a specific functional form for the relationship between the inputs
    and the outputs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 基于数学函数的模型，也称为参数模型，是假设输入与输出之间存在特定函数形式的模型。
- en: The most basic algorithm used for optimizing the parameters of mathematical
    functions-based models is gradient descent. Gradient descent is an iterative optimization
    algorithm that starts with an initial guess for the parameter values and then
    updates them based on the gradient of the loss function with respect to the parameters.
    This process continues until the algorithm converges to a minimum of the loss
    function.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 用于优化数学函数模型参数的最基本算法是梯度下降。梯度下降是一种迭代优化算法，从参数值的初始猜测开始，然后根据损失函数对参数的梯度更新参数。这个过程持续进行，直到算法收敛到损失函数的最小值。
- en: For non-convex functions, stochastic gradient descent (SGD) is often used instead
    of gradient descent. SGD randomly samples a subset of the data at each iteration
    to compute the gradient, which can make it faster and more efficient than gradient
    descent.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非凸函数，通常使用随机梯度下降（SGD）代替梯度下降。SGD在每次迭代中随机抽样数据的一个子集来计算梯度，这使得它比梯度下降更快、更高效。
- en: In neural networks, backpropagation is used to compute the gradients of the
    loss function with respect to the parameters. Backpropagation is essentially just
    the chain rule of calculus applied to the composite function represented by the
    neural network. It allows the gradient to be efficiently computed for each layer
    of the network, which is essential for training deep neural networks.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，反向传播用于计算损失函数相对于参数的梯度。反向传播本质上是将微积分的链式法则应用于神经网络表示的复合函数。它允许对网络的每一层有效地计算梯度，这是训练深度神经网络所必需的。
- en: For deep learning models, more advanced optimization techniques are often used
    to improve performance. These include techniques such as momentum, which helps
    the algorithm to avoid getting stuck in local minima, and adaptive learning rate
    methods, which automatically adjust the learning rate during training to improve
    convergence speed and stability.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度学习模型，通常使用更高级的优化技术来提高性能。这些技术包括如动量，它帮助算法避免陷入局部最小值，以及自适应学习率方法，它在训练过程中自动调整学习率，以提高收敛速度和稳定性。
- en: In summary, gradient descent is the basic algorithm used for optimizing the
    parameters of mathematical functions-based models. For non-convex functions, stochastic
    gradient descent is often used instead. Backpropagation is used for computing
    gradients in neural networks, and more advanced techniques are often used for
    deep learning models.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，梯度下降是优化基于数学函数的模型参数的基本算法。对于非凸函数，通常使用随机梯度下降。反向传播用于计算神经网络中的梯度，而更高级的技术常用于深度学习模型。
- en: 4\. Model tuning
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 模型调优
- en: The third facet of machine learning involves optimizing the model’s hyperparameters
    through the use of grid search. Hyperparameters are the settings or configurations
    of the model that are not learned during training but instead must be manually
    specified.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的第三个方面涉及通过使用网格搜索来优化模型的超参数。超参数是模型的设置或配置，这些设置在训练过程中不会被学习，而必须手动指定。
- en: Examples of hyperparameters include the learning rate, the number of hidden
    layers in a neural network, and the regularization strength. Through the use of
    grid search, multiple combinations of hyperparameters are evaluated to determine
    the optimal configuration for the model.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数的例子包括学习率、神经网络中的隐藏层数量和正则化强度。通过使用网格搜索，评估多个超参数组合，以确定模型的最佳配置。
- en: 'Grid search is a popular technique used to optimize the hyperparameters of
    machine learning models. However, it is not the only approach available, and there
    are several other alternatives that can be used to fine-tune the parameters of
    a model. Some of the most popular alternatives to grid search include:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索是一种用于优化机器学习模型超参数的流行技术。然而，它并不是唯一的方法，还有其他几种替代方案可以用来微调模型的参数。一些最受欢迎的网格搜索替代方法包括：
- en: 'Randomized grid search: In contrast to grid search, random search involves
    randomly sampling hyperparameters from a predefined range, allowing for a more
    efficient exploration of the parameter space.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机化网格搜索：与网格搜索不同，随机搜索涉及从预定义范围中随机抽样超参数，从而更高效地探索参数空间。
- en: 'Bayesian optimization: Bayesian optimization uses probability models to find
    the optimal set of hyperparameters by iteratively evaluating the model’s performance
    and updating the probability distributions of the hyperparameters.'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 贝叶斯优化：贝叶斯优化使用概率模型通过迭代评估模型的性能和更新超参数的概率分布来寻找最佳的超参数组合。
- en: 'Genetic algorithms: Genetic algorithms mimic the process of natural selection
    to find the optimal set of hyperparameters by generating a population of potential
    solutions, evaluating their performance, and selecting the fittest individuals
    for reproduction.'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遗传算法：遗传算法模拟自然选择的过程，通过生成潜在解决方案的种群，评估它们的性能，并选择最适合的个体进行繁殖，从而找到最佳的超参数组合。
- en: 'Gradient-based optimization: Gradient-based optimization involves using gradients
    to iteratively adjust the hyperparameters, with the aim of maximizing the performance
    of the model.'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于梯度的优化：基于梯度的优化涉及使用梯度迭代调整超参数，旨在最大化模型的性能。
- en: 'Ensemble-based optimization: Ensemble-based optimization involves combining
    multiple models with different hyperparameters to create a more robust and accurate
    final model.'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于集成的优化：基于集成的优化涉及将多个具有不同超参数的模型结合起来，以创建一个更稳健、更准确的最终模型。
- en: Each of these alternatives has its own advantages and disadvantages, and the
    best approach may depend on the specific problem being addressed, the size of
    the parameter space, and the computational resources available.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 每种替代方案都有其优缺点，最佳方法可能取决于所解决的具体问题、参数空间的大小以及可用的计算资源。
- en: 5\. Tips to Help You Learn Machine Learning Effectively
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 帮助你有效学习机器学习的技巧
- en: Now that we have a general understanding of the different categories of machine
    learning algorithms, let’s explore what we need to learn in order to create effective
    predictive models.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对不同类别的机器学习算法有了大致了解，让我们探索一下创建有效预测模型所需学习的内容。
- en: 5.1 Algorithms too hard to learn?
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 算法是否太难以学习？
- en: Let’s begin with some algorithms that may appear complex at first glance, leading
    you to believe that machine learning is a challenging field. However, by breaking
    down the process into the three phases (model, fitting, and tuning), you will
    be able to gain a clearer understanding.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些初看起来可能显得复杂的算法开始，这可能会让你认为机器学习是一个具有挑战性的领域。然而，通过将过程分解为三个阶段（模型、拟合和调优），你将能够获得更清晰的理解。
- en: For example, learning Support Vector Machines (SVM) can be daunting for aspiring
    data scientists due to the abundance of technical terms such as optimal hyperplane,
    unconstrained minimization, duality (prima and dual forms), Lagrange multipliers,
    Karush-Kuhn-Tucker conditions, quadratic programming, and more. However, it’s
    essential to understand that SVM is just a linear model, much like OLS regression,
    with the equation y = wX+b.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，学习支持向量机（SVM）对有志于数据科学的学生来说可能令人畏惧，因为存在大量技术术语，如最优超平面、无约束最小化、对偶性（原始和对偶形式）、拉格朗日乘数、Karush-Kuhn-Tucker条件、二次规划等。然而，重要的是要理解SVM只是一个线性模型，就像OLS回归一样，其方程为
    y = wX+b。
- en: While the various techniques mentioned above are utilized to optimize SVM using
    different methods, it is crucial to not become bogged down in the technicalities
    and instead focus on the fundamental concepts of SVM as a linear model.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然上述各种技术用于通过不同方法优化SVM，但至关重要的是不要被技术细节所困扰，而要专注于SVM作为线性模型的基本概念。
- en: If you are interested in further exploring this viewpoint, I will be writing
    an article on the topic. Please let me know in the comments.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣进一步探讨这个观点，我将会撰写一篇相关的文章。请在评论中告知我。
- en: 5.2 Understanding the Models
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 理解模型
- en: We have discussed the three types of machine learning algorithms — models, fitting
    algorithms, and tuning algorithms. In my view, it is important for aspiring data
    scientists to prioritize understanding the models over the other two steps.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了三种类型的机器学习算法——模型、拟合算法和调优算法。在我看来，有志于数据科学的学生应优先理解模型，而不是其他两个步骤。
- en: 'Looking from this standpoint, the machine learning models that are categorized
    into three main types help to understand their functioning:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，机器学习模型被分类为三种主要类型，有助于理解它们的功能：
- en: 'Distance-based models: In this type, KNN is not considered a proper model as
    the distance of a new observation is calculated directly. In LDA or QDA, the distance
    is calculated to a distribution.'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于距离的模型：在这种类型中，KNN不被视为一个适当的模型，因为新观察的距离是直接计算的。在LDA或QDA中，距离是计算到一个分布的。
- en: 'Decision tree-based models: Decision trees follow if-else rules and form a
    set of rules that can be used for decision-making.'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于决策树的模型：决策树遵循if-else规则，并形成一组用于决策的规则。
- en: 'Mathematical functions-based models: they can be less intuitive to understand.
    However, the functions are usually simple.'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于数学函数的模型：它们可能不容易理解。然而，这些函数通常很简单。
- en: 'Once you have a solid understanding of how the models work, fitting and tuning
    can be done using pre-existing packages: for fitting, the popular scikit-learn
    library offers a `model.fit` method, while for tuning, tools like Optuna provide
    efficient study optimization techniques with `study.optimize`. By focusing on
    understanding the models themselves, aspiring data scientists can better equip
    themselves for success in the field.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你对模型的工作原理有了深入理解，模型的拟合和调优可以通过现有的包来完成：对于拟合，流行的scikit-learn库提供了`model.fit`方法，而对于调优，像Optuna这样的工具提供了有效的研究优化技术，使用`study.optimize`。通过专注于理解模型本身，未来的数据科学家可以更好地为成功做好准备。
- en: 'For some individual models, if you adopt this approach, you can improve your
    understanding. I will write separate articles, but here are some examples:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些单独的模型，如果你采用这种方法，你可以提高对它们的理解。我会写单独的文章，但这里有一些示例：
- en: polynomial regression is a linear regression, after transforming the features
    by raising them to different powers.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式回归是线性回归，在将特征提高到不同的幂之后。
- en: Linear regression, ridge, LASSO, and SVR are the same model, only the underlying
    cost functions are different.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归、岭回归、LASSO和SVR是相同的模型，只是基础的成本函数不同。
- en: Linear regression, logistic regression, and SVM are the same model, only the
    underlying cost functions are different. And here you may notice that linear regression
    is a regressor whereas logistic regression and SVM are classifiers. Well, read
    the documentation of SGDClassifier or have a look at this article about [SGDClassifier](https://medium.com/code-like-a-girl/sgdclassifier-with-scikit-learn-untaught-lessons-you-need-to-know-7bf2f56c04dc).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归、逻辑回归和支持向量机（SVM）是相同的模型，只是基础的成本函数不同。在这里你可能会注意到，线性回归是回归器，而逻辑回归和SVM是分类器。嗯，查看SGDClassifier的文档或看看这篇关于[SGDClassifier](https://medium.com/code-like-a-girl/sgdclassifier-with-scikit-learn-untaught-lessons-you-need-to-know-7bf2f56c04dc)的文章吧。
- en: These [Top 10 Most Common Yet Confusing Machine Learning Model Names](/10-most-common-yet-confusing-machine-learning-model-names-e70595eef514)
    illustrate that understanding the models is not always straightforward.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这些[十大最常见却令人困惑的机器学习模型名称](/10-most-common-yet-confusing-machine-learning-model-names-e70595eef514)说明了理解模型并不总是直观的。
- en: 5.3 Visualizing the models
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 可视化模型
- en: Visualizations can be an incredibly helpful tool in understanding the models.
    When working with machine learning models, creating visualizations with simple
    datasets can help illustrate how the models are created and how they work.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化可以是理解模型的一个极其有用的工具。在处理机器学习模型时，使用简单的数据集创建可视化可以帮助说明模型是如何创建的以及它们是如何工作的。
- en: Here are some articles that I have written, and you can find the links below.
    These articles cover topics such as the visualization of linear regression, which
    can also be applied to ridge, lasso, and SVM, as well as neural networks. Additionally,
    there is an article about the implementation of KNN in Excel.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我写的一些文章，你可以在下面找到链接。这些文章涵盖了线性回归的可视化，这也适用于岭回归、LASSO和SVM，以及神经网络。此外，还有一篇关于在Excel中实现KNN的文章。
- en: Another method is to implement the models in Excel, as it can provide a tangible
    way to see the data and the model’s outputs.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是在Excel中实现这些模型，因为它可以提供一种直观的方式来查看数据和模型的输出。
- en: '[Visualization of linear regression](/linear-regression-visualized-and-better-understood-c8f7b9c69810)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性回归的可视化](/linear-regression-visualized-and-better-understood-c8f7b9c69810)'
- en: '[Visualization of neural networks](https://medium.com/geekculture/a-visual-definition-of-artificial-neural-networks-part-1-24dc5b94958)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[神经网络的可视化](https://medium.com/geekculture/a-visual-definition-of-artificial-neural-networks-part-1-24dc5b94958)'
- en: '[Visualization of Decision Tree Regressors](/decision-tree-regressor-a-visual-guide-with-scikit-learn-2aa9e01f5d7f)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[决策树回归器的可视化](/decision-tree-regressor-a-visual-guide-with-scikit-learn-2aa9e01f5d7f)'
- en: '[Nearest Neighbors Regressors — A Visual Guide](https://medium.com/towards-data-science/nearest-neighbors-regressors-a-visual-guide-78595b78072e)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[最近邻回归器 — 可视化指南](https://medium.com/towards-data-science/nearest-neighbors-regressors-a-visual-guide-78595b78072e)'
- en: Stay tuned, I will publish more!
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 敬请关注，我会发布更多内容！
- en: '![](../Images/20698b15e54fdab9c0ce87e530a62dc0.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20698b15e54fdab9c0ce87e530a62dc0.png)'
- en: '[Visualization of linear regression](/linear-regression-visualized-and-better-understood-c8f7b9c69810)
    — image by author'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[线性回归的可视化](/linear-regression-visualized-and-better-understood-c8f7b9c69810)
    — 图片由作者提供'
- en: '![](../Images/acd5a0286b2b3d43450d3e604b5d5292.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/acd5a0286b2b3d43450d3e604b5d5292.png)'
- en: '[K Nearest Neighbors Regressors with different feature scales](https://medium.com/towards-data-science/nearest-neighbors-regressors-a-visual-guide-78595b78072e)
    — image by author'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[不同特征尺度下的K最近邻回归器](https://medium.com/towards-data-science/nearest-neighbors-regressors-a-visual-guide-78595b78072e)——图片来源于作者'
- en: 5.4 Using Excel to Understand the Fitting Process
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 使用Excel理解拟合过程
- en: Understanding the fitting process can be daunting at first. However, if you
    want to learn, it’s important to start with a solid understanding of how the model
    works. One tool that can be particularly helpful in this regard is Microsoft Excel.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 理解拟合过程起初可能会感到困难。然而，如果你想学习，就需要从对模型如何工作的扎实理解开始。在这方面，Microsoft Excel是一个特别有帮助的工具。
- en: Excel is a widely used spreadsheet program that can be used to visualize and
    manipulate data. In the context of machine learning, it can be used to demonstrate
    how the fitting process works for simple models like linear regression. By using
    Excel, you can see how the algorithm is actually implemented step by step.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Excel是一个广泛使用的电子表格程序，可以用来可视化和操作数据。在机器学习的背景下，它可以用来演示简单模型如线性回归的拟合过程。通过使用Excel，你可以逐步看到算法的实际实现。
- en: One important thing to keep in mind is that Excel is not the most efficient
    tool for machine learning. While it can be a useful way to gain a basic understanding
    of the fitting process with a simple dataset.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的一点是，Excel不是进行机器学习的最有效工具。尽管它可以用来理解简单数据集的拟合过程。
- en: Using Excel to understand the fitting process can be a helpful tool for beginners
    in machine learning. It provides a simple and accessible way to visualize the
    algorithms and see how they work.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Excel理解拟合过程对机器学习初学者来说是一个有用的工具。它提供了一种简单且可访问的方式来可视化算法并查看其工作原理。
- en: I have written several articles on gradient descent for linear regression, logistic
    regression, and neural networks. You can access these articles through the following
    links.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经写了几篇关于线性回归、逻辑回归和神经网络的梯度下降文章。你可以通过以下链接访问这些文章。
- en: '[**K-Nearest neighbors** in Excel](/k-nearest-neighbors-in-excel-76edd4102a5b)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Excel中的**K-最近邻**](/k-nearest-neighbors-in-excel-76edd4102a5b)'
- en: '[Linear Regression With Gradient Descent in Excel](/linear-regression-from-scratch-in-excel-3d8192214752)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在Excel中进行梯度下降的线性回归](/linear-regression-from-scratch-in-excel-3d8192214752)'
- en: '[Logistic Regression With Gradient Descent in Excel](/logistic-regression-with-gradient-descent-in-excel-52a46c46f704)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在Excel中进行梯度下降的逻辑回归](/logistic-regression-with-gradient-descent-in-excel-52a46c46f704)'
- en: '[Neural Network Classifier from Scratch in Excel](/neural-network-from-scratch-in-excel-4774f6131cdb)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从零开始在Excel中创建神经网络分类器](/neural-network-from-scratch-in-excel-4774f6131cdb)'
- en: '[Decision Tree Regressors in Excel](/decision-tree-regressor-in-excel-2d29d16df1db)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Excel中的决策树回归器](/decision-tree-regressor-in-excel-2d29d16df1db)'
- en: '[Implementing KNN in Excel](/k-nearest-neighbors-in-excel-76edd4102a5b)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在Excel中实现KNN](/k-nearest-neighbors-in-excel-76edd4102a5b)'
- en: '[K-means from Scratch in Excel](/k-means-from-scratch-in-excel-bb60778d186e)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从零开始在Excel中实现K均值算法](/k-means-from-scratch-in-excel-bb60778d186e)'
- en: '[Neural Network with Backpropagation in Excel](https://pub.towardsai.net/building-a-neural-network-with-backpropagation-in-excel-b6c5d41b1c2)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Excel中的反向传播神经网络](https://pub.towardsai.net/building-a-neural-network-with-backpropagation-in-excel-b6c5d41b1c2)'
- en: Stay tuned more articles are coming!
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 敬请关注，更多文章即将发布！
- en: 'Additionally, if you would like to receive the corresponding Excel/Google sheet
    files, please consider supporting me on Kofi via this link: [https://ko-fi.com/s/4ddca6dff1](https://ko-fi.com/s/4ddca6dff1).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你希望获得相应的Excel/Google表格文件，请通过以下链接支持我：[https://ko-fi.com/s/4ddca6dff1](https://ko-fi.com/s/4ddca6dff1)。
- en: '![](../Images/4ab61635bd2890369cf005687b74a267.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ab61635bd2890369cf005687b74a267.png)'
- en: machine learning algorithms with Excel — image by author
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Excel进行机器学习算法——图片来源于作者
- en: 5.5 Testing with simple datasets
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 使用简单数据集进行测试
- en: To gain a comprehensive understanding of machine learning algorithms, implementing
    them from scratch can be an effective approach. However, this method can be quite
    time-consuming and may require a high level of technical proficiency. An alternative
    approach is to use pre-existing packages or libraries to create and visualize
    the output of the model with simple datasets.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面理解机器学习算法，从零开始实现它们可以是一个有效的方法。然而，这种方法可能会非常耗时，并且需要较高的技术水平。另一种方法是使用现有的包或库来创建和可视化模型的输出，特别是使用简单的数据集。
- en: By utilizing these packages, you can easily experiment with different parameters
    and test various machine learning algorithms. This approach can help you to understand
    the inner workings of the algorithms while also enabling you to quickly assess
    their effectiveness on specific datasets.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这些包，你可以轻松地实验不同的参数并测试各种机器学习算法。这种方法有助于你理解算法的内部工作原理，同时也能快速评估它们在特定数据集上的有效性。
- en: By using such datasets, it becomes easy to visualize both the inputs and outputs
    of the model. This, in turn, allows for greater insight into how the model is
    making predictions. Moreover, by changing the hyperparameters and other aspects
    of the model, we can also visualize their impact on the model’s predictions.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这样的数据集，可以轻松地可视化模型的输入和输出。这反过来允许我们更深入地了解模型如何进行预测。此外，通过更改超参数和模型的其他方面，我们还可以可视化它们对模型预测的影响。
- en: This approach can help beginners to get started with machine learning and develop
    a better understanding of how different algorithms work. It is an excellent way
    to gain practical experience and experiment with different models without spending
    too much time on implementation.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以帮助初学者入门机器学习，并对不同算法的工作原理有更好的理解。这是获得实际经验和实验不同模型的绝佳方式，而无需在实现上花费太多时间。
- en: 6\. Conclusion
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: In conclusion, machine learning can be a complex field to navigate, especially
    for aspiring data scientists. However, understanding the three main types of machine
    learning algorithms — models, fitting algorithms, and tuning algorithms — and
    categorizing them based on their objectives and complexity can help provide an
    overall understanding of how they work. By prioritizing understanding the models,
    visualizing them, and implementing them in tools like Excel, we can demystify
    the fitting and tuning process.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，机器学习可以是一个复杂的领域，特别是对于有志成为数据科学家的人员。然而，理解三种主要的机器学习算法——模型、拟合算法和调优算法——并根据它们的目标和复杂性对它们进行分类，可以帮助提供它们工作原理的整体理解。通过优先理解模型、可视化它们，并在像Excel这样的工具中实现它们，我们可以揭示拟合和调优过程的神秘面纱。
- en: 'It’s essential to keep learning about different aspects of machine learning,
    such as classification vs regression, handling missing values, and variable importance,
    to deepen our understanding of this field continuously. If you want to learn more,
    check out this article: [Overview of Supervised Machine Learning Algorithms](/overview-of-supervised-machine-learning-algorithms-a5107d036296).'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 持续学习机器学习的不同方面，例如分类与回归、处理缺失值和变量重要性，对于不断加深我们对这一领域的理解至关重要。如果你想了解更多，请查看这篇文章：[监督机器学习算法概述](/overview-of-supervised-machine-learning-algorithms-a5107d036296)。
- en: 'I write about machine learning and data science and I try to simplify complex
    concepts in a clear way. Please follow me with the link below and get full access
    to my articles: [https://medium.com/@kezhanshi/membership](https://medium.com/@angela.shi/membership)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我写关于机器学习和数据科学的文章，并尝试以清晰的方式简化复杂概念。请通过下面的链接关注我，获取我的文章的全部访问权限：[https://medium.com/@kezhanshi/membership](https://medium.com/@angela.shi/membership)
