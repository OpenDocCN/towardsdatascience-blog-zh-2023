["```py\ndf = spark.read.csv(\"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\", header=\"true\", inferSchema=\"true\")\ndisplay(df)\n```", "```py\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder\n\ncat_cols= [\"cut\", \"color\", \"clarity\"]\nstages = [] # Stages in Pipeline\n\nfor c in cat_cols:\n    stringIndexer = StringIndexer(inputCol=c, outputCol=c + \"_index\")\n    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], \\\n            outputCols=[c + \"_vec\"])    \n    stages += [stringIndexer, encoder] # Stages will be run later on\n```", "```py\nfrom pyspark.ml.feature import VectorAssembler\n\n# Transform all features into a vector\nnum_cols = [\"carat\", \"depth\", \"table\", \"x\", \"y\", \"z\"]\nassemblerInputs = [c + \"_vec\" for c in cat_cols] + num_cols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]\n\n# Create pipeline and use on dataset\npipeline = Pipeline(stages=stages)\ndf = pipeline.fit(df).transform(df)\n```", "```py\ntrain, test = df.randomSplit([0.90, 0.1], seed=123)\nprint('Train dataset count:', train.count())\nprint('Test dataset count:', test.count())\n```", "```py\nfrom pyspark.ml.feature import StandardScaler\n\n# Fit scaler to train dataset\nscaler = StandardScaler().setInputCol('features') \\\n        .setOutputCol('scaled_features')\nscaler_model = scaler.fit(train)\n\n# Scale train and test features\ntrain = scaler_model.transform(train)\ntest = scaler_model.transform(test)\n```", "```py\nfrom pyspark.ml.regression import LinearRegression\n\nlr = LinearRegression(featuresCol='scaled_features', labelCol='price')\nlr_model = lr.fit(train)\n```", "```py\ntrain_predictions = lr_model.transform(train)\ntest_predictions = lr_model.transform(test)\n```", "```py\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nevaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n                 labelCol=\"price\", metricName=\"r2\")\n\nprint(\"Train R2:\", evaluator.evaluate(train_predictions))\nprint(\"Test R2:\", evaluator.evaluate(test_predictions))\n```", "```py\nprint(\"Coefficients: \" + str(lr_model.coefficients))\nprint(\"Intercept: \" + str(lr_model.intercept))\n```", "```py\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nlist_extract = []\nfor i in df.schema['features'].metadata[\"ml_attr\"][\"attrs\"]:\n    list_extract = list_extract + df.schema['features'] \\\n                    .metadata[\"ml_attr\"][\"attrs\"][i]\nvarlist = pd.DataFrame(list_extract)\nvarlist['weight'] = varlist['idx'].apply(lambda x: coef[x])\nweights = varlist.sort_values('weight', ascending = False)\n```", "```py\ndef show_values(axs, space=.01):\n    def _single(ax):\n        for p in ax.patches:\n            _x = p.get_x() + p.get_width() + float(space)\n            _y = p.get_y() + p.get_height() - (p.get_height()*0.5)\n            value = '{:.2f}'.format(p.get_width())\n            ax.text(_x, _y, value, ha=\"left\")\n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _single(ax)\n    else:\n        _single(axs)\n\ndef plot_feature_weights(df):\n    plt.figure(figsize=(10, 8))\n    p = sns.barplot(x=df['weight'], y=df['name'])\n    show_values(p, space=0)\n\n    plt.title('Feature Weights')\n    plt.xlabel('Weight')\n    plt.ylabel('Feature Name')\n\nplot_feature_weights(weights)\n```"]