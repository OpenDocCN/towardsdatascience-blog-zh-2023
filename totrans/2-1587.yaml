- en: Nonlinear Dimension Reduction, Kernel PCA (kPCA), and Multidimensional Scaling
    — An Easy Tutorial with Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非线性维度降低、核PCA（kPCA）和多维尺度分析— Python简单教程
- en: 原文：[https://towardsdatascience.com/nonlinear-dimension-reduction-kernel-pca-kpca-and-multidimensional-scaling-an-easy-tutorial-63429ee9d0ae](https://towardsdatascience.com/nonlinear-dimension-reduction-kernel-pca-kpca-and-multidimensional-scaling-an-easy-tutorial-63429ee9d0ae)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/nonlinear-dimension-reduction-kernel-pca-kpca-and-multidimensional-scaling-an-easy-tutorial-63429ee9d0ae](https://towardsdatascience.com/nonlinear-dimension-reduction-kernel-pca-kpca-and-multidimensional-scaling-an-easy-tutorial-63429ee9d0ae)
- en: How to Flatten your Swiss-Roll without Destroying It!!
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何在不破坏瑞士卷的情况下将其展平!!
- en: '[](https://medium.com/@biman.pph?source=post_page-----63429ee9d0ae--------------------------------)[![Biman
    Chakraborty](../Images/c0bd6ee0a1b09456bd9e6aae0969da18.png)](https://medium.com/@biman.pph?source=post_page-----63429ee9d0ae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----63429ee9d0ae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----63429ee9d0ae--------------------------------)
    [Biman Chakraborty](https://medium.com/@biman.pph?source=post_page-----63429ee9d0ae--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@biman.pph?source=post_page-----63429ee9d0ae--------------------------------)[![Biman
    Chakraborty](../Images/c0bd6ee0a1b09456bd9e6aae0969da18.png)](https://medium.com/@biman.pph?source=post_page-----63429ee9d0ae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----63429ee9d0ae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----63429ee9d0ae--------------------------------)
    [Biman Chakraborty](https://medium.com/@biman.pph?source=post_page-----63429ee9d0ae--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----63429ee9d0ae--------------------------------)
    ·11 min read·Dec 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----63429ee9d0ae--------------------------------)
    ·阅读时长11分钟·2023年12月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3d4589e1299bc55b786feb6867522a9f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d4589e1299bc55b786feb6867522a9f.png)'
- en: Swiss Roll Data (Image by Author)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 瑞士卷数据（作者提供的图片）
- en: In my article on [**Principal Component Analysis (PCA) — An Easy Tutorial with
    Python**](https://medium.com/@biman.pph/principal-component-analysis-pca-an-easy-tutorial-with-python-c623b583cf29),
    I have discussed how PCA can be used to reduce the dimensionality of the data
    while reserving the distance between pairs of points as much as possible. I illustrated
    some examples with MNIST hand-written data sets and how PCA can reduce the dimensionality
    of the data from 784 to 35 and still being able to use supervised learning techniques
    with high degree of accuracy.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的文章 [**主成分分析（PCA）— Python简单教程**](https://medium.com/@biman.pph/principal-component-analysis-pca-an-easy-tutorial-with-python-c623b583cf29)
    中，我讨论了如何使用PCA来减少数据的维度，同时尽可能保留点对点之间的距离。我用MNIST手写数据集举了一些例子，说明PCA如何将数据的维度从784降到35，并且仍然能够使用高准确度的监督学习技术。
- en: In this article, we start with an example of a simple **Swiss Roll** data in
    three dimension where the true manifold of the data has a dimension 2 and we will
    start with PCA.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们以一个简单的**瑞士卷**数据的三维示例开始，其中数据的真实流形具有2维，我们将从PCA开始。
- en: 'Example: Swiss Roll Dataset'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：瑞士卷数据集
- en: Figure 1 shows a simulated Swiss Roll data with 𝑛=2000 points using `sklearn`
    library. The scatter plot shows points with different colors lying in different
    parts of the spiral.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图1显示了使用`sklearn`库模拟的瑞士卷数据，包含𝑛=2000个点。散点图显示了不同颜色的点分布在螺旋的不同部分。
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/c0f92a0c3c6453b87bc572c07eb7af4b.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0f92a0c3c6453b87bc572c07eb7af4b.png)'
- en: 'Figure 1: Three dimensional view of the Swiss Roll Data (Image by Author)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：瑞士卷数据的三维视图（作者提供的图片）
- en: We first apply PCA to this dataset and visualise the first two components in
    Figure 2\. We observe that it still retains the spiral shape of the data. The
    points in different sections of the spiral are not separable using linear boundaries
    and most of the classification methods will fail with the reduced data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先对这个数据集应用PCA，并在图2中可视化前两个主成分。我们观察到它仍然保留了数据的螺旋形状。螺旋的不同部分的点无法使用线性边界分离，大多数分类方法在降维后的数据上将会失败。
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/b485c9205012915ae3182f5198d4ff66.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b485c9205012915ae3182f5198d4ff66.png)'
- en: 'Figure 2: First two Principal Component Dimensions for Swiss Roll Data (Image
    by Author)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：瑞士卷数据的前两个主成分维度（作者提供的图片）
- en: It does not unroll the underlying 2 dimensions. Why is it so? To understand,
    let us look into Figure 3, where the Euclidean distance between two points A and
    B are shown in blue dashed line. Though these two points are in completely different
    parts of the spiral, they are close to each other in Euclidean distance.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 它没有展开潜在的二维空间。为什么会这样？为了理解这一点，我们来看一下图 3，其中两点 A 和 B 之间的欧几里得距离用蓝色虚线表示。尽管这两点位于螺旋的完全不同部分，它们在欧几里得距离上却很接近。
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/5b02b9c1235fb1f2217eaf0ef5aa89b2.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b02b9c1235fb1f2217eaf0ef5aa89b2.png)'
- en: 'Figure 3: Geodesic distance versus Euclidean distance for the Swiss Roll Data
    (Image by Author)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：瑞士卷数据的测地距离与欧几里得距离（图片由作者提供）
- en: In PCA, the Euclidean distances are preserved. However, the distance between
    these two points A and B along the spiral manifold is shown by the red line, which
    shows that these two points are far away in the manifold. The key difference is
    here that the manifold is not linear. Eucldean distance or PCA works quite well
    when we have linear manifolds. But quite often, the data is not on linear manifolds
    as is evident on this example dataset Other image data like even hand-written
    digits data are some good examples of non-linear manifolds of high dimensional
    data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PCA 中，欧几里得距离被保留。然而，这两点 A 和 B 沿螺旋流形的距离由红线显示，表明这两点在流形上相距较远。关键区别在于流形不是线性的。当我们处理线性流形时，欧几里得距离或
    PCA 的效果非常好。但数据往往不在直线流形上，如这个示例数据集所示。其他图像数据，如手写数字数据，是高维数据中非线性流形的好例子。
- en: We need to define the distances differently to capture such differences. But
    before that let us first discuss how one can construct the principal components
    using the distances.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要以不同的方式定义距离以捕捉这种差异。但在此之前，我们先讨论一下如何利用距离构建主成分。
- en: 'Principal Components: Mathematical Formulation'
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分：数学公式
- en: Given a 𝑛×𝑝 data matrix 𝐗, the *principal component directions* are defined
    to be the 𝑝-dimensional orthonormal vectors along which the sample variance of
    𝐗 is successively maximized. For centered 𝐗, that is the sum of each column of
    𝐗 is 0, the 𝑘-th principal component direction is
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个 𝑛×𝑝 数据矩阵 𝐗，*主成分方向* 被定义为在这些方向上，𝐗 的样本方差依次被最大化的 𝑝 维正交向量。对于中心化的 𝐗，即 𝐗 的每一列之和为
    0，第 𝑘 个主成分方向是
- en: '![](../Images/b02776b66494e2f6bf72d88f9f2aee28.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b02776b66494e2f6bf72d88f9f2aee28.png)'
- en: The 𝑛-dimensional vector 𝐗𝑣_𝑘 is called the 𝑘-th *principal component score*
    of 𝐗 and 𝑢_𝑘=(𝐗𝑣_𝑘)/𝑑_k is the normalized 𝑘-th principal component score, with
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 𝑛 维向量 𝐗𝑣_𝑘 称为 𝐗 的第 𝑘 个 *主成分得分*，且 𝑢_𝑘=(𝐗𝑣_𝑘)/𝑑_k 是归一化的第 𝑘 个主成分得分，其公式为
- en: '![](../Images/8b1838ca409f0be310882ec2147e1a82.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b1838ca409f0be310882ec2147e1a82.png)'
- en: The quantity *d²_k/n* is the amount of variance explained by 𝑣_𝑘.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 数量 *d²_k/n* 是 𝑣_𝑘 解释的方差量。
- en: The *singular value decomposition* of 𝐗 as 𝐗 = 𝑈𝐷𝑉^⊤ describes all the prinicipal
    component scores and variances with 𝑈 being a 𝑛×𝑝 dimensional matrix with columns
    𝑢_1,𝑢_2,…,𝑢_𝑝, 𝑉 being a 𝑝×*p* dimensional matrix with columns 𝑣_1,𝑣_2,…,𝑣_𝑝 and
    𝐷 is 𝑝×𝑝 diagonal matrix with diagonal elements given by 𝑑_1,𝑑_2,…,𝑑_𝑝.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 𝐗 的 *奇异值分解* 𝐗 = 𝑈𝐷𝑉^⊤ 描述了所有主成分得分和方差，其中 𝑈 是一个 𝑛×𝑝 维矩阵，列为 𝑢_1,𝑢_2,…,𝑢_𝑝，𝑉 是一个
    𝑝×*p* 维矩阵，列为 𝑣_1,𝑣_2,…,𝑣_𝑝，𝐷 是一个 𝑝×𝑝 的对角矩阵，对角元素为 𝑑_1,𝑑_2,…,𝑑_𝑝。
- en: Let us consider the first 𝑘 principal components scores 𝐗𝑣_1=𝑑_1𝑢_1, …, 𝐗𝑣_𝑘=𝑑_𝑘𝑢_𝑘
    as the new feature vectors. Then we can write this as 𝐙=𝐗𝑉_𝑘=(𝑈𝐷)_𝑘, that is,
    the first 𝑘 columns of the matrix (𝑈𝐷) and think of **Z** as are new low-dimensional
    representation for 𝐗.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑前 𝑘 个主成分得分 𝐗𝑣_1=𝑑_1𝑢_1, …, 𝐗𝑣_𝑘=𝑑_𝑘𝑢_𝑘 作为新的特征向量。然后我们可以将其写成 𝐙=𝐗𝑉_𝑘=(𝑈𝐷)_𝑘，也就是矩阵
    (𝑈𝐷) 的前 𝑘 列，并将 **Z** 视为 𝐗 的新低维表示。
- en: The rows 𝑧_1,…,𝑧_𝑛 of 𝐙 are the data points in this new low-dimensional representation.
    We have argued earlier that
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 𝐙 的行 𝑧_1,…,𝑧_𝑛 是这个新低维表示中的数据点。我们之前讨论过
- en: '![](../Images/2def003874b70a1bf1da09cf5ff20bab.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2def003874b70a1bf1da09cf5ff20bab.png)'
- en: The Euclidean distance between the 𝑖 and 𝑗 points in the lower dimensional represnetation
    is approximately equal to the original Euclidean distance between these two points.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在低维表示中，𝑖 和 𝑗 点之间的欧几里得距离大致等于这两点之间的原始欧几里得距离。
- en: The Inner-Product Matrix
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内积矩阵
- en: The 𝑛×𝑛 dimensional matrix 𝐗𝐗^⊤ is known as the *inner-product matrix* whose
    (𝑖,𝑗)-th element is given by *𝑥_i*^⊤*x_j*, the inner product between the 𝑖-th
    and the 𝑗-th rows of the matrix 𝐗. From above, we have
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 𝑛×𝑛 维矩阵 𝐗𝐗^⊤ 被称为 *内积矩阵*，其 (𝑖,𝑗) 元素由 *𝑥_i*^⊤*x_j* 给出，即矩阵 𝐗 的第 𝑖 行和第 𝑗 行之间的内积。从上面我们可以得出
- en: '![](../Images/41e964d69a8d8b80a9a803a87cf9788f.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41e964d69a8d8b80a9a803a87cf9788f.png)'
- en: Thus we can write,
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们可以写道，
- en: '![](../Images/348ef807fdd5f1fcd20c9b1cac3d53cd.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/348ef807fdd5f1fcd20c9b1cac3d53cd.png)'
- en: This is called an *eigendecomposition* of 𝐗𝐗^⊤ because the columns of 𝑈 are
    eigenvectors of 𝐗𝐗^⊤. From this representation we can simply compute the eigendecomposition
    or *factorize* the inner product matrix 𝐗𝐗^⊤, and then the principal component
    scores are given by the columns of 𝑈𝐷, that is, 𝑑_𝑗𝑢_𝑗, 𝑗=1,…,𝑝. This shows that
    principal components scores can be computed if only the inner product matrix is
    given instead of the original data points.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这称为 𝐗𝐗^⊤ 的 *特征分解*，因为 𝑈 的列是 𝐗𝐗^⊤ 的特征向量。从这个表示中，我们可以简单地计算特征分解或 *分解* 内积矩阵 𝐗𝐗^⊤，然后主成分得分由
    𝑈𝐷 的列给出，即 𝑑_𝑗𝑢_𝑗，𝑗=1,…,𝑝。这表明，如果仅给出内积矩阵而不是原始数据点，则可以计算主成分得分。
- en: Low Dimensional Representation from Distances Only
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 仅从距离中获得的低维表示
- en: Suppose that we only have the distances between the data points instead of the
    original data. That is, we have the Euclidean distances
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们只有数据点之间的距离，而没有原始数据。也就是说，我们有欧几里得距离。
- en: '![](../Images/2ff5e219a62a1444a5dbe10678df0de8.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ff5e219a62a1444a5dbe10678df0de8.png)'
- en: or all 𝑖 and 𝑗. Can we still recover the principal component directions from
    these distances?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 或者所有 𝑖 和 𝑗。我们还能从这些距离中恢复主成分方向吗？
- en: Let us first define a 𝑛×𝑛 dimensional distance matrix Δ with (𝑖,𝑗)-th element
    given by Δ_𝑖𝑗. We can recover the inner product matrix 𝐵=𝐗𝐗^⊤ from the distance
    matrix Δ.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先定义一个 𝑛×𝑛 维的距离矩阵 Δ，其中 (𝑖,𝑗) 元素由 Δ_𝑖𝑗 给出。我们可以从距离矩阵 Δ 恢复内积矩阵 𝐵=𝐗𝐗^⊤。
- en: Create the 𝑛×𝑛 matrix 𝐴 with its (𝑖,𝑗)-th element given by
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 𝑛×𝑛 矩阵 𝐴，其 (𝑖,𝑗) 元素由
- en: '![](../Images/94081d5c34cd40576399f4fbb621f06f.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94081d5c34cd40576399f4fbb621f06f.png)'
- en: 2\. Double center 𝐴, that is, center both the columns and rows of 𝐴 to recover
    the matrix 𝐵 by using the transformation *B* = (*I* — *M*)*A*(*I* — *M*) where
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 对 𝐴 进行双重中心化，即同时中心化 𝐴 的列和行，通过使用变换 *B* = (*I* — *M*)*A*(*I* — *M*) 来恢复矩阵 𝐵，其中
- en: '![](../Images/245f14fda5cd995dadec21369c009103.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/245f14fda5cd995dadec21369c009103.png)'
- en: Kernel PCA
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核主成分分析
- en: Kernel PCA simply mimics this procedure by replacing the inner product matrix
    𝐵 by the kernel matrix 𝐾=((𝐾_𝑖𝑗)), where 𝐾_𝑖𝑗=<𝜙(𝑥_𝑖),𝜙(𝑥_𝑗)>, the inner-product
    between the feature vectors 𝜙(𝑥_𝑖) and 𝜙(𝑥_𝑗). Here 𝜙 is a nonlinear map from
    ℝ^𝑝 → 𝐹, a feature space of arbitrary dimension. The idea is similar to the kernels
    in support vector machines (SVM) for classification problems. We are projecting
    the observations to a higher-dimensional space and then obtaining the principal
    components in that space. We can simply define, 𝐾_𝑖𝑗=Φ(𝑥_𝑖,𝑥_𝑗), where for the
    radial kernel,
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 核主成分分析通过用核矩阵 𝐾=((𝐾_𝑖𝑗)) 替换内积矩阵 𝐵 来简单模拟此过程，其中 𝐾_𝑖𝑗=<𝜙(𝑥_𝑖),𝜙(𝑥_𝑗)>，即特征向量 𝜙(𝑥_𝑖)
    和 𝜙(𝑥_𝑗) 之间的内积。这里 𝜙 是从 ℝ^𝑝 → 𝐹 的非线性映射，𝐹 是任意维度的特征空间。这个想法类似于用于分类问题的支持向量机（SVM）中的核函数。我们将观察值投影到一个更高维的空间中，然后在该空间中获得主成分。我们可以简单地定义
    𝐾_𝑖𝑗=Φ(𝑥_𝑖,𝑥_𝑗)，对于径向基核，
- en: '![](../Images/dc02b10e3fbe8bb019f55ca574647ae4.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc02b10e3fbe8bb019f55ca574647ae4.png)'
- en: and for the polynomial kernel,
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多项式核，
- en: '![](../Images/4649f21b400fb488eb04b8c50bbe8c98.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4649f21b400fb488eb04b8c50bbe8c98.png)'
- en: where 𝛾, 𝑐 and 𝑑 are the parameters of the respective kernel functions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 𝛾、𝑐 和 𝑑 是相应核函数的参数。
- en: 'The algorithm can be described as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 算法可以描述如下：
- en: Define the 𝑛×𝑛 kernel inner product matrix 𝐾 as 𝐾=((Φ(𝑥_𝑖,𝑥_𝑗)).
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 𝑛×𝑛 核内积矩阵 𝐾 定义为 𝐾=((Φ(𝑥_𝑖,𝑥_𝑗))。
- en: Use eigendecomposition of 𝐾 to extract the eigenvalues and the eigenvectors
    of 𝐾.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 𝐾 的特征分解来提取 𝐾 的特征值和特征向量。
- en: The eigenvectors of 𝐾 will give the principal component scores.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 𝐾 的特征向量将给出主成分得分。
- en: This is a nonlinear dimension reduction and we can illustrate the use of kernel
    PCA for our **Swiss Roll** data discussed in the previous example.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种非线性降维，我们可以通过在前面示例中讨论的 **瑞士卷** 数据来说明核主成分分析的使用。
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/92d3b95533f338cfa745aa90c02763ee.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92d3b95533f338cfa745aa90c02763ee.png)'
- en: 'Figure 4: Kernel PCA Dimensions for the Swiss Roll Data (Image by Author)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：瑞士卷数据的核主成分分析维度（作者提供的图片）
- en: In the above, we have used an `rbf` kernel with 𝛾 = 0.002\. Though the results
    improved from PCA, it still does not unroll the swiss roll, but picks the manifold
    very well.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述内容中，我们使用了一个`rbf`核，𝛾 = 0.002。尽管结果相比主成分分析有所改进，但它仍然没有展开瑞士卷数据，而是很好地捕捉了流形。
- en: We illustrate the kernel PCA with a different simulated data set below.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下面用不同的模拟数据集展示了核主成分分析。
- en: '![](../Images/afb45d342afc007dc080d3b4c5129506.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afb45d342afc007dc080d3b4c5129506.png)'
- en: 'Figure 5: Kernel PCA dimensions for the simulated data on the left. (Image
    by Author)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：左侧模拟数据的核PCA维度。（作者提供的图片）
- en: In the left, we have the simulated data with 3 concentric circles having uniform
    distribution with radius 1.0, 2.8 and 5.0 respectively. On the right hand side,
    we plot the kernel PCA components with `rbf` kernel and 𝛾=0.3\. We observe a nice
    separation of the three clusters of the data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧，我们有一个包含半径分别为1.0、2.8和5.0的3个同心圆的模拟数据，分布均匀。在右侧，我们绘制了使用`rbf`核和𝛾=0.3的核PCA组件。我们观察到数据的三个簇之间有很好的分离。
- en: Multi-Dimensional Scaling
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多维缩放
- en: We have discussed in our [article on PCA](https://medium.com/@biman.pph/principal-component-analysis-pca-an-easy-tutorial-with-python-c623b583cf29)
    that it tries to preserve the distance between the observations in a lower dimensional
    representation. In other words, if 𝑧_1,𝑧_2,…,𝑧_𝑛 are the lower dimensional representation
    of 𝑥_1,𝑥_2,…,𝑥_𝑛, then PCA minimizes
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[关于PCA的文章](https://medium.com/@biman.pph/principal-component-analysis-pca-an-easy-tutorial-with-python-c623b583cf29)中讨论过，它试图在低维表示中保留观察之间的距离。换句话说，如果
    𝑧_1,𝑧_2,…,𝑧_𝑛 是 𝑥_1,𝑥_2,…,𝑥_𝑛 的低维表示，那么PCA最小化
- en: '![](../Images/406a8f6936d372b3d893200d8eceb320.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/406a8f6936d372b3d893200d8eceb320.png)'
- en: 'We now generalize this idea by defining a *stress* function as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在通过定义一个*压力*函数来推广这个想法，如下所示：
- en: '![](../Images/9275d4a9db046d34836840fed3fe2499.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9275d4a9db046d34836840fed3fe2499.png)'
- en: where 𝑑_𝑖𝑗 is a distance between 𝑥_𝑖 and 𝑥_𝑗. Usually, we chose Euclidean distances,
    but other distances can be used as well.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 𝑑_𝑖𝑗 是 𝑥_𝑖 和 𝑥_𝑗 之间的距离。通常，我们选择欧几里得距离，但也可以使用其他距离。
- en: '**Multidimensional scaling** seeks values 𝑧_1,𝑧_2,…,𝑧_𝑛∈ℝ^𝑘 to minimize the
    *stress function*, 𝑆_𝑀(𝑧_1,𝑧_2,…,𝑧_𝑛).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**多维缩放**寻求值 𝑧_1,𝑧_2,…,𝑧_𝑛∈ℝ^𝑘，以最小化*压力函数* 𝑆_𝑀(𝑧_1,𝑧_2,…,𝑧_𝑛)。'
- en: This is known as *least squares* or *Kruskal–Shephard scaling*. The idea is
    to find a lower-dimensional representation of the data that preserves the pairwise
    distances as well as possible. Notice that the approximation is in terms of the
    distances rather than squared distances.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为*最小二乘*或*Kruskal–Shephard缩放*。这个想法是找到一个低维的数据表示，尽可能保留成对的距离。请注意，这种近似是基于距离而不是平方距离的。
- en: Let us look into its implementation for the **Swiss Roll** data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它在**瑞士卷**数据上的实现。
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/eeb6759c19e5ca3e289ddd9a0f4e497c.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eeb6759c19e5ca3e289ddd9a0f4e497c.png)'
- en: 'Figure 6: First 2 dimensions of the classical multidimensional scaling. (Image
    by Author)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：经典多维缩放的前两个维度。（作者提供的图片）
- en: We observe that the results are very similar to the kernel PCA.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到结果与核主成分分析（kernel PCA）非常相似。
- en: So far, we have not gone beyond Euclidian distances. But we mentioned earlier
    that in the Swiss roll data, the Euclidian distances are not ideal.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们还没有超越欧几里得距离。但我们之前提到，在瑞士卷数据中，欧几里得距离并不理想。
- en: There is a class of methods which construct a fancier distance 𝑑_𝑖𝑗 between
    high-dimensional points 𝑥_1,…,𝑥_𝑛∈ℝ^𝑝, and then they feed these 𝑑_𝑖𝑗 through multidimensional
    scaling to get a low-dimensional representation 𝑧_1,…,𝑧_𝑛∈ℝ^𝑘. In this case, we
    don’t just get principal component scores, and our low-dimensional representation
    can end up being a *nonlinear function* of the data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 有一类方法构造一个更复杂的距离 𝑑_𝑖𝑗 来度量高维点 𝑥_1,…,𝑥_𝑛∈ℝ^𝑝 之间的距离，然后将这些 𝑑_𝑖𝑗 通过多维缩放处理，以获得低维表示
    𝑧_1,…,𝑧_𝑛∈ℝ^𝑘。这样，我们不仅得到主成分得分，我们的低维表示可能最终成为数据的*非线性函数*。
- en: Tangent distance
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 切向距离
- en: '*Tangent distance* is an example of a fancier metric that we can run through
    multidimensional scaling (though used elsewhere too).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*切向距离*是一个我们可以通过多维缩放（虽然也用于其他地方）运行的更复杂的度量。'
- en: A motivating example is the *handwritten digits data* we used earlier. Here,
    we have *16 \times 16* images, treated as points 𝑥_𝑖∈ℝ²⁵⁶ (i.e., they are unraveled
    into vectors). If we take, e.g., a “3” and *rotate* it through a small angle,
    we would like for the rotated image to be considered close to the original image.
    This is not neccessarily true of Euclidean distance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一个激励示例是我们之前使用的 *手写数字数据*。这里，我们有 *16 \times 16* 的图像，将其视为点 𝑥_𝑖∈ℝ²⁵⁶（即，它们被展开成向量）。例如，如果我们取一个“3”并
    *旋转* 它一个小角度，我们希望旋转后的图像被认为接近原始图像。这在欧几里得距离中不一定成立。
- en: '![](../Images/18d00ffbd85d32f0aed9fe8fc806839b.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18d00ffbd85d32f0aed9fe8fc806839b.png)'
- en: 'Figure 7: Original images of “3” and a rotated image of “3” (Image by Author)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：原始“3”和旋转后的“3”图像（图片由作者提供）
- en: We could define Δ_𝑖𝑗^rotation to be the shortest Euclidean distance between
    a rotated version of 𝑥_𝑖 and rotated version of 𝑥_𝑗. However, you can immediately
    see that there is a problem in rotating the digits “6” and “9”.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义 Δ_𝑖𝑗^rotation 为旋转后的 𝑥_𝑖 和旋转后的 𝑥_𝑗 之间的最短欧几里得距离。然而，你可以立即发现旋转数字“6”和“9”存在问题。
- en: We need something easier to calculate, and that restricts attention to *small
    rotations*. It helps to think of a set of rotations of an image as defining a
    curve in ℝ^𝑝 — an image 𝑥_𝑖 is a point in ℝ^𝑝, and as we rotate it in either directions,
    we get a curve.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一些更容易计算的东西，并且将注意力限制在 *小旋转* 上。可以将图像的旋转集视为定义了 ℝ^𝑝 中的曲线——一个图像 𝑥_𝑖 是 ℝ^𝑝 中的一个点，当我们在任意方向上旋转它时，我们得到一条曲线。
- en: '**Tangent distance** Δ_𝑖𝑗^tangent is defined by first computing the tangent
    line to each curve at the observed image, and then using the shortest Euclidean
    distance between tangent lines.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**切线距离** Δ_𝑖𝑗^tangent 通过首先计算每条曲线在观察到的图像处的切线，然后使用切线之间的最短欧几里得距离来定义。'
- en: Isometric Feature Mapping (Isomap)
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 等距特征映射（Isomap）
- en: '*Isometric feature mapping* (Isomap) learns structure in a more general setting
    to define distances. The basic idea is to construct a graph 𝐺=(𝑉,𝐸), i.e., construct
    edges 𝐸 between vertices 𝑉={1,…,𝑛}, based on the structure between 𝑥_1,…,𝑥_𝑛∈ℝ^𝑝
    . Then we define a graph distance Δ_𝑖𝑗^Isomap between 𝑥_𝑖 and 𝑥_𝑗, and use multidimensional
    scaling for our low-dimensional representation'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*等距特征映射*（Isomap）在更一般的设置中学习结构以定义距离。基本思想是构造一个图 𝐺=(𝑉,𝐸)，即在顶点 𝑉={1,…,𝑛} 之间构造边 𝐸，基于
    𝑥_1,…,𝑥_𝑛∈ℝ^𝑝 之间的结构。然后我们定义 𝑥_𝑖 和 𝑥_𝑗 之间的图距离 Δ_𝑖𝑗^Isomap，并使用多维缩放进行低维表示。'
- en: '**Constructing the graph**: for each pair 𝑖,𝑗, we connect 𝑖,𝑗 with an edge
    if either:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**构造图**：对于每对 𝑖,𝑗，如果满足以下任一条件，我们将 𝑖,𝑗 用边连接：'
- en: 𝑥_𝑖 is one of 𝑥_𝑗’s 𝑚 nearest neighbors, or
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 𝑥_𝑖 是 𝑥_𝑗 的 𝑚 个最近邻之一，或者
- en: 𝑥_𝑗 is one of 𝑥_𝑖’s 𝑚 nearest neighbors The weight of this edge 𝑒 = {𝑖,𝑗} is
    then 𝑤_𝑒=‖𝑥_𝑖−𝑥_𝑗‖.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 𝑥_𝑗 是 𝑥_𝑖 的 𝑚 个最近邻之一。这条边 𝑒 = {𝑖,𝑗} 的权重为 𝑤_𝑒=‖𝑥_𝑖−𝑥_𝑗‖。
- en: '**Defining graph distances**: now that we have built a graph, i.e., we have
    built an edge set 𝐸, we define the graph distance Δ_𝑖𝑗^Isomap between 𝑥_𝑖 and
    𝑥_𝑗 to be the shortest path in our graph from 𝑖 to 𝑗:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义图距离**：现在我们已经构建了图，即我们已经构建了边集 𝐸，我们定义图距离 Δ_𝑖𝑗^Isomap 为从 𝑖 到 𝑗 的最短路径：'
- en: '![](../Images/cc23df2584e0c951fd57658a7f031969.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc23df2584e0c951fd57658a7f031969.png)'
- en: (This can be computed by, e.g., *Dijkstra’s algorithm* or *Floyd’s algorithm*)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: （这可以通过例如 *迪杰斯特拉算法* 或 *弗洛伊德算法* 计算）
- en: Let us now look into its implementation for the **Swiss roll** data.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在深入研究其在 **瑞士卷** 数据上的实现。
- en: '[PRE5]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/561bb0d2fdfe53172bb154e1baa0330b.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/561bb0d2fdfe53172bb154e1baa0330b.png)'
- en: 'Figure 8: Two dimensional representation of the isomap of the Swiss Roll data.
    (Image by Author)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：瑞士卷数据的二维表示。（图片由作者提供）
- en: With the number of neighbours 𝑚=7, the multodimemsional scaling with the **isomap**
    distances now unrolls the **Swiss roll** data.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在邻居数量 𝑚=7 的情况下，多维缩放与 **isomap** 距离现在展开了 **瑞士卷** 数据。
- en: Local Linear Embedding
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 局部线性嵌入
- en: Another nonlinear dimension reduction method is **Local linear embedding** (LLE),
    which is a similar method in spirit but its details are very different. It doesn’t
    use multidimensional scaling.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种非线性维度约减方法是 **局部线性嵌入**（LLE），它在精神上类似但细节却大相径庭。它不使用多维缩放。
- en: 'The basic idea has two steps:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想分为两个步骤：
- en: Learn a bunch of local approximations to the structure between 𝑥_1,…,𝑥_𝑛∈ℝ^𝑝
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习一组局部近似来描述 𝑥_1,…,𝑥_𝑛∈ℝ^𝑝 之间的结构
- en: Learn a low-dimensional representation 𝑧_1,…,𝑧_𝑛∈ℝ^𝑘 that best matches these
    local approximations
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习一个低维表示 𝑧_1,…,𝑧_𝑛∈ℝ^𝑘，最好与这些局部近似匹配
- en: What is meant by such local approximations? We simply try to predict each 𝑥_𝑖
    by a linear function of nearby points 𝑥_𝑗 (hence the name local linear embedding).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是局部近似？我们只是尝试用附近点𝑥_𝑗的线性函数来预测每个𝑥_𝑖（因此得名局部线性嵌入）。
- en: For each 𝑥_𝑖, we first find its 𝑚 nearest neighbours, and collect their indices
    as N(𝑖). Then we build a weight vector 𝑤_𝑖∈ℝ^𝑛, setting 𝑤_𝑖𝑗=0 for 𝑗∉N(𝑖) and
    setting 𝑤_𝑖𝑗 for 𝑗∈N(𝑖) by minimizing
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个𝑥_𝑖，我们首先找到它的𝑚个最近邻，并将它们的索引收集为N(𝑖)。然后我们构建一个权重向量𝑤_𝑖∈ℝ^𝑛，设置𝑤_𝑖𝑗=0（当𝑗∉N(𝑖)时），并通过最小化来设置𝑤_𝑖𝑗（当𝑗∈N(𝑖)时）。
- en: '![](../Images/a4923ea91402950b401d3d06a3d26653.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4923ea91402950b401d3d06a3d26653.png)'
- en: Finally, we take these weights 𝑤_1,…,𝑤_𝑛∈ℝ^𝑛 and we fit the low-dimensional
    representation 𝑧_1,…,𝑧_𝑛∈ℝ^𝑘, by minimizing
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们取这些权重𝑤_1,…,𝑤_𝑛∈ℝ^𝑛，并通过最小化来拟合低维表示𝑧_1,…,𝑧_𝑛∈ℝ^𝑘。
- en: '![](../Images/9eefa5853f41c28e0d40e8a4b2f49bd6.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9eefa5853f41c28e0d40e8a4b2f49bd6.png)'
- en: We again illustrate the use of *Local Linear Embedding* using the Swiss roll
    data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次使用*局部线性嵌入*（Local Linear Embedding）来说明如何处理瑞士卷数据。
- en: '[PRE6]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/72c3d9c2a1d5d397d5364b7eed5e0827.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72c3d9c2a1d5d397d5364b7eed5e0827.png)'
- en: 'Figure 9: Local Linear Embedding dimensions for the Swiss Roll data. (Image
    by Author)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：瑞士卷数据的局部线性嵌入维度。（作者提供的图像）
- en: '*Local Linear Embedding* also yielded better dimension reduction than kernel
    PCA or classical MDS, though it is not as good as *Isomap*.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*局部线性嵌入*的降维效果优于核PCA或经典MDS，尽管不如*Isomap*。'
- en: In this article, we have learnt a few nonlinear dimension reduction technique
    by generalizing the concepts from PCA. However, there is no single method which
    works best for all sort of data for dimension reduction. Depending on the nature
    of the data, we should decide about the dimension reduction technique to be used.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们通过对PCA概念的推广学习了一些非线性降维技术。然而，没有一种单一的方法可以适用于所有类型的数据降维。根据数据的性质，我们应选择合适的降维技术。
- en: Hope, you have enjoyed the article!!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你喜欢这篇文章！！
- en: For consulting on any data science problems, contact [biman.pph@gmail.com](mailto:biman.pph@gmail.com)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 有关数据科学问题的咨询，请联系 [biman.pph@gmail.com](mailto:biman.pph@gmail.com)
