- en: Nonlinear Dimension Reduction, Kernel PCA (kPCA), and Multidimensional Scaling
    â€” An Easy Tutorial with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/nonlinear-dimension-reduction-kernel-pca-kpca-and-multidimensional-scaling-an-easy-tutorial-63429ee9d0ae](https://towardsdatascience.com/nonlinear-dimension-reduction-kernel-pca-kpca-and-multidimensional-scaling-an-easy-tutorial-63429ee9d0ae)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to Flatten your Swiss-Roll without Destroying It!!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@biman.pph?source=post_page-----63429ee9d0ae--------------------------------)[![Biman
    Chakraborty](../Images/c0bd6ee0a1b09456bd9e6aae0969da18.png)](https://medium.com/@biman.pph?source=post_page-----63429ee9d0ae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----63429ee9d0ae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----63429ee9d0ae--------------------------------)
    [Biman Chakraborty](https://medium.com/@biman.pph?source=post_page-----63429ee9d0ae--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----63429ee9d0ae--------------------------------)
    Â·11 min readÂ·Dec 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d4589e1299bc55b786feb6867522a9f.png)'
  prefs: []
  type: TYPE_IMG
- en: Swiss Roll Data (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: In my article on [**Principal Component Analysis (PCA) â€” An Easy Tutorial with
    Python**](https://medium.com/@biman.pph/principal-component-analysis-pca-an-easy-tutorial-with-python-c623b583cf29),
    I have discussed how PCA can be used to reduce the dimensionality of the data
    while reserving the distance between pairs of points as much as possible. I illustrated
    some examples with MNIST hand-written data sets and how PCA can reduce the dimensionality
    of the data from 784 to 35 and still being able to use supervised learning techniques
    with high degree of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we start with an example of a simple **Swiss Roll** data in
    three dimension where the true manifold of the data has a dimension 2 and we will
    start with PCA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Swiss Roll Dataset'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Figure 1 shows a simulated Swiss Roll data with ğ‘›=2000 points using `sklearn`
    library. The scatter plot shows points with different colors lying in different
    parts of the spiral.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c0f92a0c3c6453b87bc572c07eb7af4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Three dimensional view of the Swiss Roll Data (Image by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: We first apply PCA to this dataset and visualise the first two components in
    Figure 2\. We observe that it still retains the spiral shape of the data. The
    points in different sections of the spiral are not separable using linear boundaries
    and most of the classification methods will fail with the reduced data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b485c9205012915ae3182f5198d4ff66.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: First two Principal Component Dimensions for Swiss Roll Data (Image
    by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: It does not unroll the underlying 2 dimensions. Why is it so? To understand,
    let us look into Figure 3, where the Euclidean distance between two points A and
    B are shown in blue dashed line. Though these two points are in completely different
    parts of the spiral, they are close to each other in Euclidean distance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5b02b9c1235fb1f2217eaf0ef5aa89b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Geodesic distance versus Euclidean distance for the Swiss Roll Data
    (Image by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: In PCA, the Euclidean distances are preserved. However, the distance between
    these two points A and B along the spiral manifold is shown by the red line, which
    shows that these two points are far away in the manifold. The key difference is
    here that the manifold is not linear. Eucldean distance or PCA works quite well
    when we have linear manifolds. But quite often, the data is not on linear manifolds
    as is evident on this example dataset Other image data like even hand-written
    digits data are some good examples of non-linear manifolds of high dimensional
    data.
  prefs: []
  type: TYPE_NORMAL
- en: We need to define the distances differently to capture such differences. But
    before that let us first discuss how one can construct the principal components
    using the distances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Principal Components: Mathematical Formulation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given a ğ‘›Ã—ğ‘ data matrix ğ—, the *principal component directions* are defined
    to be the ğ‘-dimensional orthonormal vectors along which the sample variance of
    ğ— is successively maximized. For centered ğ—, that is the sum of each column of
    ğ— is 0, the ğ‘˜-th principal component direction is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b02776b66494e2f6bf72d88f9f2aee28.png)'
  prefs: []
  type: TYPE_IMG
- en: The ğ‘›-dimensional vector ğ—ğ‘£_ğ‘˜ is called the ğ‘˜-th *principal component score*
    of ğ— and ğ‘¢_ğ‘˜=(ğ—ğ‘£_ğ‘˜)/ğ‘‘_k is the normalized ğ‘˜-th principal component score, with
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b1838ca409f0be310882ec2147e1a82.png)'
  prefs: []
  type: TYPE_IMG
- en: The quantity *dÂ²_k/n* is the amount of variance explained by ğ‘£_ğ‘˜.
  prefs: []
  type: TYPE_NORMAL
- en: The *singular value decomposition* of ğ— as ğ— = ğ‘ˆğ·ğ‘‰^âŠ¤ describes all the prinicipal
    component scores and variances with ğ‘ˆ being a ğ‘›Ã—ğ‘ dimensional matrix with columns
    ğ‘¢_1,ğ‘¢_2,â€¦,ğ‘¢_ğ‘, ğ‘‰ being a ğ‘Ã—*p* dimensional matrix with columns ğ‘£_1,ğ‘£_2,â€¦,ğ‘£_ğ‘ and
    ğ· is ğ‘Ã—ğ‘ diagonal matrix with diagonal elements given by ğ‘‘_1,ğ‘‘_2,â€¦,ğ‘‘_ğ‘.
  prefs: []
  type: TYPE_NORMAL
- en: Let us consider the first ğ‘˜ principal components scores ğ—ğ‘£_1=ğ‘‘_1ğ‘¢_1, â€¦, ğ—ğ‘£_ğ‘˜=ğ‘‘_ğ‘˜ğ‘¢_ğ‘˜
    as the new feature vectors. Then we can write this as ğ™=ğ—ğ‘‰_ğ‘˜=(ğ‘ˆğ·)_ğ‘˜, that is,
    the first ğ‘˜ columns of the matrix (ğ‘ˆğ·) and think of **Z** as are new low-dimensional
    representation for ğ—.
  prefs: []
  type: TYPE_NORMAL
- en: The rows ğ‘§_1,â€¦,ğ‘§_ğ‘› of ğ™ are the data points in this new low-dimensional representation.
    We have argued earlier that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2def003874b70a1bf1da09cf5ff20bab.png)'
  prefs: []
  type: TYPE_IMG
- en: The Euclidean distance between the ğ‘– and ğ‘— points in the lower dimensional represnetation
    is approximately equal to the original Euclidean distance between these two points.
  prefs: []
  type: TYPE_NORMAL
- en: The Inner-Product Matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ğ‘›Ã—ğ‘› dimensional matrix ğ—ğ—^âŠ¤ is known as the *inner-product matrix* whose
    (ğ‘–,ğ‘—)-th element is given by *ğ‘¥_i*^âŠ¤*x_j*, the inner product between the ğ‘–-th
    and the ğ‘—-th rows of the matrix ğ—. From above, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41e964d69a8d8b80a9a803a87cf9788f.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus we can write,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/348ef807fdd5f1fcd20c9b1cac3d53cd.png)'
  prefs: []
  type: TYPE_IMG
- en: This is called an *eigendecomposition* of ğ—ğ—^âŠ¤ because the columns of ğ‘ˆ are
    eigenvectors of ğ—ğ—^âŠ¤. From this representation we can simply compute the eigendecomposition
    or *factorize* the inner product matrix ğ—ğ—^âŠ¤, and then the principal component
    scores are given by the columns of ğ‘ˆğ·, that is, ğ‘‘_ğ‘—ğ‘¢_ğ‘—, ğ‘—=1,â€¦,ğ‘. This shows that
    principal components scores can be computed if only the inner product matrix is
    given instead of the original data points.
  prefs: []
  type: TYPE_NORMAL
- en: Low Dimensional Representation from Distances Only
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose that we only have the distances between the data points instead of the
    original data. That is, we have the Euclidean distances
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ff5e219a62a1444a5dbe10678df0de8.png)'
  prefs: []
  type: TYPE_IMG
- en: or all ğ‘– and ğ‘—. Can we still recover the principal component directions from
    these distances?
  prefs: []
  type: TYPE_NORMAL
- en: Let us first define a ğ‘›Ã—ğ‘› dimensional distance matrix Î” with (ğ‘–,ğ‘—)-th element
    given by Î”_ğ‘–ğ‘—. We can recover the inner product matrix ğµ=ğ—ğ—^âŠ¤ from the distance
    matrix Î”.
  prefs: []
  type: TYPE_NORMAL
- en: Create the ğ‘›Ã—ğ‘› matrix ğ´ with its (ğ‘–,ğ‘—)-th element given by
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/94081d5c34cd40576399f4fbb621f06f.png)'
  prefs: []
  type: TYPE_IMG
- en: 2\. Double center ğ´, that is, center both the columns and rows of ğ´ to recover
    the matrix ğµ by using the transformation *B* = (*I* â€” *M*)*A*(*I* â€” *M*) where
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/245f14fda5cd995dadec21369c009103.png)'
  prefs: []
  type: TYPE_IMG
- en: Kernel PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kernel PCA simply mimics this procedure by replacing the inner product matrix
    ğµ by the kernel matrix ğ¾=((ğ¾_ğ‘–ğ‘—)), where ğ¾_ğ‘–ğ‘—=<ğœ™(ğ‘¥_ğ‘–),ğœ™(ğ‘¥_ğ‘—)>, the inner-product
    between the feature vectors ğœ™(ğ‘¥_ğ‘–) and ğœ™(ğ‘¥_ğ‘—). Here ğœ™ is a nonlinear map from
    â„^ğ‘ â†’ ğ¹, a feature space of arbitrary dimension. The idea is similar to the kernels
    in support vector machines (SVM) for classification problems. We are projecting
    the observations to a higher-dimensional space and then obtaining the principal
    components in that space. We can simply define, ğ¾_ğ‘–ğ‘—=Î¦(ğ‘¥_ğ‘–,ğ‘¥_ğ‘—), where for the
    radial kernel,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc02b10e3fbe8bb019f55ca574647ae4.png)'
  prefs: []
  type: TYPE_IMG
- en: and for the polynomial kernel,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4649f21b400fb488eb04b8c50bbe8c98.png)'
  prefs: []
  type: TYPE_IMG
- en: where ğ›¾, ğ‘ and ğ‘‘ are the parameters of the respective kernel functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the ğ‘›Ã—ğ‘› kernel inner product matrix ğ¾ as ğ¾=((Î¦(ğ‘¥_ğ‘–,ğ‘¥_ğ‘—)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use eigendecomposition of ğ¾ to extract the eigenvalues and the eigenvectors
    of ğ¾.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The eigenvectors of ğ¾ will give the principal component scores.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is a nonlinear dimension reduction and we can illustrate the use of kernel
    PCA for our **Swiss Roll** data discussed in the previous example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/92d3b95533f338cfa745aa90c02763ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Kernel PCA Dimensions for the Swiss Roll Data (Image by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: In the above, we have used an `rbf` kernel with ğ›¾ = 0.002\. Though the results
    improved from PCA, it still does not unroll the swiss roll, but picks the manifold
    very well.
  prefs: []
  type: TYPE_NORMAL
- en: We illustrate the kernel PCA with a different simulated data set below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/afb45d342afc007dc080d3b4c5129506.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Kernel PCA dimensions for the simulated data on the left. (Image
    by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: In the left, we have the simulated data with 3 concentric circles having uniform
    distribution with radius 1.0, 2.8 and 5.0 respectively. On the right hand side,
    we plot the kernel PCA components with `rbf` kernel and ğ›¾=0.3\. We observe a nice
    separation of the three clusters of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Dimensional Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have discussed in our [article on PCA](https://medium.com/@biman.pph/principal-component-analysis-pca-an-easy-tutorial-with-python-c623b583cf29)
    that it tries to preserve the distance between the observations in a lower dimensional
    representation. In other words, if ğ‘§_1,ğ‘§_2,â€¦,ğ‘§_ğ‘› are the lower dimensional representation
    of ğ‘¥_1,ğ‘¥_2,â€¦,ğ‘¥_ğ‘›, then PCA minimizes
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/406a8f6936d372b3d893200d8eceb320.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We now generalize this idea by defining a *stress* function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9275d4a9db046d34836840fed3fe2499.png)'
  prefs: []
  type: TYPE_IMG
- en: where ğ‘‘_ğ‘–ğ‘— is a distance between ğ‘¥_ğ‘– and ğ‘¥_ğ‘—. Usually, we chose Euclidean distances,
    but other distances can be used as well.
  prefs: []
  type: TYPE_NORMAL
- en: '**Multidimensional scaling** seeks values ğ‘§_1,ğ‘§_2,â€¦,ğ‘§_ğ‘›âˆˆâ„^ğ‘˜ to minimize the
    *stress function*, ğ‘†_ğ‘€(ğ‘§_1,ğ‘§_2,â€¦,ğ‘§_ğ‘›).'
  prefs: []
  type: TYPE_NORMAL
- en: This is known as *least squares* or *Kruskalâ€“Shephard scaling*. The idea is
    to find a lower-dimensional representation of the data that preserves the pairwise
    distances as well as possible. Notice that the approximation is in terms of the
    distances rather than squared distances.
  prefs: []
  type: TYPE_NORMAL
- en: Let us look into its implementation for the **Swiss Roll** data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/eeb6759c19e5ca3e289ddd9a0f4e497c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: First 2 dimensions of the classical multidimensional scaling. (Image
    by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: We observe that the results are very similar to the kernel PCA.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have not gone beyond Euclidian distances. But we mentioned earlier
    that in the Swiss roll data, the Euclidian distances are not ideal.
  prefs: []
  type: TYPE_NORMAL
- en: There is a class of methods which construct a fancier distance ğ‘‘_ğ‘–ğ‘— between
    high-dimensional points ğ‘¥_1,â€¦,ğ‘¥_ğ‘›âˆˆâ„^ğ‘, and then they feed these ğ‘‘_ğ‘–ğ‘— through multidimensional
    scaling to get a low-dimensional representation ğ‘§_1,â€¦,ğ‘§_ğ‘›âˆˆâ„^ğ‘˜. In this case, we
    donâ€™t just get principal component scores, and our low-dimensional representation
    can end up being a *nonlinear function* of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Tangent distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Tangent distance* is an example of a fancier metric that we can run through
    multidimensional scaling (though used elsewhere too).'
  prefs: []
  type: TYPE_NORMAL
- en: A motivating example is the *handwritten digits data* we used earlier. Here,
    we have *16 \times 16* images, treated as points ğ‘¥_ğ‘–âˆˆâ„Â²âµâ¶ (i.e., they are unraveled
    into vectors). If we take, e.g., a â€œ3â€ and *rotate* it through a small angle,
    we would like for the rotated image to be considered close to the original image.
    This is not neccessarily true of Euclidean distance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/18d00ffbd85d32f0aed9fe8fc806839b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Original images of â€œ3â€ and a rotated image of â€œ3â€ (Image by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: We could define Î”_ğ‘–ğ‘—^rotation to be the shortest Euclidean distance between
    a rotated version of ğ‘¥_ğ‘– and rotated version of ğ‘¥_ğ‘—. However, you can immediately
    see that there is a problem in rotating the digits â€œ6â€ and â€œ9â€.
  prefs: []
  type: TYPE_NORMAL
- en: We need something easier to calculate, and that restricts attention to *small
    rotations*. It helps to think of a set of rotations of an image as defining a
    curve in â„^ğ‘ â€” an image ğ‘¥_ğ‘– is a point in â„^ğ‘, and as we rotate it in either directions,
    we get a curve.
  prefs: []
  type: TYPE_NORMAL
- en: '**Tangent distance** Î”_ğ‘–ğ‘—^tangent is defined by first computing the tangent
    line to each curve at the observed image, and then using the shortest Euclidean
    distance between tangent lines.'
  prefs: []
  type: TYPE_NORMAL
- en: Isometric Feature Mapping (Isomap)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Isometric feature mapping* (Isomap) learns structure in a more general setting
    to define distances. The basic idea is to construct a graph ğº=(ğ‘‰,ğ¸), i.e., construct
    edges ğ¸ between vertices ğ‘‰={1,â€¦,ğ‘›}, based on the structure between ğ‘¥_1,â€¦,ğ‘¥_ğ‘›âˆˆâ„^ğ‘
    . Then we define a graph distance Î”_ğ‘–ğ‘—^Isomap between ğ‘¥_ğ‘– and ğ‘¥_ğ‘—, and use multidimensional
    scaling for our low-dimensional representation'
  prefs: []
  type: TYPE_NORMAL
- en: '**Constructing the graph**: for each pair ğ‘–,ğ‘—, we connect ğ‘–,ğ‘— with an edge
    if either:'
  prefs: []
  type: TYPE_NORMAL
- en: ğ‘¥_ğ‘– is one of ğ‘¥_ğ‘—â€™s ğ‘š nearest neighbors, or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ğ‘¥_ğ‘— is one of ğ‘¥_ğ‘–â€™s ğ‘š nearest neighbors The weight of this edge ğ‘’ = {ğ‘–,ğ‘—} is
    then ğ‘¤_ğ‘’=â€–ğ‘¥_ğ‘–âˆ’ğ‘¥_ğ‘—â€–.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Defining graph distances**: now that we have built a graph, i.e., we have
    built an edge set ğ¸, we define the graph distance Î”_ğ‘–ğ‘—^Isomap between ğ‘¥_ğ‘– and
    ğ‘¥_ğ‘— to be the shortest path in our graph from ğ‘– to ğ‘—:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cc23df2584e0c951fd57658a7f031969.png)'
  prefs: []
  type: TYPE_IMG
- en: (This can be computed by, e.g., *Dijkstraâ€™s algorithm* or *Floydâ€™s algorithm*)
  prefs: []
  type: TYPE_NORMAL
- en: Let us now look into its implementation for the **Swiss roll** data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/561bb0d2fdfe53172bb154e1baa0330b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Two dimensional representation of the isomap of the Swiss Roll data.
    (Image by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: With the number of neighbours ğ‘š=7, the multodimemsional scaling with the **isomap**
    distances now unrolls the **Swiss roll** data.
  prefs: []
  type: TYPE_NORMAL
- en: Local Linear Embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another nonlinear dimension reduction method is **Local linear embedding** (LLE),
    which is a similar method in spirit but its details are very different. It doesnâ€™t
    use multidimensional scaling.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic idea has two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Learn a bunch of local approximations to the structure between ğ‘¥_1,â€¦,ğ‘¥_ğ‘›âˆˆâ„^ğ‘
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learn a low-dimensional representation ğ‘§_1,â€¦,ğ‘§_ğ‘›âˆˆâ„^ğ‘˜ that best matches these
    local approximations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is meant by such local approximations? We simply try to predict each ğ‘¥_ğ‘–
    by a linear function of nearby points ğ‘¥_ğ‘— (hence the name local linear embedding).
  prefs: []
  type: TYPE_NORMAL
- en: For each ğ‘¥_ğ‘–, we first find its ğ‘š nearest neighbours, and collect their indices
    as N(ğ‘–). Then we build a weight vector ğ‘¤_ğ‘–âˆˆâ„^ğ‘›, setting ğ‘¤_ğ‘–ğ‘—=0 for ğ‘—âˆ‰N(ğ‘–) and
    setting ğ‘¤_ğ‘–ğ‘— for ğ‘—âˆˆN(ğ‘–) by minimizing
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4923ea91402950b401d3d06a3d26653.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, we take these weights ğ‘¤_1,â€¦,ğ‘¤_ğ‘›âˆˆâ„^ğ‘› and we fit the low-dimensional
    representation ğ‘§_1,â€¦,ğ‘§_ğ‘›âˆˆâ„^ğ‘˜, by minimizing
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9eefa5853f41c28e0d40e8a4b2f49bd6.png)'
  prefs: []
  type: TYPE_IMG
- en: We again illustrate the use of *Local Linear Embedding* using the Swiss roll
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/72c3d9c2a1d5d397d5364b7eed5e0827.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Local Linear Embedding dimensions for the Swiss Roll data. (Image
    by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Local Linear Embedding* also yielded better dimension reduction than kernel
    PCA or classical MDS, though it is not as good as *Isomap*.'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we have learnt a few nonlinear dimension reduction technique
    by generalizing the concepts from PCA. However, there is no single method which
    works best for all sort of data for dimension reduction. Depending on the nature
    of the data, we should decide about the dimension reduction technique to be used.
  prefs: []
  type: TYPE_NORMAL
- en: Hope, you have enjoyed the article!!
  prefs: []
  type: TYPE_NORMAL
- en: For consulting on any data science problems, contact [biman.pph@gmail.com](mailto:biman.pph@gmail.com)
  prefs: []
  type: TYPE_NORMAL
