["```py\n# Load data\n(X_train, y_train), (X_test, y_test) = load_my_data()\n# Print an example\n#print_image(X_train, 42)\n\n# Normalize the inputs\nX_train = X_train / 255.\nX_test = X_test / 255.\n\n# Define the optimizers to test\nmy_optimizers = {\"Mini-batch GD\":tf.keras.optimizers.SGD(learning_rate = 0.001, momentum = 0.0),\n                 \"Momentum GD\":tf.keras.optimizers.SGD(learning_rate = 0.001, momentum = 0.9),\n                 \"RMS Prop\":tf.keras.optimizers.RMSprop(learning_rate = 0.001, rho = 0.9),\n                 \"Adam\":tf.keras.optimizers.Adam(learning_rate = 0.001, beta_1 = 0.9, beta_2 = 0.999)\n    }\n\nhistories = {}\nfor optimizer_name, optimizer in my_optimizers.items():\n    # Define a neural network\n\n    my_network = tf.keras.models.Sequential([\n                tf.keras.layers.Flatten(input_shape=(28, 28)),                    \n                tf.keras.layers.Dense(128, activation='relu'),\n                tf.keras.layers.Dense(64, activation='relu'),\n                tf.keras.layers.Dense(10, activation='softmax')\n                ])\n    # Compile the model\n    my_network.compile(optimizer=optimizer,\n                       loss='sparse_categorical_crossentropy', # since labels are more than 2 and not one-hot-encoded\n                       metrics=['accuracy'])\n\n    # Train the model\n    print('Training the model with optimizer {}'.format(optimizer_name))\n    histories[optimizer_name] = my_network.fit(X_train, y_train, epochs=50, validation_split=0.1, verbose=1)\n\n# Plot learning curves\nfor optimizer_name, history in histories.items():\n    loss = history.history['loss']\n    epochs = range(1,len(loss)+1)\n    plt.plot(epochs, loss, label=optimizer_name)\n    plt.legend(loc=\"upper right\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n```"]