- en: Debugging SageMaker Endpoints With Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/debugging-sagemaker-endpoints-with-docker-7a703fae3a26](https://towardsdatascience.com/debugging-sagemaker-endpoints-with-docker-7a703fae3a26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An Alternative To SageMaker Local Mode
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ram-vegiraju.medium.com/?source=post_page-----7a703fae3a26--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----7a703fae3a26--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7a703fae3a26--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7a703fae3a26--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----7a703fae3a26--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7a703fae3a26--------------------------------)
    ·6 min read·Jun 16, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0b957e3ecdec75d862d1a637b72d9fe3.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Unsplash](https://unsplash.com/photos/1VW6HLOQE5A) by [**Mohammad
    Rahmani**](https://unsplash.com/@afgprogrammer)
  prefs: []
  type: TYPE_NORMAL
- en: A pain point with getting started with [SageMaker Real-Time Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html)
    is that it is hard to debug at times. When creating an endpoint there are a number
    of ingredients you need to make sure are baked properly for successful deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Proper file structuring of model artifacts depending on the Model Server and
    Container that you are utilizing. Essentially the model.tar.gz you provide must
    be in a format that is compliant with the Model Server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have a custom inference script that implements pre and post processing
    for your model, you need to ensure that the handlers implemented are compliant
    with your model server and that there are no scripting errors at the code level
    either.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Previously we have discussed [SageMaker Local Mode](/debugging-sagemaker-endpoints-quickly-with-local-mode-2975bd55f6f7),
    but at the time of this article Local Mode does not support all hosting options
    and model servers that are available for SageMaker Deployment.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this limitation we take a look at using [Docker](https://www.docker.com/)
    with a sample model and how we can test/debug our model artifacts and inference
    script prior to SageMaker Deployment. In this specific example we will utilize
    the [BART Model](https://huggingface.co/facebook/bart-large) that I have covered
    in my [last article](/deploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c)
    and see how we can host it with Docker.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**: For those of you new to AWS, make sure you make an account at the
    following [link](https://aws.amazon.com/console/) if you want to follow along.
    The article also assumes an intermediate understanding of SageMaker Deployment,
    I would suggest following this [article](https://aws.amazon.com/blogs/machine-learning/part-2-model-hosting-patterns-in-amazon-sagemaker-getting-started-with-deploying-real-time-models-on-sagemaker/)
    for understanding Deployment/Inference more in depth. An intermediate level of
    understanding of Docker will also be helpful to fully understand this example.'
  prefs: []
  type: TYPE_NORMAL
- en: How Does SageMaker Hosting Work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we can get to the code portion of this article, let’s take a look at
    how SageMaker actually serves requests. At it’s core SageMaker Inference has two
    constructs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Container**: This establishes the runtime environment for the model, it is
    also integrated with the model server that you are utilizing. You can either utilize
    one of the existing [Deep Learning Containers](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)
    (DLCs) or [Build Your Own Container](/bring-your-own-container-with-amazon-sagemaker-37211d8412f4).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Artifacts**: In the [CreateModel](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_model.html)
    API call we specify an S3 URL with the model data present in the format of a model.tar.gz
    (tarball). This model data is loaded into the opt/ml/model directory on the container,
    this also includes any inference script that you provide.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key here is that the container needs a web server implemented that responds
    to port 8080 on the /invocations and /ping paths. An example of a web server we
    have implemented with these paths is Flask during a [Bring Your Own Container](/bring-your-own-container-with-amazon-sagemaker-37211d8412f4)
    example.
  prefs: []
  type: TYPE_NORMAL
- en: With Docker what we will do is expose this port and point towards our local
    script and model artifacts, this way we simulate the way a SageMaker Endpoint
    is expected to behave.
  prefs: []
  type: TYPE_NORMAL
- en: Testing With Docker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For simplicity’s sake we will utilize my BART example from my last article,
    you can grab the artifacts from this [repository](https://github.com/RamVegiraju/SageMaker-Docker-Local).
    Here you should see the following files:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a559972afa9346ef4d835a2cc816ca9.png)'
  prefs: []
  type: TYPE_IMG
- en: '**model.py**: This is the inference script that we are working with. In this
    case we are utilizing DJL Serving which expects a model.py with a handler function
    implementing inference. **Your inference script still needs to be compatible with
    the format that the model server expects.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**requirements.txt**: Any additional dependencies that your model.py script
    requires. For DJL Serving PyTorch is already installed beforehand, we use numpy
    for data processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**serving.properties**: This is also a DJL specific file, here you can define
    any configuration at the model level (ex: workers per model)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have our model artifacts, now we need the container that we will be utilizing.
    In this case we can retrieve the existing DJL DeepSpeed image. For an extensive
    list of the images that are already provided by AWS please reference this [guide](https://github.com/aws/deep-learning-containers/blob/master/available_images.md).
    You can also build your own image locally and point towards that. In this case
    we are operating in a [SageMaker Classic Notebook Instance](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html)
    environment which comes with Docker pre-installed as well.
  prefs: []
  type: TYPE_NORMAL
- en: To work with existing AWS provided images we first need to login to [AWS Elastic
    Container Registry (ECR)](/pushing-docker-images-to-amazon-elastic-container-registry-830c301b8971)
    to retrieve the image, you can do that with the following shell command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You should see a login succeeded message similar to the following.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d3ce47666e9dc95565e62207f8ab99e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Login Succeeded (Screenshot by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Once logged in we can get to the path where our model artifacts are stored and
    run the following command which will launch the model server. If you have not
    already retrieved the image, this will also be pulled from ECR.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'A few key points here:'
  prefs: []
  type: TYPE_NORMAL
- en: We are exposing port 8080 as SageMaker Inference expects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also point towards the existing image. This string is dependent on the region
    and model you are operating in. You can also utilize the SageMaker Python SDK
    [retrieve image_uri API call](https://sagemaker.readthedocs.io/en/stable/api/utility/image_uris.html)
    to identify the appropriate image to pull here.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/80edbf854b73b7dc1e6d507a14c38b3e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image being retrieved (Screenshot by Author)
  prefs: []
  type: TYPE_NORMAL
- en: After the image has been pulled you see that the model server has been started.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/83e5eb09c3d3f4933b06c64e739c025d.png)'
  prefs: []
  type: TYPE_IMG
- en: DJL Server Started (Screenshot by Author)
  prefs: []
  type: TYPE_NORMAL
- en: We can also verify this container is running by utilizing the following Docker
    command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ff955e4533bcb8ba0705f8c14abf5cf0.png)'
  prefs: []
  type: TYPE_IMG
- en: Container Started (Screenshot by Author)
  prefs: []
  type: TYPE_NORMAL
- en: We see that the API is exposed via port 8080 which we can send sample requests
    to via curl. Notice we specify the /invocations path that SageMaker Containers
    expect.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We then see inference returned for the request and also the model server tracking
    the response and emitting our logging statements from our inference script.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2d1771223fea354eb27a90380051f9d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample Request (Screenshot by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s break our model.py and see if we can catch the error early with Docker.
    Here in the inference function I add a syntactically incorrect print statement
    and restart my model server to see if this error is captured.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We can then see this error captured by the model server when we execute the
    docker run command.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7f75fef77d00382b2822a06edc0e80f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Error captured by Model Server (Screenshot by Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that you are not limited to utilizing just curl to test your container.
    We can also use something like the [Python requests](https://pypi.org/project/requests/)
    library to interface and work with the container. A sample request would look
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Utilizing something like requests you can run larger scale load tests on the
    container. Note that the hardware you are running the container on is what is
    being utilized (think of this as your equivalent to the instance behind a SageMaker
    Endpoint).
  prefs: []
  type: TYPE_NORMAL
- en: Additional Resources & Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://github.com/RamVegiraju/SageMaker-Docker-Local/tree/master?source=post_page-----7a703fae3a26--------------------------------)
    [## GitHub - RamVegiraju/SageMaker-Docker-Local: How to locally test SageMaker
    Inference with Docker'
  prefs: []
  type: TYPE_NORMAL
- en: 'How to locally test SageMaker Inference with Docker - GitHub - RamVegiraju/SageMaker-Docker-Local:
    How to locally test…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/RamVegiraju/SageMaker-Docker-Local/tree/master?source=post_page-----7a703fae3a26--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code for the entire example at the link above. With SageMaker
    Inference you want to avoid the pain of waiting for the endpoint to create to
    capture any errors. Utilizing this approach you can work with any SageMaker container
    to test and debug your model artifacts and inference scripts.
  prefs: []
  type: TYPE_NORMAL
- en: As always feel free to leave any feedback or questions, thank you for reading!
  prefs: []
  type: TYPE_NORMAL
- en: '*If you enjoyed this article feel free to connect with me on* [*LinkedIn*](https://www.linkedin.com/in/ram-vegiraju-81272b162/)
    *and subscribe to my Medium* [*Newsletter*](https://ram-vegiraju.medium.com/subscribe)*.
    If you’re new to Medium, sign up using my* [*Membership Referral*](https://ram-vegiraju.medium.com/membership)*.*'
  prefs: []
  type: TYPE_NORMAL
