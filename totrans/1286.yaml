- en: Applying Hyperparameter Tuning To Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/hyperparameter-tuning-neural-networks-101-ca1102891b27](https://towardsdatascience.com/hyperparameter-tuning-neural-networks-101-ca1102891b27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How you can improve the “learning” of neural networks through tuning hyperparameters
    with an example in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@egorhowell?source=post_page-----ca1102891b27--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----ca1102891b27--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ca1102891b27--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ca1102891b27--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----ca1102891b27--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ca1102891b27--------------------------------)
    ·9 min read·Nov 18, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b66a379b90bb74a1caf0ca7ed45e511.png)'
  prefs: []
  type: TYPE_IMG
- en: Neural-network icons created by Vectors Tank — Flaticon. neural-network icons.
    [https://www.flaticon.com/free-icons/neural](https://www.flaticon.com/free-icons/neural-network)
  prefs: []
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In my previous post, we discussed how [***neural networks***](https://medium.com/gitconnected/intro-perceptron-architecture-neural-networks-101-2a487062810c)
    predict and learn from the data. There are two processes responsible for this:
    the [***forward pass***](/neural-networks-forward-pass-and-backpropagation-be3b75a1cfcc)
    and backward pass, also known as [***backpropagation***](https://en.wikipedia.org/wiki/Backpropagation).
    You can learn more about it here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----ca1102891b27--------------------------------)
    [## Forward Pass & Backpropagation: Neural Networks 101'
  prefs: []
  type: TYPE_NORMAL
- en: Explaining how neural networks “train” and “learn” patterns in data by hand
    and in code using PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----ca1102891b27--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: This post will dive into how we can optimise this “learning” and “training”
    process to increase the performance of our model. The areas we will cover are
    computational improvements and [***hyperparameter tuning***](/optimise-your-hyperparameter-tuning-with-hyperopt-861573239eb5)and
    how to implement it in PyTorch!
  prefs: []
  type: TYPE_NORMAL
- en: But, before all that good stuff, let’s quickly jog our memory about neural networks!
  prefs: []
  type: TYPE_NORMAL
- en: 'Quick Recap: What are Neural Networks?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neural networks are large mathematical expressions that try to find the “right”
    function that can map a set of inputs to their corresponding outputs. An example
    of a neural network is depicted below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94cdac9897a5e852d9cd6edd425fcba0.png)'
  prefs: []
  type: TYPE_IMG
- en: A basic two-hidden multi-layer perceptron. Diagram by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each hidden-layer neuron carries out the following computation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57f8921c8f7e374d7f075b2f4ae32e6b.png)'
  prefs: []
  type: TYPE_IMG
- en: The process carried out inside each neuron. Diagram by author.
  prefs: []
  type: TYPE_NORMAL
- en: '***Inputs:*** *These are the features of our dataset.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Weights:*** *Coefficients that scale the inputs. The goal of the algorithm
    is to find the most optimal coefficients through* [***gradient descent***](/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c)*.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Linear Weighted Sum:*** *Sum up the products of the inputs and weights and
    add a bias/offset term,* ***b****.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Hidden Layer:*** *Multiple neurons are stored to learn patterns in the dataset.
    The superscript refers to the layer and the subscript to the number of neuron
    in that layer.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Arrows:*** *These are the weights for the network from their corresponding
    inputs, whether it be the features or the hidden layer outputs. I have omitted
    writing them explicitly on the diagram them for a cleaner plot.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[***ReLU Activation Function***](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)***:***
    *The most popular* [***activation function***](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)*as
    it is computationally efficient and intuitive. See more about it* [*here*](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)*.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I have [linked a fantastic video here](https://www.youtube.com/watch?v=0QczhVg5HaI)
    explaining how neural networks can learn anything for some more context!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'If you want a more thorough introduction, check my previous post here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----ca1102891b27--------------------------------)
    [## Intro, Perceptron & Architecture: Neural Networks 101'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to Neural Networks and their building blocks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----ca1102891b27--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Computational Improvements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps the main optimisation technique that made neural networks so commonly
    accessible nowadays is parallel processing.
  prefs: []
  type: TYPE_NORMAL
- en: The volume of data is also another main reason why neural networks are so effective
    in practice.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Each layer can be formulated as a large matrix with its associated inputs,
    weights, and biases. For example, consider this basic output from a neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/83f85cd34579fe553967381e057e723d.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear output from neuron. Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here ***x*** is the inputs, ***w*** is the weights, ***b*** is the bias and
    ***z*** is the final output. The above formula can be re-written in matrix form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a1cd2bd222767bd7c853cfa26a56e22c.png)'
  prefs: []
  type: TYPE_IMG
- en: Vectorised implementation for a neuron output. Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: Without this vectorised implementation, the runtime for training neural networks
    would be enormous as we would start to rely on loops. In that case, we would multiply
    each weight and input and then add them one after the other to get ***z***. Whereas
    vectorising the implementation, it can be done in one whole step.
  prefs: []
  type: TYPE_NORMAL
- en: There is a great article [linked here](/understand-vectorization-for-deep-learning-d712d260ab0f)
    and [a video here](https://www.youtube.com/watch?v=kkWRbIb42Ms) that compares
    the runtime for a vectorised approach vs using loops.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Most deep learning frameworks like [***PyTorch***](https://pytorch.org/) and
    [***TensorFlow***](https://www.tensorflow.org/) do this for you under the hood,
    so you don’t have to worry too much about it!
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The search space for neural network architectures and parameters is unimaginably
    huge, if not infinite. Several libraries exist to help you tune the parameters
    such as [***hyperopt***](/optimise-your-hyperparameter-tuning-with-hyperopt-861573239eb5),
    [***optuna***](https://optuna.org/) or just regular [***sci-kit learn***](https://scikit-learn.org/stable/).
  prefs: []
  type: TYPE_NORMAL
- en: There are also many more libraries out there, [see here for a list.](/10-hyperparameter-optimization-frameworks-8bc87bc8b7e3)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: They differ in their approach some use a simple grid search or random search,
    whereas others have more sophisticated methods such as [***Bayesian optimisation***](https://pub.towardsai.net/conditional-probability-and-bayes-theorem-simply-explained-788a6361f333)
    or even evolutionary algorithms like [***genetic algorithms***](/from-biology-to-computing-an-introduction-to-genetic-algorithms-b39476743483).
    One method is not better than the other, it all comes down to how you devise your
    search space and compute resources.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to have some knowledge of your ideal parameters to not waste
    a significant amount of time searching over unnecessary values and converge quickly.
    Let’s now run through some of the main ones you ought to look at!
  prefs: []
  type: TYPE_NORMAL
- en: Number of Hidden Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is something called the [***universal approximation theorem***](https://en.wikipedia.org/wiki/Universal_approximation_theorem)
    that basically says a single hidden layer neural network can learn any function,
    provided it has enough neurons.
  prefs: []
  type: TYPE_NORMAL
- en: However, having one layer with loads of neurons is not ideal, and it is better
    to have several layers with fewer neurons. The hypothesis for this being is that
    each layer learns something new and at a more granular level. Whereas with one
    hidden layer, the network is expected to learn every nuance of the dataset all
    at once.
  prefs: []
  type: TYPE_NORMAL
- en: In general, a couple of hidden layers are often good enough. For example, on
    the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database), a model with
    one hidden layer and a couple of hundred neurons has [97% accuracy](https://paperswithcode.com/sota/image-classification-on-mnist),
    but a network with two hidden layers and the same number of neurons has [98% accuracy](https://paperswithcode.com/sota/image-classification-on-mnist).
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST dataset contains many examples of handwritten digits.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Of course, like any hyperparameter, we should apply some tuning process to iterate
    over a different number of layers in combination with other hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Number of Neurons in Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The input and output layers are pre-defined on the number of neurons they can
    have. The input layer size must equal the number of features in the dataset. If
    your dataset has 50 features, then your input layer will have 50 neurons. Similarly,
    the output layer needs to be appropriate for the problem. If you are predicting
    house prices, then the output layer will only have 1 neuron. However, if you are
    trying to classify single digits, like in the MNIST dataset, then you need 10
    output neurons.
  prefs: []
  type: TYPE_NORMAL
- en: For the hidden layers, you can go a bit wild! The search space of the number
    of neurons is massive. However, it’s best to overkill it slightly with how many
    you have and use a technique such as [***early stopping***](https://en.wikipedia.org/wiki/Early_stopping#:~:text=In%20machine%20learning%2C%20early%20stopping,training%20data%20with%20each%20iteration.)to
    prevent overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Another key idea is to ensure that each layer has enough neurons to have *representational
    power.* If you are trying to predict a 3D image, with 2 neurons you can only work
    in 2D, hence will lose some information about the signal.
  prefs: []
  type: TYPE_NORMAL
- en: Learning Rate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The learning rate determines how quickly the algorithm can converge to the optimal
    as it’s responsible for the step size used during backpropagation. It’s probably
    one of the most important hyperparameters for training neural networks. Too high
    and learning will diverge, too low, and the algorithm will take ages to converge.
  prefs: []
  type: TYPE_NORMAL
- en: I would typically just hyperparameter-tune my learning rate over a wide search
    space between ***0.001*** and ***1***, this is seen as the *traditional* learning
    rate that most commonly comes up in literature.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the best ways to find the optimal learning rate is through [***learning
    rate scheduling***](/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1).
    The schedule reduces the learning rate as training progresses, so take smaller
    step sizes near and around the optimum. Let’s break down some of the common ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Time Based Decay:**'
  prefs: []
  type: TYPE_NORMAL
- en: The learning rate decreases at a certain rate overtime.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a3593cb7ee892ae8a131bf0a2e1d3468.png)'
  prefs: []
  type: TYPE_IMG
- en: Time based decay. Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: Here, ***α*** is the learning rate, ***α_0*** is the initial learning rate**,
    *decay***is the decay rate and ***epoch*** is the number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: An epoch is one training cycle of a neural network using all of the training
    data.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Step Decay:**'
  prefs: []
  type: TYPE_NORMAL
- en: The learning rate is reduced by a certain factor after a given number of training
    epochs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e6ad17edcc7651647d45e3d1e108de32.png)'
  prefs: []
  type: TYPE_IMG
- en: Step decay. Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: Where ***factor*** is the factor that the learning rate will be reduced by and
    ***step*** is the number of epochs after which the learning rate should be reduced.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exponential Decay:**'
  prefs: []
  type: TYPE_NORMAL
- en: The learning rate will decrease exponentially at each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1bfad8027a68b154c55f4297e6c66676.png)'
  prefs: []
  type: TYPE_IMG
- en: Exponential decay. Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: '**Others:**'
  prefs: []
  type: TYPE_NORMAL
- en: There are also many other learning rate schedules such as *performance scheduling*,
    *1cycle scheduling*, and *power scheduling*. It is important to remember that
    one scheduling method is not better than another and it’s ideal to try several
    to determine which is the best fit for your model and data.
  prefs: []
  type: TYPE_NORMAL
- en: Batch Size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When training neural networks, there are three common variations of gradient
    descent, which are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Batch Gradient Descent**](/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a)**:**
    Use the entire training dataset to compute the gradient of the loss function.
    This is the most robust but not computationally feasible for large datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Stochastic Gradient Descent**](https://realpython.com/gradient-descent-algorithm-python/#:~:text=Stochastic%20gradient%20descent%20is%20an,used%20in%20machine%20learning%20applications.)**:**
    Use a single data point to compute the gradient of the loss function. This method
    is the quickest but the estimate can be noisy and the convergence path slow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Mini-Batch Gradient Descent**](/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a)**:**
    Use a subset of the training dataset to compute the gradient of the loss function.
    The size of the batches varies and depends on the size of the dataset. This is
    the best of both worlds of both batch and stochastic gradient descent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tricky part is finding the best batch size to carry out mini-batch gradient
    descent with. It’s recommended to use the largest batch sizes possible that can
    fit inside your computer’s GPU RAM as they parallelize the computation.
  prefs: []
  type: TYPE_NORMAL
- en: Number of Iterations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the number of epochs, which is the total forward and backward passes
    that we carry out for the neural network. In reality, it’s better to use early
    stopping and set the number of iterations arbitrarily high. This removes the chance
    of terminating learning too early.
  prefs: []
  type: TYPE_NORMAL
- en: Activation Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most networks use the [***ReLU***](/breaking-linearity-with-relu-d2cfa7ebf264),
    primarily due to its computational efficiencies, but there are others out there.
    One of my previous posts summarised the main ones along with their pros and cons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----ca1102891b27--------------------------------)
    [## Activation Functions & Non-Linearity: Neural Networks 101'
  prefs: []
  type: TYPE_NORMAL
- en: Explaining why neural networks can learn (nearly) anything and everything
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----ca1102891b27--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The selected activation function is important for the output layer to ensure
    your prediction is in the right context of the problem. For example, if you are
    predicting probability, then you would use a [***sigmoid***](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)
    activation function.
  prefs: []
  type: TYPE_NORMAL
- en: However, in my opinion, testing different activation functions in the hidden
    layers won’t move the needle of performance that much compared to the other hyperparameters
    we have discussed above.
  prefs: []
  type: TYPE_NORMAL
- en: Other Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are also other hyperparameters that can tuned for the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[***Optimising algorithm***](/optimizers-for-training-neural-network-59450d71caf6)
    ***(cover in next article!)***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[***Regularisation methods***](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/)
    ***(cover in future!)***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[***Weight initialisation***](https://www.deeplearning.ai/ai-notes/initialization/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[***Loss function***](/loss-functions-and-their-use-in-neural-networks-a470e703f1e9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Below is some boilerplate code that carries out hyperparameter tuning for a
    neural network in PyTorch using hyperopt for the MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: GitHub Gist by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is available on my GitHub here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Neural%20Networks/hyperparam_tune.py?source=post_page-----ca1102891b27--------------------------------)
    [## Medium-Articles/Neural Networks/hyperparam_tune.py at main · egorhowell/Medium-Articles'
  prefs: []
  type: TYPE_NORMAL
- en: Code I use in my medium blog/articles. Contribute to egorhowell/Medium-Articles
    development by creating an account on…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Neural%20Networks/hyperparam_tune.py?source=post_page-----ca1102891b27--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Summary & Further Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks have many hyperparameters and infinite architectures, this makes
    finding the best combination very difficult. Fortunately, packages such as ***optuna***
    and ***hyperpot*** exist that carry out this process for us in a smart way. The
    hyperparameters that are often best to tune are the number of hidden layers, the
    number of neurons, and the learning rate. These often give us the most ‘bang for
    our buck’ when developing neural net models. The number of epochs is made redundant
    through the use of early stopping, and the activation function chosen also often
    has a minimal effect on the performance. However, it is always important to consider
    what type of problem you are trying to solve when considering the structure of
    your input and output layers as well as the output layer’s activation function.
  prefs: []
  type: TYPE_NORMAL
- en: References & Further Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Andrej Karpathy Neural Network Course*](https://www.youtube.com/watch?v=i94OvYb6noo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*PyTorch site*](https://pytorch.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Another example of training a neural network by hand*](/training-a-neural-network-by-hand-1bcac4d82a6e)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition.
    Aurélien Géron. September 2019\. Publisher(s): O’Reilly Media, Inc. ISBN: 9781492032649*](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)*.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another Thing!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I have a free newsletter, [**Dishing the Data**](https://dishingthedata.substack.com/),
    where I share weekly tips for becoming a better Data Scientist. There is no “fluff”
    or “clickbait,” just pure actionable insights from a practicing Data Scientist.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://newsletter.egorhowell.com/?source=post_page-----ca1102891b27--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
  prefs: []
  type: TYPE_NORMAL
- en: How To Become A Better Data Scientist. Click to read Dishing The Data, by Egor
    Howell, a Substack publication with…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----ca1102891b27--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Connect With Me!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[**YouTube**](https://www.youtube.com/@egorhowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Twitter**](https://twitter.com/EgorHowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**GitHub**](https://github.com/egorhowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
