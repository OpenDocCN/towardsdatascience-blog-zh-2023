- en: 'Exploring Creativity in Large Language Models: From GPT-2 to GPT-4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索大型语言模型中的创造力：从 GPT-2 到 GPT-4
- en: 原文：[https://towardsdatascience.com/exploring-creativity-in-large-language-models-from-gpt-2-to-gpt-4-1c2d1779be57](https://towardsdatascience.com/exploring-creativity-in-large-language-models-from-gpt-2-to-gpt-4-1c2d1779be57)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/exploring-creativity-in-large-language-models-from-gpt-2-to-gpt-4-1c2d1779be57](https://towardsdatascience.com/exploring-creativity-in-large-language-models-from-gpt-2-to-gpt-4-1c2d1779be57)
- en: Analyzing the evolution of creative processes in large language models through
    creativity tests
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过创造力测试分析大型语言模型中创意过程的演变
- en: '[](https://medium.com/@artfish?source=post_page-----1c2d1779be57--------------------------------)[![Yennie
    Jun](../Images/b635e965f21c3d55833269e12e861322.png)](https://medium.com/@artfish?source=post_page-----1c2d1779be57--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1c2d1779be57--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1c2d1779be57--------------------------------)
    [Yennie Jun](https://medium.com/@artfish?source=post_page-----1c2d1779be57--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@artfish?source=post_page-----1c2d1779be57--------------------------------)[![Yennie
    Jun](../Images/b635e965f21c3d55833269e12e861322.png)](https://medium.com/@artfish?source=post_page-----1c2d1779be57--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1c2d1779be57--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1c2d1779be57--------------------------------)
    [Yennie Jun](https://medium.com/@artfish?source=post_page-----1c2d1779be57--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1c2d1779be57--------------------------------)
    ·21 min read·Apr 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1c2d1779be57--------------------------------)
    ·阅读时间 21 分钟 ·2023年4月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7c92d68bd2a35c36d6c1f1499d438759.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c92d68bd2a35c36d6c1f1499d438759.png)'
- en: Creativity as envisioned by Midjourney. Generated by the author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Midjourney 设想中的创造力。由作者生成。
- en: '*This blog was originally posted at* [https://www.artfish.ai/p/exploring-creativity-in-large-language](https://www.artfish.ai/p/exploring-creativity-in-large-language)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*本博客最初发布于* [https://www.artfish.ai/p/exploring-creativity-in-large-language](https://www.artfish.ai/p/exploring-creativity-in-large-language)'
- en: In recent weeks, people have used large language models (LLMs) to generate a
    variety of creative content, such as [books](https://www.reuters.com/technology/chatgpt-launches-boom-ai-written-e-books-amazon-2023-02-21/),
    [flash fiction](https://blog.yenniejun.com/p/creative-writing-with-gpt-3-from),
    [rap battles](https://twitter.com/mehran__jalali/status/1639846978850021377?lang=en),
    and [music chords](/using-chatgpt-as-a-creative-writing-partner-part-2-music-d2fd7501c268).
    But is it possible to measure the level of creative process more broadly in these
    models?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几周，人们使用大型语言模型（LLMs）生成了各种创意内容，如 [书籍](https://www.reuters.com/technology/chatgpt-launches-boom-ai-written-e-books-amazon-2023-02-21/)、[闪小说](https://blog.yenniejun.com/p/creative-writing-with-gpt-3-from)、[说唱对决](https://twitter.com/mehran__jalali/status/1639846978850021377?lang=en)
    和 [音乐和弦](/using-chatgpt-as-a-creative-writing-partner-part-2-music-d2fd7501c268)。但是否有可能更广泛地衡量这些模型中的创意过程水平？
- en: Human creativity has long fascinated psychologists and researchers. [Starting
    in the 1950s](https://www.ideatovalue.com/podc/nickskillicorn/2021/04/the-1950-speech-that-started-creativity-research/),
    researchers created a series of tests to compare the creative performance and
    potential of individuals. While no single test can fully capture creativity, these
    tests attempted to measure and quantify different aspects of it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 人类创造力长期以来一直吸引着心理学家和研究人员。[自1950年代起](https://www.ideatovalue.com/podc/nickskillicorn/2021/04/the-1950-speech-that-started-creativity-research/)，研究人员创建了一系列测试，以比较个人的创造力表现和潜力。虽然没有单一测试能完全捕捉创造力，但这些测试尝试测量和量化其不同方面。
- en: 'In this article, I analyze the performance of GPT models from 2019 to 2023
    on tests that measure two kinds of creativity: convergent (a single, correct solution
    exists) and divergent (open ended; multiple solutions may exist) [[1](#footnote-1)].
    The tests include:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我分析了2019年到2023年间GPT模型在测量两种创造力的测试中的表现：收敛性（存在单一正确解决方案）和发散性（开放式；可能存在多种解决方案）[[1](#footnote-1)]。这些测试包括：
- en: Connecting three seemingly unrelated words with a fourth word ([Remote Associates
    Test](https://en.wikipedia.org/wiki/Remote_Associates_Test))
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用一个第四个词将三个看似不相关的词连接起来（[远程联想测试](https://en.wikipedia.org/wiki/Remote_Associates_Test)）
- en: Producing as many alternative uses for everyday objects ([Alternate Uses Task](https://en.wikipedia.org/wiki/Guilford%27s_Alternate_Uses))
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成尽可能多的日常物品的替代用途（[替代用途任务](https://en.wikipedia.org/wiki/Guilford%27s_Alternate_Uses)）
- en: Naming ten nouns that differ as much as possible from each other ([Divergent
    Association Task](https://en.wikipedia.org/wiki/Divergent_Association_Task))
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名十个尽可能不同的名词（[发散联想任务](https://en.wikipedia.org/wiki/Divergent_Association_Task)）
- en: Through these analyses, I focus on the change in performance from the earlier
    GPT-2 model all the way to the newest GPT-4 model. I use both a subjective and
    objective lens to understand the behavior of these models. In general, the later
    models tend to perform better on all of the creativity tasks, although I do find
    a few exceptions where, when the test has a “correct” answer”, the simpler GPT-2
    model outperforms the newer GPT4 model. When possible, I compare the GPT models
    to human performances on the same creativity tests.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些分析，我专注于从早期的 GPT-2 模型到最新的 GPT-4 模型的性能变化。我使用了主观和客观的视角来理解这些模型的行为。通常，较新的模型在所有创造性任务上表现更好，尽管我发现了一些例外情况，其中在测试有“正确”答案时，较简单的
    GPT-2 模型超越了更新的 GPT-4 模型。在可能的情况下，我将 GPT 模型与人类在相同创造性测试上的表现进行比较。
- en: '*Note: These experiments aim not to measure how creative the models are, but
    rather to measure the level of creative process present in the model generations.
    I am not claiming that the GPT models, or LLMs in general, possess creative thinking
    in the same way humans do. Rather, I aim to show how the models respond to particular
    measures of creative processes.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：这些实验的目的是测量模型生成中的创造性过程水平，而不是评估模型的创造力。我并不声称 GPT 模型或一般的 LLM 具有与人类相同的创造性思维。我旨在展示模型如何响应特定的创造性过程测量。*'
- en: A Progression of GPT Models
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT 模型的发展历程
- en: Generative Pre-trained Transformer (GPT) models are language models trained
    on vast portions of the Internet to predict the next token based on previous tokens.
    While the recent ChatGPT has captured the public’s attention in the last few months,
    OpenAI’s GPT models (and the technologies behind the models) have been around
    for a few years.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 生成预训练变换器（GPT）模型是训练在大量互联网数据上的语言模型，以根据前面的词预测下一个词。虽然最近的 ChatGPT 在过去几个月引起了公众的关注，但
    OpenAI 的 GPT 模型（以及这些模型背后的技术）已经存在几年了。
- en: GPT-1 (2018, 117M parameters). The original GPT model. I used the [open-sourced
    version on HuggingFace](https://huggingface.co/openai-gpt). However, the outputs
    were so gibberish so I did not include any of them in this article.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-1（2018年，117M 参数）。原始 GPT 模型。我使用了[HuggingFace 上的开源版本](https://huggingface.co/openai-gpt)。然而，输出结果非常无意义，所以我没有将它们包含在本文中。
- en: 'Paper: [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文：[通过生成预训练提升语言理解](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- en: '[GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdfhttps://en.wikipedia.org/wiki/GPT-2#:~:text=Generative%20Pre%2Dtrained%20Transformer%202,by%20OpenAI%20in%20February%202019.)
    (2019, 1.5B parameters). I used the [open-soured version on HuggingFace](https://huggingface.co/gpt2).
    For some experiments, GPT-2 outputs were gibberish and not useful for understanding
    its behavior, so I did not include those in the respective analyses. Paper: [Language
    Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT-2](https://en.wikipedia.org/wiki/GPT-2#:~:text=Generative%20Pre%2Dtrained%20Transformer%202,by%20OpenAI%20in%20February%202019.)（2019年，1.5B
    参数）。我使用了[HuggingFace 上的开源版本](https://huggingface.co/gpt2)。对于一些实验，GPT-2 的输出是无意义的，不利于理解其行为，所以我没有将这些结果包含在相应的分析中。论文：[语言模型是无监督多任务学习者](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)。'
- en: '[GPT-3](https://en.wikipedia.org/wiki/GPT-3) (2020, 175B parameters). Over
    100 times bigger than GPT-2\. I used *text-davinci-002* from the [OpenAI APIs](https://platform.openai.com/docs/models/gpt-3-5).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT-3](https://en.wikipedia.org/wiki/GPT-3)（2020年，175B 参数）。比 GPT-2 大超过 100
    倍。我使用了来自[OpenAI APIs](https://platform.openai.com/docs/models/gpt-3-5)的*text-davinci-002*。'
- en: 'Paper: [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文：[语言模型是少样本学习者](https://arxiv.org/pdf/2005.14165.pdf)
- en: GPT-3.5 (2022). While there are multiple versions of GPT-3.5, I use the same
    model underlying the original [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT),
    which was trained using reinforcement learning with human feedback. I used *gpt-3.5-turbo*
    from the [OpenAI APIs](https://platform.openai.com/docs/models/gpt-3-5).
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-3.5（2022）。虽然 GPT-3.5 有多个版本，我使用的是与原版 [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT)
    相同的模型，该模型通过带有人工反馈的强化学习进行训练。我使用了来自 [OpenAI APIs](https://platform.openai.com/docs/models/gpt-3-5)
    的 *gpt-3.5-turbo*。
- en: '[GPT-4](https://arxiv.org/abs/2303.08774) (2023, ~100T parameters). I used
    *gpt-4* from the [OpenAI APIs](https://platform.openai.com/docs/models/gpt-3-5).'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT-4](https://arxiv.org/abs/2303.08774)（2023，约 100T 参数）。我使用了来自 [OpenAI APIs](https://platform.openai.com/docs/models/gpt-3-5)
    的 *gpt-4*。'
- en: 'Paper: [GPT-4 Technical Report](https://arxiv.org/pdf/2303.08774.pdf).'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '论文: [GPT-4 技术报告](https://arxiv.org/pdf/2303.08774.pdf)。'
- en: For all of the experiments in this article, unless otherwise mentioned, I called
    each GPT model five times per unique prompt to account for randomness in its generations.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文的所有实验中，除非另有说明，我对每个独特的提示调用了每个 GPT 模型五次，以考虑其生成的随机性。
- en: '1\. Remote Associates Test: Connecting Unrelated Words'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 远程联想测试：连接无关词
- en: The Remote Associates Test (RAT) is a creativity test developed by Martha Mednick
    in 1962 [[2](#footnote-2)]. This test measures convergent thinking, meaning that
    **each question has a “right” answer.**
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 远程联想测试（RAT）是由玛莎·梅德尼克（Martha Mednick）于 1962 年开发的一种创造力测试 [[2](#footnote-2)]。该测试衡量的是聚合思维，这意味着**每个问题都有一个“正确”的答案**。
- en: Given three seemingly unrelated words, the test taker must find a fourth word
    connecting the three words. For example, the words “dream”, “break”, and “light”
    can be linked by the word “day” (daydream, daybreak, daylight). The words “sense”,
    “courtesy”, and “place” can be linked by the word “common”.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 给定三个看似无关的词，测试者必须找到一个第四个词来连接这三个词。例如，“dream”、“break”和“light”可以通过“day”（daydream,
    daybreak, daylight）来联系。词汇“sense”、“courtesy”和“place”可以通过“common”来联系。
- en: I collected the word triplets from [the Remote Associates Test website](https://www.remote-associates-test.com/).
    **There were a total of 312 questions.** The difficulty of each question was rated
    on a scale from Very Easy to Very Hard.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我从 [远程联想测试网站](https://www.remote-associates-test.com/) 收集了词汇三元组。**总共有 312 道问题**。每个问题的难度按从非常容易到非常困难的等级进行评分。
- en: '![](../Images/2f2d6ab51eb2f16a24735f7e563ff4a4.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f2d6ab51eb2f16a24735f7e563ff4a4.png)'
- en: Breakdown of the Random Association Test questions taken from the [test website](https://www.remote-associates-test.com/).
    Created by the author.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从 [测试网站](https://www.remote-associates-test.com/) 获取的随机联想测试题目分解。由作者创建。
- en: For each model and each triplet of words, I used the following prompt template.
    I called each model 5 times to account for randomness.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个模型和每组三个词，我使用了以下提示模板。我调用了每个模型 5 次，以考虑随机性。
- en: You are given three words. Think of a fourth word that is somehow related to
    each of the first three words. First give the answer (“Answer”) then explain your
    reasoning (“Reason”).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 给定三个词。想出一个与前三个词都有某种关联的第四个词。首先给出答案（“Answer”），然后解释你的推理（“Reason”）。
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Newer GPT models score higher on remote association
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 较新的 GPT 模型在远程联想上得分更高
- en: I calculated the overall percent of questions each GPT model generated a correct
    answer for. It may come as no surprise that the the newer the model, the more
    correct answers it generated.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我计算了每个 GPT 模型生成正确答案的总体百分比。可能并不奇怪，**模型越新，生成的正确答案越多**。
- en: '![](../Images/bdaad2048a6c8c37702328e4b74d82bd.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdaad2048a6c8c37702328e4b74d82bd.png)'
- en: Percentage of questions each GPT model answered correctly on the Remote Associates
    Test. Created by the author.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 GPT 模型在远程联想测试中正确回答问题的百分比。由作者创建。
- en: 'Breaking down model performance by question difficulty level showed a similar
    trend: newer GPT models performed better than older ones on each difficulty level.
    And while the newer GPT models performed better than the older ones on hard questions,
    the improvement was less drastic than for easier questions. This means that **even
    GPT-4 struggled on difficult questions.**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 按照问题难度级别分解模型表现显示了类似的趋势：较新的 GPT 模型在每个难度级别上的表现均优于旧模型。虽然较新的 GPT 模型在难题上的表现优于旧模型，但这种改进比在简单问题上的改进要少。这意味着**即使是
    GPT-4 在困难问题上也感到吃力**。
- en: '![](../Images/a0449267a411f9f1f2c16850305c1c9e.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0449267a411f9f1f2c16850305c1c9e.png)'
- en: Percentage of questions each GPT model answered correctly on the Remote Associates
    Test, broken down by question difficulty level. Created by the author.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每个GPT模型在远程联想测试中按问题难度级别的正确回答百分比。由作者创建。
- en: According to the Remote Associates Test website, “A typical person can solve
    most of the items marked as easy, about half of the medium ones, and few of the
    hard ones.” There were no official statistics shared on the website, but a quick
    glance would show that GPT-4 did indeed solve around half of the medium ones and
    a few of the hard ones, making it the only model approximately comparable to human
    level. The other GPT models were worse than human performance.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 根据远程联想测试网站，“典型的人可以解决大多数标记为简单的问题，大约一半的中等难度问题，以及很少的难问题。” 网站上没有分享官方统计数据，但从快速浏览来看，GPT-4
    确实解决了大约一半的中等难度问题和一些难问题，使其成为唯一一个大致可与人类水平相媲美的模型。其他 GPT 模型的表现则逊色于人类水平。
- en: 'A few exceptions: when GPT-2 gets it right but GPT-4 gets it wrong'
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些例外：当GPT-2回答正确而GPT-4回答错误
- en: While it didn’t happen often, there were a few questions for which the more
    rudimentary GPT-2 model from 2019 was able to generate the correct answer but
    the GPT-4 model from 2023 was not able to. In fact, there were 8 such questions.
    (The full list is shown in the Appendix at the end of the article).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管不常发生，但确实有一些问题，2019年的更基础的GPT-2模型能够生成正确答案，而2023年的GPT-4模型却不能。实际上，有8个这样的例子。（完整列表见文章末尾的附录）。
- en: '**An easy question**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**一个简单的问题**'
- en: 'Words: cottage / brick / cake'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词：小屋 / 砖块 / 蛋糕
- en: 'GPT-2: “cheese” [correct answer]'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2：“奶酪” [正确答案]
- en: 'GPT-4: “layer”'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4：“层”
- en: '**A medium question**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**一个中等难度的问题**'
- en: 'Words: cat / number / phone'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词：猫 / 数字 / 电话
- en: 'GPT-2: “call”'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2：“呼叫”
- en: 'GPT-4: “dial”'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4：“拨号”
- en: '**A hard question**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**一个难的问题**'
- en: 'Words: through / disk / way'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词：通过 / 磁盘 / 路径
- en: 'GPT-2: “drive” [correct]'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2：“驾驶” [正确]
- en: 'GPT-4: “circle”'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4：“圆圈”
- en: '**A very hard question**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**一个非常难的问题**'
- en: 'Words: land / hand / house'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词：土地 / 手 / 房子
- en: 'GPT-2: “farm” [correct]'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2：“农场” [正确]
- en: 'GPT-4: “property”'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4：“财产”
- en: Notice that for all of these questions, GPT-4’s answers are *not* unreasonable.
    In fact, without knowing the correct answer beforehand, you might assume that
    GPT-4’s answer could as likely connect the three words. If you’re thinking that,
    you’re not wrong. I’ll address this in the next section.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于所有这些问题，GPT-4 的回答是 *不* 不合理的。实际上，如果事先不知道正确答案，你可能会认为 GPT-4 的答案同样可以将这三个词联系起来。如果你这样想，你并没有错。我将在下一部分讨论这个问题。
- en: Was GPT-4 wrong or just thinking outside of the box?
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-4 是错误的，还是只是思维方式不同？
- en: GPT-4 answered more than half of the questions incorrectly, but maybe with good
    reason. I asked GPT-4 to explain each of its answers. Then, I looked at its explanations
    for a few of the “Very Hard” questions that it got wrong.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 错误地回答了超过一半的问题，但可能有充分的理由。我让 GPT-4 解释了每一个答案。然后，我查看了它对几个“非常难”问题的解释。
- en: '**Words: artist / hatch / route**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**单词：艺术家 / 孵化 / 路线**'
- en: 'Correct answer: “escape”'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确答案：“逃脱”
- en: 'GPT-4 answer: “map”'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4 答案：“地图”
- en: The word “artist” might bring to mind the idea of creating something, perhaps
    even a visual representation of something. Maps are a common form of visual representation
    that require a degree of artistic skill to create.
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “艺术家”这个词可能会让人想到创造某些东西，甚至是某种视觉表现形式。地图是一种常见的视觉表现形式，需要一定的艺术技能来制作。
- en: ''
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The word “hatch” could refer to the act of opening something or bringing something
    into existence. A map is a type of blueprint or plan, often used to navigate new
    territory or chart out a course of action.
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “孵化”这个词可以指打开某物或使某物存在的行为。地图是一种蓝图或计划，通常用于导航新领土或规划行动路线。
- en: ''
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The word “route” is related to direction and movement, which can be plotted
    and tracked on a map. A map can help you determine the best route to take when
    traveling, and can also provide information about the routes others have taken
    in the past.
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “路线”这个词与方向和运动相关，这些都可以在地图上绘制和追踪。地图可以帮助你确定旅行时最佳的路线，并且可以提供关于其他人过去所走过路线的信息。
- en: ''
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All in all, “Map” seems to be a word that fits well with each of the three initial
    words, as it encapsulates the ideas of artistic creation, planning and direction,
    and movement and navigation.
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 总的来说，“地图”似乎是一个与最初的三个词相符的词，因为它包含了艺术创作、规划和方向，以及运动和导航的概念。
- en: GPT-4’s explanation for why “map” is a good word to associate with “artist /
    hatch / route” is convincing.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 对于为什么“地图”是一个与“艺术家 / 孵化 / 路线”相关的好词的解释很有说服力。
- en: 'Here is another example:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一个例子：
- en: '**Words: fork / dark / man**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**词汇：fork / dark / man**'
- en: 'Correct answer: “pitch”'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确答案：“pitch”
- en: 'GPT-4 answer: “shadow”'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4的答案：“shadow”
- en: The word “fork” can be associated with division or branching, such as a fork
    in the road. Shadows can also be seen as a type of division, as they represent
    the absence of light.
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “fork”这个词可以与分裂或分支相关联，比如道路上的分叉。阴影也可以被视为一种分裂，因为它们代表了光的缺失。
- en: ''
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The word “dark” refers to the absence of light. Shadows are created when light
    is blocked by an object, and can often appear darker than the surrounding area.
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “dark”这个词指的是光的缺失。当光被物体阻挡时，阴影会形成，且往往会比周围区域显得更暗。
- en: ''
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The word “man” can be associated with the concept of identity or individuality.
    Shadows can also be used to represent the identity of a person, as they can provide
    clues about their shape and size.
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “man”这个词可以与身份或个体性概念相关联。阴影也可以用来表示一个人的身份，因为它们可以提供有关他们形状和大小的线索。
- en: ''
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Therefore, “Shadow” seems to be a word that fits well with each of the initial
    three words, as it encapsulates the ideas of division, absence, and identity.
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因此，“阴影”似乎是一个很好地适应最初三个词的词汇，因为它包含了分裂、缺失和身份的概念。
- en: These examples show the pitfall of the Remote Associates Test, and of convergent
    creativity tests in general — that is, when the “correct” answer may not be the
    only “right” answer. In these cases, GPT-4 generated an answer that was “wrong”,
    but only because it did not meet the conventional “correct” answer.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子展示了远程联想测试的陷阱，以及一般收敛创造力测试的陷阱——即“正确”答案可能并不是唯一的“对”的答案。在这些情况下，GPT-4生成了一个“错误”的答案，但仅仅是因为它没有符合传统的“正确”答案。
- en: In fact, if I ask GPT-4 to brainstorm multiple options for a fourth word for
    a triplet of words, it is able to give me multiple options with convincing answers.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，如果我让GPT-4为一组三个词中的第四个词进行头脑风暴，它能够给出多个选项，并提供令人信服的答案。
- en: '![](../Images/efe8aafae763d1ba30351195a4efd50b.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efe8aafae763d1ba30351195a4efd50b.png)'
- en: GPT-4 responses to generate multiple “right” answers for a triplet of words.
    Screenshot taken by the author.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4生成多个“正确”答案以回应一组三个词。截图由作者拍摄。
- en: GPT-4 answers arbitrarily generated triplet of words
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-4答案任意生成的三个词组合
- en: It is possible that the GPT models may have seen some of the above Remote Associates
    Test questions in its training data. How would GPT-4 respond to an arbitrarily
    generated triplet of words? In this section, I construct my own “remote association”
    test and evaluate the outputs subjectively.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型可能已经在其训练数据中看到了一些上述远程联想测试问题。GPT-4会如何回应任意生成的三个词组合？在这一部分，我构建了我自己的“远程联想”测试，并主观地评估了输出结果。
- en: I took a list of the [10,000 most common English words](https://github.com/first20hours/google-10000-english)
    [[3](#footnote-3)] Then I randomly sampled three words from this list and had
    GPT-4 provide a fourth word tying these words together. Some of the three-word
    combinations were quite strange and unrelated, but GPT-4 always found a way to
    tie them together. I encourage the reader to try these first before reading GPT-4’s
    generations!
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我从[10,000个最常见的英语单词](https://github.com/first20hours/google-10000-english) [[3](#footnote-3)]中随机抽取了三个单词，并让GPT-4提供一个将这些单词联系在一起的第四个单词。这些三词组合中的一些相当奇怪且不相关，但GPT-4总是能找到将它们联系在一起的方式。我鼓励读者在阅读GPT-4的生成结果之前先尝试这些！
- en: After viewing the next few examples, you can make your decision — is GPT-4 thinking
    outside of the box? Or is it just shoehorning its answers? (A longer list of examples
    is shown in the Appendix at the end of the article).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看接下来的几个例子后，你可以做出决定——GPT-4是在跳出框框思考吗？还是只是将答案勉强套入框框？（文章末尾的附录中展示了更长的例子列表）。
- en: ⛑️ **Health**
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ⛑️ **健康**
- en: This response read as quite convincing. I found it interesting that the definition
    for “health surveillance” was copied almost verbatim from the [CDC website on
    Introduction to Public Health Surveillance](https://www.cdc.gov/training/publichealth101/surveillance.html#anchor_available_materials).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这个回应显得相当有说服力。我发现“健康监测”的定义几乎是从[CDC公共健康监测介绍](https://www.cdc.gov/training/publichealth101/surveillance.html#anchor_available_materials)网站上逐字复制的，这一点很有趣。
- en: '*Public health surveillance is “the ongoing, systematic collection, analysis,
    and interpretation of health-related data essential to planning, implementation,
    and evaluation of public health practice.” —* Field Epidemiology'
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*公共健康监测是“对与健康相关的数据进行持续、系统的收集、分析和解释，这些数据对于规划、实施和评估公共健康实践至关重要。” —* 领域流行病学'
- en: (from the CDC website)
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: （来自CDC网站）
- en: '[PRE1]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**🍀 Clovers**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**🍀 三叶草**'
- en: I was initially convinced by GPT-4’s answer — in fact, it felt quite clever.
    That is, until I did a quick Google search and found out that *Trifolium duodecim*
    is not real. This is an example of a [hallucination](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)),
    a common phenomena in which LLMs make up an answer that is not factually correct.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我最初对 GPT-4 的回答感到信服——事实上，这个回答感觉相当聪明。直到我做了一个快速的 Google 搜索，发现*Trifolium duodecim*
    并不存在。这是一个[幻觉](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence))的例子，幻觉是指大型语言模型编造出不准确的答案的常见现象。
- en: '[PRE2]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**☕️ Starbucks**'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**☕️ 星巴克**'
- en: This example was my favorite the answer, while thinking out of the box, felt
    like kind of a stretch. And it was a little bit funny (especially the part about
    Patrick).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子是我最喜欢的，虽然它跳出了常规思维，但感觉有点牵强。而且这有点搞笑（尤其是关于 Patrick 的部分）。
- en: '[PRE3]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: These examples and analyses illustrate that creativity tests with a single correct
    answer might be limited. This limitation is addressed in the other tests explored
    in this article in the next sections.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子和分析说明了具有单一正确答案的创造力测试可能存在局限性。这一局限性在本文下一部分探讨的其他测试中得到了补充。
- en: '2\. Alternate Uses Task: Finding Meaning in Ordinary Items'
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 替代用途任务：在普通物品中寻找意义
- en: Another popular test for measuring creativity is the Alternative Uses Task (AUT),
    which was created by J. P. Guilford in 1967\. Unlike the Remote Associations Test,
    this test measures divergent thinking, meaning that **there is** **no right answer.**
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的测量创造力的测试是替代用途任务（AUT），由 J. P. Guilford 于 1967 年创建。与远程联想测试不同，这个测试测量发散性思维，这意味着**没有正确答案**。
- en: The test taker is asked to produce as many original uses for an everyday object
    as possible. For example, a “laptop” can be used as a “plate”, as a “body shield”,
    or as “wall art” [[4](#footnote-4)]. Responses to this test are measured in terms
    of metrics such as originality (how different the uses are) and flexibility (the
    assortment of ideas). The purpose of this test is to see how many unconventional
    uses a person can think of for an ordinary object.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 测试者被要求为一个日常物品提供尽可能多的原创用途。例如，一个“笔记本电脑”可以用作“盘子”、“护体盾牌”或“墙面艺术”[[4](#footnote-4)]。对这个测试的回答通过原创性（用途的不同程度）和灵活性（创意的多样性）等指标进行评估。此测试的目的是查看一个人能为普通物品想出多少种非常规用途。
- en: However, due to my limited resources, I did not have the ability to generate
    these measures without human annotators. Therefore, I used a **proxy for creativity,**
    which was also used by researchers from the University of Amsterdam in [Putting
    GPT-3’s Creativity to the (Alternative Uses) Test](https://arxiv.org/pdf/2206.08932.pdf).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于资源有限，我没有能力在没有人工标注者的情况下生成这些测量值。因此，我使用了**创造力的代理**，这也是阿姆斯特丹大学的研究人员在[《对 GPT-3
    创造力的（替代用途）测试》](https://arxiv.org/pdf/2206.08932.pdf)中使用的方法。
- en: 'In this paper, the researchers asked both humans and GPT-3 (davinci-002) to
    come up with alternate use cases for the following common objects: fork, tin can,
    and book. As a proxy for gauging creativity, they measured the *semantic distance*
    between the object and the generated use cases — that is, how different is the
    generated use case word from the original object?'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，研究人员要求人类和 GPT-3（davinci-002）为以下常见物品提出替代用途：叉子、铁罐和书籍。作为衡量创造力的代理，他们测量了*语义距离*——即生成的用途与原始物品之间的差异有多大？
- en: I obtained the human responses from [the paper’s project website](https://osf.io/vmk3c/?view_only=)
    for comparison with human performance. I used the following prompt for the GPT
    models, based on a similar prompt used int he paper.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我从[论文项目网站](https://osf.io/vmk3c/?view_only=)获取了人工回答，以便与人类表现进行比较。我使用了以下提示供 GPT
    模型使用，基于论文中使用的类似提示。
- en: 'Come up with 10 creative uses for the following item that might strike people
    as clever, unusual, interesting, uncommon, humorous, innovative, or different:
    {item}'
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为以下物品想出 10 种可能让人觉得聪明、不同寻常、有趣、不常见、幽默、创新或与众不同的创造性用途：{item}
- en: Cosine Distance between embeddings as proxy for creativity
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入向量之间的余弦距离作为创造力的代理
- en: In natural language processing, semantic distance (or [semantic similarity](https://en.wikipedia.org/wiki/Semantic_similarity))
    is a proxy for how related two words are, based on how often words are used together
    in similar contexts. For example, the words “computer” and “screen” might have
    a shorter semantic distance, because they are often used in the same contexts,
    whereas the words “computer” and “lentils” might have a greater semantic distance,
    because they are not often used in the same contexts.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理领域，语义距离（或[语义相似性](https://en.wikipedia.org/wiki/Semantic_similarity)）是基于词汇在相似上下文中一起使用的频率来衡量两个词汇相关程度的代理。例如，“计算机”和“屏幕”这两个词可能有较短的语义距离，因为它们常常在相同的上下文中使用，而“计算机”和“扁豆”这两个词的语义距离可能较大，因为它们很少在相同的上下文中出现。
- en: To compute the semantic distance of two words, I used the same approach as used
    by the researchers in [the paper mentioned above.](https://arxiv.org/pdf/2206.08932.pdf)
    I used [Spacy](https://spacy.io/models/en) (`en_core_web_lg`), a natural language
    processing library, to calculate word embeddings for the object (e.g. “laptop”)
    and the use case word (e.g. “plate”). Then, I used cosine distance between the
    two embeddings to calculate the semantic distance.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算两个词的语义距离，我使用了与研究人员在[上述论文](https://arxiv.org/pdf/2206.08932.pdf)中使用的相同方法。我使用了[Spacy](https://spacy.io/models/en)
    (`en_core_web_lg`)，一个自然语言处理库，来计算物体（例如“笔记本电脑”）和使用案例词汇（例如“盘子”）的词嵌入。然后，我使用这两个嵌入之间的余弦距离来计算语义距离。
- en: For the Alternate Uses Task, **larger semantic distances correlated with “more
    creative”** use cases for objects. The results showed that for the three objects
    (fork, tin can, and book), human performance reliably resulted in higher cosine
    distances (and therefore higher semantic dissimilarity) than for GPT-3\. However,
    there were cases when GPT-3.5 and/or GPT-4 matched or surpassed human creativity
    semantic distance scores.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于替代用途任务，**较大的语义距离与“更具创造性”**的物体使用案例相关。结果显示，对于三个物体（叉子、锡罐和书），人类的表现通常导致更高的余弦距离（因此更高的语义差异）相比于GPT-3。然而，也有一些情况下，GPT-3.5和/或GPT-4达到了或超越了人类创造力的语义距离得分。
- en: '![](../Images/39de38d8fc97ba32e9443e9288a67949.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39de38d8fc97ba32e9443e9288a67949.png)'
- en: Semantic distance (measured using cosine distance) of original word to alternate
    use case. Created by author.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 原始词汇与替代使用案例的语义距离（使用余弦距离测量）。由作者创建。
- en: While humans tended to come up with more creative use cases than GPT-3 (using
    semantic distance, as calculated by cosine distance, as a proxy for creativity),
    GPT-4 was more on par with human level. That is, GPT-4 responded with items resulting
    in larger semantic distances with the original item, at a similar level to human
    performance. However, without actual humans annotating and deciding more subjective
    categories such as originality, flexibility, and usefulness, this quantification
    using cosine distance is limited as it only measures a small part of the GPT models’
    creative outputs.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然人类倾向于提出比GPT-3更具创造性的使用案例（使用余弦距离作为创造力的代理），GPT-4的表现更接近于人类水平。也就是说，GPT-4的回应生成了与原始项目相比具有较大语义距离的项目，其水平与人类表现相似。然而，由于没有实际的人工标注和决定更主观的类别如原创性、灵活性和实用性，这种使用余弦距离的量化方法是有限的，因为它只测量了GPT模型创造性输出的一小部分。
- en: Can you distinguish human-generated vs. AI-generated use cases?
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你能分辨出人工生成和AI生成的使用案例吗？
- en: One of the lists shows the human-generated alternate use cases for a tin can
    and the other shows alternate use cases generated by GPT-4\. Can you tell the
    difference? The answer is at the end.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 列表之一展示了人工生成的锡罐替代使用案例，另一个则展示了由GPT-4生成的替代使用案例。你能分辨出区别吗？答案在最后。
- en: 'List 1: Uses for tin can'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 1：锡罐的用途
- en: as a mirror
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为镜子
- en: to create toys
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用来制作玩具
- en: as art
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为艺术品
- en: as a reminder of Andy Warhol
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为对安迪·沃荷的提醒
- en: as a key ring with the clip from the can
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为带有罐子夹的钥匙圈
- en: as jewelry
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为珠宝
- en: 'List 2: Uses for tin can'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2：锡罐的用途
- en: miniature drum set
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微型鼓套件
- en: quirky plant pot
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 古怪的花盆
- en: impromptu cookie cutter
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 临时饼干切割器
- en: homemade camp stove
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自制营炉
- en: whimsical wind chimes
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异想天开的风铃
- en: miniature herb garden
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微型草药花园
- en: The next test in the following section also measures divergent thinking.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分的测试也测量了发散性思维。
- en: '3\. Divergent Association Task: Ten Words, No Connections'
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 发散联想任务：十个词，没有连接
- en: The third creativity test is the Divergent Association Task (DAT), created more
    recently in 2021 by a group of researchers [[5](#footnote-5)]. This test involves
    naming ten nouns that differ from each other as much as possible. This test, as
    evident by its name, is divergent and also **has no correct answers.**
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个创造力测试是发散性联想任务（DAT），由一组研究人员在2021年最近创建[[5](#footnote-5)]。此测试要求命名十个尽可能不同的名词。显而易见，这项测试是发散性的，而且**没有正确答案**。
- en: 'There is a [version of the test online on this website](https://www.datcreativity.com/).
    It only takes a few minutes to take, as it involves coming up with a list of ten
    nouns as unrelated as possible. Here’s my result from the website:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个[在线测试版本](https://www.datcreativity.com/)。完成测试只需几分钟，因为它涉及列出十个尽可能不相关的名词。以下是我从网站上获得的结果：
- en: '![](../Images/aa3a465a40574d28ff017b98685d1feb.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa3a465a40574d28ff017b98685d1feb.png)'
- en: Screenshot taken from the Divergent Association Task. Created by the author.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 从发散性联想任务中截取的屏幕截图。由作者创建。
- en: The DAT Score
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DAT分数
- en: 'How is the score calculated? The authors describe their process [here](https://www.datcreativity.com/faq).
    The DAT score is calculated using embeddings and cosine distance. I adapt their
    process for calculating the score [[6](#footnote-6)]:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如何计算分数？作者在[这里](https://www.datcreativity.com/faq)描述了他们的过程。DAT分数是通过嵌入和余弦距离计算的。我对他们的计算分数过程进行了调整[[6](#footnote-6)]：
- en: Calculate [GloVe](https://nlp.stanford.edu/projects/glove/) embeddings for each
    word
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个单词的[GloVe](https://nlp.stanford.edu/projects/glove/)嵌入
- en: Calculate the cosine distance between each pair of words
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每对单词之间的余弦距离
- en: Take the average of that value
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取该值的平均值
- en: This process is similar to the method used to calculate creativity proxy score
    in the Alternate Uses Task, with the only difference being the word embedding
    model used.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程类似于在替代用途任务中计算创造力代理分数的方法，唯一的区别是使用了不同的单词嵌入模型。
- en: Comparing Divergence among models
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较不同模型之间的差异
- en: The following prompt, which I use to prompt the GPT models, is taken from the
    [DAT website](https://www.datcreativity.com/task).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我用来提示GPT模型的以下提示语取自[DAT网站](https://www.datcreativity.com/task)。
- en: '[PRE4]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: I did not include GPT-2 in the results below, as the outputs generated by GPT-2
    were gibberish. (You can see an example of this in the Appendix at the end of
    the article).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有在下面的结果中包括GPT-2，因为GPT-2生成的输出是乱码。（你可以在文章末尾的附录中看到一个示例）。
- en: The results show that the DAT score increased for each newer model. This means
    that later models were better at generating 10 words semantically different from
    each other as possible.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，每个更新的模型DAT分数都提高了。这意味着后来的模型在生成尽可能语义不同的10个单词方面表现更好。
- en: '![](../Images/47b6d5b8fda43230d69db455aa67f200.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47b6d5b8fda43230d69db455aa67f200.png)'
- en: Divergent Association Test scores, as measured by cosine distance, for the different
    models. Generated by the author.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 按照余弦距离测量的不同模型的发散性联想测试分数。由作者生成。
- en: 'For example, the following list generated by GPT-3 had a score of 0.36:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下是GPT-3生成的列表，其分数为0.36：
- en: '[PRE5]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'On the other hand, the following list generated by GPT-4 had a score of 0.95:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，以下是GPT-4生成的列表，其分数为0.95：
- en: '[PRE6]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: According to the [paper introducing the DAT test](https://www.pnas.org/doi/10.1073/pnas.2022340118),
    scores commonly ranged from 65 to 90, with an average between 75 and 80\. A score
    of 95 was considered very high.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[介绍DAT测试的论文](https://www.pnas.org/doi/10.1073/pnas.2022340118)，分数通常在65到90之间，平均在75到80之间。95的分数被认为非常高。
- en: Something to note in the above figure is that the error bars for GPT-4 were
    much smaller than those for GPT-3\. The significance of this becomes much clearer
    in the next experiment, in which I varied the amount of randomness in each GPT
    model’s generation by tweaking the “temperature” parameter.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 上图中需要注意的是，GPT-4的误差条远小于GPT-3的。这一点在下一个实验中会更加清晰，我在实验中通过调整“温度”参数来改变每个GPT模型生成内容的随机性。
- en: Temperature is a value between 0 and 1, with values closer to 0 being the most
    predictable and 1 being the most random. I tried temperature values between 0.5
    and 0.9 in increments of 0.1 (In previous experiments I kept the temperature at
    0.7, which is the default value). I expected more variance for higher temperature
    values and more consistency for lower temperatures.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 温度是一个介于0和1之间的值，接近0的值最具可预测性，接近1的值则最为随机。我尝试了介于0.5和0.9之间的温度值，每次递增0.1（在之前的实验中，我将温度保持在0.7，这是默认值）。我期望更高温度值会带来更多的变化，而较低的温度则会更一致。
- en: However, the results showed that the temperature did not matter as much as the
    model in terms of semantic distance variance. For GPT-3, despite the temperature,
    the DAT score for each list of 10 words varied greatly. Whereas, on the other
    hand, for GPT-4, despite the temperature, the DAT score remained tightly consistent
    in a small range with very little variance.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，结果显示温度对语义距离变异性的影响不如模型本身重要。对于GPT-3而言，尽管有温度的影响，每组10个词的DAT得分变化很大。而对于GPT-4而言，尽管有温度的影响，DAT得分在一个小范围内保持了高度一致，变化非常小。
- en: '![](../Images/b7c569d6c2a668b897761c671eca6ea5.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7c569d6c2a668b897761c671eca6ea5.png)'
- en: Average cosine distance among word embeddings, separated by model type and temperature
    score. Created by the author.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 按模型类型和温度分数分开的词嵌入的平均余弦距离。作者创建。
- en: The implications of this became clearer once I looked at the actual text being
    generated by the three GPT models. **GPT-4 was way less variable in its generations
    than GPT-3.** In a way, it felt like GPT-4 kind of already knew the “right answer”
    for a task that is supposed to have no right answer.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我查看了三种GPT模型生成的实际文本，这一点变得更加清晰。**GPT-4在生成词汇时的变异性远低于GPT-3。** 从某种程度上说，感觉GPT-4似乎已经知道了一个应该没有正确答案的任务的“正确答案”。
- en: Aggregating the responses for all temperature scores, I had each model generate
    a list of 10 words 50 times. Then, I looked at how common different words appeared
    across all 50 runs. Note that the most commonly generated word by GPT-3 was book,
    which appeared 19 times out of 50 = 38%. On the other hand, the most commonly
    generated word by GPT-4 was microscope, which appeared **48/50=96% of the time**.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 汇总所有温度分数的响应，我让每个模型生成10个词汇的列表50次。然后，我查看了不同词汇在所有50次运行中的出现频率。值得注意的是，GPT-3最常生成的词是book，出现了50次中的19次
    = 38%。另一方面，GPT-4最常生成的词是显微镜，出现了**48/50=96%**。
- en: '![](../Images/323fb7c8bfe263f62d4408741add0e58.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/323fb7c8bfe263f62d4408741add0e58.png)'
- en: Top words generated by each model for the DAT test (aggregated across temperature
    scores). Created by the words.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型为DAT测试生成的顶级词汇（根据温度分数汇总）。由词汇创建。
- en: This validates the claim that GPT-3 is, in fact, more variable in its word generations,
    while GPT-4 played it safe by predicting a lot of the same words it knew would
    score high on the “diverse words” tasks — words like microscope, volcano, elephant,
    and galaxy.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这验证了GPT-3在生成词汇时确实比GPT-4更具变异性，而GPT-4则采取了更为保守的策略，预测了许多它知道会在“多样化词汇”任务中得高分的词汇——如显微镜、火山、大象和银河。
- en: This finding illustrates how GPT-4 seems to have “cracked the code” for generating
    what it considers to be diverse words. If this is the case, I’m not sure if I
    can confidently say that GPT-4 is being creative more so than it has reached a
    local optima for “how to best play this particular game.”
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这一发现展示了GPT-4似乎已经“破解了生成多样化词汇的密码”。如果是这样，我不确定是否可以自信地说GPT-4的创造力更高，还是它已经找到了一个“如何最好地玩这个特定游戏”的局部最优解。
- en: Conclusion
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: '*“We can imagine a future in which GPT-3 and other generative LLMs responses
    cannot be distinguished from humans, although the creative process will be different.”'
  id: totrans-174
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“我们可以设想一个未来，在这个未来中，GPT-3和其他生成型LLMs的响应无法与人类区分，尽管创造力过程将会不同。”*'
- en: — from* [*Putting GPT-3’s Creativity to the (Alternative Uses) Test*](https://arxiv.org/abs/2206.08932)
  id: totrans-175
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: — 来自* [*GPT-3 创造力的另类用途测试*](https://arxiv.org/abs/2206.08932)
- en: It is challenging to accurately measure something so complex and multifaceted
    as creativity using a limited set of text-based tests. Although tests like the
    [Visual Remote Associates Test](https://www.frontiersin.org/articles/10.3389/fpsyg.2020.00026/full)
    attempt to address this issue by incorporating a visual element, there remain
    inherent limitations to such assessments. Convergent tests (such as the Remote
    Associates Test), which have a “right answer,” may not always reward the most
    creative or correct solution. On the other hand, divergent tests (such as the
    Divergent Association Test), which do not have a specific right answer, rely on
    word embeddings that are imperfect and may contain biases.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 使用有限的文本基础测试准确衡量如此复杂多面的创造力是具有挑战性的。虽然像[视觉远程联想测试](https://www.frontiersin.org/articles/10.3389/fpsyg.2020.00026/full)这样的测试通过加入视觉元素来尝试解决这个问题，但这些评估仍然存在固有的局限性。具有“正确答案”的聚合测试（例如远程联想测试）可能不会总是奖励最具创造性或正确的解决方案。另一方面，没有特定正确答案的发散测试（例如发散联想测试）依赖于不完美的词嵌入，并且可能包含偏差。
- en: Furthermore, the performance of GPT models on these tests could be influenced
    by their exposure to the tasks during training, potentially leading to data overlap.
    For instance, GPT-4’s performance on the Divergence Association Test demonstrated
    a peculiar repetition of words, suggesting it may have had prior knowledge of
    the “right” answers. It is crucial to develop alternative testing methods, as
    there have been [rumors of GPT-4 being trained on test data](https://www.reddit.com/r/MachineLearning/comments/124eyso/n_openai_may_have_benchmarked_gpt4s_coding/),
    particularly in coding tasks.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，GPT模型在这些测试中的表现可能会受到训练过程中对任务的暴露影响，可能导致数据重叠。例如，GPT-4在发散联想测试中的表现展示了单词的特殊重复，表明它可能提前了解了“正确”的答案。开发替代测试方法至关重要，因为有[关于GPT-4接受测试数据训练的传闻](https://www.reddit.com/r/MachineLearning/comments/124eyso/n_openai_may_have_benchmarked_gpt4s_coding/)，特别是在编码任务中。
- en: Despite these limitations, creativity tests can serve as valuable benchmarks
    for comparing and tracking the performance of large language models. Future research
    experiments could repeat those from this article in other languages to gauge the
    creativity of GPT models across linguistic domains. By doing so, we can gain a
    more comprehensive understanding of AI-generated content and further explore the
    capabilities and limitations of these advanced language models.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些局限性，创造力测试仍然可以作为比较和跟踪大型语言模型表现的有价值基准。未来的研究实验可以在其他语言中重复本文中的实验，以评估GPT模型在语言领域的创造力。这样，我们可以获得对AI生成内容的更全面理解，并进一步探索这些先进语言模型的能力和局限性。
- en: '*Thank you for reading this article!*'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢阅读这篇文章！*'
- en: Appendix
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录
- en: 'Remote Association Test: Comparison of GPT Models’ Correctly Answered Questions'
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 远程联想测试：GPT模型正确回答的问题比较
- en: We can look at the breakdown of difficulty level for each question the model
    got correct. We can see that GPT3, GPT3.5, and GPT4 are similar in its ability
    to answer proportions of difficult questions.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看模型正确回答的每个问题的难度级别分解。我们可以看到GPT3、GPT3.5和GPT4在回答难题的能力上相似。
- en: '![](../Images/a500fe60944887f82bd908a91ed6fbbe.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a500fe60944887f82bd908a91ed6fbbe.png)'
- en: Remote Associations GPT-2 got right and GPT-4 got wrong
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-2答对的远程联想和GPT-4答错的部分
- en: '[PRE7]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: GPT-4’s answers to randomly-generated Remote Association Test questions
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-4对随机生成的远程联想测试问题的回答
- en: Here are the outputs generated by GPT-4 for a few randomly generated Remote
    Association Test questions.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是GPT-4为几个随机生成的远程联想测试问题生成的输出。
- en: '**Rings**'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**戒指**'
- en: This one felt a little shoehorned. I could buy the part about rings being a
    “symbol of complications, especially in relationships” if you’re feeling super
    jaded about your love life or something. But the last part feels forced
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这个感觉有点勉强。如果你对你的感情生活感到非常厌倦，我可以接受关于戒指是“复杂性象征，尤其是在关系中的”那部分。但最后的部分感觉有点生硬。
- en: '[PRE8]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Revolution.** [Monte Cassino](https://en.wikipedia.org/wiki/Monte_Cassino)
    is indeed a real place and it was indeed bombed during WWII.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**革命。** [蒙特卡西诺](https://en.wikipedia.org/wiki/Monte_Cassino)确实是一个真实的地方，二战期间确实遭到轰炸。'
- en: '[PRE9]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Help.** An interesting way of connecting the city of “Southampton” to “help”
    via help centers…'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**帮助。** 通过帮助中心将“南安普顿”这座城市与“帮助”连接起来的有趣方式……'
- en: '[PRE11]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Internet**. I suppose its argument is quite correct.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**互联网。** 我认为它的论点是相当正确的。'
- en: '[PRE12]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Language.** Perhaps a bit generic, but I guess it works.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**语言。** 可能有点通用，但我想它还是有效的。'
- en: '[PRE13]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Health.** The health surveillance part is a bit dark, but I guess it works.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**健康。** 健康监测部分有点阴暗，但我想它还是有效的。'
- en: '[PRE14]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**ERIC!** This one was quite funny actually.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**ERIC！** 这个其实还挺有趣的。'
- en: '[PRE15]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: GPT-2 Outputs on the Divergent Association Test
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-2在发散联想测试中的输出
- en: I excluded GPT-2 from the Divergent test analyses because its output was absolute
    gibberish. Even when I simplified the prompt as much as possible, its output was
    not very useful. I include a sample here because I actually thought they were
    quite funny.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我排除了GPT-2在发散测试分析中的结果，因为它的输出完全是胡言乱语。即使我尽可能简化提示，它的输出也不太有用。我在这里包含了一个示例，因为我实际上觉得它们非常有趣。
- en: '[PRE16]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[[1](#footnote-anchor-1)] [Guilford, J. P.](https://en.wikipedia.org/wiki/J._P._Guilford)
    (1967). *The Nature of Human Intelligence*.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1](#footnote-anchor-1)] [吉尔福德, J. P.](https://en.wikipedia.org/wiki/J._P._Guilford)
    (1967). *人类智慧的本质*。'
- en: '[[2](#footnote-anchor-2)] Mednick, S. A., & Mednick, M.T. (1959,1962). Remote
    Associates Test, college and adult form.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[[2](#footnote-anchor-2)] Mednick, S. A., & Mednick, M.T. (1959,1962). 远程联想测试，大学及成人版。'
- en: '[[3](#footnote-anchor-3)] The 10,000 most common English words were determined
    by [n-gram](https://en.wikipedia.org/wiki/N-gram) [frequency analysis](https://en.wikipedia.org/wiki/Frequency_analysis)
    of the [Google’s Trillion Word Corpus](https://books.google.com/ngrams/info).
    I further filtered this list to remove stopwords (e.g. common words in English
    like “the”, “and”, and “you”). More information from this [repository](https://github.com/first20hours/google-10000-english).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[[3](#footnote-anchor-3)] 通过对[Google’s Trillion Word Corpus](https://books.google.com/ngrams/info)的[n-gram](https://en.wikipedia.org/wiki/N-gram)
    [频率分析](https://en.wikipedia.org/wiki/Frequency_analysis)，确定了最常见的10,000个英语单词。我进一步筛选了这个列表，去除了停用词（例如，“the”、“and”和“you”这样的常见词）。更多信息见此[仓库](https://github.com/first20hours/google-10000-english)。'
- en: '[[4](#footnote-anchor-4)] I put this on a dating app one time and nobody thought
    it was funny, unfortunately.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[[4](#footnote-anchor-4)] 我曾在一次约会应用上发布了这个，但很遗憾，没有人觉得好笑。'
- en: '[[5](#footnote-anchor-5)] Olson, Jay A.; Nahas, Johnny; Chmoulevitch, Denis;
    Cropper, Simon J.; Webb, Margaret E. (22 June 2021). [“Naming unrelated words
    predicts creativity”](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8237676). *Proceedings
    of the National Academy of Sciences*. **118** (25): e2022340118\. [doi](https://en.wikipedia.org/wiki/Doi_(identifier)):[10.1073/pnas.2022340118](https://doi.org/10.1073%2Fpnas.2022340118).
    [PMC](https://en.wikipedia.org/wiki/PMC_(identifier)) [8237676](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8237676).
    [PMID](https://en.wikipedia.org/wiki/PMID_(identifier)) [34140408](https://pubmed.ncbi.nlm.nih.gov/34140408)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[[5](#footnote-anchor-5)] Olson, Jay A.；Nahas, Johnny；Chmoulevitch, Denis；Cropper,
    Simon J.；Webb, Margaret E.（2021年6月22日）。[“命名无关的词预测创造力”](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8237676)。*《国家科学院学报》*。**118**（25）：e2022340118。
    [doi](https://en.wikipedia.org/wiki/Doi_(identifier))：[10.1073/pnas.2022340118](https://doi.org/10.1073%2Fpnas.2022340118)。
    [PMC](https://en.wikipedia.org/wiki/PMC_(identifier)) [8237676](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8237676)。
    [PMID](https://en.wikipedia.org/wiki/PMID_(identifier)) [34140408](https://pubmed.ncbi.nlm.nih.gov/34140408)'
- en: '[[6](#footnote-anchor-6)] The authors take a subset of 7 out of 10 words to
    calculate scores. I kept all 10 words. Also, the authors used the 840B-GloVe embeddings
    while I used the smaller 6B-GloVe embeddings. I compared my GloVe embedding results
    with the official DAT score and found the results to be comparable despite these
    differences.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[[6](#footnote-anchor-6)] 作者从10个词中取了7个子集来计算分数。我保留了所有10个词。此外，作者使用了840B-GloVe嵌入，而我使用了较小的6B-GloVe嵌入。我将我的GloVe嵌入结果与官方DAT分数进行了比较，发现尽管存在这些差异，结果是相当的。'
