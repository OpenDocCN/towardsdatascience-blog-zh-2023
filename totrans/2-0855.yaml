- en: 'Exploring Creativity in Large Language Models: From GPT-2 to GPT-4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/exploring-creativity-in-large-language-models-from-gpt-2-to-gpt-4-1c2d1779be57](https://towardsdatascience.com/exploring-creativity-in-large-language-models-from-gpt-2-to-gpt-4-1c2d1779be57)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Analyzing the evolution of creative processes in large language models through
    creativity tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@artfish?source=post_page-----1c2d1779be57--------------------------------)[![Yennie
    Jun](../Images/b635e965f21c3d55833269e12e861322.png)](https://medium.com/@artfish?source=post_page-----1c2d1779be57--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1c2d1779be57--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1c2d1779be57--------------------------------)
    [Yennie Jun](https://medium.com/@artfish?source=post_page-----1c2d1779be57--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1c2d1779be57--------------------------------)
    ·21 min read·Apr 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c92d68bd2a35c36d6c1f1499d438759.png)'
  prefs: []
  type: TYPE_IMG
- en: Creativity as envisioned by Midjourney. Generated by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '*This blog was originally posted at* [https://www.artfish.ai/p/exploring-creativity-in-large-language](https://www.artfish.ai/p/exploring-creativity-in-large-language)'
  prefs: []
  type: TYPE_NORMAL
- en: In recent weeks, people have used large language models (LLMs) to generate a
    variety of creative content, such as [books](https://www.reuters.com/technology/chatgpt-launches-boom-ai-written-e-books-amazon-2023-02-21/),
    [flash fiction](https://blog.yenniejun.com/p/creative-writing-with-gpt-3-from),
    [rap battles](https://twitter.com/mehran__jalali/status/1639846978850021377?lang=en),
    and [music chords](/using-chatgpt-as-a-creative-writing-partner-part-2-music-d2fd7501c268).
    But is it possible to measure the level of creative process more broadly in these
    models?
  prefs: []
  type: TYPE_NORMAL
- en: Human creativity has long fascinated psychologists and researchers. [Starting
    in the 1950s](https://www.ideatovalue.com/podc/nickskillicorn/2021/04/the-1950-speech-that-started-creativity-research/),
    researchers created a series of tests to compare the creative performance and
    potential of individuals. While no single test can fully capture creativity, these
    tests attempted to measure and quantify different aspects of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, I analyze the performance of GPT models from 2019 to 2023
    on tests that measure two kinds of creativity: convergent (a single, correct solution
    exists) and divergent (open ended; multiple solutions may exist) [[1](#footnote-1)].
    The tests include:'
  prefs: []
  type: TYPE_NORMAL
- en: Connecting three seemingly unrelated words with a fourth word ([Remote Associates
    Test](https://en.wikipedia.org/wiki/Remote_Associates_Test))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Producing as many alternative uses for everyday objects ([Alternate Uses Task](https://en.wikipedia.org/wiki/Guilford%27s_Alternate_Uses))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naming ten nouns that differ as much as possible from each other ([Divergent
    Association Task](https://en.wikipedia.org/wiki/Divergent_Association_Task))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Through these analyses, I focus on the change in performance from the earlier
    GPT-2 model all the way to the newest GPT-4 model. I use both a subjective and
    objective lens to understand the behavior of these models. In general, the later
    models tend to perform better on all of the creativity tasks, although I do find
    a few exceptions where, when the test has a “correct” answer”, the simpler GPT-2
    model outperforms the newer GPT4 model. When possible, I compare the GPT models
    to human performances on the same creativity tests.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: These experiments aim not to measure how creative the models are, but
    rather to measure the level of creative process present in the model generations.
    I am not claiming that the GPT models, or LLMs in general, possess creative thinking
    in the same way humans do. Rather, I aim to show how the models respond to particular
    measures of creative processes.*'
  prefs: []
  type: TYPE_NORMAL
- en: A Progression of GPT Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative Pre-trained Transformer (GPT) models are language models trained
    on vast portions of the Internet to predict the next token based on previous tokens.
    While the recent ChatGPT has captured the public’s attention in the last few months,
    OpenAI’s GPT models (and the technologies behind the models) have been around
    for a few years.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-1 (2018, 117M parameters). The original GPT model. I used the [open-sourced
    version on HuggingFace](https://huggingface.co/openai-gpt). However, the outputs
    were so gibberish so I did not include any of them in this article.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paper: [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdfhttps://en.wikipedia.org/wiki/GPT-2#:~:text=Generative%20Pre%2Dtrained%20Transformer%202,by%20OpenAI%20in%20February%202019.)
    (2019, 1.5B parameters). I used the [open-soured version on HuggingFace](https://huggingface.co/gpt2).
    For some experiments, GPT-2 outputs were gibberish and not useful for understanding
    its behavior, so I did not include those in the respective analyses. Paper: [Language
    Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GPT-3](https://en.wikipedia.org/wiki/GPT-3) (2020, 175B parameters). Over
    100 times bigger than GPT-2\. I used *text-davinci-002* from the [OpenAI APIs](https://platform.openai.com/docs/models/gpt-3-5).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paper: [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: GPT-3.5 (2022). While there are multiple versions of GPT-3.5, I use the same
    model underlying the original [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT),
    which was trained using reinforcement learning with human feedback. I used *gpt-3.5-turbo*
    from the [OpenAI APIs](https://platform.openai.com/docs/models/gpt-3-5).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GPT-4](https://arxiv.org/abs/2303.08774) (2023, ~100T parameters). I used
    *gpt-4* from the [OpenAI APIs](https://platform.openai.com/docs/models/gpt-3-5).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paper: [GPT-4 Technical Report](https://arxiv.org/pdf/2303.08774.pdf).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For all of the experiments in this article, unless otherwise mentioned, I called
    each GPT model five times per unique prompt to account for randomness in its generations.
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Remote Associates Test: Connecting Unrelated Words'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Remote Associates Test (RAT) is a creativity test developed by Martha Mednick
    in 1962 [[2](#footnote-2)]. This test measures convergent thinking, meaning that
    **each question has a “right” answer.**
  prefs: []
  type: TYPE_NORMAL
- en: Given three seemingly unrelated words, the test taker must find a fourth word
    connecting the three words. For example, the words “dream”, “break”, and “light”
    can be linked by the word “day” (daydream, daybreak, daylight). The words “sense”,
    “courtesy”, and “place” can be linked by the word “common”.
  prefs: []
  type: TYPE_NORMAL
- en: I collected the word triplets from [the Remote Associates Test website](https://www.remote-associates-test.com/).
    **There were a total of 312 questions.** The difficulty of each question was rated
    on a scale from Very Easy to Very Hard.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f2d6ab51eb2f16a24735f7e563ff4a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Breakdown of the Random Association Test questions taken from the [test website](https://www.remote-associates-test.com/).
    Created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: For each model and each triplet of words, I used the following prompt template.
    I called each model 5 times to account for randomness.
  prefs: []
  type: TYPE_NORMAL
- en: You are given three words. Think of a fourth word that is somehow related to
    each of the first three words. First give the answer (“Answer”) then explain your
    reasoning (“Reason”).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Newer GPT models score higher on remote association
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I calculated the overall percent of questions each GPT model generated a correct
    answer for. It may come as no surprise that the the newer the model, the more
    correct answers it generated.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bdaad2048a6c8c37702328e4b74d82bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Percentage of questions each GPT model answered correctly on the Remote Associates
    Test. Created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Breaking down model performance by question difficulty level showed a similar
    trend: newer GPT models performed better than older ones on each difficulty level.
    And while the newer GPT models performed better than the older ones on hard questions,
    the improvement was less drastic than for easier questions. This means that **even
    GPT-4 struggled on difficult questions.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a0449267a411f9f1f2c16850305c1c9e.png)'
  prefs: []
  type: TYPE_IMG
- en: Percentage of questions each GPT model answered correctly on the Remote Associates
    Test, broken down by question difficulty level. Created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: According to the Remote Associates Test website, “A typical person can solve
    most of the items marked as easy, about half of the medium ones, and few of the
    hard ones.” There were no official statistics shared on the website, but a quick
    glance would show that GPT-4 did indeed solve around half of the medium ones and
    a few of the hard ones, making it the only model approximately comparable to human
    level. The other GPT models were worse than human performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few exceptions: when GPT-2 gets it right but GPT-4 gets it wrong'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While it didn’t happen often, there were a few questions for which the more
    rudimentary GPT-2 model from 2019 was able to generate the correct answer but
    the GPT-4 model from 2023 was not able to. In fact, there were 8 such questions.
    (The full list is shown in the Appendix at the end of the article).
  prefs: []
  type: TYPE_NORMAL
- en: '**An easy question**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Words: cottage / brick / cake'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-2: “cheese” [correct answer]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-4: “layer”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A medium question**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Words: cat / number / phone'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-2: “call”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-4: “dial”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A hard question**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Words: through / disk / way'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-2: “drive” [correct]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-4: “circle”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A very hard question**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Words: land / hand / house'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-2: “farm” [correct]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-4: “property”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice that for all of these questions, GPT-4’s answers are *not* unreasonable.
    In fact, without knowing the correct answer beforehand, you might assume that
    GPT-4’s answer could as likely connect the three words. If you’re thinking that,
    you’re not wrong. I’ll address this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Was GPT-4 wrong or just thinking outside of the box?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GPT-4 answered more than half of the questions incorrectly, but maybe with good
    reason. I asked GPT-4 to explain each of its answers. Then, I looked at its explanations
    for a few of the “Very Hard” questions that it got wrong.
  prefs: []
  type: TYPE_NORMAL
- en: '**Words: artist / hatch / route**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Correct answer: “escape”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-4 answer: “map”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The word “artist” might bring to mind the idea of creating something, perhaps
    even a visual representation of something. Maps are a common form of visual representation
    that require a degree of artistic skill to create.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The word “hatch” could refer to the act of opening something or bringing something
    into existence. A map is a type of blueprint or plan, often used to navigate new
    territory or chart out a course of action.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The word “route” is related to direction and movement, which can be plotted
    and tracked on a map. A map can help you determine the best route to take when
    traveling, and can also provide information about the routes others have taken
    in the past.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All in all, “Map” seems to be a word that fits well with each of the three initial
    words, as it encapsulates the ideas of artistic creation, planning and direction,
    and movement and navigation.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: GPT-4’s explanation for why “map” is a good word to associate with “artist /
    hatch / route” is convincing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is another example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Words: fork / dark / man**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Correct answer: “pitch”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-4 answer: “shadow”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The word “fork” can be associated with division or branching, such as a fork
    in the road. Shadows can also be seen as a type of division, as they represent
    the absence of light.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The word “dark” refers to the absence of light. Shadows are created when light
    is blocked by an object, and can often appear darker than the surrounding area.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The word “man” can be associated with the concept of identity or individuality.
    Shadows can also be used to represent the identity of a person, as they can provide
    clues about their shape and size.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Therefore, “Shadow” seems to be a word that fits well with each of the initial
    three words, as it encapsulates the ideas of division, absence, and identity.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: These examples show the pitfall of the Remote Associates Test, and of convergent
    creativity tests in general — that is, when the “correct” answer may not be the
    only “right” answer. In these cases, GPT-4 generated an answer that was “wrong”,
    but only because it did not meet the conventional “correct” answer.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, if I ask GPT-4 to brainstorm multiple options for a fourth word for
    a triplet of words, it is able to give me multiple options with convincing answers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/efe8aafae763d1ba30351195a4efd50b.png)'
  prefs: []
  type: TYPE_IMG
- en: GPT-4 responses to generate multiple “right” answers for a triplet of words.
    Screenshot taken by the author.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 answers arbitrarily generated triplet of words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is possible that the GPT models may have seen some of the above Remote Associates
    Test questions in its training data. How would GPT-4 respond to an arbitrarily
    generated triplet of words? In this section, I construct my own “remote association”
    test and evaluate the outputs subjectively.
  prefs: []
  type: TYPE_NORMAL
- en: I took a list of the [10,000 most common English words](https://github.com/first20hours/google-10000-english)
    [[3](#footnote-3)] Then I randomly sampled three words from this list and had
    GPT-4 provide a fourth word tying these words together. Some of the three-word
    combinations were quite strange and unrelated, but GPT-4 always found a way to
    tie them together. I encourage the reader to try these first before reading GPT-4’s
    generations!
  prefs: []
  type: TYPE_NORMAL
- en: After viewing the next few examples, you can make your decision — is GPT-4 thinking
    outside of the box? Or is it just shoehorning its answers? (A longer list of examples
    is shown in the Appendix at the end of the article).
  prefs: []
  type: TYPE_NORMAL
- en: ⛑️ **Health**
  prefs: []
  type: TYPE_NORMAL
- en: This response read as quite convincing. I found it interesting that the definition
    for “health surveillance” was copied almost verbatim from the [CDC website on
    Introduction to Public Health Surveillance](https://www.cdc.gov/training/publichealth101/surveillance.html#anchor_available_materials).
  prefs: []
  type: TYPE_NORMAL
- en: '*Public health surveillance is “the ongoing, systematic collection, analysis,
    and interpretation of health-related data essential to planning, implementation,
    and evaluation of public health practice.” —* Field Epidemiology'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: (from the CDC website)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**🍀 Clovers**'
  prefs: []
  type: TYPE_NORMAL
- en: I was initially convinced by GPT-4’s answer — in fact, it felt quite clever.
    That is, until I did a quick Google search and found out that *Trifolium duodecim*
    is not real. This is an example of a [hallucination](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)),
    a common phenomena in which LLMs make up an answer that is not factually correct.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**☕️ Starbucks**'
  prefs: []
  type: TYPE_NORMAL
- en: This example was my favorite the answer, while thinking out of the box, felt
    like kind of a stretch. And it was a little bit funny (especially the part about
    Patrick).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: These examples and analyses illustrate that creativity tests with a single correct
    answer might be limited. This limitation is addressed in the other tests explored
    in this article in the next sections.
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Alternate Uses Task: Finding Meaning in Ordinary Items'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another popular test for measuring creativity is the Alternative Uses Task (AUT),
    which was created by J. P. Guilford in 1967\. Unlike the Remote Associations Test,
    this test measures divergent thinking, meaning that **there is** **no right answer.**
  prefs: []
  type: TYPE_NORMAL
- en: The test taker is asked to produce as many original uses for an everyday object
    as possible. For example, a “laptop” can be used as a “plate”, as a “body shield”,
    or as “wall art” [[4](#footnote-4)]. Responses to this test are measured in terms
    of metrics such as originality (how different the uses are) and flexibility (the
    assortment of ideas). The purpose of this test is to see how many unconventional
    uses a person can think of for an ordinary object.
  prefs: []
  type: TYPE_NORMAL
- en: However, due to my limited resources, I did not have the ability to generate
    these measures without human annotators. Therefore, I used a **proxy for creativity,**
    which was also used by researchers from the University of Amsterdam in [Putting
    GPT-3’s Creativity to the (Alternative Uses) Test](https://arxiv.org/pdf/2206.08932.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, the researchers asked both humans and GPT-3 (davinci-002) to
    come up with alternate use cases for the following common objects: fork, tin can,
    and book. As a proxy for gauging creativity, they measured the *semantic distance*
    between the object and the generated use cases — that is, how different is the
    generated use case word from the original object?'
  prefs: []
  type: TYPE_NORMAL
- en: I obtained the human responses from [the paper’s project website](https://osf.io/vmk3c/?view_only=)
    for comparison with human performance. I used the following prompt for the GPT
    models, based on a similar prompt used int he paper.
  prefs: []
  type: TYPE_NORMAL
- en: 'Come up with 10 creative uses for the following item that might strike people
    as clever, unusual, interesting, uncommon, humorous, innovative, or different:
    {item}'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Cosine Distance between embeddings as proxy for creativity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In natural language processing, semantic distance (or [semantic similarity](https://en.wikipedia.org/wiki/Semantic_similarity))
    is a proxy for how related two words are, based on how often words are used together
    in similar contexts. For example, the words “computer” and “screen” might have
    a shorter semantic distance, because they are often used in the same contexts,
    whereas the words “computer” and “lentils” might have a greater semantic distance,
    because they are not often used in the same contexts.
  prefs: []
  type: TYPE_NORMAL
- en: To compute the semantic distance of two words, I used the same approach as used
    by the researchers in [the paper mentioned above.](https://arxiv.org/pdf/2206.08932.pdf)
    I used [Spacy](https://spacy.io/models/en) (`en_core_web_lg`), a natural language
    processing library, to calculate word embeddings for the object (e.g. “laptop”)
    and the use case word (e.g. “plate”). Then, I used cosine distance between the
    two embeddings to calculate the semantic distance.
  prefs: []
  type: TYPE_NORMAL
- en: For the Alternate Uses Task, **larger semantic distances correlated with “more
    creative”** use cases for objects. The results showed that for the three objects
    (fork, tin can, and book), human performance reliably resulted in higher cosine
    distances (and therefore higher semantic dissimilarity) than for GPT-3\. However,
    there were cases when GPT-3.5 and/or GPT-4 matched or surpassed human creativity
    semantic distance scores.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/39de38d8fc97ba32e9443e9288a67949.png)'
  prefs: []
  type: TYPE_IMG
- en: Semantic distance (measured using cosine distance) of original word to alternate
    use case. Created by author.
  prefs: []
  type: TYPE_NORMAL
- en: While humans tended to come up with more creative use cases than GPT-3 (using
    semantic distance, as calculated by cosine distance, as a proxy for creativity),
    GPT-4 was more on par with human level. That is, GPT-4 responded with items resulting
    in larger semantic distances with the original item, at a similar level to human
    performance. However, without actual humans annotating and deciding more subjective
    categories such as originality, flexibility, and usefulness, this quantification
    using cosine distance is limited as it only measures a small part of the GPT models’
    creative outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Can you distinguish human-generated vs. AI-generated use cases?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the lists shows the human-generated alternate use cases for a tin can
    and the other shows alternate use cases generated by GPT-4\. Can you tell the
    difference? The answer is at the end.
  prefs: []
  type: TYPE_NORMAL
- en: 'List 1: Uses for tin can'
  prefs: []
  type: TYPE_NORMAL
- en: as a mirror
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: to create toys
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as art
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as a reminder of Andy Warhol
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as a key ring with the clip from the can
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as jewelry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'List 2: Uses for tin can'
  prefs: []
  type: TYPE_NORMAL
- en: miniature drum set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: quirky plant pot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: impromptu cookie cutter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: homemade camp stove
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: whimsical wind chimes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: miniature herb garden
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next test in the following section also measures divergent thinking.
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Divergent Association Task: Ten Words, No Connections'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The third creativity test is the Divergent Association Task (DAT), created more
    recently in 2021 by a group of researchers [[5](#footnote-5)]. This test involves
    naming ten nouns that differ from each other as much as possible. This test, as
    evident by its name, is divergent and also **has no correct answers.**
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a [version of the test online on this website](https://www.datcreativity.com/).
    It only takes a few minutes to take, as it involves coming up with a list of ten
    nouns as unrelated as possible. Here’s my result from the website:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa3a465a40574d28ff017b98685d1feb.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot taken from the Divergent Association Task. Created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The DAT Score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'How is the score calculated? The authors describe their process [here](https://www.datcreativity.com/faq).
    The DAT score is calculated using embeddings and cosine distance. I adapt their
    process for calculating the score [[6](#footnote-6)]:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate [GloVe](https://nlp.stanford.edu/projects/glove/) embeddings for each
    word
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the cosine distance between each pair of words
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the average of that value
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process is similar to the method used to calculate creativity proxy score
    in the Alternate Uses Task, with the only difference being the word embedding
    model used.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Divergence among models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following prompt, which I use to prompt the GPT models, is taken from the
    [DAT website](https://www.datcreativity.com/task).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: I did not include GPT-2 in the results below, as the outputs generated by GPT-2
    were gibberish. (You can see an example of this in the Appendix at the end of
    the article).
  prefs: []
  type: TYPE_NORMAL
- en: The results show that the DAT score increased for each newer model. This means
    that later models were better at generating 10 words semantically different from
    each other as possible.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/47b6d5b8fda43230d69db455aa67f200.png)'
  prefs: []
  type: TYPE_IMG
- en: Divergent Association Test scores, as measured by cosine distance, for the different
    models. Generated by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following list generated by GPT-3 had a score of 0.36:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, the following list generated by GPT-4 had a score of 0.95:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: According to the [paper introducing the DAT test](https://www.pnas.org/doi/10.1073/pnas.2022340118),
    scores commonly ranged from 65 to 90, with an average between 75 and 80\. A score
    of 95 was considered very high.
  prefs: []
  type: TYPE_NORMAL
- en: Something to note in the above figure is that the error bars for GPT-4 were
    much smaller than those for GPT-3\. The significance of this becomes much clearer
    in the next experiment, in which I varied the amount of randomness in each GPT
    model’s generation by tweaking the “temperature” parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Temperature is a value between 0 and 1, with values closer to 0 being the most
    predictable and 1 being the most random. I tried temperature values between 0.5
    and 0.9 in increments of 0.1 (In previous experiments I kept the temperature at
    0.7, which is the default value). I expected more variance for higher temperature
    values and more consistency for lower temperatures.
  prefs: []
  type: TYPE_NORMAL
- en: However, the results showed that the temperature did not matter as much as the
    model in terms of semantic distance variance. For GPT-3, despite the temperature,
    the DAT score for each list of 10 words varied greatly. Whereas, on the other
    hand, for GPT-4, despite the temperature, the DAT score remained tightly consistent
    in a small range with very little variance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b7c569d6c2a668b897761c671eca6ea5.png)'
  prefs: []
  type: TYPE_IMG
- en: Average cosine distance among word embeddings, separated by model type and temperature
    score. Created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The implications of this became clearer once I looked at the actual text being
    generated by the three GPT models. **GPT-4 was way less variable in its generations
    than GPT-3.** In a way, it felt like GPT-4 kind of already knew the “right answer”
    for a task that is supposed to have no right answer.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating the responses for all temperature scores, I had each model generate
    a list of 10 words 50 times. Then, I looked at how common different words appeared
    across all 50 runs. Note that the most commonly generated word by GPT-3 was book,
    which appeared 19 times out of 50 = 38%. On the other hand, the most commonly
    generated word by GPT-4 was microscope, which appeared **48/50=96% of the time**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/323fb7c8bfe263f62d4408741add0e58.png)'
  prefs: []
  type: TYPE_IMG
- en: Top words generated by each model for the DAT test (aggregated across temperature
    scores). Created by the words.
  prefs: []
  type: TYPE_NORMAL
- en: This validates the claim that GPT-3 is, in fact, more variable in its word generations,
    while GPT-4 played it safe by predicting a lot of the same words it knew would
    score high on the “diverse words” tasks — words like microscope, volcano, elephant,
    and galaxy.
  prefs: []
  type: TYPE_NORMAL
- en: This finding illustrates how GPT-4 seems to have “cracked the code” for generating
    what it considers to be diverse words. If this is the case, I’m not sure if I
    can confidently say that GPT-4 is being creative more so than it has reached a
    local optima for “how to best play this particular game.”
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*“We can imagine a future in which GPT-3 and other generative LLMs responses
    cannot be distinguished from humans, although the creative process will be different.”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — from* [*Putting GPT-3’s Creativity to the (Alternative Uses) Test*](https://arxiv.org/abs/2206.08932)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is challenging to accurately measure something so complex and multifaceted
    as creativity using a limited set of text-based tests. Although tests like the
    [Visual Remote Associates Test](https://www.frontiersin.org/articles/10.3389/fpsyg.2020.00026/full)
    attempt to address this issue by incorporating a visual element, there remain
    inherent limitations to such assessments. Convergent tests (such as the Remote
    Associates Test), which have a “right answer,” may not always reward the most
    creative or correct solution. On the other hand, divergent tests (such as the
    Divergent Association Test), which do not have a specific right answer, rely on
    word embeddings that are imperfect and may contain biases.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the performance of GPT models on these tests could be influenced
    by their exposure to the tasks during training, potentially leading to data overlap.
    For instance, GPT-4’s performance on the Divergence Association Test demonstrated
    a peculiar repetition of words, suggesting it may have had prior knowledge of
    the “right” answers. It is crucial to develop alternative testing methods, as
    there have been [rumors of GPT-4 being trained on test data](https://www.reddit.com/r/MachineLearning/comments/124eyso/n_openai_may_have_benchmarked_gpt4s_coding/),
    particularly in coding tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these limitations, creativity tests can serve as valuable benchmarks
    for comparing and tracking the performance of large language models. Future research
    experiments could repeat those from this article in other languages to gauge the
    creativity of GPT models across linguistic domains. By doing so, we can gain a
    more comprehensive understanding of AI-generated content and further explore the
    capabilities and limitations of these advanced language models.
  prefs: []
  type: TYPE_NORMAL
- en: '*Thank you for reading this article!*'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Remote Association Test: Comparison of GPT Models’ Correctly Answered Questions'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can look at the breakdown of difficulty level for each question the model
    got correct. We can see that GPT3, GPT3.5, and GPT4 are similar in its ability
    to answer proportions of difficult questions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a500fe60944887f82bd908a91ed6fbbe.png)'
  prefs: []
  type: TYPE_IMG
- en: Remote Associations GPT-2 got right and GPT-4 got wrong
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: GPT-4’s answers to randomly-generated Remote Association Test questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here are the outputs generated by GPT-4 for a few randomly generated Remote
    Association Test questions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Rings**'
  prefs: []
  type: TYPE_NORMAL
- en: This one felt a little shoehorned. I could buy the part about rings being a
    “symbol of complications, especially in relationships” if you’re feeling super
    jaded about your love life or something. But the last part feels forced
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Revolution.** [Monte Cassino](https://en.wikipedia.org/wiki/Monte_Cassino)
    is indeed a real place and it was indeed bombed during WWII.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Help.** An interesting way of connecting the city of “Southampton” to “help”
    via help centers…'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**Internet**. I suppose its argument is quite correct.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Language.** Perhaps a bit generic, but I guess it works.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '**Health.** The health surveillance part is a bit dark, but I guess it works.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**ERIC!** This one was quite funny actually.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: GPT-2 Outputs on the Divergent Association Test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I excluded GPT-2 from the Divergent test analyses because its output was absolute
    gibberish. Even when I simplified the prompt as much as possible, its output was
    not very useful. I include a sample here because I actually thought they were
    quite funny.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[[1](#footnote-anchor-1)] [Guilford, J. P.](https://en.wikipedia.org/wiki/J._P._Guilford)
    (1967). *The Nature of Human Intelligence*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[2](#footnote-anchor-2)] Mednick, S. A., & Mednick, M.T. (1959,1962). Remote
    Associates Test, college and adult form.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[3](#footnote-anchor-3)] The 10,000 most common English words were determined
    by [n-gram](https://en.wikipedia.org/wiki/N-gram) [frequency analysis](https://en.wikipedia.org/wiki/Frequency_analysis)
    of the [Google’s Trillion Word Corpus](https://books.google.com/ngrams/info).
    I further filtered this list to remove stopwords (e.g. common words in English
    like “the”, “and”, and “you”). More information from this [repository](https://github.com/first20hours/google-10000-english).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[4](#footnote-anchor-4)] I put this on a dating app one time and nobody thought
    it was funny, unfortunately.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[5](#footnote-anchor-5)] Olson, Jay A.; Nahas, Johnny; Chmoulevitch, Denis;
    Cropper, Simon J.; Webb, Margaret E. (22 June 2021). [“Naming unrelated words
    predicts creativity”](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8237676). *Proceedings
    of the National Academy of Sciences*. **118** (25): e2022340118\. [doi](https://en.wikipedia.org/wiki/Doi_(identifier)):[10.1073/pnas.2022340118](https://doi.org/10.1073%2Fpnas.2022340118).
    [PMC](https://en.wikipedia.org/wiki/PMC_(identifier)) [8237676](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8237676).
    [PMID](https://en.wikipedia.org/wiki/PMID_(identifier)) [34140408](https://pubmed.ncbi.nlm.nih.gov/34140408)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[6](#footnote-anchor-6)] The authors take a subset of 7 out of 10 words to
    calculate scores. I kept all 10 words. Also, the authors used the 840B-GloVe embeddings
    while I used the smaller 6B-GloVe embeddings. I compared my GloVe embedding results
    with the official DAT score and found the results to be comparable despite these
    differences.'
  prefs: []
  type: TYPE_NORMAL
