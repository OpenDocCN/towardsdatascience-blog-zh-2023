- en: 'Exploring Creativity in Large Language Models: From GPT-2 to GPT-4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åˆ›é€ åŠ›ï¼šä» GPT-2 åˆ° GPT-4
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/exploring-creativity-in-large-language-models-from-gpt-2-to-gpt-4-1c2d1779be57](https://towardsdatascience.com/exploring-creativity-in-large-language-models-from-gpt-2-to-gpt-4-1c2d1779be57)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/exploring-creativity-in-large-language-models-from-gpt-2-to-gpt-4-1c2d1779be57](https://towardsdatascience.com/exploring-creativity-in-large-language-models-from-gpt-2-to-gpt-4-1c2d1779be57)
- en: Analyzing the evolution of creative processes in large language models through
    creativity tests
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€šè¿‡åˆ›é€ åŠ›æµ‹è¯•åˆ†æå¤§å‹è¯­è¨€æ¨¡å‹ä¸­åˆ›æ„è¿‡ç¨‹çš„æ¼”å˜
- en: '[](https://medium.com/@artfish?source=post_page-----1c2d1779be57--------------------------------)[![Yennie
    Jun](../Images/b635e965f21c3d55833269e12e861322.png)](https://medium.com/@artfish?source=post_page-----1c2d1779be57--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1c2d1779be57--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1c2d1779be57--------------------------------)
    [Yennie Jun](https://medium.com/@artfish?source=post_page-----1c2d1779be57--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@artfish?source=post_page-----1c2d1779be57--------------------------------)[![Yennie
    Jun](../Images/b635e965f21c3d55833269e12e861322.png)](https://medium.com/@artfish?source=post_page-----1c2d1779be57--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1c2d1779be57--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1c2d1779be57--------------------------------)
    [Yennie Jun](https://medium.com/@artfish?source=post_page-----1c2d1779be57--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1c2d1779be57--------------------------------)
    Â·21 min readÂ·Apr 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1c2d1779be57--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 21 åˆ†é’Ÿ Â·2023å¹´4æœˆ11æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7c92d68bd2a35c36d6c1f1499d438759.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c92d68bd2a35c36d6c1f1499d438759.png)'
- en: Creativity as envisioned by Midjourney. Generated by the author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Midjourney è®¾æƒ³ä¸­çš„åˆ›é€ åŠ›ã€‚ç”±ä½œè€…ç”Ÿæˆã€‚
- en: '*This blog was originally posted at* [https://www.artfish.ai/p/exploring-creativity-in-large-language](https://www.artfish.ai/p/exploring-creativity-in-large-language)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ¬åšå®¢æœ€åˆå‘å¸ƒäº* [https://www.artfish.ai/p/exploring-creativity-in-large-language](https://www.artfish.ai/p/exploring-creativity-in-large-language)'
- en: In recent weeks, people have used large language models (LLMs) to generate a
    variety of creative content, such as [books](https://www.reuters.com/technology/chatgpt-launches-boom-ai-written-e-books-amazon-2023-02-21/),
    [flash fiction](https://blog.yenniejun.com/p/creative-writing-with-gpt-3-from),
    [rap battles](https://twitter.com/mehran__jalali/status/1639846978850021377?lang=en),
    and [music chords](/using-chatgpt-as-a-creative-writing-partner-part-2-music-d2fd7501c268).
    But is it possible to measure the level of creative process more broadly in these
    models?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘å‡ å‘¨ï¼Œäººä»¬ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆäº†å„ç§åˆ›æ„å†…å®¹ï¼Œå¦‚ [ä¹¦ç±](https://www.reuters.com/technology/chatgpt-launches-boom-ai-written-e-books-amazon-2023-02-21/)ã€[é—ªå°è¯´](https://blog.yenniejun.com/p/creative-writing-with-gpt-3-from)ã€[è¯´å”±å¯¹å†³](https://twitter.com/mehran__jalali/status/1639846978850021377?lang=en)
    å’Œ [éŸ³ä¹å’Œå¼¦](/using-chatgpt-as-a-creative-writing-partner-part-2-music-d2fd7501c268)ã€‚ä½†æ˜¯å¦æœ‰å¯èƒ½æ›´å¹¿æ³›åœ°è¡¡é‡è¿™äº›æ¨¡å‹ä¸­çš„åˆ›æ„è¿‡ç¨‹æ°´å¹³ï¼Ÿ
- en: Human creativity has long fascinated psychologists and researchers. [Starting
    in the 1950s](https://www.ideatovalue.com/podc/nickskillicorn/2021/04/the-1950-speech-that-started-creativity-research/),
    researchers created a series of tests to compare the creative performance and
    potential of individuals. While no single test can fully capture creativity, these
    tests attempted to measure and quantify different aspects of it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: äººç±»åˆ›é€ åŠ›é•¿æœŸä»¥æ¥ä¸€ç›´å¸å¼•ç€å¿ƒç†å­¦å®¶å’Œç ”ç©¶äººå‘˜ã€‚[è‡ª1950å¹´ä»£èµ·](https://www.ideatovalue.com/podc/nickskillicorn/2021/04/the-1950-speech-that-started-creativity-research/)ï¼Œç ”ç©¶äººå‘˜åˆ›å»ºäº†ä¸€ç³»åˆ—æµ‹è¯•ï¼Œä»¥æ¯”è¾ƒä¸ªäººçš„åˆ›é€ åŠ›è¡¨ç°å’Œæ½œåŠ›ã€‚è™½ç„¶æ²¡æœ‰å•ä¸€æµ‹è¯•èƒ½å®Œå…¨æ•æ‰åˆ›é€ åŠ›ï¼Œä½†è¿™äº›æµ‹è¯•å°è¯•æµ‹é‡å’Œé‡åŒ–å…¶ä¸åŒæ–¹é¢ã€‚
- en: 'In this article, I analyze the performance of GPT models from 2019 to 2023
    on tests that measure two kinds of creativity: convergent (a single, correct solution
    exists) and divergent (open ended; multiple solutions may exist) [[1](#footnote-1)].
    The tests include:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘åˆ†æäº†2019å¹´åˆ°2023å¹´é—´GPTæ¨¡å‹åœ¨æµ‹é‡ä¸¤ç§åˆ›é€ åŠ›çš„æµ‹è¯•ä¸­çš„è¡¨ç°ï¼šæ”¶æ•›æ€§ï¼ˆå­˜åœ¨å•ä¸€æ­£ç¡®è§£å†³æ–¹æ¡ˆï¼‰å’Œå‘æ•£æ€§ï¼ˆå¼€æ”¾å¼ï¼›å¯èƒ½å­˜åœ¨å¤šç§è§£å†³æ–¹æ¡ˆï¼‰[[1](#footnote-1)]ã€‚è¿™äº›æµ‹è¯•åŒ…æ‹¬ï¼š
- en: Connecting three seemingly unrelated words with a fourth word ([Remote Associates
    Test](https://en.wikipedia.org/wiki/Remote_Associates_Test))
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”¨ä¸€ä¸ªç¬¬å››ä¸ªè¯å°†ä¸‰ä¸ªçœ‹ä¼¼ä¸ç›¸å…³çš„è¯è¿æ¥èµ·æ¥ï¼ˆ[è¿œç¨‹è”æƒ³æµ‹è¯•](https://en.wikipedia.org/wiki/Remote_Associates_Test)ï¼‰
- en: Producing as many alternative uses for everyday objects ([Alternate Uses Task](https://en.wikipedia.org/wiki/Guilford%27s_Alternate_Uses))
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naming ten nouns that differ as much as possible from each other ([Divergent
    Association Task](https://en.wikipedia.org/wiki/Divergent_Association_Task))
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Through these analyses, I focus on the change in performance from the earlier
    GPT-2 model all the way to the newest GPT-4 model. I use both a subjective and
    objective lens to understand the behavior of these models. In general, the later
    models tend to perform better on all of the creativity tasks, although I do find
    a few exceptions where, when the test has a â€œcorrectâ€ answerâ€, the simpler GPT-2
    model outperforms the newer GPT4 model. When possible, I compare the GPT models
    to human performances on the same creativity tests.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: These experiments aim not to measure how creative the models are, but
    rather to measure the level of creative process present in the model generations.
    I am not claiming that the GPT models, or LLMs in general, possess creative thinking
    in the same way humans do. Rather, I aim to show how the models respond to particular
    measures of creative processes.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: A Progression of GPT Models
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative Pre-trained Transformer (GPT) models are language models trained
    on vast portions of the Internet to predict the next token based on previous tokens.
    While the recent ChatGPT has captured the publicâ€™s attention in the last few months,
    OpenAIâ€™s GPT models (and the technologies behind the models) have been around
    for a few years.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: GPT-1 (2018, 117M parameters). The original GPT model. I used the [open-sourced
    version on HuggingFace](https://huggingface.co/openai-gpt). However, the outputs
    were so gibberish so I did not include any of them in this article.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paper: [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdfhttps://en.wikipedia.org/wiki/GPT-2#:~:text=Generative%20Pre%2Dtrained%20Transformer%202,by%20OpenAI%20in%20February%202019.)
    (2019, 1.5B parameters). I used the [open-soured version on HuggingFace](https://huggingface.co/gpt2).
    For some experiments, GPT-2 outputs were gibberish and not useful for understanding
    its behavior, so I did not include those in the respective analyses. Paper: [Language
    Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GPT-3](https://en.wikipedia.org/wiki/GPT-3) (2020, 175B parameters). Over
    100 times bigger than GPT-2\. I used *text-davinci-002* from the [OpenAI APIs](https://platform.openai.com/docs/models/gpt-3-5).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paper: [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: GPT-3.5 (2022). While there are multiple versions of GPT-3.5, I use the same
    model underlying the original [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT),
    which was trained using reinforcement learning with human feedback. I used *gpt-3.5-turbo*
    from the [OpenAI APIs](https://platform.openai.com/docs/models/gpt-3-5).
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-3.5ï¼ˆ2022ï¼‰ã€‚è™½ç„¶ GPT-3.5 æœ‰å¤šä¸ªç‰ˆæœ¬ï¼Œæˆ‘ä½¿ç”¨çš„æ˜¯ä¸åŸç‰ˆ [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT)
    ç›¸åŒçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡å¸¦æœ‰äººå·¥åé¦ˆçš„å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä½¿ç”¨äº†æ¥è‡ª [OpenAI APIs](https://platform.openai.com/docs/models/gpt-3-5)
    çš„ *gpt-3.5-turbo*ã€‚
- en: '[GPT-4](https://arxiv.org/abs/2303.08774) (2023, ~100T parameters). I used
    *gpt-4* from the [OpenAI APIs](https://platform.openai.com/docs/models/gpt-3-5).'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT-4](https://arxiv.org/abs/2303.08774)ï¼ˆ2023ï¼Œçº¦ 100T å‚æ•°ï¼‰ã€‚æˆ‘ä½¿ç”¨äº†æ¥è‡ª [OpenAI APIs](https://platform.openai.com/docs/models/gpt-3-5)
    çš„ *gpt-4*ã€‚'
- en: 'Paper: [GPT-4 Technical Report](https://arxiv.org/pdf/2303.08774.pdf).'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'è®ºæ–‡: [GPT-4 æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/pdf/2303.08774.pdf)ã€‚'
- en: For all of the experiments in this article, unless otherwise mentioned, I called
    each GPT model five times per unique prompt to account for randomness in its generations.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡çš„æ‰€æœ‰å®éªŒä¸­ï¼Œé™¤éå¦æœ‰è¯´æ˜ï¼Œæˆ‘å¯¹æ¯ä¸ªç‹¬ç‰¹çš„æç¤ºè°ƒç”¨äº†æ¯ä¸ª GPT æ¨¡å‹äº”æ¬¡ï¼Œä»¥è€ƒè™‘å…¶ç”Ÿæˆçš„éšæœºæ€§ã€‚
- en: '1\. Remote Associates Test: Connecting Unrelated Words'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. è¿œç¨‹è”æƒ³æµ‹è¯•ï¼šè¿æ¥æ— å…³è¯
- en: The Remote Associates Test (RAT) is a creativity test developed by Martha Mednick
    in 1962 [[2](#footnote-2)]. This test measures convergent thinking, meaning that
    **each question has a â€œrightâ€ answer.**
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¿œç¨‹è”æƒ³æµ‹è¯•ï¼ˆRATï¼‰æ˜¯ç”±ç›èÂ·æ¢…å¾·å°¼å…‹ï¼ˆMartha Mednickï¼‰äº 1962 å¹´å¼€å‘çš„ä¸€ç§åˆ›é€ åŠ›æµ‹è¯• [[2](#footnote-2)]ã€‚è¯¥æµ‹è¯•è¡¡é‡çš„æ˜¯èšåˆæ€ç»´ï¼Œè¿™æ„å‘³ç€**æ¯ä¸ªé—®é¢˜éƒ½æœ‰ä¸€ä¸ªâ€œæ­£ç¡®â€çš„ç­”æ¡ˆ**ã€‚
- en: Given three seemingly unrelated words, the test taker must find a fourth word
    connecting the three words. For example, the words â€œdreamâ€, â€œbreakâ€, and â€œlightâ€
    can be linked by the word â€œdayâ€ (daydream, daybreak, daylight). The words â€œsenseâ€,
    â€œcourtesyâ€, and â€œplaceâ€ can be linked by the word â€œcommonâ€.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸‰ä¸ªçœ‹ä¼¼æ— å…³çš„è¯ï¼Œæµ‹è¯•è€…å¿…é¡»æ‰¾åˆ°ä¸€ä¸ªç¬¬å››ä¸ªè¯æ¥è¿æ¥è¿™ä¸‰ä¸ªè¯ã€‚ä¾‹å¦‚ï¼Œâ€œdreamâ€ã€â€œbreakâ€å’Œâ€œlightâ€å¯ä»¥é€šè¿‡â€œdayâ€ï¼ˆdaydream,
    daybreak, daylightï¼‰æ¥è”ç³»ã€‚è¯æ±‡â€œsenseâ€ã€â€œcourtesyâ€å’Œâ€œplaceâ€å¯ä»¥é€šè¿‡â€œcommonâ€æ¥è”ç³»ã€‚
- en: I collected the word triplets from [the Remote Associates Test website](https://www.remote-associates-test.com/).
    **There were a total of 312 questions.** The difficulty of each question was rated
    on a scale from Very Easy to Very Hard.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä» [è¿œç¨‹è”æƒ³æµ‹è¯•ç½‘ç«™](https://www.remote-associates-test.com/) æ”¶é›†äº†è¯æ±‡ä¸‰å…ƒç»„ã€‚**æ€»å…±æœ‰ 312 é“é—®é¢˜**ã€‚æ¯ä¸ªé—®é¢˜çš„éš¾åº¦æŒ‰ä»éå¸¸å®¹æ˜“åˆ°éå¸¸å›°éš¾çš„ç­‰çº§è¿›è¡Œè¯„åˆ†ã€‚
- en: '![](../Images/2f2d6ab51eb2f16a24735f7e563ff4a4.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f2d6ab51eb2f16a24735f7e563ff4a4.png)'
- en: Breakdown of the Random Association Test questions taken from the [test website](https://www.remote-associates-test.com/).
    Created by the author.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä» [æµ‹è¯•ç½‘ç«™](https://www.remote-associates-test.com/) è·å–çš„éšæœºè”æƒ³æµ‹è¯•é¢˜ç›®åˆ†è§£ã€‚ç”±ä½œè€…åˆ›å»ºã€‚
- en: For each model and each triplet of words, I used the following prompt template.
    I called each model 5 times to account for randomness.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªæ¨¡å‹å’Œæ¯ç»„ä¸‰ä¸ªè¯ï¼Œæˆ‘ä½¿ç”¨äº†ä»¥ä¸‹æç¤ºæ¨¡æ¿ã€‚æˆ‘è°ƒç”¨äº†æ¯ä¸ªæ¨¡å‹ 5 æ¬¡ï¼Œä»¥è€ƒè™‘éšæœºæ€§ã€‚
- en: You are given three words. Think of a fourth word that is somehow related to
    each of the first three words. First give the answer (â€œAnswerâ€) then explain your
    reasoning (â€œReasonâ€).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸‰ä¸ªè¯ã€‚æƒ³å‡ºä¸€ä¸ªä¸å‰ä¸‰ä¸ªè¯éƒ½æœ‰æŸç§å…³è”çš„ç¬¬å››ä¸ªè¯ã€‚é¦–å…ˆç»™å‡ºç­”æ¡ˆï¼ˆâ€œAnswerâ€ï¼‰ï¼Œç„¶åè§£é‡Šä½ çš„æ¨ç†ï¼ˆâ€œReasonâ€ï¼‰ã€‚
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Newer GPT models score higher on remote association
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¾ƒæ–°çš„ GPT æ¨¡å‹åœ¨è¿œç¨‹è”æƒ³ä¸Šå¾—åˆ†æ›´é«˜
- en: I calculated the overall percent of questions each GPT model generated a correct
    answer for. It may come as no surprise that the the newer the model, the more
    correct answers it generated.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¡ç®—äº†æ¯ä¸ª GPT æ¨¡å‹ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆçš„æ€»ä½“ç™¾åˆ†æ¯”ã€‚å¯èƒ½å¹¶ä¸å¥‡æ€ªï¼Œ**æ¨¡å‹è¶Šæ–°ï¼Œç”Ÿæˆçš„æ­£ç¡®ç­”æ¡ˆè¶Šå¤š**ã€‚
- en: '![](../Images/bdaad2048a6c8c37702328e4b74d82bd.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdaad2048a6c8c37702328e4b74d82bd.png)'
- en: Percentage of questions each GPT model answered correctly on the Remote Associates
    Test. Created by the author.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ª GPT æ¨¡å‹åœ¨è¿œç¨‹è”æƒ³æµ‹è¯•ä¸­æ­£ç¡®å›ç­”é—®é¢˜çš„ç™¾åˆ†æ¯”ã€‚ç”±ä½œè€…åˆ›å»ºã€‚
- en: 'Breaking down model performance by question difficulty level showed a similar
    trend: newer GPT models performed better than older ones on each difficulty level.
    And while the newer GPT models performed better than the older ones on hard questions,
    the improvement was less drastic than for easier questions. This means that **even
    GPT-4 struggled on difficult questions.**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‰ç…§é—®é¢˜éš¾åº¦çº§åˆ«åˆ†è§£æ¨¡å‹è¡¨ç°æ˜¾ç¤ºäº†ç±»ä¼¼çš„è¶‹åŠ¿ï¼šè¾ƒæ–°çš„ GPT æ¨¡å‹åœ¨æ¯ä¸ªéš¾åº¦çº§åˆ«ä¸Šçš„è¡¨ç°å‡ä¼˜äºæ—§æ¨¡å‹ã€‚è™½ç„¶è¾ƒæ–°çš„ GPT æ¨¡å‹åœ¨éš¾é¢˜ä¸Šçš„è¡¨ç°ä¼˜äºæ—§æ¨¡å‹ï¼Œä½†è¿™ç§æ”¹è¿›æ¯”åœ¨ç®€å•é—®é¢˜ä¸Šçš„æ”¹è¿›è¦å°‘ã€‚è¿™æ„å‘³ç€**å³ä½¿æ˜¯
    GPT-4 åœ¨å›°éš¾é—®é¢˜ä¸Šä¹Ÿæ„Ÿåˆ°åƒåŠ›**ã€‚
- en: '![](../Images/a0449267a411f9f1f2c16850305c1c9e.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0449267a411f9f1f2c16850305c1c9e.png)'
- en: Percentage of questions each GPT model answered correctly on the Remote Associates
    Test, broken down by question difficulty level. Created by the author.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªGPTæ¨¡å‹åœ¨è¿œç¨‹è”æƒ³æµ‹è¯•ä¸­æŒ‰é—®é¢˜éš¾åº¦çº§åˆ«çš„æ­£ç¡®å›ç­”ç™¾åˆ†æ¯”ã€‚ç”±ä½œè€…åˆ›å»ºã€‚
- en: According to the Remote Associates Test website, â€œA typical person can solve
    most of the items marked as easy, about half of the medium ones, and few of the
    hard ones.â€ There were no official statistics shared on the website, but a quick
    glance would show that GPT-4 did indeed solve around half of the medium ones and
    a few of the hard ones, making it the only model approximately comparable to human
    level. The other GPT models were worse than human performance.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®è¿œç¨‹è”æƒ³æµ‹è¯•ç½‘ç«™ï¼Œâ€œå…¸å‹çš„äººå¯ä»¥è§£å†³å¤§å¤šæ•°æ ‡è®°ä¸ºç®€å•çš„é—®é¢˜ï¼Œå¤§çº¦ä¸€åŠçš„ä¸­ç­‰éš¾åº¦é—®é¢˜ï¼Œä»¥åŠå¾ˆå°‘çš„éš¾é—®é¢˜ã€‚â€ ç½‘ç«™ä¸Šæ²¡æœ‰åˆ†äº«å®˜æ–¹ç»Ÿè®¡æ•°æ®ï¼Œä½†ä»å¿«é€Ÿæµè§ˆæ¥çœ‹ï¼ŒGPT-4
    ç¡®å®è§£å†³äº†å¤§çº¦ä¸€åŠçš„ä¸­ç­‰éš¾åº¦é—®é¢˜å’Œä¸€äº›éš¾é—®é¢˜ï¼Œä½¿å…¶æˆä¸ºå”¯ä¸€ä¸€ä¸ªå¤§è‡´å¯ä¸äººç±»æ°´å¹³ç›¸åª²ç¾çš„æ¨¡å‹ã€‚å…¶ä»– GPT æ¨¡å‹çš„è¡¨ç°åˆ™é€Šè‰²äºäººç±»æ°´å¹³ã€‚
- en: 'A few exceptions: when GPT-2 gets it right but GPT-4 gets it wrong'
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸€äº›ä¾‹å¤–ï¼šå½“GPT-2å›ç­”æ­£ç¡®è€ŒGPT-4å›ç­”é”™è¯¯
- en: While it didnâ€™t happen often, there were a few questions for which the more
    rudimentary GPT-2 model from 2019 was able to generate the correct answer but
    the GPT-4 model from 2023 was not able to. In fact, there were 8 such questions.
    (The full list is shown in the Appendix at the end of the article).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ä¸å¸¸å‘ç”Ÿï¼Œä½†ç¡®å®æœ‰ä¸€äº›é—®é¢˜ï¼Œ2019å¹´çš„æ›´åŸºç¡€çš„GPT-2æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ­£ç¡®ç­”æ¡ˆï¼Œè€Œ2023å¹´çš„GPT-4æ¨¡å‹å´ä¸èƒ½ã€‚å®é™…ä¸Šï¼Œæœ‰8ä¸ªè¿™æ ·çš„ä¾‹å­ã€‚ï¼ˆå®Œæ•´åˆ—è¡¨è§æ–‡ç« æœ«å°¾çš„é™„å½•ï¼‰ã€‚
- en: '**An easy question**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸€ä¸ªç®€å•çš„é—®é¢˜**'
- en: 'Words: cottage / brick / cake'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å•è¯ï¼šå°å±‹ / ç –å— / è›‹ç³•
- en: 'GPT-2: â€œcheeseâ€ [correct answer]'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2ï¼šâ€œå¥¶é…ªâ€ [æ­£ç¡®ç­”æ¡ˆ]
- en: 'GPT-4: â€œlayerâ€'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4ï¼šâ€œå±‚â€
- en: '**A medium question**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸€ä¸ªä¸­ç­‰éš¾åº¦çš„é—®é¢˜**'
- en: 'Words: cat / number / phone'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å•è¯ï¼šçŒ« / æ•°å­— / ç”µè¯
- en: 'GPT-2: â€œcallâ€'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2ï¼šâ€œå‘¼å«â€
- en: 'GPT-4: â€œdialâ€'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4ï¼šâ€œæ‹¨å·â€
- en: '**A hard question**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸€ä¸ªéš¾çš„é—®é¢˜**'
- en: 'Words: through / disk / way'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å•è¯ï¼šé€šè¿‡ / ç£ç›˜ / è·¯å¾„
- en: 'GPT-2: â€œdriveâ€ [correct]'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2ï¼šâ€œé©¾é©¶â€ [æ­£ç¡®]
- en: 'GPT-4: â€œcircleâ€'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4ï¼šâ€œåœ†åœˆâ€
- en: '**A very hard question**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸€ä¸ªéå¸¸éš¾çš„é—®é¢˜**'
- en: 'Words: land / hand / house'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å•è¯ï¼šåœŸåœ° / æ‰‹ / æˆ¿å­
- en: 'GPT-2: â€œfarmâ€ [correct]'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2ï¼šâ€œå†œåœºâ€ [æ­£ç¡®]
- en: 'GPT-4: â€œpropertyâ€'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4ï¼šâ€œè´¢äº§â€
- en: Notice that for all of these questions, GPT-4â€™s answers are *not* unreasonable.
    In fact, without knowing the correct answer beforehand, you might assume that
    GPT-4â€™s answer could as likely connect the three words. If youâ€™re thinking that,
    youâ€™re not wrong. Iâ€™ll address this in the next section.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå¯¹äºæ‰€æœ‰è¿™äº›é—®é¢˜ï¼ŒGPT-4 çš„å›ç­”æ˜¯ *ä¸* ä¸åˆç†çš„ã€‚å®é™…ä¸Šï¼Œå¦‚æœäº‹å…ˆä¸çŸ¥é“æ­£ç¡®ç­”æ¡ˆï¼Œä½ å¯èƒ½ä¼šè®¤ä¸º GPT-4 çš„ç­”æ¡ˆåŒæ ·å¯ä»¥å°†è¿™ä¸‰ä¸ªè¯è”ç³»èµ·æ¥ã€‚å¦‚æœä½ è¿™æ ·æƒ³ï¼Œä½ å¹¶æ²¡æœ‰é”™ã€‚æˆ‘å°†åœ¨ä¸‹ä¸€éƒ¨åˆ†è®¨è®ºè¿™ä¸ªé—®é¢˜ã€‚
- en: Was GPT-4 wrong or just thinking outside of the box?
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-4 æ˜¯é”™è¯¯çš„ï¼Œè¿˜æ˜¯åªæ˜¯æ€ç»´æ–¹å¼ä¸åŒï¼Ÿ
- en: GPT-4 answered more than half of the questions incorrectly, but maybe with good
    reason. I asked GPT-4 to explain each of its answers. Then, I looked at its explanations
    for a few of the â€œVery Hardâ€ questions that it got wrong.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 é”™è¯¯åœ°å›ç­”äº†è¶…è¿‡ä¸€åŠçš„é—®é¢˜ï¼Œä½†å¯èƒ½æœ‰å……åˆ†çš„ç†ç”±ã€‚æˆ‘è®© GPT-4 è§£é‡Šäº†æ¯ä¸€ä¸ªç­”æ¡ˆã€‚ç„¶åï¼Œæˆ‘æŸ¥çœ‹äº†å®ƒå¯¹å‡ ä¸ªâ€œéå¸¸éš¾â€é—®é¢˜çš„è§£é‡Šã€‚
- en: '**Words: artist / hatch / route**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**å•è¯ï¼šè‰ºæœ¯å®¶ / å­µåŒ– / è·¯çº¿**'
- en: 'Correct answer: â€œescapeâ€'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ­£ç¡®ç­”æ¡ˆï¼šâ€œé€ƒè„±â€
- en: 'GPT-4 answer: â€œmapâ€'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4 ç­”æ¡ˆï¼šâ€œåœ°å›¾â€
- en: The word â€œartistâ€ might bring to mind the idea of creating something, perhaps
    even a visual representation of something. Maps are a common form of visual representation
    that require a degree of artistic skill to create.
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œè‰ºæœ¯å®¶â€è¿™ä¸ªè¯å¯èƒ½ä¼šè®©äººæƒ³åˆ°åˆ›é€ æŸäº›ä¸œè¥¿ï¼Œç”šè‡³æ˜¯æŸç§è§†è§‰è¡¨ç°å½¢å¼ã€‚åœ°å›¾æ˜¯ä¸€ç§å¸¸è§çš„è§†è§‰è¡¨ç°å½¢å¼ï¼Œéœ€è¦ä¸€å®šçš„è‰ºæœ¯æŠ€èƒ½æ¥åˆ¶ä½œã€‚
- en: ''
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The word â€œhatchâ€ could refer to the act of opening something or bringing something
    into existence. A map is a type of blueprint or plan, often used to navigate new
    territory or chart out a course of action.
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œå­µåŒ–â€è¿™ä¸ªè¯å¯ä»¥æŒ‡æ‰“å¼€æŸç‰©æˆ–ä½¿æŸç‰©å­˜åœ¨çš„è¡Œä¸ºã€‚åœ°å›¾æ˜¯ä¸€ç§è“å›¾æˆ–è®¡åˆ’ï¼Œé€šå¸¸ç”¨äºå¯¼èˆªæ–°é¢†åœŸæˆ–è§„åˆ’è¡ŒåŠ¨è·¯çº¿ã€‚
- en: ''
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The word â€œrouteâ€ is related to direction and movement, which can be plotted
    and tracked on a map. A map can help you determine the best route to take when
    traveling, and can also provide information about the routes others have taken
    in the past.
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œè·¯çº¿â€è¿™ä¸ªè¯ä¸æ–¹å‘å’Œè¿åŠ¨ç›¸å…³ï¼Œè¿™äº›éƒ½å¯ä»¥åœ¨åœ°å›¾ä¸Šç»˜åˆ¶å’Œè¿½è¸ªã€‚åœ°å›¾å¯ä»¥å¸®åŠ©ä½ ç¡®å®šæ—…è¡Œæ—¶æœ€ä½³çš„è·¯çº¿ï¼Œå¹¶ä¸”å¯ä»¥æä¾›å…³äºå…¶ä»–äººè¿‡å»æ‰€èµ°è¿‡è·¯çº¿çš„ä¿¡æ¯ã€‚
- en: ''
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All in all, â€œMapâ€ seems to be a word that fits well with each of the three initial
    words, as it encapsulates the ideas of artistic creation, planning and direction,
    and movement and navigation.
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ€»çš„æ¥è¯´ï¼Œâ€œåœ°å›¾â€ä¼¼ä¹æ˜¯ä¸€ä¸ªä¸æœ€åˆçš„ä¸‰ä¸ªè¯ç›¸ç¬¦çš„è¯ï¼Œå› ä¸ºå®ƒåŒ…å«äº†è‰ºæœ¯åˆ›ä½œã€è§„åˆ’å’Œæ–¹å‘ï¼Œä»¥åŠè¿åŠ¨å’Œå¯¼èˆªçš„æ¦‚å¿µã€‚
- en: GPT-4â€™s explanation for why â€œmapâ€ is a good word to associate with â€œartist /
    hatch / routeâ€ is convincing.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 å¯¹äºä¸ºä»€ä¹ˆâ€œåœ°å›¾â€æ˜¯ä¸€ä¸ªä¸â€œè‰ºæœ¯å®¶ / å­µåŒ– / è·¯çº¿â€ç›¸å…³çš„å¥½è¯çš„è§£é‡Šå¾ˆæœ‰è¯´æœåŠ›ã€‚
- en: 'Here is another example:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '**Words: fork / dark / man**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'Correct answer: â€œpitchâ€'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-4 answer: â€œshadowâ€'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The word â€œforkâ€ can be associated with division or branching, such as a fork
    in the road. Shadows can also be seen as a type of division, as they represent
    the absence of light.
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The word â€œdarkâ€ refers to the absence of light. Shadows are created when light
    is blocked by an object, and can often appear darker than the surrounding area.
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The word â€œmanâ€ can be associated with the concept of identity or individuality.
    Shadows can also be used to represent the identity of a person, as they can provide
    clues about their shape and size.
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Therefore, â€œShadowâ€ seems to be a word that fits well with each of the initial
    three words, as it encapsulates the ideas of division, absence, and identity.
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: These examples show the pitfall of the Remote Associates Test, and of convergent
    creativity tests in general â€” that is, when the â€œcorrectâ€ answer may not be the
    only â€œrightâ€ answer. In these cases, GPT-4 generated an answer that was â€œwrongâ€,
    but only because it did not meet the conventional â€œcorrectâ€ answer.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: In fact, if I ask GPT-4 to brainstorm multiple options for a fourth word for
    a triplet of words, it is able to give me multiple options with convincing answers.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/efe8aafae763d1ba30351195a4efd50b.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: GPT-4 responses to generate multiple â€œrightâ€ answers for a triplet of words.
    Screenshot taken by the author.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 answers arbitrarily generated triplet of words
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is possible that the GPT models may have seen some of the above Remote Associates
    Test questions in its training data. How would GPT-4 respond to an arbitrarily
    generated triplet of words? In this section, I construct my own â€œremote associationâ€
    test and evaluate the outputs subjectively.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: I took a list of the [10,000 most common English words](https://github.com/first20hours/google-10000-english)
    [[3](#footnote-3)] Then I randomly sampled three words from this list and had
    GPT-4 provide a fourth word tying these words together. Some of the three-word
    combinations were quite strange and unrelated, but GPT-4 always found a way to
    tie them together. I encourage the reader to try these first before reading GPT-4â€™s
    generations!
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: After viewing the next few examples, you can make your decision â€” is GPT-4 thinking
    outside of the box? Or is it just shoehorning its answers? (A longer list of examples
    is shown in the Appendix at the end of the article).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: â›‘ï¸ **Health**
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: This response read as quite convincing. I found it interesting that the definition
    for â€œhealth surveillanceâ€ was copied almost verbatim from the [CDC website on
    Introduction to Public Health Surveillance](https://www.cdc.gov/training/publichealth101/surveillance.html#anchor_available_materials).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '*Public health surveillance is â€œthe ongoing, systematic collection, analysis,
    and interpretation of health-related data essential to planning, implementation,
    and evaluation of public health practice.â€ â€”* Field Epidemiology'
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: (from the CDC website)
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**ğŸ€ Clovers**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: I was initially convinced by GPT-4â€™s answer â€” in fact, it felt quite clever.
    That is, until I did a quick Google search and found out that *Trifolium duodecim*
    is not real. This is an example of a [hallucination](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)),
    a common phenomena in which LLMs make up an answer that is not factually correct.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**â˜•ï¸ Starbucks**'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: This example was my favorite the answer, while thinking out of the box, felt
    like kind of a stretch. And it was a little bit funny (especially the part about
    Patrick).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: These examples and analyses illustrate that creativity tests with a single correct
    answer might be limited. This limitation is addressed in the other tests explored
    in this article in the next sections.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Alternate Uses Task: Finding Meaning in Ordinary Items'
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another popular test for measuring creativity is the Alternative Uses Task (AUT),
    which was created by J. P. Guilford in 1967\. Unlike the Remote Associations Test,
    this test measures divergent thinking, meaning that **there is** **no right answer.**
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: The test taker is asked to produce as many original uses for an everyday object
    as possible. For example, a â€œlaptopâ€ can be used as a â€œplateâ€, as a â€œbody shieldâ€,
    or as â€œwall artâ€ [[4](#footnote-4)]. Responses to this test are measured in terms
    of metrics such as originality (how different the uses are) and flexibility (the
    assortment of ideas). The purpose of this test is to see how many unconventional
    uses a person can think of for an ordinary object.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: However, due to my limited resources, I did not have the ability to generate
    these measures without human annotators. Therefore, I used a **proxy for creativity,**
    which was also used by researchers from the University of Amsterdam in [Putting
    GPT-3â€™s Creativity to the (Alternative Uses) Test](https://arxiv.org/pdf/2206.08932.pdf).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, the researchers asked both humans and GPT-3 (davinci-002) to
    come up with alternate use cases for the following common objects: fork, tin can,
    and book. As a proxy for gauging creativity, they measured the *semantic distance*
    between the object and the generated use cases â€” that is, how different is the
    generated use case word from the original object?'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: I obtained the human responses from [the paperâ€™s project website](https://osf.io/vmk3c/?view_only=)
    for comparison with human performance. I used the following prompt for the GPT
    models, based on a similar prompt used int he paper.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'Come up with 10 creative uses for the following item that might strike people
    as clever, unusual, interesting, uncommon, humorous, innovative, or different:
    {item}'
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Cosine Distance between embeddings as proxy for creativity
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In natural language processing, semantic distance (or [semantic similarity](https://en.wikipedia.org/wiki/Semantic_similarity))
    is a proxy for how related two words are, based on how often words are used together
    in similar contexts. For example, the words â€œcomputerâ€ and â€œscreenâ€ might have
    a shorter semantic distance, because they are often used in the same contexts,
    whereas the words â€œcomputerâ€ and â€œlentilsâ€ might have a greater semantic distance,
    because they are not often used in the same contexts.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: To compute the semantic distance of two words, I used the same approach as used
    by the researchers in [the paper mentioned above.](https://arxiv.org/pdf/2206.08932.pdf)
    I used [Spacy](https://spacy.io/models/en) (`en_core_web_lg`), a natural language
    processing library, to calculate word embeddings for the object (e.g. â€œlaptopâ€)
    and the use case word (e.g. â€œplateâ€). Then, I used cosine distance between the
    two embeddings to calculate the semantic distance.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: For the Alternate Uses Task, **larger semantic distances correlated with â€œmore
    creativeâ€** use cases for objects. The results showed that for the three objects
    (fork, tin can, and book), human performance reliably resulted in higher cosine
    distances (and therefore higher semantic dissimilarity) than for GPT-3\. However,
    there were cases when GPT-3.5 and/or GPT-4 matched or surpassed human creativity
    semantic distance scores.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/39de38d8fc97ba32e9443e9288a67949.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: Semantic distance (measured using cosine distance) of original word to alternate
    use case. Created by author.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: While humans tended to come up with more creative use cases than GPT-3 (using
    semantic distance, as calculated by cosine distance, as a proxy for creativity),
    GPT-4 was more on par with human level. That is, GPT-4 responded with items resulting
    in larger semantic distances with the original item, at a similar level to human
    performance. However, without actual humans annotating and deciding more subjective
    categories such as originality, flexibility, and usefulness, this quantification
    using cosine distance is limited as it only measures a small part of the GPT modelsâ€™
    creative outputs.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Can you distinguish human-generated vs. AI-generated use cases?
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the lists shows the human-generated alternate use cases for a tin can
    and the other shows alternate use cases generated by GPT-4\. Can you tell the
    difference? The answer is at the end.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'List 1: Uses for tin can'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: as a mirror
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: to create toys
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as art
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as a reminder of Andy Warhol
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as a key ring with the clip from the can
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as jewelry
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'List 2: Uses for tin can'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: miniature drum set
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: quirky plant pot
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: impromptu cookie cutter
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: homemade camp stove
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: whimsical wind chimes
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: miniature herb garden
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next test in the following section also measures divergent thinking.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Divergent Association Task: Ten Words, No Connections'
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The third creativity test is the Divergent Association Task (DAT), created more
    recently in 2021 by a group of researchers [[5](#footnote-5)]. This test involves
    naming ten nouns that differ from each other as much as possible. This test, as
    evident by its name, is divergent and also **has no correct answers.**
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a [version of the test online on this website](https://www.datcreativity.com/).
    It only takes a few minutes to take, as it involves coming up with a list of ten
    nouns as unrelated as possible. Hereâ€™s my result from the website:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa3a465a40574d28ff017b98685d1feb.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: Screenshot taken from the Divergent Association Task. Created by the author.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: The DAT Score
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'How is the score calculated? The authors describe their process [here](https://www.datcreativity.com/faq).
    The DAT score is calculated using embeddings and cosine distance. I adapt their
    process for calculating the score [[6](#footnote-6)]:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Calculate [GloVe](https://nlp.stanford.edu/projects/glove/) embeddings for each
    word
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the cosine distance between each pair of words
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the average of that value
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process is similar to the method used to calculate creativity proxy score
    in the Alternate Uses Task, with the only difference being the word embedding
    model used.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Divergence among models
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following prompt, which I use to prompt the GPT models, is taken from the
    [DAT website](https://www.datcreativity.com/task).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: I did not include GPT-2 in the results below, as the outputs generated by GPT-2
    were gibberish. (You can see an example of this in the Appendix at the end of
    the article).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: The results show that the DAT score increased for each newer model. This means
    that later models were better at generating 10 words semantically different from
    each other as possible.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/47b6d5b8fda43230d69db455aa67f200.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: Divergent Association Test scores, as measured by cosine distance, for the different
    models. Generated by the author.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following list generated by GPT-3 had a score of 0.36:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'On the other hand, the following list generated by GPT-4 had a score of 0.95:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: According to the [paper introducing the DAT test](https://www.pnas.org/doi/10.1073/pnas.2022340118),
    scores commonly ranged from 65 to 90, with an average between 75 and 80\. A score
    of 95 was considered very high.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Something to note in the above figure is that the error bars for GPT-4 were
    much smaller than those for GPT-3\. The significance of this becomes much clearer
    in the next experiment, in which I varied the amount of randomness in each GPT
    modelâ€™s generation by tweaking the â€œtemperatureâ€ parameter.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Temperature is a value between 0 and 1, with values closer to 0 being the most
    predictable and 1 being the most random. I tried temperature values between 0.5
    and 0.9 in increments of 0.1 (In previous experiments I kept the temperature at
    0.7, which is the default value). I expected more variance for higher temperature
    values and more consistency for lower temperatures.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: However, the results showed that the temperature did not matter as much as the
    model in terms of semantic distance variance. For GPT-3, despite the temperature,
    the DAT score for each list of 10 words varied greatly. Whereas, on the other
    hand, for GPT-4, despite the temperature, the DAT score remained tightly consistent
    in a small range with very little variance.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œç»“æœæ˜¾ç¤ºæ¸©åº¦å¯¹è¯­ä¹‰è·ç¦»å˜å¼‚æ€§çš„å½±å“ä¸å¦‚æ¨¡å‹æœ¬èº«é‡è¦ã€‚å¯¹äºGPT-3è€Œè¨€ï¼Œå°½ç®¡æœ‰æ¸©åº¦çš„å½±å“ï¼Œæ¯ç»„10ä¸ªè¯çš„DATå¾—åˆ†å˜åŒ–å¾ˆå¤§ã€‚è€Œå¯¹äºGPT-4è€Œè¨€ï¼Œå°½ç®¡æœ‰æ¸©åº¦çš„å½±å“ï¼ŒDATå¾—åˆ†åœ¨ä¸€ä¸ªå°èŒƒå›´å†…ä¿æŒäº†é«˜åº¦ä¸€è‡´ï¼Œå˜åŒ–éå¸¸å°ã€‚
- en: '![](../Images/b7c569d6c2a668b897761c671eca6ea5.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7c569d6c2a668b897761c671eca6ea5.png)'
- en: Average cosine distance among word embeddings, separated by model type and temperature
    score. Created by the author.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‰æ¨¡å‹ç±»å‹å’Œæ¸©åº¦åˆ†æ•°åˆ†å¼€çš„è¯åµŒå…¥çš„å¹³å‡ä½™å¼¦è·ç¦»ã€‚ä½œè€…åˆ›å»ºã€‚
- en: The implications of this became clearer once I looked at the actual text being
    generated by the three GPT models. **GPT-4 was way less variable in its generations
    than GPT-3.** In a way, it felt like GPT-4 kind of already knew the â€œright answerâ€
    for a task that is supposed to have no right answer.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘æŸ¥çœ‹äº†ä¸‰ç§GPTæ¨¡å‹ç”Ÿæˆçš„å®é™…æ–‡æœ¬ï¼Œè¿™ä¸€ç‚¹å˜å¾—æ›´åŠ æ¸…æ™°ã€‚**GPT-4åœ¨ç”Ÿæˆè¯æ±‡æ—¶çš„å˜å¼‚æ€§è¿œä½äºGPT-3ã€‚** ä»æŸç§ç¨‹åº¦ä¸Šè¯´ï¼Œæ„Ÿè§‰GPT-4ä¼¼ä¹å·²ç»çŸ¥é“äº†ä¸€ä¸ªåº”è¯¥æ²¡æœ‰æ­£ç¡®ç­”æ¡ˆçš„ä»»åŠ¡çš„â€œæ­£ç¡®ç­”æ¡ˆâ€ã€‚
- en: Aggregating the responses for all temperature scores, I had each model generate
    a list of 10 words 50 times. Then, I looked at how common different words appeared
    across all 50 runs. Note that the most commonly generated word by GPT-3 was book,
    which appeared 19 times out of 50 = 38%. On the other hand, the most commonly
    generated word by GPT-4 was microscope, which appeared **48/50=96% of the time**.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: æ±‡æ€»æ‰€æœ‰æ¸©åº¦åˆ†æ•°çš„å“åº”ï¼Œæˆ‘è®©æ¯ä¸ªæ¨¡å‹ç”Ÿæˆ10ä¸ªè¯æ±‡çš„åˆ—è¡¨50æ¬¡ã€‚ç„¶åï¼Œæˆ‘æŸ¥çœ‹äº†ä¸åŒè¯æ±‡åœ¨æ‰€æœ‰50æ¬¡è¿è¡Œä¸­çš„å‡ºç°é¢‘ç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒGPT-3æœ€å¸¸ç”Ÿæˆçš„è¯æ˜¯bookï¼Œå‡ºç°äº†50æ¬¡ä¸­çš„19æ¬¡
    = 38%ã€‚å¦ä¸€æ–¹é¢ï¼ŒGPT-4æœ€å¸¸ç”Ÿæˆçš„è¯æ˜¯æ˜¾å¾®é•œï¼Œå‡ºç°äº†**48/50=96%**ã€‚
- en: '![](../Images/323fb7c8bfe263f62d4408741add0e58.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/323fb7c8bfe263f62d4408741add0e58.png)'
- en: Top words generated by each model for the DAT test (aggregated across temperature
    scores). Created by the words.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ¨¡å‹ä¸ºDATæµ‹è¯•ç”Ÿæˆçš„é¡¶çº§è¯æ±‡ï¼ˆæ ¹æ®æ¸©åº¦åˆ†æ•°æ±‡æ€»ï¼‰ã€‚ç”±è¯æ±‡åˆ›å»ºã€‚
- en: This validates the claim that GPT-3 is, in fact, more variable in its word generations,
    while GPT-4 played it safe by predicting a lot of the same words it knew would
    score high on the â€œdiverse wordsâ€ tasks â€” words like microscope, volcano, elephant,
    and galaxy.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™éªŒè¯äº†GPT-3åœ¨ç”Ÿæˆè¯æ±‡æ—¶ç¡®å®æ¯”GPT-4æ›´å…·å˜å¼‚æ€§ï¼Œè€ŒGPT-4åˆ™é‡‡å–äº†æ›´ä¸ºä¿å®ˆçš„ç­–ç•¥ï¼Œé¢„æµ‹äº†è®¸å¤šå®ƒçŸ¥é“ä¼šåœ¨â€œå¤šæ ·åŒ–è¯æ±‡â€ä»»åŠ¡ä¸­å¾—é«˜åˆ†çš„è¯æ±‡â€”â€”å¦‚æ˜¾å¾®é•œã€ç«å±±ã€å¤§è±¡å’Œé“¶æ²³ã€‚
- en: This finding illustrates how GPT-4 seems to have â€œcracked the codeâ€ for generating
    what it considers to be diverse words. If this is the case, Iâ€™m not sure if I
    can confidently say that GPT-4 is being creative more so than it has reached a
    local optima for â€œhow to best play this particular game.â€
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€å‘ç°å±•ç¤ºäº†GPT-4ä¼¼ä¹å·²ç»â€œç ´è§£äº†ç”Ÿæˆå¤šæ ·åŒ–è¯æ±‡çš„å¯†ç â€ã€‚å¦‚æœæ˜¯è¿™æ ·ï¼Œæˆ‘ä¸ç¡®å®šæ˜¯å¦å¯ä»¥è‡ªä¿¡åœ°è¯´GPT-4çš„åˆ›é€ åŠ›æ›´é«˜ï¼Œè¿˜æ˜¯å®ƒå·²ç»æ‰¾åˆ°äº†ä¸€ä¸ªâ€œå¦‚ä½•æœ€å¥½åœ°ç©è¿™ä¸ªç‰¹å®šæ¸¸æˆâ€çš„å±€éƒ¨æœ€ä¼˜è§£ã€‚
- en: Conclusion
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: '*â€œWe can imagine a future in which GPT-3 and other generative LLMs responses
    cannot be distinguished from humans, although the creative process will be different.â€'
  id: totrans-174
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*â€œæˆ‘ä»¬å¯ä»¥è®¾æƒ³ä¸€ä¸ªæœªæ¥ï¼Œåœ¨è¿™ä¸ªæœªæ¥ä¸­ï¼ŒGPT-3å’Œå…¶ä»–ç”Ÿæˆå‹LLMsçš„å“åº”æ— æ³•ä¸äººç±»åŒºåˆ†ï¼Œå°½ç®¡åˆ›é€ åŠ›è¿‡ç¨‹å°†ä¼šä¸åŒã€‚â€*'
- en: â€” from* [*Putting GPT-3â€™s Creativity to the (Alternative Uses) Test*](https://arxiv.org/abs/2206.08932)
  id: totrans-175
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€” æ¥è‡ª* [*GPT-3 åˆ›é€ åŠ›çš„å¦ç±»ç”¨é€”æµ‹è¯•*](https://arxiv.org/abs/2206.08932)
- en: It is challenging to accurately measure something so complex and multifaceted
    as creativity using a limited set of text-based tests. Although tests like the
    [Visual Remote Associates Test](https://www.frontiersin.org/articles/10.3389/fpsyg.2020.00026/full)
    attempt to address this issue by incorporating a visual element, there remain
    inherent limitations to such assessments. Convergent tests (such as the Remote
    Associates Test), which have a â€œright answer,â€ may not always reward the most
    creative or correct solution. On the other hand, divergent tests (such as the
    Divergent Association Test), which do not have a specific right answer, rely on
    word embeddings that are imperfect and may contain biases.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æœ‰é™çš„æ–‡æœ¬åŸºç¡€æµ‹è¯•å‡†ç¡®è¡¡é‡å¦‚æ­¤å¤æ‚å¤šé¢çš„åˆ›é€ åŠ›æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚è™½ç„¶åƒ[è§†è§‰è¿œç¨‹è”æƒ³æµ‹è¯•](https://www.frontiersin.org/articles/10.3389/fpsyg.2020.00026/full)è¿™æ ·çš„æµ‹è¯•é€šè¿‡åŠ å…¥è§†è§‰å…ƒç´ æ¥å°è¯•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†è¿™äº›è¯„ä¼°ä»ç„¶å­˜åœ¨å›ºæœ‰çš„å±€é™æ€§ã€‚å…·æœ‰â€œæ­£ç¡®ç­”æ¡ˆâ€çš„èšåˆæµ‹è¯•ï¼ˆä¾‹å¦‚è¿œç¨‹è”æƒ³æµ‹è¯•ï¼‰å¯èƒ½ä¸ä¼šæ€»æ˜¯å¥–åŠ±æœ€å…·åˆ›é€ æ€§æˆ–æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚å¦ä¸€æ–¹é¢ï¼Œæ²¡æœ‰ç‰¹å®šæ­£ç¡®ç­”æ¡ˆçš„å‘æ•£æµ‹è¯•ï¼ˆä¾‹å¦‚å‘æ•£è”æƒ³æµ‹è¯•ï¼‰ä¾èµ–äºä¸å®Œç¾çš„è¯åµŒå…¥ï¼Œå¹¶ä¸”å¯èƒ½åŒ…å«åå·®ã€‚
- en: Furthermore, the performance of GPT models on these tests could be influenced
    by their exposure to the tasks during training, potentially leading to data overlap.
    For instance, GPT-4â€™s performance on the Divergence Association Test demonstrated
    a peculiar repetition of words, suggesting it may have had prior knowledge of
    the â€œrightâ€ answers. It is crucial to develop alternative testing methods, as
    there have been [rumors of GPT-4 being trained on test data](https://www.reddit.com/r/MachineLearning/comments/124eyso/n_openai_may_have_benchmarked_gpt4s_coding/),
    particularly in coding tasks.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼ŒGPTæ¨¡å‹åœ¨è¿™äº›æµ‹è¯•ä¸­çš„è¡¨ç°å¯èƒ½ä¼šå—åˆ°è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹ä»»åŠ¡çš„æš´éœ²å½±å“ï¼Œå¯èƒ½å¯¼è‡´æ•°æ®é‡å ã€‚ä¾‹å¦‚ï¼ŒGPT-4åœ¨å‘æ•£è”æƒ³æµ‹è¯•ä¸­çš„è¡¨ç°å±•ç¤ºäº†å•è¯çš„ç‰¹æ®Šé‡å¤ï¼Œè¡¨æ˜å®ƒå¯èƒ½æå‰äº†è§£äº†â€œæ­£ç¡®â€çš„ç­”æ¡ˆã€‚å¼€å‘æ›¿ä»£æµ‹è¯•æ–¹æ³•è‡³å…³é‡è¦ï¼Œå› ä¸ºæœ‰[å…³äºGPT-4æ¥å—æµ‹è¯•æ•°æ®è®­ç»ƒçš„ä¼ é—»](https://www.reddit.com/r/MachineLearning/comments/124eyso/n_openai_may_have_benchmarked_gpt4s_coding/)ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼–ç ä»»åŠ¡ä¸­ã€‚
- en: Despite these limitations, creativity tests can serve as valuable benchmarks
    for comparing and tracking the performance of large language models. Future research
    experiments could repeat those from this article in other languages to gauge the
    creativity of GPT models across linguistic domains. By doing so, we can gain a
    more comprehensive understanding of AI-generated content and further explore the
    capabilities and limitations of these advanced language models.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æœ‰è¿™äº›å±€é™æ€§ï¼Œåˆ›é€ åŠ›æµ‹è¯•ä»ç„¶å¯ä»¥ä½œä¸ºæ¯”è¾ƒå’Œè·Ÿè¸ªå¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°çš„æœ‰ä»·å€¼åŸºå‡†ã€‚æœªæ¥çš„ç ”ç©¶å®éªŒå¯ä»¥åœ¨å…¶ä»–è¯­è¨€ä¸­é‡å¤æœ¬æ–‡ä¸­çš„å®éªŒï¼Œä»¥è¯„ä¼°GPTæ¨¡å‹åœ¨è¯­è¨€é¢†åŸŸçš„åˆ›é€ åŠ›ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—å¯¹AIç”Ÿæˆå†…å®¹çš„æ›´å…¨é¢ç†è§£ï¼Œå¹¶è¿›ä¸€æ­¥æ¢ç´¢è¿™äº›å…ˆè¿›è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›å’Œå±€é™æ€§ã€‚
- en: '*Thank you for reading this article!*'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ„Ÿè°¢é˜…è¯»è¿™ç¯‡æ–‡ç« ï¼*'
- en: Appendix
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é™„å½•
- en: 'Remote Association Test: Comparison of GPT Modelsâ€™ Correctly Answered Questions'
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿œç¨‹è”æƒ³æµ‹è¯•ï¼šGPTæ¨¡å‹æ­£ç¡®å›ç­”çš„é—®é¢˜æ¯”è¾ƒ
- en: We can look at the breakdown of difficulty level for each question the model
    got correct. We can see that GPT3, GPT3.5, and GPT4 are similar in its ability
    to answer proportions of difficult questions.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥æŸ¥çœ‹æ¨¡å‹æ­£ç¡®å›ç­”çš„æ¯ä¸ªé—®é¢˜çš„éš¾åº¦çº§åˆ«åˆ†è§£ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°GPT3ã€GPT3.5å’ŒGPT4åœ¨å›ç­”éš¾é¢˜çš„èƒ½åŠ›ä¸Šç›¸ä¼¼ã€‚
- en: '![](../Images/a500fe60944887f82bd908a91ed6fbbe.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a500fe60944887f82bd908a91ed6fbbe.png)'
- en: Remote Associations GPT-2 got right and GPT-4 got wrong
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-2ç­”å¯¹çš„è¿œç¨‹è”æƒ³å’ŒGPT-4ç­”é”™çš„éƒ¨åˆ†
- en: '[PRE7]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: GPT-4â€™s answers to randomly-generated Remote Association Test questions
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-4å¯¹éšæœºç”Ÿæˆçš„è¿œç¨‹è”æƒ³æµ‹è¯•é—®é¢˜çš„å›ç­”
- en: Here are the outputs generated by GPT-4 for a few randomly generated Remote
    Association Test questions.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯GPT-4ä¸ºå‡ ä¸ªéšæœºç”Ÿæˆçš„è¿œç¨‹è”æƒ³æµ‹è¯•é—®é¢˜ç”Ÿæˆçš„è¾“å‡ºã€‚
- en: '**Rings**'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**æˆ’æŒ‡**'
- en: This one felt a little shoehorned. I could buy the part about rings being a
    â€œsymbol of complications, especially in relationshipsâ€ if youâ€™re feeling super
    jaded about your love life or something. But the last part feels forced
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ„Ÿè§‰æœ‰ç‚¹å‹‰å¼ºã€‚å¦‚æœä½ å¯¹ä½ çš„æ„Ÿæƒ…ç”Ÿæ´»æ„Ÿåˆ°éå¸¸åŒå€¦ï¼Œæˆ‘å¯ä»¥æ¥å—å…³äºæˆ’æŒ‡æ˜¯â€œå¤æ‚æ€§è±¡å¾ï¼Œå°¤å…¶æ˜¯åœ¨å…³ç³»ä¸­çš„â€é‚£éƒ¨åˆ†ã€‚ä½†æœ€åçš„éƒ¨åˆ†æ„Ÿè§‰æœ‰ç‚¹ç”Ÿç¡¬ã€‚
- en: '[PRE8]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Revolution.** [Monte Cassino](https://en.wikipedia.org/wiki/Monte_Cassino)
    is indeed a real place and it was indeed bombed during WWII.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**é©å‘½ã€‚** [è’™ç‰¹å¡è¥¿è¯º](https://en.wikipedia.org/wiki/Monte_Cassino)ç¡®å®æ˜¯ä¸€ä¸ªçœŸå®çš„åœ°æ–¹ï¼ŒäºŒæˆ˜æœŸé—´ç¡®å®é­åˆ°è½°ç‚¸ã€‚'
- en: '[PRE9]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Help.** An interesting way of connecting the city of â€œSouthamptonâ€ to â€œhelpâ€
    via help centersâ€¦'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¸®åŠ©ã€‚** é€šè¿‡å¸®åŠ©ä¸­å¿ƒå°†â€œå—å®‰æ™®é¡¿â€è¿™åº§åŸå¸‚ä¸â€œå¸®åŠ©â€è¿æ¥èµ·æ¥çš„æœ‰è¶£æ–¹å¼â€¦â€¦'
- en: '[PRE11]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Internet**. I suppose its argument is quite correct.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**äº’è”ç½‘ã€‚** æˆ‘è®¤ä¸ºå®ƒçš„è®ºç‚¹æ˜¯ç›¸å½“æ­£ç¡®çš„ã€‚'
- en: '[PRE12]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Language.** Perhaps a bit generic, but I guess it works.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¯­è¨€ã€‚** å¯èƒ½æœ‰ç‚¹é€šç”¨ï¼Œä½†æˆ‘æƒ³å®ƒè¿˜æ˜¯æœ‰æ•ˆçš„ã€‚'
- en: '[PRE13]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Health.** The health surveillance part is a bit dark, but I guess it works.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¥åº·ã€‚** å¥åº·ç›‘æµ‹éƒ¨åˆ†æœ‰ç‚¹é˜´æš—ï¼Œä½†æˆ‘æƒ³å®ƒè¿˜æ˜¯æœ‰æ•ˆçš„ã€‚'
- en: '[PRE14]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**ERIC!** This one was quite funny actually.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**ERICï¼** è¿™ä¸ªå…¶å®è¿˜æŒºæœ‰è¶£çš„ã€‚'
- en: '[PRE15]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: GPT-2 Outputs on the Divergent Association Test
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-2åœ¨å‘æ•£è”æƒ³æµ‹è¯•ä¸­çš„è¾“å‡º
- en: I excluded GPT-2 from the Divergent test analyses because its output was absolute
    gibberish. Even when I simplified the prompt as much as possible, its output was
    not very useful. I include a sample here because I actually thought they were
    quite funny.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ’é™¤äº†GPT-2åœ¨å‘æ•£æµ‹è¯•åˆ†æä¸­çš„ç»“æœï¼Œå› ä¸ºå®ƒçš„è¾“å‡ºå®Œå…¨æ˜¯èƒ¡è¨€ä¹±è¯­ã€‚å³ä½¿æˆ‘å°½å¯èƒ½ç®€åŒ–æç¤ºï¼Œå®ƒçš„è¾“å‡ºä¹Ÿä¸å¤ªæœ‰ç”¨ã€‚æˆ‘åœ¨è¿™é‡ŒåŒ…å«äº†ä¸€ä¸ªç¤ºä¾‹ï¼Œå› ä¸ºæˆ‘å®é™…ä¸Šè§‰å¾—å®ƒä»¬éå¸¸æœ‰è¶£ã€‚
- en: '[PRE16]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[[1](#footnote-anchor-1)] [Guilford, J. P.](https://en.wikipedia.org/wiki/J._P._Guilford)
    (1967). *The Nature of Human Intelligence*.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1](#footnote-anchor-1)] [å‰å°”ç¦å¾·, J. P.](https://en.wikipedia.org/wiki/J._P._Guilford)
    (1967). *äººç±»æ™ºæ…§çš„æœ¬è´¨*ã€‚'
- en: '[[2](#footnote-anchor-2)] Mednick, S. A., & Mednick, M.T. (1959,1962). Remote
    Associates Test, college and adult form.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[[2](#footnote-anchor-2)] Mednick, S. A., & Mednick, M.T. (1959,1962). è¿œç¨‹è”æƒ³æµ‹è¯•ï¼Œå¤§å­¦åŠæˆäººç‰ˆã€‚'
- en: '[[3](#footnote-anchor-3)] The 10,000 most common English words were determined
    by [n-gram](https://en.wikipedia.org/wiki/N-gram) [frequency analysis](https://en.wikipedia.org/wiki/Frequency_analysis)
    of the [Googleâ€™s Trillion Word Corpus](https://books.google.com/ngrams/info).
    I further filtered this list to remove stopwords (e.g. common words in English
    like â€œtheâ€, â€œandâ€, and â€œyouâ€). More information from this [repository](https://github.com/first20hours/google-10000-english).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[[4](#footnote-anchor-4)] I put this on a dating app one time and nobody thought
    it was funny, unfortunately.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[[5](#footnote-anchor-5)] Olson, Jay A.; Nahas, Johnny; Chmoulevitch, Denis;
    Cropper, Simon J.; Webb, Margaret E. (22 June 2021). [â€œNaming unrelated words
    predicts creativityâ€](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8237676). *Proceedings
    of the National Academy of Sciences*. **118** (25): e2022340118\. [doi](https://en.wikipedia.org/wiki/Doi_(identifier)):[10.1073/pnas.2022340118](https://doi.org/10.1073%2Fpnas.2022340118).
    [PMC](https://en.wikipedia.org/wiki/PMC_(identifier)) [8237676](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8237676).
    [PMID](https://en.wikipedia.org/wiki/PMID_(identifier)) [34140408](https://pubmed.ncbi.nlm.nih.gov/34140408)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[[6](#footnote-anchor-6)] The authors take a subset of 7 out of 10 words to
    calculate scores. I kept all 10 words. Also, the authors used the 840B-GloVe embeddings
    while I used the smaller 6B-GloVe embeddings. I compared my GloVe embedding results
    with the official DAT score and found the results to be comparable despite these
    differences.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
