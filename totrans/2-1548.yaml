- en: 'Multiple Linear Regression: A Deep Dive'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/multiple-linear-regression-a-deep-dive-f104c8ede236](https://towardsdatascience.com/multiple-linear-regression-a-deep-dive-f104c8ede236)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Multiple Linear Regression from Scratch: Deep Understanding'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://zubairhossain.medium.com/?source=post_page-----f104c8ede236--------------------------------)[![Md.
    Zubair](../Images/1b983a23226ce7561796fa5b28c00d65.png)](https://zubairhossain.medium.com/?source=post_page-----f104c8ede236--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f104c8ede236--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f104c8ede236--------------------------------)
    [Md. Zubair](https://zubairhossain.medium.com/?source=post_page-----f104c8ede236--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f104c8ede236--------------------------------)
    ·10 min read·Mar 3, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e2b58dacca35db939172a64442a9a1a.png)'
  prefs: []
  type: TYPE_IMG
- en: Multiple Regression with Two Features (x1 and x2) (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We, human beings, have been trying to create intelligent systems for a long
    ago. Because if we can automate a system, it will make our life easier and can
    be used as our assistant. Machine learning makes it happen. There are many machine
    learning algorithms for solving different problems. This article will introduce
    a machine learning algorithm ***which can solve the regression problem (prediction
    of continuous value) with multiple variables***. *Suppose you are running a real
    estate business.* As a business owner, you should have a proper idea about the
    price of buildings, lands, etc., to make your business profitable. It is quite
    difficult for a person to track the prices in a wide range of areas. An efficient
    machine learning regression model may serve you a lot. Just imagine you are entering
    inputs like location, size and other relevant information into a system, and it
    is automatically showing you the price. Multiple Linear Regression can exactly
    do the same. Isn’t it interesting!
  prefs: []
  type: TYPE_NORMAL
- en: I will explain the process of multiple linear regression and show you the implementation
    from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: '`*[N.B. — If you don’t have a clear concept about simple linear regression,
    I suggest you go through the* [***article***](/deep-understanding-of-simple-linear-regression-3776afe34473)*before
    diving into the multiple linear regression.]*`'
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`[**What is multiple linear regression?**](#d06b)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`[**When do we use multiple linear regression?**](#04ce)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`[**Multiple linear regression in detail**](#acd9)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`[**Vectorized Method of Linear Regression**](#9bb5)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`[**Hands-on implementation with python**](#89a6)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Multiple Linear Regression Problem?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In simple linear regression, only one feature (independent variable) can exist.
    But in multiple linear regression, there is more than one feature. Both of them
    predict continuous values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3f5c73a2144aae49dea8047541555edb.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple Linear Regression Problem (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: Look at the table. There is the price of a product against a weight. With linear
    regression, if we can fit a line for the given value, we can easily predict the
    price by inserting the weight of a product into the model. The same process is
    applicable for multiple linear regression. Instead of one feature, there will
    be multiple features (independent variables).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/44c3716ee6328a5196202ea213da7b01.png)'
  prefs: []
  type: TYPE_IMG
- en: Multiple Linear Regression Problem (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: The above example has two features `“**Age”**`and `“**Income”**`. We have to
    predict the monthly `“**Expenditure**”` for two new features. It’s an example
    of multiple linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple linear regression is not limited to only two features. It may have
    more than two features.
  prefs: []
  type: TYPE_NORMAL
- en: When do We Use Multiple Linear Regression?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Simple or univariate linear regression works only for predicting continuous
    values from one independent feature.
  prefs: []
  type: TYPE_NORMAL
- en: The process of simple linear regression doesn’t work for multiple features.
    We must apply multiple linear regression if we need to predict continuous values
    from more than one feature (variables). It is worth mentioning that the data must
    be linearly distributed. Non-linear data is not suitable for linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Linear Regression in Detail
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s try to visually represent the multiple linear regression. I have tried
    to keep the model simple with only two independent variables (features).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/620266b0960579cfcaee8f50a7699e47.png)'
  prefs: []
  type: TYPE_IMG
- en: Multiple Linear Regression with Two Features (x1 and x2) (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: '`*x1 and x2*`are the two features (independent variables). Suppose `x1=4` and
    `x2=5`. We will get the point `**A**` if we project these values on the `x1-x2`
    plane. In the multiple regression model, we need to create a regression plane
    from our dataset, as shown in the diagram. Now, drawing a vertical line on the
    regression plane will intersect the plane to a certain point. We will get the
    predicted value by drawing a horizontal line from the intersecting point to the
    y-axis. The predicted value is the intersected point of the `y-axis`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`*[N.B. — I have visualized the multiple linear regression with only two features
    for demonstration purposes because it is impossible to visualize more than two
    features. In the case of higher features, the processes are the same.]*`'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s Try to Dig Deeper
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In simple linear regression, we predict a dependent value based on an independent
    value. `*[Read the* [*previous article*](https://medium.com/towards-data-science/deep-understanding-of-simple-linear-regression-3776afe34473)
    *for a more detailed explanation of simple linear regression.]*`
  prefs: []
  type: TYPE_NORMAL
- en: For example, a simple linear regression equation, `***yi=mxi + c***`. Here,
    `‘m’` is the slope of the regression line, and `‘c’` is the `y-intercept` value.
  prefs: []
  type: TYPE_NORMAL
- en: '*In case of more than one independent variable, we need to extend our regression
    equation as follows.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4bac68e72d9f2c7d944ee0ba53a967b6.png)'
  prefs: []
  type: TYPE_IMG
- en: Where,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`y` indicates the dependent variable (predicted regression value).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`x1,x2, ……,xn` are the different independent variables.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`m1, m2, m3,…….,mn` symbolize the slope coefficients of different independent
    variables.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`m0` is the intersect point value of the `y-axis`.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now, we will get the independent features from the dataset. Our main challenge
    is to find out the coefficient values of the slopes `(m0,m2,……., mn )`.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider the multiple linear regression problem dataset shown in the[***first
    section***](#2148).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/33c3f961cffed45c2613cfd83cb8c307.png)'
  prefs: []
  type: TYPE_IMG
- en: It is easy to predict the ‘Expenditure ’of individuals if we have the optimum
    value of `*m0, m1 and m2*`. We can easily get the `‘Expenditure’` by putting the
    `Age` and `Income` value.
  prefs: []
  type: TYPE_NORMAL
- en: But there is no straightforward way to find the optimum value of the coefficients.
    To do so, we need to minimize the cost (loss) function with the help of Gradient
    Descent.
  prefs: []
  type: TYPE_NORMAL
- en: '***A Bit Detail about Gradient descent —***'
  prefs: []
  type: TYPE_NORMAL
- en: Before diving into the gradient descent, we should have a clear idea about the
    cost function. The cost function is nothing but an error function. It measures
    the accuracy of a predicted model. We will use the following error function as
    a cost function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e7d95965217b6dd764982ce6ef86857.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ***y̅i*** is the predicted value, and ***yi*** is the actual value.
  prefs: []
  type: TYPE_NORMAL
- en: '`**Gradient descent**` is an optimization algorithm. We use this algorithm
    to minimize the cost function by optimizing the coefficients of the regression
    equations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc1e55c8b4782ee0ac2c8976034e7d13.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient Descent (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: The red curve is the derivative of the cost function. To optimize the coefficient,
    we randomly assign a weight for the coefficient. Now, we will calculate the derivative
    of the cost function. We will consider the simple linear regression equation to
    make it simple.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s replace ***y̅i*** with `*(mxi+c)*`*. It implies the equation as follows
    —*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/7431df945e99e4b0fc07b394789295b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 2\. The partial derivative w.r.t `**m**`and `**c**`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7b5bdabf48b82b223db99a677585b67.png)![](../Images/ef07c5f7d2ce94ae22d48caf1f9cdcb0.png)'
  prefs: []
  type: TYPE_IMG
- en: '`*[N.B. — You may find some cost functions which are multiplied with 1/2n instead
    of 1/n. It is not a big deal. If you use 1/2n, the derivative will neutralize
    it, and the output will be 1/n instead of 2/n. In the implementation section,
    we also use 1/2n.]*`'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Now, we will update the value of **m** and **c** iteratively with the following
    equations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d4fe0d657fe31aa2bad86f0c62d5ff7.png)![](../Images/84c3589429cb49c7da46f8106c65c942.png)'
  prefs: []
  type: TYPE_IMG
- en: '***α*** is the learning rate that indicates how much we move in each step to
    minimize the cost function (shown in the figure). The iteration will be continued
    until the cost function is significantly minimized.'
  prefs: []
  type: TYPE_NORMAL
- en: '*For multiple linear regression, the whole process is the same. Let’s again
    consider the equation for multiple linear regression.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4bac68e72d9f2c7d944ee0ba53a967b6.png)'
  prefs: []
  type: TYPE_IMG
- en: We will get a common form if we calculate the derivative for the coefficients
    like the simple linear equation (shown above).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c3af208c64709badc4531691d36fddf.png)'
  prefs: []
  type: TYPE_IMG
- en: Where `***j***` takes the values `***1,2,…..,n,***`representing the features.
  prefs: []
  type: TYPE_NORMAL
- en: For `*m0*`, the derivative will be —
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e02828992c45150536d1f69082acf5c.png)'
  prefs: []
  type: TYPE_IMG
- en: We will update all the coefficients simultaneously with the following formula.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/70ce2d35fa61d6877662ad9a02ecf6c6.png)'
  prefs: []
  type: TYPE_IMG
- en: And for *m0,* we will use the equation below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/60dc4bd9b3060b64e58ae9cf0fd5fa0e.png)'
  prefs: []
  type: TYPE_IMG
- en: We will continuously update all the coefficients to fit the model and calculate
    the costs. If the cost is significantly low, we will stop updating the coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: '`But the process is computationally expensive and time-consuming. Vectorization
    makes it easy to implement.`'
  prefs: []
  type: TYPE_NORMAL
- en: Vectorized Method of Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s consider the multiple linear regression again.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8eeec80ff1a25027662cd5f910d6fd7b.png)'
  prefs: []
  type: TYPE_IMG
- en: We have added a constant `*xi0=1*` for the convenience of calculation. It doesn’t
    affect the previous equation. Let’s see the vectorized representation of the equation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba0e7895037b86c62f6c3af7aed138f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Vectorized Implementation of Linear Regression Equation (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Here, `yi=1…..z, **z**` is the number of total dataset instances. X holds all
    the feature values up to ***z*** instances.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b0837c72421b7604018b90357b88280f.png)'
  prefs: []
  type: TYPE_IMG
- en: In short, the vectorized equation is —
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a81c39e0ae405cca7eb49a71c26d011b.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, *the vectorized cost function’s* derivative will be as follows ([Details
    explanation](https://math.stackexchange.com/questions/2887916/cost-function-vectorized-implementation)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7b5134543338f7dd70b7e286e18d43b.png)'
  prefs: []
  type: TYPE_IMG
- en: It’s time to update the weights with the formula given below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1bed66b2a2526b16a623a3da870b4c5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Yeah! We have completed our theoretical process. It’s time to codify the whole
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Python Implementation from Scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s time to make our hands dirty with hands-on coding. I will show step-by-step
    guidelines.
  prefs: []
  type: TYPE_NORMAL
- en: '`*[N.B. — We use the* [*Boston House Price*](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)
    *dataset for demonstration purposes.* *It is registered under the public domain.
    Download from*[***here***](https://www.kaggle.com/code/prasadperera/the-boston-housing-dataset/data)***.]***`'
  prefs: []
  type: TYPE_NORMAL
- en: '***Importing libraries***'
  prefs: []
  type: TYPE_NORMAL
- en: '***Reading the dataset***'
  prefs: []
  type: TYPE_NORMAL
- en: We see there are no column names in the main dataset. In the next step, we will
    set the column names according to the documentation.
  prefs: []
  type: TYPE_NORMAL
- en: '***Setting Column Names***'
  prefs: []
  type: TYPE_NORMAL
- en: We have successfully added the columns to our Dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: Along the article, our main focus is to understand the internal process of multiple
    linear regression. So, we will be focused mainly on the implementation rather
    than the effectiveness of our project. To keep our model simple, we will consider
    the highly correlated features.
  prefs: []
  type: TYPE_NORMAL
- en: '***Let’s find the correlation with the target column*** `***‘MEDV’***`'
  prefs: []
  type: TYPE_NORMAL
- en: For our convenience, we have picked up three features, `**‘RM’, ‘DIS’, and ‘B’**,`
    with the target value `**‘MEDV’**`.
  prefs: []
  type: TYPE_NORMAL
- en: '***Normalize the features***'
  prefs: []
  type: TYPE_NORMAL
- en: Normalization reduces the calculational complexity of our model. So, we will
    normalize our features.
  prefs: []
  type: TYPE_NORMAL
- en: '***Splitting the dataset into test and train sets***'
  prefs: []
  type: TYPE_NORMAL
- en: We have to split the dataset into train and test sets for evaluation purposes.
    With the training set, we will train the model and evaluate our model with the
    test set.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have split 75% data for the training and kept 25% for testing.
  prefs: []
  type: TYPE_NORMAL
- en: '***Gradient descent optimization function with vectorized implementation***'
  prefs: []
  type: TYPE_NORMAL
- en: '***Let’s call the function to find out our optimum value of the coefficients.***'
  prefs: []
  type: TYPE_NORMAL
- en: The function returns two values, coefficients (w) and a list of loss values.
  prefs: []
  type: TYPE_NORMAL
- en: '***Visualization of the optimization over the iteration***'
  prefs: []
  type: TYPE_NORMAL
- en: '***Creating a prediction function for predicting new values***'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s predict the values for test features.
  prefs: []
  type: TYPE_NORMAL
- en: '***Comparing our scratch model with the standard scikit-learn library***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before jumping to the comparison, we have to create a prediction model for multiple
    linear regression with `**scikit-learn**`.
  prefs: []
  type: TYPE_NORMAL
- en: '***Linear regression with scikit-learn***'
  prefs: []
  type: TYPE_NORMAL
- en: '***Comparing the models in terms of MSE***'
  prefs: []
  type: TYPE_NORMAL
- en: It seems the two MSE values are very much similar. Even our model’s MSE is slightly
    less than the scikit-lean model.
  prefs: []
  type: TYPE_NORMAL
- en: '***How much do the two models’ predictions differ?***'
  prefs: []
  type: TYPE_NORMAL
- en: Results show that our optimization is perfect, and it works similarly to the
    benchmark model of scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We mainly use multiple linear regression for solving the prediction problem
    of continuous value from multiple features. We can implement linear regression
    with many benchmark libraries and tools. But we can’t understand how the algorithm
    works internally. With this article, I have tried to explain the process of multiple
    linear regression from the very core. I believe it will deliver you a crystal-clear
    idea about the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '`*You may download the full notebook from* [***here***](https://github.com/Zubair063/ML_articles/blob/main/Multiple%20Linear%20Regression/Multiple%20Linear%20Regression%20from%20scratch%20.ipynb)*.*`'
  prefs: []
  type: TYPE_NORMAL
- en: '*[The* [***article***](https://medium.com/analytics-vidhya/multiple-linear-regression-from-scratch-using-python-db9368859f)
    *guide me to represent the idea of multiple linear regression.]*'
  prefs: []
  type: TYPE_NORMAL
- en: '`***Some other articles about the scratch implementation of ML algorithms are
    in the pipeline.***`'
  prefs: []
  type: TYPE_NORMAL
- en: '*Previous articles on* ***Machine Learning from Scratch Series*** *are as follows.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Previous articles of* `***Machine Learning from Scratch Series***`*are as
    follows.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/deep-understanding-of-simple-linear-regression-3776afe34473?source=post_page-----f104c8ede236--------------------------------)
    [## Deep Understanding of Simple Linear Regression'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear Regression from Scratch: Detailed Explanation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/deep-understanding-of-simple-linear-regression-3776afe34473?source=post_page-----f104c8ede236--------------------------------)
    [](/unsupervised-learning-and-k-means-clustering-from-scratch-f4e5e9947c39?source=post_page-----f104c8ede236--------------------------------)
    [## K-means Clustering from Scratch
  prefs: []
  type: TYPE_NORMAL
- en: 'K-means: The Best ML Algorithm to Cluster Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/unsupervised-learning-and-k-means-clustering-from-scratch-f4e5e9947c39?source=post_page-----f104c8ede236--------------------------------)
    [](/knn-algorithm-from-scratch-37febe0c15b3?source=post_page-----f104c8ede236--------------------------------)
    [## KNN Algorithm from Scratch
  prefs: []
  type: TYPE_NORMAL
- en: Implementation and Details Explanation of the KNN Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/knn-algorithm-from-scratch-37febe0c15b3?source=post_page-----f104c8ede236--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '`**If you are a beginner don’t miss out to read the following series.**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/ultimate-guide-to-statistics-for-data-science-a3d8f1fd69a7?source=post_page-----f104c8ede236--------------------------------)
    [## Ultimate Guide to Statistics for Data Science'
  prefs: []
  type: TYPE_NORMAL
- en: 'Statistics at a glance for data science: standard guidelines'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/ultimate-guide-to-statistics-for-data-science-a3d8f1fd69a7?source=post_page-----f104c8ede236--------------------------------)
  prefs: []
  type: TYPE_NORMAL
