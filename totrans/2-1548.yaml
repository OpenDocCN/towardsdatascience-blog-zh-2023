- en: 'Multiple Linear Regression: A Deep Dive'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多元线性回归：深入探讨
- en: 原文：[https://towardsdatascience.com/multiple-linear-regression-a-deep-dive-f104c8ede236](https://towardsdatascience.com/multiple-linear-regression-a-deep-dive-f104c8ede236)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/multiple-linear-regression-a-deep-dive-f104c8ede236](https://towardsdatascience.com/multiple-linear-regression-a-deep-dive-f104c8ede236)
- en: 'Multiple Linear Regression from Scratch: Deep Understanding'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从零开始的多元线性回归：深入理解
- en: '[](https://zubairhossain.medium.com/?source=post_page-----f104c8ede236--------------------------------)[![Md.
    Zubair](../Images/1b983a23226ce7561796fa5b28c00d65.png)](https://zubairhossain.medium.com/?source=post_page-----f104c8ede236--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f104c8ede236--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f104c8ede236--------------------------------)
    [Md. Zubair](https://zubairhossain.medium.com/?source=post_page-----f104c8ede236--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://zubairhossain.medium.com/?source=post_page-----f104c8ede236--------------------------------)[![Md.
    Zubair](../Images/1b983a23226ce7561796fa5b28c00d65.png)](https://zubairhossain.medium.com/?source=post_page-----f104c8ede236--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f104c8ede236--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f104c8ede236--------------------------------)
    [Md. Zubair](https://zubairhossain.medium.com/?source=post_page-----f104c8ede236--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f104c8ede236--------------------------------)
    ·10 min read·Mar 3, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f104c8ede236--------------------------------)
    ·10 分钟阅读·2023年3月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7e2b58dacca35db939172a64442a9a1a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e2b58dacca35db939172a64442a9a1a.png)'
- en: Multiple Regression with Two Features (x1 and x2) (Image By Author)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 两个特征（x1 和 x2）的多元回归（作者图片）
- en: Motivation
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动机
- en: We, human beings, have been trying to create intelligent systems for a long
    ago. Because if we can automate a system, it will make our life easier and can
    be used as our assistant. Machine learning makes it happen. There are many machine
    learning algorithms for solving different problems. This article will introduce
    a machine learning algorithm ***which can solve the regression problem (prediction
    of continuous value) with multiple variables***. *Suppose you are running a real
    estate business.* As a business owner, you should have a proper idea about the
    price of buildings, lands, etc., to make your business profitable. It is quite
    difficult for a person to track the prices in a wide range of areas. An efficient
    machine learning regression model may serve you a lot. Just imagine you are entering
    inputs like location, size and other relevant information into a system, and it
    is automatically showing you the price. Multiple Linear Regression can exactly
    do the same. Isn’t it interesting!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们人类已经尝试了很长时间去创建智能系统。因为如果我们能自动化一个系统，它将使我们的生活更轻松，并且可以作为我们的助手。机器学习使这一切成为可能。解决不同问题的机器学习算法有很多。本文将介绍一种可以用多个变量解决回归问题（连续值预测）的机器学习算法。*假设你在经营房地产生意。*
    作为业务负责人，你应该对建筑物、土地等的价格有一个合理的了解，以便使你的业务盈利。对于一个人来说，跟踪广泛区域的价格是相当困难的。一个高效的机器学习回归模型可以大有帮助。想象一下你输入的位置、大小和其他相关信息，系统会自动显示价格。多元线性回归正是可以做到这一点。难道不很有趣吗！
- en: I will explain the process of multiple linear regression and show you the implementation
    from scratch.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我将解释多元线性回归的过程，并展示从零开始的实现。
- en: '`*[N.B. — If you don’t have a clear concept about simple linear regression,
    I suggest you go through the* [***article***](/deep-understanding-of-simple-linear-regression-3776afe34473)*before
    diving into the multiple linear regression.]*`'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '`*[注意 — 如果你对简单线性回归没有清晰的概念，我建议你先阅读* [***这篇文章***](/deep-understanding-of-simple-linear-regression-3776afe34473)*再深入了解多元线性回归。]*`'
- en: Table of Contents
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录
- en: '`[**What is multiple linear regression?**](#d06b)`'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`[**什么是多元线性回归？**](#d06b)`'
- en: '`[**When do we use multiple linear regression?**](#04ce)`'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`[**我们什么时候使用多元线性回归？**](#04ce)`'
- en: '`[**Multiple linear regression in detail**](#acd9)`'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`[**多元线性回归的详细介绍**](#acd9)`'
- en: '`[**Vectorized Method of Linear Regression**](#9bb5)`'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`[**线性回归的向量化方法**](#9bb5)`'
- en: '`[**Hands-on implementation with python**](#89a6)`'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`[**用 Python 的动手实现**](#89a6)`'
- en: What is Multiple Linear Regression Problem?
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是多元线性回归问题？
- en: In simple linear regression, only one feature (independent variable) can exist.
    But in multiple linear regression, there is more than one feature. Both of them
    predict continuous values.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单线性回归中，只能存在一个特征（自变量）。但在多重线性回归中，特征的数量多于一个。它们都用于预测连续值。
- en: '![](../Images/3f5c73a2144aae49dea8047541555edb.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f5c73a2144aae49dea8047541555edb.png)'
- en: Simple Linear Regression Problem (Image By Author)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 简单线性回归问题（作者提供的图像）
- en: Look at the table. There is the price of a product against a weight. With linear
    regression, if we can fit a line for the given value, we can easily predict the
    price by inserting the weight of a product into the model. The same process is
    applicable for multiple linear regression. Instead of one feature, there will
    be multiple features (independent variables).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 看看表格。表中是产品价格与重量的关系。通过线性回归，如果我们可以为给定的值拟合一条线，我们可以通过将产品的重量插入模型来轻松预测价格。相同的过程也适用于多重线性回归。不同的是，特征不止一个，而是多个（自变量）。
- en: '![](../Images/44c3716ee6328a5196202ea213da7b01.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44c3716ee6328a5196202ea213da7b01.png)'
- en: Multiple Linear Regression Problem (Image by Author)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 多重线性回归问题（作者提供的图像）
- en: The above example has two features `“**Age”**`and `“**Income”**`. We have to
    predict the monthly `“**Expenditure**”` for two new features. It’s an example
    of multiple linear regression.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例有两个特征 `“**年龄**”` 和 `“**收入**”`。我们需要预测这两个新特征的每月 `“**支出**”`。这就是多重线性回归的一个示例。
- en: Multiple linear regression is not limited to only two features. It may have
    more than two features.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 多重线性回归不限于两个特征。它可以有两个以上的特征。
- en: When do We Use Multiple Linear Regression?
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时使用多重线性回归？
- en: Simple or univariate linear regression works only for predicting continuous
    values from one independent feature.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 简单或单变量线性回归仅适用于从一个自变量预测连续值。
- en: The process of simple linear regression doesn’t work for multiple features.
    We must apply multiple linear regression if we need to predict continuous values
    from more than one feature (variables). It is worth mentioning that the data must
    be linearly distributed. Non-linear data is not suitable for linear regression.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 简单线性回归的过程不适用于多个特征。如果我们需要从多个特征（变量）中预测连续值，我们必须应用多重线性回归。值得一提的是，数据必须是线性分布的。非线性数据不适合线性回归。
- en: Multiple Linear Regression in Detail
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多重线性回归的详细信息
- en: Let’s try to visually represent the multiple linear regression. I have tried
    to keep the model simple with only two independent variables (features).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试以可视化的方式表示多重线性回归。我试图保持模型简单，只包含两个自变量（特征）。
- en: '![](../Images/620266b0960579cfcaee8f50a7699e47.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/620266b0960579cfcaee8f50a7699e47.png)'
- en: Multiple Linear Regression with Two Features (x1 and x2) (Image By Author)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 两个特征（x1 和 x2）的多重线性回归（作者提供的图像）
- en: '`*x1 and x2*`are the two features (independent variables). Suppose `x1=4` and
    `x2=5`. We will get the point `**A**` if we project these values on the `x1-x2`
    plane. In the multiple regression model, we need to create a regression plane
    from our dataset, as shown in the diagram. Now, drawing a vertical line on the
    regression plane will intersect the plane to a certain point. We will get the
    predicted value by drawing a horizontal line from the intersecting point to the
    y-axis. The predicted value is the intersected point of the `y-axis`.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`*x1 和 x2*` 是两个特征（自变量）。假设 `x1=4` 和 `x2=5`。如果将这些值投影到 `x1-x2` 平面上，我们将得到点 `**A**`。在多重回归模型中，我们需要从数据集中创建一个回归平面，如图所示。现在，在回归平面上画一条垂直线会将平面划分为一个特定的点。通过从交点向
    y 轴画一条水平线，我们可以得到预测值。预测值是 `y 轴` 的交点。'
- en: '`*[N.B. — I have visualized the multiple linear regression with only two features
    for demonstration purposes because it is impossible to visualize more than two
    features. In the case of higher features, the processes are the same.]*`'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`*[注 — 我仅使用两个特征可视化多重线性回归以作演示，因为可视化超过两个特征是不可能的。在特征更多的情况下，过程是相同的。]*`'
- en: Let’s Try to Dig Deeper
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们尝试更深入地探讨
- en: In simple linear regression, we predict a dependent value based on an independent
    value. `*[Read the* [*previous article*](https://medium.com/towards-data-science/deep-understanding-of-simple-linear-regression-3776afe34473)
    *for a more detailed explanation of simple linear regression.]*`
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单线性回归中，我们根据一个自变量预测一个因变量。 `*[阅读* [*上一篇文章*](https://medium.com/towards-data-science/deep-understanding-of-simple-linear-regression-3776afe34473)
    *以获取关于简单线性回归的更详细解释。]*`
- en: For example, a simple linear regression equation, `***yi=mxi + c***`. Here,
    `‘m’` is the slope of the regression line, and `‘c’` is the `y-intercept` value.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个简单的线性回归方程，`***yi=mxi + c***`。这里，`‘m’`是回归线的斜率，而`‘c’`是`y`轴截距值。
- en: '*In case of more than one independent variable, we need to extend our regression
    equation as follows.*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*在有多个自变量的情况下，我们需要扩展回归方程，如下所示。*'
- en: '![](../Images/4bac68e72d9f2c7d944ee0ba53a967b6.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bac68e72d9f2c7d944ee0ba53a967b6.png)'
- en: Where,
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 其中，
- en: ''
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`y` indicates the dependent variable (predicted regression value).'
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`y`表示因变量（预测回归值）。'
- en: ''
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`x1,x2, ……,xn` are the different independent variables.'
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`x1,x2, ……,xn`是不同的自变量。'
- en: ''
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`m1, m2, m3,…….,mn` symbolize the slope coefficients of different independent
    variables.'
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`m1, m2, m3,…….,mn`象征着不同自变量的斜率系数。'
- en: ''
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`m0` is the intersect point value of the `y-axis`.'
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`m0`是`y`轴的截距点值。'
- en: Now, we will get the independent features from the dataset. Our main challenge
    is to find out the coefficient values of the slopes `(m0,m2,……., mn )`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将从数据集中获取自变量。我们面临的主要挑战是找出斜率的系数值`(m0,m2,……., mn )`。
- en: Let’s consider the multiple linear regression problem dataset shown in the[***first
    section***](#2148).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑[***第一部分***](#2148)中显示的多元线性回归问题数据集。
- en: '![](../Images/33c3f961cffed45c2613cfd83cb8c307.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33c3f961cffed45c2613cfd83cb8c307.png)'
- en: It is easy to predict the ‘Expenditure ’of individuals if we have the optimum
    value of `*m0, m1 and m2*`. We can easily get the `‘Expenditure’` by putting the
    `Age` and `Income` value.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有`*m0, m1 和 m2*`的最佳值，预测个人的`‘支出’`就很容易了。我们可以通过输入`年龄`和`收入`值来轻松获得`‘支出’`。
- en: But there is no straightforward way to find the optimum value of the coefficients.
    To do so, we need to minimize the cost (loss) function with the help of Gradient
    Descent.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 但没有直接的方法来找到系数的最佳值。为此，我们需要通过梯度下降来最小化代价（损失）函数。
- en: '***A Bit Detail about Gradient descent —***'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '***关于梯度下降的详细信息 —***'
- en: Before diving into the gradient descent, we should have a clear idea about the
    cost function. The cost function is nothing but an error function. It measures
    the accuracy of a predicted model. We will use the following error function as
    a cost function.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入梯度下降之前，我们应该对代价函数有一个清晰的了解。代价函数就是一个误差函数。它衡量预测模型的准确性。我们将使用以下误差函数作为代价函数。
- en: '![](../Images/0e7d95965217b6dd764982ce6ef86857.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e7d95965217b6dd764982ce6ef86857.png)'
- en: Here, ***y̅i*** is the predicted value, and ***yi*** is the actual value.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，***y̅i***是预测值，***yi***是实际值。
- en: '`**Gradient descent**` is an optimization algorithm. We use this algorithm
    to minimize the cost function by optimizing the coefficients of the regression
    equations.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`**梯度下降**`是一个优化算法。我们使用这个算法通过优化回归方程的系数来最小化代价函数。'
- en: '![](../Images/fc1e55c8b4782ee0ac2c8976034e7d13.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc1e55c8b4782ee0ac2c8976034e7d13.png)'
- en: Gradient Descent (Image By Author)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降（图片作者提供）
- en: The red curve is the derivative of the cost function. To optimize the coefficient,
    we randomly assign a weight for the coefficient. Now, we will calculate the derivative
    of the cost function. We will consider the simple linear regression equation to
    make it simple.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 红色曲线是代价函数的导数。为了优化系数，我们随机分配系数的权重。现在，我们将计算代价函数的导数。为了简化起见，我们将考虑简单的线性回归方程。
- en: Let’s replace ***y̅i*** with `*(mxi+c)*`*. It implies the equation as follows
    —*
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们用`*(mxi+c)*`替换`***y̅i***`。这意味着方程如下 —*
- en: '![](../Images/7431df945e99e4b0fc07b394789295b2.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7431df945e99e4b0fc07b394789295b2.png)'
- en: 2\. The partial derivative w.r.t `**m**`and `**c**`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 对`**m**`和`**c**`的偏导数。
- en: '![](../Images/c7b5bdabf48b82b223db99a677585b67.png)![](../Images/ef07c5f7d2ce94ae22d48caf1f9cdcb0.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7b5bdabf48b82b223db99a677585b67.png)![](../Images/ef07c5f7d2ce94ae22d48caf1f9cdcb0.png)'
- en: '`*[N.B. — You may find some cost functions which are multiplied with 1/2n instead
    of 1/n. It is not a big deal. If you use 1/2n, the derivative will neutralize
    it, and the output will be 1/n instead of 2/n. In the implementation section,
    we also use 1/2n.]*`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`*[注意 — 你可能会发现一些代价函数是乘以1/2n而不是1/n。这没什么大不了的。如果你使用1/2n，导数会中和它，结果将是1/n而不是2/n。在实现部分，我们也使用1/2n。]*`'
- en: 3\. Now, we will update the value of **m** and **c** iteratively with the following
    equations.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 现在，我们将通过以下方程迭代更新**m**和**c**的值。
- en: '![](../Images/3d4fe0d657fe31aa2bad86f0c62d5ff7.png)![](../Images/84c3589429cb49c7da46f8106c65c942.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d4fe0d657fe31aa2bad86f0c62d5ff7.png)![](../Images/84c3589429cb49c7da46f8106c65c942.png)'
- en: '***α*** is the learning rate that indicates how much we move in each step to
    minimize the cost function (shown in the figure). The iteration will be continued
    until the cost function is significantly minimized.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '***α*** 是学习率，指示我们在每一步中移动的距离以最小化成本函数（如图所示）。迭代将继续进行，直到成本函数显著最小化。'
- en: '*For multiple linear regression, the whole process is the same. Let’s again
    consider the equation for multiple linear regression.*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*对于多重线性回归，整个过程是相同的。让我们再次考虑多重线性回归的方程。*'
- en: '![](../Images/4bac68e72d9f2c7d944ee0ba53a967b6.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bac68e72d9f2c7d944ee0ba53a967b6.png)'
- en: We will get a common form if we calculate the derivative for the coefficients
    like the simple linear equation (shown above).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们像简单线性方程一样计算系数的导数，我们将得到一种通用形式（如上所示）。
- en: '![](../Images/0c3af208c64709badc4531691d36fddf.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c3af208c64709badc4531691d36fddf.png)'
- en: Where `***j***` takes the values `***1,2,…..,n,***`representing the features.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `***j***` 取值为 `***1,2,…..,n,***` 代表特征。
- en: For `*m0*`, the derivative will be —
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `*m0*`，导数将是 —
- en: '![](../Images/0e02828992c45150536d1f69082acf5c.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e02828992c45150536d1f69082acf5c.png)'
- en: We will update all the coefficients simultaneously with the following formula.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下公式同时更新所有系数。
- en: '![](../Images/70ce2d35fa61d6877662ad9a02ecf6c6.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70ce2d35fa61d6877662ad9a02ecf6c6.png)'
- en: And for *m0,* we will use the equation below.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *m0,* 我们将使用以下方程。
- en: '![](../Images/60dc4bd9b3060b64e58ae9cf0fd5fa0e.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60dc4bd9b3060b64e58ae9cf0fd5fa0e.png)'
- en: We will continuously update all the coefficients to fit the model and calculate
    the costs. If the cost is significantly low, we will stop updating the coefficients.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不断更新所有系数以拟合模型并计算成本。如果成本显著降低，我们将停止更新系数。
- en: '`But the process is computationally expensive and time-consuming. Vectorization
    makes it easy to implement.`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`但这个过程在计算上是昂贵且耗时的。向量化使实现变得简单。`'
- en: Vectorized Method of Linear Regression
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量化线性回归方法
- en: Let’s consider the multiple linear regression again.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次考虑多重线性回归。
- en: '![](../Images/8eeec80ff1a25027662cd5f910d6fd7b.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8eeec80ff1a25027662cd5f910d6fd7b.png)'
- en: We have added a constant `*xi0=1*` for the convenience of calculation. It doesn’t
    affect the previous equation. Let’s see the vectorized representation of the equation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加了一个常数 `*xi0=1*` 以方便计算。它不会影响前面的方程。让我们看看方程的向量化表示。
- en: '![](../Images/ba0e7895037b86c62f6c3af7aed138f3.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba0e7895037b86c62f6c3af7aed138f3.png)'
- en: Vectorized Implementation of Linear Regression Equation (Image by Author)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归方程的向量化实现（图像由作者提供）
- en: Here, `yi=1…..z, **z**` is the number of total dataset instances. X holds all
    the feature values up to ***z*** instances.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`yi=1…..z, **z**` 是总数据集实例的数量。X 存储了所有特征值，直到 ***z*** 实例。
- en: '![](../Images/b0837c72421b7604018b90357b88280f.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0837c72421b7604018b90357b88280f.png)'
- en: In short, the vectorized equation is —
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，向量化方程是 —
- en: '![](../Images/a81c39e0ae405cca7eb49a71c26d011b.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a81c39e0ae405cca7eb49a71c26d011b.png)'
- en: Now, *the vectorized cost function’s* derivative will be as follows ([Details
    explanation](https://math.stackexchange.com/questions/2887916/cost-function-vectorized-implementation)).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，*向量化成本函数的* 导数如下（[详细解释](https://math.stackexchange.com/questions/2887916/cost-function-vectorized-implementation)）。
- en: '![](../Images/f7b5134543338f7dd70b7e286e18d43b.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7b5134543338f7dd70b7e286e18d43b.png)'
- en: It’s time to update the weights with the formula given below.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候使用下面给出的公式更新权重了。
- en: '![](../Images/1bed66b2a2526b16a623a3da870b4c5d.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bed66b2a2526b16a623a3da870b4c5d.png)'
- en: Yeah! We have completed our theoretical process. It’s time to codify the whole
    process.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 是的！我们已经完成了理论过程。现在是时候将整个过程编写成代码了。
- en: Python Implementation from Scratch
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从头开始的 Python 实现
- en: It’s time to make our hands dirty with hands-on coding. I will show step-by-step
    guidelines.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候开始动手编写代码了。我将逐步展示指南。
- en: '`*[N.B. — We use the* [*Boston House Price*](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)
    *dataset for demonstration purposes.* *It is registered under the public domain.
    Download from*[***here***](https://www.kaggle.com/code/prasadperera/the-boston-housing-dataset/data)***.]***`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`*[附注 — 我们使用* [*波士顿房价*](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)
    *数据集进行演示。* *它注册在公共领域。可从*[***这里***](https://www.kaggle.com/code/prasadperera/the-boston-housing-dataset/data)***]***`'
- en: '***Importing libraries***'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '***导入库***'
- en: '***Reading the dataset***'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '***读取数据集***'
- en: We see there are no column names in the main dataset. In the next step, we will
    set the column names according to the documentation.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到主数据集中没有列名。在下一步中，我们将根据文档设置列名。
- en: '***Setting Column Names***'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '***设置列名***'
- en: We have successfully added the columns to our Dataframe.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功地将列添加到 Dataframe 中。
- en: Along the article, our main focus is to understand the internal process of multiple
    linear regression. So, we will be focused mainly on the implementation rather
    than the effectiveness of our project. To keep our model simple, we will consider
    the highly correlated features.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们的主要重点是理解多元线性回归的内部过程。因此，我们将主要关注实现过程，而不是项目的有效性。为了保持模型的简单性，我们将考虑高度相关的特征。
- en: '***Let’s find the correlation with the target column*** `***‘MEDV’***`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '***让我们找到与目标列的相关性*** `***‘MEDV’***`'
- en: For our convenience, we have picked up three features, `**‘RM’, ‘DIS’, and ‘B’**,`
    with the target value `**‘MEDV’**`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们选择了三个特征，`**‘RM’, ‘DIS’, 和 ‘B’**`，以及目标值 `**‘MEDV’**`。
- en: '***Normalize the features***'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '***规范化特征***'
- en: Normalization reduces the calculational complexity of our model. So, we will
    normalize our features.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化减少了模型的计算复杂度。因此，我们将对特征进行归一化。
- en: '***Splitting the dataset into test and train sets***'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '***将数据集拆分为测试集和训练集***'
- en: We have to split the dataset into train and test sets for evaluation purposes.
    With the training set, we will train the model and evaluate our model with the
    test set.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须将数据集拆分为训练集和测试集，以便进行评估。使用训练集，我们将训练模型，并用测试集评估我们的模型。
- en: Here, we have split 75% data for the training and kept 25% for testing.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将 75% 的数据用于训练，其余 25% 用于测试。
- en: '***Gradient descent optimization function with vectorized implementation***'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '***带有向量化实现的梯度下降优化函数***'
- en: '***Let’s call the function to find out our optimum value of the coefficients.***'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '***让我们调用函数找出系数的最佳值。***'
- en: The function returns two values, coefficients (w) and a list of loss values.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数返回两个值，系数（w）和一个损失值列表。
- en: '***Visualization of the optimization over the iteration***'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '***优化过程的迭代可视化***'
- en: '***Creating a prediction function for predicting new values***'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '***创建预测函数以预测新值***'
- en: Let’s predict the values for test features.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对测试特征进行预测。
- en: '***Comparing our scratch model with the standard scikit-learn library***'
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***将我们的自定义模型与标准 scikit-learn 库进行比较***'
- en: Before jumping to the comparison, we have to create a prediction model for multiple
    linear regression with `**scikit-learn**`.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行比较之前，我们需要创建一个用于多元线性回归的预测模型，使用 `**scikit-learn**`。
- en: '***Linear regression with scikit-learn***'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '***使用 scikit-learn 的线性回归***'
- en: '***Comparing the models in terms of MSE***'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '***从 MSE 的角度比较模型***'
- en: It seems the two MSE values are very much similar. Even our model’s MSE is slightly
    less than the scikit-lean model.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 两个 MSE 值非常相似。即使我们的模型的 MSE 稍低于 scikit-learn 模型。
- en: '***How much do the two models’ predictions differ?***'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '***这两个模型的预测差异有多大？***'
- en: Results show that our optimization is perfect, and it works similarly to the
    benchmark model of scikit-learn.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明我们的优化非常完美，并且与 scikit-learn 的基准模型效果类似。
- en: Conclusion
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: We mainly use multiple linear regression for solving the prediction problem
    of continuous value from multiple features. We can implement linear regression
    with many benchmark libraries and tools. But we can’t understand how the algorithm
    works internally. With this article, I have tried to explain the process of multiple
    linear regression from the very core. I believe it will deliver you a crystal-clear
    idea about the algorithm.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要使用多元线性回归来解决从多个特征预测连续值的问题。我们可以使用许多基准库和工具来实现线性回归。但我们无法了解算法的内部工作原理。通过这篇文章，我尝试从最基本的方面解释多元线性回归的过程。我相信这将为您提供对算法的清晰理解。
- en: '`*You may download the full notebook from* [***here***](https://github.com/Zubair063/ML_articles/blob/main/Multiple%20Linear%20Regression/Multiple%20Linear%20Regression%20from%20scratch%20.ipynb)*.*`'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`*您可以从* [***这里***](https://github.com/Zubair063/ML_articles/blob/main/Multiple%20Linear%20Regression/Multiple%20Linear%20Regression%20from%20scratch%20.ipynb)
    *下载完整的笔记本。*`'
- en: '*[The* [***article***](https://medium.com/analytics-vidhya/multiple-linear-regression-from-scratch-using-python-db9368859f)
    *guide me to represent the idea of multiple linear regression.]*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*[这篇* [***文章***](https://medium.com/analytics-vidhya/multiple-linear-regression-from-scratch-using-python-db9368859f)
    *指导我如何展示多元线性回归的概念。]*'
- en: '`***Some other articles about the scratch implementation of ML algorithms are
    in the pipeline.***`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`***关于 ML 算法从头实现的其他文章正在筹备中。***`'
- en: '*Previous articles on* ***Machine Learning from Scratch Series*** *are as follows.*'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*之前关于* ***从头开始学习机器学习系列*** *的文章如下。*'
- en: '*Previous articles of* `***Machine Learning from Scratch Series***`*are as
    follows.*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*《从头开始的机器学习系列》*`***Machine Learning from Scratch Series***`*的先前文章如下。*'
- en: '[](/deep-understanding-of-simple-linear-regression-3776afe34473?source=post_page-----f104c8ede236--------------------------------)
    [## Deep Understanding of Simple Linear Regression'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/deep-understanding-of-simple-linear-regression-3776afe34473?source=post_page-----f104c8ede236--------------------------------)
    [## 深入理解简单线性回归'
- en: 'Linear Regression from Scratch: Detailed Explanation'
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从零开始的线性回归：详细解释
- en: towardsdatascience.com](/deep-understanding-of-simple-linear-regression-3776afe34473?source=post_page-----f104c8ede236--------------------------------)
    [](/unsupervised-learning-and-k-means-clustering-from-scratch-f4e5e9947c39?source=post_page-----f104c8ede236--------------------------------)
    [## K-means Clustering from Scratch
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/deep-understanding-of-simple-linear-regression-3776afe34473?source=post_page-----f104c8ede236--------------------------------)
    [](/unsupervised-learning-and-k-means-clustering-from-scratch-f4e5e9947c39?source=post_page-----f104c8ede236--------------------------------)
    [## 从零开始的 K-means 聚类
- en: 'K-means: The Best ML Algorithm to Cluster Data'
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K-means：用于数据聚类的最佳 ML 算法
- en: towardsdatascience.com](/unsupervised-learning-and-k-means-clustering-from-scratch-f4e5e9947c39?source=post_page-----f104c8ede236--------------------------------)
    [](/knn-algorithm-from-scratch-37febe0c15b3?source=post_page-----f104c8ede236--------------------------------)
    [## KNN Algorithm from Scratch
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/unsupervised-learning-and-k-means-clustering-from-scratch-f4e5e9947c39?source=post_page-----f104c8ede236--------------------------------)
    [](/knn-algorithm-from-scratch-37febe0c15b3?source=post_page-----f104c8ede236--------------------------------)
    [## 从零开始的 KNN 算法
- en: Implementation and Details Explanation of the KNN Algorithm
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: KNN 算法的实现和细节解释
- en: towardsdatascience.com](/knn-algorithm-from-scratch-37febe0c15b3?source=post_page-----f104c8ede236--------------------------------)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/knn-algorithm-from-scratch-37febe0c15b3?source=post_page-----f104c8ede236--------------------------------)
- en: '`**If you are a beginner don’t miss out to read the following series.**`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`**如果你是初学者，不要错过阅读以下系列。**`'
- en: '[](/ultimate-guide-to-statistics-for-data-science-a3d8f1fd69a7?source=post_page-----f104c8ede236--------------------------------)
    [## Ultimate Guide to Statistics for Data Science'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/ultimate-guide-to-statistics-for-data-science-a3d8f1fd69a7?source=post_page-----f104c8ede236--------------------------------)
    [## 数据科学的统计学终极指南'
- en: 'Statistics at a glance for data science: standard guidelines'
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据科学中的统计学一览：标准指南
- en: towardsdatascience.com](/ultimate-guide-to-statistics-for-data-science-a3d8f1fd69a7?source=post_page-----f104c8ede236--------------------------------)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/ultimate-guide-to-statistics-for-data-science-a3d8f1fd69a7?source=post_page-----f104c8ede236--------------------------------)
