- en: Cracking Open the Hugging Face Transformers Library
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 破解Hugging Face Transformers库
- en: 原文：[https://towardsdatascience.com/cracking-open-the-hugging-face-transformers-library-350aa0ef0161](https://towardsdatascience.com/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/cracking-open-the-hugging-face-transformers-library-350aa0ef0161](https://towardsdatascience.com/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
- en: A quick-start guide to using open-source LLMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用开源LLMs的快速入门指南
- en: '[](https://shawhin.medium.com/?source=post_page-----350aa0ef0161--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----350aa0ef0161--------------------------------)[](https://towardsdatascience.com/?source=post_page-----350aa0ef0161--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----350aa0ef0161--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----350aa0ef0161--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shawhin.medium.com/?source=post_page-----350aa0ef0161--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----350aa0ef0161--------------------------------)[](https://towardsdatascience.com/?source=post_page-----350aa0ef0161--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----350aa0ef0161--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----350aa0ef0161--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----350aa0ef0161--------------------------------)
    ·10 min read·Aug 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----350aa0ef0161--------------------------------)
    ·10分钟阅读·2023年8月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: This is the 3rd article in a [series on using large language models (LLMs)](/a-practical-introduction-to-llms-65194dda1148)
    in practice. Here I will give a beginner-friendly guide to the Hugging Face Transformers
    library, which provides an easy and cost-free way to work with a wide variety
    of open-source language models. I will start by reviewing key concepts and then
    dive into example Python code.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个关于实践中使用大型语言模型（LLMs）的[系列文章中的第3篇](/a-practical-introduction-to-llms-65194dda1148)。在这里，我将为Hugging
    Face Transformers库提供一个适合初学者的指南，该库提供了一种简单且无成本的方式来使用各种开源语言模型。我将从回顾关键概念开始，然后深入示例Python代码。
- en: '![](../Images/4250c47bc3e0aefc26a5d1c0a7bb88e2.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4250c47bc3e0aefc26a5d1c0a7bb88e2.png)'
- en: Photo by [Jéan Béller](https://unsplash.com/@chinatravelchannel?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Jéan Béller](https://unsplash.com/@chinatravelchannel?utm_source=medium&utm_medium=referral)
    提供，发布在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In the [previous article](/cracking-open-the-openai-python-api-230e4cae7971)
    of this series, we explored the OpenAI Python API and used it to make a custom
    chatbot. One downside of this API, however, is that API calls cost money, which
    may not scale well for some use cases.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本系列的[上一篇文章](/cracking-open-the-openai-python-api-230e4cae7971)中，我们探索了OpenAI
    Python API，并用它制作了一个定制的聊天机器人。然而，该API的一个缺点是API调用需要付费，这可能在某些用例中不太适用。
- en: In these scenarios, it may be advantageous to turn to open-source solutions.
    One popular way to do this is via Hugging Face’s Transformers library.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，转向开源解决方案可能会很有优势。一种流行的方法是通过**Hugging Face**的Transformers库来实现。
- en: '**What is Hugging Face?**'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**Hugging Face是什么？**'
- en: '**Hugging Face** is an **AI company that has become a major hub for open-source
    machine learning (ML)**. Their platform has 3 major elements which allow users
    to access and share machine learning resources.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**Hugging Face** 是一家**已经成为开源机器学习（ML）主要中心的AI公司**。他们的平台有3个主要元素，允许用户访问和分享机器学习资源。'
- en: First is their rapidly growing repository of pre-trained open-source ML **models**
    for things such as natural language processing (NLP), computer vision, and more.
    Second is their library of **datasets** for training ML models for almost any
    task. Third, and finally, is **Spaces** which is a collection of open-source ML
    apps hosted by Hugging Face.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是他们快速增长的预训练开源ML **模型**库，用于自然语言处理（NLP）、计算机视觉等。第二是他们的**数据集**库，用于训练几乎任何任务的ML模型。第三，也是最后，是**Spaces**，这是一个由Hugging
    Face托管的开源ML应用集合。
- en: The power of these resources is that they are community generated, which leverages
    all the benefits of open-source (i.e. cost-free, wide diversity of tools, high-quality
    resources, and rapid pace of innovation). While these make building powerful ML
    projects more accessible than before, there is another key element of the Hugging
    Face ecosystem — the Transformers library.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些资源的力量在于它们是社区生成的，利用了开源的所有优势（即免费、大量的工具、多样的资源和快速的创新步伐）。虽然这些使得构建强大的 ML 项目比以往更容易，但
    Hugging Face 生态系统的另一个关键元素是——Transformers 库。
- en: 🤗**Transformers**
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🤗**Transformers**
- en: '**Transformers** is a **Python library that makes downloading and training
    state-of-the-art ML models easy**. Although it was initially made for developing
    language models, its functionality has expanded to include models for computer
    vision, audio processing, and beyond.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformers** 是一个 **使下载和训练最先进的 ML 模型变得简单的 Python 库**。虽然它最初是为开发语言模型而创建的，但它的功能已经扩展到包括计算机视觉、音频处理等模型。'
- en: Two big strengths of this library are, **one**, it easily integrates with Hugging
    Face’s (previously mentioned) Models, Datasets, and Spaces repositories, and **two**,
    the library supports other popular ML frameworks such as PyTorch and TensorFlow.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库的两个主要优点是，**一**，它能够轻松与 Hugging Face（前面提到的）的 Models、Datasets 和 Spaces 仓库集成，**二**，这个库支持其他流行的
    ML 框架，如 PyTorch 和 TensorFlow。
- en: This results in a simple and flexible all-in-one platform for downloading, training,
    and deploying machine learning models and apps.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得下载、训练和部署机器学习模型和应用程序变得简单而灵活。
- en: '**Pipeline()**'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Pipeline()**'
- en: The easiest way to start using the library is via the *pipeline()* function,
    which abstracts NLP (and other) tasks into 1 line of code. For example, if we
    want to do sentiment analysis, we would need to select a model, tokenize the input
    text, pass it through the model, and decode the numerical output to determine
    the sentiment label (positive or negative).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个库的最简单方法是通过 *pipeline()* 函数，它将 NLP（和其他）任务抽象成 1 行代码。例如，如果我们想做情感分析，我们需要选择一个模型，对输入文本进行分词，经过模型处理，然后解码数值输出以确定情感标签（积极或消极）。
- en: While this may seem like a lot of steps, we can do all this in 1 line via the
    *pipeline()* function, as shown in the code snippet below.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这看起来有很多步骤，但我们可以通过 *pipeline()* 函数在 1 行代码中完成所有这些步骤，如下面的代码片段所示。
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Of course, sentiment analysis is not the only thing we can do here. Almost any
    NLP task can be done in this way e.g. summarization, translation, question-answering,
    feature extraction (i.e. text embedding), text generation, zero-shot-classification,
    and more — *for a full list of the built-in tasks, check out the* [*pipleine()
    documentation*](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.task).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，情感分析并不是我们在这里唯一能做的事情。几乎任何 NLP 任务都可以用这种方式完成，例如摘要、翻译、问答、特征提取（即文本嵌入）、文本生成、零样本分类等
    —— *有关内置任务的完整列表，请查看* [*pipeline() 文档*](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.task)。
- en: In the above example code, since we did not specify a model, the default model
    for sentiment analysis was used (i.e. *distilbert-base-uncased-finetuned-sst-2-english*).
    However, if we wanted to be more explicit, we could have used the following line
    of code.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例代码中，由于我们没有指定模型，因此使用了默认的情感分析模型（即 *distilbert-base-uncased-finetuned-sst-2-english*）。然而，如果我们想更明确一些，可以使用以下代码行。
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: One of the greatest benefits of the Transformers library is we could have just
    as easily used any of the 28,000+ text classification models on Hugging Face’s
    [Models repository](https://huggingface.co/models?pipeline_tag=text-classification&sort=trending)
    by simply changing the model name passed into the *pipeline()* function.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 库的最大优点之一是我们可以通过简单地更改传递给 *pipeline()* 函数的模型名称，轻松使用 Hugging Face
    的 [Models repository](https://huggingface.co/models?pipeline_tag=text-classification&sort=trending)
    上的任何 28,000 多个文本分类模型。
- en: '**Models**'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**模型**'
- en: There is a massive repository of pre-trained models available on [Hugging Face](https://huggingface.co/models)
    (277,528 at the time of writing this). Almost all these models can be easily used
    via Transformers, using the same syntax we saw in the above code block.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [Hugging Face](https://huggingface.co/models) 上有大量的预训练模型（截至写作时为 277,528 个）。几乎所有这些模型都可以通过
    Transformers 轻松使用，使用与上述代码块中相同的语法。
- en: However, the models on Hugging Face **aren’t only for the Transformers library.**
    There are models for other popular machine learning frameworks e.g. PyTorch, Tensorflow,
    Jax. This makes Hugging Face’s Models repository useful to ML practitioners beyond
    the context of the Transformers library.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Hugging Face上的模型**不仅仅用于Transformers库。** 还有其他流行机器学习框架的模型，如PyTorch、Tensorflow、Jax。这使得Hugging
    Face的模型库对超越Transformers库的机器学习从业者也很有用。
- en: To see what navigating the repository looks like, let’s consider an example.
    Say we want a model that can do text generation, but we want it to be available
    via the Transformers library so we can use it in one line of code (as we did above).
    We can easily view all models that fit these criteria using the “Tasks” and “Libraries”
    filters.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解浏览代码库的样子，我们可以考虑一个例子。假设我们需要一个可以进行文本生成的模型，但我们希望它可以通过Transformers库使用，这样我们就能用一行代码来实现（如上所示）。我们可以轻松地使用“任务”和“库”过滤器来查看符合这些条件的所有模型。
- en: A model that meets these criteria is the newly released Llama 2\. More specifically,
    *Llama-2–7b-chat-hf*, which is a model in the Llama 2 family with about 7 billion
    parameters, optimized for chat, and in the Hugging Face Transformers format. We
    can get more information about this model via its **model card**, which is shown
    in the figure below.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一个符合这些标准的模型是新发布的Llama 2。更具体地说，*Llama-2–7b-chat-hf*，这是Llama 2系列中的一个模型，具有大约70亿个参数，优化用于聊天，并且采用Hugging
    Face Transformers格式。我们可以通过它的**模型卡**获得更多有关该模型的信息，如下图所示。
- en: '![](../Images/6f60ee56c40846bbb992c4274c843c92.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f60ee56c40846bbb992c4274c843c92.png)'
- en: Touring the [Llama-2–7b-chat-hf model card](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf).
    Image by author.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览[Llama-2–7b-chat-hf模型卡](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)。图片由作者提供。
- en: '**Installing** 🤗**Transformers (with Conda)**'
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**安装** 🤗**Transformers（使用Conda）**'
- en: Now that we have a basic idea of the resources offered by Hugging Face and the
    Transformers library let’s see how we can use them. We start by installing the
    library and other dependencies.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对Hugging Face和Transformers库提供的资源有了基本了解，让我们看看如何使用它们。我们从安装库和其他依赖项开始。
- en: Hugging Face provides an [installation guide](https://huggingface.co/docs/transformers/installation)
    on its website. So, I won’t try to (poorly) duplicate that guide here. However,
    I will provide a quick 2-step guide on **how to set up the conda environment for
    the example code below**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 在其网站上提供了一个[安装指南](https://huggingface.co/docs/transformers/installation)。因此，我不会在这里（不熟练地）重复那个指南。不过，我会提供一个简短的2步指南，说明**如何为下面的示例代码设置conda环境**。
- en: '**Step 1)** The first step is to download the hf-env.yml file available at
    the [GitHub repository](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/hugging-face).
    You can either download the file directly or clone the whole repo.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1)** 第一步是下载位于[GitHub仓库](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/hugging-face)的hf-env.yml文件。你可以直接下载文件或克隆整个仓库。'
- en: '**Step 2)** Next, in your terminal (or anaconda command prompt), you can create
    a new conda environment based on hf-env.yml using the following commands'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2)** 接下来，在你的终端（或anaconda命令提示符）中，你可以使用以下命令基于hf-env.yml创建一个新的conda环境。'
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This may take a couple of minutes to install, but once it’s complete, you should
    be ready to go!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要几分钟来安装，但一旦完成，你应该就可以开始使用了！
- en: '**Example Code: NLP with** 🤗**Transformers**'
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**示例代码：使用** 🤗**Transformers进行NLP**'
- en: With the necessary libraries installed, let’s jump into some example code. Here
    we will survey **3 NLP use cases**, namely, **sentiment analysis, summarization,
    and conversational text generation**, using the *pipeline()* function.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装了必要的库之后，让我们进入一些示例代码。这里我们将调查**3个NLP用例**，即**情感分析、摘要和对话文本生成**，使用*pipeline()*函数。
- en: Toward the end, we will use Gradio to quickly generate a User Interface (UI)
    for any of these use cases and deploy it as an app on Hugging Face Spaces. All
    example code is available on the [GitHub repository](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/hugging-face).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后，我们将使用Gradio快速生成一个用户界面（UI）来处理这些用例，并将其作为应用部署在Hugging Face Spaces上。所有示例代码都可以在[GitHub仓库](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/hugging-face)上找到。
- en: '**Sentiment Analysis**'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**情感分析**'
- en: We start sentiment analysis. Recall from earlier when we used the pipeline function
    to do something like the code block below, where we create a classifier that can
    label the input text as being either positive or negative.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始情感分析。回忆一下我们之前使用pipeline函数来完成类似下面的代码块的操作，其中我们创建了一个分类器，可以将输入文本标记为正面或负面。
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: To go one step further, instead of processing text one by one, we can pass a
    list to the classifier to process as a batch.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更进一步，我们可以将文本列表传递给分类器进行批处理，而不是逐个处理。
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: However, the text classification models on Hugging Face are not limited to just
    positive-negative sentiment. For example, the “*roberta-base-go_emotions*” model
    by SamLowe generates a suite of class labels. We can just as easily apply this
    model to text, as shown in the code snippet below.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Hugging Face 上的文本分类模型不仅限于正面-负面情感。例如，SamLowe 的“*roberta-base-go_emotions*”模型生成了一套类别标签。我们可以像下面的代码片段所示一样轻松地将此模型应用于文本。
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Summarization**'
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**总结**'
- en: Another way we can use the *pipeline()* function is for text summarization.
    Although this is an entirely different task than sentiment analysis, the syntax
    is almost identical.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将 *pipeline()* 函数用于文本总结。虽然这与情感分析是完全不同的任务，但语法几乎是相同的。
- en: We first load in a summarization model. Then pass in some text along with a
    couple of input parameters.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载一个总结模型。然后传入一些文本以及几个输入参数。
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: For more sophisticated use cases, it may be necessary to use multiple models
    in succession. For example, we can apply sentiment analysis to the summarized
    text to speed up the runtime.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的用例，可能需要连续使用多个模型。例如，我们可以对总结后的文本应用情感分析以加快运行时间。
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Conversational**'
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**对话式**'
- en: Finally, we can use models developed specifically to generate conversational
    text. Since conversations require past prompts and responses to be passed to subsequent
    model responses, the syntax is a little different here. However, we start by instantiating
    our model using the *pipeline()* function.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用专门开发的模型来生成对话文本。由于对话需要将过去的提示和响应传递到后续的模型响应中，因此语法在这里略有不同。不过，我们首先使用 *pipeline()*
    函数来实例化我们的模型。
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Next, we can use the *Conversation()* class to handle the back-and-forth. We
    initialize it with a user prompt, then pass it into the chatbot model from the
    previous code block.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用 *Conversation()* 类来处理来回对话。我们用用户提示初始化它，然后将其传递给前一个代码块中的聊天机器人模型。
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: To keep the conversation going, we can use the *add_user_input()* method to
    add another prompt to the conversation. We then pass the conversation object back
    into the chatbot.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持对话进行，我们可以使用 *add_user_input()* 方法向对话中添加另一个提示。然后我们将对话对象传递回聊天机器人。
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Chatbot UI with Gradio**'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**使用 Gradio 的聊天机器人界面**'
- en: While we get the base chatbot functionality with the Transformer library, this
    is an inconvenient way to interact with a chatbot. To make the interaction a bit
    more intuitive, we can use **Gradio** to **spin up a front end in a few lines
    of Python code**.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们通过 Transformer 库获得了基本的聊天机器人功能，但这与聊天机器人交互的方式并不方便。为了使交互更直观，我们可以使用 **Gradio**
    来 **用几行 Python 代码启动前端**。
- en: This is done with the code shown below. At the top, we initialize two lists
    to store user messages and model responses, respectively. Then we define a function
    that will take the user prompt and generate a chatbot output. Next, we create
    the chat UI using the Gradio *ChatInterface()* class. Finally, we launch the app.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过下面的代码完成。在顶部，我们初始化两个列表，分别用于存储用户消息和模型响应。然后我们定义一个函数，该函数将接收用户提示并生成聊天机器人输出。接下来，我们使用
    Gradio 的*ChatInterface()* 类创建聊天界面。最后，我们启动应用程序。
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This will spin up the UI via a local URL. If the window does not open automatically,
    you can copy and paste the URL directly into your browser.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这将通过本地 URL 启动用户界面。如果窗口没有自动打开，您可以直接将 URL 复制并粘贴到浏览器中。
- en: '![](../Images/ec9de13ac0daaafaa94c91ccc160b476.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec9de13ac0daaafaa94c91ccc160b476.png)'
- en: Gradio interface. GIF by author.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Gradio 界面。动图由作者提供。
- en: '**Hugging Face Spaces**'
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Hugging Face Spaces**'
- en: To go one step further, we can quickly deploy this UI via **Hugging Face Spaces**.
    These are **Git repositories hosted by Hugging Face and augmented by computational
    resources**. Both free and paid options are available depending on the use case.
    Here we will stick with the free option.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更进一步，我们可以通过 **Hugging Face Spaces** 快速部署此用户界面。这些是 **由 Hugging Face 托管并由计算资源增强的
    Git 仓库**。根据使用情况，有免费和付费选项可供选择。在这里我们将坚持使用免费选项。
- en: To make a new Space, we first go to the [Spaces page](https://huggingface.co/spaces)
    and click “Create new space”. Then, configure the Space by giving it the name
    e.g. “my-first-space” and selecting Gradio as the SDK. Then hit “Create Space”.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个新的 Space，我们首先访问 [Spaces 页面](https://huggingface.co/spaces) 并点击“创建新空间”。然后，通过给
    Space 命名（例如“my-first-space”）并选择 Gradio 作为 SDK 来配置 Space。然后点击“创建 Space”。
- en: '![](../Images/90ba96ca925a2806699593653b3bc14b.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90ba96ca925a2806699593653b3bc14b.png)'
- en: Hugging Face Space configuration. Image by author.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face Space 配置。图片由作者提供。
- en: Next, we need to upload app.py and requirements.txt files to the Space. The
    app.py file houses the code we used to generate the Gradio UI, and the requirements.txt
    file specifies the app’s dependencies. The files for this example are available
    at the [GitHub repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/hugging-face/my-first-space)
    and the [Hugging Face Space](https://huggingface.co/spaces/shawhin/my-first-space/tree/main).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将 app.py 和 requirements.txt 文件上传到 Space。app.py 文件包含了我们用于生成 Gradio UI
    的代码，而 requirements.txt 文件指定了应用程序的依赖项。这个示例的文件可以在 [GitHub 仓库](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/hugging-face/my-first-space)
    和 [Hugging Face Space](https://huggingface.co/spaces/shawhin/my-first-space/tree/main)
    中找到。
- en: Finally, we push the code to the Space just like we would to GitHub. The end
    result is a public application hosted on Hugging Face Spaces.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将代码推送到 Space，就像我们推送到 GitHub 一样。最终结果是一个托管在 Hugging Face Spaces 上的公开应用程序。
- en: '**App link**: [https://huggingface.co/spaces/shawhin/my-first-space](https://huggingface.co/spaces/shawhin/my-first-space)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用链接**: [https://huggingface.co/spaces/shawhin/my-first-space](https://huggingface.co/spaces/shawhin/my-first-space)'
- en: '**Conclusion**'
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: Hugging Face has become synonymous with open-source language models and machine
    learning. The biggest advantage of their ecosystem is it gives small-time developers,
    researchers, and tinkers access to powerful ML resources.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 已成为开源语言模型和机器学习的代名词。他们生态系统的最大优势在于为小型开发者、研究人员和爱好者提供了强大的机器学习资源。
- en: While we covered a lot of material in this post, we’ve only scratched the surface
    of what the Hugging Face ecosystem can do. In future articles of this series,
    we will explore more advanced use cases and cover [how to fine-tune models](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)
    using 🤗Transformers.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在这篇文章中覆盖了很多内容，但我们仅仅触及了 Hugging Face 生态系统所能做的皮毛。在本系列的未来文章中，我们将深入探讨更多高级用例，并介绍
    [如何微调模型](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)
    使用 🤗Transformers。
- en: '👉 **More on LLMs**: [Introduction](/a-practical-introduction-to-llms-65194dda1148)
    | [OpenAI API](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)
    | [Prompt Engineering](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)
    |'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '👉 **更多关于LLMs**: [介绍](/a-practical-introduction-to-llms-65194dda1148) | [OpenAI
    API](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)
    | [提示工程](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)
    |'
- en: '[Fine-tuning](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)
    | [Build an LLM](/how-to-build-an-llm-from-scratch-8c477768f1f9) | [QLoRA](/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)
    | [RAG](https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac)
    | [Text Embeddings](/text-embeddings-classification-and-semantic-search-8291746220be)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[微调](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)
    | [构建LLM](/how-to-build-an-llm-from-scratch-8c477768f1f9) | [QLoRA](/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)
    | [RAG](https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac)
    | [文本嵌入](/text-embeddings-classification-and-semantic-search-8291746220be)'
- en: '![Shaw Talebi](../Images/02eefb458c6eeff7cd29d40c212e3b22.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![Shaw Talebi](../Images/02eefb458c6eeff7cd29d40c212e3b22.png)'
- en: '[Shaw Talebi](https://shawhin.medium.com/?source=post_page-----350aa0ef0161--------------------------------)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[Shaw Talebi](https://shawhin.medium.com/?source=post_page-----350aa0ef0161--------------------------------)'
- en: Large Language Models (LLMs)
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型 (LLMs)
- en: '[View list](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c?source=post_page-----350aa0ef0161--------------------------------)13
    stories![](../Images/82e865594c68f5307e75665842d197bb.png)![](../Images/b9436354721f807e0390b5e301be2119.png)![](../Images/59c8db581de77a908457dec8981f3c37.png)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c?source=post_page-----350aa0ef0161--------------------------------)13个故事![](../Images/82e865594c68f5307e75665842d197bb.png)![](../Images/b9436354721f807e0390b5e301be2119.png)![](../Images/59c8db581de77a908457dec8981f3c37.png)'
- en: Resources
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '**Connect**: [My website](https://shawhintalebi.com/) | [Book a call](https://calendly.com/shawhintalebi)
    | [Ask me anything](https://shawhintalebi.com/contact/)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**联系**: [我的网站](https://shawhintalebi.com/) | [预约电话](https://calendly.com/shawhintalebi)
    | [向我提问](https://shawhintalebi.com/contact/)'
- en: '**Socials**: [YouTube 🎥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)
    | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**社交媒体**: [YouTube 🎥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)
    | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)'
- en: '**Support**: [Buy me a coffee](https://www.buymeacoffee.com/shawhint) ☕️'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持**: [请我喝咖啡](https://www.buymeacoffee.com/shawhint) ☕️'
- en: '[](https://shawhin.medium.com/subscribe?source=post_page-----350aa0ef0161--------------------------------)
    [## Get FREE access to every new story I write'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shawhin.medium.com/subscribe?source=post_page-----350aa0ef0161--------------------------------)
    [## 免费获取我写的每一个新故事'
- en: Get FREE access to every new story I write P.S. I do not share your email with
    anyone By signing up, you will create a…
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 免费获取我写的每一个新故事 附言：我不会与任何人分享你的电子邮件 注册后，你将创建一个…
- en: shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----350aa0ef0161--------------------------------)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----350aa0ef0161--------------------------------)
- en: '[1] Hugging Face — [https://huggingface.co/](https://huggingface.co/)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Hugging Face — [https://huggingface.co/](https://huggingface.co/)'
- en: '[2] Hugging Face Course — [https://huggingface.co/learn/nlp-course/chapter1/1](https://huggingface.co/learn/nlp-course/chapter1/1)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Hugging Face 课程 — [https://huggingface.co/learn/nlp-course/chapter1/1](https://huggingface.co/learn/nlp-course/chapter1/1)'
