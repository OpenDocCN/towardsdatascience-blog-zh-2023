- en: Cracking Open the Hugging Face Transformers Library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/cracking-open-the-hugging-face-transformers-library-350aa0ef0161](https://towardsdatascience.com/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A quick-start guide to using open-source LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://shawhin.medium.com/?source=post_page-----350aa0ef0161--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----350aa0ef0161--------------------------------)[](https://towardsdatascience.com/?source=post_page-----350aa0ef0161--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----350aa0ef0161--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----350aa0ef0161--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----350aa0ef0161--------------------------------)
    ¬∑10 min read¬∑Aug 5, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: This is the 3rd article in a [series on using large language models (LLMs)](/a-practical-introduction-to-llms-65194dda1148)
    in practice. Here I will give a beginner-friendly guide to the Hugging Face Transformers
    library, which provides an easy and cost-free way to work with a wide variety
    of open-source language models. I will start by reviewing key concepts and then
    dive into example Python code.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4250c47bc3e0aefc26a5d1c0a7bb88e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [J√©an B√©ller](https://unsplash.com/@chinatravelchannel?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In the [previous article](/cracking-open-the-openai-python-api-230e4cae7971)
    of this series, we explored the OpenAI Python API and used it to make a custom
    chatbot. One downside of this API, however, is that API calls cost money, which
    may not scale well for some use cases.
  prefs: []
  type: TYPE_NORMAL
- en: In these scenarios, it may be advantageous to turn to open-source solutions.
    One popular way to do this is via Hugging Face‚Äôs Transformers library.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is Hugging Face?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Hugging Face** is an **AI company that has become a major hub for open-source
    machine learning (ML)**. Their platform has 3 major elements which allow users
    to access and share machine learning resources.'
  prefs: []
  type: TYPE_NORMAL
- en: First is their rapidly growing repository of pre-trained open-source ML **models**
    for things such as natural language processing (NLP), computer vision, and more.
    Second is their library of **datasets** for training ML models for almost any
    task. Third, and finally, is **Spaces** which is a collection of open-source ML
    apps hosted by Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: The power of these resources is that they are community generated, which leverages
    all the benefits of open-source (i.e. cost-free, wide diversity of tools, high-quality
    resources, and rapid pace of innovation). While these make building powerful ML
    projects more accessible than before, there is another key element of the Hugging
    Face ecosystem ‚Äî the Transformers library.
  prefs: []
  type: TYPE_NORMAL
- en: ü§ó**Transformers**
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Transformers** is a **Python library that makes downloading and training
    state-of-the-art ML models easy**. Although it was initially made for developing
    language models, its functionality has expanded to include models for computer
    vision, audio processing, and beyond.'
  prefs: []
  type: TYPE_NORMAL
- en: Two big strengths of this library are, **one**, it easily integrates with Hugging
    Face‚Äôs (previously mentioned) Models, Datasets, and Spaces repositories, and **two**,
    the library supports other popular ML frameworks such as PyTorch and TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: This results in a simple and flexible all-in-one platform for downloading, training,
    and deploying machine learning models and apps.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipeline()**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The easiest way to start using the library is via the *pipeline()* function,
    which abstracts NLP (and other) tasks into 1 line of code. For example, if we
    want to do sentiment analysis, we would need to select a model, tokenize the input
    text, pass it through the model, and decode the numerical output to determine
    the sentiment label (positive or negative).
  prefs: []
  type: TYPE_NORMAL
- en: While this may seem like a lot of steps, we can do all this in 1 line via the
    *pipeline()* function, as shown in the code snippet below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Of course, sentiment analysis is not the only thing we can do here. Almost any
    NLP task can be done in this way e.g. summarization, translation, question-answering,
    feature extraction (i.e. text embedding), text generation, zero-shot-classification,
    and more ‚Äî *for a full list of the built-in tasks, check out the* [*pipleine()
    documentation*](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.task).
  prefs: []
  type: TYPE_NORMAL
- en: In the above example code, since we did not specify a model, the default model
    for sentiment analysis was used (i.e. *distilbert-base-uncased-finetuned-sst-2-english*).
    However, if we wanted to be more explicit, we could have used the following line
    of code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: One of the greatest benefits of the Transformers library is we could have just
    as easily used any of the 28,000+ text classification models on Hugging Face‚Äôs
    [Models repository](https://huggingface.co/models?pipeline_tag=text-classification&sort=trending)
    by simply changing the model name passed into the *pipeline()* function.
  prefs: []
  type: TYPE_NORMAL
- en: '**Models**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a massive repository of pre-trained models available on [Hugging Face](https://huggingface.co/models)
    (277,528 at the time of writing this). Almost all these models can be easily used
    via Transformers, using the same syntax we saw in the above code block.
  prefs: []
  type: TYPE_NORMAL
- en: However, the models on Hugging Face **aren‚Äôt only for the Transformers library.**
    There are models for other popular machine learning frameworks e.g. PyTorch, Tensorflow,
    Jax. This makes Hugging Face‚Äôs Models repository useful to ML practitioners beyond
    the context of the Transformers library.
  prefs: []
  type: TYPE_NORMAL
- en: To see what navigating the repository looks like, let‚Äôs consider an example.
    Say we want a model that can do text generation, but we want it to be available
    via the Transformers library so we can use it in one line of code (as we did above).
    We can easily view all models that fit these criteria using the ‚ÄúTasks‚Äù and ‚ÄúLibraries‚Äù
    filters.
  prefs: []
  type: TYPE_NORMAL
- en: A model that meets these criteria is the newly released Llama 2\. More specifically,
    *Llama-2‚Äì7b-chat-hf*, which is a model in the Llama 2 family with about 7 billion
    parameters, optimized for chat, and in the Hugging Face Transformers format. We
    can get more information about this model via its **model card**, which is shown
    in the figure below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6f60ee56c40846bbb992c4274c843c92.png)'
  prefs: []
  type: TYPE_IMG
- en: Touring the [Llama-2‚Äì7b-chat-hf model card](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf).
    Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Installing** ü§ó**Transformers (with Conda)**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a basic idea of the resources offered by Hugging Face and the
    Transformers library let‚Äôs see how we can use them. We start by installing the
    library and other dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face provides an [installation guide](https://huggingface.co/docs/transformers/installation)
    on its website. So, I won‚Äôt try to (poorly) duplicate that guide here. However,
    I will provide a quick 2-step guide on **how to set up the conda environment for
    the example code below**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1)** The first step is to download the hf-env.yml file available at
    the [GitHub repository](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/hugging-face).
    You can either download the file directly or clone the whole repo.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2)** Next, in your terminal (or anaconda command prompt), you can create
    a new conda environment based on hf-env.yml using the following commands'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This may take a couple of minutes to install, but once it‚Äôs complete, you should
    be ready to go!
  prefs: []
  type: TYPE_NORMAL
- en: '**Example Code: NLP with** ü§ó**Transformers**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the necessary libraries installed, let‚Äôs jump into some example code. Here
    we will survey **3 NLP use cases**, namely, **sentiment analysis, summarization,
    and conversational text generation**, using the *pipeline()* function.
  prefs: []
  type: TYPE_NORMAL
- en: Toward the end, we will use Gradio to quickly generate a User Interface (UI)
    for any of these use cases and deploy it as an app on Hugging Face Spaces. All
    example code is available on the [GitHub repository](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/hugging-face).
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentiment Analysis**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We start sentiment analysis. Recall from earlier when we used the pipeline function
    to do something like the code block below, where we create a classifier that can
    label the input text as being either positive or negative.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To go one step further, instead of processing text one by one, we can pass a
    list to the classifier to process as a batch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: However, the text classification models on Hugging Face are not limited to just
    positive-negative sentiment. For example, the ‚Äú*roberta-base-go_emotions*‚Äù model
    by SamLowe generates a suite of class labels. We can just as easily apply this
    model to text, as shown in the code snippet below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Summarization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another way we can use the *pipeline()* function is for text summarization.
    Although this is an entirely different task than sentiment analysis, the syntax
    is almost identical.
  prefs: []
  type: TYPE_NORMAL
- en: We first load in a summarization model. Then pass in some text along with a
    couple of input parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: For more sophisticated use cases, it may be necessary to use multiple models
    in succession. For example, we can apply sentiment analysis to the summarized
    text to speed up the runtime.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Conversational**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we can use models developed specifically to generate conversational
    text. Since conversations require past prompts and responses to be passed to subsequent
    model responses, the syntax is a little different here. However, we start by instantiating
    our model using the *pipeline()* function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Next, we can use the *Conversation()* class to handle the back-and-forth. We
    initialize it with a user prompt, then pass it into the chatbot model from the
    previous code block.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: To keep the conversation going, we can use the *add_user_input()* method to
    add another prompt to the conversation. We then pass the conversation object back
    into the chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Chatbot UI with Gradio**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While we get the base chatbot functionality with the Transformer library, this
    is an inconvenient way to interact with a chatbot. To make the interaction a bit
    more intuitive, we can use **Gradio** to **spin up a front end in a few lines
    of Python code**.
  prefs: []
  type: TYPE_NORMAL
- en: This is done with the code shown below. At the top, we initialize two lists
    to store user messages and model responses, respectively. Then we define a function
    that will take the user prompt and generate a chatbot output. Next, we create
    the chat UI using the Gradio *ChatInterface()* class. Finally, we launch the app.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This will spin up the UI via a local URL. If the window does not open automatically,
    you can copy and paste the URL directly into your browser.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec9de13ac0daaafaa94c91ccc160b476.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradio interface. GIF by author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Hugging Face Spaces**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To go one step further, we can quickly deploy this UI via **Hugging Face Spaces**.
    These are **Git repositories hosted by Hugging Face and augmented by computational
    resources**. Both free and paid options are available depending on the use case.
    Here we will stick with the free option.
  prefs: []
  type: TYPE_NORMAL
- en: To make a new Space, we first go to the [Spaces page](https://huggingface.co/spaces)
    and click ‚ÄúCreate new space‚Äù. Then, configure the Space by giving it the name
    e.g. ‚Äúmy-first-space‚Äù and selecting Gradio as the SDK. Then hit ‚ÄúCreate Space‚Äù.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/90ba96ca925a2806699593653b3bc14b.png)'
  prefs: []
  type: TYPE_IMG
- en: Hugging Face Space configuration. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to upload app.py and requirements.txt files to the Space. The
    app.py file houses the code we used to generate the Gradio UI, and the requirements.txt
    file specifies the app‚Äôs dependencies. The files for this example are available
    at the [GitHub repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/hugging-face/my-first-space)
    and the [Hugging Face Space](https://huggingface.co/spaces/shawhin/my-first-space/tree/main).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we push the code to the Space just like we would to GitHub. The end
    result is a public application hosted on Hugging Face Spaces.
  prefs: []
  type: TYPE_NORMAL
- en: '**App link**: [https://huggingface.co/spaces/shawhin/my-first-space](https://huggingface.co/spaces/shawhin/my-first-space)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hugging Face has become synonymous with open-source language models and machine
    learning. The biggest advantage of their ecosystem is it gives small-time developers,
    researchers, and tinkers access to powerful ML resources.
  prefs: []
  type: TYPE_NORMAL
- en: While we covered a lot of material in this post, we‚Äôve only scratched the surface
    of what the Hugging Face ecosystem can do. In future articles of this series,
    we will explore more advanced use cases and cover [how to fine-tune models](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)
    using ü§óTransformers.
  prefs: []
  type: TYPE_NORMAL
- en: 'üëâ **More on LLMs**: [Introduction](/a-practical-introduction-to-llms-65194dda1148)
    | [OpenAI API](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)
    | [Prompt Engineering](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)
    |'
  prefs: []
  type: TYPE_NORMAL
- en: '[Fine-tuning](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)
    | [Build an LLM](/how-to-build-an-llm-from-scratch-8c477768f1f9) | [QLoRA](/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)
    | [RAG](https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac)
    | [Text Embeddings](/text-embeddings-classification-and-semantic-search-8291746220be)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Shaw Talebi](../Images/02eefb458c6eeff7cd29d40c212e3b22.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Shaw Talebi](https://shawhin.medium.com/?source=post_page-----350aa0ef0161--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c?source=post_page-----350aa0ef0161--------------------------------)13
    stories![](../Images/82e865594c68f5307e75665842d197bb.png)![](../Images/b9436354721f807e0390b5e301be2119.png)![](../Images/59c8db581de77a908457dec8981f3c37.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Connect**: [My website](https://shawhintalebi.com/) | [Book a call](https://calendly.com/shawhintalebi)
    | [Ask me anything](https://shawhintalebi.com/contact/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Socials**: [YouTube üé•](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)
    | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Support**: [Buy me a coffee](https://www.buymeacoffee.com/shawhint) ‚òïÔ∏è'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://shawhin.medium.com/subscribe?source=post_page-----350aa0ef0161--------------------------------)
    [## Get FREE access to every new story I write'
  prefs: []
  type: TYPE_NORMAL
- en: Get FREE access to every new story I write P.S. I do not share your email with
    anyone By signing up, you will create a‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----350aa0ef0161--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Hugging Face ‚Äî [https://huggingface.co/](https://huggingface.co/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Hugging Face Course ‚Äî [https://huggingface.co/learn/nlp-course/chapter1/1](https://huggingface.co/learn/nlp-course/chapter1/1)'
  prefs: []
  type: TYPE_NORMAL
