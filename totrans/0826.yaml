- en: Estimating Total Experimentation Impact
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/estimating-total-experimentation-impact-ab6cd56bffb](https://towardsdatascience.com/estimating-total-experimentation-impact-ab6cd56bffb)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to control for false-discovery and selection biases when measuring your
    organization’s total impact
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dataneversleeps.medium.com/?source=post_page-----ab6cd56bffb--------------------------------)[![Jared
    M. Maruskin, PhD](../Images/771dcac046565d4760077afecb3fadef.png)](https://dataneversleeps.medium.com/?source=post_page-----ab6cd56bffb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ab6cd56bffb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ab6cd56bffb--------------------------------)
    [Jared M. Maruskin, PhD](https://dataneversleeps.medium.com/?source=post_page-----ab6cd56bffb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ab6cd56bffb--------------------------------)
    ·16 min read·Sep 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94df2a1643befd711d43b8bf8f43ca3d.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [CHUTTERSNAP](https://unsplash.com/@chuttersnap?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data-driven organizations often run hundreds or thousands of experiments at
    any given time, but what is the net impact of all of these experiments? A naive
    approach is to sum the difference-in-means across all experiments that resulted
    in a significant and positive treatment effect and that were rolled out into production.
    This estimate, however, can be extremely biased, even if we assume there are no
    correlations between individual experiments. We will run a simulation of 10,000
    experiments and show that this naive approach overestimates the actual impact
    delivered by *45%*!
  prefs: []
  type: TYPE_NORMAL
- en: 'We review a theoretical bias correction formula, due to *Lee and Shen* [1].
    This approach, however, suffers from two defects: first, though it is theoretically
    unbiased, we show that its corresponding plug-in estimator nonetheless suffers
    from significant bias for similar reasons as the original problem. Second, it
    does not attribute impact to individual level experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this post, we explore two sources of bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '*False-discovery bias* — the estimate is inflated due to false positives;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Selection bias* — the estimate is inflated due to a bias introduced by the
    decision criterion: underestimates of the treatment effect are censored (false
    negatives), whereas overestimates are rewarded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To address false discovery, we will construct a probability that a given result
    is actually non-zero. This probability is constructed by comparing the *p*-value
    density to the referred residual density from the true nulls.
  prefs: []
  type: TYPE_NORMAL
- en: To address selection bias, we will compute a *posterior distribution* for each
    experimental result, using the empirical distribution, corrected for false discovery,
    as our prior.
  prefs: []
  type: TYPE_NORMAL
- en: This process yields an accurate estimate of the average experimental impact
    across our simulated series of experiments, reducing the original *45%* error
    using the empirical measurements alone to a 0.4*%* error.
  prefs: []
  type: TYPE_NORMAL
- en: The Effect Distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So we’ve ran a bunch of experiments and want to quantify our total experimental
    impact delivered. To do this, we need to consider *the effect distribution*, or
    the distribution of treatment effects. To conceptualize this, imagine running
    many many experiments. Each experiment will have a true value of its effect θ*ᵢ*,
    which we may regard as a random variable drawn from some true effect distribution
    *p*. Each experiment then estimates *θᵢ* through *Xᵢ* (which we may regard as
    the average difference-in-means)*.* We thus obtain the following model for our
    observations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *σᵢ² ~ 1/nₛ*, where *nₛ* is the sample size. We will consider the true
    effect distribution to be a mixture of three subpopulations: true nulls, positives,
    and negatives. To describe the effect distribution, we consider three parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '*λ = P(θ != 0)* (*effectiveness*): the fraction of experiments with a nonzero
    effect,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*κ = P(θ > 0 | θ != 0)* (*asymmetry*): the fraction of experiments with positive
    effect, out of those that have a nonzero effect,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ρ = P(θ > δ | θ > 0)* (*advantage*): the fraction of experiments with a *practically*
    significant effect (i.e., whose effect size is greater than *δ*), out of those
    that have a positive (significant) effect.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These parameters are illustrated in the figure below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/019afe10fb49b721c056bb6c06e97033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Distribution of treatment effects: *(1-λ)* are null, κ λ are positive, and
    κ λ ρ are practically significant. Image by Author.'
  prefs: []
  type: TYPE_NORMAL
- en: For newer products, it may be easy to come up with feature ideas that result
    in a significant effect, so that λ would be high. For mature products, however,
    it may be the case that it is very rare for an experiment to lead to a significant
    change, and *λ* would be low.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, a given team might be very good at proposing product-change ideas
    that actually result in a *positive* effect, so that *κ > 1/2*, but another team
    might propose a bunch of bad ideas that result in performance loss, so that that
    the effects tend to be negative, so that *κ < 1/2*.
  prefs: []
  type: TYPE_NORMAL
- en: Theoretical Bias Estimator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consider the random set *A* consisting of the positive significant results from
    a series of experiments, with decision criterion *X_i > c*. Define the total true
    and estimated effects as
  prefs: []
  type: TYPE_NORMAL
- en: 'The total expected bias can be described as *b = E[Sₐ— Tₐ].* If *Xᵢ ~ N(θᵢ,
    σᵢ)*, *Lee and Shen* [1] show that the total expected bias is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: 'where *ϕ* is the density of the standard normal distribution. The details of
    this are further reviewed in [2]. There are two main drawbacks to this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: It relies on the true effects *θᵢ*. To actually use it, we must therefore replace
    the true effects with the observed effects, the so-called *plug-in estimator*.
    In our simulation, we show that the plug-in estimator can lead to a significantly
    biased estimate. (We will also show how to correct for this by taking into account
    false-discovery.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The summation is over *all* experiments, not limited to those with a positive
    significant result. So even correcting for the first problem, it does not provide
    an estimate of impact at the individual experiment level, the so-called *attribution
    problem*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Simulating an Experimentation Platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Few Assumptions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start by making a few assumptions about the set of experiments we are
    analyzing:'
  prefs: []
  type: TYPE_NORMAL
- en: each experiment measures the difference-in-means for the same top-line metric
    (e.g., DAUs, time spent, revenue);
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: each experiment was run using the same significance level α and power β for
    effect size δ (which is the minimum effect that would be *practically significant*);
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: each experiment therefore has the same sample size *nₛ* and rejection criterion
    *Xᵢ > c*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For purposes of calculation, we will use *α = 0.05*, *β = 0.8* for effect size
    *δ = 0.1*, which yield *c* = 0.07 and *nₛ* = 785\. Note: we regard *α* as a two-sided
    significance. The actual significance for the positive test is *α/2.*'
  prefs: []
  type: TYPE_NORMAL
- en: Simulation in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We next construct a simulated experimentation platform in Python, as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We first assign *(1-λ)* of the experiments to have a true null effect.
  prefs: []
  type: TYPE_NORMAL
- en: For the remainder of experiments, we use a gamma distribution, ensuring that
    the survival function at *δ* is *ρ*.
  prefs: []
  type: TYPE_NORMAL
- en: With the **PlatformSim** class, we can run a simulation with a few lines of
    code. We put the results into a dataframe **df**, and create a separate dataframe
    **dfs** for positive results. *Note:* this can be easily modified to model different
    distributions for the positive / negative wings. For the purpose of our simulation,
    however, we used a gamma random variable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Using our simulation, we can easily measure both the observed average treatment
    effect as well as the actual treatment effect, for experiments that had a significant
    and positive effect. In this simulation, the true average effect is *0.0792*,
    whereas our measured effect is *0.1132,* a +43% error! We also observe a false-discovery
    rate of 27%.
  prefs: []
  type: TYPE_NORMAL
- en: We also observe that the theoretical bias-correction formula works perfectly
    when using the true *θᵢ*s, with a 0.19% error, but fails when swapped for the
    plug-in estimator, which produces a 21% error. The theoretical estimator overcorrects
    for bias. This is due to the large number of true nulls and the fact that the
    bias formula picks up observations randomly scattered to the right, but not those
    scattered to the left, as the estimator value peaks for observed values near the
    decision boundary.
  prefs: []
  type: TYPE_NORMAL
- en: We can plot a histogram of the effect distribution. Below is a histogram of
    the true effects, with the positive test results highlighted in orange.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5157b96be91b93902aa7b770172307e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of true effects, with orange representing the positive results.
    Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: We notice there are a large number of false positives, much more than our *α/2
    = 2.5%* positive significance level. The fraction of false positives to overall
    positives is known as the *false discovery rate*, and we will discuss how to estimate
    it and adjust for it momentarily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, when conducting a series of experiments, one would not have access to
    this true distribution of treatment effects, but instead only to the distribution
    of *observed* treatment effects, which is plotted below. Here, we highlight those
    experiments in which the true effect actually was positive in orange. This view
    gives us insight into the second type of bias we must address: *selection bias*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/712c4eeb58b66369c6130b7e9bc40bd4.png)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of measured effects, with orange representing the experiments for
    which there actually was an effect. The decision boundary is represented by the
    dashed vertical line. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand selection bias, consider that the *false negatives* have true
    effects to the right of the vertical dashed line. For every experiment scattered
    to the left, one will be scattered to the right. So the false negatives may be
    regarded as a reflection of true positives that are overestimates. The selection
    process itself therefore creates a bias: only if the observed effect is greater
    than *c* do we consider the result to be statistically significant. We are therefore
    selecting the observations that randomly have a higher effect than their true
    effect, and ignoring the observations that have a lower effect. All of the false
    negatives highlighted in the above figure “balance out’’ with a true positives
    whose measured effect is inflated.'
  prefs: []
  type: TYPE_NORMAL
- en: False Discovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we discuss some theoretical relationships between *λ*, *κ*,
    power, and the false-discovery rate. We will use these in our simulation later
    in this note.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating *λ* and *κ*
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to estimate the false-discovery rate using the observational data that
    would be available in practice, we need to first estimate the population parameters
    *λ* and *κ*.
  prefs: []
  type: TYPE_NORMAL
- en: To estimate *λ*, we consider the fraction *f* of experiments that led to statistically
    significant results. These consist of true positives and false positives . Of
    the *(1-λ)* experiments with no effect, we expect *α (1 — λ)* to produce false
    positives. Of the *λ* experiments with an actual effect, we expect *βₐ λ* to produce
    true positives, where *βₐ* is the average power over this set of experiments.
    The expected fraction of significant results is therefore given by
  prefs: []
  type: TYPE_NORMAL
- en: which may be rearranged to produce the estimate
  prefs: []
  type: TYPE_NORMAL
- en: where *βₐ* is the average power of the observed significant results. (I used
    β^* in displayed equations for βₐ, but they are the same.)
  prefs: []
  type: TYPE_NORMAL
- en: Estimating False-Discovery Rate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *false-discovery rate* (FDR) is the fraction of positive results that are
    false positives. We observed a 27% false-discovery rate in our simulation.
  prefs: []
  type: TYPE_NORMAL
- en: To estimate this, we will need the *average positive power*
  prefs: []
  type: TYPE_NORMAL
- en: Here, *β(θ) = P(X > c | θ)* is the probability of observing a positive significant
    effect, given *θ*. For the following, we will assume the average negative power
    *β₋* is negligible.
  prefs: []
  type: TYPE_NORMAL
- en: The FDR can be expressed using Bayes’ law by the relation
  prefs: []
  type: TYPE_NORMAL
- en: We will later use this formula to obtain an estimate of 27.6% for our false
    discovery rate. The approximate equality reminds us that we are ignoring the power
    of the negative distribution, which in our simulation is negligible.
  prefs: []
  type: TYPE_NORMAL
- en: Density Estimates and *p-probability*
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We next explore a method that can adjust for false-discovery bias using a two-step
    approach:'
  prefs: []
  type: TYPE_NORMAL
- en: determine the probability that each positive result is a true positive;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: compute the discounted average treatment effect, where we discount each experiment’s
    effect by the probability that it is a false positive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to devise a probability that an experiment produced an actual effect,
    we turn to the *p*-values. The key observation here is that the distribution of
    *p*-values under the null hypothesis is a uniform distribution. This is evident
    in below figure, which represents a histogram of all our *p*-values, highlighting
    the *p*-values corresponding to actual nulls in orange.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1067ac954f092c632b035f3ade051afe.png)'
  prefs: []
  type: TYPE_IMG
- en: Histogram of p-values; p-values of actual nulls are plotted in orange. Image
    by Author.
  prefs: []
  type: TYPE_NORMAL
- en: Let *f(p)* be the observed probability density of *p*-values, and let *fₐ* represent
    the average probability density of *p*-values over the half-interval *[0.5, 1]*.
    We may infer that *fₐ* is the average probability density of the true nulls, which
    is constant over the full interval *[0, 1]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of *p*-values, we reject the null hypothesis whenever *p < α = 0.05*.
    For a given bin of experiments, say at *pᵢ*, the probability that an experiment
    is a true null is therefore given by *fₐ / f(pᵢ),* and the probability that it
    is a true non-null is therefore *πᵢ = P(θᵢ != 0 | pᵢ) = 1 — fₐ / f(pᵢ)*, which
    we define as the *p*-*probability*. Note that this definition is two-sided: a
    large positive or negative observation will have equivalent *p*-probabilities,
    depending only on the magnitude of the effect.'
  prefs: []
  type: TYPE_NORMAL
- en: Empirical Density Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to implement this, we need an empirical density estimate. The standard
    go-to is the *binned density estimate*, defined by
  prefs: []
  type: TYPE_NORMAL
- en: 'where the *i*th bin is defined by *b_i = [i h , (i+1)h)*, for *i=0, n_b-1*,
    and *h=1/n_b* is the bin width. The bin-size suffers from the bias-variance tradeoff:
    if the bin size is too small, there is high variance, but if it is too large,
    there is high bias. The problem with this estimate is that there is a spike in
    probability near the boundary *p=0*. The bin size must be sufficiently small to
    capture the near-zero behavior, which leads to a suboptimal estimate for the rest
    of the unit interval, with high variance.'
  prefs: []
  type: TYPE_NORMAL
- en: We therefore propose an alternate empirical density, which we call the *nearest-neighbors
    density estimate:*
  prefs: []
  type: TYPE_NORMAL
- en: Here, NN*ₖ(x)* represents the *k* “nearest neighbors” to *x* in the set *{X₁,
    … Xₙ}*, and range represents the difference between the max and min of a set.
  prefs: []
  type: TYPE_NORMAL
- en: The binned estimate holds the bin sized constant and measures the variable count
    of data points within each bin, whereas the nearest-neighbors estimate holds the
    count fixed and measures the variable bin size.
  prefs: []
  type: TYPE_NORMAL
- en: In order to achieve this in code, we compute the nearest-neighbors density of
    *p*-values on a grid over *[0, 1]* with intervals of 0.001\. This is achieved
    in the code below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The numerically computed density function is plotted in the figure below, along
    with a normalized histogram of *p*-values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec8e22e6af95dd7d42daa3a9d9a4d1fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Histogram of p-values; empirical nearest-neighbors density function. Image by
    Author.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating False Discovery Rate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we use our *p*-probabilities to estimate the false discovery rate, using
    the calculations from earlier, as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This results in an estimated false-discovery rate of 27.6%, compared to an actual
    true rate of 27.0%.
  prefs: []
  type: TYPE_NORMAL
- en: Correcting Impact Estimates for False Discovery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We may also use our *p*-probabilities to control for false discovery.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here, we recompute the theoretical bias estimate, weighting each observation
    by its corresponding *p*-probability, yielding an approximate impact with a 3.6%
    error. (Much better than the earlier uncorrected estimate!) It also substantially
    improves our empirical observed impact estimate, which now results in only a 13.3%
    error.
  prefs: []
  type: TYPE_NORMAL
- en: Selection Bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Calibration for True Positives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get a sense for what is causing the selection bias, we can bin the data by
    observed impacts *X*, filter for true positives, and plot the actual impacts within
    each bin. This is shown in the calibration plot below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ff9a36eb039554681737f76d0ac0ee1.png)'
  prefs: []
  type: TYPE_IMG
- en: Theoretical (blue) and observed (red) average impacts, filtered for true positives,
    and binned by observed difference-in-means. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: Posterior Estimates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Given the population, it is no longer true that the expected value of *θᵢ*
    is the observed mean *Xᵢ*. To account for the population effect, we will instead
    consider the *posterior effect distribution* for each individual experiment *P(θᵢ
    | Xᵢ)*. For the *prior,* we will just use the best thing we have: the empirical
    distribution, adjusted for the *p*-probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: where *πᵢ* is the *p*-probability for the *i*th experiment. We therefore weight
    each observation point with the probability that it represents a true non-null,
    and then coalesce the probabilities of null across all experiments at the origin.
  prefs: []
  type: TYPE_NORMAL
- en: The posterior distribution of *θ* for the *i*th experiment can be expressed
    as
  prefs: []
  type: TYPE_NORMAL
- en: where *nₛ* is the sample size for that experiment. (Recall *Xᵢ* is the *average*
    over a sample of size *nₛ*.)
  prefs: []
  type: TYPE_NORMAL
- en: We can operationalize these formula in the EffectDistribution class, shown below.
    The __init__ method computes the prior, and the getUpdate method returns the posterior.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can now run through all of our positive significant results, computing both
    the new expected impact as well as a random sample from the posterior distribution,
    for each.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We call this procedure the *posterior-effect impact estimate*, as it is based
    on the posterior distribution for each individual experiment. Our final corrected
    estimate shows a 0.39% error, though the bootstrap confidence bounds are (-3.3%,
    4.0%). The bootstrap sample is shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8537e71911317983c73d2317271bbaa9.png)'
  prefs: []
  type: TYPE_IMG
- en: Posterior-Effect Bootstrap Estimate. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, we can look at the overall final calibration plot (not filtered for
    true positives!):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bcf9b9d1cc21e8e34ba3fe61e3f6dfef.png)'
  prefs: []
  type: TYPE_IMG
- en: Calibration plot, showing posterior-effect estimates (blue) and actual impacts
    (red), binned by observed values. Diagonal dashed line represents the uncorrected
    estimates X=X. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When running a large series of experiments, the true total impact is often biased
    due to *false discovery* and *selection*.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we saw that these biases are worsened by high fraction *λ* of true
    nulls*.* This shows that it is crucial to run *high-quality* experiments that
    have a greater chance of actually making a meaningful impact. The *run-everything-you-can-think-of-and-see-what-sticks*
    approach, on the other hand, will lead to high false-discovery rate and high bias.
    This challenge is harder for more mature products, where it is difficult to come
    up with new meaningful ideas, than newer products, where many changes may lead
    to product improvements.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to correct for these, one must account for the *totality* of all experiments
    ran, in a two-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the probability that each experiment is actually a non-null result,
    using the *p*-values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the posterior distribution of the effect for each positive significant
    result, using the *p*-probability corrected empirical distribution (of all results)
    as the prior.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In our simulation, the naive observed average impact resulted in a 45% error
    compared to the true effect; correction for the *p-*probabilities reduced this
    error to 13%, and the final posterior estimates further reduced the error to sub
    1%, with a confidence interval of ±3%.
  prefs: []
  type: TYPE_NORMAL
- en: Additional Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a more mathematical approach to this problem, check out [1]. This paper
    gives an explicit formula that corrects for overall bias in measuring the impact.
  prefs: []
  type: TYPE_NORMAL
- en: '[1] M.R. Lee and M. Shen, [Winner’s curse: bias estimation for total effects
    of features in online controlled experiments](https://dl.acm.org/doi/10.1145/3219819.3219905)
    (2018), KDD 18: Proceedings of the 24th ACM SIGKDD International Conference on
    Knowledge Discovery & Data Mining, July 2018, p. 491–499'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] M. Berk, [How to Measure Your Team’s A/B Testing Impact](https://medium.com/towards-data-science/how-to-measure-your-teams-a-b-testing-impact-f74c6f2b4660)
    (2021), *Towards Data Science,* June 2021.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images by the Author.*'
  prefs: []
  type: TYPE_NORMAL
