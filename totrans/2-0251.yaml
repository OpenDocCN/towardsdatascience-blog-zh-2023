- en: Achieving Greater Self-Consistency in Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/achieving-greater-self-consistency-in-large-language-models-6e6cb5f3c5b7](https://towardsdatascience.com/achieving-greater-self-consistency-in-large-language-models-6e6cb5f3c5b7)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@alcarazanthony1?source=post_page-----6e6cb5f3c5b7--------------------------------)[![Anthony
    Alcaraz](../Images/6a71a1752677bd07c384246fb0c7f7e8.png)](https://medium.com/@alcarazanthony1?source=post_page-----6e6cb5f3c5b7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6e6cb5f3c5b7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6e6cb5f3c5b7--------------------------------)
    [Anthony Alcaraz](https://medium.com/@alcarazanthony1?source=post_page-----6e6cb5f3c5b7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6e6cb5f3c5b7--------------------------------)
    ·8 min read·Dec 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*Artificial intelligence software was used to enhance the grammar, flow, and
    readability of this article’s text.*'
  prefs: []
  type: TYPE_NORMAL
- en: When LLMs are used to evaluate qualities like the correctness, accuracy, or
    relevance of a piece of text, consistency is paramount. If an LLM exhibits inconsistent
    judgements, then its evaluations become unreliable and untrustworthy.
  prefs: []
  type: TYPE_NORMAL
- en: If an LLM evaluates the reasoning quality of arguments, but contradicts itself
    by rating an invalid argument as more logically sound than a perfectly valid one,
    then it fails as an arbiter of reason. Its evaluations lose credibility due to
    the model’s own lack of logical consistency.
  prefs: []
  type: TYPE_NORMAL
- en: When such inconsistencies appear, there is no stable basis for comparison between
    the LLM’s assessments of different pieces of text. If the model arbitrarily contradicts
    itself, then sentences cannot be reliably ranked against one another based on
    the model’s inconsistent scorings.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, inconsistency destroys the grounds for comparison that evaluations
    aim to provide in the first place. If an LLM cannot demonstrate consistent application
    of assessment criteria, then using it to evaluate text loses all effectiveness
    and utility.
  prefs: []
  type: TYPE_NORMAL
- en: So, consistency in judgement and evaluation is mandatory for LLMs employed to
    score or judge textual qualities and features. Without a high level of stability
    in its assessments, grounded in a consistent understanding of concepts being evaluated,
    the basis for comparison falls apart when leveraging LLM output as a form of evaluation
    or scoring.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling multiple solutions reveals consistency between outputs strongly correlates
    with quality. However, existing consistency techniques rely on extracting and
    matching closed-form answers, restricting their applicability. This article explores
    methods to enhance self-consistency without such constraints, while also grounding
    decisions in real-world knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8f1bb2f51d372d80a08c389810cc3436.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '**The Need for Self-Consistency**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite rapid progress, logical failures and falsehoods continue hindering reliable
    reasoning in state-of-the-art models. For complex multi-step analysis or free-form
    generation, models often contradict themselves or invent unsupported facts.
  prefs: []
  type: TYPE_NORMAL
- en: This manifests in two key ways — inconsistent open-ended generation, and incoherent
    inferences. When performing open-ended tasks, models generate conflicting outputs
    when sampled multiple times on the same input. Meanwhile for chained reasoning,
    models draw irrational conclusions that violate basic transitive properties.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a model may determine A > B and B > C in a ranking comparison,
    but then inaccurately assess C > A, resulting in circular contradictions. Such
    failures to preserve transitive coherence fundamentally undermine reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques like self-consistency help address inconsistency by sampling multiple
    candidate solutions and selecting outputs that display consensus. The key insight
    is that consensus acts as an effective proxy measure for quality and coherence.
  prefs: []
  type: TYPE_NORMAL
- en: However, existing self-consistency approaches rely on strict answer formats
    to allow extraction and comparison across responses. This significantly restricts
    their applicability to open-ended tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Advancing internal self-consistency requires a two-pronged approach. First,
    consistency techniques like USC eliminate format constraints to enable open-ended
    selection. Meanwhile, contrastive ranking methods enforce logical constraints
    on latent representations, ensuring models preserve ordering relationships during
    multi-step inference.
  prefs: []
  type: TYPE_NORMAL
- en: However, solely enhancing internal consistency without grounding in structured
    external knowledge remains insufficient for accurate reasoning. Language models
    lack the dynamic updates, logical expressiveness, and empirical verifiability
    crucially needed to inform interpretations.
  prefs: []
  type: TYPE_NORMAL
- en: Advancing reliability therefore necessitates improving self-consistency across
    diverse tasks while integrating structured repositories of world knowledge. Consistency
    techniques should operate flexibly across free-form outputs, coupled with frameworks
    leveraging rich semantic representations to inform decisions with logic-driven,
    externally-verified facts.
  prefs: []
  type: TYPE_NORMAL
- en: This combination of model-internal consensus and structured external grounding
    complements strengths of each approach to replicate multifaceted human reasoning.
    Only their orchestration can overcome inherent limitations, incrementally advancing
    AI capabilities towards matching diverse and fluid cognition.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Universal Self-Consistency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://arxiv.org/abs/2311.17311?source=post_page-----6e6cb5f3c5b7--------------------------------#:~:text=Universal%20Self%2DConsistency%20for%20Large%20Language%20Model%20Generation,-Xinyun%20Chen%2C%20Renat&text=Self%2Dconsistency%20with%20chain%2Dof,large%20language%20models%20%28LLMs%29.)
    [## Universal Self-Consistency for Large Language Model Generation'
  prefs: []
  type: TYPE_NORMAL
- en: Self-consistency with chain-of-thought prompting (CoT) has demonstrated remarkable
    performance gains on various…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: arxiv.org](https://arxiv.org/abs/2311.17311?source=post_page-----6e6cb5f3c5b7--------------------------------#:~:text=Universal%20Self%2DConsistency%20for%20Large%20Language%20Model%20Generation,-Xinyun%20Chen%2C%20Renat&text=Self%2Dconsistency%20with%20chain%2Dof,large%20language%20models%20%28LLMs%29.)
  prefs: []
  type: TYPE_NORMAL
- en: Chen 2023 et al. proposes Universal Self-Consistency (USC) to enable self-consistency
    without answer extraction across diverse applications. USC relies on the language
    model’s own capability to select the most consistent response from multiple candidate
    solutions it originally produced.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, USC concatenates all sampled responses into a context, then constructs
    a prompt asking the model to choose the one with highest consensus. Eliminating
    specialized aggregation logic, USC is applicable even to free-form generation
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Experiments demonstrate USC matches performance of standard self-consistency
    on mathematical reasoning and code generation benchmarks which permit answer extraction.
    Crucially, USC also improves open-ended question answering, summarization, and
    creative writing where existing techniques falter.
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting Reasoning with External Knowledge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While USC offers framework-agnostic consistency, external knowledge remains
    vital for accurate and robust reasoning. Language models lack the dynamic updates,
    factual integrity, and logical expressiveness of curated knowledge repositories.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge graphs allow incorporating empirically-grounded details and interlinked
    factual relations, enabling more consistent interpretation of situations per the
    latest information. They facilitate accessing contextual empirical evidence to
    substantiate decisions instead of relying solely on innate biases embedded in
    model weights.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, knowledge graphs manage the evolution of facts over time, ensuring
    reasoning relies on the most up-to-date information. They also encapsulate domain
    logic to explicitly encode rules, constraints, and ontological taxonomies. This
    allows sound deductive reasoning undiscernible from model training data alone.
  prefs: []
  type: TYPE_NORMAL
- en: Empirically, retrieval-augmented generation using external knowledge graphs
    demonstrates more consistent output by grounding responses in verified facts rather
    than hallucinations. Parallel querying of knowledge graphs — even duplicate copies
    — further improves consistency by accumulating evidence from multiple perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestrating USC’s structured reasoning with a retrieval-augmented system leverages
    external knowledge to engender a modular hybrid architecture. USC contributes
    the reasoning “backbone” while parallel retrieval supplies relevant factual details
    from trustworthy knowledge repositories, enhancing interpretation consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting Consistency with Seeded Sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://github.com/openai/openai-cookbook/blob/main/examples/Deterministic_outputs_with_the_seed_parameter.ipynb?source=post_page-----6e6cb5f3c5b7--------------------------------)
    [## openai-cookbook/examples/Deterministic_outputs_with_the_seed_parameter.ipynb
    at main ·…'
  prefs: []
  type: TYPE_NORMAL
- en: Examples and guides for using the OpenAI API. Contribute to openai/openai-cookbook
    development by creating an account…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/openai/openai-cookbook/blob/main/examples/Deterministic_outputs_with_the_seed_parameter.ipynb?source=post_page-----6e6cb5f3c5b7--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: In open-ended generation tasks, consistency between solutions strongly correlates
    with quality. However, large language models exhibit inherent randomness that
    produces variation across outputs.
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate this, OpenAI’s Chat Completions API provides a seed parameter that
    seeds the random number generator for deterministic sampling. Using the same seed
    and parameters will yield identical or very similar outputs each time.
  prefs: []
  type: TYPE_NORMAL
- en: This allows combining Universal Self-Consistency, which has the model select
    the most consistent response from candidates, with seeded sampling to evaluate
    the exact same responses across requests.
  prefs: []
  type: TYPE_NORMAL
- en: There remains a small chance of variation from non-determinism in computers.
    Additionally, changes to the system_fingerprint indicate backend updates affecting
    determinism across requests.
  prefs: []
  type: TYPE_NORMAL
- en: Besides seeding, factors like lower temperature and careful prompt engineering
    also reduce variability in model-generated text. Overall, seeded sampling lets
    us harness consistency for improved reliability.
  prefs: []
  type: TYPE_NORMAL
- en: By choreographing seeded candidates with USC-based selection, we construct solutions
    methodically grounded in both external knowledge and sampling determinism. This
    manifests emergent capabilities surpassing individual techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Contrastive-consistent ranking (CCR)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While previous sections focus on classification and open-ended generation, consistency
    in rankings elicited from language models is also vital. Recent work explores
    contrast-consistent ranking (CCR), which adapts Contrast-Consistent Search (CCS)
    to impose logical constraints on mapping item representations to a consistent
    scale.
  prefs: []
  type: TYPE_NORMAL
- en: CCR probes item vectors to find this consistent ranking direction without supervision.
    Experiments on prompting baselines and CCR variants demonstrate improved consistency
    and generalization.
  prefs: []
  type: TYPE_NORMAL
- en: By extending consistency-based techniques like USC to rankings, CCR offers a
    way to limit unpredictability and align model outputs across diverse tasks. Both
    aim to improve reliability by selecting solutions or rankings judged internally
    coherent by the model itself.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://arxiv.org/abs/2309.06991?source=post_page-----6e6cb5f3c5b7--------------------------------)
    [## Unsupervised Contrast-Consistent Ranking with Language Models'
  prefs: []
  type: TYPE_NORMAL
- en: Language models contain ranking-based knowledge and are powerful solvers of
    in-context ranking tasks. For instance…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: arxiv.org](https://arxiv.org/abs/2309.06991?source=post_page-----6e6cb5f3c5b7--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: CCR probing trains an additional “probe” model to find a latent ranking direction
    in the vector space of a fixed language model.
  prefs: []
  type: TYPE_NORMAL
- en: It does not directly prompt or finetune the language model itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The language model is used to simply generate vector representations of items
    through input prompts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These item vectors are then fed into the CCR probe, which is trained on an unsupervised
    loss function to map representations onto a consistent ranking scale.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So in summary, CCR probing introduces an external probe model that wraps the
    language model to perform ranking in its vector space. The language model remains
    static. Only the additional probe model is trained with contrastive objectives
    to uncover inherent rankings.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](/achieving-structured-reasoning-with-llms-in-chaotic-contexts-with-thread-of-thought-prompting-and-a4b8018b619a?source=post_page-----6e6cb5f3c5b7--------------------------------)
    [## Achieving Structured Reasoning with LLMs in Chaotic Contexts with Thread of
    Thought Prompting and…'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) demonstrated impressive few-shot learning capabilities,
    rapidly adapting to new tasks with…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/achieving-structured-reasoning-with-llms-in-chaotic-contexts-with-thread-of-thought-prompting-and-a4b8018b619a?source=post_page-----6e6cb5f3c5b7--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: We implement a RAG system accessing multiple knowledge graphs indexed with vector
    search for fast fact retrieval. Query engines interface the indexes and encapsulate
    passage search. Helper tools wrap engines, facilitating integration. Separate
    agents house tools, interfacing the LLM. A super-agent oversees tool coordination.
  prefs: []
  type: TYPE_NORMAL
- en: The system leverages Thread-of-Thought (ToT) prompting for structured reasoning
    over retrieved passages. ToT guides the model through step-wise analysis, enhancing
    understanding. Parallel asynchronous retrieval allows simultaneously querying
    all graphs, accelerating context accumulation.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal knowledge graphs using diverse algorithms and embeddings provide
    varied perspectives. Personalized PageRank traversal supports flexible inference
    along indirect connections. Approximate nearest neighbor search enables efficient
    lookups. Embeddings enable analogical reasoning via vector arithmetic.
  prefs: []
  type: TYPE_NORMAL
- en: We implement a staged retrieval approach, first querying a domain ontology to
    establish basic concepts and terminology related to the question. Ontologies provide
    formal definitions and high-level relationships between entities, allowing grounded
    understanding before retrieving full knowledge graphs.
  prefs: []
  type: TYPE_NORMAL
- en: The ontology results are appended to the original question to provide context.
    This enhanced query is then used to retrieve passages from the multiple knowledge
    graphs. Seeding retrieval with ontology information primes the RAG system for
    more consistent and relevant passages tailored to the grounded aspects.
  prefs: []
  type: TYPE_NORMAL
- en: This technique combines top-down hierarchical reasoning from the ontology with
    complementary bottom-up factual details from knowledge graphs. The ontology clarifies
    ambiguous or broad terminology, while knowledge graphs provide in-depth relational
    information on refined query focuses.
  prefs: []
  type: TYPE_NORMAL
- en: Choreographing different knowledge sources in this staged manner allows smoothly
    transitioning between levels of abstraction for reinforced consistency. Retrieval
    flows from high-level ontology concepts down to specific contextual passages pertinent
    for the question.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Impact
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Combining USC and RAG complements consistency-based thinking with grounding
    in external knowledge. USC contributes the reasoning structure while RAG expands
    information breadth. Together these offset LLM limitations to better replicate
    human cognition.
  prefs: []
  type: TYPE_NORMAL
- en: This orchestration also enhances accuracy, speed, and coverage. Retrieved facts
    fill knowledge gaps for sound decisions. Parallel knowledge access accelerates
    understanding. Different knowledge graphs broaden conceptual connections considered.
  prefs: []
  type: TYPE_NORMAL
- en: Through modular augmentation, we gracefully scale emergent capabilities beyond
    inherent model aptitudes. As LLMs and knowledge bases mature, this composable
    paradigm will facilitate progressively advancing AI reasoning abilities.
  prefs: []
  type: TYPE_NORMAL
