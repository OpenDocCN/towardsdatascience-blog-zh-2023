["```py\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\n\nfrom airflow.models import DAG\nfrom airflow.operators.dummy import DummyOperator\n\nwith DAG(\n    catchup=False,\n    dag_id='my_daily_dag'\n    start_date=datetime(2023, 7, 26),\n    default_args={\n        'owner': 'airflow',\n        'retries': 1,\n        'retry_delay': timedelta(minutes=2),\n    },\n    schedule_interval='0 5 * * *',\n    max_active_runs=1,\n) as dag:\n   DummyOperator(task_id='dummy_task')\n```", "```py\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\n\nfrom airflow.models import DAG\nfrom airflow.operators.dummy import DummyOperator\n\nwith DAG(\n    catchup=False,\n    dag_id='my_daily_dag'\n    start_date=datetime(2023, 7, 26),\n    default_args={\n        'owner': 'airflow',\n        'retries': 1,\n        'retry_delay': timedelta(minutes=2),\n    },\n    schedule_interval='0 6-20 * * *',  # At :00 every hour between 6AM-8PM\n    max_active_runs=1,\n) as dag:\n   DummyOperator(task_id='dummy_task')\n```", "```py\n24 + (hourly_dag_interval_start_hour - 5)\n```", "```py\nfrom datetime import datetime, timezone\n\ndatetime.now(timezone.utc).hour - 1\n```", "```py\nexecution_delta=timedelta(hours=24 + datetime.now(timezone.utc).hour - 1 - 5)\n```", "```py\nfrom datetime import datetime, timedelta, timezone\nfrom pathlib import Path\n\nfrom airflow.models import DAG\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.sensors.external_task import ExternalTaskSensor\n\nwith DAG(\n    catchup=False,\n    dag_id='my_daily_dag'\n    start_date=datetime(2023, 7, 26),\n    default_args={\n        'owner': 'airflow',\n        'retries': 1,\n        'retry_delay': timedelta(minutes=2),\n    },\n    schedule_interval='0 6-20 * * *',  # At :00 every hour between 6AM-8PM\n    max_active_runs=1,\n) as dag:\n    sensor_task = ExternalTaskSensor(\n        task_id='daily_dag_completed_successfully',\n        external_dag_id='my_daily_dag',\n        soft_fail=True,\n        check_existence=True,\n        execution_delta=timedelta(hours=24 + datetime.now(timezone.utc).hour - 1 - 5),\n        poke_interval=30,\n        timeout=120,\n    )\n\n    dummy_task = DummyOperator(task_id='dummy_task')\n\n    sensor_task >> dummy_task\n```", "```py\nimport logging \n\nfrom airflow.exceptions import AirflowSkipException\nfrom airflow.models import DagRun\n\ndef get_most_recent_dag_run(current_logical_dt):\n    dag_id = 'my_daily_dag'\n    # Get the historical DagRuns of the daily DAG\n    dag_runs = DagRun.find(dag_id=dag_id)\n\n    # Sort DagRuns on descending order such that the first element\n    # in the list, corresponds to the latest DagRun of the daily DAG\n    dag_runs.sort(key=lambda x: x.execution_date, reverse=True)\n\n    # If the daily DAG was not executed ever before, simply raise an \n    # exception to skip. \n    if not dag_runs:\n        logging.info(f'No DAG runs found for {dag_id}. Skipping..')\n        raise AirflowSkipException\n\n    # Get the latest DagRun of the daily DAG\n    latest_daily_dag_run = dag_runs[0]\n\n    # Subtract one day from hourly's DAG current execution_date in order to \n    # align with the daily DAG's scedule\n    current_logical_dt_yesterday = current_logical_dt.subtract(hours=24)\n\n    # if year/month/day of daily's DAG execution_date and hourly's DAG execution_date\n    # (minus one day) are the same, it means the daily DAG was executed today. \n    # We therefore return the execution_date of the latest daily DagRun. \n    # It's state (i.e. if successful) will be handled by the sensor and the configuration \n    # we provide to it. \n    if (\n        current_logical_dt_yesterday.day == latest_daily_dag_run.execution_date.day\n        and current_logical_dt_yesterday.month == latest_daily_dag_run.execution_date.month\n        and current_logical_dt_yesterday.year == latest_daily_dag_run.execution_date.year\n    ):\n        logging.info(f'DAG run was found for {dag_id} today.')\n        return latest_daily_dag_run.execution_date\n\n    # Alternatively, return the current execution_date of the hourly DAG\n    # This is the default value the sensor would otherwise use, and essentially\n    # it means that the sensor won't be triggered given that the intervals between \n    # the daily DAG and the sensor won't align. \n    return current_logical_dt\n```", "```py\nimport logging\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\n\nfrom airflow.exceptions import AirflowSkipException\nfrom airflow.models import DAG, DagRun\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.sensors.external_task import ExternalTaskSensor\n\ndef get_most_recent_dag_run(current_logical_dt):\n    dag_id = 'my_daily_dag'\n    # Get the historical DagRuns of the daily DAG\n    dag_runs = DagRun.find(dag_id=dag_id)\n\n    # Sort DagRuns on descending order such that the first element\n    # in the list, corresponds to the latest DagRun of the daily DAG\n    dag_runs.sort(key=lambda x: x.execution_date, reverse=True)\n\n    # If the daily DAG was not executed ever before, simply raise an \n    # exception to skip. \n    if not dag_runs:\n        logging.info(f'No DAG runs found for {dag_id}. Skipping..')\n        raise AirflowSkipException\n\n    # Get the latest DagRun of the daily DAG\n    latest_daily_dag_run = dag_runs[0]\n\n    # Subtract one day from hourly DAG's current execution_date in order to \n    # align with the daily DAG's scedule\n    current_logical_dt_yesterday = current_logical_dt.subtract(hours=24)\n\n    # if year/month/day of daily DAG's execution_date and hourly DAG's execution_date\n    # (minus one day) are the same, it means the daily DAG was executed today. \n    # We therefore return the execution_date of the latest daily DagRun. \n    # It's state (i.e. if successful) will be handled by the sensor and the configuration \n    # we provide to it. \n    if (\n        current_logical_dt_yesterday.day == latest_daily_dag_run.execution_date.day\n        and current_logical_dt_yesterday.month == latest_daily_dag_run.execution_date.month\n        and current_logical_dt_yesterday.year == latest_daily_dag_run.execution_date.year\n    ):\n        logging.info(f'DAG run was found for {dag_id} today.')\n        return latest_daily_dag_run.execution_date\n\n    # Alternatively, return the current execution_date of the hourly DAG\n    # This is the default value the sensor would otherwise use, and essentially\n    # it means that the sensor won't be triggered given that the intervals between \n    # the daily DAG and the sensor won't align. \n    return current_logical_dt\n\nwith DAG(\n    catchup=False,\n    dag_id='my_daily_dag'\n    start_date=datetime(2023, 7, 26),\n    default_args={\n        'owner': 'airflow',\n        'retries': 1,\n        'retry_delay': timedelta(minutes=2),\n    },\n    schedule_interval='0 6-20 * * *',  # At :00 every hour between 6AM-8PM\n    max_active_runs=1,\n) as dag:\n    sensor_task = ExternalTaskSensor(\n        task_id='daily_dag_completed_successfully',\n        external_dag_id='my_daily_dag',\n        soft_fail=True,\n        check_existence=True,\n        execution_function_fn=get_most_recent_dag_run,\n        poke_interval=30,\n        timeout=120,\n    )\n\n    dummy_task = DummyOperator(task_id='dummy_task')\n\n    sensor_task >> dummy_task\n```", "```py\nfrom airflow.exceptions import AirflowSkipException\nfrom airflow.models import DagRun\nfrom airflow.utils.state import DagRunState\n\ndef check_daily_dag_success_today(**kwargs):\n    dag_id = 'my_daily_dag'\n    # Get the historical DagRuns of the daily DAG\n    dag_runs = DagRun.find(dag_id=dag_id)\n\n    # Sort DagRuns on descending order such that the first element\n    # in the list, corresponds to the latest DagRun of the daily DAG\n    dag_runs.sort(key=lambda x: x.execution_date, reverse=True)\n\n    # If the daily DAG was not executed ever before, simply raise an\n    # exception to skip.\n    if not dag_runs:\n        logging.info(f'No DAG runs found for {dag_id}. Skipping..')\n        raise AirflowSkipException\n\n    # Get the latest DagRun of the daily DAG\n    latest_daily_dag_run = dag_runs[0]\n\n    # Subtract one day from hourly DAG's current execution_date in order to\n    # align with the daily DAG's schedule\n    data_interval_start = kwargs['data_interval_start']\n    data_interval_start_yesterday = data_interval_start.subtract(hours=24)\n\n    # Check the intervals and the success of the daily DAg's DagRun. If conditions are not met,\n    # DAG run should be skipped.\n    if not (\n        latest_daily_dag_run.state == DagRunState.SUCCESS\n        and data_interval_start_yesterday.day == latest_daily_dag_run.execution_date.day\n        and data_interval_start_yesterday.month == latest_daily_dag_run.execution_date.month\n        and data_interval_start_yesterday.year == latest_daily_dag_run.execution_date.year\n    ):\n        logging.info(f'No successful DAG run was found for {dag_id} today. Skipping..')\n        raise AirflowSkipException\n\n    logging.info(f'Successful DAG run was found for {dag_id} today.')\n```", "```py\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\n\nfrom airflow.exceptions import AirflowSkipException\nfrom airflow.models import DAG, DagRun\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.operators.python import PythonOperator\n\ndef check_daily_dag_success_today(**kwargs):\n    dag_id = 'my_daily_dag'\n    # Get the historical DagRuns of the daily DAG\n    dag_runs = DagRun.find(dag_id=dag_id)\n\n    # Sort DagRuns on descending order such that the first element\n    # in the list, corresponds to the latest DagRun of the daily DAG\n    dag_runs.sort(key=lambda x: x.execution_date, reverse=True)\n\n    # If the daily DAG was not executed ever before, simply raise an\n    # exception to skip.\n    if not dag_runs:\n        logging.info(f'No DAG runs found for {dag_id}. Skipping..')\n        raise AirflowSkipException\n\n    # Get the latest DagRun of the daily DAG\n    latest_daily_dag_run = dag_runs[0]\n\n    # Subtract one day from hourly DAG's current execution_date in order to\n    # align with the daily DAG's schedule\n    data_interval_start = kwargs['data_interval_start']\n    data_interval_start_yesterday = data_interval_start.subtract(hours=24)\n\n    # Check the intervals and the success of the daily DAg's DagRun. If conditions are not met,\n    # DAG run should be skipped.\n    if not (\n        latest_daily_dag_run.state == DagRunState.SUCCESS\n        and data_interval_start_yesterday.day == latest_daily_dag_run.execution_date.day\n        and data_interval_start_yesterday.month == latest_daily_dag_run.execution_date.month\n        and data_interval_start_yesterday.year == latest_daily_dag_run.execution_date.year\n    ):\n        logging.info(f'No successful DAG run was found for {dag_id} today. Skipping..')\n        raise AirflowSkipException\n\n    logging.info(f'Successful DAG run was found for {dag_id} today.')\n\nwith DAG(\n    catchup=False,\n    dag_id='my_daily_dag'\n    start_date=datetime(2023, 7, 26),\n    default_args={\n        'owner': 'airflow',\n        'retries': 1,\n        'retry_delay': timedelta(minutes=2),\n    },\n    schedule_interval='0 6-20 * * *',  # At :00 every hour between 6AM-8PM\n    max_active_runs=1,\n) as dag:\n   check_task = PythonOperator(\n       task_id='check_daily_dag', \n       python_callable=check_daily_dag_success_today,\n   )\n   dummy_task = DummyOperator(task_id='dummy_task')\n\n   check_task >> dummy_task\n```"]