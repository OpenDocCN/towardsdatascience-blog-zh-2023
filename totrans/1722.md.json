["```py\ndata_pipelines = ['p1','p2','p3']\nprocessed_tables = []\nfor table in data_pipelines:\n    processed_tables.append(table)\n```", "```py\nprocessed_tables = [table for table in data_pipelines]\n```", "```py\ndef etl(item):\n    # Do some data transformation here\n    return json.dumps(item)\n\ndata = u\"\\n\".join(etl(item) for item in json_data)\n```", "```py\n import io\nimport json\n\ndef etl(item):\n    return json.dumps(item)\n\n# Text file loaded as a blob\nblob = \"\"\"\n        [\n{\"id\":\"1\",\"first_name\":\"John\"},\n{\"id\":\"2\",\"first_name\":\"Mary\"}\n]\n\"\"\"\njson_data = json.loads(blob)\ndata_str = u\"\\n\".join(etl(item) for item in json_data)\n\nprint(data_str)\ndata_file = io.BytesIO(data_str.encode())\n\n# This data file is ready for BigQuery as Newline delimited JSON\nprint(data_file)\n```", "```py\n{\"id\": \"1\", \"first_name\": \"John\"}\n{\"id\": \"2\", \"first_name\": \"Mary\"}\n<_io.BytesIO object at 0x10c732430>\n```", "```py\nfor line in open('very_big_file.csv'):\n    validate_schema(line)\n\n# or the same using list comprehension:\ndata_errors = [validate_schema(line) for line in open('very_big_file.csv')]\n```", "```py\n# Create a file first: ./very_big_file.csv as:\n# transaction_id,user_id,total_cost,dt\n# 1,John,10.99,2023-04-15\n# 2,Mary, 4.99,2023-04-12\n\n# Example.py\ndef etl(item):\n    # Do some etl here\n    return item.replace(\"John\", '****') \n\n# Create a generator \ndef batch_read_file(file_object, batch_size=19):\n    \"\"\"Lazy function (generator) can read a file in chunks.\n    Default chunk: 1024 bytes.\"\"\"\n    while True:\n        data = file_object.read(batch_size)\n        if not data:\n            break\n        yield data\n# and read in chunks\nwith open('very_big_file.csv') as f:\n    for batch in batch_read_file(f):\n        print(etl(batch))\n\n# In command line run\n# Python example.py \n```", "```py\ntransaction_id,user\n_id,total_cost,dt\n1\n,****,10.99,2023-04\n-15\n2,Mary, 4.99,20\n23-04-12\n```", "```py\nimport io\nimport re\ndef etl(item):\n    print(f'Transforming item: {item}')\n    return item.replace(\"John\", '****')\n\n# Helper function to split our text file into chunks\n# using separator\ndef splitStr(string, sep=\"\\s+\"):\n    if sep=='':\n        return (c for c in string)\n    else:\n        return (_.group(1) for _ in re.finditer(f'(?:^|{sep})((?:(?!{sep}).)*)', string))\n\n# Text file loaded as a blob\nblob = \"\"\"transaction_id,user_id,total_cost,dt\n1,John,10.99,2023-04-15\n2,Mary, 4.99,2023-04-12\n\"\"\"\n\n# data = blob.split(\"\\n\") # We wouldn't want to do this on large datasets \n# as it would require to load big data file as a whole in the first place\n# consuming lots of memory\n\n# We would want to use our generator helper function\n# and process data in chunks\ndata = splitStr(blob, sep='\\n')\ndata_str = u\"\\n\".join(etl(item) for item in data)\n\nprint('New file contents:')\nprint(data_str)\ndata_file = io.BytesIO(data_str.encode())\n\nprint('This data file is ready for BigQuery:')\nprint(data_file) \n```", "```py\npython example.py                                  ✔  48  19:52:06  dataform_env\nTransforming item: transaction_id,user_id,total_cost,dt\nTransforming item: 1,John,10.99,2023-04-15\nTransforming item: 2,Mary, 4.99,2023-04-12\nTransforming item:\nNew file contents:\ntransaction_id,user_id,total_cost,dt\n1,****,10.99,2023-04-15\n2,Mary, 4.99,2023-04-12\n\nThis data file is ready for BigQuery:\n<_io.BytesIO object at 0x103672980>\n```", "```py\nclass ConnectionDataRecord(object):\n    def __init__(self, user, ts):\n        self.user = user\n        self.ts = ts\n\n    @property\n    def user(self):\n        return self._user\n\n    @description.setter\n    def user(self, d):\n        if not d: raise Exception(\"user cannot be empty\")\n        self._user = d\n\n    @property\n    def ts(self):\n        return self._ts\n\n    @value.setter\n    def ts(self, v):\n        if not (v > 0): raise Exception(\"value must be greater than zero\")\n        self._ts = v\n```", "```py\nfrom pydantic import BaseModel\n\nclass ConnectionDataRecord(BaseModel):\n    user: str\n    ts: int\n\nrecord = ConnectionDataRecord(user=\"user1\", ts=123456789)\n```", "```py\ndef etl_decorator(func):\n    def wrapper():\n        result = func()\n        return f'Processing {result}' \n    return wrapper\n\n@etl_decorator\ndef unzip_data():\n    return \"unzipped data\"\n\nprint(unzip_data())  # Output: Processing unzipped data\n```", "```py\n@dag(default_args=default_args, tags=['etl'])\ndef etl_pipeline():\n\n    @task()\n    def extract():\n        return json.loads(data_string)    \n    @task(multiple_outputs=True)\n    def transform(order_data_dict: dict):\n        return {\"total_count\": len(order_data_dict)}    \n    @task()\n    def load(total_order_value: float):\n        print(f\"Total order value is: {total_count}\")    \n\n    extracted = extract()\n    transformed = transform(extracted)\n    load(transformed[\"total_count\"])\n```", "```py\nresponse = requests.get('https://api.nasa.gov/neo/rest/v1/feed?start_date=2015-09-07&end_date=2015-09-08&api_key=your_api_key'))\nprint(response.json())\n```", "```py\nimport requests\nsession = requests.Session()\n\nurl=\"https://api.nasa.gov/neo/rest/v1/feed\"\napiKey=\"your_api_key\"\nrequestParams = {\n    'api_key': apiKey,\n    'start_date': '2023-04-20',\n    'end_date': '2023-04-21'\n}\nresponse = session.get(url, params = requestParams, stream=True)\nprint(response.status_code)\n```", "```py\nfrom google.cloud import dataform_v1beta1\nimport requests\nimport google.auth.transport.requests\nfrom google.oauth2 import service_account\n...\n# Get Dataform and BigQuery credentials from encrypted file:\nprint(f'Getting BigQuery credentials from encrypted file...')\ncredentials = service_account.Credentials.from_service_account_file(\n    './../credentials.json'\n    , scopes=['https://www.googleapis.com/auth/cloud-platform'])\n\ndef create_dataform_workflow_config(credentials, id, workflow_config, repository_id):\n    '''\n    The function will create a schedule (workflow) in Dataform configs by making a direct API call\n    and using request_params with requests module\n    , i.e.\n    https://cloud.google.com/dataform/reference/rest/v1beta1/projects.locations.repositories.workflowConfigs/create\n    https://cloud.google.com/dataform/reference/rest/v1beta1/projects.locations.repositories.workflowConfigs#WorkflowConfig\n    If successful will create a workflow:\n    {'name': 'projects/my-project-data-staging/locations/us-central1/repositories/dataform-poc/workflowConfigs/test_live_20230831', 'releaseConfig': 'projects/my-project-data-staging/locations/us-central1/repositories/dataform-poc/releaseConfigs/staging', 'invocationConfig': {'includedTags': ['test']}, 'cronSchedule': '40 13 * * *', 'timeZone': 'Africa/Abidjan'}\n\n    If the workflow exists an error will be sent:\n    {'error': {'code': 409, 'message': \"Resource 'projects/123456789/locations/us-central1/repositories/dataform-poc/workflowConfigs/test_live_20230831' already exists\", 'status': 'ALREADY_EXISTS', 'details': [{'@type': 'type.googleapis.com/google.rpc.ResourceInfo', 'resourceName': 'projects/123456789/locations/us-central1/repositories/dataform-poc/workflowConfigs/test_live_20230831'}]}}\n\n    Accepts workflow_config as request_body, i.e.\n     request_body = {\n             # \"name\": \"projects/123456789/locations/us-central1/repositories/dataform-poc/workflowConfigs/test_live_20230830\",\n             \"releaseConfig\": \"projects/my-project-data-staging/locations/us-central1/repositories/dataform-poc/releaseConfigs/staging\",\n             \"invocationConfig\": {\n                 \"includedTags\": [\n                     \"test\"\n                 ]\n             },\n             \"cronSchedule\": \"40 13 * * *\",\n             \"timeZone\": \"Africa/Abidjan\"\n     }\n    '''\n    request = google.auth.transport.requests.Request()\n    credentials.refresh(request)\n\n    print('Creating a workflow...')\n    # Make the request\n    try:\n        session = requests\n        url=f'https://dataform.googleapis.com/v1beta1/projects/123456789/locations/us-central1/repositories/{repository_id}/workflowConfigs/'\n\n        headers = {\n            \"Authorization\": \"Bearer \" + credentials.token,\n            \"Content-Type\" : \"application/json; charset=utf-8\"\n\n        }\n        query_params = {\n            \"workflowConfigId\": id\n        }\n        request_body = workflow_config\n\n        page_result = session.post(url, params=query_params, json=request_body, headers=headers)\n        print(page_result.json())\n    except Exception as e:\n        print(e)\n```", "```py\nimport time\nimport requests\n\ndef etl_data_generator():\n    yield b\"Foo\"\n    time.sleep(3)\n    yield b\"Bar\"\n\nrequests.post(\"http://some.api.endpoint\", data=etl_data_generator())\n```", "```py\nfrom ratelimit import limits\nimport requests\nCALLS = 10\nTIME_PERIOD = 900   # time period in seconds\n\n@limits(calls=CALLS, period=TIME_PERIOD)\ndef call_api():\n    response = requests.get('https://api.example.com/data')\n    if response.status_code != 200:\n        raise Exception('API response: {}'.format(response.status_code))\n    return response.json()\n```", "```py\nimport requests\n\ndef pull_data(url, requestParams):\n    return requests.get(url, params = requestParams, stream=True)\n\nfor table in api_endpoints_list:\n    data = pull_data(table.api_url, table.requestParams)\n    etl(data)\n```", "```py\nimport asyncio\nimport aiohttp\n\nasync def pull_data(session, url, requestParams):\n  async with session.get(url, params = requestParams, stream=True) as response:\n    return await response\n\nasync def main():\n    async with aiohttp.ClientSession() as session:\n        tasks = [pull_data(session, url, requestParams) for table in api_endpoints_list:]\n        tasks_data = await asyncio.gather(*tasks)\n        for data in tasks_data:\n            etl(task_data)\n\nasyncio.run(main())\n```", "```py\nasync def etl():\n    job1 = asyncio.create_task(perform_etl_script1())\n    job2 = asyncio.create_task(read_s3_data())\n    job3 = asyncio.create_task(upload_s3_data())\n\n    await job2\n    await job1\n    await job3\n```", "```py\nimport math \nnumbers = [10,20]\nfactorials = list(map(lambda i: math.factorial(int(math.sqrt(i**3))), numbers))\nprint(factorials)\n# Output:\n# [8222838654177922817725562880000000, 16507955160908461081216919262453619309839666236496541854913520707833171034378509739399912570787600662729080382999756800000000000000000000]\n```", "```py\n numbers = [10,21]\neven_numbers = list(filter(lambda i: i% 2 == 0, numbers))\nprint(even_numbers)\n# Output:\n# [10]\n```", "```py\nbatchsize = 10 ** 5\nwith pd.read_csv(filename, chunksize=batchsize) as reader:\n    for batch in reader:\n        etl(batch)\n```", "```py\nbatch_data=pd.read_table('recommendation_data.csv',chunksize=100000,sep=';',\\\n       names=['group','user','rating','date','id'],index_col='id',\\\n       header=None,parse_dates=['date'])\n\ndf=pd.DataFrame()\n%time df=pd.concat(batch.groupby(['group','user',batch['date'].map(lambda x: x.year)])['rating'].agg(['sum']) for batch in batch_data)\n```", "```py\nfrom joblib import dump, load\nimport os\nimport numpy as np\nimport joblib\n\nfilename = os.path.join(savedir, 'test.joblib')\nto_persist = [('foo', [1, 2, 3]), ('bar', np.arange(5))]\n\n# Save a model\njoblib.dump(to_persist, filename)  \n# ['...test.joblib']\n\n# Load a model\njoblib.load(filename)\n# [('foo', [1, 2, 3]), ('bar', array([0, 1, 2, 3, 4]))]\n```", "```py\n # WRITE\nwith open(filename, 'wb') as fo:\n   joblib.dump(model, fo)\n\n# READ\nwith open(filename, 'rb') as fo:  \n   model = joblib.load(fo)\n```", "```py\nimport tempfile\nimport boto3\nimport joblib\n\ns3_client = boto3.client('s3')\nbucket_name = \"my-bucket\"\nkey = \"model.pkl\"\n\n# WRITE\nwith tempfile.TemporaryFile() as fp:\n    joblib.dump(model, fp)\n    fp.seek(0)\n    s3_client.put_object(Body=fp.read(), Bucket=bucket_name, Key=key)\n\n# READ\nwith tempfile.TemporaryFile() as fp:\n    s3_client.download_fileobj(Fileobj=fp, Bucket=bucket_name, Key=key)\n    fp.seek(0)\n    model = joblib.load(fp)\n\n# DELETE\ns3_client.delete_object(Bucket=bucket_name, Key=key)\n```", "```py\nimport time \nfrom joblib import Parallel,delayed \nimport math \n\nt1 = time.time() \n\n# Normal \nresults = [math.factorial(int(math.sqrt(i**3))) for i in range(1000,2000)] \n\nt2 = time.time() \n\nprint('\\nComputing time {:.2f} s'\n      .format(t2 - t1))\n\n# Using all CPU cores\nt1 = time.time()\nresults = Parallel(n_jobs=-1)(delayed(math.factorial) (int(math.sqrt(i**3))) for i in range(1000,2000)) \n\nt2 = time.time()\nprint('\\nComputing time {:.2f} s'\n      .format(t2 - t1))\n```", "```py\n# The output:\nComputing time 59.67 s\n\nComputing time 12.18 s\n```", "```py\n# ./prime.py\nimport math\n\ndef is_prime(num):\n    '''Check if num is prime or not.\n    '''\n    for i in range(2,int(math.sqrt(num))+1):\n        if num%i==0:\n            return False\n    return True\n```", "```py\n# ./test.py\nimport unittest\nfrom prime import is_prime\n\nclass TestPrime(unittest.TestCase):\n\n    def test_thirteen(self):\n        self.assertTrue(is_prime(13))\n```", "```py\npython -m unittest test.py\n# Output:\n# .\n# ----------------------------------------------------------------------\n# Ran 1 test in 0.000s\n\n# OK\n```", "```py\n# ./test.py\nimport unittest\nfrom prime import is_prime\n\nclass TestPrime(unittest.TestCase):\n\n    def test_thirteen(self):\n        self.assertTrue(is_prime(13))\n    def test_four(self):\n        self.assertFalse(is_prime(4))\n```", "```py\npython -m unittest test.py\n# Output:\n# ..\n# ----------------------------------------------------------------------\n# Ran 2 tests in 0.000s\n\n# OK\n```", "```py\n# ./asteroids.py\nimport requests\nAPI_KEY=\"fsMlsu69Y7KdMNB4P2m9sqIpw5TGuF9IuYkhURzW\"\nASTEROIDS_API_URL=\"https://api.nasa.gov/neo/rest/v1/feed\"\n\ndef get_data():\n    print('Fetching data from NASA Asteroids API...')\n    session = requests.Session()\n    url=ASTEROIDS_API_URL\n    apiKey=API_KEY\n    requestParams = {\n        'api_key': apiKey,\n        'start_date': '2023-04-20',\n        'end_date': '2023-04-21'\n    }\n    response = requests.get(url, params = requestParams)\n    print(response.status_code)\n    near_earth_objects = (response.json())['near_earth_objects']\n    return near_earth_objects\n\ndef save_data():\n    # Do some ETL here\n    data = get_data()\n    return data\n\nprint(save_data())\n```", "```py\n# python ./asteroids.py\n\nFetching data from NASA Asteroids API...\n200\n{'2023-04-20': [{'links': {'self': 'http://api.nasa.gov/neo/rest/v1/neo/2326291?api_key=fsMlsu69Y7KdMNB4P2m9sqIpw5TGuF9IuYkhURzW'}, 'id': '2326291', 'neo_reference_id': '2326291', 'name': '326291 (1998 HM3)', 'nasa_jpl_url': 'http://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2326291', 'absolute_magnitude_h': 19.0, 'estimated_diameter': {'kilometers': {'estimated_diameter_min': 0.4212646106, 'estimated_diameter_max': 0.9419763057}, 'meters': {'estimated_diameter_min': 421.2646105562, 'estimated_diameter_max': 941.9763057186}, 'miles': {'estimated_diameter_min': 0.2617616123, 'estimated_diameter_max': 0.5853167591}, 'feet': {'estimated_diameter_min': 1382.1017848971, 'estimated_diameter_max': 3090.4735428537}}, 'is_potentially_hazardous_asteroid': False, 'close_approach_data':\n....\n```", "```py\n# ./test_etl.py\nimport unittest\nfrom asteroids import *\n\nimport unittest.mock as mock\n\nclass TestEtl(unittest.TestCase):\n\n    def test_asteroids_etl(self): \n        with mock.patch('asteroids.get_data') as GetDataMock:\n            GetDataMock.return_value = ['asteroid_1', 'asteroid_2']\n            self.assertEqual(['1', '2'], save_data())\n```", "```py\nAssertionError: Lists differ: ['1', '2'] != ['asteroid_1', 'asteroid_2']\n\nFirst differing element 0:\n'1'\n'asteroid_1'\n\n- ['1', '2']\n+ ['asteroid_1', 'asteroid_2']\n\n----------------------------------------------------------------------\nRan 1 test in 0.001s\n\nFAILED (failures=1)\n```", "```py\n# asteroids.py\nimport requests\nimport json\nimport tracemalloc\n\ntracemalloc.start()\n\nAPI_KEY=\"fsMlsu69Y7KdMNB4P2m9sqIpw5TGuF9IuYkhURzW\"\nASTEROIDS_API_URL=\"https://api.nasa.gov/neo/rest/v1/feed\"\n\n@profile\ndef get_data():\n    print('Fetching data from NASA Asteroids API...')\n    session = requests.Session()\n    url=ASTEROIDS_API_URL\n    apiKey=API_KEY\n    requestParams = {\n        'api_key': apiKey,\n        'start_date': '2023-04-20',\n        'end_date': '2023-04-27'\n    }\n    response = requests.get(url, params = requestParams).text\n    with open('out.csv', 'w') as fd:\n        fd.write(response)\n\nget_data()\n\nprint(tracemalloc.get_traced_memory())\n\ntracemalloc.stop()\n```", "```py\nFetching data from NASA Asteroids API...\n(85629, 477039)\n```", "```py\n# asteroids_stream.py\nimport requests\nimport json\nimport tracemalloc\n\ntracemalloc.start()\n\nAPI_KEY=\"fsMlsu69Y7KdMNB4P2m9sqIpw5TGuF9IuYkhURzW\"\nASTEROIDS_API_URL=\"https://api.nasa.gov/neo/rest/v1/feed\"\n\ndef get_data():\n    print('Fetching data from NASA Asteroids API...')\n    session = requests.Session()\n    url=ASTEROIDS_API_URL\n    apiKey=API_KEY\n    requestParams = {\n        'api_key': apiKey,\n        'start_date': '2023-04-20',\n        'end_date': '2023-04-27'\n    }\n    response = session.get(url, params = requestParams, stream = True)\n    print('Saving to disk...')\n    with open('out.csv', 'wb') as fd:\n        for chunk in response.iter_content(chunk_size=1024):\n            fd.write(chunk)\n\nget_data()\n\nprint(tracemalloc.get_traced_memory())\n\ntracemalloc.stop()\n```", "```py\n# asteroids_stream.py\nFetching data from NASA Asteroids API...\nSaving to disk...\n(85456, 215260)\n```", "```py\n# nasa.py\nimport boto3\nimport requests\nimport os\nS3_DATA = os.environ['S3_DATA_BUCKET'] #\"your.datalake.bucket\"\nAPI_KEY=\"fsMlsu69Y7KdMNB4P2m9sqIpw5TGuF9IuYkhURzW\"\nASTEROIDS_API_URL=\"https://api.nasa.gov/neo/rest/v1/feed\"\n\nprint('Fetching data from NASA Asteroids API...')\nsession = requests.Session()\nurl=ASTEROIDS_API_URL\napiKey=API_KEY\nrequestParams = {\n    'api_key': apiKey,\n    'start_date': '2023-04-20',\n    'end_date': '2023-04-21'\n}\nresponse = session.get(url, params = requestParams, stream=True)\nprint(response.status_code)\n# Perform Multi-part upload to AWS S3 datalake:\ns3_bucket = S3_DATA # i.e. 'data.staging.aws'\ns3_file_path = 'nasa/test_nasa_.csv' # i.e. \"path_in_s3\"\ns3 = boto3.client('s3')\nprint('Saving to S3\\. Run to download: aws s3 cp s3://{}/{} ./'.format(s3_bucket,s3_file_path))\nwith response as part:\n    part.raw.decode_content = True\n    conf = boto3.s3.transfer.TransferConfig(multipart_threshold=10000, max_concurrency=4)\n    s3.upload_fileobj(part.raw, s3_bucket, s3_file_path, Config=conf)\n```", "```py\nS3_DATA_BUCKET=\"your.staging.databucket\" python nasa.py\n# Output:\n# Fetching data from NASA Asteroids API...\n# 200\n# Saving to S3\\. Run to download: aws s3 cp s3://your.staging.databucket/nasa/test_nasa_.csv ./\n```"]