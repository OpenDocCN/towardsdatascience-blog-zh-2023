- en: Pushing the Limits of the Two-Tower Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推动双塔模型的极限
- en: 原文：[https://towardsdatascience.com/pushing-the-limits-of-the-two-tower-model-a577090e5140](https://towardsdatascience.com/pushing-the-limits-of-the-two-tower-model-a577090e5140)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/pushing-the-limits-of-the-two-tower-model-a577090e5140](https://towardsdatascience.com/pushing-the-limits-of-the-two-tower-model-a577090e5140)
- en: Where the assumptions behind the two-tower model architecture break — and how
    to go beyond
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双塔模型架构背后的假设在哪些地方失效——以及如何超越这些限制
- en: '[](https://medium.com/@samuel.flender?source=post_page-----a577090e5140--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----a577090e5140--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a577090e5140--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a577090e5140--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----a577090e5140--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samuel.flender?source=post_page-----a577090e5140--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----a577090e5140--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a577090e5140--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a577090e5140--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----a577090e5140--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a577090e5140--------------------------------)
    ·8 min read·Dec 10, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a577090e5140--------------------------------)
    ·阅读时长8分钟·2023年12月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0368a7208d8a12f6c14723d1d837cc2c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0368a7208d8a12f6c14723d1d837cc2c.png)'
- en: (Image created by the author using generative AI)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （图像由作者使用生成式 AI 创建）
- en: '[Two-tower models](https://medium.com/towards-data-science/the-rise-of-two-tower-models-in-recommender-systems-be6217494831)
    are among the most common architectural design choices in modern recommender systems
    — the key idea is to have one tower that learns relevance, and a second, shallow,
    tower that learns observational biases such as position bias.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[双塔模型](https://medium.com/towards-data-science/the-rise-of-two-tower-models-in-recommender-systems-be6217494831)
    是现代推荐系统中最常见的架构设计选择之一——关键思想是一个塔学习相关性，第二个浅层塔学习观察性偏差，如位置偏差。'
- en: 'In this post, we’ll take a closer look at two assumptions behind two-tower
    models, in particular:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将仔细审视双塔模型背后的两个假设，特别是：
- en: the **factorization assumption**, i.e. the hypothesis that we can simply multiply
    the probabilities computed by the two towers (or add their logits), and
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**因式分解假设**，即假设我们可以简单地将两个塔计算的概率相乘（或将它们的对数相加），以及'
- en: the **positional independence assumption**, i.e. the hypothesis that the only
    variable that determines position bias is the position of the item itself, and
    not the context in which it is impressed.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**位置独立性假设**，即假设决定位置偏差的唯一变量是项目本身的位置，而不是影响它的上下文。'
- en: We’ll see where both of these assumptions break, and how to go beyond these
    limitations with newer algorithms such as the MixEM model, the Dot Product model,
    and XPA.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到这两个假设在哪些地方失效，以及如何通过新算法如 MixEM 模型、点积模型和 XPA 超越这些限制。
- en: Let’s start with a very brief reminder.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个非常简短的提醒开始。
- en: '[](/the-rise-of-two-tower-models-in-recommender-systems-be6217494831?source=post_page-----a577090e5140--------------------------------)
    [## The Rise of Two-Tower Models in Recommender Systems'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/the-rise-of-two-tower-models-in-recommender-systems-be6217494831?source=post_page-----a577090e5140--------------------------------)
    [## 双塔模型在推荐系统中的兴起'
- en: A deep-dive into the latest technology used to debias ranking models
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深入探讨用于去偏排名模型的最新技术
- en: towardsdatascience.com](/the-rise-of-two-tower-models-in-recommender-systems-be6217494831?source=post_page-----a577090e5140--------------------------------)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/the-rise-of-two-tower-models-in-recommender-systems-be6217494831?source=post_page-----a577090e5140--------------------------------)
- en: 'Two-tower models: the story so far'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双塔模型：迄今为止的故事
- en: 'The primary learning objective for the ranking models in recommender systems
    is relevance: we want the model to predict the best possible piece of content
    given the context. Here, context simply means everything that we’ve learned about
    the user, for example from their previous engagement or search histories, depending
    on the application.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统中排名模型的主要学习目标是相关性：我们希望模型在给定上下文的情况下预测最佳的内容。这里，上下文简单地指我们从用户的先前互动或搜索历史中学到的一切，具体取决于应用场景。
- en: However, ranking models usually exhibit certain observation biases, that is,
    the tendency for users to engage more or less with an impression depending on
    how it was presented to them. The most prominent observation bias is position
    bias — the tendency of users to engage more with items that are shown first.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，排名模型通常表现出某些观察偏差，即用户根据展示方式对印象的参与程度的倾向。最突出的观察偏差是位置偏差——用户更倾向于与最先展示的项目进行互动。
- en: 'The key idea in two-tower models is to train two “towers”, that is, neural
    networks, in parallel, the main tower for learning relevance, and a second, shallow,
    tower for learning all sorts of observation biases in the data. The logits from
    the two towers can then be added to compute the final predictions, as done in
    YouTube’s “[Watch Next](https://daiwk.github.io/assets/youtube-multitask.pdf)”
    paper:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 双塔模型的关键思想是并行训练两个“塔”，即神经网络，一个主塔用于学习相关性，另一个浅层塔用于学习数据中的各种观察偏差。然后可以将两个塔的对数值相加以计算最终预测，如YouTube的“[Watch
    Next](https://daiwk.github.io/assets/youtube-multitask.pdf)”论文中所述：
- en: '![](../Images/584941fc2372fb657c41691a8f0ce54e.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/584941fc2372fb657c41691a8f0ce54e.png)'
- en: ([Zhao et al 2019](https://daiwk.github.io/assets/youtube-multitask.pdf))
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ([赵等 2019](https://daiwk.github.io/assets/youtube-multitask.pdf))
- en: 'The underlying hypothesis is that by having a dedicated tower for biases, the
    main tower can focus on the main learning objective, that is, relevance. Indeed,
    empirically two-tower models have been shown to bring substantial modeling improvements:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 基础假设是，通过为偏差设置专门的塔，主塔可以专注于主要的学习目标，即相关性。实际上，经验上，双塔模型已经被证明在建模方面带来了显著的改进：
- en: '[Huawei’s PAL](https://github.com/tangxyw/RecSysPapers/blob/main/Debias/%5B2019%5D%5BHuawei%5D%5BPAL%5D%20a%20position-bias%20aware%20learning%20framework%20for%20CTR%20prediction%20in%20live%20recommender%20systems.pdf)
    improved click-through rates in the Huawei App Store by 25%,'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[华为的 PAL](https://github.com/tangxyw/RecSysPapers/blob/main/Debias/%5B2019%5D%5BHuawei%5D%5BPAL%5D%20a%20position-bias%20aware%20learning%20framework%20for%20CTR%20prediction%20in%20live%20recommender%20systems.pdf)
    在华为应用商店的点击率提高了 25%。'
- en: '[YouTube’s two-tower additive model](https://daiwk.github.io/assets/youtube-multitask.pdf)
    improved engagement rates by 0.24%, and'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[YouTube 的双塔加性模型](https://daiwk.github.io/assets/youtube-multitask.pdf)将互动率提高了
    0.24%，并且'
- en: '[AirBnb’s two-tower model](https://arxiv.org/pdf/2002.05515.pdf) improved booking
    rates by 0.7% .'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AirBnb 的双塔模型](https://arxiv.org/pdf/2002.05515.pdf)将预订率提高了 0.7%。'
- en: In short, two-tower models work by decomposing the learning objective into relevance
    and bias, and have shown to bring substantial improvements in ranking models across
    the industry.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，双塔模型通过将学习目标分解为相关性和偏差来工作，并已被证明在整个行业的排名模型中带来了显著的改进。
- en: The factorization assumption
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分解假设
- en: Two-tower models rely on the factorization assumption, that is, the hypothesis
    that we can factorize click predictions as
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 双塔模型依赖于分解假设，即我们可以将点击预测分解为
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'that is, the product of click probability given that the item was observed
    by the user (the first factor) times the probability that the item was observed
    given the position (and other observational features). TouTube re-formulated this
    as a sum of logits instead of a product of probabilities, which is roughly equivalent:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 即，在用户观察到该项目的情况下的点击概率（第一个因素）与在给定位置（以及其他观察特征）的情况下观察到该项目的概率的乘积。YouTube将其重新表述为对数值的和，而不是概率的乘积，这在大致上是等效的：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'However, it’s easy to see where this factorization assumption breaks. For example,
    consider a scenario where the training data consists of two different types of
    users, Type 1 and Type 2:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，很容易看出这个分解假设在哪里破裂。例如，考虑一个训练数据包含两种不同类型用户的场景，类型 1 和类型 2：
- en: Type 1 users always click on the first item they’re shown. They’re impatient
    and seek immediate rewards.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类型 1 用户总是点击他们被展示的第一个项目。他们急于获得即时奖励。
- en: Type 2 users always scroll through the items they’re being shown until they
    find exactly what they’re looking for. They’re patient and choosy.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类型2用户总是滚动浏览他们被展示的项目，直到找到他们确切需要的内容。他们耐心且挑剔。
- en: Now, our factorization assumption breaks because `p(seen|position)` is not constant
    across the data, but depends on the user segment! The current model can’t model
    a dataset generated by a mixture of user cohorts, each with different click behaviors
    and biases.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的分解假设破裂了，因为`p(seen|position)`在数据中不是常量，而是取决于用户群体！当前模型无法建模由具有不同点击行为和偏见的用户群体混合生成的数据集。
- en: 'Instead of factorizing, we should therefore do something else. But what? Two
    solutions have been proposed in the recent Google [paper](https://dl.acm.org/doi/pdf/10.1145/3477495.3531837)
    “Revisiting Two-tower Models for Unbiased Learning to Rank”:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们应该做些其他的事情，而不是分解。那是什么呢？最近的Google[论文](https://dl.acm.org/doi/pdf/10.1145/3477495.3531837)《重新审视双塔模型以进行无偏排序学习》中提出了两个解决方案：
- en: the dot-product model, and
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点积模型，以及
- en: the Mix-EM model.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mix-EM模型。
- en: Let’s take a closer look at how they work.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解它们的工作原理。
- en: 'The key idea behind the **dot-product model** is to have the two towers not
    output probabilities or logits but instead embeddings, and then combine the two
    embeddings with a dot product:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**点积模型**的核心思想是让两个塔模型不输出概率或logits，而是输出嵌入，然后通过点积将两个嵌入组合起来：'
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This works because embeddings are more expressive then just a logit, and they
    allow us to encode different degrees of biases for different users in different
    dimensions of the embedding space.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 之所以有效，是因为嵌入表示比单一的logit更具表达能力，它们允许我们在嵌入空间的不同维度中编码不同用户的偏见程度。
- en: In the **Mix-EM** model, instead of a single two-tower model we have multiple
    (the authors consider 2), and then apply the Expectation-Maximization (EM) algorithm,
    where
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在**Mix-EM**模型中，我们不是使用单一的双塔模型，而是使用多个（作者考虑了2个），然后应用期望最大化（EM）算法，其中
- en: in the E step, we assign training examples to one of the pair of two-tower models,
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在E步中，我们将训练样本分配给一对双塔模型中的一个，
- en: in the M step, we minimize the overall loss over the pair of two-tower models.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在M步中，我们最小化这一对双塔模型的总体损失。
- en: The hypothesis behind this approach is that different two-tower models will
    eventually learn to handle different user segments (such as our Type 1 and Type
    2 users introduced above).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法背后的假设是，不同的双塔模型最终将学会处理不同的用户群体（例如我们上面介绍的类型1和类型2用户）。
- en: The authors tested both algorithms in production on recommendations in the Google
    Chrome Web Store and found
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在Google Chrome Web商店的推荐中测试了这两种算法，并发现
- en: +0.47% NDGC@5 with the dot-product model,
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用点积模型，NDGC@5提升了+0.47%。
- en: +1.1% NDGC@5 with MixEM,
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MixEM，NDGC@5提升了+1.1%。
- en: compared to the standard two-tower additive model. This result confirms that
    indeed the factorization assumption does not hold for this problem, and quantifies
    how much performance we’re losing due to this failure. Both the Dot-product model
    and the MixEM model work better because they’re more expressive ways to model
    the combination of the two towers, MixEM more so than than the Dot-product model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于标准的双塔加性模型，这个结果证实了分解假设确实不适用于这个问题，并量化了我们由于这种失败而损失的性能。点积模型和MixEM模型的效果更好，因为它们是更具表达力的方式来建模两个塔的组合，MixEM比点积模型更好。
- en: The independence assumption
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 独立假设
- en: Another assumption behind the standard two-tower model is that an item impressed
    at position p is independent of the items that were shown at the neighboring positions,
    p-1, p+1, and so on.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 标准双塔模型背后的另一个假设是，在位置p展示的项目与邻近位置p-1、p+1等位置展示的项目是独立的。
- en: However, this is not the case in the real world, argue the authors of the recent
    Google paper [Cross-Positional Attention for Debiasing Clicks](https://hongleizhuang.github.io/files/WWW2021.pdf).
    Click probabilities, so their argument, depend not only on the item that is shown,
    but also what is shown nearby, especially so when items have both horizontal and
    vertical positions such as in the Google Chrome Store.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并非现实世界的情况，最近的Google论文[跨位置注意力以去偏见点击](https://hongleizhuang.github.io/files/WWW2021.pdf)的作者认为。点击概率不仅依赖于显示的项目，还依赖于附近显示的内容，尤其是在项目具有水平和垂直位置的情况下，例如在Google
    Chrome商店中。
- en: 'In order to go beyond this limitation, the authors propose XPA, short for cross-positional
    attention. Here’s how it works:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了超越这一限制，作者提出了XPA，即跨位置注意力。其工作原理如下：
- en: the input to the bias tower is not just the position of the current item, but
    also all positions of all neighboring items, represented as a single embedding.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差塔的输入不仅仅是当前项目的位置，还包括所有邻近项目的位置，这些位置被表示为一个单一的嵌入向量。
- en: the input to the relevance tower is not just the feature vector for the current
    item, but also all feature vectors of all neighboring items, represented as a
    single embedding.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关性塔的输入不仅仅是当前项目的特征向量，还包括所有邻近项目的特征向量，这些特征向量被表示为一个单一的嵌入向量。
- en: 'The key question then is how do we represent all neighboring item’s with a
    single embedding? This is done with a cross-attention layer, which first computes
    attention weights using a scaled softmax:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 那么关键问题是如何用单一嵌入向量表示所有邻近项目？这通过交叉注意力层来实现，该层首先使用缩放的 softmax 计算注意力权重：
- en: '![](../Images/12ea1d976f0a4025bc93cd4edde957a8.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12ea1d976f0a4025bc93cd4edde957a8.png)'
- en: 'and then computes the neighboring item embeddings (denoted with a tilde) as
    w weighted sum, where the weights are the attention weights:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后计算邻近项目的嵌入（用波浪号表示）作为加权和，其中权重是注意力权重：
- en: '![](../Images/92da8ed1503bf874f5e3e8e5978a7f79.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92da8ed1503bf874f5e3e8e5978a7f79.png)'
- en: 'Here’s an architecture diagram showing how it works — note how the two towers
    don’t just take as inputs a single item but multiple items at a time, because
    it looks at all the neighbors of each item:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个架构图显示其工作原理——注意这两个塔不仅仅接收单一项目作为输入，而是一次处理多个项目，因为它查看了每个项目的所有邻居：
- en: '![](../Images/ea5c86c665536017afafc1eb2cb2dc02.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea5c86c665536017afafc1eb2cb2dc02.png)'
- en: ([Zhuang et al 2021](https://hongleizhuang.github.io/files/WWW2021.pdf))
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ([Zhuang et al 2021](https://hongleizhuang.github.io/files/WWW2021.pdf))
- en: The final prediction is then the combination of 4 terms, the 2 relevance tower
    outputs (for the item itself and for its neighbors), and the 2 bias tower outputs
    (again, for the item itself and for its neighbors), scaled by a sigmoid.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最终预测是四个项的组合，其中包括两个相关性塔的输出（分别针对项目本身及其邻居）和两个偏差塔的输出（同样分别针对项目本身及其邻居），经过一个 sigmoid
    函数进行缩放。
- en: Let’s take a look at the results. On production data from the Chrome Web Store,
    the authors find
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看结果。在来自 Chrome Web Store 的生产数据中，作者发现
- en: +11.8% click-through rate of XPA vs the naive baseline (not using positions
    at all)
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XPA 相较于基础的天真模型（完全不使用位置）提高了 +11.8% 的点击率。
- en: +6.2% click-through rate of XPA vs the standard two-tower model.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XPA 相较于标准双塔模型提高了 +6.2% 的点击率。
- en: 6.2% is a huge improvement — with that, the authors have proven the effectiveness
    of leveraging not just the items position but also which items are shown in its
    vicinity.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 6.2% 是一个巨大的提升——通过这一点，作者证明了不仅仅利用项目的位置，还利用显示在其附近的项目的有效性。
- en: It’s also interesting to look at the cross-attention matrix, a_jk, as a function
    of positions j and k. (“Position” here means the position in the Chrome Web Store
    UI, enumerated from top left to bottom right.)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 观察交叉注意力矩阵 a_jk 作为位置 j 和 k 的函数也是很有趣的。（“位置”在这里指的是 Chrome Web Store UI 中的位置，从左上到右下进行编号。）
- en: '![](../Images/0fcb6ddf9c8e726d3ebcc37248702d09.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0fcb6ddf9c8e726d3ebcc37248702d09.png)'
- en: ([Zhuang et al 2021](https://hongleizhuang.github.io/files/WWW2021.pdf))
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ([Zhuang et al 2021](https://hongleizhuang.github.io/files/WWW2021.pdf))
- en: The existence of off-diagonal elements in this matrix is proof that cross-attention
    is required to model this data — a vanilla two-tower model would only be able
    to model the diagonal element, not those off-diagonals. The off-diagonal attention
    weights exist because if a user clicks on an item in one row in the Chrome Store,
    they’re relatively more likely to click on another item in that same row.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵中存在的非对角线元素证明了需要交叉注意力来建模这些数据——一个普通的双塔模型只能建模对角线元素，而不能处理这些非对角线元素。非对角线注意力权重的存在是因为如果用户点击了
    Chrome Store 中某一行的一个项目，他们相对更可能点击同一行中的另一个项目。
- en: Take-away
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收获
- en: Let’s recap. Two-tower models improve the ranking models behind recommender
    systems by allowing them to decompose the learning objective into relevance +
    bias.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下。双塔模型通过允许将学习目标分解为相关性 + 偏差，来改进推荐系统背后的排名模型。
- en: The factorization assumption is the assumption that we can simply add the logits
    from the two towers (or multiply their probabilities). This assumption breaks
    as soon as we consider that there are different user segments in the data, each
    exhibiting different types and degrees of observational biases. Google’s MixEM
    and dot-product models have been shown to go beyond this limitation.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 因子分解假设是指我们可以简单地将两个塔的logits相加（或将它们的概率相乘）。这个假设在我们考虑到数据中存在不同用户群体时就会破裂，每个群体展示了不同类型和程度的观察偏差。谷歌的MixEM和点积模型已被证明能突破这一限制。
- en: The independence assumption is the assumption that the observation bias only
    depends on the position of an item, not on the neighboring items. This assumption
    breaks in real-world applications where user engagement indeed depends on the
    neighborhood of an item, not just its position alone. XPA, which leverages cross-positional
    attention from items at neighboring positions, is an algorithm to go beyond this
    limitation.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 独立假设是指观察偏差仅依赖于项目的位置，而不依赖于邻近项目。这个假设在现实应用中会破裂，因为用户参与确实依赖于项目的邻域，而不仅仅是它的位置。XPA利用来自邻近位置的交叉位置注意力，是突破这一限制的算法。
- en: To summarize, the standard two-tower model assumed that users are all the same,
    and that positions are all the same — MixEM (and the dot-product model) goes beyond
    the first limitation, and XPA goes beyond the second.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，标准的双塔模型假设用户都是相同的，位置也是相同的——MixEM（以及点积模型）突破了第一个限制，而XPA突破了第二个限制。
- en: '*Want to impress your peers or nail your next ML interview with in-depth knowledge
    of the latest ML technologies and breakthroughs?* [*Subscribe to my Newsletter*](https://mlfrontiers.substack.com)*!*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*想要以深入了解最新机器学习技术和突破来打动同行或在下一个机器学习面试中表现出色吗？* [*订阅我的新闻通讯*](https://mlfrontiers.substack.com)*！*'
