- en: Pushing the Limits of the Two-Tower Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/pushing-the-limits-of-the-two-tower-model-a577090e5140](https://towardsdatascience.com/pushing-the-limits-of-the-two-tower-model-a577090e5140)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Where the assumptions behind the two-tower model architecture break — and how
    to go beyond
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samuel.flender?source=post_page-----a577090e5140--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----a577090e5140--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a577090e5140--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a577090e5140--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----a577090e5140--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a577090e5140--------------------------------)
    ·8 min read·Dec 10, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0368a7208d8a12f6c14723d1d837cc2c.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image created by the author using generative AI)
  prefs: []
  type: TYPE_NORMAL
- en: '[Two-tower models](https://medium.com/towards-data-science/the-rise-of-two-tower-models-in-recommender-systems-be6217494831)
    are among the most common architectural design choices in modern recommender systems
    — the key idea is to have one tower that learns relevance, and a second, shallow,
    tower that learns observational biases such as position bias.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this post, we’ll take a closer look at two assumptions behind two-tower
    models, in particular:'
  prefs: []
  type: TYPE_NORMAL
- en: the **factorization assumption**, i.e. the hypothesis that we can simply multiply
    the probabilities computed by the two towers (or add their logits), and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the **positional independence assumption**, i.e. the hypothesis that the only
    variable that determines position bias is the position of the item itself, and
    not the context in which it is impressed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll see where both of these assumptions break, and how to go beyond these
    limitations with newer algorithms such as the MixEM model, the Dot Product model,
    and XPA.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with a very brief reminder.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/the-rise-of-two-tower-models-in-recommender-systems-be6217494831?source=post_page-----a577090e5140--------------------------------)
    [## The Rise of Two-Tower Models in Recommender Systems'
  prefs: []
  type: TYPE_NORMAL
- en: A deep-dive into the latest technology used to debias ranking models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/the-rise-of-two-tower-models-in-recommender-systems-be6217494831?source=post_page-----a577090e5140--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Two-tower models: the story so far'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The primary learning objective for the ranking models in recommender systems
    is relevance: we want the model to predict the best possible piece of content
    given the context. Here, context simply means everything that we’ve learned about
    the user, for example from their previous engagement or search histories, depending
    on the application.'
  prefs: []
  type: TYPE_NORMAL
- en: However, ranking models usually exhibit certain observation biases, that is,
    the tendency for users to engage more or less with an impression depending on
    how it was presented to them. The most prominent observation bias is position
    bias — the tendency of users to engage more with items that are shown first.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key idea in two-tower models is to train two “towers”, that is, neural
    networks, in parallel, the main tower for learning relevance, and a second, shallow,
    tower for learning all sorts of observation biases in the data. The logits from
    the two towers can then be added to compute the final predictions, as done in
    YouTube’s “[Watch Next](https://daiwk.github.io/assets/youtube-multitask.pdf)”
    paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/584941fc2372fb657c41691a8f0ce54e.png)'
  prefs: []
  type: TYPE_IMG
- en: ([Zhao et al 2019](https://daiwk.github.io/assets/youtube-multitask.pdf))
  prefs: []
  type: TYPE_NORMAL
- en: 'The underlying hypothesis is that by having a dedicated tower for biases, the
    main tower can focus on the main learning objective, that is, relevance. Indeed,
    empirically two-tower models have been shown to bring substantial modeling improvements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Huawei’s PAL](https://github.com/tangxyw/RecSysPapers/blob/main/Debias/%5B2019%5D%5BHuawei%5D%5BPAL%5D%20a%20position-bias%20aware%20learning%20framework%20for%20CTR%20prediction%20in%20live%20recommender%20systems.pdf)
    improved click-through rates in the Huawei App Store by 25%,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[YouTube’s two-tower additive model](https://daiwk.github.io/assets/youtube-multitask.pdf)
    improved engagement rates by 0.24%, and'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AirBnb’s two-tower model](https://arxiv.org/pdf/2002.05515.pdf) improved booking
    rates by 0.7% .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In short, two-tower models work by decomposing the learning objective into relevance
    and bias, and have shown to bring substantial improvements in ranking models across
    the industry.
  prefs: []
  type: TYPE_NORMAL
- en: The factorization assumption
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Two-tower models rely on the factorization assumption, that is, the hypothesis
    that we can factorize click predictions as
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'that is, the product of click probability given that the item was observed
    by the user (the first factor) times the probability that the item was observed
    given the position (and other observational features). TouTube re-formulated this
    as a sum of logits instead of a product of probabilities, which is roughly equivalent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'However, it’s easy to see where this factorization assumption breaks. For example,
    consider a scenario where the training data consists of two different types of
    users, Type 1 and Type 2:'
  prefs: []
  type: TYPE_NORMAL
- en: Type 1 users always click on the first item they’re shown. They’re impatient
    and seek immediate rewards.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Type 2 users always scroll through the items they’re being shown until they
    find exactly what they’re looking for. They’re patient and choosy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, our factorization assumption breaks because `p(seen|position)` is not constant
    across the data, but depends on the user segment! The current model can’t model
    a dataset generated by a mixture of user cohorts, each with different click behaviors
    and biases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of factorizing, we should therefore do something else. But what? Two
    solutions have been proposed in the recent Google [paper](https://dl.acm.org/doi/pdf/10.1145/3477495.3531837)
    “Revisiting Two-tower Models for Unbiased Learning to Rank”:'
  prefs: []
  type: TYPE_NORMAL
- en: the dot-product model, and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the Mix-EM model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s take a closer look at how they work.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key idea behind the **dot-product model** is to have the two towers not
    output probabilities or logits but instead embeddings, and then combine the two
    embeddings with a dot product:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This works because embeddings are more expressive then just a logit, and they
    allow us to encode different degrees of biases for different users in different
    dimensions of the embedding space.
  prefs: []
  type: TYPE_NORMAL
- en: In the **Mix-EM** model, instead of a single two-tower model we have multiple
    (the authors consider 2), and then apply the Expectation-Maximization (EM) algorithm,
    where
  prefs: []
  type: TYPE_NORMAL
- en: in the E step, we assign training examples to one of the pair of two-tower models,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in the M step, we minimize the overall loss over the pair of two-tower models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hypothesis behind this approach is that different two-tower models will
    eventually learn to handle different user segments (such as our Type 1 and Type
    2 users introduced above).
  prefs: []
  type: TYPE_NORMAL
- en: The authors tested both algorithms in production on recommendations in the Google
    Chrome Web Store and found
  prefs: []
  type: TYPE_NORMAL
- en: +0.47% NDGC@5 with the dot-product model,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: +1.1% NDGC@5 with MixEM,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: compared to the standard two-tower additive model. This result confirms that
    indeed the factorization assumption does not hold for this problem, and quantifies
    how much performance we’re losing due to this failure. Both the Dot-product model
    and the MixEM model work better because they’re more expressive ways to model
    the combination of the two towers, MixEM more so than than the Dot-product model.
  prefs: []
  type: TYPE_NORMAL
- en: The independence assumption
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another assumption behind the standard two-tower model is that an item impressed
    at position p is independent of the items that were shown at the neighboring positions,
    p-1, p+1, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: However, this is not the case in the real world, argue the authors of the recent
    Google paper [Cross-Positional Attention for Debiasing Clicks](https://hongleizhuang.github.io/files/WWW2021.pdf).
    Click probabilities, so their argument, depend not only on the item that is shown,
    but also what is shown nearby, especially so when items have both horizontal and
    vertical positions such as in the Google Chrome Store.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to go beyond this limitation, the authors propose XPA, short for cross-positional
    attention. Here’s how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: the input to the bias tower is not just the position of the current item, but
    also all positions of all neighboring items, represented as a single embedding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the input to the relevance tower is not just the feature vector for the current
    item, but also all feature vectors of all neighboring items, represented as a
    single embedding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The key question then is how do we represent all neighboring item’s with a
    single embedding? This is done with a cross-attention layer, which first computes
    attention weights using a scaled softmax:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/12ea1d976f0a4025bc93cd4edde957a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'and then computes the neighboring item embeddings (denoted with a tilde) as
    w weighted sum, where the weights are the attention weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/92da8ed1503bf874f5e3e8e5978a7f79.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here’s an architecture diagram showing how it works — note how the two towers
    don’t just take as inputs a single item but multiple items at a time, because
    it looks at all the neighbors of each item:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea5c86c665536017afafc1eb2cb2dc02.png)'
  prefs: []
  type: TYPE_IMG
- en: ([Zhuang et al 2021](https://hongleizhuang.github.io/files/WWW2021.pdf))
  prefs: []
  type: TYPE_NORMAL
- en: The final prediction is then the combination of 4 terms, the 2 relevance tower
    outputs (for the item itself and for its neighbors), and the 2 bias tower outputs
    (again, for the item itself and for its neighbors), scaled by a sigmoid.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at the results. On production data from the Chrome Web Store,
    the authors find
  prefs: []
  type: TYPE_NORMAL
- en: +11.8% click-through rate of XPA vs the naive baseline (not using positions
    at all)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: +6.2% click-through rate of XPA vs the standard two-tower model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.2% is a huge improvement — with that, the authors have proven the effectiveness
    of leveraging not just the items position but also which items are shown in its
    vicinity.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also interesting to look at the cross-attention matrix, a_jk, as a function
    of positions j and k. (“Position” here means the position in the Chrome Web Store
    UI, enumerated from top left to bottom right.)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0fcb6ddf9c8e726d3ebcc37248702d09.png)'
  prefs: []
  type: TYPE_IMG
- en: ([Zhuang et al 2021](https://hongleizhuang.github.io/files/WWW2021.pdf))
  prefs: []
  type: TYPE_NORMAL
- en: The existence of off-diagonal elements in this matrix is proof that cross-attention
    is required to model this data — a vanilla two-tower model would only be able
    to model the diagonal element, not those off-diagonals. The off-diagonal attention
    weights exist because if a user clicks on an item in one row in the Chrome Store,
    they’re relatively more likely to click on another item in that same row.
  prefs: []
  type: TYPE_NORMAL
- en: Take-away
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s recap. Two-tower models improve the ranking models behind recommender
    systems by allowing them to decompose the learning objective into relevance +
    bias.
  prefs: []
  type: TYPE_NORMAL
- en: The factorization assumption is the assumption that we can simply add the logits
    from the two towers (or multiply their probabilities). This assumption breaks
    as soon as we consider that there are different user segments in the data, each
    exhibiting different types and degrees of observational biases. Google’s MixEM
    and dot-product models have been shown to go beyond this limitation.
  prefs: []
  type: TYPE_NORMAL
- en: The independence assumption is the assumption that the observation bias only
    depends on the position of an item, not on the neighboring items. This assumption
    breaks in real-world applications where user engagement indeed depends on the
    neighborhood of an item, not just its position alone. XPA, which leverages cross-positional
    attention from items at neighboring positions, is an algorithm to go beyond this
    limitation.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, the standard two-tower model assumed that users are all the same,
    and that positions are all the same — MixEM (and the dot-product model) goes beyond
    the first limitation, and XPA goes beyond the second.
  prefs: []
  type: TYPE_NORMAL
- en: '*Want to impress your peers or nail your next ML interview with in-depth knowledge
    of the latest ML technologies and breakthroughs?* [*Subscribe to my Newsletter*](https://mlfrontiers.substack.com)*!*'
  prefs: []
  type: TYPE_NORMAL
