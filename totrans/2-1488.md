# 精通Python中的长短期记忆：释放LSTM在NLP中的力量

> 原文：[https://towardsdatascience.com/mastering-long-short-term-memory-with-python-unleashing-the-power-of-lstm-in-nlp-381ec3430f50](https://towardsdatascience.com/mastering-long-short-term-memory-with-python-unleashing-the-power-of-lstm-in-nlp-381ec3430f50)

## 一本关于理解和实现LSTM层用于Python自然语言处理的全面指南

[](https://eligijus-bujokas.medium.com/?source=post_page-----381ec3430f50--------------------------------)[![Eligijus Bujokas](../Images/061fd30136caea2ba927140e8b3fae3c.png)](https://eligijus-bujokas.medium.com/?source=post_page-----381ec3430f50--------------------------------)[](https://towardsdatascience.com/?source=post_page-----381ec3430f50--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----381ec3430f50--------------------------------) [Eligijus Bujokas](https://eligijus-bujokas.medium.com/?source=post_page-----381ec3430f50--------------------------------)

·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----381ec3430f50--------------------------------) ·17分钟阅读·2023年11月28日

--

![](../Images/8590a68892231bb5a03f41691fc9f697.png)

[Sven Brandsma](https://unsplash.com/@seffen99?utm_source=medium&utm_medium=referral)拍摄的照片，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)。

这项工作是我[关于RNN和Python NLP的文章](https://medium.com/towards-data-science/mastering-nlp-in-depth-python-coding-for-deep-learning-models-a15055e989bf)的延续。一个简单的递归层的深度学习网络的自然发展是带有**长短期记忆**（**LSTM**）层的深度学习网络。

与RNN和NLP一样，我将尝试详细解释LSTM层，并从头编写该层的前向传播代码。

所有代码可以在这里查看: [https://github.com/Eligijus112/NLP-python](https://github.com/Eligijus112/NLP-python)

我们将使用与上一篇文章相同的¹数据集：

[PRE0]

![](../Images/bcb705d18dcb689363a428e44c88f127.png)

数据集中的随机行；作者拍摄的图片

请记住，SENTIMENT=1表示负面情感，SENTIMENT=0表示正面情感。

我们需要将文本数据转换为整数序列。不过，与上一篇文章不同的是，我们现在将创建一个字符序列，而不是单词序列。

例如，文本“*Nice Game*”可以转换为以下示例向量：

*[1, 2, 3, 4, 5, 6, 7, 8, 3]*

每个字符，包括空格和标点符号，都会有一个索引。

[PRE1]

让我们将数据拆分为训练集和测试集，并应用我们创建的函数：

[PRE2]

我们的数据中有274个独特的字符。让我们打印**word2idx**字典中的前10项：

[PRE3]

让我们将文本转换为序列：

[PRE4]

回顾一下，按词级别拆分文本导致序列的平均长度为**~22** 个令牌。现在，我们有长度为**~103** 个令牌的序列。标准差非常高，因此我们将使用最大序列长度为**200** 进行填充。

[PRE5]

到目前为止，训练集和验证集的数据集如下：

![](../Images/895432fe15cb4e3a0d476637def6883a.png)

一段数据；作者拍摄的照片

为什么我们应该从普通的 RNN 切换到 LSTM 网络？问题有两个方面：

+   一个简单的 RNN 有所谓的**消失梯度问题²**或**爆炸梯度问题**，与网络中***for*** 循环使用的权重相关。

+   网络往往会**“忘记”**长序列数据的初始步骤输入。

为了说明***遗忘***，请考虑以下示例：

在我们的数据中，平均而言，有 103 个时间步（文本中从左到右的令牌数量）。回顾 RNN 文章中的图表：

![](../Images/3de05ae2d60ac52ad02c3a95ad79fc0e.png)

展开 n 步 RNN；作者拍摄的照片

我们有相同的权重**W**，用来乘以 ReLU 层的输出。然后，我们将该信号添加到下一个时间步，依此类推。如果我们为**W**选择一个相对较小的值（例如 0.5），并且我们有 103 步时间序列数据，从第一次时间步输入到最终输出的影响大致为**0.5¹⁰³ * input1**，这大致等于零。

第二次输入的信号将是**0.5¹⁰² * input2**，以此类推。

可以看出，随着时间步的增加，初始时间步的信息在最终输出中的占比越来越少。

为了应对遗忘过去的问题，伟大的思想家们提出了用于时间序列问题的 LSTM 层³。

从内部来看，LSTM 层使用两个激活函数：

+   Sigmoid 函数

+   Tanh 函数

关于这些函数要记住的关键事实是：

+   **sigmoid** 激活函数接受实数平面上的任何值，并输出一个**在 0 和 1 之间**的值。

+   **tanh** 函数接受实数平面上的任何值，并输出一个**在 -1 和 1 之间**的值。

[PRE6]

既然我们已经了解了 sigmoid 和 tanh 激活函数，让我们回到 LSTM 层。

LSTM 层由 2 部分组成（因此得名）：

+   长期记忆块

+   短期记忆块

在每个时间步（或令牌步），LSTM 层输出两个预测：**长期预测**和**短期预测**。LSTM 单元的高层次图示可以这样可视化：

![](../Images/756d64f334e9b754acda7b6b6a6b85db.png)

展开的简单 LSTM 网络；作者拍摄的图表

在每个时间步骤，LSTM 层输出一个数字，这就是我们所称的**短期记忆输出**。它通常只是一个标量。此外，**长期记忆**标量也在 LSTM 层中计算，但它不被输出并传递到序列的第二步。值得注意的是，在每个时间步骤中，短期和长期记忆都会被更新。

现在让我们深入探讨 LSTM 层。LSTM 层的第一部分是所谓的**忘记门**操作：

![](../Images/04fd92b4546ccff1103dbc14fec6b8c5.png)

忘记门；图表由作者提供

忘记门得名于我们**计算希望保留的长期记忆百分比**。这是因为 sigmoid 激活函数会输出一个介于 0 和 1 之间的数字，我们将这个数字乘以长期记忆并传递到网络中。

我们可以开始看到在训练时会更新的权重：**w1, w2** 和 **b1**。这些权重直接影响保持的长期记忆量。

请注意，在这个步骤中，短期记忆没有调整，而是传递到网络的第二步。

[PRE7]

[PRE8]

接下来在 LSTM 层的是输入门：

![](../Images/e4e654110995642ce727a2ad2d43c203.png)

输入门；图表由作者提供

输入门仅调整 LSTM 网络的长期记忆部分，但为此，它使用当前输入和当前短期记忆值。

从图表来看，在乘法步骤之前，我们有两个输出：一个来自 sigmoid 激活函数，另一个来自 tanh 激活层。宽泛地说，sigmoid 层输出要记住的记忆百分比（0，1），而 tanh 输出记住的潜在记忆（-1，1）。

然后我们将当前长期记忆（在忘记门中稍作调整）与输入门输出相加。

[PRE9]

[PRE10]

从上面的代码片段可以看出，唯一变化的是长期记忆。

LSTM 层的最后一部分是**输出门**。输出门是我们将调整**短期**记忆的步骤：

![](../Images/7edebcc737e85b30b2762841029d4a03.png)

输出门；图表由作者提供

逻辑与之前门的逻辑非常相似：sigmoid 激活函数计算要保留的记忆百分比，而 tanh 函数计算总体信号。

[PRE11]

[PRE12]

如我们所见，输出门仅调整了短期记忆标量。

![](../Images/577260ddbca493db6a1fcf95b06d7a75.png)

LSTM 层；图表由作者提供

上图显示了忘记门、输入门和输出门的合成图⁴。

当我们有一个 x 变量的输入序列时，使用 LSTM 层的内部循环是这样的：

1.  随机初始化短期和长期记忆。

2. 对于每个**x1**到**xn**：

2.1 通过 LSTM 层向前传播。

2.2 输出短期记忆

将长期和短期记忆保存到层中。

让我们将每个门封装到一个类中，并创建一个 Python 示例。

[PRE13]

[PRE14]

现在我们将所有内容包裹在一个漂亮的 pytorch 示例中，使用 LSTM 层。语法与基本的 RNN 模型非常相似：

[PRE15]

[PRE16]

[PRE17]

[PRE18]

这篇文章深入探讨了 LSTM 单元的内部工作细节。某些 LSTM 层的实现可能与这里展示的有所不同，但长期和短期记忆的整体部分在绝大多数实现中都存在。

我希望读者现在对 LSTM 层有了更好的理解，并希望他们能够立即将其实施到他们的工作流程中！

特别感谢 StatQuest 提供的精彩讲解视频⁵。

[1]

**名称：** 推特情感分析

**网址：** [https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis)

**数据集许可证：** [https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/)

[2]

**名称：** 梯度消失问题

**网址：** [https://k21academy.com/datascience-blog/machine-learning/recurrent-neural-networks/#:~:text=Two%20Issues%20of%20Standard%20RNNs&text=RNNs%20suffer%20from%20the%20matter,of%20long%20data%20sequences%20difficult](https://k21academy.com/datascience-blog/machine-learning/recurrent-neural-networks/#:~:text=Two%20Issues%20of%20Standard%20RNNs&text=RNNs%20suffer%20from%20the%20matter,of%20long%20data%20sequences%20difficult)

[3]

**名称：** 长短期记忆

**网址：** [https://www.bioinf.jku.at/publications/older/2604.pdf](https://www.bioinf.jku.at/publications/older/2604.pdf)

**年份：** 1997

[4]

**名称：** 理解 LSTM 网络

**网址：** [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

**年份：** 2015

[5]

**名称：** 长短期记忆（LSTM），清晰解释

**网址：** [https://www.youtube.com/watch?v=YCzL96nL7j0](https://www.youtube.com/watch?v=YCzL96nL7j0&t=358s)

**年份：** 2022
