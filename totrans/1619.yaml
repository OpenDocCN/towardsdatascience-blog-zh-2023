- en: Overfitting, Underfitting, and Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/overfitting-underfitting-and-regularization-7f83dd998a62](https://towardsdatascience.com/overfitting-underfitting-and-regularization-7f83dd998a62)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Making Friends with AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The bias-variance tradeoff, part 2 of 3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://kozyrkov.medium.com/?source=post_page-----7f83dd998a62--------------------------------)[![Cassie
    Kozyrkov](../Images/ad18dd12979a4a3ec130bdf8b889af23.png)](https://kozyrkov.medium.com/?source=post_page-----7f83dd998a62--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7f83dd998a62--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7f83dd998a62--------------------------------)
    [Cassie Kozyrkov](https://kozyrkov.medium.com/?source=post_page-----7f83dd998a62--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7f83dd998a62--------------------------------)
    ·4 min read·Feb 15, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Part 1](http://bit.ly/quaesita_bivar1), we covered much of the basic terminology
    as well as a few key insights about the bias-variance formula (**MSE = Bias² +
    Variance**), including this paraphrase from [Anna Karenina](https://www.goodreads.com/work/quotes/2507928):'
  prefs: []
  type: TYPE_NORMAL
- en: All perfect models are alike, but each unhappy model can be unhappy in its own
    way.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To make the most of *this* article, I suggest taking a look at [Part 1](http://bit.ly/quaesita_bivar1)
    to make sure you’re well-situated to absorb this one.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6073e1755019bc63fad47ad5d1bed129.png)'
  prefs: []
  type: TYPE_IMG
- en: Under vs over… fitting. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: What does overfitting/underfitting have to do with it?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s say you have a [model](http://bit.ly/quaesita_emperorm) that is as good
    as you’re going to get for the information you have.
  prefs: []
  type: TYPE_NORMAL
- en: To have an even better model, you need better [data](http://bit.ly/quaesita_hist).
    In other words, more data (quantity) or *more relevant* data (quality).
  prefs: []
  type: TYPE_NORMAL
- en: When I say as *good* as you’re going to get, I mean in “good” terms of [MSE](http://bit.ly/quaesita_msefav)
    performance on data your model hasn’t seen before. (It’s supposed to [*pre*dict](http://bit.ly/quaesita_parrot),
    not *post*dict.) You’ve done a perfect job of getting what you can from the information
    you have — the rest is error you can’t do anything about with your information.
  prefs: []
  type: TYPE_NORMAL
- en: Reality = Best Model + Unavoidable Error
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But here’s the problem… we’ve jumped ahead; **you don’t have this model yet.**
  prefs: []
  type: TYPE_NORMAL
- en: 'All you have is a pile of old data to learn this model from. Eventually, if
    you’re smart, you’ll [validate this model](http://bit.ly/quaesita_idiot) on data
    it hasn’t seen before, but first you have to learn the model by finding useful
    patterns in data and trying to inch closer and closer to the stated objective:
    an MSE that’s as low as possible.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, during the [learning process](http://bit.ly/quaesita_mrbean),
    you don’t get to observe the MSE you’re after (the one that comes from reality).
    You only get to compute a shoddy version from your current training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c20873c3f1bfa1837ca701dc6174fbfd.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jason Leung](https://unsplash.com/@ninjason?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Oh, and also, in this example “you” are not a human, you’re an [optimization
    algorithm](http://bit.ly/mfml_045) that was told by your human boss to twiddle
    the dials in the model’s settings until the MSE is as low as it will go.
  prefs: []
  type: TYPE_NORMAL
- en: You say, *“Sweet! I can do this!! Boss, if you give me an extremely flexible
    model with lots of settings to fiddle (*[*neural networks*](http://bit.ly/quaesita_emperor)*,
    anyone?), I can give you a perfect training MSE. No bias and no variance.”*
  prefs: []
  type: TYPE_NORMAL
- en: The way to get a better training MSE than the true model’s test MSE is to fit
    all the noise (errors you have no predictively-useful information about) along
    with the signal. How do you achieve this little miracle? By making the model more
    complicated. Connecting the dots, essentially.
  prefs: []
  type: TYPE_NORMAL
- en: This is called [overfitting](http://bit.ly/quaesita_sydd). Such a model has
    an excellent training MSE but a whopper of a variance when you try to use it for
    anything practical. That’s what you get for trying to cheat by creating a solution
    with [more complexity than your information supports](http://bit.ly/mfml_049).
  prefs: []
  type: TYPE_NORMAL
- en: The boss is too smart for your tricks. Knowing that a [flexible](http://bit.ly/quaesita_emperor),
    complicated model allows you to score too well on your [training set](http://bit.ly/quaesita_mrbean),
    the boss changes the scoring function to penalize complexity. This is called [regularization](http://bit.ly/quaesita_059).
    (Frankly, I wish we had more regularization of engineers’ antics, to stop them
    from doing complicated things for complexity’s sake.)
  prefs: []
  type: TYPE_NORMAL
- en: Regularization essentially says, *“Each extra bit of complexity is going to
    cost you, so don’t do it unless it improves the fit by at least this amount…”*
  prefs: []
  type: TYPE_NORMAL
- en: If the boss regularizes too much — getting tyrannical about simplicity — your
    performance review is going to go terribly unless you oversimplify the model,
    so that’s what you end up doing.
  prefs: []
  type: TYPE_NORMAL
- en: This is called [underfitting](http://bit.ly/mfml_050). Such a model has an excellent
    training score (mostly because of all the simplicity bonuses it won) but a whopper
    of a bias in reality. That’s what you get for insisting that solutions should
    be simpler than your problem requires.
  prefs: []
  type: TYPE_NORMAL
- en: And with that, we’re ready for [Part 3](http://bit.ly/quaesita_bivar3), where
    we bring it all together and cram the bias-variance tradeoff into a convenient
    nutshell for you.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading! How about a course?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you had fun here and you’re looking for a leadership-oriented course designed
    to delight AI beginners and experts alike, [here’s a little something](https://bit.ly/funaicourse)
    I made for you:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/300b5280620ea948fc3dbffb708084d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Course link: [https://bit.ly/funaicourse](https://bit.ly/funaicourse)'
  prefs: []
  type: TYPE_NORMAL
- en: Looking for hands-on ML/AI tutorials?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some of my favorite 10 minute walkthroughs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[AutoML](https://console.cloud.google.com/?walkthrough_id=automl_quickstart)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Vertex AI](https://bit.ly/kozvertex)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AI notebooks](https://bit.ly/kozvertexnotebooks)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ML for tabular data](https://bit.ly/kozvertextables)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Text classification](https://bit.ly/kozvertextext)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Image classification](https://bit.ly/kozverteximage)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Video classification](https://bit.ly/kozvertexvideo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
