- en: 'Understanding TF-IDF: A Traditional Approach to Feature Extraction in NLP'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understanding-tf-idf-a-traditional-approach-to-feature-extraction-in-nlp-a5bfbe04723f](https://towardsdatascience.com/understanding-tf-idf-a-traditional-approach-to-feature-extraction-in-nlp-a5bfbe04723f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn the fundamentals of TF-IDF and how to implement it from scratch in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://itsuncheng.medium.com/?source=post_page-----a5bfbe04723f--------------------------------)[![Raymond
    Cheng](../Images/09f3d83726274188c58f8eebc79abb0e.png)](https://itsuncheng.medium.com/?source=post_page-----a5bfbe04723f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a5bfbe04723f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a5bfbe04723f--------------------------------)
    [Raymond Cheng](https://itsuncheng.medium.com/?source=post_page-----a5bfbe04723f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a5bfbe04723f--------------------------------)
    ·9 min read·Mar 30, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/93fb9b8062226265d6274ca8a0717cfe.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Aaron Burden](https://unsplash.com/@aaronburden?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature extraction is an important initial step in [NLP](https://en.m.wikipedia.org/wiki/Natural_language_processing),
    which involves transforming textual data into a mathematical representation, often
    in the form of vectors, known as [word embeddings](https://en.wikipedia.org/wiki/Word_embedding).
    Various word embedding approaches exist, ranging from classical approaches like
    [word2vec](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)
    and [GloVe](https://nlp.stanford.edu/pubs/glove.pdf) to more modern ones like
    [BERT](https://arxiv.org/pdf/1810.04805.pdf) embeddings. While [transformer](https://arxiv.org/pdf/1706.03762.pdf)-based
    embeddings dominate the field of NLP today, understanding the evolution of previous
    approaches can be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will explore a more traditional approach to feature extraction
    known as [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), which is based
    on statistical analysis. We will delve into the details of TF-IDF and its implementation,
    as well as provide a bonus application to help solidify your understanding. So,
    stay with us until the end as we uncover the ins and outs of TF-IDF!
  prefs: []
  type: TYPE_NORMAL
- en: What is TF-IDF?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TF-IDF, short for Term Frequency-Inverse Document Frequency, is a commonly used
    technique in NLP to determine the significance of words in a document or corpus.
    To give some background context, a [survey](http://nbn-resolving.de/urn:nbn:de:bsz:352-0-311312)
    conducted in 2015 showed that 83% of text-based recommender systems in digital
    libraries use TF-IDF for extracting textual features. That’s how popular the technique
    is. Essentially, it measures the importance of a word by comparing its frequency
    within a specific document with the frequency to its frequency in the entire corpus.
    The underlying assumption is that a word that occurs more frequently within a
    document but rarely in the corpus is particularly important in that document.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s take a look at the mathematical formula for calculating TF-IDF:'
  prefs: []
  type: TYPE_NORMAL
- en: TF (Term Frequency) is determined by calculating the frequency of a word in
    a document and dividing it by the total number of words in the document.
  prefs: []
  type: TYPE_NORMAL
- en: '`TF = (Number of times the word appears in the document) / (Total number of
    words in the document)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'IDF (Inverse Document Frequency), on the other hand, measures the importance
    of a word within the corpus as a whole. It is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '`IDF = log((Total number of documents in the corpus) / (Number of documents
    containing the word))`'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the TF-IDF score for a word in a given document is the product of the
    word’s TF and IDF scores. The higher the resulting TF-IDF score, the more significant
    the word is considered to be in the context of that document compared to other
    words.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing TF-IDF in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we’ve covered how TF-IDF is calculated mathematically, let’s implement
    it using Python. While there are libraries available to compute the TF-IDF features
    faster, this article will focus on building it from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up and Preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To begin, let’s import the necessary packages that we’ll need later on, such
    as the [Counter](https://docs.python.org/3/library/collections.html#collections.Counter)
    class from the [collections](https://docs.python.org/3/library/collections.html)
    module.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll define a list of documents/corpus that we’ll use as an example.
    Let’s borrow from the recent craze around [ChatGPT](https://openai.com/blog/chatgpt)
    and Generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Before calculating text features, it’s standard practice to first preprocess
    the documents in any NLP task, such as lowercase, lemmatization, stemming, stopword
    removal, etc. In this example, we’ll lowercase the documents and remove any punctuation.
    However, more preprocessing can be done depending on the task at hand, and these
    steps can be performed using NLP libraries such as [NLTK](https://www.nltk.org/)
    or [SpaCy](https://spacy.io/). We’ll also keep track of the unique set of tokens
    in the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/82a5e2cdafdbfa1b583368c6e9736eb8.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculating IDF of unique tokens in the corpus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After we have the token set, we can calculate the IDF of each token in the corpus
    using the formula given above.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/bd703ad0671c2cd98139794ca09e3091.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that words like “openai” and “gpt” have a higher IDF than words like
    “chatgpt” or “ai”, since the former terms occur less frequently in the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating TF of each token for each document
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While IDF is calculated per token on the entire corpus, TF is calculated for
    each token on a per-document basis. Using the formula for TF, we can quickly get
    the count of a token in a document and calculate its relative frequency using
    the Counter class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here’s an example of the term frequency of “chatgpt” in each document.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b57febf6079ecb51e0ab71ec749a0f95.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, onto TF-IDF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, it’s time to implement the TF-IDF function. We can first wrap the previous
    preprocessing code into a function so we can call it when calculating the TF-IDF.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Let’s try some examples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f04b85ff7921b0677fbda4afcadaa67f.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1a1e5a26a1bca510ff676fbefe9ff924.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d76775abd9825e4224e50159eb6ace36.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/29d49678ff01770241089eac059a6ffc.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/aa99415bc1f7bf4988cca71b4089bba7.png)'
  prefs: []
  type: TYPE_IMG
- en: From the above examples, we can see that “chatgpt” has a higher TF-IDF in documents
    1 and 2 than document 3 since the term doesn’t appear in document 3\. Although
    “chatgpt” occurs in document 1 and 2 only once, the former has a slightly higher
    TF since it has fewer tokens, leading to a higher TF-IDF.
  prefs: []
  type: TYPE_NORMAL
- en: The “gpt” TF-IDF is 0 in documents 1 and 3, since neither of them contains the
    word. “gpt” in document 2 is present; however, the TF-IDF is higher than the TF-IDF
    of “chatgpt” in document 1 since its rare occurrence in the corpus outweighs the
    longer length of the second document.
  prefs: []
  type: TYPE_NORMAL
- en: “generative” has a TF-IDF of 0 in documents 1 and 2 due to its non-occurrence.
    “is” has a 0 TF-IDF for all documents since it is present in all of them. Out-of-vocabulary
    tokens like “gpt4” also have a TF-IDF of 0.
  prefs: []
  type: TYPE_NORMAL
- en: And here you have it, how you could implement TF-IDF in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bonus: Building a search engine MVP with TF-IDF'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TF-IDF has many potential applications, and one of them is building a search
    engine. In this section, we will explore how we can use TF-IDF to build a simple
    search engine MVP. The approach we will take is to rank documents against a query
    by the sum of their TF-IDF terms, with a higher sum resulting in a higher document
    ranking.
  prefs: []
  type: TYPE_NORMAL
- en: To get started, let’s modify the `tf_idf` function to append the TF-IDF results
    to a list and set the TF-IDF of out-of-vocabulary words to 0.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: With this modification, we can now implement the search engine. The code below
    computes the TF-IDF values of each query term against the corpus and ranks the
    documents in order of their summation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s try searching for the query “ChatGPT AI”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1891b13cbef3b6ac4bfd5f8c46090114.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that “ChatGPT AI” is most relevant to document 1 because of its highest
    TF-IDF. This makes sense since “ChatGPT” and “AI” are both included in the document
    while each of the other two documents contains only one of the terms.
  prefs: []
  type: TYPE_NORMAL
- en: What will happen if we try “AI language models”?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c2357be84d8e9a89adeefa10949e0f70.png)'
  prefs: []
  type: TYPE_IMG
- en: Document 2 is ranked the highest, since it contains two tokens “language” and
    “models”, while the other two only contain “AI”. Document 1 is ranked higher than
    3 in this case since it is shorter, resulting in slightly higher term frequency.
  prefs: []
  type: TYPE_NORMAL
- en: And here you have it, an extremely simple search engine based on TF-IDF!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article, we learned about TF-IDF, what it is, how it works, and most
    importantly, why it’s needed in modern NLP applications. We also implemented TF-IDF
    and showed that we can even build a minimalistic search engine out of it. If you
    don’t want to implement it from scratch, you can use `[sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)`
    to implement it faster. Here are two great blog articles that cover how to use
    it: [1](https://www.analyticsvidhya.com/blog/2021/11/how-sklearns-tfidfvectorizer-calculates-tf-idf-values/)
    and [2](https://okan.cloud/posts/2022-01-16-text-vectorization-using-python-tf-idf/).'
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF is not the only feature extraction method for text. There are many more
    methods that might perform better than it, and it’s worth understanding them.
    Nonetheless, it’s still a popular text feature approach, and we might cover the
    more recent ones in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading. If this helps, feel free to follow me and subscribe for
    more articles to come.
  prefs: []
  type: TYPE_NORMAL
- en: Hope you had a great time.
  prefs: []
  type: TYPE_NORMAL
- en: Cheers.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Mikolov, Tomas, et al. “Distributed representations of words and phrases
    and their compositionality.” *Advances in neural information processing systems*
    26 (2013).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. “Glove:
    Global vectors for word representation.” *Proceedings of the 2014 conference on
    empirical methods in natural language processing (EMNLP)*. 2014.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers
    for language understanding.” *arXiv preprint arXiv:1810.04805* (2018).'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Vaswani, Ashish, et al. “Attention is all you need.” *Advances in neural
    information processing systems* 30 (2017).'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Beel, Joeran, et al. “Paper recommender systems: a literature survey.”
    *International Journal on Digital Libraries* 17 (2016): 305–338.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] “How sklearn’s Tfidfvectorizer Calculates tf-idf Values”, Analytics Vidhya,
    [https://www.analyticsvidhya.com/blog/2021/11/how-sklearns-tfidfvectorizer-calculates-tf-idf-values/](https://www.analyticsvidhya.com/blog/2021/11/how-sklearns-tfidfvectorizer-calculates-tf-idf-values/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] “Text Vectorization Using Python: TF-IDF”, Okan Bulut, [https://okan.cloud/posts/2022-01-16-text-vectorization-using-python-tf-idf/](https://okan.cloud/posts/2022-01-16-text-vectorization-using-python-tf-idf/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] “Understanding TF-IDF For Machine Learning”, Capital One, [https://www.capitalone.com/tech/machine-learning/understanding-tf-idf/](https://www.capitalone.com/tech/machine-learning/understanding-tf-idf/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] “Understanding TF-ID: A Simple Introduction”, MonkeyLearn, [https://monkeylearn.com/blog/what-is-tf-idf/](https://monkeylearn.com/blog/what-is-tf-idf/)'
  prefs: []
  type: TYPE_NORMAL
