- en: Generating Medical Images with MONAI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 MONAI 生成医疗图像
- en: 原文：[https://towardsdatascience.com/generating-medical-images-with-monai-e03310aa35e6](https://towardsdatascience.com/generating-medical-images-with-monai-e03310aa35e6)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/generating-medical-images-with-monai-e03310aa35e6](https://towardsdatascience.com/generating-medical-images-with-monai-e03310aa35e6)
- en: '*An end-to-end open-source project using the latest MONAI Generative Models
    to produce chest X-ray images from radiological reports text*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*一个端到端的开源项目，使用最新的 MONAI 生成模型从放射报告文本生成胸部 X 光图像*'
- en: '[](https://medium.com/@walhugolp?source=post_page-----e03310aa35e6--------------------------------)[![Walter
    Hugo Lopez Pinaya](../Images/0c132d0d1321790b0cea880800d231e0.png)](https://medium.com/@walhugolp?source=post_page-----e03310aa35e6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e03310aa35e6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e03310aa35e6--------------------------------)
    [Walter Hugo Lopez Pinaya](https://medium.com/@walhugolp?source=post_page-----e03310aa35e6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@walhugolp?source=post_page-----e03310aa35e6--------------------------------)[![Walter
    Hugo Lopez Pinaya](../Images/0c132d0d1321790b0cea880800d231e0.png)](https://medium.com/@walhugolp?source=post_page-----e03310aa35e6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e03310aa35e6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e03310aa35e6--------------------------------)
    [Walter Hugo Lopez Pinaya](https://medium.com/@walhugolp?source=post_page-----e03310aa35e6--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e03310aa35e6--------------------------------)
    ·14 min read·Apr 14, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e03310aa35e6--------------------------------)
    ·阅读时间 14 分钟·2023年4月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Hi everybody! In this post, we will create a Latent Diffusion Model to generate
    Chest X-Ray images using the new open-source extension for MONAI, [**MONAI Generative
    Models**](https://github.com/Project-MONAI/GenerativeModels)!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好！在这篇文章中，我们将使用 MONAI 的新开源扩展[**MONAI 生成模型**](https://github.com/Project-MONAI/GenerativeModels)创建一个潜在扩散模型来生成胸部
    X 光图像！
- en: '![](../Images/51dca716335eeca38fff769dc85c3648.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51dca716335eeca38fff769dc85c3648.png)'
- en: '**Introduction**'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: Generative AI has a huge potential for healthcare since it allows us to create
    models that learn the underlying patterns and structure of the training dataset.
    This way, we can use these generative models to create an unlimited amount of
    synthetic data with the same details and characteristics of real data but without
    their restrictions. Given its importance, we created ***MONAI Generative Models*,
    an open-source extension to the** [**MONAI platform**](https://monai.io/) containing
    the latest models (like Diffusion Models, Autoregressive Transformers, and Generative
    Adversarial Networks) and components that help with the training and evaluate
    generative models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式 AI 在医疗领域具有巨大的潜力，因为它允许我们创建学习训练数据集底层模式和结构的模型。这样，我们可以使用这些生成模型创建大量合成数据，这些数据具有真实数据的相同细节和特征，但没有其限制。鉴于其重要性，我们创建了***MONAI
    生成模型*，这是对** [**MONAI 平台**](https://monai.io/)的开源扩展，包含最新的模型（如扩散模型、自回归变换器和生成对抗网络）以及帮助训练和评估生成模型的组件。
- en: '![](../Images/c5d4f6f58dff9be2230f424112fd52a4.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5d4f6f58dff9be2230f424112fd52a4.png)'
- en: In this post, we will go through a complete project to create a Latent Diffusion
    Model (the same type of model as [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release))
    capable of **generating Chest X-Rays (CXR) images from radiological reports**.
    Here, we tried to make the code easy to understand and to be adapted to different
    environments, so, although it is not the most efficient one, I hope you enjoy
    it!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将通过一个完整的项目来创建一个潜在扩散模型（与[稳定扩散](https://stability.ai/blog/stable-diffusion-public-release)相同类型的模型），该模型能够**从放射报告中生成胸部
    X 光（CXR）图像**。在这里，我们尝试使代码易于理解并适应不同环境，所以尽管它不是最有效的，希望你喜欢！
- en: You can find the **complete open-source project at this** [**GitHub repository**](https://github.com/Warvito/generative_chestxray),
    where in this post we are referencing to the release v0.2.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这个[**GitHub 仓库**](https://github.com/Warvito/generative_chestxray)找到**完整的开源项目**，在这篇文章中，我们引用的是版本
    v0.2。
- en: Dataset
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: First, we start with the dataset. In this project, we are using the [**MIMIC
    Dataset**](https://www.nature.com/articles/s41597-019-0322-0). To access this
    dataset, it is necessary to create an account at the [Physionet portal](https://physionet.org/).
    We will use [**MIMIC-CXR-JPG**](https://physionet.org/content/mimic-cxr-jpg/2.0.0/)
    (which contains the JPG files) and [**MIMIC-C**XR](https://physionet.org/content/mimic-cxr/2.0.0/)
    (that includes the radiological reports). Both datasets are under the [*PhysioNet
    Credentialed Health Data License 1.5.0*](https://physionet.org/content/mimic-cxr/view-license/2.0.0/)*.*
    After completing the free training course, you can freely download the dataset
    using the instruction at the bottom of the dataset page. Originally, the CXR images
    have about +1000x1000 pixels. So, this step can take a while.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从数据集开始。在这个项目中，我们使用[**MIMIC数据集**](https://www.nature.com/articles/s41597-019-0322-0)。要访问这个数据集，必须在[Physionet门户](https://physionet.org/)创建一个账户。我们将使用[**MIMIC-CXR-JPG**](https://physionet.org/content/mimic-cxr-jpg/2.0.0/)（包含JPG文件）和[**MIMIC-C**XR](https://physionet.org/content/mimic-cxr/2.0.0/)（包含放射学报告）。这两个数据集都受到[*PhysioNet认证健康数据许可证1.5.0*](https://physionet.org/content/mimic-cxr/view-license/2.0.0/)*的约束。完成免费的培训课程后，你可以按照数据集页面底部的说明自由下载数据集。原始的CXR图像大约为+1000x1000像素。因此，这一步可能需要一些时间。
- en: 'Chest X-ray images are a crucial tool to provide valuable information about
    the structures and organs within the chest cavity, including the lungs, heart,
    and blood vessels, and after download, we should have more than 350k of them!
    These images are one of the three different projections: **Posterior-Anterior
    (PA), Anterior-Posterior (AP), and Lateral (LAT)**. For this project, we are interested
    only in the **PA projection**, the most common one where we can visualise most
    of the features mentioned in the radiological reports (ending with 96,162 images).
    Regarding the **reports**, we have 85,882 files, each containing several text
    sections. Here we will use the **Findings** (mainly explaining the contents in
    the image) and **Impressions** (summarising the report’s contents, like a conclusion).
    To make our models and training process more manageable, we will resize the images
    to have 512 pixels on the smallest axis. The list of scripts to automatically
    perform these initial steps can be found in [here](https://github.com/Warvito/generative_chestxray#preprocessing).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 胸部X光图像是提供关于胸腔内结构和器官（包括肺部、心脏和血管）宝贵信息的重要工具，下载后我们应该有超过350k张这样的图像！这些图像是三种不同投影中的一种：**后前位（PA）、前后位（AP）和侧位（LAT）**。对于这个项目，我们只关注**PA投影**，这是最常见的一种，可以可视化放射学报告中提到的大多数特征（共计96,162张图像）。关于**报告**，我们有85,882个文件，每个文件包含多个文本部分。我们将使用**发现**（主要解释图像内容）和**印象**（总结报告内容，如结论）。为了使我们的模型和训练过程更易于管理，我们将图像调整为最小轴512像素。自动执行这些初始步骤的脚本列表可以在[这里](https://github.com/Warvito/generative_chestxray#preprocessing)找到。
- en: '**Models**'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**模型**'
- en: '![](../Images/07b771040df2f8e61db7e586c3eef069.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07b771040df2f8e61db7e586c3eef069.png)'
- en: 'Latent Diffusion Model: The autoencoder compresses the inputted image x to
    a latent representation z, and then the diffusion model estimates the probability
    distribution of z'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在扩散模型：自编码器将输入图像x压缩为潜在表示z，然后扩散模型估计z的概率分布
- en: 'The Latent Diffusion Models are composed of several parts:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在扩散模型由几个部分组成：
- en: An **Autoencoder** that performs the compression of the inputted images into
    a smaller latent representation;
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个**自编码器**，用于将输入图像压缩为更小的潜在表示；
- en: A **Diffusion Model** that will learn the probability data distribution of the
    latent representations of the CXR;
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个**扩散模型**，将学习CXR潜在表示的概率数据分布；
- en: A **Text Encoder** which creates an embedding vector that will condition the
    sampling process. In this example, we are using a pretrained one.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个**文本编码器**，它创建一个嵌入向量，用于条件化采样过程。在这个例子中，我们使用一个预训练的编码器。
- en: Using MONAI Generative Models, we can easily create and train these models,
    so let’s start with the Autoencoder!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MONAI生成模型，我们可以轻松创建和训练这些模型，因此我们从自编码器开始吧！
- en: '**Models — Autoencoder with KL regularization**'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**模型 — 带KL正则化的自编码器**'
- en: The main goal of the Autoencoder with KL regularization (**AE-kl** or, in some
    projects, simply called as a VAE) is to be able to create a small latent representation,
    and also to reconstruct a image with high-fidelity (preserving as most as possible
    details). In this project, we are creating an autoencoder with four levels, with
    64, 128, 128, 128 channels, where we apply a downsampling block between each level,
    making the feature maps smaller as we go to the deepest layers. Although our Autoencoder
    can have blocks with self-attention, in this example, we are adopting a structure
    similar to our previous study on brain images and using no attention to save memory
    usage. Finally, our latent representation has three channels.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 带有 KL 正则化的自编码器（**AE-kl**，或在一些项目中简单称为 VAE）的主要目标是能够创建一个小的潜在表示，并且高保真地重建图像（尽可能保留细节）。在这个项目中，我们创建了一个四层的自编码器，具有
    64、128、128、128 个通道，其中在每一层之间应用了一个降采样块，使得特征图在深入层次时变小。尽管我们的自编码器可以有自注意力块，但在这个例子中，我们采用了类似于我们之前在脑部图像研究中的结构，并且不使用注意力以节省内存使用。最后，我们的潜在表示有三个通道。
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note: In our [script](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/train_aekl.py#L77),
    we are using the [**OmegaConf packag**e](https://omegaconf.readthedocs.io/en/2.3_branch/)
    to store the hyperparameters of our model. You can see the previous configuration
    in this [file](https://github.com/Warvito/generative_chestxray/blob/main/configs/stage1/aekl_v0.yaml).
    In summary, OmegaConf is a powerful tool for managing configurations in Python
    projects, particularly those that involve deep learning or other complex software
    systems. OmegaConf allows us to conveniently organise the hyperparameters in the
    .yaml files and read them in the script.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在我们的[脚本](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/train_aekl.py#L77)中，我们使用了[**OmegaConf
    包**](https://omegaconf.readthedocs.io/en/2.3_branch/)来存储我们模型的超参数。你可以在这个[文件](https://github.com/Warvito/generative_chestxray/blob/main/configs/stage1/aekl_v0.yaml)中查看之前的配置。总之，OmegaConf
    是一个强大的工具，用于管理 Python 项目的配置，特别是那些涉及深度学习或其他复杂软件系统的项目。OmegaConf 允许我们方便地在 .yaml 文件中组织超参数，并在脚本中读取它们。
- en: '**Training AE-KL**'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**训练 AE-KL**'
- en: Next, we define a few components of our training process. First, we have the
    [**KL regularisation**](https://github.com/Warvito/generative_chestxray/blob/731ee63c5fe28353ddeae548fba9286ee4232bd0/src/python/training/training_functions.py#L161).
    This part is responsible for evaluating the distance between the distribution
    of the latent space of the diffusion models and a Gaussian distribution. As proposed
    by [Rombach et al.](https://arxiv.org/abs/2112.10752), this will be used to restrict
    the variance of the latent space, which is useful when we train the diffusion
    model on it (more about it later). The forward method of our model returns the
    reconstruction, as well as the μ and σ vectors of our latent representation, which
    we use to compute the KL divergence.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义了我们训练过程中的几个组件。首先，我们有[**KL 正则化**](https://github.com/Warvito/generative_chestxray/blob/731ee63c5fe28353ddeae548fba9286ee4232bd0/src/python/training/training_functions.py#L161)。这部分负责评估扩散模型潜在空间分布与高斯分布之间的距离。正如[Rombach
    等人](https://arxiv.org/abs/2112.10752)所提出的，这将用于限制潜在空间的方差，这在我们对其进行扩散模型训练时非常有用（稍后会详细说明）。我们模型的前向方法返回重建结果，以及我们潜在表示的
    μ 和 σ 向量，我们用这些来计算 KL 散度。
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Second, we have our[**Pixel-level loss**](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/training_functions.py#L158),
    where in this project, we are adopting an L1 distance to evaluate how much our
    AE-kl reconstruction differs from the original image.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们有我们的[**像素级损失**](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/training_functions.py#L158)，在这个项目中，我们采用
    L1 距离来评估 AE-kl 重建与原始图像的差异。
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, we have our [**Perceptual-level loss**](https://github.com/Warvito/generative_chestxray/blob/731ee63c5fe28353ddeae548fba9286ee4232bd0/src/python/training/train_aekl.py#L79).
    The idea of perceptual loss is that instead of evaluating the difference between
    the inputted image and the reconstruction at the pixel level, we pass both images
    through a pre-trained model. Then, we measure the distance of the internal activations
    and feature maps. In MONAI Generative models, we made it easy to use perceptual
    networks based on networks pre-trained on medical images (available [here](https://github.com/Project-MONAI/GenerativeModels/blob/main/generative/losses/perceptual.py)).
    We have access to the **2D networks** from the **RadImageNet** study (from [Mei
    et al.](https://pubs.rsna.org/doi/10.1148/ryai.210315)), which were trained on
    more than **1.3 million medical images!** We implemented the **2.5D approach**,
    using 2D pre-trained networks to evaluate 3D images by evaluating slices. And
    finally, we have access to [**MedicalNet**](https://github.com/Tencent/MedicalNet)
    to evaluate our **3D images** in a 3D pure method. In this project, we are using
    a similar approach to [Pinaya et al.](https://arxiv.org/abs/2209.07162) and use
    the Learned Perceptual Image Patch Similarity (**LPIPS**) metric (also available
    at MONAI Generative Models).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有[**感知级损失**](https://github.com/Warvito/generative_chestxray/blob/731ee63c5fe28353ddeae548fba9286ee4232bd0/src/python/training/train_aekl.py#L79)。感知损失的想法是，不是评估输入图像与重建图像在像素级别的差异，而是将这两张图像通过一个预训练模型。然后，我们测量内部激活和特征图的距离。在MONAI生成模型中，我们简化了使用基于在医学图像上预训练的网络的感知网络（可在[此处](https://github.com/Project-MONAI/GenerativeModels/blob/main/generative/losses/perceptual.py)获取）。我们可以使用**RadImageNet**研究中的**2D网络**（来自[Mei等](https://pubs.rsna.org/doi/10.1148/ryai.210315)），这些网络在超过**130万张医学图像**上进行了训练！我们实现了**2.5D方法**，使用2D预训练网络通过评估切片来评估3D图像。最后，我们可以访问[**MedicalNet**](https://github.com/Tencent/MedicalNet)来以纯3D方法评估我们的**3D图像**。在这个项目中，我们使用了类似于[Pinaya等](https://arxiv.org/abs/2209.07162)的方法，并使用了学习到的感知图像块相似度（**LPIPS**）度量（也可在MONAI生成模型中找到）。
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Finally, we use [**Adversarial loss**](https://github.com/Warvito/generative_chestxray/blob/731ee63c5fe28353ddeae548fba9286ee4232bd0/src/python/training/training_functions.py#L148)
    to deal with the fine details of the reconstructions. The Adversarial Network
    was a [**Patch-Discriminator**](https://github.com/Warvito/generative_chestxray/blob/731ee63c5fe28353ddeae548fba9286ee4232bd0/src/python/training/train_aekl.py#L78)
    (initially proposed by the Pix2Pix study), where instead of having only one prediction
    about if the whole image was real or fake, we have predictions for several patches
    from the image.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用[**对抗损失**](https://github.com/Warvito/generative_chestxray/blob/731ee63c5fe28353ddeae548fba9286ee4232bd0/src/python/training/training_functions.py#L148)来处理重建的细节。对抗网络是[**Patch-Discriminator**](https://github.com/Warvito/generative_chestxray/blob/731ee63c5fe28353ddeae548fba9286ee4232bd0/src/python/training/train_aekl.py#L78)（最初由Pix2Pix研究提出），其中我们不仅对整个图像是否真实或虚假做出一个预测，还对图像中的多个块做出预测。
- en: Unlike the original Latent Diffusion Model and Stable Diffusion, we used discriminator
    losses from the least square GANs. Although it is not the more advanced adversarial
    loss, it has shown efficacy and stability when training on 3D medical images as
    well (but still room for improvement 😁). Although adversarial losses can be quite
    unstable, their combination with perceptual losses also helps to stabilise the
    loss of the discriminator and generator.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始的潜在扩散模型（Latent Diffusion Model）和稳定扩散（Stable Diffusion）不同，我们使用了来自最小二乘生成对抗网络（least
    square GANs）的鉴别器损失。虽然这不是更先进的对抗损失，但在对3D医学图像进行训练时已经显示出了有效性和稳定性（但仍有改进的空间😁）。尽管对抗损失可能相当不稳定，但它们与感知损失的结合也有助于稳定鉴别器和生成器的损失。
- en: Our training loops and evaluation steps can be found at [here](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/training_functions.py#L129)
    and [here](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/training_functions.py#L236).
    After train for 75 epoch, we save our model with the MLflow package. We use the
    [MLflow package](https://mlflow.org/) to better monitoring of our experiments
    since it organises information like git hash and parameters, as well as makes
    it possible to store different runs with a unique ID in groups (called experiments)
    and making easier to compare different results (similar to others tools, like
    weights and biases). The logs files for the AE-KL can be found [here](https://drive.google.com/drive/folders/1Ots9ujg4dSaTABKkyaUnFLpCKQ7kCgdK?usp=sharing).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练循环和评估步骤可以在[这里](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/training_functions.py#L129)和[这里](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/training_functions.py#L236)找到。经过75轮训练后，我们使用
    MLflow 包保存模型。我们使用[MLflow 包](https://mlflow.org/)来更好地监控实验，因为它组织了如 git hash 和参数等信息，并且使得可以用唯一的
    ID 在组（称为实验）中存储不同的运行，并且使得比较不同结果变得更容易（类似于其他工具，如 weights and biases）。AE-KL 的日志文件可以在[这里](https://drive.google.com/drive/folders/1Ots9ujg4dSaTABKkyaUnFLpCKQ7kCgdK?usp=sharing)找到。
- en: '**Models — Diffusion Model**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**模型 — 扩散模型**'
- en: Next, we need to train our diffusion model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要训练我们的扩散模型。
- en: '![](../Images/af13eea9c3b6fc26b7df7368c9e7082e.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af13eea9c3b6fc26b7df7368c9e7082e.png)'
- en: The diffusion model is a U-Net like network where traditionally, it receives
    a noisy image (or latent representation) as input and will predict its noise component.
    These models use an iterative denoising mechanism to generate images from noise
    across a Markov Chain with several steps. For this reason, the model is also conditioned
    on the timestep defining in which stage of the sampling process the model is.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型是类似于 U-Net 的网络，其中通常接收一个噪声图像（或潜在表示）作为输入，并预测其噪声成分。这些模型使用迭代去噪机制，通过具有多个步骤的马尔可夫链从噪声中生成图像。因此，该模型还依赖于时间步，以定义模型在采样过程的哪个阶段。
- en: '![](../Images/2332b550309afa5ed1003f9927d86934.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2332b550309afa5ed1003f9927d86934.png)'
- en: Using the [**DiffusionModelUNet class**](https://github.com/Project-MONAI/GenerativeModels/blob/02de27c1937f3d9ca3483ab2f4661270c9137641/generative/networks/nets/diffusion_model_unet.py#L1624),
    we can create the U-Net like network for our diffusion mdel. Our project uses
    the configuration defined in this [config file](https://github.com/Warvito/generative_chestxray/blob/main/configs/ldm/ldm_v0.yaml)
    where it defines input and output with 3 channels (as our AE-kl have a latent
    space with 3 channels), and 3 different levels with 256, 512, 768 channels. Each
    level has 2 residual blocks. As mentioned, it is important to pass the timestep
    for the model where it is used to condition the behaviour of these residual blocks.
    Finally, we define the attention mechanisms inside the network. In our case, we
    have attention blocks in the second and third levels (indicated by the attention_levels
    argument), each with 512 and 768 channels per attention head (in other words,
    we have a single attention head in each level). These attention mechanisms are
    important because they allow us to apply our **external conditioning** (the radiological
    reports) to the network via the **cross-attention method**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[**DiffusionModelUNet 类**](https://github.com/Project-MONAI/GenerativeModels/blob/02de27c1937f3d9ca3483ab2f4661270c9137641/generative/networks/nets/diffusion_model_unet.py#L1624)，我们可以为我们的扩散模型创建类似于
    U-Net 的网络。我们的项目使用在此[配置文件](https://github.com/Warvito/generative_chestxray/blob/main/configs/ldm/ldm_v0.yaml)中定义的配置，其中定义了具有
    3 个通道的输入和输出（因为我们的 AE-kl 具有 3 个通道的潜在空间），以及具有 256、512、768 通道的 3 个不同级别。每个级别有 2 个残差块。如前所述，传递时间步对模型很重要，它用于调整这些残差块的行为。最后，我们在网络内部定义注意力机制。在我们的情况下，我们在第二和第三级别（由
    attention_levels 参数指示）中有注意力块，每个注意力头有 512 和 768 个通道（换句话说，每个级别有一个注意力头）。这些注意力机制很重要，因为它们允许我们通过**交叉注意力方法**将我们的**外部条件**（放射学报告）应用到网络中。
- en: '![](../Images/2a8019f23029a2286aba03afefb10013.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a8019f23029a2286aba03afefb10013.png)'
- en: External conditioning (or “context”) is applied to the U-Net’s attention blocks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 外部条件（或“上下文”）被应用到 U-Net 的注意力块。
- en: In our project, we are using an already **trained textual encoder**. For simplicity,
    we are using the same one from the Stable Diffusion v2.1 model (“stabilityai/stable-diffusion-2–1-base”)
    to convert our text tokens into a text embedding that will be used as Key and
    Value vectors in the DiffusionModel UNet cross attention layers. Each token of
    our textual embedding have 1024 dimensions and we define it in the *“with_conditioning”*
    and “*cross_attention_dim*” arguments.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的项目中，我们使用了一个已经**训练好的文本编码器**。为简单起见，我们使用了来自稳定扩散 v2.1 模型（“stabilityai/stable-diffusion-2–1-base”）的相同编码器，将我们的文本标记转换为文本嵌入，这些嵌入将作为
    DiffusionModel UNet 交叉注意力层中的 Key 和 Value 向量。我们文本嵌入的每个标记有 1024 个维度，我们在*“with_conditioning”*和“*cross_attention_dim*”参数中定义它。
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Besides our model definition, it is important to define how the noise of the
    diffusion model will be added to the inputted images during training and removed
    during the sampling. For that, we implemented the Schedulers classes to our MONAI
    Generative Models to define the noise schedulers. In this example, we will use
    a [DDPMScheduler](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/train_ldm.py#L103),
    with 1000 time steps and the following hyperparameters.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型定义之外，重要的是定义在训练过程中将噪声添加到输入图像中以及在采样过程中去除噪声的方式。为此，我们为我们的 MONAI 生成模型实现了 Schedulers
    类以定义噪声调度器。在这个示例中，我们将使用一个[DDPMScheduler](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/train_ldm.py#L103)，它具有
    1000 个时间步长和以下超参数。
- en: '![](../Images/e27af83fafc808464975a59d74972a63.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e27af83fafc808464975a59d74972a63.png)'
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we opted for a “**v-prediction” approach**, where our U-Net will try to
    predict the velocity component (a combination of the original image and the added
    noise) instead of just the added noise. This approach has been shown to have more
    stable training and faster convergence (also used in [https://arxiv.org/abs/2210.02303](https://arxiv.org/abs/2210.02303)).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们选择了**“v-prediction”方法**，其中我们的 U-Net 将尝试预测速度分量（原始图像和添加噪声的组合），而不仅仅是添加的噪声。这种方法已被证明具有更稳定的训练和更快的收敛速度（也在[https://arxiv.org/abs/2210.02303](https://arxiv.org/abs/2210.02303)中使用）。
- en: '**Training Diffusion Model**'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**扩散模型训练**'
- en: Before training the Diffusion Model, we need to find an appropriate scaling
    factor. As mentioned in Rombach et al., the signal-to-noise ratio can affect the
    results obtained with the LDM, if the standard deviation of the latent space distribution
    is too high. If the values of the latent representation are too high, the maximum
    amount of Gaussian noise we add to it might not be enough to destroy all information.
    This way, during training, information of the original latent representation might
    be present when it was not supposed to be, making it not possible later sample
    an image from pure noise. The KL regularisation can help a little bit with this,
    but it is best practice to use a scaling factor to adapt the latent representation
    values. In this [script](https://github.com/Warvito/generative_chestxray/blob/main/src/python/training/eda_ldm_scaling_factor.py),
    we verify the size of the standard deviation of the components of the latent space
    in one of the batches of the training set. We found that our scaling factor should
    be at least **0.8221**. In our case, we used a more conservative value of 0.3
    (similar to values from Stable Diffusion).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练扩散模型之前，我们需要找到一个合适的缩放因子。如 Rombach 等人所述，如果潜在空间分布的标准差过高，信噪比可能会影响使用 LDM 获得的结果。如果潜在表示的值过高，我们添加的高斯噪声的最大量可能不足以破坏所有信息。这样，在训练过程中，原始潜在表示的信息可能会存在，而实际上不应该存在，从而使得之后从纯噪声中生成图像变得不可能。KL
    正则化可以在一定程度上有所帮助，但最佳实践是使用缩放因子来调整潜在表示的值。在这个[脚本](https://github.com/Warvito/generative_chestxray/blob/main/src/python/training/eda_ldm_scaling_factor.py)中，我们验证了训练集的一个批次中潜在空间分量的标准差的大小。我们发现我们的缩放因子应该至少为**0.8221**。在我们的案例中，我们使用了更为保守的值
    0.3（类似于稳定扩散中的值）。
- en: With the scaling factor defined, we can train our model. In [here](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/training_functions.py#L424),
    we can check the training loop.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了缩放因子后，我们可以训练我们的模型。在[这里](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/training_functions.py#L424)，我们可以查看训练循环。
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see, we first obtain the images and reports from our data loaders.
    To process our images, we used the [transforms from MONAI](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/util.py#L111)
    and added a few [custom transforms](https://github.com/Warvito/generative_chestxray/blob/main/src/python/training/custom_transforms.py)
    to extract random sentences from the radiological reports and tokenize the inputted
    text. In about 10% of the cases, we use an empty string (“” — which is a vector
    with the Begin-of-Sentence token (*value = 49406*) followed by padding tokens
    (*value = 49407*)) to be able to use classifier free guidance during the sampling.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，我们首先从数据加载器中获取图像和报告。为了处理这些图像，我们使用了 [MONAI 的转换工具](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/util.py#L111)，并添加了一些
    [自定义转换](https://github.com/Warvito/generative_chestxray/blob/main/src/python/training/custom_transforms.py)，以从放射学报告中提取随机句子并对输入文本进行分词。在大约
    10% 的情况下，我们使用一个空字符串（“” — 这是一个包含句子开始标记 (*value = 49406*) 和填充标记 (*value = 49407*)
    的向量），以便在采样过程中使用无分类器引导。
- en: Next, we obtain the latent representation and the prompt embeddings. We create
    the noise to be added, the random timesteps to be used in this iteration, and
    the desired target (velocity component). Finally, we compute our loss using the
    mean squared error.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们获取潜在表示和提示嵌入。我们创建需要添加的噪声、用于本次迭代的随机时间步，以及所需的目标（速度分量）。最后，我们使用均方误差计算损失。
- en: This training goes for 500 epochs, where the logs can be found [here](https://drive.google.com/drive/folders/1Ots9ujg4dSaTABKkyaUnFLpCKQ7kCgdK?usp=sharing).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个训练过程持续 500 个周期，日志可以在 [这里](https://drive.google.com/drive/folders/1Ots9ujg4dSaTABKkyaUnFLpCKQ7kCgdK?usp=sharing)
    找到。
- en: '**Sampling images**'
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**采样图像**'
- en: After we have both models trained, we can sample synthetic images. We use [this
    script](https://github.com/Warvito/generative_chestxray/blob/main/src/python/testing/sample_images.py).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们训练好两个模型之后，我们可以采样合成图像。我们使用 [这个脚本](https://github.com/Warvito/generative_chestxray/blob/main/src/python/testing/sample_images.py)。
- en: '![](../Images/b16bd7d88dfcf2b4792f89bd9fafaca3.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b16bd7d88dfcf2b4792f89bd9fafaca3.png)'
- en: This script uses the classifier-free guidance, which is a method proposed by
    [Ho et al.](https://arxiv.org/abs/2207.12598), to be able to enforce the text
    prompts used in image generation. In this method, we have a guidance scale that
    we can use to sacrifice the diversity of the generated data to obtain a sample
    with higher fidelity to the textual prompt. 7.0 is the default value.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本使用无分类器引导方法，这是 [Ho 等人](https://arxiv.org/abs/2207.12598) 提出的一个方法，用于在图像生成中强制执行文本提示。在这种方法中，我们有一个引导比例，可以用来牺牲生成数据的多样性，以获得与文本提示更高一致性的样本。默认值是
    7.0。
- en: In the following image, we can see how the trained model was able to learn about
    the clinical features, as well as the position and severity of them.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到训练好的模型如何学会了临床特征以及这些特征的位置和严重程度。
- en: '![](../Images/9eb959fffed9d76882c0505540cb2a3e.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9eb959fffed9d76882c0505540cb2a3e.png)'
- en: '**Evaluation**'
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**评估**'
- en: In this section, we will show how to use metrics from MONAI to evaluate the
    performance of our generative models in several aspects.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将展示如何使用 MONAI 的度量标准来评估我们生成模型的多个方面的性能。
- en: '**Quality of the Autoencoder reconstructions with MS-SSIM**'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**使用 MS-SSIM 的自编码器重建质量**'
- en: First, we verify how well our Autoencoder-kl reconstructs the input images.
    This is an important point when developing our models, because the quality of
    the compression and reconstructed data will define a ceiling for the quality of
    our sample. If the model does not learn how to decode the images from the latent
    representation well, or if it does not model our latent space well, it is not
    possible to decode the synthetic representations in a realistic way. In [this
    script](https://github.com/Warvito/generative_chestxray/blob/main/src/python/testing/compute_msssim_reconstruction.py),
    we use the 5000 images from the test set to evaluate our model. We can verify
    how well our reconstructions look using the **Multiscale Structural Similarity
    Index Measure (MS-SSIM)**. The MS-SSIM is a widely used image quality assessment
    method that measures the similarity between two images. Unlike traditional image
    quality assessment methods such as PSNR and SSIM, MS-SSIM is capable of capturing
    the structural information of an image at different scales.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们验证我们的Autoencoder-kl如何重建输入图像。这是开发我们模型时一个重要的点，因为压缩和重建数据的质量将定义我们样本质量的上限。如果模型无法很好地从潜在表示中解码图像，或者它无法很好地建模我们的潜在空间，就无法以现实的方式解码合成表示。在[这个脚本](https://github.com/Warvito/generative_chestxray/blob/main/src/python/testing/compute_msssim_reconstruction.py)中，我们使用了来自测试集的5000张图像来评估我们的模型。我们可以使用**多尺度结构相似性指标（MS-SSIM）**来验证我们的重建效果。MS-SSIM是一种广泛使用的图像质量评估方法，用于衡量两张图像之间的相似性。与传统的图像质量评估方法如PSNR和SSIM不同，MS-SSIM能够捕捉图像在不同尺度上的结构信息。
- en: In this case, the higher the value, the better the model. For our current release
    (version 0.2), we observed that our model had mean **MS-SSIM reconstructions of
    0.9789.**
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，值越高，模型越好。对于我们当前的版本（0.2），我们观察到我们的模型的**MS-SSIM重建平均值为0.9789**。
- en: '**Diversity of the samples with MS-SSIM**'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**样本的MS-SSIM多样性**'
- en: We will first evaluate the diversity of the samples generated by our model.
    For that, we compute the **Multiscale Structural Similarity Index Measure** between
    different generated images. In this project, we assume that, if our generative
    model is capable of generating diverse images, it will present a low average MS-SSIM
    value when comparing pairs of synthetic images. For example, if we had a problem
    like a mode collapse, our generated images would look similar, and the MS-SSIM
    values would be much lower than what we observe in a real dataset.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先评估我们模型生成的样本的多样性。为此，我们计算不同生成图像之间的**多尺度结构相似性指标**。在这个项目中，我们假设如果我们的生成模型能够生成多样化的图像，那么在比较合成图像对时，它会呈现出较低的平均MS-SSIM值。例如，如果我们遇到了模式崩溃问题，我们生成的图像将看起来相似，MS-SSIM值会比我们在真实数据集中观察到的值低得多。
- en: In our project, we are using unconditioned samples (samples generated with the
    “” (empty string) as a textual prompt) to maintain the natural proportion of the
    original dataset. As shown in [this script](https://github.com/Warvito/generative_chestxray/blob/main/src/python/testing/compute_msssim_sample.py),
    we select 1000 synthetic samples of our model and use the data loaders from MONAI
    to help to load all possible pairs of images. We use a nested loop to go through
    all possible pairs and ignore the cases where it is the same image selected in
    both [data loader](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/testing/compute_msssim_sample.py#L89).
    Here we can observe an **MS-SSIM of 0.4083**. We can perform the same evaluation
    in real images from the test set as a reference value. Using [this script](https://github.com/Warvito/generative_chestxray/blob/main/src/python/testing/compute_msssim_test_set.py),
    we obtain **MS-SSIM=0.4046 for the test set**, indicating that our model is generating
    images with a diversity similar to the one observed at the real data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的项目中，我们使用未条件化的样本（使用“”（空字符串）作为文本提示生成的样本）以保持原始数据集的自然比例。如[这个脚本](https://github.com/Warvito/generative_chestxray/blob/main/src/python/testing/compute_msssim_sample.py)所示，我们选择了1000个模型生成的合成样本，并使用MONAI的数据加载器来帮助加载所有可能的图像对。我们使用嵌套循环遍历所有可能的图像对，并忽略在两个[数据加载器](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/testing/compute_msssim_sample.py#L89)中选择相同图像的情况。在这里，我们可以观察到**MS-SSIM为0.4083**。我们可以在测试集的真实图像中进行相同的评估作为参考值。使用[这个脚本](https://github.com/Warvito/generative_chestxray/blob/main/src/python/testing/compute_msssim_test_set.py)，我们得到**测试集的MS-SSIM=0.4046**，这表明我们的模型生成的图像与真实数据中的多样性相似。
- en: However, diversity does not mean the images look good or realistic. So we will
    check the image quality in the next step!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，多样性并不意味着图像看起来好或真实。因此，我们将在下一步检查图像质量！
- en: '**Synthetic Image Quality with FID**'
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**合成图像质量与FID**'
- en: Finally, we measure the **Fréchet inception distance (FID)** metric of the generated
    samples ([link](https://github.com/Warvito/generative_chestxray/blob/main/src/python/testing/compute_fid.py)).
    The FID is a metric that evaluates the distribution between two groups, showing
    how similar they are. For this, we need a pre-trained neural network from which
    we can extract features that we will use to compute the distance (similar to the
    perceptual loss). In this example, we opted to use neural networks available in
    the[**torchxrayvision package**](https://github.com/mlmed/torchxrayvision). We
    used a Dense121 network (*“densenet121-res224-all”*), and we chose this network
    to be close to what is used in the literature for CXR synthetic images. From this
    network, we obtain a feature vector with 1024 dimensions. As recommended in the
    original FID paper, it is important to use a similar amount of examples compared
    to the number of features. For this reason, we use 1000 unconditioned images and
    compare them to 1000 images from the test set. For FIDs, the lower the best, and
    here we obtained a reasonable **FID=9.0237**.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们测量生成样本的**Fréchet Inception Distance (FID)**指标（[链接](https://github.com/Warvito/generative_chestxray/blob/main/src/python/testing/compute_fid.py)）。FID是一个评估两个组之间分布差异的指标，显示它们的相似度。为此，我们需要一个预训练的神经网络，从中提取特征用于计算距离（类似于感知损失）。在这个例子中，我们选择使用[**torchxrayvision包**](https://github.com/mlmed/torchxrayvision)中的神经网络。我们使用了一个Dense121网络（*“densenet121-res224-all”*），并选择了这个网络，以接近文献中用于CXR合成图像的网络。通过这个网络，我们得到一个1024维的特征向量。根据原始FID论文的建议，使用与特征数量相似数量的样本是重要的。因此，我们使用了1000张无条件图像，并将其与1000张测试集图像进行比较。对于FID来说，值越低越好，我们这里得到了一个合理的**FID=9.0237**。
- en: '**Conclusion**'
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: In this post, we present a way to develop a project with **MONAI Generative
    Models**, from downloading the data to evaluating the generative models and synthetic
    data. Although this project version could be more efficient and have better hyperparameters,
    we hope that it illustrates well the different features our extension offers.
    If you have any idea on how to improve our CXR model or if you would like to contribute
    to our package, please, just add comments on our **issue sections** at [here](https://github.com/Warvito/generative_chestxray/issues)
    or [here](https://github.com/Project-MONAI/GenerativeModels/issues).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们展示了一种使用**MONAI生成模型**开发项目的方法，从下载数据到评估生成模型和合成数据。尽管这个项目版本可能更高效并拥有更好的超参数，我们希望它能很好地展示我们扩展所提供的不同功能。如果你有任何改进我们CXR模型的想法，或者希望为我们的包做贡献，请在我们的**问题部分**中留下评论，[在这里](https://github.com/Warvito/generative_chestxray/issues)或[在这里](https://github.com/Project-MONAI/GenerativeModels/issues)。
- en: Our trained model can be found at the [**MONAI Model Zoo**](https://github.com/Project-MONAI/GenerativeModels/tree/main/model-zoo/models)together
    with our 3D Brain Generator and other models. Our Model Zoo makes it easier downloading
    the model weights and the code to perform inference.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练的模型可以在[**MONAI模型库**](https://github.com/Project-MONAI/GenerativeModels/tree/main/model-zoo/models)中找到，与我们的3D脑部生成器及其他模型一起。我们的模型库使得下载模型权重和执行推理代码更加便捷。
- en: '![](../Images/7a16c3aa279b9c4e9eaaef950449cd46.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a16c3aa279b9c4e9eaaef950449cd46.png)'
- en: For more tutorials and to learn more about our features, check out our Tutorial
    page at this [link](https://github.com/Project-MONAI/GenerativeModels/tree/main/tutorials),
    and follow me for the latest updates and more guides like this one! 😁
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 欲获取更多教程并了解我们的功能，请访问我们的教程页面，点击此[链接](https://github.com/Project-MONAI/GenerativeModels/tree/main/tutorials)，并关注我以获取最新更新和更多类似的指南！😁
- en: '*Note: All images unless otherwise noted are by the author*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：所有图片除非另有说明，均由作者提供*'
