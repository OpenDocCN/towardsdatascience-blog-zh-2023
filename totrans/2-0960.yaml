- en: Generating Medical Images with MONAI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ MONAI ç”ŸæˆåŒ»ç–—å›¾åƒ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/generating-medical-images-with-monai-e03310aa35e6](https://towardsdatascience.com/generating-medical-images-with-monai-e03310aa35e6)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/generating-medical-images-with-monai-e03310aa35e6](https://towardsdatascience.com/generating-medical-images-with-monai-e03310aa35e6)
- en: '*An end-to-end open-source project using the latest MONAI Generative Models
    to produce chest X-ray images from radiological reports text*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¼€æºé¡¹ç›®ï¼Œä½¿ç”¨æœ€æ–°çš„ MONAI ç”Ÿæˆæ¨¡å‹ä»æ”¾å°„æŠ¥å‘Šæ–‡æœ¬ç”Ÿæˆèƒ¸éƒ¨ X å…‰å›¾åƒ*'
- en: '[](https://medium.com/@walhugolp?source=post_page-----e03310aa35e6--------------------------------)[![Walter
    Hugo Lopez Pinaya](../Images/0c132d0d1321790b0cea880800d231e0.png)](https://medium.com/@walhugolp?source=post_page-----e03310aa35e6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e03310aa35e6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e03310aa35e6--------------------------------)
    [Walter Hugo Lopez Pinaya](https://medium.com/@walhugolp?source=post_page-----e03310aa35e6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@walhugolp?source=post_page-----e03310aa35e6--------------------------------)[![Walter
    Hugo Lopez Pinaya](../Images/0c132d0d1321790b0cea880800d231e0.png)](https://medium.com/@walhugolp?source=post_page-----e03310aa35e6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e03310aa35e6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e03310aa35e6--------------------------------)
    [Walter Hugo Lopez Pinaya](https://medium.com/@walhugolp?source=post_page-----e03310aa35e6--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e03310aa35e6--------------------------------)
    Â·14 min readÂ·Apr 14, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e03310aa35e6--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 14 åˆ†é’ŸÂ·2023å¹´4æœˆ14æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Hi everybody! In this post, we will create a Latent Diffusion Model to generate
    Chest X-Ray images using the new open-source extension for MONAI, [**MONAI Generative
    Models**](https://github.com/Project-MONAI/GenerativeModels)!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ï¼åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ MONAI çš„æ–°å¼€æºæ‰©å±•[**MONAI ç”Ÿæˆæ¨¡å‹**](https://github.com/Project-MONAI/GenerativeModels)åˆ›å»ºä¸€ä¸ªæ½œåœ¨æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆèƒ¸éƒ¨
    X å…‰å›¾åƒï¼
- en: '![](../Images/51dca716335eeca38fff769dc85c3648.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51dca716335eeca38fff769dc85c3648.png)'
- en: '**Introduction**'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ä»‹ç»**'
- en: Generative AI has a huge potential for healthcare since it allows us to create
    models that learn the underlying patterns and structure of the training dataset.
    This way, we can use these generative models to create an unlimited amount of
    synthetic data with the same details and characteristics of real data but without
    their restrictions. Given its importance, we created ***MONAI Generative Models*,
    an open-source extension to the** [**MONAI platform**](https://monai.io/) containing
    the latest models (like Diffusion Models, Autoregressive Transformers, and Generative
    Adversarial Networks) and components that help with the training and evaluate
    generative models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå¼ AI åœ¨åŒ»ç–—é¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œå› ä¸ºå®ƒå…è®¸æˆ‘ä»¬åˆ›å»ºå­¦ä¹ è®­ç»ƒæ•°æ®é›†åº•å±‚æ¨¡å¼å’Œç»“æ„çš„æ¨¡å‹ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›ç”Ÿæˆæ¨¡å‹åˆ›å»ºå¤§é‡åˆæˆæ•°æ®ï¼Œè¿™äº›æ•°æ®å…·æœ‰çœŸå®æ•°æ®çš„ç›¸åŒç»†èŠ‚å’Œç‰¹å¾ï¼Œä½†æ²¡æœ‰å…¶é™åˆ¶ã€‚é‰´äºå…¶é‡è¦æ€§ï¼Œæˆ‘ä»¬åˆ›å»ºäº†***MONAI
    ç”Ÿæˆæ¨¡å‹*ï¼Œè¿™æ˜¯å¯¹** [**MONAI å¹³å°**](https://monai.io/)çš„å¼€æºæ‰©å±•ï¼ŒåŒ…å«æœ€æ–°çš„æ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ã€è‡ªå›å½’å˜æ¢å™¨å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼‰ä»¥åŠå¸®åŠ©è®­ç»ƒå’Œè¯„ä¼°ç”Ÿæˆæ¨¡å‹çš„ç»„ä»¶ã€‚
- en: '![](../Images/c5d4f6f58dff9be2230f424112fd52a4.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5d4f6f58dff9be2230f424112fd52a4.png)'
- en: In this post, we will go through a complete project to create a Latent Diffusion
    Model (the same type of model as [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release))
    capable of **generating Chest X-Rays (CXR) images from radiological reports**.
    Here, we tried to make the code easy to understand and to be adapted to different
    environments, so, although it is not the most efficient one, I hope you enjoy
    it!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡ä¸€ä¸ªå®Œæ•´çš„é¡¹ç›®æ¥åˆ›å»ºä¸€ä¸ªæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆä¸[ç¨³å®šæ‰©æ•£](https://stability.ai/blog/stable-diffusion-public-release)ç›¸åŒç±»å‹çš„æ¨¡å‹ï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿ**ä»æ”¾å°„æŠ¥å‘Šä¸­ç”Ÿæˆèƒ¸éƒ¨
    X å…‰ï¼ˆCXRï¼‰å›¾åƒ**ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°è¯•ä½¿ä»£ç æ˜“äºç†è§£å¹¶é€‚åº”ä¸åŒç¯å¢ƒï¼Œæ‰€ä»¥å°½ç®¡å®ƒä¸æ˜¯æœ€æœ‰æ•ˆçš„ï¼Œå¸Œæœ›ä½ å–œæ¬¢ï¼
- en: You can find the **complete open-source project at this** [**GitHub repository**](https://github.com/Warvito/generative_chestxray),
    where in this post we are referencing to the release v0.2.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨è¿™ä¸ª[**GitHub ä»“åº“**](https://github.com/Warvito/generative_chestxray)æ‰¾åˆ°**å®Œæ•´çš„å¼€æºé¡¹ç›®**ï¼Œåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å¼•ç”¨çš„æ˜¯ç‰ˆæœ¬
    v0.2ã€‚
- en: Dataset
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°æ®é›†
- en: First, we start with the dataset. In this project, we are using the [**MIMIC
    Dataset**](https://www.nature.com/articles/s41597-019-0322-0). To access this
    dataset, it is necessary to create an account at the [Physionet portal](https://physionet.org/).
    We will use [**MIMIC-CXR-JPG**](https://physionet.org/content/mimic-cxr-jpg/2.0.0/)
    (which contains the JPG files) and [**MIMIC-C**XR](https://physionet.org/content/mimic-cxr/2.0.0/)
    (that includes the radiological reports). Both datasets are under the [*PhysioNet
    Credentialed Health Data License 1.5.0*](https://physionet.org/content/mimic-cxr/view-license/2.0.0/)*.*
    After completing the free training course, you can freely download the dataset
    using the instruction at the bottom of the dataset page. Originally, the CXR images
    have about +1000x1000 pixels. So, this step can take a while.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬ä»æ•°æ®é›†å¼€å§‹ã€‚åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨[**MIMICæ•°æ®é›†**](https://www.nature.com/articles/s41597-019-0322-0)ã€‚è¦è®¿é—®è¿™ä¸ªæ•°æ®é›†ï¼Œå¿…é¡»åœ¨[Physioneté—¨æˆ·](https://physionet.org/)åˆ›å»ºä¸€ä¸ªè´¦æˆ·ã€‚æˆ‘ä»¬å°†ä½¿ç”¨[**MIMIC-CXR-JPG**](https://physionet.org/content/mimic-cxr-jpg/2.0.0/)ï¼ˆåŒ…å«JPGæ–‡ä»¶ï¼‰å’Œ[**MIMIC-C**XR](https://physionet.org/content/mimic-cxr/2.0.0/)ï¼ˆåŒ…å«æ”¾å°„å­¦æŠ¥å‘Šï¼‰ã€‚è¿™ä¸¤ä¸ªæ•°æ®é›†éƒ½å—åˆ°[*PhysioNetè®¤è¯å¥åº·æ•°æ®è®¸å¯è¯1.5.0*](https://physionet.org/content/mimic-cxr/view-license/2.0.0/)*çš„çº¦æŸã€‚å®Œæˆå…è´¹çš„åŸ¹è®­è¯¾ç¨‹åï¼Œä½ å¯ä»¥æŒ‰ç…§æ•°æ®é›†é¡µé¢åº•éƒ¨çš„è¯´æ˜è‡ªç”±ä¸‹è½½æ•°æ®é›†ã€‚åŸå§‹çš„CXRå›¾åƒå¤§çº¦ä¸º+1000x1000åƒç´ ã€‚å› æ­¤ï¼Œè¿™ä¸€æ­¥å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ã€‚
- en: 'Chest X-ray images are a crucial tool to provide valuable information about
    the structures and organs within the chest cavity, including the lungs, heart,
    and blood vessels, and after download, we should have more than 350k of them!
    These images are one of the three different projections: **Posterior-Anterior
    (PA), Anterior-Posterior (AP), and Lateral (LAT)**. For this project, we are interested
    only in the **PA projection**, the most common one where we can visualise most
    of the features mentioned in the radiological reports (ending with 96,162 images).
    Regarding the **reports**, we have 85,882 files, each containing several text
    sections. Here we will use the **Findings** (mainly explaining the contents in
    the image) and **Impressions** (summarising the reportâ€™s contents, like a conclusion).
    To make our models and training process more manageable, we will resize the images
    to have 512 pixels on the smallest axis. The list of scripts to automatically
    perform these initial steps can be found in [here](https://github.com/Warvito/generative_chestxray#preprocessing).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: èƒ¸éƒ¨Xå…‰å›¾åƒæ˜¯æä¾›å…³äºèƒ¸è…”å†…ç»“æ„å’Œå™¨å®˜ï¼ˆåŒ…æ‹¬è‚ºéƒ¨ã€å¿ƒè„å’Œè¡€ç®¡ï¼‰å®è´µä¿¡æ¯çš„é‡è¦å·¥å…·ï¼Œä¸‹è½½åæˆ‘ä»¬åº”è¯¥æœ‰è¶…è¿‡350kå¼ è¿™æ ·çš„å›¾åƒï¼è¿™äº›å›¾åƒæ˜¯ä¸‰ç§ä¸åŒæŠ•å½±ä¸­çš„ä¸€ç§ï¼š**åå‰ä½ï¼ˆPAï¼‰ã€å‰åä½ï¼ˆAPï¼‰å’Œä¾§ä½ï¼ˆLATï¼‰**ã€‚å¯¹äºè¿™ä¸ªé¡¹ç›®ï¼Œæˆ‘ä»¬åªå…³æ³¨**PAæŠ•å½±**ï¼Œè¿™æ˜¯æœ€å¸¸è§çš„ä¸€ç§ï¼Œå¯ä»¥å¯è§†åŒ–æ”¾å°„å­¦æŠ¥å‘Šä¸­æåˆ°çš„å¤§å¤šæ•°ç‰¹å¾ï¼ˆå…±è®¡96,162å¼ å›¾åƒï¼‰ã€‚å…³äº**æŠ¥å‘Š**ï¼Œæˆ‘ä»¬æœ‰85,882ä¸ªæ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶åŒ…å«å¤šä¸ªæ–‡æœ¬éƒ¨åˆ†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨**å‘ç°**ï¼ˆä¸»è¦è§£é‡Šå›¾åƒå†…å®¹ï¼‰å’Œ**å°è±¡**ï¼ˆæ€»ç»“æŠ¥å‘Šå†…å®¹ï¼Œå¦‚ç»“è®ºï¼‰ã€‚ä¸ºäº†ä½¿æˆ‘ä»¬çš„æ¨¡å‹å’Œè®­ç»ƒè¿‡ç¨‹æ›´æ˜“äºç®¡ç†ï¼Œæˆ‘ä»¬å°†å›¾åƒè°ƒæ•´ä¸ºæœ€å°è½´512åƒç´ ã€‚è‡ªåŠ¨æ‰§è¡Œè¿™äº›åˆå§‹æ­¥éª¤çš„è„šæœ¬åˆ—è¡¨å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/Warvito/generative_chestxray#preprocessing)æ‰¾åˆ°ã€‚
- en: '**Models**'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹**'
- en: '![](../Images/07b771040df2f8e61db7e586c3eef069.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07b771040df2f8e61db7e586c3eef069.png)'
- en: 'Latent Diffusion Model: The autoencoder compresses the inputted image x to
    a latent representation z, and then the diffusion model estimates the probability
    distribution of z'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼šè‡ªç¼–ç å™¨å°†è¾“å…¥å›¾åƒxå‹ç¼©ä¸ºæ½œåœ¨è¡¨ç¤ºzï¼Œç„¶åæ‰©æ•£æ¨¡å‹ä¼°è®¡zçš„æ¦‚ç‡åˆ†å¸ƒ
- en: 'The Latent Diffusion Models are composed of several parts:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”±å‡ ä¸ªéƒ¨åˆ†ç»„æˆï¼š
- en: An **Autoencoder** that performs the compression of the inputted images into
    a smaller latent representation;
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª**è‡ªç¼–ç å™¨**ï¼Œç”¨äºå°†è¾“å…¥å›¾åƒå‹ç¼©ä¸ºæ›´å°çš„æ½œåœ¨è¡¨ç¤ºï¼›
- en: A **Diffusion Model** that will learn the probability data distribution of the
    latent representations of the CXR;
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª**æ‰©æ•£æ¨¡å‹**ï¼Œå°†å­¦ä¹ CXRæ½œåœ¨è¡¨ç¤ºçš„æ¦‚ç‡æ•°æ®åˆ†å¸ƒï¼›
- en: A **Text Encoder** which creates an embedding vector that will condition the
    sampling process. In this example, we are using a pretrained one.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª**æ–‡æœ¬ç¼–ç å™¨**ï¼Œå®ƒåˆ›å»ºä¸€ä¸ªåµŒå…¥å‘é‡ï¼Œç”¨äºæ¡ä»¶åŒ–é‡‡æ ·è¿‡ç¨‹ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„ç¼–ç å™¨ã€‚
- en: Using MONAI Generative Models, we can easily create and train these models,
    so letâ€™s start with the Autoencoder!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨MONAIç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åˆ›å»ºå’Œè®­ç»ƒè¿™äº›æ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬ä»è‡ªç¼–ç å™¨å¼€å§‹å§ï¼
- en: '**Models â€” Autoencoder with KL regularization**'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹ â€” å¸¦KLæ­£åˆ™åŒ–çš„è‡ªç¼–ç å™¨**'
- en: The main goal of the Autoencoder with KL regularization (**AE-kl** or, in some
    projects, simply called as a VAE) is to be able to create a small latent representation,
    and also to reconstruct a image with high-fidelity (preserving as most as possible
    details). In this project, we are creating an autoencoder with four levels, with
    64, 128, 128, 128 channels, where we apply a downsampling block between each level,
    making the feature maps smaller as we go to the deepest layers. Although our Autoencoder
    can have blocks with self-attention, in this example, we are adopting a structure
    similar to our previous study on brain images and using no attention to save memory
    usage. Finally, our latent representation has three channels.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰ KL æ­£åˆ™åŒ–çš„è‡ªç¼–ç å™¨ï¼ˆ**AE-kl**ï¼Œæˆ–åœ¨ä¸€äº›é¡¹ç›®ä¸­ç®€å•ç§°ä¸º VAEï¼‰çš„ä¸»è¦ç›®æ ‡æ˜¯èƒ½å¤Ÿåˆ›å»ºä¸€ä¸ªå°çš„æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶ä¸”é«˜ä¿çœŸåœ°é‡å»ºå›¾åƒï¼ˆå°½å¯èƒ½ä¿ç•™ç»†èŠ‚ï¼‰ã€‚åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå››å±‚çš„è‡ªç¼–ç å™¨ï¼Œå…·æœ‰
    64ã€128ã€128ã€128 ä¸ªé€šé“ï¼Œå…¶ä¸­åœ¨æ¯ä¸€å±‚ä¹‹é—´åº”ç”¨äº†ä¸€ä¸ªé™é‡‡æ ·å—ï¼Œä½¿å¾—ç‰¹å¾å›¾åœ¨æ·±å…¥å±‚æ¬¡æ—¶å˜å°ã€‚å°½ç®¡æˆ‘ä»¬çš„è‡ªç¼–ç å™¨å¯ä»¥æœ‰è‡ªæ³¨æ„åŠ›å—ï¼Œä½†åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç±»ä¼¼äºæˆ‘ä»¬ä¹‹å‰åœ¨è„‘éƒ¨å›¾åƒç ”ç©¶ä¸­çš„ç»“æ„ï¼Œå¹¶ä¸”ä¸ä½¿ç”¨æ³¨æ„åŠ›ä»¥èŠ‚çœå†…å­˜ä½¿ç”¨ã€‚æœ€åï¼Œæˆ‘ä»¬çš„æ½œåœ¨è¡¨ç¤ºæœ‰ä¸‰ä¸ªé€šé“ã€‚
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note: In our [script](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/train_aekl.py#L77),
    we are using the [**OmegaConf packag**e](https://omegaconf.readthedocs.io/en/2.3_branch/)
    to store the hyperparameters of our model. You can see the previous configuration
    in this [file](https://github.com/Warvito/generative_chestxray/blob/main/configs/stage1/aekl_v0.yaml).
    In summary, OmegaConf is a powerful tool for managing configurations in Python
    projects, particularly those that involve deep learning or other complex software
    systems. OmegaConf allows us to conveniently organise the hyperparameters in the
    .yaml files and read them in the script.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šåœ¨æˆ‘ä»¬çš„[è„šæœ¬](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/train_aekl.py#L77)ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†[**OmegaConf
    åŒ…**](https://omegaconf.readthedocs.io/en/2.3_branch/)æ¥å­˜å‚¨æˆ‘ä»¬æ¨¡å‹çš„è¶…å‚æ•°ã€‚ä½ å¯ä»¥åœ¨è¿™ä¸ª[æ–‡ä»¶](https://github.com/Warvito/generative_chestxray/blob/main/configs/stage1/aekl_v0.yaml)ä¸­æŸ¥çœ‹ä¹‹å‰çš„é…ç½®ã€‚æ€»ä¹‹ï¼ŒOmegaConf
    æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œç”¨äºç®¡ç† Python é¡¹ç›®çš„é…ç½®ï¼Œç‰¹åˆ«æ˜¯é‚£äº›æ¶‰åŠæ·±åº¦å­¦ä¹ æˆ–å…¶ä»–å¤æ‚è½¯ä»¶ç³»ç»Ÿçš„é¡¹ç›®ã€‚OmegaConf å…è®¸æˆ‘ä»¬æ–¹ä¾¿åœ°åœ¨ .yaml æ–‡ä»¶ä¸­ç»„ç»‡è¶…å‚æ•°ï¼Œå¹¶åœ¨è„šæœ¬ä¸­è¯»å–å®ƒä»¬ã€‚
- en: '**Training AE-KL**'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒ AE-KL**'
- en: Next, we define a few components of our training process. First, we have the
    [**KL regularisation**](https://github.com/Warvito/generative_chestxray/blob/731ee63c5fe28353ddeae548fba9286ee4232bd0/src/python/training/training_functions.py#L161).
    This part is responsible for evaluating the distance between the distribution
    of the latent space of the diffusion models and a Gaussian distribution. As proposed
    by [Rombach et al.](https://arxiv.org/abs/2112.10752), this will be used to restrict
    the variance of the latent space, which is useful when we train the diffusion
    model on it (more about it later). The forward method of our model returns the
    reconstruction, as well as the Î¼ and Ïƒ vectors of our latent representation, which
    we use to compute the KL divergence.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰äº†æˆ‘ä»¬è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‡ ä¸ªç»„ä»¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æœ‰[**KL æ­£åˆ™åŒ–**](https://github.com/Warvito/generative_chestxray/blob/731ee63c5fe28353ddeae548fba9286ee4232bd0/src/python/training/training_functions.py#L161)ã€‚è¿™éƒ¨åˆ†è´Ÿè´£è¯„ä¼°æ‰©æ•£æ¨¡å‹æ½œåœ¨ç©ºé—´åˆ†å¸ƒä¸é«˜æ–¯åˆ†å¸ƒä¹‹é—´çš„è·ç¦»ã€‚æ­£å¦‚[Rombach
    ç­‰äºº](https://arxiv.org/abs/2112.10752)æ‰€æå‡ºçš„ï¼Œè¿™å°†ç”¨äºé™åˆ¶æ½œåœ¨ç©ºé—´çš„æ–¹å·®ï¼Œè¿™åœ¨æˆ‘ä»¬å¯¹å…¶è¿›è¡Œæ‰©æ•£æ¨¡å‹è®­ç»ƒæ—¶éå¸¸æœ‰ç”¨ï¼ˆç¨åä¼šè¯¦ç»†è¯´æ˜ï¼‰ã€‚æˆ‘ä»¬æ¨¡å‹çš„å‰å‘æ–¹æ³•è¿”å›é‡å»ºç»“æœï¼Œä»¥åŠæˆ‘ä»¬æ½œåœ¨è¡¨ç¤ºçš„
    Î¼ å’Œ Ïƒ å‘é‡ï¼Œæˆ‘ä»¬ç”¨è¿™äº›æ¥è®¡ç®— KL æ•£åº¦ã€‚
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Second, we have our[**Pixel-level loss**](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/training_functions.py#L158),
    where in this project, we are adopting an L1 distance to evaluate how much our
    AE-kl reconstruction differs from the original image.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶æ¬¡ï¼Œæˆ‘ä»¬æœ‰æˆ‘ä»¬çš„[**åƒç´ çº§æŸå¤±**](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/training_functions.py#L158)ï¼Œåœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨
    L1 è·ç¦»æ¥è¯„ä¼° AE-kl é‡å»ºä¸åŸå§‹å›¾åƒçš„å·®å¼‚ã€‚
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, we have our [**Perceptual-level loss**](https://github.com/Warvito/generative_chestxray/blob/731ee63c5fe28353ddeae548fba9286ee4232bd0/src/python/training/train_aekl.py#L79).
    The idea of perceptual loss is that instead of evaluating the difference between
    the inputted image and the reconstruction at the pixel level, we pass both images
    through a pre-trained model. Then, we measure the distance of the internal activations
    and feature maps. In MONAI Generative models, we made it easy to use perceptual
    networks based on networks pre-trained on medical images (available [here](https://github.com/Project-MONAI/GenerativeModels/blob/main/generative/losses/perceptual.py)).
    We have access to the **2D networks** from the **RadImageNet** study (from [Mei
    et al.](https://pubs.rsna.org/doi/10.1148/ryai.210315)), which were trained on
    more than **1.3 million medical images!** We implemented the **2.5D approach**,
    using 2D pre-trained networks to evaluate 3D images by evaluating slices. And
    finally, we have access to [**MedicalNet**](https://github.com/Tencent/MedicalNet)
    to evaluate our **3D images** in a 3D pure method. In this project, we are using
    a similar approach to [Pinaya et al.](https://arxiv.org/abs/2209.07162) and use
    the Learned Perceptual Image Patch Similarity (**LPIPS**) metric (also available
    at MONAI Generative Models).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æœ‰[**æ„ŸçŸ¥çº§æŸå¤±**](https://github.com/Warvito/generative_chestxray/blob/731ee63c5fe28353ddeae548fba9286ee4232bd0/src/python/training/train_aekl.py#L79)ã€‚æ„ŸçŸ¥æŸå¤±çš„æƒ³æ³•æ˜¯ï¼Œä¸æ˜¯è¯„ä¼°è¾“å…¥å›¾åƒä¸é‡å»ºå›¾åƒåœ¨åƒç´ çº§åˆ«çš„å·®å¼‚ï¼Œè€Œæ˜¯å°†è¿™ä¸¤å¼ å›¾åƒé€šè¿‡ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬æµ‹é‡å†…éƒ¨æ¿€æ´»å’Œç‰¹å¾å›¾çš„è·ç¦»ã€‚åœ¨MONAIç”Ÿæˆæ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬ç®€åŒ–äº†ä½¿ç”¨åŸºäºåœ¨åŒ»å­¦å›¾åƒä¸Šé¢„è®­ç»ƒçš„ç½‘ç»œçš„æ„ŸçŸ¥ç½‘ç»œï¼ˆå¯åœ¨[æ­¤å¤„](https://github.com/Project-MONAI/GenerativeModels/blob/main/generative/losses/perceptual.py)è·å–ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨**RadImageNet**ç ”ç©¶ä¸­çš„**2Dç½‘ç»œ**ï¼ˆæ¥è‡ª[Meiç­‰](https://pubs.rsna.org/doi/10.1148/ryai.210315)ï¼‰ï¼Œè¿™äº›ç½‘ç»œåœ¨è¶…è¿‡**130ä¸‡å¼ åŒ»å­¦å›¾åƒ**ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼æˆ‘ä»¬å®ç°äº†**2.5Dæ–¹æ³•**ï¼Œä½¿ç”¨2Dé¢„è®­ç»ƒç½‘ç»œé€šè¿‡è¯„ä¼°åˆ‡ç‰‡æ¥è¯„ä¼°3Då›¾åƒã€‚æœ€åï¼Œæˆ‘ä»¬å¯ä»¥è®¿é—®[**MedicalNet**](https://github.com/Tencent/MedicalNet)æ¥ä»¥çº¯3Dæ–¹æ³•è¯„ä¼°æˆ‘ä»¬çš„**3Då›¾åƒ**ã€‚åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ç±»ä¼¼äº[Pinayaç­‰](https://arxiv.org/abs/2209.07162)çš„æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨äº†å­¦ä¹ åˆ°çš„æ„ŸçŸ¥å›¾åƒå—ç›¸ä¼¼åº¦ï¼ˆ**LPIPS**ï¼‰åº¦é‡ï¼ˆä¹Ÿå¯åœ¨MONAIç”Ÿæˆæ¨¡å‹ä¸­æ‰¾åˆ°ï¼‰ã€‚
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Finally, we use [**Adversarial loss**](https://github.com/Warvito/generative_chestxray/blob/731ee63c5fe28353ddeae548fba9286ee4232bd0/src/python/training/training_functions.py#L148)
    to deal with the fine details of the reconstructions. The Adversarial Network
    was a [**Patch-Discriminator**](https://github.com/Warvito/generative_chestxray/blob/731ee63c5fe28353ddeae548fba9286ee4232bd0/src/python/training/train_aekl.py#L78)
    (initially proposed by the Pix2Pix study), where instead of having only one prediction
    about if the whole image was real or fake, we have predictions for several patches
    from the image.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨[**å¯¹æŠ—æŸå¤±**](https://github.com/Warvito/generative_chestxray/blob/731ee63c5fe28353ddeae548fba9286ee4232bd0/src/python/training/training_functions.py#L148)æ¥å¤„ç†é‡å»ºçš„ç»†èŠ‚ã€‚å¯¹æŠ—ç½‘ç»œæ˜¯[**Patch-Discriminator**](https://github.com/Warvito/generative_chestxray/blob/731ee63c5fe28353ddeae548fba9286ee4232bd0/src/python/training/train_aekl.py#L78)ï¼ˆæœ€åˆç”±Pix2Pixç ”ç©¶æå‡ºï¼‰ï¼Œå…¶ä¸­æˆ‘ä»¬ä¸ä»…å¯¹æ•´ä¸ªå›¾åƒæ˜¯å¦çœŸå®æˆ–è™šå‡åšå‡ºä¸€ä¸ªé¢„æµ‹ï¼Œè¿˜å¯¹å›¾åƒä¸­çš„å¤šä¸ªå—åšå‡ºé¢„æµ‹ã€‚
- en: Unlike the original Latent Diffusion Model and Stable Diffusion, we used discriminator
    losses from the least square GANs. Although it is not the more advanced adversarial
    loss, it has shown efficacy and stability when training on 3D medical images as
    well (but still room for improvement ğŸ˜). Although adversarial losses can be quite
    unstable, their combination with perceptual losses also helps to stabilise the
    loss of the discriminator and generator.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŸå§‹çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelï¼‰å’Œç¨³å®šæ‰©æ•£ï¼ˆStable Diffusionï¼‰ä¸åŒï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ¥è‡ªæœ€å°äºŒä¹˜ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆleast
    square GANsï¼‰çš„é‰´åˆ«å™¨æŸå¤±ã€‚è™½ç„¶è¿™ä¸æ˜¯æ›´å…ˆè¿›çš„å¯¹æŠ—æŸå¤±ï¼Œä½†åœ¨å¯¹3DåŒ»å­¦å›¾åƒè¿›è¡Œè®­ç»ƒæ—¶å·²ç»æ˜¾ç¤ºå‡ºäº†æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ï¼ˆä½†ä»æœ‰æ”¹è¿›çš„ç©ºé—´ğŸ˜ï¼‰ã€‚å°½ç®¡å¯¹æŠ—æŸå¤±å¯èƒ½ç›¸å½“ä¸ç¨³å®šï¼Œä½†å®ƒä»¬ä¸æ„ŸçŸ¥æŸå¤±çš„ç»“åˆä¹Ÿæœ‰åŠ©äºç¨³å®šé‰´åˆ«å™¨å’Œç”Ÿæˆå™¨çš„æŸå¤±ã€‚
- en: Our training loops and evaluation steps can be found at [here](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/training_functions.py#L129)
    and [here](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/training_functions.py#L236).
    After train for 75 epoch, we save our model with the MLflow package. We use the
    [MLflow package](https://mlflow.org/) to better monitoring of our experiments
    since it organises information like git hash and parameters, as well as makes
    it possible to store different runs with a unique ID in groups (called experiments)
    and making easier to compare different results (similar to others tools, like
    weights and biases). The logs files for the AE-KL can be found [here](https://drive.google.com/drive/folders/1Ots9ujg4dSaTABKkyaUnFLpCKQ7kCgdK?usp=sharing).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯å’Œè¯„ä¼°æ­¥éª¤å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/training_functions.py#L129)å’Œ[è¿™é‡Œ](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/training_functions.py#L236)æ‰¾åˆ°ã€‚ç»è¿‡75è½®è®­ç»ƒåï¼Œæˆ‘ä»¬ä½¿ç”¨
    MLflow åŒ…ä¿å­˜æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨[MLflow åŒ…](https://mlflow.org/)æ¥æ›´å¥½åœ°ç›‘æ§å®éªŒï¼Œå› ä¸ºå®ƒç»„ç»‡äº†å¦‚ git hash å’Œå‚æ•°ç­‰ä¿¡æ¯ï¼Œå¹¶ä¸”ä½¿å¾—å¯ä»¥ç”¨å”¯ä¸€çš„
    ID åœ¨ç»„ï¼ˆç§°ä¸ºå®éªŒï¼‰ä¸­å­˜å‚¨ä¸åŒçš„è¿è¡Œï¼Œå¹¶ä¸”ä½¿å¾—æ¯”è¾ƒä¸åŒç»“æœå˜å¾—æ›´å®¹æ˜“ï¼ˆç±»ä¼¼äºå…¶ä»–å·¥å…·ï¼Œå¦‚ weights and biasesï¼‰ã€‚AE-KL çš„æ—¥å¿—æ–‡ä»¶å¯ä»¥åœ¨[è¿™é‡Œ](https://drive.google.com/drive/folders/1Ots9ujg4dSaTABKkyaUnFLpCKQ7kCgdK?usp=sharing)æ‰¾åˆ°ã€‚
- en: '**Models â€” Diffusion Model**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹ â€” æ‰©æ•£æ¨¡å‹**'
- en: Next, we need to train our diffusion model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦è®­ç»ƒæˆ‘ä»¬çš„æ‰©æ•£æ¨¡å‹ã€‚
- en: '![](../Images/af13eea9c3b6fc26b7df7368c9e7082e.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af13eea9c3b6fc26b7df7368c9e7082e.png)'
- en: The diffusion model is a U-Net like network where traditionally, it receives
    a noisy image (or latent representation) as input and will predict its noise component.
    These models use an iterative denoising mechanism to generate images from noise
    across a Markov Chain with several steps. For this reason, the model is also conditioned
    on the timestep defining in which stage of the sampling process the model is.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰©æ•£æ¨¡å‹æ˜¯ç±»ä¼¼äº U-Net çš„ç½‘ç»œï¼Œå…¶ä¸­é€šå¸¸æ¥æ”¶ä¸€ä¸ªå™ªå£°å›¾åƒï¼ˆæˆ–æ½œåœ¨è¡¨ç¤ºï¼‰ä½œä¸ºè¾“å…¥ï¼Œå¹¶é¢„æµ‹å…¶å™ªå£°æˆåˆ†ã€‚è¿™äº›æ¨¡å‹ä½¿ç”¨è¿­ä»£å»å™ªæœºåˆ¶ï¼Œé€šè¿‡å…·æœ‰å¤šä¸ªæ­¥éª¤çš„é©¬å°”å¯å¤«é“¾ä»å™ªå£°ä¸­ç”Ÿæˆå›¾åƒã€‚å› æ­¤ï¼Œè¯¥æ¨¡å‹è¿˜ä¾èµ–äºæ—¶é—´æ­¥ï¼Œä»¥å®šä¹‰æ¨¡å‹åœ¨é‡‡æ ·è¿‡ç¨‹çš„å“ªä¸ªé˜¶æ®µã€‚
- en: '![](../Images/2332b550309afa5ed1003f9927d86934.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2332b550309afa5ed1003f9927d86934.png)'
- en: Using the [**DiffusionModelUNet class**](https://github.com/Project-MONAI/GenerativeModels/blob/02de27c1937f3d9ca3483ab2f4661270c9137641/generative/networks/nets/diffusion_model_unet.py#L1624),
    we can create the U-Net like network for our diffusion mdel. Our project uses
    the configuration defined in this [config file](https://github.com/Warvito/generative_chestxray/blob/main/configs/ldm/ldm_v0.yaml)
    where it defines input and output with 3 channels (as our AE-kl have a latent
    space with 3 channels), and 3 different levels with 256, 512, 768 channels. Each
    level has 2 residual blocks. As mentioned, it is important to pass the timestep
    for the model where it is used to condition the behaviour of these residual blocks.
    Finally, we define the attention mechanisms inside the network. In our case, we
    have attention blocks in the second and third levels (indicated by the attention_levels
    argument), each with 512 and 768 channels per attention head (in other words,
    we have a single attention head in each level). These attention mechanisms are
    important because they allow us to apply our **external conditioning** (the radiological
    reports) to the network via the **cross-attention method**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[**DiffusionModelUNet ç±»**](https://github.com/Project-MONAI/GenerativeModels/blob/02de27c1937f3d9ca3483ab2f4661270c9137641/generative/networks/nets/diffusion_model_unet.py#L1624)ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºæˆ‘ä»¬çš„æ‰©æ•£æ¨¡å‹åˆ›å»ºç±»ä¼¼äº
    U-Net çš„ç½‘ç»œã€‚æˆ‘ä»¬çš„é¡¹ç›®ä½¿ç”¨åœ¨æ­¤[é…ç½®æ–‡ä»¶](https://github.com/Warvito/generative_chestxray/blob/main/configs/ldm/ldm_v0.yaml)ä¸­å®šä¹‰çš„é…ç½®ï¼Œå…¶ä¸­å®šä¹‰äº†å…·æœ‰
    3 ä¸ªé€šé“çš„è¾“å…¥å’Œè¾“å‡ºï¼ˆå› ä¸ºæˆ‘ä»¬çš„ AE-kl å…·æœ‰ 3 ä¸ªé€šé“çš„æ½œåœ¨ç©ºé—´ï¼‰ï¼Œä»¥åŠå…·æœ‰ 256ã€512ã€768 é€šé“çš„ 3 ä¸ªä¸åŒçº§åˆ«ã€‚æ¯ä¸ªçº§åˆ«æœ‰ 2 ä¸ªæ®‹å·®å—ã€‚å¦‚å‰æ‰€è¿°ï¼Œä¼ é€’æ—¶é—´æ­¥å¯¹æ¨¡å‹å¾ˆé‡è¦ï¼Œå®ƒç”¨äºè°ƒæ•´è¿™äº›æ®‹å·®å—çš„è¡Œä¸ºã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨ç½‘ç»œå†…éƒ¨å®šä¹‰æ³¨æ„åŠ›æœºåˆ¶ã€‚åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åœ¨ç¬¬äºŒå’Œç¬¬ä¸‰çº§åˆ«ï¼ˆç”±
    attention_levels å‚æ•°æŒ‡ç¤ºï¼‰ä¸­æœ‰æ³¨æ„åŠ›å—ï¼Œæ¯ä¸ªæ³¨æ„åŠ›å¤´æœ‰ 512 å’Œ 768 ä¸ªé€šé“ï¼ˆæ¢å¥è¯è¯´ï¼Œæ¯ä¸ªçº§åˆ«æœ‰ä¸€ä¸ªæ³¨æ„åŠ›å¤´ï¼‰ã€‚è¿™äº›æ³¨æ„åŠ›æœºåˆ¶å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒä»¬å…è®¸æˆ‘ä»¬é€šè¿‡**äº¤å‰æ³¨æ„åŠ›æ–¹æ³•**å°†æˆ‘ä»¬çš„**å¤–éƒ¨æ¡ä»¶**ï¼ˆæ”¾å°„å­¦æŠ¥å‘Šï¼‰åº”ç”¨åˆ°ç½‘ç»œä¸­ã€‚
- en: '![](../Images/2a8019f23029a2286aba03afefb10013.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a8019f23029a2286aba03afefb10013.png)'
- en: External conditioning (or â€œcontextâ€) is applied to the U-Netâ€™s attention blocks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¤–éƒ¨æ¡ä»¶ï¼ˆæˆ–â€œä¸Šä¸‹æ–‡â€ï¼‰è¢«åº”ç”¨åˆ° U-Net çš„æ³¨æ„åŠ›å—ã€‚
- en: In our project, we are using an already **trained textual encoder**. For simplicity,
    we are using the same one from the Stable Diffusion v2.1 model (â€œstabilityai/stable-diffusion-2â€“1-baseâ€)
    to convert our text tokens into a text embedding that will be used as Key and
    Value vectors in the DiffusionModel UNet cross attention layers. Each token of
    our textual embedding have 1024 dimensions and we define it in the *â€œwith_conditioningâ€*
    and â€œ*cross_attention_dim*â€ arguments.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªå·²ç»**è®­ç»ƒå¥½çš„æ–‡æœ¬ç¼–ç å™¨**ã€‚ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ¥è‡ªç¨³å®šæ‰©æ•£ v2.1 æ¨¡å‹ï¼ˆâ€œstabilityai/stable-diffusion-2â€“1-baseâ€ï¼‰çš„ç›¸åŒç¼–ç å™¨ï¼Œå°†æˆ‘ä»¬çš„æ–‡æœ¬æ ‡è®°è½¬æ¢ä¸ºæ–‡æœ¬åµŒå…¥ï¼Œè¿™äº›åµŒå…¥å°†ä½œä¸º
    DiffusionModel UNet äº¤å‰æ³¨æ„åŠ›å±‚ä¸­çš„ Key å’Œ Value å‘é‡ã€‚æˆ‘ä»¬æ–‡æœ¬åµŒå…¥çš„æ¯ä¸ªæ ‡è®°æœ‰ 1024 ä¸ªç»´åº¦ï¼Œæˆ‘ä»¬åœ¨*â€œwith_conditioningâ€*å’Œâ€œ*cross_attention_dim*â€å‚æ•°ä¸­å®šä¹‰å®ƒã€‚
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Besides our model definition, it is important to define how the noise of the
    diffusion model will be added to the inputted images during training and removed
    during the sampling. For that, we implemented the Schedulers classes to our MONAI
    Generative Models to define the noise schedulers. In this example, we will use
    a [DDPMScheduler](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/train_ldm.py#L103),
    with 1000 time steps and the following hyperparameters.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†æ¨¡å‹å®šä¹‰ä¹‹å¤–ï¼Œé‡è¦çš„æ˜¯å®šä¹‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†å™ªå£°æ·»åŠ åˆ°è¾“å…¥å›¾åƒä¸­ä»¥åŠåœ¨é‡‡æ ·è¿‡ç¨‹ä¸­å»é™¤å™ªå£°çš„æ–¹å¼ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„ MONAI ç”Ÿæˆæ¨¡å‹å®ç°äº† Schedulers
    ç±»ä»¥å®šä¹‰å™ªå£°è°ƒåº¦å™¨ã€‚åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ª[DDPMScheduler](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/train_ldm.py#L103)ï¼Œå®ƒå…·æœ‰
    1000 ä¸ªæ—¶é—´æ­¥é•¿å’Œä»¥ä¸‹è¶…å‚æ•°ã€‚
- en: '![](../Images/e27af83fafc808464975a59d74972a63.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e27af83fafc808464975a59d74972a63.png)'
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we opted for a â€œ**v-predictionâ€ approach**, where our U-Net will try to
    predict the velocity component (a combination of the original image and the added
    noise) instead of just the added noise. This approach has been shown to have more
    stable training and faster convergence (also used in [https://arxiv.org/abs/2210.02303](https://arxiv.org/abs/2210.02303)).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é€‰æ‹©äº†**â€œv-predictionâ€æ–¹æ³•**ï¼Œå…¶ä¸­æˆ‘ä»¬çš„ U-Net å°†å°è¯•é¢„æµ‹é€Ÿåº¦åˆ†é‡ï¼ˆåŸå§‹å›¾åƒå’Œæ·»åŠ å™ªå£°çš„ç»„åˆï¼‰ï¼Œè€Œä¸ä»…ä»…æ˜¯æ·»åŠ çš„å™ªå£°ã€‚è¿™ç§æ–¹æ³•å·²è¢«è¯æ˜å…·æœ‰æ›´ç¨³å®šçš„è®­ç»ƒå’Œæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼ˆä¹Ÿåœ¨[https://arxiv.org/abs/2210.02303](https://arxiv.org/abs/2210.02303)ä¸­ä½¿ç”¨ï¼‰ã€‚
- en: '**Training Diffusion Model**'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ‰©æ•£æ¨¡å‹è®­ç»ƒ**'
- en: Before training the Diffusion Model, we need to find an appropriate scaling
    factor. As mentioned in Rombach et al., the signal-to-noise ratio can affect the
    results obtained with the LDM, if the standard deviation of the latent space distribution
    is too high. If the values of the latent representation are too high, the maximum
    amount of Gaussian noise we add to it might not be enough to destroy all information.
    This way, during training, information of the original latent representation might
    be present when it was not supposed to be, making it not possible later sample
    an image from pure noise. The KL regularisation can help a little bit with this,
    but it is best practice to use a scaling factor to adapt the latent representation
    values. In this [script](https://github.com/Warvito/generative_chestxray/blob/main/src/python/training/eda_ldm_scaling_factor.py),
    we verify the size of the standard deviation of the components of the latent space
    in one of the batches of the training set. We found that our scaling factor should
    be at least **0.8221**. In our case, we used a more conservative value of 0.3
    (similar to values from Stable Diffusion).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ä¸ªåˆé€‚çš„ç¼©æ”¾å› å­ã€‚å¦‚ Rombach ç­‰äººæ‰€è¿°ï¼Œå¦‚æœæ½œåœ¨ç©ºé—´åˆ†å¸ƒçš„æ ‡å‡†å·®è¿‡é«˜ï¼Œä¿¡å™ªæ¯”å¯èƒ½ä¼šå½±å“ä½¿ç”¨ LDM è·å¾—çš„ç»“æœã€‚å¦‚æœæ½œåœ¨è¡¨ç¤ºçš„å€¼è¿‡é«˜ï¼Œæˆ‘ä»¬æ·»åŠ çš„é«˜æ–¯å™ªå£°çš„æœ€å¤§é‡å¯èƒ½ä¸è¶³ä»¥ç ´åæ‰€æœ‰ä¿¡æ¯ã€‚è¿™æ ·ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒåŸå§‹æ½œåœ¨è¡¨ç¤ºçš„ä¿¡æ¯å¯èƒ½ä¼šå­˜åœ¨ï¼Œè€Œå®é™…ä¸Šä¸åº”è¯¥å­˜åœ¨ï¼Œä»è€Œä½¿å¾—ä¹‹åä»çº¯å™ªå£°ä¸­ç”Ÿæˆå›¾åƒå˜å¾—ä¸å¯èƒ½ã€‚KL
    æ­£åˆ™åŒ–å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šæœ‰æ‰€å¸®åŠ©ï¼Œä½†æœ€ä½³å®è·µæ˜¯ä½¿ç”¨ç¼©æ”¾å› å­æ¥è°ƒæ•´æ½œåœ¨è¡¨ç¤ºçš„å€¼ã€‚åœ¨è¿™ä¸ª[è„šæœ¬](https://github.com/Warvito/generative_chestxray/blob/main/src/python/training/eda_ldm_scaling_factor.py)ä¸­ï¼Œæˆ‘ä»¬éªŒè¯äº†è®­ç»ƒé›†çš„ä¸€ä¸ªæ‰¹æ¬¡ä¸­æ½œåœ¨ç©ºé—´åˆ†é‡çš„æ ‡å‡†å·®çš„å¤§å°ã€‚æˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„ç¼©æ”¾å› å­åº”è¯¥è‡³å°‘ä¸º**0.8221**ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ›´ä¸ºä¿å®ˆçš„å€¼
    0.3ï¼ˆç±»ä¼¼äºç¨³å®šæ‰©æ•£ä¸­çš„å€¼ï¼‰ã€‚
- en: With the scaling factor defined, we can train our model. In [here](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/training_functions.py#L424),
    we can check the training loop.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰äº†ç¼©æ”¾å› å­åï¼Œæˆ‘ä»¬å¯ä»¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚åœ¨[è¿™é‡Œ](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/training_functions.py#L424)ï¼Œæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹è®­ç»ƒå¾ªç¯ã€‚
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see, we first obtain the images and reports from our data loaders.
    To process our images, we used the [transforms from MONAI](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/util.py#L111)
    and added a few [custom transforms](https://github.com/Warvito/generative_chestxray/blob/main/src/python/training/custom_transforms.py)
    to extract random sentences from the radiological reports and tokenize the inputted
    text. In about 10% of the cases, we use an empty string (â€œâ€ â€” which is a vector
    with the Begin-of-Sentence token (*value = 49406*) followed by padding tokens
    (*value = 49407*)) to be able to use classifier free guidance during the sampling.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ æ‰€è§ï¼Œæˆ‘ä»¬é¦–å…ˆä»æ•°æ®åŠ è½½å™¨ä¸­è·å–å›¾åƒå’ŒæŠ¥å‘Šã€‚ä¸ºäº†å¤„ç†è¿™äº›å›¾åƒï¼Œæˆ‘ä»¬ä½¿ç”¨äº† [MONAI çš„è½¬æ¢å·¥å…·](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/training/util.py#L111)ï¼Œå¹¶æ·»åŠ äº†ä¸€äº›
    [è‡ªå®šä¹‰è½¬æ¢](https://github.com/Warvito/generative_chestxray/blob/main/src/python/training/custom_transforms.py)ï¼Œä»¥ä»æ”¾å°„å­¦æŠ¥å‘Šä¸­æå–éšæœºå¥å­å¹¶å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯ã€‚åœ¨å¤§çº¦
    10% çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç©ºå­—ç¬¦ä¸²ï¼ˆâ€œâ€ â€” è¿™æ˜¯ä¸€ä¸ªåŒ…å«å¥å­å¼€å§‹æ ‡è®° (*value = 49406*) å’Œå¡«å……æ ‡è®° (*value = 49407*)
    çš„å‘é‡ï¼‰ï¼Œä»¥ä¾¿åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ä½¿ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼ã€‚
- en: Next, we obtain the latent representation and the prompt embeddings. We create
    the noise to be added, the random timesteps to be used in this iteration, and
    the desired target (velocity component). Finally, we compute our loss using the
    mean squared error.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è·å–æ½œåœ¨è¡¨ç¤ºå’Œæç¤ºåµŒå…¥ã€‚æˆ‘ä»¬åˆ›å»ºéœ€è¦æ·»åŠ çš„å™ªå£°ã€ç”¨äºæœ¬æ¬¡è¿­ä»£çš„éšæœºæ—¶é—´æ­¥ï¼Œä»¥åŠæ‰€éœ€çš„ç›®æ ‡ï¼ˆé€Ÿåº¦åˆ†é‡ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨å‡æ–¹è¯¯å·®è®¡ç®—æŸå¤±ã€‚
- en: This training goes for 500 epochs, where the logs can be found [here](https://drive.google.com/drive/folders/1Ots9ujg4dSaTABKkyaUnFLpCKQ7kCgdK?usp=sharing).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè®­ç»ƒè¿‡ç¨‹æŒç»­ 500 ä¸ªå‘¨æœŸï¼Œæ—¥å¿—å¯ä»¥åœ¨ [è¿™é‡Œ](https://drive.google.com/drive/folders/1Ots9ujg4dSaTABKkyaUnFLpCKQ7kCgdK?usp=sharing)
    æ‰¾åˆ°ã€‚
- en: '**Sampling images**'
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**é‡‡æ ·å›¾åƒ**'
- en: After we have both models trained, we can sample synthetic images. We use [this
    script](https://github.com/Warvito/generative_chestxray/blob/main/src/python/testing/sample_images.py).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬è®­ç»ƒå¥½ä¸¤ä¸ªæ¨¡å‹ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥é‡‡æ ·åˆæˆå›¾åƒã€‚æˆ‘ä»¬ä½¿ç”¨ [è¿™ä¸ªè„šæœ¬](https://github.com/Warvito/generative_chestxray/blob/main/src/python/testing/sample_images.py)ã€‚
- en: '![](../Images/b16bd7d88dfcf2b4792f89bd9fafaca3.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b16bd7d88dfcf2b4792f89bd9fafaca3.png)'
- en: This script uses the classifier-free guidance, which is a method proposed by
    [Ho et al.](https://arxiv.org/abs/2207.12598), to be able to enforce the text
    prompts used in image generation. In this method, we have a guidance scale that
    we can use to sacrifice the diversity of the generated data to obtain a sample
    with higher fidelity to the textual prompt. 7.0 is the default value.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè„šæœ¬ä½¿ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼æ–¹æ³•ï¼Œè¿™æ˜¯ [Ho ç­‰äºº](https://arxiv.org/abs/2207.12598) æå‡ºçš„ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºåœ¨å›¾åƒç”Ÿæˆä¸­å¼ºåˆ¶æ‰§è¡Œæ–‡æœ¬æç¤ºã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªå¼•å¯¼æ¯”ä¾‹ï¼Œå¯ä»¥ç”¨æ¥ç‰ºç‰²ç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§ï¼Œä»¥è·å¾—ä¸æ–‡æœ¬æç¤ºæ›´é«˜ä¸€è‡´æ€§çš„æ ·æœ¬ã€‚é»˜è®¤å€¼æ˜¯
    7.0ã€‚
- en: In the following image, we can see how the trained model was able to learn about
    the clinical features, as well as the position and severity of them.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹å›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹å¦‚ä½•å­¦ä¼šäº†ä¸´åºŠç‰¹å¾ä»¥åŠè¿™äº›ç‰¹å¾çš„ä½ç½®å’Œä¸¥é‡ç¨‹åº¦ã€‚
- en: '![](../Images/9eb959fffed9d76882c0505540cb2a3e.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9eb959fffed9d76882c0505540cb2a3e.png)'
- en: '**Evaluation**'
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**è¯„ä¼°**'
- en: In this section, we will show how to use metrics from MONAI to evaluate the
    performance of our generative models in several aspects.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ MONAI çš„åº¦é‡æ ‡å‡†æ¥è¯„ä¼°æˆ‘ä»¬ç”Ÿæˆæ¨¡å‹çš„å¤šä¸ªæ–¹é¢çš„æ€§èƒ½ã€‚
- en: '**Quality of the Autoencoder reconstructions with MS-SSIM**'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨ MS-SSIM çš„è‡ªç¼–ç å™¨é‡å»ºè´¨é‡**'
- en: First, we verify how well our Autoencoder-kl reconstructs the input images.
    This is an important point when developing our models, because the quality of
    the compression and reconstructed data will define a ceiling for the quality of
    our sample. If the model does not learn how to decode the images from the latent
    representation well, or if it does not model our latent space well, it is not
    possible to decode the synthetic representations in a realistic way. In [this
    script](https://github.com/Warvito/generative_chestxray/blob/main/src/python/testing/compute_msssim_reconstruction.py),
    we use the 5000 images from the test set to evaluate our model. We can verify
    how well our reconstructions look using the **Multiscale Structural Similarity
    Index Measure (MS-SSIM)**. The MS-SSIM is a widely used image quality assessment
    method that measures the similarity between two images. Unlike traditional image
    quality assessment methods such as PSNR and SSIM, MS-SSIM is capable of capturing
    the structural information of an image at different scales.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬éªŒè¯æˆ‘ä»¬çš„Autoencoder-klå¦‚ä½•é‡å»ºè¾“å…¥å›¾åƒã€‚è¿™æ˜¯å¼€å‘æˆ‘ä»¬æ¨¡å‹æ—¶ä¸€ä¸ªé‡è¦çš„ç‚¹ï¼Œå› ä¸ºå‹ç¼©å’Œé‡å»ºæ•°æ®çš„è´¨é‡å°†å®šä¹‰æˆ‘ä»¬æ ·æœ¬è´¨é‡çš„ä¸Šé™ã€‚å¦‚æœæ¨¡å‹æ— æ³•å¾ˆå¥½åœ°ä»æ½œåœ¨è¡¨ç¤ºä¸­è§£ç å›¾åƒï¼Œæˆ–è€…å®ƒæ— æ³•å¾ˆå¥½åœ°å»ºæ¨¡æˆ‘ä»¬çš„æ½œåœ¨ç©ºé—´ï¼Œå°±æ— æ³•ä»¥ç°å®çš„æ–¹å¼è§£ç åˆæˆè¡¨ç¤ºã€‚åœ¨[è¿™ä¸ªè„šæœ¬](https://github.com/Warvito/generative_chestxray/blob/main/src/python/testing/compute_msssim_reconstruction.py)ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ¥è‡ªæµ‹è¯•é›†çš„5000å¼ å›¾åƒæ¥è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨**å¤šå°ºåº¦ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ ‡ï¼ˆMS-SSIMï¼‰**æ¥éªŒè¯æˆ‘ä»¬çš„é‡å»ºæ•ˆæœã€‚MS-SSIMæ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„å›¾åƒè´¨é‡è¯„ä¼°æ–¹æ³•ï¼Œç”¨äºè¡¡é‡ä¸¤å¼ å›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ä¸ä¼ ç»Ÿçš„å›¾åƒè´¨é‡è¯„ä¼°æ–¹æ³•å¦‚PSNRå’ŒSSIMä¸åŒï¼ŒMS-SSIMèƒ½å¤Ÿæ•æ‰å›¾åƒåœ¨ä¸åŒå°ºåº¦ä¸Šçš„ç»“æ„ä¿¡æ¯ã€‚
- en: In this case, the higher the value, the better the model. For our current release
    (version 0.2), we observed that our model had mean **MS-SSIM reconstructions of
    0.9789.**
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå€¼è¶Šé«˜ï¼Œæ¨¡å‹è¶Šå¥½ã€‚å¯¹äºæˆ‘ä»¬å½“å‰çš„ç‰ˆæœ¬ï¼ˆ0.2ï¼‰ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æˆ‘ä»¬çš„æ¨¡å‹çš„**MS-SSIMé‡å»ºå¹³å‡å€¼ä¸º0.9789**ã€‚
- en: '**Diversity of the samples with MS-SSIM**'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ ·æœ¬çš„MS-SSIMå¤šæ ·æ€§**'
- en: We will first evaluate the diversity of the samples generated by our model.
    For that, we compute the **Multiscale Structural Similarity Index Measure** between
    different generated images. In this project, we assume that, if our generative
    model is capable of generating diverse images, it will present a low average MS-SSIM
    value when comparing pairs of synthetic images. For example, if we had a problem
    like a mode collapse, our generated images would look similar, and the MS-SSIM
    values would be much lower than what we observe in a real dataset.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é¦–å…ˆè¯„ä¼°æˆ‘ä»¬æ¨¡å‹ç”Ÿæˆçš„æ ·æœ¬çš„å¤šæ ·æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¡ç®—ä¸åŒç”Ÿæˆå›¾åƒä¹‹é—´çš„**å¤šå°ºåº¦ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ ‡**ã€‚åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å‡è®¾å¦‚æœæˆ‘ä»¬çš„ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–çš„å›¾åƒï¼Œé‚£ä¹ˆåœ¨æ¯”è¾ƒåˆæˆå›¾åƒå¯¹æ—¶ï¼Œå®ƒä¼šå‘ˆç°å‡ºè¾ƒä½çš„å¹³å‡MS-SSIMå€¼ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬é‡åˆ°äº†æ¨¡å¼å´©æºƒé—®é¢˜ï¼Œæˆ‘ä»¬ç”Ÿæˆçš„å›¾åƒå°†çœ‹èµ·æ¥ç›¸ä¼¼ï¼ŒMS-SSIMå€¼ä¼šæ¯”æˆ‘ä»¬åœ¨çœŸå®æ•°æ®é›†ä¸­è§‚å¯Ÿåˆ°çš„å€¼ä½å¾—å¤šã€‚
- en: In our project, we are using unconditioned samples (samples generated with the
    â€œâ€ (empty string) as a textual prompt) to maintain the natural proportion of the
    original dataset. As shown in [this script](https://github.com/Warvito/generative_chestxray/blob/main/src/python/testing/compute_msssim_sample.py),
    we select 1000 synthetic samples of our model and use the data loaders from MONAI
    to help to load all possible pairs of images. We use a nested loop to go through
    all possible pairs and ignore the cases where it is the same image selected in
    both [data loader](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/testing/compute_msssim_sample.py#L89).
    Here we can observe an **MS-SSIM of 0.4083**. We can perform the same evaluation
    in real images from the test set as a reference value. Using [this script](https://github.com/Warvito/generative_chestxray/blob/main/src/python/testing/compute_msssim_test_set.py),
    we obtain **MS-SSIM=0.4046 for the test set**, indicating that our model is generating
    images with a diversity similar to the one observed at the real data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æœªæ¡ä»¶åŒ–çš„æ ·æœ¬ï¼ˆä½¿ç”¨â€œâ€ï¼ˆç©ºå­—ç¬¦ä¸²ï¼‰ä½œä¸ºæ–‡æœ¬æç¤ºç”Ÿæˆçš„æ ·æœ¬ï¼‰ä»¥ä¿æŒåŸå§‹æ•°æ®é›†çš„è‡ªç„¶æ¯”ä¾‹ã€‚å¦‚[è¿™ä¸ªè„šæœ¬](https://github.com/Warvito/generative_chestxray/blob/main/src/python/testing/compute_msssim_sample.py)æ‰€ç¤ºï¼Œæˆ‘ä»¬é€‰æ‹©äº†1000ä¸ªæ¨¡å‹ç”Ÿæˆçš„åˆæˆæ ·æœ¬ï¼Œå¹¶ä½¿ç”¨MONAIçš„æ•°æ®åŠ è½½å™¨æ¥å¸®åŠ©åŠ è½½æ‰€æœ‰å¯èƒ½çš„å›¾åƒå¯¹ã€‚æˆ‘ä»¬ä½¿ç”¨åµŒå¥—å¾ªç¯éå†æ‰€æœ‰å¯èƒ½çš„å›¾åƒå¯¹ï¼Œå¹¶å¿½ç•¥åœ¨ä¸¤ä¸ª[æ•°æ®åŠ è½½å™¨](https://github.com/Warvito/generative_chestxray/blob/83f6c0892c63a1cdbf308ff654d40afc0af51bab/src/python/testing/compute_msssim_sample.py#L89)ä¸­é€‰æ‹©ç›¸åŒå›¾åƒçš„æƒ…å†µã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°**MS-SSIMä¸º0.4083**ã€‚æˆ‘ä»¬å¯ä»¥åœ¨æµ‹è¯•é›†çš„çœŸå®å›¾åƒä¸­è¿›è¡Œç›¸åŒçš„è¯„ä¼°ä½œä¸ºå‚è€ƒå€¼ã€‚ä½¿ç”¨[è¿™ä¸ªè„šæœ¬](https://github.com/Warvito/generative_chestxray/blob/main/src/python/testing/compute_msssim_test_set.py)ï¼Œæˆ‘ä»¬å¾—åˆ°**æµ‹è¯•é›†çš„MS-SSIM=0.4046**ï¼Œè¿™è¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹ç”Ÿæˆçš„å›¾åƒä¸çœŸå®æ•°æ®ä¸­çš„å¤šæ ·æ€§ç›¸ä¼¼ã€‚
- en: However, diversity does not mean the images look good or realistic. So we will
    check the image quality in the next step!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå¤šæ ·æ€§å¹¶ä¸æ„å‘³ç€å›¾åƒçœ‹èµ·æ¥å¥½æˆ–çœŸå®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€æ­¥æ£€æŸ¥å›¾åƒè´¨é‡ï¼
- en: '**Synthetic Image Quality with FID**'
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**åˆæˆå›¾åƒè´¨é‡ä¸FID**'
- en: Finally, we measure the **FrÃ©chet inception distance (FID)** metric of the generated
    samples ([link](https://github.com/Warvito/generative_chestxray/blob/main/src/python/testing/compute_fid.py)).
    The FID is a metric that evaluates the distribution between two groups, showing
    how similar they are. For this, we need a pre-trained neural network from which
    we can extract features that we will use to compute the distance (similar to the
    perceptual loss). In this example, we opted to use neural networks available in
    the[**torchxrayvision package**](https://github.com/mlmed/torchxrayvision). We
    used a Dense121 network (*â€œdensenet121-res224-allâ€*), and we chose this network
    to be close to what is used in the literature for CXR synthetic images. From this
    network, we obtain a feature vector with 1024 dimensions. As recommended in the
    original FID paper, it is important to use a similar amount of examples compared
    to the number of features. For this reason, we use 1000 unconditioned images and
    compare them to 1000 images from the test set. For FIDs, the lower the best, and
    here we obtained a reasonable **FID=9.0237**.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬æµ‹é‡ç”Ÿæˆæ ·æœ¬çš„**FrÃ©chet Inception Distance (FID)**æŒ‡æ ‡ï¼ˆ[é“¾æ¥](https://github.com/Warvito/generative_chestxray/blob/main/src/python/testing/compute_fid.py)ï¼‰ã€‚FIDæ˜¯ä¸€ä¸ªè¯„ä¼°ä¸¤ä¸ªç»„ä¹‹é—´åˆ†å¸ƒå·®å¼‚çš„æŒ‡æ ‡ï¼Œæ˜¾ç¤ºå®ƒä»¬çš„ç›¸ä¼¼åº¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªé¢„è®­ç»ƒçš„ç¥ç»ç½‘ç»œï¼Œä»ä¸­æå–ç‰¹å¾ç”¨äºè®¡ç®—è·ç¦»ï¼ˆç±»ä¼¼äºæ„ŸçŸ¥æŸå¤±ï¼‰ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬é€‰æ‹©ä½¿ç”¨[**torchxrayvisionåŒ…**](https://github.com/mlmed/torchxrayvision)ä¸­çš„ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªDense121ç½‘ç»œï¼ˆ*â€œdensenet121-res224-allâ€*ï¼‰ï¼Œå¹¶é€‰æ‹©äº†è¿™ä¸ªç½‘ç»œï¼Œä»¥æ¥è¿‘æ–‡çŒ®ä¸­ç”¨äºCXRåˆæˆå›¾åƒçš„ç½‘ç»œã€‚é€šè¿‡è¿™ä¸ªç½‘ç»œï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ª1024ç»´çš„ç‰¹å¾å‘é‡ã€‚æ ¹æ®åŸå§‹FIDè®ºæ–‡çš„å»ºè®®ï¼Œä½¿ç”¨ä¸ç‰¹å¾æ•°é‡ç›¸ä¼¼æ•°é‡çš„æ ·æœ¬æ˜¯é‡è¦çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†1000å¼ æ— æ¡ä»¶å›¾åƒï¼Œå¹¶å°†å…¶ä¸1000å¼ æµ‹è¯•é›†å›¾åƒè¿›è¡Œæ¯”è¾ƒã€‚å¯¹äºFIDæ¥è¯´ï¼Œå€¼è¶Šä½è¶Šå¥½ï¼Œæˆ‘ä»¬è¿™é‡Œå¾—åˆ°äº†ä¸€ä¸ªåˆç†çš„**FID=9.0237**ã€‚
- en: '**Conclusion**'
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç»“è®º**'
- en: In this post, we present a way to develop a project with **MONAI Generative
    Models**, from downloading the data to evaluating the generative models and synthetic
    data. Although this project version could be more efficient and have better hyperparameters,
    we hope that it illustrates well the different features our extension offers.
    If you have any idea on how to improve our CXR model or if you would like to contribute
    to our package, please, just add comments on our **issue sections** at [here](https://github.com/Warvito/generative_chestxray/issues)
    or [here](https://github.com/Project-MONAI/GenerativeModels/issues).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§ä½¿ç”¨**MONAIç”Ÿæˆæ¨¡å‹**å¼€å‘é¡¹ç›®çš„æ–¹æ³•ï¼Œä»ä¸‹è½½æ•°æ®åˆ°è¯„ä¼°ç”Ÿæˆæ¨¡å‹å’Œåˆæˆæ•°æ®ã€‚å°½ç®¡è¿™ä¸ªé¡¹ç›®ç‰ˆæœ¬å¯èƒ½æ›´é«˜æ•ˆå¹¶æ‹¥æœ‰æ›´å¥½çš„è¶…å‚æ•°ï¼Œæˆ‘ä»¬å¸Œæœ›å®ƒèƒ½å¾ˆå¥½åœ°å±•ç¤ºæˆ‘ä»¬æ‰©å±•æ‰€æä¾›çš„ä¸åŒåŠŸèƒ½ã€‚å¦‚æœä½ æœ‰ä»»ä½•æ”¹è¿›æˆ‘ä»¬CXRæ¨¡å‹çš„æƒ³æ³•ï¼Œæˆ–è€…å¸Œæœ›ä¸ºæˆ‘ä»¬çš„åŒ…åšè´¡çŒ®ï¼Œè¯·åœ¨æˆ‘ä»¬çš„**é—®é¢˜éƒ¨åˆ†**ä¸­ç•™ä¸‹è¯„è®ºï¼Œ[åœ¨è¿™é‡Œ](https://github.com/Warvito/generative_chestxray/issues)æˆ–[åœ¨è¿™é‡Œ](https://github.com/Project-MONAI/GenerativeModels/issues)ã€‚
- en: Our trained model can be found at the [**MONAI Model Zoo**](https://github.com/Project-MONAI/GenerativeModels/tree/main/model-zoo/models)together
    with our 3D Brain Generator and other models. Our Model Zoo makes it easier downloading
    the model weights and the code to perform inference.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹å¯ä»¥åœ¨[**MONAIæ¨¡å‹åº“**](https://github.com/Project-MONAI/GenerativeModels/tree/main/model-zoo/models)ä¸­æ‰¾åˆ°ï¼Œä¸æˆ‘ä»¬çš„3Dè„‘éƒ¨ç”Ÿæˆå™¨åŠå…¶ä»–æ¨¡å‹ä¸€èµ·ã€‚æˆ‘ä»¬çš„æ¨¡å‹åº“ä½¿å¾—ä¸‹è½½æ¨¡å‹æƒé‡å’Œæ‰§è¡Œæ¨ç†ä»£ç æ›´åŠ ä¾¿æ·ã€‚
- en: '![](../Images/7a16c3aa279b9c4e9eaaef950449cd46.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a16c3aa279b9c4e9eaaef950449cd46.png)'
- en: For more tutorials and to learn more about our features, check out our Tutorial
    page at this [link](https://github.com/Project-MONAI/GenerativeModels/tree/main/tutorials),
    and follow me for the latest updates and more guides like this one! ğŸ˜
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æ¬²è·å–æ›´å¤šæ•™ç¨‹å¹¶äº†è§£æˆ‘ä»¬çš„åŠŸèƒ½ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„æ•™ç¨‹é¡µé¢ï¼Œç‚¹å‡»æ­¤[é“¾æ¥](https://github.com/Project-MONAI/GenerativeModels/tree/main/tutorials)ï¼Œå¹¶å…³æ³¨æˆ‘ä»¥è·å–æœ€æ–°æ›´æ–°å’Œæ›´å¤šç±»ä¼¼çš„æŒ‡å—ï¼ğŸ˜
- en: '*Note: All images unless otherwise noted are by the author*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„ï¼šæ‰€æœ‰å›¾ç‰‡é™¤éå¦æœ‰è¯´æ˜ï¼Œå‡ç”±ä½œè€…æä¾›*'
