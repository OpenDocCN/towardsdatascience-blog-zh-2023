- en: Your Dataset Has Missing Values? Do Nothing!
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你的数据集有缺失值？什么都不做！
- en: 原文：[https://towardsdatascience.com/your-dataset-has-missing-values-do-nothing-10d1633b3727](https://towardsdatascience.com/your-dataset-has-missing-values-do-nothing-10d1633b3727)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/your-dataset-has-missing-values-do-nothing-10d1633b3727](https://towardsdatascience.com/your-dataset-has-missing-values-do-nothing-10d1633b3727)
- en: Models can handle missing values out-of-the-box more effectively than imputation
    methods. An empirical proof
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型能够比填补方法更有效地处理缺失值。这是一个实证证明。
- en: '[](https://medium.com/@mazzanti.sam?source=post_page-----10d1633b3727--------------------------------)[![Samuele
    Mazzanti](../Images/432477d6418a3f79bf25dec42755d364.png)](https://medium.com/@mazzanti.sam?source=post_page-----10d1633b3727--------------------------------)[](https://towardsdatascience.com/?source=post_page-----10d1633b3727--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----10d1633b3727--------------------------------)
    [Samuele Mazzanti](https://medium.com/@mazzanti.sam?source=post_page-----10d1633b3727--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mazzanti.sam?source=post_page-----10d1633b3727--------------------------------)[![Samuele
    Mazzanti](../Images/432477d6418a3f79bf25dec42755d364.png)](https://medium.com/@mazzanti.sam?source=post_page-----10d1633b3727--------------------------------)[](https://towardsdatascience.com/?source=post_page-----10d1633b3727--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----10d1633b3727--------------------------------)
    [Samuele Mazzanti](https://medium.com/@mazzanti.sam?source=post_page-----10d1633b3727--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----10d1633b3727--------------------------------)
    ·10 min read·Oct 9, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----10d1633b3727--------------------------------)
    ·10分钟阅读·2023年10月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d3685f99b6f73b5c92ca622966a838de.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3685f99b6f73b5c92ca622966a838de.png)'
- en: '[Image by Author]'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[作者提供的图片]'
- en: Missing values are very common in real datasets. Over time, many methods have
    been proposed to deal with this issue. Usually, they consist either in removing
    data that contain missing values or in imputing them with some techniques.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失值在真实数据集中非常常见。随着时间的推移，许多方法被提出以解决这个问题。通常，这些方法要么是删除包含缺失值的数据，要么是使用一些技术进行填补。
- en: 'In this article, I will test a third alternative:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将测试第三种替代方案：
- en: Doing nothing.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 什么都不做。
- en: 'Indeed, the best models for tabular datasets (namely, XGBoost, LightGBM, and
    CatBoost) can natively handle missing values. So, the question I will try to answer
    is:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，最适合表格数据集的模型（即XGBoost、LightGBM和CatBoost）可以原生处理缺失值。因此，我将尝试回答的问题是：
- en: Can these models handle missing values effectively, or would we obtain a better
    result with a preliminary imputation?
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这些模型是否能有效处理缺失值，还是通过预处理填补能获得更好的结果？
- en: Who said we should care about nulls?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谁说我们应该关心缺失值？
- en: There seems to be a **widespread belief that we must do *something* about missing
    values**. For instance, I asked ChatGPT what should I do if my dataset contain
    missing values, and it suggested 10 different ways to get rid of them (you can
    read the full answer [here](https://chat.openai.com/share/d65dcaff-ce67-4fac-b54a-31b1f00f50ba)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎存在一种**普遍的信念，认为我们必须对缺失值做*一些事情***。例如，我问了ChatGPT，如果我的数据集包含缺失值应该怎么办，它建议了10种不同的解决方法（你可以在[这里](https://chat.openai.com/share/d65dcaff-ce67-4fac-b54a-31b1f00f50ba)阅读完整回答）。
- en: But where does this belief ​​come from?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这种信念来自哪里呢？
- en: Usually, these kinds of opinions originate from historical models, particularly
    from linear regression. This is also the case. Let’s see why.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些观点源于历史模型，特别是线性回归。这也是如此。让我们看看原因。
- en: 'Suppose that we have this dataset:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有这样一个数据集：
- en: '![](../Images/c8c027fa0e94bd6e129ff265794c0b29.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8c027fa0e94bd6e129ff265794c0b29.png)'
- en: A dataset with missing values. [Image by Author]
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个缺失值的数据集。[作者提供的图片]
- en: If we tried to train a linear regression on these features, we would get an
    error. In fact, to be able to make predictions, linear regression needs to multiply
    each feature by a numeric coefficient. If one or more features are missing, it’s
    impossible to make a prediction for that row.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试在这些特征上训练线性回归，会出现错误。事实上，为了能够进行预测，线性回归需要将每个特征乘以一个数值系数。如果一个或多个特征缺失，就无法对该行进行预测。
- en: This is why many imputation methods have been proposed. For instance, one of
    the simplest possibilities is to replace the nulls with the feature’s mean.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么提出了许多填补方法。例如，最简单的方法之一是用特征的均值替换缺失值。
- en: '![](../Images/4929f5be7de782e268bd095b09c6a716.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4929f5be7de782e268bd095b09c6a716.png)'
- en: Imputation with the feature’s mean. [Image by Author]
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用特征均值填补。[图片由作者提供]
- en: Another, more sophisticated, approach is to use the relationships between the
    variables to predict the most likely value to fill that specific entry. This entails
    training one predictive model for each feature (using the other features as predictors).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种更复杂的方法是利用变量之间的关系来预测填补特定条目的最可能值。这意味着为每个特征训练一个预测模型（使用其他特征作为预测变量）。
- en: '![](../Images/d7070bd6fc47236c66194948f8c3291c.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7070bd6fc47236c66194948f8c3291c.png)'
- en: 'Iterative Imputation: each feature is estimated from all the others. [Image
    by Author]'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代填补：每个特征通过其他所有特征来估计。[图片由作者提供]
- en: However, not all models are like linear regression.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并非所有模型都像线性回归一样。
- en: Indeed, as luck would have it, the best-performing models for tabular tasks
    (namely, tree-based models, such as XGBoost, LightGBM, and CatBoost) can handle
    missing values natively.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，碰巧的是，表格任务中表现最好的模型（即树模型，如 XGBoost、LightGBM 和 CatBoost）可以原生处理缺失值。
- en: How is that possible?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这怎么可能？
- en: 'Because in tree-like structures, missing values can be treated just like other
    values, i.e. the model can assign them to a branch of the tree. For example, this
    image comes from XGBoost documentation:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在树状结构中，缺失值可以像其他值一样处理，即模型可以将它们分配到树的某个分支上。例如，这张图片来自 XGBoost 文档：
- en: '![](../Images/1b94dc1f7b9c122ef454db1b0f830d15.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b94dc1f7b9c122ef454db1b0f830d15.png)'
- en: How tree-based models handle missing values. [Image from [XGBoost documentation](https://xgboost.readthedocs.io/en/stable/tutorials/feature_interaction_constraint.html)]
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 树模型如何处理缺失值。[图片来自[XGBoost 文档](https://xgboost.readthedocs.io/en/stable/tutorials/feature_interaction_constraint.html)]
- en: As you can see, for each split XGBoost chooses a default branch on which missing
    values (if any) are routed.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，对于每个分裂，XGBoost 会选择一个默认分支，将缺失值（如果有的话）路由到该分支上。
- en: So, are we saying that just because these models can handle missing values,
    we should avoid imputation? I never said that.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们是否说因为这些模型可以处理缺失值，我们就应该避免填补？我从未这么说过。
- en: As data scientists, we usually want to know well **in practice**. So, the focus
    of the next paragraphs will be to compare the model performance with and without
    imputation and see which approach performs better.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家，我们通常希望在**实践中**有深刻的了解。因此，接下来的段落将重点比较有无填补的模型性能，并观察哪种方法表现更好。
- en: Experimenting with real datasets
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用真实数据集进行实验
- en: 'Our objective is to compare two approaches:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是比较两种方法：
- en: Train and test the model on the dataset **with missing values**.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**具有缺失值**的数据集上训练和测试模型。
- en: Train and test the model on the dataset with no missing values (after they have
    been **imputed through some imputation method**).
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在没有缺失值的数据集上训练和测试模型（在这些缺失值**通过某种填补方法填补后**）。
- en: I will use 14 real datasets that are available in [Pycaret](https://github.com/pycaret/pycaret)
    (a Python library under [MIT license](https://github.com/pycaret/pycaret/blob/master/LICENSE)).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用 14 个在[Pycaret](https://github.com/pycaret/pycaret)中提供的真实数据集（这是一个受[MIT 许可证](https://github.com/pycaret/pycaret/blob/master/LICENSE)保护的
    Python 库）。
- en: These datasets do not contain missing values, so we will need to fabricate them.
    I will call this procedure null-sowing because it involves disseminating null
    values on the original dataset (i.e. “canceling” some of the original values).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据集不包含缺失值，所以我们需要人为制造这些缺失值。我将这个过程称为空值播撒，因为它涉及在原始数据集中散布空值（即“取消”一些原始值）。
- en: 'How many values are we going to cancel? To ensure that the experiment is representative,
    we will try with different percentages of nulls: 5%, 10%, 20% and 50%.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要取消多少值？为了确保实验具有代表性，我们将尝试不同百分比的空值：5%、10%、20% 和 50%。
- en: '![](../Images/9f70de3fcb9fb2eafdda9def43362c60.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f70de3fcb9fb2eafdda9def43362c60.png)'
- en: Null-sowing with different percentages. [Image by Author]
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 不同百分比的空值播撒。[图片由作者提供]
- en: 'I use two different strategies to create null values:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用两种不同的策略来创建空值：
- en: '**Random**: a random value between 0 and 1 is attributed to each entry of the
    dataset and, if smaller than the threshold, the entry is canceled.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机**：为数据集的每个条目分配一个介于 0 和 1 之间的随机值，如果小于阈值，则该条目被取消。'
- en: '**Non-random**: for each feature, I sort the values either in ascending or
    descending order, and attribute a proportional probability of being canceled based
    on the position of the value in the sorted sequence.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非随机**：对于每个特征，我将值按升序或降序排序，并根据值在排序序列中的位置分配一个相应的取消概率。'
- en: Moreover, for each combination, I will try 25 different random train/test splits.
    The purpose of doing this is to ensure that the results we observe are consistent
    over many repetitions and not simply due to chance.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于每种组合，我将尝试25种不同的随机训练/测试划分。这样做的目的是确保我们观察到的结果在多次重复中是一致的，而不仅仅是偶然的。
- en: To sum up, these are all the combinations that I will try.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，这些都是我将尝试的所有组合。
- en: '![](../Images/d89a9ff89269302e399f86efce281bd8.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d89a9ff89269302e399f86efce281bd8.png)'
- en: Combinations that form our experiment. [Image by Author]
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 形成我们实验的组合。[图像来源：作者]
- en: 'For each of these 2,800 combinations, I will compute the average precision
    (on the test set) of LightGBM with three different approaches:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这2,800种组合中的每一种，我将计算LightGBM在三种不同方法下的平均精度（在测试集上）：
- en: Using the original dataset (with **no missing values**). I will call this average
    precision `ap_original`.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用原始数据集（**没有缺失值**）。我将称这个平均精度为`ap_original`。
- en: Using the dataset **containing the missing values I have sown.** I will call
    this average precision`ap_noimpute`.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用包含**我已经撒入缺失值的数据集**。我将称这个平均精度为`ap_noimpute`。
- en: Using the dataset where the **missing values have been filled** using Scikit-Learn’s
    IterativeImputer. I will call this average precision`ap_impute`.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**已用Scikit-Learn的IterativeImputer填充缺失值**的数据集。我将称这个平均精度为`ap_impute`。
- en: 'Let’s clarify the difference between these three metrics with the aid of a
    figure:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们借助图示来阐明这三种指标之间的差异：
- en: '![](../Images/b7d9be40c12e0e1242648b95d2421250.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7d9be40c12e0e1242648b95d2421250.png)'
- en: 3 versions of the dataset. A different LightGBM has been trained on each version.
    [Image by Author]
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 3个版本的数据集。每个版本上训练了不同的LightGBM。[图像来源：作者]
- en: Results
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: 'I tried all the 2,800 combinations listed above and tracked `ap_original`,
    `ap_noimpute`, and `ap_impute` for each of them. I stored the results in a table
    like this:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我尝试了上述所有2,800种组合，并跟踪了每一种的`ap_original`、`ap_noimpute`和`ap_impute`。我将结果存储在类似这样的表格中：
- en: '![](../Images/6018e60a88ced90fabf8cf304100cf45.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6018e60a88ced90fabf8cf304100cf45.png)'
- en: Results of the experiment. [Image by Author]
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果。[图像来源：作者]
- en: This table has 112 rows (i.e. 14 datasets x 2 null-sowing strategies x 4 null
    frequencies). The first three columns indicate which dataset, null-sowing strategy,
    and null frequency identify that row. Then, there are three columns that store
    the average precisions reported by that approach on the 25 bootstrap iterations.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表格有112行（即14个数据集 x 2种缺失值撒入策略 x 4种缺失频率）。前三列指示哪个数据集、缺失值撒入策略和缺失频率标识那一行。然后，有三列存储该方法在25次bootstrap迭代中报告的平均精度。
- en: 'To make it even clearer, let’s take the fourth column of the first row. This
    is an array of 25 elements: each of them is the average precision that LightGBM
    realized — for that particular train/test split (i.e. bootstrap iteration) — on
    the original dataset (that’s why it’s called `ap_original`) for dataset “bank”
    with 5% of missing values randomly sown.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更加清楚，让我们看看第一行的第四列。这是一个包含25个元素的数组：每个元素都是LightGBM在原始数据集上（因此称之为`ap_original`）在特定训练/测试划分（即bootstrap迭代）中实现的平均精度，数据集“bank”包含5%的随机撒入缺失值。
- en: Of course, we are not interested in each single bootstrap iteration, so we need
    to aggregate the arrays in some way. Since we want to compare the three approaches
    with each other, the most simple aggregation is counting the number of times that
    one approach is superior to another (meaning it has a higher average precision).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们对每一个单独的bootstrap迭代并不感兴趣，所以我们需要以某种方式聚合这些数组。由于我们希望比较三种方法，最简单的聚合方式是统计一种方法优于另一种方法的次数（即它具有更高的平均精度）。
- en: 'Thus, for each row of the table, I calculated the percentage of times that:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于表格中的每一行，我计算了：
- en: '`ap_original` > `ap_noimpute`, which means that the model on the original dataset
    is better than the model on the dataset containing missing values.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ap_original` > `ap_noimpute`，这意味着在原始数据集上的模型优于在包含缺失值的数据集上的模型。'
- en: '`ap_noimpute` > `ap_impute`, which means that the model on the dataset containing
    missing values is better than the model on the dataset filled with IterativeImputer.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ap_noimpute` > `ap_impute`，这意味着在包含缺失值的数据集上的模型优于在使用IterativeImputer填充的数据集上的模型。'
- en: 'This is the result:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '![](../Images/f88a284d0462b9eb4a1f91106ca88972.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f88a284d0462b9eb4a1f91106ca88972.png)'
- en: Results of the experiment. [Image by Author]
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果。[图像来源：作者]
- en: For example, if we take the last column of the first row, it means that `ap_noimpute`
    is greater than `ap_impute` in 60% out of the 25 iterations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们取第一行的最后一列，这意味着在25次迭代中，`ap_noimpute`在60%的情况下大于`ap_impute`。
- en: 'Let’s look at the column called `original_>_noimpute`. Apparently, it’s 100%
    most of the time. And this makes sense: it’s reasonable that the model trained
    on the original dataset does better than the model trained on the dataset in which
    we have canceled some entries.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看名为`original_>_noimpute`的列。显然，大多数情况下是100%。这也是合理的：模型在原始数据集上训练的效果优于在取消了一些条目的数据集上训练的模型。
- en: 'But the most important information for us is in the last column. In fact:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 但对我们来说最重要的信息在最后一列。实际上：
- en: when `noimpute_>_impute` is greater than 50%, it means that the model trained
    on the null dataset outperformed most of the times the model trained on the imputed
    dataset.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当`noimpute_>_impute`大于50%时，这意味着在大多数情况下，基于空数据集训练的模型优于基于填补数据集训练的模型。
- en: when `noimpute_>_impute` is less than 50%, the opposite.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当`noimpute_>_impute`小于50%时，情况正好相反。
- en: So we could be tempted to simply take the mean of this column and decide which
    approach works better based on whether the global mean is above or below 50%.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可能会倾向于简单地取这一列的均值，并根据全局均值是高于还是低于50%来决定哪种方法效果更好。
- en: But if we do that, we could be misled by the effect of chance. Indeed, if a
    value is close to 50%, like 48% or 52%, this could be easily due to randomness.
    To account for that, we need to frame this as a statistical test.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果我们这样做，我们可能会被偶然性所误导。实际上，如果一个值接近50%，比如48%或52%，这可能很容易是由于随机性造成的。为了考虑这一点，我们需要将其框定为一个统计测试。
- en: Setting up a statistical test
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设定统计测试
- en: To avoid being fooled by chance, I will take a couple of precautions.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免被偶然性欺骗，我将采取几个预防措施。
- en: First, I will keep only the cases in which the average precision on the original
    dataset is greater than the average precision on the missing dataset more than
    90% of the time (in other words, I will keep only the rows where `original_>_noimpute`
    > .90).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我只保留那些原始数据集上的平均精度大于缺失数据集上的平均精度超过90%的情况（换句话说，我只保留`original_>_noimpute` > .90的行）。
- en: I will do this because I want to keep only the cases where missing values have
    a clear negative impact on the performance of the model. After doing that, out
    of the initial 112 rows of the table, only 48 remain.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我这样做是因为我想保留只有那些缺失值对模型性能有明显负面影响的情况。在这样做之后，最初112行的表格中，只剩下48行。
- en: Secondly, I will need to compute a “significance threshold” that helps us to
    understand whether a value is significant or not.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我需要计算一个“显著性阈值”，帮助我们理解一个值是否显著。
- en: We already said that when a value is very close to 50%, like 48% or 52%, then
    it’s probably just due to chance. But how close is “very close”? To answer this,
    we need some statistics.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经说过，当一个值非常接近50%时，比如48%或52%，那么这很可能只是由于偶然性造成的。但是“非常接近”到底有多接近呢？要回答这个问题，我们需要一些统计数据。
- en: '**The hypothesis we want to test** (a.k.a. the null hypothesis) is that **imputing
    or not imputing makes no difference**. This is like saying that the probability
    of `ap_impute` being greater than `ap_noimpute` (or vice versa) is 50%. Since
    we have 25 independent iterations, we can compute the probability of obtaining
    a specific result through the binomial distribution.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们想要测试的假设**（即零假设）是**填补或不填补没有区别**。这就像说`ap_impute`大于`ap_noimpute`（或反之）的概率是50%。由于我们有25次独立的迭代，我们可以通过二项分布计算获得特定结果的概率。'
- en: For example, let’s say that, out of 25 iterations, `ap_impute` has been better
    than `ap_noimpute` in 10 iterations. How compatible is this result with our hypothesis?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设在25次迭代中，`ap_impute`在10次迭代中优于`ap_noimpute`。这个结果与我们的假设有多兼容？
- en: '[PRE0]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: So, the probability of obtaining a result as extreme as 10, given our hypothesis,
    is 42%.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，获得一个与10一样极端的结果的概率，在我们的假设下，是42%。
- en: 'Note that I multiplied the cumulative distribution function of the binomial
    distribution by 2 because we are interested in the two-tailed p-value. We can
    double-check this by looking at the p-value associated with any possible outcome:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我将二项分布的累积分布函数乘以2，因为我们对双尾p值感兴趣。我们可以通过查看与任何可能结果相关的p值来再次确认这一点：
- en: '![](../Images/43fb8e6d269971d07c24751c321dec24.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43fb8e6d269971d07c24751c321dec24.png)'
- en: P-values associated with any possible outcome (out of 25 iterations). [Image
    by Author]
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何可能结果相关的p值（来自25次迭代）。 [图片来源：作者]
- en: 'The p-value associated with 12 and 13 is exactly 100%. And this makes sense:
    the probability of obtaining a result at least as extreme as 12 or 13 is necessarily
    100% since they are the least extreme results, given our hypothesis.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与12和13相关的p值恰好是100%。这也是合理的：获得至少与12或13一样极端的结果的概率必然是100%，因为它们是最不极端的结果，鉴于我们的假设。
- en: So, what is our significance level? As per convention, I will take a significance
    level of 1%. However, we must consider that we have many runs, not just one, so
    we must adjust our significance level accordingly. Since we have 48 runs, I will
    use the [Bonferroni correction](https://en.wikipedia.org/wiki/Bonferroni_correction)
    and simply divide 1% by 48, obtaining a final significance level of 0.0002.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们的显著性水平是多少？按照惯例，我将取1%的显著性水平。然而，我们必须考虑到我们有许多次运行，而不仅仅是一次，因此我们必须相应地调整显著性水平。由于我们有48次运行，我将使用[Bonferroni校正](https://en.wikipedia.org/wiki/Bonferroni_correction)，简单地将1%除以48，得到最终显著性水平0.0002。
- en: Comparing this number with the p-values listed above, this means we will consider
    a run significant only if it has less than 3 or more than 22 (both inclusive)
    successes.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个数字与上述p值进行比较，这意味着我们将只在运行中成功次数少于3次或多于22次（包括）时考虑显著。
- en: Now that we have taken measures to avoid being fooled by chance, we are ready
    to look at the results.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经采取措施以避免被偶然因素欺骗，我们准备查看结果。
- en: For simplicity, let’s take as metric the percentage of times that `ap_noimpute`
    is greater than `ap_impute`. For example, if the model based on the dataset containing
    missing values has a better average precision than the model on the imputed dataset
    for 10 out of the 25 iterations, this metric will be 40%.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我们将度量标准设为`ap_noimpute`大于`ap_impute`的次数百分比。例如，如果基于包含缺失值的数据集的模型在25次迭代中的10次中表现出比插补数据集的模型更好的平均精度，则该度量为40%。
- en: Since we have 48 runs, we will have 48 values. So let’s create a bar plot to
    see all of them.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有48次运行，我们将得到48个值。因此，让我们创建一个柱状图来查看所有值。
- en: '![](../Images/2cbd44a59a1c5bbce9d11a18adb1fe5f.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2cbd44a59a1c5bbce9d11a18adb1fe5f.png)'
- en: Results of the experiment. [Image by Author]
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果。[作者提供的图片]
- en: 'The red dashed lines identify the significance threshold: any value that is
    smaller than 12% (inclusive) or greater than 88% (inclusive) is significant (12%
    and 88% correspond respectively to 3/25 and 22/25).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 红色虚线标识了显著性阈值：任何小于12%（包括）或大于88%（包括）的值都是显著的（12%和88%分别对应3/25和22/25）。
- en: From the bar plot, we can see that **when we don’t impute the missing values
    we obtain a better result in 30 out of 48 runs** (63%). In 7 of these 30 cases,
    the result is so extreme that it’s also statistically significant according to
    a 1% p-value with Bonferroni adjustment. Instead, in the 18 cases in which imputing
    wins, the result is never significantly different from pure chance.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从柱状图中，我们可以看到**当我们不插补缺失值时，在48次运行中的30次中获得了更好的结果**（63%）。在这30个案例中的7个，结果极端到根据1% p值及Bonferroni调整也是统计显著的。相反，在18个插补胜出的案例中，结果从未显著地不同于纯粹的偶然。
- en: Based on these results, we can say that **either the difference between imputing
    and not imputing is not significant or it is significant in favor of not imputing**.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些结果，我们可以说**插补与不插补之间的差异要么不显著，要么显著地偏向于不插补**。
- en: In short, there is no reason to impute missing values.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，没有理由插补缺失值。
- en: Conclusions
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we empirically proved that the difference between imputing
    and not imputing is either not significant or significant in favor of not imputing.
    If you also consider that not imputing missing values will keep your pipeline
    cleaner and faster, leaving nulls in your dataset should be the standard, when
    you can do that.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们通过实证证明了插补与不插补之间的差异要么不显著，要么显著地偏向于不插补。如果你还考虑到不插补缺失值会使你的管道更干净、更快速，那么当可以做到这一点时，留空数据集中的空值应该是标准。
- en: 'Of course, this is not always possible. Some models that cannot handle missing
    values: take for instance Linear Regression or K-Means.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这并非总是可能的。一些不能处理缺失值的模型，比如线性回归或K均值。
- en: However, the good news is that when you use the most common models for tabular
    tasks (i.e. tree-based models), not imputing the missing values and just letting
    the model handle them is the most effective and efficient approach.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，好消息是，当你使用最常见的表格任务模型（即基于树的模型）时，不插补缺失值而让模型处理它们是最有效和高效的方法。
- en: '*You can reproduce all the code used for this article with* [*this notebook*](https://github.com/smazzanti/tds_your_dataset_has_missing_values_do_nothing/blob/main/missing-values-do-nothing.ipynb)*.*'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以通过* [*这个笔记本*](https://github.com/smazzanti/tds_your_dataset_has_missing_values_do_nothing/blob/main/missing-values-do-nothing.ipynb)*复现本文中使用的所有代码。*'
- en: '*Thank you for reading!*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢阅读！*'
- en: '*If you find my work useful, you can subscribe to* [***get an email every time
    that I publish a new article***](https://medium.com/@mazzanti.sam/subscribe) *(usually
    once a month).*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你觉得我的工作有用，可以订阅* [***每次我发布新文章时收到邮件***](https://medium.com/@mazzanti.sam/subscribe)
    *(通常每月一次)*。'
- en: '*Want to show me your support for my work? You can* [***buy me a cappuccino***](https://ko-fi.com/samuelemazzanti)*.*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*想要对我的工作表示支持吗？你可以* [***请我喝杯卡布奇诺***](https://ko-fi.com/samuelemazzanti)*。*'
- en: '*If you’d like,* [***add me on Linkedin***](https://www.linkedin.com/in/samuelemazzanti/)*!*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你愿意，* [***在Linkedin上添加我***](https://www.linkedin.com/in/samuelemazzanti/)*！*'
