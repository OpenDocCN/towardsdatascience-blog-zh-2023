- en: 'Prompt Engineering 101: Zero, One, and Few-Shot Prompting'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/prompt-engineering-101-zero-one-and-few-shot-prompting-1e8ced03d434](https://towardsdatascience.com/prompt-engineering-101-zero-one-and-few-shot-prompting-1e8ced03d434)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An introduction to a basic prompt engineering strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@aashishnair?source=post_page-----1e8ced03d434--------------------------------)[![Aashish
    Nair](../Images/23f4b3839e464419332b690a4098d824.png)](https://medium.com/@aashishnair?source=post_page-----1e8ced03d434--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1e8ced03d434--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1e8ced03d434--------------------------------)
    [Aashish Nair](https://medium.com/@aashishnair?source=post_page-----1e8ced03d434--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e8ced03d434--------------------------------)
    ·5 min read·Sep 19, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc425a1abba413deb8fc810c761d1831.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Alexandra_Koch](https://pixabay.com/users/alexandra_koch-621802/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=7720802)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=7720802)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite their seemingly supernatural capabilities, LLMs are ultimately predictive
    models that simply predict the next word in the sequence of words based on the
    provided context.
  prefs: []
  type: TYPE_NORMAL
- en: As such, their performances don’t just hinge on the vast volumes of data they
    are trained with; they also depend heavily on the context provided through the
    users’ inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Frequent users of LLM-powered chatbots are aware of the importance of context.
    Without sufficient context, chatbots, whether they are publicly available services
    (e.g., ChatGPT) or custom-made LLM products, will struggle to carry out the more
    complex instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we delve into one of the most basic strategies used to guide LLMs to
    answer prompts correctly: providing context within user prompts. This is often
    carried out with 3 different methods: zero-shot prompting, one-shot prompting,
    and few-shot prompting.'
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Shot Prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’ve interacted with an LLM-powered chatbot before, you’ve likely already
    used zero-shot prompting unwittingly. Zero-shot prompting entails relying solely
    on an LLM’s pre-trained information to answer a given user prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, let’s suppose that we used ChatGPT as a sentiment classifier.
    An input with zero-shot prompting would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User Prompt:** “Determine the sentiment of this sentence.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Sentence: ‘This basketball has a lot of weight’.”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**ChatGPT:** “The sentiment of the sentence ‘This basketball has a lot of weight’
    is neutral. It does not convey a positive or negative sentiment but rather provides
    factual information about the weight of the basketball.”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When provided the query, the LLM, with no additional context, evaluates this
    sentence to be neutral.
  prefs: []
  type: TYPE_NORMAL
- en: While this method can usually yield satisfactory responses, it may be insufficient
    when the complexity of the question is high or when the criteria for the answer
    are different from the norm.
  prefs: []
  type: TYPE_NORMAL
- en: For such cases, users can consider providing additional context in their prompts.
  prefs: []
  type: TYPE_NORMAL
- en: One-Shot Prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One-shot prompting entails providing an example in the user prompt to guide
    the LLM towards to desired generated response.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, ChatGPT deemed that the sentence, “This basketball
    has a lot of weight” is neutral.
  prefs: []
  type: TYPE_NORMAL
- en: However, certain users (e.g., retailers selling basketballs) may deem heavy
    basketballs to be faulty products. Thus, they would need to ensure that their
    LLM gives the term “heavy” a negative connotation.
  prefs: []
  type: TYPE_NORMAL
- en: A simple way to do so would be to provide an example in the query itself.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt:**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '“Sentence: “This basketball is easy to carry”.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Answer: “The sentiment of this sentence is positive”.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Determine the sentiment of the sentence.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Sentence: “This basketball has a lot of weight”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Answer:”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**ChatGPT: ‘**The sentiment of the sentence “This basketball has a lot of weight”
    is negative. It expresses that the basketball is heavy, which is typically seen
    as a negative attribute when it comes to ease of use or play.’'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This time, ChatGPT deems the same sentence to be negative, since it uses the
    provided example as a reference.
  prefs: []
  type: TYPE_NORMAL
- en: Few-Shot Prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Users will not always be able to provide all of the context that the LLM will
    need to function properly with just one example.
  prefs: []
  type: TYPE_NORMAL
- en: For such cases, they can provide more context with few-shot prompting, which
    entails providing multiple examples in the user prompt.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous examples, we explored the use of ChatGPT to classify sentence
    sentiments as “positive” and “negative”.
  prefs: []
  type: TYPE_NORMAL
- en: However, what if we wanted a wider range of sentiment labels? For instance,
    we could use the labels “extremely positive”, “positive”, “somewhat positive”,
    “neutral”, “somewhat negative”, “negative”, and “extremely negative”.
  prefs: []
  type: TYPE_NORMAL
- en: To enable ChatGPT (or another LLM product) to incorporate specific grading criteria,
    few-shot prompting can be an effective strategy.
  prefs: []
  type: TYPE_NORMAL
- en: '**User Prompt:**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '“Sentence: This basketball fits in my bag.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Sentiment: somewhat positive.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Sentence: This basketball deflates after just one game!'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Sentiment: extremely negative.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Sentence: This basketball is orange.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Sentiment: neutral.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Sentence: This basketball’s feel is a bit better than I am used to.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sentiment:”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**ChatGPT:** “Somewhat Positive”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: By introducing ChatGPT to the new range of sentiment labels, ChatGPT can be
    “taught” to classify sentences with those same labels.
  prefs: []
  type: TYPE_NORMAL
- en: Why We Can’t Use Infinite Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, it’s tempting to consider including as many examples as possible
    in user prompts to guarantee a successful outcome.
  prefs: []
  type: TYPE_NORMAL
- en: After all, the more context, the better, right?
  prefs: []
  type: TYPE_NORMAL
- en: While that could be the case, LLMs are constrained by a limited number of useable
    tokens. Every LLM has a **context window**, the maximum number of tokens that
    the model can take as input.
  prefs: []
  type: TYPE_NORMAL
- en: The number of tokens afforded to users depends on the underlying model itself.
    Oftentimes, the specific figures can be found in the documentation.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the token limits of OpenAI’s GPT-3.5 models are detailed in the
    OpenAI API documentation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/66ded1f0ea457a88cc32d18d08553c9d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Token Limits for GPT-3.5 Models (Source: [Models — OpenAI API](https://platform.openai.com/docs/models/gpt-3-5))'
  prefs: []
  type: TYPE_NORMAL
- en: Users exhaust tokens when writing user prompts and, as a result, don’t have
    the bandwidth to bombard LLMs with countless examples.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, they have to figure out how much context an LLM will need to perform
    a given task adequately, and then determine how they can provide that context
    in as few tokens as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Key Takeaways
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/485eb33f1f57fd35c494dadcda69356b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Prateek Katyal](https://unsplash.com/@prateekkatyal?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Overall, users can yield satisfactory results from their LLM *without* providing
    additional context within their prompts since these models are already trained
    with high volumes of data.
  prefs: []
  type: TYPE_NORMAL
- en: However, for cases where prompts are complex or where responses have to meet
    unique criteria, users can benefit from including one or more examples in their
    prompts (i.e., one-shot prompting, few-shot prompting), provided that they do
    so with the allotted number of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Whether you are using a ready-made chatbot or are building your own custom-made
    LLM product, using zero-shot, one-shot, and few-shot prompting is a simple yet
    effective way to obtain adequate results.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs: []
  type: TYPE_NORMAL
