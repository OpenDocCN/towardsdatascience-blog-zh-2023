["```py\n{\"title\": \"April\", \"doc\": \"April is the fourth month of the year in the Julian and Gregorian calendars, and comes between March and May. It is one of four months to have 30 days.\", \"id\": 0}\n{\"title\": \"August\", \"doc\": \"August (Aug.) is the eighth month of the year in the Gregorian calendar, coming between July and September. It has 31 days. It is named after the Roman emperor Augustus Caesar.\", \"id\": 1}\n```", "```py\nwith open(input_file, \"r\") as f:\n    for line in f.readlines():\n        try: \n            example = json.loads(line.strip(\"\\n\"))\n            self.id2text[example[\"id\"]] = example.get(\"title\", \"\") + self.tokenizer.sep_token + example.get(\"doc\", \"\")\n        except Exception as _:\n            continue\n        if len(self.id2text) >= self.max_index_count: \n            break\n```", "```py\n @torch.no_grad()\n  def _get_batch_embedding(self, x: List[str]):\n      '''\n      Get embedding of a single batch\n      Parameters\n      ----------\n      x: List of text to encode \n      '''\n      batch_dict = self.tokenizer(x, max_length=512, padding=True, truncation=True, return_tensors='pt')\n      outputs = self.model(**batch_dict)\n      embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n      # normalize embeddings\n      return F.normalize(embeddings, p=2, dim=1)\n\n  def _get_all_embeddings(self):\n      '''\n      Get embedding of all data points\n      '''\n      data_loader = DataLoader(list(self.id2text.values()), batch_size=8)\n      embeddings = []\n      for bs in tqdm(data_loader):\n          embeddings += self._get_batch_embedding(bs)\n      embeddings = [e.tolist() for e in embeddings]\n      return embeddings\n```", "```py\nself.index = hnswlib.Index(space = 'cosine', dim=self.dim)\nself.index.init_index(max_elements =self.max_index_count, ef_construction = 200, M = 16)\nself.index.add_items(embeddings, ids)\nself.index.set_ef(50)\nprint(f\"Finish building ann index, took {time()-start:.2f}s\")\nself.index.save_index(self.index_file) # so we don't need to do everything once again\n```", "```py\n def get_nn(self, text: List[str], topk:int=1):\n      embeddings = self._get_batch_embedding(text)\n      labels, distances = self.index.knn_query(embeddings.detach().numpy(), k=topk)\n      # map id back to wiki passage\n      nb_texts = [self.map_id_to_text(label) for label in labels]\n      if self.debug:\n          for i in range(len(text)):\n              print(f\"Query={text[i]}, neighbor_id={labels[i]}, neighbor={nb_texts[i]}, distances={distances[i]}\")\n      return nb_texts, labels, distances\n```", "```py\nNO_WIKI_PROMPT_TEMPLATE = \"\"\"\nAnswer the following question:\n\nQuestion: ```", "```py\n\nYour response:\n\"\"\"\n\nHAVE_WIKI_PROMPT_TEMPLATE = \"\"\"\nYou will be provided with the following information:\n1\\. A question delimited with triple backticks.\n2\\. Addition information that is related to the question.\n\nPerform the following tasks:\n1\\. Understand the provided information.\n2\\. Use the provided information and answer the question.\n\nQuestion: ```", "```py\nAddition information: ```", "```py\n\nYour response:\n\"\"\"\n```", "```py\ndef get_chat_completion(\n    messages: dict,\n    model: str = \"gpt-3.5-turbo\",\n    max_retries: int = 3,\n    debug: bool = False\n):\n    '''\n    Gets a chat completion from the OpenAI API.\n\n    Parameters\n    ----------\n    messages : dict\n        input messages to use, e.g: {\"user\", \"what is the meaning of BERT\"}\n    model : str, optional\n        The OPEN AI model to use. Here we set default value to \"gpt-3.5-turbo\".\n    max_retries : int, optional\n        The maximum number of retries to use. Defaults to 3.\n    debug: bool\n        If we want to debug or not\n    '''\n    model_dict = {\"model\": model}\n    error_msg = None\n    error_type = None\n    if debug:\n        logging.warning(f\"Sending chat with message={messages}, model_dict={model_dict}...\")\n    for _ in range(max_retries):\n        try:\n          completion = openai.ChatCompletion.create(\n              temperature=0.0, messages=messages, **model_dict\n          )\n          return completion\n        except Exception as e:\n          error_msg = str(e)\n          error_type = type(e).__name__\n          sleep(3)\n    print(\n            f\"Could not obtain the completion after {max_retries} retries: `{error_type} ::\"\n            f\" {error_msg}`\" \n```"]