- en: How to improve translation for low-resource languages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-improve-translation-for-low-resource-languages-8b19dbe7eb6b](https://towardsdatascience.com/how-to-improve-translation-for-low-resource-languages-8b19dbe7eb6b)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Case of evaluation of African languages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@konkiewicz.m?source=post_page-----8b19dbe7eb6b--------------------------------)[![Magdalena
    Konkiewicz](../Images/907835d35b700811d993866e6524e33a.png)](https://medium.com/@konkiewicz.m?source=post_page-----8b19dbe7eb6b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8b19dbe7eb6b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8b19dbe7eb6b--------------------------------)
    [Magdalena Konkiewicz](https://medium.com/@konkiewicz.m?source=post_page-----8b19dbe7eb6b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8b19dbe7eb6b--------------------------------)
    ·7 min read·Jan 16, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b3f929a137bf81296822e97b27b4229.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Gerd Altmann](https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=110775)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=110775)
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**'
  prefs: []
  type: TYPE_NORMAL
- en: An improvement in the quality of Machine Translation systems in recent years
    has caused solutions and devices using them to become available to the everyday
    consumer. Many apps offer automatic translation of text, voice, or images, allowing
    a user to communicate or understand the language without knowing it.
  prefs: []
  type: TYPE_NORMAL
- en: This is however not the case for all languages. In fact, African languages which
    make up around a third of languages in the world do not benefit from those advances
    to the same extent. This is because of the few Natural Language Processing and
    Machine Translation resources dedicated to them over the past years.
  prefs: []
  type: TYPE_NORMAL
- en: The latest global conference on Machine Translation (WMT2022) hosted a special
    task prepared by researchers from Meta, Microsoft, and Toloka in order to tackle
    the problem. It resulted in the evaluation of 24 different African languages produced
    by 14 different Machine Translation systems using automatic scores as well as
    human evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: This article summarizes the main achievements of this project.
  prefs: []
  type: TYPE_NORMAL
- en: '**Challenge**'
  prefs: []
  type: TYPE_NORMAL
- en: The study has included 24 different African languages pictured below as well
    as English and French used for evaluation purposes. This allowed examining around
    100 different language directions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c5a1cf13b63051d8829273ce2026c30.png)'
  prefs: []
  type: TYPE_IMG
- en: 24 African languages that participated in the task — Photo by the Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Datasets**'
  prefs: []
  type: TYPE_NORMAL
- en: Public resources for African languages are very limited and participants were
    encouraged to contribute novel monolingual, bilingual, and multilingual datasets
    involving evaluated languages. Below you can see information about six new datasets
    that have been submitted.
  prefs: []
  type: TYPE_NORMAL
- en: '[**LAVA**](https://drive.google.com/drive/folders/179AkJ0P3fZMFS0rIyEBBDZ-WICs2wpWU)
    **— c**orpus with millions of parallel bilingual sentences that covers five African
    languages (afr, kin, lug, nya, saw).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**MAFAND-MT**](https://github.com/masakhane-io/lafand-mt) — corpus with a
    few thousand high-quality, human-translated parallel sentences for 21 African
    languages in the news domain that covered 14 languages that were evaluated in
    the challenge (amh, hau, ibo, kin, lug, luo, nya, sna, swh, tsn, wol, xho, yor,
    zul).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**KenTrans**](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi%3A10.7910%2FDVN%2F6N5V1K)
    — corpus with 12400 sentences translated between Swahili and 2 other Kenya Languages:
    Dholuo and Luhya.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Monolingual African languages from ParaCraw**](https://data.statmt.org/martin/)l
    — corpus with monolingual data that comes from the internet archives and targeted
    crawls. It covers 22 evaluated languages (afr, amh, fuv, hau, ibo, kam, lin, lug,
    luo, nso, nya, orm, sna, ssw, swh, tsn, tso, umb, wol, xho, yor, zul)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**SA Languages**](https://drive.google.com/drive/folders/1jYwpxEdRxqXlB7BSmE6JxDar61U91xfI)
    **—** corpora constructed using publicly available data from South African Government
    websites. It covers 4 evaluated languages (nso, tsn, xho, zul).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**WebCrawlAfrican**](https://github.com/pavanpankaj/Web-Crawl-African) **—**
    parallel corpora comprising of almost 700000 sentences for different African languages
    and English. It covers 15 languages involved in the challenge(afr, ling, ssw,
    amh, lug, tsn, nya, hau, orm, xho, ibo, tso, yor, swh, zul).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Systems and teams**'
  prefs: []
  type: TYPE_NORMAL
- en: Translation systems participating in the challenge were only allowed to use
    for training the resources above or two other well-known corpora. There were fourteen
    system submissions from eight teams.
  prefs: []
  type: TYPE_NORMAL
- en: '**CAIR ANVITA —** The team submitted an English-centric multilingual transformer
    model for 24 African languages. The authors applied several heuristics to filter
    the data and showed that using a larger transformer model performs much better
    than a smaller version.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cape Town** — Their system was focused on eight South and South-East African
    languages. The authors trained a multilingual NMT system and explored the usage
    of synthetic and sampling techniques to balance the corpora.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GMU** — The team focused on fine-tuning pre-trained multilingual DeltaLM
    in 26 languages. Fine-tuning was based on language and language family.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IIAI DenTra** — They submitted a model that combines a traditional translation
    loss with self-supervised tasks and uses unlabeled monolingual data. The model
    performs denoising tasks with both translation and back-translation and covers
    24 languages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Masakhane —** They submitted a model based on M2M-100 by fine-tuning it on
    positive samples (clean data) versus negative samples coming from automatically
    aligned data. The model showed significant improvements from using the filtered
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tencent Borderline —** They submitted **a** large transformer model (1.02B
    params) augmented with data for zero-shot pairs through tagged back-translation
    and self-translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bytedance VolcTrans —** Their system used parallel and monolingual data that
    was additionally cleaned using heuristics (correcting punctuation mismatches,
    removing overly short/long sentences, or noisy sentences). Additionally, the data
    is augmented with back-translation coming from English and French.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SRPH-DAI —** They submitted a model based on mT5 with additional adapters
    fine-tuned to each translation task, and then merged using adapter fusion. The
    model used data filtered using a set of heuristics and did not use any other data-augmentation
    techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Systems Evaluation**'
  prefs: []
  type: TYPE_NORMAL
- en: The systems were judged using [BLUE](https://en.wikipedia.org/wiki/BLEU) , C[hrF](https://aclanthology.org/W15-3049/),
    and human evaluation. The first two are well-known automatic metrics used in Machine
    Translation. Human evaluation, on the other hand, has not been used so extensively
    in previous studies due to resource limitations and annotators' availability.
    Therefore it is an important shift in this area of research.
  prefs: []
  type: TYPE_NORMAL
- en: In this study, human evaluation was performed using crowdsourcing and [Toloka](https://toloka.ai/).
    It involved carefully chosen annotators judging the quality of the translation
    on a scale from 0 to 100 as seen in the screenshot below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d867f8410ebe9498454fbb82f4f159f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Interface with an annotated task comprising the source sentence and two translations
    randomly chosen from the outputs of the submitted models for the respective language
    pair (Afrikaans — English in the screenshot).— photo by the Author
  prefs: []
  type: TYPE_NORMAL
- en: Crowdsourcing can be an effective way to source the annotators when evaluating
    machine translation systems but it requires careful selection of the participants
    and rigid quality control mechanisms. Annotators’ fluency in both source and target
    languages needs to be ensured and in this study, it was done via geographical
    selection and language tests. Additionally, the same task was given to several
    annotators, and then the results were aggregated in order to gain more confidence
    about the score. The exact pipeline used in this challenge is pictured below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d98d59fc725d8433006a5afbeb336c29.png)'
  prefs: []
  type: TYPE_IMG
- en: Toloka Crowd Machine Translation Evaluation Pipeline and Quality Control — photo
    by the Author
  prefs: []
  type: TYPE_NORMAL
- en: An interesting finding about human evaluation was that it was highly correlated
    to other metrics such as BLUE and ChrF which proves it to be a valid method of
    scoring this type of system.
  prefs: []
  type: TYPE_NORMAL
- en: '**Results**'
  prefs: []
  type: TYPE_NORMAL
- en: The best-performing system was Tencent Borderline and it outperformed other
    systems in all evaluation metrics including BLUE, ChrF, and human evaluation.
    It was closely followed by the GMU system which placed second. The screenshot
    below shows detailed performance results for each system.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ccd2d6feef886bbc7fbde9de5f0612a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Results of system evaluation for African languages from WMT2022 as shown in
    [research paper](https://www.statmt.org/wmt22/pdf/2022.wmt-1.72.pdf) — Screenshot
    by the Author
  prefs: []
  type: TYPE_NORMAL
- en: It was clear that the data used to train the systems had an important impact
    on the performance. Systems using more data, using data augmentation techniques,
    and extensive heuristics to clean data performed better across different language
    pairs.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, there was a large progress in comparison to the previous year's
    results. The average system has seen an increase of about 7.5 BLEU points across
    72 language pairs (it went from 15.09 to 22.60). It is important to mention that
    the improvements were seen with English-centric pairs as well as with pairs in
    african-to-african languages where the latter one is specifically challenging.
    Below you can see the top language pairs sorted by increase in improvement.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8eff3bf47f7fe056b81ec6a72c5a629c.png)'
  prefs: []
  type: TYPE_IMG
- en: The top-10 language pairs with the largest improvement over last year’s best
    result — Screenshot by the Author
  prefs: []
  type: TYPE_NORMAL
- en: Another important observation is that South African languages seem to be easier
    to translate (average ∼29 BLEU) while languages from West Africa, are more challenging
    (average ∼15 BLEU). Also, translating into African languages is harder than translating
    from African languages.
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, you have learned about the latest advancement in Machine Translation
    for African languages as presented in WMT2022.
  prefs: []
  type: TYPE_NORMAL
- en: There were around 100 language pairs that have been evaluated and the translation
    quality have shown some significant improvements in comparison to previous years.
    Some of the novelties in the field have been using extensive human evaluation
    to score the system and including data track where participants submitted new
    language corpora.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in knowing more details you should read the following
    [paper](https://www.statmt.org/wmt22/pdf/2022.wmt-1.72.pdf) that has been dedicated
    to this task.
  prefs: []
  type: TYPE_NORMAL
- en: '*PS: I am writing articles that explain basic Data Science concepts in a simple
    and comprehensible manner on Medium and* [***aboutdatablog.com***](https://www.aboutdatablog.com/)*.
    You can subscribe to my* [***email list***](https://medium.com/subscribe/@konkiewicz.m)
    *to get notified every time I write a new article. And if you are not a Medium
    member yet you can join* [***here***](https://medium.com/@konkiewicz.m/membership)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Below there are some other posts you may enjoy:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/detecting-and-fixing-data-drift-in-computer-vision-8d0853af1246?source=post_page-----8b19dbe7eb6b--------------------------------)
    [## Detecting and fixing data drift in Computer Vision'
  prefs: []
  type: TYPE_NORMAL
- en: Practical case study with code that you can run
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/detecting-and-fixing-data-drift-in-computer-vision-8d0853af1246?source=post_page-----8b19dbe7eb6b--------------------------------)
    [](/top-8-magic-commands-in-jupyter-notebook-c1582e813560?source=post_page-----8b19dbe7eb6b--------------------------------)
    [## Top 8 magic commands in Jupyter Notebook
  prefs: []
  type: TYPE_NORMAL
- en: Boost your productivity by learning the most useful commands
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/top-8-magic-commands-in-jupyter-notebook-c1582e813560?source=post_page-----8b19dbe7eb6b--------------------------------)
    [](/how-to-do-data-labeling-versioning-and-management-for-ml-ba0345e7988?source=post_page-----8b19dbe7eb6b--------------------------------)
    [## How to do data labeling, versioning, and management for ML
  prefs: []
  type: TYPE_NORMAL
- en: A case study of enriching food dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-do-data-labeling-versioning-and-management-for-ml-ba0345e7988?source=post_page-----8b19dbe7eb6b--------------------------------)
    [](/jupyter-notebook-autocompletion-f291008c66c?source=post_page-----8b19dbe7eb6b--------------------------------)
    [## Jupyter notebook autocompletion
  prefs: []
  type: TYPE_NORMAL
- en: The best productivity tool for Data Scientist you should be using if you are
    not doing it yet…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/jupyter-notebook-autocompletion-f291008c66c?source=post_page-----8b19dbe7eb6b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
