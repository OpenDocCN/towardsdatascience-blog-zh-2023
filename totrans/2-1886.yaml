- en: Solve a mystery box like a data scientist
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/solve-a-mystery-box-like-a-data-scientist-f9ee9570ba52](https://towardsdatascience.com/solve-a-mystery-box-like-a-data-scientist-f9ee9570ba52)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Get data, train ViT, minimize problem; and way too overkill
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dennisbakhuis.medium.com/?source=post_page-----f9ee9570ba52--------------------------------)[![Dennis
    Bakhuis](../Images/4dc6dca031cdedbb044a1d0a6b142186.png)](https://dennisbakhuis.medium.com/?source=post_page-----f9ee9570ba52--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f9ee9570ba52--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f9ee9570ba52--------------------------------)
    [Dennis Bakhuis](https://dennisbakhuis.medium.com/?source=post_page-----f9ee9570ba52--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f9ee9570ba52--------------------------------)
    ¬∑17 min read¬∑Jan 13, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9a4946ac13a6d2ed63ff0370c95ba55.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A mystery box, the process of collecting data, and eventually an
    open lock.'
  prefs: []
  type: TYPE_NORMAL
- en: What happens when a data scientist gets a riddle in form of a box? Of course
    he will (try) approach it as a data problem. In this article I will describe the
    whole process, and to be honest, it was not as easy as I thought. As with many
    problems, you can get completely lost and only by talking to a couple of friends,
    I got back on track again.
  prefs: []
  type: TYPE_NORMAL
- en: As a data scientist, I like to approach this problem in a data manner. I realize
    that this method is far from the most obvious solution. But it was a very fun
    endeavor. Collecting too much data, train a transformer model to extract values
    from a video, and eventually use a `minimizer` to find the solution. This article
    is a summary of this (mostly) fun journey!
  prefs: []
  type: TYPE_NORMAL
- en: 'I have divided this article in a couple of (for me) logical steps. Feel free
    to skip to parts you like:'
  prefs: []
  type: TYPE_NORMAL
- en: What is this ‚Äúmystery box‚Äù you‚Äôre talking about
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A formal problem description
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collecting the required data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Processing the data goodness (label, train, inference)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analyze the dataset and find the goal
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Going to the location
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*All images in this article have been taken or are generated by me unless stated
    otherwise in the separate captions (which is none in this article).*'
  prefs: []
  type: TYPE_NORMAL
- en: '[All code](https://github.com/dennisbakhuis/mystery_box) for this project is
    shared in Notebooks and is available on my [Github account](https://github.com/dennisbakhuis).
    If you have any comments or questions, I like to hear them through [LinkedIn](https://linkedin.com/in/dennisbakhuis).'
  prefs: []
  type: TYPE_NORMAL
- en: What is this ‚Äúmystery box‚Äù you‚Äôre talking about?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For my birthday my friend Sander gave me a mysterious box and with the big grin
    on his face, this would keep me busy for a while.
  prefs: []
  type: TYPE_NORMAL
- en: The mystery box immediately looked very intriguing (see figure 1). It was clear
    that the box was created by Sander himself as it had the very distinct fused deposition
    modeling (FDM) printing pattern. The front showed a common LCD screen and a red
    button. On the left side, a relatively large padlock was visible. This is one
    of these code padlocks that required a four digit code to open. The padlock keeps
    a sliding lid from opening at the bottom of the box. On the right side is a small
    excess compartment that holds a 9 volt battery. Pretty smart to have the battery
    outside so that it can be exchanged when the power is getting low.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/64fec328a8b6c2848e19b61816d78386.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: This is the mystery box: a 3D printed device with a screen, a button,
    and a padlock.'
  prefs: []
  type: TYPE_NORMAL
- en: From a creators point of view, the mystery box has a quite interesting design.
    The top side (as seen in figure 2) was used as base during printing as this side
    is much coarser than the others. This makes the excess battery holder also printable
    without support. Still, I am not completely sure how the hole for the LCD screen
    is created. The most simple method would be to print light supports which are
    cut away. For the ledges that are used for the padlock, the remains of the printing
    support are still visible. All with all, a nice project to keep your printer running
    for at least 8 hours.
  prefs: []
  type: TYPE_NORMAL
- en: 'The red button is a kind of toggle press button. When pressing you hear it
    latch and the LCD screen turns on. After a few seconds the box greets you with
    ‚ÄúHi!!!‚Äù a classical gag at the [Physics of Fluids research group](https://pof.tnw.utwente.nl/).
    The next couple of messages shown on the screen are in Dutch. For an overview
    of the screens see figure 3\. Here are some quick translations:'
  prefs: []
  type: TYPE_NORMAL
- en: Hi!!! -> <low pitched voice> Hi </low pitched voice>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Van harte gefeliciteerd! -> Sincere gratulations!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vind de twee punten en ga -> find the two coordinates and go
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: naar het midden van de punten‚Ä¶ -> to the center of the coordinates‚Ä¶
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zoekt gps‚Ä¶ Een moment‚Ä¶ -> Searching gps‚Ä¶ One moment‚Ä¶
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Afstand(p1)-Afstand(p2)=1048m -> Distance(p1)-Distance(p2)=1048m
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/a85c19a920176ede8eda0d9d89fa108d.png)![](../Images/10368af95b1b1e248cbbd335ed33d8da.png)![](../Images/44ad73d3fed17db0650ba714b8e1f176.png)![](../Images/0cd976e47b101c9f15fcead048fbe269.png)![](../Images/eefa6f7b926a318d23cc38f9e52f75eb.png)![](../Images/acb403272a2e6c5765a23f65180e27e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Turning on the device shows various messages in Dutch, including
    a couple of hints.'
  prefs: []
  type: TYPE_NORMAL
- en: After the two hint screens, the system is busy with searching for a GPS signal.
    This process, the so called ‚Äòtime to first fix‚Äô (TTFF) can be lengthy as the device
    always has to do a so called ‚Äúcold boot‚Äù. This can take up to five minutes according
    to the standard.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, after a GPS fix, the box comes into its main operation screen that
    shows us its only output: the distance to point 1 minus the distance to point
    2 which is a integer value in meters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sander also gave some additional clarification of the unit. The unit outputs
    a value which is the difference between two distances: d1 and d2\. These distances
    are the distance between the current location of the box and two unknown coordinates.
    The goal is to find the coordinate exactly between these two unknown coordinates.'
  prefs: []
  type: TYPE_NORMAL
- en: This all sounds easy but lets have a look at a more formal description.
  prefs: []
  type: TYPE_NORMAL
- en: A formal problem description
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of the puzzle is to bring it to a specific location. If this location
    is correct, the box will show the code for the padlock. What makes this problem
    challenging is that the mystery box does not show the distance to that specific
    location.
  prefs: []
  type: TYPE_NORMAL
- en: The goal location *G* is exactly in the center of a line between two for us
    unknown coordinates *p1* and *p2*. The distances *d1* and *d2* are the distance
    between the current box location *B* and the coordinates *p1* and *p2*. What the
    box outputs on the screen is the difference between *d1* and *d2* which we will
    call *A*. Figure 4 shows a simplified schematic of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a00e990bae292802ad00d1ee586501e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: A simple problem overview. The goal G is exactly between unknown
    coordinates p1 and p2\. The only information the box returns is A which is the
    difference between distances d1 and d2\. The distances d1 and d2 are respectively
    the distance between the current box location B and coordinates p1 and p2 respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: The schematic in figure 4 shows that we have a vector problem. If we would assume
    a 2d problem space, we need two values for each coordinate to uniquely identify
    a location in the problem space.
  prefs: []
  type: TYPE_NORMAL
- en: There exists a point on which the difference *A* is zero. This is when the distance
    *d1* is equal to *d2*. In a 2d space this results in a line that is perpendicular
    to the line between *p1* and *p2* (see Figure 5). So technically, if we find two
    different locations on which A is equal to zero, we could connect those points
    and know that the goal is on that line.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e1766d9c18cb777a4a72e1e4eda3fd9e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: When A is equal to zero, d1 and d2 are equal. There exists a line
    perpendicular to the goal coordinate for which A is equal to zero. We could find
    this line if we find two different locations for which A is zero.'
  prefs: []
  type: TYPE_NORMAL
- en: To solve the equation we could try to solve a system of equations (four unknowns
    requires four equations) but our problem is non-linear. This makes some nasty
    equations when trying to isolate terms in the equation (Equation 1). In the equation
    *x* and *y* are the two dimensions of the current box location coordinate. Maybe
    not the most elegant but the equation can be solved numerically by minimizing
    an error function. This is exactly what we are doing in machine learning with
    gradient descent and the more recent forward-forward method.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/12531c0988a6e241316ff91965638518.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation 1: Difference of distances equation for the simplified case.'
  prefs: []
  type: TYPE_NORMAL
- en: Until now we have described the problem that is only valid within the flat earth
    society. We used straight lines for our distances and this would mean that we
    need to dig tunnels in our spherical world. Instead of calculating distances on
    a plane, we need to calculate distances on the surface of a sphere. I have updated
    the problem schematic in Figure 6.
  prefs: []
  type: TYPE_NORMAL
- en: Until now we have described the problem that is only valid within the flat earth
    society.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/3dd6d149f8d5390e3d7e37297a44954a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: We are not living a a flat earth. The problem is actually on the
    surface of a sphere.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculating the distance between two points of the surface of a sphere is far
    from trivial. This involves applying [the great circle distance formula](https://en.wikipedia.org/wiki/Great-circle_distance),
    which looks rather messy. Luckily, the people invented a new trigonometric function
    to make it look much more elegant: [the Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula).
    If you like a good read on the problem here is a great article on [the underground
    mathematics](https://undergroundmathematics.org/trigonometry-compound-angles/the-great-circle-distance).'
  prefs: []
  type: TYPE_NORMAL
- en: Using the Haversine formula to calculate a single distance is quite doable,
    however, our mystery box outputs the difference between two distances *A*. This
    makes the final equation to solve extremely nasty and I think that numerically
    solving the equation makes the most sense.
  prefs: []
  type: TYPE_NORMAL
- en: But before we can solve anything, we need acquire data. In the next section
    I‚Äôll the describe elaborate setup.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting the required data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The biggest problem when trying to approach a physical riddle as a data problem
    is that there is most probably no data. This means that we need to collect our
    own data. From my experience as a physicist, collecting data is no easy task.
    A lot of things can (and probably will) go wrong. To be honest, I have probably
    biked more than 80km for this project.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of things can (and probably will) go wrong.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There are multiple ways we can approach the data collection. First, we could
    just move around by bike üö¥ (we are in üá≥üá± so yes we üö¥ üòÉ), and stop to write down
    the current latitude and longitude and the output of the box *A*. Technically,
    a few points would suffice but where is the fun in that.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of doing all this manual work, why not do some classic *over-engineering*
    and get a video of the mystery box output and match that to the data of a GPS
    logger. To do this, we need to build some kind of contraption that fixes a camera
    in front of the LCD screen of the box. The most straight forward way I could think
    of is strapping my mobile phone to some sort of container that has the minimum
    distance required to get a sharp image with the camera. A plastic box just had
    the right dimensions so I could put the box in the box and create some box-ception
    (we have to go deeper). The camera need to be sturdy such that the movement between
    camera and LCD screen was at a minimum. This calls for some serious duct-tape.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have a solution to record the box output as a movie. The regular camera
    app of my phone was not great so I used an app called [HD Camera](https://play.google.com/store/apps/details?id=photo.android.hd.camera&hl=en&gl=US).
    To get the current location we need a GPS logger. For this I downloaded [GPS logger](https://play.google.com/store/apps/details?id=eu.basicairdata.graziano.gpslogger&hl=en&gl=US)
    from the Playstore. With these two apps install I am ready to collect some data.
    The whole setup is shown in Figure 7.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72f886aa8131325f28f5df6cdfaa7214.png)![](../Images/74de9ef9a4e9648aa7f541fd56b5880b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Setup for collecting data. The mystery box is box-ceptioned into
    another box. On top of the box a mobile phone is strapped such that the camera
    has clear view of the LCD screen. The mobile phone as two apps active, a camera
    app and a GPS logger. The GPS logger is in floating app mode such that it keeps
    being active and not turned off by the phones power saving settings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I have already mentioned the many things that can go wrong when doing experiments.
    Here are my confessions of my many mistakes:'
  prefs: []
  type: TYPE_NORMAL
- en: During my first ride the battery died. When still recording while the phone
    dies, you lose the recording. This made me add a descent power bank to the setup.
    ( üö¥ ~12 km).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next ride my storage was full. I did not see the error message until late in
    the ride. ( üö¥ ~10 km).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During another ride, I did not liked the lighting due to the sun. ( üö¥ ~9 km).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now I liked the footage but somehow I did not have any GPS data. ( üö¥ ~9 km).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Again, no GPS data. but I suspect some battery saving issue. ( üö¥ ~9 km).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I made the GPS logger foreground and HD Camera background. Now I had GPS data
    but somehow, the camera App stopped. ( üö¥ ~9 km).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After making the GPS logger app a floating app (some multi-task magic) I got
    both, GPS and image data. ( üö¥ ~9 km).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I deleted my repository, including all recordings. ( üö¥ ~9 km).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/1ac68dfbce9f423eef306d7a331e9224.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The setup in my bike crate.'
  prefs: []
  type: TYPE_NORMAL
- en: So in total I biked almost 80 km to get useful footage accompanied by GPS data.
    Next, we are going to investigate this two data sources and combine them such
    that we have *A = f(x,y)*.
  prefs: []
  type: TYPE_NORMAL
- en: Processing the data goodness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have two data sources, we need to massage them into something useful.
    We will first transform the GPS data into our default DataFrame. Next, we will
    convert the video of 4.7GB also into a table of *A* values. This will be some
    decent data reduction üòÉ.
  prefs: []
  type: TYPE_NORMAL
- en: Converting GPS logger data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While the camera on the phone recorded the changing values of the LCD screen,
    the GPS logger app recorded the current latitude and longitude values. These are
    stored with a recording rate of 1Hz in a so called GPX file, for which a nifty
    library in Python exists. It is conveniently called `gpxpy` and after a pip install
    the data can be loaded in a bliss:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 1: Importing the GPS data and creating a DataFrame. Easy as pie!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/958214377ec9021b3ac2acb9aa876e7a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Output 1: Sample of the newly created dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have a DataFrame with 2133 measurements that include latitude, longitude,
    elevation, and time. To check if these coordinates make any sense we could simply
    create a parametric plot using Matplotlib. However, there is another great library
    in Python specifically for maps called `folium`. Lets visualize what I have biked:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 2: visualizing the GPS coordinates using Folium.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/354671e85a07cc33269963801d004edc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Output 2: A map of the beautiful Enschede and the exact route I biked.'
  prefs: []
  type: TYPE_NORMAL
- en: In the beautiful Folium map where we use ‚Äútamen watercolor‚Äù tiles we see exactly
    the ring that I biked to collect the data. Before we store the DataFrame to disk,
    we need fix the timezone in the time column. Currently, the timezone is set to
    ‚Äúz‚Äù and I am not sure what that is. To remove any complications, lets localize
    the time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 3: Correct time and store DataFrame into a Parquet file.'
  prefs: []
  type: TYPE_NORMAL
- en: The GPS data is ready to be used. Next, we need to process the video footage
    into something useful.
  prefs: []
  type: TYPE_NORMAL
- en: Creating training set for image detector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have a video that shows all output of the mystery box during the bike ride.
    We need to extract the *A* value from each frame and its corresponding time. Using
    this time, we can link the *A* value to its GPS location. To extract the *A* value,
    we will train a [DONUT based model](https://arxiv.org/abs/2111.15664). This is
    an end-to-end based model and does not need any optical character recognition
    (OCR). To train a model we need a labeled dataset. First, lets inspect the video
    data using [OpenCV](https://pypi.org/project/opencv-python/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 4: Import video as stream and check frame rate.'
  prefs: []
  type: TYPE_NORMAL
- en: The video was record with a frame rate of 30 frames per second. The duration
    of the video is about 35 minutes so this would mean more than 63k frames. This
    is a bit much, especially due to most GPS sensors for Arduino have a update frequency
    > 1Hz. Therefore, lets only select one frame each second and put them into a regular
    list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 5: Extract frames from video at a frame rate of 1 FPS.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/63438b1f42ff406aeab82accb94a088e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Output 3: a single frame from the video (left) and the final pre-processed
    frame (right).'
  prefs: []
  type: TYPE_NORMAL
- en: We have extracted 2119 frames which is already quite a reduction of the original
    63k we started with. When looking at the frame in Output 3 (left) we see that
    a lot of pixel in the frame are not very interesting to us. In our preprocessing
    we will crop the image such that we are only left with the A value. We will also
    reduce the image to black and white which might help with contrast problems later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 6: preprocess images such that we isolate the A value.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we only need to save the list. This is a relatively easy task to parallelize
    as it is not depended of the previous input, therefore, we use `[joblib](https://joblib.readthedocs.io/en/latest/parallel.html)`
    for this step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 7: Store the images, parallel of course!'
  prefs: []
  type: TYPE_NORMAL
- en: We now have a dataset, but it is not yet labeled. To label, I have created a
    tool to label data in Jupyter Lab (or Notebook) which is called `[Pigeon-XT](https://github.com/dennisbakhuis/pigeonXT)`.
    It is the extended version of the Pigeon labeling tool created by Anastasis Germanidis.
    We will label 250 examples, which took me about 15 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 8: Label the data comfortably in our Jupyter Notebook.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we have a labeled dataset. It is small, about 10% of the complete dataset,
    but we should be able to train a model that we can use to fill in the missing
    portion of the set. To make our life easier we can use the `[datasets](https://github.com/huggingface/datasets)`
    library from ü§ó Huggingface. We can import our data with minor adjustments as an
    Imagefolder dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 9: Create a Huggingface datasets dataset object.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3727d04be9fa2c29c6e6abe8ac2f6eb5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Output 4: the dataset object makes handling the data very easy. Maybe it would
    even be better doing the transform using its .map() function.'
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, we need to save the dataset. What is nice, is to share your
    dataset on the Huggingface hub. This is extremely easy using the `[huggingface_hub](https://github.com/huggingface/huggingface_hub)`
    package. For this you also need an account on the hub. The amazing thing about
    sharing on the hub is that you can now download this dataset without doing all
    the processing I did above.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 10: Save the dataset and push the it to the Huggingface hub.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we have everything to train a model to infere the missing labels. Lets do
    some training üî•üî•üî•!
  prefs: []
  type: TYPE_NORMAL
- en: Training a üç© DONUT model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can finally start with training our model. We will be training a [DONUT](https://arxiv.org/abs/2111.15664)
    based model created by [Naver AI Lab](https://naverlabs.com/). DONUT is a true
    end-to-end model. You input an image and you will get a JSON object with key-value
    pairs out. The most amazing part is that it can do this without OCR. Previous
    state of the art methods such as [LayoutLM](https://arxiv.org/abs/2204.08387)
    use a two-step approach: first use OCR to extract all text and their locations
    and second, input an image and text data in a model to extract information.'
  prefs: []
  type: TYPE_NORMAL
- en: The problem with extracting information from documents is that a lot of information
    is encoded in the layout. The human brain understands the relation between data
    when it is presented as a header, in a table, or as a caption. Using OCR methods,
    this information is lost. Methods like LayoutLM use the image to bring back the
    layout information. DONUT can do this in one go and has OCR build in. From what
    I have seen it works quite good but is very sensitive to changes in documents.
    When a document has minor layout changes, but still the same information to extract,
    DONUT fails quite easily.
  prefs: []
  type: TYPE_NORMAL
- en: Our images are not really documents, but they have very clear text and are from
    a layout point of view very constant. Therefore, DONUT would be a great (and overkill)
    method to extract the *A* value. I followed the same procedure as CloveAI team
    did in their [repository](https://github.com/clovaai/donut). [Philipp Schmid](https://www.linkedin.com/in/philipp-schmid-a6a2bb196/)
    wrote a [great article](https://www.philschmid.de/fine-tuning-donut) on how to
    train your own DONUT type model. First we need to prepare our dataset for the
    specific DONUT inputs of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 11: converting the JSON objects to tokens DONUT expects.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that the data is in the form expected by DONUT we need to tokenize the data.
    This is a step common to transformer language models such as GPT<n> and BERT but
    now also used for images in Vision Transformers (ViT).
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 12: tokenize the data for input into the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we have everything ready to do some actual training üî•. Nowadays, this is
    incredibly easy as you do not have to write your training loop anymore. While
    I first was a bit sad as it felt quite cool to write the loop, it was quite repetitive.
    Huggingface gives you the trainer object and it is just perfect for the job.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 13: the complete code for the training loop has simplified a lot!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01876e94ee685cff3e3f7af27f891153.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Output 5: training in progress.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After evaluation of the test dataset we get an evaluation loss of 0.007 which
    is pretty descent. From our 38 test examples 37 were detected correctly. The one
    that is incorrect was missing one digit: 059 instead of 1059\. 37/38 makes an
    accuracy of 97% which is probably enough to label the complete set. Before we
    start inferring the missing values, lets save the model and also push it to the
    hub.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 14: save the model and push it to the hub.'
  prefs: []
  type: TYPE_NORMAL
- en: Now lets do some inferring on the missing labels.
  prefs: []
  type: TYPE_NORMAL
- en: Use model to infer missing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally we can use our model to infere the labels. The images are already prepared
    so we can simply use those. The time of each frame is encoded in its filename
    in seconds. There will be a small mismatch as the frame rate is not exactly 30
    frames per second but the difference will be small. Lets first create a function
    to do the inference for us:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 15: code for inferring an image.'
  prefs: []
  type: TYPE_NORMAL
- en: We could now do inference on each image one by one, however, that would take
    about an hour. Lets do this in parallel but this is far from trivial. For simple
    things, `joblib` is great. However, when using models, serializing large models
    for each iteration makes it slower then doing them just one-by-one. To solve this
    problem I wrote `[tqdm_batch](https://github.com/dennisbakhuis/tqdm_batch)` a
    while ago.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel jobs are not trivial. Read my [article](/parallel-batch-processing-in-python-8dcce607d226)üèÉüèªüí®!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Code 16: parallelize the inference step and divide the work between four workers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/427bcf6a9366233992535bfc7428cb41.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Output 6: running 4 workers in parallel using tqdm_batch.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our A values from the box, we can build our final dataset and
    combine it with GPS data. As always, the prediction was not perfect and we have
    to do some massaging. As a data scientist you actual are an data masseuse.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 15: basic cleaning of or dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Almost there, the only problem are the wrongly detected A values. These can
    be filtered using a rolling filter. We will remove values that have a greater
    difference than 5 meter (arbitrary small number) from the moving average and interpolate
    them with the values left.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 16: filtering the noisy data using a moving average filter and an interpolate.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7371184d7dcaeb9a64007b84af971b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Output 7: final result after applying the filter. Pretty neat!'
  prefs: []
  type: TYPE_NORMAL
- en: This filtering looks pretty neat. Now we are ready to combine this dataset with
    our previously recorded GPS data.
  prefs: []
  type: TYPE_NORMAL
- en: Combining the image and GPS data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Combining the A value to the GPS data should not be much of a problem. For each
    row in the GPS dataset we will look for the closest match in time and use the
    A value of that row.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 17: Adding the *A* value to the gps dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: This is it! So much effort which was probably never really needed. But we used
    machine learning and that is the only thing that matters!
  prefs: []
  type: TYPE_NORMAL
- en: we used machine learning and that is the only thing that matters!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Next, we will use this dataset and try to find the goal coordinate.
  prefs: []
  type: TYPE_NORMAL
- en: Analyze the dataset and find the goal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As previously discussed, we will try to find the two unknown locations *p1*
    and *p2*. If we know these two points, we will also know the goals location which
    should be exactly between those points. To find the locations, we will use the
    `[minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize)`
    form `[Scipy](https://scipy.org/)`. While I could write my own Haversine function
    I will just use the `[haversine](https://github.com/mapado/haversine)` package.
    To use `minimize` we need to write an error function. This function will calculate
    the difference between the known *A* and the calculated A for all our values in
    the dataset and sum the absolute differences. The minimize function should minimize
    this error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 18: lets minimize this shit.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a1c1bd318ca766a86d5c51dd0240e0b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Output 8: The final solution (green) is exactly between points p1 and p2 (blue).
    The goal is also on the perpendicular line of my found zero points.'
  prefs: []
  type: TYPE_NORMAL
- en: This is it, I have found the location. The goal is exactly between points p1
    and p2\. This point also connects to a perpendicular line between my previously
    found zero points. But the biggest hint is that the goal location is a [BBQ restaurant](https://marcook.nl/).
    Sander and I are very fond of burgers and this place has a great selection of
    those.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0987263246d81a45b333f840195534f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Output 9: The difference between my found coordinate and the actual goal is
    93 meters.'
  prefs: []
  type: TYPE_NORMAL
- en: The deviation from the actual point and the one I found is 93 meters. This could
    have many reasons. First, the extracted A value only has round numbers. While
    this has an effect, I suspect that this is only a minor effect which should negligible
    when averaging over a larger dataset. Of course, the filtering (smoothing) has
    an effect. Maybe there is some noise added on purpose, not sure if I could have
    noticed that. Anyhow, I am pretty happy with the result.
  prefs: []
  type: TYPE_NORMAL
- en: Going to the location
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It took quite some time but now I am finally certain where to location is. I
    took the car and drove to Marcook, a BBQ restaurant just outside of Enschede.
    If I would turn on the mystery box in that location it should show me the code
    to unlock its mysteries.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9aed31317a2c5f446f5c0642f045d5f7.png)![](../Images/212bb3e05852d0b2e28b7cc236b3b814.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: the mystery box unlocks all its mysteries.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Indeed, after waiting a couple of minutes to get a GPS fix the box almost instantly
    responded with ‚Äòproficiat‚Äô. Congratulations for making it this far. The next screen
    of the device shows the code: 7631.'
  prefs: []
  type: TYPE_NORMAL
- en: To be honest, this was not an easy challenge. But it was a fun journey and in
    the end we are gonna eat some burgers. Can hardly get any better.
  prefs: []
  type: TYPE_NORMAL
- en: All code for this project is on [Github](https://github.com/dennisbakhuis/mystery_box).
    Most of the smaller datasets are included in the Git repository however the larger
    video is shared through my Dropbox. The dataset used for the ViT training, the
    DONUT processor, and the DONUT model are also shared on the ü§ó [Huggingface hub](https://huggingface.co/bakhuisdennis).
    If you have any questions, feel free to contact me on [LinkedIn](https://linkedin.com/in/dennisbakhuis).
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, I want to thank üôè *Sander* for the fun experience. I am
    sorry I took so long, but I guess I over-killed it a bit. It sure was fun! Burgers?
  prefs: []
  type: TYPE_NORMAL
