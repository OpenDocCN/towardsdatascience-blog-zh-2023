- en: Getting Started with Multimodality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/getting-started-with-multimodality-eab5f6453080](https://towardsdatascience.com/getting-started-with-multimodality-eab5f6453080)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/ae8e7dbea616220acfe1963d16441f60.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created with Microsoft Designer
  prefs: []
  type: TYPE_NORMAL
- en: Understanding vision capabilities of Large Multimodal Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://valentinaalto.medium.com/?source=post_page-----eab5f6453080--------------------------------)[![Valentina
    Alto](../Images/888b8aa17759d8dd5332d8fd4653cf05.png)](https://valentinaalto.medium.com/?source=post_page-----eab5f6453080--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eab5f6453080--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----eab5f6453080--------------------------------)
    [Valentina Alto](https://valentinaalto.medium.com/?source=post_page-----eab5f6453080--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----eab5f6453080--------------------------------)
    ·9 min read·Dec 27, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: The recent advances in Generative AI have enabled the development of Large Multimodal
    Models (LMMs) that can process and generate different types of data, such as text,
    images, audio, and video.
  prefs: []
  type: TYPE_NORMAL
- en: LMMs share with “standard” Large Language Models (LLMs) the capability of generalization
    and adaptation typical of Large Foundation Models. However, LMMs are capable of
    processing data that goes beyond text, including images, audio, and video.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most prominent examples of large multimodal models is GPT4V(ision),
    the latest iteration of the Generative Pre-trained Transformer (GPT) family. GPT-4
    can perform various tasks that require both natural language understanding and
    computer vision, such as image captioning, visual question answering, text-to-image
    synthesis, and image-to-text translation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The GPT4V (along with its newer version, the GPT-4-turbo vision), has proved
    extraordinary capabilities, including:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematical reasoning over numerical problems:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/22d6be0bb9790388f39c6d37bdf13ce6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Generating code from sketches:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/6316903035a05f865e00a2bb57a5cebf.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author
  prefs: []
  type: TYPE_NORMAL
- en: 'OCR:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/5a44d2077c76c2466a816925bc5fd34e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Description of artistic heritages:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/bc79aa0a1075216f7e5b904316adf525.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author
  prefs: []
  type: TYPE_NORMAL
- en: And many others.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we are going to focus on LMMs’ vision capabilities and how
    they differ from the standard Computer Vision algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: What is Computer Vision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Computer Vision (CV) is a field of artificial intelligence (AI) that enables
    computers and systems to derive meaningful information from digital images, videos,
    and other visual inputs. It uses machine learning and neural networks to teach
    computers to see, observe, and understand. The goal is to mimic the human visual
    system, automating tasks that the human visual system can do. For example, computer
    vision can be used to recognize objects in an image, detect events, estimate 3D
    poses, and restore images.
  prefs: []
  type: TYPE_NORMAL
- en: Since CV relies on neural networks, that are nothing but mathematical model,
    we need to convert images into numerical input, so that they can be processed
    by our models.
  prefs: []
  type: TYPE_NORMAL
- en: An image is a multi-dimensional array of pixels. For a grayscale image, this
    is a 2D array where each pixel corresponds to a different shade of gray. For a
    colored image, it’s a 3D array (height, width, and color channels), where each
    color channel (Red, Green, Blue — RGD) has a separate 2D array of pixel intensities
    in that color.
  prefs: []
  type: TYPE_NORMAL
- en: Each pixel intensity is a numerical value. The most common scenario is that
    this value ranges from 0 (black) to 255 (white). The combination of these pixels
    makes up what we visually perceive as an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following illustration shows an example of a RGD image (note: pixels’ values
    are meant for examples only and do not represent real values).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/28bd1c135856c67ae69a1b0fa8ff4fd4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author
  prefs: []
  type: TYPE_NORMAL
- en: Now, the question is how to pre-process these multi-dimensional array in such
    a way that the CV model is able to understand them and preserve as much information
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Before the emergence of multimodal models, computer vision used to rely on specialized
    models that were designed and trained for specific tasks, such as object detection,
    face recognition, scene segmentation, and optical character recognition. One of
    the most popular class of models in this field is that of **Convolutional Neural
    Networks**.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Convolutional Neural Networks](/deep-learning-for-image-recognition-convolutional-neural-network-with-tensorflow-de6349c31c07)
    are nothing but Neural Networks that exhibit, in at least one layer, the mathematical
    operation of **convolution**.'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution is an element-wise multiplication between two matrices (representing,
    respectively, a filter specialized in detecting specific features and an equally-sized
    region of the image being processed) with the final summation of the outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5f4e3d259ced2dedb5443306808fe1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider the following illustration as an example.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b91a0a8886414ba5952e443d5bbdf6ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author
  prefs: []
  type: TYPE_NORMAL
- en: 'In the picture, I’ve examined an easy task: we have an alphabet of four letters
    — A, B, C and D — and our algorithm is asked to recognize our input letter (in
    our case, a ‘C’). The input image is passed through a filter specialized, for
    example, in corner detection, then reduced in dimensionality, flattened and processed
    through the fully-connected layers.'
  prefs: []
  type: TYPE_NORMAL
- en: Visual Transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A first alternative to CNN was introduced with Vision Transformer (original
    paper [here](https://arxiv.org/pdf/2010.11929.pdf)), that share with LLMs the
    core architecture (encoder/decoder).
  prefs: []
  type: TYPE_NORMAL
- en: As per traditional transformers, also in this case the core mechanism is **Attention**
    (original paper [here](https://arxiv.org/abs/1706.03762)), that allows the model
    to selectively focus on different parts of the input sequence when making predictions.
    This concept is applied by teaching the model to focus on certain parts of the
    input data and disregard others to better solve the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: The **revolutionary aspect** of attention in Transformers is that it dispenses
    with recurrence (read more about Recurrent Neural Networks (RNNs) [here](https://medium.com/analytics-vidhya/recurrent-neural-networks-97f3b034e70))
    and convolutions, which were extensively relied upon by previous models. The Transformer
    is the first model relying entirely on attention to compute representations of
    its input and output without using sequence-aligned RNNs or convolution. This
    allows the Transformer to capture a wider range of relationships between the words
    in a sentence and learn a more nuanced representation of the input.
  prefs: []
  type: TYPE_NORMAL
- en: In Vision Transformers, images are processed differently than in CNNs. In fact,
    an image is divided into so-called patches, typically of the size of 16x16 pixles.
    Then, each patch is flattened into a 1D vector and tokenized, in the same way
    we do with text data in a standard transformer like GPT-3.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e3477a3a624f8feb7af4b3d6fca0a9a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author
  prefs: []
  type: TYPE_NORMAL
- en: Now, these tokenized patches will be fed into the Vision Tranformer model and
    further converted into lower-dimensional vectors via a **linear projection layer,**
    so that we can work with less memory and computational powerwhile preserving information.
    Plus, as in standard transformer, each token is associated with an indicator about
    its position in the overall context of the image via a **positional embedding
    layer.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7f96677230d6aa881aac2a5fd2742aec.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the positional-embedded tokens are passed into the main block of the
    model, that is the transformer encoder. Below you can see an illustration of a
    Vision Transformer (in this case, the scenario is a classification task):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/032f0d41f4c82441273a59836659205f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [https://arxiv.org/pdf/2010.11929v2.pdf](https://arxiv.org/pdf/2010.11929v2.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the Transformer encoder exhibit the **attention mechanism**
    mentioned above.
  prefs: []
  type: TYPE_NORMAL
- en: Putting all together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The idea is that of projecting images’ embeddings in the same latent space as
    language, so that, given a human input made of image + text, the model is able
    to gather the relevant context from an embedding space covering both images and
    texts.
  prefs: []
  type: TYPE_NORMAL
- en: A first example of an image and text model was introduced by OpenAI with CLIP
    (Contrastive Language-Image Pretraining).
  prefs: []
  type: TYPE_NORMAL
- en: '**CLIP**'
  prefs: []
  type: TYPE_NORMAL
- en: '[CLIP](https://openai.com/research/clip) is a model developed by OpenAI that’s
    designed to understand images and text together. It’s like a bridge that connects
    the world of images and the world of words.'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you have a bunch of images and sentences, and you want to match each
    image with the sentence that best describes it. That’s essentially what CLIP does.
    It’s trained to understand which images and sentences are similar to each other.
  prefs: []
  type: TYPE_NORMAL
- en: The cool thing about CLIP is that it doesn’t need to be specifically trained
    on a certain task to do well at it. For example, if you have a new set of images
    and sentences that CLIP has never seen before (in a so-called zero-shot approach),
    it can still do a pretty good job at matching them up. This is because CLIP has
    learned a general understanding of images and text from a huge amount of data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1848d49317a2224a1b0f12faf4deaf8c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [CLIP: Connecting text and images (openai.com)](https://openai.com/research/clip)'
  prefs: []
  type: TYPE_NORMAL
- en: While CLIP was still a predictor model, the state-of-the-art LMMs are meant
    to interact with humans as AI assistant. In other words, they are instruct models.
  prefs: []
  type: TYPE_NORMAL
- en: '**LLaVA**'
  prefs: []
  type: TYPE_NORMAL
- en: A great example of an assistant LMM is the open-source model [LLaVA (Large Language
    and Vision Assistant)](https://llava-vl.github.io/), that combines the above mentioned
    CLIP for image encoding and the base LLM [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/),
    for instruction understanding.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of LLaVA is that of having a linear layer to connect image features
    produced by CLIP into the word embedding space of the language model Vicuna.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52bcd962eecc01cc06d1b8d0575f4850.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [2304.08485.pdf (arxiv.org)](https://arxiv.org/pdf/2304.08485.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: To do so, researchers introduced a trainable projection matrix W that converts
    image features into embedding vectors of the same dimensions as word embeddings
    processed by the language model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kosmos-1**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example is Kosmos-1, introduced by Microsoft Research in [this paper.](https://arxiv.org/pdf/2302.14045.pdf)
    The approach behind this model is that of having a transformer decoder that perceives
    general modalities in a unified way: inputs are flattened into 1D vectors of tokens
    and tagged with special start and end-of-sequences special tokens (texts as *<s>text</s>*,
    images as *<image>image</image>*). Once tokenized, inputs are encoded via embedding
    modules (specific per data format), then fed into the decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In their paper, authors describe the training process of the model, that included
    the following data corpora:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Monomodal, text corpora** →used for **representation learning** (that is,
    the ability to produce meaningful representation of language, keeping the underneath
    semantic structure) and general purpose language tasks, such as instructions-following,
    in-context learning and similar.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image-caption pairs** →needed as the bridge for the model to link images
    with their description'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interleaved Image-text data** →needed to further align the perception of
    general modalities with language models and improve model’s few-shot ability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The below picture shows some examples of Kosmos-1 capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/99296ea927285d89586122a946bf1507.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [2302.14045.pdf (arxiv.org)](https://arxiv.org/pdf/2302.14045.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the trend of creating a common embedding space for both images and
    words, so that the model can “perceive” both the data formats — texts and images
    — is paving the way to powerful Large Multimodal Models. Plus, pre-trained models
    such as CLIP are being widely used to produce image representations, that can
    be then further converted into the word embedding space.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond vision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generally speaking, the main idea behind multimodal models is to create consistent
    representations of a given concept across different modalities. In the previous
    section, we saw how we can use a Vision Transformer to embed images into lower-dimensional
    vectors in a latent space. Similarly, we could create encoders for each modality
    and using an objective function that encourages the models to produce similar
    embeddings for similar data pairs.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s consider the model [MACAW-LLM](https://github.com/lyuchenyang/Macaw-LLM),
    a multi-modal language model that is able to process images, video, audio, and
    text data, built upon the foundations of CLIP (images and video), [Whisper](https://openai.com/research/whisper)
    (audio), and [LLaMA](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)
    (text).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5da82d88f802560230b4eaf262d358c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [lyuchenyang/Macaw-LLM: Macaw-LLM: Multi-Modal Language Modeling with
    Image, Video, Audio, and Text Integration (github.com)](https://github.com/lyuchenyang/Macaw-LLM)'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, different embedding modules are used to produce a “shared” embedding
    space, which is aligned with the word embedding space used by the LLM (the Meta
    AI’s LLaMA in this case).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multi-modality is paving the way towards a new waves of applications and use
    cases. It is also a further milestone towards the concept of [Artificial General
    Intelligence (AGI)](https://openai.com/blog/planning-for-agi-and-beyond), since
    it is making AI systems more and more prone to “perceive” as humans do.
  prefs: []
  type: TYPE_NORMAL
- en: 'Needless to say, Large Multimodal Models come with even greater responsabilities
    in terms of ethical considerations: biases, discriminations, privacy violations
    and many others risks are at the core of LMMs’ researches, and now more than even
    [Human Alignment](https://openai.com/blog/introducing-superalignment) is a top
    priority while developing AI systems.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[https://arxiv.org/pdf/2010.11929.pdf](https://arxiv.org/pdf/2010.11929.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://medium.com/analytics-vidhya/recurrent-neural-networks-97f3b034e70](https://medium.com/analytics-vidhya/recurrent-neural-networks-97f3b034e70)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[lyuchenyang/Macaw-LLM: Macaw-LLM: Multi-Modal Language Modeling with Image,
    Video, Audio, and Text Integration (github.com)](https://github.com/lyuchenyang/Macaw-LLM)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2302.14045.pdf (arxiv.org)](https://arxiv.org/pdf/2302.14045.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[CLIP: Connecting text and images (openai.com)](https://openai.com/research/clip)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://ai.meta.com/blog/large-language-model-llama-meta-ai/](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://openai.com/blog/introducing-superalignment](https://openai.com/blog/introducing-superalignment)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://openai.com/blog/our-approach-to-alignment-research](https://openai.com/blog/our-approach-to-alignment-research)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
