- en: Gradient Boosted Linear Regression in Excel
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Excel 中进行梯度提升线性回归
- en: 原文：[https://towardsdatascience.com/gradient-boosted-linear-regression-in-excel-a08522f13d6a](https://towardsdatascience.com/gradient-boosted-linear-regression-in-excel-a08522f13d6a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gradient-boosted-linear-regression-in-excel-a08522f13d6a](https://towardsdatascience.com/gradient-boosted-linear-regression-in-excel-a08522f13d6a)
- en: To even better understand Gradient Boosting
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为了更好地理解梯度提升
- en: '[](https://medium.com/@angela.shi?source=post_page-----a08522f13d6a--------------------------------)[![Angela
    and Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----a08522f13d6a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a08522f13d6a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a08522f13d6a--------------------------------)
    [Angela and Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----a08522f13d6a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@angela.shi?source=post_page-----a08522f13d6a--------------------------------)[![Angela
    和 Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----a08522f13d6a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a08522f13d6a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a08522f13d6a--------------------------------)
    [Angela 和 Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----a08522f13d6a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a08522f13d6a--------------------------------)
    ·7 min read·Mar 17, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a08522f13d6a--------------------------------)
    ·阅读时间 7 分钟·2023 年 3 月 17 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Gradient Boosting is an ensemble method that is usually applied to decision
    trees. It is so often that we usually say **Gradient Boosting** to refer to **Gradient
    Boosted Decision Trees**. In scikit learn for example, the estimators GradientBoostingRegressor
    or GradientBoostingClassifier use Decision Trees. However, as an ensemble method,
    it is possible to apply it to other base models such as **linear regression**.
    But there is a trivial conclusion that you may already know:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升是一种通常应用于决策树的集成方法。它如此常见，以至于我们通常用**梯度提升**来指代**梯度提升决策树**。例如，在 scikit-learn
    中，估算器 GradientBoostingRegressor 或 GradientBoostingClassifier 使用决策树。然而，作为一种集成方法，它也可以应用于其他基础模型，如**线性回归**。但有一个显而易见的结论，你可能已经知道：
- en: Gradient Boosted Linear Regression is, well, Linear Regression.
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 梯度提升线性回归就是，嗯，线性回归。
- en: But it is still interesting to implement it and moreover, we will do it in Excel,
    so even if you are not familiar to program complex algorithms, you can still understand
    the algorithmic steps.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 但实现它仍然很有趣，而且我们将通过 Excel 来完成它，因此即使你不熟悉编程复杂算法，你也能理解这些算法步骤。
- en: Machine Learning in Three steps
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的三步法
- en: 'I wrote [an article to always distinguish three steps of machine learning to
    learn it in an effective way](https://medium.com/towards-data-science/machine-learning-in-three-steps-how-to-efficiently-learn-it-aefcf423a9e1),
    and let’s apply the principle to Gradient Boosted Linear Regression, here are
    the three steps:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我写了 [一篇文章来始终区分机器学习的三个步骤，以有效学习](https://medium.com/towards-data-science/machine-learning-in-three-steps-how-to-efficiently-learn-it-aefcf423a9e1)，让我们将这个原则应用到梯度提升线性回归中，这里是三个步骤：
- en: '**1\. Model: Linear Regression** is a machine learning model in the sense that
    it takes input (features) to predict an output'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1. 模型：线性回归** 是一种机器学习模型，因为它接受输入（特征）来预测输出。'
- en: '**1bis. Ensemble method: Gradient Boosting** is an ensemble method and it is
    not a model itself (in the sense that it does not take inputs to predict output
    for a target variable). It has to be applied to some base model to create a meta-model.
    Here we will create a meta-model that is Gradient Boosted Linear Regression.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1bis. 集成方法：梯度提升** 是一种集成方法，它本身并不是一个模型（因为它不直接接受输入来预测目标变量的输出）。它必须应用于某个基础模型来创建一个元模型。这里我们将创建一个梯度提升的线性回归元模型。'
- en: '**2\. Model fitting:** Linear Regression has to be fitted, which means their
    coefficients have to be optimized for a given training dataset. **Gradient Descent**
    is a fitting algorithm that can be applied to linear regression. But it is not
    the only one. In the case of linear regression, there is an exact solution that
    can be expressed in mathematical formulas. It is also worth noting that there
    is no fitting algorithm for ensemble methods.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2\. 模型拟合：**线性回归必须进行拟合，这意味着其系数必须针对给定的训练数据集进行优化。**梯度下降**是一种可以应用于线性回归的拟合算法。但它并不是唯一的。在线性回归的情况下，有一个可以用数学公式表示的精确解。还值得注意的是，集成方法没有拟合算法。'
- en: '**3\. Model tuning** consists of optimizing the hyperparameters of the model
    or the meta-model. Here, we will encounter two: the learning rate of the gradient
    boosting algorithm, and the number of steps.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3\. 模型调整**包括优化模型或元模型的超参数。在这里，我们将遇到两个超参数：梯度提升算法的学习率和步骤数。'
- en: It was machine learning made easy in three steps!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是三步搞定的机器学习！
- en: A Simple Linear Regression as a Base Model
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作为基础模型的简单线性回归
- en: 'Here we will use a simple linear regression as the base model, with a simple
    dataset of ten observations. We will focus on the gradient boosting part, and
    for the fitting, we will use a function in Google Sheet (it also works in Excel):
    LINEST to estimate the coefficients of the linear regression.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用一个简单的线性回归作为基础模型，并使用一个包含十个观察值的简单数据集。我们将重点关注梯度提升部分，对于拟合，我们将使用Google Sheet中的一个函数（在Excel中也适用）：LINEST来估计线性回归的系数。
- en: '![](../Images/7e99bdcedbb5485d22a28ecbd5bbe014.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e99bdcedbb5485d22a28ecbd5bbe014.png)'
- en: Gradient Boosted Linear Regression \ Simple dataset with linear regression —
    Image by author
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升的线性回归 \ 简单数据集与线性回归 — 作者图像
- en: 'I will use a Google Sheet to demonstrate the implementation process for gradient
    boosting in this article. If you’d like to access this sheet, as well as others
    I’ve developed — such as linear regression with gradient descent, logistic regression,
    neural networks with backpropagation, KNN, k-means, and more to come — please
    consider supporting me on Ko-fi. You can find all of these resources at the following
    link: [https://ko-fi.com/s/4ddca6dff1](https://ko-fi.com/s/4ddca6dff1)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用Google Sheet来演示本文中梯度提升的实现过程。如果你想访问这个表格以及我开发的其他表格——例如带有梯度下降的线性回归、逻辑回归、具有反向传播的神经网络、KNN、k-means等——请考虑在Ko-fi上支持我。你可以在以下链接找到所有这些资源：[https://ko-fi.com/s/4ddca6dff1](https://ko-fi.com/s/4ddca6dff1)
- en: Gradient Boosting algorithm
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度提升算法
- en: Here are the main steps of the Gradient Boosting algorithm
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是梯度提升算法的主要步骤
- en: 'Initialization: we will choose the average value to be the first step of the
    gradient boosting algorithm.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化：我们将选择平均值作为梯度提升算法的第一步。
- en: 'Residual Errors Calculation: we calculate the residual errors between the predicted
    values (for the first step, it is the average value) and the actual values in
    the training data.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 残差误差计算：我们计算预测值（对于第一步，它是平均值）和训练数据中的实际值之间的残差误差。
- en: 'Fitting Linear Regression to Residuals: we create a linear regression model
    to fit the residuals.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合残差的线性回归：我们创建一个线性回归模型来拟合残差。
- en: 'Adding the New Model to the Ensemble: Combine the previous model and the new
    model to create a new ensemble model. Here, we have to apply a learning rate or
    shrinkage as a hyperparameter to the weak model.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新模型添加到集成中：将之前的模型和新模型结合起来，创建一个新的集成模型。在这里，我们必须将学习率或收缩作为超参数应用于弱模型。
- en: 'Repeating the process: Repeat steps 2–4 until you have reached the specified
    number of boosting stages or until the error has converged.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复过程：重复步骤2–4，直到达到指定的提升阶段数或直到误差收敛。
- en: 'That’s it! This is the basic procedure for performing a Gradient Boosting applied
    to Linear Regression. I wanted to keep the description in a simple way, and here
    we can write some equations to illustrate each iteration:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！这是应用于线性回归的梯度提升的基本过程。我想以简单的方式描述，并且我们可以写一些方程式来说明每次迭代：
- en: 'Step1: f0 = average value of actual y'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤1：f0 = 实际y的平均值
- en: 'Step 2: resd1 = y — f0'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤2：resd1 = y — f0
- en: 'Step 3: resdfit1 = a0 x + b0 to predict y — f0'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤3：resdfit1 = a0 x + b0 以预测 y — f0
- en: 'Step 4: f1 = f0 — learning_rate * (a0 x + b0)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤4：f1 = f0 — learning_rate * (a0 x + b0)
- en: 'Step 2–2: resd2 = y — f1'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤2–2：resd2 = y — f1
- en: 'Step 3–2: resdfit2 = a1 x + b1 to predict y — f1'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤3–2：resdfit2 = a1 x + b1 以预测 y — f1
- en: 'Step 4–2: f2 = f1-learning_rate * (a1 x + b1) which can be developed as : f0
    — learning_rate * (a0 x + b0) — learning_rate * (a1 x + b1)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤 4–2：f2 = f1-learning_rate * (a1 x + b1)，可以展开为：f0 — learning_rate * (a0 x
    + b0) — learning_rate * (a1 x + b1)
- en: …
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: …
- en: Comparison with Gradient Boosted Trees
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与梯度提升树的比较
- en: If you look carefully at the algorithm of the previous section, you may notice
    two bizarre things.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细查看上一节的算法，你可能会注意到两个奇怪的现象。
- en: First, in step 2, we fit a linear regression to residuals, it will take time
    and algorithmic steps to achieve the model fitting steps, instead of fitting a
    linear regression to residuals, we can directly fit a linear regression to the
    actual values of y and we already would find the final optimal model!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在第 2 步中，我们将线性回归拟合到残差上，这需要时间和算法步骤来实现模型拟合步骤，而不是将线性回归拟合到残差上，我们可以直接将线性回归拟合到 y
    的实际值上，我们已经会找到最终的最佳模型！
- en: Secondly, when adding a linear regression to another linear regression, it is
    still a linear regression.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，当将一个线性回归添加到另一个线性回归时，它仍然是线性回归。
- en: 'For example, we can rewrite f2 as: f2 = f0 — learning_rate *(b0+b1) — learning_rate
    * (a0+a1) x'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以将 f2 重写为：f2 = f0 — learning_rate * (b0+b1) — learning_rate * (a0+a1)
    x
- en: It is a linear regression!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是线性回归！
- en: For decision trees, these two bizarre things won’t happen, since adding a tree
    to another is not the same as growing a tree a step further.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于决策树，这两种奇怪的情况不会发生，因为将一棵树添加到另一棵树上与将一棵树进一步生长是不一样的。
- en: 'Before we go to the implementation part, one more question: what if we set
    the learning rate to 1? What happens to Gradient Boosted Linear Regression?'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入实现部分之前，还有一个问题：如果我们将学习率设置为 1，会发生什么？梯度提升线性回归会怎样？
- en: Gradient Boosted Linear Regression Implementation
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度提升线性回归实现
- en: The implementation of these formulas is straightforward in Google Sheet or Excel.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Sheet 或 Excel 中实现这些公式是直接的。
- en: The table below shows the training dataset along with the different steps of
    the gradient boosting steps
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了训练数据集以及梯度提升步骤的不同阶段
- en: '![](../Images/e4a3bc5214bf0953573269a85edae9c0.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4a3bc5214bf0953573269a85edae9c0.png)'
- en: Gradient Boosted Linear Regression with all steps in Excel — Image by author
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Excel 中进行的梯度提升线性回归 — 作者图像
- en: For each fitting step, we use the Excel function LINEST
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一步拟合，我们使用 Excel 函数 LINEST
- en: '![](../Images/73e69ea880691b8a4ba439c8ce3597fc.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73e69ea880691b8a4ba439c8ce3597fc.png)'
- en: Gradient Boosted Linear Regression with formula for coefficient estimation —
    Image by author
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 带有系数估计公式的梯度提升线性回归 — 作者图像
- en: We will only do 2 iterations and we can guess how it goes for more iterations.
    Here below is a graphic to show the models at each iteration. The different shades
    of red illustrate the convergence of the model and we also show the final model
    that is directly found with gradient descent applied directly to y.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只进行 2 次迭代，我们可以猜测更多迭代的情况。下面是一个图形，展示了每次迭代的模型。不同的红色阴影展示了模型的收敛情况，我们还展示了通过梯度下降直接应用于
    y 找到的最终模型。
- en: '![](../Images/89585707920f3f8eee794a49aa074f12.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89585707920f3f8eee794a49aa074f12.png)'
- en: Gradient Boosted Linear Regression — Image by author
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升线性回归 — 作者图像
- en: Tuning hyperparameters
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整超参数
- en: 'There are two hyperparameters we can tune: the number of iterations and the
    learning rate.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调整两个超参数：迭代次数和学习率。
- en: For the number of iterations, we only implemented two, but it is easy to imagine
    more and we can stop by examining the magnitude of the residuals.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于迭代次数，我们只实施了两个，但很容易想象更多的情况，我们可以通过检查残差的大小来停止。
- en: For the learning rate, we can change it in Google Sheet and see what happens.
    When the learning rate is small, the “learning process” will be slow. And if the
    learning rate is 1, we can see that the convergence is achieved at iteration 1.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于学习率，我们可以在 Google Sheet 中进行调整，看看会发生什么。当学习率很小时，“学习过程”将会很慢。如果学习率为 1，我们可以看到收敛在第
    1 次迭代时就达到了。
- en: '![](../Images/62e1436ba66af65ede15e0f35e9ad318.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62e1436ba66af65ede15e0f35e9ad318.png)'
- en: Gradient Boosted Linear Regression with learning rate =1— Image by author
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率 =1 的梯度提升线性回归 — 作者图像
- en: And the residuals of iteration 1 are already zeros.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 并且第 1 次迭代的残差已经是零。
- en: '![](../Images/945f4fd2daf9f6d2822a1dd580b4f449.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/945f4fd2daf9f6d2822a1dd580b4f449.png)'
- en: Gradient Boosted Linear Regression with learning rate =1— Image by author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率 =1 的梯度提升线性回归 — 作者图像
- en: If the learning rate is higher than 1, then the model will diverge.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果学习率高于 1，模型将会发散。
- en: '![](../Images/e89d36302fe07714acf85c1d6d7d4c16.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e89d36302fe07714acf85c1d6d7d4c16.png)'
- en: Gradient Boosted Linear Regression Divergence— Image by author
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升线性回归的发散—— 作者提供的图像
- en: Comparison with Gradient Descent Algorithm
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与梯度下降算法的比较
- en: How the learning rate and the number of iterations work in Gradient Boosting
    is very similar to Gradient Descent… Oh wait, they are very similar and they actually
    are the same algorithm! in the sense that in the case of Classic Gradient Descent,
    the algorithm is applied to parameters of the model such as the weights or coefficients
    of the linear regression. And in the case of Gradient Boosting, the algorithm
    is applied to models.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升中的学习率和迭代次数的工作方式与梯度下降非常相似……哦，等等，它们非常相似，实际上是相同的算法！在经典梯度下降的情况下，该算法应用于模型的参数，如线性回归的权重或系数。而在梯度提升的情况下，该算法应用于模型。
- en: Even the word “boosting” only means “adding” and it is the exact same procedure
    in the classic gradient descent algorithm which consists of adding the descent
    step by step from the initial (randomly chosen) starting points.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 即使“提升”这个词只意味着“添加”，它与经典梯度下降算法中的步骤完全相同，都是从初始（随机选择的）起点一步步添加下降步骤。
- en: 'If you are not convinced, you can read this article about this [Gradient Descent
    vs. Gradient Boosting: A Side-by-Side Comparison](https://medium.com/towards-data-science/gradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还不信服，可以阅读这篇关于[梯度下降与梯度提升的对比：逐步比较](https://medium.com/towards-data-science/gradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712)的文章。
- en: '**Conclusion**'
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: I hope that you gained more insights into how Gradient Boosting works. Here
    are the main takeaways.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你对梯度提升的工作原理有了更多的了解。以下是主要要点。
- en: Excel is an excellent way to understand how algorithms work.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Excel 是理解算法如何工作的绝佳方式。
- en: Gradient Boosting is an Ensemble Method that can be applied to any base model.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升是一种集成方法，可以应用于任何基础模型。
- en: 'Gradient Boosting is Gradient Descent in the sense that they are the same algorithm
    but applied to different objects: parameters vs. functions or models.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升在某种意义上与梯度下降相同，因为它们是相同的算法，但应用于不同的对象：参数与函数或模型。
- en: Gradient Boosting can be applied to Linear Regression but it is only for the
    purpose to understand the algorithm because in practice you don’t need to because
    Gradient Boosted Linear Regression is Linear Regression.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升可以应用于线性回归，但这只是为了理解算法，因为在实际应用中你并不需要这样做，因为梯度提升线性回归就是线性回归。
- en: 'I write about machine learning and data science and I try to simplify complex
    concepts in a clear way. Please follow me with the link below and get full access
    to my articles: [https://medium.com/@angela.shi/membership](https://medium.com/@angela.shi/membership)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我写关于机器学习和数据科学的文章，并尝试以清晰的方式简化复杂的概念。请通过下面的链接关注我，并获得我文章的完整访问权限：[https://medium.com/@angela.shi/membership](https://medium.com/@angela.shi/membership)
