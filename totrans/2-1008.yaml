- en: Gradient Boosted Linear Regression in Excel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/gradient-boosted-linear-regression-in-excel-a08522f13d6a](https://towardsdatascience.com/gradient-boosted-linear-regression-in-excel-a08522f13d6a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To even better understand Gradient Boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@angela.shi?source=post_page-----a08522f13d6a--------------------------------)[![Angela
    and Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----a08522f13d6a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a08522f13d6a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a08522f13d6a--------------------------------)
    [Angela and Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----a08522f13d6a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a08522f13d6a--------------------------------)
    ·7 min read·Mar 17, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradient Boosting is an ensemble method that is usually applied to decision
    trees. It is so often that we usually say **Gradient Boosting** to refer to **Gradient
    Boosted Decision Trees**. In scikit learn for example, the estimators GradientBoostingRegressor
    or GradientBoostingClassifier use Decision Trees. However, as an ensemble method,
    it is possible to apply it to other base models such as **linear regression**.
    But there is a trivial conclusion that you may already know:'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Boosted Linear Regression is, well, Linear Regression.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But it is still interesting to implement it and moreover, we will do it in Excel,
    so even if you are not familiar to program complex algorithms, you can still understand
    the algorithmic steps.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning in Three steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I wrote [an article to always distinguish three steps of machine learning to
    learn it in an effective way](https://medium.com/towards-data-science/machine-learning-in-three-steps-how-to-efficiently-learn-it-aefcf423a9e1),
    and let’s apply the principle to Gradient Boosted Linear Regression, here are
    the three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Model: Linear Regression** is a machine learning model in the sense that
    it takes input (features) to predict an output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1bis. Ensemble method: Gradient Boosting** is an ensemble method and it is
    not a model itself (in the sense that it does not take inputs to predict output
    for a target variable). It has to be applied to some base model to create a meta-model.
    Here we will create a meta-model that is Gradient Boosted Linear Regression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2\. Model fitting:** Linear Regression has to be fitted, which means their
    coefficients have to be optimized for a given training dataset. **Gradient Descent**
    is a fitting algorithm that can be applied to linear regression. But it is not
    the only one. In the case of linear regression, there is an exact solution that
    can be expressed in mathematical formulas. It is also worth noting that there
    is no fitting algorithm for ensemble methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3\. Model tuning** consists of optimizing the hyperparameters of the model
    or the meta-model. Here, we will encounter two: the learning rate of the gradient
    boosting algorithm, and the number of steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It was machine learning made easy in three steps!
  prefs: []
  type: TYPE_NORMAL
- en: A Simple Linear Regression as a Base Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here we will use a simple linear regression as the base model, with a simple
    dataset of ten observations. We will focus on the gradient boosting part, and
    for the fitting, we will use a function in Google Sheet (it also works in Excel):
    LINEST to estimate the coefficients of the linear regression.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e99bdcedbb5485d22a28ecbd5bbe014.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient Boosted Linear Regression \ Simple dataset with linear regression —
    Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'I will use a Google Sheet to demonstrate the implementation process for gradient
    boosting in this article. If you’d like to access this sheet, as well as others
    I’ve developed — such as linear regression with gradient descent, logistic regression,
    neural networks with backpropagation, KNN, k-means, and more to come — please
    consider supporting me on Ko-fi. You can find all of these resources at the following
    link: [https://ko-fi.com/s/4ddca6dff1](https://ko-fi.com/s/4ddca6dff1)'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Boosting algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here are the main steps of the Gradient Boosting algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialization: we will choose the average value to be the first step of the
    gradient boosting algorithm.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Residual Errors Calculation: we calculate the residual errors between the predicted
    values (for the first step, it is the average value) and the actual values in
    the training data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fitting Linear Regression to Residuals: we create a linear regression model
    to fit the residuals.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Adding the New Model to the Ensemble: Combine the previous model and the new
    model to create a new ensemble model. Here, we have to apply a learning rate or
    shrinkage as a hyperparameter to the weak model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeating the process: Repeat steps 2–4 until you have reached the specified
    number of boosting stages or until the error has converged.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'That’s it! This is the basic procedure for performing a Gradient Boosting applied
    to Linear Regression. I wanted to keep the description in a simple way, and here
    we can write some equations to illustrate each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step1: f0 = average value of actual y'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 2: resd1 = y — f0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 3: resdfit1 = a0 x + b0 to predict y — f0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 4: f1 = f0 — learning_rate * (a0 x + b0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 2–2: resd2 = y — f1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 3–2: resdfit2 = a1 x + b1 to predict y — f1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 4–2: f2 = f1-learning_rate * (a1 x + b1) which can be developed as : f0
    — learning_rate * (a0 x + b0) — learning_rate * (a1 x + b1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: …
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparison with Gradient Boosted Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you look carefully at the algorithm of the previous section, you may notice
    two bizarre things.
  prefs: []
  type: TYPE_NORMAL
- en: First, in step 2, we fit a linear regression to residuals, it will take time
    and algorithmic steps to achieve the model fitting steps, instead of fitting a
    linear regression to residuals, we can directly fit a linear regression to the
    actual values of y and we already would find the final optimal model!
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, when adding a linear regression to another linear regression, it is
    still a linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can rewrite f2 as: f2 = f0 — learning_rate *(b0+b1) — learning_rate
    * (a0+a1) x'
  prefs: []
  type: TYPE_NORMAL
- en: It is a linear regression!
  prefs: []
  type: TYPE_NORMAL
- en: For decision trees, these two bizarre things won’t happen, since adding a tree
    to another is not the same as growing a tree a step further.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we go to the implementation part, one more question: what if we set
    the learning rate to 1? What happens to Gradient Boosted Linear Regression?'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Boosted Linear Regression Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The implementation of these formulas is straightforward in Google Sheet or Excel.
  prefs: []
  type: TYPE_NORMAL
- en: The table below shows the training dataset along with the different steps of
    the gradient boosting steps
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4a3bc5214bf0953573269a85edae9c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient Boosted Linear Regression with all steps in Excel — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: For each fitting step, we use the Excel function LINEST
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/73e69ea880691b8a4ba439c8ce3597fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient Boosted Linear Regression with formula for coefficient estimation —
    Image by author
  prefs: []
  type: TYPE_NORMAL
- en: We will only do 2 iterations and we can guess how it goes for more iterations.
    Here below is a graphic to show the models at each iteration. The different shades
    of red illustrate the convergence of the model and we also show the final model
    that is directly found with gradient descent applied directly to y.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89585707920f3f8eee794a49aa074f12.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient Boosted Linear Regression — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Tuning hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two hyperparameters we can tune: the number of iterations and the
    learning rate.'
  prefs: []
  type: TYPE_NORMAL
- en: For the number of iterations, we only implemented two, but it is easy to imagine
    more and we can stop by examining the magnitude of the residuals.
  prefs: []
  type: TYPE_NORMAL
- en: For the learning rate, we can change it in Google Sheet and see what happens.
    When the learning rate is small, the “learning process” will be slow. And if the
    learning rate is 1, we can see that the convergence is achieved at iteration 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62e1436ba66af65ede15e0f35e9ad318.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient Boosted Linear Regression with learning rate =1— Image by author
  prefs: []
  type: TYPE_NORMAL
- en: And the residuals of iteration 1 are already zeros.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/945f4fd2daf9f6d2822a1dd580b4f449.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient Boosted Linear Regression with learning rate =1— Image by author
  prefs: []
  type: TYPE_NORMAL
- en: If the learning rate is higher than 1, then the model will diverge.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e89d36302fe07714acf85c1d6d7d4c16.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient Boosted Linear Regression Divergence— Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Comparison with Gradient Descent Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How the learning rate and the number of iterations work in Gradient Boosting
    is very similar to Gradient Descent… Oh wait, they are very similar and they actually
    are the same algorithm! in the sense that in the case of Classic Gradient Descent,
    the algorithm is applied to parameters of the model such as the weights or coefficients
    of the linear regression. And in the case of Gradient Boosting, the algorithm
    is applied to models.
  prefs: []
  type: TYPE_NORMAL
- en: Even the word “boosting” only means “adding” and it is the exact same procedure
    in the classic gradient descent algorithm which consists of adding the descent
    step by step from the initial (randomly chosen) starting points.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are not convinced, you can read this article about this [Gradient Descent
    vs. Gradient Boosting: A Side-by-Side Comparison](https://medium.com/towards-data-science/gradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I hope that you gained more insights into how Gradient Boosting works. Here
    are the main takeaways.
  prefs: []
  type: TYPE_NORMAL
- en: Excel is an excellent way to understand how algorithms work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient Boosting is an Ensemble Method that can be applied to any base model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gradient Boosting is Gradient Descent in the sense that they are the same algorithm
    but applied to different objects: parameters vs. functions or models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient Boosting can be applied to Linear Regression but it is only for the
    purpose to understand the algorithm because in practice you don’t need to because
    Gradient Boosted Linear Regression is Linear Regression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I write about machine learning and data science and I try to simplify complex
    concepts in a clear way. Please follow me with the link below and get full access
    to my articles: [https://medium.com/@angela.shi/membership](https://medium.com/@angela.shi/membership)'
  prefs: []
  type: TYPE_NORMAL
