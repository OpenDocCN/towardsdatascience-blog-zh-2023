- en: Understand & Implement Masked AutoRegressive Flow with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understand-implement-masked-autoregressive-flow-with-tensorflow-9c361cd1354c](https://towardsdatascience.com/understand-implement-masked-autoregressive-flow-with-tensorflow-9c361cd1354c)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Flow-Based Models for Density Estimation with TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://saptashwa.medium.com/?source=post_page-----9c361cd1354c--------------------------------)[![Saptashwa
    Bhattacharyya](../Images/b01238113a1f6b91cb6fb0fbfa50303a.png)](https://saptashwa.medium.com/?source=post_page-----9c361cd1354c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9c361cd1354c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9c361cd1354c--------------------------------)
    [Saptashwa Bhattacharyya](https://saptashwa.medium.com/?source=post_page-----9c361cd1354c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9c361cd1354c--------------------------------)
    ·8 min read·Feb 21, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/891350cbc87604e6ea8e847db10585fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig: From random to not-so-random! Source: Author’s Notebook (see References
    below).'
  prefs: []
  type: TYPE_NORMAL
- en: Previously we went through the details of the [mathematics behind Normalizing
    Flows](/getting-started-with-normalizing-flows-linear-algebra-probability-f2b863ff427d)
    and some examples of [transforming probability distributions](/transforming-probability-distributions-using-normalizing-flows-bcc5ed6ac2c9).
    Here we combine all these concepts to understand Autoregressive Flows and how
    to implement them using TensorFlow Probability library. What can you expect from
    this post —
  prefs: []
  type: TYPE_NORMAL
- en: Why Triangular Matrices are crucial for Autoregressive Flows?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Basic constructions of Autoregressive Flow-based models
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: — Masked Autoregressive Flow (MAF)
  prefs: []
  type: TYPE_NORMAL
- en: — Inverse Autoregressive Flow (IAF)
  prefs: []
  type: TYPE_NORMAL
- en: 3\. How to implement MAF in TensorFlow and train them for density estimation
    tasks?
  prefs: []
  type: TYPE_NORMAL
- en: Without any delay, let’s begin!
  prefs: []
  type: TYPE_NORMAL
- en: 'Computational Issues in Normalizing Flows:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we discuss models such as Masked Autoregressive Flows etc., we review
    the change of variable rules for 1D and higher Dimensional scenarios, which will
    help us appreciate the computational cost in Normalizing Flows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Previously [we went into detail](https://medium.com/towards-data-science/getting-started-with-normalizing-flows-linear-algebra-probability-f2b863ff427d)
    to derive the change of variable rule where we start from a base distribution
    *u* and a bijector *ϕ* such that *x* = *ϕ*(*u*). In this case, we can simply write
    the change of variables rule as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6f7d2931f1a46dcdc234bcce784d7183.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Eq. 1: Change of variable rule for 1D'
  prefs: []
  type: TYPE_NORMAL
- en: 'For Normalizing Flows we compose (‘chain’) several bijectors to transform a
    simple distribution into a more complicated one. For example, we can compose *K*
    bijection operations as below to transform our base distribution (say *u_0*) to
    our desired complex distribution *x* as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/47d8e9e76cdbb006878c15a0a66e86b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Eq. 2: Compose bijectors to transform simple distribution into a complicated
    one'
  prefs: []
  type: TYPE_NORMAL
- en: 'For *K* transformations we can modify Eq. 1 as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c32c9c9f7b7deb3d4c0ba073909392e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Eq. 3: Rewriting Eq. 1 from 1 bijection operation to K transformations.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the biggest issues with implementing Normalizing Flows is the computational
    complexity of the log-det-jacobian. **Calculating the determinant of the Jacobian
    of *n*×*n* via a process like Gaussian Elimination has a** [**runtime complexity**](https://en.wikipedia.org/wiki/Gaussian_elimination#Computing_determinants)
    **of O(*n³*).**
  prefs: []
  type: TYPE_NORMAL
- en: So we need some simplifications in the procedures described above and we can
    now move to learn some of the simplifications that will help us reduce the computational
    complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Triangular Matrix & Autoregressive Flow:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If the transformation matrix is triangular, then calculating the determinant
    is fairly easy. For an *n × n* square matrix, the runtime complexity is *O(n).*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b87f1d1f0d9bf67fc01059e34562652.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculating the Determinant Triangular Matrix only requires diagonal elements.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see above that for triangular matrices (upper/lower), we need only
    the diagonal elements to calculate the determinants, so the runtime complexity
    is linear.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider the lower triangular matrix where *a[i][j]*=0; *j* > *i*. We
    impose a very similar concept in Autoregressive flows.
  prefs: []
  type: TYPE_NORMAL
- en: We consider a *D* dimensional vector *u* and it goes through 1,2,…*K* transformations
    just as before. The idea of Flow-based models is to use a transformation (or chain
    of transformations) *ϕ* operating on a real vector *u* sampled from p_u(u). Before
    in the basics of [Bijection & Diffeomorphism](https://medium.com/towards-data-science/getting-started-with-normalizing-flows-linear-algebra-probability-f2b863ff427d),
    we discussed that when *u as a D dimensional vector gets transformed to x via
    K diffeomorphisms, we are saying that the base distribution is D dimensional and
    so will the final distribution (x) be. With this, we impose our Autoregressive
    condition to get a triangular matrix for log-det-Jacobian calculation as below:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/656e9be26e4582a8408d23d6f7f6192b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Eq. 4: The autoregressive condition (eq. on left), gives rise to a triangular
    log-det jacobian matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have ever used ARIMA models for time series analysis, then you know
    that the *autoregressive* term suggests the time series is based on past values.
    So we can constraint sequential data [*x*1, *x*2, …, *xD*] where each output (at
    a particular step) only depends on the observed values before and not the future
    ones. In a more mathematical notation, this would be the probability of observing
    *xi* conditioned on *x1*,…,*xi*−1 and the product of these conditional probabilities
    gives us the probability of observing the complete sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c406f35f47c2ff9c18919c47638c86d6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Eq. 5: Product of conditional probabilities gives the probability of observing
    the full data X.'
  prefs: []
  type: TYPE_NORMAL
- en: The modeling of this conditional density is of our choice and several proposals
    have been made from simple univariate Gaussian to even a neural network. Let’s
    discuss a few of the popular ones!
  prefs: []
  type: TYPE_NORMAL
- en: 'Masked Autoregressive Flow (MAF):'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For MAF, the conditionals as described above in Eq. 5 would be taken as a simple
    Normal distribution as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8a6b5846d54ab4d3425f6dcd4da5659.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Eq. 6: Conditionals above in Eq. 5 are assumed to be simple Gaussians'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s also possible to generate new data starting from the base distribution
    *u* as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38ee95250c026eaff43457b2270cebc3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Eq. 7: Generating new points given the base distribution (u), i.e. a set of
    random numbers'
  prefs: []
  type: TYPE_NORMAL
- en: 'The above equation tells us another way to think about Autoregressive models
    as a transformation *f* from the space of random numbers (*u*) to the data *x*.
    Since the transformations are affine (scale & shift), to get back our base variables
    *u_i*, we don’t need to invert the functions themselves. This is also mentioned
    in MADE paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db54a75149303105e80444256727dc71.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Eq. 8: Invert transformation from Eq. 7 to get back the base variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '**This is extremely important for training strategy as we do not need to explicitly
    calculate the inverse of the functions *f_αi*, *f_μi*, and just need to evaluate
    them once (say during forward pass), we can use non-invertible functions like
    RELU.**'
  prefs: []
  type: TYPE_NORMAL
- en: There’s an excellent pictorial representation of the forward pass (and the inverse)
    MAF is given in [*Eric Jang’s blog*](https://blog.evjang.com/2018/01/nf2.html)(Check
    the references)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7706871f9aff789f898316e9a9a1b85d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 2: The forward pass of MAF. Source: [Eric Jang’s Amazing Blog](https://blog.evjang.com/2018/01/nf2.html)
    on Normalizing Flows.'
  prefs: []
  type: TYPE_NORMAL
- en: These are the basis of [Masked AutoRegressive Flow for Density Estimation](https://arxiv.org/pdf/1705.07057.pdf)
    paper and hopefully, this Normalizing Flows series is helping you to decipher
    large chunks of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inverse Autoregressive Flow (IAF):'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The transformation rules for IAF are also clearly explained in the [MAF](https://arxiv.org/pdf/1705.07057.pdf)
    paper. The main difference from MAF in IAF is that for calculating the scale and
    shift variables (for affine transformation) we use the random variables (*u*)
    instead of the data variables (*x*). Below are the transformation rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a1bd9784aa7adbf94bb3f64adc7155a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Eq. 9: Compare this to Eq. 7 to see the difference between MAF and IAF'
  prefs: []
  type: TYPE_NORMAL
- en: The uncanny similarity between MAF and IAF is hard to miss; inverse of the IAF
    is the forward pass of the MAF!
  prefs: []
  type: TYPE_NORMAL
- en: 'Train MAF with TensorFlow Probability:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After the basics, now we are ready to use the TensorFlow Probability to implement
    MAF and train it to produce a particular distribution. Let’s make use of `sklearn.datasets`
    , specifically the `make_circles` dataset as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Load Dataset:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b93a370d0081b43605a2051e75255d35.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 3: Our Target Distribution [Source: Author’s Notebook]'
  prefs: []
  type: TYPE_NORMAL
- en: 'This will be our target distribution, datapoints distributed in circular manner.
    Our target will be starting from a random distribution to reach this orderly distribution
    using MAF. In TensorFlow Probability, it is fairly easy to implement MAF as it
    exists as a bijector called `MaskedAutoregressiveFlow` . Within this bijector,
    for the affine function (shift and scale), we can use another bijector `AutoregressiveNetwork`
    that implements the [Masked AutoEncoder for Density Estimation (MADE)](https://arxiv.org/abs/1502.03509)
    architecture, which the authors suggested for a computationally inexpensive way
    to obtain joint probabilities from a single pass through an autoencoder. Let’s
    see the implementation below:'
  prefs: []
  type: TYPE_NORMAL
- en: Forward Pass through the MAF w base distribution as Normal
  prefs: []
  type: TYPE_NORMAL
- en: We started with a normal distribution and define the MAF function where the
    affine transformation is defined by the MADE architecture. The MADE architecture
    contains 2 hidden layers with 32 units each and ‘Relu’ activation. Following the
    previous post, where we describe how to transform the distribution using `TransformedDistribution`
    , we use the Normal distribution as the base and MAF as a bijector. Finally, we
    plot the contour plot of the probability of the transformed distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1da180ae07a8e1222c53d1b37fb9102.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 4: Starting from a Normal distribution we pass it through MAF where the
    affine transformation is provided by MADE structure (with Relu as activation).
    Source: Author’s notebook'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7ae0c6dbfd245653e8b9dbce1ad5660.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 5: Same as fig. 4, but instead of Relu as an activation we use sigmoid
    here. Source: Author’s notebook.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we define the forward pass, we now are ready to train the Flow model.
    We are minimizing the negative log-likelihood and let’s just start with only one
    bijector (MAF network) to train:'
  prefs: []
  type: TYPE_NORMAL
- en: Training Loop with only bijection operation (MAF)
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected, once we plot the trained distribution, it’s easy to see that the
    result is far away from the true distribution. We sample from the trained distribution
    to plot the figure below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a013f0df2f7bbc04a9d7e63b5ffedbee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 6: Starting from a random distribution (middle), using only bijector,
    we can’t replicate the true distribution. Source: Author’s notebook'
  prefs: []
  type: TYPE_NORMAL
- en: 'But we also know that the idea of the flow-based models is to chain bijectors
    to transform from a simple to a complicated distribution. Here instead of only
    1 bijector, now we chain 4 bijectors (MAF) and minimize the negative log-likelihood
    of the final distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The permutation part ensures that the different dimensions of the *D* dimensional
    data (here *D*=2) influence each other. If the ordering of the dimensions never
    changes, it greatly reduces the expressive power of chaining bijectors in normalizing
    flows. While chaining the bijectors, we discard the last permutation as it is
    irrelevant in training (no other operation follows that permutation). With this
    more expressive model, we can expect something interesting! Let’s see the contour
    plot of the probability density of the trained distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89e36272b9caa05b0179c0de7a4f6460.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 7: Looks pretty similar to our target data (fig. 3)!! Source: Author’s
    notebook!'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is pretty cool, but also we can plot the sample distribution as our base
    normal distribution passes through these 4 MAF bijectors, let’s see:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ddb43d602eaf8898859767c92731329e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 8: Transforming from random samples to circular samples by chaining 4
    MAF bijectors. Source: Author’s notebook.'
  prefs: []
  type: TYPE_NORMAL
- en: We started from the very basics of [Normalizing Flows](https://medium.com/towards-data-science/getting-started-with-normalizing-flows-linear-algebra-probability-f2b863ff427d)
    i.e. diffeomorphism, and transformation rules for probability distribution etc.
    Then we used TensorFlow probability library to implement some of those concepts
    to transform probability distributions like from Normal to Bi-Modal in the [second
    post](https://medium.com/towards-data-science/transforming-probability-distributions-using-normalizing-flows-bcc5ed6ac2c9).
    Finally, we went through the basics of state of the art autoregressive flow models
    and why triangular matrices are important in this regard. Finally, we implemented
    MAF using TensorFlow and saw an example of training chained MAFs to transform
    normal to a bit more complex distribution. I hope this helps you to get started
    with flow-based models and also take steps towards diffusion models!!
  prefs: []
  type: TYPE_NORMAL
- en: '***If you’re interested in further fundamental machine learning concepts, you
    can consider joining Medium using*** [***My Link***](https://saptashwa.medium.com/membership)***.
    You don’t pay anything extra but I’ll get a tiny commission. Appreciate you all!!***'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@saptashwa/membership?source=post_page-----9c361cd1354c--------------------------------)
    [## Join Medium with my referral link - Saptashwa Bhattacharyya'
  prefs: []
  type: TYPE_NORMAL
- en: More from Saptashwa (and many other writers on Medium). Your membership fee
    directly supports Saptashwa and other…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@saptashwa/membership?source=post_page-----9c361cd1354c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [Masked Autoregressive Flow for Density Estimation](https://arxiv.org/abs/1705.07057):
    Papamakarios, G. et.al.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Density Estimation Using RealNVP](https://arxiv.org/abs/1605.08803): Dinh,
    L. et.al.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [Normalizing Flows: Part 2](https://blog.evjang.com/2018/01/nf2.html);
    Blog by Jang, E.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [Flow Based Models](https://lilianweng.github.io/posts/2018-10-13-flow-models/);
    Blog by Weng, L.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [TensorFlow Probability Library: Specially MAF](https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/MaskedAutoregressiveFlow).'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Notebook for Codes Used Here: [My GitHub](https://github.com/suvoooo/Learn-TensorFlow/blob/master/TF-Proba/Norm-Flows/NormFlows_AutoReg.ipynb)'
  prefs: []
  type: TYPE_NORMAL
