- en: 'Green AI: Methods and Solutions to Improve AI Sustainability'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/green-ai-methods-and-solutions-to-improve-ai-sustainability-861d69dec658](https://towardsdatascience.com/green-ai-methods-and-solutions-to-improve-ai-sustainability-861d69dec658)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A technical look at a long overdue topic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://pecciaf.medium.com/?source=post_page-----861d69dec658--------------------------------)[![Federico
    Peccia](../Images/48ad8401c28e87717718f58336cc64cf.png)](https://pecciaf.medium.com/?source=post_page-----861d69dec658--------------------------------)[](https://towardsdatascience.com/?source=post_page-----861d69dec658--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----861d69dec658--------------------------------)
    [Federico Peccia](https://pecciaf.medium.com/?source=post_page-----861d69dec658--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----861d69dec658--------------------------------)
    ·9 min read·Jun 26, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e26e571fe94f4badd859d8df3d7613de.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Benjamin Davies](https://unsplash.com/@bendavisual?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you opened this article, you have probably heard about the current controversy
    regarding the safety and trustability of current Large Language Models (LLMs).
    The open letter signed by well-known names in the computer science world like
    Steve Wozniak, Gary Marcus and Stuart Russel presented their concerns on this
    matter and asked for a 6-month stop in the training of LLMs. But there is another
    topic which is slowly gaining a lot of attention, which will perhaps motivate
    another open letter in the near future: energy consumption and carbon footprint
    of the training and inference of AI models.'
  prefs: []
  type: TYPE_NORMAL
- en: It is estimated that *only* the training of the popular GPT-3 model, a 175-billion
    parameter LLM, emitted approximately 502 tonnes of CO2 [1]. There are even [online
    calculators](https://mlco2.github.io/impact/) available to estimate the emissions
    of training a particular model. But the training step is not the only one consuming
    energy. After training, during the inference phase, an AI model is executed thousands
    or millions of times per day. Even if each execution consumes a small amount of
    energy, the accumulated consumption over weeks, months, and years can become a
    huge problem.
  prefs: []
  type: TYPE_NORMAL
- en: This is why the concept of Green AI is becoming increasingly popular. Its main
    focus is to find solutions and develop techniques to improve the sustainability
    of AI, by reducing its energy consumption and carbon footprint. In this article,
    I aim to present an overview of some techniques and methodologies that are being
    actively researched, that can be used to improve this concept, and that are not
    usually discussed in an accessible manner. At the end of this article, you will
    find resources and references related to the topics discussed.
  prefs: []
  type: TYPE_NORMAL
- en: Although this article is focused on the technical methodologies enabling energy
    savings when deploying AI algorithms, it is important to have a general grasp
    of them even if you are *not* a researcher. Are you the person responsible for
    training the AI algorithm of your company? Well, perhaps you can keep some optimizations
    in mind during the training that will improve the energy consumption of the algorithm
    once deployed. Are you perhaps the person responsible for selecting the hardware
    on which your algorithm will be deployed? Then keep an eye open for the concepts
    mentioned in this article, as they can be a sign of cutting-edge, optimized hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Computer architecture basics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to understand this article it is essential to have a basic understanding
    of computer architecture, and how the software and the hardware interact with
    each other. This is a very complex topic, but I will try to provide a quick summary
    before entering the main part of the article.
  prefs: []
  type: TYPE_NORMAL
- en: 'You have probably heard about the bit, the most simple information unit inside
    any computer, and the reason the digital world exists. Bits can only take two
    states: 0 or 1\. A group of 8 bits is called a byte. For the purposes of this
    article, we can think about any computer architecture in terms of 2 hardware components
    which manipulate and store these bytes: the computation units and the memory.'
  prefs: []
  type: TYPE_NORMAL
- en: The computation units are the ones responsible for taking a number of bytes
    as input and generating another group of bytes as output. For example, if we want
    to multiply 7 x 6, we would insert the bytes representing 7 into one of the inputs
    of a multiplier and the bytes representing 6 in the other input. The output of
    the multiplier would give us the bytes representing the number 42, the result
    of the multiplication. This multiplication takes a certain amount of time and
    energy until the result is available at the output of the multiplier.
  prefs: []
  type: TYPE_NORMAL
- en: The memory is where the bytes are stored for future use. Reading and writing
    bytes from memory (also called “accessing” the memory) takes time *and* energy.
    In computer architecture, there are usually multiple “levels” in the memory hierarchy,
    with the ones closer to the computations units having the fastest access times
    and the less energy consumption per byte read, and the ones further away being
    the slowest and more energy-demanding memories. The main idea behind this hierarchical
    organization of memory is data reuse. Data used very often is brought from the
    last memory level into the closest one and reused as many times as possible. This
    concept is called “caching”, and these faster and closest memories are called
    L1 and L2 caches.
  prefs: []
  type: TYPE_NORMAL
- en: The software is responsible for orchestrating the movement of data from the
    memory into the computation units, and then storing the results into the memory.
    As such, software decisions can really affect the energy consumption of a system.
    For example, if the software requests data that is not available in the L1 cache,
    the hardware first needs to fetch it from the L2 level or even from the last level,
    incurring time delays and more energy consumption.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques for Green AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that the computer architecture basics are established, we can focus on
    the specific techniques and methodologies used in Green AI. These are grouped
    into two distinct categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Hardware optimizations, like voltage/frequency scaling or approximate computing.
    These techniques work on the actual physical design and properties of the electronic
    circuits.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Software optimizations, like pruning, quantization, fine-tuning, and others.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'DVFS: Dynamic voltage and frequency scaling'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The power consumption of standard silicon-based electronic circuits is directly
    related to the voltage used in the circuit and its working frequency. Under the
    same operating conditions, if any of these parameters is reduced, the same happens
    to the power consumption. Could we exploit this behaviour to make the execution
    of AI algorithms *greener*?
  prefs: []
  type: TYPE_NORMAL
- en: Of course! Imagine we have a small embedded device connected to a battery, receiving
    multiple requests (each one with its own criticality and constraints), processing
    them with an AI algorithm, and then sending the results back. We want the processing
    of the AI algorithm to consume as less energy as possible so that we can keep
    the battery running as long as possible, right? Could we actually dynamically
    change the voltage and the operating frequency of the device when less critical
    tasks arrive, and then return them to normal operating conditions when critical
    tasks need to be processed?
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the device executing the AI algorithm, this is a completely valid
    option! In fact, this is an active research field. If you are interested in reading
    more about it, I recommend you to take a look at “AutoScale: Energy Efficiency
    Optimization for Stochastic Edge Inference Using Reinforcement Learning” by Kim
    [2] or “Multi-Agent Collaborative Inference Via DNN Decoupling: Intermediate Feature
    Compression and Edge Learning” by Hao [3], which provide good examples of how
    this technique can be used to reduce the energy consumption of AI algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Approximate computing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When executing a mathematical operation on a CPU or a GPU, we are usually used
    to expecting exact results for the requested calculation, right? This is normally
    the case when using consumer-type hardware. Given that multiplication is one of
    the most used mathematical operations in an AI algorithm, we expect to obtain
    an exact result when multiplying two integer numbers, and a really good approximation
    when multiplying two floating point numbers (this approximation is usually so
    precise, it is not a problem for basic user programs). Why should we even *consider*
    the possibility of inserting two integer numbers and NOT obtaining the correct
    mathematical result?
  prefs: []
  type: TYPE_NORMAL
- en: 'But a new approach is being actively researched in the last few years. The
    question is simple: is there a way to design simpler multipliers, which consume
    less physical area and less energy, by sacrificing accuracy in the multiplication
    result? But more importantly, can these new multipliers be used in real applications,
    without significantly hurting their performance? The answer to both of these questions
    is actually yes. This is the computing paradigm known as approximate computing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is absolutely fascinating! There are already works presenting approximate
    multipliers that are able to provide exact results for the multiplication of two
    integers which only provide incorrect results for a reduced number of input combinations,
    but are able to provide energy reductions in the order of 20% for the execution
    of entire models. If you are interested in this incredible technique, I encourage
    you to take a look at “Approximate Computing for ML: State-of-the-art, Challenges
    and Visions” by Zervakis [4], which provides a nice overview of the specific works
    focused on this topic.'
  prefs: []
  type: TYPE_NORMAL
- en: Pruning and quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For people familiar with the training of AI algorithms, especially Neural Networks,
    these two techniques should sound familiar. For those not familiar with those
    terms, the concepts are really worth reading about.
  prefs: []
  type: TYPE_NORMAL
- en: Pruning is a method based on the idea that there is a lot of redundancy in the
    parameters of a Neural Network, which are the ones containing the knowledge of
    the network. This is why a lot of them can be removed without actually hurting
    the prediction of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization means to represent the parameters of a network using fewer bytes.
    Remember how we said that computers represent numbers using a certain amount of
    bytes? Well, usually networks are trained using a representation called “floating
    point”, where each number can be 4 or 8 bytes long. But there are techniques to
    actually represent these parameters using only 1 byte (the “integer” representation)
    and still have a similar or sometimes even equal prediction quality.
  prefs: []
  type: TYPE_NORMAL
- en: I am sure you are already imagining how these two techniques help reduce the
    energy consumption of a Neural Network. For pruning, if fewer parameters are needed
    to process one input, two things happen that improve the energy consumption of
    the algorithm. First, fewer computations need to be executed in the computation
    units. Second, because there are fewer computations to make, less data is read
    from memory. For quantization, multiplying two numbers represented as integers
    using only one byte requires a much smaller and simple hardware multiplier, which
    in turn requires less energy to do the actual multiplication. Finally, if the
    size of each parameter is reduced from 8 bytes to 1 byte, this means that the
    amount of data that needs to be read from memory is also 8 times smaller, thus
    greatly reducing the energy consumption needed to process one input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Do you want to read more about it? Take a look at “Lightweight Parameter Pruning
    for Energy-Efficient Deep Learning: A Binarized Gating Module Approach” by Zhi
    [5] or “Pruning for Power: Optimizing Energy Efficiency in IoT with Neural Network
    Pruning” by Widmann [6] for examples of current work on the topic.'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given the closed nature of a lot of the latest LLMs, a significant amount of
    computer power is being used just to *replicate* the results of these models.
    If these models were opened to the public, the technique known as fine-tuning
    could be applied to them. This is a method by which only some of the parameters
    of a pre-trained model are modified during a fine-tuning training procedure, to
    specialize the network for a particular task. This process usually requires fewer
    training iterations and thus consumes less energy than retraining a whole network
    from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: This is why opening these models to the public would help not only the people
    trying to build products with them but also the researchers that are retraining
    them from scratch and thus consuming a lot of energy that could be saved.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I hope you found these techniques and methods as fascinating as I found them.
    It is reassuring and comforting to know that there are people actively researching
    these techniques and trying to improve as much as possible on a topic as important
    as energy savings and carbon footprint.
  prefs: []
  type: TYPE_NORMAL
- en: But we cannot sit down and relax, offloading the responsibility of finding optimized
    solutions to the researchers working on these topics. Are you starting a new project?
    Check first if you can fine-tune a pre-trained model. Is your hardware optimized
    to run pruned algorithms, but you don't have the expertise to efficiently apply
    this technique? Go out there, spend some time learning it or find someone who
    already has the skill. In the long run, it will be worth it, not only for you
    and your company but for our planet Earth as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to follow me on [Twitter](https://twitter.com/PecciaF) or [LinkedIn](https://www.linkedin.com/in/fpecc/)
    and let me know what you think of this article, or [buy me a coffee](https://www.buymeacoffee.com/pecciaf)
    if you really liked it!
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] [“Estimating the carbon footprint of BLOOM, a 176B parameter language model”](https://arxiv.org/pdf/2211.02001.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [“AutoScale: Energy Efficiency Optimization for Stochastic Edge Inference
    Using Reinforcement Learning”](https://microarch.org/micro53/papers/738300b082.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [“Multi-Agent Collaborative Inference Via DNN Decoupling: Intermediate
    Feature Compression and Edge Learning”](https://arxiv.org/abs/2205.11854)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [“Approximate Computing for ML: State-of-the-art, Challenges and Visions”](http://slam.ece.utexas.edu/pubs/aspdac21.AxC.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[5] “Lightweight Parameter Pruning for Energy-Efficient Deep Learning: A Binarized
    Gating Module Approach”](https://arxiv.org/abs/2302.10798)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[6] “Pruning for Power: Optimizing Energy Efficiency in IoT with Neural Network
    Pruning”](https://link.springer.com/chapter/10.1007/978-3-031-34204-2_22)'
  prefs: []
  type: TYPE_NORMAL
