# 线性代数的全景视角：方程组、线性回归和神经网络

> 原文：[https://towardsdatascience.com/a-birds-eye-view-of-linear-algebra-systems-of-equations-linear-regression-and-neural-networks-fe5b88a57f66](https://towardsdatascience.com/a-birds-eye-view-of-linear-algebra-systems-of-equations-linear-regression-and-neural-networks-fe5b88a57f66)

## 谦逊的矩阵乘法及其逆几乎是许多简单机器学习模型中的核心内容

[](https://medium.com/@rohitpandey576?source=post_page-----fe5b88a57f66--------------------------------)[![Rohit Pandey](../Images/af817d8f68f2984058f0afb8fd7ecbe9.png)](https://medium.com/@rohitpandey576?source=post_page-----fe5b88a57f66--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fe5b88a57f66--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fe5b88a57f66--------------------------------) [Rohit Pandey](https://medium.com/@rohitpandey576?source=post_page-----fe5b88a57f66--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fe5b88a57f66--------------------------------) ·18 分钟阅读·2023年12月28日

--

![](../Images/3c6e139e72e16ccb0e834afec721d153.png)

图片由 midjourney 提供

这是进行中的线性代数书籍《线性代数的全景视角》的第四章。到目前为止的目录：

1.  [第一章：基础](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-the-basics-29ad2122d98f)

1.  第二章：[映射的度量——行列式](https://medium.com/p/1e5fd752a3be)

1.  第三章：[为什么矩阵乘法是这样？](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-why-is-matrix-multiplication-like-that-a4d94067651e)

1.  第四章（当前）：方程组、线性回归和神经网络

1.  第五章：[秩和零化以及为什么行秩等于列秩](/a-birds-eye-view-of-linear-algebra-rank-nullity-and-why-row-rank-equals-column-rank-bc084e0e1075)

本博客中的所有图片，除非另有说明，均由作者提供。

# I) 引言

现代人工智能模型利用高维向量空间来编码信息。并且***我们*** 用于推理高维空间及其映射的工具是线性代数。

在这一领域内，矩阵乘法（以及它的逆）几乎是构建许多简单机器学习模型所需的全部内容。这也是为什么花时间深入理解它是一个很好的投资。这就是我们在[第三章](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-why-is-matrix-multiplication-like-that-a4d94067651e)中所做的。

这些简单模型，虽然在自身之内有用，却构成了更复杂的机器学习和人工智能模型的基础，这些复杂模型具备最先进的性能。

在这一章中，我们将介绍一些这些应用（从线性回归到初级神经网络）。

但首先，我们需要转到最简单的情况和最简单的模型——当数据点的数量等于模型参数的数量时，即求解线性方程组的情况。

# II) 线性方程组

我们终于来到了线性代数的核心（在本书的背景下）。求解线性方程组是我们最初发现线性代数的方式，这个领域的大多数概念的动机在这个应用中有着深远的根基。

让我们从简单的一维情况开始。除法的概念根植于一维线性方程中。

*ax = b*

这个方程的意思是，“哪个数字在乘以*a*后得到*b*”。解决方案定义了标量除法：

*x = b/a* _(1)

这是在一维的情况下，即*x*。当进入多维时，最有趣的事情发生了。所以我们不仅有一个变量*x*，还有*n*个变量（*x_1, x_2, x_3, …, x_n*）。

一旦进入多维空间，线性代数就会出现。

![](../Images/2f774d782fa426023a1b05562e31b18e.png)

当问题变得多维时，线性代数就会出现。图像来自midjourney。

现在，我们的方程应包括*n*个变量，即*x_1, x_2, x_3, …, x_n*。就像*x*之前有系数*a*一样，这次每个*x_i*都有自己的系数*a_i*。

![](../Images/85b15880feff61042a6d27878e9c2b6a.png)

但与一维情况不同，这个方程不足以求解*x*，因为它不能唯一地确定*x*。例如，我们可以随意选择*x_2, x_3,…, x_n*（例如：全部为0），只有这样我们才能得到唯一的*x_1*值。如果我们已经知道了*x_1, x_2, …, x_n*的值，并且想以方程组的形式传达它们，它们会看起来像这样：

![](../Images/bac0590dc865ba0985cf0515d38be4d3.png)

因此，我们需要*n*个方程（等于变量的数量）。现在可以通过“混合”上述方程来创建任何一般系统，即取它们的线性组合。这允许添加任意两个方程，并将任意方程乘以任意标量。这些操作显然不会改变系统的解（或其存在性问题）。

例如，我们可以将第二个方程乘以三加到第一个方程中，得到：

![](../Images/d8f833bb4004d7c74524b1038ba05380.png)

然后我们可以用这个方程替换第一个方程（*x_1=4.6*），得到的方程组仍然有相同的解（因为我们可以通过从新的第一个方程中减去三倍的第二个方程来撤销我们所做的更改）。如果我们以这种方式非常彻底地“混合”，随机替换其中一个方程，并重复多次，我们将得到*n*个方程，每个方程涉及所有*n*个变量的倍数。

![](../Images/4ee12009646ef9d2cba54bbb94bbc80c.png)

系数 *a_{ij}* 看起来非常像一个方阵的元素。确实，根据我们在第3章中学到的矩阵向量乘法（见III-A节中的动画-1），我们可以将方程组表示为：

![](../Images/6f6ee90f99fcf1a6e9349e9a75b502e8.png)

方程（1）线性方程组的矩阵形式。

就像在方程（1）中我们取了乘法逆来得到标量变量 *x (x=b/a)* 一样，这里我们取矩阵乘法的逆来得到向量 *x*。

![](../Images/df7466a98205a67f83917570fcf70642.png)

我们已经达到了一切开始的标志性方程。线性代数的起点。线性方程组。计算逆矩阵及其相关操作有一个完整的科学背景，我们将在后续章节中讨论。

注意，*A* 矩阵的行数是系统中的方程数，列数是变量数。

从几何上讲，每个方程在空间中都是一个维度降低的超平面。考虑以下方程组：

![](../Images/7fea6e24adffbec2cf05a7a6a1e86c97.png)

这个方程组的解是*(x=0, y=0, z=1)*。由于这个方程组中有三个变量（*x, y* 和 *z*），因此矢量空间是三维的，如下图-1所示。请参阅该图以获取后续讨论。

*x=0* 方程对应于这个三维空间中的黄色超平面。由于该方程表示增加一个约束，满足该方程的空间的维度降低一个（*3–1=2*）。类似地，*y=0* 方程对应于蓝色超平面，也二维。现在我们有两个方程，因此两个约束。两个方程都满足的子空间的维度将降低两个，变成*3–2=1*维。一维子空间就是一条直线，实际上我们得到的是绿色直线图-1。最后，当我们加入第三个方程 *x+y+z=1*，它由粉色平面表示。现在我们有三个方程/约束。它们将三维空间的维度限制为3。因此，我们剩下的维度是*3–3=0*（即一个点），确实，我们得到红色点，它是同时满足系统中所有三个方程的唯一矢量空间元素。

![](../Images/9eaac84a66add4954150e439f1952df8.png)

图-1：三变量的方程组。空间是三维的。每个方程切割出一个低一维的超平面。两个方程的组合去除两个维度，依此类推。图像由作者提供。

在上面的例子中，我们有 *n* 个方程和 *n* 个变量。一般来说，它们不一定相同。假设有 *n* 个方程和 *m* 个变量。从上面的图可以清楚地看出：

+   当*n<m*时，我们有更多的变量而不是方程，系统具有无限解。可以把方程看作是要求。要求指定得不够多，可能的解就会很多。

+   当*n>m*时，我们将有更多的方程而不是变量。现在有了太多的约束条件，无法在向量空间中找到满足所有约束的点。

+   所以当*n=m*时，我们应该总是有一个唯一解？不完全是，还有一些情况可能会出错（所有三种情况）。

## II-A) 问题发生的原因

**一致性**

使任何系统没有解的一个因素是矛盾。例如，第一个方程是*2x+y=3*，第二个方程是2*x+y = 5*。现在，无论我们再添加多少个方程，都无法同时满足这两个方程（它们相互矛盾）。任何包含这两个方程的系统都是不一致的。

**依赖性**

接下来，系统可能会欺骗我们，看起来好像有比实际更多的方程式。最明显的情况是如果我们简单地复制其中一个方程。这将使系统增加一个方程，使得技术上我们现在有了*m+1*个方程。然而，很明显，我们并没有真正增加任何新信息。我们新增的方程是多余的。这被称为依赖系统，因为我们的一些方程没有带来任何新的信息，而是“依赖”于其他方程。如果我们直接抄袭一个方程，那将很容易被发现。但可以以更隐蔽的方式做到这一点。我们可以通过对几个方程进行线性组合来创建一个新的方程。线性组合可以做到几乎不被发现混入了一个“冒牌方程”。当然，一旦这样的依赖方程被加入到系统中，就很难分辨哪个是“冒牌”方程，就像下面的三只蜘蛛侠一样。

![](../Images/caf534a1f63b31cf151d423005032d8d.png)

两个鲍勃。但只能有一个。另一个是冒牌的。很难判断哪一个。就像通过对其他方程进行线性组合引入的依赖方程一样。图像来源：midjourney。

一致性和依赖性这两个问题在*A*矩阵（来自方程（1））方面表现相同。向量*b*然后决定系统是否不一致（没有解）或依赖（无限多个解）。发生的情况是，*A*的某些行变成了其他行的线性组合。

## II-B) 数据分析的情况

对我们最感兴趣的情况是建立在数据之上的模型。如果你有独立收集的数据点，并且数据收集过程是随机的（就像数据收集应该是的那样），几乎可以肯定你的数据矩阵的行不会线性相关（在概率术语中，"[几乎肯定](https://en.wikipedia.org/wiki/Almost_surely)"）。所以，我们不必担心得到一个不一致或相关的系统。

此外，我们通常有比列数（变量/特征）多得多的行（数据点）。所以，矩阵将是“瘦长的”，其中 *n>m*。

![](../Images/8f8a0257f0933335933906c026e2a415.png)

一个高而瘦的矩形数据矩阵，行数多于列数。图片由 midjourney 提供。

我们正处于无解方程组的领域。如果我们选择任何 *m* 个 *n* 个方程并删除/忽略其余的方程，那么我们将回到 *m=m* 的情况，这时方程数和变量数相等，现在将会有一个唯一的解。

因此，超平面对应于系统中方程的总数是 (*n* 选择 *m*) 个点。这些点中没有一个点会同时位于所有超平面上。

尽管没有一个点同时满足所有方程，我们仍然可以问：“在整个向量空间中哪个点最接近满足所有方程”。这就是线性回归的作用所在。

## **III) 线性回归**

![](../Images/9b61ce9e94a1b0d7305e2e036150475e.png)

线性回归绘制一个最接近你数据点的线性超平面。就像忍者尝试以一种尽可能接近最多叶子的方式挥动他的剑一样。图片由 midjourney 提供。

首先，在线性回归的背景下，矩阵和向量的命名与方程组有所不同。现在，系数矩阵 *A* 变成了包含数据的矩阵，我们称之为 *X*。这个矩阵的每一行，即行向量 *x_i*，是一个数据条目。向量 *x* 包含了方程 (1) 中的未知变量。现在，未知变量是线性回归系数，我们用 *𝛽* 表示。最后，线性系统的右边向量 *b* 变成了包含因变量的向量，我们称之为 *y*。因此，在线性回归的背景下，方程 (1) 变成了：

![](../Images/d8509af2748e53c64e7a7a3027387cb1.png)

Eq (2)：线性回归的基本方程。图片由作者提供。

展开后，这个方程看起来如下：

![](../Images/dd16e8cb07bddee8c058fe6df57d0683.png)

注意第一列的*1*。这对应于常数项。例如，在单变量的情况下，如果没有那一列，我们的模型将会是：*y = m.x.* 这将排除像 *y=x+1* 这样的直线。如果要考虑像 *y = mx+c* （我们在大多数情况下确实需要）的直线，我们需要那一列*1*。

就像线性方程组一样，我们希望对两边进行*X*的乘法逆运算，找到*𝛽*。如图所示：

![](../Images/e4b7b4373714516a8aff0531a10a9373.png)

这个方程没有任何意义。矩阵 *X* 是矩形的，所以它不能被逆转。

不幸的是，这没有意义。只有方阵才可以逆转。矩形矩阵则不行。我们的数据矩阵 *X* 是一个矩形矩阵，行数（*n*）大于列数（*m*）。

一种理解这一点的方法是，它表示一个方程数量多于变量数量的线性系统，因此第二部分的论点适用。就矩阵背后的线性映射而言，对于一个“瘦长”的矩阵（如 *X* 所示），行数多于列数，它不会是一个一对一的映射（同一空间中的多个点会映射到第二个空间中的同一点，这种情况是不允许的）。

是否有一种“技巧”可以让乘以 𝛽 的矩阵变成方阵？矩阵 *X* 目前是 *n⨉m*。如果我们乘以另一个 *m⨉n* 的矩阵 *U*，那么得到的矩阵 *V* 将是方阵 *m⨉m*。

![](../Images/c724f4c232c633cddaa7f1681c1d03fa.png)

通过对方程（2）进行 *U* 的左乘，我们可以逆转结果矩阵并得到 𝛽。

![](../Images/538314b27c122ecfc4158219482ec859.png)

现在，问题是，我们从哪里得到这个 *U* 矩阵（*m⨉n*）？我们拥有的是 *X* 矩阵，*n⨉m*（即数据本身）。我们可以做的一件事是将 *X* 矩阵翻转，使其行变成列，列变成行。这种对矩阵的操作称为转置，记作 X^T，如下图所示。

![](../Images/ce5afab467995b5e57c21423cef1568a.png)

矩阵的转置。图片来源：[维基百科](https://en.wikipedia.org/wiki/Transpose) 关于转置的文章。

因此，我们可以用 *X* 的转置来替代 *U*。这将得到：

![](../Images/4df291644e59c673764233817454e49c.png)

方程（3）：线性回归系数。

我们现在已经找到了回归模型的系数。如果我们得到一个新的数据点，*x_new*（以行向量的形式），我们可以将其与*𝛽*进行点积，从而获得相应的*y*。

![](../Images/deeca6076c56e7dd972bbdf81b5d664c.png)

其中 𝛽 由上述方程（3）给出。现在，我们提供了用*X^T*替代*U*的动机，因为这是一个显而易见的选择，符合我们所需的维度。然而，相同的公式还有更强的数学动机。

## III-A) 数学动机

我们需要为𝛽（方程（3）中的那个）提供一个具体的值。因此，让我们思考不同𝛽值的情况。如动画-1、第三章III-A节（[矩阵乘法](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-why-is-matrix-multiplication-like-that-a4d94067651e)）中所述，我们可以将 *X*𝛽 理解为将 *X* 的列向量拆开并通过线性组合将它们组合成一个单一的列向量。更具体地说，我们将 *X* 的第一个列向量乘以𝛽向量的第一个元素，将第二个列向量乘以第二个元素，以此类推，然后将所有结果相加。因此，当我们改变𝛽向量时，我们就在探索矩阵 *X* 的“列空间”，即通过 *X* 的列向量的线性组合可以得到的所有向量的集合。然后，我们还有向量 *y*，这是我们方程（2）的右侧。就像 *X* 的列向量一样，这个向量的维度也是 *n*（数据点的数量/矩阵中的行数）。

这个向量空间（包含 *y* 和 *X* 的列向量）的维度是 *n*。另一方面，*X* 的列向量的数量是 *m*。请记住，我们的情况是 *n>>m*。所以，*X* 的列空间（由这些 *m* 个向量张成的空间）的维度是 *m*。这比 *n* 要低得多，而 *n* 是那些列向量所在的更大空间。

向量 *y* 生活在与 *n* 维度相同的更大空间中。由于列向量的数量 *m* 比 *n* 小得多，因此向量 *y* 几乎肯定不会在由 *X* 的列向量所张成的列空间中。

如果是这样，我们会有一个𝛽可以完全满足所有方程。这是不可能做到的（如前所述）。但是，我们仍然希望找到一个𝛽，使其尽可能接近向量 *y*。为此，我们需要最小化 *y* 和 *X*𝛽之间的距离。

让我们来看一个具体的例子。假设我们有一个数据集，其中 *n=3* 个数据点。在我们的回归模型中，我们选择 *m=2* 个特征。方程 *X𝛽=y* 看起来是这样的：

![](../Images/1d0925cc07a7ea9d60383892997d6628.png)

例如，矩阵 *X* 有两个列向量，*[1,1,1]* 和 *[2,5,7]*。它们存在于一个三维空间中（*n=3*）。这两个列向量在下面的图2中用蓝色绘制。由这些向量（列空间）张成的空间是二维的（*m=2*），其中一部分被粉色阴影覆盖。

![](../Images/b9b472f068d83b4bde301f5057de70bd.png)

图2：展示线性回归作为数据矩阵 X 的列空间探索。图片由作者提供。

现在，看看黑色向量、灰色向量和红色向量形成的三角形。黑色向量是 *X𝛽*，红色向量是 *y*，灰色向量是 *d*。这三者形成一个三角形，因此满足：

![](../Images/4f21e4a02d2d69b8291133b7a59ae053.png)

我们希望找到这个 *d* 向量在 *X* 的列空间（由 𝛽 控制）中最接近 *y* 的点。

我们通过将列向量的转置与其自身进行矩阵乘法来获得列向量的平方长度。

![](../Images/c61f481b7cb31beb88bf77f4ba3ec792.png)

最后，让我们对 *𝛽* 取导数并设置为 *0*。这将给出最小化向量 *d* 的平方长度的 𝛽。

![](../Images/78bbdad103f792818909ee130e02f5a9.png)

在这里，我们使用来自[矩阵宝典](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)的方程（78），[2]。这导致：

![](../Images/44d119216d0ea765d0504d35161f49b0.png)

这与方程（3）是相同的。

我们在这里用 *X* 的列空间来激励（如[1]中更详细地解释）。相同的方程也可以被激励为最小化预测和实际值向量 *y* 中的平方误差项。这种方法在[2]中涵盖。

## III-B) 在线线性回归

![](../Images/fc9238e9e85c96c937a424b6170d16bb.png)

在线线性回归。数据以恒定的流入方式每天进入。我们需要在这个过程中保持我们的线性回归模型的最新状态。图像来源于 midjourney。

到目前为止，我们把线性回归看作一个静态模型，其中数据矩阵 *X* 和对应的响应向量 *y* 已经给定。很常见的是，数据会随着时间作为一个恒定的流入。一些数据点今天可能会丢失，明天可能会更多，以此类推。我们希望我们的模型使用一个滚动窗口的数据（比如30天），并且参数每天更新一次，针对过去30天的数据。显而易见的方法是每天使用过去30天的数据应用方程（3），并每天刷新参数。但如果数据点的数量 *n*（矩阵 *X* 的行数）非常大呢？这会使方程（3）的计算变得非常昂贵。如果你考虑昨天的模型与今天的模型。由于滚动窗口，大部分数据都是相同的。第31天，我们使用了第1天到第30天的数据，而第32天，我们使用了第2天到第31天的数据。第2天到第30天的数据是共同的。第32天的模型与第31天的模型不同之处在于它不考虑第1天丢失的所有数据点，但考虑了第31天丢失的数据点。除此之外，绝大多数数据（第2天到第29天）是共同的。因此，忽略这一点并每天从头开始训练整个模型似乎是浪费的。如果我们可以将方程（3）重新表述为对 *X* 的行 *x_i* 的某些函数的求和，我们可以在新数据到来时不断增加贡献，同时减去掉出滚动窗口的旧数据的贡献。

在第 3 章中我们讨论的矩阵乘法的众多解释之一可以帮助我们做到这一点。在[第 3 章](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-why-is-matrix-multiplication-like-that-a4d94067651e) 的 III-B 节（大约动画 5）中，介绍了将矩阵乘法解释为两个矩阵行的外积之和。利用这一点，我们可以将方程 (3) 左侧的矩阵平方为：

![](../Images/63d949b23d420e4f65cf1fe5193d7e5c.png)

方程 (4) 图片由作者提供

在这里，向量 *x_i* 是矩阵 *X* 的第 *i* 行。它有一行和 *m* 列 (*1*⨉*m*)。

同样，那个方程左侧的向量可以写成：

![](../Images/4a4efd17ea90a82712d09b9b413d8494.png)

方程 (5) 图片由作者提供

由于这两个项已经以 *n* 个数据点的总和表示，我们可以随意地添加或删除来自单个数据点的贡献。我们可以基于过去 7 天内的数据点，保持方程 (4) 中的矩阵和方程 (5) 中的向量的不断更新。如果某个数据点超出了 30 天的窗口，我们可以从两个方程中减去它对应的项，如果有新的数据点进入，我们可以将其项添加到两个方程中。而且由于 *m* 很小，我们可以高效地计算 𝛽，每次更新这些项时保持其最新。这种方法也可以用来为数据点添加权重。如果（例如）你从多个来源获取数据并希望对其中一些进行加权，这将非常有用。你只需将第 *i* 个数据点的权重 *w_i* 乘到方程 (4) 和 (5) 的每一项中。

# IV) 神经网络

![](../Images/90a8651ed207be30f96827b8cacb9991.png)

神经网络架构由向量层组成。第一层是输入层，最后一层是输出层，中间的所有层都是隐藏层。图片由 Midjourney 提供。

神经网络是受生物大脑神经连接启发的机器学习模型。它们是我们追求人工通用智能的当前选择武器。所有近期的进展，从文本到图像再到对话机器人，都使用了以神经网络为核心的模型。

线性回归可以被看作是最简单的神经网络。现在我们关注推断，即在给定模型输入实例的情况下获得输出向量的过程。对于线性回归，我们将获得一个行向量 *x* 作为输入和一个单一的标量值 *y* 作为输出。参数向量 𝛽 将输入转换为输出。

![](../Images/9e1f1c9dbebdc8c1dda272454bb725ab.png)

在线性回归中，我们得到一个对应于新数据点 x_j 的向量。这与参数向量 beta 相乘以产生响应 y_j，它是一个单一的标量。图片由作者提供。

要将其扩展到神经网络，我们以两种方式进行概括。

首先，为了使模型能够输出各种有趣的内容，如图像、句子、视频和宇宙飞船，我们需要它输出的不仅仅是一个标量（如线性回归），而是一个向量。如果这个向量的维度足够高，我们期望的任何复杂响应都可以有效地嵌入其相应的向量空间中。

因此，我们需要将线性回归的向量->标量情况改为向量->向量。标量输出只是一个特例，因为它是一个一维向量。这将上述图像更改为：

![](../Images/0f8b0d9706704844d15d87a110a11969.png)

一般来说，我们希望响应 `y` 是一个向量而不是一个标量。这样，我们可以在那个向量空间中嵌入关于现实世界的复杂信息。图片由作者提供。

现在，之前的参数向量 𝛽 将需要变成参数矩阵。这是一个线性映射，将输入向量映射到输出向量。

但是，现实世界中大多数有趣的关系都是非线性的。为了适应这一点，我们首先插入了一堆中间的“隐藏层”，如下图中的蓝色部分所示。

![](../Images/f8be912112e18df8d6db0f6ff35087ae.png)

深度神经网络的结构。图中的蓝色隐藏层是模型的中间层。图片由作者提供。

以这种方式添加层本身不会改变任何东西。它仍然等同于没有任何隐藏层的原始模型。要看到这一点，请注意，下面图中的参数矩阵 *B_1, B_2, B_3, …* 只是相乘并变成另一个参数矩阵。

*B = (B_1\. B_2\. B_3.B_4)*

但这个简单的技巧使得这种方法从只能任意逼近线性映射变成可以任意逼近*任何*映射。为了使隐藏层值得我们费心，我们在每一层添加一个简单的逐元素非线性函数 *f*。这是一个简单的一维函数 *f*，它接受一个标量作为输入并返回一个标量作为输出。我们只需在乘以下一个参数矩阵 *B_j* 之前，将 *f* 应用于向量的每个元素。这个 *f* 的一个流行选择是 [Sigmoid 函数](https://en.wikipedia.org/wiki/Sigmoid_function)。

[通用逼近定理](https://en.wikipedia.org/wiki/Universal_approximation_theorem) [4] 说，这种架构可以任意逼近两个向量空间之间的任何映射（线性或非线性）。

# V) 结论

谦逊的矩阵乘法是一种极其强大的工具。你会在最简单的模型以及最复杂、最前沿的模型中找到它。在本章中，我们回顾了一些以矩阵乘法为核心引擎的简单模型。在本书的后续章节中，我们将探索更多线性代数概念，突出它们在现代 AI 模型中的作用。

如果你喜欢这篇文章，给我买杯咖啡吧 :) [https://www.buymeacoffee.com/w045tn0iqw](https://www.buymeacoffee.com/w045tn0iqw)

# 参考文献

[1] 看待线性代数的美妙方式: [https://medium.com/towards-data-science/a-beautiful-way-of-looking-at-linear-regressions-a4df174cdce](https://medium.com/towards-data-science/a-beautiful-way-of-looking-at-linear-regressions-a4df174cdce)

[2] 从最小二乘法推导线性回归: [https://stats.stackexchange.com/questions/46151/how-to-derive-the-least-square-estimator-for-multiple-linear-regression](https://stats.stackexchange.com/questions/46151/how-to-derive-the-least-square-estimator-for-multiple-linear-regression)

[3] 矩阵宝典: [https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)

[4] 通用逼近定理: [https://en.wikipedia.org/wiki/Universal_approximation_theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)
