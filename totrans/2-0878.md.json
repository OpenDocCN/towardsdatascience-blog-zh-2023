["```py\ndef load_emails_txt(path: str, split_str: str = \"From r  \") -> list[str]:\n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n        text = file.read()\n\n    emails = text.split(split_str)\n\n    return emails\n```", "```py\nemails = load_emails_txt(\"fradulent_emails.txt\")\nemails_pl = pl.DataFrame({\"emails\": emails})\n\nprint(len(emails))\n>>> 3977\n```", "```py\nFrom r  Wed Oct 30 21:41:56 2002\nReturn-Path: <james_ngola2002@maktoob.com>\nX-Sieve: cmu-sieve 2.0\nReturn-Path: <james_ngola2002@maktoob.com>\nMessage-Id: <200210310241.g9V2fNm6028281@cs.CU>\nFrom: \"MR. JAMES NGOLA.\" <james_ngola2002@maktoob.com>\nReply-To: james_ngola2002@maktoob.com\nTo: webmaster@aclweb.org\nDate: Thu, 31 Oct 2002 02:38:20 +0000\nSubject: URGENT BUSINESS ASSISTANCE AND PARTNERSHIP\nX-Mailer: Microsoft Outlook Express 5.00.2919.6900 DM\nMIME-Version: 1.0\nContent-Type: text/plain; charset=\"us-ascii\"\nContent-Transfer-Encoding: 8bit\nStatus: O\n```", "```py\nemail_pattern = r\"From:\\s*([^<\\n\\s]+)\"\nsubject_pattern = r\"Subject:\\s*(.*)\"\nname_email_pattern = r'From:\\s*\"?([^\"<]+)\"?\\s*<([^>]+)>'\n```", "```py\nemails_pl = emails_pl.with_columns(\n    # Extract the first match group as email\n    pl.col(\"emails\").str.extract(name_email_pattern, 1).alias(\"sender_name\"),\n    # Extract the second match group as email\n    pl.col(\"emails\").str.extract(name_email_pattern, 2).alias(\"sender_email\"),\n    # Extract the subject \n    pl.col(\"emails\").str.extract(subject_pattern, 1).alias(\"subject\"),\n).with_columns(\n    # In cases where we didn't extract email\n    pl.when(pl.col(\"sender_email\").is_null())\n    # Try another pattern (just email)\n    .then(pl.col(\"emails\").str.extract(email_pattern, 1))\n    # If we do have an email, do nothing\n    .otherwise(pl.col(\"sender_email\"))\n    .alias(\"sender_email\")\n)\n```", "```py\nemails_pl = emails_pl.with_columns(\n    # Apply operations to the emails column\n    pl.col(\"emails\")\n    # Make these two statuses the same\n    .str.replace(\"Status: RO\", \"Status: O\", literal=True)\n    # Split using the status string\n    .str.split(\"Status: O\")\n    # Get the second element\n    .arr.get(1)\n    # Rename the field\n    .alias(\"email_text\")\n)\n```", "```py\ndef extract_fields(emails: pl.DataFrame) -> pl.DataFrame:\n    email_pattern = r\"From:\\s*([^<\\n\\s]+)\"\n    subject_pattern = r\"Subject:\\s*(.*)\"\n    name_email_pattern = r'From:\\s*\"?([^\"<]+)\"?\\s*<([^>]+)>'\n\n    emails = (\n        emails.with_columns(\n            pl.col(\"emails\").str.extract(name_email_pattern, 2).alias(\"sender_email\"),\n            pl.col(\"emails\").str.extract(name_email_pattern, 1).alias(\"sender_name\"),\n            pl.col(\"emails\").str.extract(subject_pattern, 1).alias(\"subject\"),\n        )\n        .with_columns(\n            pl.when(pl.col(\"sender_email\").is_null())\n            .then(pl.col(\"emails\").str.extract(email_pattern, 1))\n            .otherwise(pl.col(\"sender_email\"))\n            .alias(\"sender_email\")\n        )\n        .with_columns(\n            pl.col(\"emails\")\n            .str.replace(\"Status: RO\", \"Status: O\", literal=True)\n            .str.split(\"Status: O\")\n            .arr.get(1)\n            .alias(\"email_text\")\n        )\n    )\n\n    return emails\n```", "```py\ndef email_features(data: pl.DataFrame, col: str) -> pl.DataFrame:\n    data = data.with_columns(\n        pl.col(col).str.n_chars().alias(f\"{col}_length\"),\n    ).with_columns(\n        (pl.col(col).str.count_match(r\"[A-Z]\") / pl.col(f\"{col}_length\")).alias(\n            f\"{col}_percent_capital\"\n        ),\n        (pl.col(col).str.count_match(r\"[^A-Za-z ]\") / pl.col(f\"{col}_length\")).alias(\n            f\"{col}_percent_digits\"\n        ),\n    )\n\n    return data\n```", "```py\nemails_pl = emails_pl.with_columns(\n    # Apply operations to the emails text column\n    pl.col(\"email_text\")\n    # Remove all the data in <..> (HTML tags)\n    .str.replace_all(r\"<.*?>\", \"\")\n    # Replace non-alphabetic characters (except whitespace) in text\n    .str.replace_all(r\"[^a-zA-Z\\s]+\", \" \")\n    # Replace multiple whitespaces with one whitespace \n    # We need to do this because of the previous cleaning step\n    .str.replace_all(r\"\\s+\", \" \")\n    # Make all text lowercase\n    .str.to_lowercase()\n    # Keep the field's name\n    .keep_name()\n)\n```", "```py\ndef email_clean(\n    data: pl.DataFrame, col: str, new_col_name: str | None = None\n) -> pl.DataFrame:\n    data = data.with_columns(\n        pl.col(col)\n        .str.replace_all(r\"<.*?>\", \" \")\n        .str.replace_all(r\"[^a-zA-Z\\s]+\", \" \")\n        .str.replace_all(r\"\\s+\", \" \")\n        .str.to_lowercase()\n        .alias(new_col_name if new_col_name is not None else col)\n    )\n\n    return data\n```", "```py\nemails_pl = emails_pl.with_columns(\n  pl.col(\"email_text\").str.split(\" \").alias(\"email_text_tokenised\")\n)\n```", "```py\ndef tokenise_text(data: pl.DataFrame, col: str, split_token: str = \" \") -> pl.DataFrame:\n    data = data.with_columns(pl.col(col).str.split(split_token).alias(f\"{col}_tokenised\"))\n\nreturn data\n```", "```py\nstops = set(\n    stopwords.words(\"english\")\n    + [\"\", \"nbsp\", \"content\", \"type\", \"text\", \"charset\", \"iso\", \"qzsoft\"]\n)\n```", "```py\nemails_pl = emails_pl.with_columns(\n    # Apply to the tokenised column (it's a list)\n    pl.col(\"email_text_tokenised\")\n    # For every element, check if it's not in a stopwords list and only then return it\n    .arr.eval(\n            pl.when(\n                (~pl.element().is_in(stopwords)) & (pl.element().str.n_chars() > 2)\n            ).then(pl.element())\n        )\n    # For every element of a new list, drop nulls (previously items that were in stopwords list)\n    .arr.eval(pl.element().drop_nulls())\n    .keep_name()\n)\n```", "```py\ndef remove_stopwords(\n    data: pl.DataFrame, stopwords: set | list, col: str\n) -> pl.DataFrame:\n    data = data.with_columns(\n        pl.col(col)\n        .arr.eval(pl.when(~pl.element().is_in(stopwords)).then(pl.element()))\n        .arr.eval(pl.element().drop_nulls())\n    )\n    return data\n```", "```py\nemails = load_emails_txt(\"fradulent_emails.txt\")\nemails_pl = pl.DataFrame({\"emails\": emails})\n\nemails_pl = (\n    emails_pl.pipe(extract_fields)\n    .pipe(email_features, \"email_text\")\n    .pipe(email_features, \"sender_email\")\n    .pipe(email_features, \"subject\")\n    .pipe(email_clean, \"email_text\")\n    .pipe(email_clean, \"sender_name\")\n    .pipe(email_clean, \"subject\")\n    .pipe(tokenise_text, \"email_text\")\n    .pipe(tokenise_text, \"subject\")\n    .pipe(remove_stopwords, stops, \"email_text_tokenised\")\n    .pipe(remove_stopwords, stops, \"subject_tokenised\")\n)\n```", "```py\n# Word cloud function\ndef generate_word_cloud(text: str):\n    wordcloud = WordCloud(\n        max_words=100, background_color=\"white\", width=1600, height=800\n    ).generate(text)\n\n    plt.figure(figsize=(20, 10), facecolor=\"k\")\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.tight_layout(pad=0)\n    plt.show()\n\n# Prepare data for word cloud\ntext_list = emails_pl.select(pl.col(\"email_text_tokenised\").arr.join(\" \"))[\n    \"email_text_tokenised\"\n].to_list()\nall_emails = \" \".join(text_list)\n\ngenerate_word_cloud(all_emails)\n```", "```py\n# TF-IDF with 500 words\nvectorizer = TfidfVectorizer(max_features=500)\ntransformed_text = vectorizer.fit_transform(text_list)\ntf_idf = pd.DataFrame(transformed_text.toarray(), columns=vectorizer.get_feature_names_out())\n\n# Cluster into 5 clusters\nn = 5\ncluster = KMeans(n_clusters=n, n_init='auto')\nclusters = cluster.fit_predict(tf_idf)\n\nfor c in range(n):\n    cluster_texts = np.array(text_list)[clusters==c]\n    cluster_text = ' '.join(list(cluster_texts))\n\n    generate_word_cloud(cluster_text)\n```"]