- en: PCA vs Autoencoders for a Small Dataset in Dimensionality Reduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA与自编码器在小数据集上的降维比较
- en: 原文：[https://towardsdatascience.com/pca-vs-autoencoders-for-a-small-dataset-in-dimensionality-reduction-67b15318dea0](https://towardsdatascience.com/pca-vs-autoencoders-for-a-small-dataset-in-dimensionality-reduction-67b15318dea0)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/pca-vs-autoencoders-for-a-small-dataset-in-dimensionality-reduction-67b15318dea0](https://towardsdatascience.com/pca-vs-autoencoders-for-a-small-dataset-in-dimensionality-reduction-67b15318dea0)
- en: 'Neural Networks and Deep Learning Course: Part 45'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络和深度学习课程：第45部分
- en: '[](https://rukshanpramoditha.medium.com/?source=post_page-----67b15318dea0--------------------------------)[![Rukshan
    Pramoditha](../Images/b80426aff64ff186cb915795644590b1.png)](https://rukshanpramoditha.medium.com/?source=post_page-----67b15318dea0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----67b15318dea0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----67b15318dea0--------------------------------)
    [Rukshan Pramoditha](https://rukshanpramoditha.medium.com/?source=post_page-----67b15318dea0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://rukshanpramoditha.medium.com/?source=post_page-----67b15318dea0--------------------------------)[![Rukshan
    Pramoditha](../Images/b80426aff64ff186cb915795644590b1.png)](https://rukshanpramoditha.medium.com/?source=post_page-----67b15318dea0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----67b15318dea0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----67b15318dea0--------------------------------)
    [Rukshan Pramoditha](https://rukshanpramoditha.medium.com/?source=post_page-----67b15318dea0--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----67b15318dea0--------------------------------)
    ·8 min read·Feb 16, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----67b15318dea0--------------------------------)
    ·阅读时间8分钟·2023年2月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/40244380e27388a744474a8be2aa482a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40244380e27388a744474a8be2aa482a.png)'
- en: Photo by [Robert Katzki](https://unsplash.com/@ro_ka?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/jbtfM0XBeRc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Robert Katzki](https://unsplash.com/@ro_ka?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/photos/jbtfM0XBeRc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: '***Can general machine learning algorithms outperform neural networks with
    small datasets?***'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '***小数据集上，通用机器学习算法是否能超越神经网络？***'
- en: In general, deep learning algorithms such as neural networks require a massive
    amount of data to achieve reasonable performance. So, neural networks like autoencoders
    can benefit from very large datasets that we use to train the models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，深度学习算法如神经网络需要大量数据才能实现合理的性能。因此，像自编码器这样的神经网络可以从我们用来训练模型的非常大的数据集中获益。
- en: Sometimes, general machine learning algorithms can outperform neural network
    algorithms when they are trained with very small datasets.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，当通用机器学习算法在非常小的数据集上训练时，它们的表现可能会超越神经网络算法。
- en: Autoencoders can also be used in dimensionality reduction applications, even
    though they are widely used in other popular applications such as image denoising,
    image generation, image colorization, image compression, image super-resolution,
    etc.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器也可以用于降维应用，尽管它们在图像去噪、图像生成、图像着色、图像压缩、图像超分辨率等其他流行应用中被广泛使用。
- en: Earlier, we compared the performance of autoencoders in dimensionality reduction
    against PCA by training the models on the *very large* MNIST dataset. There, the
    autoencoder model easily outperformed the PCA model [ref¹] because the MNIST data
    is large and non-linear.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 早些时候，我们通过在*非常大的* MNIST 数据集上训练模型，比较了自编码器在降维中的表现与 PCA 的表现。在那里，自编码器模型轻松超越了 PCA
    模型 [参考¹]，因为 MNIST 数据集大且具有非线性特征。
- en: 'ref¹: [*How Autoencoders Outperform PCA in Dimensionality Reduction*](/how-autoencoders-outperform-pca-in-dimensionality-reduction-1ae44c68b42f)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 参考¹：[*自编码器如何在降维中超越PCA*](/how-autoencoders-outperform-pca-in-dimensionality-reduction-1ae44c68b42f)
- en: Autoencoders work well with large and non-linear data.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 自编码器在处理大规模和非线性数据时表现良好。
- en: Even though autoencoders are a type of neural network, it is still possible
    to use them with smaller datasets to achieve some performance with the correct
    model architecture and hyperparameter values.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自编码器是一种神经网络，但仍然可以使用较小的数据集，只要选择正确的模型架构和超参数值，就能够取得一定的性能。
- en: Autoencoders are powerful and flexible enough to capture complex and non-linear
    patterns in data.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 自编码器功能强大且灵活，足以捕捉数据中的复杂和非线性模式。
- en: Today, we will compare the performance of an autoencoder (the neural network
    model) in dimensionality reduction against PCA (the general machine learning algorithm)
    by training the models on the *very small* Winedataset.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们将通过在*非常小*的葡萄酒数据集上训练模型，比较自编码器（神经网络模型）在降维方面与 PCA（通用机器学习算法）的表现。
- en: The Wine dataset has 178 samples and 13 features. This dataset is very small
    when we compare it with the MNIST dataset which has 60,000 samples and 784 features!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 葡萄酒数据集有 178 个样本和 13 个特征。与 MNIST 数据集的 60,000 个样本和 784 个特征相比，这个数据集非常小！
- en: Load the Wine dataset
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载葡萄酒数据集
- en: The Wine dataset comes preloaded with Sciki-learn and can be loaded as follows.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 葡萄酒数据集已经预加载到 Scikit-learn 中，可以按如下方式加载。
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/6b5b070277c2b9ca837b575f18bdc5a6.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b5b070277c2b9ca837b575f18bdc5a6.png)'
- en: '**The shape of the Wine dataset** (Image by author)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**葡萄酒数据集的形状**（作者提供的图片）'
- en: Run PCA with Wine data
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用葡萄酒数据运行 PCA
- en: First, we will need to select the best number of components by running the PCA
    with all components [ref²].
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要通过使用所有组件运行 PCA 来选择最佳组件数量 [ref²]。
- en: 'ref²: [*3 Easy Steps to Perform Dimensionality Reduction Using Principal Component
    Analysis (PCA)*](https://rukshanpramoditha.medium.com/3-easy-steps-to-perform-dimensionality-reduction-using-principal-component-analysis-pca-79121998b991)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 'ref²: [*进行主成分分析 (PCA) 以实现降维的 3 个简单步骤*](https://rukshanpramoditha.medium.com/3-easy-steps-to-perform-dimensionality-reduction-using-principal-component-analysis-pca-79121998b991)'
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/5e27b552287a6852bfbd89582152b636.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e27b552287a6852bfbd89582152b636.png)'
- en: '**Cumulative explained variance plot** (Image by author)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**累计解释方差图**（作者提供的图片）'
- en: By analyzing the above cumulative explained variance plot, I’ve decided to keep
    the first seven components which capture about 90% variance in the data. So, those
    seven components will accurately represent the original Wine dataset.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析上述累计解释方差图，我决定保留前七个组件，这些组件捕获了数据中约 90% 的方差。因此，这七个组件将准确地代表原始葡萄酒数据集。
- en: Run PCA again with selected components
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用选定的组件重新运行 PCA
- en: To apply dimensionality reduction to the Wine dataset, we need to run PCA again
    with the selected components and apply the transformation.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要将降维应用于葡萄酒数据集，我们需要使用选定的组件重新运行 PCA 并应用转换。
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/ef1bd7818c895ecf64c547c39298fb86.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef1bd7818c895ecf64c547c39298fb86.png)'
- en: '**The shape of the reduced Wine dataset after applying PCA** (Image by author)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用 PCA 后的葡萄酒数据集的形状**（作者提供的图片）'
- en: Now, there are only seven components (features) in the dataset. So, the dimensionality
    of the data has been reduced!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数据集中只有七个组件（特征）。因此，数据的维度已经减少！
- en: Perform dimensionality reduction with an autoencoder
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自编码器进行降维
- en: The following code defines an autoencoder in which the encoder part can be used
    to obtain the lower dimensional (encoded) representation of the data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码定义了一个自编码器，其中编码器部分可以用于获得数据的低维（编码）表示。
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/0a10b14840a711dc24bfdc8682bb472e.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a10b14840a711dc24bfdc8682bb472e.png)'
- en: '**The shape of the reduced Wine dataset after auto-encoding data** (Image by
    author)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**自编码数据后的葡萄酒数据集的形状**（作者提供的图片）'
- en: The autoencoder model has an input layer, encoding layers and decoding layers.
    The input dimension is the number of input features in the Wine dataset, which
    is 13\. The latent vector dimension is 7 which is equal to the number of components
    that we selected earlier in PCA.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器模型具有输入层、编码层和解码层。输入维度是葡萄酒数据集中的输入特征数，即 13。潜在向量维度为 7，等于我们在 PCA 中先前选择的组件数量。
- en: All layers are connected using the Keras Functional API method [ref³]. Then,
    the whole autoencoder model is created by connecting the input layer and decoder
    part.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所有层都使用 Keras 功能 API 方法 [ref³] 连接。然后，通过连接输入层和解码器部分创建整个自编码器模型。
- en: 'ref³: [*Two Different Ways to Build Keras Models: Sequential API and Functional
    API*](https://rukshanpramoditha.medium.com/two-different-ways-to-build-keras-models-sequential-api-and-functional-api-868e64594820)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 'ref³: [*构建 Keras 模型的两种不同方式：顺序 API 和功能 API*](https://rukshanpramoditha.medium.com/two-different-ways-to-build-keras-models-sequential-api-and-functional-api-868e64594820)'
- en: Then, we compile the whole autoencoder with the Adam optimizer and the mean
    squared error (mse) loss function.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用 Adam 优化器和均方误差（mse）损失函数编译整个自编码器。
- en: The model is trained on the standardized (scaled) Wine data for 100 epochs with
    a batch size of 16.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在标准化（缩放）的葡萄酒数据上训练 100 个周期，批量大小为 16。
- en: The latent vector represents the most important features of the input data in
    a lower-dimensional form [ref⁴]. Therefore, after training the whole autoencoder,
    we can use its encoder part to obtain the lower dimensional (encoded) representation
    of the data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在向量表示了输入数据中最重要的特征的低维形式 [ref⁴]。因此，在训练完整个自动编码器之后，我们可以使用其编码器部分来获得数据的低维（编码）表示。
- en: 'ref⁴: [*An Introduction to Autoencoders in Deep Learning*](https://rukshanpramoditha.medium.com/an-introduction-to-autoencoders-in-deep-learning-ab5a5861f81e)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 'ref⁴: [*深度学习中的自动编码器简介*](https://rukshanpramoditha.medium.com/an-introduction-to-autoencoders-in-deep-learning-ab5a5861f81e)'
- en: The `encoder_model`is created by connecting the input layer and the encoder
    part. Then, we can call its predict() method on the scaled Wine data to obtain
    the lower dimensional (encoded) representation of the Wine dataset which is represented
    by the variable, `X_encoded`. Since the **“latent vector dimension”** is set to
    7, the encoded representation has 7 features and the dimensionality of the data
    has been reduced!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`encoder_model` 是通过连接输入层和编码器部分创建的。然后，我们可以在缩放后的 Wine 数据上调用它的 predict() 方法，以获得
    Wine 数据集的低维（编码）表示，这由变量 `X_encoded` 表示。由于**“潜在向量维度”**被设置为 7，因此编码表示具有 7 个特征，数据的维度已被降低！'
- en: Compare both PCA and autoencoder models by visualizing data
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过可视化数据来比较 PCA 和自动编码器模型
- en: Visualization of high-dimensional data can be achieved through dimensionality
    reduction [ref⁵]. So, dimensionality reduction is extremely useful for data visualization.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 高维数据的可视化可以通过降维实现 [ref⁵]。因此，降维对于数据可视化极为有用。
- en: 'ref⁵: [*11 Different Uses of Dimensionality Reduction*](/11-different-uses-of-dimensionality-reduction-4325d62b4fa6)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 'ref⁵: [*降维的 11 种不同用途*](/11-different-uses-of-dimensionality-reduction-4325d62b4fa6)'
- en: By using only two components (dimensions), we plot the Wine dataset outputs
    returned by both PCA and autoencoder models.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用两个组件（维度），我们绘制了 PCA 和自动编码器模型返回的 Wine 数据集输出。
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/958a23c84db65d7e1e3acbfd732c3734.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/958a23c84db65d7e1e3acbfd732c3734.png)'
- en: '**Two-dimensional representations of Wine data: PCA vs Autoencoder** (Image
    by author)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**Wine 数据的二维表示：PCA 与自动编码器**（图像作者）'
- en: As you can see, the two-dimensional representation of Wine data obtained using
    PCA shows a clear separation between the three classes of wine, but the separation
    is not good enough in autoencoder output.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，使用 PCA 获得的 Wine 数据的二维表示显示了三种葡萄酒类别之间的明显分离，但在自动编码器输出中分离效果不够好。
- en: PCA works well with small datasets like the Wine dataset.
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: PCA 在小数据集如 Wine 数据集上效果很好。
- en: '**Note:** The output of the autoencoder (right plot) may vary significantly
    due to the stochastic nature of the algorithm and the values of hyperparameters
    such as the number of hidden layers and hidden units, type of activation function
    used in each layer, type of loss function, type of optimizer, number of epochs
    and the batch size! But, the separation of the three classes may not be as good
    as the PCA output.'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 由于算法的随机特性以及超参数的值，如隐藏层的数量和隐藏单元、每层使用的激活函数类型、损失函数类型、优化器类型、训练轮数和批量大小，自动编码器的输出（右图）可能会有显著变化！但是，三个类别的分离可能不如
    PCA 输出的效果好。'
- en: 'PCA vs Autoencoder: Which is better in dimensionality reduction?'
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA 与自动编码器：哪种在降维方面更好？
- en: The choice of PCA and autoencoder for dimensionality reduction depends on the
    following factors.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 PCA 和自动编码器进行降维取决于以下因素。
- en: Size of the dataset
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的大小
- en: The complexity of the dataset (linear or non-linear, image or numerical data)
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的复杂性（线性还是非线性，图像还是数值数据）
- en: Goals of the analysis
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析的目标
- en: Availability of computational resources
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算资源的可用性
- en: Interpretability
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释性
- en: PCA works well with small datasets. It can also be used with larger datasets.
    However, autoencoders work really well with very large datasets.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 在小数据集上效果很好。它也可以用于较大的数据集。然而，自动编码器在非常大的数据集上效果非常好。
- en: PCA works well with linear data as it is a linear dimensionality reduction technique.
    It is not effective with non-linear data. In contrast, autoencoders can easily
    capture complex and non-linear patterns in the data. So, they work well with non-linear
    data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 对线性数据效果很好，因为它是一种线性降维技术。对于非线性数据效果不佳。相反，自动编码器可以轻松捕捉数据中的复杂和非线性模式。因此，它们在非线性数据上表现良好。
- en: Autoecnder works well with image data. PCA works well with numerical data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器在图像数据上表现良好。PCA 在数值数据上表现良好。
- en: There is no way to determine the importance of each component (feature) in the
    latent vector of an autoencoder model. But in PCA, we can create the cumulative
    explained variance plot for that.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 无法确定自编码器模型潜在向量中每个组件（特征）的重要性。但是在PCA中，我们可以为此创建累积解释方差图。
- en: Since an autoencoder is a neural network, its architecture may become complex.
    In addition to that, it requires a massive amount of data. Therefore, autoencoders
    require more computational resources than PCA.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于自编码器是神经网络，它的架构可能变得复杂。此外，它还需要大量的数据。因此，自编码器比PCA需要更多的计算资源。
- en: Autoecndoers are black-box models. So, they are hard to interpret. We don’t
    know how they select important features from the data we provide. So, the interpretation
    of those models is very difficult.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是黑箱模型。因此，它们难以解释。我们不知道它们如何从我们提供的数据中选择重要特征。因此，这些模型的解释非常困难。
- en: Conclusions
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Both PCA and autoencoders can be used to perform dimensionality reduction. PCA
    is a general machine-learning algorithm while autoencoders are a type of neural
    network architecture that require large datasets and a lot of computational resources.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: PCA和自编码器都可以用于执行降维。PCA是一个通用的机器学习算法，而自编码器是一种神经网络架构，需大数据集和大量计算资源。
- en: Autoencoders work well with large and non-linear data. They are powerful and
    flexible enough to capture complex and non-linear patterns in data, but may fail
    to outperform general machine learning algorithms like PCA with smaller datasets!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器在处理大规模和非线性数据时表现良好。它们足够强大和灵活，可以捕捉数据中的复杂和非线性模式，但在较小的数据集上可能无法超越通用机器学习算法如PCA！
- en: If you really want to consider class separability while performing dimensionality
    reduction, LDA (Linear Discriminant Analysis) is the best choice. Read the complete
    guide below.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你真的想在执行降维时考虑类可分性，LDA（线性判别分析）是最佳选择。请阅读下面的完整指南。
- en: '[*LDA Is More Effective than PCA for Dimensionality Reduction in Classification
    Datasets*](/lda-is-highly-effective-than-pca-for-dimensionality-reduction-in-classification-datasets-4489eade632)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[*LDA在分类数据集中比PCA更有效*](/lda-is-highly-effective-than-pca-for-dimensionality-reduction-in-classification-datasets-4489eade632)'
- en: This is the end of today’s article.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的文章到此结束。
- en: '**Please let me know if you’ve any questions or feedback.**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你有任何问题或反馈，请告诉我。**'
- en: How about an AI course?
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 那么AI课程怎么样？
- en: '[![](../Images/61d87653a43e302c4f2a839ae046ac31.png)](https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/61d87653a43e302c4f2a839ae046ac31.png)](https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75)'
- en: '**Join my Neural Networks and Deep Learning Course, the first ever on Medium**
    (Screenshot by author)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**加入我的神经网络和深度学习课程，这是Medium上的首个课程**（作者截图）'
- en: Support me as a writer
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持我作为作者
- en: '*I hope you enjoyed reading this article. If you’d like to support me as a
    writer, kindly consider* [***signing up for a membership***](https://rukshanpramoditha.medium.com/membership)
    *to get unlimited access to Medium. It only costs $5 per month and I will receive
    a portion of your membership fee.*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*我希望你喜欢阅读这篇文章。如果你愿意支持我作为作者，请* [***注册会员***](https://rukshanpramoditha.medium.com/membership)
    *以获得对Medium的无限制访问权限。每月只需$5，我将从你的会员费中获得一部分。*'
- en: '[](https://rukshanpramoditha.medium.com/membership?source=post_page-----67b15318dea0--------------------------------)
    [## Join Medium with my referral link - Rukshan Pramoditha'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://rukshanpramoditha.medium.com/membership?source=post_page-----67b15318dea0--------------------------------)
    [## 通过我的推荐链接加入Medium - Rukshan Pramoditha'
- en: Read every story from Rukshan Pramoditha (and thousands of other writers on
    Medium). Your membership fee directly…
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读Rukshan Pramoditha的每一篇故事（以及Medium上成千上万的其他作者的故事）。您的会员费用直接…
- en: rukshanpramoditha.medium.com](https://rukshanpramoditha.medium.com/membership?source=post_page-----67b15318dea0--------------------------------)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[rukshanpramoditha.medium.com](https://rukshanpramoditha.medium.com/membership?source=post_page-----67b15318dea0--------------------------------)'
- en: Join my private list of emails
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我的私人邮件列表
- en: '*Never miss a great story from me again. By* [***subscribing to my email list***](https://rukshanpramoditha.medium.com/subscribe)*,
    you will directly receive my stories as soon as I publish them.*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*不要再错过我的精彩故事。通过* [***订阅我的邮件列表***](https://rukshanpramoditha.medium.com/subscribe)*，你将直接在我发布故事时收到。*'
- en: Thank you so much for your continuous support! See you in the next article.
    Happy learning to everyone!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢你的持续支持！下篇文章见。祝大家学习愉快！
- en: Wine dataset info
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 葡萄酒数据集信息
- en: '**Dataset source:** You can download the original dataset [here](https://archive.ics.uci.edu/ml/datasets/Wine).'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集来源：** 你可以从[这里](https://archive.ics.uci.edu/ml/datasets/Wine)下载原始数据集。'
- en: '**Dataset license:** This dataset is available under the [*CC BY 4.0*](https://creativecommons.org/licenses/by/4.0/)
    (*Creative Commons Attribution 4.0*) license.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集许可证：** 该数据集在[*CC BY 4.0*](https://creativecommons.org/licenses/by/4.0/)（*知识共享署名
    4.0*）许可证下提供。'
- en: '**Citation:** Lichman, M. (2013). UCI Machine Learning Repository [[https://archive.ics.uci.edu/ml](https://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**引用：** Lichman, M. (2013). UCI机器学习库 [[https://archive.ics.uci.edu/ml](https://archive.ics.uci.edu/ml)]。加州尔湾：加利福尼亚大学信息与计算机科学学院。'
- en: '[Rukshan Pramoditha](https://medium.com/u/f90a3bb1d400?source=post_page-----67b15318dea0--------------------------------)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[Rukshan Pramoditha](https://medium.com/u/f90a3bb1d400?source=post_page-----67b15318dea0--------------------------------)'
- en: '**2023–02–16**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**2023年2月16日**'
