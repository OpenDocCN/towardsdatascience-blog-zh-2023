- en: 'BERTopic: What Is So Special About v0.16?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/bertopic-what-is-so-special-about-v0-16-64d5eb3783d9](https://towardsdatascience.com/bertopic-what-is-so-special-about-v0-16-64d5eb3783d9)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exploring Zero-Shot Topic Modeling, Model Merging, and LLMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@maartengrootendorst?source=post_page-----64d5eb3783d9--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----64d5eb3783d9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----64d5eb3783d9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----64d5eb3783d9--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----64d5eb3783d9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----64d5eb3783d9--------------------------------)
    ·8 min read·Dec 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dfb10d9cf8adc4cc4209ef052d1984d2.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: My ambition for [BERTopic](https://github.com/MaartenGr/BERTopic) is to make
    it the **one-stop shop** for topic modeling by allowing for significant flexibility
    and modularity.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: That has been the goal for the last few years and with the [release of v0.16](https://github.com/MaartenGr/BERTopic/releases/tag/v0.16.0),
    I believe we are a BIG step closer to achieving that.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s take a small step back. *What is BERTopic?*
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Well, BERTopic is a topic modeling framework that allows users to essentially
    create their version of a topic model. With many variations of topic modeling
    implemented, the idea is that it should support almost any use case.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/49c2eac172607192b89273d7ba3c958f.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: The modular nature of BERTopic allows you to build your topic model however
    you want. Switching components allows BERTopic to grow with the latest developments
    in Language AI.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'With [v0.16](https://github.com/MaartenGr/BERTopic/releases/tag/v0.16.0), several
    features were implemented that I believe will take BERTopic to the next level,
    namely:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Shot Topic Modeling
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model Merging
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More Large Language Model (LLM) Support
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/e3dc2396fcba0eabdc3089c841e69f3f.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Just a few of BERTopic’s capabilities.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we will go through what these features are and for which use
    cases they could be helpful.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, you can install BERTopic (with HF datasets) as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can also follow along with the [Google Colab Notebook](https://colab.research.google.com/drive/113Eg-cq9wUuOuNwXO40Zo3AlMgaW2Go1?usp=sharing)
    to make sure everything works as intended.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '**UPDATE**: I uploaded a video version to YouTube that goes more in-depth into
    how to use these new features:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Zero-Shot Topic Modeling: A Flexible Technique'
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Zero-shot techniques generally refer to having no examples to train your data
    on. Although you know the target, it is not assigned to your data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 零-shot 技术通常指的是没有用于训练数据的示例。尽管你知道目标是什么，但它并没有被分配给你的数据。
- en: In BERTopic, we use Zero-shot Topic Modeling to find pre-defined topics in large
    amounts of documents.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在 BERTopic 中，我们使用零-shot 主题建模来在大量文档中找到预定义的主题。
- en: Imagine you have ArXiv abstracts about Machine Learning and you know that the
    topic “*Large Language Models*” is in there. With Zero-shot Topic Modeling, you
    can ask BERTopic to find all documents related to “Large Language Models”.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你有关于机器学习的 ArXiv 摘要，并且你知道“*大型语言模型*”这个主题在其中。通过零-shot 主题建模，你可以让 BERTopic 找到所有与“大型语言模型”相关的文档。
- en: In essence, it is nothing more than semantic search! But… there is a neat trick
    ;-)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，它不过是语义搜索！但是……有一个很酷的技巧 ;-)
- en: When you try to find those documents related to “Large Language Models”, there
    will be many left not about those topics. So, what do you do with those topics?
    You use BERTopic to find all topics that were left!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当你试图找到与“大型语言模型”相关的文档时，会有许多文档与这些主题无关。那么，你会如何处理这些主题？你可以使用 BERTopic 来找到所有剩下的主题！
- en: '![](../Images/27e6574d50cce294aefaf97e289314a7.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27e6574d50cce294aefaf97e289314a7.png)'
- en: 'As a result, you will have three scenarios of Zero-shot Topic Modeling:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，你将有三种零-shot 主题建模的场景：
- en: '**No zero-shot topics were detected**. This means that none of the documents
    would fit with the predefined topics and a regular BERTopic would be run.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未检测到零-shot 主题**。这意味着没有文档符合预定义的主题，因此将运行常规的 BERTopic。'
- en: '**Only zero-shot topics were detected**. Here, we would not need to find additional
    topics since all original documents were assigned to one of the predefined topics.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仅检测到零-shot 主题**。在这种情况下，我们不需要寻找额外的主题，因为所有原始文档都已被分配到预定义的主题之一。'
- en: '**Both zero-shot topics and clustered topics were detected**. This means that
    some documents would fit with the predefined topics whereas others would not.
    For the latter, new topics were found.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检测到零-shot 主题和聚类主题**。这意味着一些文档会符合预定义的主题，而其他文档则不符合。对于后者，发现了新的主题。'
- en: 'Using Zero-shot BERTopic is straightforward:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用零-shot BERTopic 非常简单：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can view the three pre-defined topics along with several newly discovered
    topics:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看三个预定义的主题以及几个新发现的主题：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/46b38e80f4d38334c92b4d6b2fd3925d.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46b38e80f4d38334c92b4d6b2fd3925d.png)'
- en: Note that although we have pre-defined names for the topics, we allow BERTopic
    for additional representations.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管我们对主题有预定义的名称，但我们允许 BERTopic 进行额外的表示。
- en: This gives exciting new insight into pre-defined topics!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这为预定义的主题提供了令人兴奋的新见解！
- en: So… when do you use Zero-shot Topic Modeling?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 那么……你什么时候使用零-shot 主题建模？
- en: If you already know some of the topics in your data, this is a great solution
    for finding them! Since it can discover both pre-defined and new topics, is an
    incredibly flexible technique.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经知道数据中的一些主题，这是一个很好的解决方案来找到它们！因为它可以发现预定义的和新的主题，是一种非常灵活的技术。
- en: 'Model Merging: Federated and Incremental Learning'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型合并：联邦学习和增量学习
- en: This is a fun new feature, *model merging*!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有趣的新功能，*模型合并*！
- en: Model merging refers to BERTopic’s capability to combine multiple pre-trained
    BERTopic models to create one large topic model. It explores which topics should
    be merged and which should remain separate.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 模型合并指的是 BERTopic 将多个预训练的 BERTopic 模型合并为一个大型主题模型的能力。它探索哪些主题应该合并，哪些主题应该保持分开。
- en: It works as follows. When we pass a list of models to this new feature, `.merge_models`,
    the first model in the list is chosen as the baseline. This baseline is used to
    check whether all other models contain new topics based on the similarity between
    their topic embeddings.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 它的工作原理如下。当我们将一系列模型传递给这个新功能`.merge_models`时，列表中的第一个模型被选择为基准。这个基准模型用来检查所有其他模型是否包含基于主题嵌入相似性的新的主题。
- en: Dissimilar topics are added to the baseline model whereas similar topics are
    assigned to the topic of the baseline. This means that we need the embedding models
    to be the same.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的主题被添加到基准模型中，而相似的主题则被分配到基准主题中。这意味着我们需要嵌入模型是相同的。
- en: '![](../Images/e86b0503a7c4538ca331cd911112f58c.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e86b0503a7c4538ca331cd911112f58c.png)'
- en: When merging BERTopic models, duplicate topics will be merged and all other
    topics will be kept the same.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在合并 BERTopic 模型时，重复的主题将被合并，所有其他主题将保持不变。
- en: 'Merging pre-trained BERTopic models is straightforward and only requires a
    few lines of code:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 合并预训练的 BERTopic 模型很简单，只需要几行代码：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: And that is it! With a single function, `.merge_models`, you can merge pre-trained
    BERTopic models.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！通过一个函数`.merge_models`，你可以合并预训练的BERTopic模型。
- en: 'The benefit of merging pre-trained models is that it allows for a variety of
    creative and useful use cases. For instance, we could use it for:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 合并预训练模型的好处在于，它允许多种创造性和有用的应用场景。例如，我们可以用它来：
- en: '*Incremental Learning* — We can continuously discover new topics by iteratively
    merging models. This can be used for issue tickets to quickly uncover pressing
    bugs/issues.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*增量学习* — 我们可以通过迭代合并模型来不断发现新主题。这可以用于问题票证，以快速发现紧迫的错误/问题。'
- en: '*Batched Learning* — Compute and memory problems can arise with large datasets
    or when you simply do not have the hardware for it. By splitting the training
    process up into smaller models, we can get similar performance whilst reducing
    the necessary compute.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*批量学习* — 对于大型数据集，或者当你的硬件资源不足时，计算和内存问题可能会出现。通过将训练过程拆分为更小的模型，我们可以在减少所需计算的同时获得类似的性能。'
- en: '*Federated Learning* —Merging models allow for the training to be distributed
    among different clients who do not wish to share their data. This increases privacy
    and security with respect to their data especially if a non-keyword-based method
    is used for generating the representations, such as using a [Large Language Model](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html#zephyr-mistral-7b).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*联邦学习* — 合并模型允许将训练分布在不同的客户端之间，这些客户端不愿分享他们的数据。这增加了数据的隐私和安全，特别是如果使用基于非关键词的方法来生成表示，例如使用[大型语言模型](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html#zephyr-mistral-7b)。'
- en: Federated Learning is rather straightforward, simply run `.merge_models` on
    your central server.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习相当简单，只需在你的中央服务器上运行`.merge_models`。
- en: The other two, incremental and batched learning, might require a bit of an example!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 另外两个，增量学习和批量学习，可能需要一些示例！
- en: Incremental and Batched Learning
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增量学习和批量学习
- en: To perform both *incremental* and *batched* learning, we are going to mimic
    a typical `.partial_fit` pipeline. Here, we will train a base model first and
    then iteratively add a small newly trained model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行*增量*和*批量*学习，我们将模拟一个典型的`.partial_fit`管道。在这里，我们将首先训练一个基础模型，然后迭代地添加一个新的小型训练模型。
- en: 'In each iteration, we can check any topics that were added to the base model:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，我们可以检查是否有任何主题被添加到基础模型中：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To illustrate, this will give back newly found topics such as:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，这将返回新发现的主题，例如：
- en: 'The following topics are newly found:'
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 以下是新发现的主题：
- en: '['
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '['
- en: ‘50_forecasting_predicting_prediction_stocks’,
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ‘50_forecasting_predicting_prediction_stocks’,
- en: ‘51_activity_activities_accelerometer_accelerometers’,
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ‘51_activity_activities_accelerometer_accelerometers’,
- en: ‘57_rnns_deepcare_neural_imputation’
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ‘57_rnns_deepcare_neural_imputation’
- en: ']'
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ']'
- en: It retains everything from the original model, including
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 它保留了原始模型中的所有内容，包括
- en: Not only do we reduce the compute by splitting the training up into chunks,
    but we can monitor any new topics that were added to the model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅通过将训练过程拆分为多个部分来减少计算量，还可以监控模型中新增的主题。
- en: In practice, you can train a new model with a frequency that fits your use case.
    You might check for new topics monthly, weekly, or even daily if you have enough
    data.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 实际操作中，你可以使用适合你用例的频率来训练新模型。你可以每月、每周，甚至每天检查新主题，只要你有足够的数据。
- en: More Large Language Model Support
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多大型语言模型支持
- en: Although we could use Large Language Models (LLMs) for a while now in BERTopic,
    the v0.16 release has several smaller additions that make working with LLMs a
    nicer experience!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们现在可以在BERTopic中使用大型语言模型（LLMs），但v0.16版本发布了几个较小的功能，使得使用LLMs的体验更加愉快！
- en: 'To sum up, the following were added:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，以下内容被添加了：
- en: '[*llama-cpp-python*](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html#llamacpp):
    Load any GGUF-compatible LLM with llama.cpp'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*llama-cpp-python*](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html#llamacpp)：使用llama.cpp加载任何GGUF兼容的LLM'
- en: '[*Truncate documents*](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html#truncating-documents):
    Use a variety of techniques to truncate documents when passing them to any LLM.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*截断文档*](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html#truncating-documents)：使用各种技术在传递给任何LLM时截断文档。'
- en: '*LangChain*: Support for LCEL Runnables by [@joshuasundance-swca](https://github.com/joshuasundance-swca)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LangChain*：支持[@joshuasundance-swca](https://github.com/joshuasundance-swca)的LCEL
    Runnables'
- en: Let’s explore a short example of the first two features, *llama.cpp* and *document
    truncation*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索前两个功能的简短示例，*llama.cpp* 和 *文档截断*。
- en: When you pass documents to any LLM module, they might exceed its token limit.
    Instead, we can truncate the documents passed to the LLM by defining a `tokenizer`
    and a `doc_length`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将文档传递给任何LLM模块时，它们可能会超出其令牌限制。相反，我们可以通过定义`tokenizer`和`doc_length`来截断传递给LLM的文档。
- en: '![](../Images/5a2bd99fada7d93ba07dbbaf4f854106.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a2bd99fada7d93ba07dbbaf4f854106.png)'
- en: The different methods for tokenization when truncating documents.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 截断文档时不同的分词方法。
- en: The definition of a `doc_length` depends on the tokenizer you use. For example,
    a value of 100 can refer to truncating by the number of tokens or even characters.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`doc_length`的定义取决于你使用的分词器。例如，100的值可以指按令牌数或字符数进行截断。'
- en: '![](../Images/6a1b08a2ba4009bd08970a4be14fe09f.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a1b08a2ba4009bd08970a4be14fe09f.png)'
- en: Before documents are added to the prompt, they can be truncated first based
    on the tokenization strategy.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在将文档添加到提示中之前，可以根据分词策略首先对其进行截断。
- en: 'To use this together with `llama-cpp-python` , let’s consider the following
    example. First, we install the necessary packages, prepare the environment, and
    download a small but capable model ([Zephyr-7B](https://huggingface.co/TheBloke/zephyr-7B-alpha-GGUF)):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 要将其与`llama-cpp-python`一起使用，我们可以考虑以下示例。首先，我们安装必要的包，准备环境，并下载一个小而强大的模型（[Zephyr-7B](https://huggingface.co/TheBloke/zephyr-7B-alpha-GGUF)）：
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Loading a GGUF model with `llama-cpp-python` in BERTopic is straightforward:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在BERTopic中用`llama-cpp-python`加载GGUF模型非常简单：
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: And that is it! We created a model that truncates input documents and creates
    interesting topic representations without being constrained by its token limit.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们创建了一个可以截断输入文档并在不受令牌限制约束的情况下创建有趣主题表示的模型。
- en: Thank You For Reading!
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: If you are, like me, passionate about *AI* and/or *Psychology*, please feel
    free to add me on [**LinkedIn**](https://www.linkedin.com/in/mgrootendorst/)and
    [**Twitter**](https://twitter.com/MaartenGr), or subscribe to my [**Newsletter**](http://maartengrootendorst.substack.com/).
    You can also find some of my content on my [**Personal Website**](https://maartengrootendorst.com/).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你和我一样，对*人工智能*和/或*心理学*充满热情，请随时在[**LinkedIn**](https://www.linkedin.com/in/mgrootendorst/)和[**Twitter**](https://twitter.com/MaartenGr)上加我，或订阅我的[**新闻通讯**](http://maartengrootendorst.substack.com/)。你还可以在我的[**个人网站**](https://maartengrootendorst.com/)上找到我的一些内容。
- en: '[](https://maartengrootendorst.substack.com/?source=post_page-----64d5eb3783d9--------------------------------)
    [## Exploring Language Models | Substack'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://maartengrootendorst.substack.com/?source=post_page-----64d5eb3783d9--------------------------------)
    [## 探索语言模型 | Substack'
- en: Writing about the intersection of AI, Language Models, and Psychology.
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 撰写关于人工智能、语言模型和心理学交集的内容。
- en: maartengrootendorst.substack.com](https://maartengrootendorst.substack.com/?source=post_page-----64d5eb3783d9--------------------------------)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: maartengrootendorst.substack.com](https://maartengrootendorst.substack.com/?source=post_page-----64d5eb3783d9--------------------------------)
- en: '*All images without a source credit were created by the author — Which means
    all of them, I like creating my own images ;)*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有未注明来源的图像均由作者创建——这意味着所有图像都是如此，我喜欢自己创作图像；)*'
