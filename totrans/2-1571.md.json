["```py\n#Load required Python packages\n\nimport numpy as np \nimport pandas as pd \n\n!pip install pyspark #Install PySpark\nimport pyspark\nfrom pyspark.sql.window import Window  #For use of Window Function\nfrom pyspark.sql import functions as F #For use of Window Function\nfrom pyspark.sql import SparkSession   #For initiating PySpark API in Python\n```", "```py\n#Read in game.csv\n\npath_games = \"/directory/game_synthetic.csv\"  #Replace with your own directory and data\ndata_raw_games = pd.read_csv(path_games, encoding = 'ISO-8859-1')\n```", "```py\n#Format the 'game_date' column (if it was defaulted to string at ingestion)\n#into Date format\n\ndata_raw_games['GAME_DATE'] = pd.to_datetime(data_raw_games['game_date'], \\\n                              format='%Y-%m-%d')\n```", "```py\n#Create a 'GAME_DATE_minus_ONE' column for each row\n\ndata_raw_games['GAME_DATE_minus_ONE'] = pd.DatetimeIndex(data_raw_games['GAME_DATE']) \\\n                              + pd.DateOffset(-1)\n```", "```py\n#Create two dataframes, one for results of home teams and \n#one for results of away teams, and merge at the end\n\ndata_games_frame_1 = data_raw_games.sort_values(['game_id'])\ndata_games_frame_2 = data_raw_games.sort_values(['game_id'])\n\ndata_games_frame_1['TEAM_ID'] = data_games_frame_1['team_id_home']\ndata_games_frame_2['TEAM_ID'] = data_games_frame_2['team_id_away']\n\ndata_games_frame_1['WIN_FLAG'] = (data_games_frame_1['win_loss_home'] == 'W')\ndata_games_frame_2['WIN_FLAG'] = (data_games_frame_1['win_loss_home'] != 'W')\n\ndata_games_frame_1['TEAM_NAME'] = data_games_frame_1['team_name_home']\ndata_games_frame_2['TEAM_NAME'] = data_games_frame_2['team_name_away']\n\ndata_games_frame_1['TEAM_NAME_OPP'] = data_games_frame_1['team_name_away']\ndata_games_frame_2['TEAM_NAME_OPP'] = data_games_frame_2['team_name_home']\n\ndata_games_frame_1['HOME_FLAG'] = 'Home'\ndata_games_frame_2['HOME_FLAG'] = 'Away'\n\n#Merge the two dataframes above\ndata_games = pd.concat([data_games_frame_1, data_games_frame_2], axis = 0).drop(['team_id_home', 'team_id_away'], axis = 1)\\\n                              .sort_values(['game_id']).reset_index(drop = True)\n```", "```py\n#Select relevant columns from the dataset\n\ncol_spark = [\n\n               'GAME_DATE'\n              ,'GAME_DATE_minus_ONE'\n              ,'TEAM_ID'\n              ,'TEAM_NAME'\n              ,'TEAM_NAME_OPP'\n              ,'HOME_FLAG'\n              ,'WIN_FLAG'\n              ,'SCORE'\n              ,'season_id'             \n\n              ]\n\ndf_spark_feed = data_games[col_spark]\n```", "```py\n#Initiate PySpark session\n\nspark_1= SparkSession.builder.appName('app_1').getOrCreate()\ndf_1 = spark_1.createDataFrame(df_spark_feed)\n\n#Create window by each team\nWindow_Team_by_Date = Window.partitionBy(\"TEAM_ID\").orderBy(\"GAME_DATE\")\n\n#Return date of previous game using the lag function\ndf_spark = df_1.withColumn(\"GAME_DATE_PREV_GAME\", F.lag(\"GAME_DATE\", 1).over(Window_Team_by_Date)) \\\n#Flag back-to-back games using a when statement  \n               .withColumn(\"Back_to_Back_FLAG\", F.when(F.col(\"GAME_DATE_minus_ONE\") == F.col(\"GAME_DATE_PREV_GAME\"), 1) \\\n               .otherwise(0)) \n\n#Convert Spark dataframe to Pandas dataframe\ndf = df_spark.toPandas()\n```", "```py\n#Select relevant columns\n\ncol = [\n         'TEAM_NAME'\n        ,'TEAM_NAME_OPP'\n        ,'GAME_DATE'\n        ,'HOME_FLAG'\n        ,'WIN_FLAG'\n\n      ]\n\n#Filter for back-to-back games\ndf_b2b_interim = df[df['Back_to_Back_FLAG'] == 1]\n\n#Show selected columns only\ndf_b2b = df_b2b_interim[col].sort_values(['TEAM_NAME', 'GAME_DATE']).reset_index(drop = True)\n```", "```py\n#Create window by season ID\n\nWindow_Team_by_Season = Window.partitionBy(\"TEAM_ID\").orderBy(\"season_id\")\n```", "```py\n spark_1= SparkSession.builder.appName('app_1').getOrCreate()\ndf_1 = spark_1.createDataFrame(df_spark_feed)\n\nWindow_Team = Window.partitionBy(\"TEAM_ID\").orderBy(\"HOME_FLAG\")\ndf_spark = df_1.withColumn(\"SCORE_AVG\", F.avg(\"SCORE\").over(Window_Team)) \\\n               .withColumn(\"SCORE_STD\", F.stddev(\"SCORE\").over(Window_Team))\n\ndf = df_spark.toPandas()\ndf.groupby(['TEAM_NAME', 'HOME_FLAG'])[\"SCORE_AVG\", \"SCORE_STD\"].mean()\n```"]