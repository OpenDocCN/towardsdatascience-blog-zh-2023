- en: Building Machine Learning Operations for Businesses
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/building-machine-learning-operations-for-businesses-6d0bfbbf2139](https://towardsdatascience.com/building-machine-learning-operations-for-businesses-6d0bfbbf2139)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Blueprint for Effective MLOps to Support Your AI Strategy
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://johnadeojo.medium.com/?source=post_page-----6d0bfbbf2139--------------------------------)[![John
    Adeojo](../Images/f6460fae462b055d36dce16fefcd142c.png)](https://johnadeojo.medium.com/?source=post_page-----6d0bfbbf2139--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6d0bfbbf2139--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6d0bfbbf2139--------------------------------)
    [John Adeojo](https://johnadeojo.medium.com/?source=post_page-----6d0bfbbf2139--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6d0bfbbf2139--------------------------------)
    ·11 min read·Jun 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ea4fe645f53e8e55cf846fb65f88f51.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Generated with Midjourney'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Background — Navigating MLOps
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In my career, I’ve noticed that the key to successful AI strategies lies in
    the ability to deploy machine learning models into production, thus unlocking
    their commercial potential at scale. Yet, this is no small feat — it involves
    the integration of various technologies, teams, and often necessitates a cultural
    shift within organisations, a system referred to as MLOps.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: However, there’s no one-size-fits-all MLOps strategy. In this piece, I offer
    a flexible MLOps blueprint that can be a starting point or a means to fine-tune
    your current workflow. Although the MLOps journey can be complex, I strongly advise
    viewing it as an indispensable initial step in integrating AI into your business,
    rather than a secondary consideration.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: MLOps Goes Beyond Technology
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/901a4d0642db9fedd5769cf01e93fb4f.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: The components of successful MLOps strategies'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Before diving into the technicalities, I’d like to share (non-technical) insights
    from my experience observing various MLOps strategies. MLOps is more than just
    technology — it hinges on three key components: Investment, Culture, and Technology.
    Companies that have considered all three from the outset tend to have more success
    with their strategies. A common mistake I’ve seen is businesses prioritising investment
    in solutions without considering requisite cultural shifts. This oversight could
    critically undermine your strategy, potentially wasting funds and diminishing
    confidence from your executives or investors'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Culture
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Introducing a new culture to any business is no mean feat requiring wholehearted
    support from its people. A common pitfall I have seen is when businesses abruptly
    replace old tools with new, shiny ones without considering cultural change. This
    approach can breed resentment and result in these tools being overlooked or misused.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: On the contrary, companies managing cultural change effectively have involved
    end users in crafting the MLOps strategy and assigned them responsibilities promoting
    ownership. Moreover, they’ve furnished essential support and training to upskill
    users rewarding engagement in these initiatives.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: A solution may indeed be technically superior, but without driving cultural
    change, it risks inefficacy. After all, it’s people who operate technologies,
    not the other way around.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Technology
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the sake of brevity, I’ve defined technology as a combination of both the
    technical infrastructure and data management services.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: An effective MLOps strategy is built on top of a mature data ecosystem. By leveraging
    data management tools, data scientists should be empowered to access data for
    model development in a secure and regulatory compliant way.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: From the viewpoint of technical infrastructure, we should be empowering data
    scientists and ML engineers to access the hardware and software required to facilitate
    the development and delivery of AI products. For many companies, leveraging cloud
    infrastructure is essential an enabler for this.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Investment
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are no shortcuts in MLOps, particularly when it comes to investment. An
    efficient MLOps strategy should prioritise investments in both people and technology.
    A recurring issue I encounter with clients is the tendency to construct an MLOps
    strategy centred on a single data scientist due to budget constraints. In such
    cases, I generally recommend a reassessment, or at the very least, a tempering
    of expectations.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: From the outset, it’s imperative to establish the extent of your investment
    in innovation and its duration. In truth, ongoing investment is vital if you wish
    for AI to become fundamental to your operations and to yield the associated benefits.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '*For a view on developing AI strategies, you may wish to read my article on
    crafting AI strategies with Wardley Maps:*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[](/building-ai-strategies-for-businesses-7b2e900399b7?source=post_page-----6d0bfbbf2139--------------------------------)
    [## Building AI Strategies for Businesses'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: The art of crafting an AI strategy through Wardley Maps
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/building-ai-strategies-for-businesses-7b2e900399b7?source=post_page-----6d0bfbbf2139--------------------------------)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: A High-level Blueprint for MLOps
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we’ve laid the groundwork, we shall delve into some of the technical
    components of MLOps. To aid visualisation, I’ve designed a flowchart illustrating
    relationships between the processes. Where dashed lines are present, data flows.
    Where a solid line exists, there’s a transition from one activity to another.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eba6feab722166c41b33f42095f3ae24.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: High Level MLOps Workflow'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Model Development Laboratory
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of model development is inherently unpredictable and iterative.
    Firms that fail to recognise this will struggle to build effective AI strategies.
    In truth, model development tends to be the most chaotic aspect of the workflow,
    filled with experimentation, repetition, and frequent failures. All these elements
    are essential in exploring new solutions; this is where innovation is born. Thus,
    what do data scientists need? The freedom to experiment, innovate, and collaborate.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: There’s a prevailing belief that data scientists should be adhering to software
    engineering best practices in their code writing. Whilst I don’t disagree with
    this sentiment, there’s a time and place for everything. I don’t believe that
    model development labs are necessarily the arena for this. Instead of attempting
    to quell this chaos, we should embrace it as a necessary part of the workflow,
    and seek to utilise tools that help us to manage it — an effective model development
    lab should provide this. Let’s examine some potential components.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Experimentation & Prototyping — Jupyter Labs
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Jupyter Labs](https://jupyter.org/) offers a versatile Integrated Development
    Environment (IDE) suitable for the creation of preliminary models and proof-of-concepts.
    It provides access to notebooks, scripts, and command line interfaces, all features
    that are often well known to data scientists.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: As an open-source tool, Jupyter Labs boasts seamless integration with Python
    and R, encompassing the majority of contemporary data science model development
    tasks. Most data science workloads can be conducted in the lab IDE.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Environment Management — Anaconda
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/b7931c9d60f0ff84046c434571a5fdfa.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Schematic of Anaconda virtual environments and model sharing
    in model development labs'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Effective environment management can streamline subsequent MLOps workflow steps,
    focusing on safe access to open-source libraries and reproducing the development
    environment. [Anaconda](https://docs.anaconda.com/), a package manager, allows
    data scientists to create virtual environments and install necessary libraries
    and packages for model development with its simple Command-Line Interface (CLI).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Anaconda also offers repository mirroring, which assesses open-source packages
    for secure commercial use, though the associated risks of third-party management
    should be considered. The use of virtual environments is crucial in managing the
    experimental phase, essentially providing a contained space for all packages and
    dependencies for a given experiment.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Version Control & Collaboration — GitHub Desktop
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Collaboration is a crucial part of a successful model development lab, and leveraging
    [GitHub Desktop](https://desktop.github.com/) is an effective way to facilitate
    this. Data scientists, through GitHub Desktop, can create a repo for each lab.
    Each repo stores the model development notebook or script, along with an *environment.yml*
    file that instructs Anaconda on how to reproduce the environment in which the
    notebook was developed on another machine.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: The combination of all three lab components Jupyter Labs, Anaconda, and GitHub
    provides data scientists with a safe space to experiment, innovate, and collaborate.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Model Pipeline Development
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In my discussions with clients who are in the early stages of their MLOps maturity,
    there seems to be this idea that data scientists develop models and then “hand
    over” to machine learning engineers to “productionise”. This approach doesn’t
    work and is probably the quickest way to lose your machine learning engineers.
    Nobody wants to deal with someone else’s messy code, and quite frankly it’s unfair
    to expect this of your engineers.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, organisations need to foster a culture where data scientists are responsible
    for developing models within data labs and then formalising them as end-to-end
    model pipelines. Here’s why:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists understand their models better than anyone else. Making them
    responsible for creating the model pipeline will improve efficiency.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You establish a culture of software engineering best practices at every stage
    of development.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning engineers can focus on aspects of their job that add value,
    such as resource provisioning, scaling, automation, instead of refactoring someone’s
    notebook.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building end-to-end pipelines may seem daunting at first, but thankfully there
    are tools targeted at data scientists to help them achieve this.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Model Pipeline Build — Kedro
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Kedro](https://kedro.org/) is a Python open-source framework from [McKinsey
    Quantum Black](https://www.mckinsey.com/capabilities/quantumblack/how-we-help-clients)
    to assist data scientists in building model pipelines.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Kedro provides a standard template for building end-to-end model pipelines
    with software engineering best practices. The concept behind it is to encourage
    data scientists to build modular, reproducible, and maintainable code. Once a
    data scientist completes the Kedro workflow, they’ve essentially built something
    that can be more easily deployed to a production environment. Here are the overarching
    concepts:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '**Project Template**: Kedro provides a standard and easy-to-use project template,
    enhancing structure, collaboration, and efficiency.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Catalog**: The Data Catalog in Kedro is the registry of all data sources
    that the project can use. It provides a straightforward way to define how and
    where data is stored.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data engineering Catalog as defined by Kedro project taken from [https://docs.kedro.org/en/0.18.1/faq/faq.html](https://docs.kedro.org/en/0.18.1/faq/faq.html)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipelines**: Kedro structures your data processing as a pipeline of dependent
    tasks, enforcing a clear code structure and visualising data flow and dependencies.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nodes**: In Kedro, a Node is a wrapper for a Python function that names the
    inputs and outputs of that function, serving as the building blocks of a Kedro
    pipeline.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Configuration**: Kedro manages different configurations for various environments
    (development, production, etc.) without hardcoding any configuration into your
    code.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**I/O**: In Kedro, I/O operations are abstracted from the actual computation,
    which increases code testability and modularity and eases switching between different
    data sources.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modularity and Reusability**: Kedro promotes a modular coding style that
    results in reusable, maintainable and testable code.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing**: Kedro integrates with PyTest, a testing framework in Python, making
    it easy to write tests for your pipeline.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Versioning**: Kedro supports versioning for data and code, enabling reproduction
    of any previous state of your pipeline.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logging**: Kedro offers a standardised logging system for tracking events
    and changes.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hooks and Plugins**: Kedro supports hooks and plugins, extending the framework
    capabilities as per project requirements.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with other tools**: Kedro can be integrated with various tools
    like Jupyter notebook, Dask, Apache Spark, and others to facilitate different
    aspects of a data science workflow.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All Kedro projects follow this basic template. Enforcing this standard across
    your data science teams will enable reproducibility and maintainability.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more extensive overview of the Kedro framework, please visit these resources:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'Kedro Documentation: [link](https://docs.kedro.org/en/stable/)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Importance of Layered thinking in Data Engineering: [link](/the-importance-of-layered-thinking-in-data-engineering-a09f685edc71)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Registry & Storage — Data Version Control (DVC)
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Registry and storage underpin reproducibility in machine learning, something
    that any business looking to incorporate ML should bear in mind. ML models are
    essentially composed of code, data, model artefacts, and environment — all of
    which must be traceable for reproducibility.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[DVC](https://dvc.org/doc/start/data-management/data-versioning?tab=Windows-Cmd-)
    is a tool that provides version control and tracking for models and data. While
    GitHub could be an alternative, it’s limited in its capacity to store large objects,
    posing issues for extensive datasets or models. DVC essentially extends Git, offering
    the same version control capabilities while enabling storage of larger datasets
    and models in a DVC repo, which can be either local or cloud-based.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: In commercial settings, there are obvious security benefits to versioning your
    code in a Git repo, while storing actual model artefacts and data separately in
    a controlled environment.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '*Remember, model reproducibility will become increasingly important as regulations
    tighten around the use of AI commercially. Reproducibility facilitates auditability.*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Model Pipeline Deployment — Docker
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/17e1b55c335f63bf75678d6fc0f64fdb.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Schematic of Docker deployment of inference pipeline. Note
    this same approach can be applied to the model monitoring and retraining pipeline'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Deployment isn’t merely a single task but rather a meticulously crafted fusion
    of tools, activities, and processes; [Docker](https://docs.docker.com/) ties all
    these together for model deployment. Crucial for intricate ML applications with
    numerous dependencies, Docker ensures consistency across any machine by encapsulating
    the application with its environment.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: The process begins with a Dockerfile; Docker then uses its commands to construct
    an image, a ready-packaged model pipeline fit for any Docker-enabled machine.
    Teamed with Kedro’s pipeline functionality, Docker can proficiently deploy both
    model retraining and inference pipelines, assuring reproducibility across all
    stages of the ML workflow.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Model Monitoring & Retraining Pipeline — MLflow
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over time, machine learning models suffer from performance deterioration, which
    can be due to [concept drift](https://machinelearningmastery.com/gentle-introduction-concept-drift-machine-learning/)
    or [data drift](https://www.datacamp.com/tutorial/understanding-data-drift-model-drift).
    We want to be able to monitor when our models’ performance begins to falter and
    re-train them when necessary. [MLflow](https://mlflow.org/docs/latest/index.html)
    provides us the ability to do this via its tracking API. The tracking API should
    be incorporated into the model training and inference pipelines built by the data
    scientists. Although I have specified MLflow for tracking in the model monitoring
    and retraining pipeline, tracking can also be done in the model development lab,
    particularly for experiment tracking.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: The Inference Endpoint
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given that the inference pipeline has been encapsulated into a Dockerfile, we
    can create a Docker image of the pipeline anywhere to be used as an API endpoint
    for any app. Depending on the use case, we will have to decide where we deploy
    the Docker image. That, however, is beyond the scope of this article.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Roles & Responsibilities
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Assigning distinct roles and responsibilities within MLOps is pivotal to its
    success. The multifaceted nature of MLOps, which spans across disciplines necessitates
    a clear demarcation of roles. This ensures that each task is performed efficiently.
    Further, it fosters accountability, facilitating a quicker resolution of issues.
    Lastly, clear delegation reduces confusion and overlap, making the team more efficient
    and helping to maintain a harmonious working environment. It’s much like a well-oiled
    machine, with each cog playing its part to perfection.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Data Scientists
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Role**: The main function of data scientists within MLOps strategies is to
    concentrate on model development. This encompasses initial experiments, prototyping
    and setting up modelling pipelines for validated models.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Responsibilities**: Data scientists ensure models adhere to machine learning
    best practices and align with business cases. Beyond lab tasks, they engage with
    business stakeholders to identify impactful solutions. They take full ownership
    for the data labs, a lead data scientist should set the operating rhythm and best
    practices for setting up labs.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine Learning Engineers
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Role**: ML engineers oversee the MLOps’ technical infrastructure, exploring
    innovative solutions, crafting strategies alongside data scientists, and enhancing
    process efficiencies.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Responsibilities**: They ensure the functionality of the technical infrastructure,
    monitor performance of components to control costs, and confirm production models
    meet demand at the required scale.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Governance Professionals
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Role**: Data governance professionals maintain security and data privacy
    policies, playing a pivotal role in the secure transfer of data within the MLOps
    framework.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Responsibilities**: Although data governance is everyone’s responsibility,
    these professionals create policies and ensure compliance through regular checks
    and audits. They keep up with regulations and ensure compliance from all data
    consumers.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Navigating the realm of MLOps is a task that demands deliberate planning, the
    right blend of technology and talent, and an organisational culture that endorses
    change and learning.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: The journey may appear complex, but by employing a well-designed blueprint and
    by approaching MLOps as a holistic, iterative process rather than a one-off project,
    you can derive immense value from your AI strategies. Remember, though, that no
    single approach fits every scenario. It’s crucial to tailor your strategy to your
    specific needs and to remain agile to changing circumstances.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '*Follow me on* [*LinkedIn*](https://www.linkedin.com/in/john-adeojo/)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '*Subscribe to medium to get more insights from me:*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://johnadeojo.medium.com/membership?source=post_page-----6d0bfbbf2139--------------------------------)
    [## Join Medium with my referral link — John Adeojo'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: I share data science projects, experiences, and expertise to assist you on your
    journey You can sign up to medium via…
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: johnadeojo.medium.com](https://johnadeojo.medium.com/membership?source=post_page-----6d0bfbbf2139--------------------------------)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '*Should you be interested in integrating AI or data science into your business
    operations, we invite you to schedule a complimentary initial consultation with
    us:*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.data-centric-solutions.com/book-online?source=post_page-----6d0bfbbf2139--------------------------------)
    [## Book Online | Data-Centric Solutions'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Discover our expertise in helping businesses achieve ambitious goals with a
    free consultation. Our data scientists and…
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.data-centric-solutions.com](https://www.data-centric-solutions.com/book-online?source=post_page-----6d0bfbbf2139--------------------------------)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.data-centric-solutions.com](https://www.data-centric-solutions.com/book-online?source=post_page-----6d0bfbbf2139--------------------------------)'
