- en: 'L1 vs L2 Regularization in Machine Learning: Differences, Advantages and How
    to Apply Them in Python'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/l1-vs-l2-regularization-in-machine-learning-differences-advantages-and-how-to-apply-them-in-72eb12f102b5](https://towardsdatascience.com/l1-vs-l2-regularization-in-machine-learning-differences-advantages-and-how-to-apply-them-in-72eb12f102b5)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Delving into L1 and L2 regularization techniques in Machine Learning to explain
    why they are important to prevent model overfitting*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@theDrewDag?source=post_page-----72eb12f102b5--------------------------------)[![Andrea
    D''Agostino](../Images/58c7c218815f25278aae59cea44d8771.png)](https://medium.com/@theDrewDag?source=post_page-----72eb12f102b5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----72eb12f102b5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----72eb12f102b5--------------------------------)
    [Andrea D''Agostino](https://medium.com/@theDrewDag?source=post_page-----72eb12f102b5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----72eb12f102b5--------------------------------)
    ¬∑8 min read¬∑Feb 23, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/423a619cb1e90d37b508328d6453e1d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning is a discipline that is experiencing enormous development in
    the technological and industrial fields.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to its algorithms and modeling techniques, it is possible to build models
    capable of learning from past data, generalizing and making predictions on new
    data.
  prefs: []
  type: TYPE_NORMAL
- en: However, in some cases, **models can overfit the training data and lose their
    ability to generalize**. This phenomenon is called *overfitting*.
  prefs: []
  type: TYPE_NORMAL
- en: It is quite important for analysts to understand what overfitting is and why
    it represents one of the main obstacles in machine learning when it comes to creating
    a predictive model.
  prefs: []
  type: TYPE_NORMAL
- en: A general idea of overfitting is this one
  prefs: []
  type: TYPE_NORMAL
- en: When a model is too complex or fits too well with the training data, it can
    become very accurate for that specific data, but will generalize poorly on data
    it has never seen before. **This means that the model will be ineffective when
    applied to new data in real life**.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Want to learn more about overfitting? Read the article titled [*Overcome the
    biggest obstacle in machine learning: Overfitting*](https://medium.com/towards-data-science/overcome-the-biggest-obstacle-in-machine-learning-overfitting-cca026873970)published
    on TDS'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Regularization techniques can be used to prevent overfitting.**'
  prefs: []
  type: TYPE_NORMAL
- en: The term *regularization* encompasses a set of techniques that **tend to simplify
    a predictive model**. In this article, we will focus on two regularization techniques,
    **L1 and L2**, explain their differences and show how to apply them in Python.
  prefs: []
  type: TYPE_NORMAL
- en: What is regularization and why is it important?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In simple terms, regularizing a model means changing its learning behavior during
    the training phase.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization helps prevent overfitting by **adding a penalty on model complexity**
    ‚Äî if a model is too complex, it will be penalized during training, which helps
    maintain a good balance between the model‚Äôs complexity and its ability to generalize
    about data he has never seen before.
  prefs: []
  type: TYPE_NORMAL
- en: To add an L1 or L2 regularization, **we are going to alter the loss function
    of the model**. This is the function that the learning algorithm tries to optimize
    during the training phase.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization occurs by assigning a penalty that increases based on how complex
    the model becomes.
  prefs: []
  type: TYPE_NORMAL
- en: If we take linear regression as an example, MSE (mean squared error ‚Äî read more
    about the evaluation metrics of a regression model here) is the typical loss function
    and can be expressed as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/76b22fdf87b529c31efc3660b8bd724d.png)'
  prefs: []
  type: TYPE_IMG
- en: Where the goal of the algorithm is to minimize the difference between the prediction
    *f(x)* and the observed *y*.
  prefs: []
  type: TYPE_NORMAL
- en: In the equation, *f(x)* is the regression line, and this will have be equal
    to
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c521b27104c9033d345afdac196b95ad.png)'
  prefs: []
  type: TYPE_IMG
- en: The algorithm will therefore have to find the values of the parameters *w* and
    *b* from the training set by minimizing MSE.
  prefs: []
  type: TYPE_NORMAL
- en: A model is considered less complex if some parameters *w* are close to or equal
    to zero.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: L1 vs L2 regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let‚Äôs see the differences between L1 and L2 regularization.
  prefs: []
  type: TYPE_NORMAL
- en: L1 regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: L1 regularization, **also known as ‚ÄúLasso‚Äù**, adds a penalty on the sum of the
    absolute values of the model weights.
  prefs: []
  type: TYPE_NORMAL
- en: '**This means that weights that do not contribute much to the model will be
    zeroed**, which can lead to automatic feature selection (as weights corresponding
    to less important features will in fact be zeroed).'
  prefs: []
  type: TYPE_NORMAL
- en: This makes L1 particularly useful for feature selection problems and sparse
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Taking the MSE formula above as an example, an L1 regularization would look
    like this
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5946f1f3ce1469a686f87f68e1975f7d.png)'
  prefs: []
  type: TYPE_IMG
- en: where *C* is a **model hyperparameter that controls the intensity of the regularization.**
    The higher the value of *C*, the more our weights will tend towards zero.
  prefs: []
  type: TYPE_NORMAL
- en: In jargon, this is would be called **a sparse model**, where most of the parameters
    have the value of zero.
  prefs: []
  type: TYPE_NORMAL
- en: The risk here is that **a very high value of *C* will cause the model to underfit**,
    which is the opposite of overfitting ‚Äî i.e. it won‚Äôt capture the patterns in our
    data.
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On the other hand, L2 regularization, also called **Ridge regularization**,
    adds the square of the weights to the regularization term.
  prefs: []
  type: TYPE_NORMAL
- en: '**This means that larger weights are reduced but not zeroed**, which leads
    to models with fewer variables than L1 regularization but with more distributed
    weights.'
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization is especially useful when you have many highly correlated
    variables, as it tends to ‚Äúspread‚Äù the weight across all the variables instead
    of focusing on just a few of them.
  prefs: []
  type: TYPE_NORMAL
- en: As before, let‚Äôs see how the initial equation changes to integrate L2
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a0e14dbcb4f39719b2ff763aa445b6b2.png)'
  prefs: []
  type: TYPE_IMG
- en: L2 regularization can improve model stability when training data is noisy or
    incomplete, by reducing the impact of outliers or noise on variables.
  prefs: []
  type: TYPE_NORMAL
- en: How to apply regularization in Sklearn and Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example we will see how to applyregularization to a logistic regression
    model for a classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: '**We will see how the performance changes for different values of C** and compare
    how accurate the model is in modeling the input data.'
  prefs: []
  type: TYPE_NORMAL
- en: We will use the famous *breast cancer dataset* from Sklearn. Let‚Äôs start by
    seeing how to import it, along with all the libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Being a classification problem, we will use accuracy to measure the performance
    of the model. Read one of my articles on [how to measure the performance of binary
    classification models](https://medium.com/towards-data-science/the-explanation-you-need-on-binary-classification-metrics-321d280b590f)
    if you are interested in learning more.
  prefs: []
  type: TYPE_NORMAL
- en: Now let‚Äôs create a function to apply the comparison between L1 and L2 regularization
    on the dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We apply this logic by looking at the L1 regularization
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3a79f0cff34cf347a844335ebcbc87b0.png)'
  prefs: []
  type: TYPE_IMG
- en: How L1 regularization impacts model performance. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: We see how the L1 regularization flattens the model coefficients close to zero
    for many levels of C. **The coefficients with the highest values are, according
    to the model, the most important features for the prediction.**
  prefs: []
  type: TYPE_NORMAL
- en: We also see the onset of overfitting ‚Äî at *C=100*, the performance of the training
    set increases while that in the test set decreases.
  prefs: []
  type: TYPE_NORMAL
- en: We now apply the same function to evaluate the effects of L2.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/213b3269b4e3e6e88891fe5e904ae8ad.png)'
  prefs: []
  type: TYPE_IMG
- en: How L2 regularization impacts model performance. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The coefficients are always above zero, **thus creating a distribution of gradually
    increasing weights for the most relevant features**. We note a very slight overfitting
    after the value of *C=100.*
  prefs: []
  type: TYPE_NORMAL
- en: Other regularization techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to L1 and L2 regularizations, there are other regularization techniques
    that can be used in machine learning models. Among these techniques we find **dropout
    and early stopping.**
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dropout is a technique used in neural networks to reduce overfitting. **Dropout
    works by randomly shutting down some neurons during the training phase**, forcing
    the neural network to find alternative ways to represent the data.
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Early stopping is another technique used to avoid overfitting in machine learning
    models. **This technique consists of stopping model training when the performance
    on the validation set starts to deteriorate**. This prevents the model from overlearning
    the training data and not generalizing well on data not seen before.
  prefs: []
  type: TYPE_NORMAL
- en: Want to learn more about early stopping? Read the article titled [*Early Stopping
    in TensorFlow ‚Äî prevent overfitting of a neural network*](/control-the-training-of-your-neural-network-in-tensorflow-with-callbacks-ba2cc0c2fbe8)published
    on TDS
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In general, overfitting can be avoided by using a combination of regularization
    techniques. However, the choice of the most appropriate techniques will depend
    on the characteristics of the dataset and the machine learning model used.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, regularization is an important machine learning technique that
    helps improve model performance **by avoiding overfitting on training data.**
  prefs: []
  type: TYPE_NORMAL
- en: L1 and L2 regularizations are the most used techniques for this purpose, but
    there are others too that can be useful depending on context. For example, dropout
    is almost always seen in the context of deep learning, that is with neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: In our example we have seen how regularization affects the performance of the
    logistic regression model and how the value of C affects the regularization itself.
    We also examined how model coefficients change as the value of C changes, and
    how L1 and L2 regularization affect model coefficients differently.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for taking the time to read my article! üòä
  prefs: []
  type: TYPE_NORMAL
- en: Till next time!
  prefs: []
  type: TYPE_NORMAL
- en: '**If you want to support my content creation activity, feel free to follow
    my referral link below and join Medium‚Äôs membership program**. I will receive
    a portion of your investment and you‚Äôll be able to access Medium‚Äôs plethora of
    articles on data science and more in a seamless way.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@theDrewDag/membership?source=post_page-----72eb12f102b5--------------------------------)
    [## Join Medium with my referral link - Andrea D''Agostino'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Andrea D'Agostino (and thousands of other writers on Medium).
    Your membership fee directly‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@theDrewDag/membership?source=post_page-----72eb12f102b5--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Recommended Reads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the interested, here are a list of books that I recommended for each ML-related
    topic. There are ESSENTIAL books in my opinion and have greatly impacted my professional
    career.
  prefs: []
  type: TYPE_NORMAL
- en: '*Disclaimer: these are Amazon affiliate links. I will receive a small commission
    from Amazon for referring you these items. Your experience won‚Äôt change and you
    won‚Äôt be charged more, but it will help me scale my business and produce even
    more content around AI.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Intro to ML**: [*Confident Data Skills: Master the Fundamentals of Working
    with Data and Supercharge Your Career*](https://amzn.to/3WZ51cE)by Kirill Eremenko'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sklearn / TensorFlow**: [*Hands-On Machine Learning with Scikit-Learn, Keras,
    and TensorFlow*](https://amzn.to/3jseVGb) by Aurelien G√©ron'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NLP**: [*Text as Data: A New Framework for Machine Learning and the Social
    Sciences*](https://amzn.to/3l9FO22)by Justin Grimmer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sklearn / PyTorch**: [*Machine Learning with PyTorch and Scikit-Learn: Develop
    machine learning and deep learning models with Python*](https://amzn.to/3wYZf0e)
    by Sebastian Raschka'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Viz**: [*Storytelling with Data: A Data Visualization Guide for Business
    Professionals*](https://amzn.to/3HUtGtB) by Cole Knaflic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Useful Links (written by me)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Learn how to perform a top-tier Exploratory Data Analysis in Python**: [*Exploratory
    Data Analysis in Python ‚Äî A Step-by-Step Process*](/exploratory-data-analysis-in-python-a-step-by-step-process-d0dfa6bf94ee)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learn the basics of TensorFlow**: [*Get started with TensorFlow 2.0 ‚Äî Introduction
    to deep learning*](https://medium.com/towards-data-science/a-comprehensive-introduction-to-tensorflows-sequential-api-and-model-for-deep-learning-c5e31aee49fa)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perform text clustering with TF-IDF in Python**: [*Text Clustering with TF-IDF
    in Python*](https://medium.com/mlearning-ai/text-clustering-with-tf-idf-in-python-c94cd26a31e7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
