- en: '3D Deep Learning Python Tutorial: PointNet Data Preparation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f](https://towardsdatascience.com/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hands-On Tutorial, Deep Dive, 3D Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Ultimate Python Guide to structure large LiDAR point cloud for training
    a 3D Deep Learning Semantic Segmentation Model with the PointNet Architecture.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@florentpoux?source=post_page-----90398f880c9f--------------------------------)[![Florent
    Poux, Ph.D.](../Images/74df1e559b2edefba71ffd0d1294a251.png)](https://medium.com/@florentpoux?source=post_page-----90398f880c9f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----90398f880c9f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----90398f880c9f--------------------------------)
    [Florent Poux, Ph.D.](https://medium.com/@florentpoux?source=post_page-----90398f880c9f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----90398f880c9f--------------------------------)
    ¬∑30 min read¬∑May 31, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23e707b7b223e287573f055b8cbb2a60.png)'
  prefs: []
  type: TYPE_IMG
- en: This creative illustration visually highlights how 3D Deep Learning could represent
    a top-down scene in a way it is easy to separate between classes. If you like
    these, contact [Marina T√ºnsmeyer](http://mimatelier.com/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The application field of 3D deep learning has snowballed in recent years. We
    have superb applications in various areas, including robotics, autonomous driving
    & mapping, medical imaging, and entertainment. When we look at the results, we
    are often awed (but not all the time üòÅ), and we may think: ‚ÄúI will use this model
    right now for my application!‚Äù. But unfortunately, the nightmare begins: 3D Deep
    Learning implementation. Even if new coding libraries aim at simplifying the process,
    implementing an end-to-end 3D Deep Learning model is a feat, especially if you
    are isolated in a dark corner of some gloomy cave.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a17e508f9b1bc782eb13b6b65a6cde85.png)'
  prefs: []
  type: TYPE_IMG
- en: This is how it feels to code 3D Deep Learning. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: One of the most overlooked pain points in 3D deep learning frameworks is preparing
    the data to **be usable** by a selected learning architecture. I do not mean a
    nice research dataset but an actual (messy) data silo on which you want to develop
    an application. This is even steeper in the case of large and complex 3D point
    cloud datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Oh, but do you see where we are going with this article? You dreamed it (Don‚Äôt
    hide it, I know üòâ), and we will cover it at the proper coding depth. This hands-on
    tutorial explores how to efficiently prepare 3D point clouds from an Aerial LiDAR
    campaign to be used with the most popular 3D deep learning point-based model:
    the PointNet Architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: We cover the entire data preparation pipeline, from 3D data curation to feature
    extraction and normalization. It provides the knowledge and practical Python skills
    to tackle real-world 3D Deep Learning problems.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e3562ca370d6d278c11635d8b34757d.png)'
  prefs: []
  type: TYPE_IMG
- en: The PointNet Data Preparation Workflow for 3D Semantic Segmentation. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: By following this tutorial, you will be able to apply these techniques to your
    own 3D point cloud datasets and use any of them to train a PointNet Semantic Segmentation
    model. Are you ready?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'üéµ**Note to Readers***: This hands-on guide is part of a* [***UTWENTE***](https://www.itc.nl/)
    *joint work with my dear colleague* [***Prof. Sander Oude Elberink***](https://people.utwente.nl/s.j.oudeelberink)*.
    We acknowledge the financial contribution from the digital twins* [*@ITC*](http://twitter.com/ITC)
    *-project granted by the ITC faculty of the University of Twente.*'
  prefs: []
  type: TYPE_NORMAL
- en: 3D Deep Learning Essentials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 3D Semantic Segmentation VS Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The fundamental difference between 3D semantic segmentation and classification
    for 3D point clouds is that segmentation aims to assign a label to each point
    in the point cloud. In contrast, classification seeks to assign a single label
    to the entire cloud.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1f556b730b7907f9c25b9033d1c0c5b7.png)'
  prefs: []
  type: TYPE_IMG
- en: The difference between the classification model and the semantic segmentation
    model. In both cases, we pass a point cloud, but for the classification task,
    the whole point cloud is the entity, whereas, in the Semantic Segmentation case,
    each point is an entity to classify. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: For example, using the PointNet architecture, 3D point cloud classification
    involves passing the entire point cloud through the Network and outputting a single
    label representing the entire cloud. In contrast, the semantic segmentation ‚Äúheader‚Äù
    will assign a label to each point in the cloud. The difference in approach is
    because segmentation requires a more detailed understanding of the 3D space represented,
    as it seeks to identify and label individual objects or regions within the point
    cloud. In contrast, classification only requires a high-level understanding of
    the overall shape or composition of the point cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, while 3D semantic segmentation and classification are essential tasks
    for analyzing 3D point cloud data, the main difference is the level of detail
    and granularity required in the labeling process.
  prefs: []
  type: TYPE_NORMAL
- en: And as you guessed it, we will attack semantic segmentation because it requires
    a more detailed understanding of the space being analyzed, which is so much fun
    üòÅ.
  prefs: []
  type: TYPE_NORMAL
- en: But before that, let us take a tiny step back to better grasp how PointNet Architecture
    works, shall we?
  prefs: []
  type: TYPE_NORMAL
- en: 'PointNet: A Point-based 3D Deep Learning Architecture'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Turning complex topics into small chunk-wise bits of knowledge is my specialty.
    But I must admit that, when touching upon 3D Deep Learning, the complexity of
    the function learned through the different processes within the neural Network
    and the empirical nature of hyper-parameter determination are important challenges.
    To overcome, hun? üòÅ
  prefs: []
  type: TYPE_NORMAL
- en: First, let us recap what PointNet is. PointNet is one of the pioneers in Neural
    Networks for 3D deep learning. If you understand PointNet, you can use all the
    other advanced models. But, of course, understanding is only a part of the equation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/977877dcb2d06fa1636fdd3356d96303.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The PointNet Architecture has the ability to attack three semantization applications:
    Classification, Part-Segmentation, and Semantic Segmentation. ¬© F. Poux'
  prefs: []
  type: TYPE_NORMAL
- en: The other part is making the scary thing work and extending it to use it with
    your data! And this is a challenging feat! Even for seasoned coders. Therefore,
    we divide into several parts the process. Today, it is about preparing the data
    so that we are sure we have something that works in real conditions.
  prefs: []
  type: TYPE_NORMAL
- en: To prepare the data correctly, it is essential to understand the building block
    of the Network. Let me give you the critical aspects of what to consider when
    preparing your data with the Network below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f3210ed58bfea5e2afda6856cb79faa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The PoinNet Model Architecture as described by the authors of the paper: [ArXiv
    Paper](https://arxiv.org/pdf/1612.00593.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of PointNet consists of several layers of neural networks that
    process the point cloud data. The input to PointNet is a simple set of points,
    each represented by its 3D coordinates and additional features such as color or
    intensity. These points are fed into successive shared Multi-Layer Perceptron
    (MLP) network that learns to extract local features from each point.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2dd6409b2608bc0f614285a612ac89ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The MLP in the PointNet Architecture as described by the authors of the paper:
    [ArXiv Paper](https://arxiv.org/pdf/1612.00593.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ö **Note**: *An MLP is a neural network of multiple layers of connected nodes
    or neurons. Each neuron in the MLP receives input from the neurons in the previous
    layer, applies a transformation to this input using weights and biases, and then
    passes the result to the neurons in the next layer. The weights and biases in
    the MLP are learned during training using backpropagation, which adjusts them
    to minimize the difference between the Network‚Äôs predictions and the true output.*'
  prefs: []
  type: TYPE_NORMAL
- en: These MLPs are fully connected layers, each followed by what we call ‚Äúa non-linear
    activation function‚Äù (such as ReLU). The number of neurons in each layer (E.g.,
    64) and the number of layers themselves (E.g., 2) can be adjusted depending on
    the specific task and the complexity of the input point cloud data. As you can
    guess, the more neurons and layers, the more complex the targeted problem can
    be because of the combinatorial possibilities given by the architecture plasticity.
    If we continue to explore the PointNet architecture, we see that we describe the
    original n input points, with 1024 features that span from the initial ones provided
    (X, Y, and Z). This is where the architecture provides a global description of
    the input point cloud by using a max-pooling operation to the locally learned
    features to get a global feature vector that summarizes the entire point cloud.
    This global feature vector is then fed through several fully connected layers
    to produce the final output of the Classification head, i.e., the score for k
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cfccefae9c1356f71ebad1bb415fd7a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The MaxPool and MLP of PoinNet Model Architecture as described by the authors
    of the paper: [ArXiv Paper](https://arxiv.org/pdf/1612.00593.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: If you notice closely, the semantic segmentation head in PointNet is a fully
    connected network that concatenates the global feature vector and the local feature
    vectors to produce a per-point score or label for each point in the input point
    cloud data. The semantic segmentation head consists of several fully connected
    layers with ReLU activation functions and a final softmax layer. The output of
    the final softmax layer represents the per-point probability distribution over
    the different semantic labels or classes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91d55f8e974c16c23c108d75eb491b61.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Segmentation Head of the PoinNet Model Architecture as described by the
    paper''s authors: [ArXiv Paper](https://arxiv.org/pdf/1612.00593.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: The PointNet Architecture can capture important geometric and contextual information
    for tasks such as object classification and segmentation in 3D data by learning
    local and global features from each point in the input point cloud. One of the
    critical innovations of PointNet is using a symmetric function in the max-pooling
    operation, which ensures that the output is invariant to the order of the input
    points. This makes PointNet robust to variations in the ordering of the input
    points, which is essential in 3D data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are ready to attack heads on preparing the data for PointNet. Which
    point cloud do we mean in the beginning? Do we feed a complete point cloud?
  prefs: []
  type: TYPE_NORMAL
- en: 'PointNet: Data preparation key aspects'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'On a high-level view, if we study the [original paper published](https://arxiv.org/pdf/1612.00593.pdf),
    we can see that PointNet functions in a very straightforward manner:'
  prefs: []
  type: TYPE_NORMAL
- en: We take a point cloud and normalize the data to a canonical space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We compute a bunch of features (without ingesting our knowledge but by leveraging
    the network capabilities to create cool ones)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We aggregate these features into a global signature for the considered cloud.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Option 1*: we use this global signature to classify the point cloud'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Option 2*: we combine this global signature with the local signature and build
    even sharper features for Semantic Segmentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/7b517b55b4f933bf61007f4e08ab20f5.png)'
  prefs: []
  type: TYPE_IMG
- en: The five steps of PointNet towards either Semantic Segmentation or Classification
    tasks. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: It is all about features, meaning the chunk we provide the Network should be
    very relevant. E.g., giving the entire point cloud will not work, giving a tiny
    sample will not work, and giving structured samples with a different distribution
    than what will be fed will not work. So how do we do that?
  prefs: []
  type: TYPE_NORMAL
- en: Let us follow a linear ten-steps process to obtain well-thought 3D point cloud
    training/inference-ready datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e3562ca370d6d278c11635d8b34757d.png)'
  prefs: []
  type: TYPE_IMG
- en: The PointNet Data Preparation Workflow. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Prepare your working environment'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/e5938db254abaaf0372ea2c43c7c6c0a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this article, we use two main components: CloudCompare and JupyterLab IDE
    (+ Python). For a detailed view of the best possible setup, I strongly encourage
    you to follow this article which goes into well-needed detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----90398f880c9f--------------------------------)
    [## 3D Python Workflows for LiDAR City Models: A Step-by-Step Guide'
  prefs: []
  type: TYPE_NORMAL
- en: The Ultimate Guide to unlocking a streamlined workflow for 3D City Modelling
    Applications. The tutorial covers Python‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----90398f880c9f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: We will have a specific stack of libraries organized into main libraries, plotting
    libraries, and utility libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ö **Note**: *If you work in a local environment, I recommend for this tutorial
    to run pip for your package management* (pip install library_name)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The two main libraries that we will use are NumPy and Pytorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Numpy**: NumPy is a Python library for working with numerical data, and it
    provides functions for manipulating arrays and matrices, mathematical operations,
    and linear algebra functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pytorch**: Pytorch is a popular deep learning framework in Python. It provides
    tools for building and training neural networks and optimizing and evaluating
    models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, we support these with two plotting libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Matplotlib**: Matplotlib is a Python library for creating visualizations
    such as plots, charts, and graphs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plotly**: Plotly is a Python library for creating **interactive** visualizations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And finally, we will also use three utility modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '**os**: The os module in Python provides a way of using operating system-dependent
    functionality. It provides functions for interacting with the file system, such
    as creating, deleting, and renaming files and directories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**glob**: The glob module in Python provides a way of matching files and directories
    using patterns. For example, it can find all files with a specific extension in
    a directory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**random** (Optional): The `random` library is a built-in module that provides
    functions for generating random numbers, selecting random items from a list, and
    shuffling sequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once this is done, we are ready to get to the second aspect: Getting our hands
    on new 3D point cloud datasets!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: 3D Data Curation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/a5e20e985dd57cccb438a4637463b0d9.png)'
  prefs: []
  type: TYPE_IMG
- en: For this tutorial, we go east of the Netherlands, near Enschede, where the University
    of Twente shines üåû. Here, we select a part of the AHN4 dataset, which would have
    a good proportion of trees, ground, buildings, and a bit of water as well üöø. Let
    us say enough in a tile to have sufficient points for each class!
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ö **Note**: *We will train on imbalanced datasets, with a high predominance
    of ground points compared to the other classes. This is not an ideal scenario,
    where the MLP and semantic segmentation head may be biased towards predicting
    the majority class labels and ignore the minority class labels. This can result
    in inaccurate segmentation and misclassification of minority class points. Still,
    several techniques can be used to mitigate the effects of imbalanced classes,
    such as data augmentation, oversampling or undersampling of the minority class,
    and using weighted loss functions. This is for another time.* üòâ'
  prefs: []
  type: TYPE_NORMAL
- en: 'To gather the dataset, we access the open data portal [geotiles.nl](https://geotiles.nl/).
    We zoom in onto a part of interest waiting to have the _XX (to have a data size
    coherent), and then, we download the .laz dataset, as illustrated below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1652f394039a471da8eebb3ae293dd6.png)'
  prefs: []
  type: TYPE_IMG
- en: Gathering a Point Cloud Dataset from the AHN4 LiDAR Campaign in the Netherlands.
    ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: Also, we can prepare some compelling use cases where you would like to test
    your model on tile(s) of interest later. This can be, for example, where you live
    if some open data is available there.
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ö **Note**: *If you want to put later on your model to a true challenge, downloading
    a tile in another land is a great generalization test! For example, you could
    download a* [*LiDAR HD*](https://geoservices.ign.fr/lidarhd) *point cloud tile
    to see the differences/improvements if used for training or testing.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have your point cloud in the .laz file format let us explore the
    characteristics given by the info file that you can also view or download:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c7883e8cee2dad643b7fdd45639c294.png)'
  prefs: []
  type: TYPE_IMG
- en: A Selected informational document on the selected 3D LiDAR Point Cloud dataset.
    ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: This permits a good grasp of the data content, a crucial first step when building
    qualitative datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2cccf5613291347f8208381a21944c7b.png)'
  prefs: []
  type: TYPE_IMG
- en: This shows the content of the additional information on the point cloud. ¬© F.
    Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'As you scroll through the various information points, several fields are interesting
    to note:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This small file selection hints that we will deal with around 32 million data
    points for our experiments, which have colors, intensity, and a Near Infrared
    Field if we want to steepen our model later on.
  prefs: []
  type: TYPE_NORMAL
- en: Very nice! Now that we have the software stack installed and the 3D point cloud
    downloaded, we can jump onto a 3D Data Analysis to ensure the input fed to our
    model holds its promises.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: 3D Data Analysis (CloudCompare)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/ce5da5ed1260e2c915501f876ca57766.png)'
  prefs: []
  type: TYPE_IMG
- en: It is time to load the 3D aerial point cloud file into the software [CloudCompare](https://cloudcompare.org/).
    First, open CloudCompare on your computer until an empty GUI appears, which functions
    as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5835e687656ac5c5a90468fe7fa06d51.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The GUI of CloudCompare. Source: [learngeodata.eu](http://learngeodata.eu)'
  prefs: []
  type: TYPE_NORMAL
- en: From there, we load the .laz file we downloaded by drag-drop and select some
    attributes from the menu that pops out on import, as illustrated below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b203996c036f86e53f4b5ac0b5fd6e28.png)'
  prefs: []
  type: TYPE_IMG
- en: Importing a 3D Point Cloud in CloudCompare. We make sure to select relevant
    features to load. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ö **Note**: *We unselect all fields to pre-select some features that bring
    uncorrelated or low-correlated information and the labels for each point that
    hint at possible ground truth. We will thus only keep the intensity and classification
    field. Indeed, as we target Aerial point clouds, we want something that can generalize
    quite efficiently. Therefore, we aim at features likely found in unlabelled data
    that we want our model to perform on later. On top, the point cloud has RGB information,
    which is also a sound choice.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this stage, the **seven** selected features are the following: X, Y, and
    Z (spatial), R, G, B (radiometry), and intensity. On top, we keep the AHN4 labels
    per point from the Classification field of the .laz file. Once your 3D aerial
    point cloud is successfully imported into CloudCompare, we are ready for analysis
    and visualization. We can quickly review the two extra fields (intensity and classification)
    from the ‚Äú`Object Properties panel` (3)‚Äù. If we study the intensity, we notice
    some outlier points that shift our feature vector a bit, as shown below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/307997f24d50733e36cbacf3eab5ff5e.png)![](../Images/2f693937a99497d559776950f99ad7e3.png)'
  prefs: []
  type: TYPE_IMG
- en: The Intensity-colored point cloud and the histogram of the repartition. ¬© F.
    Poux
  prefs: []
  type: TYPE_NORMAL
- en: This is the first observation we must address if we want to use this as an input
    feature for PointNet.
  prefs: []
  type: TYPE_NORMAL
- en: Concerning the color values (Red, Green, Blue), they are obtained from another
    sensor, possibly at another time. Therefore, as they are merged from the available
    ortho imagery on the zone, we may have some precision/reprojection issues. But
    as you can imagine, having the ability to separate green elements from red ones
    should give us a clear indication of the probability a point belong to the vegetation
    classüòÅ.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf669bf4b9eaaa56b0230b3b0fae2209.png)'
  prefs: []
  type: TYPE_IMG
- en: The LiDAR dataset is colored with the ortho imagery to get R, G,B features.
    ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: We have a point cloud, with 32 million points expressed in a cartesian system
    (X, Y, Z), each having an intensity feature and colors (Red, Green, and Blue).
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ö **Note**: *You can save this stage for later, as you may have a vast choice
    of features such as the one illustrated below, which is the Near InfraRed (NIR)
    Channel available on the dataset. For example, this is a convenient field that
    can highlight healthy (or not) vegetation.* üòâ'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dfe86ac1fc5d20e6293eb31f7c43f894.png)'
  prefs: []
  type: TYPE_IMG
- en: The Near Infrared feature. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: We have another last scalar field if you scroll the available ones. The Classification
    field, of course! And this is very handy to help us create a labeled dataset to
    avoid going from scratch (thank you OpenData ! üëå)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4c87a229f0f68f78c29dcf7b5429bcfc.png)'
  prefs: []
  type: TYPE_IMG
- en: The provided classification. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ö **Note**: *For the sake of pedagogical training, we will consider the classification
    the ground truth for the rest of the tutorial. However, know that the classification
    was achieved with some uncertainty and that if you want the best-performing model,
    have to be fixed. Indeed, there is a famous saying with 3D Deep Learning: Garbage
    in = Garbage out. Therefore, the quality of your data should be paramount.*'
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs focus on refining the labeling phase.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4\. 3D Data Labelling (Labels Concatenation)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/b068f5f131c82160cb08b76f3f9f291b.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay, so before jumping in on this step, I must say something. 3D point cloud
    labeling to train a supervised 3D semantic segmentation learning model is a (painfully)
    manual process. The goal is to assign labels to individual points in a 3D point
    cloud. The main critical objective of this process includes identifying the target
    objects in the point cloud, selecting the appropriate labeling technique, and
    ensuring the accuracy of the labeling process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e191ebfd0b0200319a3fa40a6bd9096.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An example of a labeling process: labeling clusters VS labeling individual
    points. ¬© F. Poux'
  prefs: []
  type: TYPE_NORMAL
- en: To identify the objects or regions in the point cloud that require labeling,
    we manually inspect the cloud or by using algorithms that automatically detect
    particular objects or regions based on their features, such as size, shape, or
    color.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://learngeodata.eu/?source=post_page-----90398f880c9f--------------------------------)
    [## 3D Academy - Point Cloud Online Course'
  prefs: []
  type: TYPE_NORMAL
- en: The best 3D Online Courses for Teachers, Researchers, Developers, and Engineers.
    Master 3D Point Cloud Processing and‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: learngeodata.eu](https://learngeodata.eu/?source=post_page-----90398f880c9f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we start with an advantage: the point cloud is already classified.
    The first step is thus to extract each class as an independent point cloud, as
    illustrated below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a96b31c90a448c3ff94dff8288789a7c.png)'
  prefs: []
  type: TYPE_IMG
- en: First, we select the point cloud and switch the ‚Äúcolors‚Äù property from RGB to
    Scalar Field. We then ensure we visualize the Classification Scalar Field. From
    there, we go to EDIT > Scalar Field > Split Cloud by Integer Value, resulting
    in one point cloud per class in the point cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the various classes that we get as clouds, we see that :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: From there, we can rework `class 1 = vegetation + clutter`.
  prefs: []
  type: TYPE_NORMAL
- en: The appropriate labeling technique must be selected based on the specific task
    and the available data. For example, we can use an unsupervised technique for
    more exploratory analysis and iteratively take some color thresholding by selecting
    candidate points in the vegetation, as illustrated below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/522101de53d3a12b1147e36185ae82c9.png)'
  prefs: []
  type: TYPE_IMG
- en: Segmenting the point cloud based on color information, in order to create sharper
    labels in a semi-automatic fashion. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: This will give inaccurate results but may speed up manually selecting any point
    that belongs to the vegetation.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, ensuring the accuracy of the labeling process is critical for producing
    reliable results. This can be achieved through manual verification or quality
    control techniques such as cross-validation or inter-annotator agreement.
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ö **Note**: *It is good to grasp the jargon, but do not be scared. These concepts
    can be covered at a later stage. One thing at a time.* üòâ'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ultimately, the labeling process‚Äôs accuracy will directly impact subsequent
    tasks‚Äô performance, incl. 3D Semantic Segmentation. In our case, we organize the
    data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We execute this within CloudCompare.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/13bdd0ee2f63786bb858e28b0590edd6.png)'
  prefs: []
  type: TYPE_IMG
- en: Organization of the various classes within CloudCompare. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: After renaming for clarity our different clouds (initialization), we will (1)
    fuse the clutter in one single cloud, (2) delete the Classification field for
    all the clouds, (3) recreate a classification field with the new numbering, (4)
    clone all the clouds and (5) merge the cloned clouds, as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9246ba132e80e03851d3b68b13f5ba5c.png)'
  prefs: []
  type: TYPE_IMG
- en: Initial preparation of our labels into the new point cloud. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2984926c752b2b5391615bda185f70c3.png)'
  prefs: []
  type: TYPE_IMG
- en: The Data Preparation Phase is Executed within CloudCompare. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have a labeled dataset with a specific point label repartition.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3252ee1a485053c68189023813c73c5.png)![](../Images/d15e1321ad392c59e82714c17eb58ac8.png)'
  prefs: []
  type: TYPE_IMG
- en: We notice that out of our 32,080,350 points, 23,131,067 belong to the ground
    (72%), 7,440,825 to the vegetation (23%), 1,146,575 to buildings (4%), 191,039
    to water (less than 1%), and the remaining 170,844 are not labeled (class 0).
    This will be very interesting because we are in this specific imbalance case with
    predominant classes.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have analyzed what our point cloud contains and refined the labels,
    we can dive into feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5\. 3D Feature Selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/e2b0c2ad5eedfee4eb23e276a88572b8.png)'
  prefs: []
  type: TYPE_IMG
- en: When using the PointNet architecture for 3D point cloud semantic segmentation,
    feature selection is essential in preparing the data for training.
  prefs: []
  type: TYPE_NORMAL
- en: In traditional machine learning methods, feature engineering is often required
    to select and extract relevant features from the data. However, this step can
    be avoided with deep learning methods like PointNet since the model can learn
    to extract features from the data automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, ensuring that the input data contains the necessary information for
    the model to learn relevant and deducted features is still essential. We use seven
    features: `X`, `Y`, `Z` (spatial attributes), `R`, `G`, `B` (radiometric attributes),
    and the intensity `I` (LiDAR-derived).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This is our reference. It means that we will build our model with this input,
    and any other dataset we would like to process with the trained PointNet model
    must contain these same features. Before moving within Python, the last step is
    to structure the data according to the Architecture specifications.
  prefs: []
  type: TYPE_NORMAL
- en: Step 6\. Data Structuration (Tiling)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/42c2dea08aaaf1a4b1414d386c5aa486.png)'
  prefs: []
  type: TYPE_IMG
- en: For several reasons, structuring a 3D point cloud into square tiles is essential
    when processing it with the neural network architecture PointNet.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/201d5711e9e23692b603a0205f583a20.png)'
  prefs: []
  type: TYPE_IMG
- en: The tile definition within this workflow is to train PointNet 3D Deep Learning
    Architecture. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: First, PointNet requires the input data to be of fixed size, meaning that all
    input samples should have the same number of points. By dividing a 3D point cloud
    into square tiles, we can ensure that each tile has a more homogeneous number
    of points, allowing PointNet to process them consistently and effectively without
    the extra overhead or unreversible loss when sampling to the final fixed-point
    number.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/40b8d89038937bdcdf14757276160636.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of the impact of sampling strategies on the 3D Point Cloud dataset.
    ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'üå± **Growing**: *With PointNet, we need to have the input tile to a fixed number
    of points, recommended at 4096 points by the original paper‚Äôs authors. This means
    that a sampling strategy will be needed (****which is not done in CloudCompare****).
    As you can see from the illustration above, sampling the point cloud with different
    strategies will yield different results and object identification capabilities
    (E.,g. the electrical pole on the right). Do you think this impacts the 3D Deep
    Learning architecture performances?*'
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, PointNet‚Äôs architecture involves a shared multi-layer perceptron (MLP)
    applied to each point independently, which means that the Network processes each
    point in isolation from its neighbors. By structuring the point cloud into tiles,
    we can preserve the local context of each point within its tile while still allowing
    the Network to process points independently, enabling it to extract some meaningful
    features from the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ecdd116091d1b33e4c152f15903a4f1.png)'
  prefs: []
  type: TYPE_IMG
- en: The resulting 3D point cloud tile. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: Finally, structuring the 3D point cloud into tiles can also improve the computational
    efficiency of the neural Network, as it allows for parallel processing of the
    tiles, reducing the overall processing time required to analyze the entire point
    cloud (on GPU).
  prefs: []
  type: TYPE_NORMAL
- en: We use the ‚ÄúCross Section‚Äù tool (1) to achieve this feat. We set up the size
    to 100 meters (2), we then shift along X and Y (minus) to get as close as possible
    to the lowest corner of the initial tile (3), we use the multiple slice button
    (4), we repeat along and Y axis (5) and get the resulting square tiles (6), as
    shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa7854693120b84501c12c0d7faca466.png)'
  prefs: []
  type: TYPE_IMG
- en: The process to automate the tile creation within CloudCompare. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3b33c2155b7469d98c831fd5aa8d380c.png)'
  prefs: []
  type: TYPE_IMG
- en: The live process to automate the tile creation within CloudCompare. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: This allows defining tiles of approximately one hundred meters by one hundred
    meters along X and Y axes. We obtain 143 tiles, from which we discard the last
    13 tiles, as they could be more representative of what we want our input to be
    (i.e., they are not square because they are on the edge). With the remaining 130
    tiles, we choose a good (around 20%) of representative tiles (holding Shift +
    Selection), as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61b8548c1516f4385b7d17717237774f.png)'
  prefs: []
  type: TYPE_IMG
- en: Selection and manual split into training and testing set for PointNet. ¬© F.
    Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'üå± **Growing**: *We split our data between training and testing following an
    80/20 percent scheme. At this stage, what do you think about this approach? What
    would be, in your opinion, a good strategy?*'
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this process, we have around 100 tiles in the train set and 30
    tiles in the test set, each holding the original number of points. We then select
    one folder and export each tile as an ASCII file, as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cc137efa069ddc07435b100aa870e872.png)'
  prefs: []
  type: TYPE_IMG
- en: Exporting the point cloud tiles to use with PointNet
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ö **Note**: *CloudCompare allows to export all the point clouds independently
    within a directory when choosing to export as an ASCII file. He will automatically
    indent after the last character, using a ‚Äú*`*_*`*‚Äù character to ensure consistency.
    This is very handy and can be used/abused.*'
  prefs: []
  type: TYPE_NORMAL
- en: Structuring a 3D point cloud into square tiles is an essential preprocessing
    step when using PointNet. It allows for consistent input data size, preserves
    local context, and improves computational efficiency, all of which contribute
    to more accuracy and efficient processing of the data. This is the final step
    before moving on to 3D Python üéâ.
  prefs: []
  type: TYPE_NORMAL
- en: Step 7\. 3D Python Data Loading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/e1aae6522def0e1a19d8596c449407f2.png)'
  prefs: []
  type: TYPE_IMG
- en: It is time to ingest the point cloud tiles in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we import the libraries that we need. If you use the Google Colab
    version accessible here: üíª Google Colab Code, then it is important to run the
    first line as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'For any setup, we have to import the various libraries as illustrated below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Great! From there, we split the datafile names in our respective folders in
    `pointcloud_train_files`, and `pointcloud_test_files`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'ü¶ö **Note**: *we have two folders in our explorer: the train folder, and the
    test folder, both in the* `AHN4_33EZ2_12` *folder. What we do here is first to
    give the path to the root folder, and then, we will collect with glob all the
    files in train and test with the* `***` *that means ‚Äú*`select all.‚Äù` *A convenient
    way to deal with multiple files!*'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this step, two variables hold the paths to all the tiles we prepared. To
    ensure that this is correct, we can print one element taken randomly from a 0
    to 20 random distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Great, so what we can do, is thus to split further our dataset into three variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '**valid_list**: This holds the validation data paths. The validation split
    helps to improve the model performance by fine-tuning the model after each epoch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**train_list**: This holds the training data paths, which is the data set on
    which the training takes place.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**test_list**: This holds the test data paths. The test set informs us about
    the final accuracy of the model after completing the training phase'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is done using the friendly numpy functions that work on the array indexes
    from the list. Indeed, we randomly extract 20% from the `pointcloud_train_files`,
    then split what is retained for validation vs. what is not retained and constitutes
    the `train_list` variable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We then randomly study the properties of one data file by looking at the median,
    the standard deviation, and the min-max values with the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Which gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can notice, there is one central element that we have to address: data
    normalization. Indeed, so to avoid any mismatch, we need to be in this ‚Äúcanonical
    space,‚Äù which means we can replicate the same experimental context in the feature
    space. Using a T-Net would be like killing a fly ü™∞ with a bazooka. This is fine,
    but if we can avoid and use an actual coherent approach, it would be smarter üòÅ.'
  prefs: []
  type: TYPE_NORMAL
- en: Step 8\. 3D Python Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/5ed3e236dc6c9533e86a3b7760f58254.png)'
  prefs: []
  type: TYPE_IMG
- en: Normalizing a 3D point cloud tile before feeding it to the PointNet architecture
    is crucial for three main reasons. First, normalization ensures that the input
    data is centered around the origin, which is essential for PointNet‚Äôs architecture
    which applies an MLP to each point independently. The MLP is more effective when
    the input data is centered around the origin, which allows for more meaningful
    feature extraction and better overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e7e59d8169a06fc63c88463815748d87.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of the normalization impact on the results of training 3D Deep
    Learning Models. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'üå± **Growing**: *Some sort of intuition is also good before normalizing bluntly.
    For example, we predominantly use gravity-based scenes, meaning that the Z-axis
    is almost always colinear to the Z-axis. Therefore, how would you approach this
    normalization?*'
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, normalization scales the point cloud data to a consistent range, which
    helps prevent saturation of the activation functions within the MLP. This allows
    the Network to learn from the entire range of input values, improving its ability
    to accurately classify or segment the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4f256a4ace3ba7acdfac220fe4cdac1.png)'
  prefs: []
  type: TYPE_IMG
- en: The problem with [0,1] scaling illustrated on the intensity field of 3D point
    clouds. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: Finally, normalization can help reduce the impact of different scales in the
    point cloud data, which can be caused by differences in sensor resolutions or
    distances from the scanned objects (which is a bit flattened in the case of Aerial
    LiDAR data). This improves the consistency of the data and the Network‚Äôs ability
    to extract meaningful features from it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, let us get on it. For our experiments, we will first capture the minimum
    value of the features in `min_f`, and the average in `mean_f`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'ü¶ö **Note**: *We transposed our dataset to handle the data and the indexes much
    more efficiently and conveniently. Therefore, to take the* `X-axis` *elements
    of the point cloud, we can just pass* `cloud_data[0]` *instead of* `cloud_data[:,0]`*,
    which involves a bit of overhead.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now normalize the different features to use in our PointNet networks.
    First, the spatial coordinates X, Y, and Z. We will center our data on the planimetric
    axis (X and Y) and ensure that we subtract the minimal value of Z to account for
    discrimination between roofs and ground, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Great, now we can scale our colors by ensuring we are in a [0,1] range. This
    is done by dividing the max value (255) for all our colors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will attack the normalization of the intensity feature. Here, we
    will work with quantiles to obtain a normalization robust to outliers, as we saw
    when exploring our data. This is done in a three-stage process. First, we compute
    the interquartile difference `IQR`, which is the difference between the 75th and
    25th quantile. Then we subtract the median from all the observations and divide
    by the interquartile difference. Finally, we subtract the minimum value of the
    intensity to have a significant normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Wonderful! At this stage, we have a point cloud normalized and ready to be fed
    to a PointNet architecture. But automating this process is the next logical step
    to execute this on all the tiles.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Point Cloud Tile Load and Normalize function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We create a function `cloud_loader` that takes as input the path to a tile
    `tile_path`, and a string of features used, `features_used`, and outputs a `cloud_data`
    variable, which holds the normalized features, along with its ground-truth variable
    `gt` that holds the labels of each point. The function will act as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8098b012294e7a76b4bb61349c8e7bbd.png)'
  prefs: []
  type: TYPE_IMG
- en: The definition of a cloud loading function to process point cloud datasets and
    make them ready for training. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'This translates into a simple `cloud_loader` function, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This function is now used to obtain both point cloud features and labels as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'üå± **Growing**: *As you can see, we pass a string for the features. This is
    very convenient for our different ‚Äò*`*if*`*‚Äô tests indeed. However, note that
    we do not return errors if what is fed to the function is not expected. This is
    not a standard code practice, but this extends the scope of this tutorial*. *I
    recommend checking out PEP-8 guidelines if you want to start with beautiful code
    writing.*'
  prefs: []
  type: TYPE_NORMAL
- en: Step 9\. 3D Python Interactive Visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/38078309e7c97475a7442c021a49a0f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we want to parallel a previous article, accessible here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----90398f880c9f--------------------------------)
    [## 3D Python Workflows for LiDAR City Models: A Step-by-Step Guide'
  prefs: []
  type: TYPE_NORMAL
- en: The Ultimate Guide to unlocking a streamlined workflow for 3D City Modelling
    Applications. The tutorial covers Python‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----90398f880c9f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'We can visualize our dataset with Open3D. First, we need to install a specific
    version (if working on a Jupyter Notebook environment, such as Google Colab or
    the CRIB platform) and load it in our script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'ü¶ö **Note**: T*he ‚Äú*`!`*‚Äù before pip is used when you work on Google Colab to
    say it should use the environment console directly. If you work locally, you should
    delete this character and use* `pip install open3d==0.16` *directly.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we run the following successive steps :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4c17ac7191f63ac5111517f2b34dcc5.png)'
  prefs: []
  type: TYPE_IMG
- en: The drawing function to plot interactive 3D scenes directly within Google Colab.
    ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'Which translates into the following code lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'ü¶ö**Note**: *As our pc variable capturing the* `cloud_data` *output of our*
    `cloud_loader` *function is transposed, we must not forget to transpose it back
    when plotting with* `open3d`*.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous code snippet will output the following visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4bdc90b04fb717cd5e4223afe246cdbd.png)'
  prefs: []
  type: TYPE_IMG
- en: The results of plotting scenes with plotly and R,G,B fields. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ö **Note**: *when using the* `draw_plotly` *function, we do not have a direct
    hand in the scaling of the plot, and we can notice that we have non-equal scales
    for* `*X*`*,* `*Y,*` *and* `*Z*`*, which emphasizes the Z largely in that case.*
    üòÅ'
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the limitations that you can notice, we create a custom visualization
    function to visualize a random tile so that running the function: `visualize_input_tile`
    outputs an interactive `plotly` visualization that lets us switch the rendering
    mode.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To test the provided function, we first need to define the class names in our
    experiments: `class_names = [‚Äòunclassified‚Äô, ‚Äòground‚Äô, ‚Äòvegetation‚Äô, ‚Äòbuildings‚Äô,
    ‚Äòwater‚Äô]`. Then, we provide the cloud features `cloud_features=‚Äôxyzi‚Äô`, randomly
    select a point cloud captured in the variable `selection`, and visualize the tile.
    This translates into the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: which outputs the interactive scene below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/30e3f33bf92ffaab3aa2cf9ac4ee8593.png)'
  prefs: []
  type: TYPE_IMG
- en: interactive 3D scene within Google Colab. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ö **Note**: *You can use the button to switch rendering modes between the feature
    intensity and the labels from the loaded features of interest.*'
  prefs: []
  type: TYPE_NORMAL
- en: We have a working solution for loading, normalizing, and visualizing a single
    tile in Python. The last step is to create what we call a tensor for use with
    the PointNet Architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Step 10\. Tensor Creation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/5a4bea8711a8eb6399d3d4d008598f37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'I want to show you how we can use PyTorch as an initiation. For clarity concerns,
    let me quickly define the primary Python object type we manipulate with this library:
    a tensor.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A PyTorch tensor is a multi-dimensional array used for storing and manipulating
    data in PyTorch. It is similar to a NumPy array but with the added benefit of
    being optimized for use with deep learning models. Tensors can be created using
    the `torch.tensor()` function and initialized with data or created as an empty
    tensor with a specified shape. For example, to create a 3x3 tensor with random
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'which will output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Pretty straightforward, hun? Now, to make things easier for us, there is also
    a tiny Pytorch library that we can use to prepare lists of datasets. This library
    is called `TorchNet`. `TorchNet` is designed to simplify the process of building
    and training complex neural network architectures by providing a set of predefined
    modules and helper functions for everyday tasks such as data loading, validation,
    and testing.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main advantages of `TorchNet` is its modular design, which allows
    users to easily construct complex neural network architectures by combining a
    series of pre-built modules. This can save significant time and effort compared
    to building neural networks from scratch, especially when new to deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ö **Note**: *Besides its modular design, TorchNet provides several helper functions
    for common deep learning tasks, such as data augmentation, early stopping, and
    model checkpointing. This can help users to achieve better results and optimize
    their neural network architectures more efficiently*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install `torchnet` version `0.0.4` and import it into our script, we can
    do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We also import another utility module called `functools`. This module is for
    higher-order functions that act on or return other functions. For this, add to
    the import statements `import functools`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, any callable object can be treated as a function for the purposes
    of this module. From this additional setup, it is straightforward to generate
    the train, validation, and test sets with the following four lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we would like to explore, you can use indexes like a classical numpy
    array to retrieve a tensor on a specific position, such as `train_set[1]`, which
    outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8de3614b4899010c759ea2104f9c0e0a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we have to save our results to a Python object to use straight out
    of the box for the following steps, such as PointNet training. We are using the
    library pickle, which is handy for saving Python objects. To save an object, just
    run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to test your setup, you can also run the following lines of code
    and ensure that you retrieve back what you intend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'üíª Get Access to the Code here: [Google Colab](https://colab.research.google.com/drive/1pqBqGPV36_gxi4yjPiTUR3fb5IldpdaS?usp=sharing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'üçá Get Access to the Data here: [3D Datasets](https://drive.google.com/drive/folders/1RPCX2NCBn24g4lC3qS_xhuS1peR_EnxM?usp=sharing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'üë®‚Äçüè´3D Data Processing and AI Courses: [3D Academy](https://learngeodata.eu/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'üìñ Subscribe to get early access to 3D Tutorials: [3D AI Automation](https://medium.com/@florentpoux/subscribe)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'üíÅ Support my work with Medium ü§ü: [Medium Subscription](https://medium.com/@florentpoux/membership)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/9d299f0b910437eb61e9167de8868a19.png)'
  prefs: []
  type: TYPE_IMG
- en: üîÆ Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! In this hands-on tutorial, we explored the critical steps in
    preparing 3D point cloud data from aerial LiDAR scans for use with the PointNet
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e3562ca370d6d278c11635d8b34757d.png)'
  prefs: []
  type: TYPE_IMG
- en: The 3D Deep Learning Data Preparation Workflow for PointNet. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: By following this step-by-step guide, you have learned how to clean, process
    LiDAR point clouds, extract relevant features, and normalize the data for 3D deep
    learning models. We have also discussed some key considerations in working with
    3D point cloud data, such as tile size, normalization, and data augmentation.
    You can apply these techniques to your 3D point cloud datasets and use them for
    training and testing PointNet models for object classification and segmentation.
    The field of 3D deep learning is rapidly evolving, and this tutorial is a cornerstone
    that provides a solid foundation for you to explore this exciting area further.
  prefs: []
  type: TYPE_NORMAL
- en: ü§ø Going Further
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: But the learning journey does not end here. Our lifelong search begins, and
    future steps will dive into deepening 3D Voxel work, Artificial Intelligence for
    3D data, exploring semantics, and digital twinning. On top, we will analyze point
    clouds with deep learning techniques and unlock advanced 3D LiDAR analytical workflows.
    A lot to be excited about!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Qi, C. R., Su, H., Mo, K., & Guibas, L. J. (2017). Pointnet: Deep learning
    on point sets for 3d classification and segmentation. In *Proceedings of the IEEE
    conference on computer vision and pattern recognition* (pp. 652‚Äì660).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Poux, F., & Billen, R. (2019). Voxel-based 3D point cloud semantic segmentation:
    Unsupervised geometric and relationship featuring vs deep learning methods. *ISPRS
    International Journal of Geo-Information*, *8*(5), 213.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Xu, S., Vosselman, G., & Elberink, S. O. (2014). Multiple-entity based classification
    of airborne laser scanning data in urban areas. *ISPRS Journal of photogrammetry
    and remote sensing*, *88*, 1‚Äì15.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
