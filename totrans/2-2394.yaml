- en: Word Embeddings, Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入的解释
- en: 原文：[https://towardsdatascience.com/word-embeddings-explained-c07c5ea44d64](https://towardsdatascience.com/word-embeddings-explained-c07c5ea44d64)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/word-embeddings-explained-c07c5ea44d64](https://towardsdatascience.com/word-embeddings-explained-c07c5ea44d64)
- en: '[](https://medium.com/@dataemporium?source=post_page-----c07c5ea44d64--------------------------------)[![Ajay
    Halthor](../Images/1be821c8d8ed336b9ecedcf94f960ede.png)](https://medium.com/@dataemporium?source=post_page-----c07c5ea44d64--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c07c5ea44d64--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c07c5ea44d64--------------------------------)
    [Ajay Halthor](https://medium.com/@dataemporium?source=post_page-----c07c5ea44d64--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@dataemporium?source=post_page-----c07c5ea44d64--------------------------------)[![Ajay
    Halthor](../Images/1be821c8d8ed336b9ecedcf94f960ede.png)](https://medium.com/@dataemporium?source=post_page-----c07c5ea44d64--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c07c5ea44d64--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c07c5ea44d64--------------------------------)
    [Ajay Halthor](https://medium.com/@dataemporium?source=post_page-----c07c5ea44d64--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c07c5ea44d64--------------------------------)
    ·7 min read·May 30, 2023
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c07c5ea44d64--------------------------------)
    ·7 min read·2023年5月30日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In natural language processing, we work with words. However, computers cannot
    directly understand words, necessitating their conversion into numerical representations.
    These numeric representations, known as vectors or embeddings, comprise numbers
    that can be either interpretable or non-interpretable by humans. In this blog,
    we will delve into the advancements made in learning these word representations
    over time.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理领域，我们处理的是单词。然而，计算机无法直接理解单词，因此需要将其转换为数值表示。这些数值表示，称为向量或嵌入，由可以被人类解释或无法解释的数字组成。在这篇博客中，我们将深入探讨这些词汇表示的学习进展。
- en: 1 N-grams
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 N-grams
- en: '![](../Images/b72ba194d24fb7590374f82b5317df2e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b72ba194d24fb7590374f82b5317df2e.png)'
- en: 'Figure 1: N-gram vector representation of a sentence (image by author)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 句子的 N-gram 向量表示（作者提供的图片）'
- en: Let’s take the example of n-grams to understand the process better. Imagine
    we have a sentence that we want the computer to comprehend. To achieve this, we
    convert the sentence into a numeric representation. This representation includes
    various combinations of words, such as unigrams (single words), bigrams (pairs
    of words), trigrams (groups of three words), and even higher-order n-grams. The
    result is a vector that could represent any English sentence.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以 n-grams 为例，更好地理解这一过程。假设我们有一句话，希望计算机能够理解。为此，我们将句子转换为数值表示。这种表示包括各种词汇组合，如
    unigrams（单词）、bigrams（词对）、trigrams（三词组）以及更高阶的 n-grams。结果是一个可以表示任何英语句子的向量。
- en: In Figure 1, let’s consider encoding the sentence “This is a good day“. Say
    the first position of the vector represents the number of cases the bigram “good
    day” occurs in the original sentence. Since it occurs once, the numeric representation
    is “1” for this first position. In the same way, we can represent every unigram,
    diagram and trigram with different positions in this vector.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 1 中，考虑对句子“This is a good day”进行编码。假设向量的第一个位置表示在原句中出现的 bigram “good day”的次数。由于它出现了一次，因此这个位置的数值表示为“1”。同样，我们可以用这个向量的不同位置来表示每一个
    unigram、bigram 和 trigram。
- en: 'A major upside for this model is **interpretability**. Each number in this
    vector has some meaning humans can associate with. When making predictions, it’s
    not difficult to see what influenced the outcome. However, this numerical representation
    has one major downside: **Curse of Dimensionality.** This n-gram vector is large.
    If used for statistical modeling, specific parts of this vector need to be cherry
    picked. The reason for this is the curse of dimensionality. As the number of dimensions
    in the vector increases, the distance between representations of sentences increases.
    This is great for representing more information. But if it’s too sparse, it becomes
    difficult for a statistical model to tell which sentences are closer physically
    ( and hence in meaning ) to each other. Furthermore, cherry picking is a manual
    process and the developer might miss some useful n-gram representations in the
    process.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型的一个主要优点是**可解释性**。这个向量中的每个数字都有一定的含义，人类可以关联。当进行预测时，不难看出什么影响了结果。然而，这种数值表示有一个主要的缺点：**维度灾难**。这个n-gram向量很大。如果用于统计建模，需要从中挑选特定的部分。原因在于维度灾难。随着向量维度的增加，句子表示之间的距离也增加。这有利于表示更多信息，但如果过于稀疏，统计模型难以判断哪些句子在物理上（从而在意义上）更接近。此外，挑选是一个手动过程，开发人员可能会遗漏一些有用的n-gram表示。
- en: 2 Neural Networks
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 神经网络
- en: '![](../Images/988c6b4b9d3978c15a57e9df041a1234.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/988c6b4b9d3978c15a57e9df041a1234.png)'
- en: 'Figure 2: Neural Probabilistic Language Model (Bengio et al., 2003)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：神经概率语言模型（Bengio等，2003）
- en: To solve this shortcoming, a [neural probabilistic language model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
    was introduced in 2003\. Language Models predict a word that comes next in a sequence.
    For example, a trained language model will take the sequence of words “I want
    a French” and should generate the next word “Toast”. The neural language model
    illustrated in Figure 2 works in much the same way where use the context of N
    previous words to predict the next word.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这一不足，[神经概率语言模型](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)在2003年被引入。语言模型预测序列中下一个出现的词。例如，一个训练好的语言模型会将“我想要一个法式”这个词序列，生成下一个词“吐司”。如图2所示的神经语言模型在很大程度上以相似的方式工作，即使用前N个词的上下文来预测下一个词。
- en: For each word, we learn a dense representation, which is a vector containing
    a fixed number of numbers to represent each word. Unlike n-grams, these individual
    numbers in the vectors are not directly interpretable by humans. However, they
    capture various nuances and patterns that humans might not be aware of.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个词，我们学习一个密集表示，这是一个包含固定数量数字的向量来表示每个词。与n-gram不同，向量中的这些数字不易被人类直接解释。然而，它们捕捉了人类可能未察觉的各种细微差别和模式。
- en: The exciting part is that since this is a neural network, **we can train it
    end-to-end** to grasp the concept of language modeling and learn all the word
    vector representations simultaneously. However, training such a model can be **computationally
    expensive**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 激动人心的部分是，由于这是一个神经网络，**我们可以端到端地训练它**以掌握语言建模的概念，并同时学习所有词向量表示。然而，训练这样的模型可能**计算开销很大**。
- en: To illustrate, if we represent each word with a 100-number vector, and we need
    to concatenate all these vectors, it would involve thousands of numbers. Considering
    a vocabulary size of tens of thousands of words or more, we could end up with
    millions or even tens of millions of parameters to compute. This becomes a challenge
    when dealing with large vocabularies, a substantial number of examples, or higher
    dimensions for each word representation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们用一个包含100个数字的向量来表示每个词，而我们需要将这些向量连接起来，这将涉及到成千上万的数字。考虑到词汇量可能达到数万个或更多，我们可能会面临数百万甚至数千万个参数需要计算。这在处理大型词汇表、大量示例或每个词表示的高维度时成为一个挑战。
- en: Ideally, larger dimensions would enable us to capture more intricate complexities
    of language, given its inherently complex nature.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，较大的维度将使我们能够捕捉到语言的更复杂特性，考虑到语言本质上的复杂性。
- en: Over the next decade, various architectures had been introduced to enhance the
    quality of word embeddings. One such architecture is described in the paper [Fast
    Semantic Extraction Using a Novel Neural Network Architecture](https://aclanthology.org/P07-1071.pdf).
    It introduces the concept of incorporating positional information for each word
    to improve the embeddings. However, this approach also suffers from the drawback
    of being computationally expensive to train solely for learning word embeddings.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的十年里，推出了各种架构来提升词嵌入的质量。一个这样的架构在论文中描述了，[使用新颖的神经网络架构进行快速语义提取](https://aclanthology.org/P07-1071.pdf)。它引入了为每个词汇加入位置性信息的概念，以改进嵌入。然而，这种方法也存在训练时计算开销大的缺点，仅用于学习词嵌入。
- en: 3 Word2Vec
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 Word2Vec
- en: '![](../Images/588923fac01d6dc482b74cf53209faf5.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/588923fac01d6dc482b74cf53209faf5.png)'
- en: 'Figure 3: Efficient Estimation of Word Representations in Vector Space (Milikov
    et al., 2013)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：向量空间中词表示的高效估计（Milikov等，2013）
- en: In 2013, a significant breakthrough in generating word embeddings came with
    the introduction of Word2Vec. [The paper](https://arxiv.org/pdf/1301.3781.pdf)
    presented two models, namely the Continuous Bag of Words (CBOW) and the Skip-gram
    model, which aimed to **preserve simplicity** while understanding word embeddings.
    In the CBOW model, the current word is predicted based on the two preceding and
    two succeeding words. The projection layer represents the word embedding for that
    specific word. The Skip-gram model, on the other hand, performs a similar task
    but in reverse, predicting the contextual surrounding words given a word. Again,
    the projection layer represents the vector representation of the current word.
    After training either of these networks, a table of words and their corresponding
    embeddings is obtained. This architecture is simpler with fewer parameters, marking
    the era of pre-trained word embeddings and the concept of word2vec.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在2013年，生成词嵌入的一个重大突破是Word2Vec的引入。[这篇论文](https://arxiv.org/pdf/1301.3781.pdf)提出了两种模型，即连续词袋模型（CBOW）和Skip-gram模型，旨在**保持简单性**同时理解词嵌入。在CBOW模型中，当前词是基于前两个和后两个词进行预测的。投影层表示该特定词的词嵌入。而Skip-gram模型则执行类似的任务，但方向相反，给定一个词预测其上下文周围的词。同样，投影层表示当前词的向量表示。在训练这些网络之后，会得到一个词及其对应嵌入的表格。这种架构更简单，参数更少，标志着预训练词嵌入时代和word2vec概念的到来。
- en: However, this approach also has some limitations. Firstly, it generates the
    exact **same vector representation for every occurrence of a word**, regardless
    of its context. For example, the word “queen” in “drag queen” and “queen” in “king
    and queen” would have identical word embeddings, even though they carry different
    meanings. Additionally, the generation of these word embeddings **considers a
    limited context window**, only looking at the previous two words and the next
    two words during the training phase. This limitation affects the model’s contextual
    awareness.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法也有一些局限性。首先，它生成**每次出现的词的相同向量表示**，无论其上下文如何。例如，“drag queen”中的“queen”和“king
    and queen”中的“queen”将具有相同的词嵌入，尽管它们含义不同。此外，这些词嵌入的生成**考虑了有限的上下文窗口**，在训练阶段仅查看前两个词和后两个词。这一局限性影响了模型的上下文意识。
- en: 4 ELMo
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 ELMo
- en: '![](../Images/75f0de9124f038b718357b4c76d5e96e.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75f0de9124f038b718357b4c76d5e96e.png)'
- en: 'Figure 4: Deep contextualized word representations (Peters et al., 2018)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：深度上下文化的词表示（Peters等，2018）
- en: To enhance the quality of generated embeddings, [ELMo](https://arxiv.org/abs/1802.05365)
    (Embeddings from Language Models) was introduced in 2018\. ELMo, a bidirectional
    LSTM (Long Short-Term Memory) model, addresses both language modeling and the
    creation of dense word embeddings within the same training process. This model
    effectively **captures context information** in longer sentences by leveraging
    LSTM cells. However, similar to LSTM models, ELMo shares certain drawbacks. Training
    these models can be **slow**, and they employ a truncated version of backpropagation
    known as BPTT (Backpropagation Through Time). Furthermore, they are **not truly
    bidirectional** since they learn the forward and backward contexts separately
    before concatenating them, which may result in the loss of some contextual information.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高生成嵌入的质量，[ELMo](https://arxiv.org/abs/1802.05365)（来自语言模型的嵌入）在 2018 年被引入。ELMo
    是一种双向 LSTM（长短期记忆）模型，在同一个训练过程中处理语言建模和创建密集的词嵌入。该模型通过利用 LSTM 单元，有效地**捕捉上下文信息**，尤其是在较长的句子中。然而，与
    LSTM 模型类似，ELMo 也存在一些缺陷。训练这些模型可能**较慢**，并且它们采用了一种称为 BPTT（时间反向传播）的截断版本。此外，它们**并非真正双向**，因为它们分别学习前向和后向上下文，然后将其连接，这可能导致一些上下文信息的丢失。
- en: 5 Transformers
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 Transformers
- en: '![](../Images/1d808e6bbe3bacb4794106514f4684e9.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d808e6bbe3bacb4794106514f4684e9.png)'
- en: 'Figure 5: Attention is all you need (Vaswani et al., 2017)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：Attention is all you need（Vaswani 等，2017）
- en: Shortly before the introduction of ELMo, the [Attention Is All You Need paper](https://arxiv.org/pdf/1706.03762.pdf)
    presented the Transformer neural network architecture. Transformers consist of
    an encoder and a decoder that both incorporate positional encodings to generate
    word vectors with contextual awareness. For example, when inputting the sentence
    “I am Ajay,” the encoder generates three dense word embedding representations,
    preserving the word meanings. Transformers also address the downsides of LSTM
    models. They are faster to train since data can be processed in parallel, leveraging
    GPUs. Furthermore, Transformers are deeply bidirectional because they employ an
    attention mechanism that allows words to focus on preceding and succeeding words
    simultaneously, enabling effective contextual understanding.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ELMo 引入之前不久，[Attention Is All You Need 论文](https://arxiv.org/pdf/1706.03762.pdf)
    提出了 Transformer 神经网络架构。Transformers 包括一个编码器和一个解码器，这两个部分都结合了位置编码以生成具有上下文意识的词向量。例如，当输入句子
    “I am Ajay” 时，编码器生成三个密集的词嵌入表示，保留词义。Transformers 还解决了 LSTM 模型的缺点。由于数据可以并行处理，利用
    GPU，训练速度更快。此外，Transformers 是深度双向的，因为它们采用了一个注意力机制，使得词语可以同时关注前后词语，从而实现有效的上下文理解。
- en: The main issue with transformers is for different language tasks, **we need
    a lot of data**. However, if humans have some inherent understanding of language,
    then they don’t need to see a ton of examples to understand how to answer questions
    or translate.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 的主要问题是对于不同的语言任务，**我们需要大量的数据**。然而，如果人类对语言有一定的内在理解，那么他们不需要看到大量的例子就能理解如何回答问题或进行翻译。
- en: 6 BERT and GPT
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 BERT 和 GPT
- en: '![](../Images/aa460336713537c1a7c4f9e1a7d40777.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa460336713537c1a7c4f9e1a7d40777.png)'
- en: 'Figure 6: BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding (Devlin et al., 2019)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：BERT：深度双向 Transformers 的语言理解预训练（Devlin 等，2019）
- en: To overcome the limitations of the Transformer model in language tasks, two
    powerful models called [BERT](https://www.youtube.com/watch?v=xI0HHN5XKDo) (Bidirectional
    Encoder Representations from Transformers) and [GPT](https://www.youtube.com/watch?v=3IweGfgytgY)
    (Generative Pre-trained Transformer) were introduced. These models utilize transfer
    learning, which involves two phases of training.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服 Transformer 模型在语言任务中的局限性，引入了两个强大的模型，[BERT](https://www.youtube.com/watch?v=xI0HHN5XKDo)（双向编码器表示从
    Transformers）和 [GPT](https://www.youtube.com/watch?v=3IweGfgytgY)（生成预训练 Transformer）。这些模型利用了迁移学习，涉及两个训练阶段。
- en: In the first phase, known as pretraining, the models learn about general language
    understanding, context, and grammar from a large amount of data. They acquire
    a strong foundation of knowledge during this phase. In the second phase, known
    as fine-tuning, the models are trained on specific tasks by providing them with
    task-specific data. This fine-tuning process allows the models to specialize in
    performing the desired task **without requiring a massive amount of task-specific
    data**.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一阶段，称为预训练阶段，模型从大量数据中学习关于一般语言理解、上下文和语法的知识。在这个阶段，它们获得了坚实的知识基础。在第二阶段，称为微调阶段，模型通过提供特定任务的数据进行训练。这一微调过程使模型能够专注于执行期望的任务**而无需大量的任务特定数据**。
- en: 'BERT is pretrained on two tasks: Masked Language Modeling and Next Sentence
    Prediction. Through this pretraining, BERT gains a deep understanding of the context
    and meaning of each word, resulting in improved word embeddings. It can then be
    fine-tuned on specific tasks such as question answering or translation, using
    relatively less task-specific data.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 在两个任务上进行了预训练：掩蔽语言建模和下一个句子预测。通过这种预训练，BERT 对每个单词的上下文和含义有了深刻的理解，从而改进了词嵌入。它可以在特定任务上进行微调，如问答或翻译，使用相对较少的任务特定数据。
- en: Similarly, GPT is pretrained on language modeling, which involves predicting
    the next word in a sentence. This pretraining helps GPT develop a comprehensive
    understanding of language. Afterward, it can be fine-tuned on specific tasks to
    leverage its language understanding capabilities in the same way BERT can.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，GPT 在语言建模上进行了预训练，这涉及预测句子中的下一个单词。这种预训练帮助 GPT 发展了对语言的全面理解。之后，它可以在特定任务上进行微调，以便像
    BERT 一样利用其语言理解能力。
- en: Both BERT and GPT, with their Transformer architecture and the ability to learn
    various language tasks, offer superior word embeddings compared to earlier approaches.
    This is why GPT, in particular, serves as the foundation for many modern language
    models like [ChatGPT](https://www.youtube.com/watch?v=NpmnWgQgcsA&list=PLTl9hO2Oobd9coYT6XsTraTBo4pL1j4HJ),
    enabling advanced natural language processing and generation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 和 GPT 都具备 Transformer 架构和学习各种语言任务的能力，相比于早期方法，提供了更优越的词嵌入。这就是为什么 GPT 尤其作为许多现代语言模型的基础，例如
    [ChatGPT](https://www.youtube.com/watch?v=NpmnWgQgcsA&list=PLTl9hO2Oobd9coYT6XsTraTBo4pL1j4HJ)，使得先进的自然语言处理和生成成为可能。
- en: 7 Conclusion
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this blog, we have explored how computers comprehend language through representations
    known as “embeddings”. We have witnessed the advancements made in recent years,
    particularly with the rise of transformers as the foundation of modern language
    models. If you’re interested in building your own transformer model from scratch,
    check out [this playlist of videos](https://www.youtube.com/watch?v=QCJQG4DuHT0&list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4)
    that delve into the code and theory behind it. Happy learning!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客中，我们探讨了计算机如何通过被称为“嵌入”的表示来理解语言。我们见证了近年来的进展，特别是随着变换器的兴起，成为现代语言模型的基础。如果你对从头开始构建自己的变换器模型感兴趣，查看
    [这份视频播放列表](https://www.youtube.com/watch?v=QCJQG4DuHT0&list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4)，其中深入探讨了相关的代码和理论。祝学习愉快！
