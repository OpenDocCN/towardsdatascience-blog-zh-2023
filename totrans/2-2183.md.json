["```py\n# Create example data set\nx = 2 * np.random.rand(100,1)\ny = 5 + 4 * x + np.random.randn(100,1)\n\n# Make dataframe\nregression_df = pd.DataFrame({'x':x.flatten(), 'y':y.flatten()})\n\n# Plot\nsns.lmplot(x='x', y='y', data=regression_df, fit_reg=False)\n```", "```py\n# Function to compute batch gradient descent\ndef batch_gradient_descent(x, y, learning_rate, iterations):\n\n    '''\n    Batch Gradient Descent implication. Inputs data,   \n    learning rate, and number of iterations. Random m and\n    b values are given to start the iteration. Returns optimal \n    model parameters as well as historical loss values.  \n    '''\n\n    m, b = 0.5, 0.5 \n    params, loss = [], [] \n    N = len(x)\n\n    for iteration in range(iterations):\n\n        func = y - (m*x + b)\n\n        # Updating m and b\n        m -= learning_rate * (-2 * x.T.dot(func).sum() / N)\n        b -= learning_rate * (-2 * func.sum() / N)\n\n        params.append((m, b))\n        loss.append(mean_squared_error(y, (m*x + b)))        \n\n    return m, b, params, loss\n```", "```py\n# Find optimal parameters using BGD\nm, b, params, loss = batch_gradient_descent(x, y, learning_rate=0.01, \niterations=1000)\n\n# Predict y values using optimal parameters\ny_predicted = m*x + b\n\n# Print optimal parameters and final loss value\nprint(\"m:\", m, \"b:\", b)\nprint(\"MSE:\", mean_squared_error(y, y_predicted))\n\n# Plot actual vs predicted value plot as well as historical loss values\nplot_regression(x, y, y_predicted, params=params, \ntitle=\"Batch Gradient Descent with Learning Rate=0.01\")\n```", "```py\n# Find optimal parameters using BGD\nm, b, params, loss = gradient_descent(x, y, learning_rate=0.001,\niterations=1000)\n\n# Predict y values using optimal parameters\ny_predicted = m*x + b\n\n# Print optimal parameters and final loss value\nprint(\"m:\", m, \"b:\", b)\nprint(\"MSE:\", mean_squared_error(y, y_predicted))\n\n# Plot actual vs predicted value plot as well as historical loss values\nplot_regression(x, y, y_predicted, params=params, \ntitle=\"Batch Gradient Descent with Learning Rate=0.01\")\n```", "```py\n# Find optimal parameters using BGD\nm, b, params, loss = gradient_descent(x, y, learning_rate=0.1,\niterations=1000)\n\n# Predict y values using optimal parameters\ny_predicted = m*x + b\n\n# Print optimal parameters and final loss value\nprint(\"m:\", m, \"b:\", b)\nprint(\"MSE:\", mean_squared_error(y, y_predicted))\n\n# Plot actual vs predicted value plot as well as historical loss values\nplot_regression(x, y, y_predicted, params=params, \ntitle=\"Batch Gradient Descent with Learning Rate=0.01\")\n```", "```py\n# Function to compute stochastic gradient descent \ndef stochastic_gradient_descent(x, y, learning_rate, iterations):\n\n    '''\n    Stochastic Gradient Descent Implication. Inputs data,   \n    learning rate, and number of iterations. Random m and\n    b values are given to start the iteration. Index of the \n    random data is updated for each iteration. Returns \n    optimal model parameters as well as historical loss values.  \n    '''\n\n    m, b = 0.5, 0.5 # initial parameters\n    params, loss = [], [] # lists to store learning process\n\n    for iteration in range(iterations):\n\n        # Sample a random index for loss calculation\n        indexes = np.random.randint(0, len(x), 1)\n\n        xi = np.take(x, indexes)\n        yi = np.take(y, indexes)\n        N = len(xi)\n\n        func = yi - (m*xi + b)\n\n        # Updating parameters m and b\n        m -= learning_rate * (-2 * xi.dot(func).sum() / N)\n        b -= learning_rate * (-2 * func.sum() / N)\n\n        params.append((m, b))\n        loss.append(mean_squared_error(y, m*x+b))        \n\n    return m, b, params, loss\n```", "```py\n# Find optimal parameters using SGD\nm, b, params, loss = stochastic_gradient_descent(x, y, learning_rate=0.01, \niterations=1000)\n\n# Predict y values using optimal parameters\ny_pred = m*x + b\n\n# Print final loss value\nprint(\"MSE:\", mean_squared_error(y, y_pred))\n\n# Plot actual vs predicted value plot as well as historical loss values\nplot_regression(x, y, y_pred, params=params, title=\"Stochastic Gradient Descent with Learning Rate=0.01\")\n```", "```py\n# Function to compute mini-batch gradient descent\ndef mini_batch_gradient_descent(x, y, learning_rate, iterations, batch_size):\n\n    '''\n    Mini-Batch Gradient Descent implication. Inputs data,   \n    learning rate, number of iterations and batch size.\n    Random m and b values are given to start iteration. \n    Index of the random data is updated for each iteration \n    per batch. Returns optimal model parameters as well \n    as historical loss values.  \n    '''\n\n    m, b = 0.5, 0.5 \n    params, loss = [], [] \n\n    for iteration in range(iterations):\n\n        indexes = np.random.randint(0, len(x), batch_size)\n\n        xi = np.take(x, indexes)\n        yi = np.take(y, indexes)\n        N = len(xi)\n\n        func = yi - (m*xi + b)\n\n        # Updating parameters m and b\n        m -= learning_rate * (-2 * xi.dot(func).sum() / N)\n        b -= learning_rate * (-2 * func.sum() / N)\n\n        params.append((m, b))\n        loss.append(mean_squared_error(y, m*x+b))        \n\n    return m, b, params, loss\n```", "```py\n# Find optimal parameters using MBGD\nm, b, params, loss = mini_batch_gradient_descent(x, y, learning_rate=0.01,\niterations=1000, batch_size=10)\n\n# Predict y values using optimal parameters\ny_pred = m*x + b\n\n# Print final loss value\nprint(\"MSE:\",mean_squared_error(y, y_pred))\n\n# Plot actual vs predicted value plot as well as historical loss values\nplot_regression(x, y, y_pred, params=params,\ntitle=\"Mini-Batch Gradient Descent with Learning Rate=0.01\")\n```"]