- en: 'XGBoost: How Deep Learning Can Replace Gradient Boosting and Decision Trees
    — Part 2: Training'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-part-2-training-b432620750f8](https://towardsdatascience.com/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-part-2-training-b432620750f8)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@guillaume.saupin?source=post_page-----b432620750f8--------------------------------)[![Saupin
    Guillaume](../Images/d9112d3cdfe6f335b6ff2c875fba6bb5.png)](https://medium.com/@guillaume.saupin?source=post_page-----b432620750f8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b432620750f8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b432620750f8--------------------------------)
    [Saupin Guillaume](https://medium.com/@guillaume.saupin?source=post_page-----b432620750f8--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b432620750f8--------------------------------)
    ·6 min read·Sep 21, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/528a94d279856509bbfad323f8112359.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Simon Wilkes](https://unsplash.com/@simonfromengland?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: A world without *if*
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a previous article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-291dc9365656?source=post_page-----b432620750f8--------------------------------)
    [## XGBoost: How Deep Learning Can Replace Gradient Boosting and Decision Trees
    — Part 1'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, you will learn about rewriting decision trees using a Differentiable
    Programming approach, as proposed…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-291dc9365656?source=post_page-----b432620750f8--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: you have learned about rewriting decision trees using a Differentiable Programming
    approach, as suggested by the NODE paper. The idea of this paper is to replace
    XGBoost by a Neural Network.
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, after explaining why the process of building Decision Trees
    is not differentiable, it introduced the necessary mathematical tools to regularize
    the two main elements associated with a decision node:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature Selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Branch detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The NODE paper shows that both can be handled using the entmax function.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we have shown how to create a binary tree **without using comparison
    operators**.
  prefs: []
  type: TYPE_NORMAL
- en: The previous article ended with open questions regarding training a regularized
    decision tree. It’s time to answer these questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re interested in a deep dive in Gradient Boosting Methods, have a look
    at my book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://amzn.to/3EKdKsC?source=post_page-----b432620750f8--------------------------------)
    [## Practical Gradient Boosting: A deep dive into Gradient Boosting in Python'
  prefs: []
  type: TYPE_NORMAL
- en: This book on Gradient Boosting methods is intended for students, academics,
    engineers, and data scientists who wish to…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: amzn.to](https://amzn.to/3EKdKsC?source=post_page-----b432620750f8--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: A smooth decision node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, based on what we presented in the previous article, let’s create a new
    Python class: `SmoothBinaryNode` .'
  prefs: []
  type: TYPE_NORMAL
- en: 'This class encodes the behavior of a smooth binary node. There are two key
    parts in its code :'
  prefs: []
  type: TYPE_NORMAL
- en: The selection of the features, handled by the function `_choices`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The evaluation of these features, with respect to a given threshold, and the
    identification of the path to follow: `left` or `right` . All this is managed
    by the methods `left` and `right` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smooth binary node. Code by the author.
  prefs: []
  type: TYPE_NORMAL
- en: As explained in the previous article, the key to regularize a binary node (and
    hence to allow its training) is to use the `entmax` function.
  prefs: []
  type: TYPE_NORMAL
- en: The almighty dot product
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Please note that both feature selection and left and right branch selection
    is done thanks to a `dot product.` The dot product is a simple operation, as long
    as one only considers its implementation, but in reality, it is very powerful.
  prefs: []
  type: TYPE_NORMAL
- en: It can be used to make projections, compute `cosinus` , find the intersection
    between rays and triangles, … But this is another story.
  prefs: []
  type: TYPE_NORMAL
- en: A simple binary node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s see with a few lines of code how we can use this new class on a very
    basic binary tree with only one node:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the smooth binary node. Code by the author.
  prefs: []
  type: TYPE_NORMAL
- en: In this snippet, we configure our node so that its left leave contains a `1`
    and the right one a `1` . This is defined by the parameter `leaves` . We also
    set the threshold to 50, as you can see in the parameter `biais` . Finally, the
    feature selection is done through the weights defined by `weights`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is another slightly more complex example, with a two-level tree:'
  prefs: []
  type: TYPE_NORMAL
- en: 2 levels binary tree. Code by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The principle is similar to the previous example, except that 2 nodes are added
    to the root.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters to learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous example, we have manually define three parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The **weights** are used to select features. Thanks to the `entmax` function,
    the features with the highest weight with respect to the other will be chosen.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the feature is selected, we need to find the best **threshold** to split
    data into two subsets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we have to define the value attached to the leaves of the tree, here
    the parameter `leaves` .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Those are the parameters that will be learned during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the objective
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As always when doing Machine Learning, the goal is to find the combination of
    parameters that minimize some cost functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most current one is the `Mean Squared Error` , which is quite simple to
    compute and can be written in Python as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Mean Squared Error. Code by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Note that there is a little subtlety in the code above, as the `mse`function
    creates a closure that captures the tree object.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply this function to the simple, one-level tree defined below:'
  prefs: []
  type: TYPE_NORMAL
- en: The error is null, as expected. Code by the author.
  prefs: []
  type: TYPE_NORMAL
- en: As expected, the error is null.
  prefs: []
  type: TYPE_NORMAL
- en: Learning with Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If there is one mathematical tool that is associated with Machine Learning,
    it’s Gradient Descent. Deep Learning, simple Neural Networks, and even Gradient
    Boosting (but in a functional space) use Gradient Descent to minimize some kind
    of cost function to train a model.
  prefs: []
  type: TYPE_NORMAL
- en: The main difficulty in this process is to compute the gradient for a complex
    function, which is the mathematical composition of linear function (layer inputs)
    and non-linear function (activation functions).
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though this difficulty has prevented the success of Neural Network methods
    for a few decades, the underlying mathematical principles have been known for
    many years: differentiation rules.'
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, computing the gradient of a complex function, being the composition
    of many linear and non-linear base functions can be done in a simple **and** efficient
    way with a single line of code, using automatic differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the code for the class `SmoothBinaryNode` , we have isolated
    these 3 parameters, `weights, bias, leaves` in the variable `params` .
  prefs: []
  type: TYPE_NORMAL
- en: Using the Gradient Descent method is the standard way to optimize the parameters
    to minimize the error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both Automatic Differentiation and Gradient Descent are concepts that I explore
    in my book ***70 mathematical concepts***:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://amzn.to/3ZpI8lm?source=post_page-----b432620750f8--------------------------------)
    [## Unveiling 70 Mathematical Concepts with Python: A Practical Guide to Exploring
    Mathematics Through…'
  prefs: []
  type: TYPE_NORMAL
- en: 'Buy Unveiling 70 Mathematical Concepts with Python: A Practical Guide to Exploring
    Mathematics Through Python on…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: amzn.to](https://amzn.to/3ZpI8lm?source=post_page-----b432620750f8--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: As its name implies, Gradient Descent requires computing the Gradient of the
    error function, with respect to the parameters we want to optimize.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to the library `jax` , it’s damned simple to compute the gradient of
    any function, using `grad` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Jax use `automatic differentiation` to automatically and efficiently compute
    derivative. If you’re interested in this compelling method, have a look at my
    introductory article on the subject :'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/differentiable-programming-from-scratch-abba0ebebc1c?source=post_page-----b432620750f8--------------------------------)
    [## Differentiable Programming from Scratch'
  prefs: []
  type: TYPE_NORMAL
- en: Last year I had the great opportunity to attend a talk with Yann Lecun, at the
    Facebook Artificial Intelligence…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/differentiable-programming-from-scratch-abba0ebebc1c?source=post_page-----b432620750f8--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the ways to implement it is shown in this piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: Training a Smooth Binary Node with Gradient Descent. Code by the author.
  prefs: []
  type: TYPE_NORMAL
- en: This code uses the library `jax` first to compute the gradient of the error
    for a set of parameters that is optimal, i.e. a set of parameters for which the
    error is zero. Hence we ensure that as expected the gradient is null in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Then we slightly perturb the `leaves` parameter and ensure that the gradient
    is non-null in the direction of this parameter. This is hopefully the case. Same
    when modifying the `bias` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, starting from a perturbated, non-optimal set of parameters, we use
    the Gradient Descent method to iteratively update them.
  prefs: []
  type: TYPE_NORMAL
- en: After an arbitrary 1000 iterations, the parameters converge to another set that
    minimizes the error.
  prefs: []
  type: TYPE_NORMAL
- en: Vanishing Gradient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I must confess that I cheated a little in the example above. I deliberately
    chose parameters in a non-flat region of the error function.
  prefs: []
  type: TYPE_NORMAL
- en: If you remember the shape of the entmax function, when using a high `alpha`
    the transition from 0 to 1 is very steep. This means that for any values slightly
    distant from 0, the curve of the function is completely flat.
  prefs: []
  type: TYPE_NORMAL
- en: As the gradient is by definition the slope of a curve, the gradient is null
    in this region.
  prefs: []
  type: TYPE_NORMAL
- en: As the Gradient Descent method updates the parameters by adding a small increment
    being the product of the `learning rate` and the gradient, the optimization fails.
  prefs: []
  type: TYPE_NORMAL
- en: One option to avoid this annoying limitation is to perform batch normalization
    to ensure that the feature remains in a region where the `entmax` function is
    not flat
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/646ded91846ff6aa7fa8814985c9837d.png)](https://www.buymeacoffee.com/guillaumes0)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.buymeacoffee.com/guillaumes0](https://www.buymeacoffee.com/guillaumes0)'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen in this series of two articles how to regularize decision trees.
    We have shown how to replace feature selection and branch selection with the `entmax`
    function and a `dot product.`
  prefs: []
  type: TYPE_NORMAL
- en: This regularization allows to use `smooth decision trees` in the mathematical
    framework of `differentiable programming.`
  prefs: []
  type: TYPE_NORMAL
- en: Being able to use this formalism is very powerful, as it allows us to mix any
    kind of Neural Network (a complex differentiable function) with decision trees.
  prefs: []
  type: TYPE_NORMAL
