- en: How to Do Cross-Validation Effectively
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-do-cross-validation-effectively-1bbeb1d69ee8](https://towardsdatascience.com/how-to-do-cross-validation-effectively-1bbeb1d69ee8)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A guide to cross-validation best practices: re-training and nesting'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://vcerq.medium.com/?source=post_page-----1bbeb1d69ee8--------------------------------)[![Vitor
    Cerqueira](../Images/9e52f462c6bc20453d3ea273eb52114b.png)](https://vcerq.medium.com/?source=post_page-----1bbeb1d69ee8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1bbeb1d69ee8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1bbeb1d69ee8--------------------------------)
    [Vitor Cerqueira](https://vcerq.medium.com/?source=post_page-----1bbeb1d69ee8--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1bbeb1d69ee8--------------------------------)
    ·6 min read·Feb 6, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ae7c75ea0734849abe1195cffecca97.png)'
  prefs: []
  type: TYPE_IMG
- en: '[5-fold Monte Carlo cross-validation](https://medium.com/towards-data-science/monte-carlo-cross-validation-for-time-series-ed01c41e2995).
    Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation is a critical factor for building robust machine learning models.
    But, it is often not applied to its full potential.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we’ll explore two important practices to get the most out
    of cross-validation: re-training and nesting.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: What is cross-validation?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cross-validation is a technique for evaluating the performance of a model.
  prefs: []
  type: TYPE_NORMAL
- en: This process usually involves testing several techniques. Or doing hyperparameter
    optimization of a particular method. In such cases, your goal is to check which
    alternative is best for the input data.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to select the approach that maximizes performance. This is the model
    that will be deployed into production. Besides, you also want to get a reliable
    estimate of that model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Re-training After Cross-Validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/0842a1b43d9c8f5bb44fee8cb06c0ff4.png)'
  prefs: []
  type: TYPE_IMG
- en: After cross-validation, you should re-train the best model using all available
    data. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you do cross-validation to select a model. You test many alternatives
    using 5-fold cross-validation. Then, a linear regression comes out on top.
  prefs: []
  type: TYPE_NORMAL
- en: What should you do next?
  prefs: []
  type: TYPE_NORMAL
- en: Should you re-train the linear regression using all available data? or should
    you use the models trained during cross-validation?
  prefs: []
  type: TYPE_NORMAL
- en: This part creates some confusion among data scientists — not only among beginners
    but also among more seasoned professionals.
  prefs: []
  type: TYPE_NORMAL
- en: After cross-validation, you should re-train the best approach using all available
    data. Here’s a quote taken from the legendary book *Elements of Statistical Learning
    [1]*(parenthesis mine)*:*
  prefs: []
  type: TYPE_NORMAL
- en: Our final chosen model [after cross-validation] is f(x), which we then fit to
    all the data.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But, this idea is not consensual.
  prefs: []
  type: TYPE_NORMAL
- en: Some practitioners keep the best models trained during cross-validation. Following
    the example above, you’d keep 5 linear regression models. Then, during the deployment
    stage, you’d average their predictions for each prediction.
  prefs: []
  type: TYPE_NORMAL
- en: That’s not how cross-validation works.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two problems with this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: It uses fewer data for training;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It leads to increased costs due to having to maintain many models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fewer data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By not re-training, you’re not using all available instances for creating a
    model.
  prefs: []
  type: TYPE_NORMAL
- en: This can lead to a sub-optimal model unless you have tons of data. Training
    with all available instances is likely to generalize better.
  prefs: []
  type: TYPE_NORMAL
- en: Re-training is especially important in time series because the most recent observations
    are used for testing. By not re-training in these, the model might miss newly
    emerged patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Increased costs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One can argue that combining the 5 models trained during cross-validation leads
    to better performance.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, it’s important to understand the implications. You’re no longer using a
    simple, interpretable, linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Your model is an ensemble whose individual models are trained by random subsampling.
    Random subsampling is a way of [introducing diversity in ensembles](https://medium.com/towards-data-science/how-to-measure-and-improve-the-diversity-of-forecasting-ensembles-2ec899014d6).
    Ensembles often perform better than single models. But, they also lead to extra
    costs and lower transparency.
  prefs: []
  type: TYPE_NORMAL
- en: What if you just keep one, instead of combining all models?
  prefs: []
  type: TYPE_NORMAL
- en: That would solve the problem of increased costs. Yet, it’s not clear which version
    of the model you should choose.
  prefs: []
  type: TYPE_NORMAL
- en: There are two reasons re-training can be skipped. If the data set is large or
    if re-training is too costly. These two issues are often linked.
  prefs: []
  type: TYPE_NORMAL
- en: Re-training — Practical example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here’s an example of how you can re-train the best model after cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The goal is to optimize the number of trees in a Random Forest. This is done
    with the *GridSearchCV* class from *scikit-learn.* You can set the parameter *refit=True,*
    and the best model is re-trained after cross-validation automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can do this explicitly by getting the best parameters from *GridSearchCV*
    to initialize a new model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Getting Reliable Performance Estimates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When developing a model, you want to achieve three things:'
  prefs: []
  type: TYPE_NORMAL
- en: Select a model among many alternatives;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the selected model and deploy it;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get a reliable estimate of the performance of the selected model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cross-validation and re-training cover the first two points, but not the third.
  prefs: []
  type: TYPE_NORMAL
- en: Why is that?
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation is often repeated several times before selecting a final model.
    You test different transformations and hyperparameters. So, you end up adjusting
    your method until you’re happy with the result.
  prefs: []
  type: TYPE_NORMAL
- en: This can lead to overfitting because the details of the validation sets can
    leak into the model. Thus, the performance estimate you get from cross-validation
    can be too optimistic. You can read more about this in the article in reference
    [2].
  prefs: []
  type: TYPE_NORMAL
- en: This is one of the reasons why Kaggle competitions have two leaderboards, one
    public and another private. This prevents competitors from overfitting the test
    set.
  prefs: []
  type: TYPE_NORMAL
- en: So, how do you solve this problem?
  prefs: []
  type: TYPE_NORMAL
- en: 'You should make an extra evaluation step. After cross-validation, you evaluate
    the selected model in a held-out test set. The full workflow is like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the available data into training and testing sets;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply cross-validation with the training set to select a model;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Re-train the chosen model using the training data and evaluate it on the test
    set. This provides you with an unbiased performance estimate;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Re-train the chosen model using all available data and deploy it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here’s a visual description of this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/19bde3c87a9d8918ee34d28dd41448a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Applying cross-validation with training data. After cross-validation, re-training
    the chosen model and evaluate it on the test set. Finally, re-train the chosen
    model and deploy it. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Practical example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here’s a practical example of the complete process using *scikit-learn.* You
    can check the comments for more context.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Nested cross-validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The above is a simplified version of what’s called nested cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: In nested cross-validation, you carry out a full internal cross-validation process
    in each fold of an external cross-validation process. The goal of the internal
    process is to select the best model. Then, the external process provides unbiased
    performance estimates for this model.
  prefs: []
  type: TYPE_NORMAL
- en: Nested cross-validation becomes inefficient quite quickly. It’s only practical
    on small data sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most practitioners settle for the process exemplified above. If you have a
    large data set, you can also replace the cross-validation procedure with a single
    split. This way, you get three partitions: training, validation, and testing.'
  prefs: []
  type: TYPE_NORMAL
- en: Key Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nesting and re-training are two essential aspects of cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: The performance estimates of the model selected by cross-validation can be too
    optimistic. So, you should make a three-way split to get reliable estimates. A
    three-way split is a form of nested cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: After selecting a model, or estimating its performance, you should re-train
    it with all available data. This way, the model is likely to perform better in
    new observations.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading, and see you in the next story!
  prefs: []
  type: TYPE_NORMAL
- en: Related Articles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[4 Things to Do When Applying Cross-Validation with Time Series](/4-things-to-do-when-applying-cross-validation-with-time-series-c6a5674ebf3a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Monte Carlo Cross-Validation for Time Series](/monte-carlo-cross-validation-for-time-series-ed01c41e2995)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [Hastie, Trevor, et al. *The elements of statistical learning: data mining,
    inference, and prediction*. Vol. 2\. New York: springer, 2009](https://hastie.su.domains/Papers/ESLII.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Cawley, Gavin C., and Nicola LC Talbot. “On over-fitting in model selection
    and subsequent selection bias in performance evaluation.” *The Journal of Machine
    Learning Research* 11 (2010): 2079–2107.'
  prefs: []
  type: TYPE_NORMAL
