- en: A fAIry tale of the Inductive Bias
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-fairy-tale-of-the-inductive-bias-d418fc61726c](https://towardsdatascience.com/a-fairy-tale-of-the-inductive-bias-d418fc61726c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '|INDUCTIVE BIAS| TRANSFORMERS| COMPUTER VISION|'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Do we need inductive bias? How simple models can reach the performance of complex
    models
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----d418fc61726c--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----d418fc61726c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d418fc61726c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d418fc61726c--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----d418fc61726c--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d418fc61726c--------------------------------)
    ·18 min read·Jul 10, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/278a752aca7a7bf5da388e32bb3ac8a8.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: Photo by [Natalia Y.](https://unsplash.com/@foxfox?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen in recent years deep learning has had exponential growth both
    in use and in the number of models. What paved the way for this success is perhaps
    the [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning) itself-the
    idea that a model could be trained with a large amount of data and then used for
    a myriad of specific tasks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'In recent years, a paradigm has emerged: [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    (or otherwise based on this model) is used for NLP applications. While for images,
    [vision transformers](https://en.wikipedia.org/wiki/Vision_transformer) or [convolutional
    networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) are used
    instead.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----d418fc61726c--------------------------------)
    [## The Infinite Babel Library of LLMs'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'Open-source, data, and attention: How the future of LLMs will change'
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----d418fc61726c--------------------------------)
    [](/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b?source=post_page-----d418fc61726c--------------------------------)
    [## META’s Hiera: reduce complexity to increase accuracy'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Simplicity allows AI to reach incredible performance and surprising speed
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b?source=post_page-----d418fc61726c--------------------------------)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, while we have plenty of work showing in practice that these
    models work well, the theoretical understanding of why has lagged behind. This
    is because these models are very broad and it comes difficult to experiment. The
    fact that [Vision Transformers](https://en.wikipedia.org/wiki/Vision_transformer)
    outperform convolutional neural networks [by having a theoretically less inductive
    bias for vision](/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b)
    shows that there is a theoretical gap to be filled.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，虽然我们有大量实践工作证明这些模型效果良好，但理论上的理解却滞后。这是因为这些模型非常广泛，实验起来很困难。[视觉变换器](https://en.wikipedia.org/wiki/Vision_transformer)的表现优于卷积神经网络，[因为它们在视觉上具有理论上更少的归纳偏差](/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b)，这表明存在一个需要填补的理论空白。
- en: 'This article focuses on:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文重点讨论：
- en: What exactly is inductive bias? Why this is important and what inductive bias
    do our favorite models have?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归纳偏差究竟是什么？为什么这很重要，我们最喜欢的模型有什么归纳偏差？
- en: The inductive bias of transformers and CNNs. What are the differences between
    these two models and why these discussions are important?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器和CNN的归纳偏差。这两种模型之间有什么区别，为什么这些讨论很重要？
- en: How can we study inductive bias? How to be able to leverage the similarity between
    different models in order to capture their differences.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何研究归纳偏差？如何利用不同模型之间的相似性来捕捉它们的差异。
- en: Can a model with weak inductive bias succeed in the same in computer vision?
    a field where inductive bias is traditionally believed to be important instead.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有弱归纳偏差的模型能否在计算机视觉领域取得成功？这是一个传统上被认为归纳偏差很重要的领域。
- en: What is inductive bias?
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是归纳偏差？
- en: '![](../Images/7330727da0ff61425df2b2883b29de9e.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7330727da0ff61425df2b2883b29de9e.png)'
- en: Photo by [Raphael Schaller](https://unsplash.com/@raphaelphotoch?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Raphael Schaller](https://unsplash.com/@raphaelphotoch?utm_source=medium&utm_medium=referral)提供，拍摄于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Learning is the process of apprehending useful knowledge by observing and interacting
    with the world. It involves searching a space of solutions for one expected to
    provide a better explanation of the data or to achieve higher rewards. But in
    many cases, there are multiple solutions which are equally good. ([source](https://en.wikipedia.org/wiki/Fact,_Fiction,_and_Forecast))
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 学习是通过观察和与世界互动来获取有用知识的过程。它涉及在解决方案空间中搜索，以找到一个能够更好地解释数据或获得更高奖励的解决方案。但在许多情况下，存在多个同样好的解决方案。
    ([source](https://en.wikipedia.org/wiki/Fact,_Fiction,_and_Forecast))
- en: Imagine encountering a swan in a lake. From this simple swan, we might assume
    that all swans are white (until we see a [black swan](https://en.wikipedia.org/wiki/Black_swan_theory)),
    that they are waterfowl, that they feed on fish, and so on.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下在湖中遇到一只天鹅。从这只简单的天鹅，我们可能会假设所有的天鹅都是白色的（直到我们看到一只[黑天鹅](https://en.wikipedia.org/wiki/Black_swan_theory)），它们是水禽，它们以鱼为食，等等。
- en: This process is called [inductive reasoning](https://en.wikipedia.org/wiki/Inductive_reasoning).
    From a simple observation, we may be able to derive thousands (if not billions)
    of hypotheses, and clearly, not all of them are true. In fact, we might think
    that the swan is incapable of flight since we only observe it swimming at that
    time.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程被称为[归纳推理](https://en.wikipedia.org/wiki/Inductive_reasoning)。从一个简单的观察中，我们可能能够推导出成千上万（甚至数十亿）个假设，显然，所有的假设并不都是真实的。实际上，我们可能会认为天鹅无法飞行，因为我们当时只观察到它在游泳。
- en: Obviously, it is difficult to be able to decide which hypothesis is correct
    without direct observations. So according to [Occam’s principle](https://en.wikipedia.org/wiki/Occam%27s_razor),
    we could state “Swans can swim in lakes.”
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，没有直接观察很难决定哪个假设是正确的。所以根据[奥卡姆剃刀原则](https://en.wikipedia.org/wiki/Occam%27s_razor)，我们可以说“天鹅可以在湖中游泳”。
- en: Why is this important for machine learning?
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么这对机器学习很重要？
- en: A dataset is a collection of observations, and we want to create a model that
    can [generalize](https://developers.google.com/machine-learning/crash-course/generalization/video-lecture)
    from these observations. The idea is that starting from our dataset we can infer
    some rules that are also valid for the general population. In other words, we
    can consider our model as a set of hypotheses.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是观察结果的集合，我们想要创建一个可以从这些观察结果中[泛化](https://developers.google.com/machine-learning/crash-course/generalization/video-lecture)的模型。其理念是，从我们的数据集中，我们可以推断出一些对总体人群也适用的规则。换句话说，我们可以将我们的模型视为一组假设。
- en: In theory, the hypothesis space is infinite. In fact, if we consider two points
    in [Cartesian space](https://en.wikipedia.org/wiki/Cartesian_coordinate_system)
    it can pass a straight line but infinite curves. Without more points, we cannot
    know which hypothesis is the most correct.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，假设空间是无限的。实际上，如果我们考虑[**笛卡尔空间**](https://en.wikipedia.org/wiki/Cartesian_coordinate_system)中的两个点，它可以通过一条直线但无限多的曲线。在没有更多点的情况下，我们无法知道哪种假设是最正确的。
- en: Generally, the simplest hypothesis is the most correct one. A curve that fits
    the points perfectly is generally [overfitting](https://en.wikipedia.org/wiki/Overfitting).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，最简单的假设是最正确的。一个完美拟合点的曲线通常是[**过拟合**](https://en.wikipedia.org/wiki/Overfitting)。
- en: '![](../Images/3988a13e30781fff3edd2b6f4eb01095.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3988a13e30781fff3edd2b6f4eb01095.png)'
- en: image from [here](https://en.wikipedia.org/wiki/Overfitting)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自[这里](https://en.wikipedia.org/wiki/Overfitting)
- en: Inductive bias can be defined as the prioritization of certain hypotheses (thus
    reducing the hypothesis space). For example, when there is a [regression task](https://en.wikipedia.org/wiki/Regression_analysis)
    we decide to consider linear models, at this time we are reducing our hypothesis
    space by allowing our hypotheses to be only linear.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 归纳偏差可以定义为对某些假设的优先考虑（从而减少假设空间）。例如，当我们面对一个[**回归任务**](https://en.wikipedia.org/wiki/Regression_analysis)时，我们决定考虑线性模型，这时我们通过使假设仅限于线性模型来减少我们的假设空间。
- en: '![](../Images/638a306aca2d817b6794915b837180eb.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/638a306aca2d817b6794915b837180eb.png)'
- en: 'linear regression. image source: [here](https://en.wikipedia.org/wiki/Linear_regression)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归。图片来源：[这里](https://en.wikipedia.org/wiki/Linear_regression)
- en: An inductive bias allows a learning algorithm to prioritize one solution (or
    interpretation) over another, independent of the observed data ([source](https://arxiv.org/pdf/1806.01261.pdf))
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 归纳偏差使得学习算法可以在观察到的数据之外优先考虑一种解决方案（或解释）而非另一种，[**来源**](https://arxiv.org/pdf/1806.01261.pdf)
- en: On the one hand, we have different types of data, and we have different types
    of models with different assumptions and different inductive biases (i.e., different
    reductions in the [hypothesis space](https://stats.stackexchange.com/questions/183989/what-exactly-is-a-hypothesis-space-in-machine-learning)).
    One might therefore be tempted to choose one model for all types of data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，我们有不同类型的数据，以及具有不同假设和不同归纳偏差的不同类型模型（即，[**假设空间**](https://stats.stackexchange.com/questions/183989/what-exactly-is-a-hypothesis-space-in-machine-learning)的不同简化）。因此，人们可能会倾向于为所有类型的数据选择一个模型。
- en: In 1997, though, the [no-free lunch theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem)
    ended this temptation. No one model can work for all situations. In fact, there
    is no optimal bias that allows a model to generalize for all tasks. In other words,
    assumptions in a model that may be optimal for one task may not be optimal for
    another.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而在1997年，[**无免费午餐定理**](https://en.wikipedia.org/wiki/No_free_lunch_theorem)结束了这种诱惑。没有一个模型可以适用于所有情况。实际上，没有一种最优的偏差可以使模型对所有任务进行泛化。换句话说，一个任务的最优假设可能对另一个任务并不最优。
- en: This is one reason why we use convolutional neural networks for images, RNNs
    (or LSTMs) for text sequences, and so on.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们为什么对图像使用卷积神经网络，对文本序列使用RNN（或LSTM）等的原因。
- en: '![](../Images/6525c8da72d4cbba03643a63a3298837.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6525c8da72d4cbba03643a63a3298837.png)'
- en: 'To better understand here are some examples of inductive bias:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，以下是一些归纳偏差的例子：
- en: '[**Decision trees**](https://en.wikipedia.org/wiki/Decision_tree) are based
    on the assumption that a task can be solved by a series of binary decisions (binary
    splits).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**决策树**](https://en.wikipedia.org/wiki/Decision_tree)基于这样的假设：一个任务可以通过一系列的二元决策（二元拆分）来解决。'
- en: '[**Regularization**](https://en.wikipedia.org/wiki/Regularization_(mathematics)),
    the assumption is directed to solutions where the parameters have small values.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**正则化**](https://en.wikipedia.org/wiki/Regularization_(mathematics))的假设是指向参数具有小值的解决方案。'
- en: '[**Fully connected layer**](https://www.oreilly.com/library/view/tensorflow-for-deep/9781491980446/ch04.html).
    There is an all-to-all bias where all the units of layer i are connected with
    the following layer j (all the neurons in one layer are connected to the next
    layer). Which it s meaning there is a very weak relational bias since any unit
    can interact with other units.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**全连接层**](https://www.oreilly.com/library/view/tensorflow-for-deep/9781491980446/ch04.html)是一种全对全的偏差，其中层i的所有单元都与下一层j相连（一个层中的所有神经元都与下一层相连）。这意味着存在一种非常弱的关系偏差，因为任何单元都可以与其他单元进行交互。'
- en: '[**Convolutional neural networks**](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    are based on the idea of locality where the features are drawn using local pixels
    and are combined in hierarchical patterns. In another world, we are assuming that
    pixels that are near are actually related and this relationship should be considered
    by the model (during the convolutional step).'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**卷积神经网络**](https://en.wikipedia.org/wiki/Convolutional_neural_network)基于局部性的理念，即特征通过局部像素提取，并以层次化模式组合。在另一种世界观中，我们假设相邻的像素实际上是相关的，这种关系应当被模型考虑（在卷积步骤中）。'
- en: '[**Recurrent neural networks**](https://en.wikipedia.org/wiki/Recurrent_neural_network)
    have a bias related to sequentiality since each word is processed in sequence.
    There is also temporal equivariance (or recursion) because the weights are reused
    for all the elements of the sequence (we update the hidden state).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**递归神经网络**](https://en.wikipedia.org/wiki/Recurrent_neural_network)具有与序列性相关的偏差，因为每个词是按顺序处理的。由于权重在序列的所有元素中被重用（我们更新隐藏状态），因此还存在时间等变性（或递归性）。'
- en: '[**Transformer**](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))**.**
    It has not a strong inductive bias, which should provide more flexibility (at
    the cost of high data for training). In fact, in a low data regimen, the model
    is performing worst than others.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**变换器**](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))**。**
    它没有强的归纳偏差，这应当提供更多的灵活性（但需要大量的训练数据）。实际上，在数据较少的情况下，该模型的表现往往不如其他模型。'
- en: '![](../Images/5c054cd6a3b12bd68912a55acc04d922.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c054cd6a3b12bd68912a55acc04d922.png)'
- en: 'image source: [here](https://arxiv.org/pdf/1806.01261.pdf)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: [这里](https://arxiv.org/pdf/1806.01261.pdf)'
- en: '**The inductive bias of CNN and transformer**'
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**CNN和变换器的归纳偏差**'
- en: '![](../Images/0788572e8b5790c78658ffd20b94acb7.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0788572e8b5790c78658ffd20b94acb7.png)'
- en: Photo by [Tudose Alexandru](https://unsplash.com/de/@pandatudii?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Tudose Alexandru](https://unsplash.com/de/@pandatudii?utm_source=medium&utm_medium=referral)拍摄，发布于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '[Convolutional neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    for a long time dominated computer vision, until [Vision Transformers](https://en.wikipedia.org/wiki/Vision_transformer)
    came along. As we mentioned above, CNNs are based on the principle that neighboring
    pixels have a relationship. Therefore during [convolution](https://en.wikipedia.org/wiki/Convolution),
    several pixels share the same weight.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[卷积神经网络](https://en.wikipedia.org/wiki/Convolutional_neural_network)长期以来主导了计算机视觉领域，直到[视觉变换器](https://en.wikipedia.org/wiki/Vision_transformer)的出现。正如我们前面提到的，CNN基于相邻像素之间存在关系的原理。因此，在[卷积](https://en.wikipedia.org/wiki/Convolution)过程中，几个像素共享相同的权重。'
- en: Moreover, the use of the [pooling layer](https://www.geeksforgeeks.org/cnn-introduction-to-pooling-layer/)
    is used to achieve [translational invariance](https://stats.stackexchange.com/questions/208936/what-is-translation-invariance-in-computer-vision-and-convolutional-neural-netwo).
    Which is meaning that a pattern is recognized wherever it is in the image (for
    example, a face that is at the left or right corner of an image)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，[池化层](https://www.geeksforgeeks.org/cnn-introduction-to-pooling-layer/)的使用旨在实现[平移不变性](https://stats.stackexchange.com/questions/208936/what-is-translation-invariance-in-computer-vision-and-convolutional-neural-netwo)。这意味着，无论模式出现在图像的何处（例如，图像的左角或右角），它都会被识别。
- en: These biases are very effective for processing natural image data because there
    is high covariance within local neighborhoods, which diminishes with distance,
    and because the statistics are mostly stationary across an image. ([source](https://arxiv.org/abs/1806.01261))
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这些偏差对于处理自然图像数据非常有效，因为局部邻域内具有较高的协方差，而随着距离的增加，这种协方差会减小，并且统计特性在整张图像上大致是稳定的。（[来源](https://arxiv.org/abs/1806.01261)）
- en: These biases actually were inspired by the [inferior temporal cortex](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6404234/),
    which seems to provide the corresponding biological for scale, translation, and
    rotation invariance. These biases were considered important for the CNN to be
    resistant to changes in image translation, scaling, or other deformations and
    therefore incorporated through convolution and pooling.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些偏差实际上受到了[下颞皮层](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6404234/)的启发，该区域似乎提供了对应的生物学基础，用于尺度、平移和旋转不变性。这些偏差被认为对CNN在面对图像平移、缩放或其他变形时的鲁棒性很重要，因此通过卷积和池化加以应用。
- en: '![](../Images/416fc7dd7fc8c6971899c6d95bf223a2.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/416fc7dd7fc8c6971899c6d95bf223a2.png)'
- en: 'image source: [here](https://arxiv.org/abs/1801.01450)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/1801.01450)
- en: On the other hand, images are complex and information-rich objects. Given their
    use, an attempt was made to understand in more detail what [CNNs](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    see and what other biases are present.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，图像是复杂且信息丰富的对象。鉴于其用途，尝试更详细地理解[CNN](https://en.wikipedia.org/wiki/Convolutional_neural_network)所看到的内容以及存在的其他偏差。
- en: '[In a 2017 study](https://arxiv.org/pdf/1706.08606.pdf), the authors showed
    that [Inception models](https://www.geeksforgeeks.org/ml-inception-network-v1/)
    (a type of CNNs) have a strong “shape bias.” In other words, CNNs rely more on
    the shape of an object to recognize it than on other types of patterns. The authors
    used an image triplet to classify an object and used an image that had the same
    color but a different shape (color match) or the same shape but different color
    (shape match) to study whether the pattern gave more importance to shape or color.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[在2017年的一项研究中](https://arxiv.org/pdf/1706.08606.pdf)，作者展示了[Inception模型](https://www.geeksforgeeks.org/ml-inception-network-v1/)（一种CNN）的“形状偏差”很强。换句话说，CNN在识别对象时更依赖于对象的形状而非其他类型的模式。作者使用了一个图像三联体来分类一个对象，并使用了颜色相同但形状不同（颜色匹配）或形状相同但颜色不同（形状匹配）的图像来研究模式是否更注重形状或颜色。'
- en: '![](../Images/1b784192e0b0bda23ba25f045096bd3e.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b784192e0b0bda23ba25f045096bd3e.png)'
- en: 'image source: [here](https://arxiv.org/abs/1706.08606)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/1706.08606)
- en: '[In a later study](https://arxiv.org/pdf/1811.12231.pdf), some authors showed
    instead that more than color is a texture that is important for the model. The
    authors used ResNet50 to test this hypothesis.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[在一项后续研究中](https://arxiv.org/pdf/1811.12231.pdf)，一些作者则展示了，模型更关注的是纹理而非颜色。作者使用了ResNet50来测试这一假设。'
- en: '![](../Images/d23cd21bef549162d393942df58037c9.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d23cd21bef549162d393942df58037c9.png)'
- en: 'image source: [here](https://arxiv.org/abs/1811.12231)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/1811.12231)
- en: They showed that in case of texture-shape conflict the model tends to use texture.
    so for the authors, CNNs have a strong “texture bias”
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 他们展示了在纹理与形状冲突的情况下，模型倾向于使用纹理。因此，对作者来说，CNN具有强烈的“纹理偏差”。
- en: '![](../Images/71b2101f93a2d1fa6df09bceaa83d541.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71b2101f93a2d1fa6df09bceaa83d541.png)'
- en: 'image source: [here](https://arxiv.org/abs/1811.12231)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/1811.12231)
- en: 'The authors conclude, however, that models that have a shape bias are more
    robust:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，作者总结道，具有形状偏差的模型更具鲁棒性：
- en: Remarkably, networks with a higher shape bias are inherently more robust to
    many different image distortions (for some even reaching or surpassing human performance,
    despite never being trained on any of them) and reach higher performance on classification
    and object recognition tasks. ([here](https://arxiv.org/pdf/1811.12231.pdf))
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 值得注意的是，具有较高形状偏差的网络在对许多不同图像扭曲的鲁棒性上天生更强（对于一些甚至达到了或超过了人类表现，尽管从未接受过这些扭曲的训练），并在分类和物体识别任务中表现更好。
    ([这里](https://arxiv.org/pdf/1811.12231.pdf))
- en: So actually, for images, it would be desirable to have shape bias. This can
    be achieved by using either the appropriate dataset or [by using data augmentation
    techniques](https://arxiv.org/pdf/1911.09071.pdf) that include color distortion,
    noise, and blur (which precisely decrease texture bias). While conversely, random
    cropping increases texture bias.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，对于图像来说，具有形状偏差是理想的。这可以通过使用适当的数据集或[使用数据增强技术](https://arxiv.org/pdf/1911.09071.pdf)来实现，这些技术包括颜色失真、噪声和模糊（这些恰好减少了纹理偏差）。相反，随机裁剪会增加纹理偏差。
- en: '![](../Images/7340b40268cc08ae0b2f18a2e825374b.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7340b40268cc08ae0b2f18a2e825374b.png)'
- en: 'image source: [here](https://arxiv.org/abs/1911.09071)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/1911.09071)
- en: So concretely we can say that the bias depends not only on the architecture
    for a [CNN](https://en.wikipedia.org/wiki/Convolutional_neural_network) but also
    on the dataset with which it is trained. Depending on the dataset a CNN acquires
    a bias toward either shape or texture.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们可以说偏差不仅依赖于[卷积神经网络（CNN）](https://en.wikipedia.org/wiki/Convolutional_neural_network)的架构，还依赖于训练时使用的数据集。根据数据集的不同，CNN会倾向于形状或纹理。
- en: '[The authors of a study](https://arxiv.org/abs/2010.05981) state that these
    biases are complementary. The model can then focus on either texture or shape
    for prediction. Sometimes, however, only one of these two elements is not enough
    for correct prediction (reduces performance). The authors state that as the model
    can learn either bias it can also “*automatically figure out how to avoid being
    biased toward either shape or texture from their training samples.*” In other
    words, using examples that are conflicting (for texture and shape) can instruct
    the model not to have bias.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[一项研究的作者](https://arxiv.org/abs/2010.05981) 表示，这些偏差是互补的。模型可以专注于纹理或形状进行预测。然而，有时，仅这两种元素中的一种不足以进行正确预测（降低了性能）。作者表示，由于模型可以学习任一偏差，它还可以“*自动找出如何避免对形状或纹理有偏见。*”换句话说，使用具有冲突的（纹理和形状）示例可以指导模型避免偏见。'
- en: '![](../Images/88f7467e4c1ffdd560e9b6fad8b6c873.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88f7467e4c1ffdd560e9b6fad8b6c873.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2010.05981.pdf)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[这里](https://arxiv.org/pdf/2010.05981.pdf)
- en: '[The Vision transformer](https://en.wikipedia.org/wiki/Vision_transformer)
    is derived from the transformer, and as mentioned above it is a model that does
    not have a strong bias.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[视觉变换器](https://en.wikipedia.org/wiki/Vision_transformer) 来源于变换器，正如前面提到的，它是一个没有强偏差的模型。'
- en: Some studies have shown that there are still several similarities between [CNNs](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    and ViTs. In fact, ViTs also learn a layer-by-layer hierarchical view that can
    also be visualized.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究表明，[CNNs](https://en.wikipedia.org/wiki/Convolutional_neural_network) 和
    ViTs 之间仍有几个相似之处。实际上，ViTs 也学习了层级视图，并且可以被可视化。
- en: '![](../Images/44b389f9e66fbd4e4b77177c398793b1.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44b389f9e66fbd4e4b77177c398793b1.png)'
- en: 'image: [source](https://arxiv.org/pdf/2212.06727.pdf)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图像：[来源](https://arxiv.org/pdf/2212.06727.pdf)
- en: '[](https://pub.towardsai.net/a-visual-journey-in-what-vision-transformers-see-9db9c8ba62d4?source=post_page-----d418fc61726c--------------------------------)
    [## A Visual Journey in What Vision-Transformers See'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pub.towardsai.net/a-visual-journey-in-what-vision-transformers-see-9db9c8ba62d4?source=post_page-----d418fc61726c--------------------------------)
    [## 视觉变换器所见的视觉之旅'
- en: How some of the largest models see the world
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一些最大的模型如何看待世界
- en: pub.towardsai.net](https://pub.towardsai.net/a-visual-journey-in-what-vision-transformers-see-9db9c8ba62d4?source=post_page-----d418fc61726c--------------------------------)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[pub.towardsai.net](https://pub.towardsai.net/a-visual-journey-in-what-vision-transformers-see-9db9c8ba62d4?source=post_page-----d418fc61726c--------------------------------)'
- en: 'A later study though suggests that [ViTs](https://en.wikipedia.org/wiki/Vision_transformer)
    actually have a higher shape bias than [CNNs](https://en.wikipedia.org/wiki/Convolutional_neural_network).
    Which is actually surprising. Moreover, the authors point out how this shape bias
    plays a positive role in robustness to image corruption:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，后来的研究表明，[ViTs](https://en.wikipedia.org/wiki/Vision_transformer) 实际上具有比 [CNNs](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    更高的形状偏差。这实际上令人惊讶。此外，作者指出这种形状偏差在图像损坏的鲁棒性方面发挥了积极作用：
- en: '[…] highlight a general inverse relationship between shape bias and mean corruption
    error. As a model is more robust to common corruptions (smaller mCE), its shape
    bias increases. ([source](https://arxiv.org/abs/2106.13122))'
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[…] 强调了形状偏差与均值损坏误差之间的一般性反向关系。模型对常见损坏的鲁棒性越高（即更小的mCE），其形状偏差就越大。([来源](https://arxiv.org/abs/2106.13122))'
- en: '![](../Images/0e04cf6fad9e3b49710f997409f73bc7.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e04cf6fad9e3b49710f997409f73bc7.png)'
- en: 'shape bias of vision transformers. image source: [here](https://arxiv.org/abs/2106.13122)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉变换器的形状偏差。图像来源：[这里](https://arxiv.org/abs/2106.13122)
- en: Several groups hypothesized that adding adequate inductive bias might have allowed
    ViTs to outperform CNNs even without having to train them with millions of images.
    On the one hand, this hypothesis led to the creation of so many models [but made
    training extremely inefficient](/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 几个研究小组假设，通过添加适当的归纳偏差，可能使 ViTs 即使不使用数百万张图像进行训练也能超越 CNNs。另一方面，这一假设导致了大量模型的创建[但使训练极其低效](/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b)。
- en: 'So remains the question:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 所以问题仍然存在：
- en: how much of the lack of inductive bias can be compensated by scaling of parameters
    and the number of training examples?
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参数和训练样本数量的扩展能在多大程度上弥补缺乏归纳偏差的问题？
- en: How to study inductive bias?
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何研究归纳偏差？
- en: '![](../Images/14cdb32ee83108d6bbf0f85a4784ebe2.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14cdb32ee83108d6bbf0f85a4784ebe2.png)'
- en: Photo by [Aaron Burden](https://unsplash.com/@aaronburden?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Aaron Burden](https://unsplash.com/@aaronburden?utm_source=medium&utm_medium=referral)拍摄，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: As we have seen, several open questions remain. Although there are a great many
    studies on [CNNs](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    and [ViTs](https://en.wikipedia.org/wiki/Vision_transformer), many points of theoretical
    background behind these advances in performance remain obscure.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，仍然存在几个未解的问题。尽管有大量关于[CNNs](https://en.wikipedia.org/wiki/Convolutional_neural_network)和[ViTs](https://en.wikipedia.org/wiki/Vision_transformer)的研究，但这些性能提升背后的许多理论背景仍然不清楚。
- en: “MLPs are the simplest of these neural network architectures that hinge on this
    stacking idea, and thus provide a minimal model for an effective theory of deep
    learning.” ([source](https://arxiv.org/abs/2106.10165))
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “MLP是这些神经网络架构中最简单的一种，依赖于这种堆叠思想，因此提供了一个有效深度学习理论的最简模型。” ([source](https://arxiv.org/abs/2106.10165))
- en: In general, many such studies on more theoretical aspects are conducted using
    [multi-layer perceptrons](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    (MLPs). This is because it is a layer composed of simple matrix multiplications
    encapsulated in a nonlinear function. Its simplicity allows many experiments to
    be conducted at a low computational cost. Studies then conducted on simpler models
    are then translated to more complex and sophisticated models. MLP has inferior
    performance in many settings, though, leaving open how much of what is seen can
    then be carried over to models that have far superior performance.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，许多关于更理论方面的研究都是使用[多层感知器](https://en.wikipedia.org/wiki/Multilayer_perceptron)（MLP）进行的。这是因为它是由简单的矩阵乘法组成的层，封装在一个非线性函数中。其简单性允许在较低的计算成本下进行许多实验。然后对更简单模型进行的研究被转化为更复杂和精细的模型。然而，MLP在许多情况下性能较差，这留给我们的是，如何将观察到的内容转移到具有远超性能的模型中。
- en: 'On the other hand, MLP has another advantage, it has a weak inductive bias.
    Which makes it a good candidate for ViT studies. There is also a derivative model
    that has even less inductive bias: [MLP-Mixer](https://arxiv.org/abs/2105.01601)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，MLP还有一个优势，即其具有较弱的归纳偏差。这使得它成为ViT研究的一个良好候选模型。还有一个衍生模型，归纳偏差更小：[MLP-Mixer](https://arxiv.org/abs/2105.01601)
- en: '![](../Images/427eb1e5c288aab1ba585158e40f7613.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/427eb1e5c288aab1ba585158e40f7613.png)'
- en: 'image source: [here](https://arxiv.org/abs/2105.01601)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2105.01601)
- en: Intriguingly, MLP-Mixer uses neither convolution nor self-attention. Instead,
    it relies on multi-layer perception layers that are either applied to spatial
    locations or feature channels. All this is thanks to the clever usage of matrix
    multiplication and non-linearity.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，MLP-Mixer既不使用卷积也不使用自注意力。相反，它依赖于多层感知器层，这些层应用于空间位置或特征通道。这一切都得益于矩阵乘法和非线性的巧妙使用。
- en: Briefly, patches are linearly projected into an embedding space (then transformed
    into tabular data that can be harnessed by MLP). After that, we have a series
    of mixer layers. The input enters and is transposed, after which we have a simple
    fully connected layer. This layer identifies features that are common in the patches
    (aggregating channels). Then the result is transposed and a second fully connected
    layer to identify features in the patches themselves (associating it with the
    channel).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，图像块被线性投影到嵌入空间（然后转换为可以被MLP利用的表格数据）。之后，我们有一系列混合层。输入数据进入并转置，然后我们有一个简单的全连接层。这个层识别在图像块中常见的特征（聚合通道）。然后结果被转置，并通过第二个全连接层来识别图像块本身的特征（与通道关联）。
- en: 'In addition, there are also [skip connections](https://en.wikipedia.org/wiki/Residual_neural_network),
    [GELU](https://paperswithcode.com/paper/gaussian-error-linear-units-gelus) as
    a non-linear function, and [layer normalization](https://arxiv.org/abs/1607.06450).
    In addition, the authors comment:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有[跳跃连接](https://en.wikipedia.org/wiki/Residual_neural_network)、作为非线性函数的[GELU](https://paperswithcode.com/paper/gaussian-error-linear-units-gelus)和[层归一化](https://arxiv.org/abs/1607.06450)。另外，作者评论道：
- en: Our architecture can be seen as a unique CNN, which uses (1×1) convolutions
    for channel mixing, and single-channel depth-wise convolutions for token mixing.
    However, the converse is not true as CNNs are not special cases of Mixer. ([source](https://arxiv.org/abs/2105.01601))
  id: totrans-106
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们的架构可以看作是一个独特的CNN，它使用（1×1）卷积进行通道混合，使用单通道深度卷积进行令牌混合。然而，反之则不然，因为CNN不是Mixer的特例。（[来源](https://arxiv.org/abs/2105.01601)）
- en: '![](../Images/50a7a5be9818b3ee9409ea9002e89cdf.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50a7a5be9818b3ee9409ea9002e89cdf.png)'
- en: 'image source: [here](https://arxiv.org/abs//2306.13575)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs//2306.13575)
- en: Another interesting relationship is that convolution [can be seen as a special
    case of an MLP](https://arxiv.org/abs//2306.13575), where the matrix of W weights
    is sparse and has shared entries. This sharing of the weights does in fact cause
    the learning to be spatially localized (as we mentioned above about the spatial
    bias of convolution).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的关系是，卷积[可以看作是MLP的特例](https://arxiv.org/abs//2306.13575)，其中W权重矩阵是稀疏的并具有共享的条目。权重的这种共享确实导致学习在空间上是局部化的（正如我们上文提到的卷积的空间偏差）。
- en: 'Considering a matrix W, an image 2x3x1 pixels, and a filter f of size 2x2 this
    relationship becomes clear:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个矩阵W、一个2x3x1像素的图像和一个2x2的滤波器f，这种关系变得非常清晰：
- en: '![](../Images/ae0c8735774a415a5384388edd228ad7.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae0c8735774a415a5384388edd228ad7.png)'
- en: 'image source: [here](https://arxiv.org/abs//2306.13575)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs//2306.13575)
- en: This has the advantage of making the model translation invariant, sacrificing
    the robustness of [MLPs](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    if there are permutations in the image.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这具有使模型具有平移不变性的优势，但如果图像中存在排列变换，则牺牲了[MLPs](https://en.wikipedia.org/wiki/Multilayer_perceptron)的鲁棒性。
- en: But what about the Vision Transformers?
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 那么，关于视觉变换器（Vision Transformers）呢？
- en: There is also a close relationship between [ViTs](https://en.wikipedia.org/wiki/Vision_transformer)
    and convolution (despite having the same biases). In fact, as shown above, self-attention
    layers process images in a similar way to convolution layers. In [a 2020 paper](https://arxiv.org/abs/1911.03584),
    the authors show how [self-attention](https://en.wikipedia.org/wiki/Attention_(machine_learning))
    layers can express any convolutional layer.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[ViTs](https://en.wikipedia.org/wiki/Vision_transformer)和卷积之间也有密切的关系（尽管它们具有相同的偏差）。实际上，正如上文所示，自注意力层以类似于卷积层的方式处理图像。在[a
    2020 paper](https://arxiv.org/abs/1911.03584)中，作者展示了[自注意力](https://en.wikipedia.org/wiki/Attention_(machine_learning))层如何表达任何卷积层。'
- en: So as we said there are strong relationships between MLP, MLP-mixer, convolutional
    networks, and Vision Transformer. While these models have strong differences in
    inductive bias and how they process images.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 所以正如我们所说，MLP、MLP-mixer、卷积网络和视觉变换器之间存在强烈的关系。虽然这些模型在归纳偏差和处理图像的方式上有很大的不同。
- en: '![](../Images/6037dcaaf8cb199d9e2c6f45cf5d7ea1.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6037dcaaf8cb199d9e2c6f45cf5d7ea1.png)'
- en: 'image source: [here](https://arxiv.org/abs//2306.13575)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs//2306.13575)
- en: In summary, since there are strong relationships and correspondences between
    the various models, but also differences between induction bias, we can use [MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    as a simple model to understand whether the lack of inductive bias can be compensated
    by scaling and examples in the training set.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，由于各种模型之间存在强烈的关系和对应性，但也有归纳偏差的差异，我们可以使用[MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron)作为一个简单的模型来理解是否通过缩放和训练集中示例的增加可以弥补缺乏归纳偏差的问题。
- en: David against Goliath
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大卫与歌利亚
- en: '![](../Images/6e3a1a8826927456d46dec71a799bb59.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e3a1a8826927456d46dec71a799bb59.png)'
- en: Photo by [Sean Robertson](https://unsplash.com/ko/@knuknuk?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[肖恩·罗伯逊](https://unsplash.com/ko/@knuknuk?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)拍摄
- en: '[In a recent paper, they did exactly that](https://arxiv.org/abs//2306.13575).
    They took [MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron), a model
    that is simple in structure anyway and tried to understand what was happening
    with scaling. **Can scaling improve the performance of the simple fully connected
    layer?**'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[在一篇近期的论文中，他们正是这样做的](https://arxiv.org/abs//2306.13575)。他们采用了[MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron)，一个结构上简单的模型，试图理解缩放时发生了什么。**缩放能改善简单全连接层的性能吗？**'
- en: 'The authors took an MLP and built a model where they stacked equal layers of
    MLPs of the same size. Taking advantage of recent literature, they added [layer
    normalization](https://arxiv.org/abs/1607.06450) and skip connections to see if
    they made the training more stable. They also created a simple architecture called
    inverted bottleneck, where with two weight matrices they expand and collapse in
    the same block the input:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 作者采用了一个 MLP 并构建了一个模型，他们堆叠了相同大小的 MLP 层。利用最近的文献，他们添加了[层归一化](https://arxiv.org/abs/1607.06450)和跳跃连接，以查看这些是否使训练更稳定。他们还创建了一个简单的架构，称为反向瓶颈，在这个架构中，通过两个权重矩阵，他们在同一块中扩展和收缩输入：
- en: '![](../Images/28ec3c531acf1099b68f23e24217c67b.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28ec3c531acf1099b68f23e24217c67b.png)'
- en: 'inverted block. image source: [here](https://arxiv.org/abs//2306.13575)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 反向块。图像来源：[这里](https://arxiv.org/abs//2306.13575)
- en: On the one hand, it is true that these additions increase the [inductive bias](https://en.wikipedia.org/wiki/Inductive_bias)
    but compared to modern complex architectures this is negligible. After that, they
    decided to explore what happens when comparing [MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    with other models in [computer vision](https://en.wikipedia.org/wiki/Computer_vision)
    tasks (where generally [MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    performance is far inferior).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，确实这些添加增加了[归纳偏差](https://en.wikipedia.org/wiki/Inductive_bias)，但与现代复杂架构相比，这几乎可以忽略不计。之后，他们决定探索将[MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron)与其他模型在[计算机视觉](https://en.wikipedia.org/wiki/Computer_vision)任务中比较时的情况（通常[MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron)的表现远远逊色）。
- en: '![](../Images/99a1a8d9486e7b6dab6b49961bc408ab.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99a1a8d9486e7b6dab6b49961bc408ab.png)'
- en: 'image source: [here](https://arxiv.org/abs//2306.13575)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[这里](https://arxiv.org/abs//2306.13575)
- en: 'The authors tested these architectures with interesting results on some popular
    computer vision datasets:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在一些流行的计算机视觉数据集上测试了这些架构，得到了有趣的结果：
- en: the [MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron) standard goes
    directly into overfitting.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron) 标准直接进入过拟合状态。'
- en: Adding [data augmentation](https://en.wikipedia.org/wiki/Data_augmentation)
    marginally improves performance.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加[数据增强](https://en.wikipedia.org/wiki/Data_augmentation)略微提高了性能。
- en: Using bottleneck increases performance. Using data augmentation with an inverted
    bottleneck has a significantly higher impact (about 20 % in performance gain).
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用瓶颈增加了性能。使用反向瓶颈的数据增强对性能有显著更高的影响（约 20% 的性能提升）。
- en: Despite this, [ResNet18](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html)
    has far superior performance.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管如此，[ResNet18](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html)
    的性能远远优于。
- en: These data are in line with the literature, where it is stated that with a small
    sample size (after all, these datasets are small) [inductive bias](https://en.wikipedia.org/wiki/Inductive_bias)
    is important. In fact, the same has been observed with [ViTs](https://en.wikipedia.org/wiki/Vision_transformer)
    and MLP mixers.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据与文献一致，文献指出，在样本量较小的情况下（毕竟这些数据集较小），[归纳偏差](https://en.wikipedia.org/wiki/Inductive_bias)很重要。事实上，[ViTs](https://en.wikipedia.org/wiki/Vision_transformer)和
    MLP mixers 也观察到了同样的现象。
- en: In recent years, the advantage of large models has been that they can be trained
    on large amounts of images and then transfer knowledge to smaller datasets (transfer
    learning). For this, the authors used [ImageNet21k](https://arxiv.org/abs/2104.10972)
    (12 million images and 11k classes). After that, they conducted [fine-tuning](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning))
    on a new task.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大模型的优势在于它们可以在大量图像上进行训练，然后将知识转移到较小的数据集（迁移学习）。为此，作者使用了[ImageNet21k](https://arxiv.org/abs/2104.10972)（1200万张图像和11k类别）。之后，他们在新任务上进行了[微调](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning))。
- en: '![](../Images/aa44819198362be5749ae6928cd72c64.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa44819198362be5749ae6928cd72c64.png)'
- en: 'image source: [here](https://arxiv.org/abs//2306.13575)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[这里](https://arxiv.org/abs//2306.13575)
- en: The results are surprising, the model is able to transfer what it has learned
    about a dataset to another task. Moreover, the results are far superior to what
    has been seen before.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 结果令人惊讶，该模型能够将其对数据集的学习转移到另一个任务上。此外，结果远远优于以往所见。
- en: While of course pre-trained on a large quantity of data, we nevertheless want
    to highlight that such an MLP becomes competitive with a ResNet18 trained from
    scratch for all the datasets, except for ImageNet1k where performance falls surprisingly
    short. ([source](https://arxiv.org/abs//2306.13575))
  id: totrans-140
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This confirms that [MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    is a good proxy for being able to analyze [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning),
    data augmentation, and other theoretical elements. This is surprising because
    it is an elementary model in comparison with modern models.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Another surprising result is that using large [batch sizes](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)
    in training increases performance.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a53df083ca89e74eca826725883e3803.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs//2306.13575)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Generally, the opposite effect is observed. Especially in the case of [CNN](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    where one tries to preserve the performance of small [batch size](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)
    when training with many more examples. After all, using a small batch means doing
    many more gradient updates during an [epoch](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)
    (at the cost of longer training, though). On the other hand, large batch sizes
    are faster and can be split across multiple devices with decisive time gain.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: In addition, [some observations conducted on transformers](https://arxiv.org/abs/2001.08361)
    would seem that even these wide models benefit from a larger [batch size](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56d040102c94739f967c4f9904d7561e.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2001.08361)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, there has been much discussion in recent years about the scaling
    law: according to which as parameters increase there is a highly predictable increase
    in performance (and a quantifiable power law follows). This scaling law has been
    observed for LLMs although in recent times several groups have questioned it.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----d418fc61726c--------------------------------)
    [## Emergent Abilities in AI: Are We Chasing a Myth?'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Changing Perspective on Large Language Models emerging properties
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----d418fc61726c--------------------------------)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Although the discussion on scaling law is still open, it is still interesting
    to analyze whether this is possible with simple models like [MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    (after all, MLP by increasing the number of parameters should tend to overfit).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: In this study, the authors also defined a family of models with an increasing
    number of parameters.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2db27fd4359298759b487bd14420eb5b.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2db27fd4359298759b487bd14420eb5b.png)'
- en: 'image source: [here](https://arxiv.org/abs//2306.13575)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[here](https://arxiv.org/abs//2306.13575)
- en: Indeed, [MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron) also seems
    to exhibit power-law-like behavior.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，[MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron) 似乎也展现了类似幂律的行为。
- en: '![](../Images/2d9cfd645c8563dba1888ded46073eb3.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d9cfd645c8563dba1888ded46073eb3.png)'
- en: 'image source: [here](https://arxiv.org/abs//2306.13575)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[here](https://arxiv.org/abs//2306.13575)
- en: This is definitely an interesting result because it shows even a simple model
    like [MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron) can show a presumed
    power law.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实是一个有趣的结果，因为它表明，即使像 [MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    这样的简单模型也可以展示假定的幂律行为。
- en: '[MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron) is a model that
    was not designed to work with images. In fact, the authors note that [MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    given its bad inductive bias is more dependent on the number of examples. So yes
    one can compensate for a weak [inductive bias](https://en.wikipedia.org/wiki/Inductive_bias),
    but this requires a large number of examples.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron) 是一个并非为处理图像而设计的模型。事实上，作者指出，由于
    [MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron) 的归纳偏差较差，它更依赖于示例的数量。因此，虽然可以通过大量示例来弥补弱的
    [归纳偏差](https://en.wikipedia.org/wiki/Inductive_bias)，但这需要大量的示例。'
- en: A decidedly interesting point is that all these models were run on a single
    GPU. A single epoch for [ImageNet21k](https://arxiv.org/abs/2104.10972) with the
    largest architecture took 450 seconds on a single 24 GB GPU. In other words, these
    experiments can be run quickly on any commercial GPU.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常有趣的点是，这些模型都在单个 GPU 上运行。对于 [ImageNet21k](https://arxiv.org/abs/2104.10972)
    的最大架构，单个周期在单个 24 GB GPU 上花费了 450 秒。换句话说，这些实验可以在任何商业 GPU 上快速运行。
- en: 'The authors point out that [MLPs](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    are clearly more efficient and much larger batches can be used:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 作者指出，[MLPs](https://en.wikipedia.org/wiki/Multilayer_perceptron) 显然更高效，可以使用更大的批量：
- en: As it quickly becomes eminent, MLPs require significantly less FLOPs to make
    predictions on individual images, in essence utilizing their parameters a lot
    more methodically. As a result, latency and throughput are significantly better
    compared to other candidate architectures. (source)
  id: totrans-164
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 正如很快会显而易见的那样，MLP 在对单个图像进行预测时需要显著更少的 FLOPs，本质上更有条理地利用其参数。因此，与其他候选架构相比，延迟和吞吐量显著更好。（来源）
- en: '![](../Images/47edd5072fcaab87c3c8dc9e6be00d39.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47edd5072fcaab87c3c8dc9e6be00d39.png)'
- en: 'image source: [here](https://arxiv.org/abs//2306.13575)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[here](https://arxiv.org/abs//2306.13575)
- en: Conclusions
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: '![](../Images/800abe1819fa05a1d81be24ca9cec38d.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/800abe1819fa05a1d81be24ca9cec38d.png)'
- en: Photo by [Philip Myrtorp](https://unsplash.com/@philipmyr?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Philip Myrtorp](https://unsplash.com/@philipmyr?utm_source=medium&utm_medium=referral)
    拍摄，发布在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
    上。
- en: '[Inductive bias](https://en.wikipedia.org/wiki/Inductive_bias) is one of the
    fundamental concepts of machine learning. In general, it is one of the main reasons
    why depending on the type of data we choose one model and not another. Although
    there have been a lot of studies still there are gaps in theory.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[归纳偏差](https://en.wikipedia.org/wiki/Inductive_bias) 是机器学习的基本概念之一。一般来说，这是我们根据数据类型选择一个模型而非另一个模型的主要原因之一。尽管已经有很多研究，但理论上仍存在空白。'
- en: It is intriguing to think how narrow the field of hypotheses a priori can lead
    to better results. That comes at a cost, though, both level of theory and model
    complexity. As we have seen previously trying to add inductive bias to models
    of ViTs has led to creating increasingly complex and computationally inefficient
    models.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 令人着迷的是，考虑到假设先验领域的狭窄可能导致更好的结果。不过，这也付出了代价，包括理论水平和模型复杂性。如前所述，尝试向 ViTs 模型添加归纳偏差会导致创建越来越复杂且计算上低效的模型。
- en: Although MLP is an extremely simple model, it has the advantage of being computationally
    efficient, which is why it has been used for many studies to try to fill theoretical
    holes. One of the main problems is that the performance of MLP in computer vision
    is far inferior to other models.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 MLP 是一个极其简单的模型，但它在计算上具有高效的优势，这也是它被用于许多研究以填补理论空白的原因之一。主要问题之一是 MLP 在计算机视觉中的表现远远逊色于其他模型。
- en: Recent results show that with the right accommodations, this gap can be overcome.
    Also, the lack of inductive bias can be compensated for with scaling. So MLPs
    can be a good proxy for studying modern architectures and how they behave in different
    situations.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的结果显示，通过适当的调整，可以克服这一差距。此外，缺乏归纳偏差可以通过扩展来弥补。因此，MLP 可以作为研究现代架构及其在不同情况下表现的良好代理。
- en: Why is all this important?
  id: totrans-174
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么这一切如此重要？
- en: 'In general, the last few years of research in AI have focused on a single paradigm:
    more parameters, more data. There has been a new race to the percentage point
    of accuracy. After all, though, the architecture of the Transformer remained the
    same since 2017.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，近年来的 AI 研究集中在一个单一的范式上：更多的参数，更多的数据。为了提高准确率，出现了新的竞争。尽管如此，自 2017 年以来，变换器的架构一直没有改变。
- en: 'These huge models have a not inconsiderable training cost. In recent months
    a research interest in alternatives has begun to grow: both in terms of obtaining
    the same results with fewer parameters and in looking for an alternative to the
    transformer ( and its quadratic computational cost).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这些庞大的模型具有相当高的训练成本。最近几个月，对替代方案的研究兴趣开始增长：既包括用更少的参数获得相同的结果，也包括寻找替代于变换器（及其平方计算成本）的方法。
- en: '[](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----d418fc61726c--------------------------------)
    [## Welcome Back 80s: Transformers Could Be Blown Away by Convolution'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[## Welcome Back 80s: Transformers Could Be Blown Away by Convolution](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----d418fc61726c--------------------------------)'
- en: The Hyena model shows how convolution could be faster than self-attention
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hyena 模型展示了卷积如何比自注意力更快
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----d418fc61726c--------------------------------)
    [](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----d418fc61726c--------------------------------)
    [## META’s LLaMA: A small language model beating giants'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----d418fc61726c--------------------------------)
    [**META’s LLaMA: A small language model beating giants**](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----d418fc61726c--------------------------------)'
- en: META open-source model will help us to understand how LMs biases arise
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: META 开源模型将帮助我们理解语言模型的偏见是如何产生的
- en: medium.com](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----d418fc61726c--------------------------------)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----d418fc61726c--------------------------------)'
- en: In each case, academic research is being forced to chase industry-led research.
    Very few institutions can afford to train an LLM from scratch. [Yet studies like
    this](https://arxiv.org/abs//2306.13575) show that results can be obtained at
    scale even with simple models like MLP. This opens up very interesting perspectives
    to better understand model behavior and start thinking about an alternative to
    the transformer.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在每种情况下，学术研究都被迫追赶以行业为主导的研究。很少有机构能够从头训练一个大型语言模型。[然而，像这样的研究](https://arxiv.org/abs//2306.13575)
    表明，即使是简单的模型如 MLP 也可以大规模获得结果。这为更好地理解模型行为并开始思考变换器的替代方案提供了非常有趣的视角。
- en: '**What do you think? Let me know in the comments.**'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**你怎么看？请在评论中告诉我。**'
- en: 'If you have found this interesting:'
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果你觉得这有趣：
- en: '*You can look for my other articles, you can also* [***subscribe***](https://salvatore-raieli.medium.com/subscribe)
    *to get notified when I publish articles, you can* [***become a Medium member***](https://medium.com/@salvatore-raieli/membership)
    *to access all its stories (affiliate links of the platform for which I get small
    revenues without cost to you) and you can also connect or reach me on*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***.***'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以查看我的其他文章，也可以* [***订阅***](https://salvatore-raieli.medium.com/subscribe)
    *以在我发布文章时获得通知，你还可以* [***成为 Medium 会员***](https://medium.com/@salvatore-raieli/membership)
    *来访问所有故事（平台的附属链接，我从中获得少量收入，您无需支付额外费用），也可以在* [***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)
    ***上与我联系或找到我。***'
- en: '*Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.*'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是我 GitHub 仓库的链接，我计划在这里收集与机器学习、人工智能等相关的代码和资源。*'
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----d418fc61726c--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----d418fc61726c--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: 机器学习、人工智能、数据科学的教程…'
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习、人工智能和数据科学的教程，包含数学解释和可重复使用的代码（用 Python 编写）
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----d418fc61726c--------------------------------)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----d418fc61726c--------------------------------)'
- en: '*or you may be interested in one of my recent articles:*'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '*或者你可能对我最近的一篇文章感兴趣：*'
- en: '[](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----d418fc61726c--------------------------------)
    [## The AI college student goes back to the bench'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----d418fc61726c--------------------------------)
    [## AI 大学生重返实验室'
- en: How LLM can solve college exams and why this is important
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型语言模型如何解决大学考试以及这为何重要
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----d418fc61726c--------------------------------)
    [](https://levelup.gitconnected.com/can-we-detect-ai-generated-text-91293463dc52?source=post_page-----d418fc61726c--------------------------------)
    [## Can we detect AI-generated text?
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----d418fc61726c--------------------------------)
    [](https://levelup.gitconnected.com/can-we-detect-ai-generated-text-91293463dc52?source=post_page-----d418fc61726c--------------------------------)
    [## 我们能检测 AI 生成的文本吗？'
- en: Watermarking could be the solution for detecting it
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 水印可能是检测的解决方案
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/can-we-detect-ai-generated-text-91293463dc52?source=post_page-----d418fc61726c--------------------------------)
    [](/say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=post_page-----d418fc61726c--------------------------------)
    [## Say Once! Repeating Words Is Not Helping AI
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/can-we-detect-ai-generated-text-91293463dc52?source=post_page-----d418fc61726c--------------------------------)
    [](/say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=post_page-----d418fc61726c--------------------------------)
    [## 说一次！重复单词对 AI 无帮助'
- en: How and why is repeating tokens harming LLMs? Why is this a problem?
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重复标记如何以及为何会伤害大型语言模型？这是一个问题吗？
- en: towardsdatascience.com](/say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=post_page-----d418fc61726c--------------------------------)
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=post_page-----d418fc61726c--------------------------------)'
- en: Reference
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Here is the list of the principal references I consulted to write this article,
    only the first name for an article is cited.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我撰写本文时参考的主要文献列表，仅列出了每篇文章的第一个名字。
- en: Goodman, Nelson. *Fact, Fiction, and Forecast* (Fourth Edition). Harvard University
    Press, 1983
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Goodman, Nelson. *《事实、虚构与预测》*（第四版）。哈佛大学出版社，1983年
- en: Battaglia et al, 2018, Relational inductive biases, deep learning, and graph
    networks, [link](https://arxiv.org/abs/1806.01261)
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Battaglia 等人, 2018, 《关系归纳偏差、深度学习与图网络》，[链接](https://arxiv.org/abs/1806.01261)
- en: Kauderer-Abrams, 2017, Quantifying Translation-Invariance in Convolutional Neural
    Networks, [link](https://arxiv.org/abs/1801.01450)
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kauderer-Abrams, 2017, 《卷积神经网络中的平移不变性定量化》，[链接](https://arxiv.org/abs/1801.01450)
- en: 'Ritter et al, 2017, Cognitive Psychology for Deep Neural Networks: A Shape
    Bias Case Study, [link](https://arxiv.org/abs/1706.08606)'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ritter 等人, 2017, 《深度神经网络的认知心理学：形状偏置案例研究》，[链接](https://arxiv.org/abs/1706.08606)
- en: Conway et al, 2018, The Organization and Operation of Inferior Temporal Cortex,
    [link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6404234/)
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Conway 等人, 2018, 《下颞皮层的组织与功能》，[链接](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6404234/)
- en: Geirhos et al, 2022, ImageNet-trained CNNs are biased towards texture; increasing
    shape bias improves accuracy and robustness, [link](https://arxiv.org/abs/1811.12231)
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Geirhos 等人, 2022, 《ImageNet 训练的 CNN 对纹理存在偏见；增加形状偏置可以提高准确性和鲁棒性》，[链接](https://arxiv.org/abs/1811.12231)
- en: Hermann et al, 2020, The Origins and Prevalence of Texture Bias in Convolutional
    Neural Networks, [link](https://arxiv.org/abs/1911.09071)
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hermann 等人, 2020, 《卷积神经网络中的纹理偏置的起源与流行》，[链接](https://arxiv.org/abs/1911.09071)
- en: Li et al, 2021, Shape-Texture Debiased Neural Network Training, [link](https://arxiv.org/abs/2010.05981)
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Li 等人, 2021, 《形状-纹理去偏神经网络训练》，[链接](https://arxiv.org/abs/2010.05981)
- en: Ghiasi et al, 2022, What do Vision Transformers Learn? A Visual Exploration,
    [link](https://arxiv.org/abs/2212.06727)
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ghiasi 等人, 2022, 视觉变换器学到了什么？视觉探索，[link](https://arxiv.org/abs/2212.06727)
- en: 'Morrison et al, 2021, Exploring Corruption Robustness: Inductive Biases in
    Vision Transformers and MLP-Mixers, [link](https://arxiv.org/abs/2106.13122)'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Morrison 等人, 2021, 探索腐败鲁棒性：视觉变换器和 MLP-Mixer 的归纳偏差，[link](https://arxiv.org/abs/2106.13122)
- en: Mormille et al, 2023, Introducing inductive bias on vision transformers through
    Gram matrix similarity based regularization, [link](https://link.springer.com/article/10.1007/s10015-022-00845-9)
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mormille 等人, 2023, 通过基于 Gram 矩阵相似度的正则化在视觉变换器上引入归纳偏差，[link](https://link.springer.com/article/10.1007/s10015-022-00845-9)
- en: 'Tolstikhin et al, 2021, MLP-Mixer: An all-MLP Architecture for Vision, [link](https://arxiv.org/abs/2105.01601)'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Tolstikhin 等人, 2021, MLP-Mixer：一种全 MLP 架构用于视觉，[link](https://arxiv.org/abs/2105.01601)
- en: Cordonnier et al, 2020, On the Relationship between Self-Attention and Convolutional
    Layers, [link](https://arxiv.org/abs/1911.03584)
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cordonnier 等人, 2020, 自注意力与卷积层之间的关系，[link](https://arxiv.org/abs/1911.03584)
- en: 'Bachmann et al 2023, Scaling MLPs: A Tale of Inductive Bias, [link](https://arxiv.org/abs//2306.13575)'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bachmann 等人, 2023, 扩展 MLP：归纳偏差的故事，[link](https://arxiv.org/abs//2306.13575)
- en: Kaplan et al, 2020, Scaling Laws for Neural Language Models, [link](https://arxiv.org/abs/2001.08361)
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kaplan 等人, 2020, 神经语言模型的扩展规律，[link](https://arxiv.org/abs/2001.08361)
- en: Lei Ba et al, 2016, Layer Normalization, [link](https://arxiv.org/abs/1607.06450)
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lei Ba 等人, 2016, 层归一化，[link](https://arxiv.org/abs/1607.06450)
- en: He et al, 2015, Deep Residual Learning for Image Recognition, [link](https://arxiv.org/abs/1512.03385)
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: He 等人, 2015, 深度残差学习用于图像识别，[link](https://arxiv.org/abs/1512.03385)
- en: Ridnik et al, 2021, ImageNet-21K Pretraining for the Masses, [link](https://arxiv.org/abs/2104.10972)
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ridnik 等人, 2021, 面向大众的 ImageNet-21K 预训练，[link](https://arxiv.org/abs/2104.10972)
- en: '[Sharad Joshi](https://medium.com/u/b88796fee2b6?source=post_page-----d418fc61726c--------------------------------),
    2022, Everything you need to know about : Inductive bias, MLearning.ai'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Sharad Joshi](https://medium.com/u/b88796fee2b6?source=post_page-----d418fc61726c--------------------------------),
    2022, 你需要了解的一切：归纳偏差，MLearning.ai'
