- en: How to Evaluate the Performance of Your ML/ AI Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-evaluate-the-performance-of-your-ml-ai-models-ba1debc6f2fa](https://towardsdatascience.com/how-to-evaluate-the-performance-of-your-ml-ai-models-ba1debc6f2fa)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An accurate evaluation is the only way to performance improvement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://saraametwalli.medium.com/?source=post_page-----ba1debc6f2fa--------------------------------)[![Sara
    A. Metwalli](../Images/d6861f7bc1879bf68d4b7116c335c7e5.png)](https://saraametwalli.medium.com/?source=post_page-----ba1debc6f2fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ba1debc6f2fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ba1debc6f2fa--------------------------------)
    [Sara A. Metwalli](https://saraametwalli.medium.com/?source=post_page-----ba1debc6f2fa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ba1debc6f2fa--------------------------------)
    ·8 min read·May 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/179ab3fd989ac42746ac78c4f5aab453.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Scott Graham](https://unsplash.com/@homajob?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Learning by doing is one of the best approaches to learning anything, from tech
    to a new language or cooking a new dish. Once you have learned the basics of a
    field or an application, you can build on that knowledge by acting. Building models
    for various applications is the best way to make your knowledge concrete regarding
    machine learning and artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Though both fields (or really sub-fields, since they do overlap) have applications
    in a wide variety of contexts, the steps to learning how to build a model are
    more or less the same regardless of the target application field.
  prefs: []
  type: TYPE_NORMAL
- en: AI language models such as [ChatGPT](https://openai.com/blog/chatgpt) and [Bard](https://bard.google.com/)
    are gaining popularity and interest from both tech novices and general audiences
    because they can be very useful in our daily lives.
  prefs: []
  type: TYPE_NORMAL
- en: Now that more models are being released and presented, one may ask, what makes
    a “*good*” AI/ ML model, and how can we evaluate the performance of one?
  prefs: []
  type: TYPE_NORMAL
- en: This is what we are going to cover in this article. But again, we assume you
    already have an AI or ML model built. Now, you want to evaluate and improve its
    performance (if necessary). But, again, regardless of the type of model you have
    and your end application, you can take steps to evaluate your model and improve
    its performance.
  prefs: []
  type: TYPE_NORMAL
- en: To help us follow through with the concepts, let’s use the [Wine](https://archive.ics.uci.edu/ml/datasets/Wine)
    dataset from sklearn [1], apply the support vector classifier (SVC), and then
    test its metrics.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s jump right in…
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s import the libraries we will use (don’t worry about what each of
    those do now, we’ll get to that!).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now, we read our dataset, apply the classifier, and evaluate it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 1\. Split the dataset for better analysis.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on your stage in the learning process, you may need access to a large
    amount of data that you can use for training and testing, and evaluating. Also,
    you can use different data to train and test your model because that will prevent
    you from genuinely assessing your model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome that challenge, split your data into three smaller random sets and
    use them for training, testing, and validating.
  prefs: []
  type: TYPE_NORMAL
- en: A good rule of thumb to do that split is a 60,20,20 approach. You would use
    60% of the data for training, 20% for validation, and 20% for testing. You need
    to shuffle your data before you do the split to ensure a better representation
    of that data.
  prefs: []
  type: TYPE_NORMAL
- en: I know that may sound complicated, but luckily, ticket-learn came to the rescue
    by offering a function to perform that split for you, train_test_split().
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we can take our dataset and split it like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Then use the training portion of it as input to the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we have some results to “evaluate.”
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Define your evaluation metrics.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before starting the evaluation process, we must ask ourselves an essential
    question about the model we use: ***what would make this model good?***'
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer to this question depends on the model and how you plan to use it.
    That being said, there are standard evaluation metrics that data scientists use
    when they want to test the performance of an AI/ ML model, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy** is the percentage of correct predictions by the model out of the
    total prediction. That means, when I run the model, how many predictions are true
    among all predictions? This article goes into depth about testing the accuracy
    of a model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Precision** is the percentage of true positive predictions by the model out
    of all positive predictions. Unfortunately, precision and accuracy are often confused;
    one way to make the difference between them clear is to think of accuracy as the
    closeness of the predictions to the actual values, while precision is how close
    the correct predictions are to each other. So, accuracy is an absolute measure,
    yet both are important to evaluate the model’s performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Recall** is the proportion of true positive predictions from all actual positive
    instances in the dataset. Recall aims to find related predictions within a dataset.
    Mathematically, if we increase the recall, we decrease the precision of the model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**F1 score is t**he combination mean of precision and recall, providing a balanced
    measure of a model’s performance using both precision and recall. This video by
    [CodeBasics](https://codebasics.io/) discusses the relation between precision,
    recall, and F1 score and how to find the optimal balance of those evaluation metrics.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Video By [CodeBasics](https://codebasics.io/)
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s calculate the different metrics for the predicted data. The way we
    will do that is by first displaying the confusion matrix. The confusion matrix
    is simply the actual results of data vs. the predicted results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The confusion matrix to our dataset will look something like,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e3673b3531192ce6e6d186a4cdb84a3.png)'
  prefs: []
  type: TYPE_IMG
- en: If we look at this confusion matrix, we can see that the actual value was “1”
    in some cases while the predicted value was “0”. Which means the classifier is
    not a %100 accurate.
  prefs: []
  type: TYPE_NORMAL
- en: We can calculate this classifier's accuracy, precision, recall, and f1 score
    using this code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'For this particular example, the results for those are:'
  prefs: []
  type: TYPE_NORMAL
- en: Precision = 0.889
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recall = 0.889
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accuracy = 0.889
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: F1 score = 0.889
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Though you can really use different approaches to evaluate your models, some
    evaluation methods will better estimate the model’s performance based on the model
    type. For example, in addition to the above methods, if the model you’re evaluating
    is a regression (or it includes regression) model, you can also use:'
  prefs: []
  type: TYPE_NORMAL
- en: '**- Mean Squared Error (MSE)** mathematically is the average of the squared
    differences between predicted and actual values.'
  prefs: []
  type: TYPE_NORMAL
- en: '**- Mean Absolute Error (MAE)** is the average of the absolute differences
    between predicted and actual values.'
  prefs: []
  type: TYPE_NORMAL
- en: Those two metrics are closely related, but implementation-wise, MAE is simpler
    (at least mathematically) than MSE. However, MAE doesn’t do well with significant
    errors, unlike MSE, which emphasizes the errors (because it squares them).
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Validate and tune the model’s hyperparameters.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before discussing hyperparameters, let’s first differentiate between a hyperparameter
    and a parameter. A parameter is a way a model is defined to solve a problem. In
    contrast, hyperparameters are used to test, validate, and optimize the model’s
    performance. Hyperparameters are often chosen by the data scientists (or the client,
    in some cases) to control and validate the learning process of the model and hence,
    its performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different types of hyperparameters that you can use to validate your
    model; some are general and can be used on any model, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning Rate:** this hyperparameter controls how much the model needs to
    be changed in response to some error when the model’s parameters are updated or
    altered. Choosing the optimal learning rate is a trade-off with the time needed
    for the training process. If the learning rate is low, then it may slow down the
    training process. In contrast, if the learning rate is too high, the training
    process will be faster, but the model performance may suffer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch Size:** The size of your training dataset will significantly affect
    the model’s training time and learning rate. So, finding the optimal batch size
    is a skill that is often developed as you build more models and grow your experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of Epochs:** An epoch is a complete cycle for training the machine
    learning model. The number of epochs to use varies from one model to another.
    Theoretically, more epochs lead to fewer errors in the validation process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to the above hyperparameters, there are model-specific hyperparameters
    such as regularization strength or the number of hidden layers in implementing
    a neural network. This 15 mins Video by [APMonitor](https://apmonitor.com/pds/index.php/Main/HyperparameterOptimization)
    explores various hyperparameters and their differences.
  prefs: []
  type: TYPE_NORMAL
- en: Video by [APMonitor](https://apmonitor.com/pds/index.php/Main/HyperparameterOptimization)
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Iterate and refine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Validating an AI/ ML model is not a linear process but more of an iterative
    one. You go through the data split, the hyperparameters tuning, analyzing, and
    validating the results often more than once. The number of times you repeat that
    process depends on the analysis of the results. For some models, you may only
    need to do this once; for others, you may need to do it a couple of times.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to repeat the process, you will use the insights from the previous
    evaluation to improve the model’s architecture, training process, or hyperparameter
    settings until you are satisfied with the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you start building your own ML and AI models, you will quickly realize
    that choosing and implementing the model is the easy part of the workflow. However,
    testing and evaluation is the part that will take most of the development process.
    Evaluating an AI/ ML model is an iterative and often time-consuming process, and
    it requires careful analysis, experimentation, and fine-tuning to achieve the
    desired performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, the more experience you have building more models, the more systematic
    the process of evaluating your model’s performance will get. And it’s a worthwhile
    skill considering the importance of evaluating your model, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating our models allows us to objectively measures the model’s metrics
    which helps in understanding its strengths and weaknesses and provides insights
    into its predictive or decision-making capabilities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If different models that can solve the same problems exist, then evaluating
    them enables us to compare their performance and choose the one that suits our
    application best.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluation provides insights into the model’s weaknesses, allowing for improvements
    through analyzing the errors and areas where the model underperforms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, have patience and keep building models; it gets better and more efficient
    with the more models you build. Don’t let the process details discourage you.
    It may look like a complex process, but once you understand the steps, it will
    become second nature to you.
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Lichman, M. (2013). [UCI Machine Learning Repository.](https://archive.ics.uci.edu/ml)
    Irvine, CA: University of California,'
  prefs: []
  type: TYPE_NORMAL
- en: School of Information and Computer Science. (CC BY 4.0)
  prefs: []
  type: TYPE_NORMAL
