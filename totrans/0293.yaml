- en: An Implementation of VGG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/an-implementation-of-vgg-dea082804e14](https://towardsdatascience.com/an-implementation-of-vgg-dea082804e14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A beginner-friendly tutorial
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mina.ghashami?source=post_page-----dea082804e14--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----dea082804e14--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dea082804e14--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dea082804e14--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----dea082804e14--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dea082804e14--------------------------------)
    ·9 min read·Oct 31, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we look at a VGG implementation and its training on STL10 [2,
    3] dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We reviewed the VGG architecture in a [previous post](https://medium.com/towards-data-science/image-classification-for-beginners-8546aa75f331).
    Please take a look if you are unfamiliar.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/image-classification-for-beginners-8546aa75f331?source=post_page-----dea082804e14--------------------------------)
    [## Image Classification For Beginners'
  prefs: []
  type: TYPE_NORMAL
- en: VGG and ResNet architecture from 2014
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/image-classification-for-beginners-8546aa75f331?source=post_page-----dea082804e14--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell,
  prefs: []
  type: TYPE_NORMAL
- en: '***VGG*** *stands for* ***Visual Geometry Group*** *and is a research group
    at the university of Oxford. In 2014, they designed a deep convolutional neural
    network architecture for image classification task and named it after themselves;
    i.e. VGG [1].*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The VGGNet comes in few configurations such as VGG16 (with 16 layers) and VGG19
    (with 19 layers).
  prefs: []
  type: TYPE_NORMAL
- en: 'VGG16’s architecture is as below: it has 13 convolutional layers and 3 fully
    connected layers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1d4da4d93ace6621f6ce11b018e4bb23.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Model Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s implement VGG16 in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Note, the implementation is structured in terms of two attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'features: contains all the convolutional and max pool layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'classifier: contains fully connected layer and the softmax layer for classification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also note we have passed the *input_channel* as an input argument. This parameter
    is 3 if the images are colored, and it is 1 if images are greyscale.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, the first fully connected layer is *nn.Linear(512 * 3* 3,
    4096).* The reason its input dimension is *512*3*3* is because we have set it
    up so that it works for our input images which are *96*96*. If we pass images
    of different sizes, we have to change this value. For example, for 224*224 images
    this layer becomes *nn.Linear(512 * 7* 7, 4096).*
  prefs: []
  type: TYPE_NORMAL
- en: 'We then implement the *forward()* method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now that the network is complete, let’s pass a random tensor through it and
    see how its shape changes as it goes through various layers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And it prints the following shapes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: So the final output is a 10-dimensional vector which represents probability
    of the image belonging to any of the 10 classes.
  prefs: []
  type: TYPE_NORMAL
- en: Data Transformation — STL10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let’s train it on STL10 dataset [2,3] which is licensed for commercial
    use. This dataset contains 5000 colored images in 10 categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each image is 96x96 pixels, and the 10 categories are as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s load the data and take a look at few images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'and it prints these images with their labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b470475af27e16e006cd7b1b4ddb4edd.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s normalize the data. To normalize the data we compute the *mean*
    and *std* first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note that in *trainloader* we are setting *batch_size = len(trainset)* so that
    we load the whole dataset for computing mean and std. Later, when we want to train
    the model, we load data in smaller batch of 128 images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice from above that *np_images* is of shape (5000, 3, 96, 96) i.e. it is
    5000 images of 96x96 pixels in color scale (note the number of channels is 3 which
    indicates images are colored). So the mean and std are as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**mean = [**0.44671103, 0.43980882, 0.40664575]'
  prefs: []
  type: TYPE_NORMAL
- en: '**std =** [0.2603408, 0.25657743, 0.2712671'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use this mean and std to normalize both test and train data. Let’s
    define transformations of each dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Training the Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We first define the hyper-parameters such as:'
  prefs: []
  type: TYPE_NORMAL
- en: learning rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: learning rate scheduler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'loss function: which is cross entropy for classification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: optimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Method 1:** train_batch: for all batches in data, it trains the model, computes
    the loss and update the parameters. This method applies backpropagation and computes
    training loss.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Method 2**: is the validate_batch function where we validate the model on
    a batch from test loader. Often after each epoch we call this function to get
    the performance of model at the end of each epoch. This function computes the
    loss on test set (the unseen data) and DOES NOT apply any backpropagation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Let the actual training start …
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For every epoch, we train the model and check the performance of the model on
    test dataset. We call *vgg_scheduler.step()* then inform scheduler to increment
    its internal counter and update the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We see the following performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We see the accuracy of the model at epoch 11 reaches 80.8% on test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s look at 10 images and what model has predicted for their label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: For example, we see the following image that is a bird and model has predicted
    it correctly to be a bird.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/336b04bab40050a1c566163b49fec182.png)'
  prefs: []
  type: TYPE_IMG
- en: image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'And we see an example of a wrong prediction where the image is a plane but
    VGG predicts it as a bird:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1507f014439c9d579fec6483005cf3f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our implementation part on VGG model. We see that VGG has a very
    deep architecture with many parameters, however its implementation is quite straightforward
    and this is due to its uniformity in the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have reviewed [VGG and ResNet in concept](/image-classification-for-beginners-8546aa75f331)
    and only VGG in code. In the next post, we can look at ResNet implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Let me know if you have any comment or questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have any questions or suggestions, feel free to reach out to me:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Email: mina.ghashami@gmail.com'
  prefs: []
  type: TYPE_NORMAL
- en: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Very deep convolutional networks for large scale image recognition](https://arxiv.org/pdf/1409.1556.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://pytorch.org/vision/main/generated/torchvision.datasets.STL10.html](https://pytorch.org/vision/main/generated/torchvision.datasets.STL10.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://cs.stanford.edu/~acoates/stl10/](https://cs.stanford.edu/~acoates/stl10/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
