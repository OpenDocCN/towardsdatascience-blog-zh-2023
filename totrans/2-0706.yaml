- en: Democratizing Machine Learning with AWS SageMaker AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/democratizing-machine-learning-with-aws-sagemaker-automl-150299c70396](https://towardsdatascience.com/democratizing-machine-learning-with-aws-sagemaker-automl-150299c70396)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://brus-patrick63.medium.com/?source=post_page-----150299c70396--------------------------------)[![Patrick
    Brus](../Images/a252fe1c4f7a9ed2225d415571137e45.png)](https://brus-patrick63.medium.com/?source=post_page-----150299c70396--------------------------------)[](https://towardsdatascience.com/?source=post_page-----150299c70396--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----150299c70396--------------------------------)
    [Patrick Brus](https://brus-patrick63.medium.com/?source=post_page-----150299c70396--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----150299c70396--------------------------------)
    ·16 min read·Apr 25, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d6177546bde28b57814053a64914628.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Joshua Sortino](https://unsplash.com/@sortino?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI is still one of the hottest topics as of now, especially with the rise of
    ChatGPT. Many companies are now trying to make use of AI in order to extract useful
    insights from data that can be used to optimize their processes or bring out better
    products.
  prefs: []
  type: TYPE_NORMAL
- en: However, building effective AI models requires a lot of expertise in different
    fields, like data preprocessing, model selection, hyperparameter tuning and many
    more. All these fields can be time-consuming and require specialized knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: This is where AutoML comes into play. AutoML automates many of the above-mentioned
    fields required for building an AI model.
  prefs: []
  type: TYPE_NORMAL
- en: AutoML is rapidly becoming a popular solution for businesses and data scientists.
    It empowers organizations to leverage ML and AI to make informed decisions, without
    requiring them to be experts in data science. With the increasing demand for ML
    in businesses, AutoML provides an easy and efficient way to create accurate models,
    regardless of one’s expertise.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we’ll examine one very popular AutoML tool available in the
    market today, AWS SageMaker AutoML, and demonstrate how it can be used to solve
    complex ML use cases.
  prefs: []
  type: TYPE_NORMAL
- en: I will train a model with the old-fashioned manual approach and compare the
    results to the ones that AWS SageMaker AutoML produces.
  prefs: []
  type: TYPE_NORMAL
- en: I will use the credit card fraud detection dataset from Kaggle for this comparison
    [1]. You can find the dataset [here](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud).
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this article, you’ll have a clear understanding of how AutoML
    can help leverage ML to drive meaningful insights and make informed decisions.
  prefs: []
  type: TYPE_NORMAL
- en: AWS SageMaker AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/f89b70e37f4074e49b6a1784eb5d35ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Overview of AWS SageMaker AutoML (Image by author, based on [2]).'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1 gives an overview on the different steps that AWS SageMaker AutoML
    solves.
  prefs: []
  type: TYPE_NORMAL
- en: 'It includes the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Preparation:** You can easily upload your data to Amazon S3\. Once your
    data is uploaded, SageMaker AutoML automatically analyzes your data in order to
    detect any missing values, outliers or data types that need to be transformed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Automatic Model Creation:** AWS SageMaker AutoML automatically trains multiple
    machine learning models with different hyperparameters and algorithms to determine
    the best model for your data. It also provides automatic model tuning, which adjusts
    the hyperparameters of the selected models to further optimize their performance.
    It also creates the notebooks for running the model selection automatically for
    you, so that you have a full visibility about what is executed during this process.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model Deployment:** Once the best model has been selected, AWS SageMaker
    offers to deploy the model to a SageMaker endpoint or a batch transform job, where
    it can be used to make predictions on new data. On top of it, AWS SageMaker Model
    Monitor can be utilized in order to be alerted if any issues arise (like data
    drifts, concept drifts, …). It also provides tools for retraining the model with
    new data, as well as updating the model’s hyperparameters or algorithms to improve
    its performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AWS SageMaker AutoML offers a [Python SDK](https://sagemaker.readthedocs.io/en/stable/api/training/automl.html)
    that can be used for starting your AutoML job and a [GitHub repository](https://github.com/aws/amazon-sagemaker-examples/tree/main/autopilot)
    with various different notebook examples on how to utilize the AutoML SDK for
    concrete ML use cases.
  prefs: []
  type: TYPE_NORMAL
- en: There are also other powerful and well-known AutoML tools available in the market,
    such as Google Cloud AutoML and H2O.ai, which also have their own unique strengths
    and weaknesses.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud AutoML is known for its ease of use and intuitive interface, which
    makes it perfect in case you are new to ML and also not that deep into coding.
    Google Cloud AutoML supports image data, video data, text data and tabluar data.
    You can read more about that [here](https://cloud.google.com/vertex-ai/docs/training-overview?hl=en#automl).
  prefs: []
  type: TYPE_NORMAL
- en: H20.ai is known for its speed and scalability, making it a good option for large
    datasets and complex models. H20.ai offers interfaces in R, Python, or a web GUI.
    You can read more about its features [here](https://h2o.ai/platform/h2o-automl/).
  prefs: []
  type: TYPE_NORMAL
- en: Manual Training Approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before I’m going to use AWS SageMaker AutoML to come up with a classifier for
    the credit card dataset, I first train a model in the classic way: Doing everything
    myself from scratch.'
  prefs: []
  type: TYPE_NORMAL
- en: This later helps me to have a baseline and to compare my own approach to the
    AutoML approach from AWS, with the expectation that AWS SageMaker AutoML outperforms
    my manual, semi-optimal, approach.
  prefs: []
  type: TYPE_NORMAL
- en: For the manual approach, I make use of Scikit-learn and I will run through the
    steps highlighted in the next chapters.
  prefs: []
  type: TYPE_NORMAL
- en: You can also find the complete notebook in my GitHub repository [here](https://github.com/patrickbrus/aws-auto-ml-credit-fraud/blob/master/manual-approach/credit-fraud-ml.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: '**Data preparation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I load the dataset from a CSV file and first check the distribution of the dataset.
    This shows that the dataset is highly imbalanced, with only **0.17%** of all samples
    being positive.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset itself doesn’t contain any missing values.
  prefs: []
  type: TYPE_NORMAL
- en: I then split the dataset with an 80/20 split into train and test and scale the
    data to be in the range 0–1, while the standard scaler is only trained on the
    training set to avoid some overly optimistic results.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code for these steps below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Typically, extensive Exploratory Data Analysis (EDA) would also be part of the
    data preparation step. But for the sake of this experiment, I did not do extensive
    EDA, as the dataset is already well-prepared for ML.
  prefs: []
  type: TYPE_NORMAL
- en: But just keep in mind that this part is also crucial to the success of your
    ML training and also takes some time typically.
  prefs: []
  type: TYPE_NORMAL
- en: Model Selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step is to figure out what ML algorithm is best suited for the data.
    For this purpose, I first train a very simple baseline model using logistic regression.
    This is to already have something simple that I can then compare more complex
    algorithms to.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal should always be: Keep it simple! Don’t start with a Neural Network,
    which is harder to explain at the end, if also a simple algorithm, like logistic
    regression, could do the job.'
  prefs: []
  type: TYPE_NORMAL
- en: The logistic regression model achieved an F1-Score of **70.6**%. I am using
    the F1-Score for this dataset, as it is highly imbalanced and the accuracy would
    not deliver a meaningful measure of the model, as only predicting all classes
    as negative would already lead to an accuracy of more than **99%**!
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code for training the baseline model below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Okay, we now have a baseline. Let’s now try out different classification algorithms
    with their default hyperparameters, and let’s see what algorithm performs best
    on the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'I used a 5-fold cross-validation to train each of the following models:'
  prefs: []
  type: TYPE_NORMAL
- en: decision tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: supported vector machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-nearest neighbors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: random forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ada-boost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The random forest algorithm delivered the best results with an F1-Score of **86.9%**,
    followed by the nearest neighbor algorithm with an F1-Score of **84.8%**. Not
    bad!
  prefs: []
  type: TYPE_NORMAL
- en: Next step is to fine tune the winner (random forest).
  prefs: []
  type: TYPE_NORMAL
- en: For this, I selected some values of the hyperparameters that I want to try out
    and used a randomized cross-validation search in order to find the best set of
    hyperparameters leading to the best model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The best model scored more or less the same as the one that I got without tuning
    the hyperparameters. What a waste of time ;)
  prefs: []
  type: TYPE_NORMAL
- en: Model Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Last but not least, I evaluate my final model on the hold-out test set to see
    how it would perform on real-world data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This gives me an F1-Score of **82%,** so close to our validation results but
    a bit below.
  prefs: []
  type: TYPE_NORMAL
- en: I know that one could get more out of the model by tuning it a bit more. But
    the goal in this article is to just do some basic ML and compare the results to
    AutoML to already see how good AutoML performs and how the effort is to run an
    AutoML job in comparison to only do the basic training myself.
  prefs: []
  type: TYPE_NORMAL
- en: Training with AWS SageMaker AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Okay, now that I have my baseline, I can try to get a better model with less
    effort using AWS SageMaker AutoML.
  prefs: []
  type: TYPE_NORMAL
- en: I will again guide you through the different steps and provide the code snippets
    for all of these steps. You can also find the complete notebook in my GitHub repository
    [here](https://github.com/patrickbrus/aws-auto-ml-credit-fraud/blob/master/sagemaker-auto-ml/fraud-detection-auto-ml-sagemaker.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Data Upload
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For SageMaker AutoML to work, the data needs to be stored in s3\. Therefore,
    I am first creating a bucket and then uploading the CSV file to this bucket (Gif
    1).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e923a4bc225c20097934150620449b6e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Gif 1: Creating s3 bucket and uploading CSV file to this bucket (Gif by author).'
  prefs: []
  type: TYPE_NORMAL
- en: Setup SageMaker Notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step is to setup the environment where I can run the AutoML job in.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this, I first create a notebook in SageMaker where I can then create
    and run the code in.
  prefs: []
  type: TYPE_NORMAL
- en: In AWS, access to services by other services is handled with IAM roles. The
    SageMaker notebook gets an IAM role attached with some default access rights.
    But for accessing my before created s3 bucket, I first have to explicitly adapt
    the policy attached to this role.
  prefs: []
  type: TYPE_NORMAL
- en: Gif 2 shows the complete process of creating the notebook and how I adapted
    the policy of the notebook role. The quality is unfortunately not that good, but
    I still think it is valuable to see the sequence of actions taken. But I also
    added some screenshots of the exact settings when creating the notebook (Figure
    2), and I added the complete policy I attached to the notebook role in my GitHub
    repository [here](https://github.com/patrickbrus/aws-auto-ml-credit-fraud/blob/master/sagemaker-auto-ml/notebook-role-policy.json).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e107018d2acca85d14be0000890bfe55.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Gif 2: Process of creating a notebook and adapting the policy of the attached
    IAM role (Gif by author).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8e346589e66bbb7879cbd5d54085127e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Settings of AWS SageMaker notebook creation (Image by author).'
  prefs: []
  type: TYPE_NORMAL
- en: Run AWS SageMaker AutoML Job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now I can finally start running some code.
  prefs: []
  type: TYPE_NORMAL
- en: The code I am running is mostly copied and adapted from [this](https://gitlab.com/juliensimon/aim307/-/blob/master/aim307.ipynb)
    AWS tutorial notebook.
  prefs: []
  type: TYPE_NORMAL
- en: As a first step, I am loading the data from s3 into a Pandas dataframe and setup
    some general variables required for SageMaker AutoML later.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Then I split the dataset into a training set and a hold-out test set. I will
    then use the latter to compare the AutoML model to my own trained model. I will
    then upload the data to the s3 bucket created by SageMaker so that the AutoML
    job can access it directly from s3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now I can set up the AutoML job and launch it. You can find more about the required
    input parameters and settings in the SageMaker SDK documentation [here](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_auto_ml_job.html#).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The job now runs in the background and creates the required AWS resources for
    you.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can then run the following code in order to track the progress of the AutoML
    job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The job is running through the following stages:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing Data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feature Engineering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model Tuning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Merging AutoML Tasks Reports
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AWS SageMaker is generating two notebooks for you. One for exploring the data
    and one for defining the different candidates that are evaluated on the dataset.
    You can get these notebooks if you are interested in what code AWS SageMaker is
    running for these stages.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to list all the experiments that AWS SageMaker has executed and
    you can list all the explored candidates, too. I did not add the code in this
    article, but you can find it in my [notebook](https://github.com/patrickbrus/aws-auto-ml-credit-fraud/blob/master/sagemaker-auto-ml/fraud-detection-auto-ml-sagemaker.ipynb),
    if you are curios how this works.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate Best Candidate on Testset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now it is time to test the best candidate on the hold-out test set. This is
    the most interesting part to me, as it shows whether AutoML can score something
    better than my manual approach.
  prefs: []
  type: TYPE_NORMAL
- en: First, I am retrieving the best candidate from the AWS SageMaker AutoML job.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Next, I will host this model as an endpoint in AWS where I can send data to
    for inference.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: And last but not least, read the CSV file with the test data and send the data
    to the final model for inference. I then compare the predictions to the ground
    truth and then use a quite manual approach to count up the true positives, true
    negatives, false positives and false negatives.
  prefs: []
  type: TYPE_NORMAL
- en: I honestly was just too lazy to implement something on my own and just adated
    the code again from the AWS SageMaker tutorial notebook that you can find [here](https://gitlab.com/juliensimon/aim307/-/blob/master/aim307.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The final model achieves an F1-Score of **96%** on the hold-out test set!
  prefs: []
  type: TYPE_NORMAL
- en: This is awesome! As comparison, the model I trained with Scikit-Learn only achieved
    an F1-Score of **82%**.
  prefs: []
  type: TYPE_NORMAL
- en: Shortcomings of AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AutoML is definetly powerful and can help to speed up the ML development cycle.
    But there are also shortcomings of using AutoML that should be recognized.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main limitations of AutoML is that it can be somewhat of a black
    box approach, as it automates much of the process of building a machine learning
    model. This can make it difficult for data scientists to fully understand how
    the model works and may limit their ability to fine-tune the model or debug any
    issues that arise.
  prefs: []
  type: TYPE_NORMAL
- en: AWS tries to combat this by providing jupyter notebooks that show the code for
    stages like data exploration or exploring different ML candidates. This already
    helps to get some insights, but if there are any issues on the code or the findings
    that the AutoML job had, then the data scientist has no influence on changing
    the code behind, as everything is auto-generated.
  prefs: []
  type: TYPE_NORMAL
- en: Another potential drawback of using AutoML is that it can be less flexible than
    a more traditional approach to building a machine learning model. AutoML is optimized
    for efficiency and ease of use, but this may come at the expense of customization
    options or the ability to work with specialized datasets or models.
  prefs: []
  type: TYPE_NORMAL
- en: For this article, I have used a very simple dataset and AWS SageMaker AutoML
    was able to train a good candidate. But if there is a more challenging dataset,
    than it is not clear how well AutoML would perform on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Personal Findings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, I want to highlight my personal findings while using AWS SageMaker
    AutoML.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing I recognized is that it is quite complex to use and to set it
    up. I personally have some more experience in using AWS in general and also in
    coding, but if there is a person with not that mutch of prior knowledge, then
    the learning curve of using AWS SageMaker AutoML could be too steep.
  prefs: []
  type: TYPE_NORMAL
- en: I also think that the documentation is not that well. I had a hard time in the
    beginning to find something about end-to-end usecases in the documentation. I
    could then find video trainings and GitHub repositories with example notebooks,
    but I personally like written documentation more than videos.
  prefs: []
  type: TYPE_NORMAL
- en: And please be careful about the costs. I initially played around with SageMaker
    AutoML and had the AutoML job running more often, because I wanted to try out
    different things there. But that wasn’t as cheap as I hoped and I ended up paying
    more on this experiment than I planned.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, I compared AWS SageMaker AutoML to manually training a model
    using Scikit-Learn.
  prefs: []
  type: TYPE_NORMAL
- en: As dataset, I decided to use the fraud detection dataset, as this has some difficulties
    in it because of the high degree of imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: I then manually tried to find a good classifier and did the same with AWS SageMaker
    AutoML.
  prefs: []
  type: TYPE_NORMAL
- en: I finally compared the results of both approaches on a hold-out test set, where
    AWS SageMaker AutoML outscored my manual approach, as it achieved an F1-Score
    of **96%**, compared to **82%**.
  prefs: []
  type: TYPE_NORMAL
- en: This shows that it totally makes sense to use AWS SageMaker AutoML to train
    ML on your dataset in order to quickly come up with a classifier that you can
    use.
  prefs: []
  type: TYPE_NORMAL
- en: It is even not required to have expertise on the field of ML for making use
    of AWS SageMaker AutoML.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, my manual approach has to be taken with a grain of salt.
  prefs: []
  type: TYPE_NORMAL
- en: I did not invest a lot of time to fine tune my final classifier and I am pretty
    sure with a bit more invested time I would have scored also better results on
    the hold-out test set.
  prefs: []
  type: TYPE_NORMAL
- en: At least I hope so ;)
  prefs: []
  type: TYPE_NORMAL
- en: But the idea of this article was to show how easy it can be to just make use
    of an AutoML library to come up with a classifier for your ML dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This at the end doesn’t need to be the final one that you put into a product,
    but you could at least do a quick initial proof-of-concept in order to see whether
    you can get something useful out of your data.
  prefs: []
  type: TYPE_NORMAL
- en: Outlook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I till now only took a deeper look into AWS SageMaker AutoML, but it would definetly
    be interesting to also check other offerings, like Google Cloud AutoML.
  prefs: []
  type: TYPE_NORMAL
- en: I am planing on doing a complete evaluation of Google Clouds Vertex AI offering
    in the near future and will then write about my findings in Medium as well.
  prefs: []
  type: TYPE_NORMAL
- en: Then I can also talk specifically about how Sagemaker performs compared to Google
    Clouds offer.
  prefs: []
  type: TYPE_NORMAL
- en: So follow me in case you don’t want to miss that!
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading my article to the end! I hope you enjoyed this article.
    If you want to read more articles like this in the future, follow me to stay updated.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Join my email list if you want to learn more about machine learning and
    the cloud.**](https://medium.com/subscribe/@brus-patrick63)'
  prefs: []
  type: TYPE_NORMAL
- en: Contact
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[**LinkedIn**](https://www.linkedin.com/in/patrick-brus/) |[**GitHub**](https://github.com/patrickbrus)'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1]: Machine Learning Group — ULB, [“Credit Card Fraud Detection”](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud),
    Kaggle, 2018, [Database Contents License](https://opendatacommons.org/licenses/dbcl/1-0/)
    (DbCL) v1.0'
  prefs: []
  type: TYPE_NORMAL
- en: '[2]: AWS, [Amazon SageMaker Autopilot](https://aws.amazon.com/sagemaker/autopilot/?sagemaker-data-wrangler-whats-new.sort-by=item.additionalFields.postDateTime&sagemaker-data-wrangler-whats-new.sort-order=desc)
    (accessed 4/1/2023)'
  prefs: []
  type: TYPE_NORMAL
