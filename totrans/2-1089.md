# Meta的人工智能如何基于参考旋律生成音乐

> 原文：[https://towardsdatascience.com/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783](https://towardsdatascience.com/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783)

## MusicGen分析

[](https://medium.com/@maxhilsdorf?source=post_page-----de34acd783--------------------------------)[![Max Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page-----de34acd783--------------------------------)[](https://towardsdatascience.com/?source=post_page-----de34acd783--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----de34acd783--------------------------------) [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page-----de34acd783--------------------------------)

·发布在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----de34acd783--------------------------------) ·10分钟阅读·2023年6月23日

--

![](../Images/c46f4b570f4f5b87a647a152468cb6be.png)

图片来源：作者。

# Meta的MusicGen

2023年6月13日，Meta（前身为Facebook）在音乐和人工智能界掀起了波澜，发布了他们的生成音乐模型MusicGen。这个模型不仅在能力上超越了今年早些时候发布的Google MusicLM，还使用了授权音乐数据进行训练，并且对非商业用途开源。

这意味着你不仅可以阅读[研究论文](https://arxiv.org/abs/2306.05284)或听取[演示](https://ai.honu.io/papers/musicgen/)，还可以从[GitHub](https://github.com/facebookresearch/audiocraft)复制他们的代码，或者在[HuggingFace](https://huggingface.co/spaces/facebook/MusicGen)上的网页应用中试验模型。

除了从文本提示生成音频外，MusicGen还可以基于给定的参考旋律生成音乐，这一功能被称为旋律条件化。在这篇博客文章中，我将演示Meta如何将这个有用且迷人的功能实现到他们的模型中。但在我们深入之前，让我们先了解旋律条件化在实际中是如何工作的。

# 展示

## 基础曲目

以下是我为这篇文章制作的一段短电子音乐片段。它包含电子鼓、突出的808低音和两个切分合成器。在听的时候，试着识别出这段音乐的“主要旋律”。

使用MusicGen，我现在可以在其他风格中生成与相同主要旋律相符的音乐。我所需的只是我的基础曲目和描述新作品声音的文本提示。

## 管弦乐变奏

> 一段宏伟的管弦乐编曲，配有震撼的打击乐、史诗般的铜管号角和高亢的弦乐，营造出适合英雄战斗的电影氛围。

## 雷鬼变奏

> 经典雷鬼曲目加电子吉他独奏

## 爵士变奏

> 流行爵士乐，包含萨克斯风独奏、钢琴和弦，以及完整的军鼓

## 结果有多好？

尽管 MusicGen 并没有严格遵循我的文本提示，并创作出略微不同于我要求的音乐，但生成的作品仍然准确反映了所请求的音乐类型，更重要的是，每一首作品都展示了其对基础曲目主旋律的独特解读。

虽然结果并不完美，但我发现这个模型的能力相当令人印象深刻。自发布以来，MusicGen 一直是 HuggingFace 上最受欢迎的模型之一，这进一步强调了它的重要性。话虽如此，让我们更深入地探讨旋律条件化工作的技术方面。

# 文本到音乐模型的训练方式

![](../Images/aaef8f27c95e8c387eb3636fcd546e9b.png)

三个文本-音乐对，作为训练模型如 MusicLM 或 MusicGen 的用途。图片由作者提供。

几乎所有当前的生成音乐模型在训练期间都遵循相同的程序。它们配备了一个包含大量音乐曲目及其对应文本描述的数据库。模型学习单词和声音之间的关系，以及如何将给定的文本提示转换为连贯且愉悦的音乐作品。在训练过程中，模型通过将其自身创作与数据集中的真实音乐曲目进行比较来优化自身创作。这使得模型能够识别自身的优点和需要改进的地方。

问题在于，一旦机器学习模型针对特定任务进行训练，比如文本到音乐生成，它就局限于那个特定任务。虽然可以让 MusicGen 执行一些它没有明确训练过的任务，比如延续给定的音乐片段，但不能期望它能处理所有的音乐生成请求。例如，它不能简单地将一段旋律转变为不同的音乐风格。这就像把土豆扔进烤面包机期待出来的是薯条一样。相反，必须训练一个单独的模型来实现这种功能。

# 对训练配方的简单调整

让我们探讨一下 Meta 如何调整模型训练过程，以使 MusicGen 能够根据文本提示生成给定旋律的变体。然而，这种方法存在几个挑战。主要障碍之一是识别歌曲的“旋律”并以计算上有意义的方式表示它的模糊性。尽管如此，为了从更广泛的层面理解新的训练过程，假设我们对什么构成“旋律”以及如何轻松提取并输入到模型中的达成共识。在这种情况下，调整后的训练方法可以概述如下：

![](../Images/c2ff451f59be81accfe76a431e7f968f.png)

三个文本-音乐-旋律对，作为教学 MusicGen 旋律条件生成的用途。

对于数据库中的每个曲目，第一步是提取其旋律。随后，模型会接收曲目的文本描述和相应的旋律，促使模型重建原始曲目。本质上，这种方法简化了原始的训练目标，即模型仅仅被要求根据文本重建曲目。

要理解我们这样做的原因，让我们问问自己AI模型在这个训练过程中学到了什么。从本质上讲，它学会了如何将旋律转化为基于文本描述的完整音乐作品。这意味着在训练结束后，我们可以提供旋律给模型，并要求它创作任何风格、情绪或乐器的音乐作品。对模型来说，这与它在训练过程中成功完成过无数次的“半盲”生成任务是一样的。

了解了Meta用来教模型旋律条件音乐生成的技术后，我们仍然需要解决如何精确定义“旋律”的挑战。

# 什么是“旋律”？

事实上，除非所有乐器同时演奏，否则没有客观的方法来确定或提取多声部音乐作品中的“旋律”。虽然通常会有一个突出的乐器，如声乐、吉他或小提琴，但这并不一定意味着其他乐器不属于“旋律”的一部分。以皇后乐队的《波希米亚狂想曲》为例。当你想到这首歌时，你可能首先会回忆起弗雷迪·摩克瑞的主要声乐旋律。然而，这是否意味着前奏中的钢琴、中段的背景歌手和“你认为你能石化我……”之前的电吉他并不属于旋律的一部分？

提取歌曲的“旋律”的一种方法是将最突出、通常是混音中最响亮的旋律视为最主导的旋律。色谱图是一种广泛使用的表示方法，直观地展示了整个曲目中最主导的音乐音符。下面，你可以找到参考曲目的色谱图，最初包括完整的乐器配置，然后去除鼓和贝斯。在左侧，旋律中最相关的音符（B，F#，G）用蓝色突出显示。

![](../Images/61f9dfe653cc45855c69396818f30a2c.png)

两个色谱图都准确描绘了主要的旋律音符，其中不含鼓和贝斯的曲目版本提供了更清晰的旋律可视化。Meta的研究也揭示了相同的观察结果，这促使他们使用其源分离工具（[DEMUCS](https://github.com/facebookresearch/demucs)）去除曲目中的干扰节奏元素。这一过程得出的“旋律”表现足够代表，然后可以输入到模型中。

总结来说，我们现在可以将这些信息连接起来，以理解请求 MusicGen 执行旋律条件生成时的基本过程。以下是工作流程的可视化表示：

![](../Images/d60310a09049230a2fcef54f043799b4.png)

MusicGen 如何生成旋律条件音乐输出。图片由作者提供。

# 局限性

![](../Images/668f4fbadf28ea7be86fa0e016e86bb4.png)

[Xavier von Erlach](https://unsplash.com/@xavier_von_erlach?utm_source=medium&utm_medium=referral) 拍摄的照片，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

虽然 MusicGen 在旋律条件生成方面展现了有希望的进展，但仍需认识到技术仍在不断完善中。即使去掉鼓和贝斯，色度图谱（chromagrams）也无法完美地表示乐曲的旋律。一个局限性是色度图谱将所有音符分类到 12 个西方音高类别中，这意味着它们能捕捉到两个音高类别之间的过渡，但不能显示旋律的方向（上升或下降）。

例如，从 C4 移动到 G4（一个纯五度）之间的旋律间隔与从 C4 移动到 G3（一个纯四度）之间的差异显著。然而，在色度图谱中，这两个间隔会显得相同。问题在于八度跳跃会加剧，因为色度图谱会显示旋律停留在相同音符上。考虑一下色度图谱如何误解 Céline Dion 在“我心永恒”（“My Heart Will Go On”）中的“wher-e-ver you are”这一情感丰富的八度跳跃为稳定的旋律移动。为此，只需查看下面 A-ha 的“Take on Me”合唱部分的色度图谱。这是否反映了你对歌曲旋律的想法？

![](../Images/0d0811f82da144bb314f95daf6ccc889.png)

“Take on Me”（A-ha）的合唱部分的色度图谱，去掉了贝斯和鼓。图片由作者提供。

另一个挑战是色度图谱的固有偏差。它在捕捉某些歌曲的旋律时表现良好，而在其他歌曲中则完全失误。这种偏差是系统性的，而非随机的。色度图谱更能准确地表示具有主导旋律、最小音程跳跃和统一演奏的歌曲，相比之下，对于复杂旋律分布在多个乐器上并具有大音程跳跃的歌曲则表现不佳。

此外，生成 AI 模型本身的局限性也值得注意。输出的音频与人工制作的音乐仍有明显差异，并且在六秒的时间间隔内保持一致风格仍然是一个挑战。此外，MusicGen 在忠实捕捉文本提示的复杂方面上存在不足，之前提供的示例便证明了这一点。旋律条件生成要达到既能用于娱乐和灵感激发，也能生成适合最终用户的音乐的水平，还需要进一步的技术进步。

# 未来展望

![](../Images/a340ae4f2737c60c6d2ecd4b7095b9b5.png)

图片由 [Marc Sendra Martorell](https://unsplash.com/pt-br/@marcsm?utm_source=medium&utm_medium=referral) 提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

## 我们如何改进AI？

从我的角度来看，未来研究应主要关注的一个问题是如何从曲目中提取和表征“旋律”。虽然色谱图是一种成熟且直接的信号处理方法，但也有许多较新的实验性方法使用深度学习来实现这一目标。看到像Meta这样的公司从这些进展中获得灵感将是令人兴奋的，这些进展中的许多内容都在[Reddy et al. (2022)](https://arxiv.org/pdf/2202.01078.pdf)的一份全面的72页综述中进行了介绍。

关于模型本身的质量，无论是音频质量还是对文本输入的理解，都可以通过扩大模型和训练数据的规模以及开发更高效的算法来提高。依我看来，2023年1月发布的MusicLM类似于“GPT-2时刻”。我们开始见证这些模型的能力，但在各个方面仍需显著改进。如果这个类比成立，我们可以预期，类似GPT-3的音乐生成模型的发布可能会比我们预期的要早。

## 这对音乐家有何影响？

与生成音乐AI经常出现的情况一样，关于其对音乐创作者工作的潜在负面影响也引发了关注。我预计，未来通过创造现有旋律的变体来谋生将变得越来越困难。这在铃声制作等场景中特别明显，企业可以轻松地以最低成本为新的广告活动或个性化广告生成大量变化的铃声旋律。这无疑对依靠这些活动作为主要收入来源的音乐家构成了威胁。我重申我的呼吁，希望参与创作那些因其客观音乐特质而非主观人类特质（如库存音乐或铃声）的音乐创作者探索其他收入来源，为未来做准备。

从积极的一面来看，旋律条件的音乐生成提供了一种令人难以置信的工具，能够提升人类的创造力。如果有人开发出引人入胜且令人难忘的旋律，他们可以快速生成其在各种风格中的可能效果。这一过程有助于确定理想的音乐风格和类型，以使音乐栩栩如生。此外，它提供了一个机会，可以重新审视个人音乐目录中的过去项目，探索将其转化为不同风格或类型时的潜力。最后，这项技术降低了没有正式音乐培训的创造性人士进入这一领域的门槛。现在，任何人都可以想出一个旋律，将其哼唱到智能手机麦克风中，并与朋友、家人分享其杰出的编排，甚至尝试触及更广泛的受众。

AI音乐生成是否对我们的社会有益的问题仍然有待讨论。然而，我坚信旋律条件的音乐生成是这一技术真正提升专业人士和有志创作者工作的一种应用场景。它通过提供新的探索途径来增加价值。我热切期待在不久的将来见证这一领域的进一步进展。

如果你对音乐与AI的交汇点感兴趣，你可能也会喜欢我关于这一主题的其他一些文章：

1.  [谷歌如何使用虚假数据集来训练生成音乐AI](/how-google-used-fake-datasets-to-train-generative-music-ai-def6f3f71f19)

1.  [聊天机器人即将颠覆音乐搜索](/chatbots-are-about-to-disrupt-music-search-1e4a4cd7ba01)

1.  [MusicLM — 谷歌是否解决了AI音乐生成问题？](https://medium.com/towards-data-science/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c)
