- en: Mixture Models, Latent Variables and the Expectation Maximization Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/mixture-models-latent-variables-and-the-expectation-maximization-algorithm-e5b18e15faa](https://towardsdatascience.com/mixture-models-latent-variables-and-the-expectation-maximization-algorithm-e5b18e15faa)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://reoneo.medium.com/?source=post_page-----e5b18e15faa--------------------------------)[![Reo
    Neo](../Images/a3c192dafc1222b06b2e7fcf4d35cb27.png)](https://reoneo.medium.com/?source=post_page-----e5b18e15faa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e5b18e15faa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e5b18e15faa--------------------------------)
    [Reo Neo](https://reoneo.medium.com/?source=post_page-----e5b18e15faa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e5b18e15faa--------------------------------)
    ·8 min read·Mar 19, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eae2fea5e6268b616a5acfcaca0178b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Sung Shin](https://unsplash.com/ko/@ironstagram?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning has always been fascinating to me. It is a way to learn
    about data without manual labeling effort and allows for the identification of
    patterns within the dataset. Out of the various unsupervised learning techniques,
    the simplest of which is clustering. In essence, a clustering algorithm aims to
    find data points that are similar to one another. By clustering data points together
    we can derive valuable insights about the dataset and what the various clusters
    represent.
  prefs: []
  type: TYPE_NORMAL
- en: This article aims to provide an in-depth look at the Gaussian Mixture Model
    clustering algorithm, how it models the data, and more importantly how Expectation-Maximization
    can be used to fit the model on a new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is a mixture model?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Essentially, a mixture model (or mixture distribution) is a **combination of
    multiple probability distributions into a single one**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a74bf32579fb4ddda706f9cbdcbde8e5.png)'
  prefs: []
  type: TYPE_IMG
- en: PDF of Mixture Model
  prefs: []
  type: TYPE_NORMAL
- en: To combine these distributions together, we assign a **weight** to each component
    distribution such that the total probability under the distribution sums to 1\.
    A simple example would be a mixture distribution that consists of 2 gaussians.
    We can have 2 distributions of different means and variance and combine the 2
    using different weights.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/31a44cd2b833fc89d86614e39ecaf2e1.png)![](../Images/d3812ae7c15398d04fc9df4fd09221db.png)'
  prefs: []
  type: TYPE_IMG
- en: 2 GMMs with the same mean but different π values — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Concretely, we can think of this distribution arising from a two-step generative
    process. In this process, a data point can be generated from n different probability
    distributions. First, we determine which probability distribution it is from.
    This probability is simply the weight π_i. Once the component probability distribution
    has been selected, the data point can be generated by simulating the component
    probability distribution itself.
  prefs: []
  type: TYPE_NORMAL
- en: '**GMM**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A gaussian mixture model is essentially a mixture model in which **all the component
    distributions are gaussian distributions.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25c24b86e8715bae51890cf334bdd409.png)'
  prefs: []
  type: TYPE_IMG
- en: PDF of GMM Model
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s try to understand why the gaussian distribution is used to model the
    components of the mixture. When looking at a dataset, we want to cluster similar
    points together. These clusters will usually be spherical or elliptical in nature
    since we want points that are close together to be clustered.
  prefs: []
  type: TYPE_NORMAL
- en: As such, a normal distribution is a good model for the cluster. The mean of
    the distribution will be the **center of the cluster** while the **shape** and
    **spread** of the cluster can be well modeled by the **covariance of the distribution**.
  prefs: []
  type: TYPE_NORMAL
- en: The second variable for the cluster will be the **relative sizes of different
    clusters**. In an organic dataset, we don’t usually expect the clusters to be
    of the same size and this means that some clusters will have more points than
    others. The size of the clustering will then be determined by the cluster weights
    π_i.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of clustering, we assume that there are *k* influencing factors
    that affect the generation of the data. Each influencing factor has a **different
    weight** which corresponds to the cluster weight π.
  prefs: []
  type: TYPE_NORMAL
- en: '**Defining the Terms**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before going thru the model derivation, let’s first define a few key terms.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1d1d0584c23cf379e660fa695b1bf071.png)'
  prefs: []
  type: TYPE_IMG
- en: Mathematical Function for Cluster Responsibilities
  prefs: []
  type: TYPE_NORMAL
- en: 'θ: This refers to the parameters of the GMM model which are the means and covariances
    of the clusters θ = {μ, Σ,π}. There are *k* values of θ since each cluster has
    a corresponding parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y_n* — This refers to the n-th datapoint within the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*z_n* — This refers to the cluster that the nth datapoint is from'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*r_nk* — Cluster responsibilities (explained in next section)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fitting the GMM model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have our model, we want to use it to gain insights into the data.
    For a clustering problem, this would be the predicted cluster assignments ***z_n^***.
    Instead of solving for a definite cluster assignment, the GMM model allows us
    to calculate the cluster probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'This value ***r_nk*** is the influence of cluster *k* on the nth data point
    and has the name cluster responsibility. This approach is known as soft clustering
    where each data point is **fractionally assigned** to each cluster instead of
    the datapoint being **assigned to a single cluster outright**. Being a probability
    distribution, the vector ***r_nk*** will sum to 1 for all points. Mathematically,
    this value is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2da31cc2dee72f76ca49ca59b5fc6314.png)'
  prefs: []
  type: TYPE_IMG
- en: Cluster responsibility definition
  prefs: []
  type: TYPE_NORMAL
- en: From the definition, we can see that this is a posterior probability of z_n
    given the data y_n and parameters θ.
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of this approach allows for **identifying data points that the model
    is more unsure about**. Alternatively, if we want to find the most probable cluster,
    we can convert our results back to **hard clustering** by assigning each datapoint
    to the cluster with the highest responsibility.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b5087a6590f16e0305846428a919fd82.png)'
  prefs: []
  type: TYPE_IMG
- en: Argmax function on cluster responsibility for hard clustering
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Bayes Rule, we can breakdown the posterior even further:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1d1d0584c23cf379e660fa695b1bf071.png)'
  prefs: []
  type: TYPE_IMG
- en: Expansion of cluster responsibilities using Bayes Rule
  prefs: []
  type: TYPE_NORMAL
- en: In the numerator, the posterior is broken down into the prior ***p(z_n=k|θ)***
    and the likelihood ***p(y_n|z_n=k, θ)***. The denominator is then the normalizing
    constant which takes into account **all possible clusters**.
  prefs: []
  type: TYPE_NORMAL
- en: Here we also encounter an issue with solving for the posterior. We are unable
    to solve for the responsibilities without first knowing the parameter estimates
    θ. At the same time, without knowing the cluster responsibilities, there is no
    way to derive good estimates for the responsibilities. Thus, we use the **Expecation-Maximization
    algorithm** which is used to solve latent variable models.
  prefs: []
  type: TYPE_NORMAL
- en: EM Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A latent variable is simply a variable that is **never explicitly observed**
    in the dataset. In this case, the latent variable is the cluster assignments ***z_n***.
  prefs: []
  type: TYPE_NORMAL
- en: 'This algorithm is an **iterative 2-step algorithm**. It starts with randomly
    initialized values for the parameter *θ* and then iteratively refines estimates
    for the latent variable *z_n* and the parameters *θ* on each iteration. For each
    iteration, there are 2 steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**E-Step**: In the E-step we try to estimate the probability distribution of
    the latents *z_n* **given the current estimates for *θ***. This is equivalent
    to constructing an **expectation of the conditional log-likelihood given Z and
    *θ***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/b5a0cdcfe0f49ad03cbf201b8e545880.png)'
  prefs: []
  type: TYPE_IMG
- en: Expected conditional log-likelihood given the current parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**M-Step:** In the M-step, **we generate a new set of estimates for *θ* given
    the current expected Z** (from the E-step). This is done by maximizing the term
    in the M-step w.r.t the parameters *θ*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the above steps sound really confusing, don’t worry! Let’s look at this example
    in the context of GMM.
  prefs: []
  type: TYPE_NORMAL
- en: '**E-Step for GMM**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/1d1d0584c23cf379e660fa695b1bf071.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s try to solve the equation for cluster responsibilities that we derived
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the numerator, we have the first term ***p(z_n=k|θ)*** which is the probability
    that the data point belongs to cluster *k*. This term does not consider the likelihood
    and thus will just be *π_k*. The second term ***p(y_n|z_n=k,θ)*** is the likelihood
    **given that it belongs to cluster k** which is the gaussian likelihood. Essentially,
    this probability becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/052b0e47abf7e506b86bb968bbbf9845.png)'
  prefs: []
  type: TYPE_IMG
- en: Simplified derivation of Cluster Responsibilities
  prefs: []
  type: TYPE_NORMAL
- en: For each point in the dataset, we calculate ***r_n*** a vector of size *k* that
    represents the probability of the point belonging to each cluster. Since we have
    our randomly initialized values of μ and Σ this is solvable. Once, the E-step
    is completed we can move to the M-step.
  prefs: []
  type: TYPE_NORMAL
- en: '**M-Step for GMM**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the M-step, we use the **cluster responsibilities in the E-step** to **obtain
    a better estimate for the cluster parameters**. There are 3 parameters **{π, μ,
    Σ}** and we can perform this optimization.
  prefs: []
  type: TYPE_NORMAL
- en: The first parameter ***π_k*** represents a categorical distribution and has
    a simple MLE estimate.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d16ed55bf765a13a9b6796ca934a3d2.png)'
  prefs: []
  type: TYPE_IMG
- en: MLE Estimate for π for the GMM
  prefs: []
  type: TYPE_NORMAL
- en: For each point in the dataset, we have the probability of the point belonging
    to cluster *k*. If we take the average across the dataset, this gives us the most
    likely *π* value given the data.
  prefs: []
  type: TYPE_NORMAL
- en: As for μ and Σ, they can be optimized in a similar fashion to gaussian MLE.
    The difference here is that *r_nk* acts as a weighting parameter for the calculation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bfeae7e63c5d0faf3924cef1918e470a.png)'
  prefs: []
  type: TYPE_IMG
- en: MLE Estimate for μ and Σ for the GMM
  prefs: []
  type: TYPE_NORMAL
- en: For the parameter μ, we take a weighted average of the features associated with
    cluster *k*. We repeat the same process for the variance, take the weighted average
    and this gives us the estimate for Σ.
  prefs: []
  type: TYPE_NORMAL
- en: With our improved parameters, we have completed 1 iteration of the EM algorithm.
    We can repeat the same steps until the algorithm converges and there is little
    change in the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Difference from K-Means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While GMM models might be more difficult to implement, they offer 2 unique advantages
    over the more commonly used K-Means algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Elliptical Clusters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: K-Means uses L2 distance which causes the clusters to be spherical in nature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GMM allows for more flexibility. Firstly, different variances (diagonal entries)
    can account for relative differences in the spread of the features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allowing the covariance matrix to have non-diagonal entries can also allow for
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2\. Soft Clustering
  prefs: []
  type: TYPE_NORMAL
- en: As discussed previously, the GMM model allows for soft clustering where each
    datapoint is assigned fractionally to different clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interestingly, the K-Means algorithm can be viewed as a special form of GMM.
    If we restrict the algorithm to only having a unit covariance matrix (Σ = *I),*
    the parameter optimization would be the minimization of the L2 Loss function.
    This is the same as K-Means where the squared distance between the data points
    and the clusters is minimized.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, to convert the soft clustering to hard clustering, we can
    set the cluster to the most probable cluster. By taking the *argmax*, during the
    E-step, each data point is only assigned to one cluster. This is the same as K-Means
    where each iteration has the datapoints reassigned to the nearest cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In a future blog post, I will be going through a “from-scratch” python implementation
    of the EM algorithm and explore some of the characteristics of fitting a GMM model.
  prefs: []
  type: TYPE_NORMAL
