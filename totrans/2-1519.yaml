- en: Mixture Models, Latent Variables and the Expectation Maximization Algorithm
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/mixture-models-latent-variables-and-the-expectation-maximization-algorithm-e5b18e15faa](https://towardsdatascience.com/mixture-models-latent-variables-and-the-expectation-maximization-algorithm-e5b18e15faa)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://reoneo.medium.com/?source=post_page-----e5b18e15faa--------------------------------)[![Reo
    Neo](../Images/a3c192dafc1222b06b2e7fcf4d35cb27.png)](https://reoneo.medium.com/?source=post_page-----e5b18e15faa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e5b18e15faa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e5b18e15faa--------------------------------)
    [Reo Neo](https://reoneo.medium.com/?source=post_page-----e5b18e15faa--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e5b18e15faa--------------------------------)
    ·8 min read·Mar 19, 2023
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eae2fea5e6268b616a5acfcaca0178b9.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
- en: Photo by [Sung Shin](https://unsplash.com/ko/@ironstagram?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning has always been fascinating to me. It is a way to learn
    about data without manual labeling effort and allows for the identification of
    patterns within the dataset. Out of the various unsupervised learning techniques,
    the simplest of which is clustering. In essence, a clustering algorithm aims to
    find data points that are similar to one another. By clustering data points together
    we can derive valuable insights about the dataset and what the various clusters
    represent.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: This article aims to provide an in-depth look at the Gaussian Mixture Model
    clustering algorithm, how it models the data, and more importantly how Expectation-Maximization
    can be used to fit the model on a new dataset.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '**What is a mixture model?**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Essentially, a mixture model (or mixture distribution) is a **combination of
    multiple probability distributions into a single one**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a74bf32579fb4ddda706f9cbdcbde8e5.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: PDF of Mixture Model
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: To combine these distributions together, we assign a **weight** to each component
    distribution such that the total probability under the distribution sums to 1\.
    A simple example would be a mixture distribution that consists of 2 gaussians.
    We can have 2 distributions of different means and variance and combine the 2
    using different weights.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/31a44cd2b833fc89d86614e39ecaf2e1.png)![](../Images/d3812ae7c15398d04fc9df4fd09221db.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: 2 GMMs with the same mean but different π values — Image by author
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Concretely, we can think of this distribution arising from a two-step generative
    process. In this process, a data point can be generated from n different probability
    distributions. First, we determine which probability distribution it is from.
    This probability is simply the weight π_i. Once the component probability distribution
    has been selected, the data point can be generated by simulating the component
    probability distribution itself.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们可以将这种分布视为由一个两步生成过程产生。在这个过程中，一个数据点可以从 n 个不同的概率分布中生成。首先，我们确定它来自哪个概率分布。这个概率就是权重
    π_i。一旦选择了组件概率分布，数据点可以通过模拟组件概率分布本身来生成。
- en: '**GMM**'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**GMM**'
- en: A gaussian mixture model is essentially a mixture model in which **all the component
    distributions are gaussian distributions.**
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯混合模型本质上是一个混合模型，其中 **所有组件分布都是高斯分布。**
- en: '![](../Images/25c24b86e8715bae51890cf334bdd409.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25c24b86e8715bae51890cf334bdd409.png)'
- en: PDF of GMM Model
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: GMM 模型的概率密度函数
- en: Now let’s try to understand why the gaussian distribution is used to model the
    components of the mixture. When looking at a dataset, we want to cluster similar
    points together. These clusters will usually be spherical or elliptical in nature
    since we want points that are close together to be clustered.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试理解为什么使用高斯分布来建模混合组件。当查看数据集时，我们希望将相似的点聚集在一起。这些聚类通常是球形或椭圆形的，因为我们希望相近的点被聚集在一起。
- en: As such, a normal distribution is a good model for the cluster. The mean of
    the distribution will be the **center of the cluster** while the **shape** and
    **spread** of the cluster can be well modeled by the **covariance of the distribution**.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正态分布是聚类的一个良好模型。分布的均值将是 **聚类的中心**，而 **形状** 和 **分布** 可以通过 **分布的协方差** 来很好地建模。
- en: The second variable for the cluster will be the **relative sizes of different
    clusters**. In an organic dataset, we don’t usually expect the clusters to be
    of the same size and this means that some clusters will have more points than
    others. The size of the clustering will then be determined by the cluster weights
    π_i.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的第二个变量将是 **不同聚类的相对大小**。在一个真实数据集中，我们通常不期望聚类的大小相同，这意味着一些聚类将比其他聚类包含更多的点。聚类的大小将由聚类权重
    π_i 决定。
- en: In the context of clustering, we assume that there are *k* influencing factors
    that affect the generation of the data. Each influencing factor has a **different
    weight** which corresponds to the cluster weight π.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类的背景下，我们假设有 *k* 个影响因素影响数据的生成。每个影响因素有一个 **不同的权重**，对应于聚类权重 π。
- en: '**Defining the Terms**'
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**定义术语**'
- en: Before going thru the model derivation, let’s first define a few key terms.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入模型推导之前，我们首先定义一些关键术语。
- en: '![](../Images/1d1d0584c23cf379e660fa695b1bf071.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d1d0584c23cf379e660fa695b1bf071.png)'
- en: Mathematical Function for Cluster Responsibilities
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类责任的数学函数
- en: 'θ: This refers to the parameters of the GMM model which are the means and covariances
    of the clusters θ = {μ, Σ,π}. There are *k* values of θ since each cluster has
    a corresponding parameter.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: θ：这是指 GMM 模型的参数，包括聚类的均值和协方差 θ = {μ, Σ,π}。因为每个聚类都有相应的参数，所以有 *k* 个 θ 值。
- en: '*y_n* — This refers to the n-th datapoint within the dataset'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y_n* — 这是指数据集中的第 n 个数据点'
- en: '*z_n* — This refers to the cluster that the nth datapoint is from'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*z_n* — 这是指第 n 个数据点所属的聚类'
- en: '*r_nk* — Cluster responsibilities (explained in next section)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*r_nk* — 聚类责任（在下一节中解释）'
- en: '**Fitting the GMM model**'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**拟合 GMM 模型**'
- en: Now that we have our model, we want to use it to gain insights into the data.
    For a clustering problem, this would be the predicted cluster assignments ***z_n^***.
    Instead of solving for a definite cluster assignment, the GMM model allows us
    to calculate the cluster probabilities.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经有了模型，我们希望利用它来洞察数据。对于一个聚类问题，这将是预测的聚类分配 ***z_n^***。与其求解确定的聚类分配，GMM 模型允许我们计算聚类概率。
- en: 'This value ***r_nk*** is the influence of cluster *k* on the nth data point
    and has the name cluster responsibility. This approach is known as soft clustering
    where each data point is **fractionally assigned** to each cluster instead of
    the datapoint being **assigned to a single cluster outright**. Being a probability
    distribution, the vector ***r_nk*** will sum to 1 for all points. Mathematically,
    this value is defined as:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2da31cc2dee72f76ca49ca59b5fc6314.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: Cluster responsibility definition
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: From the definition, we can see that this is a posterior probability of z_n
    given the data y_n and parameters θ.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of this approach allows for **identifying data points that the model
    is more unsure about**. Alternatively, if we want to find the most probable cluster,
    we can convert our results back to **hard clustering** by assigning each datapoint
    to the cluster with the highest responsibility.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b5087a6590f16e0305846428a919fd82.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: Argmax function on cluster responsibility for hard clustering
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Bayes Rule, we can breakdown the posterior even further:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1d1d0584c23cf379e660fa695b1bf071.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: Expansion of cluster responsibilities using Bayes Rule
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: In the numerator, the posterior is broken down into the prior ***p(z_n=k|θ)***
    and the likelihood ***p(y_n|z_n=k, θ)***. The denominator is then the normalizing
    constant which takes into account **all possible clusters**.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Here we also encounter an issue with solving for the posterior. We are unable
    to solve for the responsibilities without first knowing the parameter estimates
    θ. At the same time, without knowing the cluster responsibilities, there is no
    way to derive good estimates for the responsibilities. Thus, we use the **Expecation-Maximization
    algorithm** which is used to solve latent variable models.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: EM Algorithm
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A latent variable is simply a variable that is **never explicitly observed**
    in the dataset. In this case, the latent variable is the cluster assignments ***z_n***.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'This algorithm is an **iterative 2-step algorithm**. It starts with randomly
    initialized values for the parameter *θ* and then iteratively refines estimates
    for the latent variable *z_n* and the parameters *θ* on each iteration. For each
    iteration, there are 2 steps:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '**E-Step**: In the E-step we try to estimate the probability distribution of
    the latents *z_n* **given the current estimates for *θ***. This is equivalent
    to constructing an **expectation of the conditional log-likelihood given Z and
    *θ***'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/b5a0cdcfe0f49ad03cbf201b8e545880.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: Expected conditional log-likelihood given the current parameters
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '**M-Step:** In the M-step, **we generate a new set of estimates for *θ* given
    the current expected Z** (from the E-step). This is done by maximizing the term
    in the M-step w.r.t the parameters *θ*'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the above steps sound really confusing, don’t worry! Let’s look at this example
    in the context of GMM.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '**E-Step for GMM**'
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/1d1d0584c23cf379e660fa695b1bf071.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: Let’s try to solve the equation for cluster responsibilities that we derived
    earlier.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试求解之前推导的集群责任方程。
- en: 'For the numerator, we have the first term ***p(z_n=k|θ)*** which is the probability
    that the data point belongs to cluster *k*. This term does not consider the likelihood
    and thus will just be *π_k*. The second term ***p(y_n|z_n=k,θ)*** is the likelihood
    **given that it belongs to cluster k** which is the gaussian likelihood. Essentially,
    this probability becomes:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分子，我们有第一个项***p(z_n=k|θ)***，这是数据点属于集群*k*的概率。这个项不考虑似然，因此将只是*π_k*。第二个项***p(y_n|z_n=k,θ)***是**在属于集群k的情况下的似然**，这是高斯似然。实质上，这个概率变为：
- en: '![](../Images/052b0e47abf7e506b86bb968bbbf9845.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/052b0e47abf7e506b86bb968bbbf9845.png)'
- en: Simplified derivation of Cluster Responsibilities
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 简化集群责任的推导
- en: For each point in the dataset, we calculate ***r_n*** a vector of size *k* that
    represents the probability of the point belonging to each cluster. Since we have
    our randomly initialized values of μ and Σ this is solvable. Once, the E-step
    is completed we can move to the M-step.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据集中的每个点，我们计算***r_n***，一个大小为*k*的向量，表示点属于每个集群的概率。由于我们有随机初始化的μ和Σ，这个问题是可以解决的。一旦E步完成，我们可以进入M步。
- en: '**M-Step for GMM**'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**GMM的M-Step**'
- en: In the M-step, we use the **cluster responsibilities in the E-step** to **obtain
    a better estimate for the cluster parameters**. There are 3 parameters **{π, μ,
    Σ}** and we can perform this optimization.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在M步中，我们使用**E步中的集群责任**来**获得集群参数的更好估计**。有3个参数**{π, μ, Σ}**，我们可以进行这种优化。
- en: The first parameter ***π_k*** represents a categorical distribution and has
    a simple MLE estimate.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数***π_k***代表一个分类分布，并具有简单的MLE估计。
- en: '![](../Images/4d16ed55bf765a13a9b6796ca934a3d2.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d16ed55bf765a13a9b6796ca934a3d2.png)'
- en: MLE Estimate for π for the GMM
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: GMM的π的MLE估计
- en: For each point in the dataset, we have the probability of the point belonging
    to cluster *k*. If we take the average across the dataset, this gives us the most
    likely *π* value given the data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据集中的每个点，我们有点属于集群*k*的概率。如果我们在数据集中取平均，这将给我们提供给定数据的最可能的*π*值。
- en: As for μ and Σ, they can be optimized in a similar fashion to gaussian MLE.
    The difference here is that *r_nk* acts as a weighting parameter for the calculation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于μ和Σ，它们可以以类似于高斯MLE的方式进行优化。不同之处在于*r_nk*作为计算的加权参数。
- en: '![](../Images/bfeae7e63c5d0faf3924cef1918e470a.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfeae7e63c5d0faf3924cef1918e470a.png)'
- en: MLE Estimate for μ and Σ for the GMM
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: GMM的μ和Σ的MLE估计
- en: For the parameter μ, we take a weighted average of the features associated with
    cluster *k*. We repeat the same process for the variance, take the weighted average
    and this gives us the estimate for Σ.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于参数μ，我们对与集群*k*相关联的特征取加权平均。我们对方差执行相同的过程，取加权平均，这给我们提供了Σ的估计。
- en: With our improved parameters, we have completed 1 iteration of the EM algorithm.
    We can repeat the same steps until the algorithm converges and there is little
    change in the parameters.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们改进的参数，我们完成了EM算法的1次迭代。我们可以重复相同的步骤，直到算法收敛，参数变化很小。
- en: Difference from K-Means
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与K-Means的区别
- en: While GMM models might be more difficult to implement, they offer 2 unique advantages
    over the more commonly used K-Means algorithm.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GMM模型可能更难实现，但它们比更常用的K-Means算法具有两个独特的优势。
- en: Elliptical Clusters
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 椭圆形集群
- en: K-Means uses L2 distance which causes the clusters to be spherical in nature
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-Means使用L2距离，这导致集群在本质上是球形的。
- en: GMM allows for more flexibility. Firstly, different variances (diagonal entries)
    can account for relative differences in the spread of the features.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GMM允许更多的灵活性。首先，不同的方差（对角线条目）可以解释特征扩展的相对差异。
- en: Allowing the covariance matrix to have non-diagonal entries can also allow for
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许协方差矩阵具有非对角条目也可以允许
- en: 2\. Soft Clustering
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 软聚类
- en: As discussed previously, the GMM model allows for soft clustering where each
    datapoint is assigned fractionally to different clusters
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前所述，GMM模型允许软聚类，其中每个数据点以部分的方式分配到不同的集群。
- en: Interestingly, the K-Means algorithm can be viewed as a special form of GMM.
    If we restrict the algorithm to only having a unit covariance matrix (Σ = *I),*
    the parameter optimization would be the minimization of the L2 Loss function.
    This is the same as K-Means where the squared distance between the data points
    and the clusters is minimized.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，K-Means 算法可以看作是 GMM 的一种特殊形式。如果我们将算法限制为仅具有单位协方差矩阵（Σ = *I*），则参数优化将是最小化 L2
    损失函数。这与 K-Means 相同，其中最小化数据点与簇之间的平方距离。
- en: At the same time, to convert the soft clustering to hard clustering, we can
    set the cluster to the most probable cluster. By taking the *argmax*, during the
    E-step, each data point is only assigned to one cluster. This is the same as K-Means
    where each iteration has the datapoints reassigned to the nearest cluster.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，为了将软聚类转换为硬聚类，我们可以将簇设置为最可能的簇。通过取*argmax*，在 E 步骤中，每个数据点只被分配到一个簇。这与 K-Means
    相同，其中每次迭代都将数据点重新分配到最近的簇。
- en: In a future blog post, I will be going through a “from-scratch” python implementation
    of the EM algorithm and explore some of the characteristics of fitting a GMM model.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的博客文章中，我将详细讲解一个“从头开始”的 Python 实现 EM 算法，并探索拟合 GMM 模型的一些特征。
