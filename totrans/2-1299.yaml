- en: 'Illuminating Insights: GPT Extracts Meaning from Charts and Tables'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/illuminating-insights-gpt-extracts-meaning-from-charts-and-tables-a0b71c991d34](https://towardsdatascience.com/illuminating-insights-gpt-extracts-meaning-from-charts-and-tables-a0b71c991d34)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Using GPT Vision to interpret and aggregate image data.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ilia.teimouri?source=post_page-----a0b71c991d34--------------------------------)[![Ilia
    Teimouri PhD](../Images/0eb948c4d3f81c116cd16fa4d5016629.png)](https://medium.com/@ilia.teimouri?source=post_page-----a0b71c991d34--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a0b71c991d34--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a0b71c991d34--------------------------------)
    [Ilia Teimouri PhD](https://medium.com/@ilia.teimouri?source=post_page-----a0b71c991d34--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a0b71c991d34--------------------------------)
    ·7 min read·Dec 24, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6f4c9df91feb7c242dca20cf31c52358.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [David Travis](https://unsplash.com/@dtravisphd) on [Unsplash](https://unsplash.com).
  prefs: []
  type: TYPE_NORMAL
- en: Integrating visual inputs like images alongside text and speech into large language
    models (LLMs) is considered an important new direction in AI research by many
    experts in the field. By augmenting these models to handle multiple modes of data
    beyond just language, there is potential to significantly broaden the scope of
    applications they can be utilised for as well as enhance their overall intelligence
    and performance on existing NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The promise of multimodal AI spans from more engaging user experiences like
    conversational agents that can see their surroundings and refer to objects around
    them, to robots that can fluidly translate commands into physical actions using
    combined knowledge of language and vision. By uniting historically separate areas
    of AI around a unified model architecture, multimodality may accelerate progress
    in tasks relying on multiple skills like visual question answering or image captioning.
    The synergies between learning algorithms, data types, and model designs across
    fields could lead to rapid advancement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many companies have already embraced multimodality in various forms: [OpenAI](https://chat.openai.com),
    [Anthropic](http://claude.ai), Google ([Bard](https://bard.google.com) and [Gemini](https://deepmind.google/technologies/gemini/#introduction))
    allow you to upload your own image or text data and chat with them.'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I hope to demonstrate a straightforward yet powerful application
    of large language models with computer vision in finance. Equity researchers and
    investment banking analysts may find this especially useful, as you likely spend
    considerable time reading reports and statements containing various tables and
    graphs. Reading lengthly tables and graphs and interpreting them correctly requires
    a great amount of time, knowledge in the field as well as adequate focus to avoid
    mistakes. More tediously, analysts occasionally need to manually enter tabular
    data from PDFs simply to create new charts. An automated solution could alleviate
    these pains by extracting and interpreting key information without the capacity
    for human oversight or fatigue.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, by combining NLP with computer vision, we can create an assistant to
    handle many repetitive analytical tasks, freeing analysts to focus on higher-level
    strategy and decision making.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, there were a lot of advances in using [Optical Character Recognition](https://en.wikipedia.org/wiki/Optical_character_recognition)
    or Visual Document Understanding ([image to text](https://huggingface.co/models?pipeline_tag=image-to-text))
    to extract text from image / PDF data. However, due to the nature of currently
    available training data, the existing methods still struggle with complex layouts
    and formatting found in many financial statements, research reports, and regulatory
    filings.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4V(ision) for tables and graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Back in Sept 2023, OpenAI released [GPT-4 Vision](https://openai.com/research/gpt-4v-system-card).
    According to OpenAI:'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 with vision (GPT-4V) enables users to instruct GPT-4 to analyze image
    inputs provided by the user.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: GPT-4V’s visual skills come from GPT-4, so both models were trained in a similar
    way. First, researchers fed the system huge amounts of text to teach it the fundamentals
    of language. Its goal was to predict the next word in a document. Then came the
    finesse training using an approach called reinforcement learning from human feedback,
    or RLHF for short. This involves fine-tuning the model further based on positive
    reactions from human trainers to produce outputs we find truly helpful.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I’m going to make a Steamlit application where the user can
    upload an image and ask various questions about the image. The images that I am
    going to use are screenshots of a financial PDF document. In fact, the document
    is a publicly available [Fund Fact Sheet](https://www.ubs.com/2/e/files/RET/FS_RET_LU2408467723_GB_EN.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main parts to this code, first is a function to encode the image
    from a given file path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'you need this function as the model expect your input image to be in base 64
    encoded format. The next main part of the code would be the way you send your
    request to the OpenAI’s API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: where we set the model name to be `gpt-4-vision-preview`. As you can see this
    is quite different than the usual text to text OpenAI’s API calls. In this case,
    we define a json object called **payload** that contains your text, as well as
    your image data.
  prefs: []
  type: TYPE_NORMAL
- en: You can expand the `get_image_analysis` method to send multiple images, or control
    how the model should process the images via `detail` parameter. See more [here](https://platform.openai.com/docs/guides/vision).
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the code is mainly the Streamlit method, where we allow users to
    upload their image and interact with the image by asking questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Full code: (also available on [Github](https://github.com/iteimouri/GPT-Vision-for-Finance/tree/main))'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Output and summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let us look at a few examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29792265352740d8b2e97c75cf4f081d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image is generated by the author. The graph is from the publicly available [UBS
    Fund Fact Sheet](https://www.ubs.com/2/e/files/RET/FS_RET_LU2408467723_GB_EN.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: In this example the question is asked about the peak of the performance. We
    can see that the peak is identified correctly by the model. The model could also
    understand that the dotted line is the index performance from the plot’s legend.
    Understanding dashed and dotted lines are generally difficult in computer vision
    but **given a good quality screenshot** (with enough details), GPT Vision could
    pass the task easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69cdb2a7eb6d28965195d9f43637b0da.png)'
  prefs: []
  type: TYPE_IMG
- en: Image is generated by the author. The material is from the publicly available
    [UBS Fund Fact Sheet](https://www.ubs.com/2/e/files/RET/FS_RET_LU2408467723_GB_EN.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, I have tried to examine how good is the model in: 1) extracting
    the relevant table among other data 2) extracting relevant part of the table 3)
    some basic mathematical operations.'
  prefs: []
  type: TYPE_NORMAL
- en: As demonstrated, the model successfully fulfilled all three requirements for
    this task — no small feat given the complexity involved traditionally. Manually,
    an analyst would have struggled extracting the two-column table locked within
    a PDF, even using optical character recognition (OCR) tools. Additional coding
    would be needed to parse figures into a structured dataframe amenable to aggregation.
    This could consume substantial time before answering the original question. However
    here, one achieves the desired result with only a prompt. Avoiding the cumbersome
    workflow of deciphering images, scraping data, wrangling spreadsheets, and writing
    scripts unlocks tremendous efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/edf565aa98095582b29a80e8090fadd2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image is generated by the author. The material is from the publicly available
    [UBS Fund Fact Sheet](https://www.ubs.com/2/e/files/RET/FS_RET_LU2408467723_GB_EN.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Sorting algorithms systematically reorder the elements of a list or array based
    on a specified ordering rule. However, unlike traditional code, LLMs like GPT
    do not have predefined sorting routines hardcoded within.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, GPT is trained to predict the next word in a sequence given prior context.
    With sufficient data and model capacity, the ability to sort emerges from learning
    textual patterns.
  prefs: []
  type: TYPE_NORMAL
- en: The example above demonstrates this — GPT correctly sorts two columns in a table
    extracted from a PDF screenshot, a non-trivial feat requiring optical character
    recognition, data extraction, and manipulation skills. Even in Excel, multi-column
    sorting requires some expertise. But by simply providing the goal in a prompt,
    GPT handles these complex steps automatically behind the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than following rigid, step-by-step instructions like traditional algorithms,
    LLMs like GPT develop sorting capabilities through recognizing relationships in
    text during training. This allows them to absorb a variety of abilities from their
    diverse exposure, as opposed to being limited by predefined programming.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why is this a big deal?**'
  prefs: []
  type: TYPE_NORMAL
- en: By harnessing this flexibility into specialised tasks like we see here, prompts
    can unlock efficient problem solving that would otherwise demand significant manual
    effort and technical knowledge.
  prefs: []
  type: TYPE_NORMAL
