- en: Build Deployable Machine Learning Pipelines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/build-deployable-machine-learning-pipelines-a6d7035816a6](https://towardsdatascience.com/build-deployable-machine-learning-pipelines-a6d7035816a6)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Leverage Kedro to build production-ready machine learning pipelines
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://johnadeojo.medium.com/?source=post_page-----a6d7035816a6--------------------------------)[![John
    Adeojo](../Images/f6460fae462b055d36dce16fefcd142c.png)](https://johnadeojo.medium.com/?source=post_page-----a6d7035816a6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a6d7035816a6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a6d7035816a6--------------------------------)
    [John Adeojo](https://johnadeojo.medium.com/?source=post_page-----a6d7035816a6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a6d7035816a6--------------------------------)
    ·8 min read·Jun 30, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec434ef42ac9cdfcc488fd5ed7688ab9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Generated with Midjourney'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Background — Notebooks are not “Deployable”
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many data scientists’ initial encounters with coding take place through notebook-style
    user interfaces. Notebooks are indispensable for exploration — a critical aspect
    of our workflow. However, they’re not designed to be production-ready. This is
    a key issue I’ve observed among numerous clients, some of whom inquire about ways
    to productionise their notebooks. Rather than productionising your notebooks,
    the optimal route to production readiness is to craft modular, maintainable, and
    reproducible code.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, I present an example of a modular ML pipeline for training
    a model to classify fraudulent credit card transactions. By the conclusion of
    this article, I hope that you will:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Gain an appreciation and understanding of modular ML pipelines.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feel inspired to build one for yourself.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you want to reap the benefits of deploying your machine learning models for
    maximum effect, writing modular code is an important step to take.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: First a quick definition of [modular](https://en.wikipedia.org/wiki/Modular_programming)
    code. Modular code is software design paradigm that emphasizes separating a program
    out into independent, interchangeable modules. We should aim to approach this
    state with our machine learning pipelines.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Quick Detour — The project, the Data, and Approach
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The machine learning project is sourced from [Kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud).
    The dataset consists of 284,807 anonymised credit card transactions for which
    492 are fraudulent. The task is to build a classifier to detect the fraudulent
    transactions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: The Data for this project is licensed for any purpose including commercial use
    under the [Open Data Commons](https://opendatacommons.org/licenses/dbcl/1-0/).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: I have used a deep learning approach leveraging the [Ludwig](https://ludwig.ai/latest/)
    , an open-source framework for declarative deep learning. I won’t go into the
    details of Ludwig here, however I have previously written an article on the [framework](https://medium.com/towards-data-science/ludwig-a-friendlier-deep-learning-framework-946ee3d3b24).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: The Ludwig deep neural network is configured with a **.yaml** file. For those
    that are curious you can find this in the [model registry GitHub](https://github.com/john-adeojo/Credit-Card-Fraud-Model-Registry/blob/main/model%20yaml%20files/model_1a.yaml).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Building Modular Pipelines with Kedro
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building modular machine learning pipelines has been made easier with open-source
    tools, my favourite of these is [Kedro](https://kedro.org/). Not only because
    I have seen this used successfully in industry, but also because it helped me
    develop my software engineering skills.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Kedro is an open-source framework (licensed under Apache 2.0) for creating reproducible,
    maintainable and modular data science code. I came across it while I was developing
    the AI strategy for a bank, considering which tools my team could utilise to build
    production-ready code.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '*Disclaimer: I have no affiliation with Kedro or McKinsey’s QuantumBlack, the
    creators of this open-source tool.*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: The Model Training Pipeline
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/521cb43a198ee7b9415263cf30f40f77.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: End-to-end model training pipeline generated with Kedro viz'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Kedro conveniently allows you to visualise your pipelines, a fantastic feature
    that can help bring clarity to your code. The pipeline is standard for machine
    learning so I will only briefly touch on each aspect.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '**Import Data**: Import the credit card transaction data from an external source.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Split Data**: Use random split to split the data into training and holdout
    sets.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Run Experiment**: Uses the Ludwig framework to train a deep neural network
    on the train data set. The Ludwig experiment API conveniently saves model artefacts
    for every experiment run.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Run Predictions**: Uses the model trained in the previous step to run predictions
    on the holdout dataset.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model Diagnostics**: Produces two diagnostic charts. First, the tracking
    the cross-entropy loss over each epoch. Second, the [ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)
    measuring the model’s performance on the holdout dataset.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/5b057563d48b3bc1b69a199f33954e1d.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Loss Curve from model training process'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9023e2e090e82f08b52fe22fbc91d82.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: ROC Curve from model evaluation on the holdout dataset'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Core Components of the Pipeline
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have established a high-level view, let’s get into some of the core
    components of this pipeline.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Project Structure
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Kedro provides a templated directory structure that is established when you
    initiate a project. From this base, you can programmatically add more pipelines
    to your directory structure. This standardised structure ensures that every machine
    learning project is identical and easy to document, thereby facilitating ease
    of maintenance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Kedro 提供了一个模板化的目录结构，这个结构在你启动项目时就已经建立。从这个基础上，你可以以编程方式将更多的管道添加到你的目录结构中。这种标准化的结构确保了每个机器学习项目的一致性和易于文档化，从而便于维护。
- en: Data Management
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据管理
- en: Data plays a crucial role in machine learning. The ability to track your data
    becomes even more essential when employing machine learning models in a commercial
    setting. You often find yourself facing audits, or the necessity to productionise
    or reproduce your pipeline on someone else’s machine.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 数据在机器学习中扮演着至关重要的角色。当在商业环境中使用机器学习模型时，跟踪数据的能力变得尤为重要。你经常会面临审计，或者需要在他人的机器上生产化或重现你的管道。
- en: Kedro offers two methods for enforcing best practices in data management. The
    first is a directory structure, designed for machine learning workloads, providing
    distinct locations for the intermediate tables produced during data transformation
    and the model artefacts. The second method is the [data catalogue](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/conf/base/catalog.yml).
    As part of the Kedro workflow, you are required to register datasets within a
    .yaml configuration file, thereby enabling you to leverage these datasets in your
    pipelines. This approach may initially seem unusual, but it allows you and others
    working on your pipeline to track data with ease.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Kedro 提供了两种方法来强制执行数据管理的最佳实践。第一种是目录结构，专为机器学习工作负载设计，为数据转换过程中生成的中间表和模型工件提供了明确的位置。第二种方法是[data
    catalogue](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/conf/base/catalog.yml)。作为
    Kedro 工作流的一部分，你需要在 .yaml 配置文件中注册数据集，从而在管道中利用这些数据集。这种方法初看可能不寻常，但它使你和其他参与管道工作的人员能够轻松跟踪数据。
- en: Orchestration — Nodes and Pipelines
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调度 — 节点和管道
- en: This is really where the magic happens. Kedro provides you with pipeline functionality
    straight out of the box.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是魔法发生的地方。Kedro 提供了开箱即用的管道功能。
- en: The initial building block of your pipeline is the [nodes](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/src/fraud_detection_model/pipelines/train_model/nodes.py).
    Each executable piece of code can be encapsulated within a Node, which is simply
    a Python function that accepts an input and yields an output. You can then structure
    a [pipeline](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/src/fraud_detection_model/pipelines/train_model/pipeline.py)
    as a series of nodes. Pipelines are easily constructed by invoking the node and
    specifying the inputs and outputs. Kedro determines the execution order.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 管道的初始构建块是[nodes](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/src/fraud_detection_model/pipelines/train_model/nodes.py)。每个可执行的代码片段可以封装在一个节点中，节点只是一个接受输入并产生输出的
    Python 函数。然后，你可以将一个[管道](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/src/fraud_detection_model/pipelines/train_model/pipeline.py)结构化为一系列节点。通过调用节点并指定输入和输出，可以轻松构建管道。Kedro
    会确定执行顺序。
- en: Once pipelines are constructed, they are registered in the provided **pipeline_registry.py**
    file. The beauty of this approach is that you can create multiple pipelines. This
    is particularly beneficial in machine learning, where you might have a data processing
    pipeline, a model training pipeline, an inference pipeline, and so forth.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦管道构建完成，它们会被注册在提供的**pipeline_registry.py**文件中。这种方法的美妙之处在于，你可以创建多个管道。这在机器学习中尤其有用，你可能会有一个数据处理管道、一个模型训练管道、一个推理管道等。
- en: Once set up, it’s straightforward enough to modify aspects of your pipeline.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置完成，修改管道的各个方面就会变得相当简单。
- en: Code snippet showing example of nodes.py script
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 显示 nodes.py 脚本示例的代码片段
- en: Code snippet showing example of Pipeline script
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 显示 Pipeline 脚本示例的代码片段
- en: Configuration
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置
- en: Kedro’s best practices stipulate that all configurations should be handled through
    the provided [**parameters.yml**](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/conf/base/parameters.yml)
    file. From a machine learning perspective, hyperparameters fall into this category.
    This approach streamlines experimentation, as you can simply substitute one **parameters.yml**
    file with a set of hyperparameters for another, which is also much easier to track.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Kedro 的最佳实践规定所有配置应通过提供的 [**parameters.yml**](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/conf/base/parameters.yml)
    文件来处理。从机器学习的角度来看，超参数也属于这一类别。这种方法简化了实验过程，因为你可以简单地用另一组超参数替换一个 **parameters.yml**
    文件，这也更容易追踪。
- en: I have also included the locations of my Ludwig deep neural network [**model.yaml**](https://github.com/john-adeojo/Credit-Card-Fraud-Model-Registry/blob/main/model%20yaml%20files/model_1a.yaml)
    and the data source within the **parameters.yml** configuration. Should the model
    or the location of the data change — for instance, when moving between developers’
    machines — it would be incredibly straightforward to adjust these settings.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我还在 **parameters.yml** 配置中包含了我的 Ludwig 深度神经网络 [**model.yaml**](https://github.com/john-adeojo/Credit-Card-Fraud-Model-Registry/blob/main/model%20yaml%20files/model_1a.yaml)
    和数据源的位置。如果模型或数据位置发生变化——例如，在开发者之间转移时——调整这些设置会非常简单。
- en: Code snippet showing the contents of the parameters.yml file
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 显示`parameters.yml`文件内容的代码片段
- en: Reproducibility
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可复现性
- en: Kedro includes a [**requirements.txt**](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/src/requirements.txt)
    file as part of the templated structure. This makes it really straightforward
    to monitor your environment and exact library versions. However, should you prefer,
    you can employ other environment management methods, such as an **environment.yml**
    file.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Kedro 包含一个 [**requirements.txt**](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/src/requirements.txt)
    文件作为模板结构的一部分。这使得监控你的环境和确切的库版本变得非常简单。然而，如果你愿意，你可以使用其他环境管理方法，如 **environment.yml**
    文件。
- en: Establishing a Workflow
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建立工作流
- en: 'If you’re developing machine learning pipelines and considering using Kedro,
    it can initially present a steep learning curve, but adopting a standard workflow
    will simplify the process. Here’s my suggested workflow:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在开发机器学习管道并考虑使用 Kedro，起初可能会有陡峭的学习曲线，但采用标准工作流会简化这一过程。以下是我建议的工作流：
- en: '**Establish your working environment**: I prefer using Anaconda for this task.
    I typically use an [**environment.yml**](https://github.com/john-adeojo/kedro-project/blob/main/environment.yml)
    file, containing all the dependencies needed for my environment, and employ the
    Anaconda Powershell command line to [create](https://gist.github.com/john-adeojo/fa22082328e65d0260fefd9b149e4b69)
    my environment from this.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**建立你的工作环境**：我更喜欢使用 Anaconda 来完成这项任务。我通常使用一个 [**environment.yml**](https://github.com/john-adeojo/kedro-project/blob/main/environment.yml)
    文件，其中包含我环境所需的所有依赖项，并使用 Anaconda Powershell 命令行来 [创建](https://gist.github.com/john-adeojo/fa22082328e65d0260fefd9b149e4b69)
    我的环境。'
- en: '**Create a Kedro project**: Once you have Kedro installed — which should hopefully
    be declared in your **environment.yml** — you can [create](https://docs.kedro.org/en/stable/get_started/new_project.html)
    a Kedro project through the [Anaconda](https://www.anaconda.com/) command line
    interface.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建 Kedro 项目**：一旦你安装了 Kedro——希望它已在你的 **environment.yml** 中声明——你可以通过 [Anaconda](https://www.anaconda.com/)
    命令行界面 [创建](https://docs.kedro.org/en/stable/get_started/new_project.html) 一个 Kedro
    项目。'
- en: '**Explore in Jupyter Notebooks**: I construct an initial pipeline in Jupyter
    notebooks, a process familiar to most data scientists. The only difference is
    that, once your pipeline is built, you should tidy it up so that each cell could
    serve as one Node in your Kedro pipeline.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在 Jupyter Notebooks 中探索**：我在 Jupyter notebooks 中构建初始管道，这个过程对大多数数据科学家来说都很熟悉。唯一的不同之处在于，一旦你的管道构建完成，你应该整理它，以便每个单元格可以作为你
    Kedro 管道中的一个节点。'
- en: '**Register your data**: Record the input and outputs for each data processing
    or data ingestion step in the data [catalogue](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/conf/base/catalog.yml).'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**注册你的数据**：在数据 [catalogue](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/conf/base/catalog.yml)
    中记录每个数据处理或数据摄取步骤的输入和输出。'
- en: '**Add your pipeline**: After conducting your exploration in notebooks, you’ll
    want to [create a pipeline](https://docs.kedro.org/en/stable/tutorial/create_a_pipeline.html#:~:text=Note,the%20starter%20project.).
    This is achieved through the command line interface. Running this command will
    add an additional folder to ‘pipelines’, bearing the name of the pipeline you’ve
    just created. It’s within this folder that you’ll construct your nodes and pipelines.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Define your pipeline**: This is the stage where you start transferring the
    code from your Jupyter notebooks into the [**node.py**](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/src/fraud_detection_model/pipelines/train_model/nodes.py)
    file in your pipeline folder, ensuring that nodes you intend to be part of a pipeline
    have inputs and outputs. Once the nodes are set up, proceed to define your pipeline
    in the [**pipeline.py**](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/src/fraud_detection_model/pipelines/train_model/pipeline.py)
    file.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Register your pipelines**: The [**pipeline_registry.py**](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/src/fraud_detection_model/pipeline_registry.py)
    file offers a template for you to register your newly created pipeline.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Run your project**: Once established, you can [run](https://docs.kedro.org/en/stable/nodes_and_pipelines/run_a_pipeline.html)
    any pipeline through the CLI and also [visualise](https://docs.kedro.org/en/stable/visualisation/index.html)
    your project.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Production-ready pipelines fit into a wider ecosystem of machine learning operations.
    Read my article on MLOps for a deeper dive.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[](/building-machine-learning-operations-for-businesses-6d0bfbbf2139?source=post_page-----a6d7035816a6--------------------------------)
    [## Building Machine Learning Operations for Businesses'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: A Blueprint for Effective MLOps to Support Your AI Strategy
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/building-machine-learning-operations-for-businesses-6d0bfbbf2139?source=post_page-----a6d7035816a6--------------------------------)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kedro is an excellent framework for delivering production-ready machine learning
    pipelines. Beyond the functionality discussed in this article, there are numerous
    integrations with other open-source libraries, as well as packages for documentation
    and testing. Kedro doesn’t solve every issue related to model deployment — for
    instance, model versioning is likely better handled by another tool such as DVC.
    However, it will assist data scientists in a commercial setting to produce more
    maintainable, modular, and reproducible code that is ready for production. There
    is a relatively steep learning curve for complete novices, but the documentation
    is clear and includes guided tutorials. As with any of these packages, the best
    way to learn is to dive in and experiment.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Link to the full [GitHub repo](https://github.com/john-adeojo/kedro-project/tree/main)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '*Follow me on* [*LinkedIn*](https://www.linkedin.com/in/john-adeojo/)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '*Subscribe to medium to get more insights from me:*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://johnadeojo.medium.com/membership?source=post_page-----a6d7035816a6--------------------------------)
    [## Join Medium with my referral link — John Adeojo'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: I share data science projects, experiences, and expertise to assist you on your
    journey You can sign up to medium via…
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: johnadeojo.medium.com](https://johnadeojo.medium.com/membership?source=post_page-----a6d7035816a6--------------------------------)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '*Should you be interested in integrating AI or data science into your business
    operations, we invite you to schedule a complimentary initial consultation with
    us:*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.data-centric-solutions.com/book-online?source=post_page-----a6d7035816a6--------------------------------)
    [## Book Online | Data-Centric Solutions'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Discover our expertise in helping businesses achieve ambitious goals with a
    free consultation. Our data scientists and…
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.data-centric-solutions.com](https://www.data-centric-solutions.com/book-online?source=post_page-----a6d7035816a6--------------------------------)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
