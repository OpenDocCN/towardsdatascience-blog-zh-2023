- en: What are Multimodal models?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/what-are-multimodal-models-fe118f3ef963](https://towardsdatascience.com/what-are-multimodal-models-fe118f3ef963)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Give LLMs the ability to see!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)[![Omer
    Mahmood](../Images/0c87da4134bea397c77bc4ba6640e34b.png)](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)
    [Omer Mahmood](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)
    ¬∑6 min read¬∑Oct 16, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b7fb0fc89f35adbb6b9d33055d2f0d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of [Mecari text & image embeddings demo](https://atlas.nomic.ai/map/vertex-mercari)
    running on Atlas by Nomic.
  prefs: []
  type: TYPE_NORMAL
- en: Who is this post for?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Reader Audience [üü¢‚ö™Ô∏è‚ö™Ô∏è]:** AI beginners, familiar with popular concepts,
    models and their applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Level [üü¢üü¢Ô∏è‚ö™Ô∏è]:** Intermediate topic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complexity [üü¢‚ö™Ô∏è‚ö™Ô∏è]:** Easy to digest, no mathematical formulas or complex
    theory here'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‚ùìWhy It Matters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Foundational large language models (LLMs), pre-trained on huge datasets are
    pretty efficient at handling generic, multi-tasking via prompts through zero-shot,
    few-shot or transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, examples of these models like [PaLM2](https://ai.google/discover/palm2/)
    and [GPT4](https://openai.com/research/gpt-4) have revolutionised the way we interact
    with computers **using text as an input**, but‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: What if there was a way to extend the intelligence of these models, by enabling
    them to use different modalities of input, such as photos, audio, and video? **Or
    in other words, make them Multimodal!**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It could greatly improve how we search for things on the web, or even understand
    the world around us for example in real world applications such as medicine and
    pathology.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a solution! Multimodal deep learning models can combine the embeddings
    from different types of input, enabling, for example, an LLM to ‚Äúsee‚Äù what you
    are asking for, and return relevant results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**‚ö°Ô∏èStick around if** you want to learn more about how this all works and play
    around with a working demo!'
  prefs: []
  type: TYPE_NORMAL
- en: üî• How does it work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**It starts with embeddings**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most powerful building blocks of training deep learning models is
    the creation of embedding vectors.
  prefs: []
  type: TYPE_NORMAL
- en: During training, the model encodes the different categories (for example, people,
    foods, and toys) it encounters into their numerical representation aka. an Embedding,
    that is stored as a vector of numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings are useful when we want to move from a sparse representation of category
    (or class) for example a long string of text or an image, to something that is
    more compact, and can be reused across other models too.
  prefs: []
  type: TYPE_NORMAL
- en: Put simply, Embeddings provide a way for us to store the meaning of things in
    a shorthand form, that machines can use to quickly compare and search against!
  prefs: []
  type: TYPE_NORMAL
- en: To be a bit more concrete, with vision models trained on different images, images
    with a similar appearance and meaning will be placed closely together in the embedding
    space it creates.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7ec53cf44032053dd9980ef256f49a2.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1: Visualisation of embeddings that represent different categories
    of images, and similar images in the same embedding space. Illustration by the
    author.*'
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúThe model can map an image to an embedding, which is a location in the space.
    Therefore, if you look around the embedding, you can find other images with similar
    appearance and meaning. This is how image similarity search works!‚Äù[1]
  prefs: []
  type: TYPE_NORMAL
- en: '**Training on image and text pairs**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a similar fashion to the previous example, deep learning models can also
    be trained using related pairings of text and images. This happens using a combination
    of other models, as illustrated in the Figure below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4fd63d2d4fcf69eae568d4383c13edc2.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2: Using a combination of models to train on image and text pairs aka
    ‚ÄúCoCa‚Äù. Illustration by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs breakdown what‚Äôs happening above, when we pass in an image and text pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image Encoder:** a model to obtain image embeddings,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unimodel Text Encoder:** a text model to obtain text embeddings. This model
    is fed by the Image Encoder model, taking source image embeddings as input and
    to produce a representation of the sequence of images and text pairs aka. **Self
    Attention**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multimodal Text Encoder:** a model to learn the relationships between them.
    More formally, the technique is called **Cross Attention**: an attention mechanism
    in Transformer architecture that mixes two different embedding sequences, or in
    our case images and the text that describes them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This design, or one way of combining models is known as Contrastive Captioning
    or simply ‚ÄòCoCa‚Äô see [here](https://arxiv.org/abs/2205.01917) if you want to delve
    deeper!
  prefs: []
  type: TYPE_NORMAL
- en: '**‚ÄúThe result: a foundational ‚ÄòVision Language Model (VLM)‚Äô‚Ä¶**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Where we have built a shared embedding space for images and texts with a fixed
    dimension, organized by their meanings. In this space, images and text with similar
    meanings are placed close together.‚Äù[1]
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a6598a08a9054ae15d9ab90bf549733c.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3: Visualisation of embeddings for related images and text. Illustration
    by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: This means that you can search for images based on text (text-to-image search)
    or search for text based on images (image-to-text search).
  prefs: []
  type: TYPE_NORMAL
- en: This is the basic idea behind how Google Search finds relevant results across
    images and text.
  prefs: []
  type: TYPE_NORMAL
- en: üîç Try a quick demo!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Want to see how text and image embeddings work in a real world application?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can play around for free using a demo running on Altas by Nomic.
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúAtlas enables you to Interact, discover insights and build with unstructured
    text, image and audio data.‚Äù[2]
  prefs: []
  type: TYPE_NORMAL
- en: Here is a search demo that brings together text and image embeddings based on
    items available on the Mercari website (a Japanese e-commerce retailer).
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Navigate to: [https://atlas.nomic.ai/map/vertex-mercari](https://atlas.nomic.ai/map/vertex-mercari)
    (you might need to create a free account to gain access).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ce2cc59b9cacf30d4972defb136e1780.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Screenshot of Atlas by Nomic user interface, running Mecari embeddings
    demo.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. You can move your mouse cursor around the ‚Äòdot-cloud‚Äô vizualisation that
    represents the embedding space you can search against.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Or in the ‚ÄúSelection Tools‚Äù in the top-right corner of the interface you
    can type a text search query, filter by certain attributes or ‚Äòlasso‚Äô your own
    selection.
  prefs: []
  type: TYPE_NORMAL
- en: It really is that simple! If you want to learn more about how it was put together,
    take a look [here](https://ai-demos.dev/).
  prefs: []
  type: TYPE_NORMAL
- en: üç± Multimodal LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yes, there‚Äôs more to it than what we have discussed. In the case of LLMs, complex
    [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    architectures are used to convert text and other data to vectors and back using
    tokenization, positional encoding, and embedding layers.
  prefs: []
  type: TYPE_NORMAL
- en: But broadly speaking, the same principle we‚Äôve discussed earlier, enables us
    to use images to complement the text prompts we pass to LLMs aka. Multimodal LLMs,
    which let the user specify any vision or language task.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal LLMs are a recent and powerful development, examples such [GPT-4V](https://openai.com/research/gpt-4)
    and [MedPalm M](https://medika.life/a-new-era-in-medical-ai-the-power-of-med-palm-m/)
    are pushing the boundaries of how we use LLMs because they can provide rich responses
    based on image and text inputs.
  prefs: []
  type: TYPE_NORMAL
- en: For example, MedPalm M makes it possible to ask the model to diagnose a patient
    based on an X-ray image, with results that would rival a qualified physician!
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that Multimodal LLMs are not perfect and can still be
    susceptible to the limitations of earlier models.
  prefs: []
  type: TYPE_NORMAL
- en: They are still not fully reliable (i.e. they can ‚Äúhallucinate‚Äù facts and make
    reasoning errors). Great care should be taken when using language model outputs,
    particularly in high-stakes contexts, such as medical diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring that appropriate safeguards and processes are in place is critical
    (such as human review, grounding with additional context, or even avoiding high-stakes
    use cases altogether).
  prefs: []
  type: TYPE_NORMAL
- en: This is an exciting space representing the next evolution of LLMs!
  prefs: []
  type: TYPE_NORMAL
- en: üèÅ The wrap up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this post we introduced:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How embeddings are used to enable models to capture (encode) input data they
    are trained on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concepts of Multimodal deep learning models that are typically composed
    of multiple unimodal neural networks, which process each input modality separately.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A simple demo incorporating text and image embeddings to illustrate the Multimodal
    Search use case when searching for products on a website.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally we also discussed some potential use cases and indeed real world examples
    of Multimodal LLMs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**üëãüèºThanks for stopping by, I hope you enjoyed this post, and I‚Äôll see you
    on the next!**'
  prefs: []
  type: TYPE_NORMAL
- en: üìö References & Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] ‚ÄúWhat is Multimodal Search: ‚ÄòLLMs with vision‚Äô change businesses‚Äù -[https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search](https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [https://atlas.nomic.ai/](https://atlas.nomic.ai/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Publications:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '‚ÄúTowards Generalist Biomedical AI‚Äù aka. The MedPalm M paper, arXiv:2307.14334v1
    [cs.CL] 26 Jul 2023: [https://arxiv.org/pdf/2307.14334.pdf?ref=maginative.com](https://arxiv.org/pdf/2307.14334.pdf?ref=maginative.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '‚ÄúCoCa: Contrastive Captioners are Image-Text Foundation Models‚Äù, arXiv:2205.01917v2
    [cs.CV] 14 Jun 2022: [https://arxiv.org/pdf/2205.01917.pdf](https://arxiv.org/pdf/2205.01917.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
