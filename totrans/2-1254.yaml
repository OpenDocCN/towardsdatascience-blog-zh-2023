- en: How to Train BERT for Masked Language Modeling Tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-train-bert-for-masked-language-modeling-tasks-3ccce07c6fdc](https://towardsdatascience.com/how-to-train-bert-for-masked-language-modeling-tasks-3ccce07c6fdc)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hands-on guide to building language model for MLM tasks from scratch using Python
    and Transformers library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ransakaravihara.medium.com/?source=post_page-----3ccce07c6fdc--------------------------------)[![Ransaka
    Ravihara](../Images/ac09746938c10ad8f157d46ea0de27ca.png)](https://ransakaravihara.medium.com/?source=post_page-----3ccce07c6fdc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3ccce07c6fdc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3ccce07c6fdc--------------------------------)
    [Ransaka Ravihara](https://ransakaravihara.medium.com/?source=post_page-----3ccce07c6fdc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3ccce07c6fdc--------------------------------)
    ·7 min read·Oct 17, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f2df84bc5e99738d8d9c3264785f2390.png)'
  prefs: []
  type: TYPE_IMG
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, large language models(LLMs) have taken all the attention from
    the machine learning community. Before LLMs came in, we had a crucial research
    phase on various language modeling techniques, including masked language modeling,
    causal language modeling, and sequence-to-sequence language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: From the above list, masked language models such as BERT became more usable
    in downstream NLP tasks such as classification and clustering. Thanks to libraries
    such as Hugging Face Transformers, adapting these models for downstream tasks
    became more accessible and manageable. Also thanks to the open-source community,
    we have plenty of language models to choose from covering widely used languages
    and domains.
  prefs: []
  type: TYPE_NORMAL
- en: GitHub repository for this tutorial
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/Ransaka/Train-BERT-for-Masked-Language-Modeling-Tasks?source=post_page-----3ccce07c6fdc--------------------------------)
    [## GitHub - Ransaka/Train-BERT-for-Masked-Language-Modeling-Tasks: GitHub repo
    for TDS article "How to…'
  prefs: []
  type: TYPE_NORMAL
- en: GitHub repo for TDS article "How to Train BERT for Masked Language Modeling
    Tasks" - GitHub …
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/Ransaka/Train-BERT-for-Masked-Language-Modeling-Tasks?source=post_page-----3ccce07c6fdc--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tune or build one from scratch?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When adapting existing language models to your specific use cases, sometimes
    we can use existing models without further tuning (so-called fine-tuning). For
    example, if you want an English sentiment/intent detection model, you can go into
    HuggingFace.co and find a suitable model for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b7b3c524c332f97ac2b6e822de2a7ac0.png)'
  prefs: []
  type: TYPE_IMG
- en: However, you can only expect this for some of the tasks encountered in the real
    world. That's where we need an additional technique called fine-tuning. First,
    you must choose a base model that will be fine-tuned. Here, you must be careful
    about the selected model and your target language's lexical similarity.
  prefs: []
  type: TYPE_NORMAL
- en: However, if you can't find a suitable model retrained on the desired language,
    consider building one from scratch. In this tutorial, we will implement the BERT
    model for the masked language model.
  prefs: []
  type: TYPE_NORMAL
- en: BERT Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though describing BERT architecture is out of the scope of this tutorial,
    for the sake of clarity, let's go through it very narrowly. BERT, or **B**idirectional
    **E**ncoder **R**epresentations from **T**ransformers, belongs to the encoder-only
    transformer family. It was introduced in the year 2018 by researchers at Google.
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper abstract:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We introduce a new language representation model called BERT, which stands for
    Bidirectional Encoder Representations from Transformers. Unlike recent language
    representation models, BERT is designed to pre-train deep bidirectional representations
    from unlabeled text by jointly conditioning on both left and right context in
    all layers. As a result, the pre-trained BERT model can be fine-tuned with just
    one additional output layer to create state-of-the-art models for a wide range
    of tasks, such as question answering and language inference, without substantial
    task-specific architecture modifications.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art
    results on eleven natural language processing tasks, including pushing the GLUE
    score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6%
    absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point
    absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Paper: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the above, we can see an interesting keyword, Bidirectional. Bidirectional
    nature gives human-like power to BERT. Assume you have to fill in a blank like
    the below one,
  prefs: []
  type: TYPE_NORMAL
- en: “War may sometimes be a necessary evil. But no matter how necessary, it is always
    an _____, never a good.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'To guess the word comes into a blank position, you should be aware of a few
    things: words before empty, words after blank, and the overall context of the
    sentence. Adapting this human nature, BERT works the same way. During training,
    we hide some words and ask BERT to try predicting those. When training is finished,
    BERT can predict masked tokens based on their before and after words. To do this,
    the model should allocate different attention to words presented in the input
    sequence, which may significantly impact predicting masked tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b479a89ad5558c20c076ccbff0a93ddc.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author via [https://huggingface.co/spaces/exbert-project/exbert](https://huggingface.co/spaces/exbert-project/exbert)
  prefs: []
  type: TYPE_NORMAL
- en: As you can see here, the model sees a suitable word for the hidden position
    as *evil* and the first sentence's *evil* as necessary to make this prediction.
    This is a noticeable point and implies that the model understands the context
    of the input sequence. This context awareness allows BERT to generate meaningful
    sentence embeddings for given tasks. Further, these embeddings can be used in
    downstream tasks such as clustering and classification. Enough about BERT; let's
    build one from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Defining BERT model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We generally have BERT*(base)* and BERT*(large).* Both have 64 dimensions per
    head. The *large* variant contains 24 encoder layers, while the *base* variant
    only has 12\. However, we are not limited to these configurations. Surprisingly,
    we have complete control over defining the model using Hugging Face Transformers
    library. All we have to do is define desired model configurations using the *BertConfig*
    class.
  prefs: []
  type: TYPE_NORMAL
- en: I chose 6 heads and 384 total model dimensions to comply with the original implementation.
    In this way, each head has 64 dimensions similar to the original implementation.
    Let's initialize our BERT model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Training a tokenizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, I won't describe how tokenization works under the hood. Instead, let's
    train one from scratch using Hugging Face tokenizers library. Please note that
    the tokenizer used in the original BERT implementation is WordPiece tokenizer,
    yet another subword-based tokenization method. You can learn more about this tokenization
    using the neat HuggingFace resource below.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt&source=post_page-----3ccce07c6fdc--------------------------------)
    [## WordPiece tokenization — Hugging Face NLP Course'
  prefs: []
  type: TYPE_NORMAL
- en: We're on a journey to advance and democratize artificial intelligence through
    open source and open science.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: huggingface.co](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt&source=post_page-----3ccce07c6fdc--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The dataset used here is the Sinhala-400M dataset *(under apache-2.0)*. You
    can follow the same with any dataset you have.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/377b2ebfccb2a7f8d25df41b71c6627c.png)'
  prefs: []
  type: TYPE_IMG
- en: As you may notice, some Sinhalese words have been typed using English as well.
    Let's train a tokenizer for these corpora.
  prefs: []
  type: TYPE_NORMAL
- en: Let's import the necessary modules first. The good thing about training tokenizers
    using Hugging Face Tokenizers library is that we can use existing tokenizers and
    replace only vocabulary (and merge where applicable) per our training corpus.
    This means tokenization steps such as pre-tokenization and post-tokenization will
    be preserved. For this, we can use a method, *train_new_from_iterator* BertTokenizer
    class*.*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c66a51e3ac4dad4f8e2c5244908f2549.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a9a95cf24c4b15bff37110a33f2e4635.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see words like 'cricketer' decomposed into *cricket* and *##er,* indicating
    that the tokenizer has been adequately trained. However, try out different vocab
    sizes; mine is 5000, which is relatively small but suitable for this toy example.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can save the trained tokenizer into our directory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Define data collator and tokenize dataset.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's define a collator for MLM tasks. Here, we will mask 15% of tokens. Anyway,
    we can set different masking probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Let's tokenize the dataset using a previously created tokenizer. I'm replacing
    the original *LineByLineTextDataset* using my custom class utilizing Hugging Face
    accelerate.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Let's tokenize the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Alright, let's code our training loop.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We can invoke the trainer using its *train()* method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/05feb8f3954884ffa738ba0f551bdaee.png)'
  prefs: []
  type: TYPE_IMG
- en: After sufficient training, our model can be used for downstream tasks such as
    zero-shot classification and clustering. You may find the example using this Hugging
    Face space for more details.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://huggingface.co/spaces/Ransaka/sinhala-embedding-space?source=post_page-----3ccce07c6fdc--------------------------------)
    [## Sinhala Embedding Space - a Hugging Face Space by Ransaka'
  prefs: []
  type: TYPE_NORMAL
- en: Discover amazing ML apps made by the community
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: huggingface.co](https://huggingface.co/spaces/Ransaka/sinhala-embedding-space?source=post_page-----3ccce07c6fdc--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With limited resources, pre-trained models may only recognize specific linguistic
    patterns, but they can still be helpful for particular use cases. It is highly
    recommended to fine-tune when possible.
  prefs: []
  type: TYPE_NORMAL
- en: '*In this article, all images, unless otherwise noted, are by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An Explorable BERT — [https://huggingface.co/spaces/exbert-project/exbert](https://huggingface.co/spaces/exbert-project/exbert)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BERT Paper — [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dataset — [https://huggingface.co/datasets/Ransaka/Sinhala-400M](https://huggingface.co/datasets/Ransaka/Sinhala-400M)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
