- en: BEV Perception in Mass Production Autonomous Driving
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模生产自主驾驶中的BEV感知
- en: 原文：[https://towardsdatascience.com/bev-perception-in-mass-production-autonomous-driving-c6e3f1e46ae0](https://towardsdatascience.com/bev-perception-in-mass-production-autonomous-driving-c6e3f1e46ae0)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/bev-perception-in-mass-production-autonomous-driving-c6e3f1e46ae0](https://towardsdatascience.com/bev-perception-in-mass-production-autonomous-driving-c6e3f1e46ae0)
- en: The Recipe of XNet from Xpeng Motors
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 小鹏汽车的XNet配方
- en: '[](https://medium.com/@patrickllgc?source=post_page-----c6e3f1e46ae0--------------------------------)[![Patrick
    Langechuan Liu](../Images/fecbf85146a9bde21e6b2251538ddd65.png)](https://medium.com/@patrickllgc?source=post_page-----c6e3f1e46ae0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c6e3f1e46ae0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c6e3f1e46ae0--------------------------------)
    [Patrick Langechuan Liu](https://medium.com/@patrickllgc?source=post_page-----c6e3f1e46ae0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@patrickllgc?source=post_page-----c6e3f1e46ae0--------------------------------)[![Patrick
    Langechuan Liu](../Images/fecbf85146a9bde21e6b2251538ddd65.png)](https://medium.com/@patrickllgc?source=post_page-----c6e3f1e46ae0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c6e3f1e46ae0--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c6e3f1e46ae0--------------------------------)
    [Patrick Langechuan Liu](https://medium.com/@patrickllgc?source=post_page-----c6e3f1e46ae0--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c6e3f1e46ae0--------------------------------)
    ·9 min read·Jun 19, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[数据科学前沿](https://towardsdatascience.com/?source=post_page-----c6e3f1e46ae0--------------------------------)
    ·阅读时间9分钟·2023年6月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: This blog post is based on the invited talk in the *End-to-end Autonomous Driving
    Workshop* at CVPR 2023 held in Vancouver, titled “The Practice of Mass Production
    Autonomous Driving in China”. The recording of the keynote can be found [here](https://www.youtube.com/watch?v=d6ucRgDDUWQ&t=162s).
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本博客文章基于在2023年CVPR于温哥华举行的*端到端自主驾驶研讨会*上的邀请演讲，演讲题为“中国大规模生产自主驾驶的实践”。主旨演讲的录音可以在[这里](https://www.youtube.com/watch?v=d6ucRgDDUWQ&t=162s)找到。
- en: BEV (bird’s-eye-view) perception has witnessed great progress over the past
    few years. It directly perceives the environment around autonomous driving vehicles.
    **BEV perception** can be seen as an **end-to-end perception** system, and an
    important step toward an end-to-end autonomous driving system. Here we define
    an end-to-end autonomous driving system as fully differentiable pipelines that
    take raw sensor data as input and produce a high-level driving plan or low-level
    control actions as output.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: BEV（鸟瞰视角）感知在过去几年中取得了巨大进展。它直接感知自主驾驶车辆周围的环境。**BEV感知**可以被看作是一个**端到端感知**系统，也是实现端到端自主驾驶系统的重要一步。我们将端到端自主驾驶系统定义为完全可微分的管道，这些管道以原始传感器数据作为输入，输出高层次的驾驶计划或低层次的控制动作。
- en: '[](https://opendrivelab.com/e2ead/cvpr23.html?source=post_page-----c6e3f1e46ae0--------------------------------)
    [## CVPR 2023 Autonomous Driving Workshop | OpenDriveLab'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://opendrivelab.com/e2ead/cvpr23.html?source=post_page-----c6e3f1e46ae0--------------------------------)
    [## CVPR 2023 自主驾驶研讨会 | OpenDriveLab'
- en: We are proud to announce four brand-new challenges this year, in collaboration
    with our partners -Vision-Centric…
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们很自豪地宣布今年与我们的合作伙伴共同推出四个全新挑战 - 以视觉为中心…
- en: opendrivelab.com](https://opendrivelab.com/e2ead/cvpr23.html?source=post_page-----c6e3f1e46ae0--------------------------------)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[opendrivelab.com](https://opendrivelab.com/e2ead/cvpr23.html?source=post_page-----c6e3f1e46ae0--------------------------------)'
- en: The autonomous driving community has witnessed rapid growth in approaches that
    embrace an end-to-end algorithm framework. We will discuss the necessity of end-to-end
    approaches from the first principle. Then we will review the efforts to deploy
    the BEV perception algorithm onto mass production vehicles, taking the development
    of XNet, the BEV perception architecture from Xpeng as an example. Finally, we
    will brainstorm about the future of BEV perception toward fully end-to-end autonomous
    driving.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自主驾驶社区见证了采用端到端算法框架的方法的快速增长。我们将从基本原理讨论端到端方法的必要性。然后，我们将回顾将BEV感知算法部署到大规模生产车辆的努力，以小鹏的BEV感知架构XNet为例。最后，我们将对BEV感知的未来进行头脑风暴，以实现完全端到端的自主驾驶。
- en: The Necessity of End-to-end Systems
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 端到端系统的必要性
- en: In solving any engineering problem, it is often necessary to use a divide-and-conquer
    approach to find practical solutions quickly. This strategy involves breaking
    down the big problem into smaller, relatively well-defined components that can
    be solved independently. While this approach assists in delivering a complete
    product quickly, it also increases the risk of being stuck at a local optimum
    solution. To reach the global optimum solution, all components must be optimized
    together in an end-to-end fashion.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cd46416c815dae75d45c756ea2ca7334.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: The performance growth curve for Divide-and-conquer vs End-to-end (chart made
    by author)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The 80–20 rule reinforces the concept that 80% of the desired performance can
    be achieved with only 20% of the total effort. The advantage of using a divide-and-conquer
    approach is that it allows developers to work quickly using minimal effort. However,
    the downside is that this method often leads to a performance ceiling at the 80%
    mark. To overcome the performance limit and get out of the local optimum, developers
    must optimize certain components together, which is the first step in developing
    an end-to-end solution. This process must be repeated several times, breaking
    performance ceilings over and over until a fully end-to-end solution is achieved.
    The resulting curve may take the form of a series of sigmoid curves until the
    global optimal solution is approximated. One example of an effort toward an end-to-end
    solution is the development of BEV perception algorithm.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Perception 2.0: End-to-end Perception'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In traditional autonomous driving stacks, 2D images are fed into the perception
    module to generate 2D results. Sensor fusion is then utilized to reason between
    2D results from multiple cameras and elevate these to 3D. The resulting 3D objects
    are subsequently sent to downstream components, such as prediction and planning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ef5c86c32ddd46d2be8ea1b5510384d8.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: BEV perception is essentially end-to-end perception (image made by author)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: However, the sensor fusion step requires a lot of handcrafted rules to fuse
    the perception results from several camera streams. Each camera only perceives
    a portion of the object to be observed, so combining the obtained information
    necessitates careful adjustment of the fusion logic. We are essentially doing
    back-propagation through the engineers’ heads. Moreover, developing and maintaining
    these rules creates a set of complications, leading to numerous issues in complex
    urban environments.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this challenge, we can apply the Bird’s Eye View (BEV) perception
    model, which allows us to perceive the environment directly in the BEV space.
    The BEV perception stack combines two separate components into a single solution,
    thereby eliminating the brittle human-crafted logic. BEV perception is essentially
    an **end-to-end perception** solution. This marks a critical step toward an end-to-end
    autonomous driving system.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'XNet: BEV Perception Stack from Xpeng Motors'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'XNet: Xpeng Motors 的 BEV 感知堆栈'
- en: The BEV perception architecture from Xpeng is codenamed XNet. It was first publicly
    introduced in [Xpeng 1024 Tech Day in 2022](https://www.youtube.com/watch?v=0dEoctcK09Q).
    The visualization below depicts the onboard XNet perception architecture in action.
    The red vehicle in the middle represents the autonomous driving vehicle as it
    navigates a roundabout. The surrounding static environment is entirely detected
    by onboard perception, and no HD Map is used. We can observe that XNet accurately
    detects a wide range of dynamic and static objects around the vehicle.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Xpeng 的 BEV 感知架构代号为 XNet。它首次公开介绍是在 [2022年 Xpeng 1024 科技日](https://www.youtube.com/watch?v=0dEoctcK09Q)
    上。下方的可视化图展示了车载 XNet 感知架构的实际运行情况。中间的红色车辆代表自动驾驶车辆在环形交叉路口导航。周围的静态环境完全由车载感知系统检测，无需使用高清地图。我们可以观察到
    XNet 精确地检测了车辆周围的各种动态和静态物体。
- en: The Xpeng AI team started experimenting with the XNet architecture over two
    years ago (early 2021), and it has since undergone several iterations before arriving
    at its current form. We exploit Convolutional Neural Network (CNN) backbones to
    generate image features, while the multicamera features are transposed into the
    BEV space through a transformer structure. Specifically, a cross-attention module
    was used. The BEV features from several past frames are then fused with the ego
    pose — both spatially and temporally — to decode the dynamic and static elements
    from the fused features.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Xpeng AI 团队在两年前（2021年初）开始对 XNet 架构进行实验，并经过多次迭代后达到了现在的形式。我们利用卷积神经网络（CNN）主干生成图像特征，同时通过变换器结构将多摄像头特征转置到
    BEV 空间。具体而言，使用了交叉注意力模块。来自多个过去帧的 BEV 特征随后与自我姿态——在空间和时间上——融合，以解码融合特征中的动态和静态元素。
- en: '![](../Images/9b842cb903eed4b72c7a424e035210b9.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b842cb903eed4b72c7a424e035210b9.png)'
- en: XNet results and architecture
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: XNet 结果与架构
- en: The vision-centric BEV perception architecture improves the cost-effectiveness
    for mass deployment of autonomous driving solutions, reducing the need for more
    expensive hardware components. The accurate 3D detections and velocity unfold
    a new dimension of redundancy and reduce the reliance on LiDARs and radars. Furthermore,
    the real-time 3D sensible environment perception lessens the dependency on HD
    maps. Both capabilities contribute significantly to a more reliable and cost-effective
    autonomous driving solution.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以视觉为中心的 BEV 感知架构提高了大规模部署自动驾驶解决方案的性价比，减少了对更昂贵硬件组件的需求。准确的 3D 检测和速度展现了冗余的新维度，减少了对激光雷达和雷达的依赖。此外，实时
    3D 可感知环境减少了对高清地图的依赖。这两种能力都显著地有助于更可靠且具有成本效益的自动驾驶解决方案。
- en: '**The Challenges of XNet**'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**XNet 的挑战**'
- en: Deploying such a neural network onto production vehicles presents several challenges.
    Firstly, millions of multicamera video clips are necessary to train XNet. These
    clips involve around one billion objects requiring annotation. Based on the current
    annotation efficiency, approximately **2,000 human-years** are needed for annotation.
    Unfortunately, this means that for the in-house annotation team of around 1000
    people at Xpeng, such a task would take around two years to accomplish, which
    is not acceptable. From a model training perspective, it would take **almost a
    year** to train such a network using a single machine. Furthermore, deploying
    such a network without any optimization on an NVIDIA Orin platform would take
    **122% computing power of one chip**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 将这样的神经网络部署到生产车辆上存在多个挑战。首先，训练 XNet 需要数百万个多摄像头视频片段。这些片段涉及约十亿个需要标注的物体。根据当前的标注效率，大约需要**2,000
    人年**来完成标注。不幸的是，这意味着对于 Xpeng 约1000人的内部标注团队来说，这项任务需要大约两年才能完成，这是不可接受的。从模型训练的角度来看，使用一台机器训练这样的网络将需要**接近一年**。此外，在
    NVIDIA Orin 平台上未经优化地部署这样的网络将消耗**一个芯片的122%计算能力**。
- en: All of these issues present challenges that we have to address for successful
    training and deployment of such a complex and large model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些问题都对成功训练和部署如此复杂且庞大的模型提出了挑战。
- en: Autolabel
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动标注
- en: To improve annotation efficiency, we have developed a highly effective autolabel
    system. This offline sensor fusion stack enhances efficiency by up to 45,000 times,
    enabling us to complete annotation tasks that would have required 200 human years
    in just 17 days.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4450b96601971cbf724e35a9ab4663a0.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: Autolabel system significantly boost annotation efficiency
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Above is the lidar-based autolabel system, and we also developed a system that
    solely relies on vision sensors. This enables us to annotate clips obtained from
    the customer fleets that don not have lidars. This is a critical part of the data
    closed loop and enhancing the development of a self-evolving perception system.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Large Scale Training
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We optimized the training pipeline for XNet from two perspectives. Firstly,
    we applied mixed precision training and operator optimization techniques to streamline
    the training process on a single node, which reduced the training time by a factor
    of 10\. Next, we partnered with Alicloud and built a GPU cluster with a computation
    power of 600 PFLOPS, allowing us to scale out the training from a single machine
    to multiple machines. This reduced the training time even further, although the
    process was not straightforward as we needed to carefully tweak the training procedure
    to achieve near-linear performance scaling. Overall, we reduced the training time
    for XNet from 276 days to a mere 11 hours. Note that as we add more data into
    the training process, the training time naturally increases, calling for additional
    optimization. Therefore, scaling out optimization remains a continuous and critical
    effort.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/606d40ee55dd1bbd67a487868b4e68e6.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: Optimization of large scale parallel training pipeline for XNet
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Efficient Deployment on Orin
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We noted that without any optimization, running XNet on an Nvidia Orin chip
    would require 122% of the chip’s computation power. On analyzing the profiling
    chart displayed at the beginning, we observed that the transformer module consumed
    most of the runtime. This is understandable as the transformer module had not
    received much attention during the Orin chip’s initial design phase. As a result,
    we needed to redesign the transformer module and attention mechanism to support
    the Orin platform, allowing us to achieve a 3x speedup.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ba6e350f5231816c60474732c6b9cb1.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: Extreme optimization of transformer-based XNet on the Orin platform
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Motivated to optimize further, we progressed to optimize the network through
    pruning, resulting in an additional 2.6x speedup. Lastly, employing workload balancing
    between GPU and DLA, we achieved a further 1.7x speedup.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: With these various optimization techniques, we reduced XNet’s GPU utilization
    from 122% to just 9%. This freed us to explore new possibilities in architecture
    on the Orin platform.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Self-evolving Data Engine
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the implementation of XNet architecture, we can now initiate data-driven
    iterations to boost the model’s performance. To accomplish this, we first identify
    corner cases on the car and then deploy configurable triggers to the customer
    fleet to collect relevant images. Subsequently, we retrieve images from collected
    data, based on a short description in natural language or an image itself. In
    doing so, we leverage recent advancements in large language models to increase
    the efficiency of dataset curation and annotation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/391d72a6b66e90910e8d16230089d311.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: Data engine helps improve XNet performance
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: With the XNet architecture and data engine, we have created a scalable and self-evolving
    perception system.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: The Future
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The latest release of Xpeng Highway NGP 2.0 unifies highway and city pilot solutions,
    allowing users to drop a pin in a different city and have a smooth experience
    from start to finish. This unification is made possible by XNet, which provides
    a solid foundation for a unified stack across all scenarios. The ultimate goal
    is to enable point-to-point user experience with end-to-end autonomous driving.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: In order to make the autonomous driving system end-to-end differentiable, another
    critical missing piece is a machine-learning based planning stack. Learning-based
    planning solutions can be largely divided into imitation learning or reinforcement
    learning approaches. Recent progress in large language models (LLMs) also spells
    great potential for the advancement of this important topic. The following [Github
    repo](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving) is a live
    collection of relevant work in the burgeoning field of end-to-end autonomous driving.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving?source=post_page-----c6e3f1e46ae0--------------------------------)
    [## GitHub - OpenDriveLab/End-to-end-Autonomous-Driving: All you need for End-to-end
    Autonomous Driving'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: All you need for End-to-end Autonomous Driving. Contribute to OpenDriveLab/End-to-end-Autonomous-Driving
    development by…
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving?source=post_page-----c6e3f1e46ae0--------------------------------)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Takeaways
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Divide and conquer reaches 80% of performance with 20% effort. End-to-end approaches
    aim to break the 80% performance ceiling, at potentially much greater cost.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XNet is an end-to-end perception system and one critical step toward an end-to-end
    full-stack solution. It requires significant engineering effort (80%) according
    to the 80–20 rule.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The large amount of annotation needed for XNet calls for automatic annotation,
    as human annotation is not feasible. The autolabel system can boost efficiency
    by 45000 times.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large scale training requires optimization of training on a single machine,
    and scaling out from one machine to multiple machines.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XNet deployment on Nvidia Orin platform requires refactoring of transformer
    module.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the unique challenges in deploying mass-production autonomous driving in
    China, please refer to the following link. This was also part of the same invited
    talk at CVPR 2023.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对在中国部署大规模生产自动驾驶所面临的独特挑战，请参阅以下链接。这也是在 CVPR 2023 上的邀请讲座的一部分。
- en: '[](https://medium.com/@patrickllgc/challenges-of-mass-production-autonomous-driving-in-china-407c7e2dc5d8?source=post_page-----c6e3f1e46ae0--------------------------------)
    [## Challenges of Mass Production Autonomous Driving in China'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@patrickllgc/challenges-of-mass-production-autonomous-driving-in-china-407c7e2dc5d8?source=post_page-----c6e3f1e46ae0--------------------------------)
    [## 中国大规模生产自动驾驶面临的挑战'
- en: And Xpeng’s Answer to Them
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 以及 Xpeng 对这些问题的回答
- en: medium.com](https://medium.com/@patrickllgc/challenges-of-mass-production-autonomous-driving-in-china-407c7e2dc5d8?source=post_page-----c6e3f1e46ae0--------------------------------)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@patrickllgc/challenges-of-mass-production-autonomous-driving-in-china-407c7e2dc5d8?source=post_page-----c6e3f1e46ae0--------------------------------)
- en: A review of academic research on Transformer-based BEV perception algorithm.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一项关于基于 Transformer 的 BEV 感知算法的学术研究回顾。
- en: '[https://medium.com/towards-data-science/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944](https://medium.com/towards-data-science/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://medium.com/towards-data-science/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944](https://medium.com/towards-data-science/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944)'
- en: 'Xpeng 1024 Tech Day 2022: [https://www.youtube.com/watch?v=0dEoctcK09Q](https://www.youtube.com/watch?v=0dEoctcK09Q)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xpeng 1024 科技日 2022: [https://www.youtube.com/watch?v=0dEoctcK09Q](https://www.youtube.com/watch?v=0dEoctcK09Q)'
