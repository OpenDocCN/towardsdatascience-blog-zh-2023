- en: BEV Perception in Mass Production Autonomous Driving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/bev-perception-in-mass-production-autonomous-driving-c6e3f1e46ae0](https://towardsdatascience.com/bev-perception-in-mass-production-autonomous-driving-c6e3f1e46ae0)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Recipe of XNet from Xpeng Motors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@patrickllgc?source=post_page-----c6e3f1e46ae0--------------------------------)[![Patrick
    Langechuan Liu](../Images/fecbf85146a9bde21e6b2251538ddd65.png)](https://medium.com/@patrickllgc?source=post_page-----c6e3f1e46ae0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c6e3f1e46ae0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c6e3f1e46ae0--------------------------------)
    [Patrick Langechuan Liu](https://medium.com/@patrickllgc?source=post_page-----c6e3f1e46ae0--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c6e3f1e46ae0--------------------------------)
    ·9 min read·Jun 19, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: This blog post is based on the invited talk in the *End-to-end Autonomous Driving
    Workshop* at CVPR 2023 held in Vancouver, titled “The Practice of Mass Production
    Autonomous Driving in China”. The recording of the keynote can be found [here](https://www.youtube.com/watch?v=d6ucRgDDUWQ&t=162s).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: BEV (bird’s-eye-view) perception has witnessed great progress over the past
    few years. It directly perceives the environment around autonomous driving vehicles.
    **BEV perception** can be seen as an **end-to-end perception** system, and an
    important step toward an end-to-end autonomous driving system. Here we define
    an end-to-end autonomous driving system as fully differentiable pipelines that
    take raw sensor data as input and produce a high-level driving plan or low-level
    control actions as output.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://opendrivelab.com/e2ead/cvpr23.html?source=post_page-----c6e3f1e46ae0--------------------------------)
    [## CVPR 2023 Autonomous Driving Workshop | OpenDriveLab'
  prefs: []
  type: TYPE_NORMAL
- en: We are proud to announce four brand-new challenges this year, in collaboration
    with our partners -Vision-Centric…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: opendrivelab.com](https://opendrivelab.com/e2ead/cvpr23.html?source=post_page-----c6e3f1e46ae0--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The autonomous driving community has witnessed rapid growth in approaches that
    embrace an end-to-end algorithm framework. We will discuss the necessity of end-to-end
    approaches from the first principle. Then we will review the efforts to deploy
    the BEV perception algorithm onto mass production vehicles, taking the development
    of XNet, the BEV perception architecture from Xpeng as an example. Finally, we
    will brainstorm about the future of BEV perception toward fully end-to-end autonomous
    driving.
  prefs: []
  type: TYPE_NORMAL
- en: The Necessity of End-to-end Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In solving any engineering problem, it is often necessary to use a divide-and-conquer
    approach to find practical solutions quickly. This strategy involves breaking
    down the big problem into smaller, relatively well-defined components that can
    be solved independently. While this approach assists in delivering a complete
    product quickly, it also increases the risk of being stuck at a local optimum
    solution. To reach the global optimum solution, all components must be optimized
    together in an end-to-end fashion.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cd46416c815dae75d45c756ea2ca7334.png)'
  prefs: []
  type: TYPE_IMG
- en: The performance growth curve for Divide-and-conquer vs End-to-end (chart made
    by author)
  prefs: []
  type: TYPE_NORMAL
- en: The 80–20 rule reinforces the concept that 80% of the desired performance can
    be achieved with only 20% of the total effort. The advantage of using a divide-and-conquer
    approach is that it allows developers to work quickly using minimal effort. However,
    the downside is that this method often leads to a performance ceiling at the 80%
    mark. To overcome the performance limit and get out of the local optimum, developers
    must optimize certain components together, which is the first step in developing
    an end-to-end solution. This process must be repeated several times, breaking
    performance ceilings over and over until a fully end-to-end solution is achieved.
    The resulting curve may take the form of a series of sigmoid curves until the
    global optimal solution is approximated. One example of an effort toward an end-to-end
    solution is the development of BEV perception algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perception 2.0: End-to-end Perception'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In traditional autonomous driving stacks, 2D images are fed into the perception
    module to generate 2D results. Sensor fusion is then utilized to reason between
    2D results from multiple cameras and elevate these to 3D. The resulting 3D objects
    are subsequently sent to downstream components, such as prediction and planning.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ef5c86c32ddd46d2be8ea1b5510384d8.png)'
  prefs: []
  type: TYPE_IMG
- en: BEV perception is essentially end-to-end perception (image made by author)
  prefs: []
  type: TYPE_NORMAL
- en: However, the sensor fusion step requires a lot of handcrafted rules to fuse
    the perception results from several camera streams. Each camera only perceives
    a portion of the object to be observed, so combining the obtained information
    necessitates careful adjustment of the fusion logic. We are essentially doing
    back-propagation through the engineers’ heads. Moreover, developing and maintaining
    these rules creates a set of complications, leading to numerous issues in complex
    urban environments.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this challenge, we can apply the Bird’s Eye View (BEV) perception
    model, which allows us to perceive the environment directly in the BEV space.
    The BEV perception stack combines two separate components into a single solution,
    thereby eliminating the brittle human-crafted logic. BEV perception is essentially
    an **end-to-end perception** solution. This marks a critical step toward an end-to-end
    autonomous driving system.
  prefs: []
  type: TYPE_NORMAL
- en: 'XNet: BEV Perception Stack from Xpeng Motors'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The BEV perception architecture from Xpeng is codenamed XNet. It was first publicly
    introduced in [Xpeng 1024 Tech Day in 2022](https://www.youtube.com/watch?v=0dEoctcK09Q).
    The visualization below depicts the onboard XNet perception architecture in action.
    The red vehicle in the middle represents the autonomous driving vehicle as it
    navigates a roundabout. The surrounding static environment is entirely detected
    by onboard perception, and no HD Map is used. We can observe that XNet accurately
    detects a wide range of dynamic and static objects around the vehicle.
  prefs: []
  type: TYPE_NORMAL
- en: The Xpeng AI team started experimenting with the XNet architecture over two
    years ago (early 2021), and it has since undergone several iterations before arriving
    at its current form. We exploit Convolutional Neural Network (CNN) backbones to
    generate image features, while the multicamera features are transposed into the
    BEV space through a transformer structure. Specifically, a cross-attention module
    was used. The BEV features from several past frames are then fused with the ego
    pose — both spatially and temporally — to decode the dynamic and static elements
    from the fused features.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b842cb903eed4b72c7a424e035210b9.png)'
  prefs: []
  type: TYPE_IMG
- en: XNet results and architecture
  prefs: []
  type: TYPE_NORMAL
- en: The vision-centric BEV perception architecture improves the cost-effectiveness
    for mass deployment of autonomous driving solutions, reducing the need for more
    expensive hardware components. The accurate 3D detections and velocity unfold
    a new dimension of redundancy and reduce the reliance on LiDARs and radars. Furthermore,
    the real-time 3D sensible environment perception lessens the dependency on HD
    maps. Both capabilities contribute significantly to a more reliable and cost-effective
    autonomous driving solution.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Challenges of XNet**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deploying such a neural network onto production vehicles presents several challenges.
    Firstly, millions of multicamera video clips are necessary to train XNet. These
    clips involve around one billion objects requiring annotation. Based on the current
    annotation efficiency, approximately **2,000 human-years** are needed for annotation.
    Unfortunately, this means that for the in-house annotation team of around 1000
    people at Xpeng, such a task would take around two years to accomplish, which
    is not acceptable. From a model training perspective, it would take **almost a
    year** to train such a network using a single machine. Furthermore, deploying
    such a network without any optimization on an NVIDIA Orin platform would take
    **122% computing power of one chip**.
  prefs: []
  type: TYPE_NORMAL
- en: All of these issues present challenges that we have to address for successful
    training and deployment of such a complex and large model.
  prefs: []
  type: TYPE_NORMAL
- en: Autolabel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To improve annotation efficiency, we have developed a highly effective autolabel
    system. This offline sensor fusion stack enhances efficiency by up to 45,000 times,
    enabling us to complete annotation tasks that would have required 200 human years
    in just 17 days.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4450b96601971cbf724e35a9ab4663a0.png)'
  prefs: []
  type: TYPE_IMG
- en: Autolabel system significantly boost annotation efficiency
  prefs: []
  type: TYPE_NORMAL
- en: Above is the lidar-based autolabel system, and we also developed a system that
    solely relies on vision sensors. This enables us to annotate clips obtained from
    the customer fleets that don not have lidars. This is a critical part of the data
    closed loop and enhancing the development of a self-evolving perception system.
  prefs: []
  type: TYPE_NORMAL
- en: Large Scale Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We optimized the training pipeline for XNet from two perspectives. Firstly,
    we applied mixed precision training and operator optimization techniques to streamline
    the training process on a single node, which reduced the training time by a factor
    of 10\. Next, we partnered with Alicloud and built a GPU cluster with a computation
    power of 600 PFLOPS, allowing us to scale out the training from a single machine
    to multiple machines. This reduced the training time even further, although the
    process was not straightforward as we needed to carefully tweak the training procedure
    to achieve near-linear performance scaling. Overall, we reduced the training time
    for XNet from 276 days to a mere 11 hours. Note that as we add more data into
    the training process, the training time naturally increases, calling for additional
    optimization. Therefore, scaling out optimization remains a continuous and critical
    effort.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/606d40ee55dd1bbd67a487868b4e68e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Optimization of large scale parallel training pipeline for XNet
  prefs: []
  type: TYPE_NORMAL
- en: Efficient Deployment on Orin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We noted that without any optimization, running XNet on an Nvidia Orin chip
    would require 122% of the chip’s computation power. On analyzing the profiling
    chart displayed at the beginning, we observed that the transformer module consumed
    most of the runtime. This is understandable as the transformer module had not
    received much attention during the Orin chip’s initial design phase. As a result,
    we needed to redesign the transformer module and attention mechanism to support
    the Orin platform, allowing us to achieve a 3x speedup.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ba6e350f5231816c60474732c6b9cb1.png)'
  prefs: []
  type: TYPE_IMG
- en: Extreme optimization of transformer-based XNet on the Orin platform
  prefs: []
  type: TYPE_NORMAL
- en: Motivated to optimize further, we progressed to optimize the network through
    pruning, resulting in an additional 2.6x speedup. Lastly, employing workload balancing
    between GPU and DLA, we achieved a further 1.7x speedup.
  prefs: []
  type: TYPE_NORMAL
- en: With these various optimization techniques, we reduced XNet’s GPU utilization
    from 122% to just 9%. This freed us to explore new possibilities in architecture
    on the Orin platform.
  prefs: []
  type: TYPE_NORMAL
- en: Self-evolving Data Engine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the implementation of XNet architecture, we can now initiate data-driven
    iterations to boost the model’s performance. To accomplish this, we first identify
    corner cases on the car and then deploy configurable triggers to the customer
    fleet to collect relevant images. Subsequently, we retrieve images from collected
    data, based on a short description in natural language or an image itself. In
    doing so, we leverage recent advancements in large language models to increase
    the efficiency of dataset curation and annotation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/391d72a6b66e90910e8d16230089d311.png)'
  prefs: []
  type: TYPE_IMG
- en: Data engine helps improve XNet performance
  prefs: []
  type: TYPE_NORMAL
- en: With the XNet architecture and data engine, we have created a scalable and self-evolving
    perception system.
  prefs: []
  type: TYPE_NORMAL
- en: The Future
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The latest release of Xpeng Highway NGP 2.0 unifies highway and city pilot solutions,
    allowing users to drop a pin in a different city and have a smooth experience
    from start to finish. This unification is made possible by XNet, which provides
    a solid foundation for a unified stack across all scenarios. The ultimate goal
    is to enable point-to-point user experience with end-to-end autonomous driving.
  prefs: []
  type: TYPE_NORMAL
- en: In order to make the autonomous driving system end-to-end differentiable, another
    critical missing piece is a machine-learning based planning stack. Learning-based
    planning solutions can be largely divided into imitation learning or reinforcement
    learning approaches. Recent progress in large language models (LLMs) also spells
    great potential for the advancement of this important topic. The following [Github
    repo](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving) is a live
    collection of relevant work in the burgeoning field of end-to-end autonomous driving.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving?source=post_page-----c6e3f1e46ae0--------------------------------)
    [## GitHub - OpenDriveLab/End-to-end-Autonomous-Driving: All you need for End-to-end
    Autonomous Driving'
  prefs: []
  type: TYPE_NORMAL
- en: All you need for End-to-end Autonomous Driving. Contribute to OpenDriveLab/End-to-end-Autonomous-Driving
    development by…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving?source=post_page-----c6e3f1e46ae0--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Divide and conquer reaches 80% of performance with 20% effort. End-to-end approaches
    aim to break the 80% performance ceiling, at potentially much greater cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XNet is an end-to-end perception system and one critical step toward an end-to-end
    full-stack solution. It requires significant engineering effort (80%) according
    to the 80–20 rule.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The large amount of annotation needed for XNet calls for automatic annotation,
    as human annotation is not feasible. The autolabel system can boost efficiency
    by 45000 times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large scale training requires optimization of training on a single machine,
    and scaling out from one machine to multiple machines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XNet deployment on Nvidia Orin platform requires refactoring of transformer
    module.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the unique challenges in deploying mass-production autonomous driving in
    China, please refer to the following link. This was also part of the same invited
    talk at CVPR 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](https://medium.com/@patrickllgc/challenges-of-mass-production-autonomous-driving-in-china-407c7e2dc5d8?source=post_page-----c6e3f1e46ae0--------------------------------)
    [## Challenges of Mass Production Autonomous Driving in China'
  prefs: []
  type: TYPE_NORMAL
- en: And Xpeng’s Answer to Them
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@patrickllgc/challenges-of-mass-production-autonomous-driving-in-china-407c7e2dc5d8?source=post_page-----c6e3f1e46ae0--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: A review of academic research on Transformer-based BEV perception algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://medium.com/towards-data-science/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944](https://medium.com/towards-data-science/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Xpeng 1024 Tech Day 2022: [https://www.youtube.com/watch?v=0dEoctcK09Q](https://www.youtube.com/watch?v=0dEoctcK09Q)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
