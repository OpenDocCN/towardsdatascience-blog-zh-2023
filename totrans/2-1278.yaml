- en: How You Can (and Why You Should) Access Amazon S3 Resources with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-you-can-and-why-you-should-access-amazon-s3-resources-with-python-86462c6a8f97](https://towardsdatascience.com/how-you-can-and-why-you-should-access-amazon-s3-resources-with-python-86462c6a8f97)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Use automation to move data to and from the cloud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@aashishnair?source=post_page-----86462c6a8f97--------------------------------)[![Aashish
    Nair](../Images/23f4b3839e464419332b690a4098d824.png)](https://medium.com/@aashishnair?source=post_page-----86462c6a8f97--------------------------------)[](https://towardsdatascience.com/?source=post_page-----86462c6a8f97--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----86462c6a8f97--------------------------------)
    [Aashish Nair](https://medium.com/@aashishnair?source=post_page-----86462c6a8f97--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----86462c6a8f97--------------------------------)
    ·7 min read·Jan 24, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/206b4eb369a3a357dd723f699da2f412.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Lia Trevarthen](https://unsplash.com/@melodi2?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Simple Storage Service (S3) provides users with cheap, secure, and easy-to-manage
    storage infrastructure. It is viable to move files in and out of S3 buckets with
    the AWS console itself, but AWS offers the option to facilitate these operations
    with code as well.
  prefs: []
  type: TYPE_NORMAL
- en: For Python, the AWS software development kit (SDK) offers boto3, which allows
    users to create, configure, and utilize S3 buckets and objects with code. Here,
    we delve into the basic boto3 functions for S3 resources and consider how they
    can be used to automize operations.
  prefs: []
  type: TYPE_NORMAL
- en: Why Boto3?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You might be wondering if there is any point in learning to use yet another
    tool when you can already access the S3 service through the AWS console. Granted,
    with AWS’s simple and user-friendly user interface (UI), it is easy to move files
    to and from S3 buckets.
  prefs: []
  type: TYPE_NORMAL
- en: However, using the typical click-and-drag approach isn’t viable when operations
    need to scale up. It’s one thing to handle 1-10 files at a time, but would you
    fare with 100s or 1000s of files? Such an undertaking would naturally be time-consuming
    if done manually.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, manual tasks also leave users prone to making mistakes. When moving
    large quantities of data around manually, can you guarantee that you won’t mistakenly
    omit or include the wrong files?
  prefs: []
  type: TYPE_NORMAL
- en: Those that value efficiency or consistency will no doubt see the merit in using
    Python scripts to automate the use of S3 resources.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start with the prerequisites for using boto3.
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Installation**'
  prefs: []
  type: TYPE_NORMAL
- en: To use boto3, you’ll have to install the AWS SDK if you haven’t already with
    the following command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**2\. IAM user**'
  prefs: []
  type: TYPE_NORMAL
- en: You will also need an IAM user account with permission to use S3 resources.
  prefs: []
  type: TYPE_NORMAL
- en: To get a user identity, log in to AWS with the root user account. Head to the
    Identity and Access Management (IAM) section and add a new user. Assign this user
    identity a policy that will grant access to S3 resources. The simplest option
    is to select the “AmazonS3FullAccess” permission policy, but you can find or create
    policies that are more tailored to your needs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7d15f42d09f3e2e9133f7af27e76d55e.png)'
  prefs: []
  type: TYPE_IMG
- en: AmasonS3FullAccess Policy (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: After picking the policy, complete the remaining prompts and create the user
    identity. You should be able to see your newly created identity in the console.
    In this example, we use a user identity called “s3_demo”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b6e864a43d0120b55d948d9ad02a1079.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, click on the user name (not the check box) and go to the security credentials
    tab. In the access key section, select “create access key” and answer all prompts.
    You will then receive your access key and secret access key.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e810b4460031006880daf76ad3ee6e65.png)'
  prefs: []
  type: TYPE_IMG
- en: These keys are required for accessing the S3 resources with boto3, so they will
    naturally be incorporated into the Python script.
  prefs: []
  type: TYPE_NORMAL
- en: Basic commands
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Boto3 comprises functions that can provision and configure various AWS resources,
    but the focus of this article will be on handling S3 buckets and objects. The
    key benefit of boto3 is that it executes tasks like uploading and downloading
    files with a simple one-liner.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a client
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To access S3 resources with boto3 (or any AWS resource), a low-level service
    client needs to be created first.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a client requires inputting the service name, region name, access key,
    and secret access key.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a bucket
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start by creating a bucket, which we will call “a-random-bucket-2023”.
    This can be achieved with the `create_bucket` function.
  prefs: []
  type: TYPE_NORMAL
- en: If you refresh the S3 section on the AWS console, you should see the newly created
    bucket.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4e540bf3df69b26e50fdd3d2b04c5a69.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating a Bucket (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: Listing buckets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To list the available buckets, users can use the `list_buckets` function. This
    returns a dictionary with many key-value pairs. To see just the names of the buckets,
    retrieve the value of the ‘Buckets’ key in the dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/126bcdf1b4db7ff7737dd3f80b7d1623.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: Upload a file to a bucket
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Files can be uploaded with the `upload_file` function. In this case, let’s upload
    “mock_data.xlsx” to the bucket.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth distinguishing the difference between the `Filename` and `Key` parameters.
    `Filename` refers to the name of the file that is being transferred, while `Key`
    refers to the name that is assigned to the object stored in the bucket.
  prefs: []
  type: TYPE_NORMAL
- en: Since “First Mock Data.xlsx” was assigned to the `Key` parameter, that will
    be the name of the object when it is uploaded to the bucket.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/49b5fd4d6469d067446f2b440030d7a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Adding an Object (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: Uploading a Pandas data frame to a bucket
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since we’re working on Python, it’s worth knowing how to upload a Pandas data
    frame directly to the bucket. This can be achieved with the io module. In the
    following snippet, we upload the data frame “df” to the bucket.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5418fe0dc2eb05393e62a53aa96016b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Uploading a Pandas Data Frame (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: List objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The objects in a given bucket can be listed with the `list_objects` function.
  prefs: []
  type: TYPE_NORMAL
- en: The output of the function itself is a large dictionary with a bunch of metadata,
    but the object names can be found by accessing the “Contents” key.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98160b3dcea31cf20ed03b1abbb42d66.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: Download files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Objects can be downloaded from a bucket with the `download_file` function.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Objects can be deleted with the `deleted_object` function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9442266dab87360ca06c05172d0d0922.png)'
  prefs: []
  type: TYPE_IMG
- en: Deleting an Object (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: Deleting buckets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, users can delete buckets with the `delete_bucket` function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3bd6671a5b85e13ccb3eb24213deceb.png)'
  prefs: []
  type: TYPE_IMG
- en: Deleting a Bucket (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: Case Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve explored some of the basic boto3 functions for using S3 resources. However,
    performing a case study is the best way to demonstrate why boto3 is such a powerful
    tool.
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem Statement 1:** We are interested in the books that are being published
    on different dates. Obtain data on published books using the [NYT Books API](https://developer.nytimes.com/docs/books-product/1/overview)
    and upload it to an S3 bucket.'
  prefs: []
  type: TYPE_NORMAL
- en: Once again, we start by creating a low-level service client.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create a bucket that will contain all of these files.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/50b284e8ca13c3c91068df58efad9fcc.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating a Bucket (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to pull the data with the NYT Books API and upload them to the
    bucket. We can perform a data pull for a given date with the following `get_data`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a preview of what the output of the `get_data` function looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/556947ff5f22c2473a6798db4f04c458.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to utilize this function for data collection. One option
    is to use to run this function and upload the output to the S3 bucket every day
    (or use a job scheduler). Another option is to use loops to collect data for books
    published in multiple days.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are interested in books published in the last 7 days, we can parse through
    each day with a loop. For each day, we:'
  prefs: []
  type: TYPE_NORMAL
- en: make a call with the API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: store the response into a data frame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: upload the data frame to the bucket
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These steps are executed with the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a56a0a59a3d6ce5d3225c51cfa247cc2.png)'
  prefs: []
  type: TYPE_IMG
- en: Uploading Files to the Bucket(Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: With just several lines of code, we are able to make multiple API calls and
    upload the responses to the bucket in one well swoop!
  prefs: []
  type: TYPE_NORMAL
- en: At some point, there might be a need to extract some insight from the data uploaded
    to a bucket. It’s natural for data analysts to receive ad hoc requests pertaining
    to the data they collect.
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem statement 2:** Find the book title, author, and publisher of the
    highest-ranking books for all dates and store the results locally .'
  prefs: []
  type: TYPE_NORMAL
- en: With Python, we can store multiple S3 objects into Pandas data frames, process
    the data frames, and load the output into a flat file.
  prefs: []
  type: TYPE_NORMAL
- en: There are two key advantages of using boto3 that are showcased in this case
    study. The first advantage of this method is that it scales well; the difference
    in time and effort needed to upload/download 1 file and 1000 files is negligible.
    The second advantage is that it allows users to seamlessly tie in other processes
    (e.g., data collection, filtering) when moving data to or from S3 buckets.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/dd4343dd7b4376960806899faac2061b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Prateek Katyal](https://unsplash.com/fr/@prateekkatyal?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, this brief boto3 primer has not only introduced you to the basic
    Python commands for managing S3 resources but also shown how they can be used
    to automate processes.
  prefs: []
  type: TYPE_NORMAL
- en: With the AWS SDK for Python, users will be able to move data to and from the
    cloud with greater efficiency and consistency. Even if you are currently content
    with provisioning and utilizing resources with AWS’s UI, you’ll no doubt run into
    cases where scalability is a priority. Knowing the basics of boto3 will ensure
    that you are well prepared for such situations.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs: []
  type: TYPE_NORMAL
