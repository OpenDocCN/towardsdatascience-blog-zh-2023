- en: 7 of the Most Used Feature Engineering Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/7-of-the-most-used-feature-engineering-techniques-bcc50f48474d](https://towardsdatascience.com/7-of-the-most-used-feature-engineering-techniques-bcc50f48474d)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hands-on Feature Engineering with Scikit-Learn, Tensorflow, Pandas and Scipy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dmnkplzr.medium.com/?source=post_page-----bcc50f48474d--------------------------------)[![Dominik
    Polzer](../Images/7e48cd15df31a0ab961391c0d57521de.png)](https://dmnkplzr.medium.com/?source=post_page-----bcc50f48474d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bcc50f48474d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bcc50f48474d--------------------------------)
    [Dominik Polzer](https://dmnkplzr.medium.com/?source=post_page-----bcc50f48474d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bcc50f48474d--------------------------------)
    ·37 min read·Jan 9, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38d0b2c91b74d55357c4d376ef78449e.png)'
  prefs: []
  type: TYPE_IMG
- en: 7 of the most used Feature Engineering Techniques — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Table of content
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Feature engineering describes the process of formulating relevant features
    that describe the underlying data science problem as accurately as possible and
    make it possible for algorithms to understand and learn patterns. In other words:'
  prefs: []
  type: TYPE_NORMAL
- en: Features you provide serve as a way to communicate your own understanding and
    knowledge about the world to your model
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Each feature describes a kind of information “piece”. The sum of these pieces
    allows the algorithm to draw conclusions about the target variables — at least
    if you have a data set that actually contains information about your target variable.
  prefs: []
  type: TYPE_NORMAL
- en: According to the [Forbes magazine](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/?sh=6d63250f6f63),
    Data Scientists spend about 80% of their time collecting and preparing relevant
    data, with the data cleaning and data organizing alone taking up about 60% of
    the time.
  prefs: []
  type: TYPE_NORMAL
- en: But this time is well spent.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'I believe that the quality of the data, as well as the proper preparation of
    the data set features, have a greater impact on the success of a machine learning
    model than any other part of the ML pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7d3fc33cbf40e45965906c0e267892f.png)'
  prefs: []
  type: TYPE_IMG
- en: '*A standard Machine Learning pipeline — Inspired by [*Sarkar et al., 2018*]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'What Forbes magazine considers as “cleaning and organizing” is usually broken
    down into two to three subcategories in the ML pipeline (I have highlighted them
    with a yellow background in the image above):'
  prefs: []
  type: TYPE_NORMAL
- en: '**(1) Data (Pre-)Processing:** The initial preparation of the data — for example,
    smoothing a signal, dealing with outliers, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: (2) **Feature engineering:** Defining input features for our model — e.g., by
    converting an (acoustic) signal to the frequency domain using the fast Fourier
    transform (FFT) allows us to extract crucial information from the raw signal.
  prefs: []
  type: TYPE_NORMAL
- en: (3) **Feature Selection:** Selection of features that have a significant impact
    on the target variable. By selecting the important features and thus reducing
    the dimensionality, we can significantly reduce the modeling costs and increase
    the robustness and performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need Feature Engineering?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Andrew Ng frequently advocates a so-called ***data-centric approach***, which
    emphasizes the importance of selecting and curating data, rather than simply trying
    to collect more and more data. The goal is to ensure that the data is of high
    quality and relevance to the problem being addressed and to continually improve
    the data set through data cleaning, feature engineering and data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need feature engineering and a data-centric approach when we have
    Deep Learning?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This approach is particularly useful for use cases where it is costly or otherwise
    difficult to collect large amounts of data. [Brown, 2022] This might be the case
    when the data is difficult to access, or when there are strict regulations or
    other barriers to collecting and storing large amounts of data, e.g.:'
  prefs: []
  type: TYPE_NORMAL
- en: '***In the manufacturing sector***, equipping production facilities with comprehensive
    sensor technology and connecting them to databases is expensive. As a result,
    many plants do not yet collect data at all. Even when machines are able to collect
    and store data, they are rarely connected to a central data lake.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I***n the supply chain context***, every company has only a certain number of
    orders that it processes every day. So if we want to predict the demand for a
    product that is in very irregular demand, we sometimes have only a few data points
    available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In such areas, feature engineering probably has the greatest leverage to increase
    the performance of the models. Here, the creativity of the engineers and data
    scientists is required to enhance the data set quality to a sufficient level.
    The process is rarely straightforward, but rather experimental and iterative.
  prefs: []
  type: TYPE_NORMAL
- en: When humans analyze data, they often use their past knowledge and experience
    to help them understand patterns and make predictions. For example, if someone
    is trying to estimate the temperature in different countries, they might consider
    the location of the country relative to the equator, since they know that temperatures
    tend to be higher near the equator.
  prefs: []
  type: TYPE_NORMAL
- en: However, a machine learning model does not have the same inherent understanding
    of concepts and relationships as a human does. It can only learn from the data
    it is given. Therefore, any background information or context that a human would
    use to solve a problem must be explicitly included in the data set in a numerical
    form.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62082a93409235493b84f426ecb9e547.png)'
  prefs: []
  type: TYPE_IMG
- en: Average yearly temperature per country [[Wikimedia](https://commons.wikimedia.org/wiki/File:Average_yearly_temperature_per_country.png)]
  prefs: []
  type: TYPE_NORMAL
- en: So what data do we need to make the model smarter than a human?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We could use the Google Maps API to find the location coordinates (longitude
    and latitude) of each country. Additionally, we can gather information about the
    altitude of the region and the distance from the nearest body of water for each
    country. By collecting this extra data, we hope to identify and consider possible
    factors that could affect the temperature in each country.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we have collected some data that might have an impact on the temperature,
    what next?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Once we have enough data that describes the characteristics of the problem,
    we still have to make sure that the computer can understand the data. Categorical
    data, dates, etc. must be converted into numerical values.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I am describing a few commonly used techniques for preparing
    raw data. Some techniques are used to convert categorical data into numerical
    values that can be understood by the machine learning model, such as **encoding**
    and **vectorizing**. Other techniques are used to address the distribution of
    the data, such as **transformers** and **binning**, which can help to normalize
    or standardize the data in some way. Still, other techniques are used to reduce
    the dimensionality of the dataset by generating new features, such as **hashing**
    and **principal component analysis (PCA).**
  prefs: []
  type: TYPE_NORMAL
- en: Try it yourself…
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'If you want to follow the article and try the methods yourself, you can use
    the repo below, which contains the code snippets in a Jupyter Notebook and the
    data sets used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/polzerdo55862/7-feature-engineering-techniques?source=post_page-----bcc50f48474d--------------------------------)
    [## GitHub - polzerdo55862/7-feature-engineering-techniques'
  prefs: []
  type: TYPE_NORMAL
- en: You can't perform that action at this time. You signed in with another tab or
    window. You signed out in another tab or…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/polzerdo55862/7-feature-engineering-techniques?source=post_page-----bcc50f48474d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature encoding is a process used to transform categorical data into numerical
    values that can be understood by ML algorithms. There are several types of encoding,
    including label encoding and one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/362d8f5273c387cbfdfb59a5c97c44de.png)'
  prefs: []
  type: TYPE_IMG
- en: Label and Hot Encoding — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '**Label encoding** involves assigning a numeric value to each categorical value.
    This can be effective if there is an inherent order to the categorical values,
    such as grades from A to F, which can be encoded as numeric values from 1 to 5
    (or 6). However, if there is no inherent order to the categorical values, label
    encoding may not be the best approach.'
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can use **One-hot encoding** to transform categorical values
    into numerical values. In one-hot encoding, the column of categorical values is
    split into several new columns, one for each unique categorical value.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if the categorical values are grades from A to F:'
  prefs: []
  type: TYPE_NORMAL
- en: we would have five new columns, one for each grade.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each row in the dataset would have a value of 1 in the column corresponding
    to its grade and 0 in all the other columns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This results in a so-called sparse matrix, where most of the values are 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The disadvantage of one-hot encoding is that it can significantly increase the
    size of the dataset, which can be a problem if the column you want to encode contains
    hundreds or thousands of unique categorical values.
  prefs: []
  type: TYPE_NORMAL
- en: In the following I am using Pandas, Scikit-learn or Tensorflow to apply label
    and one-hot encoding to your data set. For the example below, we are using the
    ***Census Income data set:***
  prefs: []
  type: TYPE_NORMAL
- en: '**Data set: Census Income** [License: [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/)]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/datasets/census+income](https://archive.ics.uci.edu/ml/datasets/census+income)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/datasets/uciml/adult-census-income](https://www.kaggle.com/datasets/uciml/adult-census-income)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The Census Income Data set describes the income of individuals in the United
    States. It includes their age, sex, marital status and other demographic information
    as well as their annual income, which is divided into two categories: over $50,000
    or under $50,000.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For the labelling example, we are using the “education” column of the census
    data set, which describes the highest level of education achieved by individuals
    within the population. It contains information such as whether an individual has
    completed high school, completed college, earned a graduate degree, or some other
    form of education.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s load the data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 1.1 Label Encoding using Scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Label encoding is the simplest way to convert categorical values into numerical
    values. It is a simple process of assigning a numerical value to each category.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/225efd7f24c011459e009239f752cf0a.png)'
  prefs: []
  type: TYPE_IMG
- en: Label Encoding— Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find suitable libraries in Pandas, Scikit-Learn and Tensorflow. I am
    using the Scikit-Learns Label Encoder function. It is randomly assigning integers
    to the unique categorical values, which is the simplest way of encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/64fe09474dff60fba9a6c2906d87bf08.png)'
  prefs: []
  type: TYPE_IMG
- en: This way of encoding can cause problems for some algorithms because the assigned
    integers do not necessarily reflect any inherent order or relationship between
    the categories. For example, in the case described above, the algorithm may assume
    that the categories **Doctorate** (10) and **HS-grad** (11) are more similar to
    each other than categories **Doctorate** (10) and **Bachelor** (9) and that **HS-grad**
    (11) is “higher” than **Doctorate** (10).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/50630a5229193aaf7522bd4887c793c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Label Encoding result interpretation — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have some knowledge about the specific domain or subject matter of a
    data set, we can use it to ensure that the label encoding process reflects any
    inherent order. For our example, we could try to label the education levels considering
    the order in which different degrees are obtained, e.g.:'
  prefs: []
  type: TYPE_NORMAL
- en: Doctorate is a higher academic degree compared to a Master's and Bachelor's.
    The Master's is higher than the Bachelor's.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec219c83f45cca3dc8740d7fce662877.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To apply this manual mapping to the data set, we can use the *pandas.map* function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0c06e86b3f7444f28c658bf97d102863.png)'
  prefs: []
  type: TYPE_IMG
- en: But how does this affect the model-building process?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Let''s build a simple linear regression model with the labeled encoded data:**'
  prefs: []
  type: TYPE_NORMAL
- en: For the figure below, the target variable is defined as the **probability that
    an individual has an income of more than $50,000.**
  prefs: []
  type: TYPE_NORMAL
- en: '**In the left figure**, the categorical values (such as “Bachelor” and “Doctorate”)
    have been assigned randomly to numerical values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**In the right figure**, the numerical values assigned to the categorical values
    reflect the order in which the degrees are typically obtained, with higher education
    degrees being assigned higher numerical values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: → **The right figure** shows a clear correlation between education level and
    income, which could be represented by a simple linear model.
  prefs: []
  type: TYPE_NORMAL
- en: → **In contrast, the left figure** shows a relationship between the education
    attribute and the target variable, which would require a more complex model to
    accurately represent. This could affect the interpretability of the results, increase
    the risk of overfitting and increase the computational effort required to fit
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3963001cfa9989a8450abc7b2c845a1a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Label Encoding: Linear Regression Model trained on ordered and unordered “education”
    feature — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: What do we do when values have no inherent order or we don’t have enough information
    to map it?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If the categorical values do not have an inherent order at all, it may be better
    to use a method of encoding that converts the categories into a set of numeric
    variables ***without introducing any bias***. One-hot encoding is one suitable
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 One-Hot Encoding using Scikit-learn, Pandas and Tensorflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One-hot encoding is a technique for converting categorical data into numerical
    data. It does this by creating a new binary column for each unique category in
    the data set and assigning a value of 1 to rows that belong to that category and
    a value of 0 to rows that do not.
  prefs: []
  type: TYPE_NORMAL
- en: → This process helps to avoid introducing bias into the data by not assuming
    any inherent order between the categories.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cccce5dc70f46f366bab6dd905058035.png)'
  prefs: []
  type: TYPE_IMG
- en: One-Hot Encoding — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: The process of One-Hot Encoding is pretty straightforward. We could simply implement
    it by ourselves or use one of the existing functions. Scikit-learn has the **.preprocessing.OneHotEncoder()**
    function**, Tensorflow the .one_hot()** function, and Pandas the **.get_dummies()**
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pandas.get_dummies()**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Sklearn.preprocessing.LabelBinarizer()**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Sklearn.preprocessing.OneHotEncoder()**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The problem with one-hot encoding is that it can lead to large and sparse datasets
    with high dimensionality.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Using one-hot encoding to convert a categorical feature with 10,000 unique values
    into numerical data would result in the creation of 10,000 new columns in the
    data set, each representing a different category. This can be a problem when working
    with large data sets, as it can quickly consume a lot of memory and computational
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: '*If memory and computer power are limited, it may be* ***necessary to reduce
    the number of features*** *in the data set to avoid running into memory or performance
    issues.*'
  prefs: []
  type: TYPE_NORMAL
- en: How can we reduce dimensionality to save memory?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When doing this, it is important to try to minimize the loss of information
    as much as possible. This can be achieved by carefully selecting which features
    to retain or remove, by using techniques such as ***feature selection or dimensionality
    reduction*** to identify and remove redundant or irrelevant features. [Sklearn.org][Wikipedia,
    2022]
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article I am describing two possible ways how to reduce the dimensionality
    of your data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature Hashing** — see section 2\. Feature Hashing'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Principal Component Analysis (PCA)** — see section 7\. PCA'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loss of information vs. speed vs. memory
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There is probably not one “perfect” solution for reducing the number of dimensions
    in your data set. One method may be faster but may result in the loss of a lot
    of information, while the other method preserves more information but requires
    a lot of computing resources (which may also lead to memory issues).
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Feature Hashing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature hashing is primarily a dimensionality reduction technique and is often
    used in Natural Language Processing. However, **hashing can also be useful when
    we want to vectorize categorical features with several hundred and thousand unique
    categories**. With hashing, we can limit the increase of dimensionality by assigning
    several unique values to the same hash value.
  prefs: []
  type: TYPE_NORMAL
- en: '***→ Hashing is thus a low-memory alternative to OneHotEncoding and other feature
    vectorizing methods.***'
  prefs: []
  type: TYPE_NORMAL
- en: Hashing works by applying a hash function to the features and using the hash
    values directly as indices, rather than building a hash table and looking up indices
    in it individually. The implementation in Sklearn is based on Weinberger [Weinberger
    et al., 2009].
  prefs: []
  type: TYPE_NORMAL
- en: 'We can break it down into 2 steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1:** the categorical values are first converted to a hash value using
    a hash function. The implementation in Scikit-learn uses the 32-bit variant of
    MurmurHash3 for this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/04b7b94232f8b2dc4dab62900efcbad2.png)'
  prefs: []
  type: TYPE_IMG
- en: Hashing Trick — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f4a61827e67a6d665eb7d96acfd54f1c.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 2:** Next, we reduce the dimensionality by applying a ***mod function***
    to the feature values. Using the mod function, we calculate the remainder after
    dividing the hash value by ***n_features*** (the number of features of the output
    vector).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*It is advisable to use a power of two as the* ***n_features*** *parameter;
    otherwise, the features will not map evenly to the columns.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example: Census Income data set →Encoding the Education column**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The “Education” column of the census income dataset contains 16 unique values:'
  prefs: []
  type: TYPE_NORMAL
- en: With one-hot encoding, 16 columns will be added to the dataset, each representing
    one of the 16 unique values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: → To reduce the dimensions, we set ***n_features*** to 8.
  prefs: []
  type: TYPE_NORMAL
- en: This inevitably leads to “collisions”, i.e. that different categorical values
    are mapped to the same hash columns. So in other words, we are assigning different
    values to the same bucket. Similar to what we are doing in ***Binning/Bucketizing***
    (see section 3\. Binning/Bucketizing). In feature hashing, we deal with collisions
    by simply chaining values that are assigned to the same bucket and storing them
    in a list (known as “separate chaining”).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd71d9f753687bd02e1655966d778d1d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Feature Hashing: Reducing the dimensionality by calculating the remainder as
    a new column index — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/dcafdc55149fa7ce89065cd03e81d4c8.png)'
  prefs: []
  type: TYPE_IMG
- en: A function that is doing the just described steps for us is the ***HashingVectorizer***
    function from ***Scikit-learn***.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Feature Hashing using Scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: sklearn.feature_extraction.text.HashingVectorizer
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4b4d06da441cbc3657b00e7a18f9efc9.png)'
  prefs: []
  type: TYPE_IMG
- en: 3\. Binning / Bucketizing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Binning is used for both categorical and numerical data. As the name suggests,
    the goal is to map the values of the features to “bins” and replace the original
    value with the value that represents the bin.
  prefs: []
  type: TYPE_NORMAL
- en: '**For example**, if we had a dataset with values ranging from 0 to 100 and
    we wanted to group those values into bins of size 10, we might create bins for
    values 0–9, 10–19, 20–29 and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: → In this case, the original values would be replaced with the value that represents
    the bin to which they belong, such as 10, 20, 30, etc. This can help visualize
    and analyze the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are reducing the number of unique values in the data set, it can help
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: prevent overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: increase the robustness of the model and mitigate the influence of anomalies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: reduce the model complexity and the required resources to train the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Systematic binning can help the algorithm to detect underlying patterns more
    easily and efficiently. It is especially helpful if we can already form a hypothesis
    before we are defining the bins.
  prefs: []
  type: TYPE_NORMAL
- en: 'Binning can be used for both numeric and categorical values, e.g.:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e5207be783db10ba4c34fbb715886797.png)'
  prefs: []
  type: TYPE_IMG
- en: Bucketizing for categorical and numerical features — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following, I describe how this might look for numeric and categorical
    attributes using three examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Numeric —** How binning can be used when building a streaming recommender—
    a use case I found in the book ***Feature Engineering for Machine Learning***
    by [Zheng, Alice, and Amanda Casari. 2018]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Numeric —** Census Income Data set: Binning applied to the “Age” column'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Categorical** —Binning in Supply Chain: Assign countries to bins, depending
    on the target variable'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Example 1: Streaming Recommender System — How popular is a song or video?**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you want to develop a recommender system, it is important to assign numerical
    values to the relative popularity of songs. One of the most significant attributes
    is the number of clicks, which is how often a user has listened to a song.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**However**, it is not necessarily true that a user who listens to a song 1000
    times likes it 20 times as much as someone who has heard it 50 times. Binning
    can help prevent overfitting. Therefore, it can be beneficial to divide the number
    of clicks of songs into categories.: [Zheng, Alice and Amanda Casari. 2018]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Click count >=10: Very popular'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Click count >=2: popular'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Click count <2: neutral, no statement possible'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Example 2: Age-Income Relation**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For the second example, I am again using the census income data set. The goal
    is to group individuals into age buckets.
  prefs: []
  type: TYPE_NORMAL
- en: '***Hypothesis —*** The idea is that the specific age of a person may not have
    a significant impact on their income, but rather the stage of life they are in.
    For example, a person who is 20 and still in school may have a different income
    compared to a 30-year-old young professional. Similarly, a person who is still
    working full-time at age 60 may have a different income compared to someone who
    has retired at age 70, while there is probably not much difference in income if
    the person is 40 or 50.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Bucketizing using Pandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To bucketize the data by age, I am defining three “buckets”:'
  prefs: []
  type: TYPE_NORMAL
- en: '**young** — 28 and younger'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**middle-aged** — 29 to 59'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**old-aged** — 60 and older'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6c081c192794d26340a886fb0f72bb6a.png)'
  prefs: []
  type: TYPE_IMG
- en: 3.2 Bucketizing using Tensorflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tensorflow provides a module called ***feature columns*** that contains a range
    of functions designed to help with the pre-processing of raw data. Feature Columns
    are functions that organize and interpret raw data so that a machine learning
    algorithm can interpret it and use it to learn. [[Google Developers, 2017](https://www.youtube.com/watch?v=d12ra3b_M-0)]
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow’s ***feature_column.bucketized_column*** API provides a way to bucketize
    numeric data. This API takes in a numeric column and creates a bucketized column
    based on the specified boundaries. The input numeric column can be either a continuous
    or discrete value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9602f985e76ce74320fea36746c0e35e.png)'
  prefs: []
  type: TYPE_IMG
- en: We can do the same by using the ***KBinsDiscretizer*** from ***Scikit-learn.***
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Bucketizing using Scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/dfc3604aa62472fed9992e138200887e.png)'
  prefs: []
  type: TYPE_IMG
- en: One last example for categorical values …
  prefs: []
  type: TYPE_NORMAL
- en: There is not one universally best way to represent categorical variables numerically.
    The appropriate method to use, depends on the specific use case and the information
    that you want to convey to the model. Different methods of encoding can highlight
    different aspects of the data and it is important to choose the method that best
    suits the needs of your particular use case.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 3: Bucketizing categorical values — e.g. Countries**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***Supply Chain*** — When assessing the reliability of a supplier/subcontractor,
    things like region, climate and type of transport can influence the accuracy of
    the delivery time. In our resource planning system, we usually store the supplier’s
    country and city. Since some countries are similar in some aspects and not at
    all in others, it may make sense to highlight aspects that are relevant to the
    use case.'
  prefs: []
  type: TYPE_NORMAL
- en: If we want to highlight the distance between countries and group countries in
    the same region, we could define bins for continents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we are more interested in pricing or possible revenues, the GDP per Capita
    is probably more important → Here we could group “high-cost” countries like USA,
    Germany and Japan in one bin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we want to highlight general weather conditions, we could put countries in
    the north like Canada and Norway in the same bin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/c08c19c1cdaed647e03a0f1fead8c953.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Example: Bucketizing of countries using different attributes — Image by the
    author'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformation techniques are methods used to change the form or distribution
    of a data set. Some common transformation techniques, such as the **log** or **Box-Cox
    functions**, are used to convert data that is not normally distributed into a
    form that is more symmetrical and follows a bell curve shape. These techniques
    can be useful when the data needs to meet certain assumptions that are required
    by certain statistical models or techniques. The specific transformation method
    that is used may depend on the desired outcome and the characteristics of the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1b5984dfc4621d1777a4a90f640fef6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Transformers: Transform the value distribution of your feature — Image by the
    author'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Log-Transformer using Numpy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Log transformers are used to change the scale of the data by applying a logarithmic
    function to each value. This transformation is often used to convert highly skewed
    data into data that more closely resembles a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Skewed data is by no means something unusual, there are various situations
    where data is naturally or artificially skewed, e.g.:'
  prefs: []
  type: TYPE_NORMAL
- en: The frequency with which words are used in a language follows a pattern known
    as ***Zipf’s law***
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How humans perceive different stimuli follows a pattern described by ***Stevens’
    power function.***
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distribution of the data is not symmetrical and may have a long tail of
    values that are much larger or smaller than the majority of the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e825b6f5da69b99e13d63c58fe9b0ce5.png)'
  prefs: []
  type: TYPE_IMG
- en: Normal vs. Power Law Distribution — Image by the author (Inspired by [[Geeksforgeeks,
    2022]](https://www.geeksforgeeks.org/box-cox-transformation-using-python))
  prefs: []
  type: TYPE_NORMAL
- en: Certain types of algorithms may struggle to handle this type of data and may
    produce less accurate or reliable results. Applying a power transformation, such
    as the Box-Cox or log transformation, can help to adjust the scale of the data
    and make it more suitable for these algorithms to work with. [[Geeksforgeeks,
    2020]](https://www.geeksforgeeks.org/box-cox-transformation-using-python)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at how the transformers affect real-world data. Therefore
    I am using the World Population, Online News Popularity and Boston Housing data
    sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Set 1: World Population** [License: [CC BY 3.0 IGO](https://population.un.org/wpp/Download/Standard/CSV/)]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[https://population.un.org/wpp/Download/Standard/CSV/](https://population.un.org/wpp/Download/Standard/CSV/)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The dataset contains the population of every country in the world. [United Nations,
    2022]
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Data Set 2: Online News Popularity** [License: [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/)]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/datasets/online+news+popularity](https://archive.ics.uci.edu/ml/datasets/online+news+popularity)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This dataset summarizes a heterogeneous set of features about articles published
    by Mashable in a period of two years. The goal is to predict the number of shares
    in social networks (popularity).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Data set 3: Boston Housing Dataset** [License: [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/)]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Boston Housing data set was collected by the U.S. Census Bureau in the late
    1970s and early 1980s. It contains information on the median values of homes in
    the Boston, Massachusetts area. The set includes 13 attributes, such as the average
    number of rooms per house, average distance to employment centers and property
    tax rate, as well as the median value of homes in the area.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The following function helps to plot the distributions before and after transforming
    the data sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following, I am not only testing how the data looks like, after the
    transformation, I am also interested in how this could affect the model-building
    process. I am using Polynomial Regression to build simple regression models and
    compare the performance of the models. The input data for the models is just 2-dimensional:'
  prefs: []
  type: TYPE_NORMAL
- en: for the ***Only News Popularity*** ***data set*** we try to build a model which
    is predicting the “shares” based on the “n_tokens” of the online article — so
    we try to build a model which predicts the popularity of an article based on the
    number of tokens/length of the article
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for the ***World Population data set,*** we build a simple model which is predicting
    the population based on only one input feature, the area size of the country
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example 1: World Population data set — Area size of countries'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e33c839444d0c3ba79435b1c5f4a2ca3.png)'
  prefs: []
  type: TYPE_IMG
- en: In the graph, you can see that the original distribution, which was skewed to
    the left, has been transformed into a distribution that is more symmetrical.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try the same for the Online news popularity data set:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 2: Online news popularity data set — article popularity based on article
    length'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2608171d4f1356e0d2d2b4155e01f4ce.png)'
  prefs: []
  type: TYPE_IMG
- en: This example shows the effect even better. The skewed data is transformed into
    almost “perfect” normally distributed data.
  prefs: []
  type: TYPE_NORMAL
- en: 'But can this have a positive impact on the model-building process? For this
    purpose, I create polynomial models for the raw data and the transformed data
    and compare their performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: I am aware this simple 2-dimensional example isn’t a representative example,
    as it is not possible to accurately predict the target variable using only one
    attribute. However, let’s see if the transformation is changing anything at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try it first with the Online News Popularity data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/471a56e4397fe0306a9a499eec7319af.png)'
  prefs: []
  type: TYPE_IMG
- en: Performance of a Polynomial Regression Model trained on raw and transformed
    data — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: The model trained on the transformed data is doing slightly better. Even though
    the difference in Absolute Error from 3213 to 3202 is not particularly large,
    it does indicate that transforming the data can have a positive effect on the
    training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'By plotting the transformed data and building the model, we can see, that the
    data is shifted to the right. This gives the model a little more “room” to fit
    the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the just defined function to plot the polynomial model that showed the
    best performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/852001150fc00eacc49a30643730de5d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Polynomial Regression Model: Shares of online news— Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try the same with the World Population Data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e265431b970a725198c0f03cfeb3d858.png)'
  prefs: []
  type: TYPE_IMG
- en: Performance of a Polynomial Regression Model trained on raw and transformed
    data — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the model performance is quite different between the raw and
    transformed data sets. While the models based on the raw data perform significantly
    better at low polynomial degrees, the models trained on the transformed data perform
    better when we build models with a higher polynomial degree.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a83d1f0022920430427c62acdcf8c410.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Polynomial Regression Model: Country Population — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just for comparison, this is the linear model, that showed the best performance
    for the raw data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c447df2c60debdc6908c69847bbde25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Polynomial Regression Model: Country Population — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Box-Cox Function using Scipy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another very popular transformer function is the Box-Cox function which belongs
    to the group of power transformers.
  prefs: []
  type: TYPE_NORMAL
- en: Power transformers are a family of parametric transformations that aim to transform
    any distribution of data into a data set that is normally distributed and minimize
    variance and skewness. [[Sklearn.org](https://scikit-learn/stable/modules/feature_extraction.html)]
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this flexible transformation, the Box-Cox function is defined as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/32b9f01572d327e74be6f3a372322a38.png)'
  prefs: []
  type: TYPE_IMG
- en: To use the Box-Cox function, we must determine the transformation parameter
    lambda. If you do not manually specify a lambda, the transformer tries to find
    the best lambda by maximizing the likelihood function. [[Rdocumentation.org]](https://www.rdocumentation.org/packages/EnvStats/versions/2.7.0/topics/boxcox)
    A suitable implementation can be found with the function [***scipy.stats.boxcox***](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE: The Box-Cox function must be used on data that is greater than zero.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Box-Cox transformation is more flexible than a log transformation because it
    can produce a variety of transformation shapes, including linear, quadratic and
    exponential, which may better fit the data. Additionally, Box-Cox transformations
    can be used to transform data that is both positively and negatively skewed, whereas
    a log transformation can only be used on positively skewed data.
  prefs: []
  type: TYPE_NORMAL
- en: What is the lambda parameter used for?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The parameter lambda (λ) can be adjusted to tailor the transformation to the
    characteristics of the data being analyzed. A value of 0 for lambda produces a
    log transformation, while values between 0 and 1 create increasingly “*strong*”
    transformations. If lambda is set to 1, the function does not perform any transformation
    on the data.
  prefs: []
  type: TYPE_NORMAL
- en: To show that the box-cox function can convert data that is not only skewed to
    the right, I am also using the ***AGE*** (“proportion of owner-occupied units
    built before 1940”) attribute from the Boston Housing data set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a63ad55536aced3c7f841022309b7a15.png)'
  prefs: []
  type: TYPE_IMG
- en: 5\. Normalize / Standardize
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Normalizing and Standardizing are important preprocessing steps in Machine Learning.
    They can help algorithms to converge faster and can even increase the model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '**5.1 Normalize and Standardize using Scikit-learn**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '***Scikit-learn’s MinMaxScaler*** scales features to a given range. It transforms
    features by scaling each feature to a given range between 0 and 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Scikit-learn’s StandardScaler*** transforms data to have a mean of 0 and
    a standard deviation of 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3f45f8cfcf055c799166626b0d8b0cb3.png)'
  prefs: []
  type: TYPE_IMG
- en: 6\. Feature Crossing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feature crossing is the process of connecting multiple features from a dataset
    to create a new feature. This can include combining data from other sources to
    emphasize existing correlations.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are no limits to creativity. First of all, it makes sense to make known
    correlations apparent by combining the attributes correctly, e.g.:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Example 1: Predicting the price of an apartment***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let's say we have a series of apartments we want to sell and have the technical
    blueprints and thus the dimensions of the apartments available.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'To determine a reasonable price for the apartments, the specific dimensions
    are probably not as important as the total area: whether a room is 6 m * 7 m or
    5.25 m * 8 m is not as important as the fact that the room has 42 m². If I only
    have the dimensions a and b, it makes sense to add the area as a feature as well.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/6e69dbf95fa81c057d24d85a30aa5dd3.png)'
  prefs: []
  type: TYPE_IMG
- en: Predicting the price of an apartment — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '***Example 2: Use technical know-how —Predicting the energy consumption of
    milling machines***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A few years back, I was working on a regression model that would allow me to
    predict the energy consumption of a milling machine.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When implementing a solution for a specific domain, it is always worthwhile
    to look at the available literature. Since people were interested in being able
    to calculate the power requirements of a cutting machine for minimum 50 years,
    there are 50 years of research we can use.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Even if the existing formulas are only suitable for a rough calculation, we
    can use the know-how about identified correlations between attributes and the
    target variable, the energy consumption. In the figure below you can see some
    known relationships between the power demand and the variables.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: → We can make it easier for our model by highlighting these known relationships
    as features. We could for example cross the **width of cut b** and the **cutting
    depth h** (to calculate the **cross-sectional area A**) and define it as a new
    feature. This could help the training process, especially if we are using less
    complex models.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/f6cd19012611bdb5c0db0aba575b8f7a.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculation of the power requirement of a milling machine — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: But we don’t just use it to prepare our dataset, some algorithms use Feature
    Crossing as a fundamental part of how they work.
  prefs: []
  type: TYPE_NORMAL
- en: Some ML methods and algorithms, already use feature crossing by default
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Two well-known ML techniques that use feature crossing are **polynomial regression**
    and the **kernel trick (e.g., in combination with support vector regression)**.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Feature Crossing in Polynomial Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scikit-learn does not contain a function for polynomial regression, we create
    a pipeline out of:'
  prefs: []
  type: TYPE_NORMAL
- en: a feature transformation step and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a linear regression model-building step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By combining and exponentiating features we generate several new features, which
    makes it possible to represent also non-linear relationships between the output
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say we want to build a regression model with a 2-dimensional input matrix
    X and the target variable y. Unless specifically defined, the feature transformation
    function (*sklearn.PolynomialFeatures*) transforms the matrix as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ffcb080c2e0517e1830e59c81a95e586.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can see that the new matrix contains four new columns. The attributes are
    not only potentiated but also mutually crossed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/01f404e97b6412c878d93dcacc03bca7.png)'
  prefs: []
  type: TYPE_IMG
- en: This way we can model any imaginable relationship, we just have to choose the
    right polynomial degree.
  prefs: []
  type: TYPE_NORMAL
- en: The following example shows a polynomial regression model (polynomial degree=5)
    which is predicting the median value of owner-occupied homes in Boston based on
    the attribute LSTAT (*percentage of the population that has lower socio-economic
    status*).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b0eea2dbbc4d0ac2867891fd248326c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 6.2 Feature Crossing and the Kernel-Trick
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another example where feature crossing is already inherently used is the kernel
    trick, for example in Support Vector Regression. Through a kernel function, we
    transform the data set into a higher dimensional space, which makes it possible
    to easily separate classes from each other.
  prefs: []
  type: TYPE_NORMAL
- en: For the following example, I am generating a 2-dimensional data set, which is
    pretty hard to separate in a 2-dimensional space, at least for linear models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/72c71499c8e7e1c959185d9c066becc8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By using the kernel trick, we generate a 3rd dimension. For this example, the
    kernel function is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/59851e3599b47311b9334454a60aa526.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c03994234d41a1deb44a510445014f3a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Kernel-trick: A 2-dimensional dataset transferred to a 3-dimensional space
    — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: The three-dimensional space allows the data to be separated using a simple linear
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Principal Component Analysis (PCA)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Principal Component Analysis, or PCA, reduces the dimension of a dataset by
    creating a set of new features derived from the raw features. Thus, similar to
    hashing, we reduce the complexity of the dataset and thus the computational effort
    required.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, it can help us visualize the data set. Data sets with multiple
    dimensions can be visualized in a lower-dimensional representation, while still
    preserving as much of the original variation as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ll leave out a more in-depth explanation of how it works in this section,
    as there are already some excellent sources on the subject, e.g.:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Josh Starmer: StatQuest: Principal Component Analysis (PCA), Step-by-Step](https://www.youtube.com/watch?v=FgakZw6K1QQ)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Sebastian Raschka: Principal Component Analysis in 3 simple steps](https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#pca-and-dimensionality-reduction)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As a quick summary, here are the 4 most important steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Standardize the data:** Subtract the mean from each feature and scale the
    features to unit variance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Calculate the covariance matrix of the standardized data:** This matrix will
    contain the pairwise covariances between all of the features.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Compute the eigenvectors and eigenvalues of the covariance matrix:** The
    eigenvectors determine the directions of the new feature space and represent the
    principal components and the eigenvalues determine their magnitude.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Select the principal components:** Select the first few principal components
    (usually the ones with the highest eigenvalues) and use them to project the data
    onto a lower-dimensional space. You can choose the number of components to keep,
    based on the amount of variance you want to preserve or the number of dimensions
    you want to reduce the data to.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/498c6f04306488412362e4af1c101fd3.png)'
  prefs: []
  type: TYPE_IMG
- en: How PCA works — Image by the author (inspired by [Zheng, Alice, and Amanda Casari,
    2018])
  prefs: []
  type: TYPE_NORMAL
- en: I’ll show you how you can use PCA to create new features using the ***iris data
    set.***
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Set: Iris Data Set** [License: [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/)]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/datasets/iris](https://archive.ics.uci.edu/ml/datasets/iris)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/datasets/uciml/iris](https://www.kaggle.com/datasets/uciml/iris)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Iris dataset is a dataset containing information about three species of
    Iris flowers (Iris setosa, Iris virginica and Iris versicolor). It includes data
    on the length and width of the sepals and petals of the flowers in centimeters
    and contains 150 observations.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So first, let's load the data set and have a look at the distribution of the
    data in the four dimensions (Sepal Length, Sepal Width, Petal Length, Petal Width)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/565f40c23d8c80f5f7a3b8bc8ada9a38.png)'
  prefs: []
  type: TYPE_IMG
- en: You can already see from the distribution that the individual classes can be
    distinguished relatively well based on the PetalWidth and PetalLength. In the
    other two dimensions, the distributions overlap strongly.
  prefs: []
  type: TYPE_NORMAL
- en: 'This suggests that the PetalWidth and PetalLength are probably more important
    for our model than the other two dimensions. So if I were only allowed to use
    2 dimensions as features for my model, I would take these two. If we plot these
    two dimensions, we get the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03dfabd4f17e8b30358855f01ecb3207.png)'
  prefs: []
  type: TYPE_IMG
- en: Not too bad, we see that by choosing the “most important” attributes we can
    reduce the dimension and lose as little information as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA goes one step further, it centers and scales the data and projects the
    data onto newly generated dimensions. This way we are not bound to the existing
    dimensions. Similar to a 3D representation of a dataset that we rotate in space
    until we find an orientation that allows us to separate the classes as easily
    as possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/309a65a0181f700207d68818f23459f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Iris Data set visualized in a 3d plot — Image by the Author
  prefs: []
  type: TYPE_NORMAL
- en: Overall, then, PCA transforms, scales and rotates the data until a new set of
    dimensions (the principal components) is found that capture the most important
    information of the original data.
  prefs: []
  type: TYPE_NORMAL
- en: A suitable function to apply PCA to your data set can be found in Scikit-learn,
    ***sklearn.decomposition.PCA***.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 PCA using Scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: If you compare the plot of the first two principal components (PC1 and PC2)
    with the two-dimensional plot of PetalWidth and PetalLength, you can see that
    the most important information in the data set is preserved. In addition, the
    data is centered and scaled.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96848acddd816403c43292e542b8bd07.png)'
  prefs: []
  type: TYPE_IMG
- en: PCA1 and PCA2 of the transformed Iris data — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The techniques described in the article can be applied to any type of data,
    but they can not replace methods that are specific to the field you are working
    in.
  prefs: []
  type: TYPE_NORMAL
- en: A good example is the field of acoustics. Let’s assume you have no idea about
    acoustics and signal processing ….
  prefs: []
  type: TYPE_NORMAL
- en: How would you process the airborne sound signal and what features would you
    extract from it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Would you think of decomposing the signal into individual spectral components
    to understand the composition of the signal? Probably not. Fortunately, someone
    before you has asked the same question and defined some suitable transformation
    methods, such as the Fast Fourier Transform (FFT).
  prefs: []
  type: TYPE_NORMAL
- en: Standing on the shoulders of giants
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Even though the technology is advanced, that doesn’t mean we can’t use insights
    that were made decades ago. We can learn from these past experiences and use them
    to make our model-building process more efficient. So it’s always a good idea
    to spend some time to understand the field your data is from if you want to create
    models that are effective at solving real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: '**Continue reading…**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'If you enjoyed reading and want to continue reading about Machine Learning
    concepts, algorithms and applications, you can find a list of my related articles
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dominik Polzer](../Images/7c3581b08252e49c58af78556cd5f714.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Dominik Polzer](https://dmnkplzr.medium.com/?source=post_page-----bcc50f48474d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Machine Learning: Concepts, Techniques and Applications'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://dmnkplzr.medium.com/list/machine-learning-concepts-techniques-and-applications-e83997daeb7a?source=post_page-----bcc50f48474d--------------------------------)5
    stories![](../Images/ede57ccab552f553442f43dc51c077c9.png)![](../Images/71c8b7eee050a29f4fa563afb35f5da5.png)![](../Images/2ec6d4edef1c2c0c8d5a301825a6b833.png)'
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in signing up with Medium to get unlimited access to all
    stories, you can support me by using my [referral link](https://dmnkplzr.medium.com/membership).
    This will earn me a small commission at no additional cost to you.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Box, G E P, and D R Cox. “An Analysis of Transformations.” : 43.'
  prefs: []
  type: TYPE_NORMAL
- en: Brown, Sara. 2022\. “Why It’s Time for ‘Data-Centric Artificial Intelligence.’”
    *MIT Sloan*. [https://mitsloan.mit.edu/ideas-made-to-matter/why-its-time-data-centric-artificial-intelligence](https://mitsloan.mit.edu/ideas-made-to-matter/why-its-time-data-centric-artificial-intelligence)
    (October 30, 2022).
  prefs: []
  type: TYPE_NORMAL
- en: '[educative.io](http://educative.io). “Feature Selection and Feature Engineering
    — Machine Learning System Design.” *Educative: Interactive Courses for Software
    Developers*. [https://www.educative.io/courses/machine-learning-system-design/q2AwDN4nZ73](https://www.educative.io/courses/machine-learning-system-design/q2AwDN4nZ73)
    (November 25, 2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Feature Engineering with H2O — Dmitry Larko, Senior Data Scientist,* [*H2O.Ai*](http://H2O.Ai).
    2017\. [https://www.youtube.com/watch?v=irkV4sYExX4](https://www.youtube.com/watch?v=irkV4sYExX4)
    (September 5, 2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[featureranking.com](http://featureranking.com). “Case Study: Predicting Income
    Status.” *[*[*www.featureranking.com*](http://www.featureranking.com).]([http://www.featureranking.com](http://www.featureranking.com)./)
    [https://www.featureranking.com/tutorials/machine-learning-tutorials/case-study-predicting-income-status/](https://www.featureranking.com/tutorials/machine-learning-tutorials/case-study-predicting-income-status/)
    (November 26, 2022).'
  prefs: []
  type: TYPE_NORMAL
- en: Geeksforgeeks. 2020\. “Python | Box-Cox Transformation.” *GeeksforGeeks*. [https://www.geeksforgeeks.org/box-cox-transformation-using-python/](https://www.geeksforgeeks.org/box-cox-transformation-using-python/)
    (December 25, 2022).
  prefs: []
  type: TYPE_NORMAL
- en: 'Google Developers. 2017\. “Intro to Feature Engineering with TensorFlow — Machine
    Learning Recipes #9.” [https://www.youtube.com/watch?v=d12ra3b_M-0](https://www.youtube.com/watch?v=d12ra3b_M-0)
    (September 5, 2022).'
  prefs: []
  type: TYPE_NORMAL
- en: Google Developers. “Introducing TensorFlow Feature Columns.” [https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html](https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html)
    (September 21, 2022).
  prefs: []
  type: TYPE_NORMAL
- en: '[Heavy.ai](http://Heavy.ai). 2022\. “What Is Feature Engineering? Definition
    and FAQs | [HEAVY.AI](http://HEAVY.AI).” [https://www.heavy.ai/technical-glossary/feature-engineering](https://www.heavy.ai/technical-glossary/feature-engineering)
    (September 19, 2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[heavy.ai](http://heavy.ai). “Feature Engineering.” [https://www.heavy.ai/technical-glossary/feature-engineering](https://www.heavy.ai/technical-glossary/feature-engineering).'
  prefs: []
  type: TYPE_NORMAL
- en: Koehrsten, Will. “Introduction to Manual Feature Engineering.” [https://kaggle.com/code/willkoehrsen/introduction-to-manual-feature-engineering](https://kaggle.com/code/willkoehrsen/introduction-to-manual-feature-engineering)
    (September 5, 2022).
  prefs: []
  type: TYPE_NORMAL
- en: Kousar, Summer. “EDA on Cyber Security Salary.” [https://kaggle.com/code/summerakousar/eda-on-cyber-security-salary](https://kaggle.com/code/summerakousar/eda-on-cyber-security-salary)
    (September 7, 2022).
  prefs: []
  type: TYPE_NORMAL
- en: Moody, John. 1988\. “Fast Learning in Multi-Resolution Hierarchies.” In *Advances
    in Neural Information Processing Systems*, Morgan-Kaufmann. [https://proceedings.neurips.cc/paper/1988/hash/82161242827b703e6acf9c726942a1e4-Abstract.html](https://proceedings.neurips.cc/paper/1988/hash/82161242827b703e6acf9c726942a1e4-Abstract.html)
    (November 28, 2022).
  prefs: []
  type: TYPE_NORMAL
- en: Poon, Wing. 2022\. “Feature Engineering for Machine Learning (1/3).” *Medium*.
    [https://towardsdatascience.com/feature-engineering-for-machine-learning-a80d3cdfede6](/feature-engineering-for-machine-learning-a80d3cdfede6)
    (September 19, 2022).
  prefs: []
  type: TYPE_NORMAL
- en: 'Poon, Wing. “Feature Engineering for Machine Learning (2/3) Part 2: Feature
    Generation.” : 10.'
  prefs: []
  type: TYPE_NORMAL
- en: '[pydata.org](http://pydata.org). “Pandas.Get_dummies — Pandas 1.5.2 Documentation.”
    [https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html)
    (November 25, 2022).'
  prefs: []
  type: TYPE_NORMAL
- en: Raschka, Sebastian. “Principal Component Analysis.” *Sebastian Raschka, PhD*.
    [https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html](https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html)
    (December 17, 2022).
  prefs: []
  type: TYPE_NORMAL
- en: '[Rdocumentation.org](http://Rdocumentation.org). “Boxcox Function — RDocumentation.”
    [https://www.rdocumentation.org/packages/EnvStats/versions/2.7.0/topics/boxcox](https://www.rdocumentation.org/packages/EnvStats/versions/2.7.0/topics/boxcox)
    (December 25, 2022).'
  prefs: []
  type: TYPE_NORMAL
- en: Sarkar, Dipanjan. 2019a. “Categorical Data.” *Medium*. [https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63](/understanding-feature-engineering-part-2-categorical-data-f54324193e63)
    (September 19, 2022).
  prefs: []
  type: TYPE_NORMAL
- en: Sarkar, Dipanjan. 2019b. “Continuous Numeric Data.” *Medium*. [https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b](/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b)
    (September 19, 2022).
  prefs: []
  type: TYPE_NORMAL
- en: 'Sarkar, Dipanjan, Raghav Bali, and Tushar Sharma. 2018\. *Practical Machine
    Learning with Python*. Berkeley, CA: Apress. [http://link.springer.com/10.1007/978-1-4842-3207-1](http://link.springer.com/10.1007/978-1-4842-3207-1)
    (November 25, 2022).'
  prefs: []
  type: TYPE_NORMAL
- en: Siddhartha. 2020\. “Demonstration of TensorFlow Feature Columns (Tf.Feature_column).”
    *ML Book*. [https://medium.com/ml-book/demonstration-of-tensorflow-feature-columns-tf-feature-column-3bfcca4ca5c4](https://medium.com/ml-book/demonstration-of-tensorflow-feature-columns-tf-feature-column-3bfcca4ca5c4)
    (September 21, 2022).
  prefs: []
  type: TYPE_NORMAL
- en: '[Sklearn.org](http://Sklearn.org). “6.2\. Feature Extraction.” *scikit-learn*.
    [https://scikit-learn/stable/modules/feature_extraction.html](https://scikit-learn/stable/modules/feature_extraction.html)
    (November 28, 2022a).'
  prefs: []
  type: TYPE_NORMAL
- en: '[spark.apache.org](http://spark.apache.org). “Extracting, Transforming and
    Selecting Features — Spark 3.3.1 Documentation.” [https://spark.apache.org/docs/latest/ml-features](https://spark.apache.org/docs/latest/ml-features)
    (December 1, 2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Tensorflow.org](http://Tensorflow.org). “Tf.One_hot | TensorFlow v2.11.0.”
    *TensorFlow*. [https://www.tensorflow.org/api_docs/python/tf/one_hot](https://www.tensorflow.org/api_docs/python/tf/one_hot)
    (November 25, 2022).'
  prefs: []
  type: TYPE_NORMAL
- en: United Nations, Department of Economic and Social Affairs, Population Division
    (2022). *World Population Prospects 2022, Online Edition*.
  prefs: []
  type: TYPE_NORMAL
- en: '[Valohai.com](http://Valohai.com). 2022\. “What Is a Machine Learning Pipeline?”
    [https://valohai.com/machine-learning-pipeline/](https://valohai.com/machine-learning-pipeline/)
    (September 19, 2022).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Votava, Adam. 2022\. “Keeping Up With Data #105.” *Medium*. [https://adamvotava.medium.com/keeping-up-with-data-105-6a2a8a41f4b6](https://adamvotava.medium.com/keeping-up-with-data-105-6a2a8a41f4b6)
    (October 21, 2022).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Weinberger, Kilian et al. 2009\. “Feature Hashing for Large Scale Multitask
    Learning.” In *Proceedings of the 26th Annual International Conference on Machine
    Learning — ICML ’09*, Montreal, Quebec, Canada: ACM Press, 1–8\. [http://portal.acm.org/citation.cfm?doid=1553374.1553516](http://portal.acm.org/citation.cfm?doid=1553374.1553516)
    (December 25, 2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '*What Makes a Good Feature? — Machine Learning Recipes #3*. 2016\. [https://www.youtube.com/watch?v=N9fDIAflCMY](https://www.youtube.com/watch?v=N9fDIAflCMY)
    (September 5, 2022).'
  prefs: []
  type: TYPE_NORMAL
- en: Wikimedia. “Average yearly temperature per country.png.” [https://commons.wikimedia.org/wiki/File:Average_yearly_temperature_per_country.png](https://commons.wikimedia.org/wiki/File:Average_yearly_temperature_per_country.png)
    (December 25, 2022).
  prefs: []
  type: TYPE_NORMAL
- en: Wikipedia. 2022\. “Feature Hashing.” *Wikipedia*. [https://en.wikipedia.org/w/index.php?title=Feature_hashing&oldid=1114513799](https://en.wikipedia.org/w/index.php?title=Feature_hashing&oldid=1114513799)
    (November 28, 2022).
  prefs: []
  type: TYPE_NORMAL
- en: Zheng, Alice and Amanda Casari. 2018\. *Feature Engineering for Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
