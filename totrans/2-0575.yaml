- en: CountVectorizer to Extract Features from Texts in Python, in Detail
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/countvectorizer-to-extract-features-from-texts-in-python-in-detail-0e7147c10753](https://towardsdatascience.com/countvectorizer-to-extract-features-from-texts-in-python-in-detail-0e7147c10753)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/07e11611df2a6a84e10f07ef5553f473.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Towfiqu barbhuiya](https://unsplash.com/@towfiqu999999?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Everything you need to know to use CountVectorizer efficiently in Sklearn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://rashida00.medium.com/?source=post_page-----0e7147c10753--------------------------------)[![Rashida
    Nasrin Sucky](../Images/42bd057e8eca255907c43c29a498f2ca.png)](https://rashida00.medium.com/?source=post_page-----0e7147c10753--------------------------------)[](https://towardsdatascience.com/?source=post_page-----0e7147c10753--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----0e7147c10753--------------------------------)
    [Rashida Nasrin Sucky](https://rashida00.medium.com/?source=post_page-----0e7147c10753--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----0e7147c10753--------------------------------)
    ·7 min read·Oct 21, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: The most basic data processing that any Natural Language Processing (NLP) project
    requires is to convert the text data to the numeric data. As long as the data
    is in text form we cannot do any kind of computation action on it.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple methods available for this text-to-numeric data conversion.
    This tutorial will explain one of the most basic vectorizers, the CountVectorizer
    method in the scikit-learn library.
  prefs: []
  type: TYPE_NORMAL
- en: This method is very simple. It takes the frequency of occurrence of each word
    as the numeric value. An example will make it clear.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: We will import the CountVectorizer method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Call the method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fit the text data to the CountVectorizer method and, convert that to an array.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here I have the numeric values representing the text data above.
  prefs: []
  type: TYPE_NORMAL
- en: '**How do we know which values represent which words in the text?**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To make that clear, it will be helpful to convert the array into a DataFrame
    where column names will be the words themselves.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/94764d6278f9ff68ed119eb1bdaeacbd.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Author
  prefs: []
  type: TYPE_NORMAL
- en: Now, it shows clearly. The value of the word ‘also’ is 1 which means ‘also’
    appeared only once in the test. The word ‘aunt’ came twice in the text. So, the
    value of the word ‘aunt’ is 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the last example, all the sentences were in one string. So, we got only
    one row of data for four sentences. Let’s rearrange the text and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This time we have a two-dimensional array with one individual list for each
    string in the text. Putting this array in a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6078ed9e6f2bfa6b211dec032da19692.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Author
  prefs: []
  type: TYPE_NORMAL
- en: Look carefully at this DataFrame. All the words are there as column names. Each
    row represents a string in the text and the value in the column shows how many
    times the word appeared in the string. If the word doesn’t appear, the value is
    zero.
  prefs: []
  type: TYPE_NORMAL
- en: There are some parameters available for [CountVectorizer method](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)
    in sklearn library that are worth checking.
  prefs: []
  type: TYPE_NORMAL
- en: lowercase
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you notice by default CountVectorizer method converts all the words to lowercase.
    If you do not want that you need to set lowercase = False.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f02a7874d5534e076a2e241c828d0433.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Author
  prefs: []
  type: TYPE_NORMAL
- en: Now, the words are taken the way it is in the text. The word ‘My’ came twice
    in the DataFrame as ‘My’, and ‘my’.
  prefs: []
  type: TYPE_NORMAL
- en: stop_words
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The stop_words are the words that we can consider unnecessary for the analytics.
    In our text, I may think ‘also’, ‘is’, and ‘to’ are not necessary words. I can
    simply exclude them which is a very important part of data processing for most
    analytics or machine learning models. Here we have only 4 strings. But in real-world
    analytics, we need to deal with thousands of strings. Thousands of strings may
    involve thousands of words and each word becomes a feature. If we can exclude
    some of the frequently appearing or not so necessary for the model, it will save
    a lot of computational effort.
  prefs: []
  type: TYPE_NORMAL
- en: There are default lists of stop words in CountVectorizer method itself for a
    lot of major languages. Here is an example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/48451efb85beb3f43c23c8a849add729.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Author
  prefs: []
  type: TYPE_NORMAL
- en: Look! A lot of the words are gone!
  prefs: []
  type: TYPE_NORMAL
- en: 'If you think the words that are gone are not enough for you or too many words
    are gone, please provide your own list of stop_words. For example, if I only want
    ‘also’, ‘is’, ‘am’, and ‘to’ to be excluded, I will provide the list of stop_words
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: max_df
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is another way of eliminating words. If we use max_df = 0.5 that means
    if a word appears in more than 50% of the documents or strings then that will
    be eliminated. An integer value can be used as max_df as well. Max_df = 20 means
    if a word exists in more than 20 documents it will be eliminated.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate this, I created a new text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/87bcda6c7b82b0fa5e88ee8cf2012fc1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Author
  prefs: []
  type: TYPE_NORMAL
- en: ‘Lilly’ appeared in 4 documents out of 5\. So it is eliminated. Same as ‘is’.
  prefs: []
  type: TYPE_NORMAL
- en: min_df
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is the opposite of max_df. If a document appears less than a proportion
    or a specified they are eliminated by min_df. In this example, I am using the
    same text as the last example and setting min_df = 2\. So, any word that exists
    in less than 2 documents is eliminated.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/48996fc533d77f80db94cd3603d64e54.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Author
  prefs: []
  type: TYPE_NORMAL
- en: We have only three words left as we already have just 5 documents. This can
    be useful in machine learning projects.
  prefs: []
  type: TYPE_NORMAL
- en: When we are trying to extract a trend, the words that only exist seldom in a
    couple of documents out of thousands of documents, are not very helpful.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: max_features
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is another useful feature. When we have thousands of words, it is computationally
    expensive and time-consuming. If we have a total of 10000 words that becomes 10000
    features. Now if you think only the top 2000 words might be good enough based
    on the term frequency, you can simply use max_features = 2000\. Here we even do
    not have that many words. So, I will use max_features = 5.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f8e45d120b386f71038045cf86780d5b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Author
  prefs: []
  type: TYPE_NORMAL
- en: Here we have the top five words that appeared the most.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article tried to explain the CountVectorizer method and how you can best
    use this method of text processing. The parameters I explained here can make your
    analytics or Natural Language Processing models efficient if used correctly. These
    parameters can be used alone or you can use some of them together with one another
    based on your need. There is a lot of scope for experiment. There are more sophisticated
    methods to vectorize text data nowadays. But this simple method still works in
    many cases.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to follow me on [Twitter](https://twitter.com/rashida048) and like
    my [Facebook](https://www.facebook.com/rashida.smith.161) page.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want a video version of this tutorial, here is the link:'
  prefs: []
  type: TYPE_NORMAL
- en: More Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://pub.towardsai.net/a-complete-exploratory-data-analysis-in-python-a2148daac072?source=post_page-----0e7147c10753--------------------------------)
    [## A Complete Exploratory Data Analysis in Python'
  prefs: []
  type: TYPE_NORMAL
- en: Data Cleaning, Analysis, Visualization, Feature Selection, Predictive Modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pub.towardsai.net](https://pub.towardsai.net/a-complete-exploratory-data-analysis-in-python-a2148daac072?source=post_page-----0e7147c10753--------------------------------)
    [](/30-very-useful-pandas-functions-for-everyday-data-analysis-tasks-f1eae16409af?source=post_page-----0e7147c10753--------------------------------)
    [## 30 Very Useful Pandas Functions for Everyday Data Analysis Tasks
  prefs: []
  type: TYPE_NORMAL
- en: Pandas Cheatsheet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/30-very-useful-pandas-functions-for-everyday-data-analysis-tasks-f1eae16409af?source=post_page-----0e7147c10753--------------------------------)
    [](/6-tips-for-dealing-with-null-values-e16d1d1a1b33?source=post_page-----0e7147c10753--------------------------------)
    [## 6 Tips for Dealing With Null Values
  prefs: []
  type: TYPE_NORMAL
- en: Includes Iterative Method, Mean and Median Fill with Groupby, Mean and Median
    Fill
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/6-tips-for-dealing-with-null-values-e16d1d1a1b33?source=post_page-----0e7147c10753--------------------------------)
    [](https://pub.towardsai.net/a-detailed-tutorial-on-polynomial-regression-in-python-overview-implementation-and-overfitting-e319fc7e5b8f?source=post_page-----0e7147c10753--------------------------------)
    [## A Detailed Tutorial on Polynomial Regression in Python, Overview, Implementation,
    and Overfitting
  prefs: []
  type: TYPE_NORMAL
- en: Complete code in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pub.towardsai.net](https://pub.towardsai.net/a-detailed-tutorial-on-polynomial-regression-in-python-overview-implementation-and-overfitting-e319fc7e5b8f?source=post_page-----0e7147c10753--------------------------------)
    [](/tensorflow-model-training-using-gradienttape-f2093646ab13?source=post_page-----0e7147c10753--------------------------------)
    [## TensorFlow Model Training Using GradientTape
  prefs: []
  type: TYPE_NORMAL
- en: Use of GradientTape to Update the Weights
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/tensorflow-model-training-using-gradienttape-f2093646ab13?source=post_page-----0e7147c10753--------------------------------)
    [](/anomaly-detection-in-tensorflow-and-keras-using-the-autoencoder-method-5600aca29c50?source=post_page-----0e7147c10753--------------------------------)
    [## Anomaly Detection in TensorFlow and Keras Using the Autoencoder Method
  prefs: []
  type: TYPE_NORMAL
- en: A cutting-edge unsupervised method for noise removal, dimensionality reduction,
    anomaly detection, and more
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/anomaly-detection-in-tensorflow-and-keras-using-the-autoencoder-method-5600aca29c50?source=post_page-----0e7147c10753--------------------------------)
  prefs: []
  type: TYPE_NORMAL
