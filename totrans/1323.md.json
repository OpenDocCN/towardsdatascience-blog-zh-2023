["```py\nfrom diffusers import DiffusionPipeline\npipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\npipeline.to(\"cuda\")\nimage = pipeline(\"A cute cat playing piano\").images[0]\nimage.save(\"image_of_cat_playing_piano.png\")\n```", "```py\npipeline = DiffusionPipeline.from_pretrained(\"/model/custom_model.safetensors\")\n```", "```py\ngit clone https://github.com/huggingface/diffusers.git\n```", "```py\npython convert_original_stable_diffusion_to_diffusers.py --from_safetensors --checkpoint_path=\"D:\\stable-diffusion-webui\\models\\Stable-diffusion\\deliberate_v2.safetensors\" --dump_path='D:\\sd_models\\deliberate_v2' --device='cuda:0'\n```", "```py\nfrom diffusers import DiffusionPipeline\npipeline = DiffusionPipeline.from_pretrained(\n    r\"D:\\sd_models\\deliberate_v2\"\n)\npipeline.to(\"cuda\")\nimage = pipeline(\"A cute cat playing piano\").images[0]\nimage.save(\"image_of_cat_playing_piano.png\")\n```", "```py\nfrom diffusers import DiffusionPipeline\nimport torch # <----- Line 1 added\npipeline = DiffusionPipeline.from_pretrained(\n    r\"D:\\sd_models\\deliberate_v2\"\n    ,torch_dtype        = torch.float16 # <----- Line 2 Added\n)\npipeline.to(\"cuda\")\nimage = pipeline(\"A cute cat playing piano\").images[0]\nimage.save(\"image_of_cat_playing_piano.png\")\n```", "```py\n...\npipeline.to(\"cuda\")\npipeline.enable_xformers_memory_efficient_attention()  <--- one line added\n...\n```", "```py\npipeline = DiffusionPipeline.from_pretrained(\n    model_path,\n    custom_pipeline=\"lpw_stable_diffusion\",  #<--- code added\n    torch_dtype=torch.float16\n)\n```", "```py\nfrom diffusers import DiffusionPipeline\nimport torch\npipeline = DiffusionPipeline.from_pretrained(\n    r\"D:\\sd_models\\deliberate_v2\"\n    ,custom_pipeline = \"lpw_stable_diffusion\"  #<--- code added\n    ,torch_dtype        = torch.float16\n)\npipeline.to(\"cuda\")\npipeline.enable_xformers_memory_efficient_attention()\nprompt = \"\"\"\nBabel tower falling down, walking on the starlight, dreamy ultra wide shot\n, atmospheric, hyper realistic, epic composition, cinematic, octane render\n, artstation landscape vista photography by Carr Clifton & Galen Rowell, 16K resolution\n, Landscape veduta photo by Dustin Lefevre & tdraw, detailed landscape painting by Ivan Shishkin\n, DeviantArt, Flickr, rendered in Enscape, Miyazaki, Nausicaa Ghibli, Breath of The Wild\n, 4k detailed post processing, artstation, rendering by octane, unreal engine\n\"\"\"\nimage = pipeline(prompt).images[0]\nimage.save(\"goodbye_babel_tower.png\")\n```", "```py\nfrom safetensors.torch import load_file\ndef __load_lora(\n    pipeline\n    ,lora_path\n    ,lora_weight=0.5\n):\n    state_dict = load_file(lora_path)\n    LORA_PREFIX_UNET = 'lora_unet'\n    LORA_PREFIX_TEXT_ENCODER = 'lora_te'\n\n    alpha = lora_weight\n    visited = []\n\n    # directly update weight in diffusers model\n    for key in state_dict:\n\n        # as we have set the alpha beforehand, so just skip\n        if '.alpha' in key or key in visited:\n            continue\n\n        if 'text' in key:\n            layer_infos = key.split('.')[0].split(LORA_PREFIX_TEXT_ENCODER+'_')[-1].split('_')\n            curr_layer = pipeline.text_encoder\n        else:\n            layer_infos = key.split('.')[0].split(LORA_PREFIX_UNET+'_')[-1].split('_')\n            curr_layer = pipeline.unet\n\n        # find the target layer\n        temp_name = layer_infos.pop(0)\n        while len(layer_infos) > -1:\n            try:\n                curr_layer = curr_layer.__getattr__(temp_name)\n                if len(layer_infos) > 0:\n                    temp_name = layer_infos.pop(0)\n                elif len(layer_infos) == 0:\n                    break\n            except Exception:\n                if len(temp_name) > 0:\n                    temp_name += '_'+layer_infos.pop(0)\n                else:\n                    temp_name = layer_infos.pop(0)\n\n        # org_forward(x) + lora_up(lora_down(x)) * multiplier\n        pair_keys = []\n        if 'lora_down' in key:\n            pair_keys.append(key.replace('lora_down', 'lora_up'))\n            pair_keys.append(key)\n        else:\n            pair_keys.append(key)\n            pair_keys.append(key.replace('lora_up', 'lora_down'))\n\n        # update weight\n        if len(state_dict[pair_keys[0]].shape) == 4:\n            weight_up = state_dict[pair_keys[0]].squeeze(3).squeeze(2).to(torch.float32)\n            weight_down = state_dict[pair_keys[1]].squeeze(3).squeeze(2).to(torch.float32)\n            curr_layer.weight.data += alpha * torch.mm(weight_up, weight_down).unsqueeze(2).unsqueeze(3)\n        else:\n            weight_up = state_dict[pair_keys[0]].to(torch.float32)\n            weight_down = state_dict[pair_keys[1]].to(torch.float32)\n            curr_layer.weight.data += alpha * torch.mm(weight_up, weight_down)\n\n        # update visited list\n        for item in pair_keys:\n            visited.append(item)\n\n    return pipeline\n```", "```py\nfrom diffusers import DiffusionPipeline\nimport torch\npipeline = DiffusionPipeline.from_pretrained(\n    r\"D:\\sd_models\\deliberate_v2\"\n    ,custom_pipeline = \"lpw_stable_diffusion\"  \n    ,torch_dtype        = torch.float16\n)\nlora = (r\"D:\\sd_models\\Lora\\Moxin_10.safetensors\",0.8)\npipeline = __load_lora(pipeline=pipeline,lora_path=lora[0],lora_weight=lora[1])\npipeline.to(\"cuda\")\npipeline.enable_xformers_memory_efficient_attention()\n\nprompt = \"\"\"\nshukezouma,negative space,shuimobysim \na branch of flower, traditional chinese ink painting\n\"\"\"\nimage = pipeline(prompt).images[0]\nimage.save(\"a branch of flower.png\")\n```", "```py\ndef load_textual_inversion(\n    learned_embeds_path\n    , text_encoder\n    , tokenizer\n    , token = None\n    , weight = 0.5\n):\n    '''\n    Use this function to load textual inversion model in model initilization stage \n    or image generation stage. \n    '''\n    loaded_learned_embeds = torch.load(learned_embeds_path, map_location=\"cpu\")\n    string_to_token = loaded_learned_embeds['string_to_token']\n    string_to_param = loaded_learned_embeds['string_to_param']\n\n    # separate token and the embeds\n    trained_token = list(string_to_token.keys())[0]\n    embeds = string_to_param[trained_token]\n    embeds = embeds[0] * weight\n\n    # cast to dtype of text_encoder\n    dtype = text_encoder.get_input_embeddings().weight.dtype\n    embeds.to(dtype)\n\n    # add the token in tokenizer\n    token = token if token is not None else trained_token\n    num_added_tokens = tokenizer.add_tokens(token)\n    if num_added_tokens == 0:\n        #print(f\"The tokenizer already contains the token {token}.The new token will replace the previous one\")\n        raise ValueError(f\"The tokenizer already contains the token {token}. Please pass a different `token` that is not already in the tokenizer.\")\n\n    # resize the token embeddings\n    text_encoder.resize_token_embeddings(len(tokenizer))\n\n    # get the id for the token and assign the embeds\n    token_id = tokenizer.convert_tokens_to_ids(token)\n    text_encoder.get_input_embeddings().weight.data[token_id] = embeds\n    return (tokenizer,text_encoder)\n```", "```py\nfrom diffusers import DiffusionPipeline\nimport torch\npipeline = DiffusionPipeline.from_pretrained(\n    r\"D:\\sd_models\\deliberate_v2\"\n    ,custom_pipeline = \"lpw_stable_diffusion\"  \n    ,torch_dtype        = torch.float16\n    ,safety_checker     = None\n)\n\ntextual_inversion_path = r\"D:\\sd_models\\embeddings\\style-empire.pt\"\n\ntokenizer       = pipeline.tokenizer\ntext_encoder    = pipeline.text_encoder \nload_textual_inversion(\n    learned_embeds_path     = textual_inversion_path\n    , tokenizer             = tokenizer\n    , text_encoder          = text_encoder\n    , token                 = 'styleempire'\n)\n\npipeline.to(\"cuda\")\npipeline.enable_xformers_memory_efficient_attention()\n\nprompt = \"\"\"\nstyleempire,award winning beautiful street, storm,((dark storm clouds))\n, fluffy clouds in the sky, shaded flat illustration, digital art\n, trending on artstation, highly detailed, fine detail, intricate\n, ((lens flare)), (backlighting), (bloom)\n\"\"\"\nneg_prompt = \"\"\"\n cartoon, 3d, ((disfigured)), ((bad art)), ((deformed)), ((poorly drawn))\n , ((extra limbs)), ((close up)), ((b&w)), weird colors, blurry\n , hat, cap, glasses, sunglasses, lightning, face\n\"\"\"\n\ngenerator = torch.Generator(\"cuda\").manual_seed(1)\nimage = pipeline(\n    prompt\n    ,negative_prompt =neg_prompt\n    ,generator       = generator\n).images[0]\nimage.save(\"tv_test.png\")\n```", "```py\npipeline.enable_attention_slicing()\n```", "```py\npipeline.enable_model_cpu_offload()\n```", "```py\nBabel tower falling down, walking on the starlight, dreamy ultra wide shot\n, atmospheric, hyper realistic, epic composition, cinematic, octane render\n, artstation landscape vista photography by Carr Clifton & Galen Rowell, 16K resolution\n, Landscape veduta photo by Dustin Lefevre & tdraw, detailed landscape painting by Ivan Shishkin\n, DeviantArt, Flickr, rendered in Enscape, Miyazaki, Nausicaa Ghibli, Breath of The Wild\n, 4k detailed post processing, artstation, rendering by octane, unreal engine\n```"]