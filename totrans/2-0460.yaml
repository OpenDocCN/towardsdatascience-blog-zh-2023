- en: BYOL â€”The Alternative to Contrastive Self-Supervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BYOL â€”å¯¹æ¯”è‡ªç›‘ç£å­¦ä¹ çš„æ›¿ä»£æ–¹æ³•
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c](https://towardsdatascience.com/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c](https://towardsdatascience.com/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c)
- en: '[ğŸš€Saschaâ€™s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[ğŸš€Saschaçš„è®ºæ–‡ä¿±ä¹éƒ¨](https://towardsdatascience.com/tagged/saschas-paper-club)'
- en: 'Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning by J.
    Grill et. al.'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning ç”± J.
    Grill ç­‰äºº'
- en: '[](https://medium.com/@SaschaKirch?source=post_page-----5d0a26983d7c--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----5d0a26983d7c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5d0a26983d7c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5d0a26983d7c--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5d0a26983d7c--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch?source=post_page-----5d0a26983d7c--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----5d0a26983d7c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5d0a26983d7c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5d0a26983d7c--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5d0a26983d7c--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5d0a26983d7c--------------------------------)
    Â·10 min readÂ·Sep 7, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5d0a26983d7c--------------------------------)
    Â·10åˆ†é’Ÿé˜…è¯»Â·2023å¹´9æœˆ7æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In todayâ€™s paper analysis we will have a close look into the paper behind BYOL
    (**B**ootstrap **Y**our **O**wn **L**atent). It provides an alternative to contrastive
    self-supervised learning techniques for representation learning removing the need
    for a large corpus of negative samples and gigantic batch sizes. Furthermore it
    is a landmark paper on the path of understanding todayâ€™s state-of-the-art foundation
    models such as the [DINO](https://arxiv.org/abs/2104.14294) family, including
    [DINOv2](https://arxiv.org/abs/2304.07193).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»Šå¤©çš„è®ºæ–‡åˆ†æä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨å…³äº BYOL (**B**ootstrap **Y**our **O**wn **L**atent) çš„è®ºæ–‡ã€‚å®ƒæä¾›äº†ä¸€ä¸ªå¯¹æ¯”è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå»é™¤å¯¹å¤§é‡è´Ÿæ ·æœ¬å’Œåºå¤§æ‰¹é‡å¤§å°çš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨ç†è§£ä»Šå¤©æœ€å…ˆè¿›çš„åŸºç¡€æ¨¡å‹ï¼ˆå¦‚
    [DINO](https://arxiv.org/abs/2104.14294) ç³»åˆ—ï¼ŒåŒ…æ‹¬ [DINOv2](https://arxiv.org/abs/2304.07193)ï¼‰çš„é“è·¯ä¸Šå…·æœ‰é‡Œç¨‹ç¢‘å¼çš„æ„ä¹‰ã€‚
- en: While contrastive self-supervised learning frameworks still feel kind of intuitive,
    BYOL can be confusing and intimidating at first. Therefore, itâ€™s a great paper
    to analyze together. So letâ€™s dive into it and strip it down to uncover its core
    ideas!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å¯¹æ¯”è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ä»ç„¶æœ‰äº›ç›´è§‚ï¼Œä½† BYOL èµ·åˆå¯èƒ½ä¼šè®©äººæ„Ÿåˆ°å›°æƒ‘å’Œä¸å®‰ã€‚å› æ­¤ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è®ºæ–‡æ¥ä¸€èµ·åˆ†æã€‚è®©æˆ‘ä»¬æ·±å…¥äº†è§£å®ƒï¼Œæ­ç¤ºå…¶æ ¸å¿ƒæ€æƒ³å§ï¼
- en: '![](../Images/63946f35ab17c2db935b29bf3105ec08.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63946f35ab17c2db935b29bf3105ec08.png)'
- en: Image created from [publication](https://arxiv.org/abs/2006.07733) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Sascha Kirch](https://medium.com/@SaschaKirch) åˆ›ä½œï¼Œæ¥æºäº [å‡ºç‰ˆç‰©](https://arxiv.org/abs/2006.07733)
- en: '**Paper:** [Bootstrap your own latent: A new approach to self-supervised Learning](https://arxiv.org/abs/2006.07733)
    by Jean-Bastien Grill et. al., 13 Jun. 2020'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**è®ºæ–‡ï¼š** [Bootstrap your own latent: A new approach to self-supervised Learning](https://arxiv.org/abs/2006.07733)
    ç”± Jean-Bastien Grill ç­‰äººï¼Œ2020å¹´6æœˆ13æ—¥'
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/deepmind/deepmind-research/tree/master/byol)'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**èµ„æºï¼š** [GitHub](https://github.com/deepmind/deepmind-research/tree/master/byol)'
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** similarity learning, representation learning, computer vision,
    foundation models'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ç±»åˆ«ï¼š** ç›¸ä¼¼æ€§å­¦ä¹ ã€è¡¨å¾å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€åŸºç¡€æ¨¡å‹'
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[**å…¶ä»–è®²è§£**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
- en: '[[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    â€” [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    â€” [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    â€” [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    â€” [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    â€” [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    â€” [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
- en: Outline
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤§çº²
- en: Context & Background
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: èƒŒæ™¯ä¸æ¦‚è¿°
- en: Claimed Contributions
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å£°ç§°çš„è´¡çŒ®
- en: Method
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ–¹æ³•
- en: Experiments
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®éªŒ
- en: Conclusion
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Further Readings & Resources
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»ä¸èµ„æº
- en: Context & Background
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: èƒŒæ™¯ä¸æ¦‚è¿°
- en: BYOL falls into the category of self-supervised representation learning via
    similarity learning. Self-supervised means no explicit ground truth labels are
    provided, but a supervision signal might be constructed from unlabeled data. Representation
    learning means the model learns to encode its input into a lower dimensional and
    semantically rich representation space. And finally in similarity learning features
    that are similar are mapped close to each other in the latent representation space,
    while non-similar features are mapped further apart. These representations are
    crucial in many deep learning tasks that built upon these representations to for
    example generate new data, perform classification, segmentation or monocular depth
    estimation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: BYOL å±äºé€šè¿‡ç›¸ä¼¼æ€§å­¦ä¹ è¿›è¡Œçš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ã€‚è‡ªç›‘ç£æ„å‘³ç€æ²¡æœ‰æä¾›æ˜ç¡®çš„çœŸå®æ ‡ç­¾ï¼Œä½†å¯ä»¥ä»æœªæ ‡è®°çš„æ•°æ®ä¸­æ„å»ºç›‘ç£ä¿¡å·ã€‚è¡¨ç¤ºå­¦ä¹ æ„å‘³ç€æ¨¡å‹å­¦ä¹ å°†è¾“å…¥ç¼–ç åˆ°ä¸€ä¸ªç»´åº¦è¾ƒä½ä¸”è¯­ä¹‰ä¸°å¯Œçš„è¡¨ç¤ºç©ºé—´ä¸­ã€‚æœ€åï¼Œåœ¨ç›¸ä¼¼æ€§å­¦ä¹ ä¸­ï¼Œç›¸ä¼¼çš„ç‰¹å¾åœ¨æ½œåœ¨è¡¨ç¤ºç©ºé—´ä¸­è¢«æ˜ å°„å¾—ç›¸äº’æ¥è¿‘ï¼Œè€Œä¸ç›¸ä¼¼çš„ç‰¹å¾åˆ™è¢«æ˜ å°„å¾—æ›´è¿œã€‚è¿™äº›è¡¨ç¤ºåœ¨è®¸å¤šæ·±åº¦å­¦ä¹ ä»»åŠ¡ä¸­è‡³å…³é‡è¦ï¼Œè¿™äº›ä»»åŠ¡åˆ©ç”¨è¿™äº›è¡¨ç¤ºæ¥ç”Ÿæˆæ–°æ•°æ®ã€æ‰§è¡Œåˆ†ç±»ã€åˆ†å‰²æˆ–å•ç›®æ·±åº¦ä¼°è®¡ç­‰ã€‚
- en: Many successful methods, such as [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500),
    [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05),
    [MoCo](https://arxiv.org/abs/1911.05722) or [SimCLR](https://arxiv.org/abs/2002.05709)
    use a contrastive learning approach. In contrastive learning, a score for matching
    data pairs is maximized, while a score for non-matching data is minimized. This
    process does heavily depend on the batch size and the number of negative samples
    provided during training. This dependency makes data collection and training more
    challenging.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šæˆåŠŸçš„æ–¹æ³•ï¼Œå¦‚[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)ã€[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)ã€[MoCo](https://arxiv.org/abs/1911.05722)æˆ–[SimCLR](https://arxiv.org/abs/2002.05709)ä½¿ç”¨äº†å¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•ã€‚åœ¨å¯¹æ¯”å­¦ä¹ ä¸­ï¼Œæœ€å¤§åŒ–åŒ¹é…æ•°æ®å¯¹çš„å¾—åˆ†ï¼ŒåŒæ—¶æœ€å°åŒ–ä¸åŒ¹é…æ•°æ®çš„å¾—åˆ†ã€‚è¿™ä¸€è¿‡ç¨‹ä¸¥é‡ä¾èµ–äºè®­ç»ƒæœŸé—´æä¾›çš„æ‰¹é‡å¤§å°å’Œè´Ÿæ ·æœ¬æ•°é‡ã€‚è¿™ç§ä¾èµ–æ€§ä½¿å¾—æ•°æ®æ”¶é›†å’Œè®­ç»ƒå˜å¾—æ›´åŠ å…·æœ‰æŒ‘æˆ˜æ€§ã€‚
- en: 'BYOL aims to:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: BYOLçš„ç›®æ ‡æ˜¯ï¼š
- en: Get rid for the need of negative samples and large batch size as required for
    contrastive learning.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‘†è„±å¯¹å¯¹æ¯”å­¦ä¹ æ‰€éœ€çš„è´Ÿæ ·æœ¬å’Œå¤§æ‰¹é‡å¤§å°çš„éœ€æ±‚ã€‚
- en: Decrease the dependency to domain specific augmentations to be applicable to
    other domains as language or images.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‡å°‘å¯¹ç‰¹å®šé¢†åŸŸæ•°æ®å¢å¼ºçš„ä¾èµ–ï¼Œä½¿å…¶é€‚ç”¨äºå…¶ä»–é¢†åŸŸï¼Œå¦‚è¯­è¨€æˆ–å›¾åƒã€‚
- en: Among many references made in the paper, BYOL highlights its similarities to
    [mean teacher](https://arxiv.org/abs/1703.01780), the [momentum encoder](https://arxiv.org/abs/1911.05722)
    and [predictions of bootstrapped latents (PBL)](https://arxiv.org/abs/2004.14646).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®ºæ–‡ä¸­æåˆ°çš„è®¸å¤šå‚è€ƒæ–‡çŒ®ä¸­ï¼ŒBYOL çªå‡ºäº†å®ƒä¸[å¹³å‡æ•™å¸ˆ](https://arxiv.org/abs/1703.01780)ã€[åŠ¨é‡ç¼–ç å™¨](https://arxiv.org/abs/1911.05722)å’Œ[å¼•å¯¼æ½œåœ¨é¢„æµ‹
    (PBL)](https://arxiv.org/abs/2004.14646)çš„ç›¸ä¼¼æ€§ã€‚
- en: Claimed Contributions (According to Authors)
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å£°ç§°çš„è´¡çŒ®ï¼ˆæ ¹æ®ä½œè€…çš„è¯´æ³•ï¼‰
- en: Introduction of BYOL (Bootstrap your own latent), a self-supervised representation
    learning method that does not require negative pairs (as in contrastive learning)
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BYOLï¼ˆBootstrap your own latentï¼‰çš„ä»‹ç»ï¼Œè¿™æ˜¯ä¸€ç§è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œä¸éœ€è¦è´Ÿå¯¹ï¼ˆå¦‚å¯¹æ¯”å­¦ä¹ ä¸­çš„ï¼‰ã€‚
- en: BYOL representations are shown to outperform the state-of-the-art (at the time
    of the paperâ€™s release)
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BYOL è¡¨ç¤ºæ³•è¢«è¯æ˜ä¼˜äºå½“æ—¶çš„æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ˆè®ºæ–‡å‘å¸ƒæ—¶ï¼‰ã€‚
- en: BYOL is shown to be more resilient to batch size and used image augmentations
    compared to its contrastive counterparts
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸å¯¹æ¯”å­¦ä¹ æ³•ç›¸æ¯”ï¼ŒBYOL æ˜¾ç¤ºå‡ºå¯¹æ‰¹é‡å¤§å°å’Œä½¿ç”¨çš„å›¾åƒå¢å¼ºæ–¹æ³•æ›´å…·å¼¹æ€§ã€‚
- en: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
- en: '[Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5d0a26983d7c--------------------------------)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5d0a26983d7c--------------------------------)'
- en: Paper Walkthroughs by Sascha Kirch
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sascha Kirch çš„è®ºæ–‡è§£æ
- en: '[View list](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2?source=post_page-----5d0a26983d7c--------------------------------)7
    stories![â€œDDPMâ€Šâ€”â€ŠDenoising Diffusion Probabilistic Models â€œ paper illustration
    by Sascha Kirch](../Images/6e785c0a911386676abebe0fa646f483.png)![â€œDepth Anythingâ€
    paper illustration by Sascha Kirch](../Images/bd8cd71a02e42cf64d0afd39f41f48e0.png)![](../Images/8708d91a4a1902cef889ced95d46fc39.png)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2?source=post_page-----5d0a26983d7c--------------------------------)7
    ä¸ªæ•…äº‹ï¼[â€œDDPMâ€Šâ€”â€Šå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹â€ è®ºæ–‡æ’å›¾ï¼Œç”± Sascha Kirch ç»˜åˆ¶](../Images/6e785c0a911386676abebe0fa646f483.png)![â€œDepth
    Anythingâ€ è®ºæ–‡æ’å›¾ï¼Œç”± Sascha Kirch ç»˜åˆ¶](../Images/bd8cd71a02e42cf64d0afd39f41f48e0.png)![](../Images/8708d91a4a1902cef889ced95d46fc39.png)'
- en: Method
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–¹æ³•
- en: Now that we have seen what BYOL claims to solve letâ€™s try to understand how
    this is achieved. First letâ€™s observe the architecture presented in Fig.1
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¢ç„¶æˆ‘ä»¬å·²ç»äº†è§£äº† BYOL å£°ç§°è¦è§£å†³çš„é—®é¢˜ï¼Œè®©æˆ‘ä»¬å°è¯•ç†è§£å¦‚ä½•å®ç°è¿™ä¸€ç›®æ ‡ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬è§‚å¯Ÿå›¾ 1 ä¸­å‘ˆç°çš„æ¶æ„ã€‚
- en: '![](../Images/153409ea2efbcbf75ad2378f1063bab6.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/153409ea2efbcbf75ad2378f1063bab6.png)'
- en: 'Fig. 1: Framework architecture. [Image Source](https://arxiv.org/abs/2006.07733)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šæ¡†æ¶æ¶æ„ã€‚ [å›¾åƒæ¥æº](https://arxiv.org/abs/2006.07733) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    çš„æ³¨é‡Šã€‚
- en: 'BYOL consists of two networks: the online network and the target network. The
    online network consists of three submodules, namely the encoder, projector and
    predictor. The target network consists of two submodules, namely the encoder and
    projector. The encoder and predictor of both networks share the exact same architecture,
    they only differ in their model weights. While the online network is optimized
    during training, the target network updates its weights by an exponentially moving
    average of itself and the online network.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: BYOL ç”±ä¸¤ä¸ªç½‘ç»œç»„æˆï¼šåœ¨çº¿ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œã€‚åœ¨çº¿ç½‘ç»œç”±ä¸‰ä¸ªå­æ¨¡å—ç»„æˆï¼Œå³ç¼–ç å™¨ã€æŠ•å½±å™¨å’Œé¢„æµ‹å™¨ã€‚ç›®æ ‡ç½‘ç»œç”±ä¸¤ä¸ªå­æ¨¡å—ç»„æˆï¼Œå³ç¼–ç å™¨å’ŒæŠ•å½±å™¨ã€‚ä¸¤ä¸ªç½‘ç»œçš„ç¼–ç å™¨å’Œé¢„æµ‹å™¨å…·æœ‰å®Œå…¨ç›¸åŒçš„æ¶æ„ï¼Œå®ƒä»¬ä»…åœ¨æ¨¡å‹æƒé‡ä¸Šæœ‰æ‰€ä¸åŒã€‚åœ¨çº¿ç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œä¼˜åŒ–ï¼Œè€Œç›®æ ‡ç½‘ç»œåˆ™é€šè¿‡åœ¨çº¿ç½‘ç»œå’Œè‡ªèº«çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡æ¥æ›´æ–°å…¶æƒé‡ã€‚
- en: '**Encoder** â€” The encoder consists of a ResNet convolutional neuronal network.
    It translates the input image into a latent representation.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¼–ç å™¨** â€”â€” ç¼–ç å™¨ç”±ä¸€ä¸ª ResNet å·ç§¯ç¥ç»ç½‘ç»œç»„æˆã€‚å®ƒå°†è¾“å…¥å›¾åƒè½¬æ¢ä¸ºæ½œåœ¨è¡¨ç¤ºã€‚'
- en: '**Projector** â€” Projects the latent space from a 4096-dimensional space into
    a 256-dimensional space via a multi-layer perceptron network (MLP). I guess the
    projector is not critical for the framework to work, but 256 is simply a convenient
    output dimension often used in the field of representation learning.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**æŠ•å½±å™¨** â€”â€” é€šè¿‡å¤šå±‚æ„ŸçŸ¥å™¨ç½‘ç»œï¼ˆMLPï¼‰å°†æ½œåœ¨ç©ºé—´ä» 4096 ç»´ç©ºé—´æŠ•å½±åˆ° 256 ç»´ç©ºé—´ã€‚æˆ‘çŒœæµ‹æŠ•å½±å™¨å¯¹æ¡†æ¶çš„å·¥ä½œå¹¶ä¸å…³é”®ï¼Œä½† 256
    åªæ˜¯è¡¨ç¤ºå­¦ä¹ é¢†åŸŸä¸­å¸¸ç”¨çš„æ–¹ä¾¿è¾“å‡ºç»´åº¦ã€‚'
- en: '**Predictor** â€” Aims to predict the projected latent space of the target network
    from the projected latent space of the online network. Crucial to avoid representation
    collapse.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**é¢„æµ‹å™¨** â€”â€” æ—¨åœ¨ä»åœ¨çº¿ç½‘ç»œçš„æŠ•å½±æ½œåœ¨ç©ºé—´ä¸­é¢„æµ‹ç›®æ ‡ç½‘ç»œçš„æŠ•å½±æ½œåœ¨ç©ºé—´ã€‚é¿å…è¡¨ç¤ºå´©æºƒè‡³å…³é‡è¦ã€‚'
- en: 'During training, two different and randomly selected augmentations are applied
    on an input image to construct two different views of that image. One view is
    fed into the online model and another view is fed into the target model. These
    augmentations include among others: resizing, flipping, cropping, color distortion,
    grayscale conversion, Gaussian blur and saturation. The training objective is
    to minimize the squared L2-distance between both networks output. After training,
    **only the encoder of the online network is kept as the final model!**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯¹è¾“å…¥å›¾åƒåº”ç”¨ä¸¤ä¸ªä¸åŒä¸”éšæœºé€‰æ‹©çš„å¢å¼ºæ¥æ„å»ºè¯¥å›¾åƒçš„ä¸¤ä¸ªä¸åŒè§†å›¾ã€‚ä¸€ä¸ªè§†å›¾è¢«è¾“å…¥åˆ°åœ¨çº¿æ¨¡å‹ä¸­ï¼Œå¦ä¸€ä¸ªè§†å›¾è¢«è¾“å…¥åˆ°ç›®æ ‡æ¨¡å‹ä¸­ã€‚è¿™äº›å¢å¼ºåŒ…æ‹¬ä½†ä¸é™äºï¼šè°ƒæ•´å¤§å°ã€ç¿»è½¬ã€è£å‰ªã€é¢œè‰²æ‰­æ›²ã€ç°åº¦è½¬æ¢ã€é«˜æ–¯æ¨¡ç³Šå’Œé¥±å’Œåº¦ã€‚è®­ç»ƒç›®æ ‡æ˜¯æœ€å°åŒ–ä¸¤ä¸ªç½‘ç»œè¾“å‡ºä¹‹é—´çš„å¹³æ–¹L2è·ç¦»ã€‚è®­ç»ƒåï¼Œ**æœ€ç»ˆåªä¿ç•™åœ¨çº¿ç½‘ç»œçš„ç¼–ç å™¨ä½œä¸ºæœ€ç»ˆæ¨¡å‹ï¼**
- en: 'Thatâ€™s all. Easy, right? ğŸ˜œ Well, after reading the paper my face was more like
    this: ğŸ˜µ While it is relatively straight forward to understand the processing of
    the framework if you break it down to its key components, gaining a intuition
    did cost me quite some time.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™æ ·ã€‚ç®€å•ï¼Œå¯¹å§ï¼ŸğŸ˜œ å¥½å§ï¼Œçœ‹å®Œè®ºæ–‡åæˆ‘çš„è¡¨æƒ…æ›´åƒè¿™æ ·ï¼šğŸ˜µ è™½ç„¶å°†æ¡†æ¶çš„å¤„ç†è¿‡ç¨‹åˆ†è§£ä¸ºå…¶å…³é”®ç»„ä»¶ç›¸å¯¹ç›´æ¥ï¼Œä½†è·å¾—ç›´è§‰ç¡®å®èŠ±è´¹äº†æˆ‘ä¸å°‘æ—¶é—´ã€‚
- en: Before we try to gain some intuition of why BYOL actually works, letâ€™s first
    strip down the presented equations and demystify them.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬å°è¯•ç†è§£ä¸ºä»€ä¹ˆBYOLå®é™…æœ‰æ•ˆä¹‹å‰ï¼Œè®©æˆ‘ä»¬é¦–å…ˆç®€åŒ–å‘ˆç°çš„æ–¹ç¨‹å¹¶æ­ç¤ºå®ƒä»¬çš„å¥¥ç§˜ã€‚
- en: Math Demystification
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°å­¦æ­ç§˜
- en: Having a rough overview of BYOLâ€™s architecture and how it is trained, letâ€™s
    have a closer look at the equations. I have to say, the math part presented in
    the paper is way more complicated than it needs to be. While in some cases it
    is presented way to complex, in other cases it lags on clarity and leaves room
    for interpretation causing confusion.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤§è‡´äº†è§£äº†BYOLçš„æ¶æ„åŠå…¶è®­ç»ƒæ–¹å¼åï¼Œè®©æˆ‘ä»¬ä»”ç»†çœ‹çœ‹è¿™äº›æ–¹ç¨‹ã€‚æˆ‘ä¸å¾—ä¸è¯´ï¼Œè®ºæ–‡ä¸­å‘ˆç°çš„æ•°å­¦éƒ¨åˆ†æ¯”å®é™…éœ€è¦çš„è¦å¤æ‚å¾—å¤šã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå®ƒçš„å±•ç¤ºæ–¹å¼è¿‡äºå¤æ‚ï¼Œè€Œåœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œå®ƒåœ¨æ¸…æ™°åº¦ä¸Šæ»åï¼Œç•™ä¸‹äº†è§£é‡Šçš„ç©ºé—´ï¼Œé€ æˆæ··æ·†ã€‚
- en: Iâ€™ll focus on those equations from which I think are important to understand
    what is happening. Letâ€™s start by analyzing them in the exact reversed order,
    because why not? ğŸ˜œ
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¼šä¸“æ³¨äºé‚£äº›æˆ‘è®¤ä¸ºç†è§£å‘ç”Ÿäº†ä»€ä¹ˆçš„æ–¹ç¨‹ã€‚æˆ‘ä»¬ä»ç²¾ç¡®å€’åºåˆ†æè¿™äº›æ–¹ç¨‹å¼€å§‹ï¼Œä¸ºä»€ä¹ˆä¸å‘¢ï¼ŸğŸ˜œ
- en: 'First, letâ€™s talk about the update of the modelsâ€™ parameters during training.
    Recall that we have two models: the online model and the target model. The online
    model is updated by optimizing a loss function using a LARS optimizer.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬è°ˆè°ˆè®­ç»ƒæœŸé—´æ¨¡å‹å‚æ•°çš„æ›´æ–°ã€‚å›é¡¾ä¸€ä¸‹ï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªæ¨¡å‹ï¼šåœ¨çº¿æ¨¡å‹å’Œç›®æ ‡æ¨¡å‹ã€‚åœ¨çº¿æ¨¡å‹é€šè¿‡ä½¿ç”¨LARSä¼˜åŒ–å™¨ä¼˜åŒ–æŸå¤±å‡½æ•°æ¥æ›´æ–°ã€‚
- en: '![](../Images/b3fdffacdfd667decdc3fe64eaba9260.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3fdffacdfd667decdc3fe64eaba9260.png)'
- en: 'Equation 1: Weight update of online network. [Source](https://arxiv.org/abs/2006.07733)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 1ï¼šåœ¨çº¿ç½‘ç»œçš„æƒé‡æ›´æ–°ã€‚[æ¥æº](https://arxiv.org/abs/2006.07733) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    çš„æ³¨é‡Š
- en: 'The equation above simply says: â€œupdate the modelâ€™s parameters theta by calling
    an optimizer function upon the current parameters, the gradients of these parameters
    with respect to a loss function and a learning rate etaâ€.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šé¢çš„æ–¹ç¨‹ç®€å•åœ°è¯´ï¼šâ€œé€šè¿‡è°ƒç”¨ä¼˜åŒ–å™¨å‡½æ•°å¯¹å½“å‰å‚æ•°ã€è¿™äº›å‚æ•°ç›¸å¯¹äºæŸå¤±å‡½æ•°çš„æ¢¯åº¦ä»¥åŠå­¦ä¹ ç‡etaè¿›è¡Œæ›´æ–°ï¼Œæ¥æ›´æ–°æ¨¡å‹çš„å‚æ•°thetaâ€ã€‚
- en: 'The target model on the other hand is not updated via optimization but by copying
    the weights from the online model and applying an exponential moving average on
    the copied updated weights and the current weights of the target network:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œç›®æ ‡æ¨¡å‹ä¸æ˜¯é€šè¿‡ä¼˜åŒ–æ¥æ›´æ–°ï¼Œè€Œæ˜¯é€šè¿‡ä»åœ¨çº¿æ¨¡å‹ä¸­å¤åˆ¶æƒé‡ï¼Œå¹¶å¯¹å¤åˆ¶çš„æ›´æ–°æƒé‡å’Œç›®æ ‡ç½‘ç»œçš„å½“å‰æƒé‡åº”ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼š
- en: '![](../Images/9325b82871ba916524d9e0601e02b0ab.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9325b82871ba916524d9e0601e02b0ab.png)'
- en: 'Equation 2: Weight update of target network. [Source](https://arxiv.org/abs/2006.07733)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 2ï¼šç›®æ ‡ç½‘ç»œçš„æƒé‡æ›´æ–°ã€‚[æ¥æº](https://arxiv.org/abs/2006.07733) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    çš„æ³¨é‡Š
- en: 'The equation above simply says: â€œupdate the modelâ€™s parameter xi by calculating
    an exponential moving average with the decay rate tau of the current weights xi
    and the updated weights of the online modelâ€. Tau follows a cosine schedule to
    decrease the contribution of the online model throughout the training.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šé¢çš„æ–¹ç¨‹ç®€å•åœ°è¯´ï¼šâ€œé€šè¿‡è®¡ç®—å½“å‰æƒé‡xiå’Œåœ¨çº¿æ¨¡å‹çš„æ›´æ–°æƒé‡çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡æ¥æ›´æ–°æ¨¡å‹çš„å‚æ•°xiâ€ã€‚Tauéµå¾ªä½™å¼¦è°ƒåº¦ï¼Œä»¥å‡å°‘æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­åœ¨çº¿æ¨¡å‹çš„è´¡çŒ®ã€‚
- en: Now letâ€™s have a look on the loss function used to update the online model.
    It is defined as the sum of two other loss functions. These losses share the same
    equation as we will see later but are calculated on two different inputs of the
    network. Recall from Fig. 1\. that two different views (i.e. v and vâ€™) are generated
    from an image x by applying different augmentations. One view is input into the
    online model and the other one into the target model. During training, two forward
    passes are performed before calculating the loss, where the input for the networks
    are swapped. The image input to the online model is input into the target model
    and vice versa.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹ç”¨äºæ›´æ–°åœ¨çº¿æ¨¡å‹çš„æŸå¤±å‡½æ•°ã€‚å®ƒè¢«å®šä¹‰ä¸ºä¸¤ä¸ªå…¶ä»–æŸå¤±å‡½æ•°çš„å’Œã€‚è¿™äº›æŸå¤±å‡½æ•°å…·æœ‰ç›¸åŒçš„æ–¹ç¨‹ï¼Œç¨åæˆ‘ä»¬å°†çœ‹åˆ°ï¼Œä½†åœ¨ç½‘ç»œçš„ä¸¤ä¸ªä¸åŒè¾“å…¥ä¸Šè®¡ç®—ã€‚å›å¿†ä¸€ä¸‹å›¾
    1\.ï¼Œä»å›¾åƒ x ç”Ÿæˆäº†ä¸¤ä¸ªä¸åŒçš„è§†è§’ï¼ˆå³ v å’Œ v'ï¼‰ï¼Œé€šè¿‡åº”ç”¨ä¸åŒçš„å¢å¼ºã€‚ä¸€ä¸ªè§†è§’è¾“å…¥åˆ°åœ¨çº¿æ¨¡å‹ä¸­ï¼Œå¦ä¸€ä¸ªè¾“å…¥åˆ°ç›®æ ‡æ¨¡å‹ä¸­ã€‚åœ¨è®­ç»ƒæœŸé—´ï¼Œè®¡ç®—æŸå¤±ä¹‹å‰æ‰§è¡Œä¸¤ä¸ªå‰å‘ä¼ é€’ï¼Œå…¶ä¸­ç½‘ç»œçš„è¾“å…¥ä¼šäº¤æ¢ã€‚è¾“å…¥åˆ°åœ¨çº¿æ¨¡å‹çš„å›¾åƒä¼šè¾“å…¥åˆ°ç›®æ ‡æ¨¡å‹ä¸­ï¼Œåä¹‹äº¦ç„¶ã€‚
- en: '![](../Images/045f6f999186a4c829428be4c00eca0e.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/045f6f999186a4c829428be4c00eca0e.png)'
- en: 'Equation 3: BYOLâ€™s loss function. [Source](https://arxiv.org/abs/2006.07733)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 3ï¼šBYOL çš„æŸå¤±å‡½æ•°ã€‚ [æ¥æº](https://arxiv.org/abs/2006.07733) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    çš„æ³¨é‡Š
- en: 'The loss for the individual forward passes is a squared L2 distance of the
    L2-normalized outputs of the online model and the target model. Letâ€™s break down
    the corresponding equation from the paper:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ªä½“å‰å‘ä¼ é€’çš„æŸå¤±æ˜¯åœ¨çº¿æ¨¡å‹å’Œç›®æ ‡æ¨¡å‹çš„ L2 å½’ä¸€åŒ–è¾“å‡ºçš„å¹³æ–¹ L2 è·ç¦»ã€‚è®©æˆ‘ä»¬åˆ†è§£è®ºæ–‡ä¸­çš„ç›¸åº”æ–¹ç¨‹ï¼š
- en: '![](../Images/2e456b27a6689941529b2fb798f4c768.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e456b27a6689941529b2fb798f4c768.png)'
- en: 'Equation 4: Individual loss function. [Source](https://arxiv.org/abs/2006.07733)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 4ï¼šä¸ªä½“æŸå¤±å‡½æ•°ã€‚ [æ¥æº](https://arxiv.org/abs/2006.07733) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    çš„æ³¨é‡Š
- en: 'Note: The paper says that this is a mean squared error, which is actually not
    correct. The L2-distance does not divide by its number of elements. I guess they
    confused it with calculating the mean over all batches.'
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ³¨ï¼šè®ºæ–‡ä¸­æåˆ°è¿™æ˜¯å‡æ–¹è¯¯å·®ï¼Œå®é™…ä¸Šå¹¶ä¸æ­£ç¡®ã€‚L2 è·ç¦»æ²¡æœ‰é™¤ä»¥å…¶å…ƒç´ æ•°é‡ã€‚æˆ‘çŒœä»–ä»¬å°†å…¶ä¸è®¡ç®—æ‰€æœ‰æ‰¹æ¬¡çš„å‡å€¼æ··æ·†äº†ã€‚
- en: Intuition of BYOL
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BYOL çš„ç›´è§‰
- en: Now as we are equipped with an understanding of the framework and the key message
    of the equations, let us try to gain some intuition. Iâ€™ll present you what the
    authors think and then Iâ€™ll try to add some intuition of my own, well knowing
    it might not be accurate ğŸ¤¡.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»ç†è§£äº†æ¡†æ¶å’Œæ–¹ç¨‹çš„æ ¸å¿ƒä¿¡æ¯ï¼Œè®©æˆ‘ä»¬å°è¯•è·å¾—ä¸€äº›ç›´è§‰ã€‚æˆ‘ä¼šå‘ˆç°ä½œè€…çš„è§‚ç‚¹ï¼Œç„¶åæˆ‘ä¼šå°è¯•åŠ å…¥ä¸€äº›æˆ‘è‡ªå·±çš„ç›´è§‰ï¼Œè™½ç„¶çŸ¥é“è¿™å¯èƒ½ä¸å‡†ç¡® ğŸ¤¡ã€‚
- en: '***How does BYOL learn its representations?*** â€” The model is encouraged to
    generate the same latent representation of its two inputs, which represent two
    different views of the same object/scene. A cat is still a cat regardless of the
    image being blurred, in grayscale or flipped. In fact, I think the heavy augmentations
    are crucial here. It basically tells the model â€œLook, these are different variations
    of the same thing, so ignore these variations and consider them equal when extracting
    representations of the object/scene!â€.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '***BYOL å¦‚ä½•å­¦ä¹ å…¶è¡¨ç¤ºï¼Ÿ*** â€” æ¨¡å‹è¢«é¼“åŠ±ç”Ÿæˆå…¶ä¸¤ä¸ªè¾“å…¥çš„ç›¸åŒæ½œåœ¨è¡¨ç¤ºï¼Œè¿™ä¸¤ä¸ªè¾“å…¥ä»£è¡¨äº†åŒä¸€å¯¹è±¡/åœºæ™¯çš„ä¸åŒè§†è§’ã€‚æ— è®ºå›¾åƒæ˜¯æ¨¡ç³Šçš„ã€é»‘ç™½çš„è¿˜æ˜¯ç¿»è½¬çš„ï¼ŒçŒ«ä»ç„¶æ˜¯çŒ«ã€‚äº‹å®ä¸Šï¼Œæˆ‘è®¤ä¸ºå¼ºå¤§çš„æ•°æ®å¢å¼ºåœ¨è¿™é‡Œè‡³å…³é‡è¦ã€‚å®ƒåŸºæœ¬ä¸Šå‘Šè¯‰æ¨¡å‹ï¼šâ€œçœ‹ï¼Œè¿™äº›æ˜¯åŒä¸€äº‹ç‰©çš„ä¸åŒå˜ä½“ï¼Œæ‰€ä»¥å¿½ç•¥è¿™äº›å˜ä½“ï¼Œå½“æå–å¯¹è±¡/åœºæ™¯çš„è¡¨ç¤ºæ—¶ï¼Œå°†å®ƒä»¬è§†ä¸ºç›¸ç­‰ï¼â€ã€‚'
- en: '***Why are the representations not collapsing?*** â€” Recall that earlier we
    said, BYOL falls into the category of similarity learning. Wouldnâ€™t it be the
    easiest way for the network, to just map everything into the same point in the
    latent space to achieve the highest similarity? In fact, this is one of the mayor
    difficulties in similarity learning and is called â€œcollapsing solutionsâ€. Contrastive
    learning approaches solve this issue by providing many negative samples for a
    given match to map similar features close to each other in the latent space while
    mapping dissimilar features farther apart. BYOL solves this issue by introducing
    an asymmetry between the online and the target network with their predictor submodule
    and by employing an update rule for the target network parameters based on the
    exponentially moving average to ensure near optimality of the predictor throughout
    training.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä¸ºä»€ä¹ˆè¡¨ç¤ºæ²¡æœ‰å´©æºƒï¼Ÿ*** â€” å›é¡¾ä¹‹å‰æˆ‘ä»¬æåˆ°ï¼ŒBYOLå±äºç›¸ä¼¼æ€§å­¦ä¹ çš„èŒƒç•´ã€‚ç½‘ç»œä¸ä¼šæœ€ç®€å•åœ°å°†æ‰€æœ‰å†…å®¹æ˜ å°„åˆ°æ½œåœ¨ç©ºé—´çš„åŒä¸€ç‚¹ä»¥å®ç°æœ€é«˜ç›¸ä¼¼åº¦å—ï¼Ÿå®é™…ä¸Šï¼Œè¿™æ˜¯ç›¸ä¼¼æ€§å­¦ä¹ ä¸­çš„ä¸€ä¸ªä¸»è¦å›°éš¾ï¼Œç§°ä¸ºâ€œå´©æºƒè§£å†³æ–¹æ¡ˆâ€ã€‚å¯¹æ¯”å­¦ä¹ æ–¹æ³•é€šè¿‡ä¸ºæ¯ä¸ªåŒ¹é…æä¾›è®¸å¤šè´Ÿæ ·æœ¬æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä»¥å°†ç›¸ä¼¼ç‰¹å¾æ˜ å°„åˆ°æ½œåœ¨ç©ºé—´ä¸­çš„å½¼æ­¤æ¥è¿‘ï¼ŒåŒæ—¶å°†ä¸ç›¸ä¼¼çš„ç‰¹å¾æ˜ å°„åˆ°æ›´è¿œçš„ä½ç½®ã€‚BYOLé€šè¿‡åœ¨åœ¨çº¿ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œä¹‹é—´å¼•å…¥ä¸å¯¹ç§°æ€§åŠå…¶é¢„æµ‹å­æ¨¡å—ï¼Œå¹¶é€šè¿‡åŸºäºæŒ‡æ•°ç§»åŠ¨å¹³å‡çš„æ›´æ–°è§„åˆ™æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä»¥ç¡®ä¿åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­é¢„æµ‹å™¨çš„è¿‘ä¼¼æœ€ä¼˜ã€‚'
- en: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----5d0a26983d7c--------------------------------)
    [## Get an email whenever Sascha Kirch publishes ğŸš€'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----5d0a26983d7c--------------------------------)
    [## è·å–Sascha Kirchå‘å¸ƒçš„æ–°é‚®ä»¶é€šçŸ¥ ğŸš€'
- en: Get an email whenever Sascha Kirch publishes ğŸš€ Looking to learn more about deep
    learning or simply stay up to dateâ€¦
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è·å–Sascha Kirchå‘å¸ƒçš„æ–°é‚®ä»¶é€šçŸ¥ ğŸš€ æƒ³è¦äº†è§£æ›´å¤šæ·±åº¦å­¦ä¹ çŸ¥è¯†æˆ–ä¿æŒæœ€æ–°åŠ¨æ€â€¦
- en: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----5d0a26983d7c--------------------------------)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----5d0a26983d7c--------------------------------)
- en: Experiments
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®éªŒ
- en: The authors of BYOL presented experiments and ablations to demonstrate the effectiveness
    of their method.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: BYOLçš„ä½œè€…å±•ç¤ºäº†å®éªŒå’Œæ¶ˆèç ”ç©¶ï¼Œä»¥è¯æ˜ä»–ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚
- en: '*Ablation on Batch Size*'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*æ‰¹é‡å¤§å°çš„æ¶ˆèç ”ç©¶*'
- en: From contrastive representation learning methods (e.g. [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    and [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05))
    we know that there is a large dependence on the batch size during training. [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    for example was trained on a batch size of 32,768, which is crazy considering
    it is a multi-modal language-image model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å¯¹æ¯”è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œ[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)å’Œ[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)ï¼‰æˆ‘ä»¬çŸ¥é“ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ‰¹é‡å¤§å°æœ‰å¾ˆå¤§çš„ä¾èµ–æ€§ã€‚ä¾‹å¦‚ï¼Œ[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)æ˜¯åœ¨æ‰¹é‡å¤§å°ä¸º32,768çš„æƒ…å†µä¸‹è®­ç»ƒçš„ï¼Œè¿™åœ¨è€ƒè™‘åˆ°å®ƒæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€è¯­è¨€-å›¾åƒæ¨¡å‹æ—¶æ˜¾å¾—éå¸¸ç–¯ç‹‚ã€‚
- en: The authors claim, since BYOL does not require negative samples, it is not as
    sensitive to lower batch sizes which they backup with the following experiment
    shown in Fig.2.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…å£°ç§°ï¼Œç”±äºBYOLä¸éœ€è¦è´Ÿæ ·æœ¬ï¼Œå®ƒå¯¹è¾ƒä½æ‰¹é‡å¤§å°çš„æ•æ„Ÿåº¦è¾ƒä½ï¼Œä»–ä»¬é€šè¿‡å›¾2æ‰€ç¤ºçš„å®éªŒè¿›è¡Œäº†éªŒè¯ã€‚
- en: '![](../Images/4bc8059a6403de2b0bca72cd8eaf9d4b.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bc8059a6403de2b0bca72cd8eaf9d4b.png)'
- en: 'Fig. 2: Impact of batch size. [Image Source](https://arxiv.org/abs/2006.07733)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2ï¼šæ‰¹é‡å¤§å°çš„å½±å“ã€‚[å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2006.07733) + [Sascha Kirch](https://medium.com/@SaschaKirch)çš„æ³¨é‡Šã€‚
- en: Sadly, this might still be too large for my private laptop ğŸ˜…
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å¯æƒœï¼Œè¿™å¯¹äºæˆ‘çš„ç§äººç¬”è®°æœ¬ç”µè„‘æ¥è¯´å¯èƒ½ä»ç„¶å¤ªå¤§äº† ğŸ˜…
- en: Ablation on Robustness of Image Augmentations
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›¾åƒå¢å¼ºçš„é²æ£’æ€§æ¶ˆèç ”ç©¶
- en: The [SimCLR](https://arxiv.org/abs/2002.05709) paper has shown that contrastive
    vision methods are sensitive to their choice on image augmentations, especially
    those affecting the color histogram. While crops of the same image share a similar
    color histogram, crops of negative pairs donâ€™t. The model can take a shortcut
    during training and focus on differences in color histograms rather than the semantic
    features.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[SimCLR](https://arxiv.org/abs/2002.05709)è®ºæ–‡è¡¨æ˜ï¼Œå¯¹æ¯”è§†è§‰æ–¹æ³•å¯¹å›¾åƒå¢å¼ºçš„é€‰æ‹©éå¸¸æ•æ„Ÿï¼Œç‰¹åˆ«æ˜¯é‚£äº›å½±å“é¢œè‰²ç›´æ–¹å›¾çš„å¢å¼ºã€‚è™½ç„¶ç›¸åŒå›¾åƒçš„è£å‰ªéƒ¨åˆ†å…±äº«ç±»ä¼¼çš„é¢œè‰²ç›´æ–¹å›¾ï¼Œä½†è´Ÿæ ·æœ¬å¯¹çš„è£å‰ªéƒ¨åˆ†åˆ™ä¸ç„¶ã€‚æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½ä¼šèµ°æ·å¾„ï¼Œä¸“æ³¨äºé¢œè‰²ç›´æ–¹å›¾çš„å·®å¼‚ï¼Œè€Œä¸æ˜¯è¯­ä¹‰ç‰¹å¾ã€‚'
- en: The authors claim that BYOL is more robust towards their choice of image augmentations,
    because of the way the online and target networks are updated. While this hypothesis
    is backed up by an experiment, there is still a strong dependency and hence a
    drop in performance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…ä»¬å£°ç§°ï¼ŒBYOLå¯¹å›¾åƒå¢å¼ºé€‰æ‹©çš„é²æ£’æ€§æ›´å¼ºï¼Œå› ä¸ºåœ¨çº¿å’Œç›®æ ‡ç½‘ç»œçš„æ›´æ–°æ–¹å¼ã€‚è™½ç„¶è¿™ä¸€å‡è®¾å¾—åˆ°äº†å®éªŒçš„æ”¯æŒï¼Œä½†ä»ç„¶å­˜åœ¨è¾ƒå¼ºçš„ä¾èµ–æ€§ï¼Œå› æ­¤æ€§èƒ½æœ‰æ‰€ä¸‹é™ã€‚
- en: '![](../Images/29ea63b28df3e850c03a04e04176a409.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29ea63b28df3e850c03a04e04176a409.png)'
- en: 'Fig. 3: Robustness towards image augmentations. [Image Source](https://arxiv.org/abs/2006.07733)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3ï¼šå¯¹å›¾åƒå¢å¼ºçš„é²æ£’æ€§ã€‚[å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2006.07733) + [Sascha Kirch](https://medium.com/@SaschaKirch)çš„æ³¨é‡Šã€‚
- en: Linear Evaluation on ImageNet
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨ImageNetä¸Šçš„çº¿æ€§è¯„ä¼°
- en: In the field of representation learning, an important characteristic is the
    modelâ€™s ability project semantically rich features into a latent space, to cluster
    similar features and to separate dissimilar features. A common test is to freeze
    the model (in case of BYOL only the encoder of the online model) and to train
    a linear classifier on top of the representations.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¡¨ç¤ºå­¦ä¹ é¢†åŸŸï¼Œä¸€ä¸ªé‡è¦çš„ç‰¹å¾æ˜¯æ¨¡å‹å°†è¯­ä¹‰ä¸°å¯Œçš„ç‰¹å¾æŠ•å°„åˆ°æ½œåœ¨ç©ºé—´çš„èƒ½åŠ›ï¼Œä»¥ä¾¿å¯¹ç›¸ä¼¼ç‰¹å¾è¿›è¡Œèšç±»ï¼Œå¹¶å°†ä¸åŒçš„ç‰¹å¾åˆ†å¼€ã€‚ä¸€ä¸ªå¸¸è§çš„æµ‹è¯•æ˜¯å†»ç»“æ¨¡å‹ï¼ˆå¯¹äºBYOLï¼Œåªå†»ç»“åœ¨çº¿æ¨¡å‹çš„ç¼–ç å™¨ï¼‰ï¼Œå¹¶åœ¨è¡¨ç¤ºçš„åŸºç¡€ä¸Šè®­ç»ƒçº¿æ€§åˆ†ç±»å™¨ã€‚
- en: Linear evaluation of BYOL has been performed on ImageNet and has been compared
    to many other models and outperforms the previous state-of-the-art of that time.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: BYOLçš„çº¿æ€§è¯„ä¼°å·²åœ¨ImageNetä¸Šè¿›è¡Œï¼Œå¹¶ä¸è®¸å¤šå…¶ä»–æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒï¼Œè¶…è¶Šäº†å½“æ—¶çš„æœ€æ–°æŠ€æœ¯ã€‚
- en: Youâ€™ll find in many papers the differentiation between ResNet-50 encoder and
    other variations of ResNet. Itâ€™s just that the ResNet-50 has been emerged to be
    the standard network to evaluate performance on.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¼šåœ¨è®¸å¤šè®ºæ–‡ä¸­å‘ç°ResNet-50ç¼–ç å™¨ä¸å…¶ä»–ResNetå˜ä½“çš„åŒºåˆ«ã€‚åªæ˜¯ResNet-50å·²ç»æˆä¸ºè¯„ä¼°æ€§èƒ½çš„æ ‡å‡†ç½‘ç»œã€‚
- en: '![](../Images/16b12891474954baee3a1cb16206edaf.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16b12891474954baee3a1cb16206edaf.png)'
- en: 'Table 1: Linear evaluation on ImageNet. [Source](https://arxiv.org/abs/2006.07733)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨1ï¼šåœ¨ImageNetä¸Šçš„çº¿æ€§è¯„ä¼°ã€‚[æ¥æº](https://arxiv.org/abs/2006.07733)
- en: Semi-Supervised Fine-Tuning for classification
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠç›‘ç£å¾®è°ƒç”¨äºåˆ†ç±»
- en: Another very typical experiment setup in representation learning is the modelâ€™s
    performance when fine-tuned to a specific downstream task and dataset.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ç¤ºå­¦ä¹ ä¸­çš„å¦ä¸€ä¸ªå…¸å‹å®éªŒè®¾ç½®æ˜¯æ¨¡å‹åœ¨ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒæ—¶çš„è¡¨ç°ã€‚
- en: Table 2 depicts the metrics when finetuning BYOL on a classification task using
    either 1% or 10% of the entire ImageNet training set.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨2å±•ç¤ºäº†åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šå¯¹BYOLè¿›è¡Œå¾®è°ƒæ—¶ä½¿ç”¨1%æˆ–10%çš„æ•´ä¸ªImageNetè®­ç»ƒé›†çš„æŒ‡æ ‡ã€‚
- en: '![](../Images/770661dfb170a1d00aa5b21ca752decc.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/770661dfb170a1d00aa5b21ca752decc.png)'
- en: 'Table 2: Semi-supervised training on ImageNet. [Source](https://arxiv.org/abs/2006.07733)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨2ï¼šåœ¨ImageNetä¸Šçš„åŠç›‘ç£è®­ç»ƒã€‚[æ¥æº](https://arxiv.org/abs/2006.07733)
- en: Transfer to Other Vision Tasks
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿ç§»åˆ°å…¶ä»–è§†è§‰ä»»åŠ¡
- en: The authors also present experiments where they transfer-learn BYOL on a semantic
    segmentation task and a monocular depth estimation task, two other important fields
    of computer vision.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…ä»¬è¿˜å±•ç¤ºäº†ä»–ä»¬åœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡å’Œå•ç›®æ·±åº¦ä¼°è®¡ä»»åŠ¡ï¼ˆè®¡ç®—æœºè§†è§‰çš„ä¸¤ä¸ªé‡è¦é¢†åŸŸï¼‰ä¸­è¿ç§»å­¦ä¹ BYOLçš„å®éªŒã€‚
- en: The differences to previous approaches are marginal, but I guess the key message
    here is, â€œWe have a different approach that works just as goodâ€
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œå·®å¼‚å¾®ä¹å…¶å¾®ï¼Œä½†æˆ‘æƒ³è¿™é‡Œçš„å…³é”®ä¿¡æ¯æ˜¯ï¼Œâ€œæˆ‘ä»¬æœ‰ä¸€ç§ä¸åŒçš„æ–¹æ³•ï¼Œæ•ˆæœåŒæ ·å‡ºè‰²ã€‚â€
- en: '![](../Images/a569a55cb7fd46bed3e371110e8cf386.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a569a55cb7fd46bed3e371110e8cf386.png)'
- en: 'Table 3: Transfer to other vision tasks. [Source](https://arxiv.org/abs/2006.07733)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨3ï¼šè¿ç§»åˆ°å…¶ä»–è§†è§‰ä»»åŠ¡ã€‚[æ¥æº](https://arxiv.org/abs/2006.07733)
- en: Conclusion
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: BYOL presented an alternative approach for self-supervised representation learning.
    By implementing two networks that perform similarity learning, BYOL can be trained
    without the need of negative training samples like those needed for contrastive
    learning approaches. To avoid collapsing solutions the target network is updated
    via EMA from the online network and an extra prediction sub-module is built on
    top of the online network.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: BYOLæå‡ºäº†ä¸€ç§è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ çš„æ›¿ä»£æ–¹æ³•ã€‚é€šè¿‡å®ç°ä¸¤ä¸ªè¿›è¡Œç›¸ä¼¼æ€§å­¦ä¹ çš„ç½‘ç»œï¼ŒBYOLå¯ä»¥åœ¨æ²¡æœ‰å¯¹æ¯”å­¦ä¹ æ–¹æ³•æ‰€éœ€çš„è´Ÿæ ·æœ¬çš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†é¿å…å´©æºƒè§£ï¼Œç›®æ ‡ç½‘ç»œé€šè¿‡EMAä»åœ¨çº¿ç½‘ç»œä¸­æ›´æ–°ï¼Œå¹¶åœ¨åœ¨çº¿ç½‘ç»œä¹‹ä¸Šæ„å»ºäº†ä¸€ä¸ªé¢å¤–çš„é¢„æµ‹å­æ¨¡å—ã€‚
- en: Further Readings & Resources
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»ä¸èµ„æº
- en: 'If you have made it so far: congratulationsğŸ‰ and thank youğŸ˜‰! Since it seems
    that you are quite interested in the topic, here are some further resources:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å·²ç»è¯»åˆ°è¿™é‡Œï¼šæ­å–œğŸ‰å¹¶æ„Ÿè°¢ğŸ˜‰ï¼æ—¢ç„¶ä½ ä¼¼ä¹å¯¹è¿™ä¸ªè¯é¢˜å¾ˆæ„Ÿå…´è¶£ï¼Œè¿™é‡Œæœ‰ä¸€äº›è¿›ä¸€æ­¥çš„èµ„æºï¼š
- en: 'Following a list of papers that built upon BYOL:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯åŸºäºBYOLçš„è®ºæ–‡åˆ—è¡¨ï¼š
- en: '[DINO: Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[DINO: è‡ªç›‘ç£è§†è§‰å˜æ¢å™¨ä¸­çš„æ–°å…´ç‰¹æ€§](https://arxiv.org/abs/2104.14294)'
- en: '[DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[DINOv2: æ— ç›‘ç£å­¦ä¹ å¼ºå¥çš„è§†è§‰ç‰¹å¾](https://arxiv.org/abs/2304.07193)'
- en: 'Here are two of my articles about the contrastive learning methods [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    and [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)
    for self-supervised representation learning:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯æˆ‘å…³äºå¯¹æ¯”å­¦ä¹ æ–¹æ³•çš„ä¸¤ç¯‡æ–‡ç« ï¼š[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)å’Œ[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)ï¼Œç”¨äºè‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼š
- en: '[](/the-clip-foundation-model-7770858b487d?source=post_page-----5d0a26983d7c--------------------------------)
    [## The CLIP Foundation Model'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/the-clip-foundation-model-7770858b487d?source=post_page-----5d0a26983d7c--------------------------------)
    [## CLIPåŸºç¡€æ¨¡å‹'
- en: Paper Summaryâ€” Learning Transferable Visual Models From Natural Language Supervision
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ€»ç»“â€”â€”ä»è‡ªç„¶è¯­è¨€ç›‘ç£ä¸­å­¦ä¹ å¯è¿ç§»çš„è§†è§‰æ¨¡å‹
- en: 'towardsdatascience.com](/the-clip-foundation-model-7770858b487d?source=post_page-----5d0a26983d7c--------------------------------)
    [](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----5d0a26983d7c--------------------------------)
    [## GLIP: Introducing Language-Image Pre-Training to Object Detection'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 'towardsdatascience.com](/the-clip-foundation-model-7770858b487d?source=post_page-----5d0a26983d7c--------------------------------)
    [](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----5d0a26983d7c--------------------------------)
    [## GLIP: å°†è¯­è¨€-å›¾åƒé¢„è®­ç»ƒå¼•å…¥ç›®æ ‡æ£€æµ‹'
- en: 'Paper Summary: Grounded Language-Image Pre-training'
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ€»ç»“ï¼šåŸºç¡€è¯­è¨€-å›¾åƒé¢„è®­ç»ƒ
- en: towardsdatascience.com](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----5d0a26983d7c--------------------------------)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----5d0a26983d7c--------------------------------)
