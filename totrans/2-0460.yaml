- en: BYOL ‚ÄîThe Alternative to Contrastive Self-Supervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c](https://towardsdatascience.com/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[üöÄSascha‚Äôs Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning by J.
    Grill et. al.'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@SaschaKirch?source=post_page-----5d0a26983d7c--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----5d0a26983d7c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5d0a26983d7c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5d0a26983d7c--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5d0a26983d7c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5d0a26983d7c--------------------------------)
    ¬∑10 min read¬∑Sep 7, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: In today‚Äôs paper analysis we will have a close look into the paper behind BYOL
    (**B**ootstrap **Y**our **O**wn **L**atent). It provides an alternative to contrastive
    self-supervised learning techniques for representation learning removing the need
    for a large corpus of negative samples and gigantic batch sizes. Furthermore it
    is a landmark paper on the path of understanding today‚Äôs state-of-the-art foundation
    models such as the [DINO](https://arxiv.org/abs/2104.14294) family, including
    [DINOv2](https://arxiv.org/abs/2304.07193).
  prefs: []
  type: TYPE_NORMAL
- en: While contrastive self-supervised learning frameworks still feel kind of intuitive,
    BYOL can be confusing and intimidating at first. Therefore, it‚Äôs a great paper
    to analyze together. So let‚Äôs dive into it and strip it down to uncover its core
    ideas!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/63946f35ab17c2db935b29bf3105ec08.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created from [publication](https://arxiv.org/abs/2006.07733) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  prefs: []
  type: TYPE_NORMAL
- en: '**Paper:** [Bootstrap your own latent: A new approach to self-supervised Learning](https://arxiv.org/abs/2006.07733)
    by Jean-Bastien Grill et. al., 13 Jun. 2020'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/deepmind/deepmind-research/tree/master/byol)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** similarity learning, representation learning, computer vision,
    foundation models'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    ‚Äî [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    ‚Äî [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    ‚Äî [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    ‚Äî [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    ‚Äî [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Outline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Context & Background
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Claimed Contributions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Method
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiments
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further Readings & Resources
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Context & Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BYOL falls into the category of self-supervised representation learning via
    similarity learning. Self-supervised means no explicit ground truth labels are
    provided, but a supervision signal might be constructed from unlabeled data. Representation
    learning means the model learns to encode its input into a lower dimensional and
    semantically rich representation space. And finally in similarity learning features
    that are similar are mapped close to each other in the latent representation space,
    while non-similar features are mapped further apart. These representations are
    crucial in many deep learning tasks that built upon these representations to for
    example generate new data, perform classification, segmentation or monocular depth
    estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Many successful methods, such as [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500),
    [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05),
    [MoCo](https://arxiv.org/abs/1911.05722) or [SimCLR](https://arxiv.org/abs/2002.05709)
    use a contrastive learning approach. In contrastive learning, a score for matching
    data pairs is maximized, while a score for non-matching data is minimized. This
    process does heavily depend on the batch size and the number of negative samples
    provided during training. This dependency makes data collection and training more
    challenging.
  prefs: []
  type: TYPE_NORMAL
- en: 'BYOL aims to:'
  prefs: []
  type: TYPE_NORMAL
- en: Get rid for the need of negative samples and large batch size as required for
    contrastive learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decrease the dependency to domain specific augmentations to be applicable to
    other domains as language or images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Among many references made in the paper, BYOL highlights its similarities to
    [mean teacher](https://arxiv.org/abs/1703.01780), the [momentum encoder](https://arxiv.org/abs/1911.05722)
    and [predictions of bootstrapped latents (PBL)](https://arxiv.org/abs/2004.14646).
  prefs: []
  type: TYPE_NORMAL
- en: Claimed Contributions (According to Authors)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction of BYOL (Bootstrap your own latent), a self-supervised representation
    learning method that does not require negative pairs (as in contrastive learning)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BYOL representations are shown to outperform the state-of-the-art (at the time
    of the paper‚Äôs release)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BYOL is shown to be more resilient to batch size and used image augmentations
    compared to its contrastive counterparts
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5d0a26983d7c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Paper Walkthroughs by Sascha Kirch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2?source=post_page-----5d0a26983d7c--------------------------------)7
    stories![‚ÄúDDPM‚Ää‚Äî‚ÄäDenoising Diffusion Probabilistic Models ‚Äú paper illustration
    by Sascha Kirch](../Images/6e785c0a911386676abebe0fa646f483.png)![‚ÄúDepth Anything‚Äù
    paper illustration by Sascha Kirch](../Images/bd8cd71a02e42cf64d0afd39f41f48e0.png)![](../Images/8708d91a4a1902cef889ced95d46fc39.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have seen what BYOL claims to solve let‚Äôs try to understand how
    this is achieved. First let‚Äôs observe the architecture presented in Fig.1
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/153409ea2efbcbf75ad2378f1063bab6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 1: Framework architecture. [Image Source](https://arxiv.org/abs/2006.07733)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch).'
  prefs: []
  type: TYPE_NORMAL
- en: 'BYOL consists of two networks: the online network and the target network. The
    online network consists of three submodules, namely the encoder, projector and
    predictor. The target network consists of two submodules, namely the encoder and
    projector. The encoder and predictor of both networks share the exact same architecture,
    they only differ in their model weights. While the online network is optimized
    during training, the target network updates its weights by an exponentially moving
    average of itself and the online network.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Encoder** ‚Äî The encoder consists of a ResNet convolutional neuronal network.
    It translates the input image into a latent representation.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Projector** ‚Äî Projects the latent space from a 4096-dimensional space into
    a 256-dimensional space via a multi-layer perceptron network (MLP). I guess the
    projector is not critical for the framework to work, but 256 is simply a convenient
    output dimension often used in the field of representation learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Predictor** ‚Äî Aims to predict the projected latent space of the target network
    from the projected latent space of the online network. Crucial to avoid representation
    collapse.'
  prefs: []
  type: TYPE_NORMAL
- en: 'During training, two different and randomly selected augmentations are applied
    on an input image to construct two different views of that image. One view is
    fed into the online model and another view is fed into the target model. These
    augmentations include among others: resizing, flipping, cropping, color distortion,
    grayscale conversion, Gaussian blur and saturation. The training objective is
    to minimize the squared L2-distance between both networks output. After training,
    **only the encoder of the online network is kept as the final model!**'
  prefs: []
  type: TYPE_NORMAL
- en: 'That‚Äôs all. Easy, right? üòú Well, after reading the paper my face was more like
    this: üòµ While it is relatively straight forward to understand the processing of
    the framework if you break it down to its key components, gaining a intuition
    did cost me quite some time.'
  prefs: []
  type: TYPE_NORMAL
- en: Before we try to gain some intuition of why BYOL actually works, let‚Äôs first
    strip down the presented equations and demystify them.
  prefs: []
  type: TYPE_NORMAL
- en: Math Demystification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having a rough overview of BYOL‚Äôs architecture and how it is trained, let‚Äôs
    have a closer look at the equations. I have to say, the math part presented in
    the paper is way more complicated than it needs to be. While in some cases it
    is presented way to complex, in other cases it lags on clarity and leaves room
    for interpretation causing confusion.
  prefs: []
  type: TYPE_NORMAL
- en: I‚Äôll focus on those equations from which I think are important to understand
    what is happening. Let‚Äôs start by analyzing them in the exact reversed order,
    because why not? üòú
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let‚Äôs talk about the update of the models‚Äô parameters during training.
    Recall that we have two models: the online model and the target model. The online
    model is updated by optimizing a loss function using a LARS optimizer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3fdffacdfd667decdc3fe64eaba9260.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation 1: Weight update of online network. [Source](https://arxiv.org/abs/2006.07733)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation above simply says: ‚Äúupdate the model‚Äôs parameters theta by calling
    an optimizer function upon the current parameters, the gradients of these parameters
    with respect to a loss function and a learning rate eta‚Äù.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The target model on the other hand is not updated via optimization but by copying
    the weights from the online model and applying an exponential moving average on
    the copied updated weights and the current weights of the target network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9325b82871ba916524d9e0601e02b0ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation 2: Weight update of target network. [Source](https://arxiv.org/abs/2006.07733)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation above simply says: ‚Äúupdate the model‚Äôs parameter xi by calculating
    an exponential moving average with the decay rate tau of the current weights xi
    and the updated weights of the online model‚Äù. Tau follows a cosine schedule to
    decrease the contribution of the online model throughout the training.'
  prefs: []
  type: TYPE_NORMAL
- en: Now let‚Äôs have a look on the loss function used to update the online model.
    It is defined as the sum of two other loss functions. These losses share the same
    equation as we will see later but are calculated on two different inputs of the
    network. Recall from Fig. 1\. that two different views (i.e. v and v‚Äô) are generated
    from an image x by applying different augmentations. One view is input into the
    online model and the other one into the target model. During training, two forward
    passes are performed before calculating the loss, where the input for the networks
    are swapped. The image input to the online model is input into the target model
    and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/045f6f999186a4c829428be4c00eca0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation 3: BYOL‚Äôs loss function. [Source](https://arxiv.org/abs/2006.07733)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss for the individual forward passes is a squared L2 distance of the
    L2-normalized outputs of the online model and the target model. Let‚Äôs break down
    the corresponding equation from the paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e456b27a6689941529b2fb798f4c768.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation 4: Individual loss function. [Source](https://arxiv.org/abs/2006.07733)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: The paper says that this is a mean squared error, which is actually not
    correct. The L2-distance does not divide by its number of elements. I guess they
    confused it with calculating the mean over all batches.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Intuition of BYOL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now as we are equipped with an understanding of the framework and the key message
    of the equations, let us try to gain some intuition. I‚Äôll present you what the
    authors think and then I‚Äôll try to add some intuition of my own, well knowing
    it might not be accurate ü§°.
  prefs: []
  type: TYPE_NORMAL
- en: '***How does BYOL learn its representations?*** ‚Äî The model is encouraged to
    generate the same latent representation of its two inputs, which represent two
    different views of the same object/scene. A cat is still a cat regardless of the
    image being blurred, in grayscale or flipped. In fact, I think the heavy augmentations
    are crucial here. It basically tells the model ‚ÄúLook, these are different variations
    of the same thing, so ignore these variations and consider them equal when extracting
    representations of the object/scene!‚Äù.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Why are the representations not collapsing?*** ‚Äî Recall that earlier we
    said, BYOL falls into the category of similarity learning. Wouldn‚Äôt it be the
    easiest way for the network, to just map everything into the same point in the
    latent space to achieve the highest similarity? In fact, this is one of the mayor
    difficulties in similarity learning and is called ‚Äúcollapsing solutions‚Äù. Contrastive
    learning approaches solve this issue by providing many negative samples for a
    given match to map similar features close to each other in the latent space while
    mapping dissimilar features farther apart. BYOL solves this issue by introducing
    an asymmetry between the online and the target network with their predictor submodule
    and by employing an update rule for the target network parameters based on the
    exponentially moving average to ensure near optimality of the predictor throughout
    training.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----5d0a26983d7c--------------------------------)
    [## Get an email whenever Sascha Kirch publishes üöÄ'
  prefs: []
  type: TYPE_NORMAL
- en: Get an email whenever Sascha Kirch publishes üöÄ Looking to learn more about deep
    learning or simply stay up to date‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----5d0a26983d7c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The authors of BYOL presented experiments and ablations to demonstrate the effectiveness
    of their method.
  prefs: []
  type: TYPE_NORMAL
- en: '*Ablation on Batch Size*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From contrastive representation learning methods (e.g. [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    and [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05))
    we know that there is a large dependence on the batch size during training. [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    for example was trained on a batch size of 32,768, which is crazy considering
    it is a multi-modal language-image model.
  prefs: []
  type: TYPE_NORMAL
- en: The authors claim, since BYOL does not require negative samples, it is not as
    sensitive to lower batch sizes which they backup with the following experiment
    shown in Fig.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4bc8059a6403de2b0bca72cd8eaf9d4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 2: Impact of batch size. [Image Source](https://arxiv.org/abs/2006.07733)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch).'
  prefs: []
  type: TYPE_NORMAL
- en: Sadly, this might still be too large for my private laptop üòÖ
  prefs: []
  type: TYPE_NORMAL
- en: Ablation on Robustness of Image Augmentations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [SimCLR](https://arxiv.org/abs/2002.05709) paper has shown that contrastive
    vision methods are sensitive to their choice on image augmentations, especially
    those affecting the color histogram. While crops of the same image share a similar
    color histogram, crops of negative pairs don‚Äôt. The model can take a shortcut
    during training and focus on differences in color histograms rather than the semantic
    features.
  prefs: []
  type: TYPE_NORMAL
- en: The authors claim that BYOL is more robust towards their choice of image augmentations,
    because of the way the online and target networks are updated. While this hypothesis
    is backed up by an experiment, there is still a strong dependency and hence a
    drop in performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29ea63b28df3e850c03a04e04176a409.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 3: Robustness towards image augmentations. [Image Source](https://arxiv.org/abs/2006.07733)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch).'
  prefs: []
  type: TYPE_NORMAL
- en: Linear Evaluation on ImageNet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the field of representation learning, an important characteristic is the
    model‚Äôs ability project semantically rich features into a latent space, to cluster
    similar features and to separate dissimilar features. A common test is to freeze
    the model (in case of BYOL only the encoder of the online model) and to train
    a linear classifier on top of the representations.
  prefs: []
  type: TYPE_NORMAL
- en: Linear evaluation of BYOL has been performed on ImageNet and has been compared
    to many other models and outperforms the previous state-of-the-art of that time.
  prefs: []
  type: TYPE_NORMAL
- en: You‚Äôll find in many papers the differentiation between ResNet-50 encoder and
    other variations of ResNet. It‚Äôs just that the ResNet-50 has been emerged to be
    the standard network to evaluate performance on.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16b12891474954baee3a1cb16206edaf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 1: Linear evaluation on ImageNet. [Source](https://arxiv.org/abs/2006.07733)'
  prefs: []
  type: TYPE_NORMAL
- en: Semi-Supervised Fine-Tuning for classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another very typical experiment setup in representation learning is the model‚Äôs
    performance when fine-tuned to a specific downstream task and dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2 depicts the metrics when finetuning BYOL on a classification task using
    either 1% or 10% of the entire ImageNet training set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/770661dfb170a1d00aa5b21ca752decc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 2: Semi-supervised training on ImageNet. [Source](https://arxiv.org/abs/2006.07733)'
  prefs: []
  type: TYPE_NORMAL
- en: Transfer to Other Vision Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors also present experiments where they transfer-learn BYOL on a semantic
    segmentation task and a monocular depth estimation task, two other important fields
    of computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: The differences to previous approaches are marginal, but I guess the key message
    here is, ‚ÄúWe have a different approach that works just as good‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a569a55cb7fd46bed3e371110e8cf386.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3: Transfer to other vision tasks. [Source](https://arxiv.org/abs/2006.07733)'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BYOL presented an alternative approach for self-supervised representation learning.
    By implementing two networks that perform similarity learning, BYOL can be trained
    without the need of negative training samples like those needed for contrastive
    learning approaches. To avoid collapsing solutions the target network is updated
    via EMA from the online network and an extra prediction sub-module is built on
    top of the online network.
  prefs: []
  type: TYPE_NORMAL
- en: Further Readings & Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you have made it so far: congratulationsüéâ and thank youüòâ! Since it seems
    that you are quite interested in the topic, here are some further resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following a list of papers that built upon BYOL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[DINO: Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here are two of my articles about the contrastive learning methods [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    and [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)
    for self-supervised representation learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/the-clip-foundation-model-7770858b487d?source=post_page-----5d0a26983d7c--------------------------------)
    [## The CLIP Foundation Model'
  prefs: []
  type: TYPE_NORMAL
- en: Paper Summary‚Äî Learning Transferable Visual Models From Natural Language Supervision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/the-clip-foundation-model-7770858b487d?source=post_page-----5d0a26983d7c--------------------------------)
    [](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----5d0a26983d7c--------------------------------)
    [## GLIP: Introducing Language-Image Pre-Training to Object Detection'
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper Summary: Grounded Language-Image Pre-training'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----5d0a26983d7c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
