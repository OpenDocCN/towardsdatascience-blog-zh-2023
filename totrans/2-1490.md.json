["```py\n#Random Guessing of Pi\n\n# Before running this code, make sure you have the necessary packages installed.\n# You can install them using \"pip install\" on your terminal or \"conda install\" if using a conda environment\n\n# Import necessary libraries\nimport random\nimport plotly.graph_objects as go\nimport numpy as np\n\n# Number of guesses to make. Adjust this for more guesses and subsequent plots\nnum_guesses = 6\n\n# Generate the coordinates for the unit circle\n# We use np.linspace to generate evenly spaced numbers over the range from 0 to 2*pi.\n# These represent the angles in the unit circle.\ntheta = np.linspace(0, 2*np.pi, 100)\n\n# The x and y coordinates of the unit circle are then the cosine and sine of these angles, respectively.\nunit_circle_x = np.cos(theta)\nunit_circle_y = np.sin(theta)\n\n# We'll make a number of guesses for the value of pi\nfor i in range(num_guesses):\n    # Make a random guess for pi between 2 and 4\n    pi_guess = random.uniform(2, 4)\n\n    # Generate the coordinates for the circle based on the guess\n    # The radius of the circle is the guessed value of pi divided by 4.\n    radius = pi_guess / 4\n\n    # The x and y coordinates of the circle are then the radius times the cosine and sine of the angles, respectively.\n    circle_x = radius * np.cos(theta)\n    circle_y = radius * np.sin(theta)\n\n    # Create a scatter plot of the circle\n    fig = go.Figure()\n\n    # Add the circle to the plot\n    # We use a Scatter trace with mode 'lines' to draw the circle.\n    fig.add_trace(go.Scatter(\n        x = circle_x,\n        y = circle_y,\n        mode='lines',\n        line=dict(\n            color='blue',\n            width=3\n        ),\n        name='Estimated Circle'\n    ))\n\n    # Add the unit circle to the plot\n    fig.add_trace(go.Scatter(\n        x = unit_circle_x,\n        y = unit_circle_y,\n        mode='lines',\n        line=dict(\n            color='green',\n            width=3\n        ),\n        name='Unit Circle'\n    ))\n\n    # Update the layout of the plot\n    # We set the title to include the guessed value of pi, and adjust the size and axis ranges to properly display the circles.\n    fig.update_layout(\n        title=f\"Fig1{chr(97 + i)}: Randomly Guessing Pi: {pi_guess}\",\n        width=600,\n        height=600,\n        xaxis=dict(\n            constrain=\"domain\",\n            range=[-1, 1]\n        ),\n        yaxis=dict(\n            scaleanchor=\"x\",\n            scaleratio=1,\n            range=[-1, 1]\n        )\n    )\n\n    # Display the plots\n    fig.show()\n```", "```py\n#Monte Carlo Estimation of Pi\n\n# Import our required libraries\nimport random\nimport math\nimport plotly.graph_objects as go\nimport plotly.io as pio\nimport numpy as np\n\n# We'll simulate throwing darts at a dartboard to estimate pi. Let's throw 10,000 darts.\nnum_darts = 10000\n\n# To keep track of darts that land in the circle.\ndarts_in_circle = 0\n\n# We'll store the coordinates of darts that fall inside and outside the circle.\nx_coords_in, y_coords_in, x_coords_out, y_coords_out = [], [], [], []\n\n# Let's generate 6 figures throughout the simulation. Therefore, we will create a new figure every 1,666 darts (10,000 divided by 6).\nnum_figures = 6\ndarts_per_figure = num_darts // num_figures\n\n# Create a unit circle to compare our estimates against. Here, we use polar coordinates and convert to Cartesian.\ntheta = np.linspace(0, 2*np.pi, 100)\nunit_circle_x = np.cos(theta)\nunit_circle_y = np.sin(theta)\n\n# We start throwing the darts (simulating random points inside a 1x1 square and checking if they fall inside a quarter circle).\nfor i in range(num_darts):\n    # Generate random x, y coordinates between -1 and 1.\n    x, y = random.uniform(-1, 1), random.uniform(-1, 1)\n\n    # If a dart (point) is closer to the origin (0,0) than the distance of 1, it's inside the circle.\n    if math.sqrt(x**2 + y**2) <= 1:\n        darts_in_circle += 1\n        x_coords_in.append(x)\n        y_coords_in.append(y)\n    else:\n        x_coords_out.append(x)\n        y_coords_out.append(y)\n\n    # After every 1,666 darts, let's see how our estimate looks compared to the real unit circle.\n    if (i + 1) % darts_per_figure == 0:\n        # We estimate pi by seeing the proportion of darts that landed in the circle (out of the total number of darts).\n        pi_estimate = 4 * darts_in_circle / (i + 1)\n\n        # Now we create a circle from our estimate to compare visually with the unit circle.\n        estimated_circle_radius = pi_estimate / 4\n        estimated_circle_x = estimated_circle_radius * np.cos(theta)\n        estimated_circle_y = estimated_circle_radius * np.sin(theta)\n\n        # Plot the results using Plotly.\n        fig = go.Figure()\n\n        # Add the darts that landed inside and outside the circle to the plot.\n        fig.add_trace(go.Scattergl(x=x_coords_in, y=y_coords_in, mode='markers', name='Darts Inside Circle', marker=dict(color='green', size=4, opacity=0.8)))\n        fig.add_trace(go.Scattergl(x=x_coords_out, y=y_coords_out, mode='markers', name='Darts Outside Circle', marker=dict(color='red', size=4, opacity=0.8)))\n\n        # Add the real unit circle and our estimated circle to the plot.\n        fig.add_trace(go.Scatter(x=unit_circle_x, y=unit_circle_y, mode='lines', name='Unit Circle', line=dict(color='green', width=3)))\n        fig.add_trace(go.Scatter(x=estimated_circle_x, y=estimated_circle_y, mode='lines', name='Estimated Circle', line=dict(color='blue', width=3)))\n\n        # Customize the plot layout.\n        fig.update_layout(title=f\"Figure {chr(97 + (i + 1) // darts_per_figure - 1)}: Thrown Darts: {(i + 1)}, Estimated Pi: {pi_estimate}\", width=600, height=600, xaxis=dict(constrain=\"domain\", range=[-1, 1]), yaxis=dict(scaleanchor=\"x\", scaleratio=1, range=[-1, 1]), legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01))\n\n        # Display the plot.\n        fig.show()\n\n        # Save the plot as a PNG image file.\n        pio.write_image(fig, f\"fig2{chr(97 + (i + 1) // darts_per_figure - 1)}.png\")\n```", "```py\n# Calculate the differences between the real pi and the estimated pi\ndiff_pi = [abs(estimate - math.pi) for estimate in pi_estimates]\n\n# Create the figure for the number of darts vs difference in pi plot (Figure 2g)\nfig2g = go.Figure(data=go.Scatter(x=num_darts_thrown, y=diff_pi, mode='lines'))\n\n# Add title and labels to the plot\nfig2g.update_layout(\n    title=\"Fig2g: Darts Thrown vs Difference in Estimated Pi\",\n    xaxis_title=\"Number of Darts Thrown\",\n    yaxis_title=\"Difference in Pi\",\n)\n\n# Display the plot\nfig2g.show()\n\n# Save the plot as a png\npio.write_image(fig2g, \"fig2g.png\")\n```", "```py\n#500 Monte Carlo Scenarios; 1,000,000 darts thrown\nimport random\nimport math\nimport plotly.graph_objects as go\nimport numpy as np\n\n# Total number of darts to throw (1M)\nnum_darts = 1000000\ndarts_in_circle = 0\n\n# Number of scenarios to record (500)\nnum_scenarios = 500\ndarts_per_scenario = num_darts // num_scenarios\n\n# Lists to store the data for each scenario\ndarts_thrown_list = []\npi_diff_list = []\n\n# We'll throw a number of darts\nfor i in range(num_darts):\n    # Generate random x, y coordinates between -1 and 1\n    x, y = random.uniform(-1, 1), random.uniform(-1, 1)\n\n    # Check if the dart is inside the circle\n    # A dart is inside the circle if the distance from the origin (0,0) is less than or equal to 1\n    if math.sqrt(x**2 + y**2) <= 1:\n        darts_in_circle += 1\n\n    # If it's time to record a scenario\n    if (i + 1) % darts_per_scenario == 0:\n        # Estimate pi with Monte Carlo method\n        # The estimate is 4 times the number of darts in the circle divided by the total number of darts\n        pi_estimate = 4 * darts_in_circle / (i + 1)\n\n        # Record the number of darts thrown and the difference between the estimated and actual values of pi\n        darts_thrown_list.append((i + 1) / 1000)  # Dividing by 1000 to display in thousands\n        pi_diff_list.append(abs(pi_estimate - math.pi))\n\n# Create a scatter plot of the data\nfig = go.Figure(data=go.Scattergl(x=darts_thrown_list, y=pi_diff_list, mode='markers'))\n\n# Update the layout of the plot\nfig.update_layout(\n    title=\"Fig2h: Difference between Estimated and Actual Pi vs. Number of Darts Thrown (in thousands)\",\n    xaxis_title=\"Number of Darts Thrown (in thousands)\",\n    yaxis_title=\"Difference between Estimated and Actual Pi\",\n)\n\n# Display the plot\nfig.show()\n# Save the plot as a png\npio.write_image(fig2h, \"fig2h.png\")\n```", "```py\n #Load and view first few rows of dataset\n\n# Import necessary libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\nimport plotly.graph_objects as go\n\n# Load the dataset\n# The dataset is available at the UCI Machine Learning Repository\n# It's a dataset about heart disease and includes various patient measurements\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n\n# Define the column names for the dataframe\ncolumn_names = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"]\n\n# Load the dataset into a pandas dataframe\n# We specify the column names and also tell pandas to treat '?' as NaN\ndf = pd.read_csv(url, names=column_names, na_values=\"?\")\n\n# Print the first few rows of the dataframe\n# This gives us a quick overview of the data\nprint(df.head())\n```", "```py\n# Preprocess\n\n# Import necessary libraries\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Identify missing values in the dataset\n# This will print the number of missing values in each column\nprint(df.isnull().sum())\n\n# Fill missing values with the median of the column\n# The SimpleImputer class from sklearn provides basic strategies for imputing missing values\n# We're using the 'median' strategy, which replaces missing values with the median of each column\nimputer = SimpleImputer(strategy='median')\n\n# Apply the imputer to the dataframe\n# The result is a new dataframe where missing values have been filled in\ndf_filled = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n\n# Print the first few rows of the filled dataframe\n# This gives us a quick check to make sure the imputation worked correctly\nprint(df_filled.head())\n\n# Identify categorical variables in the dataset\n# These are variables that contain non-numerical data\ncategorical_vars = df_filled.select_dtypes(include='object').columns\n\n# Encode categorical variables\n# The LabelEncoder class from sklearn converts each unique string into a unique integer\nencoder = LabelEncoder()\nfor var in categorical_vars:\n    df_filled[var] = encoder.fit_transform(df_filled[var])\n\n# Normalize numerical features\n# The StandardScaler class from sklearn standardizes features by removing the mean and scaling to unit variance\nscaler = StandardScaler()\n\n# Apply the scaler to the dataframe\n# The result is a new dataframe where numerical features have been normalized\ndf_normalized = pd.DataFrame(scaler.fit_transform(df_filled), columns=df_filled.columns)\n\n# Print the first few rows of the normalized dataframe\n# This gives us a quick check to make sure the normalization worked correctly\nprint(df_normalized.head())\n```", "```py\n# Logistic Regression Model - Baseline\n\n# Import necessary libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n\n# Replace the 'target' column in the normalized DataFrame with the original 'target' column\n# This is done because the 'target' column was also normalized, which is not what we want\ndf_normalized['target'] = df['target']\n\n# Binarize the 'target' column\n# This is done because the original 'target' column contains values from 0 to 4\n# We want to simplify the problem to a binary classification problem: heart disease or no heart disease\ndf_normalized['target'] = df_normalized['target'].apply(lambda x: 1 if x > 0 else 0)\n\n# Split the data into training and test sets\n# The 'target' column is our label, so we drop it from our features (X)\n# We use a test size of 20%, meaning 80% of the data will be used for training and 20% for testing\nX = df_normalized.drop('target', axis=1)\ny = df_normalized['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Implement a basic logistic regression model\n# Logistic Regression is a simple yet powerful linear model for binary classification problems\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\n# The model has been trained, so we can now use it to make predictions on unseen data\ny_pred = model.predict(X_test)\n\n# Evaluate the model\n# We use accuracy (the proportion of correct predictions) and ROC-AUC (a measure of how well the model distinguishes between classes) as our metrics\naccuracy = accuracy_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_pred)\n\n# Print the performance metrics\n# These give us an indication of how well our model is performing\nprint(\"Baseline Model \" + f'Accuracy: {accuracy}')\nprint(\"Baseline Model \" + f'ROC-AUC: {roc_auc}')\n```", "```py\n# Grid Search\n\n# Import necessary libraries\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the hyperparameters and their values\n# 'C' is the inverse of regularization strength (smaller values specify stronger regularization)\n# 'penalty' specifies the norm used in the penalization (l1 or l2)\nhyperparameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], \n                   'penalty': ['l1', 'l2']}\n\n# Implement grid search\n# GridSearchCV is a method used to tune our model's hyperparameters\n# We pass our model, the hyperparameters to tune, and the number of folds for cross-validation\n# We're using ROC-AUC as our scoring metric\ngrid_search = GridSearchCV(LogisticRegression(), hyperparameters, cv=5, scoring='roc_auc')\ngrid_search.fit(X_train, y_train)\n\n# Get the best hyperparameters\n# GridSearchCV has found the best hyperparameters for our model, so we print them out\nbest_params = grid_search.best_params_\nprint(f'Best hyperparameters: {best_params}')\n\n# Evaluate the best model\n# GridSearchCV also gives us the best model, so we can use it to make predictions and evaluate its performance\nbest_model = grid_search.best_estimator_\ny_pred_best = best_model.predict(X_test)\naccuracy_best = accuracy_score(y_test, y_pred_best)\nroc_auc_best = roc_auc_score(y_test, y_pred_best)\n\n# Print the performance metrics of the best model\n# These give us an indication of how well our model is performing after hyperparameter tuning\nprint(\"Grid Search Method \" + f'Accuracy of the best model: {accuracy_best}')\nprint(\"Grid Search Method \" + f'ROC-AUC of the best model: {roc_auc_best}')\n```", "```py\n#Monte Carlo\n\n# Import necessary libraries\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the range of hyperparameters\n# 'C' is the inverse of regularization strength (smaller values specify stronger regularization)\n# 'penalty' specifies the norm used in the penalization (l1 or l2)\nC_range = np.logspace(-3, 3, 7)\npenalty_options = ['l1', 'l2']\n\n# Initialize variables to store the best score and hyperparameters\nbest_score = 0\nbest_hyperparams = None\n\n# Perform the Monte Carlo simulation\n# We're going to perform 1000 iterations. You can play with this number to see how the performance changes.\n# Remember the Law of Large Numbers!\nfor _ in range(1000):  \n\n    # Randomly select hyperparameters from the defined range\n    C = np.random.choice(C_range)\n    penalty = np.random.choice(penalty_options)\n\n    # Create and evaluate the model with these hyperparameters\n    # We're using 'liblinear' solver as it supports both L1 and L2 regularization\n    model = LogisticRegression(C=C, penalty=penalty, solver='liblinear')\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    # Calculate the accuracy and ROC-AUC\n    accuracy = accuracy_score(y_test, y_pred)\n    roc_auc = roc_auc_score(y_test, y_pred)\n\n    # If this model's ROC-AUC is the best so far, store its score and hyperparameters\n    if roc_auc > best_score:\n        best_score = roc_auc\n        best_hyperparams = {'C': C, 'penalty': penalty}\n\n# Print the best score and hyperparameters\nprint(\"Monte Carlo Method \" + f'Best ROC-AUC: {best_score}')\nprint(\"Monte Carlo Method \" + f'Best hyperparameters: {best_hyperparams}')\n\n# Train the model with the best hyperparameters\nbest_model = LogisticRegression(**best_hyperparams, solver='liblinear')\nbest_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = best_model.predict(X_test)\n\n# Calculate and print the accuracy of the best model\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Monte Carlo Method \" + f'Accuracy of the best model: {accuracy}')\n```"]