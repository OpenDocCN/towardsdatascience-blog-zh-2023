- en: 'Hands-on Generative AI with GANs using Python: Image Generation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/hands-on-generative-ai-with-gans-using-python-image-generation-9a62e591c7c6](https://towardsdatascience.com/hands-on-generative-ai-with-gans-using-python-image-generation-9a62e591c7c6)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/8bacad55716d720d37ccf3e4e734e940.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to implement GANs with PyTorch to generate synthetic images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----9a62e591c7c6--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----9a62e591c7c6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9a62e591c7c6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9a62e591c7c6--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----9a62e591c7c6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9a62e591c7c6--------------------------------)
    ·7 min read·Mar 27, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In my [previous article](https://medium.com/towards-data-science/hands-on-generative-ai-with-gans-using-python-autoencoders-c77232b402fc),
    we learned about Autoencoders, now let’s continue to talk about Generative AI.
    By now everyone is talking about it and everyone is excited about the practical
    applications that have been developed. But we continue to see the foundations
    of these AIs step by step.
  prefs: []
  type: TYPE_NORMAL
- en: There are several Machine Learning models that allow us to build generative
    AI, to name a few we have Variational Autoencoders (VAE), autoregressive models
    and even normalizing flow models. In this article, however, we will focus on GANs.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders and GANs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous article, we dealt with autoencoders and saw their architecture,
    their use and implementation in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: In short, Autoencoders receive an input x, compress it into a vector of smaller
    size z, called the latent vector, and finally from z reconstruct x in a more or
    less approximate way.
  prefs: []
  type: TYPE_NORMAL
- en: In Autoencoder we have no data generation, but simply an approximate reconstruction
    of the input. Now imagine that we break the Autoencoder in two and consider only
    the second part, the part where from the latent vector z the image is reconstructed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/674c788436be31599035404c61b840cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Output Generation (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we can say that the architecture is generative. In fact, given
    a vector of numbers as input this creates an image! Essentially this is what a
    generative AI does. The main difference though with respect to autoencoders is
    that we know well the probability distribution from which we take the latent vector
    z. For example, a *Gaussian(0,1)*.
  prefs: []
  type: TYPE_NORMAL
- en: So we thus have a way to generate images from random numbers taken from a Gaussian
    distribution, changing these random numbers will change the images we have in
    the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01c55550204051a7c6d10b3d6487f4a3.png)'
  prefs: []
  type: TYPE_IMG
- en: Generative Model (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: GANs Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The orange network shown in the previous image can be defined as a G function
    that given the input z generates the synthetic output *x_cap*, so *x_cap = G(z)*.
  prefs: []
  type: TYPE_NORMAL
- en: The network will be initialized with random weights, so it will not initially
    be able to generate output that looks real, but only images that will contain
    noise. So we need to do some training to improve the performance of our network.
  prefs: []
  type: TYPE_NORMAL
- en: So let’s imagine that we have a human annotator telling us each time whether
    the output is good or not, whether it looks real or not.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0386f4e179933422ffda2952ed945011.png)'
  prefs: []
  type: TYPE_IMG
- en: Towards GANs (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, we cannot do network training expecting a person to make continuous
    judgments about the output. But then what can we do?
  prefs: []
  type: TYPE_NORMAL
- en: If you think about it what the annotator does, in this case, is binary classification!
    And we in Machine Learning are great at developing classifiers. So we can simply
    train a classifier that we’ll call Discriminator, and we’ll denote with the function
    D(), which has to be trained to recognize synthetic (fake) images versus real
    images. So we will feed it both fake images and real images.
  prefs: []
  type: TYPE_NORMAL
- en: So this is how our architecture changes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8bacad55716d720d37ccf3e4e734e940.png)'
  prefs: []
  type: TYPE_IMG
- en: GANs Architecture (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: In short, the architecture is not too complex. The difficulty comes at the time
    of having to train these two networks G and D.
  prefs: []
  type: TYPE_NORMAL
- en: It is clear that if in training, the two networks have to improve together,
    they need to find some kind of balance. Because if, for example, D gets too good
    at distinguishing fake images from real ones before G gets good at generating
    them, it is quite natural that G will never get better and we will never have
    our generator ready to be used.
  prefs: []
  type: TYPE_NORMAL
- en: So the two networks are said to play an adversarial game in which G must fool
    D, and D must not be fooled by G.
  prefs: []
  type: TYPE_NORMAL
- en: GANs Objective Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we want to be a bit more precise, we can say that D and G have two complementary
    goals. Let’s suppose we want to generate images.
  prefs: []
  type: TYPE_NORMAL
- en: We define by D(x) the probability that x is a real image. Obviously, the discriminator
    wants to maximize its probability of recognizing real inputs from fake inputs.
    So we want to maximize D(x) when x is drawn from our distribution of real images.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the purpose of the generator G is to fool the discriminator. So
    if *G(z)* is the fake image generated by G, *D(G(z))* is the probability that
    D will recognize a fake image as real. Then *1-D(G(z))* is the probability that
    D correctly recognizes a fake image as fake. So G’s goal is to minimize *1-D(G(z))*,
    since he does want to fool D.
  prefs: []
  type: TYPE_NORMAL
- en: 'So in the end we can sum up this game of maximization and minimization in the
    formula we find in the original paper (the formula looks a bit more concept but
    we have seen the concept):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a8adc55a900fd1a625d8c924e8c3a0e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Objective Function (src: [https://arxiv.org/pdf/1406.2661.pdf](https://arxiv.org/pdf/1406.2661.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: GANs Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now implement a GAN capable of generating MNIST images automatically.
  prefs: []
  type: TYPE_NORMAL
- en: As usual, I will run my code a cloud-based environment Deepnote but you can
    use Google Colab as well, so even those who don’t have a GPU on their laptop can
    run this code.
  prefs: []
  type: TYPE_NORMAL
- en: We start by going to check whether indeed our hardware has a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Now if you’re using Colab you can connect to Google Drive.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let’s import the needed libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Now we need to create the functions that will define our networks, generator
    and discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST images have 784 pixels (since the images are 28x28). So the generator
    given as input a random z vector of length 20 will have to output a vector of
    784 which will be our fake image.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, the discriminator will receive as input a 28x28 = 784-pixel image,
    it will have a single neuron in output that will classify the image as true or
    fake.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f42e0cc6024372aafabd130d24076065.png)'
  prefs: []
  type: TYPE_IMG
- en: Generator (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: This function is used to instantiate the generator. Each layer will use a LeakyReLU
    (a variation of the ReLU, that works best in GANs) as its activation function,
    except the output is followed by a Hyperbolic Tangent (Tanh) function that results
    in output a number in the range [-1,1].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6928d3027580731763156ede1b4577f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Discriminator (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: Instead, this function defines the discriminator network, which has the special
    feature of using dropout after hidden layers (in the base case only one hidden
    layer). The output goes through a sigmoid function since it must give us the probability
    of being a real or fake image.
  prefs: []
  type: TYPE_NORMAL
- en: Now we also download the MNIST dataset that we are going to use. The MNIST dataset
    is in a range [0,255], but we want it in the range [-1,1] so that it will be similar
    to the data generated by the Generator network. So we also apply some preprocessing
    to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Now we come to the most important part. We need to create the functions that
    define the training of our network. We have already said that we should pull the
    discriminator separately from the generator, so we will have 2 functions.
  prefs: []
  type: TYPE_NORMAL
- en: The discriminator will be trained both on the fake data and on real data. When
    we train it on real data the labels will always be *“real” = 1*. So we create
    a vector of 1 with *d_labels_real = torch.ones(batch_size, 1, device = device)*.
    Then we feed the input x to the model and calculate the loss using *Binary Cross
    Entropy*.
  prefs: []
  type: TYPE_NORMAL
- en: We do the same thing by feeding fake data. Here the labels will all be zero,
    *d_labels_fake = torch.zeros(batch_size, 1, device = device)*. The input instead
    will be the fake data, that is, the output of the generator *g_output = gen_model(input_z)*.
    And we calculate the loss in the same way.
  prefs: []
  type: TYPE_NORMAL
- en: The final loss will be the sum of the two losses.
  prefs: []
  type: TYPE_NORMAL
- en: As for the generator train function, the implementation is slightly different.
    The generator takes as input the output of the discriminator since it has to see
    if D has figured out whether it is a fake or real image. And based on that it
    calculates its loss.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can initialize our two networks.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s define a function to create network-generated samples, so as we go along
    we can see how the fake images improve as the training epochs increase.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can finally train the net! We save the losses each time in a list so
    we can plot them later.
  prefs: []
  type: TYPE_NORMAL
- en: The training should take about an hour, depending on the hardware cha you used
    certainly. But in the end, you can print out your fake data and have something
    like this.
  prefs: []
  type: TYPE_NORMAL
- en: In my case, I trained for a few epochs so the results are not great, but you
    are beginning to get a glimpse that the network was learning to generate MNIST-like
    images.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8edea14b537654ada61c0f347ca2fe77.png)'
  prefs: []
  type: TYPE_IMG
- en: Fake Data (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we looked at how the architecture of GANs in more detail. We
    studied their objective function and were able to implement a network capable
    of generating images from the MNIST dataset! The operation of these networks is
    not too complicated but their training certainly is. Since we need to find that
    balance that allows both networks to learn. If you enjoyed this article follow
    me to read the next one on DCGANs.[😉](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
  prefs: []
  type: TYPE_NORMAL
- en: The End
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Marcello Politi*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Linkedin](https://www.linkedin.com/in/marcello-politi/), [Twitter](https://twitter.com/_March08_),
    [Website](https://marcello-politi.super.site/)'
  prefs: []
  type: TYPE_NORMAL
