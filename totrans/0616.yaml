- en: Cultural Competencies for Machine Learning Risk Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/cultural-competencies-for-machine-learning-risk-management-c38616c2ccdf](https://towardsdatascience.com/cultural-competencies-for-machine-learning-risk-management-c38616c2ccdf)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*An organization''s culture is an essential aspect of responsible AI.*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://pandeyparul.medium.com/?source=post_page-----c38616c2ccdf--------------------------------)[![Parul
    Pandey](../Images/760b72a4feacfad6fc4224835c2e1f19.png)](https://pandeyparul.medium.com/?source=post_page-----c38616c2ccdf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c38616c2ccdf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c38616c2ccdf--------------------------------)
    [Parul Pandey](https://pandeyparul.medium.com/?source=post_page-----c38616c2ccdf--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c38616c2ccdf--------------------------------)
    ·8 min read·Sep 15, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d42f7a8d2151e8b4f44c97de37c88a03.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Photo by Google DeepMind](https://www.pexels.com/photo/an-artist-s-illustration-of-artificial-intelligence-ai-this-image-depicts-ai-safety-research-to-prevent-the-misuse-and-encourage-beneficial-uses-it-was-created-by-artist-khyati-trehan-17485632/)
    available for free on Pexels'
  prefs: []
  type: TYPE_NORMAL
- en: In the race for progress, we must tread carefully, for haste in engineering
    and data science can shatter more than just code.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Imagine a world where Artificial Intelligence (AI) powered systems could do
    no wrong, where they flawlessly executed their tasks without a glitch. Sounds
    like a sci-fi dream, doesn't it? Welcome to the real world of AI, where things
    don't always go as planned. An integral part of responsible AI practice involves
    preventing and addressing what we term '***AI incidents***.' This article discusses
    cultural competencies that can prevent and mitigate AI incidents, focusing on
    the concept of promoting responsible AI practices. Subsequently, we will explore
    related business processes in future articles to provide a comprehensive perspective
    on this crucial topic.
  prefs: []
  type: TYPE_NORMAL
- en: A Note on the Series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we embark on this series, it’s important to provide context. I am one of
    the co-authors of ‘[**Machine Learning for High-Risk Applications**,](https://www.amazon.in/Machine-Learning-High-Risk-Applications-Responsible/dp/1098102436)’
    along with [Patrick Hall](https://medium.com/@jphall_22520) and [James Curtis](https://james-curtis.medium.com/).
    This series is designed to offer a concise, reader-friendly companion to the book’s
    extensive content. In each article, we aim to distill the critical insights, concepts,
    and practical strategies presented in the book into easily digestible portions,
    making this knowledge accessible to a broader audience.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Defining AI incidents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Addressing AI incidents is crucial before delving into ML safety because we
    can't effectively mitigate what we don't comprehend. AI incidents encompass any
    outcomes stemming from AI systems that could potentially cause harm. The severity
    of these incidents naturally varies depending on the extent of damage they result
    in. These incidents could range from relatively minor inconveniences, such as
    [mall security robots tumbling downstairs](https://www.youtube.com/watch?v=4Pwx3U4vJKw),
    to more catastrophic events, like [self-driving cars causing pedestrian fatalities](https://www.nytimes.com/2018/03/19/technology/uber-driverless-fatality.html)
    and the [large-scale diversion of healthcare resources away from those in dire
    need](https://www.nature.com/articles/d41586-019-03228-6).
  prefs: []
  type: TYPE_NORMAL
- en: AI incidents encompass any outcomes stemming from AI systems that could potentially
    cause harm.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We can categorize the AI incidents into three major groups :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb20270c0495f980498a271043ded17f.png)'
  prefs: []
  type: TYPE_IMG
- en: '*A taxonomy of AI incidents (adapted by Author from "*[*What to Do When AI
    Fails*](https://www.oreilly.com/radar/what-to-do-when-ai-fails/)*" with permission)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Attacks —** Many parts of machine learning systems, like software and prediction
    tools, are vulnerable to cyber and insider attacks. Once an attack happens, we
    lose control, and the attackers may have their own goals related to accuracy,
    bias, privacy, reliability, and more. Researchers have extensively documented
    such attack categories, namely confidentiality, integrity, and availability attacks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/fa3ec6c2697ebda0cf5f3bff363d76cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Types of Machine Learning Attacks | Image by the Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Failures —** Failures refer to issues within AI systems, which often include
    problems like algorithmic bias, lapses in safety and performance, breaches of
    data privacy, a lack of transparency, or shortcomings in third-party system components.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Abuses —** AI tools can be abused by people with malicious intent. Hackers
    often use AI to enhance their attacks, such as in autonomous drone strikes. Additionally,
    some governments employ AI for purposes like ethnic profiling, highlighting the
    widespread misuse of AI technology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cataloging AI Incidents: The AI Incident Database'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI incidents can act as catalysts for promoting responsible technological progress
    within companies. When developing machine learning systems, it's crucial to cross-reference
    current plans with past incidents to avert potential future occurrences proactively.
    This aligns with the primary objective of ongoing endeavors to establish AI incident
    databases and their corresponding publications. An excellent example of such an
    endeavor is the [AI Incident Database](https://incidentdatabase.ai/).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c2e5cd9f99e5ce1fa102102438f3a830.png)'
  prefs: []
  type: TYPE_IMG
- en: '[https://incidentdatabase.ai/](https://incidentdatabase.ai/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'As stated on the AI Incident Database website, their mission is clear:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"The AI Incident Database is dedicated to indexing the collective history
    of harms or near harms realized in the real world by the deployment of artificial
    intelligence systems. Like similar databases in aviation and computer security,
    the AI Incident Database aims to learn from experience so we can prevent or mitigate
    bad outcomes."*'
  prefs: []
  type: TYPE_NORMAL
- en: The underlying idea here is that, like other domains, AI can also greatly benefit
    from learning from past mistakes to avoid their recurrence in the future. To achieve
    this effectively, it is imperative to maintain an accurate record of these failures.
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating ML Risks through Cultural Competencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The culture within an organization plays a pivotal role in ensuring responsible
    AI practices. This article will mainly explore some cultural strategies to achieve
    this goal. In the upcoming articles, we will also cover additional methods for
    mitigating AI-related risks, including business processes and the model risk management
    aspects. Validation, auditing, and incident response teams are crucial alongside
    developers.
  prefs: []
  type: TYPE_NORMAL
- en: '*1\. Organizational Accountability*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0c49324641b259be001b2ddb55cd7c48.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image by vectorjuice on Freepik](https://www.freepik.com/free-vector/project-delivery-abstract-concept-illustration_20770433.htm#query=responsibility&position=4&from_view=search&track=sph)'
  prefs: []
  type: TYPE_NORMAL
- en: Achieving responsible AI practices in organizations hinges on accountability,
    culture, and adherence to standards like [**Model Risk Management (MRM)**](https://www.fdic.gov/news/financial-institution-letters/2017/fil17022a.pdf)(we'll
    cover it in upcoming articles). Without consequences for ML system failures, attacks,
    or misuse, safety, and performance may be overlooked. Key cultural tenets for
    MRM include written policies and procedures, effective challenge from independent
    experts, accountable leadership (e.g., Chief Model Risk Officer), and aligning
    incentives for responsible ML implementation and not just speedy development.
  prefs: []
  type: TYPE_NORMAL
- en: Small organizations can designate individuals or groups for accountability to
    prevent incidents and reward successful systems. Collective accountability can
    lead to no one being responsible for ML risks and incidents.
  prefs: []
  type: TYPE_NORMAL
- en: '*It’s important to have an individual or group held accountable if ML systems
    cause incidents and rewarded if the systems work well. If an organization assumes
    that everyone is accountable for ML risk and AI incidents, the reality is that
    no one is accountable — Machine Learning for High Risk Applications, Chapter 1.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 2\. Culture of Effective Change
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/7b0a7bd6ea21b367a3c7a281d2587b38.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image by jcomp on Freepik](https://www.freepik.com/free-photo/hand-word-chance-business_5598002.htm#query=Culture%20of%20Effective%20Change&position=3&from_view=search&track=ais)'
  prefs: []
  type: TYPE_NORMAL
- en: A strong culture of effective change involves actively questioning and scrutinizing
    the various steps involved in developing ML (Machine Learning) systems. In a broader
    organizational context, promoting a culture of serious inquiry into ML system
    designs is essential. Such a culture increases the likelihood of developing successful
    ML systems and products while also preventing problems from escalating into harmful
    incidents. It's worth noting that effective challenges should always be constructive
    and respectful, applying uniformly to all personnel involved in ML system development.
  prefs: []
  type: TYPE_NORMAL
- en: Effective challenge fuels innovation and safeguards progress in ML development
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To implement effective challenges effectively, it should be structured. This
    could involve regular meetings, perhaps on a weekly basis, where current design
    decisions are critically examined and alternative design choices are thoughtfully
    considered. This structured approach helps ensure that the culture of effective
    challenge becomes integral to the organization's ML development process.
  prefs: []
  type: TYPE_NORMAL
- en: '*3\. Diverse and Experienced Teams*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/a2af77f88d1b2238f3067b45cf01139d.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Alexander Grey](https://unsplash.com/@sharonmccutcheon?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/sbE9zbcuiZs?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Diverse teams are crucial to unlocking fresh perspectives in designing, developing,
    and testing ML systems. Numerous examples illustrate the adverse outcomes of data
    scientists overlooking ML systems' demographic diversity. Increasing demographic
    diversity within ML teams is a potential remedy to address these oversights.
  prefs: []
  type: TYPE_NORMAL
- en: In diversity we find innovation, and in expertise we ensure safety, together
    they are the compass guiding responsible AI advancement.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dismissing domain experts is a dangerous gamble, as they bring invaluable insights
    and act as a safety net, averting potential disasters arising from misinterpretations
    of domain-specific data or results. The same principle applies to social science
    experts. Countless instances demonstrate the perils of sidelining these experts,
    whether by attempting to [automate decisions requiring specialized knowledge](https://www.wired.com/story/tech-needs-to-listen-to-actual-researchers/)
    or [ignoring their collective wisdom](https://www.technologyreview.com/2020/06/23/1004333/ai-science-publishers-perpetuate-racist-face-recognition)
    entirely in AI projects.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. *Drinking Our Own Champagne*
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/03897edd4217c9bf0e6c7ca109a4c250.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image by macrovector on Freepik](https://www.freepik.com/free-vector/qa-engineer-flat-concept-with-software-developer-symbols-flat-vector-illustration_37366036.htm#query=Testing&position=11&from_view=search&track=sph)'
  prefs: []
  type: TYPE_NORMAL
- en: '"**Drinking our own champagne**" is the practice of testing our own software
    or products within our organization, akin to "**eating our own dog food**." It''s
    a pre-alpha or pre-beta testing method that helps uncover deployment complexities
    before they impact customers or the public. This approach is crucial for identifying
    elusive issues like concept drift, algorithmic discrimination, shortcut learning,
    and underspecification that often elude standard ML development processes.'
  prefs: []
  type: TYPE_NORMAL
- en: If it’s not suitable for our organization, it may not be ready for deployment.
    It’s a sip before you serve.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*5\. Moving Fast and Breaking Things*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/1fa967c8ee9ed295ac7a98ebacb96f6d.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image by rawpixel.com on Freepik](https://www.freepik.com/free-photo/caution-alert-critical-error-failure-notice_16482263.htm#query=careful&position=8&from_view=search&track=sph)'
  prefs: []
  type: TYPE_NORMAL
- en: In the field of engineering and data science, the t mantra of "move fast and
    break things" often takes center stage. Nevertheless, this approach can be dangerous,
    especially when applied to vital ML systems such as autonomous vehicles, finance,
    healthcare, and beyond. Even a minor glitch could lead to significant harm on
    a large scale.
  prefs: []
  type: TYPE_NORMAL
- en: In the race for progress, we must tread carefully, for haste in engineering
    and data science can shatter more than just code.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It's crucial to shift our mindset to steer clear of such risks. Rather than
    solely concentrating on model accuracy, we should also prioritize understanding
    the implications and potential risks associated with our work.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, our exploration of AI incidents, cultural competencies, and risk
    mitigation strategies underscores the critical importance of responsible AI development.
    Cultivating a culture of effective challenge and accountability within organizations
    ensures a proactive stance in identifying and rectifying potential pitfalls. Furthermore,
    integrating diverse and experienced teams and rigorous in-house testing practices
    strengthens the foundation for responsible AI implementation. Lastly, there is
    a necessity to prioritize comprehensive risk assessment over haste. These measures
    collectively guide us towards a future where AI systems are not only technologically
    advanced but also ethically sound, serving as valuable tools for the betterment
    of society.
  prefs: []
  type: TYPE_NORMAL
- en: '***> Read the next articles in this series >***'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/organizational-processes-for-machine-learning-risk-management-14f4444dd07f?source=post_page-----c38616c2ccdf--------------------------------)
    [## Organizational Processes for Machine Learning Risk Management'
  prefs: []
  type: TYPE_NORMAL
- en: Organizational processes are a key nontechnical determinant of reliability in
    ML systems.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/organizational-processes-for-machine-learning-risk-management-14f4444dd07f?source=post_page-----c38616c2ccdf--------------------------------)
    [](/bridging-domains-infusing-financial-privacy-and-software-best-practices-into-ml-risk-management-3de1fa1e6dd2?source=post_page-----c38616c2ccdf--------------------------------)
    [## Bridging Domains: Infusing Financial, Privacy, and Software Best Practices
    into ML Risk Management'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding strategies that go beyond traditional Model Risk Management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/bridging-domains-infusing-financial-privacy-and-software-best-practices-into-ml-risk-management-3de1fa1e6dd2?source=post_page-----c38616c2ccdf--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References & Further Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[What to Do When AI Fails](https://www.oreilly.com/radar/what-to-do-when-ai-fails/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims](https://arxiv.org/pdf/2004.07213.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning For High-Risk Application, Chapter 1 — Contemporary Machine
    Learning Risk Management](https://www.amazon.in/Machine-Learning-High-Risk-Applications-Responsible/dp/1098102436)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
