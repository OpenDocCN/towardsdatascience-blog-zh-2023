["```py\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom celery.result import AsyncResult\nfrom typing import Any\nfrom celery_worker import generate_text_task\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napp = FastAPI()\n\nclass Item(BaseModel):\n    prompt: str\n\n@app.post(\"/generate/\")\nasync def generate_text(item: Item) -> Any:\n    task = generate_text_task.delay(item.prompt)\n    return {\"task_id\": task.id}\n\n@app.get(\"/task/{task_id}\")\nasync def get_task(task_id: str) -> Any:\n    result = AsyncResult(task_id)\n    if result.ready():\n        res = result.get()\n        return {\"result\": res[0],\n                \"time\": res[1],\n                \"memory\": res[2]}\n    else:\n        return {\"status\": \"Task not completed yet\"}\n```", "```py\nfrom celery import Celery, signals\nfrom utils import generate_output\nfrom model_loader import ModelLoader\n\ndef make_celery(app_name=__name__):\n    backend = broker = 'redis://llama2_redis_1:6379/0'\n    return Celery(app_name, backend=backend, broker=broker)\n\ncelery = make_celery()\n\nmodel_loader = None\nmodel_path = \"meta-llama/Llama-2-7b-chat-hf\"\n\n@signals.worker_process_init.connect\ndef setup_model(signal, sender, **kwargs):\n    global model_loader\n    model_loader = ModelLoader(model_path)\n\n@celery.task\ndef generate_text_task(prompt):\n    time, memory, outputs = generate_output(\n        prompt, model_loader.model, model_loader.tokenizer\n    )\n    return model_loader.tokenizer.decode(outputs[0]), time, memory\n```", "```py\nimport os\nfrom transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclass ModelLoader:\n    def __init__(self, model_path: str):\n        self.model_path = model_path\n        self.config = AutoConfig.from_pretrained(\n            self.model_path,\n            trust_remote_code=True,\n            use_auth_token=os.getenv(\"HUGGINGFACE_TOKEN\"),\n        )\n        self.model = self._load_model()\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            self.model_path, use_auth_token=os.getenv(\"HUGGINGFACE_TOKEN\")\n        )\n\n    def _load_model(self):\n        model = AutoModelForCausalLM.from_pretrained(\n            self.model_path,\n            config=self.config,\n            trust_remote_code=True,\n            load_in_4bit=True,\n            device_map=\"auto\",\n            use_auth_token=os.getenv(\"HUGGINGFACE_TOKEN\"),\n        )\n        return model\n```", "```py\ndocker run --name redis-db -p 6379:6379 -d redis\n```", "```py\nimport subprocess\nimport redis_server\n\ndef install_redis_server(redis_version):\n    try:\n        subprocess.check_call([\"pip\", \"install\", f\"redis-server=={redis_version}\"])\n        print(f\"Redis server version {redis_version} installed successfully.\")\n    except subprocess.CalledProcessError:\n        print(\"Failed to install Redis server.\")\n        exit(1)\n\ndef start_redis_server():\n    try:\n        redis_server_path = redis_server.REDIS_SERVER_PATH\n        subprocess.Popen([redis_server_path])\n        print(\"Redis server started successfully.\")\n    except Exception as e:\n        print(\"Failed to start Redis server:\", str(e))\n        exit(1)\n\ndef main():\n    redis_version = \"6.0.9\"\n    install_redis_server(redis_version)\n    start_redis_server()\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\nimport http.client\nimport json\nimport time\n\nAPI_HOST = \"localhost\"\nAPI_PORT = 8000\n\ndef generate_text(prompt):\n    conn = http.client.HTTPConnection(API_HOST, API_PORT)\n    headers = {\"Content-type\": \"application/json\"}\n    data = {\"prompt\": prompt}\n    json_data = json.dumps(data)\n    conn.request(\"POST\", \"/generate/\", json_data, headers)\n    response = conn.getresponse()\n    result = json.loads(response.read().decode())\n    conn.close()\n    return result[\"task_id\"]\n\ndef get_task_status(task_id):\n    conn = http.client.HTTPConnection(API_HOST, API_PORT)\n    conn.request(\"GET\", f\"/task/{task_id}\")\n    response = conn.getresponse()\n    status = response.read().decode()\n    conn.close()\n    return status\n\ndef main():\n    prompt = input(\"Enter the prompt: \")\n\n    task_id = generate_text(prompt)\n    while True:\n        status = get_task_status(task_id)\n        if \"Task not completed yet\" not in status:\n            print(status)\n            break\n        time.sleep(2)\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\nimport time\nimport torch\nimport functools\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef time_decorator(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        end_time = time.time()\n        exec_time = end_time - start_time\n        return (result, exec_time)\n    return wrapper\n\ndef memory_decorator(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()\n        result, exec_time = func(*args, **kwargs)\n        peak_mem = torch.cuda.max_memory_allocated()\n        peak_mem_consumption = peak_mem / 1e9\n        return peak_mem_consumption, exec_time, result\n    return wrapper\n\n@memory_decorator\n@time_decorator\ndef generate_output(prompt: str, model: AutoModelForCausalLM, tokenizer: AutoTokenizer) -> torch.Tensor:\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n    input_ids = input_ids.to(\"cuda\")\n    outputs = model.generate(input_ids, max_length=500)\n    return outputs\n```", "```py\nFROM python:3.9-slim-buster\n\nWORKDIR /app\nADD . /app\n\nRUN pip install --no-cache-dir -r requirements.txt\nEXPOSE 80\n\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n```", "```py\nfastapi==0.99.1\nuvicorn==0.22.0\npydantic==1.10.10\ncelery==5.3.1\nredis==4.6.0\npython-dotenv==1.0.0\ntransformers==4.30.2\ntorch==2.0.1\naccelerate==0.21.0\nbitsandbytes==0.41.0\nscipy==1.11.1\n```", "```py\nversion: '3'\nservices:\n  web:\n    build: .\n    command: [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n    volumes:\n      - .:/app\n    ports:\n      - 8000:80\n    depends_on:\n      - redis\n  worker:\n    build: .\n    command: celery -A celery_worker worker --loglevel=info\n    volumes:\n      - .:/app\n    depends_on:\n      - redis\n  redis:\n    image: \"redis:alpine\"\n    ports:\n      - 6379:6379\n```"]