["```py\nimport numpy as np\nimport pandas as pd\nfrom catboost import CatBoostRegressor, CatBoostError\nfrom typing import Iterable\n\nclass ConformalMultiQuantile(CatBoostRegressor):\n\n    def __init__(self, quantiles:Iterable[float], *args, **kwargs):\n\n        \"\"\"\n        Initialize a ConformalMultiQuantile object.\n\n        Parameters\n        ----------\n        quantiles : Iterable[float]\n            The list of quantiles to use in multi-quantile regression.\n        *args\n            Variable length argument list.\n        **kwargs\n            Arbitrary keyword arguments.\n        \"\"\"\n\n        kwargs['loss_function'] = self.create_loss_function_str(quantiles)\n        super().__init__(*args, **kwargs)\n        self.quantiles = quantiles\n        self.calibration_adjustments = None\n\n    @staticmethod\n    def create_loss_function_str(quantiles:Iterable[float]):\n\n        \"\"\"\n        Format the quantiles as a string for Catboost\n\n        Paramters\n        ---------\n        quantiles : Union[float, List[float]]\n            A float or list of float quantiles\n\n        Returns\n        -------\n        The loss function definition for multi-quantile regression\n        \"\"\"\n\n        quantile_str = str(quantiles).replace('[','').replace(']','')\n\n        return f'MultiQuantile:alpha={quantile_str}'\n\n    def calibrate(self, x_cal, y_cal):\n\n        \"\"\"\n        Calibrate the multi-quantile model\n\n        Paramters\n        ---------\n        x_cal : ndarray\n            Calibration inputs\n        y_cal : ndarray\n            Calibration target\n        \"\"\"\n\n        # Ensure the model is fitted\n        if not self.is_fitted():\n\n            raise CatBoostError('There is no trained model to use calibrate(). Use fit() to train model. Then use this method.')\n\n        # Make predictions on the calibration set\n        uncalibrated_preds = self.predict(x_cal)\n\n        # Compute the difference between the uncalibrated predicted quantiles and the target\n        conformity_scores = uncalibrated_preds - np.array(y_cal).reshape(-1, 1)\n\n        # Store the 1-q quantile of the conformity scores\n        self.calibration_adjustments = \\\n            np.array([np.quantile(conformity_scores[:,i], 1-q) for i,q in enumerate(self.quantiles)])\n\n    def predict(self, data, prediction_type=None, ntree_start=0, ntree_end=0, thread_count=-1, verbose=None, task_type=\"CPU\"):\n\n        \"\"\"\n        Predict using the trained model.\n\n        Parameters\n        ----------\n        data : pandas.DataFrame or numpy.ndarray\n            Data to make predictions on\n        prediction_type : str, optional\n            Type of prediction result, by default None\n        ntree_start : int, optional\n            Number of trees to start prediction from, by default 0\n        ntree_end : int, optional\n            Number of trees to end prediction at, by default 0\n        thread_count : int, optional\n            Number of parallel threads to use, by default -1\n        verbose : bool or int, optional\n            Verbosity, by default None\n        task_type : str, optional\n            Type of task, by default \"CPU\"\n\n        Returns\n        -------\n        numpy.ndarray\n            The predicted values for the input data.\n        \"\"\"\n\n        preds = super().predict(data, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\n\n        # Adjust the predicted quantiles according to the quantiles of the\n        # conformity scores\n        if self.calibration_adjustments is not None:\n\n            preds = preds - self.calibration_adjustments\n\n        return preds\n```", "```py\n# Dependencies\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom catboost import CatBoostRegressor, CatBoostError\nfrom sklearn.model_selection import train_test_split\nfrom typing import Iterable\npd.set_option('display.max.columns', None)\nsns.set()\n\n# Read in superconductivity dataset\ndata = pd.read_csv('train.csv')\n\n# Predicting critical temperature\ntarget = 'critical_temp'\n\n# 80/20 train/test split\nx_train, x_test, y_train, y_test = train_test_split(data.drop(target, axis=1), data[target], test_size=0.20)\n\n# Hold out 20% of the training data for calibration\nx_train, x_cal, y_train, y_cal = train_test_split(x_train, y_train, test_size=0.20)\n\nprint(\"Training shape:\", x_train.shape) # Training shape: (13608, 81)\nprint(\"Calibration shape:\", x_cal.shape) # Calibration shape: (3402, 81)\nprint(\"Testing shape:\", x_test.shape) # Testing shape: (4253, 81)\n```", "```py\n# Store quantiles 0.005 through 0.99 in a list\nquantiles = [q/200 for q in range(1, 200)]\n\n# Instantiate the conformal multi-quantile model\nconformal_model = ConformalMultiQuantile(iterations=100,\n                                        quantiles=quantiles,\n                                        verbose=10)\n\n# Fit the conformal multi-quantile model\nconformal_model.fit(x_train, y_train)\n\n# Get predictions before calibration\npreds_uncalibrated = conformal_model.predict(x_test)\npreds_uncalibrated = pd.DataFrame(preds_uncalibrated, columns=[f'pred_{q}' for q in quantiles])\n\n# Calibrate the model\nconformal_model.calibrate(x_cal, y_cal)\n\n# Get calibrated predictions\npreds_calibrated = conformal_model.predict(x_test)\npreds_calibrated = pd.DataFrame(preds_calibrated, columns=[f'pred_{q}' for q in quantiles])\n\npreds_calibrated.head()\n```", "```py\n# Initialize an empty DataFrame\ncomparison_df = pd.DataFrame()\n\n# For each predicted quantile\nfor i, quantile in enumerate(quantiles):\n\n    # Compute the proportion of testing observations that were less than or equal \n    # to the uncalibrated predicted quantile\n    actual_prob_uncal = np.mean(y_test.values <= preds_uncalibrated[f'pred_{quantile}'])\n\n    # Compute the proportion of testing observations that were less than or equal \n    # to the calibrated predicted quantile\n    actual_prob_cal = np.mean(y_test.values <= preds_calibrated[f'pred_{quantile}'])\n\n    comparison_df_curr = pd.DataFrame({\n                                    'desired_probability':quantile,\n                                    'actual_uncalibrated_probability':actual_prob_uncal,\n                                    'actual_calibrated_probability':actual_prob_cal}, index=[i])\n\n    comparison_df = pd.concat([comparison_df, comparison_df_curr])\n\ncomparison_df['abs_diff_uncal'] = (comparison_df['desired_probability'] - comparison_df['actual_uncalibrated_probability']).abs()\ncomparison_df['abs_diff_cal'] = (comparison_df['desired_probability'] - comparison_df['actual_calibrated_probability']).abs()\n\nprint(\"Uncalibrated quantile MAE:\", comparison_df['abs_diff_uncal'].mean()) \nprint(\"Calibrated quantile MAE:\", comparison_df['abs_diff_cal'].mean()) \n\n# Uncalibrated quantile MAE: 0.02572999018133225\n# Calibrated quantile MAE: 0.007850550660662823\n```", "```py\ncoverage_df = pd.DataFrame()\n\nfor i, alpha in enumerate(np.arange(0.01, 0.41, 0.01)):\n\n    lower_quantile = round(alpha/2, 3)\n    upper_quantile = round(1 - alpha/2, 3)\n\n    # Compare actual to expected coverage for both models\n    lower_prob_uncal = comparison_df[comparison_df['desired_probability'] == lower_quantile]['actual_uncalibrated_probability'].values[0]\n    upper_prob_uncal = comparison_df[comparison_df['desired_probability'] == upper_quantile]['actual_uncalibrated_probability'].values[0]\n\n    lower_prob_cal = comparison_df[comparison_df['desired_probability'] == lower_quantile]['actual_calibrated_probability'].values[0]\n    upper_prob_cal = comparison_df[comparison_df['desired_probability'] == upper_quantile]['actual_calibrated_probability'].values[0]\n\n    coverage_df_curr = pd.DataFrame({'desired_coverage':1-alpha,\n                                    'actual_uncalibrated_coverage':upper_prob_uncal - lower_prob_uncal,\n                                    'actual_calibrated_coverage':upper_prob_cal - lower_prob_cal}, index=[i])\n\n    coverage_df = pd.concat([coverage_df, coverage_df_curr])\n\ncoverage_df['abs_diff_uncal'] = (coverage_df['desired_coverage'] - coverage_df['actual_uncalibrated_coverage']).abs()\ncoverage_df['abs_diff_cal'] = (coverage_df['desired_coverage'] - coverage_df['actual_calibrated_coverage']).abs()\n\nprint(\"Uncalibrated Coverage MAE:\", coverage_df['abs_diff_uncal'].mean()) \nprint(\"Calibrated Coverage MAE:\", coverage_df['abs_diff_cal'].mean()) \n\n# Uncalibrated Coverage MAE: 0.03660674817775689\n# Calibrated Coverage MAE: 0.003543616270867622\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(coverage_df['desired_coverage'],\n        coverage_df['desired_coverage'],\n        label='Perfect Calibration')\nax.scatter(coverage_df['desired_coverage'],\n           coverage_df['actual_uncalibrated_coverage'],\n           color='orange',\n           label='Uncalibrated Model')\nax.scatter(coverage_df['desired_coverage'],\n           coverage_df['actual_calibrated_coverage'],\n           color='green',\n           label='Calibrated Model')\n\nax.set_xlabel('Desired Coverage')\nax.set_ylabel('Actual Coverage')\nax.set_title('Desired vs Actual Coverage')\nax.legend()\nplt.show()\n```", "```py\n# Fit a model using the training and calibration data\nregular_model = ConformalMultiQuantile(iterations=100,\n                                        quantiles=quantiles,\n                                        verbose=10)\n\nregular_model.fit(pd.concat([x_train, x_cal]), pd.concat([y_train, y_cal]))\n\n# Fit a model on the training data only\nconformal_model = ConformalMultiQuantile(iterations=100,\n                                        quantiles=quantiles,\n                                        verbose=10)\n\nconformal_model.fit(x_train, y_train)\n\n# Get predictions before calibration\npreds_uncalibrated = regular_model.predict(x_test)\npreds_uncalibrated = pd.DataFrame(preds_uncalibrated, columns=[f'pred_{q}' for q in quantiles])\n\n# Calibrate the model\nconformal_model.calibrate(x_cal, y_cal)\n\n# Get calibrated predictions\npreds_calibrated = conformal_model.predict(x_test)\npreds_calibrated = pd.DataFrame(preds_calibrated, columns=[f'pred_{q}' for q in quantiles])\n\ncomparison_df = pd.DataFrame()\n\n# Compare actual to predicted left-tailed probabilities\nfor i, quantile in enumerate(quantiles):\n\n    actual_prob_uncal = np.mean(y_test.values <= preds_uncalibrated[f'pred_{quantile}'])\n    actual_prob_cal = np.mean(y_test.values <= preds_calibrated[f'pred_{quantile}'])\n\n    comparison_df_curr = pd.DataFrame({\n                                    'desired_probability':quantile,\n                                    'actual_uncalibrated_probability':actual_prob_uncal,\n                                    'actual_calibrated_probability':actual_prob_cal}, index=[i])\n\n    comparison_df = pd.concat([comparison_df, comparison_df_curr])\n\ncomparison_df['abs_diff_uncal'] = (comparison_df['desired_probability'] - comparison_df['actual_uncalibrated_probability']).abs()\ncomparison_df['abs_diff_cal'] = (comparison_df['desired_probability'] - comparison_df['actual_calibrated_probability']).abs()\n\nprint(\"Uncalibrated quantile MAE:\", comparison_df['abs_diff_uncal'].mean()) \nprint(\"Calibrated quantile MAE:\", comparison_df['abs_diff_cal'].mean()) \n\n# Uncalibrated quantile MAE: 0.023452756375340143\n# Calibrated quantile MAE: 0.0061827359227361834\n```", "```py\nfrom sklearn.metrics import r2_score, mean_absolute_error\n\nprint(f\"Uncalibrated R2 Score: {r2_score(y_test, preds_uncalibrated.mean(axis=1))}\")\nprint(f\"Calibrated R2 Score: {r2_score(y_test, preds_calibrated.mean(axis=1))} \\n\")\n\nprint(f\"Uncalibrated MAE: {mean_absolute_error(y_test, preds_uncalibrated.mean(axis=1))}\")\nprint(f\"Calibrated MAE: {mean_absolute_error(y_test, preds_calibrated.mean(axis=1))} \\n\")\n\n# Uncalibrated R2 Score: 0.8060126144892599\n# Calibrated R2 Score: 0.8053382438575666 \n\n# Uncalibrated MAE: 10.622258046774979\n# Calibrated MAE: 10.557269513856014 \n```"]