- en: Building a Batch Data Pipeline with Athena and MySQL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Athena å’Œ MySQL æ„å»ºæ‰¹é‡æ•°æ®ç®¡é“
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/building-a-batch-data-pipeline-with-athena-and-mysql-7e60575ff39c](https://towardsdatascience.com/building-a-batch-data-pipeline-with-athena-and-mysql-7e60575ff39c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/building-a-batch-data-pipeline-with-athena-and-mysql-7e60575ff39c](https://towardsdatascience.com/building-a-batch-data-pipeline-with-athena-and-mysql-7e60575ff39c)
- en: An End-To-End Tutorial for Beginners
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆå­¦è€…çš„ç«¯åˆ°ç«¯æ•™ç¨‹
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----7e60575ff39c--------------------------------)[![ğŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----7e60575ff39c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7e60575ff39c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7e60575ff39c--------------------------------)
    [ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----7e60575ff39c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mshakhomirov.medium.com/?source=post_page-----7e60575ff39c--------------------------------)[![ğŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----7e60575ff39c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7e60575ff39c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7e60575ff39c--------------------------------)
    [ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----7e60575ff39c--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7e60575ff39c--------------------------------)
    Â·16 min readÂ·Oct 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7e60575ff39c--------------------------------)
    Â·16 min é˜…è¯»Â·2023 å¹´ 10 æœˆ 20 æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/368293b91e4bc0283007a555789b6479.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/368293b91e4bc0283007a555789b6479.png)'
- en: Photo by [Redd F](https://unsplash.com/@raddfilms?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Redd F](https://unsplash.com/@raddfilms?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥è‡ª [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In this story I will speak about one of the most popular ways to run data transformation
    tasks â€” batch data processing. This data pipeline design pattern becomes incredibly
    useful when we need to process data in chunks making it very efficient for ETL
    jobs that require scheduling. I will demonstrate how it can be achieved by building
    a data transformation pipeline using MySQL and Athena. We will use infrastructure
    as code to deploy it in the cloud.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªæ•…äº‹ä¸­ï¼Œæˆ‘å°†è®²è¿°ä¸€ç§éå¸¸æµè¡Œçš„æ•°æ®è½¬æ¢ä»»åŠ¡æ‰§è¡Œæ–¹å¼â€”â€”æ‰¹é‡æ•°æ®å¤„ç†ã€‚å½“æˆ‘ä»¬éœ€è¦ä»¥å—çŠ¶æ–¹å¼å¤„ç†æ•°æ®æ—¶ï¼Œè¿™ç§æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼å˜å¾—æå…¶æœ‰ç”¨ï¼Œéå¸¸é€‚åˆéœ€è¦è°ƒåº¦çš„
    ETL ä½œä¸šã€‚æˆ‘å°†é€šè¿‡ä½¿ç”¨ MySQL å’Œ Athena æ„å»ºæ•°æ®è½¬æ¢ç®¡é“æ¥å±•ç¤ºå¦‚ä½•å®ç°è¿™ä¸€ç›®æ ‡ã€‚æˆ‘ä»¬å°†ä½¿ç”¨åŸºç¡€è®¾æ–½å³ä»£ç åœ¨äº‘ä¸­éƒ¨ç½²å®ƒã€‚
- en: Imagine that you have just joined a company as a Data Engineer. Their data stack
    is modern, event-driven, cost-effective, flexible, and can scale easily to meet
    the growing data resources you have. External data sources and data pipelines
    in your data platform are managed by the data engineering team using a flexible
    environment setup with CI/CD GitHub integration.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸‹ï¼Œä½ åˆšåˆšä½œä¸ºæ•°æ®å·¥ç¨‹å¸ˆåŠ å…¥äº†ä¸€å®¶å…¬å¸ã€‚ä»–ä»¬çš„æ•°æ®å †æ ˆç°ä»£ã€äº‹ä»¶é©±åŠ¨ã€æˆæœ¬æ•ˆç›Šé«˜ã€çµæ´»ï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾æ‰©å±•ä»¥æ»¡è¶³ä¸æ–­å¢é•¿çš„æ•°æ®èµ„æºã€‚ä½ æ•°æ®å¹³å°ä¸­çš„å¤–éƒ¨æ•°æ®æºå’Œæ•°æ®ç®¡é“ç”±æ•°æ®å·¥ç¨‹å›¢é˜Ÿç®¡ç†ï¼Œä½¿ç”¨å…·æœ‰
    CI/CD GitHub é›†æˆçš„çµæ´»ç¯å¢ƒè®¾ç½®ã€‚
- en: As a data engineer you need to create a business intelligence dashboard that
    displays the geography of company revenue streams as shown below. Raw payment
    data is stored in the server database (MySQL). You want to build a batch pipeline
    that extracts data from that database daily, then use AWS S3 to store data files
    and Athena to process it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºæ•°æ®å·¥ç¨‹å¸ˆï¼Œä½ éœ€è¦åˆ›å»ºä¸€ä¸ªä¸šåŠ¡æ™ºèƒ½ä»ªè¡¨æ¿ï¼Œå±•ç¤ºå…¬å¸æ”¶å…¥æ¥æºçš„åœ°ç†åˆ†å¸ƒï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚åŸå§‹æ”¯ä»˜æ•°æ®å­˜å‚¨åœ¨æœåŠ¡å™¨æ•°æ®åº“ï¼ˆMySQLï¼‰ä¸­ã€‚ä½ æƒ³æ„å»ºä¸€ä¸ªæ‰¹é‡ç®¡é“ï¼Œä»è¯¥æ•°æ®åº“ä¸­æ¯æ—¥æå–æ•°æ®ï¼Œç„¶åä½¿ç”¨
    AWS S3 å­˜å‚¨æ•°æ®æ–‡ä»¶ï¼Œå¹¶ä½¿ç”¨ Athena è¿›è¡Œå¤„ç†ã€‚
- en: '![](../Images/7dc86278ad5d6755486da64418c7b7bf.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7dc86278ad5d6755486da64418c7b7bf.png)'
- en: Revenue dashboard. Image by author.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¶å…¥ä»ªè¡¨æ¿ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: Batch data pipeline
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰¹é‡æ•°æ®ç®¡é“
- en: A data pipeline can be considered as a sequence of data processing steps. Due
    to ***logical data flow connections*** between these stages, each stage generates
    an **output** that serves as an **input** for the following stage.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®ç®¡é“å¯ä»¥è¢«è§†ä¸ºä¸€ç³»åˆ—æ•°æ®å¤„ç†æ­¥éª¤ã€‚ç”±äºè¿™äº›é˜¶æ®µä¹‹é—´çš„***é€»è¾‘æ•°æ®æµè¿æ¥***ï¼Œæ¯ä¸ªé˜¶æ®µç”Ÿæˆçš„**è¾“å‡º**ä½œä¸ºä¸‹ä¸€ä¸ªé˜¶æ®µçš„**è¾“å…¥**ã€‚
- en: There is a data pipeline whenever there is data processing between points A
    and B.
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åªè¦åœ¨ç‚¹ A å’Œç‚¹ B ä¹‹é—´è¿›è¡Œæ•°æ®å¤„ç†ï¼Œå°±å­˜åœ¨æ•°æ®ç®¡é“ã€‚
- en: 'Data pipelines might be different due it their conceptual and logical nature.
    I previously wrote about it here [1]:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®ç®¡é“å¯èƒ½å› å…¶æ¦‚å¿µå’Œé€»è¾‘æ€§è´¨è€Œæœ‰æ‰€ä¸åŒã€‚æˆ‘ä¹‹å‰åœ¨è¿™é‡Œå†™è¿‡ [1]ï¼š
- en: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----7e60575ff39c--------------------------------)
    [## Data pipeline design patterns'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼](https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----7e60575ff39c--------------------------------)'
- en: Choosing the right architecture with examples
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€‰æ‹©åˆé€‚çš„æ¶æ„åŠå…¶ç¤ºä¾‹
- en: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----7e60575ff39c--------------------------------)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼](https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----7e60575ff39c--------------------------------)'
- en: 'We would want to create a data pipeline where data is being transformed in
    the following **steps**:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›åˆ›å»ºä¸€ä¸ªæ•°æ®ç®¡é“ï¼Œåœ¨ä»¥ä¸‹ **æ­¥éª¤** ä¸­è½¬æ¢æ•°æ®ï¼š
- en: 1\. Use a Lambda function that extracts data from MySQL database tables `myschema.users`
    and `myschema.transactions` into S3 datalake bucket.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. ä½¿ç”¨ Lambda å‡½æ•°å°†æ•°æ®ä» MySQL æ•°æ®åº“è¡¨ `myschema.users` å’Œ `myschema.transactions` æå–åˆ°
    S3 æ•°æ®æ¹–æ¡¶ä¸­ã€‚
- en: 2\. Add a State Machine node with Athena resource to start execution (`arn:aws:states:::athena:startQueryExecution.sync`)
    and create a database called `mydatabase`
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. æ·»åŠ ä¸€ä¸ªå…·æœ‰ Athena èµ„æºçš„çŠ¶æ€æœºèŠ‚ç‚¹ä»¥å¯åŠ¨æ‰§è¡Œ (`arn:aws:states:::athena:startQueryExecution.sync`)
    å¹¶åˆ›å»ºä¸€ä¸ªåä¸º `mydatabase` çš„æ•°æ®åº“
- en: 3\. Create another data pipeline node to show existing tables in Athena database.
    Use the output of this node to perform required data transformations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. åˆ›å»ºå¦ä¸€ä¸ªæ•°æ®ç®¡é“èŠ‚ç‚¹ä»¥æ˜¾ç¤º Athena æ•°æ®åº“ä¸­çš„ç°æœ‰è¡¨ã€‚ä½¿ç”¨è¯¥èŠ‚ç‚¹çš„è¾“å‡ºæ‰§è¡Œæ‰€éœ€çš„æ•°æ®è½¬æ¢ã€‚
- en: 'If tables donâ€™t exist then we would want our pieline to create them in Athena
    based on the data from the datalake S3 bucket. We would want to create two **external
    tables** with data from MySQL:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¡¨ä¸å­˜åœ¨ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç®¡é“åœ¨ Athena ä¸­æ ¹æ®æ¥è‡ªæ•°æ®æ¹– S3 æ¡¶çš„æ•°æ®åˆ›å»ºå®ƒä»¬ã€‚æˆ‘ä»¬å¸Œæœ›åˆ›å»ºä¸¤ä¸ª **å¤–éƒ¨è¡¨**ï¼Œæ•°æ®æ¥è‡ª MySQLï¼š
- en: mydatabase.users (LOCATION â€˜s3://<YOUR_DATALAKE_BUCKET>/data/myschema/users/â€™)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: mydatabase.users (LOCATION â€˜s3://<YOUR_DATALAKE_BUCKET>/data/myschema/users/â€™)
- en: mydatabase.transactions (LOCATION â€˜s3://<YOUR_DATALAKE_BUCKET>/data/myschema/transactions/â€™)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: mydatabase.transactions (LOCATION â€˜s3://<YOUR_DATALAKE_BUCKET>/data/myschema/transactions/â€™)
- en: 'Then we would want to create an **optimized ICEBERG** table:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¸Œæœ›åˆ›å»ºä¸€ä¸ª **ä¼˜åŒ–çš„ ICEBERG** è¡¨ï¼š
- en: 'mydatabase.user_transactions (â€˜table_typeâ€™=â€™ICEBERGâ€™, â€˜formatâ€™=â€™parquetâ€™) using
    the SQL below:'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: mydatabase.user_transactions (â€˜table_typeâ€™=â€™ICEBERGâ€™, â€˜formatâ€™=â€™parquetâ€™) ä½¿ç”¨ä»¥ä¸‹
    SQLï¼š
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will also use MERGE to update this table.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å°†ä½¿ç”¨ MERGE æ¥æ›´æ–°æ­¤è¡¨ã€‚
- en: 'MERGE is an extremely useful SQL techniques for incremental updates in tables.
    Check my previous story [3] for more advanced examples:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: MERGE æ˜¯ä¸€ç§éå¸¸æœ‰ç”¨çš„ SQL æŠ€å·§ï¼Œç”¨äºè¡¨ä¸­çš„å¢é‡æ›´æ–°ã€‚æŸ¥çœ‹æˆ‘ä¹‹å‰çš„æ•…äº‹ [3] ä»¥è·å–æ›´é«˜çº§çš„ç¤ºä¾‹ï¼š
- en: '[](/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----7e60575ff39c--------------------------------)
    [## Advanced SQL techniques for beginners'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[é«˜çº§ SQL æŠ€å·§](https://towardsdatascience.com/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----7e60575ff39c--------------------------------)'
- en: On a scale from 1 to 10 how good are your data warehousing skills?
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä» 1 åˆ° 10ï¼Œä½ çš„æ•°æ®ä»“åº“æŠ€èƒ½æœ‰å¤šå¥½ï¼Ÿ
- en: towardsdatascience.com](/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----7e60575ff39c--------------------------------)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[é«˜çº§ SQL æŠ€å·§](https://towardsdatascience.com/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----7e60575ff39c--------------------------------)'
- en: Athena can analyse structured, unstructured and semi-structured data stored
    in Amazon S3 by running attractive ad-hoc SQL queries with no need to manage the
    infrastructure.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Athena å¯ä»¥é€šè¿‡è¿è¡Œæœ‰å¸å¼•åŠ›çš„å³å¸­ SQL æŸ¥è¯¢æ¥åˆ†æå­˜å‚¨åœ¨ Amazon S3 ä¸­çš„ç»“æ„åŒ–ã€éç»“æ„åŒ–å’ŒåŠç»“æ„åŒ–æ•°æ®ï¼Œæ— éœ€ç®¡ç†åŸºç¡€è®¾æ–½ã€‚
- en: We donâ€™t need to load data and it makes it a perfect choice for our task.
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸éœ€è¦åŠ è½½æ•°æ®ï¼Œè¿™ä½¿å¾—å®ƒæˆä¸ºæˆ‘ä»¬ä»»åŠ¡çš„å®Œç¾é€‰æ‹©ã€‚
- en: It can be easily integrated with Busines Intelligence (BI) solutions such as
    Quichksight to generate reports.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå¯ä»¥è½»æ¾åœ°ä¸ Business Intelligence (BI) è§£å†³æ–¹æ¡ˆå¦‚ QuickSight é›†æˆä»¥ç”ŸæˆæŠ¥å‘Šã€‚
- en: 'ICEBERG is an extremely useful and efficient table format where several separate
    programs can handle the same dataset concurrently and consistently [2]. I previously
    wrote about it here:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ICEBERG æ˜¯ä¸€ç§éå¸¸æœ‰ç”¨ä¸”é«˜æ•ˆçš„è¡¨æ ¼æ ¼å¼ï¼Œå¤šä¸ªç‹¬ç«‹ç¨‹åºå¯ä»¥åŒæ—¶ä¸”ä¸€è‡´åœ°å¤„ç†ç›¸åŒçš„æ•°æ®é›† [2]ã€‚æˆ‘ä¹‹å‰åœ¨è¿™é‡Œå†™è¿‡ï¼š
- en: '[](/introduction-to-apache-iceberg-tables-a791f1758009?source=post_page-----7e60575ff39c--------------------------------)
    [## Introduction to Apache Iceberg Tables'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[ä»‹ç» Apache Iceberg è¡¨](https://towardsdatascience.com/introduction-to-apache-iceberg-tables-a791f1758009?source=post_page-----7e60575ff39c--------------------------------)'
- en: A few Compelling Reasons to Choose Apache Iceberg for Data Lakes
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€‰æ‹© Apache Iceberg ä½œä¸ºæ•°æ®æ¹–çš„å‡ ä¸ªæœ‰åŠ›ç†ç”±
- en: towardsdatascience.com](/introduction-to-apache-iceberg-tables-a791f1758009?source=post_page-----7e60575ff39c--------------------------------)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[ä»‹ç» Apache Iceberg è¡¨](https://towardsdatascience.com/introduction-to-apache-iceberg-tables-a791f1758009?source=post_page-----7e60575ff39c--------------------------------)'
- en: MySQL data connector
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MySQL æ•°æ®è¿æ¥å™¨
- en: Letâ€™s create an AWS Lambda Function that will be able to execute SQL queries
    in MySQL database.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª AWS Lambda å‡½æ•°ï¼Œå®ƒèƒ½å¤Ÿåœ¨ MySQL æ•°æ®åº“ä¸­æ‰§è¡Œ SQL æŸ¥è¯¢ã€‚
- en: The code is pretty simple and generic. It can be used in any serverless application
    with any cloud service provider.
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä»£ç éå¸¸ç®€å•ä¸”é€šç”¨ã€‚å®ƒå¯ä»¥åœ¨ä»»ä½•æ— æœåŠ¡å™¨åº”ç”¨ç¨‹åºä¸­ä¸ä»»ä½•äº‘æœåŠ¡æä¾›å•†ä¸€èµ·ä½¿ç”¨ã€‚
- en: 'We will use it to extract revenue data into the datalake. Suggested Lambda
    folder structure can look as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨å®ƒå°†æ”¶å…¥æ•°æ®æå–åˆ°æ•°æ®æ¹–ä¸­ã€‚å»ºè®®çš„ Lambda æ–‡ä»¶å¤¹ç»“æ„å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We will integrate this tiny service into the pipeline using AWS Step functions
    for easy **orchestration and visualisation.**
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é€šè¿‡ AWS Step Functions å°†è¿™ä¸ªå°æœåŠ¡é›†æˆåˆ°ç®¡é“ä¸­ï¼Œä»¥ä¾¿äº **ç¼–æ’å’Œå¯è§†åŒ–**ã€‚
- en: 'To create a Lambda function that can extract data from MySQL database we need
    to create a folder for our Lambda first. Create a new folder called stack` and
    then folder called `mysql_connector` in it:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿä» MySQL æ•°æ®åº“ä¸­æå–æ•°æ®çš„ Lambda å‡½æ•°ï¼Œæˆ‘ä»¬éœ€è¦å…ˆä¸ºæˆ‘ä»¬çš„ Lambda åˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ã€‚é¦–å…ˆåˆ›å»ºä¸€ä¸ªåä¸º stack`
    çš„æ–°æ–‡ä»¶å¤¹ï¼Œç„¶ååœ¨å…¶ä¸­åˆ›å»ºä¸€ä¸ªåä¸º `mysql_connector` çš„æ–‡ä»¶å¤¹ï¼š
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then we can use this code below (replace database connection settings with
    yours) to create `app.py`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç ï¼ˆå°†æ•°æ®åº“è¿æ¥è®¾ç½®æ›¿æ¢ä¸ºä½ çš„è®¾ç½®ï¼‰æ¥åˆ›å»º `app.py`ï¼š
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To deploy our microservice using AWS CLI run this in your command line (assuming
    you are in the ./stack folder):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨ AWS CLI éƒ¨ç½²æˆ‘ä»¬çš„å¾®æœåŠ¡ï¼Œè¯·åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼ˆå‡è®¾ä½ åœ¨ ./stack æ–‡ä»¶å¤¹ä¸­ï¼‰ï¼š
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Make sure that AWS Lambda role exists before running the next part ` â€” role
    arn:aws:iam::<your-aws-account-id>:role/my-lambda-role`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿åœ¨è¿è¡Œä¸‹ä¸€éƒ¨åˆ†ä¹‹å‰ AWS Lambda è§’è‰²å·²ç»å­˜åœ¨ ` â€” role arn:aws:iam::<your-aws-account-id>:role/my-lambda-role`ã€‚
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Our MySQL instance must have **S3 integration** which enables **data export
    to S3** bucket. It can be achieved by running this SQL query:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ MySQL å®ä¾‹å¿…é¡»å…·å¤‡ **S3 é›†æˆ**ï¼Œä»¥ä¾¿ **å°†æ•°æ®å¯¼å‡ºåˆ° S3** æ¡¶ã€‚è¿™å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹ SQL æŸ¥è¯¢å®ç°ï¼š
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: How to create MySQL instance
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¦‚ä½•åˆ›å»º MySQL å®ä¾‹
- en: 'We can use CloudFormation template and infrastructure as code to create MySQL
    database. Consider this AWS command:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ CloudFormation æ¨¡æ¿å’ŒåŸºç¡€è®¾æ–½å³ä»£ç æ¥åˆ›å»º MySQL æ•°æ®åº“ã€‚è€ƒè™‘è¿™ä¸ª AWS å‘½ä»¤ï¼š
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'It will use `cfn_mysql.yaml` tempalte file to create CloudFormation stack called
    MySQLDB. I previously wrote about it here [4]:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå°†ä½¿ç”¨ `cfn_mysql.yaml` æ¨¡æ¿æ–‡ä»¶æ¥åˆ›å»ºåä¸º MySQLDB çš„ CloudFormation å †æ ˆã€‚æˆ‘ä¹‹å‰åœ¨è¿™é‡Œå†™è¿‡æœ‰å…³å®ƒçš„å†…å®¹ [4]ï¼š
- en: '[](/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a?source=post_page-----7e60575ff39c--------------------------------)
    [## Create MySQL and Postgres instances using AWS Cloudformation'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a?source=post_page-----7e60575ff39c--------------------------------)
    [## ä½¿ç”¨ AWS CloudFormation åˆ›å»º MySQL å’Œ Postgres å®ä¾‹'
- en: Infrastructure as Code for database practitioners
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•°æ®åº“ä»ä¸šäººå‘˜çš„åŸºç¡€è®¾æ–½å³ä»£ç 
- en: towardsdatascience.com](/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a?source=post_page-----7e60575ff39c--------------------------------)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a?source=post_page-----7e60575ff39c--------------------------------)
- en: 'Our `cfn_mysql.yaml` should look like this:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ `cfn_mysql.yaml` åº”è¯¥å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If everything goes well we will see a new stack in our Amazon account:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¸€åˆ‡é¡ºåˆ©ï¼Œæˆ‘ä»¬å°†çœ‹åˆ° Amazon è´¦æˆ·ä¸­å‡ºç°ä¸€ä¸ªæ–°çš„å †æ ˆï¼š
- en: '![](../Images/d6ff5754b68a1d8279412c0bb82d917b.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6ff5754b68a1d8279412c0bb82d917b.png)'
- en: CloudFormation stack with MySQL instance. Image by author.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰ MySQL å®ä¾‹çš„ CloudFormation å †æ ˆã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: 'Now we can use this MySQL instance in our pipeline. We can try our SQL queries
    in any SQL tool such as SQL Workbench to populate table data. These tables will
    be used later to create external tables using Athena and can be created using
    SQL:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥åœ¨æˆ‘ä»¬çš„æ•°æ®ç®¡é“ä¸­ä½¿ç”¨è¿™ä¸ª MySQL å®ä¾‹ã€‚æˆ‘ä»¬å¯ä»¥åœ¨ä»»ä½• SQL å·¥å…·ä¸­å°è¯•æˆ‘ä»¬çš„ SQL æŸ¥è¯¢ï¼Œä¾‹å¦‚ SQL Workbenchï¼Œä»¥å¡«å……è¡¨æ•°æ®ã€‚è¿™äº›è¡¨å°†ç”¨äºç¨åä½¿ç”¨
    Athena åˆ›å»ºå¤–éƒ¨è¡¨ï¼Œå¯ä»¥é€šè¿‡ SQL åˆ›å»ºï¼š
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Process data using Athena
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Athena å¤„ç†æ•°æ®
- en: Now we would want to add a data pipeline workflow that triggers our Lambda function
    to extract data from MySQL, save it in the datalake and then start data transformation
    in Athena.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¸Œæœ›æ·»åŠ ä¸€ä¸ªæ•°æ®ç®¡é“å·¥ä½œæµï¼Œè¯¥å·¥ä½œæµè§¦å‘æˆ‘ä»¬çš„ Lambda å‡½æ•°ä»¥ä» MySQL æå–æ•°æ®ï¼Œå°†å…¶ä¿å­˜åˆ°æ•°æ®æ¹–ä¸­ï¼Œç„¶ååœ¨ Athena ä¸­å¼€å§‹æ•°æ®è½¬æ¢ã€‚
- en: 'We would want to create two external Athena tables with data from MySQL:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›ä½¿ç”¨ MySQL ä¸­çš„æ•°æ®åˆ›å»ºä¸¤ä¸ªå¤–éƒ¨ Athena è¡¨ï¼š
- en: myschema.users
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: myschema.users
- en: myschema.transactions
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: myschema.transactions
- en: Then we would want to create an optimized ICEBERG table **myschema.user_transactions**
    to connect it to our BI solution.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¸Œæœ›åˆ›å»ºä¸€ä¸ªä¼˜åŒ–çš„ ICEBERG è¡¨ **myschema.user_transactions**ï¼Œå°†å…¶è¿æ¥åˆ°æˆ‘ä»¬çš„ BI è§£å†³æ–¹æ¡ˆã€‚
- en: We would want to INSERT new data into that table using MERGE statement.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›ä½¿ç”¨ MERGE è¯­å¥å°†æ–°æ•°æ®æ’å…¥åˆ°è¯¥è¡¨ä¸­ã€‚
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'When new table is ready we can check it by running `SELECT *`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ–°è¡¨å‡†å¤‡å¥½åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è¿è¡Œ `SELECT *` æ¥æ£€æŸ¥å®ƒï¼š
- en: '![](../Images/0067053393a777846c94f0d3acd48e90.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0067053393a777846c94f0d3acd48e90.png)'
- en: mydatabase.user_transactions. Image by author.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: mydatabase.user_transactionsã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Orchestrate data pipeline using Step Functions (State Machine)
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Step Functionsï¼ˆçŠ¶æ€æœºï¼‰ç¼–æ’æ•°æ®ç®¡é“
- en: 'In the previous steps, we learned how to deploy each step of the data pipeline
    separately and then test it. In this paragraph, we will see how to create a complete
    data pipeline with required resources using infrastructure such as code and pipeline
    orchestration tool such as AWS Step Functions (State Machine). When we finish
    the pipeline graph will look like this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¹‹å‰çš„æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•åˆ†åˆ«éƒ¨ç½²æ•°æ®ç®¡é“çš„æ¯ä¸€æ­¥å¹¶è¿›è¡Œæµ‹è¯•ã€‚åœ¨è¿™ä¸€æ®µä¸­ï¼Œæˆ‘ä»¬å°†äº†è§£å¦‚ä½•ä½¿ç”¨åŸºç¡€è®¾æ–½ä»£ç å’Œç®¡é“ç¼–æ’å·¥å…·å¦‚AWS Step Functionsï¼ˆçŠ¶æ€æœºï¼‰åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„æ•°æ®ç®¡é“ã€‚å½“æˆ‘ä»¬å®Œæˆæ—¶ï¼Œç®¡é“å›¾å°†å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../Images/3d6fa66e126398bfe0bd267f57bf7059.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d6fa66e126398bfe0bd267f57bf7059.png)'
- en: Data pipeline orchestration using Step Functions. Image by author.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Step Functionsè¿›è¡Œæ•°æ®ç®¡é“ç¼–æ’ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: 'Data pipeline orchestration is a great data engineering technique that adds
    interactivity to our data pipelines. The idea was previously explained in one
    of my stories [5]:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®ç®¡é“ç¼–æ’æ˜¯ä¸€ç§å¾ˆå¥½çš„æ•°æ®å·¥ç¨‹æŠ€æœ¯ï¼Œå®ƒä¸ºæˆ‘ä»¬çš„æ•°æ®ç®¡é“å¢åŠ äº†äº’åŠ¨æ€§ã€‚è¿™ä¸ªæƒ³æ³•åœ¨æˆ‘ä¹‹å‰çš„ä¸€ç¯‡æ•…äº‹ä¸­å·²ç»è§£é‡Šè¿‡[5]ï¼š
- en: '[](/data-pipeline-orchestration-9887e1b5eb7a?source=post_page-----7e60575ff39c--------------------------------)
    [## Data Pipeline Orchestration'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/data-pipeline-orchestration-9887e1b5eb7a?source=post_page-----7e60575ff39c--------------------------------)
    [## æ•°æ®ç®¡é“ç¼–æ’'
- en: Data pipeline management done right simplifies deployment and increases the
    availability and accessibility of data forâ€¦
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•°æ®ç®¡é“ç®¡ç†å¾—å½“å¯ä»¥ç®€åŒ–éƒ¨ç½²å¹¶æé«˜æ•°æ®çš„å¯ç”¨æ€§å’Œå¯è®¿é—®æ€§â€¦â€¦
- en: towardsdatascience.com](/data-pipeline-orchestration-9887e1b5eb7a?source=post_page-----7e60575ff39c--------------------------------)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/data-pipeline-orchestration-9887e1b5eb7a?source=post_page-----7e60575ff39c--------------------------------)'
- en: 'To deploy the complete **orchestrator solution** including all required resources
    we can use CloudFormation (infrastructure as code). Consider this shell script
    below that can be run from the command line when we are in the `/stack` folder.
    Make sure <YOUR_S3_BUCKET> exists and replace it with your actual S3 bucket::'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è¦éƒ¨ç½²å®Œæ•´çš„**ç¼–æ’å™¨è§£å†³æ–¹æ¡ˆ**ï¼ŒåŒ…æ‹¬æ‰€æœ‰å¿…è¦çš„èµ„æºï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨CloudFormationï¼ˆåŸºç¡€è®¾æ–½å³ä»£ç ï¼‰ã€‚è€ƒè™‘ä¸‹é¢è¿™ä¸ªå¯ä»¥åœ¨`/stack`æ–‡ä»¶å¤¹ä¸­ä»å‘½ä»¤è¡Œè¿è¡Œçš„è„šæœ¬ã€‚ç¡®ä¿<YOUR_S3_BUCKET>å­˜åœ¨ï¼Œå¹¶å°†å…¶æ›¿æ¢ä¸ºæ‚¨çš„å®é™…S3æ¡¶ï¼š
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It will use stack.yaml to create a CloudFormation stack called BatchETLpipeline.
    It will package our Lambda function, create a package and upload it into S3 bucket.
    If this bucket doesnâ€™t exist it will create it. It will then deploy the pipeline.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå°†ä½¿ç”¨stack.yamlåˆ›å»ºä¸€ä¸ªåä¸ºBatchETLpipelineçš„CloudFormationå †æ ˆã€‚å®ƒå°†æ‰“åŒ…æˆ‘ä»¬çš„Lambdaå‡½æ•°ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å¹¶å°†å…¶ä¸Šä¼ åˆ°S3æ¡¶ä¸­ã€‚å¦‚æœè¯¥æ¡¶ä¸å­˜åœ¨ï¼Œå®ƒå°†åˆ›å»ºå®ƒã€‚ç„¶åå°†éƒ¨ç½²ç®¡é“ã€‚
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If everything goes well the stack for our new data pipeline will be deployed:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¸€åˆ‡é¡ºåˆ©ï¼Œæˆ‘ä»¬çš„æ–°æ•°æ®ç®¡é“çš„å †æ ˆå°†è¢«éƒ¨ç½²ï¼š
- en: '![](../Images/2b97e39b295eadfc8d179a499197fad0.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b97e39b295eadfc8d179a499197fad0.png)'
- en: BatchETLpipeline stack and resources. Image by author.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: BatchETLpipelineå †æ ˆå’Œèµ„æºã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: 'If we click the State Machine resource, then click â€˜Editâ€™ we will see our ETL
    pipeline as a graph:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬ç‚¹å‡»çŠ¶æ€æœºèµ„æºï¼Œç„¶åç‚¹å‡»â€˜ç¼–è¾‘â€™ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°æˆ‘ä»¬çš„ETLç®¡é“ä½œä¸ºå›¾å½¢å±•ç¤ºï¼š
- en: '![](../Images/cb9292383481ab8b5951980275a644a9.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb9292383481ab8b5951980275a644a9.png)'
- en: Workflow studio for Batch data pipeline. Image by author.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰¹é‡æ•°æ®ç®¡é“çš„å·¥ä½œæµå·¥ä½œå®¤ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: Now we can execute the pipeline to run all required data transformation steps.
    Click â€˜Start executionâ€™.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥æ‰§è¡Œç®¡é“ä»¥è¿è¡Œæ‰€æœ‰å¿…è¦çš„æ•°æ®è½¬æ¢æ­¥éª¤ã€‚ç‚¹å‡»â€˜å¼€å§‹æ‰§è¡Œâ€™ã€‚
- en: '![](../Images/3d6fa66e126398bfe0bd267f57bf7059.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d6fa66e126398bfe0bd267f57bf7059.png)'
- en: Successful execution. Image by author.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æˆåŠŸæ‰§è¡Œã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: Now we can connect our Athena tables to our **BI solution**. Connect our final
    Athena dataset `mydataset.user_transactions` to create a dashboard.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†æˆ‘ä»¬çš„Athenaè¡¨è¿æ¥åˆ°æˆ‘ä»¬çš„**BIè§£å†³æ–¹æ¡ˆ**ã€‚è¿æ¥æˆ‘ä»¬æœ€ç»ˆçš„Athenaæ•°æ®é›†`mydataset.user_transactions`ä»¥åˆ›å»ºä»ªè¡¨ç›˜ã€‚
- en: '![](../Images/3427a98727ee0bbf52ef9d94f868f567.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3427a98727ee0bbf52ef9d94f868f567.png)'
- en: Connecting a dataset in Quicksight. Image by author.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è¿æ¥Quicksightä¸­çš„æ•°æ®é›†ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: 'We just need to adjust a couple of settings to make our dashboard look like
    this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åªéœ€è°ƒæ•´å‡ ä¸ªè®¾ç½®ï¼Œä½¿æˆ‘ä»¬çš„ä»ªè¡¨ç›˜çœ‹èµ·æ¥åƒè¿™æ ·ï¼š
- en: '![](../Images/2fc0d03bb10de92d3b88a7f4a7a3b3ed.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2fc0d03bb10de92d3b88a7f4a7a3b3ed.png)'
- en: Quicksight dashboard. Image by author.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Quicksightä»ªè¡¨ç›˜ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: We would want to use `dt` as dimension and `total_cost_usd` as metric. We also
    can set a breakdown dimension for each `user_id`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›ä½¿ç”¨`dt`ä½œä¸ºç»´åº¦ï¼Œ`total_cost_usd`ä½œä¸ºæŒ‡æ ‡ã€‚æˆ‘ä»¬è¿˜å¯ä»¥ä¸ºæ¯ä¸ª`user_id`è®¾ç½®ä¸€ä¸ªæ‹†åˆ†ç»´åº¦ã€‚
- en: Conclusion
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: 'Batch data pipelines are popular because historically workloads were primarily
    batch-oriented in data environments. We have just built an ETL data pipeline to
    extract data from MySQL and transform it in datalake. This pattern works best
    for datasets that arenâ€™t very large and require continuous processing because
    Athena charges according to the volume of data scanned. The method works well
    when converting data into columnar formats like Parquet or ORC, combining several
    tiny files into bigger ones, or bucketing and adding partitions. I previously
    wrote about these big data file formats in one of my stories [6]:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰¹å¤„ç†æ•°æ®ç®¡é“å¾ˆå—æ¬¢è¿ï¼Œå› ä¸ºå†å²ä¸Šå·¥ä½œè´Ÿè½½ä¸»è¦æ˜¯æ‰¹å¤„ç†å‹çš„æ•°æ®ç¯å¢ƒã€‚æˆ‘ä»¬åˆšåˆšå»ºç«‹äº†ä¸€ä¸ª ETL æ•°æ®ç®¡é“ï¼Œä» MySQL ä¸­æå–æ•°æ®å¹¶åœ¨æ•°æ®æ¹–ä¸­è½¬æ¢ã€‚è¯¥æ¨¡å¼æœ€é€‚ç”¨äºæ•°æ®é›†ä¸å¤§ä¸”éœ€è¦æŒç»­å¤„ç†çš„æƒ…å†µï¼Œå› ä¸º
    Athena æ ¹æ®æ‰«æçš„æ•°æ®é‡æ”¶è´¹ã€‚è¿™ç§æ–¹æ³•åœ¨å°†æ•°æ®è½¬æ¢ä¸ºåˆ—å¼æ ¼å¼å¦‚ Parquet æˆ– ORC æ—¶è¡¨ç°è‰¯å¥½ï¼Œç»“åˆå‡ ä¸ªå°æ–‡ä»¶æˆè¾ƒå¤§çš„æ–‡ä»¶ï¼Œæˆ–è¿›è¡Œåˆ†æ¡¶å’Œæ·»åŠ åˆ†åŒºã€‚æˆ‘ä»¥å‰åœ¨æˆ‘çš„ä¸€ä¸ªæ•…äº‹ä¸­å†™è¿‡è¿™äº›å¤§æ•°æ®æ–‡ä»¶æ ¼å¼[6]ã€‚
- en: '[](/big-data-file-formats-explained-275876dc1fc9?source=post_page-----7e60575ff39c--------------------------------)
    [## Big Data File Formats, Explained'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/big-data-file-formats-explained-275876dc1fc9?source=post_page-----7e60575ff39c--------------------------------)
    [## å¤§æ•°æ®æ–‡ä»¶æ ¼å¼è§£æ'
- en: Parquet vs ORC vs AVRO vs JSON. Which one to choose and how to use them?
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Parquet ä¸ ORC ä¸ AVRO ä¸ JSONã€‚è¯¥é€‰æ‹©å“ªä¸€ä¸ªï¼Œå¦‚ä½•ä½¿ç”¨å®ƒä»¬ï¼Ÿ
- en: towardsdatascience.com](/big-data-file-formats-explained-275876dc1fc9?source=post_page-----7e60575ff39c--------------------------------)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/big-data-file-formats-explained-275876dc1fc9?source=post_page-----7e60575ff39c--------------------------------)
- en: We learned how to use Step Functions to orchestrate the data pipeline and visualise
    the data flow from source to final consumer and deploy it using infrastructure
    as code. This setup makes it possible to use CI/CD techniques for our data pipelines
    [7].
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•ä½¿ç”¨ Step Functions æ¥ç¼–æ’æ•°æ®ç®¡é“ï¼Œè§†è§‰åŒ–æ•°æ®æµä»æºå¤´åˆ°æœ€ç»ˆç”¨æˆ·ï¼Œå¹¶ä½¿ç”¨åŸºç¡€è®¾æ–½å³ä»£ç è¿›è¡Œéƒ¨ç½²ã€‚è¿™ä¸ªè®¾ç½®ä½¿å¾—æˆ‘ä»¬å¯ä»¥å¯¹æ•°æ®ç®¡é“åº”ç”¨
    CI/CD æŠ€æœ¯[7]ã€‚
- en: I hope this tutorial was useful for you. Let me know if you have any questions.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›è¿™ä¸ªæ•™ç¨‹å¯¹ä½ æœ‰å¸®åŠ©ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚
- en: Recommended read
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨èé˜…è¯»
- en: '[1] [https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3](/data-pipeline-design-patterns-100afa4b93e3)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3](/data-pipeline-design-patterns-100afa4b93e3)'
- en: '[2] [https://medium.com/towards-data-science/introduction-to-apache-iceberg-tables-a791f1758009](https://medium.com/towards-data-science/introduction-to-apache-iceberg-tables-a791f1758009)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://medium.com/towards-data-science/introduction-to-apache-iceberg-tables-a791f1758009](https://medium.com/towards-data-science/introduction-to-apache-iceberg-tables-a791f1758009)'
- en: '[3] [https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488](https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488](https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488)'
- en: '[4] [https://medium.com/towards-data-science/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a](https://medium.com/towards-data-science/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [https://medium.com/towards-data-science/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a](https://medium.com/towards-data-science/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a)'
- en: '[5] [https://medium.com/towards-data-science/data-pipeline-orchestration-9887e1b5eb7a](https://medium.com/towards-data-science/data-pipeline-orchestration-9887e1b5eb7a)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [https://medium.com/towards-data-science/data-pipeline-orchestration-9887e1b5eb7a](https://medium.com/towards-data-science/data-pipeline-orchestration-9887e1b5eb7a)'
- en: '[6] [https://medium.com/towards-data-science/big-data-file-formats-explained-275876dc1fc9](https://medium.com/towards-data-science/big-data-file-formats-explained-275876dc1fc9)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [https://medium.com/towards-data-science/big-data-file-formats-explained-275876dc1fc9](https://medium.com/towards-data-science/big-data-file-formats-explained-275876dc1fc9)'
- en: '[7] [https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1](https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1](https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1)'
