["```py\nn_gpu = 3\nfor i in range(n_gpu):\n  print(np.arange(30)[i:30:n_gpu])\n```", "```py\ninit_process_group(backend=\"nccl\")\ndevice = int(os.environ[\"LOCAL_RANK\"])\ntorch.cuda.set_device(device)\n```", "```py\nmodel = NeuralNetwork(args.data_size) \nmodel = model.to(device) \n\nif args.distributed:\n  model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[device])\n```", "```py\nsampler = torch.utils.data.DistributedSampler(dataset)\n```", "```py\nACC_STEPS = dist.get_world_size() # == number of GPUs\n# iterate through the data\nfor i, (idxs, row) in enumerate(loader):\n  loss = model(row)  \n  # scale loss according to accumulation steps\n  loss = loss/ACC_STEPS\n  loss.backward()\n  # keep accumualting gradients for ACC_STEPS\n  if ((i + 1) % ACC_STEPS == 0):\n    optimizer.step()  \n    optimizer.zero_grad()\n```", "```py\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\nprint(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset, Sampler\nimport argparse\nimport torch.optim as optim \nimport numpy as np\nimport random\nimport torch.backends.cudnn as cudnn\nimport torch.nn.functional as F\n\nfrom torch.distributed import init_process_group\nimport torch.distributed as dist\n\nclass data_set(Dataset):\n\n    def __init__(self, df):\n        self.df = df\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):    \n\n        sample = self.df[index]\n        return index, sample\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self, dsize):\n        super().__init__()\n        self.linear =  nn.Linear(dsize, 1, bias=False)\n        self.linear.weight.data[:] = 1.\n\n    def forward(self, x):\n        x = self.linear(x)\n        loss = x.sum()\n        return loss\n\nclass DummySampler(Sampler):\n    def __init__(self, data, batch_size, n_gpus=2):\n        self.num_samples = len(data)\n        self.b_size = batch_size\n        self.n_gpus = n_gpus\n\n    def __iter__(self):\n        ids = []\n        for i in range(0, self.num_samples, self.b_size * self.n_gpus):\n            ids.append(np.arange(self.num_samples)[i: i + self.b_size*self.n_gpus :self.n_gpus])\n            ids.append(np.arange(self.num_samples)[i+1: (i+1) + self.b_size*self.n_gpus :self.n_gpus])\n        return iter(np.concatenate(ids))\n\n    def __len__(self):\n        # print ('\\tcalling Sampler:__len__')\n        return self.num_samples\n\ndef main(args=None):\n\n    d_size = args.data_size\n\n    if args.distributed:\n        init_process_group(backend=\"nccl\")\n        device = int(os.environ[\"LOCAL_RANK\"])\n        torch.cuda.set_device(device)\n    else:\n        device = \"cuda:0\"\n\n    # fix the seed for reproducibility\n    seed = args.seed\n\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    cudnn.benchmark = True\n\n    # generate data\n    data = torch.rand(d_size, d_size)\n\n    model = NeuralNetwork(args.data_size)    \n    model = model.to(device)  \n\n    if args.distributed:\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[device])\n\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    dataset = data_set(data)\n\n    if args.distributed:\n        sampler = torch.utils.data.DistributedSampler(dataset, shuffle=False)\n    else:\n        # we define `DummySampler` for exact reproducibility with `DistributedSampler`\n        # which splits the data as described in the article. \n        sampler = DummySampler(dataset, args.batch_size)\n\n    loader = DataLoader(\n                dataset,\n                batch_size=args.batch_size,\n                num_workers=0,\n                pin_memory=True,\n                sampler=sampler,\n                shuffle=False,\n                collate_fn=None,\n            )          \n\n    if not args.distributed:\n        grads = []\n\n    # ACC_STEPS same as GPU as we need to divide the loss by this number\n    # to obtain the same gradient as from multiple GPUs that are \n    # averaged together\n    ACC_STEPS = args.acc_steps \n    optimizer.zero_grad()\n\n    for epoch in range(args.epochs):\n\n        if args.distributed:\n            loader.sampler.set_epoch(epoch)\n\n        for i, (idxs, row) in enumerate(loader):\n\n            if args.distributed:\n                optimizer.zero_grad()\n\n            row = row.to(device, non_blocking=True) \n\n            if args.distributed:\n                rank = dist.get_rank() == 0\n            else:\n                rank = True\n\n            loss = model(row)  \n\n            if args.distributed:\n                # does average gradients automatically thanks to model wrapper into \n                # `DistributedDataParallel`\n                loss.backward()\n            else:\n                # scale loss according to accumulation steps\n                loss = loss/ACC_STEPS\n                loss.backward()\n\n            if i == 0 and rank:\n                print(f\"Epoch {epoch} {100 * '='}\")\n\n            if not args.distributed:\n                if (i + 1) % ACC_STEPS == 0: # only step when we have done ACC_STEPS\n                    # acumulate grads for entire epoch\n                    optimizer.step()  \n                    optimizer.zero_grad()\n            else:\n                optimizer.step() \n\n        if not args.distributed and args.verbose:\n            print(100 * \"=\")\n            print(\"Model weights : \", model.linear.weight)\n            print(100 * \"=\")\n        elif args.distributed and args.verbose and rank:\n            print(100 * \"=\")\n            print(\"Model weights : \", model.module.linear.weight)\n            print(100 * \"=\")\n\nif __name__ == \"__main__\":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--distributed', action='store_true',)\n    parser.add_argument('--seed', default=0, type=int) \n    parser.add_argument('--epochs', default=2, type=int) \n    parser.add_argument('--batch_size', default=4, type=int) \n    parser.add_argument('--data_size', default=16, type=int) \n    parser.add_argument('--acc_steps', default=3, type=int) \n    parser.add_argument('--verbose', action='store_true',)\n\n    args = parser.parse_args()\n\n    print(args)\n\n    main(args)\n```", "```py\n# From Gradient Accumulator\nModel weights :  Parameter containing:\ntensor([[0.9472, 0.9440, 0.9527, 0.9687, 0.9570, 0.9343, 0.9411, 0.9186]],\n       device='cuda:0', requires_grad=True)\n\n# From DDP:\nModel weights :  Parameter containing:\ntensor([[0.9472, 0.9440, 0.9527, 0.9687, 0.9570, 0.9343, 0.9411, 0.9186]],\n       device='cuda:0', requires_grad=True)\n```"]