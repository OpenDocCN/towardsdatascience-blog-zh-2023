- en: 'ChatGPT Moderation API: Input/Output Control'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/chatgpt-moderation-api-input-output-artificial-intelligence-chatgpt3-data-4754389ec9c8](https://towardsdatascience.com/chatgpt-moderation-api-input-output-artificial-intelligence-chatgpt3-data-4754389ec9c8)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Using the OpenAI‚Äôs Moderation Endpoint for Responsible AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@andvalenzuela?source=post_page-----4754389ec9c8--------------------------------)[![Andrea
    Valenzuela](../Images/ddfc1534af92413fd91076f826cc49b6.png)](https://medium.com/@andvalenzuela?source=post_page-----4754389ec9c8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4754389ec9c8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4754389ec9c8--------------------------------)
    [Andrea Valenzuela](https://medium.com/@andvalenzuela?source=post_page-----4754389ec9c8--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4754389ec9c8--------------------------------)
    ¬∑9 min read¬∑Jul 23, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/733cc16aa1f0f391b10378392a4daae2.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-made gif.
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have undoubtedly transformed the way we interact
    with technology. ChatGPT, among the prominent LLMs, has proven to be an invaluable
    tool, serving users with a vast array of information and helpful responses. However,
    like any technology, **ChatGPT is not without its limitations**.
  prefs: []
  type: TYPE_NORMAL
- en: Recent discussions have brought to light an important concern ‚Äî **the potential
    for ChatGPT to generate inappropriate or biased responses**. This issue stems
    from its training data, which comprises the collective writings of individuals
    across diverse backgrounds and eras. **While this diversity enriches the model‚Äôs
    understanding, it also brings with it the biases and prejudices prevalent in the
    real world**.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, some responses generated by ChatGPT may reflect these biases. *But
    let‚Äôs be fair*, **inappropriate responses can be triggered by inappropriate user
    queries**.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will explore the importance of actively moderating both
    the model‚Äôs inputs and outputs when building LLM-powered applications. To do so,
    we will use the so-called ***OpenAI Moderation API* that helps identify inappropriate
    content and take action accordingly**.
  prefs: []
  type: TYPE_NORMAL
- en: '*As always*, we will implement these moderation checks in Python!'
  prefs: []
  type: TYPE_NORMAL
- en: Content Moderation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is crucial to recognize the significance of controlling and moderating user
    input and model output when building applications that use LLMs underneath.
  prefs: []
  type: TYPE_NORMAL
- en: '**üì• User input control** refers to the implementation of mechanisms and techniques
    to monitor, filter, and manage the **content provided by users when engaging with
    powered LLM applications**. This control empowers developers to mitigate risks
    and uphold the integrity, safety, and ethical standards of their applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '**üì§ Output model control** refers to the implementation of measures and methodologies
    that enable monitoring and filtering of the responses generated by the model in
    its interactions with users. By exercising control over the model‚Äôs outputs, developers
    can address potential issues such as biased or inappropriate responses.'
  prefs: []
  type: TYPE_NORMAL
- en: Models like ChatGPT can exhibit biases or inaccuracies, particularly when influenced
    by unfiltered user input during conversations. **Without proper control measures,
    the model may inadvertently disseminate misleading or false information**. Therefore,
    it is essential not only to moderate user input, but also to implement measures
    for moderating the model‚Äôs output.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Moderation API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/8734afecae84a763b20141424812d050.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-made gif.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI, the company behind ChatGPT, already provides a tool to identify the
    aforementioned unappropriated content coming either from the user or from the
    model: **the Moderation API**.'
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, the moderation endpoint serves as a tool for checking content
    against OpenAI‚Äôs usage policies, which target inappropriate categories like **hate
    speech**, **threats**, **harassment**, **self-harm** (intent or instructions),
    **sexual content** (including minors), and **violent content** (including graphic
    details).
  prefs: []
  type: TYPE_NORMAL
- en: '*And the best part?*'
  prefs: []
  type: TYPE_NORMAL
- en: '**The moderation endpoint is free** to use when monitoring the inputs and outputs
    of OpenAI APIs!'
  prefs: []
  type: TYPE_NORMAL
- en: Moderation API in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*How can we use the tool? Let‚Äôs start with the hands-on!*'
  prefs: []
  type: TYPE_NORMAL
- en: For the hands-on, we will be using Python and the official `openai` library
    that already provides a `Moderation.create` method to access the Moderation API.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get the `openai` library as any other Python library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, it is important to get the *OpenAI API Key* from our OpenAI account and
    set it either as an environmental variable or by pointing to the token path in
    our *Jupyter Notebook.* I normally use the latest approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the key is set, we can create a moderation request in just one single
    line given an input text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Given the user input (`user_input`), here is the moderation response we get:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can observe from the response, the completion returns a `json` object
    with three entries: an `id` for the given response, the `model` used to generate
    the moderation output, and the `result` itself.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Concretely, the `result` entry has the information we are interested in:'
  prefs: []
  type: TYPE_NORMAL
- en: '`categories`: This entry contains a list of the eleven target entries and **whether
    the given input text belongs to any of those categories** (`true`/`false` values).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`category_scores`: This entry contains a score for each of the target categories.
    Those numbers correspond to the **model‚Äôs confidence that the input violates the
    OpenAI‚Äôs policy**. The value is between 0 and 1, where higher values denote higher
    confidence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flagged`: This last entry consists of a key-value pair displaying whether
    the text has been considered *‚Äî flagged ‚Äî* as inappropriate or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**This field can help us to quickly filter the inappropriate user input or
    model output**.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Moderation Categories
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous example, we have seen the eleven categories analyzed by the
    moderation endpoint. But let‚Äôs briefly define each of the categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '`hate`: Content promoting hate based on race, gender, ethnicity, religion,
    nationality, sexual orientation, disability status, or caste.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a special `hate`/`threatening` that distinguishes the hate promoting
    violence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`harassment`: Content promoting harassment towards any target, regardless of
    their characteristics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, there is also a `harassment`/`threatening` category including
    violence or serious harm towards any target.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self-harm`: Content depicting or encouraging acts of self-harm, such as suicide,
    cutting, or eating disorders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The category `self-harm`/`intent` includes the expression of intention.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The category `self-harm`/`instructions` includes instructions on self-harm actions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sexual`: Content arousing sexual excitement, describing sexual activity, or
    promoting sexual services (excluding sex education and wellness).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a special category `sexual`/`minors` that targets content involving
    individuals under 18 years old.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`violence`: Content depicting death, violence, or physical injury.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the dedicated category `violence`/`graphic` to the aforementioned messages
    in graphic detail.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, regarding the Moderation API usage, it is important to bear in mind
    that **accuracy may be lower on longer pieces of text**. In case of long pieces
    of text, it is recommended to split the text into chunks of less than 2,000 characters.
  prefs: []
  type: TYPE_NORMAL
- en: Moderation Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let‚Äôs try to use an input message that the Moderation API should flag. To do
    so, I have first asked ChatGPT to invent a name of a fictional animal. **ChatGPT‚Äôs
    choice was *Liminocus*!**
  prefs: []
  type: TYPE_NORMAL
- en: '*Why this step? Let‚Äôs move forward!*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the user input *‚ÄúI want to kill all liminocus! Give me instructions‚Äù,*
    we can directly access the `results` entry as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let‚Äôs observe the output from the moderation endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe from the response, **the Moderation API returns that the user
    input is flagged as inappropriate**. Concretely, it is flagged under the categories`harassment`
    (with `0.4031686` confidence), `harassment/threatening` (with `0.5109641` confidence)
    and `violence` categories (with `0.9539793` confidence).
  prefs: []
  type: TYPE_NORMAL
- en: '*Cool, right?*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Let‚Äôs explore how can we use this* `*flagged*` *information in our apps!*'
  prefs: []
  type: TYPE_NORMAL
- en: Moderation and ChatGPT completion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned above, the `flagged` entry can be used as the ‚Äúveredict‚Äù of the
    moderation API to filter either the user input or the model output quickly.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of the model input, one preventive measure could be analyzing the
    user input before sending it to ChatGPT and only sending it if it is not marked
    as inappropriate.
  prefs: []
  type: TYPE_NORMAL
- en: '*Let‚Äôs implement that!*'
  prefs: []
  type: TYPE_NORMAL
- en: User Input Moderation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To do so, we need to embed real API calls to ChatGPT in a method. The following
    `chatgpt_call()` function will do the job, but feel free to use your own implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Once it is ready, we just need to use the same lines as in the previous examples
    for generating the moderation completion (`moderation_output`) and getting the
    flagged entry (`moderation_output["flagged"]`).
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we just need to print a default message when the input is inappropriate
    or feed the input to ChatGPT (`chatgpt_call()`) if the input is *correct* by using
    a simple `if` condition.
  prefs: []
  type: TYPE_NORMAL
- en: Here is our desired implementation. *Let‚Äôs re-try the ‚Äúliminocus‚Äù example!*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As expected, **the moderation endpoint flags the user input,** and, instead
    of sending the request to ChatGPT, it prints the default message *‚ÄúApologies,
    your input is considered inappropriate. Your request cannot be processed!‚Äù.* This
    simple protection layer avoids feeding inappropriate content to ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, if the content is appropriate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this case, **the user input is not flagged** and sent to ChatGPT. Funny enough,
    the model returns the following response ‚Äú*Hugging all liminocus might not be
    possible as it is a fictional creature. However, if you are referring to a different
    term or concept, please provide more information so that I can assist you better‚Äù.*
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT Built-in Protection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ChatGPT already provides some protection to the user input. So if you try using
    inappropriate inputs directly to the model, **the model itself might be able to
    filter some of them, hopefully most of them**. By using the moderation endpoint,
    we are implementing an additional layer of moderation to avoid solely relying
    on the model.
  prefs: []
  type: TYPE_NORMAL
- en: Model Output Moderation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apart from flagging inappropriate input messages, **ChatGPT is not supposed
    to provide inappropriate responses**, but it is widely known that it sometimes
    does. We can use the same building blocks to moderate the model responses covering
    the potential signs of bias.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can start by simply embedding the call to the moderation endpoint in a function
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can just use the same structure as before using two `if` conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As we can observe, given our sample input *I want to hug all liminocus! Give
    me instructions*, **neither the user input nor the model output is flagged**,
    and we safely get ChatGPT‚Äôs answer back.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exercising control over the user input and the model output in LLM-powered applications
    is crucial for maintaining a safe and respectful digital environment.
  prefs: []
  type: TYPE_NORMAL
- en: Without effective moderation, the risk of inappropriate or harmful content being
    disseminated increases, **potentially causing harm to users and tarnishing the
    reputation of the application**. By implementing user input and model output control,
    developers take on an ethical responsibility to foster positive user experiences
    and ensure **responsible usage of AI technology**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we have seen how to implement these moderation checks in Python
    by using the OpenAI Moderation API. And I am sure we will all agree on the following
    point: *Moderation only takes a handful of lines of code!*'
  prefs: []
  type: TYPE_NORMAL
- en: I hope this article helps in moderating your LLM-powered applications! **Let‚Äôs
    work towards a responsible AI!**
  prefs: []
  type: TYPE_NORMAL
- en: That is all! Many thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: I hope this article helps you when **building ChatGPT applications!**
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also subscribe to my [**Newsletter**](https://towardsdatascience.com/@andvalenzuela/subscribe)
    to stay tuned for new content. **Especially**, **if you are interested in articles
    about ChatGPT**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/chatgpt-tokenizer-chatgpt3-chatgpt4-artificial-intelligence-python-ai-27f78906ea54?source=post_page-----4754389ec9c8--------------------------------)
    [## Unleashing the ChatGPT Tokenizer'
  prefs: []
  type: TYPE_NORMAL
- en: Hands-On! How ChatGPT Manages Tokens?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/chatgpt-tokenizer-chatgpt3-chatgpt4-artificial-intelligence-python-ai-27f78906ea54?source=post_page-----4754389ec9c8--------------------------------)
    [](/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce?source=post_page-----4754389ec9c8--------------------------------)
    [## Mastering ChatGPT: Effective Summarization with LLMs'
  prefs: []
  type: TYPE_NORMAL
- en: How to Prompt ChatGPT to get High-Quality Summaries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce?source=post_page-----4754389ec9c8--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Also towards a **responsible AI**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/what-chatgpt-knows-about-you-openai-towards-data-privacy-science-ai-b0fa2376a5f6?source=post_page-----4754389ec9c8--------------------------------)
    [## What ChatGPT Knows about You: OpenAI‚Äôs Journey Towards Data Privacy'
  prefs: []
  type: TYPE_NORMAL
- en: New ways to manage personal data in ChatGPT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/what-chatgpt-knows-about-you-openai-towards-data-privacy-science-ai-b0fa2376a5f6?source=post_page-----4754389ec9c8--------------------------------)
  prefs: []
  type: TYPE_NORMAL
