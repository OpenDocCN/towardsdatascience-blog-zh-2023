["```py\nimport requests\nsession = requests.Session()\n\nurl=\"https://api.nasa.gov/neo/rest/v1/feed\"\napiKey=\"your_api_key\"\nrequestParams = {\n    'api_key': apiKey,\n    'start_date': '2023-04-20',\n    'end_date': '2023-04-21'\n}\nresponse = session.get(url, params = requestParams, stream=True)\nprint(response.status_code)\n```", "```py\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\n...\n# Authenticate BigQuery client:\nservice_acount_str = config.get('BigQuery') # Use config\ncredentials = service_account.Credentials.from_service_account_info(service_acount_str)\nclient = bigquery.Client(credentials=credentials, project=credentials.project_id)\n\n...\ndef load_table_from_dataframe(table_schema, table_name, dataset_id):\n    #! source data file format must be outer array JSON:\n    \"\"\"\n    [\n    {\"id\":\"1\"},\n    {\"id\":\"2\"}\n    ]\n    \"\"\"\n    blob = \"\"\"\n            [\n    {\"id\":\"1\",\"first_name\":\"John\",\"last_name\":\"Doe\",\"dob\":\"1968-01-22\",\"addresses\":[{\"status\":\"current\",\"address\":\"123 First Avenue\",\"city\":\"Seattle\",\"state\":\"WA\",\"zip\":\"11111\",\"numberOfYears\":\"1\"},{\"status\":\"previous\",\"address\":\"456 Main Street\",\"city\":\"Portland\",\"state\":\"OR\",\"zip\":\"22222\",\"numberOfYears\":\"5\"}]},\n    {\"id\":\"2\",\"first_name\":\"John\",\"last_name\":\"Doe\",\"dob\":\"1968-01-22\",\"addresses\":[{\"status\":\"current\",\"address\":\"123 First Avenue\",\"city\":\"Seattle\",\"state\":\"WA\",\"zip\":\"11111\",\"numberOfYears\":\"1\"},{\"status\":\"previous\",\"address\":\"456 Main Street\",\"city\":\"Portland\",\"state\":\"OR\",\"zip\":\"22222\",\"numberOfYears\":\"5\"}]}\n    ]\n    \"\"\"\n    body = json.loads(blob)\n    print(pandas.__version__)\n\n    table_id = client.dataset(dataset_id).table(table_name)\n    job_config = bigquery.LoadJobConfig()\n    schema = create_schema_from_yaml(table_schema) \n    job_config.schema = schema\n\n    df = pandas.DataFrame(\n    body,\n    # In the loaded table, the column order reflects the order of the\n    # columns in the DataFrame.\n    columns=[\"id\", \"first_name\",\"last_name\",\"dob\",\"addresses\"],\n\n    )\n    df['addresses'] = df.addresses.astype(str)\n    df = df[['id','first_name','last_name','dob','addresses']]\n\n    print(df)\n\n    load_job = client.load_table_from_dataframe(\n        df,\n        table_id,\n        job_config=job_config,\n    )\n\n    load_job.result()\n    print(\"Job finished.\")\n```", "```py\n\"\"\"DAG definition for recommendation_bespoke model training.\"\"\"\n\nimport airflow\nfrom airflow import DAG\nfrom airflow.contrib.operators.bigquery_operator import BigQueryOperator\nfrom airflow.contrib.operators.bigquery_to_gcs import BigQueryToCloudStorageOperator\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.operators.app_engine_admin_plugin import AppEngineVersionOperator\nfrom airflow.operators.ml_engine_plugin import MLEngineTrainingOperator\n\nimport datetime\n\ndef _get_project_id():\n  \"\"\"Get project ID from default GCP connection.\"\"\"\n\n  extras = BaseHook.get_connection('google_cloud_default').extra_dejson\n  key = 'extra__google_cloud_platform__project'\n  if key in extras:\n    project_id = extras[key]\n  else:\n    raise ('Must configure project_id in google_cloud_default '\n           'connection from Airflow Console')\n  return project_id\n\nPROJECT_ID = _get_project_id()\n\n# Data set constants, used in BigQuery tasks.  You can change these\n# to conform to your data.\nDATASET = 'staging' #'analytics'\nTABLE_NAME = 'recommendation_bespoke'\n\n# GCS bucket names and region, can also be changed.\nBUCKET = 'gs://rec_wals_eu'\nREGION = 'us-central1' #'europe-west2' #'us-east1'\nJOB_DIR = BUCKET + '/jobs'\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': airflow.utils.dates.days_ago(2),\n    'email': ['mike.shakhomirov@gmail.com'],\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 5,\n    'retry_delay': datetime.timedelta(minutes=5)\n}\n\n# Default schedule interval using cronjob syntax - can be customized here\n# or in the Airflow console.\nschedule_interval = '00 21 * * *'\n\ndag = DAG('recommendations_training_v6', default_args=default_args,\n          schedule_interval=schedule_interval)\n\ndag.doc_md = __doc__\n\n#\n#\n# Task Definition\n#\n#\n\n# BigQuery training data export to GCS\n\ntraining_file = BUCKET + '/data/recommendations_small.csv' # just a few records for staging\n\nt1 = BigQueryToCloudStorageOperator(\n    task_id='bq_export_op',\n    source_project_dataset_table='%s.recommendation_bespoke' % DATASET,\n    destination_cloud_storage_uris=[training_file],\n    export_format='CSV',\n    dag=dag\n)\n\n# ML Engine training job\ntraining_file = BUCKET + '/data/recommendations_small.csv'\njob_id = 'recserve_{0}'.format(datetime.datetime.now().strftime('%Y%m%d%H%M'))\njob_dir = BUCKET + '/jobs/' + job_id\noutput_dir = BUCKET\ndelimiter=','\ndata_type='user_groups'\nmaster_image_uri='gcr.io/my-project/recommendation_bespoke_container:tf_rec_latest'\n\ntraining_args = ['--job-dir', job_dir,\n                 '--train-file', training_file,\n                 '--output-dir', output_dir,\n                 '--data-type', data_type]\n\nmaster_config = {\"imageUri\": master_image_uri,}\n\nt3 = MLEngineTrainingOperator(\n    task_id='ml_engine_training_op',\n    project_id=PROJECT_ID,\n    job_id=job_id,\n    training_args=training_args,\n    region=REGION,\n    scale_tier='CUSTOM',\n    master_type='complex_model_m_gpu',\n    master_config=master_config,\n    dag=dag\n)\n\nt3.set_upstream(t1)\n```"]