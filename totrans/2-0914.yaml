- en: Forecast Multiple Time Series Like a Master
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://towardsdatascience.com/forecast-multiple-time-series-like-a-master-1579a2b6f18d](https://towardsdatascience.com/forecast-multiple-time-series-like-a-master-1579a2b6f18d)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From local to global algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@zukaschikume?source=post_page-----1579a2b6f18d--------------------------------)[![Bartosz
    SzabÅ‚owski](../Images/2115d18d8d0463a6c680b0461472c203.png)](https://medium.com/@zukaschikume?source=post_page-----1579a2b6f18d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1579a2b6f18d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1579a2b6f18d--------------------------------)
    [Bartosz SzabÅ‚owski](https://medium.com/@zukaschikume?source=post_page-----1579a2b6f18d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1579a2b6f18d--------------------------------)
    Â·30 min readÂ·Apr 26, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3393fc3c8c56dbacebe783c486b2d2e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [JesÃºs Rocha](https://unsplash.com/@jjrocha?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: I deal with forecasting multiple time series in business (to be precise, demand
    forecasting). In my previous article [*Sell Out Sell In Forecasting*](https://medium.com/towards-data-science/sell-out-sell-in-forecasting-45637005d6ee)
    I presented the methodology that I implemented at NestlÃ© for demand forecasting.
    In this article, I would like to introduce you to the universal (which doesnâ€™t
    mean ideal) algorithms currently in use for forecasting multiple time series â€”
    such as **state of the art** for time series. For a retailer or manufacturer,
    forecasting demand is key to business. It allows them to create more accurate
    production plans and optimize their inventory. Unfortunately, many companies (not
    NestlÃ© :) ) donâ€™t see this problem and they use spreadsheets with simple statistics.
    If they would change this they could significantly reduce their costs. After all,
    warehousing and out-of-date products â€” thatâ€™s an additional cost.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b8f08996d06912402198087dc3b5e19.png)'
  prefs: []
  type: TYPE_IMG
- en: How to forecast multiple time series, Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Itâ€™s hard to find someone working in Data Science who isnâ€™t familiar with [**Scikit-learn**](https://scikit-learn.org/).
    For dataframes, you can use **Scikit-learn** to do most of the elements involved
    in machine learning â€” from preprocessing to hyperparameters selection, evaluation,
    and model prediction. We can assign Linear Regression, Decision Tree, or Support
    Vector Machine (SVM) to a variable *model* and use the same methods each time,
    like *fit* and *predict*. We have a lot of flexibility, but we also have an easy
    way of implementing solutions.
  prefs: []
  type: TYPE_NORMAL
- en: For time series the situation is different. If you are experimenting and want
    to compare different algorithms, the algorithms themselves are not only a problem.
  prefs: []
  type: TYPE_NORMAL
- en: If you are starting to work with time series, you need to process them, for
    resampling or filling in missing values â€” [**Pandas**](https://pandas.pydata.org/)
    is useful for that.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to do a decomposition, visualize ACF/PACF, or check a stationarity
    test, then the [**Statsmodels**](https://www.statsmodels.org/) library is useful.
  prefs: []
  type: TYPE_NORMAL
- en: For visualization, you will probably use [**Matplotlib**](https://matplotlib.org/),
    even if not this library, there are many built on it.
  prefs: []
  type: TYPE_NORMAL
- en: The fun begins when you want to use different algorithms. When you want to use
    **ARIMA** then you will probably use [**pmdarima**](https://alkaline-ml.com/pmdarima),
    [**Prophet**](https://facebook.github.io/prophet/) is another library. Typical
    **M**achine **L**earning algorithms can be found in the previously mentioned **Scikit-learn**,
    but you may also want to use boosting models like [**LightGBM**](https://lightgbm.readthedocs.io/)
    or [**CatBoost**](https://catboost.ai/). For **D**eep **N**eural **N**etworks
    and architectures from the most recent papers, [**PyTorch Forecasting**](https://pytorch-forecasting.readthedocs.io/)
    is worth using.
  prefs: []
  type: TYPE_NORMAL
- en: WOWðŸ¤¯ A lot of libraries you probably need. If you want to be able to use the
    previously mentioned libraries it will be a lot of work, because most of them
    use different APIs, data types, and for each of the libraries with models you
    will have to prepare your own functions for backtesting and selection of hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd03ac32b8e2a4f05d9177ceaf14df14.png)'
  prefs: []
  type: TYPE_IMG
- en: Library Darts, Image by Author, Inspired by library documentation
  prefs: []
  type: TYPE_NORMAL
- en: Here helps us [**Darts**](https://unit8co.github.io/darts/), which tries to
    be a **Scikit-learn** for time series, and its purpose is precise to simplify
    working with time series. Often its functionality is based on other libraries,
    for example, it uses **Statsmodels** for decomposition. It also makes **Darts**
    work well with other libraries if something isnâ€™t implemented there, you can compare
    it to how you can mutually use **Matplotlib** to work with **Seaborn**.
  prefs: []
  type: TYPE_NORMAL
- en: Where it is necessary they have their own implementations, but they donâ€™t want
    to reinvent the wheel and use what is already there in other popular libraries
    for time series.
  prefs: []
  type: TYPE_NORMAL
- en: This idea is not new for time series, and there are other good libraries, such
    as [**sktime**](https://www.sktime.org/), [**GluonTS**](https://ts.gluon.ai/),or
    [**nixtla**](https://www.nixtla.io/), but in my opinion, **Darts** has the lowest
    entry threshold and is more complete. This is not an advertisement for this library,
    at the end of the day your forecast should bring value to the business you work
    for. You might as well write each of these models in code from scratch. I am going
    to use Darts in the below examples, but you will also be able to find these models
    (all or part of them) in the libraries mentioned above. I see space for improvement
    for the Darts library in **optimizing computation** if we want to train multiple
    local models ~ this tries **nixtla** library, which offers compatibility with
    Spark, Dask, and Ray.
  prefs: []
  type: TYPE_NORMAL
- en: 'From my perspective, Darts is already a mature library and it is still being
    developed, just look at the [**changelog**](https://github.com/unit8co/darts/blob/master/CHANGELOG.md).
    Now you can install it in the standard way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once we have the library installed in our environment then we can import it
    and use it in practice.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Single vs Multiple time series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/83b03160c0fff2b64f2e1b076f5efd04.png)'
  prefs: []
  type: TYPE_IMG
- en: One vs Multiple time series, Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Above, based on the [**Walmart dataset**](https://www.kaggle.com/datasets/yasserh/walmart-dataset),
    you can see **single** and **multiple** time series. Nowadays, many problems involve
    multiple points at the same time. This data can come from a variety of processes,
    it can be this example and my daily work which is demand forecasting, but it can
    also be energy consumption forecasting, the closing price of a company on the
    stock market, the number of bicycles rented from a station and many many others
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the time series itself, we may also have other *variables* for
    them, some of which may be known for the future and others only available for
    the past â€” about that in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I want to show you different approaches for forecasting multiple
    time series, but I want it to be practical â€” so that you are not left with just
    theory. So letâ€™s import all the libraries used later â€” darts and others well known
    by Data Scientists.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now letâ€™s load the dataset, which is about **demand forecasting** and comes
    from the [**Kaggle**](https://www.kaggle.com/competitions/demand-forecasting-kernels-only).
    If you agree with the terms of the competition on Kaggle then you can also download
    this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: There are 10 stores and in each of them are 50 items making a total of 500 time
    series.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s take a look at the 10 among the least, median, and most sold in total
    store-item combinations. To find what relations are in a time series it is often
    enough to look at it because this already tells us a lot, like a trend or seasonality,
    but often much more.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0c022ede1dbd871b535d8233bd4c8be8.png)'
  prefs: []
  type: TYPE_IMG
- en: There is a lot of similarity between these time series. Weâ€™ll check for the
    existence of seasonality (day of the week or week/month of the year) or trend
    in a moment, but you can already intuitively imagine that model learning relationships
    from all time series will be able to forecast one single time series better than
    if it only learned from its history.
  prefs: []
  type: TYPE_NORMAL
- en: I create a copy of my dataset for **EDA** (exploratory data analysis). Then
    I scale the time series using **MinMAXScaler** and then all the time series will
    be comparable to each other. Finally, I create Box Plots with which I check if
    there is a trend and seasonality.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/389fc7b02f361ab79d5cac60527593cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Yes, there is a trend and two types of seasonality. If you think that these
    time series are easy to forecast â€”you are right. The purpose of this article is
    to show you the most popular approaches to forecasting multiple time series. The
    data is not always as easy as, for example, stock market indexes, but thatâ€™s a
    topic for another article.
  prefs: []
  type: TYPE_NORMAL
- en: I think this exploration is enough to understand what relationships the model
    should learn.
  prefs: []
  type: TYPE_NORMAL
- en: Here we have **multiple time series** (sales of multiple items in multiple stores).
    For each **time series**, letâ€™s create a **TimeSeries** object from a Pandas DataFrame.
    This type is required by models from the Darts library. Then save these time series
    in a **list**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Working with multiple time series can be helpful, but often it is problematic.
    When we have one time series then we have a lot of time to work with it. Look
    at it, verify the trend and seasonality, and transform anomalies. We can polish
    our Forecast. For multiple series this approach becomes impossible. We want the
    approach to be as automatic as possible, but then we may miss details, such as
    anomalies, or perhaps we should not process each time series in the same way.
    There may be more typical problems: missing data, data drift, and rare events
    (black swans). More series can potentially help us because our model will be able
    to use more data, and therefore there will be more representative observations
    for a particular pattern. Using my work as an example â€” to forecast the demand
    caused by a promotion for product X in the following week, our model can also
    use the historical effects of such a promotion from other promotions.'
  prefs: []
  type: TYPE_NORMAL
- en: Hence the question â€” how to forecast multiple time series?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You may often ask yourself *Do I have a* ***multiple*** *or* ***multivariate***
    *time series?*. These questions are legitimate and the answer is not always clear.
    When your time series are from a **single process**, are **interconnected**, **correlated,**
    and **interact** with each other then the answer will be *my time series are*
    ***multivariate***.
  prefs: []
  type: TYPE_NORMAL
- en: When you forecast sales of product X in **multiple stores** then you have a
    **multiple time series**, but when you have an additional product Y then for a
    single store you have a **multivariate time series**, because sales of one product
    in a store can **affect** sales of **another**, or it can affect â€” it was only
    an assumption.
  prefs: []
  type: TYPE_NORMAL
- en: How to evaluate the forecast?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we get into the different approaches and models, letâ€™s discuss first
    how to measure the quality of a forecast. This is a regression problem, so we
    will still compare forecasts to true values, here there is no doubt. You can use
    metrics familiar to the regression problem, such as **RMSE**(*Root Mean Squared
    Error*), **MSE**(*Mean Squared Error*), **MAE**(*Mean Absolute Error*), or more
    typical time series metrics such as **MAPE**(*Mean Absolute Percentage Error*),
    **MARRE**(*Mean Absolute Ranged Relative Error*) or **MASE**(*Mean Absolute Scaled
    Error*). Further discussion will use **MAPE** as an evaluation metric. This article
    is not about demand forecasting, but because of the data and my experience, there
    are many references to it. So what metric to choose for that problem you can think.
    Always the metric should reflect the business objective. In this [*article*](/forecast-kpi-rmse-mae-mape-bias-cdc5703d242d),
    [Nicolas Vandeput](https://www.linkedin.com/in/vandeputnicolas/) described the
    metrics used for demand forecasting as KPIs.
  prefs: []
  type: TYPE_NORMAL
- en: We can extend this approach to multiple series and then calculate our metrics
    for all series at once or for each series separately and then aggregate them.
    So letâ€™s move on to how to evaluate a single series. Then this can be extended
    to multiple.
  prefs: []
  type: TYPE_NORMAL
- en: Yes, this is a regression problem, you may wonder why I explain it. In time
    series, time plays a key role. The data are sorted relative to time, and the observations
    are relative to each other. Therefore, it is not possible to use randomization
    when dividing the training/test set and use cross-validation, because in this
    situation there would be data leakage.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6eb56fa82acdc8a681e06625ce075fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluation of the forecasting model, Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: First, divide the data into a **train** set and a **test** set, easy right?
    The model fits on the train set and tests on the test set. We can divide this
    proportionally, for example, the test set includes 20% of the last weeks, or indicate
    from which date the test set is to be â€” perhaps due to business considerations
    it is important that the test set is the last year.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, the test set will be the last year.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: From the **train** set, we separate **validation** subsets, which we use to
    select **hyperparameters**. For models implemented in Darts, we can use the *gridsearch*
    method, but for models based on neural networks, the [**Optuna**](https://optuna.org/)
    is recommended. The gridsearch method actually has everything we need and works
    the same regardless of which model we choose.
  prefs: []
  type: TYPE_NORMAL
- en: 'Whatâ€™s important:'
  prefs: []
  type: TYPE_NORMAL
- en: parameters âžœ a dictionary with hyperparameters to check
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: series âžœ TimeSeries object or a list containing TimeSeries objects (if the algorithm
    is global)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: start âžœ *Pandas Timestamp* defines when the first forecast will occur or *float*
    as the proportion of observations before the first forecast.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: forecast_horizon âžœ number(*int*) of forecast horizons by the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: stride âžœ Offset between next predictions. To make everything in accordance with
    the art of Data Science and most reflect the actual operation of the algorithm
    then stride should be equal to 1\. But remember, after each step your algorithm
    is retrained and your grid of hyperparameters may have a lot of combinations,
    it often takes ages. Therefore, for common sense reasons, a stride can be higher
    than 1, especially since the purpose of this is to select the best hyperparameters
    and the result may (though not necessarily) be the same for a stride equal to
    1 as for a stride equal to 5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: metric âžœ function, which compares true values and predictions. The best hyperparameters
    are then selected based on this metric.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: So how to evaluate the model on the test set? We have 2 approaches.
  prefs: []
  type: TYPE_NORMAL
- en: The **first** is that we fit our model on the train set and make one forecast
    that covers the test set. In the following examples, we will use this option â€”
    it is simply faster to calculate since we generate one forecast for each time
    series.
  prefs: []
  type: TYPE_NORMAL
- en: The **second** approach is that we test different forecast horizons. We fit
    the model, make a forecast, retrain, make a forecast, and so on, but we have to
    start training the algorithm first before the end of the train set, and thatâ€™s
    so that we can compare horizons on the same data â€” weâ€™re comparing apples to apples.
    If you were to start by training on the entire train set then the further horizons
    have fewer observations. In this method, we should retrain (especially in local
    models), so we definitely have more calculations in this approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Itâ€™s also simple because the models in darts have the ability to return historical
    predictions according to the above visualization. You have already learned about
    some of the variables in the gridsearch method, which work the same here. However,
    there will be new variables here:'
  prefs: []
  type: TYPE_NORMAL
- en: retrain âžœ if equal to *True*, then after each step the model is retrained, which
    best reflects reality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: overlap_end âžœ if equal to *True*, then the predictions may extend beyond the
    dates in the test set. Useful if, we are doing a forecast for several horizons
    and the farther ones are beyond the test set and the closer ones are not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: last_points_only âžœ if equal to *False*, then the forecast for all horizons is
    returned.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: However, we want to take forecasts from the interested us horizon, and in the
    variable backtests_results, there are forecasts from different points in time.
    To take forecasts from the specific horizon then you can use my function *take_backtest_horizon*.
  prefs: []
  type: TYPE_NORMAL
- en: backtests_5W âžœ there are forecasts for each point of the test set that were
    made 5 weeks earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Variables used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In fact, the time series themselves are not fully explainable by themselves.
    Often, time series are dependent on other variables. Here we donâ€™t have them,
    but good to know how to distinguish them when they will appear in your projects.
    If we do not inform the model about an upcoming promotion/lower price then it
    will not be able to forecast increased sales. If you want to forecast the change
    in the price of a stock company, helpful can be added variables from technical
    or fundamental analysis. These variables are dependent on the price, that is,
    you can only know these indicators for the past, after all, we do not know the
    companyâ€™s financial reports in the future or its price â€” we want to forecast it
    :)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fdfdefebca39dab3d6d4fc34c837df1a.png)'
  prefs: []
  type: TYPE_IMG
- en: Variables used by algorithms for time series, Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: We can have variables that are also time series, that is, for different points
    in time they have different values, and we can have **static covariates** (constant
    over time), usually categorical variables. In our example that will be store ID
    and item ID. They are very relevant to global models. Because in 1 out of 100
    time series, you may have different relationships, and thanks to this variable
    your model can distinguish between time series.
  prefs: []
  type: TYPE_NORMAL
- en: As for variables that are time series, we can distinguish between **covariates**
    that are also **known for the future** (for example we can know what the promotional
    mechanisms are in the future and also we know them for the past) and **covariates
    known only for the past** (we can know what price competing products had, but
    we do not know what they will have in the future).
  prefs: []
  type: TYPE_NORMAL
- en: Models used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/dd185e633c9b5ffbdd0e2c15162d7077.png)'
  prefs: []
  type: TYPE_IMG
- en: Local vs Global algorithms, Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: We can separate machine learning into supervised learning, unsupervised learning,
    and reinforcement learning. When we get into the details, we can divide supervised
    learning into regression and classification. We can make a similar split for time
    series forecasting, which is using **local** or **global** algorithms to forecast
    these time series.
  prefs: []
  type: TYPE_NORMAL
- en: The **local algorithm** is fitted on a **single time series**, and only this
    time series can the model predict. More time series mean more models. Here we
    see pros and cons, simple models, but for many time series, this approach becomes
    difficult to maintain.
  prefs: []
  type: TYPE_NORMAL
- en: The **global algorithm**, on the other hand, is that **one model** can be fitted
    on **multiple time series**. So if we have **multiple time series** then we can
    have **one** model that can forecast them all. This approach is definitely more
    flexible, for example, you could use Transfer Learning. For time series it means
    that you fit the model on a different time series than you are making the prediction.
    [Here](https://unit8co.github.io/darts/examples/14-transfer-learning.html) is
    an example of this usage. One more important point related to global models, because
    I could forget it later and it is really important â€” that is, **time series scaling**.
    The most common approach is *MinMaxScaler*, but you can use something more specific
    for your data. Nevertheless, Iâ€™m not going to write here how you can scale time
    series, thatâ€™s definitely a topic for another article. Letâ€™s consider why we should
    scale our time series. The answer may be simple, many global algorithms are neural
    networks and it is a reason why we scale data, as we do with pixels for convolutional
    neural networks. However, this is not the full picture. However, we can use a
    model like Random Forest (nonparametric models) and we still should scale them.
    But STOP, why? After all, for these types of models, you donâ€™t need to scale the
    variables. The reason we should scale the time series is so that the model learns
    the relationship, not the scale, for example, for seasonality such a relationship
    might be that in the summer months, the value is on average 150% higher than in
    the winter months. Another example would be that after 3 significant increases
    are followed by a decrease. It is difficult to learn these relationships by the
    model if we do not scale time series. This is a slightly different approach to
    scaling a variable for tabular data because here we scale each time series separately.
    If we use the mentioned *MinMaxScaler* then for each time series from the train
    set the maximum value is 1\. So letâ€™s scale our data, which will be used by global
    models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In a moment you will read about the most popular **local** and **global** algorithms.
    It is impossible to describe all possible algorithms, but there are a few that
    are often used by specialists and usually meet expectations.
  prefs: []
  type: TYPE_NORMAL
- en: No free lunch theorem
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There is no answer â€” this model is the best, do not use others. Nevertheless,
    if you are creating an MVP â€” it is best to start with something simple.
  prefs: []
  type: TYPE_NORMAL
- en: In the following examples, I do not choose the best hyperparameters on validation
    sets, I use default models. So if you write to me that the model may have better
    results â€” **I already agree with you**.
  prefs: []
  type: TYPE_NORMAL
- en: Local models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we move on to specific local models, I have prepared functions for you
    to use multiprocessing for faster calculations using all Cores.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: I am going to use this function for local models to generate a single prediction
    for the test set. Also, you can use it to generate multiple historical forecasts
    (the second approach, variable ***single_forecast*** should be ***False***). Nevertheless,
    I do not do that here, because it would take a lot of time.
  prefs: []
  type: TYPE_NORMAL
- en: If you use Cluster and Spark then you can use Spark UDF and speed up the calculations
    significantly.
  prefs: []
  type: TYPE_NORMAL
- en: I know, you probably want to move to the models part. Last but not least â€” a
    function to evaluate our forecasts. I am going to use **MAPE** as a metric for
    evaluation, however, if you work on a demand forecasting project then WMAPE or
    MAE is definitely closer to business expectations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Baseline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Well, okay, but why make a neural network if a better idea is to forecast the
    value from a year ago? Thatâ€™s exactly why we create such a model first. When you
    work with real data, youâ€™d better start with such an approach too (it can also
    be the last value from the training set, NaiveDrift if thereâ€™s a trend or a combination
    of several simple methods). Then if you move on to more advanced methods you can
    evaluate how much better it is than the simpler ones, because if, for example,
    you start with a neural network and its MAPE is 10% then my question (probably
    also stakeholders) is â€” *Is it good?*
  prefs: []
  type: TYPE_NORMAL
- en: Our models(one ts=one model) below will repeat the value from a year ago.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**overall MAPE: 22.42%**'
  prefs: []
  type: TYPE_NORMAL
- en: Now letâ€™s visualize the forecast and true value on a test set for time series
    with the highest total sales.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c37de66f2d835d976d5c5f681f16803.png)'
  prefs: []
  type: TYPE_IMG
- en: It doesnâ€™t look too bad.
  prefs: []
  type: TYPE_NORMAL
- en: ARIMA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/3c037dbaa86cd65df7dd14078eb1c315.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualize how the ARIMA algorithm works, Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**ARIMA** is a statistical model, both popular and powerful in its simplicity.
    When you hear **ARIMA** it can mean this one model but also a collection of models
    that are extensions of just **ARIMA**. For this collection, we can include **ARIMAX**
    (takes into account additional variables), **SARIMA** (takes into account seasonality),
    or **VARIMA** (for multivariate time series). But letâ€™s go back to **ARIMA** (**A**uto**R**egressive
    **I**ntegrated **M**oving **A**verage), which is where it all starts. If you understand
    it well then you should have no problem using the mentioned before models.'
  prefs: []
  type: TYPE_NORMAL
- en: Many articles have already been written about this algorithm. I would like to
    give you the intuition behind this model. I hope after all you will easily implement
    it in code and understand how it works. I am going to start from the end. I myself
    remember having a question about this during recruitment and at the time I didnâ€™t
    understand it yet. The **ARMA model can only work with stationary time series**,
    so we have a component â€” **Integration** (**I**), which often (not always) **transforms
    a non-stationary into a stationary time series**. **ARMA** is the model, while
    **I** part is responsible for preparing the data for modeling, if necessary of
    course. There are a few questions you should know the answer to them, what it
    means that a time series is stationary or not, and what kind of transformation
    the **Integration** (**I**) component makes. So letâ€™s start with stationarity.
  prefs: []
  type: TYPE_NORMAL
- en: If the distribution of values (mean and variance) is invariant over time then
    the time series is stationary.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Therefore, if there is a trend and/or seasonality then a time series is non-stationary.
    To check whether your time series is stationary then the simplest method is to
    visualize it and on the graph also add the moving average and moving standard
    deviation. If they are constant over time (or close to constant) then you can
    conclude that your time series is stationary. This approach may seem naive and
    doesnâ€™t always work, because given too large a window into the rolling statistics
    you may think your time series is stationary, when in fact it is not. Another
    way is to split your time series into random partitions, for each partition calculate
    the mentioned statistics. The last method is to calculate the **A**ugmented **D**ickey-**F**uller
    (**ADF**) test. What if our time series is not stationary and we need to use the
    **Integration** (**I**) component? It makes the time series stationary (but not
    always, there are time series that canâ€™t be stationary) using **differencing**,
    that is, calculating the differences between observations. What if our time series
    isnâ€™t still stationary? We can select order **d**, which means how many times
    we are differencing time series.
  prefs: []
  type: TYPE_NORMAL
- en: This long fragment was about **Integration** (**I**) which prepares data that
    will be used by the **Autoregressive** (**AR**) and **Moving Averag**e (**MA**)
    components. **AR** is a linear regression of the last **p** values, known as lags.
    The current value is correlated with and dependent on the last values. **MA**
    is complementary and takes into account the **q** last errors in the forecast
    (assumed to be white noise) to better forecast the current point in time.
  prefs: []
  type: TYPE_NORMAL
- en: To select the order **p** for **AR** we use [**PACF**](https://en.wikipedia.org/wiki/Partial_autocorrelation_function)
    (**P**artial **A**uto**C**orrelation **F**unction), while to select the order
    **q** for **MA** we use [**ACF**](https://en.wikipedia.org/wiki/Autocorrelation)
    (**A**uto**C**orrelation **F**unction). Outside of university classes, we are
    unlikely to do this in practice because we have [**AutoARIMA**](https://unit8co.github.io/darts/generated_api/darts.models.forecasting.auto_arima.html)
    which selects the **p**, **d**, and **q** for us.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s go back to practice and implement in a similar way as we did with the
    Baseline model. As you might have read earlier thanks to the Darts library it
    is simple.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**overall MAPE: 28.18%**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/31546af743b6b1fa56efe95d9cdccb78.png)'
  prefs: []
  type: TYPE_IMG
- en: It would probably look better if I chose the parameter *m* (The period for seasonal
    differencing). You can try it and give me feedback about how the results changed.
  prefs: []
  type: TYPE_NORMAL
- en: Exponential smoothing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/8cd796b93178f570b18ddeb944b0474a.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualize how the Exponential Smoothing (ETS) algorithm works, Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Exponential smoothing** is another **family** of similar models for univariate
    time series. You can find â€˜this familyâ€™ under the term **ETS** (**E**-Error, **T**-Trend,
    **S**-Seasonal). In this method, observations are weighted, for older observations
    are lower weights, because they decrease exponentially. We can distinguish between
    three types, a simple one that assumes the future will be similar to recent values,
    an extension that handles the trend, and the last one that also handles seasonality.
    I am going to describe these three types in a moment, but a small aside now. In
    the **M**-4 **Competition** ([Makridakis Competitions](https://en.wikipedia.org/wiki/Makridakis_Competitions),
    the most well-known competition for time series forecasters) Slawek Smyl won,
    he proposed [**ES-RNN**](https://www.sciencedirect.com/science/article/abs/pii/S0169207019301153),
    which is a hybrid between **E**xponential **S**moothing and **R**ecurrent **N**eural
    **N**etwork.'
  prefs: []
  type: TYPE_NORMAL
- en: Now weâ€™re back to the topic and the first type, which is **Simple Exponential
    Smoothing**. As a Baseline model, we can choose a model that always predicts the
    last value from the training set, a bit naive approach but can give us good results.
    Another approach could be to calculate the average of the whole training set,
    but then the same importance is given to the recent as the oldest observation.
    Exponential Smoothing is a combination of these two approaches, where greater
    weights are assigned to the most recent observations. They decrease exponentially
    for older observations, meaning the oldest ones will have the smallest weights.
    It uses an **Î±** parameter, its range is between 0 and 1\. The higher the value
    is the greater the impact of the latest values on the prediction. Please take
    a look at the above graphic, where the formulas are also. They are really easy
    to understand and often these models give good results. Before we move on to more
    advanced models we should stop here for a moment, because if a simple model gives
    the same results as very advanced models (for example, deep neural networks) then
    we should stay with the simpler ones, because their operation is more predictable
    for us and once again, more people are able to understand their operation.
  prefs: []
  type: TYPE_NORMAL
- en: SES doesnâ€™t handle a trend in data by the nature of the formula. If there is
    an increasing trend then the forecast underestimates because it doesnâ€™t include
    this increase in the data. Therefore, we have another model, which is **Double
    Exponential Smoothing**. It has an additional factor that is responsible for taking
    into account the impact of the trend. We use the **Î²** parameter, which controls
    the impact of the trend change. Hence we have 2 formulas, one for **level** (Level
    equation) and the other for **trend** (Trend equation).
  prefs: []
  type: TYPE_NORMAL
- en: The **Triple Exponential Smoothing** also takes into account seasonality. You
    can know it as Holt-Wintersâ€™ seasonal method. Here another parameter, **Î³**, comes
    into the formula. This method allows changing the **level**, **trend**, and patterns
    of **seasonality** over time. Like a trend, seasonality can be additive or multiplicative,
    but here I am not going to describe details assuming that you know the difference
    or can easily find them. I just donâ€™t want to make a book out of this article
    now, and I want you to smoothly go through the whole article. ðŸ˜‰
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '**overall MAPE: 31.88%**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/caee69055bc0f0fa97677781373db3dd.png)'
  prefs: []
  type: TYPE_IMG
- en: As for ARIMA, I did not add seasonality information. Now is your step, add this
    information using the *seasonal_periods* parameter for [**Exponential Smoothing**](https://unit8co.github.io/darts/generated_api/darts.models.forecasting.exponential_smoothing.html).
  prefs: []
  type: TYPE_NORMAL
- en: Prophet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/b90f59872f2815910c9764bd0a782d1e.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualize how the Prophet algorithm works, Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '[**Prophet**](https://facebook.github.io/prophet/) was proposed in the [*Forecasting
    at scale*](https://peerj.com/preprints/3190.pdf) paper from 2017 by Facebook.
    It is both a model and a library by the same name. Like the previous models, you
    will find this one at Darts. This algorithm is a **G**eneralized **A**dditive
    **M**odel, so the forecast is the sum of components. These components are **g(*t*)**
    â€” trend, **s(*t*)** â€” seasonality (yearly, weekly, and daily), and **h(*t*)**
    â€” the effect of holidays.'
  prefs: []
  type: TYPE_NORMAL
- en: y(t) = g(t) + s(t) + h(t) + error(t)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The first one is the **trend**, it can change over time and doesnâ€™t have to
    be constant over time. When students start learning classes with analyzing time
    series, they usually work with simple time series. In a time series, they can
    see a continuous growth trend. However, in real data, the trend can change several
    times. Prophet has implemented **changepoints** (think of them as a hyperparameter,
    such as their number, range, and prior scale). These points are trend changes,
    for example, increase trend -> changepoint -> decrease trend -> changepoint ->
    stronger decrease trend, and so on. This approach is closer to what we can usually
    see in the data. Positions of these changepoints Prophet set behind of you. The
    trend function between changepoints can be a simple regression.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we have the **seasonality** function which is the **Fourier series**.
  prefs: []
  type: TYPE_NORMAL
- en: Another function is the **holiday** effect, which adds or subtracts value to
    our forecast. You can use the dates that the Prophet library provides or define
    your own events. You can imagine that the Black Friday effect significantly affects
    the value of sales. In addition, you can take into account the range around the
    date where the holiday affects the forecast, for example, Christmas does not affect
    sales on the day of the holiday, but the days before (many days).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '**overall MAPE: 14.38%**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ca2b1293788c97c73332b2ec77cf20ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Global models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now letâ€™s turn to the approach where we have one model for all time series.
    This is also known as **cross-learning**, because the model to make a good prediction
    of time series A learned the relationships from time series A and also B, C, D,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised Model ~ Time Series as a regression problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/1044de425c3f4a039c506ed1ec58736c.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualize How to create features for a supervised learning model, Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Now letâ€™s try to apply supervised learning models to time series forecasting.
    This is nothing new, but they often give great results and are better than neural
    networks (see the best solutions from M-5 Competition). However, regression models
    are not dedicated to time series, so if we want to use them we need to **convert
    the time series problem into a machine learning problem**. I wrote more about
    this in my previous article [**Sell Out Sell In Forecasting**](/sell-out-sell-in-forecasting-45637005d6ee),
    but I will also now describe how to use the well-known regression algorithms for
    this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Before I start with the previously mentioned transformation, we first need to
    **scale** the data. Earlier I described the need for this transformation for global
    models. In this case, I used *MinMaxScaler*.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is **feature engineering**, which is the transformation that was
    repeated several times. Based on the history of the time series, we create features
    that will help the model better forecast the future. These variables can refer
    to the recent history of selected time series, such as **lagged** values (for
    weekly data t-1W, t-2W, t-3W, and so on). Another example is the calculation of
    **rolling statistics**, median (it can be a median of the recent 4 weeks), mean,
    min, max, std, and whatever you are going to calculate on the window. If there
    is **seasonality** in the data then it is good to give the model a hint as to
    where in time the t is. I often use the **sin and cos of a cyclic variable** (in
    the above visualization it is a day of the year and a day of the week).
  prefs: []
  type: TYPE_NORMAL
- en: The last step is to choose a **model**, you have a wide range of choices, so
    it can be Linear regression, Linear mixed effect model, Random Forest, LightGBM,
    and much more. The choice of model depends on the nature of the time series and
    the complexity of the problem. Another question might be if you want one model
    or as many models as horizons. When you choose a model then you need to keep in
    mind its weaknesses. For example, when you choose Random Forest then remember
    that the leaf counts the mean, so it canâ€™t go beyond the training range. LightGBM
    doesnâ€™t have this problem because it doesnâ€™t count a naive mean, but regression
    is counted in the background.
  prefs: []
  type: TYPE_NORMAL
- en: Now itâ€™s time again to return to the practical part and implement the model
    in code. I chose [**LightGBM**](https://lightgbm.readthedocs.io/en/v3.3.2/) as
    the model. Using it in Darts is much easier than in a similar way I would like
    to use it without this library. As you can see in the code we are using the lags
    of the last 14 days. I also added encoders that add covariates used by the model
    and they are added automatically and everything counts inside.
  prefs: []
  type: TYPE_NORMAL
- en: '*cyclic* â€” adds 2 columns, sin cos encoding based on cyclic variable, here
    month'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*datetime_attribute* â€” adds scalar based on datetime variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*position* â€” adds the relative index positions as integer values based on time
    series index, where 0 is set at the forecasting point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '**overall MAPE: 15.01%**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/45a39d49094bcdf9d257b90ae08828b8.png)'
  prefs: []
  type: TYPE_IMG
- en: The results are really promising especially since this is one model for all
    time series. However, I would like to warn you based on my experience. These types
    of models work well based on feature engineering, which is an advantage and a
    big disadvantage. Suppose you are using lags and moving mean. You are now going
    to forecast the value of one of the time series, but you have anomalies in it
    â€” several large values before the forecast point. Your model will definitely overestimate.
    When you create variables then try to imagine what effect they will have on the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: DeepAR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/ed577969a790507e6cbac998587cca53.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of arxiv and model architecture, based on Paper
  prefs: []
  type: TYPE_NORMAL
- en: '[**DeepAR**](https://arxiv.org/abs/1704.04110) is a deep learning algorithm
    developed by Amazon team. It is designed to model the complex dependencies and
    relationships in time series data using recurrent neural networks (RNNs).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can read in Abstract (which is very close to me):'
  prefs: []
  type: TYPE_NORMAL
- en: In retail businesses, for example, forecasting demand is crucial for having
    the right inventory available at the right time at the right place.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The model is autoregressive and generates a probabilistic forecast using Monte
    Carlo samples. The NN architecture is based on the LSTM layer. With a probabilistic
    approach, we are not interested in a single good prediction, but rather in the
    full predictive distribution of where the true value can be found. Instead of
    using LSTMs to calculate predictions directly, DeepAR leverages LSTMs to parameterize
    a Gaussian likelihood function. That is, to estimate mean and standard deviation
    of the Gaussian function (Î¸ = (Î¼, Ïƒ) parameters).
  prefs: []
  type: TYPE_NORMAL
- en: DeepAR supports Future-known covariates, we donâ€™t have such, but letâ€™s create
    them. As these features, I created OHE with the day of the week and the month.
    Probably a better approach would be sin and cos, I encourage you to experiment
    and return to me with your feedback.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '**overall MAPE: 19.35%**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5e2d94c8d9ccdbe19449ff4fab8ce6c4.png)'
  prefs: []
  type: TYPE_IMG
- en: N-BEATS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/86f2949104c8355bd78ad466b50eea5f.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of arxiv and model architecture, based on Paper
  prefs: []
  type: TYPE_NORMAL
- en: '[**N-BEATS**](https://arxiv.org/abs/1905.10437) (Neural basis expansion analysis
    for interpretable time series forecasting) is a deep learning algorithm, but it
    doesnâ€™t include recurrent layers, such as LSTM or GRU.'
  prefs: []
  type: TYPE_NORMAL
- en: The architecture may seem complex, but once you get into the details it is quite
    simple and is a combination of blocks and all layers are feed forward.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s start with the smallest element, the block, each of them has one input
    and generates two outputs. The input is lockback period. Outputs are the forecast
    and the backcast. I think the idea of forecast is easy for you. Backcast is prediction,
    but for lockback period â€” it is fitted value and show how well block has relationship
    on the lookback period window.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s move on to stacks, or the combination of multiple blocks. As you read,
    each block has two outputs and one input. The next blocks are responsible for
    forecasting residuals â€” similar to what happens in boosting forest models, like
    AdaBoost. In each step, the backcast generated by the block is subtracted from
    the input of the block before. At the end, all forecasts from blocks are aggregated.
    In addition, it is an interpretable model, you can decompose and see the effect
    of trend and seasonality.
  prefs: []
  type: TYPE_NORMAL
- en: Now letâ€™s move on to the combined stacks. This part increases the depth of the
    model and provides an opportunity to learn more about complexity.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '**overall MAPE: 13.18%**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0a61c6e32b3eb36bc2e90b0ce40b9973.png)'
  prefs: []
  type: TYPE_IMG
- en: TFT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/341ef264d69b65087f3d9ac99030c59b.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of arxiv and model architecture, based on Paper
  prefs: []
  type: TYPE_NORMAL
- en: '[**Temporal Fusion Transformers**](https://arxiv.org/abs/1912.09363) (TFT)
    is a deep learning algorithm developed by Google for time series forecasting.
    It is designed to model the complex dependencies and relationships in time series
    data using a combination of transformer networks and autoregressive modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: TFT is the most complex architecture and uses various techniques underneath.
    It is like an onion, composed of many layers. Also based on my experience, it
    learns the longest time compared to the above models. TFT uses a multi-head attention
    block to find long-time patterns, but LSTM sequence-to-sequence encoders/decoders
    to find these shorter patterns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '**overall MAPE: 13.37%**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d7ca8f7f8ffb0cd88937b1b5bc969e2d.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, I wanted to show you what approaches you can choose to forecast
    multiple time series. I have provided you with fully practical code, feel free
    to use them and donâ€™t hesitate to write to me.
  prefs: []
  type: TYPE_NORMAL
- en: 'This was just an introduction to the topic. I think also relevant from the
    work of Data Scientists for Supply Chain companies are the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '- **Hierarchical forecasting** and then combining forecasts from different
    hierarchies, i.e. Hierarchical Reconciliation. We can make a forecast at the store
    level and also at the country level, but when we aggregate the forecast at the
    store level then as a sum we would like to get the same thing that the forecast
    at the country level shows. That is why Hierarchical Reconciliation is important.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Another topic is **inventory optimization**, i.e. how many products we should
    have in stock so that we donâ€™t have a situation where we donâ€™t have our products
    in stock, but on the other hand, we donâ€™t stock one product for months.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/21d1e065cb242a0fd6cb114ae9823fec.png)'
  prefs: []
  type: TYPE_IMG
- en: Questions, Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: I am aware that I have touched on many topics in this article. I wanted to give
    you an indication of the directions you can take. Perhaps some of them should
    be described in more detail here and others described in detail in a new article.
    Do not hesitate and write to me, **you can find me on** [**Linkedin**](https://www.linkedin.com/in/bartosz-szablowski/).
    In future articles, I would like them to cover detailed topics and show you how
    to implement models for time series from scratch using PyTorch library.
  prefs: []
  type: TYPE_NORMAL
- en: THANKS FOR YOUR TIME!
  prefs: []
  type: TYPE_NORMAL
- en: 'Dataset source:'
  prefs: []
  type: TYPE_NORMAL
- en: misc{demand-forecasting-kernels-only,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: author = {inversion},
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: title = {Store Item Demand Forecasting Challenge},
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: publisher = {Kaggle},
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: year = {2018},
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: url = {https://kaggle.com/competitions/demand-forecasting-kernels-only},
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: license=CC
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
