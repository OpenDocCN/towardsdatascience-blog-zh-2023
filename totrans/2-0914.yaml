- en: Forecast Multiple Time Series Like a Master
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åƒå¤§å¸ˆä¸€æ ·é¢„æµ‹å¤šä¸ªæ—¶é—´åºåˆ—
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/forecast-multiple-time-series-like-a-master-1579a2b6f18d](https://towardsdatascience.com/forecast-multiple-time-series-like-a-master-1579a2b6f18d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/forecast-multiple-time-series-like-a-master-1579a2b6f18d](https://towardsdatascience.com/forecast-multiple-time-series-like-a-master-1579a2b6f18d)
- en: From local to global algorithms
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»å±€éƒ¨åˆ°å…¨å±€ç®—æ³•
- en: '[](https://medium.com/@zukaschikume?source=post_page-----1579a2b6f18d--------------------------------)[![Bartosz
    SzabÅ‚owski](../Images/2115d18d8d0463a6c680b0461472c203.png)](https://medium.com/@zukaschikume?source=post_page-----1579a2b6f18d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1579a2b6f18d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1579a2b6f18d--------------------------------)
    [Bartosz SzabÅ‚owski](https://medium.com/@zukaschikume?source=post_page-----1579a2b6f18d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@zukaschikume?source=post_page-----1579a2b6f18d--------------------------------)[![Bartosz
    SzabÅ‚owski](../Images/2115d18d8d0463a6c680b0461472c203.png)](https://medium.com/@zukaschikume?source=post_page-----1579a2b6f18d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1579a2b6f18d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1579a2b6f18d--------------------------------)
    [Bartosz SzabÅ‚owski](https://medium.com/@zukaschikume?source=post_page-----1579a2b6f18d--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1579a2b6f18d--------------------------------)
    Â·30 min readÂ·Apr 26, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----1579a2b6f18d--------------------------------)
    Â·é˜…è¯»æ—¶é—´30åˆ†é’ŸÂ·2023å¹´4æœˆ26æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3393fc3c8c56dbacebe783c486b2d2e7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3393fc3c8c56dbacebe783c486b2d2e7.png)'
- en: Photo by [JesÃºs Rocha](https://unsplash.com/@jjrocha?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼š[JesÃºs Rocha](https://unsplash.com/@jjrocha?utm_source=medium&utm_medium=referral)
    äº[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: I deal with forecasting multiple time series in business (to be precise, demand
    forecasting). In my previous article [*Sell Out Sell In Forecasting*](https://medium.com/towards-data-science/sell-out-sell-in-forecasting-45637005d6ee)
    I presented the methodology that I implemented at NestlÃ© for demand forecasting.
    In this article, I would like to introduce you to the universal (which doesnâ€™t
    mean ideal) algorithms currently in use for forecasting multiple time series â€”
    such as **state of the art** for time series. For a retailer or manufacturer,
    forecasting demand is key to business. It allows them to create more accurate
    production plans and optimize their inventory. Unfortunately, many companies (not
    NestlÃ© :) ) donâ€™t see this problem and they use spreadsheets with simple statistics.
    If they would change this they could significantly reduce their costs. After all,
    warehousing and out-of-date products â€” thatâ€™s an additional cost.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨å•†ä¸šä¸­å¤„ç†å¤šä¸ªæ—¶é—´åºåˆ—çš„é¢„æµ‹ï¼ˆå‡†ç¡®æ¥è¯´ï¼Œæ˜¯éœ€æ±‚é¢„æµ‹ï¼‰ã€‚åœ¨æˆ‘ä¹‹å‰çš„æ–‡ç« ä¸­[*Sell Out Sell In Forecasting*](https://medium.com/towards-data-science/sell-out-sell-in-forecasting-45637005d6ee)ï¼Œæˆ‘ä»‹ç»äº†æˆ‘åœ¨é›€å·¢å®æ–½çš„éœ€æ±‚é¢„æµ‹æ–¹æ³•ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘æƒ³å‘ä½ ä»‹ç»ç›®å‰ç”¨äºé¢„æµ‹å¤šä¸ªæ—¶é—´åºåˆ—çš„é€šç”¨ï¼ˆè¿™å¹¶ä¸æ„å‘³ç€ç†æƒ³ï¼‰ç®—æ³•â€”â€”ä¾‹å¦‚**æœ€å…ˆè¿›**çš„æ—¶é—´åºåˆ—ç®—æ³•ã€‚å¯¹äºé›¶å”®å•†æˆ–åˆ¶é€ å•†æ¥è¯´ï¼Œé¢„æµ‹éœ€æ±‚å¯¹ä¸šåŠ¡è‡³å…³é‡è¦ã€‚å®ƒå…è®¸ä»–ä»¬åˆ¶å®šæ›´å‡†ç¡®çš„ç”Ÿäº§è®¡åˆ’å¹¶ä¼˜åŒ–åº“å­˜ã€‚ä¸å¹¸çš„æ˜¯ï¼Œè®¸å¤šå…¬å¸ï¼ˆå¹¶éé›€å·¢
    :) ï¼‰å¹¶æœªæ„è¯†åˆ°è¿™ä¸ªé—®é¢˜ï¼Œä»–ä»¬ä»ç„¶ä½¿ç”¨ç®€å•ç»Ÿè®¡çš„ç”µå­è¡¨æ ¼ã€‚å¦‚æœä»–ä»¬èƒ½æ”¹å˜è¿™ç§æƒ…å†µï¼Œä»–ä»¬å¯ä»¥æ˜¾è‘—é™ä½æˆæœ¬ã€‚æ¯•ç«Ÿï¼Œä»“å‚¨å’Œè¿‡æ—¶äº§å“â€”â€”è¿™ä¹Ÿæ˜¯é¢å¤–çš„æˆæœ¬ã€‚
- en: '![](../Images/6b8f08996d06912402198087dc3b5e19.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b8f08996d06912402198087dc3b5e19.png)'
- en: How to forecast multiple time series, Image by Author
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½•é¢„æµ‹å¤šä¸ªæ—¶é—´åºåˆ—ï¼Œä½œè€…æä¾›çš„å›¾ç‰‡
- en: Itâ€™s hard to find someone working in Data Science who isnâ€™t familiar with [**Scikit-learn**](https://scikit-learn.org/).
    For dataframes, you can use **Scikit-learn** to do most of the elements involved
    in machine learning â€” from preprocessing to hyperparameters selection, evaluation,
    and model prediction. We can assign Linear Regression, Decision Tree, or Support
    Vector Machine (SVM) to a variable *model* and use the same methods each time,
    like *fit* and *predict*. We have a lot of flexibility, but we also have an easy
    way of implementing solutions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆéš¾æ‰¾åˆ°ä¸€ä¸ªæ•°æ®ç§‘å­¦é¢†åŸŸçš„äººä¸ç†Ÿæ‚‰[**Scikit-learn**](https://scikit-learn.org/)ã€‚å¯¹äºæ•°æ®æ¡†æ¶ï¼Œä½ å¯ä»¥ä½¿ç”¨**Scikit-learn**æ¥å®Œæˆæœºå™¨å­¦ä¹ ä¸­æ¶‰åŠçš„å¤§éƒ¨åˆ†å…ƒç´ â€”â€”ä»é¢„å¤„ç†åˆ°è¶…å‚æ•°é€‰æ‹©ã€è¯„ä¼°å’Œæ¨¡å‹é¢„æµ‹ã€‚æˆ‘ä»¬å¯ä»¥å°†çº¿æ€§å›å½’ã€å†³ç­–æ ‘æˆ–æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰åˆ†é…ç»™å˜é‡*model*ï¼Œå¹¶æ¯æ¬¡ä½¿ç”¨ç›¸åŒçš„æ–¹æ³•ï¼Œå¦‚*fit*å’Œ*predict*ã€‚æˆ‘ä»¬æœ‰å¾ˆå¤§çš„çµæ´»æ€§ï¼Œä½†ä¹Ÿæœ‰ä¸€ç§ç®€å•çš„æ–¹å¼æ¥å®ç°è§£å†³æ–¹æ¡ˆã€‚
- en: For time series the situation is different. If you are experimenting and want
    to compare different algorithms, the algorithms themselves are not only a problem.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ—¶é—´åºåˆ—æ¥è¯´ï¼Œæƒ…å†µæ˜¯ä¸åŒçš„ã€‚å¦‚æœä½ åœ¨å®éªŒå¹¶æƒ³æ¯”è¾ƒä¸åŒçš„ç®—æ³•ï¼Œç®—æ³•æœ¬èº«ä¸ä»…ä»…æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚
- en: If you are starting to work with time series, you need to process them, for
    resampling or filling in missing values â€” [**Pandas**](https://pandas.pydata.org/)
    is useful for that.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¼€å§‹å¤„ç†æ—¶é—´åºåˆ—ï¼Œä½ éœ€è¦å¯¹å®ƒä»¬è¿›è¡Œå¤„ç†ï¼Œä¾‹å¦‚é‡æ–°é‡‡æ ·æˆ–å¡«è¡¥ç¼ºå¤±å€¼â€”â€”[**Pandas**](https://pandas.pydata.org/)å¯¹æ­¤éå¸¸æœ‰ç”¨ã€‚
- en: If you want to do a decomposition, visualize ACF/PACF, or check a stationarity
    test, then the [**Statsmodels**](https://www.statsmodels.org/) library is useful.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³è¿›è¡Œåˆ†è§£ã€å¯è§†åŒ– ACF/PACFï¼Œæˆ–æ£€æŸ¥å¹³ç¨³æ€§æµ‹è¯•ï¼Œé‚£ä¹ˆ[**Statsmodels**](https://www.statsmodels.org/)åº“å°†éå¸¸æœ‰ç”¨ã€‚
- en: For visualization, you will probably use [**Matplotlib**](https://matplotlib.org/),
    even if not this library, there are many built on it.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå¯è§†åŒ–ï¼Œä½ å¯èƒ½ä¼šä½¿ç”¨[**Matplotlib**](https://matplotlib.org/)ï¼Œå³ä½¿ä¸æ˜¯è¿™ä¸ªåº“ï¼Œä¹Ÿæœ‰è®¸å¤šå»ºç«‹åœ¨å®ƒä¹‹ä¸Šçš„åº“ã€‚
- en: The fun begins when you want to use different algorithms. When you want to use
    **ARIMA** then you will probably use [**pmdarima**](https://alkaline-ml.com/pmdarima),
    [**Prophet**](https://facebook.github.io/prophet/) is another library. Typical
    **M**achine **L**earning algorithms can be found in the previously mentioned **Scikit-learn**,
    but you may also want to use boosting models like [**LightGBM**](https://lightgbm.readthedocs.io/)
    or [**CatBoost**](https://catboost.ai/). For **D**eep **N**eural **N**etworks
    and architectures from the most recent papers, [**PyTorch Forecasting**](https://pytorch-forecasting.readthedocs.io/)
    is worth using.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ æƒ³ä½¿ç”¨ä¸åŒçš„ç®—æ³•æ—¶ï¼Œä¹è¶£æ‰å¼€å§‹ã€‚å½“ä½ æƒ³ä½¿ç”¨**ARIMA**æ—¶ï¼Œä½ å¯èƒ½ä¼šä½¿ç”¨[**pmdarima**](https://alkaline-ml.com/pmdarima)ï¼Œ[**Prophet**](https://facebook.github.io/prophet/)æ˜¯å¦ä¸€ä¸ªåº“ã€‚å…¸å‹çš„**M**achine
    **L**earningç®—æ³•å¯ä»¥åœ¨ä¹‹å‰æåˆ°çš„**Scikit-learn**ä¸­æ‰¾åˆ°ï¼Œä½†ä½ ä¹Ÿå¯èƒ½æƒ³ä½¿ç”¨åƒ[**LightGBM**](https://lightgbm.readthedocs.io/)æˆ–[**CatBoost**](https://catboost.ai/)è¿™æ ·çš„æå‡æ¨¡å‹ã€‚å¯¹äº**D**eep
    **N**eural **N**etworkså’Œæœ€æ–°è®ºæ–‡ä¸­çš„æ¶æ„ï¼Œ[**PyTorch Forecasting**](https://pytorch-forecasting.readthedocs.io/)å€¼å¾—ä½¿ç”¨ã€‚
- en: WOWğŸ¤¯ A lot of libraries you probably need. If you want to be able to use the
    previously mentioned libraries it will be a lot of work, because most of them
    use different APIs, data types, and for each of the libraries with models you
    will have to prepare your own functions for backtesting and selection of hyperparameters.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: WOWğŸ¤¯ ä½ å¯èƒ½éœ€è¦çš„åº“éå¸¸å¤šã€‚å¦‚æœä½ æƒ³èƒ½å¤Ÿä½¿ç”¨ä¸Šè¿°æåˆ°çš„åº“ï¼Œè¿™å°†æ˜¯ä¸€ä¸ªå¤§é‡çš„å·¥ä½œï¼Œå› ä¸ºå¤§å¤šæ•°åº“ä½¿ç”¨ä¸åŒçš„ APIã€æ•°æ®ç±»å‹ï¼Œå¹¶ä¸”å¯¹äºæ¯ä¸ªåº“ä¸­çš„æ¨¡å‹ï¼Œä½ éƒ½å¿…é¡»å‡†å¤‡è‡ªå·±çš„å›æµ‹å’Œè¶…å‚æ•°é€‰æ‹©å‡½æ•°ã€‚
- en: '![](../Images/bd03ac32b8e2a4f05d9177ceaf14df14.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd03ac32b8e2a4f05d9177ceaf14df14.png)'
- en: Library Darts, Image by Author, Inspired by library documentation
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Library Dartsï¼Œç”±ä½œè€…æä¾›çš„å›¾ç‰‡ï¼Œçµæ„Ÿæ¥è‡ªäºå›¾ä¹¦é¦†æ–‡æ¡£
- en: Here helps us [**Darts**](https://unit8co.github.io/darts/), which tries to
    be a **Scikit-learn** for time series, and its purpose is precise to simplify
    working with time series. Often its functionality is based on other libraries,
    for example, it uses **Statsmodels** for decomposition. It also makes **Darts**
    work well with other libraries if something isnâ€™t implemented there, you can compare
    it to how you can mutually use **Matplotlib** to work with **Seaborn**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæˆ‘ä»¬å¾—åˆ°å¸®åŠ©çš„æ˜¯[**Darts**](https://unit8co.github.io/darts/)ï¼Œå®ƒè¯•å›¾æˆä¸ºæ—¶é—´åºåˆ—çš„**Scikit-learn**ï¼Œå…¶ç›®çš„æ˜¯ç®€åŒ–æ—¶é—´åºåˆ—çš„å·¥ä½œã€‚å®ƒçš„åŠŸèƒ½é€šå¸¸åŸºäºå…¶ä»–åº“ï¼Œä¾‹å¦‚ï¼Œå®ƒä½¿ç”¨**Statsmodels**è¿›è¡Œåˆ†è§£ã€‚å¦‚æœæŸäº›åŠŸèƒ½åœ¨å…¶ä»–åº“ä¸­æ²¡æœ‰å®ç°ï¼Œ**Darts**ä¹Ÿèƒ½å¾ˆå¥½åœ°ä¸å…¶ä»–åº“åä½œï¼Œä½ å¯ä»¥å°†å…¶ä¸**Matplotlib**å’Œ**Seaborn**äº’ç›¸é…åˆä½¿ç”¨è¿›è¡Œæ¯”è¾ƒã€‚
- en: Where it is necessary they have their own implementations, but they donâ€™t want
    to reinvent the wheel and use what is already there in other popular libraries
    for time series.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¿…è¦çš„åœ°æ–¹ï¼Œå®ƒä»¬æœ‰è‡ªå·±çš„å®ç°ï¼Œä½†å®ƒä»¬ä¸æƒ³é‡æ–°å‘æ˜è½®å­ï¼Œè€Œæ˜¯ä½¿ç”¨å…¶ä»–æµè¡Œæ—¶é—´åºåˆ—åº“ä¸­å·²ç»å­˜åœ¨çš„ä¸œè¥¿ã€‚
- en: This idea is not new for time series, and there are other good libraries, such
    as [**sktime**](https://www.sktime.org/), [**GluonTS**](https://ts.gluon.ai/),or
    [**nixtla**](https://www.nixtla.io/), but in my opinion, **Darts** has the lowest
    entry threshold and is more complete. This is not an advertisement for this library,
    at the end of the day your forecast should bring value to the business you work
    for. You might as well write each of these models in code from scratch. I am going
    to use Darts in the below examples, but you will also be able to find these models
    (all or part of them) in the libraries mentioned above. I see space for improvement
    for the Darts library in **optimizing computation** if we want to train multiple
    local models ~ this tries **nixtla** library, which offers compatibility with
    Spark, Dask, and Ray.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€æ¦‚å¿µåœ¨æ—¶é—´åºåˆ—é¢†åŸŸå¹¶ä¸æ–°é²œï¼Œè¿˜æœ‰å…¶ä»–å¾ˆå¥½çš„åº“ï¼Œä¾‹å¦‚[**sktime**](https://www.sktime.org/)ã€[**GluonTS**](https://ts.gluon.ai/)æˆ–[**nixtla**](https://www.nixtla.io/)ï¼Œä½†åœ¨æˆ‘çœ‹æ¥ï¼Œ**Darts**çš„å…¥é—¨é—¨æ§›æœ€ä½ï¼ŒåŠŸèƒ½ä¹Ÿæ›´ä¸ºå®Œå–„ã€‚è¿™ä¸æ˜¯å¯¹è¿™ä¸ªåº“çš„å¹¿å‘Šï¼Œå½’æ ¹ç»“åº•ï¼Œä½ çš„é¢„æµ‹åº”è¯¥ä¸ºä½ å·¥ä½œçš„ä¼ä¸šå¸¦æ¥ä»·å€¼ã€‚ä½ ä¹Ÿå¯ä»¥ä»å¤´å¼€å§‹ç”¨ä»£ç ç¼–å†™è¿™äº›æ¨¡å‹ã€‚æˆ‘å°†ä½¿ç”¨Dartsè¿›è¡Œä»¥ä¸‹ç¤ºä¾‹ï¼Œä½†ä½ ä¹Ÿå¯ä»¥åœ¨ä¸Šè¿°æåˆ°çš„åº“ä¸­æ‰¾åˆ°è¿™äº›æ¨¡å‹ï¼ˆå…¨éƒ¨æˆ–éƒ¨åˆ†ï¼‰ã€‚å¦‚æœæˆ‘ä»¬æƒ³è®­ç»ƒå¤šä¸ªæœ¬åœ°æ¨¡å‹ï¼Œæˆ‘è®¤ä¸ºDartsåº“åœ¨**ä¼˜åŒ–è®¡ç®—**æ–¹é¢è¿˜æœ‰æ”¹è¿›çš„ç©ºé—´â€”â€”å¯ä»¥å°è¯•**nixtla**åº“ï¼Œå®ƒæä¾›ä¸Sparkã€Daskå’ŒRayçš„å…¼å®¹æ€§ã€‚
- en: 'From my perspective, Darts is already a mature library and it is still being
    developed, just look at the [**changelog**](https://github.com/unit8co/darts/blob/master/CHANGELOG.md).
    Now you can install it in the standard way:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æˆ‘çš„è§’åº¦æ¥çœ‹ï¼ŒDartså·²ç»æ˜¯ä¸€ä¸ªæˆç†Ÿçš„åº“ï¼Œå¹¶ä¸”ä»åœ¨ä¸æ–­å¼€å‘ä¸­ï¼Œåªéœ€æŸ¥çœ‹[**å˜æ›´æ—¥å¿—**](https://github.com/unit8co/darts/blob/master/CHANGELOG.md)å³å¯ã€‚ç°åœ¨ä½ å¯ä»¥æŒ‰ç…§æ ‡å‡†æ–¹å¼è¿›è¡Œå®‰è£…ï¼š
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once we have the library installed in our environment then we can import it
    and use it in practice.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬åœ¨ç¯å¢ƒä¸­å®‰è£…äº†åº“ï¼Œå°±å¯ä»¥å¯¼å…¥å¹¶åœ¨å®è·µä¸­ä½¿ç”¨å®ƒã€‚
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Single vs Multiple time series
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å•å˜é‡ä¸å¤šå˜é‡æ—¶é—´åºåˆ—
- en: '![](../Images/83b03160c0fff2b64f2e1b076f5efd04.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83b03160c0fff2b64f2e1b076f5efd04.png)'
- en: One vs Multiple time series, Image by Author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å•å˜é‡ä¸å¤šå˜é‡æ—¶é—´åºåˆ—ï¼Œä½œè€…æä¾›çš„å›¾åƒ
- en: Above, based on the [**Walmart dataset**](https://www.kaggle.com/datasets/yasserh/walmart-dataset),
    you can see **single** and **multiple** time series. Nowadays, many problems involve
    multiple points at the same time. This data can come from a variety of processes,
    it can be this example and my daily work which is demand forecasting, but it can
    also be energy consumption forecasting, the closing price of a company on the
    stock market, the number of bicycles rented from a station and many many others
    problems.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸Šå›¾æ‰€ç¤ºï¼ŒåŸºäº[**Walmart æ•°æ®é›†**](https://www.kaggle.com/datasets/yasserh/walmart-dataset)ï¼Œä½ å¯ä»¥çœ‹åˆ°**å•å˜é‡**å’Œ**å¤šå˜é‡**æ—¶é—´åºåˆ—ã€‚ç°åœ¨ï¼Œè®¸å¤šé—®é¢˜æ¶‰åŠåŒæ—¶å¤„ç†å¤šä¸ªç‚¹ã€‚è¿™äº›æ•°æ®å¯ä»¥æ¥è‡ªå„ç§è¿‡ç¨‹ï¼Œå¯ä»¥æ˜¯è¿™ä¸ªä¾‹å­å’Œæˆ‘æ—¥å¸¸å·¥ä½œçš„éœ€æ±‚é¢„æµ‹ï¼Œä¹Ÿå¯ä»¥æ˜¯èƒ½æºæ¶ˆè€—é¢„æµ‹ã€å…¬å¸è‚¡å¸‚æ”¶ç›˜ä»·ã€ä»è½¦ç«™ç§Ÿç”¨çš„è‡ªè¡Œè½¦æ•°é‡ç­‰ç­‰è®¸å¤šå…¶ä»–é—®é¢˜ã€‚
- en: In addition to the time series itself, we may also have other *variables* for
    them, some of which may be known for the future and others only available for
    the past â€” about that in a moment.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†æ—¶é—´åºåˆ—æœ¬èº«ï¼Œæˆ‘ä»¬è¿˜å¯èƒ½æœ‰å…¶ä»–*å˜é‡*ï¼Œå…¶ä¸­ä¸€äº›å¯èƒ½å·²çŸ¥æœªæ¥å€¼ï¼Œè€Œå…¶ä»–ä»…åœ¨è¿‡å»å¯ç”¨â€”â€”ç¨åä¼šè¯¦ç»†è®²è§£ã€‚
- en: In this article, I want to show you different approaches for forecasting multiple
    time series, but I want it to be practical â€” so that you are not left with just
    theory. So letâ€™s import all the libraries used later â€” darts and others well known
    by Data Scientists.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘æƒ³å‘ä½ å±•ç¤ºé¢„æµ‹å¤šä¸ªæ—¶é—´åºåˆ—çš„ä¸åŒæ–¹æ³•ï¼Œä½†æˆ‘å¸Œæœ›å®ƒæ˜¯å®ç”¨çš„â€”â€”ä»¥ä¾¿ä½ ä¸ä»…ä»…åœç•™åœ¨ç†è®ºå±‚é¢ã€‚æ‰€ä»¥è®©æˆ‘ä»¬å¯¼å…¥æ‰€æœ‰åç»­ä½¿ç”¨çš„åº“â€”â€”åŒ…æ‹¬Dartså’Œå…¶ä»–æ•°æ®ç§‘å­¦å®¶ç†ŸçŸ¥çš„åº“ã€‚
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now letâ€™s load the dataset, which is about **demand forecasting** and comes
    from the [**Kaggle**](https://www.kaggle.com/competitions/demand-forecasting-kernels-only).
    If you agree with the terms of the competition on Kaggle then you can also download
    this dataset.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬åŠ è½½æ•°æ®é›†ï¼Œè¿™ä¸ªæ•°æ®é›†æ¶‰åŠ**éœ€æ±‚é¢„æµ‹**ï¼Œå¹¶æ¥è‡ª[**Kaggle**](https://www.kaggle.com/competitions/demand-forecasting-kernels-only)ã€‚å¦‚æœä½ åŒæ„Kaggleä¸Šçš„æ¯”èµ›æ¡æ¬¾ï¼Œé‚£ä¹ˆä½ ä¹Ÿå¯ä»¥ä¸‹è½½è¿™ä¸ªæ•°æ®é›†ã€‚
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: There are 10 stores and in each of them are 50 items making a total of 500 time
    series.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å…±æœ‰10å®¶å•†åº—ï¼Œæ¯å®¶å•†åº—æœ‰50ç§å•†å“ï¼Œæ€»è®¡500ä¸ªæ—¶é—´åºåˆ—ã€‚
- en: Letâ€™s take a look at the 10 among the least, median, and most sold in total
    store-item combinations. To find what relations are in a time series it is often
    enough to look at it because this already tells us a lot, like a trend or seasonality,
    but often much more.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹åœ¨æ‰€æœ‰å•†åº—å•†å“ç»„åˆä¸­é”€å”®æœ€å°‘ã€ä¸­ç­‰å’Œæœ€å¤šçš„10ç§å•†å“ã€‚è¦æ‰¾å‡ºæ—¶é—´åºåˆ—ä¸­çš„å…³ç³»ï¼Œé€šå¸¸åªéœ€è§‚å¯Ÿå®ƒï¼Œå› ä¸ºè¿™å·²ç»èƒ½å‘Šè¯‰æˆ‘ä»¬å¾ˆå¤šä¿¡æ¯ï¼Œæ¯”å¦‚è¶‹åŠ¿æˆ–å­£èŠ‚æ€§ï¼Œä½†å¾€å¾€è¿˜æœ‰æ›´å¤šã€‚
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/0c022ede1dbd871b535d8233bd4c8be8.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c022ede1dbd871b535d8233bd4c8be8.png)'
- en: There is a lot of similarity between these time series. Weâ€™ll check for the
    existence of seasonality (day of the week or week/month of the year) or trend
    in a moment, but you can already intuitively imagine that model learning relationships
    from all time series will be able to forecast one single time series better than
    if it only learned from its history.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ—¶é—´åºåˆ—ä¹‹é—´æœ‰å¾ˆå¤šç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬å°†ç¨åæ£€æŸ¥æ˜¯å¦å­˜åœ¨å­£èŠ‚æ€§ï¼ˆå‘¨å‡ æˆ–ä¸€å¹´ä¸­çš„å‘¨/æœˆï¼‰æˆ–è¶‹åŠ¿ï¼Œä½†ä½ å¯ä»¥ç›´è§‚åœ°æƒ³è±¡ï¼Œä»æ‰€æœ‰æ—¶é—´åºåˆ—ä¸­å­¦ä¹ çš„æ¨¡å‹å°†æ¯”ä»…ä»å…¶å†å²ä¸­å­¦ä¹ çš„æ¨¡å‹æ›´å¥½åœ°é¢„æµ‹å•ä¸ªæ—¶é—´åºåˆ—ã€‚
- en: I create a copy of my dataset for **EDA** (exploratory data analysis). Then
    I scale the time series using **MinMAXScaler** and then all the time series will
    be comparable to each other. Finally, I create Box Plots with which I check if
    there is a trend and seasonality.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸º**EDA**ï¼ˆæ¢ç´¢æ€§æ•°æ®åˆ†æï¼‰åˆ›å»ºäº†æ•°æ®é›†çš„å‰¯æœ¬ã€‚ç„¶åï¼Œæˆ‘ä½¿ç”¨**MinMAXScaler**å¯¹æ—¶é—´åºåˆ—è¿›è¡Œç¼©æ”¾ï¼Œä½¿æ‰€æœ‰æ—¶é—´åºåˆ—å¯ä»¥äº’ç›¸æ¯”è¾ƒã€‚æœ€åï¼Œæˆ‘åˆ›å»ºç®±å‹å›¾ï¼Œä»¥æ£€æŸ¥æ˜¯å¦å­˜åœ¨è¶‹åŠ¿å’Œå­£èŠ‚æ€§ã€‚
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/389fc7b02f361ab79d5cac60527593cc.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/389fc7b02f361ab79d5cac60527593cc.png)'
- en: Yes, there is a trend and two types of seasonality. If you think that these
    time series are easy to forecast â€”you are right. The purpose of this article is
    to show you the most popular approaches to forecasting multiple time series. The
    data is not always as easy as, for example, stock market indexes, but thatâ€™s a
    topic for another article.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæœ‰è¶‹åŠ¿å’Œä¸¤ç§å­£èŠ‚æ€§ã€‚å¦‚æœä½ è®¤ä¸ºè¿™äº›æ—¶é—´åºåˆ—å¾ˆå®¹æ˜“é¢„æµ‹â€”â€”ä½ æ˜¯å¯¹çš„ã€‚æœ¬æ–‡çš„ç›®çš„æ˜¯å‘ä½ å±•ç¤ºé¢„æµ‹å¤šä¸ªæ—¶é—´åºåˆ—çš„æœ€æµè¡Œçš„æ–¹æ³•ã€‚æ•°æ®å¹¶ä¸æ€»æ˜¯åƒè‚¡ç¥¨å¸‚åœºæŒ‡æ•°é‚£æ ·ç®€å•ï¼Œä½†é‚£æ˜¯å¦ä¸€ä¸ªè¯é¢˜ã€‚
- en: I think this exploration is enough to understand what relationships the model
    should learn.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºè¿™ç§æ¢ç´¢è¶³ä»¥ç†è§£æ¨¡å‹åº”è¯¥å­¦ä¹ å“ªäº›å…³ç³»ã€‚
- en: Here we have **multiple time series** (sales of multiple items in multiple stores).
    For each **time series**, letâ€™s create a **TimeSeries** object from a Pandas DataFrame.
    This type is required by models from the Darts library. Then save these time series
    in a **list**.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰**å¤šä¸ªæ—¶é—´åºåˆ—**ï¼ˆå¤šä¸ªå•†åº—ä¸­çš„å¤šä¸ªå•†å“é”€å”®ï¼‰ã€‚å¯¹äºæ¯ä¸€ä¸ª**æ—¶é—´åºåˆ—**ï¼Œæˆ‘ä»¬ä» Pandas DataFrame ä¸­åˆ›å»ºä¸€ä¸ª**TimeSeries**å¯¹è±¡ã€‚è¿™ç§ç±»å‹æ˜¯
    Darts åº“ä¸­çš„æ¨¡å‹æ‰€éœ€çš„ã€‚ç„¶åå°†è¿™äº›æ—¶é—´åºåˆ—ä¿å­˜åˆ°**åˆ—è¡¨**ä¸­ã€‚
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Working with multiple time series can be helpful, but often it is problematic.
    When we have one time series then we have a lot of time to work with it. Look
    at it, verify the trend and seasonality, and transform anomalies. We can polish
    our Forecast. For multiple series this approach becomes impossible. We want the
    approach to be as automatic as possible, but then we may miss details, such as
    anomalies, or perhaps we should not process each time series in the same way.
    There may be more typical problems: missing data, data drift, and rare events
    (black swans). More series can potentially help us because our model will be able
    to use more data, and therefore there will be more representative observations
    for a particular pattern. Using my work as an example â€” to forecast the demand
    caused by a promotion for product X in the following week, our model can also
    use the historical effects of such a promotion from other promotions.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å¤„ç†å¤šä¸ªæ—¶é—´åºåˆ—å¯èƒ½å¾ˆæœ‰å¸®åŠ©ï¼Œä½†é€šå¸¸ä¹Ÿä¼šå¸¦æ¥é—®é¢˜ã€‚å½“æˆ‘ä»¬åªæœ‰ä¸€ä¸ªæ—¶é—´åºåˆ—æ—¶ï¼Œæˆ‘ä»¬æœ‰å¾ˆå¤šæ—¶é—´æ¥å¤„ç†å®ƒã€‚æŸ¥çœ‹å®ƒï¼ŒéªŒè¯è¶‹åŠ¿å’Œå­£èŠ‚æ€§ï¼Œå¹¶å¤„ç†å¼‚å¸¸ã€‚æˆ‘ä»¬å¯ä»¥ä¼˜åŒ–æˆ‘ä»¬çš„é¢„æµ‹ã€‚å¯¹äºå¤šä¸ªåºåˆ—ï¼Œè¿™ç§æ–¹æ³•å˜å¾—ä¸å¯è¡Œã€‚æˆ‘ä»¬å¸Œæœ›æ–¹æ³•å°½å¯èƒ½è‡ªåŠ¨åŒ–ï¼Œä½†è¿™æ ·å¯èƒ½ä¼šé”™è¿‡ç»†èŠ‚ï¼Œæ¯”å¦‚å¼‚å¸¸ï¼Œæˆ–è€…æˆ‘ä»¬ä¸åº”è¯¥ä»¥ç›¸åŒçš„æ–¹å¼å¤„ç†æ¯ä¸ªæ—¶é—´åºåˆ—ã€‚å¯èƒ½è¿˜æœ‰æ›´å¤šå…¸å‹çš„é—®é¢˜ï¼šç¼ºå¤±æ•°æ®ã€æ•°æ®æ¼‚ç§»å’Œç¨€æœ‰äº‹ä»¶ï¼ˆé»‘å¤©é¹…äº‹ä»¶ï¼‰ã€‚æ›´å¤šçš„åºåˆ—å¯èƒ½å¯¹æˆ‘ä»¬æœ‰å¸®åŠ©ï¼Œå› ä¸ºæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ä½¿ç”¨æ›´å¤šçš„æ•°æ®ï¼Œå› æ­¤ä¼šæœ‰æ›´å¤šçš„ä»£è¡¨æ€§è§‚å¯Ÿæ•°æ®æ¥è¯†åˆ«ç‰¹å®šæ¨¡å¼ã€‚ä»¥æˆ‘çš„å·¥ä½œä¸ºä¾‹â€”â€”ä¸ºäº†é¢„æµ‹äº§å“
    X åœ¨ä¸‹å‘¨å› ä¿ƒé”€é€ æˆçš„éœ€æ±‚ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿˜å¯ä»¥åˆ©ç”¨å…¶ä»–ä¿ƒé”€çš„å†å²æ•ˆæœã€‚
- en: Hence the question â€” how to forecast multiple time series?
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ‰€ä»¥é—®é¢˜æ¥äº†â€”â€”å¦‚ä½•é¢„æµ‹å¤šä¸ªæ—¶é—´åºåˆ—ï¼Ÿ
- en: You may often ask yourself *Do I have a* ***multiple*** *or* ***multivariate***
    *time series?*. These questions are legitimate and the answer is not always clear.
    When your time series are from a **single process**, are **interconnected**, **correlated,**
    and **interact** with each other then the answer will be *my time series are*
    ***multivariate***.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½ç»å¸¸ä¼šé—®è‡ªå·±*æˆ‘æ˜¯å¦æœ‰ä¸€ä¸ª* ***å¤šå˜é‡*** *æˆ–* ***å¤šå…ƒ*** *æ—¶é—´åºåˆ—ï¼Ÿ* è¿™äº›é—®é¢˜æ˜¯åˆç†çš„ï¼Œä½†ç­”æ¡ˆå¹¶ä¸æ€»æ˜¯æ˜ç¡®çš„ã€‚å½“ä½ çš„æ—¶é—´åºåˆ—æ¥è‡ª**å•ä¸€è¿‡ç¨‹**ï¼Œ**ç›¸äº’å…³è”**ã€**ç›¸å…³**ï¼Œå¹¶ä¸”**ç›¸äº’ä½œç”¨**æ—¶ï¼Œç­”æ¡ˆå°†æ˜¯*æˆ‘çš„æ—¶é—´åºåˆ—æ˜¯*
    ***å¤šå…ƒ***ã€‚
- en: When you forecast sales of product X in **multiple stores** then you have a
    **multiple time series**, but when you have an additional product Y then for a
    single store you have a **multivariate time series**, because sales of one product
    in a store can **affect** sales of **another**, or it can affect â€” it was only
    an assumption.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ åœ¨**å¤šä¸ªå•†åº—**é¢„æµ‹äº§å“ X çš„é”€å”®æ—¶ï¼Œä½ æœ‰ä¸€ä¸ª**å¤šæ—¶é—´åºåˆ—**ï¼Œä½†å½“ä½ æœ‰ä¸€ä¸ªé¢å¤–çš„äº§å“ Y æ—¶ï¼Œå¯¹äºå•ä¸ªå•†åº—æ¥è¯´ï¼Œä½ æœ‰ä¸€ä¸ª**å¤šå˜é‡æ—¶é—´åºåˆ—**ï¼Œå› ä¸ºä¸€ä¸ªäº§å“åœ¨å•†åº—ä¸­çš„é”€å”®å¯èƒ½**å½±å“**å¦ä¸€ä¸ªäº§å“çš„é”€å”®ï¼Œæˆ–è€…è¿™åªæ˜¯ä¸€ä¸ªå‡è®¾ã€‚
- en: How to evaluate the forecast?
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•è¯„ä¼°é¢„æµ‹ç»“æœï¼Ÿ
- en: Before we get into the different approaches and models, letâ€™s discuss first
    how to measure the quality of a forecast. This is a regression problem, so we
    will still compare forecasts to true values, here there is no doubt. You can use
    metrics familiar to the regression problem, such as **RMSE**(*Root Mean Squared
    Error*), **MSE**(*Mean Squared Error*), **MAE**(*Mean Absolute Error*), or more
    typical time series metrics such as **MAPE**(*Mean Absolute Percentage Error*),
    **MARRE**(*Mean Absolute Ranged Relative Error*) or **MASE**(*Mean Absolute Scaled
    Error*). Further discussion will use **MAPE** as an evaluation metric. This article
    is not about demand forecasting, but because of the data and my experience, there
    are many references to it. So what metric to choose for that problem you can think.
    Always the metric should reflect the business objective. In this [*article*](/forecast-kpi-rmse-mae-mape-bias-cdc5703d242d),
    [Nicolas Vandeput](https://www.linkedin.com/in/vandeputnicolas/) described the
    metrics used for demand forecasting as KPIs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬æ·±å…¥è®¨è®ºä¸åŒçš„æ–¹æ³•å’Œæ¨¡å‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆè®¨è®ºå¦‚ä½•è¡¡é‡é¢„æµ‹çš„è´¨é‡ã€‚è¿™æ˜¯ä¸€ä¸ªå›å½’é—®é¢˜ï¼Œå› æ­¤æˆ‘ä»¬ä»ç„¶ä¼šå°†é¢„æµ‹ç»“æœä¸çœŸå®å€¼è¿›è¡Œæ¯”è¾ƒï¼Œè¿™ä¸€ç‚¹æ¯«æ— ç–‘é—®ã€‚ä½ å¯ä»¥ä½¿ç”¨å›å½’é—®é¢˜ä¸­ç†Ÿæ‚‰çš„æŒ‡æ ‡ï¼Œå¦‚**RMSE**(*å‡æ–¹æ ¹è¯¯å·®*),
    **MSE**(*å‡æ–¹è¯¯å·®*), **MAE**(*å¹³å‡ç»å¯¹è¯¯å·®*)ï¼Œæˆ–è€…æ›´å…¸å‹çš„æ—¶é—´åºåˆ—æŒ‡æ ‡ï¼Œå¦‚**MAPE**(*å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®*), **MARRE**(*å¹³å‡ç»å¯¹èŒƒå›´ç›¸å¯¹è¯¯å·®*),
    æˆ–**MASE**(*å¹³å‡ç»å¯¹ç¼©æ”¾è¯¯å·®*)ã€‚è¿›ä¸€æ­¥çš„è®¨è®ºå°†ä½¿ç”¨**MAPE**ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ã€‚æœ¬æ–‡ä¸æ˜¯å…³äºéœ€æ±‚é¢„æµ‹çš„ï¼Œä½†ç”±äºæ•°æ®å’Œæˆ‘çš„ç»éªŒï¼Œæœ‰å¾ˆå¤šç›¸å…³å‚è€ƒã€‚å› æ­¤ï¼Œä½ å¯ä»¥è€ƒè™‘ä¸ºè¯¥é—®é¢˜é€‰æ‹©ä»€ä¹ˆæŒ‡æ ‡ã€‚å§‹ç»ˆé€‰æ‹©èƒ½å¤Ÿåæ˜ ä¸šåŠ¡ç›®æ ‡çš„æŒ‡æ ‡ã€‚åœ¨è¿™ç¯‡[*æ–‡ç« *](/forecast-kpi-rmse-mae-mape-bias-cdc5703d242d)ä¸­ï¼Œ[Nicolas
    Vandeput](https://www.linkedin.com/in/vandeputnicolas/) æè¿°äº†éœ€æ±‚é¢„æµ‹ä¸­ä½¿ç”¨çš„ KPI æŒ‡æ ‡ã€‚
- en: We can extend this approach to multiple series and then calculate our metrics
    for all series at once or for each series separately and then aggregate them.
    So letâ€™s move on to how to evaluate a single series. Then this can be extended
    to multiple.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†è¿™ç§æ–¹æ³•æ‰©å±•åˆ°å¤šä¸ªåºåˆ—ï¼Œç„¶åä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰åºåˆ—çš„æŒ‡æ ‡ï¼Œæˆ–è€…åˆ†åˆ«å¯¹æ¯ä¸ªåºåˆ—è®¡ç®—æŒ‡æ ‡åå†è¿›è¡Œæ±‡æ€»ã€‚æ‰€ä»¥è®©æˆ‘ä»¬ç»§ç»­æ¢è®¨å¦‚ä½•è¯„ä¼°å•ä¸ªåºåˆ—ï¼Œç„¶åå†å°†å…¶æ‰©å±•åˆ°å¤šä¸ªåºåˆ—ã€‚
- en: Yes, this is a regression problem, you may wonder why I explain it. In time
    series, time plays a key role. The data are sorted relative to time, and the observations
    are relative to each other. Therefore, it is not possible to use randomization
    when dividing the training/test set and use cross-validation, because in this
    situation there would be data leakage.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªå›å½’é—®é¢˜ï¼Œä½ å¯èƒ½ä¼šæƒ³ä¸ºä»€ä¹ˆæˆ‘è§£é‡Šè¿™ä¸€ç‚¹ã€‚åœ¨æ—¶é—´åºåˆ—ä¸­ï¼Œæ—¶é—´æ‰®æ¼”äº†å…³é”®è§’è‰²ã€‚æ•°æ®æ˜¯ç›¸å¯¹äºæ—¶é—´æ’åºçš„ï¼Œè§‚æµ‹å€¼æ˜¯ç›¸äº’å…³è”çš„ã€‚å› æ­¤ï¼Œåˆ†å‰²è®­ç»ƒ/æµ‹è¯•é›†å¹¶ä½¿ç”¨äº¤å‰éªŒè¯æ—¶æ— æ³•ä½¿ç”¨éšæœºåŒ–ï¼Œå› ä¸ºè¿™æ ·ä¼šå¯¼è‡´æ•°æ®æ³„æ¼ã€‚
- en: '![](../Images/d6eb56fa82acdc8a681e06625ce075fb.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6eb56fa82acdc8a681e06625ce075fb.png)'
- en: Evaluation of the forecasting model, Image by Author
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹æ¨¡å‹çš„è¯„ä¼°ï¼Œå›¾åƒç”±ä½œè€…æä¾›
- en: First, divide the data into a **train** set and a **test** set, easy right?
    The model fits on the train set and tests on the test set. We can divide this
    proportionally, for example, the test set includes 20% of the last weeks, or indicate
    from which date the test set is to be â€” perhaps due to business considerations
    it is important that the test set is the last year.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œå°†æ•°æ®åˆ†ä¸º**è®­ç»ƒ**é›†å’Œ**æµ‹è¯•**é›†ï¼Œå¾ˆç®€å•ï¼Œå¯¹å§ï¼Ÿæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œæ‹Ÿåˆï¼Œåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚æˆ‘ä»¬å¯ä»¥æŒ‰æ¯”ä¾‹åˆ’åˆ†ï¼Œä¾‹å¦‚ï¼Œæµ‹è¯•é›†åŒ…æ‹¬æœ€å 20%
    çš„æ•°æ®ï¼Œæˆ–è€…æŒ‡å®šæµ‹è¯•é›†çš„èµ·å§‹æ—¥æœŸâ€”â€”å¯èƒ½ç”±äºä¸šåŠ¡è€ƒè™‘ï¼Œæµ‹è¯•é›†ä¸ºè¿‡å»ä¸€å¹´æ•°æ®ä¼šæ›´é‡è¦ã€‚
- en: In our example, the test set will be the last year.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæµ‹è¯•é›†å°†æ˜¯æœ€åä¸€å¹´ã€‚
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: From the **train** set, we separate **validation** subsets, which we use to
    select **hyperparameters**. For models implemented in Darts, we can use the *gridsearch*
    method, but for models based on neural networks, the [**Optuna**](https://optuna.org/)
    is recommended. The gridsearch method actually has everything we need and works
    the same regardless of which model we choose.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä»**è®­ç»ƒ**é›†åˆ†ç¦»**éªŒè¯**å­é›†ï¼Œç”¨äºé€‰æ‹©**è¶…å‚æ•°**ã€‚å¯¹äºåœ¨ Darts ä¸­å®ç°çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨*gridsearch*æ–¹æ³•ï¼Œä½†å¯¹äºåŸºäºç¥ç»ç½‘ç»œçš„æ¨¡å‹ï¼Œæ¨èä½¿ç”¨[**Optuna**](https://optuna.org/)ã€‚gridsearch
    æ–¹æ³•å®é™…ä¸Šå…·å¤‡äº†æˆ‘ä»¬æ‰€éœ€çš„ä¸€åˆ‡ï¼Œæ— è®ºé€‰æ‹©å“ªä¸ªæ¨¡å‹éƒ½èƒ½æ­£å¸¸å·¥ä½œã€‚
- en: 'Whatâ€™s important:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦çš„æ˜¯ï¼š
- en: parameters âœ a dictionary with hyperparameters to check
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: parameters âœ ä¸€ä¸ªåŒ…å«å¾…æ£€æŸ¥è¶…å‚æ•°çš„å­—å…¸
- en: series âœ TimeSeries object or a list containing TimeSeries objects (if the algorithm
    is global)
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: series âœ TimeSeries å¯¹è±¡æˆ–åŒ…å« TimeSeries å¯¹è±¡çš„åˆ—è¡¨ï¼ˆå¦‚æœç®—æ³•æ˜¯å…¨å±€çš„ï¼‰
- en: start âœ *Pandas Timestamp* defines when the first forecast will occur or *float*
    as the proportion of observations before the first forecast.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: start âœ *Pandas Timestamp* å®šä¹‰ç¬¬ä¸€æ¬¡é¢„æµ‹å‘ç”Ÿçš„æ—¶é—´ï¼Œæˆ–è€… *float* ä½œä¸ºç¬¬ä¸€æ¬¡é¢„æµ‹ä¹‹å‰è§‚å¯Ÿå€¼çš„æ¯”ä¾‹ã€‚
- en: forecast_horizon âœ number(*int*) of forecast horizons by the model.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: forecast_horizon âœ æ¨¡å‹é¢„æµ‹çš„æ—¶é—´èŒƒå›´æ•°é‡ï¼ˆ*int*ï¼‰
- en: stride âœ Offset between next predictions. To make everything in accordance with
    the art of Data Science and most reflect the actual operation of the algorithm
    then stride should be equal to 1\. But remember, after each step your algorithm
    is retrained and your grid of hyperparameters may have a lot of combinations,
    it often takes ages. Therefore, for common sense reasons, a stride can be higher
    than 1, especially since the purpose of this is to select the best hyperparameters
    and the result may (though not necessarily) be the same for a stride equal to
    1 as for a stride equal to 5.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: stride âœ ä¸‹ä¸€æ¬¡é¢„æµ‹ä¹‹é—´çš„åç§»é‡ã€‚ä¸ºäº†ä½¿ä¸€åˆ‡ç¬¦åˆæ•°æ®ç§‘å­¦çš„è‰ºæœ¯ï¼Œå¹¶æœ€èƒ½åæ˜ ç®—æ³•çš„å®é™…æ“ä½œï¼Œstride åº”è¯¥ç­‰äº1ã€‚ä½†è¯·è®°ä½ï¼Œåœ¨æ¯ä¸€æ­¥ä¹‹åä½ çš„ç®—æ³•éƒ½ä¼šé‡æ–°è®­ç»ƒï¼Œè€Œä¸”ä½ çš„è¶…å‚æ•°ç½‘æ ¼å¯èƒ½æœ‰å¾ˆå¤šç»„åˆï¼Œè¿™é€šå¸¸éœ€è¦å¾ˆé•¿æ—¶é—´ã€‚å› æ­¤ï¼Œå‡ºäºå¸¸è¯†è€ƒè™‘ï¼Œstride
    å¯ä»¥å¤§äº1ï¼Œå°¤å…¶æ˜¯å› ä¸ºæ­¤ç›®çš„åœ¨äºé€‰æ‹©æœ€ä½³è¶…å‚æ•°ï¼Œç»“æœå¯èƒ½ï¼ˆä½†ä¸ä¸€å®šï¼‰åœ¨ stride ç­‰äº1 æ—¶ä¸ stride ç­‰äº5 æ—¶ç›¸åŒã€‚
- en: metric âœ function, which compares true values and predictions. The best hyperparameters
    are then selected based on this metric.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: metric âœ å‡½æ•°ï¼Œç”¨äºæ¯”è¾ƒçœŸå®å€¼å’Œé¢„æµ‹å€¼ã€‚ç„¶åæ ¹æ®è¿™ä¸ªæŒ‡æ ‡é€‰æ‹©æœ€ä½³è¶…å‚æ•°ã€‚
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: So how to evaluate the model on the test set? We have 2 approaches.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆå¦‚ä½•åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹å‘¢ï¼Ÿæˆ‘ä»¬æœ‰ä¸¤ç§æ–¹æ³•ã€‚
- en: The **first** is that we fit our model on the train set and make one forecast
    that covers the test set. In the following examples, we will use this option â€”
    it is simply faster to calculate since we generate one forecast for each time
    series.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¬¬ä¸€ç§**æ–¹æ³•æ˜¯æˆ‘ä»¬åœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆæ¨¡å‹ï¼Œå¹¶åšä¸€ä¸ªè¦†ç›–æµ‹è¯•é›†çš„é¢„æµ‹ã€‚åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªé€‰é¡¹â€”â€”è®¡ç®—é€Ÿåº¦æ›´å¿«ï¼Œå› ä¸ºæˆ‘ä»¬ä¸ºæ¯ä¸ªæ—¶é—´åºåˆ—ç”Ÿæˆä¸€ä¸ªé¢„æµ‹ã€‚'
- en: The **second** approach is that we test different forecast horizons. We fit
    the model, make a forecast, retrain, make a forecast, and so on, but we have to
    start training the algorithm first before the end of the train set, and thatâ€™s
    so that we can compare horizons on the same data â€” weâ€™re comparing apples to apples.
    If you were to start by training on the entire train set then the further horizons
    have fewer observations. In this method, we should retrain (especially in local
    models), so we definitely have more calculations in this approach.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¬¬äºŒç§**æ–¹æ³•æ˜¯æˆ‘ä»¬æµ‹è¯•ä¸åŒçš„é¢„æµ‹æ—¶é—´èŒƒå›´ã€‚æˆ‘ä»¬æ‹Ÿåˆæ¨¡å‹ï¼Œåšä¸€ä¸ªé¢„æµ‹ï¼Œé‡æ–°è®­ç»ƒï¼Œå†åšä¸€ä¸ªé¢„æµ‹ï¼Œä»¥æ­¤ç±»æ¨ï¼Œä½†æˆ‘ä»¬å¿…é¡»åœ¨è®­ç»ƒé›†ç»“æŸä¹‹å‰å¼€å§‹è®­ç»ƒç®—æ³•ï¼Œè¿™æ ·æˆ‘ä»¬æ‰èƒ½åœ¨ç›¸åŒçš„æ•°æ®ä¸Šæ¯”è¾ƒæ—¶é—´èŒƒå›´â€”â€”æˆ‘ä»¬åœ¨æ¯”è¾ƒç›¸åŒçš„è‹¹æœã€‚å¦‚æœä½ ä»æ•´ä¸ªè®­ç»ƒé›†å¼€å§‹è®­ç»ƒï¼Œé‚£ä¹ˆè¾ƒè¿œçš„æ—¶é—´èŒƒå›´æœ‰è¾ƒå°‘çš„è§‚å¯Ÿå€¼ã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬åº”è¯¥é‡æ–°è®­ç»ƒï¼ˆç‰¹åˆ«æ˜¯åœ¨å±€éƒ¨æ¨¡å‹ä¸­ï¼‰ï¼Œå› æ­¤è¿™ç§æ–¹æ³•ä¸­æˆ‘ä»¬è‚¯å®šä¼šæœ‰æ›´å¤šçš„è®¡ç®—ã€‚'
- en: 'Itâ€™s also simple because the models in darts have the ability to return historical
    predictions according to the above visualization. You have already learned about
    some of the variables in the gridsearch method, which work the same here. However,
    there will be new variables here:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¹Ÿå¾ˆç®€å•ï¼Œå› ä¸º darts ä¸­çš„æ¨¡å‹å…·æœ‰æ ¹æ®ä¸Šè¿°å¯è§†åŒ–è¿”å›å†å²é¢„æµ‹çš„èƒ½åŠ›ã€‚ä½ å·²ç»å­¦ä¹ äº†ä¸€äº›åœ¨ gridsearch æ–¹æ³•ä¸­ä½¿ç”¨çš„å˜é‡ï¼Œè¿™äº›å˜é‡åœ¨è¿™é‡Œä¹Ÿé€‚ç”¨ã€‚ç„¶è€Œï¼Œè¿™é‡Œä¼šæœ‰æ–°çš„å˜é‡ï¼š
- en: retrain âœ if equal to *True*, then after each step the model is retrained, which
    best reflects reality.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: retrain âœ å¦‚æœç­‰äº *True*ï¼Œåˆ™åœ¨æ¯ä¸€æ­¥ä¹‹åæ¨¡å‹ä¼šé‡æ–°è®­ç»ƒï¼Œè¿™æœ€èƒ½åæ˜ ç°å®æƒ…å†µã€‚
- en: overlap_end âœ if equal to *True*, then the predictions may extend beyond the
    dates in the test set. Useful if, we are doing a forecast for several horizons
    and the farther ones are beyond the test set and the closer ones are not.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: overlap_end âœ å¦‚æœç­‰äº *True*ï¼Œåˆ™é¢„æµ‹å¯èƒ½è¶…å‡ºæµ‹è¯•é›†ä¸­çš„æ—¥æœŸã€‚å¦‚æœæˆ‘ä»¬å¯¹å¤šä¸ªæ—¶é—´èŒƒå›´è¿›è¡Œé¢„æµ‹ï¼Œè€Œè¾ƒè¿œçš„æ—¶é—´èŒƒå›´è¶…å‡ºæµ‹è¯•é›†ï¼Œè€Œè¾ƒè¿‘çš„æ—¶é—´èŒƒå›´ä¸è¶…å‡ºï¼Œè¿™æ ·çš„è®¾ç½®å¾ˆæœ‰ç”¨ã€‚
- en: last_points_only âœ if equal to *False*, then the forecast for all horizons is
    returned.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: last_points_only âœ å¦‚æœç­‰äº *False*ï¼Œåˆ™è¿”å›æ‰€æœ‰æ—¶é—´èŒƒå›´çš„é¢„æµ‹ã€‚
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: However, we want to take forecasts from the interested us horizon, and in the
    variable backtests_results, there are forecasts from different points in time.
    To take forecasts from the specific horizon then you can use my function *take_backtest_horizon*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæˆ‘ä»¬å¸Œæœ›ä»æ„Ÿå…´è¶£çš„æ—¶é—´èŒƒå›´ä¸­è·å–é¢„æµ‹ï¼Œå¹¶ä¸”åœ¨å˜é‡ backtests_results ä¸­ï¼Œæœ‰æ¥è‡ªä¸åŒæ—¶é—´ç‚¹çš„é¢„æµ‹ã€‚è¦ä»ç‰¹å®šçš„æ—¶é—´èŒƒå›´ä¸­è·å–é¢„æµ‹ï¼Œä½ å¯ä»¥ä½¿ç”¨æˆ‘çš„å‡½æ•°
    *take_backtest_horizon*ã€‚
- en: backtests_5W âœ there are forecasts for each point of the test set that were
    made 5 weeks earlier.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: backtests_5W âœ æœ‰é’ˆå¯¹æµ‹è¯•é›†æ¯ä¸ªç‚¹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹æ˜¯åœ¨5å‘¨å‰åšå‡ºçš„ã€‚
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Variables used
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨çš„å˜é‡
- en: In fact, the time series themselves are not fully explainable by themselves.
    Often, time series are dependent on other variables. Here we donâ€™t have them,
    but good to know how to distinguish them when they will appear in your projects.
    If we do not inform the model about an upcoming promotion/lower price then it
    will not be able to forecast increased sales. If you want to forecast the change
    in the price of a stock company, helpful can be added variables from technical
    or fundamental analysis. These variables are dependent on the price, that is,
    you can only know these indicators for the past, after all, we do not know the
    companyâ€™s financial reports in the future or its price â€” we want to forecast it
    :)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œæ—¶é—´åºåˆ—æœ¬èº«å¹¶ä¸èƒ½å®Œå…¨è‡ªæˆ‘è§£é‡Šã€‚æ—¶é—´åºåˆ—é€šå¸¸ä¾èµ–äºå…¶ä»–å˜é‡ã€‚è¿™é‡Œæˆ‘ä»¬æ²¡æœ‰è¿™äº›å˜é‡ï¼Œä½†äº†è§£å¦‚ä½•åœ¨é¡¹ç›®ä¸­é‡åˆ°è¿™äº›å˜é‡æ—¶è¿›è¡ŒåŒºåˆ†æ˜¯å¾ˆæœ‰å¸®åŠ©çš„ã€‚å¦‚æœæˆ‘ä»¬æ²¡æœ‰é€šçŸ¥æ¨¡å‹å³å°†åˆ°æ¥çš„ä¿ƒé”€/é™ä»·ï¼Œé‚£ä¹ˆå®ƒå°†æ— æ³•é¢„æµ‹é”€å”®é¢çš„å¢åŠ ã€‚å¦‚æœä½ æƒ³é¢„æµ‹è‚¡ç¥¨å…¬å¸çš„ä»·æ ¼å˜åŒ–ï¼Œæ·»åŠ æ¥è‡ªæŠ€æœ¯åˆ†ææˆ–åŸºæœ¬é¢åˆ†æçš„å˜é‡å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ã€‚è¿™äº›å˜é‡ä¾èµ–äºä»·æ ¼ï¼Œå³ä½ åªèƒ½çŸ¥é“è¿™äº›æŒ‡æ ‡çš„è¿‡å»å€¼ï¼Œæ¯•ç«Ÿï¼Œæˆ‘ä»¬æ— æ³•çŸ¥é“æœªæ¥çš„å…¬å¸è´¢åŠ¡æŠ¥å‘Šæˆ–ä»·æ ¼â€”â€”æˆ‘ä»¬å¸Œæœ›è¿›è¡Œé¢„æµ‹
    :)
- en: '![](../Images/fdfdefebca39dab3d6d4fc34c837df1a.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fdfdefebca39dab3d6d4fc34c837df1a.png)'
- en: Variables used by algorithms for time series, Image by Author
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¶é—´åºåˆ—ç®—æ³•ä½¿ç”¨çš„å˜é‡ï¼Œå›¾æºä½œè€…
- en: We can have variables that are also time series, that is, for different points
    in time they have different values, and we can have **static covariates** (constant
    over time), usually categorical variables. In our example that will be store ID
    and item ID. They are very relevant to global models. Because in 1 out of 100
    time series, you may have different relationships, and thanks to this variable
    your model can distinguish between time series.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥æœ‰ä¹Ÿæ˜¯æ—¶é—´åºåˆ—çš„å˜é‡ï¼Œå³åœ¨ä¸åŒæ—¶é—´ç‚¹æœ‰ä¸åŒçš„å€¼ï¼ŒåŒæ—¶æˆ‘ä»¬ä¹Ÿå¯ä»¥æœ‰**é™æ€åå˜é‡**ï¼ˆéšæ—¶é—´ä¿æŒä¸å˜ï¼‰ï¼Œé€šå¸¸æ˜¯åˆ†ç±»å˜é‡ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œè¿™å°†æ˜¯å•†åº—IDå’Œå•†å“IDã€‚å®ƒä»¬å¯¹å…¨å±€æ¨¡å‹éå¸¸é‡è¦ã€‚å› ä¸ºåœ¨æ¯100ä¸ªæ—¶é—´åºåˆ—ä¸­ï¼Œå¯èƒ½ä¼šæœ‰ä¸åŒçš„å…³ç³»ï¼Œå€ŸåŠ©è¿™äº›å˜é‡ï¼Œä½ çš„æ¨¡å‹å¯ä»¥åŒºåˆ†ä¸åŒçš„æ—¶é—´åºåˆ—ã€‚
- en: As for variables that are time series, we can distinguish between **covariates**
    that are also **known for the future** (for example we can know what the promotional
    mechanisms are in the future and also we know them for the past) and **covariates
    known only for the past** (we can know what price competing products had, but
    we do not know what they will have in the future).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è‡³äºæ—¶é—´åºåˆ—å˜é‡ï¼Œæˆ‘ä»¬å¯ä»¥åŒºåˆ†**åå˜é‡**ï¼Œå…¶ä¸­æœ‰äº›**å·²çŸ¥æœªæ¥**ï¼ˆä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥çŸ¥é“æœªæ¥çš„ä¿ƒé”€æœºåˆ¶ï¼Œä¹ŸçŸ¥é“è¿‡å»çš„ä¿ƒé”€æœºåˆ¶ï¼‰å’Œ**ä»…å·²çŸ¥è¿‡å»çš„åå˜é‡**ï¼ˆæˆ‘ä»¬å¯ä»¥çŸ¥é“ç«äº‰äº§å“çš„ä»·æ ¼ï¼Œä½†ä¸çŸ¥é“å®ƒä»¬æœªæ¥çš„ä»·æ ¼ï¼‰ã€‚
- en: Models used
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨çš„æ¨¡å‹
- en: '![](../Images/dd185e633c9b5ffbdd0e2c15162d7077.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd185e633c9b5ffbdd0e2c15162d7077.png)'
- en: Local vs Global algorithms, Image by Author
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å±€éƒ¨ç®—æ³•ä¸å…¨å±€ç®—æ³•å¯¹æ¯”ï¼Œå›¾æºä½œè€…
- en: We can separate machine learning into supervised learning, unsupervised learning,
    and reinforcement learning. When we get into the details, we can divide supervised
    learning into regression and classification. We can make a similar split for time
    series forecasting, which is using **local** or **global** algorithms to forecast
    these time series.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†æœºå™¨å­¦ä¹ åˆ†ä¸ºç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚è¿›å…¥è¯¦ç»†å†…å®¹åï¼Œæˆ‘ä»¬å¯ä»¥å°†ç›‘ç£å­¦ä¹ åˆ†ä¸ºå›å½’å’Œåˆ†ç±»ã€‚æˆ‘ä»¬å¯ä»¥å¯¹æ—¶é—´åºåˆ—é¢„æµ‹è¿›è¡Œç±»ä¼¼çš„åˆ’åˆ†ï¼Œå³ä½¿ç”¨**å±€éƒ¨**æˆ–**å…¨å±€**ç®—æ³•æ¥é¢„æµ‹è¿™äº›æ—¶é—´åºåˆ—ã€‚
- en: The **local algorithm** is fitted on a **single time series**, and only this
    time series can the model predict. More time series mean more models. Here we
    see pros and cons, simple models, but for many time series, this approach becomes
    difficult to maintain.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**å±€éƒ¨ç®—æ³•**æ˜¯é’ˆå¯¹**å•ä¸€æ—¶é—´åºåˆ—**è¿›è¡Œæ‹Ÿåˆçš„ï¼Œæ¨¡å‹ä»…èƒ½é¢„æµ‹è¿™ä¸€æ—¶é—´åºåˆ—ã€‚æ›´å¤šçš„æ—¶é—´åºåˆ—æ„å‘³ç€æ›´å¤šçš„æ¨¡å‹ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬çœ‹åˆ°ä¼˜ç¼ºç‚¹ï¼Œæ¨¡å‹ç®€å•ï¼Œä½†å¯¹äºè®¸å¤šæ—¶é—´åºåˆ—æ¥è¯´ï¼Œè¿™ç§æ–¹æ³•å˜å¾—éš¾ä»¥ç»´æŠ¤ã€‚'
- en: The **global algorithm**, on the other hand, is that **one model** can be fitted
    on **multiple time series**. So if we have **multiple time series** then we can
    have **one** model that can forecast them all. This approach is definitely more
    flexible, for example, you could use Transfer Learning. For time series it means
    that you fit the model on a different time series than you are making the prediction.
    [Here](https://unit8co.github.io/darts/examples/14-transfer-learning.html) is
    an example of this usage. One more important point related to global models, because
    I could forget it later and it is really important â€” that is, **time series scaling**.
    The most common approach is *MinMaxScaler*, but you can use something more specific
    for your data. Nevertheless, Iâ€™m not going to write here how you can scale time
    series, thatâ€™s definitely a topic for another article. Letâ€™s consider why we should
    scale our time series. The answer may be simple, many global algorithms are neural
    networks and it is a reason why we scale data, as we do with pixels for convolutional
    neural networks. However, this is not the full picture. However, we can use a
    model like Random Forest (nonparametric models) and we still should scale them.
    But STOP, why? After all, for these types of models, you donâ€™t need to scale the
    variables. The reason we should scale the time series is so that the model learns
    the relationship, not the scale, for example, for seasonality such a relationship
    might be that in the summer months, the value is on average 150% higher than in
    the winter months. Another example would be that after 3 significant increases
    are followed by a decrease. It is difficult to learn these relationships by the
    model if we do not scale time series. This is a slightly different approach to
    scaling a variable for tabular data because here we scale each time series separately.
    If we use the mentioned *MinMaxScaler* then for each time series from the train
    set the maximum value is 1\. So letâ€™s scale our data, which will be used by global
    models.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In a moment you will read about the most popular **local** and **global** algorithms.
    It is impossible to describe all possible algorithms, but there are a few that
    are often used by specialists and usually meet expectations.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: No free lunch theorem
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There is no answer â€” this model is the best, do not use others. Nevertheless,
    if you are creating an MVP â€” it is best to start with something simple.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: In the following examples, I do not choose the best hyperparameters on validation
    sets, I use default models. So if you write to me that the model may have better
    results â€” **I already agree with you**.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Local models
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we move on to specific local models, I have prepared functions for you
    to use multiprocessing for faster calculations using all Cores.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: I am going to use this function for local models to generate a single prediction
    for the test set. Also, you can use it to generate multiple historical forecasts
    (the second approach, variable ***single_forecast*** should be ***False***). Nevertheless,
    I do not do that here, because it would take a lot of time.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: If you use Cluster and Spark then you can use Spark UDF and speed up the calculations
    significantly.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: I know, you probably want to move to the models part. Last but not least â€” a
    function to evaluate our forecasts. I am going to use **MAPE** as a metric for
    evaluation, however, if you work on a demand forecasting project then WMAPE or
    MAE is definitely closer to business expectations.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Baseline
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Well, okay, but why make a neural network if a better idea is to forecast the
    value from a year ago? Thatâ€™s exactly why we create such a model first. When you
    work with real data, youâ€™d better start with such an approach too (it can also
    be the last value from the training set, NaiveDrift if thereâ€™s a trend or a combination
    of several simple methods). Then if you move on to more advanced methods you can
    evaluate how much better it is than the simpler ones, because if, for example,
    you start with a neural network and its MAPE is 10% then my question (probably
    also stakeholders) is â€” *Is it good?*
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Our models(one ts=one model) below will repeat the value from a year ago.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**overall MAPE: 22.42%**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Now letâ€™s visualize the forecast and true value on a test set for time series
    with the highest total sales.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c37de66f2d835d976d5c5f681f16803.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: It doesnâ€™t look too bad.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: ARIMA
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/3c037dbaa86cd65df7dd14078eb1c315.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: Visualize how the ARIMA algorithm works, Image by Author
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '**ARIMA** is a statistical model, both popular and powerful in its simplicity.
    When you hear **ARIMA** it can mean this one model but also a collection of models
    that are extensions of just **ARIMA**. For this collection, we can include **ARIMAX**
    (takes into account additional variables), **SARIMA** (takes into account seasonality),
    or **VARIMA** (for multivariate time series). But letâ€™s go back to **ARIMA** (**A**uto**R**egressive
    **I**ntegrated **M**oving **A**verage), which is where it all starts. If you understand
    it well then you should have no problem using the mentioned before models.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Many articles have already been written about this algorithm. I would like to
    give you the intuition behind this model. I hope after all you will easily implement
    it in code and understand how it works. I am going to start from the end. I myself
    remember having a question about this during recruitment and at the time I didnâ€™t
    understand it yet. The **ARMA model can only work with stationary time series**,
    so we have a component â€” **Integration** (**I**), which often (not always) **transforms
    a non-stationary into a stationary time series**. **ARMA** is the model, while
    **I** part is responsible for preparing the data for modeling, if necessary of
    course. There are a few questions you should know the answer to them, what it
    means that a time series is stationary or not, and what kind of transformation
    the **Integration** (**I**) component makes. So letâ€™s start with stationarity.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šæ–‡ç« å·²ç»å¯¹æ­¤ç®—æ³•è¿›è¡Œäº†é˜è¿°ã€‚æˆ‘æƒ³ç»™ä½ è¿™ä¸ªæ¨¡å‹èƒŒåçš„ç›´è§‚ç†è§£ã€‚æˆ‘å¸Œæœ›æœ€ç»ˆä½ èƒ½è½»æ¾åœ°åœ¨ä»£ç ä¸­å®ç°å®ƒï¼Œå¹¶ç†è§£å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚æˆ‘æ‰“ç®—ä»æœ€åå¼€å§‹ã€‚æˆ‘è‡ªå·±è®°å¾—åœ¨æ‹›è˜è¿‡ç¨‹ä¸­æœ‰è¿‡ä¸€ä¸ªå…³äºè¿™ä¸ªé—®é¢˜çš„ç–‘é—®ï¼Œå½“æ—¶æˆ‘è¿˜æ²¡æœ‰ç†è§£å®ƒã€‚**ARMAæ¨¡å‹åªèƒ½ä¸å¹³ç¨³æ—¶é—´åºåˆ—ä¸€èµ·ä½¿ç”¨**ï¼Œå› æ­¤æˆ‘ä»¬æœ‰ä¸€ä¸ªç»„ä»¶â€”â€”**ç§¯åˆ†**ï¼ˆ**I**ï¼‰ï¼Œå®ƒé€šå¸¸ï¼ˆä½†ä¸æ€»æ˜¯ï¼‰**å°†éå¹³ç¨³æ—¶é—´åºåˆ—è½¬æ¢ä¸ºå¹³ç¨³æ—¶é—´åºåˆ—**ã€‚**ARMA**æ˜¯æ¨¡å‹ï¼Œè€Œ**I**éƒ¨åˆ†è´Ÿè´£ä¸ºå»ºæ¨¡å‡†å¤‡æ•°æ®ï¼ˆå½“ç„¶ï¼Œå¦‚æœéœ€è¦çš„è¯ï¼‰ã€‚ä½ åº”è¯¥çŸ¥é“å‡ ä¸ªé—®é¢˜çš„ç­”æ¡ˆï¼Œé‚£å°±æ˜¯æ—¶é—´åºåˆ—å¹³ç¨³æˆ–éå¹³ç¨³çš„å«ä¹‰ï¼Œä»¥åŠ**ç§¯åˆ†**ï¼ˆ**I**ï¼‰ç»„ä»¶è¿›è¡Œçš„è½¬æ¢ç±»å‹ã€‚æ‰€ä»¥è®©æˆ‘ä»¬ä»å¹³ç¨³æ€§å¼€å§‹ã€‚
- en: If the distribution of values (mean and variance) is invariant over time then
    the time series is stationary.
  id: totrans-118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¦‚æœå€¼çš„åˆ†å¸ƒï¼ˆå‡å€¼å’Œæ–¹å·®ï¼‰åœ¨æ—¶é—´ä¸Šæ˜¯ä¸å˜çš„ï¼Œé‚£ä¹ˆæ—¶é—´åºåˆ—å°±æ˜¯å¹³ç¨³çš„ã€‚
- en: Therefore, if there is a trend and/or seasonality then a time series is non-stationary.
    To check whether your time series is stationary then the simplest method is to
    visualize it and on the graph also add the moving average and moving standard
    deviation. If they are constant over time (or close to constant) then you can
    conclude that your time series is stationary. This approach may seem naive and
    doesnâ€™t always work, because given too large a window into the rolling statistics
    you may think your time series is stationary, when in fact it is not. Another
    way is to split your time series into random partitions, for each partition calculate
    the mentioned statistics. The last method is to calculate the **A**ugmented **D**ickey-**F**uller
    (**ADF**) test. What if our time series is not stationary and we need to use the
    **Integration** (**I**) component? It makes the time series stationary (but not
    always, there are time series that canâ€™t be stationary) using **differencing**,
    that is, calculating the differences between observations. What if our time series
    isnâ€™t still stationary? We can select order **d**, which means how many times
    we are differencing time series.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå¦‚æœå­˜åœ¨è¶‹åŠ¿å’Œ/æˆ–å­£èŠ‚æ€§ï¼Œé‚£ä¹ˆæ—¶é—´åºåˆ—å°±æ˜¯éå¹³ç¨³çš„ã€‚è¦æ£€æŸ¥æ—¶é—´åºåˆ—æ˜¯å¦å¹³ç¨³ï¼Œæœ€ç®€å•çš„æ–¹æ³•æ˜¯å°†å…¶å¯è§†åŒ–ï¼Œå¹¶åœ¨å›¾ä¸Šæ·»åŠ ç§»åŠ¨å¹³å‡å’Œç§»åŠ¨æ ‡å‡†å·®ã€‚å¦‚æœå®ƒä»¬éšæ—¶é—´ä¿æŒä¸å˜ï¼ˆæˆ–æ¥è¿‘ä¸å˜ï¼‰ï¼Œé‚£ä¹ˆä½ å¯ä»¥å¾—å‡ºä½ çš„æ—¶é—´åºåˆ—æ˜¯å¹³ç¨³çš„ç»“è®ºã€‚è¿™ç§æ–¹æ³•å¯èƒ½æ˜¾å¾—å¤©çœŸï¼Œå¹¶ä¸”å¹¶ä¸æ€»æ˜¯æœ‰æ•ˆï¼Œå› ä¸ºå¦‚æœå¯¹æ»šåŠ¨ç»Ÿè®¡æ•°æ®ä½¿ç”¨è¿‡å¤§çš„çª—å£ï¼Œä½ å¯èƒ½ä¼šè®¤ä¸ºæ—¶é—´åºåˆ—æ˜¯å¹³ç¨³çš„ï¼Œè€Œå®é™…ä¸Šå¹¶éå¦‚æ­¤ã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯å°†æ—¶é—´åºåˆ—æ‹†åˆ†æˆéšæœºåˆ†åŒºï¼Œå¯¹æ¯ä¸ªåˆ†åŒºè®¡ç®—ä¸Šè¿°ç»Ÿè®¡æ•°æ®ã€‚æœ€åä¸€ç§æ–¹æ³•æ˜¯è®¡ç®—**æ‰©å±•çš„è¿ªåŸº-ç¦å‹’**ï¼ˆ**ADF**ï¼‰æ£€éªŒã€‚å¦‚æœæˆ‘ä»¬çš„æ—¶é—´åºåˆ—ä»ç„¶ä¸æ˜¯å¹³ç¨³çš„ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨**ç§¯åˆ†**ï¼ˆ**I**ï¼‰ç»„ä»¶æ€ä¹ˆåŠï¼Ÿå®ƒé€šè¿‡**å·®åˆ†**æ¥ä½¿æ—¶é—´åºåˆ—å¹³ç¨³ï¼Œå³è®¡ç®—è§‚å¯Ÿå€¼ä¹‹é—´çš„å·®å¼‚ã€‚å¦‚æœæˆ‘ä»¬çš„æ—¶é—´åºåˆ—ä»ç„¶ä¸æ˜¯å¹³ç¨³çš„å‘¢ï¼Ÿæˆ‘ä»¬å¯ä»¥é€‰æ‹©**d**çš„é˜¶æ•°ï¼Œè¿™è¡¨ç¤ºæˆ‘ä»¬å¯¹æ—¶é—´åºåˆ—è¿›è¡Œå·®åˆ†çš„æ¬¡æ•°ã€‚
- en: This long fragment was about **Integration** (**I**) which prepares data that
    will be used by the **Autoregressive** (**AR**) and **Moving Averag**e (**MA**)
    components. **AR** is a linear regression of the last **p** values, known as lags.
    The current value is correlated with and dependent on the last values. **MA**
    is complementary and takes into account the **q** last errors in the forecast
    (assumed to be white noise) to better forecast the current point in time.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé•¿æ®µè½æ˜¯å…³äº**ç§¯åˆ†**ï¼ˆ**I**ï¼‰ï¼Œå®ƒå‡†å¤‡æ•°æ®ä»¥ä¾›**è‡ªå›å½’**ï¼ˆ**AR**ï¼‰å’Œ**ç§»åŠ¨å¹³å‡**ï¼ˆ**MA**ï¼‰ç»„ä»¶ä½¿ç”¨ã€‚**AR**æ˜¯å¯¹æœ€å**p**ä¸ªå€¼çš„çº¿æ€§å›å½’ï¼Œè¿™äº›å€¼è¢«ç§°ä¸ºæ»åã€‚å½“å‰å€¼ä¸æœ€åçš„å€¼ç›¸å…³è”ï¼Œå¹¶ä¸”ä¾èµ–äºè¿™äº›å€¼ã€‚**MA**æ˜¯è¡¥å……æ€§çš„ï¼Œè€ƒè™‘äº†é¢„æµ‹ä¸­**q**ä¸ªæœ€åçš„è¯¯å·®ï¼ˆå‡è®¾ä¸ºç™½å™ªå£°ï¼‰ï¼Œä»¥æ›´å¥½åœ°é¢„æµ‹å½“å‰çš„æ—¶é—´ç‚¹ã€‚
- en: To select the order **p** for **AR** we use [**PACF**](https://en.wikipedia.org/wiki/Partial_autocorrelation_function)
    (**P**artial **A**uto**C**orrelation **F**unction), while to select the order
    **q** for **MA** we use [**ACF**](https://en.wikipedia.org/wiki/Autocorrelation)
    (**A**uto**C**orrelation **F**unction). Outside of university classes, we are
    unlikely to do this in practice because we have [**AutoARIMA**](https://unit8co.github.io/darts/generated_api/darts.models.forecasting.auto_arima.html)
    which selects the **p**, **d**, and **q** for us.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: é€‰æ‹©**AR**çš„é˜¶æ•°**p**æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨[**PACF**](https://en.wikipedia.org/wiki/Partial_autocorrelation_function)ï¼ˆ**P**artial
    **A**uto**C**orrelation **F**unctionï¼‰ï¼Œè€Œé€‰æ‹©**MA**çš„é˜¶æ•°**q**æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨[**ACF**](https://en.wikipedia.org/wiki/Autocorrelation)ï¼ˆ**A**uto**C**orrelation
    **F**unctionï¼‰ã€‚åœ¨å¤§å­¦è¯¾ç¨‹ä¹‹å¤–ï¼Œæˆ‘ä»¬åœ¨å®é™…æ“ä½œä¸­ä¸å¤ªå¯èƒ½è¿™æ ·åšï¼Œå› ä¸ºæˆ‘ä»¬æœ‰[**AutoARIMA**](https://unit8co.github.io/darts/generated_api/darts.models.forecasting.auto_arima.html)å¯ä»¥ä¸ºæˆ‘ä»¬é€‰æ‹©**p**ã€**d**å’Œ**q**ã€‚
- en: Letâ€™s go back to practice and implement in a similar way as we did with the
    Baseline model. As you might have read earlier thanks to the Darts library it
    is simple.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›åˆ°å®è·µä¸­ï¼ŒæŒ‰ç…§ä¸åŸºçº¿æ¨¡å‹ç›¸ä¼¼çš„æ–¹å¼æ¥å®ç°ã€‚æ­£å¦‚ä½ å¯èƒ½å·²ç»äº†è§£åˆ°çš„ï¼Œç”±äº Darts åº“ï¼Œè¿™ä¸ªè¿‡ç¨‹éå¸¸ç®€å•ã€‚
- en: '[PRE15]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**overall MAPE: 28.18%**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ€»ä½“ MAPE: 28.18%**'
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../Images/31546af743b6b1fa56efe95d9cdccb78.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31546af743b6b1fa56efe95d9cdccb78.png)'
- en: It would probably look better if I chose the parameter *m* (The period for seasonal
    differencing). You can try it and give me feedback about how the results changed.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘é€‰æ‹©å‚æ•°*m*ï¼ˆå­£èŠ‚æ€§å·®åˆ†çš„å‘¨æœŸï¼‰ï¼Œç»“æœå¯èƒ½ä¼šæ›´å¥½ã€‚ä½ å¯ä»¥å°è¯•ä¸€ä¸‹ï¼Œå¹¶å‘Šè¯‰æˆ‘ç»“æœçš„å˜åŒ–æƒ…å†µã€‚
- en: Exponential smoothing
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æŒ‡æ•°å¹³æ»‘
- en: '![](../Images/8cd796b93178f570b18ddeb944b0474a.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8cd796b93178f570b18ddeb944b0474a.png)'
- en: Visualize how the Exponential Smoothing (ETS) algorithm works, Image by Author
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: å¯è§†åŒ–æŒ‡æ•°å¹³æ»‘ï¼ˆETSï¼‰ç®—æ³•çš„å·¥ä½œåŸç†ï¼Œå›¾ç‰‡ç”±ä½œè€…æä¾›
- en: '**Exponential smoothing** is another **family** of similar models for univariate
    time series. You can find â€˜this familyâ€™ under the term **ETS** (**E**-Error, **T**-Trend,
    **S**-Seasonal). In this method, observations are weighted, for older observations
    are lower weights, because they decrease exponentially. We can distinguish between
    three types, a simple one that assumes the future will be similar to recent values,
    an extension that handles the trend, and the last one that also handles seasonality.
    I am going to describe these three types in a moment, but a small aside now. In
    the **M**-4 **Competition** ([Makridakis Competitions](https://en.wikipedia.org/wiki/Makridakis_Competitions),
    the most well-known competition for time series forecasters) Slawek Smyl won,
    he proposed [**ES-RNN**](https://www.sciencedirect.com/science/article/abs/pii/S0169207019301153),
    which is a hybrid between **E**xponential **S**moothing and **R**ecurrent **N**eural
    **N**etwork.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**æŒ‡æ•°å¹³æ»‘**æ˜¯å¦ä¸€ç§ç”¨äºå•å˜é‡æ—¶é—´åºåˆ—çš„**æ¨¡å‹å®¶æ—**ã€‚ä½ å¯ä»¥åœ¨æœ¯è¯­**ETS**ï¼ˆ**E**-è¯¯å·®ï¼Œ**T**-è¶‹åŠ¿ï¼Œ**S**-å­£èŠ‚æ€§ï¼‰ä¸‹æ‰¾åˆ°â€œè¿™ä¸ªå®¶æ—â€ã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œè§‚å¯Ÿå€¼è¢«èµ‹äºˆæƒé‡ï¼Œè¾ƒæ—§çš„è§‚å¯Ÿå€¼æƒé‡è¾ƒä½ï¼Œå› ä¸ºå®ƒä»¬æŒ‰æŒ‡æ•°è¡°å‡ã€‚æˆ‘ä»¬å¯ä»¥åŒºåˆ†ä¸‰ç§ç±»å‹ï¼šä¸€ç§ç®€å•çš„ç±»å‹å‡è®¾æœªæ¥å°†ç±»ä¼¼äºè¿‘æœŸå€¼ï¼Œä¸€ç§æ‰©å±•ç±»å‹å¤„ç†è¶‹åŠ¿ï¼Œæœ€åä¸€ç§è¿˜å¤„ç†å­£èŠ‚æ€§ã€‚æˆ‘å°†ä¼šåœ¨ç¨åæè¿°è¿™ä¸‰ç§ç±»å‹ï¼Œä¸è¿‡ç°åœ¨å…ˆæ’ä¸€ä¸ªå°æ’æ›²ã€‚åœ¨**M**-4
    **æ¯”èµ›**ï¼ˆ[Makridakis æ¯”èµ›](https://en.wikipedia.org/wiki/Makridakis_Competitions)ï¼Œè¿™æ˜¯æœ€è‘—åçš„æ—¶é—´åºåˆ—é¢„æµ‹æ¯”èµ›ï¼‰ä¸­ï¼ŒSlawek
    Smyl è·èƒœï¼Œä»–æå‡ºäº†[**ES-RNN**](https://www.sciencedirect.com/science/article/abs/pii/S0169207019301153)ï¼Œè¿™æ˜¯**æŒ‡æ•°å¹³æ»‘**ä¸**é€’å½’ç¥ç»ç½‘ç»œ**çš„æ··åˆä½“ã€‚'
- en: Now weâ€™re back to the topic and the first type, which is **Simple Exponential
    Smoothing**. As a Baseline model, we can choose a model that always predicts the
    last value from the training set, a bit naive approach but can give us good results.
    Another approach could be to calculate the average of the whole training set,
    but then the same importance is given to the recent as the oldest observation.
    Exponential Smoothing is a combination of these two approaches, where greater
    weights are assigned to the most recent observations. They decrease exponentially
    for older observations, meaning the oldest ones will have the smallest weights.
    It uses an **Î±** parameter, its range is between 0 and 1\. The higher the value
    is the greater the impact of the latest values on the prediction. Please take
    a look at the above graphic, where the formulas are also. They are really easy
    to understand and often these models give good results. Before we move on to more
    advanced models we should stop here for a moment, because if a simple model gives
    the same results as very advanced models (for example, deep neural networks) then
    we should stay with the simpler ones, because their operation is more predictable
    for us and once again, more people are able to understand their operation.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å›åˆ°è¯é¢˜çš„ç¬¬ä¸€ä¸ªç±»å‹ï¼Œå³**ç®€å•æŒ‡æ•°å¹³æ»‘**ã€‚ä½œä¸ºåŸºçº¿æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ä¸€ä¸ªæ€»æ˜¯é¢„æµ‹è®­ç»ƒé›†ä¸­çš„æœ€åä¸€ä¸ªå€¼çš„æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§ç¨æ˜¾å¤©çœŸçš„æ–¹æ³•ï¼Œä½†å¯ä»¥ç»™æˆ‘ä»¬å¸¦æ¥ä¸é”™çš„ç»“æœã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯è®¡ç®—æ•´ä¸ªè®­ç»ƒé›†çš„å¹³å‡å€¼ï¼Œä½†è¿™æ ·çš„è¯ï¼Œæœ€è¿‘çš„è§‚å¯Ÿå€¼å’Œæœ€å¤è€çš„è§‚å¯Ÿå€¼ä¼šè¢«èµ‹äºˆåŒç­‰çš„é‡è¦æ€§ã€‚æŒ‡æ•°å¹³æ»‘ç»“åˆäº†è¿™ä¸¤ç§æ–¹æ³•ï¼Œèµ‹äºˆæœ€è¿‘çš„è§‚å¯Ÿå€¼æ›´å¤§çš„æƒé‡ï¼Œæƒé‡ä¼šéšç€è§‚å¯Ÿå€¼çš„å¤è€ç¨‹åº¦å‘ˆæŒ‡æ•°ä¸‹é™ï¼Œè¿™æ„å‘³ç€æœ€å¤è€çš„è§‚å¯Ÿå€¼å°†æ‹¥æœ‰æœ€å°çš„æƒé‡ã€‚å®ƒä½¿ç”¨**Î±**å‚æ•°ï¼Œå…¶èŒƒå›´åœ¨0åˆ°1ä¹‹é—´ã€‚å€¼è¶Šé«˜ï¼Œæœ€æ–°å€¼å¯¹é¢„æµ‹çš„å½±å“è¶Šå¤§ã€‚è¯·æŸ¥çœ‹ä¸Šé¢çš„å›¾å½¢ï¼Œé‚£é‡Œä¹Ÿæœ‰å…¬å¼ã€‚è¿™äº›å…¬å¼éå¸¸å®¹æ˜“ç†è§£ï¼Œé€šå¸¸è¿™äº›æ¨¡å‹èƒ½ç»™å‡ºä¸é”™çš„ç»“æœã€‚åœ¨æˆ‘ä»¬è¿›å…¥æ›´é«˜çº§çš„æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬åº”è¯¥åœ¨è¿™é‡Œç¨ä½œåœç•™ï¼Œå› ä¸ºå¦‚æœä¸€ä¸ªç®€å•çš„æ¨¡å‹ç»™å‡ºçš„ç»“æœä¸éå¸¸å…ˆè¿›çš„æ¨¡å‹ï¼ˆä¾‹å¦‚æ·±åº¦ç¥ç»ç½‘ç»œï¼‰ç›¸åŒï¼Œé‚£ä¹ˆæˆ‘ä»¬åº”è¯¥ä¿æŒä½¿ç”¨æ›´ç®€å•çš„æ¨¡å‹ï¼Œå› ä¸ºå®ƒä»¬çš„æ“ä½œå¯¹æˆ‘ä»¬æ¥è¯´æ›´å¯é¢„æµ‹ï¼Œå¹¶ä¸”æ›´å¤šçš„äººèƒ½å¤Ÿç†è§£å®ƒä»¬çš„è¿ä½œã€‚
- en: SES doesnâ€™t handle a trend in data by the nature of the formula. If there is
    an increasing trend then the forecast underestimates because it doesnâ€™t include
    this increase in the data. Therefore, we have another model, which is **Double
    Exponential Smoothing**. It has an additional factor that is responsible for taking
    into account the impact of the trend. We use the **Î²** parameter, which controls
    the impact of the trend change. Hence we have 2 formulas, one for **level** (Level
    equation) and the other for **trend** (Trend equation).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: SESæœ¬è´¨ä¸Šæ— æ³•å¤„ç†æ•°æ®ä¸­çš„è¶‹åŠ¿ã€‚å¦‚æœå­˜åœ¨ä¸Šå‡è¶‹åŠ¿ï¼Œåˆ™é¢„æµ‹ä¼šä½ä¼°ï¼Œå› ä¸ºå®ƒæ²¡æœ‰åŒ…å«è¿™ç§å¢åŠ ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æœ‰å¦ä¸€ç§æ¨¡å‹ï¼Œå³**åŒé‡æŒ‡æ•°å¹³æ»‘**ã€‚å®ƒæœ‰ä¸€ä¸ªé¢å¤–çš„å› ç´ ï¼Œç”¨äºè€ƒè™‘è¶‹åŠ¿çš„å½±å“ã€‚æˆ‘ä»¬ä½¿ç”¨**Î²**å‚æ•°ï¼Œå®ƒæ§åˆ¶è¶‹åŠ¿å˜åŒ–çš„å½±å“ã€‚å› æ­¤æˆ‘ä»¬æœ‰ä¸¤ä¸ªå…¬å¼ï¼Œä¸€ä¸ªç”¨äº**æ°´å¹³**ï¼ˆæ°´å¹³æ–¹ç¨‹ï¼‰ï¼Œå¦ä¸€ä¸ªç”¨äº**è¶‹åŠ¿**ï¼ˆè¶‹åŠ¿æ–¹ç¨‹ï¼‰ã€‚
- en: The **Triple Exponential Smoothing** also takes into account seasonality. You
    can know it as Holt-Wintersâ€™ seasonal method. Here another parameter, **Î³**, comes
    into the formula. This method allows changing the **level**, **trend**, and patterns
    of **seasonality** over time. Like a trend, seasonality can be additive or multiplicative,
    but here I am not going to describe details assuming that you know the difference
    or can easily find them. I just donâ€™t want to make a book out of this article
    now, and I want you to smoothly go through the whole article. ğŸ˜‰
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸‰é‡æŒ‡æ•°å¹³æ»‘**ä¹Ÿè€ƒè™‘äº†å­£èŠ‚æ€§ã€‚ä½ å¯ä»¥å°†å®ƒç§°ä¸ºHolt-Wintersçš„å­£èŠ‚æ€§æ–¹æ³•ã€‚è¿™é‡Œå¦ä¸€ä¸ªå‚æ•°**Î³**è¿›å…¥äº†å…¬å¼ã€‚è¿™ç§æ–¹æ³•å…è®¸**æ°´å¹³**ã€**è¶‹åŠ¿**å’Œ**å­£èŠ‚æ€§**çš„æ¨¡å¼éšæ—¶é—´å˜åŒ–ã€‚åƒè¶‹åŠ¿ä¸€æ ·ï¼Œå­£èŠ‚æ€§å¯ä»¥æ˜¯åŠ æ³•çš„æˆ–ä¹˜æ³•çš„ï¼Œä½†åœ¨è¿™é‡Œæˆ‘ä¸ä¼šæè¿°è¯¦ç»†ä¿¡æ¯ï¼Œå‡è®¾ä½ çŸ¥é“åŒºåˆ«æˆ–å¯ä»¥è½»æ¾æ‰¾åˆ°å®ƒä»¬ã€‚æˆ‘åªæ˜¯ä¸æƒ³æŠŠè¿™ç¯‡æ–‡ç« å†™æˆä¸€æœ¬ä¹¦ï¼Œç°åœ¨æˆ‘å¸Œæœ›ä½ èƒ½é¡ºåˆ©é˜…è¯»æ•´ä¸ªæ–‡ç« ã€‚ğŸ˜‰'
- en: '[PRE17]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**overall MAPE: 31.88%**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ€»ä½“MAPE: 31.88%**'
- en: '[PRE18]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/caee69055bc0f0fa97677781373db3dd.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/caee69055bc0f0fa97677781373db3dd.png)'
- en: As for ARIMA, I did not add seasonality information. Now is your step, add this
    information using the *seasonal_periods* parameter for [**Exponential Smoothing**](https://unit8co.github.io/darts/generated_api/darts.models.forecasting.exponential_smoothing.html).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: è‡³äºARIMAï¼Œæˆ‘æ²¡æœ‰æ·»åŠ å­£èŠ‚æ€§ä¿¡æ¯ã€‚ç°åœ¨è½®åˆ°ä½ äº†ï¼Œä½¿ç”¨*seasonal_periods*å‚æ•°ä¸º[**æŒ‡æ•°å¹³æ»‘**](https://unit8co.github.io/darts/generated_api/darts.models.forecasting.exponential_smoothing.html)æ·»åŠ è¿™äº›ä¿¡æ¯ã€‚
- en: Prophet
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Prophet
- en: '![](../Images/b90f59872f2815910c9764bd0a782d1e.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b90f59872f2815910c9764bd0a782d1e.png)'
- en: Visualize how the Prophet algorithm works, Image by Author
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å¯è§†åŒ–Prophetç®—æ³•çš„å·¥ä½œåŸç†ï¼Œä½œè€…æä¾›çš„å›¾ç‰‡
- en: '[**Prophet**](https://facebook.github.io/prophet/) was proposed in the [*Forecasting
    at scale*](https://peerj.com/preprints/3190.pdf) paper from 2017 by Facebook.
    It is both a model and a library by the same name. Like the previous models, you
    will find this one at Darts. This algorithm is a **G**eneralized **A**dditive
    **M**odel, so the forecast is the sum of components. These components are **g(*t*)**
    â€” trend, **s(*t*)** â€” seasonality (yearly, weekly, and daily), and **h(*t*)**
    â€” the effect of holidays.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: y(t) = g(t) + s(t) + h(t) + error(t)
  id: totrans-144
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The first one is the **trend**, it can change over time and doesnâ€™t have to
    be constant over time. When students start learning classes with analyzing time
    series, they usually work with simple time series. In a time series, they can
    see a continuous growth trend. However, in real data, the trend can change several
    times. Prophet has implemented **changepoints** (think of them as a hyperparameter,
    such as their number, range, and prior scale). These points are trend changes,
    for example, increase trend -> changepoint -> decrease trend -> changepoint ->
    stronger decrease trend, and so on. This approach is closer to what we can usually
    see in the data. Positions of these changepoints Prophet set behind of you. The
    trend function between changepoints can be a simple regression.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Next, we have the **seasonality** function which is the **Fourier series**.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Another function is the **holiday** effect, which adds or subtracts value to
    our forecast. You can use the dates that the Prophet library provides or define
    your own events. You can imagine that the Black Friday effect significantly affects
    the value of sales. In addition, you can take into account the range around the
    date where the holiday affects the forecast, for example, Christmas does not affect
    sales on the day of the holiday, but the days before (many days).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**overall MAPE: 14.38%**'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/ca2b1293788c97c73332b2ec77cf20ed.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: Global models
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now letâ€™s turn to the approach where we have one model for all time series.
    This is also known as **cross-learning**, because the model to make a good prediction
    of time series A learned the relationships from time series A and also B, C, D,
    etc.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Supervised Model ~ Time Series as a regression problem
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/1044de425c3f4a039c506ed1ec58736c.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: Visualize How to create features for a supervised learning model, Image by Author
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Now letâ€™s try to apply supervised learning models to time series forecasting.
    This is nothing new, but they often give great results and are better than neural
    networks (see the best solutions from M-5 Competition). However, regression models
    are not dedicated to time series, so if we want to use them we need to **convert
    the time series problem into a machine learning problem**. I wrote more about
    this in my previous article [**Sell Out Sell In Forecasting**](/sell-out-sell-in-forecasting-45637005d6ee),
    but I will also now describe how to use the well-known regression algorithms for
    this problem.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Before I start with the previously mentioned transformation, we first need to
    **scale** the data. Earlier I described the need for this transformation for global
    models. In this case, I used *MinMaxScaler*.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: The next step is **feature engineering**, which is the transformation that was
    repeated several times. Based on the history of the time series, we create features
    that will help the model better forecast the future. These variables can refer
    to the recent history of selected time series, such as **lagged** values (for
    weekly data t-1W, t-2W, t-3W, and so on). Another example is the calculation of
    **rolling statistics**, median (it can be a median of the recent 4 weeks), mean,
    min, max, std, and whatever you are going to calculate on the window. If there
    is **seasonality** in the data then it is good to give the model a hint as to
    where in time the t is. I often use the **sin and cos of a cyclic variable** (in
    the above visualization it is a day of the year and a day of the week).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: The last step is to choose a **model**, you have a wide range of choices, so
    it can be Linear regression, Linear mixed effect model, Random Forest, LightGBM,
    and much more. The choice of model depends on the nature of the time series and
    the complexity of the problem. Another question might be if you want one model
    or as many models as horizons. When you choose a model then you need to keep in
    mind its weaknesses. For example, when you choose Random Forest then remember
    that the leaf counts the mean, so it canâ€™t go beyond the training range. LightGBM
    doesnâ€™t have this problem because it doesnâ€™t count a naive mean, but regression
    is counted in the background.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Now itâ€™s time again to return to the practical part and implement the model
    in code. I chose [**LightGBM**](https://lightgbm.readthedocs.io/en/v3.3.2/) as
    the model. Using it in Darts is much easier than in a similar way I would like
    to use it without this library. As you can see in the code we are using the lags
    of the last 14 days. I also added encoders that add covariates used by the model
    and they are added automatically and everything counts inside.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '*cyclic* â€” adds 2 columns, sin cos encoding based on cyclic variable, here
    month'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*datetime_attribute* â€” adds scalar based on datetime variable'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*position* â€” adds the relative index positions as integer values based on time
    series index, where 0 is set at the forecasting point.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*position* â€” åŸºäºæ—¶é—´åºåˆ—ç´¢å¼•ï¼Œå°†ç›¸å¯¹ç´¢å¼•ä½ç½®æ·»åŠ ä¸ºæ•´æ•°å€¼ï¼Œå…¶ä¸­0è®¾ç½®åœ¨é¢„æµ‹ç‚¹ã€‚'
- en: '[PRE21]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**overall MAPE: 15.01%**'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ€»ä½“MAPE: 15.01%**'
- en: '[PRE22]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/45a39d49094bcdf9d257b90ae08828b8.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45a39d49094bcdf9d257b90ae08828b8.png)'
- en: The results are really promising especially since this is one model for all
    time series. However, I would like to warn you based on my experience. These types
    of models work well based on feature engineering, which is an advantage and a
    big disadvantage. Suppose you are using lags and moving mean. You are now going
    to forecast the value of one of the time series, but you have anomalies in it
    â€” several large values before the forecast point. Your model will definitely overestimate.
    When you create variables then try to imagine what effect they will have on the
    model.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœéå¸¸æœ‰å‰æ™¯ï¼Œå°¤å…¶æ˜¯å› ä¸ºè¿™æ˜¯ä¸€ä¸ªé€‚ç”¨äºæ‰€æœ‰æ—¶é—´åºåˆ—çš„æ¨¡å‹ã€‚ç„¶è€Œï¼ŒåŸºäºæˆ‘çš„ç»éªŒï¼Œæˆ‘æƒ³æé†’ä½ ã€‚è¿™äº›ç±»å‹çš„æ¨¡å‹åœ¨ç‰¹å¾å·¥ç¨‹ä¸Šè¡¨ç°è‰¯å¥½ï¼Œè¿™æ—¢æ˜¯ä¼˜åŠ¿ä¹Ÿæ˜¯å¤§ç¼ºé™·ã€‚å‡è®¾ä½ ä½¿ç”¨äº†æ»åå’Œç§»åŠ¨å¹³å‡ã€‚ç°åœ¨ä½ å°†é¢„æµ‹ä¸€ä¸ªæ—¶é—´åºåˆ—çš„å€¼ï¼Œä½†å®ƒä¸­æœ‰å¼‚å¸¸å€¼â€”â€”åœ¨é¢„æµ‹ç‚¹ä¹‹å‰æœ‰å‡ ä¸ªå¤§å€¼ã€‚ä½ çš„æ¨¡å‹è‚¯å®šä¼šé«˜ä¼°ã€‚å½“ä½ åˆ›å»ºå˜é‡æ—¶ï¼Œå°½é‡æƒ³è±¡å®ƒä»¬å¯¹æ¨¡å‹çš„å½±å“ã€‚
- en: DeepAR
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DeepAR
- en: '![](../Images/ed577969a790507e6cbac998587cca53.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed577969a790507e6cbac998587cca53.png)'
- en: Screenshot of arxiv and model architecture, based on Paper
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºè®ºæ–‡çš„arxivå’Œæ¨¡å‹æ¶æ„çš„æˆªå›¾
- en: '[**DeepAR**](https://arxiv.org/abs/1704.04110) is a deep learning algorithm
    developed by Amazon team. It is designed to model the complex dependencies and
    relationships in time series data using recurrent neural networks (RNNs).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[**DeepAR**](https://arxiv.org/abs/1704.04110) æ˜¯ç”±Amazonå›¢é˜Ÿå¼€å‘çš„ä¸€ç§æ·±åº¦å­¦ä¹ ç®—æ³•ã€‚å®ƒæ—¨åœ¨ä½¿ç”¨é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNsï¼‰å¯¹æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„å¤æ‚ä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚'
- en: 'As we can read in Abstract (which is very close to me):'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨æ‘˜è¦ä¸­é˜…è¯»åˆ°çš„ï¼ˆè¿™ä¸æˆ‘éå¸¸æ¥è¿‘ï¼‰ï¼š
- en: In retail businesses, for example, forecasting demand is crucial for having
    the right inventory available at the right time at the right place.
  id: totrans-175
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œåœ¨é›¶å”®ä¸šåŠ¡ä¸­ï¼Œé¢„æµ‹éœ€æ±‚å¯¹äºåœ¨æ­£ç¡®çš„æ—¶é—´å’Œåœ°ç‚¹æä¾›åˆé€‚çš„åº“å­˜è‡³å…³é‡è¦ã€‚
- en: The model is autoregressive and generates a probabilistic forecast using Monte
    Carlo samples. The NN architecture is based on the LSTM layer. With a probabilistic
    approach, we are not interested in a single good prediction, but rather in the
    full predictive distribution of where the true value can be found. Instead of
    using LSTMs to calculate predictions directly, DeepAR leverages LSTMs to parameterize
    a Gaussian likelihood function. That is, to estimate mean and standard deviation
    of the Gaussian function (Î¸ = (Î¼, Ïƒ) parameters).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹æ˜¯è‡ªå›å½’çš„ï¼Œå¹¶ä½¿ç”¨è’™ç‰¹å¡æ´›æ ·æœ¬ç”Ÿæˆæ¦‚ç‡é¢„æµ‹ã€‚ç¥ç»ç½‘ç»œæ¶æ„åŸºäºLSTMå±‚ã€‚é€šè¿‡æ¦‚ç‡æ–¹æ³•ï¼Œæˆ‘ä»¬ä¸å…³å¿ƒå•ä¸€çš„è‰¯å¥½é¢„æµ‹ï¼Œè€Œæ˜¯æ•´ä¸ªé¢„æµ‹åˆ†å¸ƒï¼Œæ¥ç¡®å®šçœŸå®å€¼å¯èƒ½å‡ºç°çš„ä½ç½®ã€‚DeepARä¸æ˜¯ç›´æ¥ä½¿ç”¨LSTMsè¿›è¡Œé¢„æµ‹ï¼Œè€Œæ˜¯åˆ©ç”¨LSTMsæ¥å‚æ•°åŒ–é«˜æ–¯ä¼¼ç„¶å‡½æ•°ã€‚å³ï¼Œä¼°è®¡é«˜æ–¯å‡½æ•°çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆÎ¸
    = (Î¼, Ïƒ) å‚æ•°ï¼‰ã€‚
- en: DeepAR supports Future-known covariates, we donâ€™t have such, but letâ€™s create
    them. As these features, I created OHE with the day of the week and the month.
    Probably a better approach would be sin and cos, I encourage you to experiment
    and return to me with your feedback.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: DeepARæ”¯æŒæœªæ¥å·²çŸ¥åå˜é‡ï¼Œæˆ‘ä»¬æ²¡æœ‰è¿™æ ·çš„æ•°æ®ï¼Œä½†å¯ä»¥åˆ›å»ºå®ƒä»¬ã€‚ä½œä¸ºè¿™äº›ç‰¹å¾ï¼Œæˆ‘åˆ›å»ºäº†å¸¦æœ‰æ˜ŸæœŸå‡ å’Œæœˆä»½çš„ç‹¬çƒ­ç¼–ç ï¼ˆOHEï¼‰ã€‚å¯èƒ½æ›´å¥½çš„æ–¹æ³•æ˜¯ä½¿ç”¨æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ï¼Œæˆ‘é¼“åŠ±ä½ è¿›è¡Œå®éªŒï¼Œå¹¶å°†åé¦ˆæ„è§å‘Šè¯‰æˆ‘ã€‚
- en: '[PRE23]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**overall MAPE: 19.35%**'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ€»ä½“MAPE: 19.35%**'
- en: '[PRE24]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](../Images/5e2d94c8d9ccdbe19449ff4fab8ce6c4.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e2d94c8d9ccdbe19449ff4fab8ce6c4.png)'
- en: N-BEATS
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: N-BEATS
- en: '![](../Images/86f2949104c8355bd78ad466b50eea5f.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/86f2949104c8355bd78ad466b50eea5f.png)'
- en: Screenshot of arxiv and model architecture, based on Paper
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºè®ºæ–‡çš„arxivå’Œæ¨¡å‹æ¶æ„çš„æˆªå›¾
- en: '[**N-BEATS**](https://arxiv.org/abs/1905.10437) (Neural basis expansion analysis
    for interpretable time series forecasting) is a deep learning algorithm, but it
    doesnâ€™t include recurrent layers, such as LSTM or GRU.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[**N-BEATS**](https://arxiv.org/abs/1905.10437)ï¼ˆç”¨äºå¯è§£é‡Šæ—¶é—´åºåˆ—é¢„æµ‹çš„ç¥ç»åŸºç¡€æ‰©å±•åˆ†æï¼‰æ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ ç®—æ³•ï¼Œä½†å®ƒä¸åŒ…å«é€’å½’å±‚ï¼Œå¦‚LSTMæˆ–GRUã€‚'
- en: The architecture may seem complex, but once you get into the details it is quite
    simple and is a combination of blocks and all layers are feed forward.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: æ¶æ„å¯èƒ½çœ‹èµ·æ¥å¤æ‚ï¼Œä½†ä¸€æ—¦ä½ æ·±å…¥äº†è§£ç»†èŠ‚ï¼Œå®ƒå®é™…ä¸Šéå¸¸ç®€å•ï¼Œæ˜¯ç”±å—ç»„åˆè€Œæˆï¼Œæ‰€æœ‰å±‚éƒ½æ˜¯å‰é¦ˆçš„ã€‚
- en: Letâ€™s start with the smallest element, the block, each of them has one input
    and generates two outputs. The input is lockback period. Outputs are the forecast
    and the backcast. I think the idea of forecast is easy for you. Backcast is prediction,
    but for lockback period â€” it is fitted value and show how well block has relationship
    on the lookback period window.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s move on to stacks, or the combination of multiple blocks. As you read,
    each block has two outputs and one input. The next blocks are responsible for
    forecasting residuals â€” similar to what happens in boosting forest models, like
    AdaBoost. In each step, the backcast generated by the block is subtracted from
    the input of the block before. At the end, all forecasts from blocks are aggregated.
    In addition, it is an interpretable model, you can decompose and see the effect
    of trend and seasonality.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Now letâ€™s move on to the combined stacks. This part increases the depth of the
    model and provides an opportunity to learn more about complexity.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**overall MAPE: 13.18%**'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](../Images/0a61c6e32b3eb36bc2e90b0ce40b9973.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: TFT
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/341ef264d69b65087f3d9ac99030c59b.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
- en: Screenshot of arxiv and model architecture, based on Paper
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[**Temporal Fusion Transformers**](https://arxiv.org/abs/1912.09363) (TFT)
    is a deep learning algorithm developed by Google for time series forecasting.
    It is designed to model the complex dependencies and relationships in time series
    data using a combination of transformer networks and autoregressive modeling.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: TFT is the most complex architecture and uses various techniques underneath.
    It is like an onion, composed of many layers. Also based on my experience, it
    learns the longest time compared to the above models. TFT uses a multi-head attention
    block to find long-time patterns, but LSTM sequence-to-sequence encoders/decoders
    to find these shorter patterns.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '**overall MAPE: 13.37%**'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![](../Images/d7ca8f7f8ffb0cd88937b1b5bc969e2d.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: Summary
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, I wanted to show you what approaches you can choose to forecast
    multiple time series. I have provided you with fully practical code, feel free
    to use them and donâ€™t hesitate to write to me.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'This was just an introduction to the topic. I think also relevant from the
    work of Data Scientists for Supply Chain companies are the following topics:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '- **Hierarchical forecasting** and then combining forecasts from different
    hierarchies, i.e. Hierarchical Reconciliation. We can make a forecast at the store
    level and also at the country level, but when we aggregate the forecast at the
    store level then as a sum we would like to get the same thing that the forecast
    at the country level shows. That is why Hierarchical Reconciliation is important.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '- Another topic is **inventory optimization**, i.e. how many products we should
    have in stock so that we donâ€™t have a situation where we donâ€™t have our products
    in stock, but on the other hand, we donâ€™t stock one product for months.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Questions?
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/21d1e065cb242a0fd6cb114ae9823fec.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21d1e065cb242a0fd6cb114ae9823fec.png)'
- en: Questions, Image by Author
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼Œå›¾ç‰‡ç”±ä½œè€…æä¾›
- en: I am aware that I have touched on many topics in this article. I wanted to give
    you an indication of the directions you can take. Perhaps some of them should
    be described in more detail here and others described in detail in a new article.
    Do not hesitate and write to me, **you can find me on** [**Linkedin**](https://www.linkedin.com/in/bartosz-szablowski/).
    In future articles, I would like them to cover detailed topics and show you how
    to implement models for time series from scratch using PyTorch library.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ„è¯†åˆ°åœ¨è¿™ç¯‡æ–‡ç« ä¸­æˆ‘è§¦åŠäº†è®¸å¤šä¸»é¢˜ã€‚æˆ‘æƒ³ç»™ä½ ä¸€ä¸ªå¯ä»¥é‡‡å–çš„æ–¹å‘çš„æŒ‡ç¤ºã€‚ä¹Ÿè®¸å…¶ä¸­ä¸€äº›åº”è¯¥åœ¨è¿™é‡Œæ›´è¯¦ç»†åœ°æè¿°ï¼Œè€Œå…¶ä»–çš„åˆ™åœ¨æ–°æ–‡ç« ä¸­è¯¦ç»†æè¿°ã€‚è¯·ä¸è¦çŠ¹è±«ï¼Œ**ä½ å¯ä»¥åœ¨**
    [**Linkedin**](https://www.linkedin.com/in/bartosz-szablowski/) **æ‰¾åˆ°æˆ‘**ã€‚åœ¨æœªæ¥çš„æ–‡ç« ä¸­ï¼Œæˆ‘å¸Œæœ›å®ƒä»¬èƒ½å¤Ÿæ¶µç›–è¯¦ç»†çš„ä¸»é¢˜ï¼Œå¹¶å±•ç¤ºå¦‚ä½•ä½¿ç”¨
    PyTorch åº“ä»å¤´å¼€å§‹å®ç°æ—¶é—´åºåˆ—æ¨¡å‹ã€‚
- en: THANKS FOR YOUR TIME!
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢ä½ çš„æ—¶é—´ï¼
- en: 'Dataset source:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†æ¥æºï¼š
- en: misc{demand-forecasting-kernels-only,
  id: totrans-214
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ‚é¡¹{ä»…é™éœ€æ±‚é¢„æµ‹å†…æ ¸ï¼Œ
- en: author = {inversion},
  id: totrans-215
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä½œè€… = {inversion},
- en: title = {Store Item Demand Forecasting Challenge},
  id: totrans-216
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ ‡é¢˜ = {å•†åº—å•†å“éœ€æ±‚é¢„æµ‹æŒ‘æˆ˜},
- en: publisher = {Kaggle},
  id: totrans-217
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å‡ºç‰ˆå•† = {Kaggle},
- en: year = {2018},
  id: totrans-218
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¹´ä»½ = {2018},
- en: url = {https://kaggle.com/competitions/demand-forecasting-kernels-only},
  id: totrans-219
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç½‘å€ = {https://kaggle.com/competitions/demand-forecasting-kernels-only},
- en: license=CC
  id: totrans-220
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è®¸å¯è¯ = CC
- en: '}'
  id: totrans-221
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '}'
