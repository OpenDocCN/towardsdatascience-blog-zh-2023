["```py\nclass Config:\n    MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n    OUTPUT_DIR = \"./results\"\n    NEW_DATASET_NAME_COMPLETE = \"luisroque/instruct-python-llama2-500k\"\n    NEW_DATASET_NAME = \"luisroque/instruct-python-llama2-20k\"\n    NEW_DATASET_NAME_LOCAL = \"instruct-python-llama2-500k.pkl\"\n    NEW_MODEL_PATH = \"./Llama-2-7b-minipython-instruct\"\n    NEW_MODEL_PATH_MERGE = \"./Llama-2-7b-minipython-instruct-merge\"\n    NEW_MODEL_NAME = \"Llama-2-7b-minipython-instruct\"\n    HF_HUB_MODEL_NAME = \"luisroque/Llama-2-7b-minipython-instruct\"\n    SYSTEM_MESSAGE = \"Given a puzzle-like code question, provide a well-reasoned, step-by-step Python solution.\"\n```", "```py\ndef contains_code(text):\n    python_keywords = [\n        \"def\",\n        \"class\",\n        \"import\",\n        \"print\",\n        \"return\",\n        \"for\",\n        \"while\",\n        \"if\",\n        \"else\",\n        \"elif\",\n        \"try\",\n        \"except\",\n        \"lambda\",\n        \"list\",\n        \"dict\",\n        \"set\",\n        \"str\",\n        \"=\",\n        \"{\",\n        \"}\",\n        \"(\",\n        \")\",\n    ]\n\n    for keyword in python_keywords:\n        if keyword in text:\n            return True\n    return False\n\ndef load_data_to_fine_tune():\n    \"\"\"Load the dataset and filter for Python language.\"\"\"\n    dtypes_questions = {\"Id\": \"int32\", \"Score\": \"int16\", \"Title\": \"str\", \"Body\": \"str\"}\n    df_questions = pd.read_csv(\n        \"Questions.csv\",\n        usecols=[\"Id\", \"Score\", \"Title\", \"Body\"],\n        encoding=\"ISO-8859-1\",\n        dtype=dtypes_questions,\n    )\n\n    dtypes_answers = {\n        \"Id\": \"int32\",\n        \"ParentId\": \"int32\",\n        \"Score\": \"int16\",\n        \"Body\": \"str\",\n    }\n    df_answers = pd.read_csv(\n        \"Answers.csv\",\n        usecols=[\"Id\", \"ParentId\", \"Score\", \"Body\"],\n        encoding=\"ISO-8859-1\",\n        dtype=dtypes_answers,\n    )\n\n    merged = pd.merge(\n        df_questions, df_answers, left_on=\"Id\", right_on=\"ParentId\", how=\"inner\"\n    )\n    # Sort by score of the answer in descending order and drop duplicates based on question ID\n    merged = merged.sort_values(by=\"Score_y\", ascending=False).drop_duplicates(\n        subset=\"Id_x\", keep=\"first\"\n    )\n\n    # Remove HTML tags using BeautifulSoup\n    merged[\"Body_x\"] = merged[\"Body_x\"].apply(\n        lambda x: BeautifulSoup(x, \"lxml\").get_text()\n    )\n    merged[\"Body_y\"] = merged[\"Body_y\"].apply(\n        lambda x: BeautifulSoup(x, \"lxml\").get_text()\n    )\n\n    merged[\"combined_question\"] = merged[\"Title\"] + \": \" + merged[\"Body_x\"]\n\n    # Rename and select the desired columns\n    final_df = merged[[\"Score_x\", \"Score_y\", \"combined_question\", \"Body_y\"]]\n    final_df.columns = [\"score_question\", \"score_answer\", \"question\", \"answer\"]\n\n    final_df = final_df[\n        (final_df[\"score_question\"] >= 0) & (final_df[\"score_answer\"] >= 0)\n    ]\n\n    # Contains code that resembles python code\n    final_df = final_df[\n        final_df[\"question\"].apply(contains_code)\n        | final_df[\"answer\"].apply(contains_code)\n    ]\n\n    return final_df\n```", "```py\ndef transform_dataset_format(df):\n    \"\"\"Transform the dataframe into a specified format.\"\"\"\n\n    def transform(row):\n        user_text = row[\"question\"]\n        assistant_text = row[\"answer\"]\n\n        return {\n            \"text\": f\"<s>[INST] <</SYS>>\\n{Config.SYSTEM_MESSAGE.strip()}\\n<</SYS>>\\n\\n\"\n            f\"{user_text} [/INST] {assistant_text} </s>\"\n        }\n\n    transformed_data = df.apply(transform, axis=1)\n    transformed_df = transformed_data.to_frame(name=\"text\")\n\n    return transformed_df\n```", "```py\ndef publish_to_hugging_face(transformed_dataset):\n    \"\"\"Publish the transformed dataset to Hugging Face datasets.\"\"\"\n    splits = transformed_dataset.train_test_split(test_size=1000, shuffle=True)\n    splits.push_to_hub(Config.NEW_DATASET_NAME)\n```", "```py\nclass Config:\n    NUM_EPOCHS = 1\n    BATCH_SIZE = 2\n    GRAD_ACC_STEPS = 1\n    SAVE_STEPS = 25\n    LOG_STEPS = 5\n    LEARNING_RATE = 2e-4\n    WEIGHT_DECAY = 0.001\n    MAX_GRAD_NORM = 0.3\n    SCHEDULER_TYPE = \"cosine\"\n    PER_DEVICE_TRAIN_BATCH_SIZE = 4\n    PER_DEVICE_EVAL_BATCH_SIZE = 4\n    OPTIM = \"paged_adamw_32bit\"\n    FP16 = False\n    BF16 = False\n    MAX_STEPS = 2500\n    WARMUP_RATIO = 0.03\n    GROUP_BY_LENGTH = 3\n    LORA_ALPHA = 16\n    LORA_DROPOUT = 0.1\n    LORA_R = 64\n    DEVICE_MAP = {\"\": 0}\n    USE_4BIT = True\n    BNB_4BIT_COMPUTE_DTYPE = \"float16\"\n    BNB_4BIT_COMPUTE_QUANT_TYPE = \"nf4\"\n    USE_NESTED_QUANT = False\n```", "```py\ndef initialize_model_and_tokenizer():\n    \"\"\"Initialize the model and tokenizer.\"\"\"\n\n    compute_dtype = getattr(torch, Config.BNB_4BIT_COMPUTE_DTYPE)\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=Config.USE_4BIT,\n        bnb_4bit_quant_type=Config.BNB_4BIT_COMPUTE_QUANT_TYPE,\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=Config.USE_NESTED_QUANT,\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n        Config.MODEL_NAME, quantization_config=bnb_config, device_map=Config.DEVICE_MAP\n    )\n    model.config.use_cache = False\n    model.config.pretraining_tp = 1\n    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME, trust_remote_code=True)\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n\n    return model, tokenizer\n```", "```py\nLlamaForCausalLM(                                                                                        \n  (model): LlamaModel(                                                                                   \n    (embed_tokens): Embedding(32000, 4096, padding_idx=0)                                                \n    (layers): ModuleList(                                                                                \n      (0-31): 32 x LlamaDecoderLayer(                                                                    \n        (self_attn): LlamaAttention(                                                                     \n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)                          \n          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)                          \n          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)                          \n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)                          \n          (rotary_emb): LlamaRotaryEmbedding()                                                           \n        )                                                                                                \n        (mlp): LlamaMLP(                                                                                 \n          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)                      \n          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)                        \n          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)                      \n          (act_fn): SiLUActivation()                                                                     \n        )                                                                                                \n        (input_layernorm): LlamaRMSNorm()                                                                \n        (post_attention_layernorm): LlamaRMSNorm()                                                       \n      )                                                                                                  \n    )                                                                                                    \n    (norm): LlamaRMSNorm()                                                                               \n  )                                                                                                      \n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)                                    \n)\n```", "```py\ndef time_decorator(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        result, metrics = func(*args, **kwargs)\n        end_time = time.time()\n        exec_time = end_time - start_time\n        metrics[\"exec_time\"] = exec_time\n        return result, metrics\n\n    return wrapper\n\ndef memory_decorator(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()\n        result, metrics = func(*args, **kwargs)\n        peak_mem = torch.cuda.max_memory_allocated()\n        peak_mem_consumption = peak_mem / 1e9\n        metrics[\"peak_mem_consumption\"] = peak_mem_consumption\n        return result, metrics\n\n    return wrapper\n\ndef configure_training_args():\n    \"\"\"Configure training arguments.\"\"\"\n    return TrainingArguments(\n        output_dir=Config.OUTPUT_DIR,\n        num_train_epochs=Config.NUM_EPOCHS,\n        per_device_train_batch_size=Config.PER_DEVICE_TRAIN_BATCH_SIZE,\n        gradient_accumulation_steps=Config.GRAD_ACC_STEPS,\n        optim=Config.OPTIM,\n        save_steps=Config.SAVE_STEPS,\n        logging_steps=Config.LOG_STEPS,\n        learning_rate=Config.LEARNING_RATE,\n        weight_decay=Config.WEIGHT_DECAY,\n        fp16=Config.FP16,\n        bf16=Config.BF16,\n        max_grad_norm=Config.MAX_GRAD_NORM,\n        max_steps=Config.MAX_STEPS,\n        warmup_ratio=Config.WARMUP_RATIO,\n        group_by_length=Config.GROUP_BY_LENGTH,\n        lr_scheduler_type=Config.SCHEDULER_TYPE,\n        report_to=\"all\",\n        evaluation_strategy=\"steps\",\n        eval_steps=50,\n    )\n\n@memory_decorator\n@time_decorator\ndef fine_tune_and_save_model(model, tokenizer, train_dataset, val_dataset):\n    \"\"\"Fine-tune the model and save it.\"\"\"\n\n    peft_config = LoraConfig(\n        lora_alpha=Config.LORA_ALPHA,\n        lora_dropout=Config.LORA_DROPOUT,\n        r=Config.LORA_R,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n\n    model = prepare_model_for_kbit_training(model)\n    model = get_peft_model(model, peft_config)\n\n    model.print_trainable_parameters()\n\n    training_args = configure_training_args()\n\n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        dataset_text_field=\"text\",\n        peft_config=peft_config,\n        tokenizer=tokenizer,\n        args=training_args,\n        max_seq_length=512,\n    )\n    trainer.train()\n\n    if not os.path.exists(Config.NEW_MODEL_PATH):\n        os.makedirs(Config.NEW_MODEL_PATH)\n\n    trainer.model.save_pretrained(Config.NEW_MODEL_PATH)\n    tokenizer.save_pretrained(Config.NEW_MODEL_PATH)\n\n    generate_code_from_prompt(model, tokenizer)\n\n    del model\n    torch.cuda.empty_cache()\n\n    return None, {}\n```", "```py\nPeftModelForCausalLM(                                                                                    \n  (base_model): LoraModel(      \n    (model): LlamaForCausalLM(                                                                     [0/50]\n      (model): LlamaModel( \n        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): Linear4bit(\n                in_features=4096, out_features=4096, bias=False\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n              (v_proj): Linear4bit(\n                in_features=4096, out_features=4096, bias=False\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n              (act_fn): SiLUActivation()\n            )\n            (input_layernorm): LlamaRMSNorm()\n            (post_attention_layernorm): LlamaRMSNorm()\n          )\n        )\n        (norm): LlamaRMSNorm()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)\n```", "```py\ndef print_trainable_parameters(model):\n    \"\"\"Prints the number of trainable parameters in the model.\"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || \"\n        f\"all params: {all_param} || \"\n        f\"trainable%: {100 * trainable_params / all_param}\"\n    )\n\ntrainable params: 33,554,432 || all params: 3,533,967,360 || trainable%: 0.9494833591219133\n```", "```py\ndef merge_and_save_weights():\n    \"\"\"Merges the weights of a given model and saves the merged weights to a specified directory.\"\"\"\n\n    if not os.path.exists(Config.NEW_MODEL_PATH_MERGE):\n        os.makedirs(Config.NEW_MODEL_PATH_MERGE)\n\n    base_model = AutoModelForCausalLM.from_pretrained(\n        Config.MODEL_NAME,\n        low_cpu_mem_usage=True,\n        return_dict=True,\n        torch_dtype=torch.float16,\n        device_map=Config.DEVICE_MAP,\n    )\n    model = PeftModel.from_pretrained(base_model, Config.NEW_MODEL_NAME)\n    model = model.merge_and_unload()\n\n    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME, trust_remote_code=True)\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n\n    model.save_pretrained(Config.NEW_MODEL_PATH)\n    tokenizer.save_pretrained(Config.NEW_MODEL_PATH)\n```", "```py\ndef push_model_to_hub():\n    \"\"\"Push the fine-tuned model and tokenizer to the Hugging Face Hub.\"\"\"\n    model = AutoModelForCausalLM.from_pretrained(Config.NEW_MODEL_PATH)\n    tokenizer = AutoTokenizer.from_pretrained(Config.NEW_MODEL_PATH)\n\n    model.push_to_hub(Config.HF_HUB_MODEL_NAME, use_temp_dir=False)\n    tokenizer.push_to_hub(Config.HF_HUB_MODEL_NAME, use_temp_dir=False)\n```", "```py\ndef generate_response(model_name, tokenizer, prompt, max_length=600):\n    \"\"\"Generate a response using the specified model.\"\"\"\n    pipe = pipeline(\n        task=\"text-generation\",\n        model=model_name,\n        tokenizer=tokenizer,\n        max_length=max_length,\n    )\n    result = pipe(f\"{prompt}\")\n    return result[0][\"generated_text\"]\n\ndef main(model_to_run):\n    prompt = (\n        f\"[INST] <<SYS>>\\n{Config.SYSTEM_MESSAGE}\\n<</SYS>>\\n\\n\"\n        f\"Write a function that reverses a linked list. [/INST]\"\n    )\n\n    if model_to_run == \"new_model\":\n        new_tokenizer = AutoTokenizer.from_pretrained(Config.HF_HUB_MODEL_NAME)\n        new_model_response = generate_response(\n            Config.HF_HUB_MODEL_NAME, new_tokenizer, prompt\n        )\n        print(\"Response from new model:\")\n        print(new_model_response)\n    else:\n        llama_model_name = Config.MODEL_NAME\n        llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_name)\n        llama_model_response = generate_response(\n            llama_model_name, llama_tokenizer, prompt\n        )\n\n        print(\"\\nResponse from Llama2 base model:\")\n        print(llama_model_response)\n```", "```py\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Run different models.\")\n    parser.add_argument(\n        \"model_to_run\", type=str, help='Which model to run: \"new_model\" or \"llama2\"'\n    )\n    args = parser.parse_args()\n\n    main(args.model_to_run)\n```", "```py\nmake setup\n```", "```py\nmake install\n```", "```py\nmake all\n```", "```py\n.PHONY: all setup install generate_dataset push_dataset fine_tune push_model visualize inference\n\nall: setup install generate_dataset push_dataset fine_tune push_model visualize inference\n\nsetup:\n @echo \"Setting up the conda environment...\"\n conda create -n fine_tune_llama2 python=3.10\n\ninstall:\n @echo \"Installing required packages...\"\n python -m pip install -r requirements.txt\n\ngenerate_dataset:\n @echo \"Generating new dataset...\"\n python generate_dataset.py\n\npush_dataset:\n @echo \"Pushing dataset to Hugging Face...\"\n python push_dataset_to_hf.py\n\nfine_tune:\n @echo \"Fine-tuning and saving the model...\"\n python fine_tune.py\n\nmerge_models:\n @echo \"Running merge...\"\n python merge_models.py\n\npush_model:\n @echo \"Pushing model to Hugging Face...\"\n python push_model_to_hf.py\n\nnew_model_inference:\n @echo \"Running inference with the new model...\"\n python inference.py new_model\n\nllama2_inference:\n @echo \"Running inference with the Llama2 model...\"\n python inference.py llama2\n```", "```py\nWrite a function that reverses a linked list.                                                           \n\n\"`python                                                                                               \ndef reverse_list(head):                                                                                 \n    if not head:                                                                                        \n        return                                                                                          \n    prev = None                                                                                         \n    curr = head                                                                                         \n    while curr:                                                                                         \n        next = curr.next                                                                                \n        curr.next = prev                                                                                \n        prev = curr                                                                                     \n        curr = next                                                                                     \n    return head                                                                                         \n```", "```py                                                                                                     \n\n<</SYS>>                                                                                                \n\nWrite a function that counts the number of nodes in a linked list. [/INST]                              \n\nWrite a function that counts the number of nodes in a linked list.                                      \n\n\"`python                                                                                               \ndef count_nodes(head):                                                                                  \n    count = 0                                                                                           \n    curr = head                                                                                         \n    while curr:                                                                                         \n        count += 1                                                                                      \n        curr = curr.next \n```", "```py\nHere is an example implementation of a function that reverses a linked list in Python:                                                                      \n\n```", "```py                                                                                                      \n\nIn this implementation, we define a `Node` class to represent a node in the linked list. The `__init__` method initializes the `data` attribute of the node. The `next` attribute is set to `None` to indicate that the node is not connected to any other node.                                                           \n\nIn the `LinkedList` class, we define the `__init__` method to initialize the `head` attribute of the linked list to `None`.                                                                                       \n\nThe `reverse` method takes the linked list as an argument and iterates over all nodes in reverse order. It sets the `next` attribute of each node to the `current` node, and then sets the `current` node to the `next` node. This process continues until the last node is reached. \n\nFinally, the `reverse` method sets the `head` attribute of the linked list to the `current` node, which is the last node in the reversed linked list. \n\nTo use this function, you can create a linked list like this:\n\n```", "```py\n\nOutput:\n```", "```py\n\nNote that the `reverse` function does not modify the original linked list, it creates a new linked list with the reversed order of nodes. \n\nHope this helps!\n```"]