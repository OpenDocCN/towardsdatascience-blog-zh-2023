- en: 'Zephyr 7B Beta: A Good Teacher Is All You Need'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/zephyr-7b-beta-a-good-teacher-is-all-you-need-c931fcd0bfe7](https://towardsdatascience.com/zephyr-7b-beta-a-good-teacher-is-all-you-need-c931fcd0bfe7)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Knowledge distillation for Mistral 7B
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----c931fcd0bfe7--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----c931fcd0bfe7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c931fcd0bfe7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c931fcd0bfe7--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----c931fcd0bfe7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c931fcd0bfe7--------------------------------)
    ·8 min read·Nov 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c9f381fbf16adcf625a9df3101552363.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Pixabay](https://pixabay.com/illustrations/man-drinking-booze-drinker-5334659/)
  prefs: []
  type: TYPE_NORMAL
- en: Mistral 7B is one of the [best pre-trained large language modes (LLMs)](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
    By releasing [Zephyr 7B Alpha](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha),
    Hugging Face has demonstrated that Mistral 7B fine-tuned with DPO can outperform
    chat models that are 10 times bigger and even match the performance of GPT-4 for
    some tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the “Alpha” in the name of the model, Hugging Face was obviously planning
    to release better versions of Zephyr 7B. And they indeed released [Zephyr 7B Beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)
    only 2 weeks later. There is a technical report on arXiv describing the model
    and its evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Zephyr: Direct Distillation of LM Alignment](https://arxiv.org/abs/2310.16944)
    (Tunstall et al., 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will see what makes Zephyr 7B Beta better than larger LLMs.
    More particularly, we will see how Hugging Face leveraged larger LLMs, such as
    GPT-4, to teach Mistral 7B to answer instructions and align the answers with human
    preferences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Distillation: When Smaller LLMs Learn from Larger Ones'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since Hugging Face relied on knowledge distillation (KD) to train Zephyr, let’s
    have a brief reminder of what KD is in the context of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Most LLMs are trained on texts written by humans. Human texts present a high
    diversity of sequences of tokens and vocabulary that is difficult to model. Because
    of this difficulty, we need a lot of data to train an LLM to properly model language.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a shortcut to reduce the training cost and difficulty: knowledge **distillation**
    (KD). There are many ways to do KD. In this section, I’ll only discuss the method
    used by Hugging Face.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once trained on human texts, even though LLMs can be very good at generating
    language, they only approximate the true probability distribution of language.
    LLMs generate by default much less diverse sequences of tokens than humans. *Note:
    That’s why random sampling is often introduced during inference, for instance
    via* [*nucleus sampling*](https://arxiv.org/abs/1904.09751)*, to improve the diversity
    in the generated text.*'
  prefs: []
  type: TYPE_NORMAL
- en: Since sequences of tokens generated by LLMs are less diverse than human text,
    learning to model these generated sequences is a much easier task.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, this is achieved by using a state-of-the-art model, often called
    the **teacher** model, to generate a large amount of synthetic text that will
    be used to train a smaller model, often called the **student** model. The student
    distills the knowledge of its teacher.
  prefs: []
  type: TYPE_NORMAL
- en: The student model’s training converges much faster on the generated text and
    can achieve a performance close to the teacher*.*
  prefs: []
  type: TYPE_NORMAL
- en: 'This strategy works well for training LLMs. One of the best examples of success
    that we have is Microsoft’s phi-1.5: A 1.3 billion parameter model matching the
    performance of much larger models. Microsoft’s phi-1.5 has been exclusively trained
    on synthetic data generated by other models, i.e., Microsoft’s phi-1.5 is a student
    model. *Note: Microsoft didn’t disclose what were the teacher models.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://kaitchup.substack.com/p/how-to-fine-tune-quantize-and-run?source=post_page-----c931fcd0bfe7--------------------------------)
    [## How to Fine-tune, Quantize, and Run Microsoft phi-1.5'
  prefs: []
  type: TYPE_NORMAL
- en: A model pre-trained for many tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/how-to-fine-tune-quantize-and-run?source=post_page-----c931fcd0bfe7--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face’s Zephyr 7B Beta is also a student model. All its training data
    have been generated by much larger models, hence a much better performance than
    other LLMs of similar size trained on human texts (e.g., Llama 2).
  prefs: []
  type: TYPE_NORMAL
- en: In the case of Zephyr 7B Beta, Hugging Face pushed knowledge distillation much
    further into the process of training and aligning an LLM with human preferences,
    as we will see in the next sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'dDPO: Distilled Direct Preference Optimization with Mistral 7B'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Making Zephyr 7B Beta from Mistral 7B is a three-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised fine-tuning (SFT) on instruction datasets generated by other larger
    models
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scoring/ranking LLMs’ outputs using a state-of-the-art LLM
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training DPO with the model obtained in Step 1 on the data obtained in Step
    2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distilled Supervised Fine-Tuning (dSFT)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SFT is the standard first step for training an instruct/chat model. It requires
    an instruction dataset: instructions/questions paired with answers given by humans.'
  prefs: []
  type: TYPE_NORMAL
- en: The main issue here is that collecting such a dataset is extremely expensive
    since it involves human labor. A more and more common and cheaper alternative
    is to use instruction datasets generated by other LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can find many such instruction datasets on the Hugging Face Hub that we
    can use for SFT, for instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[OpenAssistant Conversations Dataset (OASST1)](https://huggingface.co/datasets/OpenAssistant/oasst1)
    (84.4k training examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca) (4.2M training
    examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)
    (9.8k training examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For Zephyr 7B Beta, Hugging Face fine-tuned Mistral 7B on a custom version
    of [Ultrachat](https://huggingface.co/datasets/stingning/ultrachat) that they
    aggressively filtered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[HuggingFaceH4/ultrachat_200k](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)
    (MIT license), use the “sft” splits'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*we applied truecasing heuristics to fix the grammatical errors (approximately
    5% of the dataset), as well as several filters to focus on helpfulness and remove
    the undesired model responses.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hugging Face denotes this SFT “Distilled Supervised Fine-Tuning” since the fine-tuning
    is done on datasets generated by “teacher” models.
  prefs: []
  type: TYPE_NORMAL
- en: AI Feedback through Preferences (AIF)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For alignment with humans, we need a dataset of prompts paired with ranked answers.
    We can then use DPO, or RLHF, to train the model to generate preferred answers.
  prefs: []
  type: TYPE_NORMAL
- en: Ranking models’ answers is an expensive task requiring human labor. But again,
    we already have aligned LLMs that are good enough to make this ranking.
  prefs: []
  type: TYPE_NORMAL
- en: We can take an existing dataset of prompts paired with answers generated by
    different models and use a state-of-the-art LLM to rank these answers.
  prefs: []
  type: TYPE_NORMAL
- en: For this step, Hugging Face directly used the dataset [UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback).
  prefs: []
  type: TYPE_NORMAL
- en: 'UltraFeedback contains 74k prompts paired with responses generated by the following
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: LLaMA-2–7B-chat, LLaMA-2–13B-chat, LLaMA-2–70B-chat
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UltraLM-13B, UltraLM-65B
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WizardLM-7B, WizardLM-13B, WizardLM-70B
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vicuna-33B
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alpaca-7B
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Falcon-40B-instruct
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MPT-30B-chat
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StarChat-Beta
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pythia-12B
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each LLM’s output is rated by GPT-4 with a score from 1 to 5 (higher is better)
    for various criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: instruction following
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: helpfulness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: honesty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: truthfulness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For DPO, we need a “chosen” output, i.e., the output that we prefer, and a “rejected”
    output, an output that we don’t want the model to generate.
  prefs: []
  type: TYPE_NORMAL
- en: For the chosen output, the output with the highest mean score (using all criteria
    to compute this mean) is selected. For the rejected output, they randomly selected
    an output among the remaining ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'They justify this random selection as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We opted for random selection instead of selecting the lowest-scored response
    to encourage diversity and make the DPO objective more challenging*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The version of the dataset they have made and used for training DPO is here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized)
    (MIT license), use the “prefs” splits'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distilled Direct Preference Optimization (dDPO)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Instruct LLMs, e.g., chat models, are usually trained with reinforcement learning
    with human feedback (RLHF) using a Proximal Policy Optimization (PPO). It works
    well to align LLMs with human preferences but RLHF is also unstable and complicated.
    Indeed, before running RLHF, we need to train two models:'
  prefs: []
  type: TYPE_NORMAL
- en: A reference model simply trained with supervised fine-tuning (SFT) on an instruction
    dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A reward model trained to predict human preferences. The training data for this
    model are usually rankings by humans of models’ outputs for a given prompt. The
    reward model is trained to predict this ranking.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, RLHF uses 4 different models:'
  prefs: []
  type: TYPE_NORMAL
- en: The reference model trained with SFT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reward model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A value model that is usually initialized by the reward model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model (policy) that we want to train with RLHF which is usually initialized
    by the reference model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using all these models, RLHF uses RL to optimize a language model policy to
    produce responses with a high reward (according to the reward model) without drifting
    excessively far from the original reference model.
  prefs: []
  type: TYPE_NORMAL
- en: Several frameworks implement RLHF to make it computationally more efficient.
    Nonetheless, it remains a complicated and unstable process involving many models.
  prefs: []
  type: TYPE_NORMAL
- en: DPO is a simple alternative to RLHF. It implicitly optimizes the same objective
    as existing RLHF algorithms (reward maximization with a KL-divergence constraint).
    The authors of DPO demonstrate that the constrained reward maximization problem
    can be exactly optimized by solving a much simpler classification problem on human
    preferences.
  prefs: []
  type: TYPE_NORMAL
- en: Since it can be reduced to a classification problem, DPO trains the model using
    a simple binary cross-entropy objective. DPO completely eliminates the need for
    reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Given a prompt and several LLMs’ outputs ranked by humans according to their
    quality, DPO trains the model to assign a higher reward to the best outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'DPO only requires two models:'
  prefs: []
  type: TYPE_NORMAL
- en: The reference model fine-tuned with SFT on instruct datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The base model that we want to train with DPO
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/70fa71fef1c7d146088eeb5a5bf9d2e4.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration by [Rafailov et al. (2023)](https://arxiv.org/abs/2305.18290) —
    CC-BY 4.0
  prefs: []
  type: TYPE_NORMAL
- en: 'DPO is presented by Stanford in this arXiv paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)'
  prefs: []
  type: TYPE_NORMAL
- en: Note that for Zephyr, Hugging Face calls it “Distilled Direct Preference Optimization”
    only because the SFT and preferences are generated by other LLMs. The DPO process
    itself remains standard.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are interested in using DPO to fine-tune Mistral 7B, have a look at
    my tutorial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://kaitchup.substack.com/p/fine-tune-your-own-instruct-version?source=post_page-----c931fcd0bfe7--------------------------------)
    [## Fine-tune Your Own Instruct Version of Mistral 7B with Direct Preference Optimization
    (DPO)'
  prefs: []
  type: TYPE_NORMAL
- en: Making a cheap Zephyr 7B
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/fine-tune-your-own-instruct-version?source=post_page-----c931fcd0bfe7--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The Evaluation of Zephyr 7B Beta
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hugging Face has evaluated Zephyr along the following axes on MT Bench: writing,
    roleplay, reasoning, math, coding, extraction, STEM, and humanities'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ab6a054156bce278d22218d5f342cef.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration by [Tunstall et al. (2023)](https://arxiv.org/abs/2310.16944) —
    CC-BY 4.0
  prefs: []
  type: TYPE_NORMAL
- en: It is clear that Zephyr outperforms Llama 2 70B while performing closely to
    other state-of-the-art commercial LLMs. GPT-4, Zephyr’s main teacher, remains
    much better at reasoning, math, coding, and extraction.
  prefs: []
  type: TYPE_NORMAL
- en: They have also performed an ablation study to demonstrate the importance of
    DPO.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e2c781f71bc5c30bea85f74fa86313a.png)'
  prefs: []
  type: TYPE_IMG
- en: Table by [Tunstall et al. (2023)](https://arxiv.org/abs/2310.16944) —CC-BY 4.0
  prefs: []
  type: TYPE_NORMAL
- en: DPO alone (first row) poorly performs. However, the combination of DPO and SFT
    clearly outperforms SFT alone.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By relying on knowledge distillation, Hugging Face has demonstrated that it
    is possible to train and align a state-of-the-art LLM without using any human
    annotations.
  prefs: []
  type: TYPE_NORMAL
- en: Zephyr 7B Beta is a rather affordable model to make, especially compared to
    other larger models such as Llama 2 Chat 70B. However, given the size of the training
    batch (2) per GPU, and the fact that they fully fine-tuned Mistral 7B, they had
    to use 16 A100 80 GB GPUs (for up to 4 hours according to the technical report).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that you can use LoRA to train DPO with Hugging Face’s TRL library. It
    significantly reduces the memory consumption. Hugging Face didn’t use parameter-efficient
    fine-tuning methods such as LoRA, but they are rather optimistic that it would
    work as well as full fine-tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: We did not experiment with parameter-efficient techniques such as LoRA (Hu et
    al., 2021), but expect similar results to hold with these methods.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'To support my work, consider subscribing to my newsletter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://kaitchup.substack.com/?source=post_page-----c931fcd0bfe7--------------------------------)
    [## The Kaitchup - AI on a Budget | Benjamin Marie, PhD | Substack'
  prefs: []
  type: TYPE_NORMAL
- en: Weekly news, tips, and tutorials on fine-tuning, running, and serving large
    language models on your computer. Each…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: kaitchup.substack.com](https://kaitchup.substack.com/?source=post_page-----c931fcd0bfe7--------------------------------)
  prefs: []
  type: TYPE_NORMAL
