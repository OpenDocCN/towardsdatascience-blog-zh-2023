- en: The Best Optimization Algorithm for Your Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-best-optimization-algorithm-for-your-neural-network-d16d87ef15cb](https://towardsdatascience.com/the-best-optimization-algorithm-for-your-neural-network-d16d87ef15cb)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to choose it and minimize your neural network training time.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@riccardo.andreoni?source=post_page-----d16d87ef15cb--------------------------------)[![Riccardo
    Andreoni](../Images/5e22581e419639b373019a809d6e65c1.png)](https://medium.com/@riccardo.andreoni?source=post_page-----d16d87ef15cb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d16d87ef15cb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d16d87ef15cb--------------------------------)
    [Riccardo Andreoni](https://medium.com/@riccardo.andreoni?source=post_page-----d16d87ef15cb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d16d87ef15cb--------------------------------)
    ·13 min read·Oct 14, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d74e7bc343277d4b012a49a8ed2de2f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [unsplash.com](https://unsplash.com/photos/FfbVFLAVscw).'
  prefs: []
  type: TYPE_NORMAL
- en: Developing any machine learning model involves a rigorous experimental process
    that follows the [**idea-experiment-evaluation cycle**](/ai-ml-practicalities-the-cycle-of-experimentation-fd46fc1f3835).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89f8be024c84d92865f49b0077b29126.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The above cycle is repeated multiple times until satisfactory performance levels
    are achieved. The “experiment” phase involves both the coding and the training
    steps of the machine learning model. As **models become more complex** and are
    trained over much **larger datasets**, training time inevitably expands. As a
    consequence, training a large deep neural network can be painfully slow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately for data science practitioners, there exist several techniques
    to accelerate the training process, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transfer Learning**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weight Initialization**, as Glorot or He initialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch Normalization** for training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Picking a **reliable activation function**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a **faster optimizer**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While all the techniques I pointed out are important, in this post I will focus
    deeply on the last point. I will describe multiple algorithm for neural network
    parameters optimization, highlighting both their advantages and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: In the last section of this post, I will present a visualization displaying
    the comparison between the discussed optimization algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'For **practical implementation**, all the code used in this article can be
    accessed in this **GitHub repository**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[## articles/NN-optimizer at main · andreoniriccardo/articles'
  prefs: []
  type: TYPE_NORMAL
- en: Contribute to andreoniriccardo/articles development by creating an account on
    GitHub.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/andreoniriccardo/articles/tree/main/NN-optimizer?source=post_page-----d16d87ef15cb--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Batch Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditonally, Batch Gradient Descent is considered the **default choice** for
    the optimizer method in neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: After the neural network generates predictions for the entire training set *X*,
    we compare the network’s predictions to the actual labels of each training point.
    This is done to compute a cost function *J(W,b)*, which is a scalar representation
    of the capacity of the model to yield accurate predictions. The Gradient Descent
    optimization algorithm uses the cost function as a guide to tune every network’s
    parameter. This iterative process continues until the cost function is close to
    zero, or it can’t be reduced further. What GD does is to adjust each weight and
    bias of the network in a certain direction. The chosen direction is the one that
    reduces the cost function the most.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/032bf6c09d88228ba0b850a1ae27dcc4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: In visual terms, take the image above for reference. Gradient descent starts
    at the leftmost blue point. By analyzing the gradient of the cost function around
    the starting point, it adjusts the parameter (x-axis value) to the right. This
    process is repeated multiple times until the algorithm ends up with a very good
    approximation of the optimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the cost function has 2 input parameters, the function is not a line but
    instead, it creates a three-dimensional surface. The batch gradient descent steps
    can be displayed considering the level curves of the surface:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c005940ab8e986e586912d524d4b1965.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: A neural network consists of thousands or millions of parameters that affect
    the cost functions. While visualizing million-dimensional representations is infeasible,
    mathematical equations are useful to understand the gradient descent process.
  prefs: []
  type: TYPE_NORMAL
- en: 'At first, we compute the cost function *J(W,b)*, which is a function of the
    network’s weights *W* and biases *b*. Then, the backpropagation algorithm computes
    the derivative of the cost function with respect to each weight and bias of each
    layer of the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9fac62a55efc0ded9e5e592526f0797.png)![](../Images/d14e35916ec9cbcb3f90950c9b0f03b8.png)'
  prefs: []
  type: TYPE_IMG
- en: Knowing the direction towards which to adjust the parameters, we update them.
    The magnitude of each parameter’s update is regulated by the gradient itself and
    by the learning rate alpha. Alpha is a hyperparameter of the optimization algorithm
    and its value is typically maintained constant.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/653c4eb05a3afa4a33499ea2f81524f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Batch Gradient Descent offers several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simplicity**: It’s a straightforward and easy-to-understand method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyperparameter Management**: It requires tuning only one hyperparameter,
    the learning rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimality**: For convex cost functions, it reliably reaches the global optimum
    with a reasonable learning rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantages of gradient descent are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Computational Intensity**: It can be slow, particularly for large training
    sets, as it updates parameters after evaluating all examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local Optima and Saddle Points**: It might get stuck in local optima or saddle
    points, slowing convergence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradient Issues**: It is prone to vanishing gradient issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory Usage**: It requires storing the entire training data in CPU/GPU memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mini-Batch Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In batch gradient descent algorithm all training examples are used to calculate
    a single improvement step. The step will be the most accurate, as it considers
    all the available information. However, this approach often is impractically slow
    in real-world applications, suggesting the implementation of faster alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: The most common solution is called [**Mini-Batch Gradient Descent**](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/),
    and it uses only a **small subset of the training data** in order to update the
    weights’ values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the whole training set *X*, and the associated labels *Y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/775b6cf970b2d797aac374f7015b1acb.png)'
  prefs: []
  type: TYPE_IMG
- en: where *m* represents the number of training examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than feeding the whole batch to the optimization algorithm, we process
    only a small portion of the training set. Let’s say we feed the algorithm the
    subsets *X^t* and *Y^t*, each containing 512 training examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc8b776451f0b2634dcf04cb13cf92aa.png)'
  prefs: []
  type: TYPE_IMG
- en: For instance, if the total training set contains 5,120,000 points, it can be
    divided into 10,000 mini-batches, each containing 512 examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each mini-batch, we perform the classical gradient descent operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Compute the cost** relative to the mini-batch t, *J^t(W,b)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perform backpropagation** to compute the gradients of *J^t(W,b)* with respect
    to each weight and bias.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Update the parameters**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The green line in the image below shows a typical optimization path of the mini-batch
    gradient descent. While batch gradient descent follows a more direct path to the
    optimal point, mini-batch gradient descent appears to take several unnecessary
    steps, due to its limited dataset at each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c9c4d7274702fbc3b4c2dfedc82d47ce.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for that is that mini-batch gradient descent, at any time *t,* has
    only a small portion of the training set to compute its decision on. With that
    limitation on the available information, it’s clear that the route followed is
    not the most direct.
  prefs: []
  type: TYPE_NORMAL
- en: However, the significant advantage of mini-batch gradient descent, is that each
    step is extremely fast to be computed since the algorithm needs to evaluate only
    a small portion of the data instead of the whole training set. In our example,
    each step requires the evaluation of only 512 data points instead of 5 million.
    This is also the reason why almost no real application, requiring a large amount
    of data, uses batch gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: As the image shows, with mini-batch gradient descent there is no guarantee that
    the cost at iteration *t+1* is lower than the cost at iteration t, but, if the
    problem is well defined, the optimization algorithm reaches an area very close
    to the optimal point really quickly.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of the mini-batch size represents an additional hyperparameter of
    the network’s training process. A batch size equal to the total number of examples
    (*m*) corresponds to the batch gradient descent optimization. Instead, if *m=1*
    we are performing Stochastic Gradient Descent, where each training example is
    a mini-batch.
  prefs: []
  type: TYPE_NORMAL
- en: If the training set is small, batch gradient descent can be a valid option,
    otherwise, popular mini-batch sizes like 32, 64, 128, 256, and 512 are commonly
    considered. For some reason, a batch size equal to a power of 2 seems to perform
    better.
  prefs: []
  type: TYPE_NORMAL
- en: While stochastic gradient descent with a batch size of 1 is an option, it tends
    to be extremely noisy, and it can bounce further away from the minima. Moreover,
    stochastic gradient descent is very inefficient if we consider the computation
    time per example, as it cannot exploit the benefits of vectorization.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent with Momentum
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall that in both Batch and Mini-Batch Gradient Descent, the parameter update
    follows a defined formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/87fefcb663e3b7faa65b4eaa7be30835.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, in this equation, the size of each optimization step is determined
    by the learning rate, a fixed quantity, and the gradient, which is computed at
    a specific point of the cost function.
  prefs: []
  type: TYPE_NORMAL
- en: When the gradient is computed in a nearly flat section of the cost function,
    it will be very small, leading to a proportionally small step of gradient descent.
    Consider the difference the gradient difference at points A and B of the image
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/807b45e443cab6256ff027001474723b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Momentum Gradient Descent**](/gradient-descent-with-momentum-59420f626c8f)
    overcomes this issue. We can imagine momentum gradient descent as a bowling ball
    rolling down a hill, which shape is defined by the cost function. If the ball
    begins its descent from a steep portion of the slope, its movement begins slowly
    but it will quickly gain velocity and momentum. Due to its momentum, the ball
    maintains great velocity even through a -flat area of the slope.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the core concept of momentum gradient descent: the algorithm will consider
    previous gradients, and not only the one computed at iteration *t*. Similar to
    the bowling ball analogy, the gradient computed at iteration *t* defines the acceleration
    and not the speed of the ball.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Both weights and biases’ velocities are computed at each step using the previous
    velocity and the current iteration’s gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8a438ce7233a86faeebe73227e864497.png)'
  prefs: []
  type: TYPE_IMG
- en: The parameter *beta*, known as momentum, regulates how much the new velocity
    value is determined from the current slope or from the past velocity value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, parameters are updated using the computed velocity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/48791f6392f67a809a6c5c07ccae0868.png)'
  prefs: []
  type: TYPE_IMG
- en: Momentum Gradient Descent has proven superior performance compared to Mini-Batch
    Gradient Descent in most applications. The main drawback of Momentum Gradient
    Descent, with respect to standard Gradient Descent, is that it requires an additional
    parameter to tune. However, practice shows how a value of *beta* equal to 0.9
    works effectively.
  prefs: []
  type: TYPE_NORMAL
- en: RMS Prop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consider a cost function shaped similarly to an **elongated bowl**, where the
    minimum point is located in the narrowest part. The function’s contour is described
    by the level curves in the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c94d07c2aca361a7d27c0a0ee2e2f998.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: In cases where the starting point is far away from the minimum, Gradient Descent,
    also with the momentum variation, starts by following the steepest slope, which,
    in the picture above, is not the optimal path towards the minimum. The key idea
    of [**RMS Prop**](https://keras.io/api/optimizers/rmsprop) optimization algorithm
    is to correct the direction early and to aim for the global minimum more promptly.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly to Momentum Gradient Descent, RMS Prop requires fine-tuning through
    an additional hyperparameter known as the decay rate. Through practical experience,
    it has been proven that setting the decay rate to 0.9 is a good choice for most
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Adam
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[**Adam**](https://keras.io/api/optimizers/adam)and its variations are probably
    the most employed optimization algorithms for neural networks. Adam, which stands
    for **Adaptive Moment Estimation**, is derived from the combinations of Momentum
    Gradient Descent and RMS Prop together.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a mix of the two optimization methods, Adam needs two additional hyperparameters
    to be tuned (besides the learning rate *alpha*). We call them *beta_1* and *beta_2*
    and they are the hyperparameters used respectively in Momentum and in RMS Prop.
    As happened for the other discussed algorithms, effective default choices for
    *beta_1* and *beta_2* exist: *beta_1* is usually set to 0.9 and *beta_2* to 0.999\.
    In Adam appears also a parameter *epsilon* that works as a smoothing term and
    it is nearly always set to a small value like e-7 or e-8.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Adam optimization algorithm outperforms all the above-mentioned methods in
    most cases. The only exceptions can be very simple problems where simpler methods
    work faster. The efficiency of Adam does come with a trade-off: the need to fine-tune
    two additional parameters. However, in my opinion it is a small price to pay for
    its efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: Nadam and AdaMax
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is worth citing two algorithms born as modifications of the widely used
    Adam optimization: **Nadam** and **AdaMax**.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Nadam](https://keras.io/api/optimizers/Nadam), short of Nesterov-accelerated
    Adaptive Moment Estimation, enhances Adam by incorporating Nesterov Accelerated
    Gradient (NAG) into its framework. This means that Nadam not only benefits from
    the adaptive learning rates and momentum of Adam, but also from the NAG component,
    a technique that helps the algorithm predict the next step more accurately. Especially
    in high-dimensional spaces, Nadam converges faster and more effectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '[AdaMax](https://keras.io/api/optimizers/adamax), instead, adopts a slightly
    different approach. While Adam calculates adaptive learning rates based on both
    the first and second moments of the gradients, AdaMax focuses only on the maximum
    norm of the gradients. AdaMax’s simplicity and efficiency in handling sparse gradients
    make it an appealing choice for training deep neural networks, especially in tasks
    involving sparse data.'
  prefs: []
  type: TYPE_NORMAL
- en: Optimization Algorithms Comparison
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to practically test and visualize the performance of each discussed
    optimization algorithm, I trained a simple deep neural network with each one of
    the above optimizers. The job of the network is to classify fashion items displayed
    in 28x28 pixels images. The dataset is called [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist)
    (MIT licensed) and is composed of 70,000 small grey-scale images of clothing items.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm I used to test different optimizers is presented in the following
    snippet and, more completely in the GitHub repository linked below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[## articles/NN-optimizer at main · andreoniriccardo/articles'
  prefs: []
  type: TYPE_NORMAL
- en: Contribute to andreoniriccardo/articles development by creating an account on
    GitHub.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/andreoniriccardo/articles/tree/main/NN-optimizer?source=post_page-----d16d87ef15cb--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting plot is visualized in this line chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd630ab1d73df29ca08fc4dd809994a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: We can immediately see how the Momentum Gradient Descent (yellow line) is considerably
    faster than the standard Gradient Descent (blue line). Instead, RMS Prop (green
    line) seems to obtain similar results as Momentum Gradient Descent. This may be
    due to several reasons such as not perfectly tuned hyperparameters or a too-simple
    neural network. Finally, Adam optimizer (red line) looks superior, by a margin,
    to all the other methods.
  prefs: []
  type: TYPE_NORMAL
- en: Learning Rate Decay
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I want to include learning rate decay in this post because, even though it is
    not strictly an optimization algorithm, it is a powerful technique to **accelerate
    the network’s learning process**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aef64cc8a1598dbc30577bad03f3e05b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [unsplash.com](https://unsplash.com/photos/OyCl7Y4y0Bk).'
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate decay consists of the **reduction of the learning rate** hyperparameter
    *alpha* over the epochs. Theis adjustment is essential because at the initial
    stages of the optimization, the algorithm can afford to take bigger steps but,
    as it approaches the minimum point, we prefer to make smaller steps so that it
    will bounce in an area closer to the minimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'There exist several methods to decrease the learning rate over iterations.
    One approach is described by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cc1823be5f8dbec5cdcf155298ad11ed.png)'
  prefs: []
  type: TYPE_IMG
- en: where the decay rate is an additional parameter to tune.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, other possibilities are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c21313ad29ebd06c945998b69722069f.png)![](../Images/ec8ea285de373dc3220d44e361a23551.png)'
  prefs: []
  type: TYPE_IMG
- en: The first one is called Exponential Learning Rate Decay, and in the second one,
    parameter *k* is a constant.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it is also possible to apply a discrete learning rate decay, such as
    halving it after every *t* iterations, or decreasing it manually.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c2ad4436b2a23cca5d7f8e308e0a1c17.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time is a valuable, but also limited, resource for every data science practitioner.
    For this reason, mastering the tools to speed up a learning algorithm training
    can make a difference.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we have seen how the standard Gradient Descent optimizer is
    an outdated tool, and there exist several alternatives that provide better solutions
    in less time.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right optimizer for a given Machine Learning application is not
    always easy. It depends on the task and there is not a clear consensus yet on
    which one is the best. However, as we saw above, Adam optimizer with the proposed
    hyperparameters’ values is a valid choice most of the time, and knowing the functioning
    principle of the most popular ones is an excellent starting point.
  prefs: []
  type: TYPE_NORMAL
- en: If you liked this story, consider following me to be notified of my upcoming
    projects and articles!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of my past projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/social-network-analysis-with-networkx-a-gentle-introduction-6123eddced3?source=post_page-----d16d87ef15cb--------------------------------)
    [## Social Network Analysis with NetworkX: A Gentle Introduction'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how companies like Facebook and LinkedIn extract insights from networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/social-network-analysis-with-networkx-a-gentle-introduction-6123eddced3?source=post_page-----d16d87ef15cb--------------------------------)
    [](/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----d16d87ef15cb--------------------------------)
    [## Use Deep Learning to Generate Fantasy Names: Build a Language Model from Scratch'
  prefs: []
  type: TYPE_NORMAL
- en: Can a language model invent unique fantasy character names? Let’s build it from
    scratch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----d16d87ef15cb--------------------------------)
    [](/support-vector-machine-with-scikit-learn-a-friendly-introduction-a2969f2ff00d?source=post_page-----d16d87ef15cb--------------------------------)
    [## Support Vector Machine with Scikit-Learn: A Friendly Introduction'
  prefs: []
  type: TYPE_NORMAL
- en: Every data scientist should have SVM in their toolbox. Learn how to master this
    versatile model with a hands-on…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/support-vector-machine-with-scikit-learn-a-friendly-introduction-a2969f2ff00d?source=post_page-----d16d87ef15cb--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Gradient descent — Wikipedia.org](https://en.wikipedia.org/wiki/Gradient_descent)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Dive into Deep Learning — Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander
    J. Smola](https://d2l.ai/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet
    Classification — Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun](https://arxiv.org/abs/1502.01852v1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and
    Optimization — Andrew Ng](https://www.coursera.org/learn/deep-neural-network)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding the difficulty of training deep feedforward neural networks
    — Xavier Glorot, Yoshua Bengio](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition
    — Aurélien Géron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Keras Python library](https://keras.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
