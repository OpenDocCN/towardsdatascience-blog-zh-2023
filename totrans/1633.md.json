["```py\ndef self_attention(x):\n    k = x @ W_k\n    q = x @ W_q\n    v = x @ W_v\n    return softmax(q @ k.T) @ v\n\ndef transformer_block(x):\n    \"\"\" Pseudo code by author based on [2] \"\"\"\n    residual = x\n    x = self_attention(x)\n    x = layer_norm(x + residual)\n    residual = x\n    x = FFN(x)\n    x = layer_norm(x + residual)\n    return x\n```", "```py\ndef transformer_block_adapter(x):\n    \"\"\"Pseudo code from [2] \"\"\"\n    residual = x\n    x = self_attention(x)\n    x = FFN(x)  # adapter\n    x = layer_norm(x + residual)\n    residual = x\n    x = FFN(x)\n    x = FFN(x)  # adapter\n    x = layer_norm(x + residual)\n    return x\n```", "```py\ndef self_attention_ia3(x):\n    k = x @ W_k\n    q = x @ W_q\n    v = x @ W_v\n\n    k = l_k @ k  # ia3\n    v = l_v @ v  # ia3\n\n    return softmax(q @ k.T) @ v\n\ndef transformer_block_ia3(x):\n    \"\"\"Pseudo code from [2]\"\"\"\n    residual = x\n    x = self_attention_ia3(x)\n    x = layer_norm(x + residual)\n    residual = x\n    x = x @ W_1  # normal transformer\n    x = l_ff * gelu(x)  # ia3\n    x = x @ W_2\n    x = layer_norm(x + residual)\n    return x\n```", "```py\ndef prompt_tuning(seq_tokens, prompt_tokens):\n    \"\"\" Pseudo code from [2]. \"\"\"\n    x = seq_embedding(seq_tokens)\n    soft_prompt = prompt_embedding(prompt_tokens)\n    model_input = concat([soft_prompt, x], dim=seq)\n    return model(model_input)\n```", "```py\ndef transformer_block_prefix_tuning(x, soft_prompt):\n    \"\"\" Pseudo code from [2] \"\"\"\n    soft_prompt = FFN(soft_prompt)\n    model_input = concat([soft_prompt, x], dim=seq)\n    return model(model_input)\n```", "```py\ndef p_tuning(seq_tokens, prompt_tokens):\n    \"\"\"Pseudo code for p-tuning created by Author.\"\"\"\n    h = prompt_embedding(prompt_tokens)\n    h = LSMT(h, bidirectional=True)\n    h = FFN(h)\n\n    x = seq_embedding(seq_tokens)\n    model_input = concat([h, x], dim=seq)\n\n    return model(model_input)\n```", "```py\n def transformer_block_llama_adapter(x, soft_prompt, gating_factor):\n    \"\"\"LLaMA-Adapter pseudo code created by Author\"\"\"\n    residual = x\n\n    adaption_prompt = concat([soft_prompt, x], dim=seq)\n    adaption_prompt = self_attention(adaption_prompt) * gating_factor  # zero-init attention\n\n    x = self_attention(x)\n    x = adaption_prompt * x\n    x = layer_norm(x + residual)\n    residual = x\n    x = FFN(x)\n    x = layer_norm(x + residual)\n\n    return x\n```", "```py\ndef lora_linear(x, W):\n    scale = 1 / r  # r is rank\n    h = x @ W\n    h += x @ W_a @ W_b  # W_a,W_b determined based on W\n    return scale * h\n\ndef self_attention_lora(x):\n    \"\"\" Pseudo code from Lialin et al. [2].\"\"\"\n\n    k = lora_linear(x, W_k)\n    q = x @ W_q\n    v = lora_linear(x, W_v)\n    return softmax(q @ k.T) @ v\n```", "```py\n def adalora_linear(x, W, curr_sv):\n    scale = alpha / r  # r is rank\n    h = x @ W\n\n    # p, lamda, and q are related to the W matrix\n    # curr_sv marks which singular vectors we are currently optimizing. \n    h += x @ p[curr_sv] @ lamda[curr_sv] @ q[curr_sv]\n    return scale * h\n\ndef self_attention_lora(x):\n    \"\"\"\n    AdaLoRa pseudo code created by author. \n    This only shows the difference in the self_attention block. \n    Does not include code for pruning techniques.\n    \"\"\"\n    k = adalora_linear(x, W_k)\n    q = x @ W_q\n    v = adalora_linear(x, W_v)\n\n    return softmax(q @ k.T) @ v\n```"]