["```py\nclass ConvolutionLayer:\n    def __init__(self, kernel_num, kernel_size):\n        self.kernel_num = kernel_num\n        self.kernel_size = kernel_size        \n        self.kernels = np.random.randn(kernel_num, kernel_size, kernel_size) / (kernel_size**2)\n\n    def patches_generator(self, image):\n        image_h, image_w = image.shape\n        self.image = image\n        for h in range(image_h-self.kernel_size+1):\n            for w in range(image_w-self.kernel_size+1):\n                patch = image[h:(h+self.kernel_size), w:(w+self.kernel_size)]\n                yield patch, h, w\n\n    def forward_prop(self, image):\n        image_h, image_w = image.shape\n        convolution_output = np.zeros((image_h-self.kernel_size+1, image_w-self.kernel_size+1, self.kernel_num))\n        for patch, h, w in self.patches_generator(image):\n            convolution_output[h,w] = np.sum(patch*self.kernels, axis=(1,2))\n        return convolution_output\n\n    def back_prop(self, dE_dY, alpha):\n        dE_dk = np.zeros(self.kernels.shape)\n        for patch, h, w in self.patches_generator(self.image):\n            for f in range(self.kernel_num):\n                dE_dk[f] += patch * dE_dY[h, w, f]\n        self.kernels -= alpha*dE_dk\n        return dE_dk\n```", "```py\nclass MaxPoolingLayer:\n    def __init__(self, kernel_size):\n        self.kernel_size = kernel_size\n\n    def patches_generator(self, image):\n        output_h = image.shape[0] // self.kernel_size\n        output_w = image.shape[1] // self.kernel_size\n        self.image = image\n\n        for h in range(output_h):\n            for w in range(output_w):\n                patch = image[(h*self.kernel_size):(h*self.kernel_size+self.kernel_size), (w*self.kernel_size):(w*self.kernel_size+self.kernel_size)]\n                yield patch, h, w\n\n    def forward_prop(self, image):\n        image_h, image_w, num_kernels = image.shape\n        max_pooling_output = np.zeros((image_h//self.kernel_size, image_w//self.kernel_size, num_kernels))\n        for patch, h, w in self.patches_generator(image):\n            max_pooling_output[h,w] = np.amax(patch, axis=(0,1))\n        return max_pooling_output\n\n    def back_prop(self, dE_dY):\n        dE_dk = np.zeros(self.image.shape)\n        for patch,h,w in self.patches_generator(self.image):\n            image_h, image_w, num_kernels = patch.shape\n            max_val = np.amax(patch, axis=(0,1))\n\n            for idx_h in range(image_h):\n                for idx_w in range(image_w):\n                    for idx_k in range(num_kernels):\n                        if patch[idx_h,idx_w,idx_k] == max_val[idx_k]:\n                            dE_dk[h*self.kernel_size+idx_h, w*self.kernel_size+idx_w, idx_k] = dE_dY[h,w,idx_k]\n            return dE_dk\n```", "```py\nclass SoftmaxLayer:\n    def __init__(self, input_units, output_units):\n        self.weight = np.random.randn(input_units, output_units)/input_units\n        self.bias = np.zeros(output_units)\n\n    def forward_prop(self, image):\n        self.original_shape = image.shape\n        image_flattened = image.flatten()\n        self.flattened_input = image_flattened\n        first_output = np.dot(image_flattened, self.weight) + self.bias\n        self.output = first_output\n        softmax_output = np.exp(first_output) / np.sum(np.exp(first_output), axis=0)\n        return softmax_output\n\n    def back_prop(self, dE_dY, alpha):\n        for i, gradient in enumerate(dE_dY):\n            if gradient == 0:\n                continue\n            transformation_eq = np.exp(self.output)\n            S_total = np.sum(transformation_eq)\n\n            dY_dZ = -transformation_eq[i]*transformation_eq / (S_total**2)\n            dY_dZ[i] = transformation_eq[i]*(S_total - transformation_eq[i]) / (S_total**2)\n\n            dZ_dw = self.flattened_input\n            dZ_db = 1\n            dZ_dX = self.weight\n\n            dE_dZ = gradient * dY_dZ\n\n            dE_dw = dZ_dw[np.newaxis].T @ dE_dZ[np.newaxis]\n            dE_db = dE_dZ * dZ_db\n            dE_dX = dZ_dX @ dE_dZ\n\n            self.weight -= alpha*dE_dw\n            self.bias -= alpha*dE_db\n\n            return dE_dX.reshape(self.original_shape)\n```"]