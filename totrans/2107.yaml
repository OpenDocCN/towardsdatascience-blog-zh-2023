- en: 'Time-Series Forecasting: Deep Learning vs Statistics — Who Wins?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df](https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A comprehensive guide on the ultimate dilemma
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@nikoskafritsas?source=post_page-----c568389d02df--------------------------------)[![Nikos
    Kafritsas](../Images/de965cfcd8fbd8e1baf849017d365cbb.png)](https://medium.com/@nikoskafritsas?source=post_page-----c568389d02df--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c568389d02df--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c568389d02df--------------------------------)
    [Nikos Kafritsas](https://medium.com/@nikoskafritsas?source=post_page-----c568389d02df--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c568389d02df--------------------------------)
    ·14 min read·Apr 5, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91b4bda3cb99b5a14f28d669d8fa4454.png)'
  prefs: []
  type: TYPE_IMG
- en: Created with Stable Diffusion [1]
  prefs: []
  type: TYPE_NORMAL
- en: '**In recent years, Deep Learning has made remarkable progress in the field
    of NLP.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Time series, also sequential in nature, raise the question: *what happens if
    we bring the full power of pretrained transformers to time-series forecasting*?'
  prefs: []
  type: TYPE_NORMAL
- en: However, some papers, such as [2] and [3] have scrutinized Deep Learning models.
    These papers do not present the full picture. Even for NLP cases, some people
    attribute the breakthrough of GPT models to “*more data and computing power*”
    instead of “*better ML research*”.
  prefs: []
  type: TYPE_NORMAL
- en: 'This article aims to clear the confusion and provide an unbiased view, using
    reliable data and sources from both academia and industry. Specifically, we will
    cover:'
  prefs: []
  type: TYPE_NORMAL
- en: The pros and cons of **Deep Learning** and **Statistical Models**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When to use Statistical models and when Deep Learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to approach a forecasting case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to save time and money by selecting the best model for your case and dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s dive in.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve launched [**AI Horizon Forecast**](https://aihorizonforecast.substack.com)**,**
    a newsletter focusing on time-series and innovative AI research. Subscribe [here](https://aihorizonforecast.substack.com)
    to broaden your horizons!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://aihorizonforecast.substack.com/p/autogluon-timeseries-creating-powerful?source=post_page-----c568389d02df--------------------------------)
    [## AutoGluon-TimeSeries : Creating Powerful Ensemble Forecasts - Complete Tutorial'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon's framework for time-series forecasting has it all.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: aihorizonforecast.substack.com](https://aihorizonforecast.substack.com/p/autogluon-timeseries-creating-powerful?source=post_page-----c568389d02df--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Makridakis et al. Paper [4]
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can’t discuss the progress of different forecasting models without considering
    the insights gained from **Makridakis competitions (M competitions).**
  prefs: []
  type: TYPE_NORMAL
- en: Makridakis competitions are a series of large-scale challenges that demonstrate
    the latest advancements in time-series forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, Makridakis et al. published a new paper that:'
  prefs: []
  type: TYPE_NORMAL
- en: Summarizes the current state of forecasting from the first 5 M-competitions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides an extensive benchmark of various statistical, machine learning, and
    deep learning forecasting models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Note:** We will discuss the limitations of the paper later in this article.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Benchmark Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditionally, Makridakis and his associates release a paper summarizing the
    results of the last M-competition.
  prefs: []
  type: TYPE_NORMAL
- en: For the first time, however, the authors have included Deep Learning models
    in their experiments. Why?
  prefs: []
  type: TYPE_NORMAL
- en: Unlike NLP, it was only in 2018–2019 that the first DL forecasting models were
    mature enough to challenge traditional forecasting models. In fact, during the
    M4 competition in 2018, the ML/DL models ranked last.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ade71ca0a631e914e519cd932c42006e.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 1:** Forecasting accuracy (sMAPE) of the eight statistical and the
    ten ML forecasting methods examined by Makridakis et al. back in 2018\. All ML
    methods occupied the last places.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s see the DL/ML models that were used in the **new** paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multi-layer Perceptron (MLP):** Ourfamiliar feed-forward network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**WaveNet:** An autoregressive neural network that combines convolutional layers
    (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformer:** The original Transformer, introduced in 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**DeepAR**](https://medium.com/towards-data-science/deepar-mastering-time-series-forecasting-with-deep-learning-bc717771ce85)**:**
    Amazon’s first successful auto-regressive network that combines LSTMs (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Note:** The Deep Learning models of that study are not SOTA (state-of-the-art)
    anymore (more to that later). Also MLP is considered an ML and not a “deep” model.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The statistical models of the benchmark are **ARIMA** and **ETS** (Exponential
    Smoothing) — well-known & battle-tested models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover:'
  prefs: []
  type: TYPE_NORMAL
- en: The ML/DL models were fine-tuned first through hyper-parameter tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The statistical models are trained in a **series-by-series** fashion. Conversely,
    the DL models are **global** (a single model trained on all time series of the
    dataset). Hence, they take advantage of *cross-learning*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The authors used ensembling: An **Ensemble-DL** modelwas createdfrom Deep Learning
    models, and an **Ensemble-S,** consisting of statistical models. The ensembling
    method was the median of forecasts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **Ensemble-DL** consists of 200 models, with 50 models from each category:
    DeepAR, Transformer, WaveNet, and MLP.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The study utilized the M3 dataset: First, the authors tested 1,045 time series,
    and then the full dataset (3,003 series).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors measured forecasting accuracy using **MASE** (*Mean Absolute Scaled
    Error*) and **SMAPE** (*Mean Absolute Percentage Error*). These error metrics
    are commonly used in forecasting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we provide a summary of the results and conclusions obtained from the
    benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Deep Learning Models are Better
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The authors conclude that on average, DL models outperform the statistical ones.
    The result is shown in **Figure 2:**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c2c408fbc353c3770e58de0b677be013.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 2:** Average ranks and 95% confidence intervals of all models, using
    sMAPE used for ranking.'
  prefs: []
  type: TYPE_NORMAL
- en: The **Ensemble-DL** model clearly outperforms the **Ensemble-S.** Also, DeepAR
    achieves very similar results with **Ensemble-S.**
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, **Figure 2** shows that although **Ensemble-DL** outperforms
    **Ensemble-S,** onlyDeepAR beats the individual statistical models. Why is that?
  prefs: []
  type: TYPE_NORMAL
- en: We will answer that question later in the article.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. But, Deep Learning Models Are Expensive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Learning models require a lot of time to train (and money). This is expected.
    The results are shown in **Figure 3:**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ae77f636242bc7f97e59c822a7bcea8.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 3:** SMAPE vs Computational time. An ln(CT) of zero corresponds to
    about 1 minute of computational time, while an ln(CT) of 2, 4, 6, 8, and 10 correspond
    to about 7 minutes, 1 hour, 7 hours, 2 days, and 15 days, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: The computational difference is significant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence**,** lowering the forecasting error by 10% requires an extra computational
    time of about 15 days(**Ensemble-DL**). While this number seems enormous, there
    are some things to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: The authors do not specify what type of hardware they used.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They also don’t mention if any parallelization or training optimization is used.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The computational time of **Ensemble-DL** can be significantly reduced if fewer
    models are used in the ensemble. This is displayed in **Figure 4:**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/b944af6cece9a2d33eef11b463345c05.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4:** SMAPE vs number of models in the **Ensemble-DL** model.'
  prefs: []
  type: TYPE_NORMAL
- en: I mentioned previously that the **Ensemble-DL** model is an ensemble of 200
    DL models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure 4** shows that 75 models can achieve comparable accuracy to 200 models
    with only one-third of the computational cost. This number can be further reduced
    if a more clever method to do the ensemble is used.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the current paper does not explore the transfer-learning capabilities
    of Deep Learning models. We will also discuss that later.
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Ensembling is All You Need**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The power of ensembling is indisputable (**Figure 2**, **Figure 3**).
  prefs: []
  type: TYPE_NORMAL
- en: Both **Ensemble-DL** and **Ensemble-SL** are the top-performing models. The
    idea is that each individual model excels at capturing different temporal dynamics.
    Combining their predictions enables the identification of complex patterns and
    accurate extrapolation.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Short-term vs Long-Term Forecasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The authors investigated whether there is a difference in models’ ability to
    forecast in the short-term versus the long-term.
  prefs: []
  type: TYPE_NORMAL
- en: There was indeed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure 5** breaks down the accuracy of each model for each forecasting horizon.
    For example, column 1 displays the one-step ahead forecast error. Similarly, column
    18 displays the error of the 18th step-ahead forecast.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26653f7678bc42c6b860bd9c7fe8eb09.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 5:** sMAPE error of every model for 1045 series — lower is better
    ([Click here to see full image](https://gist.github.com/nkafr/ef8d2a84aafd626bc603cc4234291135))'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 3 key observations here:'
  prefs: []
  type: TYPE_NORMAL
- en: First, long-term forecasts are less accurate than short-term ones (no surprise
    here).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the first 4 horizons, **statistical models win.** Beyond that, **Deep Learning
    models start becoming better and Ensemble-DL wins.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specifically, in the first horizon, **Ensemble-S** is 8.1% more accurate. However,
    in the last horizon, **Ensemble-DL** is 8.5% more accurate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you think about this, it makes sense:'
  prefs: []
  type: TYPE_NORMAL
- en: Statistical models are auto-regressive. As the forecasting horizons increase,
    the errors accumulate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In contrast, deep learning models are multi-output models. Hence, their forecasting
    errors are distributed across the entire prediction sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The only DL autoregressive model is DeepAR. That’s why DeepAR performs very
    well in the first horizons contrary to the other DL models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5\. Do Deep Learning Models improve with more data?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous experiment, the authors used just 1,045 time series from the
    M3 dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the authors re-run their experiment using the full dataset (3,003 series).
    They also analyzed the forecasting losses per horizon. The results are shown in
    **Figure 6:**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4618207b7663ce42dc9d9917e0793f47.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6:** sMAPE error of every model for 3003 series — lower is better
    ([Click here to see full image](https://gist.github.com/nkafr/f1c86b825271abf55ce8577691ac669d))'
  prefs: []
  type: TYPE_NORMAL
- en: Now, the gap between **Ensemble-DL** and **Ensemble-S** narrowed. The statistical
    models matched the deep learning models in the first horizon, but after that,
    the Ensemble-DL outperformed them.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s further analyze the differences between **Ensemble-DL** and **Ensemble-S:**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/236bd9ebe811660305d9da620d17148e.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 7:** Percentage improvement of Ensemble-DL over Ensemble-S ([Click
    here to see full image](https://gist.github.com/nkafr/d742007a350ab94c3827e191fc7e203c))'
  prefs: []
  type: TYPE_NORMAL
- en: As the prediction step increases, Deep Learning models outperform the statistical
    ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. On Trend and Seasonality Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, the authors investigate how statistical and DL models handle important
    time series characteristics like trend and seasonality.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, the authors used the methodology by [5]. Specifically, they
    fitted a multiple linear regression model that correlated sMAPE error with 5 key
    time series characteristics: **forecastability(**randomness of errors**)**, **trend**,
    **seasonality**, **linearity**, and **stability(**optimal Box-Cox parameter transformation
    that decides data normality**)**. The results are shown in **Figure 8**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0b9990bd9c33f21416f7c186681903e3.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 8:** Linear Regression coefficients0 for different metrics. Lower
    is better'
  prefs: []
  type: TYPE_NORMAL
- en: 'We observe that:'
  prefs: []
  type: TYPE_NORMAL
- en: DL models perform better with **noisy, trended,** and **non-linear** data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical models are more appropriate for **seasonal** & **low-variance**
    data with **linear relationships.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These insights are invaluable.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, it is crucial to conduct extensive exploratory data analysis (EDA) and
    understand the nature of the data before selecting the appropriate model for your
    use case.
  prefs: []
  type: TYPE_NORMAL
- en: Study’s Limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This paper is undoubtedly one of the best studies on the current state of the
    time-series forecasting landscape, yet it has some limitations. Let’s examine
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lack of ML algorithms: Trees / Boosted Trees'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The family of Boosted Trees models has a significant place in time series forecasting
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: The most popular ones are XGBoost, LightGBM, and CatBoost. Besides, LightGBM
    won the M5 competition.
  prefs: []
  type: TYPE_NORMAL
- en: These models excel with tabular-like data. In fact, to this day, Boosted Trees
    are the best choice for tabular data. However, the M3 dataset used in this study
    is simple as it mostly contains univariate series.
  prefs: []
  type: TYPE_NORMAL
- en: In a future study, it would be a great idea to add Boosted Trees to the dataset,
    especially for more complex datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing M3 as the Benchmark Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Professor Rob Hyndman, Editor-in-Chief of the IJF journal said: “*The M3 dataset
    has been used since 2000 for testing forecasting methods; newly proposed methods
    must beat M3 to be published in IJF.*”'
  prefs: []
  type: TYPE_NORMAL
- en: However, by modern standards, the M3 dataset is considered small and simple,
    and therefore not indicative of modern forecasting applications and practical
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the choice of the dataset does not diminish the value of the study.
    Nevertheless, conducting a future benchmark with a larger dataset could provide
    valuable insights.
  prefs: []
  type: TYPE_NORMAL
- en: The Deep Learning Models are not SOTA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, it’s time to address the elephant in the room.
  prefs: []
  type: TYPE_NORMAL
- en: The Deep Learning models of the study are far from being state-of-the-art.
  prefs: []
  type: TYPE_NORMAL
- en: The study identified Amazon’s DeepAR as the best DL model in terms of theoretical
    forecasting accuracy. That’s why, DeepAR was the only model capable of outperforming
    the statistical models on an individual level. However, the DeepAR model is now
    more than 6 years old.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon has since released its improved version of DeepAR, called [**Deep GPVAR**](https://medium.com/towards-data-science/deep-gpvar-upgrading-deepar-for-multi-dimensional-forecasting-e39204d90af3).
    In fact, **Deep GPVAR** is also outdated — Amazon’s latest Deep forecasting model
    is the MQTransformer, which was published in 2020.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, other powerful models like [**Temporal Fusion Transformer**](https://medium.com/towards-data-science/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91)
    (TFT) and [**N-BEATS**](https://medium.com/towards-data-science/n-beats-time-series-forecasting-with-neural-basis-expansion-af09ea39f538)
    (which was recently outperformed by N-HITS) are also not used in the Deep Learning
    ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the deep learning models used in the study are at least two generations
    behind the current state of the art. Undoubtedly, the current generation of deep
    forecasting models would have produced much better results.
  prefs: []
  type: TYPE_NORMAL
- en: Forecasting is not Everything
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Accuracy is essential in forecasting, but it’s not the only important factor.
    Other critical areas are:'
  prefs: []
  type: TYPE_NORMAL
- en: Uncertainty quantification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forecast interpretability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero-Shot Learning / Meta-Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regime Shift Segregation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speaking of **Zero-Shot Learning,** it’s one of the most promising areas in
    AI.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot learning is a model’s ability to correctly estimate unseen data, without
    being specifically trained on them. This learning method better reflects the human
    perception.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All Deep Learning models, including the GPT models, are based on this principle.
  prefs: []
  type: TYPE_NORMAL
- en: The first well-acclaimed forecasting models that leverage this principle are
    [N-BEATS / N-HITS](https://medium.com/towards-data-science/n-beats-time-series-forecasting-with-neural-basis-expansion-af09ea39f538).
    These models can be trained on a vast time-series dataset and produce forecasts
    on completely novel data with similar accuracy as if the models had been explicitly
    trained on them.
  prefs: []
  type: TYPE_NORMAL
- en: '*Zero-shot learning* is just a specific instance of *meta-learning*. Further
    progress with meta-learning on time-series has been made since. Take the [M6 competition](https://m6competition.com)
    for example, whose goal was to find if data science forecasting & econometrics
    can be used to beat the market, like legendary investors do (e.g. Warren Buffet).
    [The winning solution was a novel architecture that used, among other, neural
    networks and meta-learning](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4355794).'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, this study does not explore the competitive advantage of Deep
    Learning models in a zero-shot learning setup.
  prefs: []
  type: TYPE_NORMAL
- en: The Nixtla Study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nixtla, a promising start-up in the field of time-series forecasting, recently
    published a [benchmark](https://github.com/Nixtla/statsforecast/tree/main/experiments/m3)
    follow-up to the Makridakis et al. paper [4].
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the Nixtla team added 2 additional models: [Complex Exponential
    Smoothing](https://onlinelibrary.wiley.com/doi/full/10.1002/nav.22074) and [Dynamic
    Optimized Theta](https://nixtla.github.io/statsforecast/models.html#dynamic-optimized-theta-method).'
  prefs: []
  type: TYPE_NORMAL
- en: The addition of these models reduced the gap between statistical and deep learning
    models. Furthermore, the Nixtla team correctly pointed out the significant difference
    in cost and resources required between the two categories.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, many data scientists are misled by the overhyped promises of Deep Learning
    and lack the proper approach to solving a forecasting problem.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss this further in the next section. But before that, we need to
    address the criticism that Deep Learning faces.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning Under Fire
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The progress of Deep Learning during the past decade is phenomenal. And there
    are no signs yet of slowing down.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, every revolutionary breakthrough that threatens to change the status
    quo is often met with skepticism and criticism. Take GPT-4 for example: this new
    development threatens 20% of US jobs in the next decade [6].'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dominance of Deep Learning and Transformers in the field of NLP is undeniable.
    And yet, people in interviews ask questions that go something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Are the advances in NLP attributed to better research, or simply to the availability
    of more data and increased computer power?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In time-series forecasting, things are worse. To understand the reason behind
    this, you must first comprehend how forecasting problems were traditionally tackled.
  prefs: []
  type: TYPE_NORMAL
- en: Before the widespread adoption of ML/DL, forecasting was all about crafting
    the right transformations for your dataset. This entailed making the time-series
    stationary, removing trends and seasonalities, accounting for volatility, and
    using techniques like box-cox transformations, among others. All of these approaches
    required manual intervention and a deep understanding of mathematics and time
    series.
  prefs: []
  type: TYPE_NORMAL
- en: With the advent of ML, time-series algorithms became more automated. You can
    readily apply them to time-series problems with little to no preprocessing aside
    from cleaning (although additional preprocessing and feature engineering always
    help). Nowadays, much of the improvement effort on such a project is limited to
    hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, people who used advanced maths & statistics cannot grasp the fact
    that an ML/DL algorithm can outperform a traditional statistical model. And the
    funny thing is, researchers have no idea how and why some DL concepts truly work.
  prefs: []
  type: TYPE_NORMAL
- en: Time-Series Forecasting in Recent Literature
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As far as I know, the current literature lacks sufficient evidence to illustrate
    the advantages and disadvantages of various categories of forecasting models.
    The 2 papers below are the most relevant:'
  prefs: []
  type: TYPE_NORMAL
- en: Are Transformers Effective for Time Series Forecasting?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One interesting paper [2] displays the weaknesses of some forecasting Transformers
    models. The paper explains for example how positional encoding schemes, used in
    modern Transformer models fail to capture the temporal dynamics of time sequences.
    This is true — **self-attention is permutation-invariant**. However, the paper
    fails to mention the Transformer models that have effectively addressed this issue.
  prefs: []
  type: TYPE_NORMAL
- en: For example, Google’s [Temporal Fusion Transformer](https://medium.com/towards-data-science/temporal-fusion-transformer-googles-model-for-interpretable-time-series-forecasting-5aa17beb621)
    (TFT) uses an encoder-decoder LSTM layer to create time-aware and context-aware
    embeddings. Also, TFT uses a novel attention mechanism, adapted for time-series
    problems to capture temporal dynamics and provide interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, Amazon’s MQTransformer uses its novel positional encoding scheme
    (***context-dependent seasonality encoding***) and attention mechanism (**f*eedback-aware
    attention***).
  prefs: []
  type: TYPE_NORMAL
- en: Do We Really Need DL Models for Time Series Forecasting?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper [3] is also interesting as it compares various forecasting methods
    across **statistical**, **Boosted Trees**, **ML**, and **DL** categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, it falls short of its title, as the best model among the 12
    models is Google’s TFT, a pure Deep Learning model. The paper mentions:'
  prefs: []
  type: TYPE_NORMAL
- en: … The results in Table 5 above underline the competitiveness of the rolling
    forecast configured GBRT, but also show that considerably stronger transformer-based
    models, such as the TFT [12], rightfully surpass the boosted regression tree performance..
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In general, be cautious when reading sophisticated forecasting papers and models,
    particularly regarding the source of publication. The [International Journal of
    Forecasting (IJF)](https://www.sciencedirect.com/journal/international-journal-of-forecasting)
    is an example of a reputable journal focusing on forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: How to Approach a Forecasting Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is not simple. Each dataset is unique, and the objectives of each project
    vary, making forecasting challenging.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, this article offers general advice that may be beneficial for
    most approaches.
  prefs: []
  type: TYPE_NORMAL
- en: As you have learned from this article, Deep Learning models are an emerging
    trend in forecasting projects, but they are still in their early stages. Despite
    their potential, they can also be a pitfall.
  prefs: []
  type: TYPE_NORMAL
- en: It is not recommended to prioritize Deep Learning models for your project right
    away. According to Makridakis et al. and Nixtla's studies, it is best to start
    with statistical models. An ensemble of 3–4 statistical models may be more powerful
    than you expect. Also, give boosted trees a try, especially if you have tabular
    data. For small datasets (in the order of thousands), these methods may be adequate.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning models may provide an additional 3–10% accuracy boost. However,
    training these models can be time-consuming and expensive. For some fields, such
    as finance and retail, that extra accuracy boost may be more beneficial and justify
    using a DL model. A more accurate product sales prediction or an ETF’s closing
    price might translate to thousands of dollars in incremental revenue.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, DL models like N-BEATS and N-HITS have transfer-learning
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: If a large enough time-series dataset is constructed, and a willing entity pre-trains
    those 2 models and shares their parameters, we could readily use these models
    and achieve top-notch forecasting accuracy (or perform a small fine-tuning to
    our dataset first).
  prefs: []
  type: TYPE_NORMAL
- en: Closing Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time-series forecasting is a key area of Data Science.
  prefs: []
  type: TYPE_NORMAL
- en: But it’s also very undervalued compared to other areas. The Makridakis et al.
    paper[4] provided some valuable insights for the future, but there is still a
    lot of work and research to be done.
  prefs: []
  type: TYPE_NORMAL
- en: On top of that, DL models in forecasting are largely unexplored.
  prefs: []
  type: TYPE_NORMAL
- en: For example, **Multi-Modal** architecures in Deep-Leaning are everywhere. These
    architectures leverage more than one domain to learn a specific task. For instance,
    [**CLIP**](https://medium.com/p/f8ee408958b1) (used by **DALLE-2** and **Stable
    Diffusion**) combines *Natural Language Processing* and *Computer Vision*.
  prefs: []
  type: TYPE_NORMAL
- en: The benchmark M3 dataset contains only 3,003 time series, each with no more
    than 500 observations. In contrast, the successful [**Deep GPVAR**](https://medium.com/p/e39204d90af3)
    forecasting model consists of an average of 44K parameters. In comparison, the
    smallest version of Facebook’s LLaMA language Transformer model has 7 billion
    parameters and was trained on 1 trillion tokens.
  prefs: []
  type: TYPE_NORMAL
- en: So, regarding the original question, there is no definitive answer as to which
    model is the best since each model has its own advantages and shortcomings.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, this article aims to provide all the necessary information to help
    you select the most suitable model for your project or case.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Follow me on [Linkedin](https://www.linkedin.com/in/nikos-kafritsas-b3699180/)!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subscribe to my [newsletter](https://aihorizonforecast.substack.com/welcome),
    AI Horizon Forecast!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](https://aihorizonforecast.substack.com/p/autogluon-timeseries-creating-powerful?source=post_page-----c568389d02df--------------------------------)
    [## AutoGluon-TimeSeries : Creating Powerful Ensemble Forecasts - Complete Tutorial'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon's framework for time-series forecasting has it all.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: aihorizonforecast.substack.com](https://aihorizonforecast.substack.com/p/autogluon-timeseries-creating-powerful?source=post_page-----c568389d02df--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Created from Stable Diffusion with the text prompt “a blue cyan time-series
    abstract, shiny, digital painting, concept art”'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Ailing Zeng et al. [*Are Transformers Effective for Time Series Forecasting?*](https://arxiv.org/pdf/2205.13504.pdf)
    (August 2022)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Shereen Elsayed et al. [*Do We Really Need Deep Learning Models for Time
    Series Forecasting?*](https://arxiv.org/pdf/2101.02118.pdf)(October 2021)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Makridakis et al. [*Statistical, machine learning and deep learning forecasting
    methods: Comparisons and ways forward*](https://www.tandfonline.com/doi/epdf/10.1080/01605682.2022.2118629)
    *(*August 2022)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Kang et al. [*Visualising forecasting algorithm performance using time
    series instance spaces*](https://www.sciencedirect.com/science/article/abs/pii/S0169207016301030)
    *(*International Journal of Forecasting, 2017)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Eloundou et al. [*GPTs are GPTs: An Early Look at the Labor Market Impact
    Potential of Large Language Models*](https://arxiv.org/pdf/2303.10130.pdf)(March
    2023)'
  prefs: []
  type: TYPE_NORMAL
- en: All images used in the article are from [4]. [The M3 dataset as well as all
    M-datasets “are free to use without further permission by the IIF”](https://forecasters.org/resources/time-series-data/).
  prefs: []
  type: TYPE_NORMAL
