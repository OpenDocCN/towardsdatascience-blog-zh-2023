- en: 'AI Music Source Separation: How it Works and Why It Is So Hard'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI音乐源分离：如何运作以及为何如此困难
- en: 原文：[https://towardsdatascience.com/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752](https://towardsdatascience.com/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752](https://towardsdatascience.com/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752)
- en: Source Separation AI, explained
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 源分离AI的解释
- en: '[](https://medium.com/@maxhilsdorf?source=post_page-----187852e54752--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page-----187852e54752--------------------------------)[](https://towardsdatascience.com/?source=post_page-----187852e54752--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----187852e54752--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page-----187852e54752--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maxhilsdorf?source=post_page-----187852e54752--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page-----187852e54752--------------------------------)[](https://towardsdatascience.com/?source=post_page-----187852e54752--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----187852e54752--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page-----187852e54752--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----187852e54752--------------------------------)
    ·9 min read·Sep 21, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----187852e54752--------------------------------)
    ·阅读时间9分钟·2023年9月21日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/1592eebc641690038dd7c665c19845a6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1592eebc641690038dd7c665c19845a6.png)'
- en: Image by Author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: Source Separation
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 源分离
- en: What is Source Separation?
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是源分离？
- en: In the field of signal processing, source separation describes the task of breaking
    down an audio signal into multiple source audio signals. This concept is not only
    relevant for music, but also for speech or machine sounds. For example, you might
    want to separate the voices of two speakers in a podcast so you can edit the voices
    separately.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在信号处理领域，源分离描述了将音频信号拆分成多个源音频信号的任务。这个概念不仅适用于音乐，也适用于语音或机器声音。例如，您可能希望将播客中两个说话者的声音分开，以便可以分别编辑这些声音。
- en: Why is Source Separation so Difficult?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么源分离如此困难？
- en: Not everyone is a musician. Even fewer people are musicians with a penchant
    for data & AI. Oftentimes, when I talk to non-musicians, I get the impression
    that they think you can simply “take the voice and remove it from the audio”.
    This makes sense, because why else would there be instrumentals on the B-side
    of albums, or thousands of karaoke versions of popular songs available at every
    pub? In fact, separating vocals from instrumental is really simple — when you
    have access to the individual tracks of the mix…
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '不是每个人都是音乐家。真正对数据和AI有兴趣的音乐家更少。通常，当我与非音乐家交谈时，我觉得他们认为你可以简单地“把声音从音频中去除”。这很合理，因为否则为什么专辑的B面会有伴奏，或者为什么在每个酒吧都有数以千计的流行歌曲的卡拉OK版本呢？事实上，分离人声和伴奏实际上非常简单——当你可以访问混音的单独轨道时… '
- en: However, in the real world, all we have is waveforms. A waveform is the closest
    computer representation we have to a real, physical audio event. The waveform
    is also the prerequisite for turning digital audio back into real sound, for example
    through speakers. This means that if you want to separate a piece of music into
    two sources (vocals and instrumental), you need to find a way to take the combined
    waveform and split it into two separate waveforms, each capturing one of the sources
    accurately and exclusively.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在现实世界中，我们只有波形。波形是我们能够最接近真实物理音频事件的计算机表示。波形也是将数字音频转回真实声音的前提，例如通过扬声器。这意味着如果你想将一段音乐分离成两个源（人声和乐器），你需要找到一种方法，将组合波形拆分成两个独立的波形，每个波形准确且独占地捕捉一个源。
- en: To highlight this, you can find three waveforms in the figure below. The first
    one represents a guitar, the second captures vocals sung over the guitar track.
    The third waveform is the combination of the guitar and vocals, i.e. the full
    song.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了突出这一点，您可以在下面的图中找到三个波形。第一个代表吉他，第二个捕捉到吉他伴奏上的人声。第三个波形是吉他和人声的组合，即整首歌曲。
- en: '![](../Images/166b5122943cb6a0df8039bbc537a066.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/166b5122943cb6a0df8039bbc537a066.png)'
- en: Waveforms of a guitar and vocals. Image by author.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 吉他和人声的波形。图片由作者提供。
- en: For me as the producer of this track, providing you with the vocals and instrumental
    is a trivial task, as I can simply send you the original recordings of both. However,
    as consumers of music produced by others, we only have access to the combined
    waveform. This makes the extraction process exceptionally hard, because no one
    can tell what exactly we would need to do with this waveform to extract the original
    sources. Each musical note consists of multiple acoustical waves oscillating at
    different frequencies. This means that when we strum the six strings of a guitar
    and sing a melody over this, the acoustical waves between the guitar and the voice
    overlap in a way that makes it almost impossible to reconstruct the original signals.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我这个曲目的制作人来说，提供你们人声和乐器的音频是一个微不足道的任务，因为我可以简单地把这两者的原始录音发给你。然而，作为他人创作的音乐的消费者，我们只能接触到合成的波形。这使得提取过程异常困难，因为没有人能够确切说明我们需要对这个波形做什么才能提取出原始音源。每个音乐音符都由多个在不同频率下振荡的声波组成。这意味着，当我们弹奏吉他的六根弦并在其上演唱旋律时，吉他和人声之间的声波重叠，使得几乎不可能重建原始信号。
- en: How Deep Learning Can Help
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习如何提供帮助
- en: Because the combined waveforms of multiple audio sources are so complex, to
    this day, no research team has come up with a clearly defined ruleset that would
    allow a computer to separate them again. However, this is where Deep Learning
    comes in. Deep Learning is a Machine Learning technique that allows the computer
    to learn from massive amounts of data to detect relevant patterns in highly complex
    data (like text, images, or audio) which it can then use to make predictions or
    solve creative tasks.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于多个音频源的合成波形非常复杂，至今还没有研究团队提出明确的规则集来使计算机重新分离它们。然而，这正是深度学习发挥作用的地方。深度学习是一种机器学习技术，允许计算机从大量数据中学习，以检测高度复杂数据（如文本、图像或音频）中的相关模式，然后用这些模式来进行预测或解决创造性任务。
- en: Deep Learning has been heavily used in the field of music AI for more than 10
    years. Would you be able to describe precisely what makes “Rock” different from
    “Metal” in a way that these rules would constitute an accurate genre classifier?
    No? Me neither. However, if we feed 10k rock tracks and 10k metal tracks into
    a Deep Learning model, it will perform this pattern-finding task for us and solve
    the problem easily. Unfortunately, this sounds easier than it is in practice.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在音乐AI领域已经使用了超过10年。你能否准确描述“摇滚”和“金属”有什么不同，使这些规则能构成一个准确的音乐类型分类器？不能？我也不能。然而，如果我们将1万首摇滚曲目和1万首金属曲目输入深度学习模型，它将为我们完成这个模式识别任务，并轻松解决问题。不幸的是，这听起来比实际操作要简单得多。
- en: In order to teach a Deep Learning model to separate instrumental and vocals
    from a piece of music, we would need a large dataset of tens of thousands of tracks (or more),
    for all of which we have the original audio sources (i.e. instrumental and vocals)
    as separate waveforms. To this day, collecting such a gigantic dataset is a huge
    challenge. However, as it turns out, we can apply some tricks to solve this problem
    without access to such a large dataset. This is where DEMUCS comes in.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了教导一个深度学习模型从一段音乐中分离乐器和人声，我们需要一个包含数万首曲目的大数据集（或更多），所有曲目都需要有原始音频源（即乐器和人声）作为独立的波形数据。至今，收集这样一个庞大的数据集仍然是一个巨大的挑战。然而，事实证明，我们可以应用一些技巧来解决这个问题，即使没有这样一个大数据集。这就是DEMUCS发挥作用的地方。
- en: How Source Separation Works
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 源分离的工作原理
- en: DEMUCS — A Go-To Source Separation Model
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DEMUCS — 一个可靠的源分离模型
- en: 'Since source separation is an active field of research, several different technical
    approaches exist. For this post, I am going to focus on one of the most well-known
    source separation frameworks: DEMUCS. DEMUCS was introduced in 2019 **[1,2]**
    and improved incrementally over the following years **[3,4]**. This makes it a
    great example because it allows us to look at the basic architecture of the AI
    model and the methods used to train it, first, before laying out the trends and
    advancements that led the researchers to update DEMUCS.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于源分离是一个活跃的研究领域，存在几种不同的技术方法。在这篇文章中，我将重点介绍最著名的源分离框架之一：DEMUCS。DEMUCS于2019年首次提出**[1,2]**，并在随后的几年中逐步改进**[3,4]**。这使得它成为一个很好的示例，因为它允许我们首先查看AI模型的基本架构和用于训练它的方法，然后再阐述导致研究人员更新DEMUCS的趋势和进展。
- en: Which Dataset Was DEMUCS Trained on?
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DEMUCS 是基于哪个数据集训练的？
- en: 'To train a Deep Learning model, we usually need a *ground truth*. For source
    separation, this means that we need a large number of tracks alongside their individual
    instrument sources. For DEMUCS, the authors used the popular MusDB dataset consisting
    of 150 songs sampled at 44.1k Hz, each with the individual waveforms for the sources:
    “drums”, “bass”, “vocals”, and “other”. In addition to that, the authors were
    able to collect 150 more tracks with instrumental sources, most likely from their
    personal music projects (or from friends).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练深度学习模型，我们通常需要一个*真实标签*。对于源分离，这意味着我们需要大量的曲目以及它们各自的乐器来源。对于 DEMUCS，作者使用了流行的
    MusDB 数据集，其中包含150首以44.1k Hz采样的歌曲，每首歌曲都有单独的“鼓”、“贝斯”、“人声”和“其他”波形。此外，作者还收集了150首更多的乐器来源的曲目，这些曲目很可能来自他们的个人音乐项目（或朋友）。
- en: However, seeing as generative music models like MusicLM or MusicGen are trained
    on **thousands of hours of music**, 300 tracks were simply not enough to produce
    a breakthrough in source separation. Unfortunately, that was all the data they
    could obtain. This is why the authors decided to use some tricks to make the best
    out of the available resources.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于生成音乐模型如 MusicLM 或 MusicGen 是在**数千小时的音乐**上训练的，300首曲目显然不足以在源分离上取得突破。不幸的是，这就是他们能够获得的所有数据。这就是为什么作者决定使用一些技巧，以充分利用现有资源。
- en: How Was the Dataset Enhanced?
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集是如何增强的？
- en: The problem with small datasets is that the model can not learn the variety
    of inputs it would encounter in the real world. Can 300 songs really reflect all
    kinds of music people would use DEMUCS for after its release? No. A common technique
    to mitigate this bias in the dataset is to perform data augmentation techniques.
    This means that the existing tracks are changed slightly using signal processing
    techniques so that they sound different while maintaining their core characteristics.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 小数据集的问题在于模型无法学习到它在现实世界中会遇到的输入的多样性。300首歌曲真的能反映人们在 DEMUCS 发布后会使用的所有类型的音乐吗？不能。缓解数据集偏差的常用技术是执行数据增强技术。这意味着对现有曲目进行轻微的信号处理，以便它们听起来不同，同时保持其核心特征。
- en: In the original DEMUCS study, the authors used a mixture of different techniques
    like pitch-shifting, temporal stretching/speed-up or changing the volume of individual
    instrument sources. This way, a wider range of sounds if covered without introducing
    any completely nonsensical data. This is why we call these methods **natural data
    augmentation.**
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始 DEMUCS 研究中，作者使用了不同的技术组合，如音高移位、时间拉伸/加速或改变单个乐器来源的音量。这样，就能覆盖更广泛的声音范围，而不会引入完全不合理的数据。这就是我们称这些方法为**自然数据增强**的原因。
- en: However, in their 2021 follow-up study **[3]**, the authors went even further
    with their data augmentation. At this point, they began to combine instrument
    sources from different tracks to produce completely new songs. To ensure some
    level of musical authenticity, they performed pitch shifting, onset detection
    and tempo adjustment before placing a “foreign” instrumental source in an existing
    track. Although this is not a form of natural data augmentation, the authors were
    able to improve on their previous work using this technique.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在他们2021年的后续研究**[3]**中，作者在数据增强方面更进一步。这时，他们开始将不同曲目的乐器来源组合起来，制作全新的歌曲。为了确保一定程度的音乐真实性，他们在将“外来”乐器来源放入现有曲目之前，进行了音高移位、起始点检测和节奏调整。虽然这不是一种自然数据增强形式，但作者通过这种技术在之前的工作基础上取得了改进。
- en: 'If data augmentation is interesting for you, you might like this blog that
    I wrote a few months ago:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据增强对你有趣，你可能会喜欢我几个月前写的这篇博客：
- en: '[](/natural-audio-data-augmentation-using-spotifys-pedalboard-212ea59d39ce?source=post_page-----187852e54752--------------------------------)
    [## Natural Audio Data Augmentation Using Spotify’s Pedalboard'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/natural-audio-data-augmentation-using-spotifys-pedalboard-212ea59d39ce?source=post_page-----187852e54752--------------------------------)
    [## 使用 Spotify 的 Pedalboard 进行自然音频数据增强'
- en: With Ready-To-Use Python Code & Presets
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提供现成的 Python 代码和预设
- en: towardsdatascience.com](/natural-audio-data-augmentation-using-spotifys-pedalboard-212ea59d39ce?source=post_page-----187852e54752--------------------------------)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/natural-audio-data-augmentation-using-spotifys-pedalboard-212ea59d39ce?source=post_page-----187852e54752--------------------------------)'
- en: What Kind of AI Model is DEMUCS?
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DEMUCS 是什么样的 AI 模型？
- en: '![](../Images/4f32e85875aa68e74aac640c7844db9e.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f32e85875aa68e74aac640c7844db9e.png)'
- en: High-Level Model Architecture of DEMUCS. Image by author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: DEMUCS 的高级模型架构。作者提供的图像。
- en: All versions of DEMUCS are based on an **Encoder-Decoder Neural Network**. In
    the figure above, you can see the basic structure of such a Neural Network. In
    essence, the **encoder** receives the track as an input and transforms it into
    what we call a **latent representation** (or embedding)**.** The purpose of the
    latent representation is simply to reduce the size of the input track. Since a
    waveform requires tens of thousands of numbers to store and process for every
    second of audio, it is difficult for the computer to deal with. The latent representation
    has a lower dimensionality and thus complexity while (hopefully) preserving all
    of the relevant information on instruments, pitch, harmony, tempo, and timbre.
    Using these compressed information, the **decoder** generates a new waveform for
    each of the instrumental sources (bass, drums, vocals & other).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所有版本的DEMUCS都基于一个**编码器-解码器神经网络**。在上图中，你可以看到这种神经网络的基本结构。本质上，**编码器**接收音轨作为输入，并将其转换为我们称之为**潜在表示**（或嵌入）**。**
    潜在表示的目的是简单地减少输入音轨的大小。由于一个波形需要数万个数字来存储和处理每秒的音频，因此计算机处理起来很困难。潜在表示具有较低的维度，从而降低了复杂性，同时（希望）保留了所有相关的信息，如乐器、音高、和声、节奏和音色。利用这些压缩的信息，**解码器**为每个乐器源（低音、鼓、声乐和其他）生成新的波形。
- en: '**A More Technical Description for the Techies:** The first three DEMUCS version
    were based on the Wave-U-Net (2018) **[5]** which itself was based on the well-known
    U-Net (2015) **[6]** architecture. This network is, in essence, an encoder-decoder
    model based on convolution and deconvolution. In 2021 **[3]**, DEMUCS was improved
    by using two Neural Networks, one that processes the waveform of a track, and
    another one that processes its spectrogram, before combining the networks for
    the final source separation step. The most recent version of DEMUCS **[4]** replaces
    some of the Wave-U-Net’s convolutional layers with a transformer encoder layers.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**针对技术人员的更技术性描述：** 前三个版本的DEMUCS基于Wave-U-Net（2018）**[5]**，而Wave-U-Net本身则基于著名的U-Net（2015）**[6]**架构。这个网络本质上是一个基于卷积和反卷积的编码器-解码器模型。在2021年**[3]**，DEMUCS通过使用两个神经网络进行了改进，一个处理音轨的波形，另一个处理其声谱图，然后将这两个网络结合以完成最终的源分离步骤。DEMUCS的最新版本**[4]**用一个变换器编码器层替代了Wave-U-Net中的一些卷积层。'
- en: How Good is DEMUCS?
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DEMUCS的效果如何？
- en: 'You can try it out yourself on this HuggingFace space: [https://huggingface.co/spaces/akhaliq/demucs](https://huggingface.co/spaces/akhaliq/demucs)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这个HuggingFace空间上自己试用：[https://huggingface.co/spaces/akhaliq/demucs](https://huggingface.co/spaces/akhaliq/demucs)
- en: This online demo is a bit slow, unfortunately. If you are a coder, I recommend
    you run DEMUCS locally using this [GitHub repository](https://github.com/facebookresearch/demucs).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这个在线演示有点慢。如果你是编码员，我建议你通过这个[GitHub 仓库](https://github.com/facebookresearch/demucs)在本地运行DEMUCS。
- en: (Last checked on 16th September 2023\. Please let me know if any links don’t
    work anymore!)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: （最后检查时间：2023年9月16日。如果有任何链接无法使用，请告知我！）
- en: Other Approaches
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他方法
- en: As with any technology, it is important to point out that DEMUCS is standing
    on the shoulders of giants, who made all the foundational work that DEMUCS could
    build on — from datasets through evaluation criteria to model architectures. Moreover,
    other researchers have recently come up with creative methods, offering a new
    perspective on source separation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何技术一样，重要的是指出DEMUCS建立在前人的基础上，他们完成了DEMUCS可以构建的所有基础工作——从数据集到评估标准，再到模型架构。此外，其他研究人员最近提出了创造性的方法，提供了源分离的新视角。
- en: 'In August 2023, a paper called “Separate Anything You Describe” **[7]** was
    met with excitement from the Music AI community. This paper implemented a fully-text-based
    source separation tool that can be queried through natural language and applied
    to all kinds of audio: speech, sounds, or music. This is implemented by connecting
    the source separation process to the AI model CLAP, which allows you to capture
    the meaning of sounds in words and vice versa.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年8月，一篇名为“Separate Anything You Describe”**[7]**的论文引起了音乐AI社区的兴奋。这篇论文实现了一个完全基于文本的源分离工具，可以通过自然语言进行查询，并适用于各种音频：语音、声音或音乐。这是通过将源分离过程连接到AI模型CLAP来实现的，该模型允许你用词汇捕捉声音的意义，反之亦然。
- en: This approach seems promising, as one key limitation of current source separation
    systems is that they are bound to a specific output structure. For example, DEMUCS
    can only separate a track into “bass”, “vocals”, “drums”, and “other”. There is
    no possibility to extract a guitar or a piano from a multi-instrument mix. With
    natural language queries, you would simply have to tell the model to retrieve
    the “piano”, the “harp”, or “that funny sound in the background” from a mix.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法看起来很有前景，因为当前源分离系统的一个关键限制是它们被绑定到特定的输出结构。例如，DEMUCS 只能将音轨分离为“低音”、“人声”、“鼓”和“其他”。从多乐器混合中提取吉他或钢琴是不可能的。通过自然语言查询，你只需告诉模型从混音中提取“钢琴”、“竖琴”或“背景中的那个有趣的声音”。
- en: On the other hand, the quality of the extracted sources is still inferior to
    that of traditional source separation systems like DEMUCS. This means that currently,
    you are trading quality for flexibility. Therefore, a hybrid system that is specialized
    in specific, commonly used instruments but remains flexible to natural language
    queries could be desirable.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，提取的音源质量仍然低于像 DEMUCS 这样的传统源分离系统。这意味着目前你在质量和灵活性之间进行权衡。因此，一个专门针对特定、常用乐器但又能保持对自然语言查询灵活的混合系统可能是值得期望的。
- en: Conclusion
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Source separation is not an easy task, as separating a set of complex acoustic
    waves in a controlled and reliable manner is difficult. With Deep Learning approaches
    like that taken by DEMUCS, we have been seeing more and more capable AI models
    in the last couple of years. Moreover, natural language queries have found their
    way into the field and are likely to increase the usefulness of AI source separation
    tools even further.
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 源分离并非易事，因为以受控和可靠的方式分离一组复杂的声学波形是困难的。通过像 DEMUCS 这样的深度学习方法，近年来我们见证了越来越多能力强大的AI模型。此外，自然语言查询已经进入该领域，并可能进一步提高AI源分离工具的实用性。
- en: 'I write a lot about AI and music. Here are some of my articles you might also
    like:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我经常写关于AI和音乐的文章。以下是一些你可能也会喜欢的文章：
- en: '[Making Music Tagging AI Explainable Through Source Separation](https://medium.com/towards-data-science/making-music-tagging-ai-explainable-through-source-separation-2d9493547a7e)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过源分离使音乐标记AI可解释](https://medium.com/towards-data-science/making-music-tagging-ai-explainable-through-source-separation-2d9493547a7e)'
- en: '[How Meta’s AI Generates Music Based on a Reference Melody](/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Meta的AI如何基于参考旋律生成音乐](/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783)'
- en: '[How Google Used Fake Datasets to Train Generative Music AI](/how-google-used-fake-datasets-to-train-generative-music-ai-def6f3f71f19)+'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[谷歌如何利用虚假数据集训练生成音乐AI](/how-google-used-fake-datasets-to-train-generative-music-ai-def6f3f71f19)+'
- en: Also, consider following me on [Medium](https://medium.com/@maxhilsdorf) for
    more articles and on [LinkedIn](https://www.linkedin.com/in/max-hilsdorf/) for
    further updates about AI & Music.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，可以考虑在 [Medium](https://medium.com/@maxhilsdorf) 上关注我以获取更多文章，并在 [LinkedIn](https://www.linkedin.com/in/max-hilsdorf/)
    上关注我以获取有关AI和音乐的最新动态。
- en: References
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '**[1] Défossez et al. (2019)**. Demucs: Deep Extractor for Music Sources with
    extra unlabeled data remixed. [https://arxiv.org/abs/1909.01174](https://arxiv.org/abs/1909.01174)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**[1] Défossez 等 (2019)**。Demucs：使用额外未标记数据混合的深度音乐源提取器。 [https://arxiv.org/abs/1909.01174](https://arxiv.org/abs/1909.01174)'
- en: '**[2] Défossez et al. (2019)**. Music Source Separation in the Waveform Domain.
    [https://arxiv.org/abs/1911.13254](https://arxiv.org/abs/1911.13254)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**[2] Défossez 等 (2019)**。波形域中的音乐源分离。 [https://arxiv.org/abs/1911.13254](https://arxiv.org/abs/1911.13254)'
- en: '**[3] Défossez (2021)**. Hybrid Spectrogram and Waveform Source Separation.
    [https://arxiv.org/abs/2111.03600](https://arxiv.org/abs/2111.03600)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**[3] Défossez (2021)**。混合频谱图和波形源分离。 [https://arxiv.org/abs/2111.03600](https://arxiv.org/abs/2111.03600)'
- en: '**[4] Rouard, Massa & Défossez (2023)**. Hybrid Transformers for Music Source
    Separation. [https://arxiv.org/abs/2211.08553](https://arxiv.org/abs/2211.08553)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**[4] Rouard, Massa & Défossez (2023)**。用于音乐源分离的混合变换器。 [https://arxiv.org/abs/2211.08553](https://arxiv.org/abs/2211.08553)'
- en: '**[5] Stoller, Evert, Dickson (2018)**. Wave-U-Net: A Multi-Scale Neural Network
    for End-to-End Audio Source Separation. [https://arxiv.org/abs/1806.03185](https://arxiv.org/abs/1806.03185)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**[5] Stoller, Evert, Dickson (2018)**。Wave-U-Net：用于端到端音频源分离的多尺度神经网络。 [https://arxiv.org/abs/1806.03185](https://arxiv.org/abs/1806.03185)'
- en: '**[6] Ronneberger, Fischer, Brocks (2015)**. U-Net: Convolutional Networks
    for Biomedical Image Segmentation. [https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**[6] Ronneberger, Fischer, Brocks（2015）**. U-Net：用于生物医学图像分割的卷积网络。 [https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)'
- en: '**[7] Liu et al. (2023)**. Separate Anything You Describe. [https://arxiv.org/abs/2308.05037](https://arxiv.org/abs/2308.05037)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**[7] 刘等人（2023）**. 分离你描述的任何事物。 [https://arxiv.org/abs/2308.05037](https://arxiv.org/abs/2308.05037)'
