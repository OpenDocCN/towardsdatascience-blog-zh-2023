- en: 'AI Music Source Separation: How it Works and Why It Is So Hard'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752](https://towardsdatascience.com/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Source Separation AI, explained
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@maxhilsdorf?source=post_page-----187852e54752--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page-----187852e54752--------------------------------)[](https://towardsdatascience.com/?source=post_page-----187852e54752--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----187852e54752--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page-----187852e54752--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----187852e54752--------------------------------)
    ·9 min read·Sep 21, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1592eebc641690038dd7c665c19845a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: Source Separation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is Source Separation?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the field of signal processing, source separation describes the task of breaking
    down an audio signal into multiple source audio signals. This concept is not only
    relevant for music, but also for speech or machine sounds. For example, you might
    want to separate the voices of two speakers in a podcast so you can edit the voices
    separately.
  prefs: []
  type: TYPE_NORMAL
- en: Why is Source Separation so Difficult?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Not everyone is a musician. Even fewer people are musicians with a penchant
    for data & AI. Oftentimes, when I talk to non-musicians, I get the impression
    that they think you can simply “take the voice and remove it from the audio”.
    This makes sense, because why else would there be instrumentals on the B-side
    of albums, or thousands of karaoke versions of popular songs available at every
    pub? In fact, separating vocals from instrumental is really simple — when you
    have access to the individual tracks of the mix…
  prefs: []
  type: TYPE_NORMAL
- en: However, in the real world, all we have is waveforms. A waveform is the closest
    computer representation we have to a real, physical audio event. The waveform
    is also the prerequisite for turning digital audio back into real sound, for example
    through speakers. This means that if you want to separate a piece of music into
    two sources (vocals and instrumental), you need to find a way to take the combined
    waveform and split it into two separate waveforms, each capturing one of the sources
    accurately and exclusively.
  prefs: []
  type: TYPE_NORMAL
- en: To highlight this, you can find three waveforms in the figure below. The first
    one represents a guitar, the second captures vocals sung over the guitar track.
    The third waveform is the combination of the guitar and vocals, i.e. the full
    song.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/166b5122943cb6a0df8039bbc537a066.png)'
  prefs: []
  type: TYPE_IMG
- en: Waveforms of a guitar and vocals. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: For me as the producer of this track, providing you with the vocals and instrumental
    is a trivial task, as I can simply send you the original recordings of both. However,
    as consumers of music produced by others, we only have access to the combined
    waveform. This makes the extraction process exceptionally hard, because no one
    can tell what exactly we would need to do with this waveform to extract the original
    sources. Each musical note consists of multiple acoustical waves oscillating at
    different frequencies. This means that when we strum the six strings of a guitar
    and sing a melody over this, the acoustical waves between the guitar and the voice
    overlap in a way that makes it almost impossible to reconstruct the original signals.
  prefs: []
  type: TYPE_NORMAL
- en: How Deep Learning Can Help
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because the combined waveforms of multiple audio sources are so complex, to
    this day, no research team has come up with a clearly defined ruleset that would
    allow a computer to separate them again. However, this is where Deep Learning
    comes in. Deep Learning is a Machine Learning technique that allows the computer
    to learn from massive amounts of data to detect relevant patterns in highly complex
    data (like text, images, or audio) which it can then use to make predictions or
    solve creative tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning has been heavily used in the field of music AI for more than 10
    years. Would you be able to describe precisely what makes “Rock” different from
    “Metal” in a way that these rules would constitute an accurate genre classifier?
    No? Me neither. However, if we feed 10k rock tracks and 10k metal tracks into
    a Deep Learning model, it will perform this pattern-finding task for us and solve
    the problem easily. Unfortunately, this sounds easier than it is in practice.
  prefs: []
  type: TYPE_NORMAL
- en: In order to teach a Deep Learning model to separate instrumental and vocals
    from a piece of music, we would need a large dataset of tens of thousands of tracks (or more),
    for all of which we have the original audio sources (i.e. instrumental and vocals)
    as separate waveforms. To this day, collecting such a gigantic dataset is a huge
    challenge. However, as it turns out, we can apply some tricks to solve this problem
    without access to such a large dataset. This is where DEMUCS comes in.
  prefs: []
  type: TYPE_NORMAL
- en: How Source Separation Works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DEMUCS — A Go-To Source Separation Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since source separation is an active field of research, several different technical
    approaches exist. For this post, I am going to focus on one of the most well-known
    source separation frameworks: DEMUCS. DEMUCS was introduced in 2019 **[1,2]**
    and improved incrementally over the following years **[3,4]**. This makes it a
    great example because it allows us to look at the basic architecture of the AI
    model and the methods used to train it, first, before laying out the trends and
    advancements that led the researchers to update DEMUCS.'
  prefs: []
  type: TYPE_NORMAL
- en: Which Dataset Was DEMUCS Trained on?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To train a Deep Learning model, we usually need a *ground truth*. For source
    separation, this means that we need a large number of tracks alongside their individual
    instrument sources. For DEMUCS, the authors used the popular MusDB dataset consisting
    of 150 songs sampled at 44.1k Hz, each with the individual waveforms for the sources:
    “drums”, “bass”, “vocals”, and “other”. In addition to that, the authors were
    able to collect 150 more tracks with instrumental sources, most likely from their
    personal music projects (or from friends).'
  prefs: []
  type: TYPE_NORMAL
- en: However, seeing as generative music models like MusicLM or MusicGen are trained
    on **thousands of hours of music**, 300 tracks were simply not enough to produce
    a breakthrough in source separation. Unfortunately, that was all the data they
    could obtain. This is why the authors decided to use some tricks to make the best
    out of the available resources.
  prefs: []
  type: TYPE_NORMAL
- en: How Was the Dataset Enhanced?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The problem with small datasets is that the model can not learn the variety
    of inputs it would encounter in the real world. Can 300 songs really reflect all
    kinds of music people would use DEMUCS for after its release? No. A common technique
    to mitigate this bias in the dataset is to perform data augmentation techniques.
    This means that the existing tracks are changed slightly using signal processing
    techniques so that they sound different while maintaining their core characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: In the original DEMUCS study, the authors used a mixture of different techniques
    like pitch-shifting, temporal stretching/speed-up or changing the volume of individual
    instrument sources. This way, a wider range of sounds if covered without introducing
    any completely nonsensical data. This is why we call these methods **natural data
    augmentation.**
  prefs: []
  type: TYPE_NORMAL
- en: However, in their 2021 follow-up study **[3]**, the authors went even further
    with their data augmentation. At this point, they began to combine instrument
    sources from different tracks to produce completely new songs. To ensure some
    level of musical authenticity, they performed pitch shifting, onset detection
    and tempo adjustment before placing a “foreign” instrumental source in an existing
    track. Although this is not a form of natural data augmentation, the authors were
    able to improve on their previous work using this technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'If data augmentation is interesting for you, you might like this blog that
    I wrote a few months ago:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/natural-audio-data-augmentation-using-spotifys-pedalboard-212ea59d39ce?source=post_page-----187852e54752--------------------------------)
    [## Natural Audio Data Augmentation Using Spotify’s Pedalboard'
  prefs: []
  type: TYPE_NORMAL
- en: With Ready-To-Use Python Code & Presets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/natural-audio-data-augmentation-using-spotifys-pedalboard-212ea59d39ce?source=post_page-----187852e54752--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: What Kind of AI Model is DEMUCS?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/4f32e85875aa68e74aac640c7844db9e.png)'
  prefs: []
  type: TYPE_IMG
- en: High-Level Model Architecture of DEMUCS. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: All versions of DEMUCS are based on an **Encoder-Decoder Neural Network**. In
    the figure above, you can see the basic structure of such a Neural Network. In
    essence, the **encoder** receives the track as an input and transforms it into
    what we call a **latent representation** (or embedding)**.** The purpose of the
    latent representation is simply to reduce the size of the input track. Since a
    waveform requires tens of thousands of numbers to store and process for every
    second of audio, it is difficult for the computer to deal with. The latent representation
    has a lower dimensionality and thus complexity while (hopefully) preserving all
    of the relevant information on instruments, pitch, harmony, tempo, and timbre.
    Using these compressed information, the **decoder** generates a new waveform for
    each of the instrumental sources (bass, drums, vocals & other).
  prefs: []
  type: TYPE_NORMAL
- en: '**A More Technical Description for the Techies:** The first three DEMUCS version
    were based on the Wave-U-Net (2018) **[5]** which itself was based on the well-known
    U-Net (2015) **[6]** architecture. This network is, in essence, an encoder-decoder
    model based on convolution and deconvolution. In 2021 **[3]**, DEMUCS was improved
    by using two Neural Networks, one that processes the waveform of a track, and
    another one that processes its spectrogram, before combining the networks for
    the final source separation step. The most recent version of DEMUCS **[4]** replaces
    some of the Wave-U-Net’s convolutional layers with a transformer encoder layers.'
  prefs: []
  type: TYPE_NORMAL
- en: How Good is DEMUCS?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can try it out yourself on this HuggingFace space: [https://huggingface.co/spaces/akhaliq/demucs](https://huggingface.co/spaces/akhaliq/demucs)'
  prefs: []
  type: TYPE_NORMAL
- en: This online demo is a bit slow, unfortunately. If you are a coder, I recommend
    you run DEMUCS locally using this [GitHub repository](https://github.com/facebookresearch/demucs).
  prefs: []
  type: TYPE_NORMAL
- en: (Last checked on 16th September 2023\. Please let me know if any links don’t
    work anymore!)
  prefs: []
  type: TYPE_NORMAL
- en: Other Approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with any technology, it is important to point out that DEMUCS is standing
    on the shoulders of giants, who made all the foundational work that DEMUCS could
    build on — from datasets through evaluation criteria to model architectures. Moreover,
    other researchers have recently come up with creative methods, offering a new
    perspective on source separation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In August 2023, a paper called “Separate Anything You Describe” **[7]** was
    met with excitement from the Music AI community. This paper implemented a fully-text-based
    source separation tool that can be queried through natural language and applied
    to all kinds of audio: speech, sounds, or music. This is implemented by connecting
    the source separation process to the AI model CLAP, which allows you to capture
    the meaning of sounds in words and vice versa.'
  prefs: []
  type: TYPE_NORMAL
- en: This approach seems promising, as one key limitation of current source separation
    systems is that they are bound to a specific output structure. For example, DEMUCS
    can only separate a track into “bass”, “vocals”, “drums”, and “other”. There is
    no possibility to extract a guitar or a piano from a multi-instrument mix. With
    natural language queries, you would simply have to tell the model to retrieve
    the “piano”, the “harp”, or “that funny sound in the background” from a mix.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the quality of the extracted sources is still inferior to
    that of traditional source separation systems like DEMUCS. This means that currently,
    you are trading quality for flexibility. Therefore, a hybrid system that is specialized
    in specific, commonly used instruments but remains flexible to natural language
    queries could be desirable.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Source separation is not an easy task, as separating a set of complex acoustic
    waves in a controlled and reliable manner is difficult. With Deep Learning approaches
    like that taken by DEMUCS, we have been seeing more and more capable AI models
    in the last couple of years. Moreover, natural language queries have found their
    way into the field and are likely to increase the usefulness of AI source separation
    tools even further.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'I write a lot about AI and music. Here are some of my articles you might also
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Making Music Tagging AI Explainable Through Source Separation](https://medium.com/towards-data-science/making-music-tagging-ai-explainable-through-source-separation-2d9493547a7e)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How Meta’s AI Generates Music Based on a Reference Melody](/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How Google Used Fake Datasets to Train Generative Music AI](/how-google-used-fake-datasets-to-train-generative-music-ai-def6f3f71f19)+'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, consider following me on [Medium](https://medium.com/@maxhilsdorf) for
    more articles and on [LinkedIn](https://www.linkedin.com/in/max-hilsdorf/) for
    further updates about AI & Music.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**[1] Défossez et al. (2019)**. Demucs: Deep Extractor for Music Sources with
    extra unlabeled data remixed. [https://arxiv.org/abs/1909.01174](https://arxiv.org/abs/1909.01174)'
  prefs: []
  type: TYPE_NORMAL
- en: '**[2] Défossez et al. (2019)**. Music Source Separation in the Waveform Domain.
    [https://arxiv.org/abs/1911.13254](https://arxiv.org/abs/1911.13254)'
  prefs: []
  type: TYPE_NORMAL
- en: '**[3] Défossez (2021)**. Hybrid Spectrogram and Waveform Source Separation.
    [https://arxiv.org/abs/2111.03600](https://arxiv.org/abs/2111.03600)'
  prefs: []
  type: TYPE_NORMAL
- en: '**[4] Rouard, Massa & Défossez (2023)**. Hybrid Transformers for Music Source
    Separation. [https://arxiv.org/abs/2211.08553](https://arxiv.org/abs/2211.08553)'
  prefs: []
  type: TYPE_NORMAL
- en: '**[5] Stoller, Evert, Dickson (2018)**. Wave-U-Net: A Multi-Scale Neural Network
    for End-to-End Audio Source Separation. [https://arxiv.org/abs/1806.03185](https://arxiv.org/abs/1806.03185)'
  prefs: []
  type: TYPE_NORMAL
- en: '**[6] Ronneberger, Fischer, Brocks (2015)**. U-Net: Convolutional Networks
    for Biomedical Image Segmentation. [https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)'
  prefs: []
  type: TYPE_NORMAL
- en: '**[7] Liu et al. (2023)**. Separate Anything You Describe. [https://arxiv.org/abs/2308.05037](https://arxiv.org/abs/2308.05037)'
  prefs: []
  type: TYPE_NORMAL
