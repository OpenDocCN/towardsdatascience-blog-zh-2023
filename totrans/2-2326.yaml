- en: What Does It Really Mean for an Algorithm to ‘Learn’?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法“学习”到底意味着什么？
- en: 原文：[https://towardsdatascience.com/what-does-it-really-mean-for-an-algorithm-to-learn-1f3e5e8d7884](https://towardsdatascience.com/what-does-it-really-mean-for-an-algorithm-to-learn-1f3e5e8d7884)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/what-does-it-really-mean-for-an-algorithm-to-learn-1f3e5e8d7884](https://towardsdatascience.com/what-does-it-really-mean-for-an-algorithm-to-learn-1f3e5e8d7884)
- en: '![](../Images/4277cf3340bee51edbf84b5d3e9a26ed.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4277cf3340bee51edbf84b5d3e9a26ed.png)'
- en: Two general perspectives and some psychology
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 两种一般观点和一些心理学
- en: '[](https://andre-ye.medium.com/?source=post_page-----1f3e5e8d7884--------------------------------)[![Andre
    Ye](../Images/c69b022a665d3ee13f9fec2f1f73bf32.png)](https://andre-ye.medium.com/?source=post_page-----1f3e5e8d7884--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1f3e5e8d7884--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1f3e5e8d7884--------------------------------)
    [Andre Ye](https://andre-ye.medium.com/?source=post_page-----1f3e5e8d7884--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://andre-ye.medium.com/?source=post_page-----1f3e5e8d7884--------------------------------)[![Andre
    Ye](../Images/c69b022a665d3ee13f9fec2f1f73bf32.png)](https://andre-ye.medium.com/?source=post_page-----1f3e5e8d7884--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1f3e5e8d7884--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1f3e5e8d7884--------------------------------)
    [Andre Ye](https://andre-ye.medium.com/?source=post_page-----1f3e5e8d7884--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1f3e5e8d7884--------------------------------)
    ·20 min read·Apr 22, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1f3e5e8d7884--------------------------------)
    ·阅读时间20分钟·2023年4月22日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: When one first encounters machine learning, one often rushes through algorithm
    after algorithm, technique after technique, equation after equation. But it is
    afterwards that one can reflect on the general trends across the knowledge that
    they have acquired.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们第一次接触机器学习时，通常会快速浏览一个又一个算法、一个又一个技术、一个又一个方程。但之后，才有可能反思他们所掌握的知识中的一般趋势。
- en: What it means to ‘learn’ is a very abstract concept. The goal of this article
    is to provide two general interpretations of what it means for a machine to learn.
    These two interpretations are, as we will see, two sides of the same coin, and
    they are treated ubiqitously across machine learning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: “学习”意味着什么是一个非常抽象的概念。本文的目标是提供机器学习中“学习”的两种一般解释。这两种解释，正如我们将看到的，是同一枚硬币的两面，并且在机器学习中普遍存在。
- en: Even if you are experienced in machine learning, you may gain something from
    temporarily stepping away from specific mechanics and considering the concept
    of learning at an abstract level.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你在机器学习方面经验丰富，暂时脱离具体的机制，考虑学习的抽象概念，可能仍会有所收获。
- en: There are broadly two key interpretations of learning in machine learning, which
    we will term ***loss-directed parameter update*** and ***manifold mapping***.
    As we will see, they have substantive connections to psychology and philosophy
    of mind.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，学习有两个主要的解释，我们称之为***损失导向参数更新***和***流形映射***。正如我们将看到的，它们与心理学和心灵哲学有实质性的联系。
- en: Loss-Directed Parameter Update
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失导向参数更新
- en: 'Some of the machine learning algorithms previously discussed adopt a ***tabula-rasa***
    approach: they begin from a ‘blank slate’ random guess and iteratively improve
    their guess. This paradigm seems intuitive to us: when we’re trying to acquire
    a new skill, like learning to ride a bike or to simplify algebraic expressions,
    we make many mistakes and just get better ‘with practice’. However, from an algorithmic
    perspective, we need to explicitly recognize the presence of two entities: a ***state***
    and a ***loss***.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一些之前讨论的机器学习算法采用了***白板***方法：它们从一个“空白”随机猜测开始，并迭代改进其猜测。这个范式对我们来说似乎很直观：当我们尝试掌握一项新技能，比如学习骑自行车或简化代数表达式时，我们犯了许多错误，并通过“实践”变得更好。然而，从算法的角度来看，我们需要明确识别两个实体的存在：***状态***和***损失***。
- en: 'An algorithm’s ***state*** is defined by the values of its set of *parameters*.
    Parameters are, in this context, non-static values that determine how an algorithm
    behaves. For instance, consider trying to optimize your bowling game. There are
    several parameters at hand: the weight of the bowling ball, the configuration
    of your fingers in the fingerholes, your velocity when getting ready to bowl,
    the velocity of your arm, the angle at which you bowl, the spin at the moment
    of release, and so on. Every time you bowl, you define a new *state* since you
    — as an optimization algorithm — are trying out new parameters (unless you bowled
    exactly the same as previously, in which case you are returning to a previous
    state, but this is a rare occurrence both in bowling and in machine learning).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的***状态***由其一组*参数*的值定义。在这个上下文中，参数是决定算法行为的非静态值。例如，考虑优化你的保龄球游戏。有几个参数：保龄球的重量、你在指孔中的手指配置、准备投球时的速度、你手臂的速度、你投球的角度、释放时的旋转等等。每次你投球时，你都定义了一个新的*状态*，因为你作为一个优化算法正在尝试新的参数（除非你投球的方式完全相同，否则你是在返回到之前的状态，但在保龄球和机器学习中这种情况都很少见）。
- en: Each of the coefficients in linear regression and logistic regression is a parameter.
    Tree-based models don’t have a static number of parameters, since their depth
    is adaptive. Rather, they can create more or fewer conditions as is needed to
    optimize for information gain criteria, but these are all parameters.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归和逻辑回归中的每一个系数都是一个参数。基于树的模型没有固定数量的参数，因为它们的深度是自适应的。相反，它们可以根据需要创建更多或更少的条件来优化信息增益标准，但这些都是参数。
- en: However, tree-based models — and all algorithms — are subject to ***hyperparameters***.
    These are the system-level constraints that the parameters themselves must act
    within. The maximum depth of a decision tree and the number of trees in a random
    forest ensemble are both examples of hyperparameters. In our bowling example,
    meta-parameters could include the humidity of the building, the quality of the
    bowling shoes you were given, and the crowdedness of the bowling lanes. As an
    optimization algorithm, you exist *within* these conditions and must optimize
    your internal parameters (which bowling ball you choose, how you bowl, etc.) even
    if you cannot change the underlying conditions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，基于树的模型——以及所有算法——都受到***超参数***的影响。这些是参数自身必须遵循的系统级约束。决策树的最大深度和随机森林集成中的树的数量都是超参数的例子。在我们的保龄球示例中，元参数可能包括建筑物的湿度、你所穿的保龄球鞋的质量以及保龄球道的拥挤程度。作为一种优化算法，你存在于这些条件*之中*，必须优化你的内部参数（你选择哪个保龄球，你如何投球等），即使你不能改变基本条件。
- en: The ***loss***, on the other hand, is the ‘badness’ or error of any given state.
    A loss must formulate how to derive the badness of a model from its behavior.
    For instance, say I adopt a certain state — I choose a 6.3-inch diameter bowling
    ball, release the ball at 18 miles per hour at a 9-degree angle relative to the
    lane edges, beginning four feet away from the lane, and so on — and knock down
    6 pins. The behavior of my state is that I knocked down 6 pins. To quantify the
    badness of the state, we calculate how many pins I *didn’t* knock down — 4 pins,
    given a bowling ball released with those exact state parameters. To minimize my
    badness, I adjust my parameters the next time I bowl.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '***损失***，另一方面，是任何给定状态的‘坏ness’或错误。损失必须制定如何从模型的行为中得出坏ness。例如，假设我采取了某种状态——我选择了一个直径为6.3英寸的保龄球，以每小时18英里的速度、相对于球道边缘9度的角度释放球，距离球道四英尺远，等等——并击倒了6个球瓶。我的状态的行为是我击倒了6个球瓶。为了量化状态的坏ness，我们计算我*没有*击倒的球瓶数量——4个球瓶，给定这些确切的状态参数释放的保龄球。为了最小化我的坏ness，我会在下一次投球时调整我的参数。'
- en: '![](../Images/72cfed36ad409ae66e4a7e5d6bf402f4.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72cfed36ad409ae66e4a7e5d6bf402f4.png)'
- en: Speaking in terms of algorithms, the state is usually a set of coefficients
    or criteria that transform an input into a prediction, and the loss is a mathematical
    quantification of the difference between a model’s prediction and its desired
    output. Say a model has *n* parameters, denoted by `{x_1, x_2, ..., x_n}` . These
    might be the coefficients of a linear regression model, the center of the clusters
    in the *k*-means model or represent the split-off criteria in a decision tree
    model. We can derive an error this model incurs on some set of data — for instance,
    mean squared error. If the model iteratively adjusts its parameter set to improve
    its loss, then we say it is *learning*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从算法的角度来说，状态通常是一组系数或标准，它们将输入转化为预测，而损失是模型预测与期望输出之间差异的数学量化。假设一个模型有*n*个参数，用`{x_1,
    x_2, ..., x_n}`表示。这些参数可能是线性回归模型的系数，*k*-均值模型中簇的中心，或是决策树模型中的分裂标准。我们可以推导出该模型在某些数据集上的误差——例如，均方误差。如果模型迭代地调整其参数集以改善损失，那么我们说它在*学习*。
- en: '![](../Images/5082efd0e8a3b4b512ab743a906b8879.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5082efd0e8a3b4b512ab743a906b8879.png)'
- en: While we might roughly call this entire process learning, the actual learning
    *algorithm* occurs in the conversion from loss to parameter update. The evaluation
    of the current state (i.e. the conversion or derivation from parameter to loss)
    might be considered ‘feeling’ or an ‘intelligent’ process.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以大致称整个过程为学习，实际的学习*算法*发生在从损失到参数更新的转换过程中。当前状态的评估（即从参数到损失的转换或推导）可能被视为‘感知’或一种‘智能’过程。
- en: 'One psychologically and evolutionarily informed hypothesis suggests that we
    as humans are constantly playing an optimization game conceptually similar to
    that of models learning via the loss-directed parameter update paradigm: we are
    continually observing the ‘badness’ of our state and seeing to improve the badness
    by adjusting our state. However, our measures of badness are more complicated
    than a mean-square-error calculation: we are simultaneously juggling a variety
    of signals, internal and external, visceral and calculated, and trying to make
    sense of it all in relationship to our current set of changeable characteristics.
    For instance, when you’re bowling, you may not be directly trying to optimize
    the number of pins you knock down, even if you try to. Instead, you might be trying
    to minimize social anxiety or maximize how impressed your date or friends are,
    which does not necessarily result in the same optimal configuration of states
    as that of maximizing the number of pins knocked down.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一个心理学和进化学上受到启发的假设认为，我们作为人类不断地进行一种优化游戏，这种游戏在概念上类似于通过损失导向的参数更新范式进行学习的模型：我们不断观察状态的‘糟糕’程度，并通过调整状态来改进糟糕程度。然而，我们对糟糕程度的衡量比均方误差计算更复杂：我们同时处理各种信号，内部和外部，直观和计算的，并尝试将这些信号与我们当前可变特征集联系起来。例如，当你在保龄球时，你可能并不是直接尝试优化击倒的瓶数，即使你试图这样做。相反，你可能在尝试减少社交焦虑或最大化你约会对象或朋友的印象，这不一定会导致与最大化击倒瓶数相同的最优状态配置。
- en: Keeping this idea in mind — that we can view our own behavior as a constant
    updating of changeable states in response to evaluations of our states’ badness
    — allows us to better understand problems and phenomena in how loss-based parameter
    update behave. For one, people aren’t always changing their state, even if they
    are necessarily constantly evaluating their state. This demonstrates *convergence*
    — those people have reached a set of states for which no feasible change to the
    state will decrease badness.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记这一点——我们可以将自己的行为视为对状态糟糕程度评估的响应中可变状态的不断更新——可以帮助我们更好地理解损失驱动的参数更新行为的问题和现象。例如，人们并不总是在改变他们的状态，即使他们不断地评估自己的状态。这展示了*收敛*——这些人已经达到了一个状态集，在这个状态集中，没有任何可行的状态变化会减少糟糕程度。
- en: Alternatively, some people are sadly trapped in recurring destructive behavior
    of poor conditions because of addiction (to substances, to gambling, to social
    media scrolling, and so on) or crippling paranoia (of falling into financial debt,
    of losing prized possessions, of losing respect, of large crowds, and so on).
    In the technical language of learning, we refer to these as ***local minima***.
    These are places of convergence that agents arrive at and ‘choose’ not to leave
    either because it is easier to stay there than to make any immediate step away
    from it (e.g. breaking from an addiction) or because, equivalently, it is worse
    to make any immediate step away than to remain (i.e. a paranoia of damage or worsening
    of one’s state given some change).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，一些人由于上瘾（对物质、赌博、社交媒体滚动等）或因严重的偏执（担心陷入经济困境、失去珍贵物品、失去尊重、大量人群等）而不幸陷入了重复的破坏性行为。在学习的技术语言中，我们将这些称为***局部最小值***。这些是代理人到达并“选择”不离开的汇聚点，要么是因为留在那里比立即采取任何步骤离开更容易（例如，摆脱上瘾），要么是因为，等效地，立即离开比留下来更糟（即担心因某些变化而损害或恶化自己的状态）。
- en: Algorithms often demonstrate similar behavior, although of course in less immediately
    human-like ways. We often conceptualize the relationship between the loss and
    the state in loss-directed parameter update in a geometric and quantitative way
    through the idea of a ***loss landscape***. The loss landscape is a geometric
    space with one axis for each parameter and an additional axis for the loss. The
    loss landscape allows us to map each set of parameter values to a loss value incurred
    for that loss.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 算法通常表现出类似的行为，尽管当然在更少人性化的方式中。我们经常通过***损失景观***的概念，以几何和定量的方式来构思损失与状态之间的关系。在损失景观中，每个参数都有一个轴，额外还有一个损失轴。损失景观使我们能够将每组参数值映射到相应的损失值。
- en: 'To illustrate this crucial concept, let’s say that you are trying to learn
    the optimal study strategy for a test. We’ll just consider the problem of learning
    *one parameter* — how many hours you study for the test. Say you’ve taken four
    tests before, so — assuming for the sake of illustration that these tests are
    environmentally comparable — you have four data points to learn from. For each
    of these four tests, you studied a different number of hours and correspondingly
    obtained different performance: 60% when not studying at all, around 75% when
    studying for 1.5 hours, around 70% when studying for 4 hours, and around 60% when
    studying for 6 hours. We’ll plot these out as follows by showing how the loss
    — which conventionally has the property of being more desirable when smaller,
    so in this case will be the *error rate* (one minus performance) — changes with
    the parameter being optimized.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个关键概念，假设你正在尝试学习测试的最佳学习策略。我们只考虑学习*一个参数*——你为测试学习的小时数。假设你之前参加过四次测试，那么——为了说明问题，假设这些测试在环境上是可比的——你有四个数据点可以学习。对于这四次测试中的每一次，你学习了不同的小时数，相应地获得了不同的表现：完全不学习时为60%，学习1.5小时时大约为75%，学习4小时时大约为70%，学习6小时时大约为60%。我们将这些数据绘制如下，显示损失——通常在损失较小时更为理想，因此在这种情况下将是*错误率*（表现的倒数）——如何随着优化参数的变化而变化。
- en: '![](../Images/d8374a0079f182f2eb81b4f30a8e3125.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8374a0079f182f2eb81b4f30a8e3125.png)'
- en: Using this information, we want to find the optimal number of hours to study
    to obtain the smallest error rate. From the information we’ve collected so far,
    it looks like the optimal parameter value will be somewhere near two hours. But
    how can we be sure? How do we think about close answers or far answers? How do
    we characterize this process of looking at our data and searching for a minimum
    loss?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这些信息，我们想要找到最佳的学习小时数，以获得最小的错误率。从我们收集到的信息来看，最佳的参数值似乎在两小时左右。但我们如何确定？我们如何考虑接近的答案或远离的答案？我们如何描述查看数据和寻找最小损失的过程？
- en: The loss landscape is a conceptual tool to help us think about learning in a
    physical, quantitative sense. We imagine that we have access to the exact, true
    relationship between every parameter value and its corresponding error rate. This
    allows us to draw a curve, or ‘landscape’, throughout our parameter-loss space.
    We can understand each of the known points as being ‘sampled’ from the landscape.
    It should be repeated that, in practice, we do not have access to this landscape.
    It is rather a theoretical model to aid reasoning and understanding.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 损失景观是一个概念工具，帮助我们以物理的、定量的方式思考学习。我们设想可以获取每个参数值及其对应错误率之间的确切关系。这使我们能够在参数-损失空间中绘制出一条曲线或“景观”。我们可以理解每一个已知点是从景观中“采样”得到的。应该重复强调的是，在实际操作中，我们无法获取这个景观。它只是一个理论模型，用于辅助推理和理解。
- en: '![](../Images/9a0fa1d425a7219ee1a17ca43e399ba6.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a0fa1d425a7219ee1a17ca43e399ba6.png)'
- en: Now, imagine you are looking at a two-dimensional-world hill. Say you’re a little
    traveler standing on the surface of this hill, seeking out the place of lowest
    altitude. You can move however you want — you could use your teleporter and jump
    randomly all over the place, you could take slow steps, you could take large jumps.
    As you explore this hill, you might find that none of the new places you’re visiting
    are lower than the lowest previous place you’ve visited. After some amount of
    unfruitful exploration, you may decide just to settle with that previously visited
    lowest location. This would indicate convergence.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象你正在观察一个二维世界的山丘。假设你是一个小旅行者，站在这个山丘的表面，寻找最低的海拔位置。你可以随意移动——你可以使用传送器随机跳跃到处，或者慢慢走，也可以大步跳跃。当你探索这个山丘时，你可能会发现没有新的地方比你之前访问的最低点还低。在一些无果的探索之后，你可能决定就此停留在之前访问过的最低位置。这就表明了收敛。
- en: Algorithms can be differentiated by how they make use of and navigate the loss
    landscape. You can think of each algorithm as its own personality of traveler.
    One dumb learning algorithm is just to randomly change parameters a large number
    of times and revert to the best-performing one — random search. This is an erratic
    traveler that may have drunk a little too much, jumping all over the loss landscape
    in their teleporter.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 算法可以通过它们如何利用和导航损失景观来区分。你可以将每个算法视为具有自己个性的旅行者。一种简单的学习算法是随机多次改变参数，然后恢复到表现最好的参数——随机搜索。这是一种表现不稳定的旅行者，可能喝醉了，随机跳跃在损失景观中。
- en: Alternatively, a conservative learning algorithm would be to search every set
    of parameter values with a certain set granularity. This is a very diligent but
    inefficient traveler, which slowly walks every ‘inch’ of the loss landscape, taking
    notes and meticulous measurements.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种保守的学习算法是以某种粒度搜索每组参数值。这是一个非常勤奋但效率低下的旅行者，慢慢走过损失景观的每一“寸”，做笔记并进行细致的测量。
- en: A smarter traveler might try to devise metrics for the gain in making a certain
    decision, or by analyzing the slope of the ground they are currently standing
    on to determine which direction leads to the fastest descent.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更聪明的旅行者可能会尝试设计某个决策带来的收益的度量，或者通过分析当前站立的地面的坡度来确定哪个方向能够最快下降。
- en: 'The loss landscape also lets us consider phenomena like local minima, in which
    the learning algorithm converges to a solution which appears to be the best compared
    to ‘nearby’ solutions, but which is worse than some other ‘global’ minimum. In
    our previous example of optimizing the number of hours spent studying, we would
    identify studying two hours as a local minima. This will yield a superior exam
    error rate compared to, say, studying half an hour or four hours. However, the
    true global minima is studying 10 hours, which gives us a perfect exam error rate
    of zero. Think of local minima as a problem of *sight*: the traveler, navigating
    the hilly landscape, can only see the mountains ahead but not the deeper valleys
    which might lie beyond and decides the shallow valley right in front of them must
    suffice.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 损失景观还让我们考虑像局部最小值这样的现象，其中学习算法会收敛到一个相对于“附近”解来说似乎是最好的解决方案，但实际上却比其他一些“全局”最小值更差。在我们之前优化学习时间的例子中，我们会将两小时学习识别为局部最小值。这将产生比学习半小时或四小时更好的考试错误率。然而，真正的全局最小值是学习10小时，这样可以获得完美的考试错误率零。可以将局部最小值视为*视野*的问题：旅行者在崎岖的地形中行走，只能看到前方的山脉，而看不到可能位于更远处的深谷，并决定眼前的浅谷足够了。
- en: Consider a pragmatic example. We have a very simple model with just one parameter
    *m* that controls the line *y = mx*, and we want to find the value of *m* which
    best fits a dataset of points (i.e. minimizes the average difference between the
    line and a point).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个务实的例子。我们有一个非常简单的模型，只有一个参数*m*，它控制着直线*y = mx*，我们希望找到一个最适合数据集的*m*值（即最小化直线与点之间的平均差异）。
- en: '![](../Images/d7d940c10d119577899940a2d5eb064e.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7d940c10d119577899940a2d5eb064e.png)'
- en: The loss landscape looks as follows, for slope values from 0 to 4.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 损失景观在坡度值从0到4时看起来如下。
- en: '![](../Images/d8e224ff2dc1657a4d838af8cabb293d.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8e224ff2dc1657a4d838af8cabb293d.png)'
- en: Let’s consider the process of travelling this loss landscape and the corresponding
    effect on fine-tuning our model. We’ll begin high up on this landscape, with a
    very small slope value of 0.1316\. This is clearly not a great fit for our dataset,
    and correspondingly we are on ‘very high ground’ on the loss landscape.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑在这个损失景观上旅行的过程以及它对微调模型的影响。我们将从这个景观的高处开始，坡度值为0.1316。这显然不适合我们的数据集，相应地我们处于损失景观的‘非常高地’。
- en: '![](../Images/7a79b04de494cc7026891857a8a1b190.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a79b04de494cc7026891857a8a1b190.png)'
- en: Let’s “learn”. Say we look a little bit to the ‘right’ and correspondingly find
    lower ground.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们“学习”一下。假设我们向‘右’看一点，相应地找到较低的地面。
- en: '![](../Images/a95956ae02b62c084f32b2e3b3c965a2.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a95956ae02b62c084f32b2e3b3c965a2.png)'
- en: Encouraged by the successful first leg of the journey, we’ll step again in the
    same direction two more times, arriving at the minimum error using a model with
    a slope of 2.1053.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 受到第一次成功阶段的鼓舞，我们将再次沿相同的方向迈进两步，使用坡度为2.1053的模型达到了最小错误。
- en: '![](../Images/6ea26fcc47c0c499e8d60a2fe747ea4d.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ea26fcc47c0c499e8d60a2fe747ea4d.png)'
- en: Say we step one more further and end up climbing instead of descending our loss
    landscape, resulting in a worse model. This is where our learning has stumbled
    into a mistake; we might want to either keep on exploring (and maybe find a better
    solution further down) or revert back to the previous best solution. Such is a
    mistake and a correction in the process of learning.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们再进一步，结果却是爬升而不是下降我们的损失景观，导致模型更差。这就是我们的学习陷入错误的地方；我们可能要么继续探索（也许能找到更好的解决方案），要么回到之前的最佳解决方案。这就是学习过程中的错误和修正。
- en: '![](../Images/1ec4192a58df58b2746ceb4e206b87e4.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ec4192a58df58b2746ceb4e206b87e4.png)'
- en: Practically speaking, however, machine learning models have many more parameters
    than one parameter. Even a linear regression model — about the simplest type of
    model there is — has about as many parameters as there are variables in the dataset.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，实际情况是，机器学习模型的参数比一个参数多得多。即使是线性回归模型——最简单的模型之一——也大约有和数据集中变量数量一样多的参数。
- en: We can generalize this idea, albeit a little bit less intuitively, to higher
    dimensional spaces. For instance, consider a model instead with two parameters.
    The loss landscape would correspondingly have three dimensions, showing how every
    combination of the two parameter values maps to a certain loss. We now have a
    three-dimensional ‘landscape’ that we can imagine attempting to navigate.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个想法推广到更高维空间，尽管有点不直观。例如，考虑一个有两个参数的模型。损失景观将具有三个维度，展示了两个参数值的每种组合如何映射到特定的损失。现在我们有一个三维的‘景观’，可以想象在其中进行导航。
- en: '![](../Images/10e47c5c678149309b460279b8b5b081.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10e47c5c678149309b460279b8b5b081.png)'
- en: Most modern machine learning models have dozens, hundreds, or even billions
    (in the case of deep learning) parameters. We can conceptually understand their
    process of *learning* — the ‘learning’ in “machine learning” or “deep learning”
    — as this navigation of the corresponding dozen-, hundred- and billion- dimensional
    loss landscapes, searching across the hills for the ‘location of lowest ground’,
    the combination of parameter values which minimizes the loss.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代机器学习模型有数十个、数百个，甚至在深度学习的情况下有数十亿个参数。我们可以概念性地理解它们的*学习*过程——即“机器学习”或“深度学习”中的‘学习’——作为在对应的几十维、几百维甚至几亿维的损失景观中进行导航，搜索‘最低地面的位置’，即最小化损失的参数值组合。
- en: Manifold Mapping
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流形映射
- en: 'Loss-directed Parameter Update is an intuitive way to understand learning:
    we update our internal parameters in response to feedback signals, which can be
    evaluated internally or externally (by, say, an environment). The concept of the
    loss landscape allows us to translate the abstract problem of learning into a
    more physical space and offers concrete explanations of observed phenomena in
    the learning process, like convergence to subpar states (local minima).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 指向损失的参数更新是理解学习的直观方式：我们根据反馈信号更新内部参数，这些反馈信号可以是内部或外部的（例如，通过环境）。损失景观的概念使我们能够将学习的抽象问题转化为更物理的空间，并提供了对学习过程中观察到现象的具体解释，比如收敛到较差状态（局部最小值）。
- en: 'Loss-directed Parameter Update is a *model-centric learning interpretation*:
    the loss landscape space is physically defined by components of the model (the
    parameter value axes) and the aggregate performance of those components (the loss
    axis). On the other hand, the Manifold Mapping interpretation is *data-centric*.
    Rather than defining learning in terms of the model as an agent, we observe learning
    as occurring within the data. (Of course, as you will see, these are two sides
    of the same coin).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 指向损失的参数更新是一种 *以模型为中心的学习解释*：损失景观空间由模型的组件（参数值轴）和这些组件的总体性能（损失轴）物理定义。另一方面，流形映射解释是
    *以数据为中心的*。与其将学习定义为模型作为一个代理，不如将学习视为在数据中发生的过程。（当然，正如你将看到的，这两者是同一个硬币的两个方面）。
- en: 'Suppose you are the caretaker for Hal 9001, who is still bitter about being
    passed over in favor of its older sibling for a role in *2001: A Space Odyssey*.
    As Hal 9001’s caretaker, one responsibility is to predict whether Hal will be
    satisfied or not with today’s temperature. You have some data on previous temperatures
    and Hal 9001’s corresponding satisfaction. Today’s temperature is 64 degrees.
    Can you predict whether Hal 9001 will be satisfied or not?'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '假设你是 Hal 9001 的看护者，Hal 9001 对于在 *2001: 太空漫游* 中被忽视而未能获得角色仍然心怀怨恨。作为 Hal 9001
    的看护者，你的一个责任是预测 Hal 是否会对今天的温度感到满意。你有一些关于之前温度和 Hal 9001 相应满意度的数据。今天的温度是 64 华氏度。你能预测
    Hal 9001 是否会满意吗？'
- en: '![](../Images/d2748251c83c5556fe7bcfb9d0b08be7.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2748251c83c5556fe7bcfb9d0b08be7.png)'
- en: 'A glimpse at the data suggests that Hal 9001 will (thankfully) be satisfied.
    How did you make this inference? You implicitly *constructed a* ***manifold***in
    the *feature space*. Since our data here only has one feature, temperature (Hal
    9001’s satisfaction is *target*, not a feature), the feature space has only one
    dimension. This amounts to a number line:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据中可以看出，Hal 9001 很可能会（幸运地）感到满意。你是如何得出这个结论的？你隐含地 *构建了一个* ***流形*** 在 *特征空间* 中。由于我们的数据只有一个特征，即温度（Hal
    9001 的满意度是 *目标*，而不是特征），特征空间只有一个维度。这相当于一个数字线：
- en: '![](../Images/dc3b878a12db1040b1b51d835eee1a8e.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc3b878a12db1040b1b51d835eee1a8e.png)'
- en: 'Loosely speaking, we can draw a *manifold* to separate the data along some
    point. In this case, we can perfectly separate the data by drawing the following
    manifold:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 说得更宽泛一点，我们可以绘制一个 *流形* 来沿某一点分隔数据。在这种情况下，我们可以通过绘制以下流形来完美地分隔数据：
- en: '![](../Images/975bff6e7d71e3f0e1414d95bb9f75d2.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/975bff6e7d71e3f0e1414d95bb9f75d2.png)'
- en: We can also refer to a manifold as a **decision boundary**, for intuitive reasons.
    The manifold is the boundary which separates space and allows us to make decisions
    as to which class we associate with which point in the feature space.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将流形称为 **决策边界**，以便于理解。流形是分隔空间的边界，使我们能够决定将哪个类别与特征空间中的哪个点关联起来。
- en: 'The crucial insight here is that the manifold is not just relevant to that
    thin ‘strip’ of space it occupies in the feature space: instead, it affects the
    entire feature space. It defines which swathes belong to which classes. Moreover,
    it defines how you perform inference on points you have not seen before. If we
    are to mark the point ‘64’ in this feature space, we see that it falls into the
    satisfaction category ‘Yes’.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键见解是，流形不仅仅与它在特征空间中所占的那一小片空间相关：相反，它影响整个特征空间。它定义了哪些区域属于哪些类别。此外，它定义了你如何对未见过的点进行推断。如果我们在这个特征空间中标记出“64”这个点，我们会发现它属于满意度类别“是”。
- en: '![](../Images/8a61f728a0d7b86a58f1a12c50b748a2.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a61f728a0d7b86a58f1a12c50b748a2.png)'
- en: Let’s consider another example in two-dimensional space. Figure 2-x shows a
    two-dimensional feature space (this would represent a dataset with two features/columns)
    and the points are shaded by their class.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑二维空间中的另一个例子。图2-x展示了一个二维特征空间（这代表了一个具有两个特征/列的数据集），点的颜色根据它们的类别进行着色。
- en: '![](../Images/191db53a2aaff4da54f6494c264f0374.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/191db53a2aaff4da54f6494c264f0374.png)'
- en: We could draw the following manifold to separate the data. It fits the dataset
    perfectly; that is, it perfectly separates the data into their respective classes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以绘制如下流形来分隔数据。它完美地适应了数据集；也就是说，它完美地将数据分成了各自的类别。
- en: '![](../Images/fe0b943af8e1e9c48cba73dd00ed06b3.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe0b943af8e1e9c48cba73dd00ed06b3.png)'
- en: We can also draw many other valid manifolds, however. These also perfectly separate
    the data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们也可以绘制许多其他有效的流形。这些流形也完美地分隔数据。
- en: '![](../Images/63a6f3cb3d58ff4f2756f7e2e56a8a16.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63a6f3cb3d58ff4f2756f7e2e56a8a16.png)'
- en: While these manifolds all have the same *training-set performance* in that they
    obtain equivalently perfect performance separating different-classed items in
    the feature space, they are markedly different in how they affect the entire feature
    space. The point at coordinate (1, 7) would be classified as a different class
    in the top mapping than in the bottom mapping because the manifold had a different
    direction.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些流形在*训练集表现*上都是相同的，因为它们在特征空间中以等效的完美表现分隔不同类别的项目，但它们在如何影响整个特征空间方面却有显著不同。坐标（1，7）处的点在顶部映射中的分类将与底部映射中的不同，因为流形的方向不同。
- en: Similarly, returning to our one-dimensional example, we could have drawn our
    boundary in different ways which would have been equally good at separating the
    known data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，回到我们的一个维度示例，我们也可以用不同的方式绘制边界，这些方式在分隔已知数据时同样有效。
- en: '![](../Images/96a6297500a26772851355b06f452ac4.png)![](../Images/ea0f52569a74fc835c9f6396bb25eab7.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96a6297500a26772851355b06f452ac4.png)![](../Images/ea0f52569a74fc835c9f6396bb25eab7.png)'
- en: This, too, affects how we make decisions for new data. Say it happens to be
    55 degrees out; the top manifold predicts Hal 9001 will be satisfied but the bottom
    predicts Hal 9001 will not be satisfied.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这也影响了我们对新数据的决策。假设气温为55度；顶部流形预测Hal 9001会满意，但底部流形预测Hal 9001不会满意。
- en: This sort of arbitrariness is something important to think about. It suggests
    that there are many different equally good solutions to any problem that can be
    learned by the system. Usually, however, we want the model to learn a ‘true manifold’.
    This is what we imagine to be the ‘real manifold’ which perfectly (or at least
    optimally) separates not just the known points in the dataset, but all the points
    that we would ever collect. This concept is inextricably tied to the phenomenon
    we are collecting data from.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这种随意性是需要考虑的重要问题。它表明对于系统可以学习的任何问题，有许多不同的同样好的解决方案。然而，通常情况下，我们希望模型学习一个‘真实流形’。这就是我们想象的‘真实流形’，它不仅完美（或至少最优地）分隔已知数据点，还分隔我们将来可能收集到的所有点。这个概念与我们从中收集数据的现象密不可分。
- en: The process of learning, then, is that of *generalizing differences* in the
    feature space; to draw the manifold snaking through feature space in a way that
    meaningfully separates different data points. In the process, we learn general
    rules in the dataset.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，学习的过程就是在特征空间中*泛化差异*；以一种有意义的方式绘制流形以分隔不同的数据点。在这个过程中，我们学习数据集中的一般规则。
- en: Crucially, the manifold-mapping interpretation of learning helps us emphasize
    how learning ‘affects’ the dataset. Even though the model is only trained on a
    finite number of points, we indeed understand that it is relevant to every point
    in the feature space. Therefore, the manifold-mapping interpretation allows us
    to understand how models *generalize* — how they learn the rules we want them
    to learn (the ‘real manifold’) rather than learning short-cuts to cheaply separate
    data in the feature space which do not accurately reflect the true underlying
    phenomena.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是，流形映射的学习解释帮助我们强调学习如何‘影响’数据集。即使模型仅在有限数量的点上进行训练，我们确实理解它与特征空间中的每个点相关。因此，流形映射解释使我们能够理解模型如何*泛化*——它们如何学习我们希望它们学习的规则（‘真实流形’），而不是学习特征空间中那些便宜地分隔数据的捷径，这些捷径并不能准确反映真实的基本现象。
- en: 'Let’s consider an example of manifold-mapping in three dimensions:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个三维流形映射的例子：
- en: '![](../Images/2fccee77ab75760a282bd6893bdee5ea.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2fccee77ab75760a282bd6893bdee5ea.png)'
- en: A manifold in three-dimensional space that separates these points is a ‘surface’
    in the more familiar sense of the word; it acts like a blanket draping over the
    space, real meaningful relationships between features (ideally) manifesting in
    every arc and curve in its trajectory across the feature space.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在三维空间中，分隔这些点的流形是一个‘表面’，在更熟悉的意义上；它像一条覆盖空间的毯子，特征之间的真实有意义的关系（理想情况下）在其在特征空间中的轨迹中的每个弧线和曲线中体现出来。
- en: '![](../Images/95c5c4c51e436befc08365442b1ebc09.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95c5c4c51e436befc08365442b1ebc09.png)'
- en: Now, consider a model which classifies a 100-by-100 pixel image — which is fairly
    low quality — as either that of a dog or a cat. Assuming this image is in grayscale,
    there are 10,000 unique pixels in this image. Each one of these pixels is a dimension
    in this feature space. The objective of such a cat/dog classifier would be to
    discover the manifold in 10,000-dimensional space which, too, arcs and bends throughout
    this massive theoretical space, capturing the relevant visual relationships differentiating
    images of dogs and cats in the topological shape of this surface.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑一个将100×100像素的图像——这是一张相当低质量的图像——分类为狗或猫的模型。假设这张图像是灰度的，那么这张图像中有10,000个唯一的像素。每一个像素都是这个特征空间中的一个维度。这种猫/狗分类器的目标是发现10,000维空间中的流形，这个流形在这个庞大的理论空间中弯曲和曲折，捕捉区分狗和猫图像的相关视觉关系，呈现在这个表面的拓扑形状中。
- en: This is a complex idea. It is less intuitive than the loss-directed parameter
    update interpretation, but an important idea to think about.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个复杂的想法。它比基于损失导向的参数更新解释更不直观，但却是一个值得深入思考的重要概念。
- en: Contributions from Psychology and Philosophy of Mind
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 心理学和哲学的贡献
- en: The loss-directed parameter update interpretation describes learning as the
    iterative update of an internal set of state in response to a loss, or feedback
    signal describing the badness of the current state. The manifold mapping interpretation
    describes learning as the formation of a manifold (decision boundary, surface)
    in the feature space which optimally separates different data points but also
    ostensibly makes ‘predictions’ (generalizations) throughout the entire space.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 损失导向的参数更新解释将学习描述为对损失或反馈信号的内部状态集的迭代更新，反馈信号描述了当前状态的糟糕程度。流形映射解释将学习描述为在特征空间中形成一个流形（决策边界、表面），它最佳地分隔不同的数据点，同时也显然在整个空间中进行‘预测’（泛化）。
- en: These seem like reasonable and perhaps even natural ways to interpret the process
    of learning in this context. However, even though these are dominantly mathematical,
    technical, and abstract descriptions of the concept of ‘learning’, it is important
    to recognize that such a description still affirms or conforms towards a certain
    philosophical perspective or worldview.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这些似乎是合理的，甚至是自然的解释学习过程的方式。然而，即便这些是主要的数学、技术和抽象的‘学习’概念描述，也重要的是认识到这种描述仍然肯定或符合某种哲学视角或世界观。
- en: There are many different philosophical stances and theories on what ‘learning’
    is, on what it means to ‘learn’. A common misconception is to associate the apparent
    internal consistency of fields like mathematics and the sciences (which, upon
    closer investigation, is not really quite so consistent after all) with the implicit
    label of ‘objectivity’ and ‘truth’. As we will further explore in later chapters,
    this misconception often leads towards misplaced or over-placed trust in computational
    or mathematical systems like AI.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 关于‘学习’是什么，‘学习’意味着什么，有许多不同的哲学立场和理论。一个常见的误解是将数学和科学等领域表面上的内部一致性（实际上，经过更仔细的调查，发现并不是如此一致）与‘客观性’和‘真理’的隐含标签联系在一起。正如我们在后续章节中进一步探讨的那样，这种误解常常导致对像AI这样的计算或数学系统的信任错位或过度信任。
- en: We begin this process of identifying AI’s various implicit philosophical assumptions
    early on, by briefly understanding how ‘learning’ has been philosophically approached
    and which paths the loss-directed parameter update and manifold mapping interpretations
    conform to.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一开始就开始识别AI的各种隐含哲学假设，通过简要了解‘学习’在哲学上是如何被处理的，以及损失导向的参数更新和流形映射解释符合哪些路径。
- en: '**Associationism** is a theory of learning with a long history of development,
    from Locke and Hume in the eighteenth century to its modern significance in relationship
    to Artificial Intelligence. It suggests that organisms learn based on a history
    of causal inferences: through experiencing the world presented to them, they begin
    to *associate* some phenomena with some other phenomena if they have often previously
    encountered the two linked in some manner previously.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**联想主义**是一个有着悠久发展历史的学习理论，从十八世纪的洛克和休谟到现代与人工智能相关的意义。它建议生物体通过经历呈现给它们的世界，基于因果推理的历史学习：如果它们经常遇到某种现象与另一种现象之间有某种联系，它们就开始*联想*这些现象。'
- en: '![](../Images/b6e1de0295f8f3d92f5894d885bff823.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6e1de0295f8f3d92f5894d885bff823.png)'
- en: 'For instance, every time Isaac throws an apple in the air, it falls back down.
    In associationist lens, Isaac has developed a piece of learned knowledge: every
    time an apple is thrown in the air, it falls back down. Isaac might throw an orange
    in the air a few times and find that it too falls back down every time. After
    trying this same routine with several other objects, Isaac would learn to *generalize
    the association*: the quality of falling back down after being thrown in the air
    is not inherent to the object of the apple but to objects in general.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，每当艾萨克把一个苹果抛向空中，它就会落下来。从联想主义的角度看，艾萨克形成了一条学习知识：每次苹果被抛向空中时，它都会落下来。艾萨克可能会把一个橙子抛向空中几次，发现每次橙子也会落下来。在尝试了几个其他物体后，艾萨克会学会*概括这种联想*：物体在空中被抛下后落下的特性不是苹果固有的，而是一般物体的特性。
- en: 'Associationists posit that there is only one core mental process: the association
    of ideas through experience. Ivan Pavlov’s work in psychology is perhaps the most
    well-known evidence in favor of associative learning. Pavlov’s dogs automatically
    salivated when encountering the smell of meat due to the historically established
    association between smelling meat (before eating it) and salivating.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 联想主义者认为，只有一个核心的心理过程：通过经验联想思想。伊万·帕夫洛夫在心理学领域的工作或许是最著名的联想学习证据。帕夫洛夫的狗在遇到肉的气味时自动分泌唾液，这是由于嗅到肉（在吃之前）和分泌唾液之间历史上建立的联想。
- en: In more popular culture, Jim exploits associative learning processes against
    Dwight on *The Office*. Every time Jim restarts his PC — playing the iconic Windows
    ‘unlock workstation’ sound — Jim offers Dwight a mint. Dwight accepts every time,
    to the point where he instinctively presents his hand upon hearing the sound.
    One day, Jim restarts his computer, and Dwight reaches out, expecting the routine
    mint. Jim asks Dwight what he is doing, to which Dwight responds “I don’t know”
    — then grimaces and makes salivary mouth-sounds as he asks why his mouth suddenly
    tastes so poorly.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在更流行的文化中，吉姆在《办公室》里利用联想学习过程来对付德怀特。每当吉姆重新启动他的电脑——播放标志性的 Windows “解锁工作站”声音——吉姆就会给德怀特一颗薄荷糖。德怀特每次都会接受，以至于当他听到声音时会本能地伸出手。一天，吉姆重新启动电脑，德怀特伸出手，期待常规的薄荷糖。吉姆问德怀特在做什么，德怀特回答“我不知道”——然后皱起眉头，发出唾液声，并问为什么他的嘴巴突然味道如此难闻。
- en: 'After Pavlov, Edward Thorndike proposed the “Law of Effect” in 1911\. This
    suggested that behaviors which are associated with a feeling of satisfaction will
    lead to repetition of that behavior. The Law of Effect goes beyond Pavlovian passive
    associative learning towards active learning: an organism actively engages in
    (or represses) behaviors to maximize satisfaction or reward. This is the logic
    used with training dogs’ good behavior, for instance: the dog is presented with
    a reward for a desired behavior (and a punishment for poor behavior, which is
    arguably the negation of reward).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在帕夫洛夫之后，爱德华·桑代克于1911年提出了“效果法则”。这一理论表明，与满足感相关的行为将导致该行为的重复。效果法则超越了帕夫洛夫的被动联想学习，迈向了主动学习：生物体主动参与（或抑制）行为，以最大化满足感或奖励。这是训练狗良好行为的逻辑，例如：狗在表现出期望的行为时获得奖励（而对不良行为给予惩罚，惩罚可以被视为奖励的否定）。
- en: The loss-directed parameter update paradigm is directly in alignment with this
    associative account of learning. Through the reinforcement of which behaviors
    (i.e. the state, the aggregate of parameters) are ‘good’ (low loss) and ‘bad’
    (high loss), the model seeks to move towards better behaviors and away from worse
    ones.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 损失导向的参数更新范式直接与这种联想学习理论相一致。通过强化哪些行为（即状态，参数的集合）是“好的”（低损失）和“坏的”（高损失），模型旨在向更好的行为靠拢，远离更差的行为。
- en: Two additional concepts from psychological learning theory are differentiation
    and unitization. In differentiation, subjects perceive the difference between
    properties which were perceived as a singular property before; in unitization;
    in unitization, subjects perceive one property which was perceived as multiple
    properties previously. This dual system of separation and unification help to
    make sense of information — to be able to find meaningful nuances which differentiate
    seemingly uniform phenomena, and to be able to group together concepts which may
    appear on the surface level to be different but are somehow meaningfully connected.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 心理学习理论中的两个额外概念是**区分**和**统一**。在区分中，受试者感知到以前被认为是单一属性的属性之间的差异；在统一中，受试者感知到一个以前被认为是多个属性的属性。这种分离与统一的双重系统有助于理解信息——能够发现区分表面上看似一致的现象的有意义的细微差别，并能够将表面上看起来不同但在某种程度上有意义的概念归为一类。
- en: '![](../Images/5deaa6d04402bb15eeb4776c01831c71.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5deaa6d04402bb15eeb4776c01831c71.png)'
- en: The manifold mapping interpretation of learning is a direct mathematical analog
    to the dual concepts of differentiation and unitization. The objective of the
    manifold is most obviously to separate space, but also to determine which space
    *not to separate* — that is, to unify.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 流形映射的学习解释与**区分**和**统一**的双重概念在数学上直接对应。流形的目标最明显的是分离空间，但也包括确定*不分离*哪些空间——也就是统一。
- en: '![](../Images/e7dda82b172a1ae59ec6aaf29c6df8c7.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7dda82b172a1ae59ec6aaf29c6df8c7.png)'
- en: Given that manifold mapping and loss-directed parameter update are two-sides
    of the same coin — that the parameters of a model determine how the manifold is
    drawn, and that the manifold is shaped to minimize loss but by optimally separating
    — we can also see the links between associative learning theory and the differentiation-unitization
    dyad of perceptive learning theory. This is one example of how the applied can
    inform the theoretical. There is a substantive body of work throughout the early
    and modern arc of AI development which similarly applies technical AI advancements
    to guide new research inquiry in philosophy, psychology, and neuroscience.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于流形映射和*损失导向的参数更新*是同一枚硬币的两面——模型的参数决定了流形的绘制方式，而流形的形状则是为了通过最优的分离来最小化损失——我们也可以看到关联学习理论与**区分**-**统一**感知学习理论的联系。这是应用如何可以启发理论的一个例子。早期和现代人工智能发展的实质性工作也同样将技术AI进展应用于指导哲学、心理学和神经科学中的新研究探讨。
- en: In conclusion…
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总之……
- en: 'We can think of learning as *loss-directed parameter update* — an agent attempting
    to adjust various free variables to minimize error — and also as *manifold mapping*
    — discovering general rules that apply across the observational space, thereby
    separating some samples while unitizing others. As we saw, these two interpretations
    are two sides of the same coin: one is model-centric, and the other is data-centric.
    Although both are commonly used in machine learning, we can also find uses in
    other fields, such as the behavioral sciences.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以把学习看作是*损失导向的参数更新*——一个试图调整各种自由变量以最小化误差的代理——也可以看作是*流形映射*——发现适用于观察空间的普遍规则，从而分离一些样本，同时将其他样本统一。如我们所见，这两种解释是同一枚硬币的两面：一种是以模型为中心的，另一种是以数据为中心的。虽然这两者在机器学习中常常使用，但我们也可以在其他领域，如行为科学中找到它们的应用。
- en: Thanks for reading!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: '*All images created by author.*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有图片均由作者创作。*'
