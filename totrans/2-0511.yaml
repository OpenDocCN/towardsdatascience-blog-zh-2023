- en: 'Classification Metrics: The Complete Guide For Aspiring Data Scientists'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类指标：为有志数据科学家准备的完整指南
- en: 原文：[https://towardsdatascience.com/classification-metrics-the-complete-guide-for-aspiring-data-scientists-9f02eab796ae](https://towardsdatascience.com/classification-metrics-the-complete-guide-for-aspiring-data-scientists-9f02eab796ae)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/classification-metrics-the-complete-guide-for-aspiring-data-scientists-9f02eab796ae](https://towardsdatascience.com/classification-metrics-the-complete-guide-for-aspiring-data-scientists-9f02eab796ae)
- en: The only guide you’ll need to master classification metrics in Machine Learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你唯一需要的指南来掌握机器学习中的分类指标
- en: '[](https://federicotrotta.medium.com/?source=post_page-----9f02eab796ae--------------------------------)[![Federico
    Trotta](../Images/e997e3a96940c16ab5071629016d82fd.png)](https://federicotrotta.medium.com/?source=post_page-----9f02eab796ae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9f02eab796ae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9f02eab796ae--------------------------------)
    [Federico Trotta](https://federicotrotta.medium.com/?source=post_page-----9f02eab796ae--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://federicotrotta.medium.com/?source=post_page-----9f02eab796ae--------------------------------)[![Federico
    Trotta](../Images/e997e3a96940c16ab5071629016d82fd.png)](https://federicotrotta.medium.com/?source=post_page-----9f02eab796ae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9f02eab796ae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9f02eab796ae--------------------------------)
    [Federico Trotta](https://federicotrotta.medium.com/?source=post_page-----9f02eab796ae--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9f02eab796ae--------------------------------)
    ·26 min read·May 15, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9f02eab796ae--------------------------------)
    ·阅读时间26分钟·2023年5月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0c0d34355b4d7101042a7934ccc24a1e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c0d34355b4d7101042a7934ccc24a1e.png)'
- en: Image from [UliSchu](https://pixabay.com/it/users/ulischu-1993560/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1205171)
    on [Pixabay](https://pixabay.com/it//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1205171)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [UliSchu](https://pixabay.com/it/users/ulischu-1993560/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1205171)
    在 [Pixabay](https://pixabay.com/it//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1205171)
- en: 'Supervised Machine Learning can be divided into two groups of problems: classification
    and regression. This article aims to be the definitive guide on classification
    metrics: so if you’re an aspiring Data Scientist or if you’re a junior one, you
    definitely need to read this.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习可以分为两类问题：分类和回归。本文旨在成为分类指标的终极指南：所以如果你是一个有志的数据科学家或初级数据科学家，你绝对需要阅读这篇文章。
- en: 'First of all, you may also like to read the my guide on the 5 metrics you need
    to know to master a regression problem:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你也许还想阅读我关于掌握回归问题所需的5个指标的指南：
- en: '[](/mastering-the-art-of-regression-analysis-5-key-metrics-every-data-scientist-should-know-1e2a8a2936f5?source=post_page-----9f02eab796ae--------------------------------)
    [## Mastering the Art of Regression Analysis: 5 Key Metrics Every Data Scientist
    Should Know'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/mastering-the-art-of-regression-analysis-5-key-metrics-every-data-scientist-should-know-1e2a8a2936f5?source=post_page-----9f02eab796ae--------------------------------)
    [## 精通回归分析艺术：每个数据科学家应该了解的5个关键指标'
- en: The definitive guide on all the knowledge you should have on the metrics used
    in regression analysis
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你应该掌握的回归分析中使用的所有指标的终极指南
- en: towardsdatascience.com](/mastering-the-art-of-regression-analysis-5-key-metrics-every-data-scientist-should-know-1e2a8a2936f5?source=post_page-----9f02eab796ae--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/mastering-the-art-of-regression-analysis-5-key-metrics-every-data-scientist-should-know-1e2a8a2936f5?source=post_page-----9f02eab796ae--------------------------------)
- en: 'Secondly, let me tell you what you’ll find here through a table of contents:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，让我通过目录告诉你你会在这里找到什么：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As usual, you’ll find Python examples to make the theory into practice.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，你会找到 Python 示例来将理论付诸实践。
- en: What is a classification problem?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是分类问题？
- en: 'In a classification problem data are labeled into classes: in other words,
    our label values represent the class to which the data points belong.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中，数据被标记为不同的类别：换句话说，我们的标签值代表数据点所属的类别。
- en: 'There are two kinds of classification problems:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 分类问题有两种类型：
- en: '**Binary classification** problems: in this case, the target values are labeled
    with a 0 or a 1.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二分类**问题：在这种情况下，目标值用0或1进行标记。'
- en: '**Multi-class** problems: in this case, the label gets multiple values (0,
    1, 2, 3, etc.), depending on the number of classes.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多类别** 问题：在这种情况下，标签有多个值（0, 1, 2, 3 等），具体取决于类别的数量。'
- en: 'Let’s visualize them. Firstly, let’s create a binary classification dataset
    as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来可视化它们。首先，让我们按照以下方式创建一个二分类数据集：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/e8c9af6da85d31641f7636adb92e2843.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8c9af6da85d31641f7636adb92e2843.png)'
- en: The binary classification problem we’ve created. Image by Federico Trotta.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的二分类问题。图片来源：Federico Trotta。
- en: 'So, this is an example of a binary classification dataset: some datapoint belongs
    to the blue class some others to the red class. Now, it doesn''t matter what these
    classes represent. They can be apples or pears, cars or trains. It doesn''t matter.
    What it’s important now is that we’ve visualized a binary classification problem.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这是一个二分类数据集的例子：一些数据点属于蓝色类别，一些属于红色类别。现在，这些类别代表什么并不重要。它们可以是苹果或梨，汽车或火车。这不重要。现在重要的是我们可视化了一个二分类问题。
- en: 'Now, let’s visualize a multi-class problem:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们可视化一个多类别问题：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/090933c895c8b6a68266f9f6b3cb678f.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/090933c895c8b6a68266f9f6b3cb678f.png)'
- en: The multi-class classification problem we’ve created. Image by Federico Trotta.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的多类别分类问题。图片来源：Federico Trotta。
- en: So, here we’ve created a classification problem with data points belonging to
    4 classes.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这里我们创建了一个分类问题，数据点属于4个类别。
- en: One issue with multi-class classification problems is understanding if all the
    classes matter. Let’s see what we mean in the next paragraph.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 多类别分类问题的一个难点是理解所有类别是否都重要。让我们在下一段中看看这是什么意思。
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Dealing with class imbalance
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理类别不平衡
- en: 'Consider the following dataset:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下数据集：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/141e4af101075559b84a2543d5f3e62e.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/141e4af101075559b84a2543d5f3e62e.png)'
- en: The imbalanced multi-class classification problem we’ve created. Image by Federico
    Trotta.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的不平衡的多类别分类问题。图片来源：Federico Trotta。
- en: As we can see, we have a lot of blue spots and also a high number of green spots.
    The red spots, instead, are very few with respect to the others.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们有很多蓝色点，还有大量绿色点。而红色点与其他点相比非常少。
- en: 'The question is: should we take into account the red spots? In other words:
    can we perform our ML analysis by deleting the red spots because are too few?'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是：我们应该考虑红色点吗？换句话说：我们可以通过删除红色点来进行机器学习分析吗？因为这些点太少了。
- en: The answer is…it depends!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是……这要视情况而定！
- en: Generally, we can ignore the values belonging to one (or more) class(es) with
    fewer observations than the others. But in specific cases, we mustn’t! And here’s
    where domain knowledge comes into the game.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们可以忽略那些观测值少于其他类别的一个（或多个）类别。但在某些特定情况下，我们绝不能这么做！这就是领域知识发挥作用的地方。
- en: 'For example, if we’re studying fraud detection in a bank firm, we expect fraud
    transactions to be rare with respect to standard transactions. This gives us an
    imbalanced dataset, meaning: we can''t delete the values belonging to the class
    with fewer observations!'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们在研究银行公司的欺诈检测，我们期望欺诈交易相对于标准交易来说是稀少的。这给我们带来了一个不平衡的数据集，这意味着：我们不能删除属于观测值较少类别的值！
- en: The same thing is if we’re studying something in the medical field. In the case
    of rare diseases, we expect them to be….rare! So, an unbalanced dataset is what
    we expect.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在医学领域研究某些问题也是一样的。在稀有疾病的情况下，我们期望这些疾病是……稀有的！因此，不平衡的数据集是我们所期望的。
- en: Anyway, we created the datasets above on purpose for educational scopes. Generally
    speaking, it’s very hard to visualize the data points because we have more than
    one feature. So, a way to evaluate class imbalance is to display a histogram of
    the labels.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我们故意创建了上面的数据集用于教学目的。一般来说，很难可视化数据点，因为我们有多个特征。因此，评估类别不平衡的一种方法是显示标签的直方图。
- en: 'Before going on…if you don’t know the difference between a histogram and a
    bar plot, you can read the following article I wrote:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前……如果你不知道直方图和条形图的区别，你可以阅读我写的以下文章：
- en: '[](/what-is-the-difference-between-a-barplot-and-a-histogram-e62d0e532e7d?source=post_page-----9f02eab796ae--------------------------------)
    [## What is the Difference between a Barplot and a Histogram?'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/what-is-the-difference-between-a-barplot-and-a-histogram-e62d0e532e7d?source=post_page-----9f02eab796ae--------------------------------)
    [## 条形图和直方图有什么区别？'
- en: They seem to be the same, but the difference between them is relevant
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它们看起来似乎是一样的，但它们之间的差异是相关的。
- en: towardsdatascience.com](/what-is-the-difference-between-a-barplot-and-a-histogram-e62d0e532e7d?source=post_page-----9f02eab796ae--------------------------------)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/what-is-the-difference-between-a-barplot-and-a-histogram-e62d0e532e7d?source=post_page-----9f02eab796ae--------------------------------)'
- en: 'So, here’s what we can do. Let’s create a dataset with three labels like the
    following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这里是我们可以做的。让我们创建一个包含三个标签的数据集，如下所示：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Even if this data frame is created on purpose, it reflects real cases because
    it’s tabular (meaning we can manipulate it with pandas). So, if we show the head
    we get:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这个数据框是故意创建的，它也反映了真实情况，因为它是表格化的（意味着我们可以用 pandas 操作它）。所以，如果我们显示头部，我们得到：
- en: '![](../Images/3eda9e34ff0a9ae0614a2d3e81141f7a.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3eda9e34ff0a9ae0614a2d3e81141f7a.png)'
- en: The head of our data frame. Image by Federico Trotta.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据框的头部。图片由 Federico Trotta 提供。
- en: 'So, to understand if our dataset may be imbalanced or not we plot a histogram
    like so:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，为了理解我们的数据集是否可能是不平衡的，我们绘制一个直方图，如下所示：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/726b2cfbe1f01824b9f61bc724cf9fa5.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/726b2cfbe1f01824b9f61bc724cf9fa5.png)'
- en: The frequencies of the three classes of our dataset. Image by Federico Trotta.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集的三个类别的频率。图片由 Federico Trotta 提供。
- en: Well, in such cases, the three classes have the same frequency. So the dataset
    is well-balanced and we must consider all the labels in our analyses.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，在这种情况下，三个类别的频率是相同的。因此，数据集是平衡的，我们必须在分析中考虑所有标签。
- en: 'Instead, this is how class imbalance is represented via a histogram:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，这是通过直方图表示的类别不平衡：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/2638ea6d4202d8c393b99d755bd7e4e9.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2638ea6d4202d8c393b99d755bd7e4e9.png)'
- en: The imbalanced dataset we created. Image by Federico Trotta.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的不平衡数据集。图片由 Federico Trotta 提供。
- en: So, in cases like that, we need to understand if class 3 has to be taken into
    account (we’re studying “rare situations”) or not (we’re studying “situations
    with no rare events”) so that we can drop all the values associated with that.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在这种情况下，我们需要了解是否需要考虑第3类（我们正在研究“稀有情况”）或不需要考虑（我们正在研究“没有稀有事件的情况”），以便可以丢弃与其相关的所有值。
- en: Now, before diving into the metrics we need to know to solve a classification
    problem, we need to understand what a classification algorithm actually does.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在深入了解解决分类问题所需的指标之前，我们需要了解分类算法实际做了什么。
- en: What a classification algorithm actually does
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类算法实际做了什么
- en: As we know, we use Machine Learning to make predictions. This means that we
    train an ML model on the available data, expecting the predictions to be as near
    as possible to the actual data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知，我们使用机器学习来进行预测。这意味着我们在可用数据上训练一个机器学习模型，期望预测结果尽可能接近实际数据。
- en: 'If you don’t know what “training an ML model” actually means, you can read
    my article here:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不知道“训练一个机器学习模型”实际意味着什么，你可以在这里阅读我的文章：
- en: '[](/what-is-a-trained-model-5c872cfa8448?source=post_page-----9f02eab796ae--------------------------------)
    [## What is a Trained Model?'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 什么是训练好的模型？](https://towardsdatascience.com/what-is-a-trained-model-5c872cfa8448?source=post_page-----9f02eab796ae--------------------------------)'
- en: Or…what does “training an ML model” mean?
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 或者……“训练一个机器学习模型”是什么意思？
- en: towardsdatascience.com](/what-is-a-trained-model-5c872cfa8448?source=post_page-----9f02eab796ae--------------------------------)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/what-is-a-trained-model-5c872cfa8448?source=post_page-----9f02eab796ae--------------------------------)'
- en: So, let’s consider a binary classification problem. Our ML model gets the features
    as input and predicts if the data points belong to class 1 or to class 2\. If
    the predictions “are perfect”, it means that our model tells us precisely which
    of the available data belongs to class 1 and which to class 2, with 0 errors.
    So, all the actual points belonging to class 1 are predicted to belong to class
    1 by our ML model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们考虑一个二分类问题。我们的机器学习模型接收特征作为输入，并预测数据点是否属于第1类或第2类。如果预测“是完美的”，这意味着我们的模型可以准确地告诉我们哪些数据属于第1类，哪些属于第2类，没有错误。所以，所有实际属于第1类的数据点都被我们的机器学习模型预测为第1类。
- en: Of course, as you imagine, a 0% error is not possible, and this is why we need
    some metrics to evaluate our ML models.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，正如你所想，0%的错误是不可能的，这就是为什么我们需要一些指标来评估我们的机器学习模型。
- en: 'So before diving into the metrics, we need to use some nomenclature:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在深入了解指标之前，我们需要使用一些术语：
- en: We define a **True Positive (TP)** as a data point belonging to a class that
    is predicted to belong to that class. For example, if the model predicts that
    an email is spam, and it is indeed spam, then that is a true positive.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将**真正例（TP）**定义为属于某一类别的数据点，并且被预测为属于该类别。例如，如果模型预测一封邮件是垃圾邮件，并且它确实是垃圾邮件，那么这就是一个真正例。
- en: We define a **True Negative (TN)** as a data point not belonging to a class
    that is predicted to not belong to that class. For example, if the model predicts
    that an email is not spam, and it is indeed not spam, then that is a true negative.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们定义**真负例（TN）**为一个数据点不属于某个类别且预测为不属于该类别的情况。例如，如果模型预测一封邮件不是垃圾邮件，而它确实不是垃圾邮件，那么这就是一个真负例。
- en: We define a **False Positive (FP)** as a data point belonging to a class that
    is predicted to belong to another class. For example, if the model predicts that
    an email is spam, but it is actually not spam, then that is a false positive.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们定义**假正例（FP）**为一个数据点属于一个类别，但预测为属于另一个类别的情况。例如，如果模型预测一封邮件是垃圾邮件，但实际上它不是垃圾邮件，那么这就是一个假正例。
- en: We define a **False Negative (FN)** as a data point not belonging to a class
    that is predicted not to belong to that class. For example, if the model predicts
    that an email is not spam, but it is actually spam, then that is a false negative.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们定义**假负例（FN）**为一个数据点不属于某个类别但预测为不属于该类别的情况。例如，如果模型预测一封邮件不是垃圾邮件，但实际上它是垃圾邮件，那么这就是一个假负例。
- en: Generally speaking, as you may imagine, we want to minimize false positives
    and false negatives while maximizing true positives and true negatives, to make
    the model as accurate as possible. This means that our ML model makes accurate
    predictions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，正如你可能想象的那样，我们希望在最大化真正例和真负例的同时，最小化假正例和假负例，以使模型尽可能准确。这意味着我们的 ML 模型做出了准确的预测。
- en: But what does “accurate” mean? We need to dive into our first classification
    metrics to understand it.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，“准确”是什么意思？我们需要深入了解我们的第一个分类指标来理解它。
- en: Accuracy
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准确率
- en: 'The first metric we take into account is accuracy. Let’s see the formula:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑的第一个指标是准确率。让我们看看公式：
- en: The formula of accuracy written by the Author on embed-dot-fun.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在 embed-dot-fun 上写出的准确率公式。
- en: So, **accuracy** is a measure of how often our ML model is correct in its predictions.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，**准确率**是衡量我们的 ML 模型在预测中多么正确的一个指标。
- en: 'For example, let’s say we have a dataset of emails that are labeled as either
    spam or not spam. We can use ML to predict whether new emails are spam or not.
    If the model correctly predicts that 80 out of 100 emails are spam, and correctly
    predicts that 90 out of 100 emails are not spam, then its accuracy would be:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个标记为垃圾邮件或非垃圾邮件的邮件数据集。我们可以使用 ML 来预测新邮件是否为垃圾邮件。如果模型正确预测 100 封邮件中有 80
    封是垃圾邮件，并且正确预测 100 封邮件中有 90 封不是垃圾邮件，那么它的准确率将是：
- en: The calculation of our example.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们示例的计算。
- en: This means that our model is able to correctly predict the class of an email
    85% of the time. A high accuracy score (near 1) indicates that the model is performing
    well, while a low accuracy score (near 0) indicates that the model needs to be
    improved. However, accuracy alone may not always be the best metric to evaluate
    a model’s performance, especially in imbalanced datasets.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们的模型能够在 85% 的情况下正确预测邮件的类别。高准确率（接近 1）表示模型表现良好，而低准确率（接近 0）则表示模型需要改进。然而，准确率单独可能并不是评估模型性能的最佳指标，尤其是在不平衡的数据集中。
- en: This is understandable because the prevalent class has “more data” labeled to
    it, so if our model is accurate it will make accurate predictions according to
    the prevalent class. In other words, our model may be biased because of the prevalent
    class.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这是可以理解的，因为流行类别有“更多的数据”被标记，因此如果我们的模型准确，它将根据流行类别做出准确预测。换句话说，我们的模型可能因为流行类别而存在偏差。
- en: 'Let’s make an example in Python creating a dataset for this purpose:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 Python 创建一个数据集作为示例：
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We have created a simple data frame with 1000 samples that can represent the
    data of some credit card transactions, for example. We have, then, created a class
    for the fraudulent transaction which is the 5% of all the observations. So, this
    dataset is clearly imbalanced.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个包含 1000 个样本的数据框，这些样本可以代表一些信用卡交易的数据。例如，我们创建了一个欺诈交易类别，它占所有观察值的 5%。所以，这个数据集显然是不平衡的。
- en: 'If our model is accurate it is because it’s biased by the 95% of the observations
    that belong to the class that represent the non-fraud transactions. So let’s split
    the data set, make predictions with the Logistic Regression model, and print the
    accuracy:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的模型准确，那是因为它受到 95% 属于非欺诈交易类别的观察值的影响。因此，让我们拆分数据集，使用逻辑回归模型进行预测，并打印准确率：
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'So, our model is 95% accurate: hooray! Now…let’s define the other metrics and
    see what they tell us about this dataset.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们的模型准确率为 95%：好极了！现在……让我们定义其他指标，并看看它们告诉我们关于这个数据集的信息。
- en: Precision and recall
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确度和召回率
- en: '**Precision** measures the ability of a classifier to not label as positive
    a sample that is negative. In other words, it measures the fraction of true positives
    among all positive predictions. Simplifying, precision tells how accurate are
    the positive predictions of our model. That’s the formula:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**精确度**衡量分类器不将负样本标记为正样本的能力。换句话说，它衡量所有正预测中真实正样本的比例。简单来说，精确度告诉我们模型的正预测有多准确。这是公式：'
- en: The formula for the precision score written by the Author on embed-dot-fun.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 公式由作者在 embed-dot-fun 上写出。
- en: Considering an email spam classification problem, precision measures how many
    of the emails that the model classified as spam are actually spam.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑电子邮件垃圾邮件分类问题时，精确度衡量模型分类为垃圾邮件的邮件中实际有多少是垃圾邮件。
- en: 'Let’s use it in our imbalanced dataset:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在不平衡数据集中使用它：
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Auch! 95% accuracy and 0% precision: what does it mean? It means that the model
    is predicting all samples as negative, or non-fraudulent. Which is wrong, of course.
    In fact, a high precision score would indicate that the model is correctly identifying
    a high proportion of fraudulent transactions among all transactions it predicts
    as fraudulent.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀！95%的准确率和0%的精确度：这意味着什么？这意味着模型将所有样本预测为负样本或非欺诈交易。这显然是错误的。实际上，高精确度分数表示模型能够在所有预测为欺诈交易的交易中正确识别出较高比例的欺诈交易。
- en: 'Then, we have the **recall** metric that measures the fraction of true positives
    among all actual positives. In other words, it measures how many of the actual
    positives are correctly predicted. Simplifying, recall tells us how well our model
    is able to find all the positive instances in our data. Here’s the formula:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有**召回率**指标，它衡量所有实际正样本中真实正样本的比例。换句话说，它衡量实际正样本中有多少被正确预测。简单来说，召回率告诉我们模型在找到数据中的所有正实例方面的能力。这是公式：
- en: The formula for the recall score written by the Author on embed-dot-fun.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 公式由作者在 embed-dot-fun 上写出。
- en: Considering an email spam classification problem, recall measures how many of
    the actual spam emails in the dataset are correctly identified as spam emails
    by our ML classifier.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑电子邮件垃圾邮件分类问题时，召回率衡量数据集中实际的垃圾邮件有多少被我们的机器学习分类器正确识别为垃圾邮件。
- en: Let’s say that we have a dataset of 1000 emails, where 200 of them are spam
    and the rest are legitimate. We train a machine learning model to classify emails
    as spam or not spam, and it predicts that 100 of the emails are spam.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个1000封邮件的数据集，其中200封是垃圾邮件，其余的是正常邮件。我们训练一个机器学习模型来将邮件分类为垃圾邮件或非垃圾邮件，它预测了100封邮件为垃圾邮件。
- en: Precision would tell us how many of those 100 predicted spam emails are actually
    spam. For example, if 90 out of the 100 predicted spam emails are actually spam,
    then the precision would be 90%. This means that out of all emails that the model
    predicted as spam, 90% of them are actually spam.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度会告诉我们这些100个预测为垃圾邮件的邮件中实际上有多少是垃圾邮件。例如，如果100个预测为垃圾邮件的邮件中有90个实际上是垃圾邮件，那么精确度就是90%。这意味着，在模型预测为垃圾邮件的所有邮件中，90%实际上是垃圾邮件。
- en: Recall, on the other hand, tells us how many of the actual spam emails the model
    correctly identified as spam. For example, if out of the 200 actual spam emails,
    the model correctly identified 150 of them as spam, then the recall would be 75%.
    This means that out of all actual spam emails, the model correctly identified
    75% of them as spam.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，召回率告诉我们模型正确识别了多少实际的垃圾邮件。例如，如果在200个实际的垃圾邮件中，模型正确识别了150个为垃圾邮件，那么召回率就是75%。这意味着在所有实际的垃圾邮件中，模型正确识别了75%。
- en: 'Now, let’s use recall in our imbalanced dataset:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在不平衡数据集中使用召回率：
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Again: we have 95% of accuracy and 0% recall. What does it mean? As before,
    it means that the model is not correctly identifying any fraudulent transactions,
    and is instead predicting all transactions as non-fraudulent. In fact, a high
    recall score would indicate that the model is correctly identifying a high proportion
    of fraudulent transactions among all actual fraudulent transactions.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 再次说明：我们有95%的准确率和0%的召回率。这意味着什么？就像之前一样，这意味着模型没有正确识别任何欺诈交易，而是将所有交易都预测为非欺诈交易。实际上，高召回率分数表示模型能够正确识别所有实际欺诈交易中的较高比例。
- en: 'So, in practice, we want to achieve a balance between precision and recall
    depending on the problem we’re studying. To do so, we often refer to other two
    metrics that consider both of them: the confusion matrix and f1-score. Let’s see
    them.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在实际操作中，我们希望根据我们研究的问题在精度和召回率之间达到平衡。为此，我们经常参考其他两个考虑到这两者的指标：混淆矩阵和F1分数。让我们看看它们。
- en: F1-score
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: F1分数
- en: '**F1-score** is an evaluation metric in Machine Learning that combines precision
    and recall into a single value in the range 0–1\. If f1-score results in a 0 value,
    then our ML model has low performance. If f1-score results in a 1 value, then
    our ML model has high performance.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**F1分数** 是一种机器学习评估指标，它将精度和召回率结合成一个范围为0-1的单一值。如果F1分数为0，则我们的机器学习模型性能较差。如果F1分数为1，则我们的机器学习模型性能较高。'
- en: This metric balances precision and recall by calculating their harmonic mean.
    This is a type of average that is more sensitive to low values, and this is why
    this metric is particularly suitable for imbalanced datasets.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这个指标通过计算精度和召回率的调和平均值来平衡精度和召回率。这是一种对低值更敏感的平均数，因此这个指标特别适用于不平衡的数据集。
- en: 'Let''s see its formula:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它的公式：
- en: The formula for the f1-score written by the Author on embed-dot-fun.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者在embed-dot-fun上撰写的F1分数公式。
- en: 'Now, we know the results we’ll gain for our imbalanced dataset (f1-score will
    be 0). But let’s see how to use it in Python:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们知道在我们不平衡的数据集上将获得的结果（F1分数将为0）。但让我们看看如何在Python中使用它：
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the context of a spam classifier, let’s say we have a dataset of 1000 emails,
    where 200 of them are spam and the rest are legitimate. We train a machine learning
    model to classify emails as spam or not spam, and it predicts that 100 of the
    emails are spam.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在垃圾邮件分类器的背景下，假设我们有一个包含1000封邮件的数据集，其中200封是垃圾邮件，其余的是合法邮件。我们训练一个机器学习模型来将邮件分类为垃圾邮件或非垃圾邮件，并且预测有100封邮件是垃圾邮件。
- en: To calculate the F1-score of the spam classifier, we first need to calculate
    its precision and recall. Let’s say that out of the 100 predicted spam emails,
    80 are actually spam. So, the precision is 80%. Also, let’s say that out of the
    200 actual spam emails, the model correctly identified 150 of them as spam. So,
    the recall is 75%.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算垃圾邮件分类器的F1分数，我们首先需要计算其精度和召回率。假设在100封预测为垃圾邮件的邮件中，有80封实际上是垃圾邮件。因此，精度为80%。另外，假设在200封实际的垃圾邮件中，模型正确识别了150封垃圾邮件。因此，召回率为75%。
- en: 'Now we can calculate the f1-score:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以计算F1分数：
- en: The calculation for the f1-score for our spam classifier written by the Author
    on embed-dot-fun.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者在embed-dot-fun上撰写的垃圾邮件分类器的F1分数计算方法。
- en: Which is a pretty good result as we’re near 1.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当不错的结果，因为我们接近1。
- en: The confusion matrix
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: The confusion matrix is a table that summarizes the performance of a classification
    model by showing the number of true positives, false positives, true negatives,
    and false negatives.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵是一个表格，通过显示真正例、假正例、真负例和假负例的数量来总结分类模型的性能。
- en: 'In a binary classification problem, the confusion matrix has two rows and two
    columns and it’s displayed like so:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在二分类问题中，混淆矩阵有两行两列，显示如下：
- en: '![](../Images/57cee4fb7ae9ca09b54dbbaafa0f08f5.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57cee4fb7ae9ca09b54dbbaafa0f08f5.png)'
- en: The confusion matrix. Image by Federico Trotta.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵。图片由费德里科·特罗塔提供。
- en: Using the spam email classification example, let’s say that our model predicted
    100 emails as spam, out of which 80 were actually spam, and 900 emails as not
    spam, out of which 20 were actually spam.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 以垃圾邮件分类为例，假设我们的模型预测了100封邮件为垃圾邮件，其中80封实际上是垃圾邮件，预测了900封邮件为非垃圾邮件，其中20封实际上是垃圾邮件。
- en: 'The confusion matrix for this example would look like that:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子的混淆矩阵会是这样的：
- en: '![](../Images/343ca86b31c37fe0577a54e5d195b60c.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/343ca86b31c37fe0577a54e5d195b60c.png)'
- en: The confusion matrix for our spam classification problem. Image by Federico
    Trotta.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的垃圾邮件分类问题的混淆矩阵。图片由费德里科·特罗塔提供。
- en: 'Now, this is a very useful visualization tool for classification for two reasons:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这是一种非常有用的分类可视化工具，有两个原因：
- en: It can help us calculate precision and recall by visualizing it
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它可以通过可视化帮助我们计算精度和召回率
- en: It immediately tells us what matters, without any calculations. What we want
    in a classification problem, in fact, is TN and TP to be the highest possible
    while FP and FN to be the lowest possible (as much as near to 0). So, if the values
    on the main diagonal are high and the values on the other positions are low, then
    our ML model has good performance.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它立即告诉我们重要的内容，而无需任何计算。实际上，我们在分类问题中希望TN和TP尽可能高，而FP和FN尽可能低（尽量接近0）。因此，如果主对角线上的值高而其他位置上的值低，那么我们的机器学习模型表现良好。
- en: 'This is the reason why I love the confusion matrix: we just need to watch the
    main diagonal (from top-left to low-right) and non-diagonal values to evaluate
    the performance of an ML classifier.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我喜欢混淆矩阵的原因：我们只需要观察主对角线（从左上到右下）和非对角线上的值来评估机器学习分类器的性能。
- en: Considering our imbalanced dataset, we obtained 0 for precision and recall and
    we said that it means that the model is not correctly identifying any fraudulent
    transactions, and is instead predicting all transactions as non-fraudulent.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们的不平衡数据集，我们获得了0的精确度和召回率，我们说这意味着模型没有正确识别任何欺诈交易，而是将所有交易预测为非欺诈的。
- en: 'This may be really difficult to visualize, because of the formulas of precision
    and recall. We have to have them clear in our minds. Since it’s not easy for me
    to have this kind of visualization, let’s apply the confusion matrix to our example
    and see what happens:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能真的很难可视化，因为精确度和召回率的公式。我们必须在脑海中把它们弄清楚。由于我很难进行这种可视化，让我们应用混淆矩阵到我们的例子中，看看会发生什么：
- en: '[PRE13]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: See what happens?! We can clearly say that our model is not performing well
    because, while it captures 285 TNs it captures 0 TPs! That’s the visual power
    of the confusion matrix!
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 看发生了什么？我们可以清楚地说我们的模型表现不好，因为它捕获了285个TNs却没有捕获任何TPs！这就是混淆矩阵的视觉力量！
- en: 'There is also another way to display the confusion matrix, and I really love
    it because it improves the visualization experience. Here’s the code:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一种展示混淆矩阵的方式，我非常喜欢，因为它改善了可视化体验。这是代码：
- en: '[PRE14]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/110c8887323aee9064c8a11ba3d6837f.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/110c8887323aee9064c8a11ba3d6837f.png)'
- en: The visualization of our confusion matrix. Image by Federico Trotta.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的混淆矩阵可视化。图片由Federico Trotta提供。
- en: 'This kind of visualization is very useful in the case of multi-class classification
    problems. Let’s see one example:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这种可视化在多类分类问题中非常有用。让我们来看一个例子：
- en: '[PRE15]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/4b7044158efaec0bf8c86607d85a6990.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b7044158efaec0bf8c86607d85a6990.png)'
- en: The visualization of our confusion matrix for a three-class problem. Image by
    Federico Trotta.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们三类问题的混淆矩阵可视化。图片由Federico Trotta提供。
- en: In these cases is not easy to understand what are the TPs, the TNs, and so on
    because we have three classes. Anyway, we can simply refer to the values on the
    main diagonal and to the non-diagonal ones. In this case, on the main diagonal,
    we have 49, 52, and 44 which are values much higher than the non-diagonal ones,
    telling us that this model is performing well (also note we’ve calculated the
    confusion matrix on the test set!).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，很难理解真正的TPs、TNs等，因为我们有三类样本。不过，我们可以简单地参考主对角线上的值和非对角线上的值。在这种情况下，主对角线上的值为49、52和44，这些值远高于非对角线上的值，表明该模型表现良好（还要注意我们是在测试集上计算了混淆矩阵！）。
- en: Sensitivity and specificity
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 灵敏度和特异性
- en: 'There are a couple of metrics that, in my personal opinion, are more suitable
    if used in some particular cases: sensitivity and specificity. Let me talk about
    them, and then we’ll discuss the usability in particular cases.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个指标，在我个人看来，在某些特定情况下更适用：灵敏度和特异性。让我讲讲这些，然后我们将讨论在特定情况下的可用性。
- en: '**Sensitivity** is the ability of a classifier to find all the positive samples:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**灵敏度** 是分类器找到所有正样本的能力：'
- en: The formula of the sensitivity written by the Author on embed-dot-fun.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在embed-dot-fun上写的灵敏度公式。
- en: Wait a second! But isn’t it the recall?!?
  id: totrans-153
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 等一下！但这不是召回率吗？！
- en: Yes, it is. It’s not a mistake. This is why I’m telling you that these metrics
    are more suitable for particular cases. But let me go on.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，确实如此。这不是一个错误。这就是为什么我告诉你这些指标更适用于特定情况。但是让我继续。
- en: 'We define **specificity** as the ability of a classifier to find all the negative
    samples:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将**特异性**定义为分类器找到所有负样本的能力：
- en: The formula of the specificity written by the Author on embed-dot-fun.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在embed-dot-fun上写的特异性公式。
- en: 'So, both of them, describe the “precision” of a test: sensitivity describes
    the probability of a positive test. Specificity of a negative one.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: In my experience, these metrics are more suitable for classifiers used in the
    medical field, biology, and so on.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s take into account a COVID test. Consider this approach (which
    can be considered Bayesian, but let’s skip that): you make a COVID test and the
    result is positive. Question: what’s the probability to get a positive test? And
    what’s the probability to get a negative test?'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words: what are the **sensitivity and the specificity of the tool**
    you used to get the result?'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, you may ask yourself: what kind of question are you asking, Federico?'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Let me make an example I lived last summer.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'Here in Italy, a positive COVID test had to be certified by someone (let’s
    skip the reasons for that): a hospital or a pharmacy, typically. So, when we had
    the symptoms what we generally did here was test for COVID at home (3-5€ COVID
    test), then go to a pharmacy and confirm (15€ COVID test).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: So, last July I had symptoms after my wife and daughters tested positive. So
    I tested home and resulted positive. Then, immediately went to the pharmacy to
    confirm, and…resulted negative!
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'How is that possible? Easy: the tool I used at home for the COVID test was
    more sensitive than the other used by the pharmacist (or, the test used by the
    pharmacist was more specific than the one I used).'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'So, as per my experience, these metrics are particularly suitable for measuring
    instruments of any kind (mechanical, electrical, etc…) and/or in some particular
    fields (like biology, medicine, etc…). Also, remembering that those metrics use
    TP, TN, FP, and FN as precision and recall: this stresses again the fact that
    these are more suitable in the case of a binary classification problem.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Of course, I’m not telling you that sensitivity and specificity **must** be
    used only in the above-mentioned cases. They’re just more suitable, in my experience.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Log loss (cross-entropy)
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Log loss — sometimes called cross-entropy — is an important metric in classification,
    and is based on probability. This score compares the predicted probability for
    each class to the actual class labels.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the formula:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: The formula of the Log Loss written by the Author on embed-dot-fun.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'Where we have:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '`n` is the total number of observations, and `i` is a single observation.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y` is the true value.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`p` is the predicted probability.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Ln` is the natural logarithm.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To calculate the predicted probability `p`, we need to use an ML model that
    can actually calculate probabilities, like Logistic Regression, for example. In
    this case, we need to use the `predict_proba()` method like so:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'So, suppose we have a binary classification problem and suppose we calculate
    the probabilities via the Logistic Regression model, and suppose the following
    table represents our results:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da12c67a25eddbc11949aae5dcafeb31.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: A table showing actual labels and probabilities calculated via the Logistic
    Regression model. Image by Federico Trotta.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'The calculation we’d perform to obtain the Log Loss is as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: The calculation of Log Loss with the values of the above table (we’ve made it
    just for the first two occurrences) made by the Author on embedd-dot-fun.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: And this results in a value near 0 that can make us satisfied, meaning our Logistic
    Regression model is predicting quite well the labels for each class. In fact,
    a Log Loss with a value of 0 represents the best fit possible. In other words,
    a model with a Log Loss of 0 predicts each observation’s probability as the true
    value.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'But, don’t be scared: we don’t need to calculate the value of Log Loss by hand.
    Luckily for us, `sklearn`came into help. So, let’s return to our imbalanced dataset.
    To calculate Log Loss in Python we type the following:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Again, we got a bad metric on the test set, confirming all of the above.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, one last consideration: Log Loss is suitable for binary classification
    problems. How about multi-class problems?'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Categorical cross-entropy
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The categorical cross-entropy metric represents the generalization of the Log
    Loss to the multi-class case.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: This metric is particularly suitable for imbalanced datasets because it takes
    into account the probability of the predicted class. This is important when we
    have an imbalanced dataset because the relative frequency of the classes can influence
    the ability of the model to correctly predict the “minority” classes.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we have:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: The formula of the Categorical cross-entropy written by the Author on embed-dot-fun.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Where the nomenclature is the same as for the Log Loss case.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in Python, we use it the same way we do with Log Loss so by invoking
    `from sklearn.metrics import log_loss`. So, this discussion was just to stress
    the fact that there is a slight difference in the case of a binary classification
    or in the case of a multi-class classification.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: AUC/ROC curve
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**ROC** stands for “Receiver Operating Characteristic” and is a graphical way
    to evaluate a classifier by plotting the true positive rate (**TPR**) against
    the false positive rate (**FPR**) at different thresholds.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '**AUC**, instead, stands for “Area Under Curve” and represents the area under
    the ROC curve. So this is an overall performance method, ranging from 0 to 1 (where
    1 means the classifier predicts 100% of the labels as the actual values), and
    it’s more suitable when comparing different classifiers.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, let''s define TPR and FPR:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: TPR is the sensitivity (which can also be called recall, as we said).
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FPR is defined as `1-specificity`.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that AUC/ROC is suitable in the case of a binary classification problem.
    In the case of a multi-class classifier, in fact, TPR and FPR should be revisited.
    This requires some work to do, so here my advice is to use it just in the case
    of a binary classification problem.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s see how to implement this in Python:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/40cc9c8df71840ffc300a7bae2e56327.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: The AUC/ROC curve for the above code. Image by Federico Trotta.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: The dotted line represents a purely random classifier (which is like randomly
    guessing a class rather than another. And, in fact, since this is a binary classification
    problem, the line has a slope of 0.5, meaning we have a 50% chance to guess it
    right). So, the more our curve is far from it, the more our model is a good one.
    Ideally, our curve should stay as much as possible on the top-left corner meaning
    a low False Positive Rate with a high True Positive Rate.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'This is why this graph is good to compare models: better models have curves
    near the top-left corner of the graph. Let’s see an example: we’ll use the same
    dataset as before, but we’ll fit the data to three different ML models.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/8b5f78633b687b924e0fea6eea71e134.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: The AUC/ROC curve for the above code. Image by Federico Trotta.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: So, in this case, the Random Forest classifier is the one that predicts better
    our data because its curve lies on the top-left corner at values higher than the
    other models.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: To conclude this section, let me remind you that, at the beginning of this paragraph,
    we said that ROC plots TPR against the FPR at different thresholds, but we haven’t
    specified anything else. So, let’s do so in the next paragraph.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Precision-recall curve
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider a binary classification problem. We fit the data to a classifier and
    it assigns any predicted value to class 1 or to class 0: what are the criteria
    used for the assignation?'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Stop reading for a bit and try to think about that.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Yes, you guessed it right: in classification problems, a classifier assigns
    a score between 0 and 1 to each sample. This indicates the probability that the
    sample belongs to the positive class.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: So, our ML models use a threshold value to convert the probability scores into
    class predictions. In other words, any sample with a probability score greater
    than the threshold is predicted as positive, for example.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, this is true even in the case of a multiclass classification problem:
    we’ve used the case of a binary classification just to simplify our reasoning.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: So, ROC curves are useful because they show how the performance of an ML model
    varies at different threshold values.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Anyway, the fact that a classifier assigns the predicted value to a class based
    on a threshold tells us that precision and recall are a trade-off (just like bias
    and variance).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, we can even plot the precision-recall curve. Let’s see how to do so,
    using the same dataset we used for the AUC/ROC curve:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/18aee6c81d55d7ff02a864c75d1a4c32.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: The precision-recall curve for the above code. Image by Federico Trotta.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: So, above we can see that precision maintains the value of 1 until circa 0.5
    recall, then it falls dramatically fast. So, we’d like to choose a precision-recall
    trade-off before this value. Let’s say at 0.4 recall.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Another great way to visualize this tradeoff is to plot precision vs recall
    as the threshold varies. Using the same dataset, this is what happens:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种很好的可视化这种权衡的方法是绘制精确度与召回率随阈值变化的图。使用相同的数据集，这就是发生的情况：
- en: '[PRE21]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](../Images/c7868e8553eea3fd3bf87250cb287238.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7868e8553eea3fd3bf87250cb287238.png)'
- en: Precision vs recall as the threshold varies. Image by Federico Trotta.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 随着阈值变化的精确度与召回率。图像由 Federico Trotta 提供。
- en: So, the above plot confirms that the threshold that balances the precision-recall
    trade-off is around 0.4, in this case.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，上图确认了在这种情况下，平衡精确度-召回率权衡的阈值大约为0.4。
- en: 'So, when someone is telling you that found an ML model with 95% precision you
    should ask: “*At what recall?*”'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，当有人告诉你找到一个 95% 精确度的机器学习模型时，你应该问：“*召回率是多少？*”
- en: 'Finally, since they use quite the same metrics, you may be wondering when we
    should use the AUC/ROC curve and when the precision-recall one. Quoting from reference
    1 (page. 92):'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于它们使用的指标非常相似，你可能会想知道何时使用 AUC/ROC 曲线，何时使用精确度-召回率曲线。引用参考文献1（第92页）：
- en: As a rule of thumb, you should prefer precision.recall curve whenever the positive
    class is rare, or when you care most about the false positives than the false
    negatives, and ROC curve otherwise
  id: totrans-234
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 作为经验法则，当正类稀有，或当你更关心假阳性而不是假阴性时，你应该优先考虑精确度-召回率曲线，否则使用 ROC 曲线。
- en: 'Bonus: KDE and learning curves'
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 额外内容：KDE 和学习曲线
- en: Among all the methods and metrics we’ve seen above that are specific to the
    classification cases, there are two that are transversal. Meaning they can be
    used both for evaluating classification and regression problems.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们看到的所有特定于分类问题的方法和指标中，有两个是跨领域的。这意味着它们可以用于评估分类和回归问题。
- en: 'There are the KDE plot and learning curves. I’ve written about them in previous
    articles, so I’ll link them below:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 有 KDE 图和学习曲线。我在之前的文章中写过它们，所以我会在下面链接它们：
- en: 'You find what a KDE is and how to use it at point 3 in the paragraph “Graphical
    methods to validate your ML model” of the following article:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下文章的“验证机器学习模型的图形方法”段落第3点找到什么是 KDE 以及如何使用它：
- en: '[](/mastering-linear-regression-the-definitive-guide-for-aspiring-data-scientists-7abd37fcb9ed?source=post_page-----9f02eab796ae--------------------------------)
    [## Mastering Linear Regression: The Definitive Guide For Aspiring Data Scientists'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 掌握线性回归：有志数据科学家的权威指南](https://medium.com/mlearning-ai/how-to-easily-validate-your-ml-models-with-learning-curves-21cc01636083?source=post_page-----9f02eab796ae--------------------------------)'
- en: All you need to know about Linear Regression is here (including an application
    in Python)
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这里是你需要知道的关于线性回归的一切（包括在 Python 中的应用）。
- en: towardsdatascience.com](/mastering-linear-regression-the-definitive-guide-for-aspiring-data-scientists-7abd37fcb9ed?source=post_page-----9f02eab796ae--------------------------------)
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](https://medium.com/mlearning-ai/how-to-easily-validate-your-ml-models-with-learning-curves-21cc01636083?source=post_page-----9f02eab796ae--------------------------------)'
- en: 'You can read about what learning curves are and how to use them here:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里阅读关于学习曲线是什么以及如何使用它们的内容：
- en: '[](https://medium.com/mlearning-ai/how-to-easily-validate-your-ml-models-with-learning-curves-21cc01636083?source=post_page-----9f02eab796ae--------------------------------)
    [## How To Easily Validate Your ML Models With Learning Curves'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 如何轻松验证你的机器学习模型与学习曲线](https://medium.com/mlearning-ai/how-to-easily-validate-your-ml-models-with-learning-curves-21cc01636083?source=post_page-----9f02eab796ae--------------------------------)'
- en: Discover the power of learning curves to validate your ML models
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 发现学习曲线的强大功能来验证你的机器学习模型。
- en: medium.com](https://medium.com/mlearning-ai/how-to-easily-validate-your-ml-models-with-learning-curves-21cc01636083?source=post_page-----9f02eab796ae--------------------------------)
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/mlearning-ai/how-to-easily-validate-your-ml-models-with-learning-curves-21cc01636083?source=post_page-----9f02eab796ae--------------------------------)'
- en: Conclusions
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: So far, we’ve seen a lot of metrics and methodologies to validate a classification
    algorithm. If you’re wondering which one to use, I always say that, while making
    experience with each one (especially, comparing them) is a good practice, it’s
    difficult to answer the question, for a lot of reasons. Often, it’s just a question
    of taste.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到很多评估分类算法的指标和方法。如果你在考虑使用哪一种，我总是说，虽然熟悉每一种（特别是比较它们）是一个好习惯，但很难回答这个问题，原因有很多。通常，这只是一个个人偏好的问题。
- en: Also, using just one metric to evaluate an ML model is not sufficient, and this
    is a rule of thumb.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，仅使用一个指标来评估机器学习模型是不够的，这是一条经验法则。
- en: If you read other articles from me, you know that I personally love to use at
    least one analytical method and one graphical one. In the case of classification
    problems, I generally use the confusion matrix and the KDE.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你读过我其他的文章，你会知道我个人喜欢使用至少一种分析方法和一种图形方法。在分类问题的情况下，我通常使用混淆矩阵和 KDE。
- en: 'But, again: it’s a matter of personal taste. My advice here is to practice
    with them and decide which ones you like, remembering that you’ll need more than
    one to make accurate decisions on your ML models.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，再次强调：这完全是个人喜好问题。我的建议是练习使用这些方法，并决定你喜欢哪些，记住你需要不止一种方法来对你的 ML 模型做出准确的判断。
- en: '**FREE PYTHON EBOOK:**'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**免费 Python 电子书：**'
- en: 'Started learning Python Data Science but struggling with it? [***Subscribe
    to my newsletter and get my free ebook: this will give you the right learning
    path to follow to learn Python for Data Science with hands-on experience.***](https://federico-trotta.ck.page/a3970f33f4)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 刚开始学习 Python 数据科学却感到困难？ [***订阅我的新闻通讯并获取我的免费电子书：这将为你提供正确的学习路径，帮助你通过动手实践学习 Python
    数据科学。***](https://federico-trotta.ck.page/a3970f33f4)
- en: 'Enjoyed the story? Become a Medium member for 5$/month [through my referral
    link](https://medium.com/@federicotrotta/membership): I’ll earn a small commission
    to no additional fee to you:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 喜欢这个故事吗？成为 Medium 会员，仅需 5$/月 [通过我的推荐链接](https://medium.com/@federicotrotta/membership)：这样我将赚取小额佣金，但对你没有额外费用：
- en: '[](https://medium.com/@federicotrotta/membership?source=post_page-----9f02eab796ae--------------------------------)
    [## Join Medium with my referral link — Federico Trotta'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@federicotrotta/membership?source=post_page-----9f02eab796ae--------------------------------)
    [## 通过我的推荐链接加入 Medium — Federico Trotta'
- en: Read every story from Federico Trotta (and thousands of other writers on Medium).
    Your membership fee directly supports…
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读 Federico Trotta 的每一篇故事（以及 Medium 上其他成千上万的作者）。你的会员费直接支持……
- en: medium.com](https://medium.com/@federicotrotta/membership?source=post_page-----9f02eab796ae--------------------------------)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@federicotrotta/membership?source=post_page-----9f02eab796ae--------------------------------)
- en: '*Bibliography and references:*'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '*参考书目和文献：*'
- en: '*[1] Hands-on Machine Learning with Scikit-Learn & Tensorflow - Aurelien Gueron*'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*[1] 使用 Scikit-Learn 和 Tensorflow 的动手机器学习 - 奥雷利安·吉龙*'
- en: '*[2] Machine Learning with PyTorch and Scikit-learn - Sebastian Raschka, Yuxi
    Liu, Vahid Mirialili*'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*[2] 使用 PyTorch 和 Scikit-learn 的机器学习 - 塞巴斯蒂安·拉施卡，刘宇溪，瓦希德·米里亚利*'
