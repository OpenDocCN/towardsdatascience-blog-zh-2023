- en: A Gentle Introduction to Bayesian Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯深度学习的温和介绍
- en: 原文：[https://towardsdatascience.com/a-gentle-introduction-to-bayesian-deep-learning-d298c7243fd6](https://towardsdatascience.com/a-gentle-introduction-to-bayesian-deep-learning-d298c7243fd6)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-gentle-introduction-to-bayesian-deep-learning-d298c7243fd6](https://towardsdatascience.com/a-gentle-introduction-to-bayesian-deep-learning-d298c7243fd6)
- en: Welcome to the exciting world of Probabilistic Programming! This article is
    a gentle introduction to the field, you only need a basic understanding of Deep
    Learning and Bayesian statistics.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欢迎来到激动人心的概率编程世界！这篇文章是对该领域的温和介绍，你只需对深度学习和贝叶斯统计有基本了解。
- en: '[](https://medium.com/@francoisporcher?source=post_page-----d298c7243fd6--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page-----d298c7243fd6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d298c7243fd6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d298c7243fd6--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page-----d298c7243fd6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@francoisporcher?source=post_page-----d298c7243fd6--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page-----d298c7243fd6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d298c7243fd6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d298c7243fd6--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page-----d298c7243fd6--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d298c7243fd6--------------------------------)
    ·8 min read·Jul 26, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d298c7243fd6--------------------------------)
    ·阅读时间8分钟·2023年7月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: By the end of this article, you should have a basic understanding of the field,
    its applications, and how it differs from more traditional deep learning methods.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到文章结束时，你应该对这个领域、它的应用以及它与更传统的深度学习方法的不同之处有一个基本的了解。
- en: If, like me, you have heard of Bayesian Deep Learning, and you guess it involves
    bayesian statistics, but you don't know exactly how it is used, you are in the
    right place.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你像我一样听说过贝叶斯深度学习，并且猜测它涉及贝叶斯统计，但你不确切知道它是如何使用的，那么你来对地方了。
- en: Limitations of Traditional Deep Learning
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传统深度学习的局限性
- en: One of the main limitation of Traditional deep learning is that even though
    they are very powerful tools, **they don’t provide a measure of their uncertainty.**
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 传统深度学习的主要局限性之一是，即使它们是非常强大的工具，**它们也不能提供不确定性的度量。**
- en: Chat GPT can say false information with blatant confidence. Classifiers output
    probabilities that are often not calibrated.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Chat GPT 可能会以明显的自信说出错误的信息。分类器的输出概率通常未经过校准。
- en: '**Uncertainty estimation is a crucial aspect of decision-making processes,**
    especially in the areas such as healthcare, self-driving cars. We want a model
    to be able to be able to estimate when its very unsure about classifying a subject
    with a brain cancer, and in this case we require further diagnosis by a medical
    expert. Similarly we want autonomous cars to be able to slow down when it identifies
    a new environment.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**不确定性估计是决策过程中的一个关键方面，** 尤其是在医疗保健、自动驾驶汽车等领域。我们希望模型能够估计在对脑癌的分类非常不确定时，并在这种情况下需要进一步的医疗专家诊断。同样，我们希望自主汽车能够在识别到新环境时减速。'
- en: To illustrate how bad a neural network can estimates the risk, let’s look at
    a very simple Classifier Neural Network with a softmax layer in the end.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明神经网络在估计风险时可能有多糟糕，我们来看看一个非常简单的带有softmax层的分类器神经网络。
- en: The softmax has a very understandable name, it is a Soft Max function, meaning
    that it is a “smoother” version of a max function. The reason for that is that
    if we had picked a “hard” max function just taking the class with the highest
    probability, we would have a zero gradient to all the other classes.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: softmax 的名字很容易理解，它是一个软最大函数，这意味着它是一个“更平滑”的最大函数。这是因为如果我们选择了一个“硬”的最大函数，只取概率最高的类别，我们将对所有其他类别的梯度为零。
- en: With a softmax, the probability of a class can be close to 1, but never exactly
    1\. And because the sum of probabilities of all classes is 1, there is still some
    gradient flowing to the other classes.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用softmax时，一个类别的概率可以接近1，但永远不可能正好是1。由于所有类别的概率总和是1，因此仍有一些梯度流向其他类别。
- en: '![](../Images/a92130db127349e0c5d835de703844bf.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a92130db127349e0c5d835de703844bf.png)'
- en: Hard max vs Soft Max, Image by Author
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 硬最大值与软最大值，图像来源：作者
- en: However, the softmax function also presents an issue. It outputs probabilities
    that are **poorly calibrated**. Small changes in the values before applying the
    softmax function are squashed by the exponential, causing minimal changes to the
    output probabilities.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，softmax函数也存在一个问题。它输出的概率**校准不佳**。在应用softmax函数之前值的微小变化会被指数函数压缩，从而导致输出概率变化极小。
- en: This often results in overconfidence, with the model giving high probabilities
    for certain classes even in the face of uncertainty, a characteristic inherent
    to the ‘max’ nature of the softmax function.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常会导致过度自信，模型在面对不确定性时仍然给出某些类别的高概率，这是softmax函数‘max’特性固有的特征。
- en: Comparing a traditional Neural Network (NN) with a Bayesian Neural Network (BNN)
    can highlight the importance of uncertainty estimation. A BNN’s certainty is high
    when it encounters familiar distributions from training data, but as we move away
    from known distributions, the uncertainty increases, providing a more realistic
    estimation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 比较传统的神经网络（NN）与贝叶斯神经网络（BNN）可以突显不确定性估计的重要性。BNN在遇到训练数据中的熟悉分布时，确定性较高，但当我们远离已知分布时，不确定性增加，从而提供更现实的估计。
- en: 'Here is what an estimation of uncertainty can look like:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是不确定性估计的一个示例：
- en: '![](../Images/3558bea757f19d5881a6071b23f46d22.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3558bea757f19d5881a6071b23f46d22.png)'
- en: Traditional NN vs Bayesian NN, Image by Author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 传统神经网络与贝叶斯神经网络，图像来源：作者
- en: You can see that when we are close to the distribution we have observed during
    training, the model is very certain, but as we move farther from the known distribution,
    the uncertainty increases.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，当我们接近训练中观察到的分布时，模型非常确定，但当我们远离已知分布时，不确定性增加。
- en: Short Recap of Bayesian Statistics
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯统计的简短回顾
- en: 'There is one central Theorem to know in Bayesian statistics: The **Bayes Theorem.**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯统计中有一个中心定理：**贝叶斯定理**。
- en: '![](../Images/9728bf959ba064a746025dcc956c14e5.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9728bf959ba064a746025dcc956c14e5.png)'
- en: Bayes Theorem, Image by Author
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理，图像来源：作者
- en: The **prior** is the distribution of theta we think is the most likely before
    any observation. For a coin toss for example we could assume that the probability
    of having a head is a gaussian around p = 0.5
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**先验**是我们在任何观察之前认为最可能的theta分布。例如，对于抛硬币，我们可以假设得到正面概率是围绕p = 0.5的高斯分布。'
- en: If we want to put as little inductive bias as possible, we could also say p
    is uniform between [0,1].
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们想尽可能减少归纳偏置，我们也可以说p在[0,1]之间是均匀的。
- en: The **likelihood** is given a parameter theta, how likely is that we got our
    observations X, Y
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**似然性**是给定参数theta的情况下，我们得到观察X，Y的可能性。'
- en: The **marginal likelihood** is the likelihood integrated over all theta possible.
    It is called “marginal” because we marginalized theta by averaging it over all
    probabilities.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边际似然性**是对所有可能的theta积分后的似然性。它被称为“边际”的原因是我们通过对所有概率进行平均来边际化theta。'
- en: The key idea to understand in Bayesian Statistics is that you start from a prior,
    it's your best guess of what the parameter could be (it is a distribution). And
    with the observations you make, you adjust your guess, and you obtain a **posterior
    distribution.**
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯统计中，关键的概念是从先验开始，它是你对参数可能值的最佳猜测（它是一个分布）。通过你所做的观察，你调整你的猜测，并获得一个**后验分布**。
- en: Note that the prior and posterior are not a punctual estimations of theta but
    a probability distribution.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，先验和后验不是theta的点估计，而是概率分布。
- en: 'To illustrate this:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以此为例：
- en: '![](../Images/d70637d97c364fedfcac19fe62985cb3.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d70637d97c364fedfcac19fe62985cb3.png)'
- en: Image by author
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：作者
- en: On this image you can see that the prior is shifted to the right, but the likelihood
    rebalances our prior to the left, and the posterior is somewhere in between.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图中，你可以看到先验向右移动，但似然性将我们的先验重新调整到左侧，而后验位于两者之间。
- en: Introduction to Bayesian Deep Learning
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯深度学习简介
- en: 'Bayesian Deep Learning is an approach that marries two powerful mathematical
    theories: **Bayesian statistics** and **Deep Learning.**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: The essential distinction from traditional Deep Learning **resides in the treatment
    of the model’s weights:**
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: In traditional Deep Learning, we train a model from scratch, we randomly initialize
    a set of weights, and train the model until it converges to a new set of parameters.
    **We learn a single set of weights.**
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, Bayesian Deep Learning adopts a more **dynamic approach**. We begin
    with a prior belief about the weights, often assuming they follow a normal distribution.
    As we expose our model to data, we adjust this belief, thus updating the posterior
    distribution of the weights. **In essence, we learn a probability distribution
    over the weights, instead of a single set.**
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: During inference, we average predictions from all models, weighting their contributions
    based on the posterior. **This means, if a set of weights is highly probable,
    its corresponding prediction is given more weight.**
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s formalize all of that:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e31e2d5e56c00afc926cb5cd67f4931.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: Inference, Image from Author
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Inference in Bayesian Deep Learning integrates over all potential values of
    theta (weights) using the posterior distribution.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: We can also see that in Bayesian Statistics, integrals are everywhere. This
    is actually the principal limitation of the Bayesian framework. These integrals
    are **often intractable** (we don't always know a primitive of the posterior).
    So we have to do very computationally expensive approximations.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '**Advantages of Bayesian Deep Learning**'
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Advantage 1: Uncertainty estimation'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Arguably the most prominent benefit of Bayesian Deep Learning is its capacity
    for uncertainty estimation. In many domains including healthcare, autonomous driving,
    language models, computer vision, and quantitative finance, the ability to quantify
    uncertainty is crucial for making informed decisions and managing risk.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Advantage 2: Improved training efficiency'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Closely tied to the concept of uncertainty estimation is improved training efficiency.
    Since Bayesian models are aware of their own uncertainty, they can prioritize
    learning from data points where the uncertainty — and hence, potential for learning
    — is highest. This approach, known as **Active Learning**, leads to impressively
    effective and efficient training.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/d7ef1987d6e5b0ec8885c5bd7996364f.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: Demonstration of the effectiveness of Active Learning, Image from Author
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: As demonstrated in the graph below, a Bayesian Neural Network using Active Learning
    achieves 98% accuracy with just 1,000 training images. In contrast, models that
    don’t exploit uncertainty estimation tend to learn at a slower pace.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'Advantage 3: Inductive Bias'
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another advantage of Bayesian Deep Learning is the effective use of **inductive
    bias through priors**. The priors allow us to encode our initial beliefs or assumptions
    about the model parameters, which can be particularly useful in scenarios where
    **domain knowledge exists.**
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯深度学习的另一个优点是通过先验有效利用**归纳偏置**。先验允许我们编码对模型参数的初始信念或假设，这在**存在领域知识**的场景中尤为有用。
- en: Consider generative AI, where the idea is to create new data (like medical images)
    that resemble the training data. For example, if you’re generating brain images,
    and you already know the general layout of a brain — white matter inside, grey
    matter outside — this knowledge can be included in your prior. This means you
    can assign a higher probability to the presence of white matter in the center
    of the image, and grey matter towards the sides.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑生成式 AI，其思想是创建与训练数据类似的新数据（例如医学图像）。例如，如果你正在生成脑部图像，并且你已经知道脑部的一般布局——白质在内，灰质在外——这些知识可以包含在你的先验中。这意味着你可以给图像中心的白质分配更高的概率，而将灰质分配到边缘。
- en: In essence, Bayesian Deep Learning not only empowers models to learn from data
    but also enables them to start learning from a point of knowledge, rather than
    starting from scratch. This makes it a potent tool for a wide range of applications.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，贝叶斯深度学习不仅使模型能够从数据中学习，还使其能够从知识点开始学习，而不是从头开始。这使其成为广泛应用的强大工具。
- en: '![](../Images/dfdcd599cad4d636e51538fcdc7467d9.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfdcd599cad4d636e51538fcdc7467d9.png)'
- en: White Matter and Gray Matter, Image by Author
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 白质和灰质，图片由作者提供
- en: Limitations of Bayesian Deep Learning
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯深度学习的局限性
- en: It seems that Bayesian Deep Learning is incredible! So why is it that this field
    is so underrated? Indeed we often talk about Generative AI, Chat GPT, SAM, or
    more traditional neural networks, but we almost never hear about Bayesian Deep
    Learning, why is that?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯深度学习看起来非常不可思议！那么为什么这个领域会被低估呢？确实，我们经常谈论生成式 AI、Chat GPT、SAM 或更传统的神经网络，但我们几乎从未听说过贝叶斯深度学习，这是为什么呢？
- en: 'Limitation 1: Bayesian Deep Learning is slooooow'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '限制 1: 贝叶斯深度学习非常**慢**'
- en: The key to understand Bayesian Deep Learning is that we “average” the predictions
    of the model, and whenever there is an average, there is an **integral** over
    the set of parameters.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 理解贝叶斯深度学习的关键在于我们“平均”模型的预测，而每当有平均时，就会有对参数集的**积分**。
- en: But **computing an integral is often intractable**, this means that there is
    no closed or explicit form that makes the computation of this integral quick.
    So we can’t compute it directly, we have to approximate the integral by sampling
    some points, and this makes the inference very slow.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 但是**计算积分通常是不可解的**，这意味着没有一个封闭或显式的形式可以使积分计算快速。因此，我们不能直接计算它，我们必须通过采样一些点来近似积分，这使得推断非常慢。
- en: Imagine that for each data point *x* we have to average out the prediction of
    10,000 models, and that each prediction can take 1s to run, we end up with a model
    that is not **scalable with a large amount of data.**
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，对于每个数据点 *x*，我们必须平均 10,000 个模型的预测，并且每个预测可能需要 1 秒来运行，这样我们最终得到的模型就是**无法扩展到大量数据**。
- en: In most of the business cases, we need fast and scalable inference, this is
    why Bayesian Deep Learning is not so popular.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数业务场景中，我们需要快速且可扩展的推断，这就是为什么贝叶斯深度学习不那么受欢迎。
- en: 'Limitation 2: Approximation Errors'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '限制 2: 近似误差'
- en: In Bayesian Deep Learning, it’s often necessary to use approximate methods,
    such as Variational Inference, to compute the posterior distribution of weights.
    These approximations can lead to errors in the final model. The quality of the
    approximation depends on the choice of the variational family and the divergence
    measure, which can be challenging to choose and tune properly.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯深度学习中，通常需要使用近似方法，如变分推断，来计算权重的后验分布。这些近似可能导致最终模型中的错误。近似的质量取决于变分家族和散度度量的选择，这可能很难选择和调整。
- en: 'Limitation 3: Increased Model Complexity and Interpretability'
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '限制 3: 模型复杂性和可解释性的增加'
- en: While Bayesian methods offer improved measures of uncertainty, this comes at
    the cost of increased model complexity. BNNs can be difficult to interpret because
    instead of a single set of weights, we now have a distribution over possible weights.
    This complexity might lead to challenges in explaining the model’s decisions,
    especially in fields where interpretability is key.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然贝叶斯方法提供了改进的不确定性度量，但这也增加了模型的复杂性。BNNs 可能难以解释，因为我们现在有的是一个可能权重的分布，而不是单一的权重集。这种复杂性可能会导致在解释模型决策时遇到挑战，特别是在解释性至关重要的领域。
- en: There is a growing interest for XAI (Explainable AI), and Traditional Deep Neural
    Networks are already challenging to interpret because it is difficult to make
    sense of the weights, Bayesian Deep Learning is even more challenging.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于XAI（可解释人工智能）的兴趣日益增长，而传统深度神经网络本身就难以解释，因为难以理解权重，贝叶斯深度学习则更具挑战性。
- en: 'Thanks for reading! Before you go:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！在你离开之前：
- en: Check my [compilation of AI tutorials](https://github.com/FrancoisPorcher/awesome-ai-tutorials)
    on Github
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看我在 Github 上的 [AI教程合集](https://github.com/FrancoisPorcher/awesome-ai-tutorials)
- en: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----d298c7243fd6--------------------------------)
    [## GitHub - FrancoisPorcher/awesome-ai-tutorials: The best collection of AI tutorials
    to make you a…'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----d298c7243fd6--------------------------------)
    [## GitHub - FrancoisPorcher/awesome-ai-tutorials: 最佳AI教程合集，让你成为…'
- en: The best collection of AI tutorials to make you a boss of Data Science! - GitHub
    …
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最佳AI教程合集，让你成为数据科学的高手！- GitHub …
- en: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----d298c7243fd6--------------------------------)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----d298c7243fd6--------------------------------)
- en: Y*ou should get my articles in your inbox.* [***Subscribe here.***](https://medium.com/@francoisporcher/subscribe)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以在你的收件箱中获取我的文章。* [***点击这里订阅。***](https://medium.com/@francoisporcher/subscribe)'
- en: '*If you want to have access to premium articles on Medium, you only need a
    membership for $5 a month. If you sign up* [***with my link***](https://medium.com/@francoisporcher/membership)*,
    you support me with a part of your fee without additional costs.*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你希望获得Medium上的高级文章，只需每月$5的会员费用。如果你通过* [***我的链接***](https://medium.com/@francoisporcher/membership)*注册，你将以不增加额外费用的方式支持我。*'
- en: If you found this article insightful and beneficial, please consider following
    me and leaving a clap for more in-depth content! Your support helps me continue
    producing content that aids our collective understanding.
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你觉得这篇文章有见地且有益，请考虑关注我并留下掌声，以便获取更多深入内容！你的支持帮助我继续制作有助于我们共同理解的内容。
- en: References
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Ghahramani, Z. (2015). Probabilistic machine learning and artificial intelligence.
    Nature, 521(7553), 452–459\. [Link](https://www.nature.com/articles/nature14541)
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ghahramani, Z. (2015). 概率机器学习与人工智能。自然，521(7553)，452–459。 [链接](https://www.nature.com/articles/nature14541)
- en: Blundell, C., Cornebise, J., Kavukcuoglu, K., & Wierstra, D. (2015). Weight
    uncertainty in neural networks. arXiv preprint arXiv:1505.05424\. [Link](https://arxiv.org/abs/1505.05424)
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Blundell, C., Cornebise, J., Kavukcuoglu, K., & Wierstra, D. (2015). 神经网络中的权重不确定性。arXiv
    预印本 arXiv:1505.05424。 [链接](https://arxiv.org/abs/1505.05424)
- en: 'Gal, Y., & Ghahramani, Z. (2016). Dropout as a Bayesian approximation: Representing
    model uncertainty in deep learning. In international conference on machine learning
    (pp. 1050–1059). [Link](https://proceedings.mlr.press/v48/gal16.pdf)'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gal, Y., & Ghahramani, Z. (2016). Dropout 作为贝叶斯近似：在深度学习中表示模型不确定性。国际机器学习会议（第1050–1059页）。
    [链接](https://proceedings.mlr.press/v48/gal16.pdf)
- en: Louizos, C., Welling, M., & Kingma, D. P. (2017). Learning sparse neural networks
    through L0 regularization. arXiv preprint arXiv:1712.01312\. [Link](https://arxiv.org/abs/1712.01312)
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Louizos, C., Welling, M., & Kingma, D. P. (2017). 通过 L0 正则化学习稀疏神经网络。arXiv 预印本
    arXiv:1712.01312。 [链接](https://arxiv.org/abs/1712.01312)
- en: Neal, R. M. (2012). Bayesian learning for neural networks (Vol. 118). Springer
    Science & Business Media. [Link](https://www.springer.com/gp/book/9780387947242)
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Neal, R. M. (2012). 贝叶斯神经网络学习（第118卷）。Springer Science & Business Media. [链接](https://www.springer.com/gp/book/9780387947242)
