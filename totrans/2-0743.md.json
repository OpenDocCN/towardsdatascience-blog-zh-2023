["```py\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import solve_ivp\n\nimport tensorflow as tf\nfrom tensorflow import keras\ntf.random.set_seed(42)\n```", "```py\ndef u_net(u_input):\n    \"\"\"Definition of the network for u prediction.\n\n    Args:\n    ----\n    u_input: input for the u-net\n\n    Outputs:\n    --------\n    output: the output of u-net\n    \"\"\"\n\n    hidden = u_input\n    for _ in range(2):\n        hidden = tf.keras.layers.Dense(50, activation=\"tanh\")(hidden)\n    output = tf.keras.layers.Dense(3)(hidden)\n\n    return output\n```", "```py\ndef f_net(f_inputs, a_init=None, b_init=None):\n    \"\"\"Definition of the network for f prediction.\n\n    Args:\n    ----\n    f_inputs: list of inputs for the f-net\n    a_init: initial value for parameter a\n    b_init: initial value for parameter b\n\n    Outputs:\n    --------\n    output: the output of f-net\n    \"\"\"\n\n    hidden = tf.keras.layers.Concatenate()(f_inputs)\n    for _ in range(2):\n        hidden = tf.keras.layers.Dense(50, activation=\"tanh\")(hidden)\n    output = tf.keras.layers.Dense(2)(hidden)\n    output = ParameterLayer(a_init, b_init)(output)\n\n    return output\n```", "```py\nclass ParameterLayer(tf.keras.layers.Layer):\n\n    def __init__(self, a, b, trainable=True):\n        super(ParameterLayer, self).__init__()\n        self._a = tf.convert_to_tensor(a, dtype=tf.float32)\n        self._b = tf.convert_to_tensor(b, dtype=tf.float32)\n        self.trainable = trainable\n\n    def build(self, input_shape):\n        self.a = self.add_weight(\"a\", shape=(1,), \n                                 initializer=tf.keras.initializers.Constant(value=self._a),\n                                 trainable=self.trainable)\n        self.b = self.add_weight(\"b\", shape=(1,), \n                                 initializer=tf.keras.initializers.Constant(value=self._b),\n                                 trainable=self.trainable)\n\n    def get_config(self):\n        return super().get_config()\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n```", "```py\ndef create_PINN(a_init=None, b_init=None, verbose=False):\n    \"\"\"Definition of a PINN.\n\n    Args:\n    ----\n    a_init: initial value for parameter a\n    b_init: initial value for parameter b\n    verbose: boolean, indicate whether to show the model summary\n\n    Outputs:\n    --------\n    model: the PINN model\n    \"\"\"\n\n    # Input\n    t_input = tf.keras.Input(shape=(1,), name=\"time\")\n\n    # u-NN\n    u = u_net(t_input)\n\n    # f-NN\n    f = f_net([t_input, u], a_init, b_init)\n\n    # PINN model\n    model = tf.keras.models.Model(inputs=t_input, outputs=[u, f])\n\n    if verbose:\n        model.summary()\n\n    return model\n```", "```py\n@tf.function\ndef ODE_residual_calculator(t, model):\n    \"\"\"ODE residual calculation.\n\n    Args:\n    ----\n    t: temporal coordinate\n    model: PINN model\n\n    Outputs:\n    --------\n    ODE_residual: residual of the governing ODE\n    \"\"\"\n\n    # Retrieve parameters\n    a = model.layers[-1].a\n    b = model.layers[-1].b\n\n    with tf.GradientTape() as tape:\n        tape.watch(t)\n        u, f = model(t)\n\n    # Calculate gradients\n    dudt = tape.batch_jacobian(u, t)[:, :, 0]\n    du1_dt, du2_dt, du3_dt = dudt[:, :1], dudt[:, 1:2], dudt[:, 2:]\n\n    # Compute residuals\n    res1 = du1_dt - f[:, :1]\n    res2 = du2_dt - f[:, 1:]\n    res3 = du3_dt - (a*u[:, :1]*u[:, 1:2] + b)\n    ODE_residual = tf.concat([res1, res2, res3], axis=1)\n\n    return ODE_residual\n```", "```py\n@tf.function\ndef train_step(X_ODE, X, y, IC_weight, ODE_weight, data_weight, model):\n    \"\"\"Calculate gradients of the total loss with respect to network model parameters.\n\n    Args:\n    ----\n    X_ODE: collocation points for evaluating ODE residuals\n    X: observed samples\n    y: target values of the observed samples\n    IC_weight: weight for initial condition loss\n    ODE_weight: weight for ODE loss\n    data_weight: weight for data loss\n    model: PINN model\n\n    Outputs:\n    --------\n    ODE_loss: calculated ODE loss\n    IC_loss: calculated initial condition loss\n    data_loss: calculated data loss\n    total_loss: weighted sum of ODE loss, initial condition loss, and data loss\n    gradients: gradients of the total loss with respect to network model parameters.\n    \"\"\"\n\n    with tf.GradientTape() as tape:\n        tape.watch(model.trainable_weights)\n\n        # Initial condition prediction\n        y_pred_IC, _ = model(tf.zeros((1, 1)))\n\n        # ODE residual\n        ODE_residual = ODE_residual_calculator(t=X_ODE, model=model)\n\n        # Data loss\n        y_pred_data, _ = model(X)\n\n        # Calculate loss\n        IC_loss = tf.reduce_mean(keras.losses.mean_squared_error(tf.constant([[1.0, 0.8, 0.5]]), y_pred_IC))\n        ODE_loss = tf.reduce_mean(tf.square(ODE_residual))\n        data_loss = tf.reduce_mean(keras.losses.mean_squared_error(y, y_pred_data))\n\n        # Weight loss\n        total_loss = IC_loss*IC_weight + ODE_loss*ODE_weight + data_loss*data_weight\n\n    gradients = tape.gradient(total_loss, model.trainable_variables)\n\n    return ODE_loss, IC_loss, data_loss, total_loss, gradients\n```", "```py\n# Set batch size\ndata_batch_size = 100\nODE_batch_size = 1000\n\n# Samples for enforcing ODE residual loss\nN_collocation = 10000\nX_train_ODE = tf.convert_to_tensor(np.linspace(0, 10, N_collocation).reshape(-1, 1), dtype=tf.float32)\ntrain_ds_ODE = tf.data.Dataset.from_tensor_slices((X_train_ODE))\ntrain_ds_ODE = train_ds_ODE.shuffle(10*N_collocation).batch(ODE_batch_size)\n\n# Samples for enforcing data loss\nX_train_data = tf.convert_to_tensor(u_obs[:, :1], dtype=tf.float32)\ny_train_data = tf.convert_to_tensor(u_obs[:, 1:], dtype=tf.float32)\ntrain_ds_data = tf.data.Dataset.from_tensor_slices((X_train_data, y_train_data))\ntrain_ds_data = train_ds_data.shuffle(10000).batch(data_batch_size)\n```", "```py\n# Set up simulation\nu_init = [1, 0.8, 0.5]\nt_span = [0, 10]\nobs_num = 1000\n\n# Solve ODEs\nu_obs = simulate_ODEs(u_init, t_span, obs_num)\n```", "```py\ndef simulate_ODEs(u_init, t_span, obs_num):\n    \"\"\"Simulate the ODE system and obtain observational data. \n\n    Args:\n    ----\n    u_init: list of initial condition for u1, u2, and u3\n    t_span: lower and upper time limit for simulation\n    obs_num: number of observational data points\n\n    Outputs:\n    --------\n    u_obs: observed data for u's\n    \"\"\"\n\n    # Target ODEs\n    def odes(t, u):\n        du1dt = np.exp(-t/10) * u[1] * u[2]\n        du2dt = u[0] * u[2]\n        du3dt = -2 * u[0] * u[1]\n        return [du1dt, du2dt, du3dt]\n\n    # Solve ODEs\n    t_eval = np.linspace(t_span[0], t_span[1], obs_num)\n    sol = solve_ivp(odes, t_span, u_init, method='RK45', t_eval=t_eval)\n\n    # Restrcture solution\n    u_obs = np.column_stack((sol.t, sol.y[0], sol.y[1], sol.y[2]))\n\n    return u_obs\n```", "```py\n# Set up training configurations\nn_epochs = 1000\nIC_weight= tf.constant(1.0, dtype=tf.float32)   \nODE_weight= tf.constant(1.0, dtype=tf.float32)\ndata_weight= tf.constant(1.0, dtype=tf.float32)\na_list, b_list = [], []\n\n# Initial value for unknown parameters\na_init, b_init = -1, 1\n\n# Set up optimizer\noptimizer = keras.optimizers.Adam(learning_rate=2e-2)\n\n# Instantiate the PINN model\nPINN = create_PINN(a_init=a_init, b_init=b_init)\nPINN.compile(optimizer=optimizer)\n\n# Configure callbacks\n_callbacks = [keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=100),\n             tf.keras.callbacks.ModelCheckpoint('PINN_model.h5', monitor='val_loss', save_best_only=True)]\ncallbacks = tf.keras.callbacks.CallbackList(\n                _callbacks, add_history=False, model=PINN)\n\n# Start training process\nfor epoch in range(1, n_epochs + 1):  \n    print(f\"Epoch {epoch}:\")\n\n    for (X_ODE), (X, y) in zip(train_ds_ODE, train_ds_data):\n\n        # Calculate gradients\n        ODE_loss, IC_loss, data_loss, total_loss, gradients = train_step(X_ODE, X, y, IC_weight, \n                                                                         ODE_weight, data_weight, PINN)\n        # Gradient descent\n        PINN.optimizer.apply_gradients(zip(gradients, PINN.trainable_variables))\n\n    # Parameter recording\n    a_list.append(PINN.layers[-1].a.numpy())\n    b_list.append(PINN.layers[-1].b.numpy())\n\n    ####### Validation\n    val_res = ODE_residual_calculator(tf.reshape(tf.linspace(0.0, 10.0, 1000), [-1, 1]), PINN)\n    val_ODE = tf.cast(tf.reduce_mean(tf.square(val_res)), tf.float32)\n\n    u_init=tf.constant([[1.0, 0.8, 0.5]])\n    val_pred_init, _ = PINN.predict(tf.zeros((1, 1)))\n    val_IC = tf.reduce_mean(tf.square(val_pred_init-u_init))\n\n    # Callback at the end of epoch\n    callbacks.on_epoch_end(epoch, logs={'val_loss': val_IC+val_ODE})\n\n    # Re-shuffle dataset\n    train_ds_data = tf.data.Dataset.from_tensor_slices((X_train_data, y_train_data))\n    train_ds_data = train_ds_data.shuffle(10000).batch(data_batch_size) \n\n    train_ds_ODE = tf.data.Dataset.from_tensor_slices((X_train_ODE))\n    train_ds_ODE = train_ds_ODE.shuffle(10*N_collocation).batch(ODE_batch_size) \n```", "```py\nX_test = np.linspace(0, 10, 1000).reshape(-1, 1)\nX_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n\nwith tf.GradientTape() as tape:\n    tape.watch(X_test)\n    u, f = PINN(X_test)\n\n# Calculate gradients\ndudt = tape.batch_jacobian(u, X_test)[:, :, 0]\ndu1_dt, du2_dt = dudt[:, :1], dudt[:, 1:2]\n\n# Visualize comparison\nfig, ax = plt.subplots(1, 2, figsize=(10, 4))\n\nax[0].scatter(du1_dt.numpy().flatten(), f[:, 0].numpy())\nax[0].set_xlabel('$du_1$/dt', fontsize=14)\nax[0].set_ylabel('$f_1$', fontsize=14)\n\nax[1].scatter(du2_dt.numpy().flatten(), f[:, 1].numpy())\nax[1].set_xlabel('$du_2$/dt', fontsize=14)\nax[1].set_ylabel('$f_2$', fontsize=14)\n\nfor axs in ax:\n    axs.tick_params(axis='both', which='major', labelsize=12)\n    axs.grid(True)\n\nplt.tight_layout()\n```", "```py\npip3 install -U pysr\n```", "```py\npython3 -m pysr install\n```", "```py\nimport pysr\npysr.install()\n```", "```py\nt = np.linspace(0, 10, 10000).reshape(-1, 1)\nu, f = PINN.predict(t, batch_size=12800)\n\n# Configure dataframe\ndf = pd.DataFrame({\n    't': t.flatten(),\n    'u1': u[:, 0],\n    'u2': u[:, 1],\n    'u3': u[:, 2],\n    'f1': f[:, 0],\n    'f2': f[:, 1]\n})\ndf.to_csv('f_NN_IO.csv', index=False)\n```", "```py\nfrom pysr import PySRRegressor\n\nmodel = PySRRegressor(\n    niterations=20,  \n    binary_operators=[\"+\", \"*\"],\n    unary_operators=[\n        \"cos\",\n        \"exp\",\n        \"sin\",\n        \"inv(x) = 1/x\",\n    ],\n    extra_sympy_mappings={\"inv\": lambda x: 1 / x},\n    loss=\"L1DistLoss()\",\n    model_selection=\"score\",\n    complexity_of_operators={\n        \"sin\": 3, \"cos\": 3, \"exp\": 3,\n        \"inv(x) = 1/x\": 3\n    }\n)\n```", "```py\ndf = pd.read_csv('f_NN_IO.csv')\nX = df.iloc[:, :4].to_numpy()\nf1 = df.loc[:, 'f1'].to_numpy()\n\nmodel.fit(X, f1)\n```"]