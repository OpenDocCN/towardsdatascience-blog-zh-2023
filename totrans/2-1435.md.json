["```py\n!pip install bitsandbytes accelerate xformers einops langchain faiss-cpu transformers sentence-transformers\n```", "```py\nfrom typing import List\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\nimport torch\nfrom langchain.prompts import PromptTemplate\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.chains import ConversationalRetrievalChain, RetrievalQA\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain_core.vectorstores import VectorStoreRetriever\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.vectorstores import FAISS\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Device:\", device)\nif device == 'cuda':\n    print(torch.cuda.get_device_name(0))\n\n# >>> Device: cuda\n# >>> Tesla T4\n```", "```py\norig_model_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\nmodel_path = \"filipealmeida/Mistral-7B-Instruct-v0.1-sharded\"\nbnb_config = BitsAndBytesConfig(\n                                load_in_4bit=True,\n                                bnb_4bit_use_double_quant=True,\n                                bnb_4bit_quant_type=\"nf4\",\n                                bnb_4bit_compute_dtype=torch.bfloat16,\n                               )\nmodel = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, quantization_config=bnb_config, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(orig_model_path)\n```", "```py\ntext_generation_pipeline = transformers.pipeline(\n    model=model,\n    tokenizer=tokenizer,\n    task=\"text-generation\",\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id,\n    repetition_penalty=1.1,\n    return_full_text=True,\n    max_new_tokens=100,\n)\nmistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n```", "```py\ntext = \"What is Mistral? Write a short answer.\"\nmistral_llm.invoke(text)\n\n#> Mistral is a type of cold front that forms over the Mediterranean \n#> Sea and moves eastward across southern Europe, bringing strong winds \n#> and sometimes severe weather conditions such as heavy rainfall, hail, \n#> and even tornadoes.\n```", "```py\nfrom langchain.prompts import PromptTemplate\n\nprompt = PromptTemplate.from_template(\n    \"Tell me a {adjective} joke about {content}.\"\n)\nprompt.format(adjective=\"funny\", content=\"chickens\")\n\nllm_chain = prompt | mistral_llm\nllm_chain.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"})\n\n#> Why don't chickens like to tell jokes? They might crack each other\n#> up and all their eggs will scramble!\n```", "```py\nllm_chain = prompt | mistral_llm\nllm_chain.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"},\n                 config={'callbacks': [ConsoleCallbackHandler()]})\n```", "```py\n[1:chain:RunnableSequence] Entering Chain run with input:\n{\n  \"adjective\": \"funny\",\n  \"content\": \"chickens\"\n}\n\n[1:chain:RunnableSequence > 2:prompt:PromptTemplate] Entering Prompt run with input:\n{\n  \"adjective\": \"funny\",\n  \"content\": \"chickens\"\n}\n\n[1:chain:RunnableSequence > 3:llm:HuggingFacePipeline] Entering LLM run with input:\n{\n  \"prompts\": [\n    \"Tell me a funny joke about chickens.\"\n  ]\n}\n\n[1:chain:RunnableSequence > 3:llm:HuggingFacePipeline] [3.60s] Exiting LLM run with output:\n{\n  \"generations\": [\n    [\n      {\n        \"text\": \"\\n\\nWhy don't chickens like to tell jokes? They might crack each other up and all their eggs will scramble!\",\n        \"generation_info\": null,\n        \"type\": \"Generation\"\n      }\n    ]\n  ],\n  \"llm_output\": null,\n  \"run\": null\n}\n```", "```py\nchat_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a helpful AI bot. Your name is {name}. Answer with short sentences.\"),\n    ]\n)\n\nllm_chain = chat_prompt | mistral_llm\nllm_chain.invoke({\"name\": \"Mistral\", \"user_input\": \"What is your name?\"})\n\n#> Mistral: Yes, I am Mistral. How can I assist you today?\n```", "```py\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\nembeddings = HuggingFaceEmbeddings(\n    model_name=\"sentence-transformers/all-MiniLM-l6-v2\",\n    model_kwargs={\"device\": \"cuda\"},\n)\n```", "```py\ndb_docs = [\n    \"Airbus's registered headquarters is located in Leiden, Netherlands.\",\n]\n```", "```py\nfrom langchain.vectorstores import FAISS\nfrom langchain_core.vectorstores import VectorStoreRetriever\n\nvector_db = FAISS.from_texts(db_docs, embeddings)\nretriever = VectorStoreRetriever(vectorstore=vector_db)\n```", "```py\ntemplate = \"\"\"You are a helpful AI assistant. Use the following pieces of context to answer the question at the end.\n              {context}\n              If you don't know the answer, just say that you don't know, don't try to make up an answer.\n              Chat history: {history}\n              Question: {question}\n              Write your answers short. Helpful Answer:\"\"\"\n\nprompt = PromptTemplate(\n        template=template, input_variables=[\"history\", \"context\", \"question\"]\n    )\nqa = RetrievalQA.from_chain_type(\n        llm=mistral_llm,\n        chain_type=\"stuff\",\n        retriever=retriever,\n        chain_type_kwargs={\n            \"verbose\": False,\n            \"prompt\": prompt,\n            \"memory\": ConversationBufferMemory(\n                memory_key=\"history\",\n                input_key=\"question\"),\n        }\n    )\n```", "```py\nqa.run(\"Hi, who are you?\")\n#> I am an AI assistant.\n\nqa.run(\"What is the range of Airbus A380?\")\n#> The range of Airbus A380 is approximately 12,497 nautical miles.\n\nqa.run(\"What is the tire diameter of Airbus A380 in centimeters?\")\n#> I don't know.\n```", "```py\ndb_docs = [\n    \"Airbus's registered headquarters is located in Leiden, Netherlands.\",\n    \"The Airbus A380 has the largest commercial plane tire size in the world at 56 inches in diameter.\"\n]\n```", "```py\nqa.run(\"What is the tire diameter of Airbus A380 in centimeters? Write a short answer.\")\n#> 142 cm\n```", "```py\nqa.run(\"What is the tire diameter of Airbus A380 in centimeters? Explain the answer in three steps.\")\n#> 1\\. The tire diameter of Airbus A380 is 56 inches in diameter.\n#> 2\\. To convert 56 inches to centimeters, we need to multiply it by 2.54 (the conversion factor from inches to centimeters).\n#> 3\\. Therefore, the tire diameter of Airbus A380 in centimeters is 142.16 cm.\n```", "```py\ntemplate = \"\"\"You are a helpful AI assistant. Use the following pieces of context to answer the question at the end.\n              {context}\n              If you don't know the answer, just say that you don't know, don't try to make up an answer.\n              Chat history: {history}\n              Question: {question}\n              Write your answers short. Helpful Answer:\"\"\"\n\nprompt = PromptTemplate(\n        template=template, input_variables=[\"history\", \"context\", \"question\"]\n    )\nqa = RetrievalQA.from_chain_type(\n        llm=mistral_llm,\n        chain_type=\"stuff\",\n        retriever=retriever,\n        chain_type_kwargs={\n            \"verbose\": True,\n            \"prompt\": prompt,\n            \"memory\": ConversationBufferMemory(\n                memory_key=\"history\",\n                input_key=\"question\"),\n        }\n    )\n```", "```py\nYou are a helpful AI assistant. Use the following pieces of context to answer the question at the end.\nThe Airbus A380 has the largest commercial plane tire size in the world at 56 inches in diameter.\nAirbus's registered headquarters is located in Leiden, Netherlands.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\nChat history. Human: Hi, who are you?\nAI:  I am an AI assistant.\nHuman: What is the range of Airbus A380?\nAI:  The range of Airbus A380 is approximately 12,497 nautical miles.\nQuestion: What is the tire diameter of Airbus A380 in centimeters? Explain the answer in three steps.\nWrite your answers short. Helpful Answer:\n```"]