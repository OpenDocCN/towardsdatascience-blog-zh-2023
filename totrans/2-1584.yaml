- en: 'NODE: Tabular-Focused Neural Trees'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NODE：专注于表格数据的神经树
- en: 原文：[https://towardsdatascience.com/node-tabular-focused-neural-trees-ee08c752fcd2](https://towardsdatascience.com/node-tabular-focused-neural-trees-ee08c752fcd2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/node-tabular-focused-neural-trees-ee08c752fcd2](https://towardsdatascience.com/node-tabular-focused-neural-trees-ee08c752fcd2)
- en: 'Exploring NODE: A Neural Decision Tree architecture for tabular data'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索 NODE：一种用于表格数据的神经决策树架构
- en: '[](https://medium.com/@upadhyan?source=post_page-----ee08c752fcd2--------------------------------)[![Nakul
    Upadhya](../Images/336cb21272e9b1f098177adbde50e92e.png)](https://medium.com/@upadhyan?source=post_page-----ee08c752fcd2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ee08c752fcd2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ee08c752fcd2--------------------------------)
    [Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----ee08c752fcd2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@upadhyan?source=post_page-----ee08c752fcd2--------------------------------)[![Nakul
    Upadhya](../Images/336cb21272e9b1f098177adbde50e92e.png)](https://medium.com/@upadhyan?source=post_page-----ee08c752fcd2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ee08c752fcd2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ee08c752fcd2--------------------------------)
    [Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----ee08c752fcd2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ee08c752fcd2--------------------------------)
    ·7 min read·Jul 4, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ee08c752fcd2--------------------------------)
    ·7分钟阅读·2023年7月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In recent years, Machine Learning has exploded in popularity, and Neural Deep
    Learning models have blown shallow models like XGBoost [4] out of the water for
    complex tasks like image and text processing. However, deep models are often less
    effective than these shallow models regarding tabular data, and no universal deep
    learning approach consistently outperforms gradient-boosting trees.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，机器学习迅猛发展，神经深度学习模型在图像和文本处理等复杂任务中已经超越了像XGBoost [4] 这样的浅层模型。然而，深度模型在处理表格数据时往往不如这些浅层模型有效，目前尚未有一种通用的深度学习方法能够
    consistently 超越梯度提升树。
- en: 'To address this gap, Russian internet services company Yandex researchers have
    proposed a new architecture: Neural Oblivious Decision Ensembles (NODE) [1]. This
    network leverages lightweight and interpretable neural decision trees and integrates
    them within a neural network framework. This allows the model to capture complex
    interactions and dependencies in tabular data while maintaining interpretability.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这一差距，俄罗斯互联网服务公司 Yandex 的研究人员提出了一种新的架构：神经遗忘决策集成（NODE） [1]。该网络利用轻量级且可解释的神经决策树，并将其整合到神经网络框架中。这使得模型能够在保持可解释性的同时，捕捉表格数据中的复杂交互和依赖关系。
- en: In this article, I aim to explain how NODE works and the various attributes
    that make it a robust yet interpretable prediction model. As usual, I encourage
    everyone to read the original paper. If you want to use NODE, please check out
    the GitHub for the model.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我旨在解释 NODE 的工作原理以及使其成为一个强大而可解释的预测模型的各种属性。像往常一样，我鼓励大家阅读原始论文。如果你想使用 NODE，请查看模型的
    GitHub。
- en: This article is part of a series on Neural Decision Trees, highly explainable
    architectures that provide predictive power equivalent to traditional deep networks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是关于神经决策树系列中的一部分，这些高度可解释的架构提供了与传统深度网络相当的预测能力。
- en: '![Nakul Upadhya](../Images/e62aa67aa11cd0f9bcd1132257fc3773.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![Nakul Upadhya](../Images/e62aa67aa11cd0f9bcd1132257fc3773.png)'
- en: '[Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----ee08c752fcd2--------------------------------)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----ee08c752fcd2--------------------------------)'
- en: Soft/Neural Decision Trees
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软/神经决策树
- en: '[View list](https://medium.com/@upadhyan/list/softneural-decision-trees-3b4a9cb97b84?source=post_page-----ee08c752fcd2--------------------------------)3
    stories![](../Images/1c7011f3e0e8d04b22fb1f8449f06dec.png)![](../Images/edc08c13ec51a68ac192363a19540624.png)![](../Images/4880435999eae926094fadd0d5cb9442.png)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@upadhyan/list/softneural-decision-trees-3b4a9cb97b84?source=post_page-----ee08c752fcd2--------------------------------)3个故事![](../Images/1c7011f3e0e8d04b22fb1f8449f06dec.png)![](../Images/edc08c13ec51a68ac192363a19540624.png)![](../Images/4880435999eae926094fadd0d5cb9442.png)'
- en: NODE Decision Tree Structure
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NODE 决策树结构
- en: Neural Decision Trees
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经决策树
- en: 'This article assumes you have some familiarity with Neural Decision Trees.
    If you don''t, I highly encourage reading [my previous article on them](/neural-networks-as-decision-trees-89cd9fdcdf6a)
    for an in-depth explanation. However, in summary: Neural Decision trees are decision
    trees that are both soft and oblique.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本文假设你对神经决策树有一定的了解。如果你没有，我强烈建议阅读[我之前关于它们的文章](/neural-networks-as-decision-trees-89cd9fdcdf6a)以获得深入的解释。然而，总结来说：神经决策树是既柔软又倾斜的决策树。
- en: An oblique tree is one where multiple variables are used to make decisions in
    each node (usually arranged in a linear combination). For example, to predict
    a car accident, an orthogonal tree may produce a branching decision using the
    rule “car_speed — speed_limit <10.” This differs from “orthogonal” trees like
    CART (the basic decision tree), which only uses one variable at any given node
    and will need more nodes to approximate the same decision boundary.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 倾斜树是指在每个节点中使用多个变量来做出决策（通常以线性组合的形式排列）。例如，为了预测汽车事故，正交树可能使用规则“car_speed — speed_limit
    <10”来产生分支决策。这不同于像CART（基本决策树）这样的“正交”树，后者在任何给定的节点只使用一个变量，并且需要更多的节点来近似相同的决策边界。
- en: A soft tree is one where all branching decisions are probabilistic, and the
    calculations at each node define the *probability* of going into a particular
    branch. This is unlike regular, “hard” decision trees like CART, where each branching
    decision is deterministic.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 柔软树是指所有分支决策都是概率性的，每个节点的计算定义了进入特定分支的*概率*。这与像CART这样的普通“硬”决策树不同，后者的每个分支决策是确定性的。
- en: Since the tree does not restrict the number of variables used in each node,
    and the branching decisions are continuous, the entire tree is differentiable.
    Since the whole tree is differentiable, it can be integrated into any Neural Network
    framework such as Pytorch or Tensorflow and trained using traditional neural optimizers
    (ex. Stochastic Gradient Descent and Adam).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于树不限制每个节点使用的变量数量，并且分支决策是连续的，因此整个树是可微分的。由于整个树是可微分的，它可以集成到任何神经网络框架中，如Pytorch或Tensorflow，并使用传统神经优化器（例如，随机梯度下降和Adam）进行训练。
- en: NODE Trees
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NODE树
- en: The decision trees NODE uses are slightly different from your traditional Neural
    Tree. Let's break down all the differences.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: NODE使用的决策树与传统神经树略有不同。让我们逐一分析这些差异。
- en: '![](../Images/9258a53fd48e5c7664d4e8767e135e3a.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9258a53fd48e5c7664d4e8767e135e3a.png)'
- en: NODE Tree. F(*) represents the branching function, and b represents the branching
    threshold. Sigma represents the probability transformation function. R is the
    leaf result (Figure from Popov et al. 2019[1])
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: NODE树。F(*)表示分支函数，b表示分支阈值。Sigma表示概率转换函数。R是叶节点结果（图来自Popov等人，2019年[1]）
- en: '***Oblivious Nature***'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '***无意识特性***'
- en: The first significant change is the fact that the trees are Oblivious. **This
    means the tree uses the same splitting weights and thresholds for all internal
    nodes at the same depth.** As a result, Oblivious Decision Trees (ODTs) can be
    represented as a decision table with 2^*d* entries (*d* being the depth). One
    benefit is that ODTs are more interpretable than traditional decision trees as
    there are fewer decisions to parse, making the decision path easier to visualize
    and understand. However, ODTs are significantly weaker learners when compared
    to traditional decision trees (again due to the constrained nature of the splitting
    functions).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个重大变化是树的特性是“无意识”的。**这意味着树在相同深度的所有内部节点上使用相同的分裂权重和阈值。**因此，无意识决策树（ODTs）可以表示为一个具有2^*d*个条目的决策表（*d*为深度）。一个好处是，ODTs比传统决策树更具可解释性，因为决策更少，更容易可视化和理解决策路径。然而，与传统决策树相比，ODTs的学习能力显著较弱（再次由于分裂函数的受限特性）。
- en: So if our goal is performance, why would we use ODTs? As the developers of CATBoost
    [2] showed, ODTs function incredibly well when ensembled together and are less
    prone to overfitting the data. Additionally, the inference of ODTs is extremely
    efficient as the splits can all be computed in parallel to find the appropriate
    entry in the table quickly.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 那么如果我们的目标是性能，为什么要使用ODTs呢？正如CATBoost的开发者[2]所展示的，ODTs在集成在一起时效果非常好，并且不易过拟合数据。此外，ODTs的推理非常高效，因为所有分裂可以并行计算，迅速找到表中的合适条目。
- en: '***Entmax for Feature Selection and Branching***'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '***用于特征选择和分支的Entmax***'
- en: NODE's second improvement over the traditional neural decision tree is the use
    of alpha-entmax [3] over sigmoid in its architecture. Alpha-entmax is a generalized
    version of softmax capable of producing sparse distributions where most of the
    result equals zero. This sparsity is controlled by a parameter (alpha hence the
    name) where the higher the alpha, the more sparse the distribution.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: NODE相对于传统神经决策树的第二个改进是其架构中使用了alpha-entmax [3]而不是sigmoid。Alpha-entmax是softmax的一个广义版本，能够产生稀疏分布，其中大部分结果为零。这种稀疏性由一个参数（因此得名alpha）控制，alpha值越高，分布越稀疏。
- en: '![](../Images/dbaae8a85e3f549e40aa07961ff59b5d.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dbaae8a85e3f549e40aa07961ff59b5d.png)'
- en: Figure from Peters et al. 2019 [3]
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图源于Peters等人2019年[3]
- en: This transformation is used in two key places. The first use is in sparse feature
    selection. NODE includes a trainable feature selection weight matrix F (of size
    d x n where n is the number of features and d is the depth of the tree) passed
    through the entmax transformation. Since most entries of the entmax transformation
    are equal to zero, this naturally results in a small number of features being
    used in each decision node.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这种变换在两个关键地方使用。第一个使用场景是稀疏特征选择。NODE包括一个可训练的特征选择权重矩阵F（大小为d x n，其中n是特征数量，d是树的深度），通过entmax变换。由于entmax变换的大多数条目都等于零，这自然会导致在每个决策节点中使用的特征数量很少。
- en: '![](../Images/cf1df8348fad7fad614485c598803124.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf1df8348fad7fad614485c598803124.png)'
- en: Branching Function (Figure from Popov et al. 2019 [1])
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 分支函数（图源于Popov等人2019年[1]）
- en: In addition to feature selection, entmax is also used for the branching probabilities.
    This is done by passing the branching function's result, subtracting a learned
    threshold, and scaling it appropriately. This value is then concatenated with
    0 and passed into the entmax function to create a 2-class probability distribution,
    which is exactly what we need for branching.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 除了特征选择，entmax还用于分支概率。这是通过传递分支函数的结果，减去一个学习的阈值，然后适当地缩放来完成的。然后将这个值与0串联，并传入entmax函数，以创建一个2类概率分布，这正是我们进行分支所需的。
- en: '![](../Images/a69565b0df4693b377f92b3454ed0348.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a69565b0df4693b377f92b3454ed0348.png)'
- en: Branching Equation from [1]. b_i is the branching threshold tau_i is a learned
    value to scale the data (Figure by author)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 分支方程见[1]。b_i是分支阈值，tau_i是缩放数据的学习值（图由作者提供）
- en: Using this, we can define a “choice” tensor C by computing the outer product
    of all branching distributions c. This can then be multiplied by the values in
    the leaf to create the result of the network.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个，我们可以通过计算所有分支分布c的外积来定义一个“选择”张量C。然后可以将其与叶子中的值相乘，以创建网络的结果。
- en: '***Ensembling***'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '***集成***'
- en: As the name suggests, these Neural Oblivious Decision Trees are Ensembled together.
    A NODE layer is defined as a concatenation of *m* individual trees, each with
    its own branching decisions and leaf values. As mentioned before, this ensembling
    synergizes with the oblivious nature of the individual trees and helps increase
    accuracy with a reduced chance of overfitting.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正如名字所示，这些神经遗忘决策树会被集成在一起。一个NODE层被定义为*m*棵单独树的串联，每棵树都有自己的分支决策和叶值。如前所述，这种集成与单个树的遗忘性质协同作用，有助于提高准确性，同时减少过拟合的可能性。
- en: Multilayer NODE
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层NODE
- en: NODE is a flexible architecture that can be trained alone (resulting in a single
    ensembling of decision trees) or with a complex multi-layer structure where each
    set of ensembles takes input from the previous layer.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: NODE是一个灵活的架构，可以单独训练（结果是单一的决策树集成）或使用复杂的多层结构，其中每组集成从前一层获取输入。
- en: '![](../Images/18de99eba40cfb3d8eebc3af2ad104fa.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18de99eba40cfb3d8eebc3af2ad104fa.png)'
- en: Multilayer NODE Architecture (Figure from Popov et al. 2019 [1])
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 多层NODE架构（图源于Popov等人2019年[1]）
- en: The multi-layer architecture of NODE closely follows the popular DenseNet architecture.
    Each NODE layer contains several trees whose outputs are concatenated and are
    inputs for subsequent layers. The final output is then obtained by averaging the
    output of all trees from all layers. Since each layer relies on chains of all
    previous predictions, the network can capture complex dependencies.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: NODE的多层架构紧密跟随了流行的DenseNet架构。每个NODE层包含若干棵树，其输出被串联起来作为后续层的输入。最终输出是通过对所有层中所有树的输出进行平均获得的。由于每一层依赖于所有之前预测的链条，网络能够捕捉到复杂的依赖关系。
- en: Experimental Performance
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验性能
- en: To test their architecture, Popov et al. (2019) compared NODE to CatBoost [2],
    XGBoost[4], a fully connected neural network, mGBDT [5], and DeepForest [6]. Their
    methodology involved testing the models on six different datasets. Specifically,
    they did a comparison using each model's default parameters and another comparison
    where each model had tuned hyperparameters.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试他们的架构，Popov等人（2019年）将NODE与CatBoost [2]、XGBoost [4]、全连接神经网络、mGBDT [5] 和DeepForest
    [6]进行了比较。他们的方法涉及在六个不同的数据集上测试这些模型。具体来说，他们进行了使用每个模型默认参数的比较，以及另一项使用调整后的超参数的比较。
- en: '![](../Images/a78387bdd0ef1915eeb400ca95e24d1b.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a78387bdd0ef1915eeb400ca95e24d1b.png)'
- en: Comparison Results of NODE Compared to other models (Figure from Popov et al.
    2019)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: NODE与其他模型的比较结果（图来源于Popov等人，2019年）
- en: The experimental results for NODE are extremely encouraging. for one, the NODE
    architecture outperforms all the other models with the default parameters. Even
    with tuned parameters, NODE outperforms most other models on 4 of the 6 chosen
    datasets.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: NODE的实验结果极为令人鼓舞。例如，NODE架构在所有其他模型的默认参数下表现优于它们。即使在调整参数的情况下，NODE在6个选定数据集中的4个数据集上仍优于大多数其他模型。
- en: Conclusion
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: By incorporating the advantages of decision trees into the neural network architecture,
    NODE opens up new possibilities for deep learning applications in domains where
    structured tabular data is prevalent, such as finance, healthcare, and customer
    analytics.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将决策树的优势融入神经网络架构，NODE为深度学习在结构化表格数据普遍存在的领域（如金融、医疗保健和客户分析）开辟了新的可能性。
- en: This isn’t to say that NODE is perfect, however. For one, the ensembling of
    the architecture means that many of the local interpretability gains from using
    neural decision trees are thrown away, and only global feature importance can
    be gleaned from the model. However, this architecture does provide the building
    blocks for improving neural interpretability, and a [follow-up model (NODE-GAM
    [7])](https://medium.com/@chkchang21/interpretable-deep-learning-models-for-tabular-data-neural-gams-500c6ecc0122)
    has been proposed to bridge the interpretability gap.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，这并不是说NODE是完美的。例如，架构的集成意味着使用神经决策树所获得的许多局部可解释性收益被舍弃，模型中只能获得全局特征重要性。然而，这一架构确实提供了改进神经可解释性的基础构件，并且已提出了一个[后续模型
    (NODE-GAM [7])](https://medium.com/@chkchang21/interpretable-deep-learning-models-for-tabular-data-neural-gams-500c6ecc0122)以弥合可解释性差距。
- en: Additionally, while NODE outperforms many shallow models, my experience using
    it has indicated that it takes longer to train, even when using GPUs (a conclusion
    supported by the experimental results provided by the paper authors).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，虽然NODE在许多浅层模型中表现优异，但我使用它的经验表明，即使使用GPU，训练时间也较长（这一结论得到了论文作者提供的实验结果的支持）。
- en: Overall this is an extremely promising approach and one I plan on actively using
    as a component of deep learning models I develop in the future.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这是一种极具前景的方法，我计划在未来开发的深度学习模型中积极使用它作为一个组件。
- en: Resources and References
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源与参考文献
- en: 'NODE Paper: [https://arxiv.org/abs/1909.06312](https://arxiv.org/abs/1909.06312)'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NODE论文：[https://arxiv.org/abs/1909.06312](https://arxiv.org/abs/1909.06312)
- en: 'NODE Code: [https://github.com/Qwicen/node](https://github.com/Qwicen/node)'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NODE代码：[https://github.com/Qwicen/node](https://github.com/Qwicen/node)
- en: 'NODE can also be found in the Pytorch Tabular package: [https://github.com/manujosephv/pytorch_tabular](https://github.com/manujosephv/pytorch_tabular)'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NODE也可以在Pytorch Tabular包中找到：[https://github.com/manujosephv/pytorch_tabular](https://github.com/manujosephv/pytorch_tabular)
- en: 'If you are interested in Interpretable Machine Learning or Time Series Forecasting,
    consider following me: [https://medium.com/@upadhyan](https://medium.com/@upadhyan).'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你对可解释机器学习或时间序列预测感兴趣，可以考虑关注我：[https://medium.com/@upadhyan](https://medium.com/@upadhyan)。
- en: 'See my other articles on neural decision trees: [https://medium.com/@upadhyan/list/3b4a9cb97b84](https://medium.com/@upadhyan/list/3b4a9cb97b84)'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看我关于神经决策树的其他文章：[https://medium.com/@upadhyan/list/3b4a9cb97b84](https://medium.com/@upadhyan/list/3b4a9cb97b84)
- en: References
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Popov, S., Morozov, S., & Babenko, A. (2019). Neural oblivious decision
    ensembles for deep learning on tabular data. *Eight International Conference on
    Learning Representations.*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Popov, S., Morozov, S., & Babenko, A. (2019). Neural oblivious decision
    ensembles for deep learning on tabular data. *第八届国际学习表征会议*.'
- en: '[2] Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., & Gulin, A.
    (2018). CatBoost: unbiased boosting with categorical features. *Advances in neural
    information processing systems*, *31*.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., & Gulin, A.
    (2018). CatBoost: unbiased boosting with categorical features. *神经信息处理系统进展*, *31*.'
- en: '[3] Peters, B., Niculae, V., & Martins, A. (2019). Sparse Sequence-to-Sequence
    Models. In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics* (pp. 1504–1519). Association for Computational Linguistics.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Peters, B., Niculae, V., & Martins, A. (2019). 稀疏序列到序列模型。发表于*第57届计算语言学协会年会论文集*（第1504–1519页）。计算语言学协会。'
- en: '[4] Chen, T., & Guestrin, C. (2016, August). Xgboost: A scalable tree boosting
    system. In *Proceedings of the 22nd acm sigkdd international conference on knowledge
    discovery and data mining* (pp. 785–794).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Chen, T., & Guestrin, C. (2016年8月). Xgboost: 一个可扩展的树提升系统。发表于*第22届ACM SIGKDD国际知识发现与数据挖掘会议论文集*（第785–794页）。'
- en: '[5] Feng, J., Yu, Y., & Zhou, Z. H. (2018). Multi-layered gradient boosting
    decision trees. *Advances in neural information processing systems*, *31*.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Feng, J., Yu, Y., & Zhou, Z. H. (2018). 多层梯度提升决策树。*神经信息处理系统进展*, *31*。'
- en: '[6] Zhou, Z. H., & Feng, J. (2019). Deep forest. *National science review*,
    *6*(1), 74–86.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Zhou, Z. H., & Feng, J. (2019). 深度森林。*国家科学评论*, *6*(1), 74–86。'
- en: '[7] Chang, C.H., Caruana, R., & Goldenberg, A. (2022). NODE-GAM: Neural Generalized
    Additive Model for Interpretable Deep Learning. In *International Conference on
    Learning Representations*.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Chang, C.H., Caruana, R., & Goldenberg, A. (2022). NODE-GAM: 神经广义加性模型用于可解释的深度学习。发表于*国际学习表征会议*。'
