- en: 'Deep Learning for Forecasting: Preprocessing and Training'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/deep-learning-for-forecasting-preprocessing-and-training-49d2198fc0e2](https://towardsdatascience.com/deep-learning-for-forecasting-preprocessing-and-training-49d2198fc0e2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to train deep neural networks using several time series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://vcerq.medium.com/?source=post_page-----49d2198fc0e2--------------------------------)[![Vitor
    Cerqueira](../Images/9e52f462c6bc20453d3ea273eb52114b.png)](https://vcerq.medium.com/?source=post_page-----49d2198fc0e2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----49d2198fc0e2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----49d2198fc0e2--------------------------------)
    [Vitor Cerqueira](https://vcerq.medium.com/?source=post_page-----49d2198fc0e2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----49d2198fc0e2--------------------------------)
    ·8 min read·Mar 22, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be89556b550a8794748eb6a4e9d68f9b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Tamara Malaniy](https://unsplash.com/de/@tamarushphotos?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: This article is a follow-up to [a previous one](https://medium.com/towards-data-science/how-to-transform-time-series-for-deep-learning-3b6abbbb3726).
    There, we learned how to transform a time series for deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'We continue to explore deep neural networks for forecasting. In this post,
    we’ll:'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to train a global forecasting model using deep learning, including
    basic preprocessing steps;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore keras callbacks to drive the training process of a neural network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Learning for Forecasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep neural networks tackle forecasting problems using auto-regression. [Auto-regression
    is a modeling technique that involves using past observations to predict future
    ones](https://medium.com/towards-data-science/machine-learning-for-forecasting-transformations-and-feature-extraction-bbbea9de0ac2).
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural networks can be designed in different ways, such as recurrent or
    convolutional architectures. Recurrent neural networks are often preferred for
    time series data. Among other reasons, this type of network excels at modeling
    long-term dependencies. This feature can have a strong impact on forecasting performance.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s how to define a specific kind of recurrent neural network called LSTM
    (Long Short-Term Memory). The comments provide a brief description of each model
    element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Before, we learned [how to transform a time series to train this model](https://medium.com/towards-data-science/how-to-transform-time-series-for-deep-learning-3b6abbbb3726).
    But, sometimes you have several time series available.
  prefs: []
  type: TYPE_NORMAL
- en: How do you handle such cases?
  prefs: []
  type: TYPE_NORMAL
- en: Using many time series for deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The rise of global methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Forecasting models are usually created with the historical data of a time series.
    Such models can be referred to as local to that time series. By contrast, global
    methods pool the historical data of many time series to build a model.
  prefs: []
  type: TYPE_NORMAL
- en: The interest in global models surged when a method called ES-RNN won the M4
    contest — a forecasting competition featuring 100000 different time series.
  prefs: []
  type: TYPE_NORMAL
- en: When and why to use a global model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Global models can provide considerable value in forecasting problems involving
    many time series. For example, in retail where the goal is to predict the sales
    of many products.
  prefs: []
  type: TYPE_NORMAL
- en: Another motivation for using this kind of approach is to have more data. Machine
    learning algorithms are likely to perform better with larger training sets. This
    is especially so with methods with lots of parameters, such as deep neural networks.
    These [are known to be data-hungry](https://medium.com/towards-data-science/machine-learning-for-forecasting-size-matters-b5271ec784dc).
  prefs: []
  type: TYPE_NORMAL
- en: Global forecasting models do not assume that the underlying time series are
    dependent. That is, the lags of one series can be used to forecast the future
    values of another series.
  prefs: []
  type: TYPE_NORMAL
- en: Rather, these techniques exploit information from many time series to estimate
    the parameters of the model. When forecasting the future of a time series, the
    main input to the model is the past recent lags of that series.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-On
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the rest of this article, we’ll explore how to train a deep neural network
    using many time series.
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll use a data set about the power consumption in 8 regions across the USA:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67e85f3526f8d72d89c3227caafe353c.png)'
  prefs: []
  type: TYPE_IMG
- en: Daily power consumption (log) in 8 regions across the USA. Data source in reference
    [1]. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to forecast power consumption in the following days. This problem
    is relevant for power systems operators. Accurate predictions help balance the
    supply and demand of energy.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can read the data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7b623e9d593f385d13d40a6ec689f169.png)'
  prefs: []
  type: TYPE_IMG
- en: Preprocessing steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When training a deep neural network with multiple time series you need to apply
    some preprocessing steps. Here, we’ll explore the following two:'
  prefs: []
  type: TYPE_NORMAL
- en: Mean-scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The available set of time series can have different scales. Thus, it’s important
    to normalize each series into a common value range. For global forecasting models,
    this is usually done by dividing each observation by the mean value of the respective
    series.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: After mean-scaling, the log transformation can also be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: '[In a previous article](https://medium.com/towards-data-science/3-ways-to-deal-with-heteroskedasticity-in-time-series-831f6499e688),
    we explore how taking the log of time series is a useful transformation to handle
    heteroskedasticity. The log transformation can also help avoid saturation areas
    of the neural network. Saturation occurs when the neural network becomes insensitive
    to different inputs. This hampers the learning process, leading to a poor model.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Auto-regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After pre-processing each time series, we need to transform them from sequences
    into a set of observations. For a single time series, you can [check the previous
    article](https://medium.com/towards-data-science/how-to-transform-time-series-for-deep-learning-3b6abbbb3726)
    to learn the details of this process.
  prefs: []
  type: TYPE_NORMAL
- en: For several time series, the idea is similar. We create a set of observations
    for each series individually. Then, these are concatenated into a single data
    set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how you can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, you combine the data of each time series by a row-wise concatenation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c12f9ea7e71bc52404b4c3477c85910f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we split the target variables from the explanatory ones [as described
    before](https://medium.com/towards-data-science/how-to-transform-time-series-for-deep-learning-3b6abbbb3726):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Using Callbacks for Training a Deep Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/8b4be613bb8df9eeacdd8a0a358c9943.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jack B](https://unsplash.com/@nervum?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural networks are iterative methods. They go over the training dataset
    several times in cycles called epochs.
  prefs: []
  type: TYPE_NORMAL
- en: In the above example, we ran 100 epochs. But, it’s not clear how many epochs
    one should run to train a network. Too few epochs can lead to underfitting; too
    many iterations lead to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: A way to handle this problem is by monitoring the performance of the neural
    network after each epoch. Each time the model improves performance, you save it
    before continuing the training process. Then, after the training is over, you
    get the best model that was saved.
  prefs: []
  type: TYPE_NORMAL
- en: In keras, you can use callbacks to handle this process for you. A callback is
    a function that performs some action during the training process. You can check
    [keras documentation](https://keras.io/api/callbacks/) for a complete list of
    the available callbacks. Or how to learn to write your own!
  prefs: []
  type: TYPE_NORMAL
- en: 'The callback that is used to save the model during training is called ModelCheckPoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Another interesting callback you can use for training is [EarlyStopping](https://keras.io/api/callbacks/early_stopping/).
    It can be used to stop training when performance has stopped improving.
  prefs: []
  type: TYPE_NORMAL
- en: Making predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After training, we can retrieve the best model and make predictions on the test
    set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Key Take-Aways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many forecasting problems involve many time series, for example in the retail
    domain. In such cases, global methods are often better to build a model;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing the time series is important before training a neural network.
    Mean scaling helps bring all series into a common value range. Taking the log
    stabilizes the variance and avoids saturation areas;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use callbacks to drive the training process of neural networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thank you for reading, and see you in the next story!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [Hourly Energy Consumption time series](https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption?resource=download)
    (License: [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/))'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Hewamalage, Hansika, Christoph Bergmeir, and Kasun Bandara. “Recurrent
    neural networks for time series forecasting: Current status and future directions.”
    *International Journal of Forecasting* 37.1 (2021): 388–427.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Slawek Smyl, Jai Ranganathan, and Andrea Pasqua. M4 Forecasting Competition:
    Introducing a New Hybrid ES-RNN Model, 2018\. URL [https://eng.uber.com/](https://eng.uber.com/)
    m4-forecasting-competition/.'
  prefs: []
  type: TYPE_NORMAL
