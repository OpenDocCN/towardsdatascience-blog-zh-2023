- en: Big Data File Formats, Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤§æ•°æ®æ–‡ä»¶æ ¼å¼è§£é‡Š
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/big-data-file-formats-explained-275876dc1fc9](https://towardsdatascience.com/big-data-file-formats-explained-275876dc1fc9)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/big-data-file-formats-explained-275876dc1fc9](https://towardsdatascience.com/big-data-file-formats-explained-275876dc1fc9)
- en: Parquet vs ORC vs AVRO vs JSON. Which one to choose and how to use them?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Parquet ä¸ ORC ä¸ AVRO ä¸ JSONã€‚è¯¥é€‰æ‹©å“ªä¸€ä¸ªï¼Œå¦‚ä½•ä½¿ç”¨ï¼Ÿ
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----275876dc1fc9--------------------------------)[![ğŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----275876dc1fc9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----275876dc1fc9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----275876dc1fc9--------------------------------)
    [ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----275876dc1fc9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mshakhomirov.medium.com/?source=post_page-----275876dc1fc9--------------------------------)[![ğŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----275876dc1fc9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----275876dc1fc9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----275876dc1fc9--------------------------------)
    [ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----275876dc1fc9--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----275876dc1fc9--------------------------------)
    Â·9 min readÂ·Feb 28, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----275876dc1fc9--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 9 åˆ†é’ŸÂ·2023å¹´2æœˆ28æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ce5abf8987056d730cd51c36eda44998.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce5abf8987056d730cd51c36eda44998.png)'
- en: Photo by [James Lee](https://unsplash.com/@picsbyjameslee?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç…§ç‰‡ç”± [James Lee](https://unsplash.com/@picsbyjameslee?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Iâ€™m a big fan of data warehouse (DWH) solutions with ELT-designed (Extract-Load-Transform)
    data pipelines. However, at some point, I faced the requirement to *process* raw
    event data in ***Cloud Storage*** and had **to choose the file format** for data
    files.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ˜¯é‡‡ç”¨ ELTï¼ˆExtract-Load-Transformï¼‰è®¾è®¡çš„æ•°æ®ä»“åº“ï¼ˆDWHï¼‰è§£å†³æ–¹æ¡ˆçš„å¿ å®ç²‰ä¸ã€‚ç„¶è€Œï¼Œåœ¨æŸäº›æ—¶å€™ï¼Œæˆ‘é¢ä¸´äº†*å¤„ç†*åŸå§‹äº‹ä»¶æ•°æ®çš„è¦æ±‚ï¼Œå¹¶ä¸”å¿…é¡»**é€‰æ‹©æ–‡ä»¶æ ¼å¼**ç”¨äºæ•°æ®æ–‡ä»¶ã€‚
- en: '*This is a typical scenario when machine learning engineers are tasked to create
    behavior datasets to train models and to generate better product recommendations
    or predict customer churn.*'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*è¿™æ˜¯ä¸€ç§å…¸å‹çš„åœºæ™¯ï¼Œå½“æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆè¢«è¦æ±‚åˆ›å»ºè¡Œä¸ºæ•°æ®é›†ä»¥è®­ç»ƒæ¨¡å‹å¹¶ç”Ÿæˆæ›´å¥½çš„äº§å“æ¨èæˆ–é¢„æµ‹å®¢æˆ·æµå¤±æ—¶ã€‚*'
- en: Choosing the right file format for our machine learning pipelines was crucial
    as it might have changed data I/O times significantly and would have definitely
    had a wider impact on our model trainer performance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæˆ‘ä»¬çš„æœºå™¨å­¦ä¹ ç®¡é“é€‰æ‹©æ­£ç¡®çš„æ–‡ä»¶æ ¼å¼è‡³å…³é‡è¦ï¼Œå› ä¸ºè¿™å¯èƒ½æ˜¾è‘—æ”¹å˜æ•°æ®çš„ I/O æ—¶é—´ï¼Œå¹¶ä¸”è‚¯å®šä¼šå¯¹æˆ‘ä»¬çš„æ¨¡å‹è®­ç»ƒæ€§èƒ½äº§ç”Ÿæ›´å¹¿æ³›çš„å½±å“ã€‚
- en: '[](https://pub.towardsai.net/supercharge-your-data-engineering-skills-with-this-machine-learning-pipeline-b69d159780b7?source=post_page-----275876dc1fc9--------------------------------)
    [## Supercharge Your Data Engineering Skills with This Machine Learning Pipeline'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pub.towardsai.net/supercharge-your-data-engineering-skills-with-this-machine-learning-pipeline-b69d159780b7?source=post_page-----275876dc1fc9--------------------------------)
    [## åˆ©ç”¨è¿™ä¸ªæœºå™¨å­¦ä¹ ç®¡é“æå‡ä½ çš„æ•°æ®å·¥ç¨‹æŠ€èƒ½'
- en: Data modeling, Python, DAGs, Big Data file formats, costsâ€¦ It covers everything
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•°æ®å»ºæ¨¡ã€Pythonã€DAGsã€å¤§æ•°æ®æ–‡ä»¶æ ¼å¼ã€æˆæœ¬â€¦â€¦åº”æœ‰å°½æœ‰ã€‚
- en: pub.towardsai.net](https://pub.towardsai.net/supercharge-your-data-engineering-skills-with-this-machine-learning-pipeline-b69d159780b7?source=post_page-----275876dc1fc9--------------------------------)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[pub.towardsai.net](https://pub.towardsai.net/supercharge-your-data-engineering-skills-with-this-machine-learning-pipeline-b69d159780b7?source=post_page-----275876dc1fc9--------------------------------)'
- en: Another thing to consider was the size of the data as we were paying already
    too much for the file storage.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªéœ€è¦è€ƒè™‘çš„å› ç´ æ˜¯æ•°æ®çš„å¤§å°ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»ä¸ºæ–‡ä»¶å­˜å‚¨æ”¯ä»˜äº†è¿‡å¤šè´¹ç”¨ã€‚
- en: This story aims to consider these important questions and other options to find
    the optimal big data file format for the data pipelines.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡æ—¨åœ¨æ¢è®¨è¿™äº›é‡è¦é—®é¢˜åŠå…¶ä»–é€‰é¡¹ï¼Œä»¥æ‰¾åˆ°æ•°æ®ç®¡é“çš„æœ€ä½³å¤§æ•°æ®æ–‡ä»¶æ ¼å¼ã€‚
- en: '*The requirement was simple, emphasized the idea of using the datalake and
    reducing the costs related to data storage in the data warehouse.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*éœ€æ±‚å¾ˆç®€å•ï¼Œå¼ºè°ƒäº†ä½¿ç”¨æ•°æ®æ¹–çš„æƒ³æ³•ï¼Œå¹¶å‡å°‘ä¸æ•°æ®ä»“åº“ä¸­æ•°æ®å­˜å‚¨ç›¸å…³çš„æˆæœ¬ã€‚*'
- en: I created an **archive** bucket that would be the cheapest storage class and
    prepared to *extract* the data from some really big tables I had in my DWH. I
    have to say, those tables were heavy and had a lot of raw events with user engagement
    data. Therefore, they were the most expensive storage part of the whole data platform.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åˆ›å»ºäº†ä¸€ä¸ª**å½’æ¡£**å­˜å‚¨æ¡¶ï¼Œè¿™æ˜¯æœ€ä¾¿å®œçš„å­˜å‚¨ç±»åˆ«ï¼Œå¹¶å‡†å¤‡*æå–*æˆ‘åœ¨ DWH ä¸­çš„ä¸€äº›éå¸¸å¤§çš„è¡¨çš„æ•°æ®ã€‚æˆ‘ä¸å¾—ä¸è¯´ï¼Œé‚£äº›è¡¨éå¸¸åºå¤§ï¼ŒåŒ…å«äº†å¤§é‡çš„ç”¨æˆ·å‚ä¸æ•°æ®åŸå§‹äº‹ä»¶ã€‚å› æ­¤ï¼Œå®ƒä»¬æ˜¯æ•´ä¸ªæ•°æ®å¹³å°ä¸­æœ€æ˜‚è´µçš„å­˜å‚¨éƒ¨åˆ†ã€‚
- en: And then I had to stop at some point to make a decision on which file format
    to use because storage size wasnâ€™t the only consideration.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä¸å¾—ä¸åœä¸‹æ¥å†³å®šä½¿ç”¨å“ªç§æ–‡ä»¶æ ¼å¼ï¼Œå› ä¸ºå­˜å‚¨å¤§å°å¹¶ä¸æ˜¯å”¯ä¸€çš„è€ƒè™‘å› ç´ ã€‚
- en: '*Indeed, in some scenarios read/write time or better schema support might be
    more important*'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*ç¡®å®ï¼Œåœ¨æŸäº›åœºæ™¯ä¸‹ï¼Œè¯»å†™æ—¶é—´æˆ–æ›´å¥½çš„æ¨¡å¼æ”¯æŒå¯èƒ½æ›´é‡è¦*'
- en: Parquet vs ORC vs AVRO vs JSON
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Parquet vs ORC vs AVRO vs JSON
- en: With the rise of Data Mesh and a considerable number of data processing tools
    available in the **Hadoop** eco-system, it might be more effective **to process
    raw event data in the data lake**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æ•°æ®ç½‘æ ¼çš„å…´èµ·ä»¥åŠ**Hadoop** ç”Ÿæ€ç³»ç»Ÿä¸­å¤§é‡æ•°æ®å¤„ç†å·¥å…·çš„å‡ºç°ï¼Œ**åœ¨æ•°æ®æ¹–ä¸­å¤„ç†åŸå§‹äº‹ä»¶æ•°æ®**å¯èƒ½æ›´æœ‰æ•ˆã€‚
- en: '*Modern data warehouse solutions are great but some* ***pricing models*** *are
    more expensive than the others and definitely more expensive than datalake tools*'
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*ç°ä»£æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆéå¸¸æ£’ï¼Œä½†æœ‰äº›* ***å®šä»·æ¨¡å‹*** *æ¯”å…¶ä»–æ¨¡å‹æ›´è´µï¼Œè€Œä¸”è‚¯å®šæ¯”æ•°æ®æ¹–å·¥å…·æ›´è´µ*'
- en: One of the **greatest benefits** of big data **file formats** is that they carry
    schema information on board. Therefore, it is much easier to load data in, split
    the data to process it more efficiently, etc. JSON canâ€™t offer that feature and
    we would want to define the schema each time we read, load or validate the data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§æ•°æ® **æ–‡ä»¶æ ¼å¼** çš„ **æœ€å¤§ä¼˜åŠ¿** ä¹‹ä¸€æ˜¯å®ƒä»¬æºå¸¦äº†æ¨¡å¼ä¿¡æ¯ã€‚å› æ­¤ï¼ŒåŠ è½½æ•°æ®ã€æ‹†åˆ†æ•°æ®ä»¥æ›´é«˜æ•ˆåœ°å¤„ç†ç­‰æ“ä½œè¦å®¹æ˜“å¾—å¤šã€‚JSON æ— æ³•æä¾›æ­¤åŠŸèƒ½ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æ¯æ¬¡è¯»å–ã€åŠ è½½æˆ–éªŒè¯æ•°æ®æ—¶å®šä¹‰æ¨¡å¼ã€‚
- en: Columnar ORC and Parquet
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ—å¼ ORC å’Œ Parquet
- en: '**Parquet** is a ***column-oriented*** data storage format designed for the
    *Apache Hadoop* ecosystem (backed by Cloudera, in collaboration with Twitter).
    It is very popular among data scientists and data engineers working with **Spark**.
    When working with huge amounts of data, you start to notice the major advantage
    of columnar data, which is achieved when a table has many more rows than columns.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**Parquet** æ˜¯ä¸€ç§ ***é¢å‘åˆ—*** çš„æ•°æ®å­˜å‚¨æ ¼å¼ï¼Œä¸“ä¸º *Apache Hadoop* ç”Ÿæ€ç³»ç»Ÿè®¾è®¡ï¼ˆç”± Cloudera æ”¯æŒï¼Œä¸
    Twitter åˆä½œï¼‰ã€‚å®ƒåœ¨ä½¿ç”¨**Spark**çš„æ•°æ®æ˜¾ç¤ºç§‘å­¦å®¶å’Œæ•°æ®å·¥ç¨‹å¸ˆä¸­éå¸¸å—æ¬¢è¿ã€‚å½“å¤„ç†å¤§é‡æ•°æ®æ—¶ï¼Œä½ ä¼šå¼€å§‹æ³¨æ„åˆ°åˆ—å¼æ•°æ®çš„ä¸»è¦ä¼˜åŠ¿ï¼Œè¿™æ˜¯åœ¨è¡¨ä¸­è¡Œæ•°è¿œå¤šäºåˆ—æ•°æ—¶å®ç°çš„ã€‚'
- en: '![](../Images/45fc6ec974c1bc982924ea6d8d9a1976.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45fc6ec974c1bc982924ea6d8d9a1976.png)'
- en: 'Parquet File Layout. Source: [apache.org](http://apache.org) (Apache 2.0 license)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet æ–‡ä»¶å¸ƒå±€ã€‚æ¥æºï¼š[apache.org](http://apache.org)ï¼ˆApache 2.0 è®¸å¯è¯ï¼‰
- en: '*Spark scales well and thatâ€™s why everybody likes it*'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Spark æ‰©å±•æ€§å¾ˆå¥½ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆå¤§å®¶å–œæ¬¢å®ƒ*'
- en: In fact, Parquet is a default data file format for Spark. **Parquet** performs
    beautifully while querying and working with analytical workloads.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼ŒParquet æ˜¯ Spark çš„é»˜è®¤æ•°æ®æ–‡ä»¶æ ¼å¼ã€‚**Parquet** åœ¨æŸ¥è¯¢å’Œå¤„ç†åˆ†æå·¥ä½œè´Ÿè½½æ—¶è¡¨ç°å‡ºè‰²ã€‚
- en: '*Columnar formats are more suitable for OLAP analytical queries. Specifically,
    we would want to retrieve only the column we need to perform an analytical query.
    That definitely has certain implications on memory and therefore, on performance
    and speed.*'
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*åˆ—å¼æ ¼å¼æ›´é€‚åˆ OLAP åˆ†ææŸ¥è¯¢ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¸Œæœ›ä»…æ£€ç´¢æ‰§è¡Œåˆ†ææŸ¥è¯¢æ‰€éœ€çš„åˆ—ã€‚è¿™å¯¹å†…å­˜æœ‰ä¸€å®šçš„å½±å“ï¼Œå› æ­¤å¯¹æ€§èƒ½å’Œé€Ÿåº¦ä¹Ÿæœ‰å½±å“ã€‚*'
- en: '**ORC (Optimised Row Columnar)** is also a column-oriented data storage format
    similar to Parquet which carries a schema on board. it means that like Parquet
    it is **self-describing** and we can use it to load data into different disks
    or nodes.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**ORCï¼ˆä¼˜åŒ–è¡Œåˆ—å¼ï¼‰** ä¹Ÿæ˜¯ä¸€ç§åˆ—å¼æ•°æ®å­˜å‚¨æ ¼å¼ï¼Œä¸ Parquet ç±»ä¼¼ï¼Œå¹¶ä¸”æºå¸¦äº†æ¨¡å¼ä¿¡æ¯ã€‚è¿™æ„å‘³ç€åƒ Parquet ä¸€æ ·ï¼Œå®ƒæ˜¯**è‡ªæè¿°**çš„ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒå°†æ•°æ®åŠ è½½åˆ°ä¸åŒçš„ç£ç›˜æˆ–èŠ‚ç‚¹ä¸­ã€‚'
- en: '![](../Images/b7301da75334cf83797ce798485045a3.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7301da75334cf83797ce798485045a3.png)'
- en: 'ORC file layout. Source: [apache.org](http://apache.org) (Apache 2.0 license)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ORC æ–‡ä»¶å¸ƒå±€ã€‚æ¥æºï¼š[apache.org](http://apache.org)ï¼ˆApache 2.0 è®¸å¯è¯ï¼‰
- en: I did a little test and it seems that both **Parquet and ORC** offer similar
    compression ratios. However, there is an opinion that ORC is more compression
    efficient.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åšäº†ä¸€ä¸ªå°æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤º **Parquet å’Œ ORC** æä¾›äº†ç±»ä¼¼çš„å‹ç¼©æ¯”ã€‚ç„¶è€Œï¼Œæœ‰ä¸€ç§è§‚ç‚¹è®¤ä¸º ORC çš„å‹ç¼©æ•ˆç‡æ›´é«˜ã€‚
- en: '*10 Mb compressed with* ***SNAPPY*** *algorithm will turn into 2.4Mb in* ***Parquet***'
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*10 Mb ä½¿ç”¨* ***SNAPPY*** *ç®—æ³•å‹ç¼©åä¼šå˜æˆ 2.4Mb çš„* ***Parquet***'
- en: However, I have a feeling that **ORC** is supported by a smaller number of Hadoop
    projects than Parquet, i.e. Hive and Pig.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿‡ï¼Œæˆ‘æ„Ÿè§‰**ORC**çš„æ”¯æŒé¡¹ç›®æ•°é‡å°‘äº Parquetï¼Œä¾‹å¦‚ Hive å’Œ Pigã€‚
- en: So if we want a wider range of tools to run our OLAP analytics in the data lake
    I would recommend using **Parquet**. It has wider project support and especially
    **Spark**.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å¸Œæœ›åœ¨æ•°æ®æ¹–ä¸­è¿è¡Œæ›´å¹¿æ³›çš„ OLAP åˆ†æå·¥å…·ï¼Œæˆ‘å»ºè®®ä½¿ç”¨**Parquet**ã€‚å®ƒæœ‰æ›´å¹¿æ³›çš„é¡¹ç›®æ”¯æŒï¼Œå°¤å…¶æ˜¯**Spark**ã€‚
- en: '*In reality, PARQUET and ORC have somewhat different columnar architecture.*'
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*å®é™…ä¸Šï¼ŒPARQUET å’Œ ORC çš„åˆ—å¼æ¶æ„æœ‰æ‰€ä¸åŒã€‚*'
- en: Both work really well saving you a great deal of disk space.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤è€…éƒ½éå¸¸æœ‰æ•ˆåœ°èŠ‚çœäº†å¤§é‡çš„ç£ç›˜ç©ºé—´ã€‚
- en: However, data in **ORC** files are organized into independent *stripes* of data.
    Each has its own separate index, row data, and footer. It enables large, efficient
    reads from HDFS, making this format.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿‡ï¼Œ**ORC** æ–‡ä»¶ä¸­çš„æ•°æ®ç»„ç»‡æˆç‹¬ç«‹çš„*æ¡å¸¦*ã€‚æ¯ä¸ªæ¡å¸¦éƒ½æœ‰è‡ªå·±ç‹¬ç«‹çš„ç´¢å¼•ã€è¡Œæ•°æ®å’Œé¡µè„šã€‚è¿™ä½¿å¾—ä» HDFS è¿›è¡Œå¤§è§„æ¨¡é«˜æ•ˆè¯»å–æˆä¸ºå¯èƒ½ã€‚
- en: The data is stored as pages in **Parquet**, and each page includes header information,
    definition level information, and repetition level information.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®ä»¥é¡µé¢å½¢å¼å­˜å‚¨åœ¨**Parquet**ä¸­ï¼Œæ¯é¡µåŒ…å«å¤´éƒ¨ä¿¡æ¯ã€å®šä¹‰çº§åˆ«ä¿¡æ¯å’Œé‡å¤çº§åˆ«ä¿¡æ¯ã€‚
- en: It is very effective when it comes to supporting a complicated **nested** data
    structure and seems to be more efficient at performing IO-type operations on data.
    The great advantage here is that **read time** can be significantly decreased
    if we choose only the columns we need.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ”¯æŒå¤æ‚çš„**åµŒå¥—**æ•°æ®ç»“æ„æ—¶ï¼Œå®ƒéå¸¸æœ‰æ•ˆï¼Œå¹¶ä¸”åœ¨æ‰§è¡Œ IO ç±»å‹çš„æ•°æ®æ“ä½œæ—¶ä¼¼ä¹æ›´é«˜æ•ˆã€‚è¿™é‡Œçš„ä¸€ä¸ªå·¨å¤§ä¼˜åŠ¿æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬åªé€‰æ‹©æ‰€éœ€çš„åˆ—ï¼Œ**è¯»å–æ—¶é—´**å¯ä»¥æ˜¾è‘—å‡å°‘ã€‚
- en: My personal experience suggests that selecting just a few columns canâ€¦ read
    the data up to 30 times (!) faster than reading the same file with the complete
    schema.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„ä¸ªäººç»éªŒè¡¨æ˜ï¼Œåªé€‰æ‹©å‡ åˆ—å¯ä»¥â€¦å°†æ•°æ®è¯»å–é€Ÿåº¦æé«˜è‡³ 30 å€ï¼ˆï¼ï¼‰ï¼Œæ¯”è¯»å–åŒ…å«å®Œæ•´æ¨¡å¼çš„ç›¸åŒæ–‡ä»¶å¿«ã€‚
- en: Read the data 30 times faster (!)
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¯»å–æ•°æ®é€Ÿåº¦å¿« 30 å€ï¼ˆï¼ï¼‰
- en: Not bad, hah? Other tests performed by data engineers confirmed that too. Iâ€™ll
    add that link at the bottom.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸é”™å§ï¼Ÿæ•°æ®å·¥ç¨‹å¸ˆè¿›è¡Œçš„å…¶ä»–æµ‹è¯•ä¹Ÿè¯å®äº†è¿™ä¸€ç‚¹ã€‚æˆ‘ä¼šåœ¨åº•éƒ¨æ·»åŠ é‚£ä¸ªé“¾æ¥ã€‚
- en: '*Long story short, choose ORC if you work on HIVE. It is better optimized for
    it. And Choose Parquet if you work with Spark as that would be the default storage
    format for it.*'
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*é•¿è¯çŸ­è¯´ï¼Œå¦‚æœä½ åœ¨ HIVE ä¸Šå·¥ä½œï¼Œé€‰æ‹© ORC æ›´ä½³ã€‚å®ƒå¯¹ HIVE æ›´ä¼˜åŒ–ã€‚è€Œé€‰æ‹© Parquet å¦‚æœä½ ä¸ Spark å·¥ä½œï¼Œå› ä¸ºè¿™å°†æ˜¯ Spark
    çš„é»˜è®¤å­˜å‚¨æ ¼å¼ã€‚*'
- en: In everything else, both ORC and Parquet share similar architecture and KPIs
    look roughly the same.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å…¶ä»–æ–¹é¢ï¼ŒORC å’Œ Parquet çš„æ¶æ„ç›¸ä¼¼ï¼ŒKPIs çœ‹èµ·æ¥å¤§è‡´ç›¸åŒã€‚
- en: AVRO
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AVRO
- en: '**AVRO** is a **row-based** storage format where data is indexed to improve
    query performance.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**AVRO** æ˜¯ä¸€ç§**åŸºäºè¡Œ**çš„å­˜å‚¨æ ¼å¼ï¼Œæ•°æ®ç»è¿‡ç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½ã€‚'
- en: It defines data types and schemas using JSON data and stores the data in a binary
    format (condensed) that help with disk space.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä½¿ç”¨ JSON æ•°æ®å®šä¹‰æ•°æ®ç±»å‹å’Œæ¨¡å¼ï¼Œå¹¶ä»¥äºŒè¿›åˆ¶æ ¼å¼ï¼ˆå‹ç¼©ï¼‰å­˜å‚¨æ•°æ®ï¼Œä»¥å¸®åŠ©èŠ‚çœç£ç›˜ç©ºé—´ã€‚
- en: '![](../Images/6535c558a34c0ed7ca5047e18c4ffd04.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6535c558a34c0ed7ca5047e18c4ffd04.png)'
- en: 'avro file structure. Source: [apache.org](http://apache.org) (Apache 2.0 license)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: avro æ–‡ä»¶ç»“æ„ã€‚æ¥æºï¼š[apache.org](http://apache.org)ï¼ˆApache 2.0 è®¸å¯è¯ï¼‰
- en: Compared to Parquet and ORC it seems that it offers less efficient compression
    but faster **write** speeds.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ Parquet å’Œ ORC ç›¸æ¯”ï¼ŒAVRO ä¼¼ä¹æä¾›äº†è¾ƒä½æ•ˆçš„å‹ç¼©ä½†æ›´å¿«çš„**å†™å…¥**é€Ÿåº¦ã€‚
- en: '*10 Mb* ***Parquet*** *compressed with* ***SNAPPY*** *algorithm will turn into
    2.4Mb*'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*10 Mb* ***Parquet*** *ç”¨* ***SNAPPY*** *ç®—æ³•å‹ç¼©åå°†å˜ä¸º 2.4Mb*'
- en: '*10 Mb* ***AVRO*** *compressed with* ***SNAPPY*** *algorithm will turn into
    6.2Mb*'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*10 Mb* ***AVRO*** *ç”¨* ***SNAPPY*** *ç®—æ³•å‹ç¼©åå°†å˜ä¸º 6.2Mb*'
- en: '*The same 10 Mb* ***AVRO*** *compressed with* ***DEFLATE*** *algorithm will
    require 4Mb of storage*'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ç›¸åŒçš„ 10 Mb* ***AVRO*** *ç”¨* ***DEFLATE*** *ç®—æ³•å‹ç¼©åå°†éœ€è¦ 4Mb å­˜å‚¨ç©ºé—´*'
- en: The main advantage of AVRO is a **schema-evolution support**. In other words,
    as data schemas evolve over time, ***AVRO*** enables those changes by tolerating
    fields that have been added, removed, or altered.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: AVRO çš„ä¸»è¦ä¼˜åŠ¿æ˜¯**æ¨¡å¼æ¼”è¿›æ”¯æŒ**ã€‚æ¢å¥è¯è¯´ï¼Œéšç€æ•°æ®æ¨¡å¼çš„æ¼”å˜ï¼Œ***AVRO*** é€šè¿‡å®¹å¿æ·»åŠ ã€åˆ é™¤æˆ–æ›´æ”¹çš„å­—æ®µæ¥æ”¯æŒè¿™äº›å˜åŒ–ã€‚
- en: '*AVRO works really well with things like added fields and changed fields, missing
    fields.*'
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*AVRO åœ¨å¤„ç†å¦‚æ–°å¢å­—æ®µã€å˜æ›´å­—æ®µå’Œç¼ºå¤±å­—æ®µæ—¶è¡¨ç°éå¸¸å¥½ã€‚*'
- en: AVRO is more efficient while working with ***write-intensive***, big-data operations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: AVRO åœ¨å¤„ç†***å†™å…¥å¯†é›†å‹***å¤§æ•°æ®æ“ä½œæ—¶æ›´é«˜æ•ˆã€‚
- en: It is a great candidate to store data in `source` layer or a `landing` area
    of our data platform because these *f****iles are usually being read as a whole***
    for further transformation depending on the data pipeline. Any schema evolution
    changes are handled with ease.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæ˜¯å­˜å‚¨æ•°æ®äºæ•°æ®å¹³å°çš„`source`å±‚æˆ–`landing`åŒºåŸŸçš„ä¸€ä¸ªä¼˜ç§€é€‰æ‹©ï¼Œå› ä¸ºè¿™äº›*æ–‡ä»¶é€šå¸¸ä¼šè¢«ä½œä¸ºæ•´ä½“è¯»å–*ä»¥è¿›è¡Œè¿›ä¸€æ­¥è½¬æ¢ï¼Œå…·ä½“å–å†³äºæ•°æ®ç®¡é“ã€‚ä»»ä½•æ¨¡å¼æ¼”å˜æ›´æ”¹éƒ½èƒ½è½»æ¾å¤„ç†ã€‚
- en: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----275876dc1fc9--------------------------------)
    [## Data pipeline design patterns'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----275876dc1fc9--------------------------------)
    [## æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼'
- en: Choosing the right architecture with examples
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€‰æ‹©åˆé€‚çš„æ¶æ„å¹¶é™„å¸¦ç¤ºä¾‹
- en: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----275876dc1fc9--------------------------------)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----275876dc1fc9--------------------------------)
- en: Even applications written in different languages can share data saved using
    AVRO. Data exchange services might require a **coding generator** to decipher
    data specifications and generate code to access data. AVRO is an ideal contender
    for scripting languages because it [doesnâ€™t require this](https://avro.apache.org/docs/1.11.1/getting-started-python/#serializing-and-deserializing-without-code-generation).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿æ˜¯ç”¨ä¸åŒè¯­è¨€ç¼–å†™çš„åº”ç”¨ç¨‹åºä¹Ÿå¯ä»¥å…±äº«ä½¿ç”¨AVROä¿å­˜çš„æ•°æ®ã€‚æ•°æ®äº¤æ¢æœåŠ¡å¯èƒ½éœ€è¦ä¸€ä¸ª**ç¼–ç ç”Ÿæˆå™¨**æ¥è§£è¯»æ•°æ®è§„æ ¼å¹¶ç”Ÿæˆè®¿é—®æ•°æ®çš„ä»£ç ã€‚AVROæ˜¯è„šæœ¬è¯­è¨€çš„ç†æƒ³é€‰æ‹©ï¼Œå› ä¸ºå®ƒ[ä¸éœ€è¦è¿™ä¸ª](https://avro.apache.org/docs/1.11.1/getting-started-python/#serializing-and-deserializing-without-code-generation)ã€‚
- en: JSON
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JSON
- en: For online applications, structured data is frequently serialized and sent using
    the JavaScript Object Notation (JSON) format.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåœ¨çº¿åº”ç”¨ç¨‹åºï¼Œç»“æ„åŒ–æ•°æ®é€šå¸¸ä»¥JavaScriptå¯¹è±¡è¡¨ç¤ºæ³•ï¼ˆJSONï¼‰æ ¼å¼è¿›è¡Œåºåˆ—åŒ–å’Œå‘é€ã€‚
- en: This implies that a significant portion of the big data is gathered and kept
    in a JSON format.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€å¤§éƒ¨åˆ†å¤§æ•°æ®ä»¥JSONæ ¼å¼æ”¶é›†å’Œå­˜å‚¨ã€‚
- en: However, because JSON is not highly typed nor schema-enriched, dealing with
    JSON files in big data technologies like Hadoop may be sluggish.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œç”±äºJSONæ—¢ä¸é«˜åº¦ç±»å‹åŒ–ä¹Ÿæ²¡æœ‰ä¸°å¯Œçš„æ¨¡å¼ï¼Œå¤„ç†å¤§æ•°æ®æŠ€æœ¯å¦‚Hadoopä¸­çš„JSONæ–‡ä»¶å¯èƒ½ä¼šå¾ˆç¼“æ…¢ã€‚
- en: Basically, itâ€™s a no-go for Big Data processing frameworks. This is the main
    reason Parquet and ORC formats were created.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬ä¸Šï¼Œå®ƒå¯¹å¤§æ•°æ®å¤„ç†æ¡†æ¶æ¥è¯´æ˜¯ä¸å¯è¡Œçš„ã€‚è¿™ä¹Ÿæ˜¯Parquetå’ŒORCæ ¼å¼è¢«åˆ›å»ºçš„ä¸»è¦åŸå› ã€‚
- en: Compression
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‹ç¼©
- en: '*How does it work?*'
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ*'
- en: A quick answer is to choose **splittable** compression types if our primary
    objective is to gain data processing performance, and non-splittable like **GZIP,
    DEFLATE** if storage costs deduction is our main objective.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯æé«˜æ•°æ®å¤„ç†æ€§èƒ½ï¼Œé‚£ä¹ˆé€‰æ‹©**å¯æ‹†åˆ†**å‹ç¼©ç±»å‹æ˜¯å¿«é€Ÿç­”æ¡ˆï¼›å¦‚æœå­˜å‚¨æˆæœ¬å‡å°‘æ˜¯æˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡ï¼Œé‚£ä¹ˆé€‰æ‹©éå¯æ‹†åˆ†çš„å¦‚**GZIP**å’Œ**DEFLATE**ã€‚
- en: '*What is splittable?*'
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*ä»€ä¹ˆæ˜¯å¯æ‹†åˆ†çš„ï¼Ÿ*'
- en: '***Splittable*** means that Hadoop Mapper can split or divide a large file
    and process it in parallel. Compressing codecs apply compression in blocks and
    when it is applied at the block level mappers can read a single file concurrently
    even if it is a very large file.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '***å¯æ‹†åˆ†***æ„å‘³ç€Hadoop Mapperå¯ä»¥æ‹†åˆ†æˆ–åˆ’åˆ†å¤§å‹æ–‡ä»¶å¹¶å¹¶è¡Œå¤„ç†å®ƒã€‚å‹ç¼©ç¼–è§£ç å™¨åœ¨å—çº§åˆ«åº”ç”¨å‹ç¼©ï¼Œå½“å®ƒåœ¨å—çº§åˆ«åº”ç”¨æ—¶ï¼Œå³ä½¿æ˜¯éå¸¸å¤§çš„æ–‡ä»¶ï¼Œæ˜ å°„å™¨ä¹Ÿå¯ä»¥åŒæ—¶è¯»å–å•ä¸ªæ–‡ä»¶ã€‚'
- en: Having said that, *splittable* compression types are **LZO**, **LZ4**, **BZIP2**
    and **SNAPPY** whereas **GZIP** isnâ€™t. These are very fast compressors and very
    fast decompressors. So if we can tolerate bigger storage then it is a better choice
    than **GZIP**.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è¯è™½å¦‚æ­¤ï¼Œ*å¯æ‹†åˆ†çš„*å‹ç¼©ç±»å‹æœ‰**LZO**ã€**LZ4**ã€**BZIP2**å’Œ**SNAPPY**ï¼Œè€Œ**GZIP**ä¸æ˜¯ã€‚è¿™äº›å‹ç¼©å™¨å’Œè§£å‹å™¨éå¸¸å¿«ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬å¯ä»¥å®¹å¿è¾ƒå¤§çš„å­˜å‚¨ï¼Œé‚£ä¹ˆå®ƒæ¯”**GZIP**æ›´å¥½çš„é€‰æ‹©ã€‚
- en: '**GZIP** on average has a 30% better compression rate and would be a good choice
    for data that doesnâ€™t require frequent access (cold storage).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**GZIP**å¹³å‡å…·æœ‰30%çš„æ›´å¥½å‹ç¼©ç‡ï¼Œå¯¹äºä¸éœ€è¦é¢‘ç¹è®¿é—®ï¼ˆå†·å­˜å‚¨ï¼‰çš„æ•°æ®æ¥è¯´æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚'
- en: External data in lakes and Data warehouse
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤–éƒ¨æ•°æ®åœ¨æ•°æ®æ¹–å’Œæ•°æ®ä»“åº“ä¸­
- en: '*Can we query external data storage using a data warehouse?*'
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ•°æ®ä»“åº“æŸ¥è¯¢å¤–éƒ¨æ•°æ®å­˜å‚¨å—ï¼Ÿ*'
- en: Typically, all modern data warehouses offer externally partitioned table features
    so we can run analytics on data lake data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œæ‰€æœ‰ç°ä»£æ•°æ®ä»“åº“éƒ½æä¾›å¤–éƒ¨åˆ†åŒºè¡¨åŠŸèƒ½ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å¯¹æ•°æ®æ¹–ä¸­çš„æ•°æ®è¿›è¡Œåˆ†æã€‚
- en: There is a number of limitation of course but in general, it is enough to connect
    two worlds in order to understand how it works.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶æœ‰ä¸€äº›é™åˆ¶ï¼Œä½†ä¸€èˆ¬æ¥è¯´ï¼Œå®ƒè¶³å¤Ÿå°†ä¸¤ä¸ªä¸–ç•Œè¿æ¥èµ·æ¥ï¼Œä»¥ä¾¿ç†è§£å®ƒçš„å·¥ä½œåŸç†ã€‚
- en: 'For example, in BigQuery we can create an external table in our data warehouse
    like so:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œåœ¨BigQueryä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·åœ¨æ•°æ®ä»“åº“ä¸­åˆ›å»ºä¸€ä¸ªå¤–éƒ¨è¡¨ï¼š
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You will notice that external tables are much slower in any DWH and are limited
    in many ways, i.e. limited concurrent queries, no DML and wildcard support and
    inconsistency if underlying data was changed during the query run.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¼šæ³¨æ„åˆ°ï¼Œåœ¨ä»»ä½•æ•°æ®ä»“åº“ä¸­ï¼Œå¤–éƒ¨è¡¨çš„é€Ÿåº¦éƒ½è¾ƒæ…¢ï¼Œå¹¶ä¸”åœ¨è®¸å¤šæ–¹é¢å—é™ï¼Œä¾‹å¦‚å¹¶å‘æŸ¥è¯¢æ•°é‡æœ‰é™ã€ä¸æ”¯æŒæ•°æ®æ“ä½œè¯­è¨€ï¼ˆDMLï¼‰å’Œé€šé…ç¬¦æ”¯æŒï¼Œå¹¶ä¸”å¦‚æœåœ¨æŸ¥è¯¢è¿è¡ŒæœŸé—´åº•å±‚æ•°æ®å‘ç”Ÿå˜åŒ–ï¼Œä¼šå‡ºç°ä¸ä¸€è‡´çš„é—®é¢˜ã€‚
- en: 'So we might want to *load* it back like so:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å¯èƒ½æƒ³è¦åƒè¿™æ ·*åŠ è½½*å®ƒï¼š
- en: '[PRE1]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Native tables are faster.
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æœ¬åœ°è¡¨æ›´å¿«ã€‚
- en: If there is another data processing tool, then we might want to use a Hive partition
    layout. This is usually required to enable external partitioning. i.e.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæœ‰å…¶ä»–æ•°æ®å¤„ç†å·¥å…·ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦ä½¿ç”¨ Hive åˆ†åŒºå¸ƒå±€ã€‚è¿™é€šå¸¸æ˜¯ä¸ºäº†å¯ç”¨å¤–éƒ¨åˆ†åŒºã€‚ä¾‹å¦‚ï¼š
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here partition keys have to be always in the same order. And for partitioning,
    we will use `key = value` pairs which will be storage folders at the same time.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œåˆ†åŒºé”®å¿…é¡»å§‹ç»ˆä¿æŒç›¸åŒçš„é¡ºåºã€‚å¯¹äºåˆ†åŒºï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `key = value` å¯¹ï¼Œè¿™äº›å¯¹ä¹Ÿå°†ä½œä¸ºå­˜å‚¨æ–‡ä»¶å¤¹ã€‚
- en: We can use it in any other data processing framework that supports *HIVE* layout.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åœ¨ä»»ä½•å…¶ä»–æ”¯æŒ *HIVE* å¸ƒå±€çš„æ•°æ®å¤„ç†æ¡†æ¶ä¸­ä½¿ç”¨å®ƒã€‚
- en: '[](https://mydataschool.com/blog/hive-partitioning-layout/?source=post_page-----275876dc1fc9--------------------------------)
    [## Hive Partitioning Layout'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mydataschool.com/blog/hive-partitioning-layout/?source=post_page-----275876dc1fc9--------------------------------)
    [## Hive åˆ†åŒºå¸ƒå±€'
- en: Lakehouse design is one of my favorites because it gives us the best of two
    worlds. In a data warehouse, we can manageâ€¦
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Lakehouse è®¾è®¡æ˜¯æˆ‘æœ€å–œæ¬¢çš„è®¾è®¡ä¹‹ä¸€ï¼Œå› ä¸ºå®ƒèåˆäº†ä¸¤è€…çš„ä¼˜ç‚¹ã€‚åœ¨æ•°æ®ä»“åº“ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç®¡ç†â€¦
- en: mydataschool.com](https://mydataschool.com/blog/hive-partitioning-layout/?source=post_page-----275876dc1fc9--------------------------------)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: mydataschool.com](https://mydataschool.com/blog/hive-partitioning-layout/?source=post_page-----275876dc1fc9--------------------------------)
- en: Conclusion
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: If we need support for Spark and a flat columnar format with efficient querying
    of complex nested data structures then we should use Parquet. It is highly integrated
    with Apache Spark and is, actually, a default file format for this framework.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬éœ€è¦å¯¹å¤æ‚åµŒå¥—æ•°æ®ç»“æ„è¿›è¡Œé«˜æ•ˆæŸ¥è¯¢çš„å¹³é¢åˆ—å¼æ ¼å¼æ”¯æŒï¼Œé‚£ä¹ˆæˆ‘ä»¬åº”è¯¥ä½¿ç”¨ Parquetã€‚å®ƒä¸ Apache Spark é«˜åº¦é›†æˆï¼Œå¹¶ä¸”å®é™…ä¸Šæ˜¯è¯¥æ¡†æ¶çš„é»˜è®¤æ–‡ä»¶æ ¼å¼ã€‚
- en: If our data platform architecture relies on data pipelines built with Hive or
    Pig then ORC data format is the better choice. Having all the advantages of columnar
    format it performs beautifully in analytical data lake queries but is better integrated
    with Hive and Pig. Therefore, the performance might be better compared to Parquet.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬çš„æ•°æ®å¹³å°æ¶æ„ä¾èµ–äºä½¿ç”¨ Hive æˆ– Pig æ„å»ºçš„æ•°æ®ç®¡é“ï¼Œé‚£ä¹ˆ ORC æ•°æ®æ ¼å¼æ˜¯æ›´å¥½çš„é€‰æ‹©ã€‚å®ƒå…·å¤‡åˆ—å¼æ ¼å¼çš„æ‰€æœ‰ä¼˜ç‚¹ï¼Œåœ¨åˆ†ææ•°æ®æ¹–æŸ¥è¯¢ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä¸
    Hive å’Œ Pig æ›´å¥½åœ°é›†æˆã€‚å› æ­¤ï¼Œä¸ Parquet ç›¸æ¯”ï¼Œå®ƒçš„æ€§èƒ½å¯èƒ½æ›´å¥½ã€‚
- en: When we need a higher compression ratio and faster read times then Parquet or
    ORC would suit better. Compared to AVRO read times might be up to 3 times faster.
    It can be improved even further if we reduce the number of selected columns to
    read.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬éœ€è¦æ›´é«˜çš„å‹ç¼©æ¯”å’Œæ›´å¿«çš„è¯»å–æ—¶é—´æ—¶ï¼ŒParquet æˆ– ORC ä¼šæ›´åˆé€‚ã€‚ä¸ AVRO ç›¸æ¯”ï¼Œè¯»å–æ—¶é—´å¯èƒ½å¿«å¤šè¾¾ 3 å€ã€‚å¦‚æœæˆ‘ä»¬å‡å°‘é€‰æ‹©è¯»å–çš„åˆ—æ•°ï¼Œè¿˜å¯ä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚
- en: However, we donâ€™t worry to much about the speed of input operations AVRO might
    be a better choice offering a reasonable compression with splittable SNAPPY, schema
    evolution support and way faster write speed (it is row-based remember?). AVRO
    is an absolute leader in *write-intensive*, big-data operations. Therefore, it
    suits better for storing the data in the source layer of our data platform.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæˆ‘ä»¬ä¸å¤ªæ‹…å¿ƒè¾“å…¥æ“ä½œçš„é€Ÿåº¦ï¼ŒAVRO å¯èƒ½æ˜¯æ›´å¥½çš„é€‰æ‹©ï¼Œå®ƒæä¾›äº†åˆç†çš„å‹ç¼©ï¼ˆä½¿ç”¨å¯æ‹†åˆ†çš„SNAPPYï¼‰ã€æ¨¡å¼æ¼”å˜æ”¯æŒï¼Œå¹¶ä¸”å†™å…¥é€Ÿåº¦æ¯”å…¶ä»–æ ¼å¼å¿«å¾—å¤šï¼ˆè®°ä½ï¼Œå®ƒæ˜¯åŸºäºè¡Œçš„ï¼‰ã€‚AVRO
    åœ¨*å†™å…¥å¯†é›†å‹*çš„å¤§æ•°æ®æ“ä½œä¸­ç»å¯¹é¢†å…ˆã€‚å› æ­¤ï¼Œå®ƒæ›´é€‚åˆç”¨äºæˆ‘ä»¬æ•°æ®å¹³å°æºå±‚çš„æ•°æ®å­˜å‚¨ã€‚
- en: '`GZIP`, `DEFLATE` and other non-splittable compression types would work best
    for cold storage where frequent access to data is not required.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`GZIP`ã€`DEFLATE` å’Œå…¶ä»–ä¸å¯æ‹†åˆ†çš„å‹ç¼©ç±»å‹åœ¨å†·å­˜å‚¨ä¸­æ•ˆæœæœ€å¥½ï¼Œè¿™é‡Œä¸éœ€è¦é¢‘ç¹è®¿é—®æ•°æ®ã€‚'
- en: Recommended read
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨èé˜…è¯»
- en: '[1\. https://cwiki.apache.org/confluence/display/hive/languagemanual+orc](https://cwiki.apache.org/confluence/display/hive/languagemanual+orc)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[1\. https://cwiki.apache.org/confluence/display/hive/languagemanual+orc](https://cwiki.apache.org/confluence/display/hive/languagemanual+orc)'
- en: '[2\. https://parquet.apache.org/](https://parquet.apache.org/)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[2\. https://parquet.apache.org/](https://parquet.apache.org/)'
- en: '[3\. https://avro.apache.org/](https://avro.apache.org/)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[3\. https://avro.apache.org/](https://avro.apache.org/)'
- en: '[4\. https://avro.apache.org/docs/1.11.1/getting-started-python/#serializing-and-deserializing-without-code-generation](https://avro.apache.org/docs/1.11.1/getting-started-python/#serializing-and-deserializing-without-code-generation)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[4\. https://avro.apache.org/docs/1.11.1/getting-started-python/#serializing-and-deserializing-without-code-generation](https://avro.apache.org/docs/1.11.1/getting-started-python/#serializing-and-deserializing-without-code-generation)'
- en: '[5\. https://avro.apache.org/docs/1.11.1/specification/](https://avro.apache.org/docs/1.11.1/specification/)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[5\. https://avro.apache.org/docs/1.11.1/specification/](https://avro.apache.org/docs/1.11.1/specification/)'
- en: '[6\. https://stackoverflow.com/questions/14820450/best-splittable-compression-for-hadoop-input-bz2](https://stackoverflow.com/questions/14820450/best-splittable-compression-for-hadoop-input-bz2)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[6\. https://stackoverflow.com/questions/14820450/best-splittable-compression-for-hadoop-input-bz2](https://stackoverflow.com/questions/14820450/best-splittable-compression-for-hadoop-input-bz2)'
- en: '[7\. https://stackoverflow.com/questions/35789412/spark-sql-difference-between-gzip-vs-snappy-vs-lzo-compression-formats](https://stackoverflow.com/questions/35789412/spark-sql-difference-between-gzip-vs-snappy-vs-lzo-compression-formats)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[7\. https://stackoverflow.com/questions/35789412/spark-sql-difference-between-gzip-vs-snappy-vs-lzo-compression-formats](https://stackoverflow.com/questions/35789412/spark-sql-difference-between-gzip-vs-snappy-vs-lzo-compression-formats)'
- en: 8\. [https://medium.com/ssense-tech/csv-vs-parquet-vs-avro-choosing-the-right-tool-for-the-right-job-79c9f56914a8](https://medium.com/ssense-tech/csv-vs-parquet-vs-avro-choosing-the-right-tool-for-the-right-job-79c9f56914a8)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://medium.com/ssense-tech/csv-vs-parquet-vs-avro-choosing-the-right-tool-for-the-right-job-79c9f56914a8](https://medium.com/ssense-tech/csv-vs-parquet-vs-avro-choosing-the-right-tool-for-the-right-job-79c9f56914a8)'
