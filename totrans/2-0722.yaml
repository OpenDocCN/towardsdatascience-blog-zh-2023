- en: Deploying a TFLite Model on GCP Serverless
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/deploying-tflite-model-on-gcp-serverless-b4cd84f86de1](https://towardsdatascience.com/deploying-tflite-model-on-gcp-serverless-b4cd84f86de1)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to deploy a quantized model in a Serverless fashion
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://vishal-ai.medium.com/?source=post_page-----b4cd84f86de1--------------------------------)[![Vishal
    Rajput](../Images/c43407d7df1f099832cbaa5381a0bb74.png)](https://vishal-ai.medium.com/?source=post_page-----b4cd84f86de1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b4cd84f86de1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b4cd84f86de1--------------------------------)
    [Vishal Rajput](https://vishal-ai.medium.com/?source=post_page-----b4cd84f86de1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b4cd84f86de1--------------------------------)
    ·11 min read·Jul 21, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Model deployment is tricky; with the continuously changing landscape of cloud
    platforms and other AI-related libraries updating almost weekly, back compatibility
    and finding the correct deployment method is a big challenge. In today’s blog
    post, we will see how to deploy a **tflite model** on the **Google Cloud Platform**
    in a **serverless** fashion.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'This blog post is structured in the following way:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Serverless and other ways of Deployment
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Quantization and TFLite?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying TFLite model using GCP Cloud Run API
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/c1719de604d34543f57af5f33b0bf5ce.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: 'Img Src: [https://pixabay.com/photos/man-pier-silhouette-sunrise-fog-8091933/](https://pixabay.com/photos/man-pier-silhouette-sunrise-fog-8091933/)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Serverless and other ways of Deployment
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Let’s first understand what do we mean by serverless because serverless doesn’t
    mean without a server.**'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An AI model, or any application for that matter can be deployed in several different
    ways with three major categorisations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '**Serverless:** In this case, the model is stored on the cloud container registry
    and only runs when a user makes a request. When a request is made, a server instance
    is automatically launched to fulfill the user request, which shuts down after
    a while. From starting, configuring, scaling, and shutting down, all of this is
    taken by the Cloud Run API provided by the Google Cloud platform. We have AWS
    Lambda and Azure Functions as alternatives in other clouds.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '**Serverless** has its own advantages and disadvantages.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: The biggest advantage is the **cost-saving**, if you don’t have a large user
    base, most of the time, the server is sitting idle, and your money is just going
    for no reason. Another advantage is that we don’t need to think about **scaling**
    the infrastructure, depending upon the load on the server, it can automatically
    replicate the number of instances and handle the traffic.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the disadvantage column, there are three things to consider. It has a **small
    payload limit**, meaning it can be used to run a bigger model. Secondly, the server
    automatically shuts down after 15 min of idle time, thus when we make a request
    after a long time, the first requests take much longer time than the consecutive
    ones, this problem is called **Cold Start Problem**. And lastly, there are **no
    proper GPU-based instances** yet for serverless.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server instances:** In this schema, the server is always up and you are always
    paying up the money even if no one requests our application. For applications
    with larger user bases, keeping the server up and running is important. In this
    strategy, we can deploy our apps in multiple ways, one way is to launch a single
    server instance that you scale manually every time the traffic increases. In practice,
    these servers are launched with the help of **Kubernetes** **clusters** which
    define the rule for scaling the infrastructure and do traffic management for us.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: The biggest advantage is that we can work with the biggest-sized models and
    applications and get precise control over our resources, from GPU-based instances
    to regular instances. But managing and scaling these server instances properly
    is quite a big task and often requires a lot of fiddling. These can get super
    expensive for **GPU-based instances** since many AI models require GPU for faster
    inference.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two great resources to understand Kubernetes and Docker:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/aiguys/docker-for-dummies-8e8edc8af0ea?source=post_page-----b4cd84f86de1--------------------------------)
    [## Docker for dummies… 🐳 🧠💡'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 🚀 Dockerize a hello-world node app with me, in 15 mins.
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'medium.com](https://medium.com/aiguys/docker-for-dummies-8e8edc8af0ea?source=post_page-----b4cd84f86de1--------------------------------)
    [](https://medium.com/aiguys/kubernetes-101-introduction-to-container-orchestration-b88e60c04ed2?source=post_page-----b4cd84f86de1--------------------------------)
    [## Kubernetes 101: Introduction to Container Orchestration 🎵 🐳'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: If you are reading this article, you most likely are to be familiar with the
    concept of containerization, images and…
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/aiguys/kubernetes-101-introduction-to-container-orchestration-b88e60c04ed2?source=post_page-----b4cd84f86de1--------------------------------)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '**Edge Deployment:** When we need the fastest response in places without internet,
    we go with edge deployment. This deployment type is meant for **IoT devices**
    and other smaller devices that do not have large memory or connection with the
    internet. For instance, if we want AI in a drone, we want the AI module to be
    deployed in the drone itself, not on some cloud server.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**边缘部署：** 当我们需要在没有互联网的地方获得最快响应时，我们选择边缘部署。这种部署类型适用于**IoT 设备**和其他没有大内存或互联网连接的小型设备。例如，如果我们希望在无人机上使用
    AI，我们希望 AI 模块部署在无人机本身上，而不是某个云服务器上。'
- en: This deployment type can only handle a very small payload due to the devices'
    hardware-based limitation. In this deployment mode, there is zero cost because
    everything runs locally. Making models small enough to fit on an IoT device is
    quite challenging and requires a completely new set of strategies.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于设备的硬件限制，这种部署类型只能处理非常小的负载。在这种部署模式下，没有成本，因为一切都在本地运行。使模型小到足以适应 IoT 设备是相当具有挑战性的，并且需要完全不同的策略。
- en: Deployment strategies have a ton of things; covering in one blog is almost impossible.
    Here’s another good blog giving an overview of the entire MLOPS strategies.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 部署策略有很多内容；在一个博客中几乎不可能覆盖所有内容。这里有另一个很好的博客提供了整个 MLOPS 策略的概述。
- en: '[](https://medium.com/aiguys/mlops-deploying-and-managing-models-at-scale-9a51f8fc0406?source=post_page-----b4cd84f86de1--------------------------------)
    [## MLOps: Managing AI models at Scale'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/aiguys/mlops-deploying-and-managing-models-at-scale-9a51f8fc0406?source=post_page-----b4cd84f86de1--------------------------------)
    [## MLOps：大规模管理 AI 模型'
- en: Model building is excellent, but if we can’t deploy these models then it becomes
    useless. Unlike Deep learning, finding…
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型构建很出色，但如果我们不能部署这些模型，它们就会变得无用。与深度学习不同，找到……
- en: medium.com](https://medium.com/aiguys/mlops-deploying-and-managing-models-at-scale-9a51f8fc0406?source=post_page-----b4cd84f86de1--------------------------------)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/aiguys/mlops-deploying-and-managing-models-at-scale-9a51f8fc0406?source=post_page-----b4cd84f86de1--------------------------------)
- en: What is Quantization and TFLite?
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是量化和 TFLite？
- en: '**Quantization** is a model compression technique in which we convert our weights
    to lower precision to **reduce the size of the model** thus making our models
    smaller and faster at inference. Quantization can greatly improve **speed** and
    is often used for edge deployment. Deploying a quantized model in a serverless
    fashion can be great cost-saving as this makes the AI model small enough to be
    used in a serverless fashion.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**量化**是一种模型压缩技术，在这种技术中，我们将权重转换为较低的精度，以**减小模型的大小**，从而使模型在推断时更小、更快。量化可以显著提高**速度**，并且通常用于边缘部署。在无服务器模式下部署量化模型可以大大节省成本，因为这使得
    AI 模型小到足以在无服务器模式下使用。'
- en: '**NOTE:** People often think they need GPU instances to serve the AI models
    as they used GPU instances to train them, but that’s not true. Most AI applications
    with CPU instances and proper deployment strategy can serve even a billion users.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 人们常常认为需要 GPU 实例来服务 AI 模型，因为他们用 GPU 实例训练了这些模型，但这并不正确。大多数 AI 应用程序通过 CPU
    实例和适当的部署策略可以服务甚至十亿用户。'
- en: Quantization is one of many ways to compress model size, there are a lot of
    other methodologies like pruning, weight sharing, etc.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是压缩模型大小的众多方法之一，还有很多其他方法，如剪枝、权重共享等。
- en: 'Here’s an article detailing all the **model compression techniques**:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一篇文章详细介绍了所有的**模型压缩技术**：
- en: '[](https://medium.com/aiguys/reducing-deep-learning-size-16bed87cccff?source=post_page-----b4cd84f86de1--------------------------------)
    [## Deep learning model compression'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/aiguys/reducing-deep-learning-size-16bed87cccff?source=post_page-----b4cd84f86de1--------------------------------)
    [## 深度学习模型压缩'
- en: With each passing year, models are getting more complex and bigger. A lot of
    AI models developed in research labs never…
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随着每年模型变得越来越复杂和庞大。很多在研究实验室开发的 AI 模型从未……
- en: medium.com](https://medium.com/aiguys/reducing-deep-learning-size-16bed87cccff?source=post_page-----b4cd84f86de1--------------------------------)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/aiguys/reducing-deep-learning-size-16bed87cccff?source=post_page-----b4cd84f86de1--------------------------------)
- en: What is TFLite
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是 TFLite
- en: According to the TensorFlow website, “TensorFlow Lite is a set of tools that
    enables on-device machine learning by helping developers run their models on mobile,
    embedded, and edge devices.”
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 TensorFlow 网站，“TensorFlow Lite 是一套工具，通过帮助开发者在移动设备、嵌入式设备和边缘设备上运行模型，实现设备上的机器学习。”
- en: There are many ways to quantize AI models; two main categorizations are **post-training
    quantization and quantization-aware training**. In the prior one, we normally
    train our models. After the training is complete, quantization is applied to model
    weights, whereas for the latter, during the training itself, quantization is active.
    Usually, quantized-aware training performs better than post-training quantization.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Let’s directly jump into the code for quantization. We are using a post-training
    quantized image segmentation model for this blog. Given below image shows the
    architecture of our AI Pipeline.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e7a7de87920f7d11c6a6c85a15be31e8.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: 'AI Pipeline architecture (Img Src: Belongs to author)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'I’m making the following assumptions here:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: You already have an image segmentation model saved in .hdf5 or .h5 format.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If not, follow this tutorial from Keras official website: [https://keras.io/examples/vision/oxford_pets_image_segmentation/](https://keras.io/examples/vision/oxford_pets_image_segmentation/)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: You have a variable called ***train_input_img_paths*** storing paths to all
    the training images. Once again, you can follow the Keras official example link
    of Step 1.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have your own custom data loaders, modify the ***represetative_dataset()***
    method.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now we are ready to deploy our TFLite model in a serverless fashion using Google
    Cloud Run API.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Deploying TFLite model using GCP Cloud Run API
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need these resources and files to deploy our model and make predictions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Dockerfile
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: app.py
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: client.py
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: requirements.txt
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: quantized model
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s first understand the flow of deployment first.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: The serverless deployment flow starts with **containerizing** of our application
    app.py (we use docker here), then **pushing the docker image to a container registry**
    (Google container registry in our case); we need a container registry to ensure
    the versioning, availability, and security of our images. Then configuring and
    deploying it to a serverless platform (Google Cloud Run API), and then allowing
    the platform to handle the execution and scaling of our functions.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: The serverless mode of deployment abstracts away infrastructure management and
    provides automatic scaling, giving us more time to focus on developing and deploying
    our application code.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '**Dockerfile**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Overall, this Dockerfile sets up the necessary environment and dependencies
    for running the Flask application (`app.py`) inside a Docker container. It ensures
    that the required Python packages and the pre-trained model file are available
    within the container.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '**app.py**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**client.py**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Note:** When I trained my image segmentation model, I used BGR format (default
    mode of OpenCV); if you used RGB, remove ***line 30*** from the app.py.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Also, put your own endpoint URL in client.py ***line 8,*** which you will get
    after successfully deploying the Google Cloud RUN API.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: And latly, use the same version of Python in your Dockerfile and local environment
    to avoid breaking anything during the deployment.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '**requirements.txt**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Quantized model**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: And lastly, we need to keep our **model_quantized_float16.tflite** in the same
    folder as our app.py as we copy our quantized model in our docker image.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how my directory looks after collecting all the resources:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c64d26ef869ef20b754722938704489.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: 'Img Src: Belongs to author'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up Serverless
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step is to get the gcloud CLI (Command Line Interface), I used Windows
    it’s pretty straightforward: [https://cloud.google.com/sdk/docs/install](https://cloud.google.com/sdk/docs/install)'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. Navigate to your folder using the standard CLI command
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 3\. Login into gcloud CLI
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This will open up a window in your browser and ask for a few permissions, allow
    that.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Setup a project in gcloud, better use the GUI interface for this.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the link to create a GCP project:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://developers.google.com/workspace/guides/create-project?source=post_page-----b4cd84f86de1--------------------------------)
    [## Create a Google Cloud project | Google Workspace | Google for Developers'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: A Google Cloud project is required to use Google Workspace APIs and build Google
    Workspace add-ons or apps. This…
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: developers.google.com](https://developers.google.com/workspace/guides/create-project?source=post_page-----b4cd84f86de1--------------------------------)
    ![](../Images/d306e4ddae1d3bba879df811fc98a7fd.png)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'GCP project Dashboard (Img Src: Belongs to author)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Setup project ID in gcloud CLI, you can see your project ID in your dashboard.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 6\. Build container in gcloud CLI. Replace <PROJECT_ID> everywhere.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/52a2c7122581c08fe5dd3009d144c776.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: 'Building container (Img Src: Belongs to author)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Push Docker image to Container registry through gcloud CLI
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/08f5edf767b9bea3b1e51f7248bbd668.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: 'Google Container registry (Img Src: Belongs to author)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Deploy the Coud RUN API through gcloud CLI. This will ask to choose server
    locations and some other authentications, allow all of them.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/797936fcd94d0d2ef0284ffcb52f90a1.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: 'Model Deployed (Img Src: Belongs to author)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: If everything is successful, you will see a link in your gcloud CLI, that you
    need to paste into your client.py. Otherwise, go to logs and try to fix the errors.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d0b43da956b78aa1de4600633cebbd39.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: 'Cloud Run API console (Img Src: Belongs to author)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '**Key things to Note here:**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: It’s almost guaranteed that something or other will break in this deployment;
    the biggest reason is version mismatch in packages.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Use the exact same version in your requirements.txt and Dockerfile as you used
    to train the model and quantize the model. Remember GCP runs quite behind the
    TF’s and Python’s latest version. It’s better to use an older version.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: I trained my model on Python 3.8.15; the rest are given in the requirements.txt.
    The errors in the logs are often unclear, so always use the exact same versions;
    if you can’t find the required versions in GCP, change the version for your local
    environment.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Next, the biggest reason for failed deployment is that you haven’t activated
    the required APIs or you don’t have the required permissions and IAM roles. It’s
    better to use the account as owner with all the permissions enabled if you use
    GCP for the first time.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，部署失败的最大原因是你没有激活所需的 API 或者你没有必要的权限和 IAM 角色。如果你第一次使用 GCP，最好使用拥有所有权限的账户作为所有者。
- en: Making predictions
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进行预测
- en: Just run `client.py` in your gcloud CLI or your standard command prompt.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 只需在你的 gcloud CLI 或标准命令提示符下运行 `client.py`。
- en: 'Here’s what my output looks like:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我的输出示例：
- en: '![](../Images/11c8a54ffa4b58e442c58f79562e5f8c.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11c8a54ffa4b58e442c58f79562e5f8c.png)'
- en: 'Model prediction (Img Src: Belongs to author)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 模型预测（图片来源：作者提供）
- en: I trained a binary image segmentation model on some private data. Due to privacy,
    I can neither reveal the details of my model nor the data. But all the mentioned
    things should work with any image segmentation model.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我在一些私人数据上训练了一个二分类图像分割模型。由于隐私原因，我不能透露我的模型或数据的详细信息。但所有提到的内容都应该适用于任何图像分割模型。
- en: Versioning
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 版本控制
- en: And lastly, if you need more resources from the start or want to minimize the
    cold start problem, we can create a new version of the same with just a few additional
    steps.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你需要从一开始就获得更多资源，或想要最小化冷启动问题，我们可以通过几个额外的步骤创建同样的新版本。
- en: '**Go to your gcloud console in** **GUI > Search cloud run API > Select the
    deployed service > Click on edit and deploy new revision button**. And you will
    get the following options, choose according to your needs, save them, and automatically,
    a new version of your model will be set up for the next sets of requests.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**前往你的 gcloud 控制台** **在 GUI > 搜索 cloud run API > 选择已部署的服务 > 点击编辑并部署新版本按钮**。你将看到以下选项，根据你的需求进行选择，保存它们，系统将自动为下一批请求设置模型的新版本。'
- en: '![](../Images/9ab775f6e179512f6bfe95c2d07eb9eb.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ab775f6e179512f6bfe95c2d07eb9eb.png)'
- en: 'Solving cold start problem (Img Src: Belongs to author)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 解决冷启动问题（图片来源：作者提供）
- en: Conclusion
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Choosing the right strategy for deployment is crucial for cost saving.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择正确的部署策略对节省成本至关重要。
- en: We can make our models faster and smaller using the quantization techniques.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用量化技术使模型更快更小。
- en: Serverless deployment with quantized model is a great strategy and can easily
    serve many requests without using costly GPU instances.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用量化模型的无服务器部署是一种很好的策略，可以轻松处理许多请求而不使用昂贵的 GPU 实例。
- en: Serverless takes away the hastle of scaling.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无服务器架构消除了扩展的麻烦。
- en: Thanks for your time and patience, happy learning ❤. Follow me for more of such
    awesome content.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你的时间和耐心，祝学习愉快 ❤。关注我，获取更多这样的精彩内容。
- en: '**Here’s my reading list for MLOps, discussing several other key concepts and
    strategies:**'
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**这是我的 MLOps 阅读清单，讨论了几个其他关键概念和策略：**'
- en: '![Vishal Rajput](../Images/a96ad11c1783cef37c01eb5b36ffbe0d.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![Vishal Rajput](../Images/a96ad11c1783cef37c01eb5b36ffbe0d.png)'
- en: '[Vishal Rajput](https://vishal-ai.medium.com/?source=post_page-----b4cd84f86de1--------------------------------)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[Vishal Rajput](https://vishal-ai.medium.com/?source=post_page-----b4cd84f86de1--------------------------------)'
- en: MLOps
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLOps
- en: '[View list](https://vishal-ai.medium.com/list/mlops-ff7f2453a835?source=post_page-----b4cd84f86de1--------------------------------)10
    stories![](../Images/f43ac8226531c0d15a3fdd2c774f939c.png)![](../Images/f6516179d1713c465b1fff8a3017344f.png)![](../Images/65ceb6ecc46ce9f7e4159979b811b308.png)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://vishal-ai.medium.com/list/mlops-ff7f2453a835?source=post_page-----b4cd84f86de1--------------------------------)10
    个故事![](../Images/f43ac8226531c0d15a3fdd2c774f939c.png)![](../Images/f6516179d1713c465b1fff8a3017344f.png)![](../Images/65ceb6ecc46ce9f7e4159979b811b308.png)'
