- en: Deploying a TFLite Model on GCP Serverless
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 GCP 无服务器架构上部署 TFLite 模型
- en: 原文：[https://towardsdatascience.com/deploying-tflite-model-on-gcp-serverless-b4cd84f86de1](https://towardsdatascience.com/deploying-tflite-model-on-gcp-serverless-b4cd84f86de1)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deploying-tflite-model-on-gcp-serverless-b4cd84f86de1](https://towardsdatascience.com/deploying-tflite-model-on-gcp-serverless-b4cd84f86de1)
- en: How to deploy a quantized model in a Serverless fashion
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何以无服务器的方式部署量化模型
- en: '[](https://vishal-ai.medium.com/?source=post_page-----b4cd84f86de1--------------------------------)[![Vishal
    Rajput](../Images/c43407d7df1f099832cbaa5381a0bb74.png)](https://vishal-ai.medium.com/?source=post_page-----b4cd84f86de1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b4cd84f86de1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b4cd84f86de1--------------------------------)
    [Vishal Rajput](https://vishal-ai.medium.com/?source=post_page-----b4cd84f86de1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://vishal-ai.medium.com/?source=post_page-----b4cd84f86de1--------------------------------)[![Vishal
    Rajput](../Images/c43407d7df1f099832cbaa5381a0bb74.png)](https://vishal-ai.medium.com/?source=post_page-----b4cd84f86de1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b4cd84f86de1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b4cd84f86de1--------------------------------)
    [Vishal Rajput](https://vishal-ai.medium.com/?source=post_page-----b4cd84f86de1--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b4cd84f86de1--------------------------------)
    ·11 min read·Jul 21, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b4cd84f86de1--------------------------------)
    ·阅读时长 11 分钟·2023年7月21日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Model deployment is tricky; with the continuously changing landscape of cloud
    platforms and other AI-related libraries updating almost weekly, back compatibility
    and finding the correct deployment method is a big challenge. In today’s blog
    post, we will see how to deploy a **tflite model** on the **Google Cloud Platform**
    in a **serverless** fashion.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署是一个棘手的问题；由于云平台和其他AI相关库的不断变化，几乎每周都有更新，因此向后兼容性和找到正确的部署方法是一个巨大的挑战。在今天的博客文章中，我们将探讨如何以**无服务器**的方式在**Google
    Cloud Platform**上部署**tflite模型**。
- en: 'This blog post is structured in the following way:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本博客文章的结构如下：
- en: Understanding Serverless and other ways of Deployment
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解无服务器架构和其他部署方式
- en: What is Quantization and TFLite?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是量化和 TFLite？
- en: Deploying TFLite model using GCP Cloud Run API
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 GCP Cloud Run API 部署 TFLite 模型
- en: '![](../Images/c1719de604d34543f57af5f33b0bf5ce.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1719de604d34543f57af5f33b0bf5ce.png)'
- en: 'Img Src: [https://pixabay.com/photos/man-pier-silhouette-sunrise-fog-8091933/](https://pixabay.com/photos/man-pier-silhouette-sunrise-fog-8091933/)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: [https://pixabay.com/photos/man-pier-silhouette-sunrise-fog-8091933/](https://pixabay.com/photos/man-pier-silhouette-sunrise-fog-8091933/)'
- en: Understanding Serverless and other ways of Deployment
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解无服务器架构和其他部署方式
- en: '**Let’s first understand what do we mean by serverless because serverless doesn’t
    mean without a server.**'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**首先让我们了解什么是无服务器架构，因为无服务器并不意味着没有服务器。**'
- en: An AI model, or any application for that matter can be deployed in several different
    ways with three major categorisations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个AI模型，或任何应用程序，实际上可以通过多种不同的方式进行部署，主要有三大类。
- en: '**Serverless:** In this case, the model is stored on the cloud container registry
    and only runs when a user makes a request. When a request is made, a server instance
    is automatically launched to fulfill the user request, which shuts down after
    a while. From starting, configuring, scaling, and shutting down, all of this is
    taken by the Cloud Run API provided by the Google Cloud platform. We have AWS
    Lambda and Azure Functions as alternatives in other clouds.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**无服务器架构：** 在这种情况下，模型存储在云容器注册表中，只有在用户发出请求时才会运行。当请求发出时，会自动启动一个服务器实例来处理用户请求，并在一段时间后关闭。从启动、配置、扩展到关闭，这一切都由
    Google Cloud 平台提供的 Cloud Run API 处理。在其他云平台中，我们有 AWS Lambda 和 Azure Functions 作为替代方案。'
- en: '**Serverless** has its own advantages and disadvantages.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**无服务器架构**有其自身的优缺点。'
- en: The biggest advantage is the **cost-saving**, if you don’t have a large user
    base, most of the time, the server is sitting idle, and your money is just going
    for no reason. Another advantage is that we don’t need to think about **scaling**
    the infrastructure, depending upon the load on the server, it can automatically
    replicate the number of instances and handle the traffic.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大的优势在于**节省成本**，如果你没有大量的用户基础，大部分时间服务器处于闲置状态，你的钱只是白白花费了。另一个优势是我们不需要考虑**扩展**基础设施，根据服务器的负载，它可以自动复制实例的数量并处理流量。
- en: In the disadvantage column, there are three things to consider. It has a **small
    payload limit**, meaning it can be used to run a bigger model. Secondly, the server
    automatically shuts down after 15 min of idle time, thus when we make a request
    after a long time, the first requests take much longer time than the consecutive
    ones, this problem is called **Cold Start Problem**. And lastly, there are **no
    proper GPU-based instances** yet for serverless.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在缺点方面，有三点需要考虑。首先是**小负载限制**，这意味着它不能用于运行更大的模型。其次，服务器在 15 分钟空闲后会自动关闭，因此当我们在很长时间后发出请求时，第一次请求比后续请求花费的时间要长，这个问题被称为**冷启动问题**。最后，目前还没有**适当的
    GPU 基于实例**可用于无服务器计算。
- en: '**Server instances:** In this schema, the server is always up and you are always
    paying up the money even if no one requests our application. For applications
    with larger user bases, keeping the server up and running is important. In this
    strategy, we can deploy our apps in multiple ways, one way is to launch a single
    server instance that you scale manually every time the traffic increases. In practice,
    these servers are launched with the help of **Kubernetes** **clusters** which
    define the rule for scaling the infrastructure and do traffic management for us.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**服务器实例：** 在这种模式中，服务器始终处于运行状态，即使没有人请求我们的应用，你也总是需要支付费用。对于用户基础较大的应用来说，保持服务器持续运行是很重要的。在这种策略下，我们可以以多种方式部署应用，其中一种方式是启动一个单一的服务器实例，并在流量增加时手动扩展。实际上，这些服务器是借助**Kubernetes**
    **集群**启动的，这些集群定义了扩展基础设施的规则，并为我们进行流量管理。'
- en: The biggest advantage is that we can work with the biggest-sized models and
    applications and get precise control over our resources, from GPU-based instances
    to regular instances. But managing and scaling these server instances properly
    is quite a big task and often requires a lot of fiddling. These can get super
    expensive for **GPU-based instances** since many AI models require GPU for faster
    inference.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大的优势在于我们可以使用最大规模的模型和应用，并精确控制我们的资源，从基于 GPU 的实例到常规实例。但正确管理和扩展这些服务器实例是一项相当大的任务，通常需要大量的调整。这些对于**基于
    GPU 的实例**来说可能非常昂贵，因为许多 AI 模型需要 GPU 以实现更快的推理。
- en: 'Two great resources to understand Kubernetes and Docker:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 和 Docker 的两个极好的资源：
- en: '[](https://medium.com/aiguys/docker-for-dummies-8e8edc8af0ea?source=post_page-----b4cd84f86de1--------------------------------)
    [## Docker for dummies… 🐳 🧠💡'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/aiguys/docker-for-dummies-8e8edc8af0ea?source=post_page-----b4cd84f86de1--------------------------------)
    [## Docker 入门… 🐳 🧠💡'
- en: 🚀 Dockerize a hello-world node app with me, in 15 mins.
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 🚀 和我一起在 15 分钟内将一个 hello-world 节点应用程序 Docker 化。
- en: 'medium.com](https://medium.com/aiguys/docker-for-dummies-8e8edc8af0ea?source=post_page-----b4cd84f86de1--------------------------------)
    [](https://medium.com/aiguys/kubernetes-101-introduction-to-container-orchestration-b88e60c04ed2?source=post_page-----b4cd84f86de1--------------------------------)
    [## Kubernetes 101: Introduction to Container Orchestration 🎵 🐳'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/aiguys/docker-for-dummies-8e8edc8af0ea?source=post_page-----b4cd84f86de1--------------------------------)
    [](https://medium.com/aiguys/kubernetes-101-introduction-to-container-orchestration-b88e60c04ed2?source=post_page-----b4cd84f86de1--------------------------------)
    [## Kubernetes 101：容器编排介绍 🎵 🐳
- en: If you are reading this article, you most likely are to be familiar with the
    concept of containerization, images and…
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如果你正在阅读这篇文章，你很可能对容器化、镜像等概念非常熟悉……
- en: medium.com](https://medium.com/aiguys/kubernetes-101-introduction-to-container-orchestration-b88e60c04ed2?source=post_page-----b4cd84f86de1--------------------------------)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/aiguys/kubernetes-101-introduction-to-container-orchestration-b88e60c04ed2?source=post_page-----b4cd84f86de1--------------------------------)
- en: '**Edge Deployment:** When we need the fastest response in places without internet,
    we go with edge deployment. This deployment type is meant for **IoT devices**
    and other smaller devices that do not have large memory or connection with the
    internet. For instance, if we want AI in a drone, we want the AI module to be
    deployed in the drone itself, not on some cloud server.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**边缘部署：** 当我们需要在没有互联网的地方获得最快响应时，我们选择边缘部署。这种部署类型适用于**IoT 设备**和其他没有大内存或互联网连接的小型设备。例如，如果我们希望在无人机上使用
    AI，我们希望 AI 模块部署在无人机本身上，而不是某个云服务器上。'
- en: This deployment type can only handle a very small payload due to the devices'
    hardware-based limitation. In this deployment mode, there is zero cost because
    everything runs locally. Making models small enough to fit on an IoT device is
    quite challenging and requires a completely new set of strategies.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于设备的硬件限制，这种部署类型只能处理非常小的负载。在这种部署模式下，没有成本，因为一切都在本地运行。使模型小到足以适应 IoT 设备是相当具有挑战性的，并且需要完全不同的策略。
- en: Deployment strategies have a ton of things; covering in one blog is almost impossible.
    Here’s another good blog giving an overview of the entire MLOPS strategies.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 部署策略有很多内容；在一个博客中几乎不可能覆盖所有内容。这里有另一个很好的博客提供了整个 MLOPS 策略的概述。
- en: '[](https://medium.com/aiguys/mlops-deploying-and-managing-models-at-scale-9a51f8fc0406?source=post_page-----b4cd84f86de1--------------------------------)
    [## MLOps: Managing AI models at Scale'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/aiguys/mlops-deploying-and-managing-models-at-scale-9a51f8fc0406?source=post_page-----b4cd84f86de1--------------------------------)
    [## MLOps：大规模管理 AI 模型'
- en: Model building is excellent, but if we can’t deploy these models then it becomes
    useless. Unlike Deep learning, finding…
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型构建很出色，但如果我们不能部署这些模型，它们就会变得无用。与深度学习不同，找到……
- en: medium.com](https://medium.com/aiguys/mlops-deploying-and-managing-models-at-scale-9a51f8fc0406?source=post_page-----b4cd84f86de1--------------------------------)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/aiguys/mlops-deploying-and-managing-models-at-scale-9a51f8fc0406?source=post_page-----b4cd84f86de1--------------------------------)
- en: What is Quantization and TFLite?
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是量化和 TFLite？
- en: '**Quantization** is a model compression technique in which we convert our weights
    to lower precision to **reduce the size of the model** thus making our models
    smaller and faster at inference. Quantization can greatly improve **speed** and
    is often used for edge deployment. Deploying a quantized model in a serverless
    fashion can be great cost-saving as this makes the AI model small enough to be
    used in a serverless fashion.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**量化**是一种模型压缩技术，在这种技术中，我们将权重转换为较低的精度，以**减小模型的大小**，从而使模型在推断时更小、更快。量化可以显著提高**速度**，并且通常用于边缘部署。在无服务器模式下部署量化模型可以大大节省成本，因为这使得
    AI 模型小到足以在无服务器模式下使用。'
- en: '**NOTE:** People often think they need GPU instances to serve the AI models
    as they used GPU instances to train them, but that’s not true. Most AI applications
    with CPU instances and proper deployment strategy can serve even a billion users.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 人们常常认为需要 GPU 实例来服务 AI 模型，因为他们用 GPU 实例训练了这些模型，但这并不正确。大多数 AI 应用程序通过 CPU
    实例和适当的部署策略可以服务甚至十亿用户。'
- en: Quantization is one of many ways to compress model size, there are a lot of
    other methodologies like pruning, weight sharing, etc.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是压缩模型大小的众多方法之一，还有很多其他方法，如剪枝、权重共享等。
- en: 'Here’s an article detailing all the **model compression techniques**:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一篇文章详细介绍了所有的**模型压缩技术**：
- en: '[](https://medium.com/aiguys/reducing-deep-learning-size-16bed87cccff?source=post_page-----b4cd84f86de1--------------------------------)
    [## Deep learning model compression'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/aiguys/reducing-deep-learning-size-16bed87cccff?source=post_page-----b4cd84f86de1--------------------------------)
    [## 深度学习模型压缩'
- en: With each passing year, models are getting more complex and bigger. A lot of
    AI models developed in research labs never…
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随着每年模型变得越来越复杂和庞大。很多在研究实验室开发的 AI 模型从未……
- en: medium.com](https://medium.com/aiguys/reducing-deep-learning-size-16bed87cccff?source=post_page-----b4cd84f86de1--------------------------------)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/aiguys/reducing-deep-learning-size-16bed87cccff?source=post_page-----b4cd84f86de1--------------------------------)
- en: What is TFLite
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是 TFLite
- en: According to the TensorFlow website, “TensorFlow Lite is a set of tools that
    enables on-device machine learning by helping developers run their models on mobile,
    embedded, and edge devices.”
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 TensorFlow 网站，“TensorFlow Lite 是一套工具，通过帮助开发者在移动设备、嵌入式设备和边缘设备上运行模型，实现设备上的机器学习。”
- en: There are many ways to quantize AI models; two main categorizations are **post-training
    quantization and quantization-aware training**. In the prior one, we normally
    train our models. After the training is complete, quantization is applied to model
    weights, whereas for the latter, during the training itself, quantization is active.
    Usually, quantized-aware training performs better than post-training quantization.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 量化 AI 模型的方法有很多；主要分为 **训练后量化和量化感知训练** 两类。在前者中，我们通常先训练模型。训练完成后，对模型权重应用量化，而在后者中，量化在训练过程中就已经激活。通常，量化感知训练的效果优于训练后量化。
- en: Let’s directly jump into the code for quantization. We are using a post-training
    quantized image segmentation model for this blog. Given below image shows the
    architecture of our AI Pipeline.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接跳到量化代码部分。我们在这个博客中使用的是训练后量化的图像分割模型。下图显示了我们 AI 管道的架构。
- en: '![](../Images/e7a7de87920f7d11c6a6c85a15be31e8.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7a7de87920f7d11c6a6c85a15be31e8.png)'
- en: 'AI Pipeline architecture (Img Src: Belongs to author)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: AI 管道架构（图片来源：作者所有）
- en: 'I’m making the following assumptions here:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里做出以下假设：
- en: You already have an image segmentation model saved in .hdf5 or .h5 format.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您已经有一个以 .hdf5 或 .h5 格式保存的图像分割模型。
- en: 'If not, follow this tutorial from Keras official website: [https://keras.io/examples/vision/oxford_pets_image_segmentation/](https://keras.io/examples/vision/oxford_pets_image_segmentation/)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不是，请参阅 Keras 官方网站的教程：[https://keras.io/examples/vision/oxford_pets_image_segmentation/](https://keras.io/examples/vision/oxford_pets_image_segmentation/)
- en: You have a variable called ***train_input_img_paths*** storing paths to all
    the training images. Once again, you can follow the Keras official example link
    of Step 1.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您有一个名为 ***train_input_img_paths*** 的变量，用于存储所有训练图像的路径。您可以再次参阅 Keras 官方示例链接的第
    1 步。
- en: If you have your own custom data loaders, modify the ***represetative_dataset()***
    method.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您有自己的自定义数据加载器，请修改 ***represetative_dataset()*** 方法。
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now we are ready to deploy our TFLite model in a serverless fashion using Google
    Cloud Run API.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备使用 Google Cloud Run API 以无服务器的方式部署我们的 TFLite 模型。
- en: Deploying TFLite model using GCP Cloud Run API
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GCP Cloud Run API 部署 TFLite 模型
- en: We need these resources and files to deploy our model and make predictions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要这些资源和文件来部署我们的模型并进行预测。
- en: Dockerfile
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dockerfile
- en: app.py
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: app.py
- en: client.py
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: client.py
- en: requirements.txt
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: requirements.txt
- en: quantized model
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化模型
- en: Let’s first understand the flow of deployment first.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来理解部署的流程。
- en: The serverless deployment flow starts with **containerizing** of our application
    app.py (we use docker here), then **pushing the docker image to a container registry**
    (Google container registry in our case); we need a container registry to ensure
    the versioning, availability, and security of our images. Then configuring and
    deploying it to a serverless platform (Google Cloud Run API), and then allowing
    the platform to handle the execution and scaling of our functions.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 无服务器部署流程从 **容器化** 应用程序 app.py（我们在这里使用 Docker）开始，然后 **将 Docker 镜像推送到容器注册表**（在我们这里是
    Google 容器注册表）；我们需要容器注册表来确保我们镜像的版本控制、可用性和安全性。接着，将其配置并部署到无服务器平台（Google Cloud Run
    API），然后让平台处理我们的函数的执行和扩展。
- en: The serverless mode of deployment abstracts away infrastructure management and
    provides automatic scaling, giving us more time to focus on developing and deploying
    our application code.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 无服务器模式的部署将基础设施管理抽象化，并提供自动扩展，使我们能够有更多时间专注于开发和部署应用程序代码。
- en: '**Dockerfile**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**Dockerfile**'
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Overall, this Dockerfile sets up the necessary environment and dependencies
    for running the Flask application (`app.py`) inside a Docker container. It ensures
    that the required Python packages and the pre-trained model file are available
    within the container.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这个 Dockerfile 设置了运行 Flask 应用程序（`app.py`）所需的环境和依赖项。它确保在容器内可以使用所需的 Python
    包和预训练模型文件。
- en: '**app.py**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**app.py**'
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**client.py**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**client.py**'
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Note:** When I trained my image segmentation model, I used BGR format (default
    mode of OpenCV); if you used RGB, remove ***line 30*** from the app.py.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 当我训练图像分割模型时，我使用了 BGR 格式（OpenCV 的默认模式）；如果您使用了 RGB，请从 app.py 中删除 ***第
    30 行***。'
- en: Also, put your own endpoint URL in client.py ***line 8,*** which you will get
    after successfully deploying the Google Cloud RUN API.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请在 client.py 的 ***第 8 行*** 中填入您在成功部署 Google Cloud RUN API 后获得的端点 URL。
- en: And latly, use the same version of Python in your Dockerfile and local environment
    to avoid breaking anything during the deployment.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了避免在部署过程中出现问题，请在 Dockerfile 和本地环境中使用相同版本的 Python。
- en: '**requirements.txt**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**requirements.txt**'
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Quantized model**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**量化模型**'
- en: And lastly, we need to keep our **model_quantized_float16.tflite** in the same
    folder as our app.py as we copy our quantized model in our docker image.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要将 **model_quantized_float16.tflite** 保存在与 app.py 相同的文件夹中，因为我们将量化模型复制到
    Docker 镜像中。
- en: 'This is how my directory looks after collecting all the resources:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我收集所有资源后目录的样子：
- en: '![](../Images/3c64d26ef869ef20b754722938704489.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c64d26ef869ef20b754722938704489.png)'
- en: 'Img Src: Belongs to author'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：属于作者
- en: Setting Up Serverless
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置无服务器环境
- en: 'The first step is to get the gcloud CLI (Command Line Interface), I used Windows
    it’s pretty straightforward: [https://cloud.google.com/sdk/docs/install](https://cloud.google.com/sdk/docs/install)'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是获取 gcloud CLI（命令行接口），我使用了 Windows，操作非常简单：[https://cloud.google.com/sdk/docs/install](https://cloud.google.com/sdk/docs/install)
- en: 2\. Navigate to your folder using the standard CLI command
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 使用标准 CLI 命令导航到你的文件夹
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 3\. Login into gcloud CLI
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 登录 gcloud CLI
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This will open up a window in your browser and ask for a few permissions, allow
    that.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这会在浏览器中打开一个窗口，并要求授予一些权限，请允许。
- en: 4\. Setup a project in gcloud, better use the GUI interface for this.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 在 gcloud 中设置项目，最好使用 GUI 界面。
- en: 'Here’s the link to create a GCP project:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是创建 GCP 项目的链接：
- en: '[](https://developers.google.com/workspace/guides/create-project?source=post_page-----b4cd84f86de1--------------------------------)
    [## Create a Google Cloud project | Google Workspace | Google for Developers'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://developers.google.com/workspace/guides/create-project?source=post_page-----b4cd84f86de1--------------------------------)
    [## 创建 Google Cloud 项目 | Google Workspace | Google 开发者'
- en: A Google Cloud project is required to use Google Workspace APIs and build Google
    Workspace add-ons or apps. This…
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Google Workspace API 和构建 Google Workspace 插件或应用程序需要一个 Google Cloud 项目。这...
- en: developers.google.com](https://developers.google.com/workspace/guides/create-project?source=post_page-----b4cd84f86de1--------------------------------)
    ![](../Images/d306e4ddae1d3bba879df811fc98a7fd.png)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: developers.google.com](https://developers.google.com/workspace/guides/create-project?source=post_page-----b4cd84f86de1--------------------------------)
    ![](../Images/d306e4ddae1d3bba879df811fc98a7fd.png)
- en: 'GCP project Dashboard (Img Src: Belongs to author)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: GCP 项目仪表板（图像来源：属于作者）
- en: 5\. Setup project ID in gcloud CLI, you can see your project ID in your dashboard.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 在 gcloud CLI 中设置项目 ID，你可以在仪表板中看到你的项目 ID。
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 6\. Build container in gcloud CLI. Replace <PROJECT_ID> everywhere.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. 在 gcloud CLI 中构建容器。将 <PROJECT_ID> 替换为实际项目 ID。
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/52a2c7122581c08fe5dd3009d144c776.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52a2c7122581c08fe5dd3009d144c776.png)'
- en: 'Building container (Img Src: Belongs to author)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 构建容器（图像来源：属于作者）
- en: 7\. Push Docker image to Container registry through gcloud CLI
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 7\. 通过 gcloud CLI 将 Docker 镜像推送到容器注册表
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/08f5edf767b9bea3b1e51f7248bbd668.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08f5edf767b9bea3b1e51f7248bbd668.png)'
- en: 'Google Container registry (Img Src: Belongs to author)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Google 容器注册表（图像来源：属于作者）
- en: 8\. Deploy the Coud RUN API through gcloud CLI. This will ask to choose server
    locations and some other authentications, allow all of them.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 8\. 通过 gcloud CLI 部署 Cloud RUN API。这会要求选择服务器位置和一些其他的身份验证，允许所有这些操作。
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/797936fcd94d0d2ef0284ffcb52f90a1.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/797936fcd94d0d2ef0284ffcb52f90a1.png)'
- en: 'Model Deployed (Img Src: Belongs to author)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 模型已部署（图像来源：属于作者）
- en: If everything is successful, you will see a link in your gcloud CLI, that you
    need to paste into your client.py. Otherwise, go to logs and try to fix the errors.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，你将在 gcloud CLI 中看到一个链接，你需要将其粘贴到 client.py 中。否则，请查看日志并尝试修复错误。
- en: '![](../Images/d0b43da956b78aa1de4600633cebbd39.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d0b43da956b78aa1de4600633cebbd39.png)'
- en: 'Cloud Run API console (Img Src: Belongs to author)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud Run API 控制台（图像来源：属于作者）
- en: '**Key things to Note here:**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**需要注意的关键点：**'
- en: It’s almost guaranteed that something or other will break in this deployment;
    the biggest reason is version mismatch in packages.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎可以保证在这个部署过程中会出现一些问题；最大的问题是包的版本不匹配。
- en: Use the exact same version in your requirements.txt and Dockerfile as you used
    to train the model and quantize the model. Remember GCP runs quite behind the
    TF’s and Python’s latest version. It’s better to use an older version.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在 requirements.txt 和 Dockerfile 中使用与你训练模型和量化模型时完全相同的版本。记住 GCP 的 TF 和 Python
    版本通常较旧，最好使用较旧的版本。
- en: I trained my model on Python 3.8.15; the rest are given in the requirements.txt.
    The errors in the logs are often unclear, so always use the exact same versions;
    if you can’t find the required versions in GCP, change the version for your local
    environment.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 Python 3.8.15 上训练了我的模型；其余的在 requirements.txt 中给出。日志中的错误通常不明确，因此请始终使用完全相同的版本；如果在
    GCP 中找不到所需的版本，请为本地环境更改版本。
- en: Next, the biggest reason for failed deployment is that you haven’t activated
    the required APIs or you don’t have the required permissions and IAM roles. It’s
    better to use the account as owner with all the permissions enabled if you use
    GCP for the first time.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，部署失败的最大原因是你没有激活所需的 API 或者你没有必要的权限和 IAM 角色。如果你第一次使用 GCP，最好使用拥有所有权限的账户作为所有者。
- en: Making predictions
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进行预测
- en: Just run `client.py` in your gcloud CLI or your standard command prompt.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 只需在你的 gcloud CLI 或标准命令提示符下运行 `client.py`。
- en: 'Here’s what my output looks like:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我的输出示例：
- en: '![](../Images/11c8a54ffa4b58e442c58f79562e5f8c.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11c8a54ffa4b58e442c58f79562e5f8c.png)'
- en: 'Model prediction (Img Src: Belongs to author)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 模型预测（图片来源：作者提供）
- en: I trained a binary image segmentation model on some private data. Due to privacy,
    I can neither reveal the details of my model nor the data. But all the mentioned
    things should work with any image segmentation model.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我在一些私人数据上训练了一个二分类图像分割模型。由于隐私原因，我不能透露我的模型或数据的详细信息。但所有提到的内容都应该适用于任何图像分割模型。
- en: Versioning
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 版本控制
- en: And lastly, if you need more resources from the start or want to minimize the
    cold start problem, we can create a new version of the same with just a few additional
    steps.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你需要从一开始就获得更多资源，或想要最小化冷启动问题，我们可以通过几个额外的步骤创建同样的新版本。
- en: '**Go to your gcloud console in** **GUI > Search cloud run API > Select the
    deployed service > Click on edit and deploy new revision button**. And you will
    get the following options, choose according to your needs, save them, and automatically,
    a new version of your model will be set up for the next sets of requests.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**前往你的 gcloud 控制台** **在 GUI > 搜索 cloud run API > 选择已部署的服务 > 点击编辑并部署新版本按钮**。你将看到以下选项，根据你的需求进行选择，保存它们，系统将自动为下一批请求设置模型的新版本。'
- en: '![](../Images/9ab775f6e179512f6bfe95c2d07eb9eb.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ab775f6e179512f6bfe95c2d07eb9eb.png)'
- en: 'Solving cold start problem (Img Src: Belongs to author)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 解决冷启动问题（图片来源：作者提供）
- en: Conclusion
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Choosing the right strategy for deployment is crucial for cost saving.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择正确的部署策略对节省成本至关重要。
- en: We can make our models faster and smaller using the quantization techniques.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用量化技术使模型更快更小。
- en: Serverless deployment with quantized model is a great strategy and can easily
    serve many requests without using costly GPU instances.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用量化模型的无服务器部署是一种很好的策略，可以轻松处理许多请求而不使用昂贵的 GPU 实例。
- en: Serverless takes away the hastle of scaling.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无服务器架构消除了扩展的麻烦。
- en: Thanks for your time and patience, happy learning ❤. Follow me for more of such
    awesome content.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你的时间和耐心，祝学习愉快 ❤。关注我，获取更多这样的精彩内容。
- en: '**Here’s my reading list for MLOps, discussing several other key concepts and
    strategies:**'
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**这是我的 MLOps 阅读清单，讨论了几个其他关键概念和策略：**'
- en: '![Vishal Rajput](../Images/a96ad11c1783cef37c01eb5b36ffbe0d.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![Vishal Rajput](../Images/a96ad11c1783cef37c01eb5b36ffbe0d.png)'
- en: '[Vishal Rajput](https://vishal-ai.medium.com/?source=post_page-----b4cd84f86de1--------------------------------)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[Vishal Rajput](https://vishal-ai.medium.com/?source=post_page-----b4cd84f86de1--------------------------------)'
- en: MLOps
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLOps
- en: '[View list](https://vishal-ai.medium.com/list/mlops-ff7f2453a835?source=post_page-----b4cd84f86de1--------------------------------)10
    stories![](../Images/f43ac8226531c0d15a3fdd2c774f939c.png)![](../Images/f6516179d1713c465b1fff8a3017344f.png)![](../Images/65ceb6ecc46ce9f7e4159979b811b308.png)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://vishal-ai.medium.com/list/mlops-ff7f2453a835?source=post_page-----b4cd84f86de1--------------------------------)10
    个故事![](../Images/f43ac8226531c0d15a3fdd2c774f939c.png)![](../Images/f6516179d1713c465b1fff8a3017344f.png)![](../Images/65ceb6ecc46ce9f7e4159979b811b308.png)'
