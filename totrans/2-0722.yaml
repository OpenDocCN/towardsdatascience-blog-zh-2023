- en: Deploying a TFLite Model on GCP Serverless
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/deploying-tflite-model-on-gcp-serverless-b4cd84f86de1](https://towardsdatascience.com/deploying-tflite-model-on-gcp-serverless-b4cd84f86de1)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to deploy a quantized model in a Serverless fashion
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://vishal-ai.medium.com/?source=post_page-----b4cd84f86de1--------------------------------)[![Vishal
    Rajput](../Images/c43407d7df1f099832cbaa5381a0bb74.png)](https://vishal-ai.medium.com/?source=post_page-----b4cd84f86de1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b4cd84f86de1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b4cd84f86de1--------------------------------)
    [Vishal Rajput](https://vishal-ai.medium.com/?source=post_page-----b4cd84f86de1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b4cd84f86de1--------------------------------)
    Â·11 min readÂ·Jul 21, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Model deployment is tricky; with the continuously changing landscape of cloud
    platforms and other AI-related libraries updating almost weekly, back compatibility
    and finding the correct deployment method is a big challenge. In todayâ€™s blog
    post, we will see how to deploy a **tflite model** on the **Google Cloud Platform**
    in a **serverless** fashion.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'This blog post is structured in the following way:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Serverless and other ways of Deployment
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Quantization and TFLite?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying TFLite model using GCP Cloud Run API
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/c1719de604d34543f57af5f33b0bf5ce.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: 'Img Src: [https://pixabay.com/photos/man-pier-silhouette-sunrise-fog-8091933/](https://pixabay.com/photos/man-pier-silhouette-sunrise-fog-8091933/)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Serverless and other ways of Deployment
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Letâ€™s first understand what do we mean by serverless because serverless doesnâ€™t
    mean without a server.**'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An AI model, or any application for that matter can be deployed in several different
    ways with three major categorisations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '**Serverless:** In this case, the model is stored on the cloud container registry
    and only runs when a user makes a request. When a request is made, a server instance
    is automatically launched to fulfill the user request, which shuts down after
    a while. From starting, configuring, scaling, and shutting down, all of this is
    taken by the Cloud Run API provided by the Google Cloud platform. We have AWS
    Lambda and Azure Functions as alternatives in other clouds.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '**Serverless** has its own advantages and disadvantages.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: The biggest advantage is the **cost-saving**, if you donâ€™t have a large user
    base, most of the time, the server is sitting idle, and your money is just going
    for no reason. Another advantage is that we donâ€™t need to think about **scaling**
    the infrastructure, depending upon the load on the server, it can automatically
    replicate the number of instances and handle the traffic.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the disadvantage column, there are three things to consider. It has a **small
    payload limit**, meaning it can be used to run a bigger model. Secondly, the server
    automatically shuts down after 15 min of idle time, thus when we make a request
    after a long time, the first requests take much longer time than the consecutive
    ones, this problem is called **Cold Start Problem**. And lastly, there are **no
    proper GPU-based instances** yet for serverless.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server instances:** In this schema, the server is always up and you are always
    paying up the money even if no one requests our application. For applications
    with larger user bases, keeping the server up and running is important. In this
    strategy, we can deploy our apps in multiple ways, one way is to launch a single
    server instance that you scale manually every time the traffic increases. In practice,
    these servers are launched with the help of **Kubernetes** **clusters** which
    define the rule for scaling the infrastructure and do traffic management for us.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: The biggest advantage is that we can work with the biggest-sized models and
    applications and get precise control over our resources, from GPU-based instances
    to regular instances. But managing and scaling these server instances properly
    is quite a big task and often requires a lot of fiddling. These can get super
    expensive for **GPU-based instances** since many AI models require GPU for faster
    inference.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two great resources to understand Kubernetes and Docker:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/aiguys/docker-for-dummies-8e8edc8af0ea?source=post_page-----b4cd84f86de1--------------------------------)
    [## Docker for dummiesâ€¦ ğŸ³ ğŸ§ ğŸ’¡'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: ğŸš€ Dockerize a hello-world node app with me, in 15 mins.
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'medium.com](https://medium.com/aiguys/docker-for-dummies-8e8edc8af0ea?source=post_page-----b4cd84f86de1--------------------------------)
    [](https://medium.com/aiguys/kubernetes-101-introduction-to-container-orchestration-b88e60c04ed2?source=post_page-----b4cd84f86de1--------------------------------)
    [## Kubernetes 101: Introduction to Container Orchestration ğŸµ ğŸ³'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: If you are reading this article, you most likely are to be familiar with the
    concept of containerization, images andâ€¦
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/aiguys/kubernetes-101-introduction-to-container-orchestration-b88e60c04ed2?source=post_page-----b4cd84f86de1--------------------------------)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '**Edge Deployment:** When we need the fastest response in places without internet,
    we go with edge deployment. This deployment type is meant for **IoT devices**
    and other smaller devices that do not have large memory or connection with the
    internet. For instance, if we want AI in a drone, we want the AI module to be
    deployed in the drone itself, not on some cloud server.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾¹ç¼˜éƒ¨ç½²ï¼š** å½“æˆ‘ä»¬éœ€è¦åœ¨æ²¡æœ‰äº’è”ç½‘çš„åœ°æ–¹è·å¾—æœ€å¿«å“åº”æ—¶ï¼Œæˆ‘ä»¬é€‰æ‹©è¾¹ç¼˜éƒ¨ç½²ã€‚è¿™ç§éƒ¨ç½²ç±»å‹é€‚ç”¨äº**IoT è®¾å¤‡**å’Œå…¶ä»–æ²¡æœ‰å¤§å†…å­˜æˆ–äº’è”ç½‘è¿æ¥çš„å°å‹è®¾å¤‡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬å¸Œæœ›åœ¨æ— äººæœºä¸Šä½¿ç”¨
    AIï¼Œæˆ‘ä»¬å¸Œæœ› AI æ¨¡å—éƒ¨ç½²åœ¨æ— äººæœºæœ¬èº«ä¸Šï¼Œè€Œä¸æ˜¯æŸä¸ªäº‘æœåŠ¡å™¨ä¸Šã€‚'
- en: This deployment type can only handle a very small payload due to the devices'
    hardware-based limitation. In this deployment mode, there is zero cost because
    everything runs locally. Making models small enough to fit on an IoT device is
    quite challenging and requires a completely new set of strategies.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±äºè®¾å¤‡çš„ç¡¬ä»¶é™åˆ¶ï¼Œè¿™ç§éƒ¨ç½²ç±»å‹åªèƒ½å¤„ç†éå¸¸å°çš„è´Ÿè½½ã€‚åœ¨è¿™ç§éƒ¨ç½²æ¨¡å¼ä¸‹ï¼Œæ²¡æœ‰æˆæœ¬ï¼Œå› ä¸ºä¸€åˆ‡éƒ½åœ¨æœ¬åœ°è¿è¡Œã€‚ä½¿æ¨¡å‹å°åˆ°è¶³ä»¥é€‚åº” IoT è®¾å¤‡æ˜¯ç›¸å½“å…·æœ‰æŒ‘æˆ˜æ€§çš„ï¼Œå¹¶ä¸”éœ€è¦å®Œå…¨ä¸åŒçš„ç­–ç•¥ã€‚
- en: Deployment strategies have a ton of things; covering in one blog is almost impossible.
    Hereâ€™s another good blog giving an overview of the entire MLOPS strategies.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: éƒ¨ç½²ç­–ç•¥æœ‰å¾ˆå¤šå†…å®¹ï¼›åœ¨ä¸€ä¸ªåšå®¢ä¸­å‡ ä¹ä¸å¯èƒ½è¦†ç›–æ‰€æœ‰å†…å®¹ã€‚è¿™é‡Œæœ‰å¦ä¸€ä¸ªå¾ˆå¥½çš„åšå®¢æä¾›äº†æ•´ä¸ª MLOPS ç­–ç•¥çš„æ¦‚è¿°ã€‚
- en: '[](https://medium.com/aiguys/mlops-deploying-and-managing-models-at-scale-9a51f8fc0406?source=post_page-----b4cd84f86de1--------------------------------)
    [## MLOps: Managing AI models at Scale'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/aiguys/mlops-deploying-and-managing-models-at-scale-9a51f8fc0406?source=post_page-----b4cd84f86de1--------------------------------)
    [## MLOpsï¼šå¤§è§„æ¨¡ç®¡ç† AI æ¨¡å‹'
- en: Model building is excellent, but if we canâ€™t deploy these models then it becomes
    useless. Unlike Deep learning, findingâ€¦
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ„å»ºå¾ˆå‡ºè‰²ï¼Œä½†å¦‚æœæˆ‘ä»¬ä¸èƒ½éƒ¨ç½²è¿™äº›æ¨¡å‹ï¼Œå®ƒä»¬å°±ä¼šå˜å¾—æ— ç”¨ã€‚ä¸æ·±åº¦å­¦ä¹ ä¸åŒï¼Œæ‰¾åˆ°â€¦â€¦
- en: medium.com](https://medium.com/aiguys/mlops-deploying-and-managing-models-at-scale-9a51f8fc0406?source=post_page-----b4cd84f86de1--------------------------------)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/aiguys/mlops-deploying-and-managing-models-at-scale-9a51f8fc0406?source=post_page-----b4cd84f86de1--------------------------------)
- en: What is Quantization and TFLite?
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯é‡åŒ–å’Œ TFLiteï¼Ÿ
- en: '**Quantization** is a model compression technique in which we convert our weights
    to lower precision to **reduce the size of the model** thus making our models
    smaller and faster at inference. Quantization can greatly improve **speed** and
    is often used for edge deployment. Deploying a quantized model in a serverless
    fashion can be great cost-saving as this makes the AI model small enough to be
    used in a serverless fashion.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**é‡åŒ–**æ˜¯ä¸€ç§æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œåœ¨è¿™ç§æŠ€æœ¯ä¸­ï¼Œæˆ‘ä»¬å°†æƒé‡è½¬æ¢ä¸ºè¾ƒä½çš„ç²¾åº¦ï¼Œä»¥**å‡å°æ¨¡å‹çš„å¤§å°**ï¼Œä»è€Œä½¿æ¨¡å‹åœ¨æ¨æ–­æ—¶æ›´å°ã€æ›´å¿«ã€‚é‡åŒ–å¯ä»¥æ˜¾è‘—æé«˜**é€Ÿåº¦**ï¼Œå¹¶ä¸”é€šå¸¸ç”¨äºè¾¹ç¼˜éƒ¨ç½²ã€‚åœ¨æ— æœåŠ¡å™¨æ¨¡å¼ä¸‹éƒ¨ç½²é‡åŒ–æ¨¡å‹å¯ä»¥å¤§å¤§èŠ‚çœæˆæœ¬ï¼Œå› ä¸ºè¿™ä½¿å¾—
    AI æ¨¡å‹å°åˆ°è¶³ä»¥åœ¨æ— æœåŠ¡å™¨æ¨¡å¼ä¸‹ä½¿ç”¨ã€‚'
- en: '**NOTE:** People often think they need GPU instances to serve the AI models
    as they used GPU instances to train them, but thatâ€™s not true. Most AI applications
    with CPU instances and proper deployment strategy can serve even a billion users.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** äººä»¬å¸¸å¸¸è®¤ä¸ºéœ€è¦ GPU å®ä¾‹æ¥æœåŠ¡ AI æ¨¡å‹ï¼Œå› ä¸ºä»–ä»¬ç”¨ GPU å®ä¾‹è®­ç»ƒäº†è¿™äº›æ¨¡å‹ï¼Œä½†è¿™å¹¶ä¸æ­£ç¡®ã€‚å¤§å¤šæ•° AI åº”ç”¨ç¨‹åºé€šè¿‡ CPU
    å®ä¾‹å’Œé€‚å½“çš„éƒ¨ç½²ç­–ç•¥å¯ä»¥æœåŠ¡ç”šè‡³åäº¿ç”¨æˆ·ã€‚'
- en: Quantization is one of many ways to compress model size, there are a lot of
    other methodologies like pruning, weight sharing, etc.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: é‡åŒ–æ˜¯å‹ç¼©æ¨¡å‹å¤§å°çš„ä¼—å¤šæ–¹æ³•ä¹‹ä¸€ï¼Œè¿˜æœ‰å¾ˆå¤šå…¶ä»–æ–¹æ³•ï¼Œå¦‚å‰ªæã€æƒé‡å…±äº«ç­‰ã€‚
- en: 'Hereâ€™s an article detailing all the **model compression techniques**:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€ç¯‡æ–‡ç« è¯¦ç»†ä»‹ç»äº†æ‰€æœ‰çš„**æ¨¡å‹å‹ç¼©æŠ€æœ¯**ï¼š
- en: '[](https://medium.com/aiguys/reducing-deep-learning-size-16bed87cccff?source=post_page-----b4cd84f86de1--------------------------------)
    [## Deep learning model compression'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/aiguys/reducing-deep-learning-size-16bed87cccff?source=post_page-----b4cd84f86de1--------------------------------)
    [## æ·±åº¦å­¦ä¹ æ¨¡å‹å‹ç¼©'
- en: With each passing year, models are getting more complex and bigger. A lot of
    AI models developed in research labs neverâ€¦
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: éšç€æ¯å¹´æ¨¡å‹å˜å¾—è¶Šæ¥è¶Šå¤æ‚å’Œåºå¤§ã€‚å¾ˆå¤šåœ¨ç ”ç©¶å®éªŒå®¤å¼€å‘çš„ AI æ¨¡å‹ä»æœªâ€¦â€¦
- en: medium.com](https://medium.com/aiguys/reducing-deep-learning-size-16bed87cccff?source=post_page-----b4cd84f86de1--------------------------------)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/aiguys/reducing-deep-learning-size-16bed87cccff?source=post_page-----b4cd84f86de1--------------------------------)
- en: What is TFLite
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯ TFLite
- en: According to the TensorFlow website, â€œTensorFlow Lite is a set of tools that
    enables on-device machine learning by helping developers run their models on mobile,
    embedded, and edge devices.â€
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ® TensorFlow ç½‘ç«™ï¼Œâ€œTensorFlow Lite æ˜¯ä¸€å¥—å·¥å…·ï¼Œé€šè¿‡å¸®åŠ©å¼€å‘è€…åœ¨ç§»åŠ¨è®¾å¤‡ã€åµŒå…¥å¼è®¾å¤‡å’Œè¾¹ç¼˜è®¾å¤‡ä¸Šè¿è¡Œæ¨¡å‹ï¼Œå®ç°è®¾å¤‡ä¸Šçš„æœºå™¨å­¦ä¹ ã€‚â€
- en: There are many ways to quantize AI models; two main categorizations are **post-training
    quantization and quantization-aware training**. In the prior one, we normally
    train our models. After the training is complete, quantization is applied to model
    weights, whereas for the latter, during the training itself, quantization is active.
    Usually, quantized-aware training performs better than post-training quantization.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s directly jump into the code for quantization. We are using a post-training
    quantized image segmentation model for this blog. Given below image shows the
    architecture of our AI Pipeline.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e7a7de87920f7d11c6a6c85a15be31e8.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: 'AI Pipeline architecture (Img Src: Belongs to author)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'Iâ€™m making the following assumptions here:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: You already have an image segmentation model saved in .hdf5 or .h5 format.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If not, follow this tutorial from Keras official website: [https://keras.io/examples/vision/oxford_pets_image_segmentation/](https://keras.io/examples/vision/oxford_pets_image_segmentation/)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: You have a variable called ***train_input_img_paths*** storing paths to all
    the training images. Once again, you can follow the Keras official example link
    of Step 1.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have your own custom data loaders, modify the ***represetative_dataset()***
    method.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now we are ready to deploy our TFLite model in a serverless fashion using Google
    Cloud Run API.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Deploying TFLite model using GCP Cloud Run API
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need these resources and files to deploy our model and make predictions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Dockerfile
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: app.py
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: client.py
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: requirements.txt
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: quantized model
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Letâ€™s first understand the flow of deployment first.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: The serverless deployment flow starts with **containerizing** of our application
    app.py (we use docker here), then **pushing the docker image to a container registry**
    (Google container registry in our case); we need a container registry to ensure
    the versioning, availability, and security of our images. Then configuring and
    deploying it to a serverless platform (Google Cloud Run API), and then allowing
    the platform to handle the execution and scaling of our functions.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: The serverless mode of deployment abstracts away infrastructure management and
    provides automatic scaling, giving us more time to focus on developing and deploying
    our application code.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '**Dockerfile**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Overall, this Dockerfile sets up the necessary environment and dependencies
    for running the Flask application (`app.py`) inside a Docker container. It ensures
    that the required Python packages and the pre-trained model file are available
    within the container.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '**app.py**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**client.py**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Note:** When I trained my image segmentation model, I used BGR format (default
    mode of OpenCV); if you used RGB, remove ***line 30*** from the app.py.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Also, put your own endpoint URL in client.py ***line 8,*** which you will get
    after successfully deploying the Google Cloud RUN API.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: And latly, use the same version of Python in your Dockerfile and local environment
    to avoid breaking anything during the deployment.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '**requirements.txt**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Quantized model**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: And lastly, we need to keep our **model_quantized_float16.tflite** in the same
    folder as our app.py as we copy our quantized model in our docker image.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how my directory looks after collecting all the resources:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c64d26ef869ef20b754722938704489.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: 'Img Src: Belongs to author'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up Serverless
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step is to get the gcloud CLI (Command Line Interface), I used Windows
    itâ€™s pretty straightforward: [https://cloud.google.com/sdk/docs/install](https://cloud.google.com/sdk/docs/install)'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. Navigate to your folder using the standard CLI command
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 3\. Login into gcloud CLI
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This will open up a window in your browser and ask for a few permissions, allow
    that.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Setup a project in gcloud, better use the GUI interface for this.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'Hereâ€™s the link to create a GCP project:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://developers.google.com/workspace/guides/create-project?source=post_page-----b4cd84f86de1--------------------------------)
    [## Create a Google Cloud project | Google Workspace | Google for Developers'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: A Google Cloud project is required to use Google Workspace APIs and build Google
    Workspace add-ons or apps. Thisâ€¦
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: developers.google.com](https://developers.google.com/workspace/guides/create-project?source=post_page-----b4cd84f86de1--------------------------------)
    ![](../Images/d306e4ddae1d3bba879df811fc98a7fd.png)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'GCP project Dashboard (Img Src: Belongs to author)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Setup project ID in gcloud CLI, you can see your project ID in your dashboard.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 6\. Build container in gcloud CLI. Replace <PROJECT_ID> everywhere.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/52a2c7122581c08fe5dd3009d144c776.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: 'Building container (Img Src: Belongs to author)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Push Docker image to Container registry through gcloud CLI
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/08f5edf767b9bea3b1e51f7248bbd668.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: 'Google Container registry (Img Src: Belongs to author)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Deploy the Coud RUN API through gcloud CLI. This will ask to choose server
    locations and some other authentications, allow all of them.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/797936fcd94d0d2ef0284ffcb52f90a1.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: 'Model Deployed (Img Src: Belongs to author)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: If everything is successful, you will see a link in your gcloud CLI, that you
    need to paste into your client.py. Otherwise, go to logs and try to fix the errors.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d0b43da956b78aa1de4600633cebbd39.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: 'Cloud Run API console (Img Src: Belongs to author)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '**Key things to Note here:**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Itâ€™s almost guaranteed that something or other will break in this deployment;
    the biggest reason is version mismatch in packages.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Use the exact same version in your requirements.txt and Dockerfile as you used
    to train the model and quantize the model. Remember GCP runs quite behind the
    TFâ€™s and Pythonâ€™s latest version. Itâ€™s better to use an older version.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: I trained my model on Python 3.8.15; the rest are given in the requirements.txt.
    The errors in the logs are often unclear, so always use the exact same versions;
    if you canâ€™t find the required versions in GCP, change the version for your local
    environment.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Next, the biggest reason for failed deployment is that you havenâ€™t activated
    the required APIs or you donâ€™t have the required permissions and IAM roles. Itâ€™s
    better to use the account as owner with all the permissions enabled if you use
    GCP for the first time.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œéƒ¨ç½²å¤±è´¥çš„æœ€å¤§åŸå› æ˜¯ä½ æ²¡æœ‰æ¿€æ´»æ‰€éœ€çš„ API æˆ–è€…ä½ æ²¡æœ‰å¿…è¦çš„æƒé™å’Œ IAM è§’è‰²ã€‚å¦‚æœä½ ç¬¬ä¸€æ¬¡ä½¿ç”¨ GCPï¼Œæœ€å¥½ä½¿ç”¨æ‹¥æœ‰æ‰€æœ‰æƒé™çš„è´¦æˆ·ä½œä¸ºæ‰€æœ‰è€…ã€‚
- en: Making predictions
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿›è¡Œé¢„æµ‹
- en: Just run `client.py` in your gcloud CLI or your standard command prompt.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: åªéœ€åœ¨ä½ çš„ gcloud CLI æˆ–æ ‡å‡†å‘½ä»¤æç¤ºç¬¦ä¸‹è¿è¡Œ `client.py`ã€‚
- en: 'Hereâ€™s what my output looks like:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘çš„è¾“å‡ºç¤ºä¾‹ï¼š
- en: '![](../Images/11c8a54ffa4b58e442c58f79562e5f8c.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11c8a54ffa4b58e442c58f79562e5f8c.png)'
- en: 'Model prediction (Img Src: Belongs to author)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹é¢„æµ‹ï¼ˆå›¾ç‰‡æ¥æºï¼šä½œè€…æä¾›ï¼‰
- en: I trained a binary image segmentation model on some private data. Due to privacy,
    I can neither reveal the details of my model nor the data. But all the mentioned
    things should work with any image segmentation model.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨ä¸€äº›ç§äººæ•°æ®ä¸Šè®­ç»ƒäº†ä¸€ä¸ªäºŒåˆ†ç±»å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚ç”±äºéšç§åŸå› ï¼Œæˆ‘ä¸èƒ½é€éœ²æˆ‘çš„æ¨¡å‹æˆ–æ•°æ®çš„è¯¦ç»†ä¿¡æ¯ã€‚ä½†æ‰€æœ‰æåˆ°çš„å†…å®¹éƒ½åº”è¯¥é€‚ç”¨äºä»»ä½•å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚
- en: Versioning
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç‰ˆæœ¬æ§åˆ¶
- en: And lastly, if you need more resources from the start or want to minimize the
    cold start problem, we can create a new version of the same with just a few additional
    steps.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå¦‚æœä½ éœ€è¦ä»ä¸€å¼€å§‹å°±è·å¾—æ›´å¤šèµ„æºï¼Œæˆ–æƒ³è¦æœ€å°åŒ–å†·å¯åŠ¨é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å‡ ä¸ªé¢å¤–çš„æ­¥éª¤åˆ›å»ºåŒæ ·çš„æ–°ç‰ˆæœ¬ã€‚
- en: '**Go to your gcloud console in** **GUI > Search cloud run API > Select the
    deployed service > Click on edit and deploy new revision button**. And you will
    get the following options, choose according to your needs, save them, and automatically,
    a new version of your model will be set up for the next sets of requests.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**å‰å¾€ä½ çš„ gcloud æ§åˆ¶å°** **åœ¨ GUI > æœç´¢ cloud run API > é€‰æ‹©å·²éƒ¨ç½²çš„æœåŠ¡ > ç‚¹å‡»ç¼–è¾‘å¹¶éƒ¨ç½²æ–°ç‰ˆæœ¬æŒ‰é’®**ã€‚ä½ å°†çœ‹åˆ°ä»¥ä¸‹é€‰é¡¹ï¼Œæ ¹æ®ä½ çš„éœ€æ±‚è¿›è¡Œé€‰æ‹©ï¼Œä¿å­˜å®ƒä»¬ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨ä¸ºä¸‹ä¸€æ‰¹è¯·æ±‚è®¾ç½®æ¨¡å‹çš„æ–°ç‰ˆæœ¬ã€‚'
- en: '![](../Images/9ab775f6e179512f6bfe95c2d07eb9eb.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ab775f6e179512f6bfe95c2d07eb9eb.png)'
- en: 'Solving cold start problem (Img Src: Belongs to author)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: è§£å†³å†·å¯åŠ¨é—®é¢˜ï¼ˆå›¾ç‰‡æ¥æºï¼šä½œè€…æä¾›ï¼‰
- en: Conclusion
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Choosing the right strategy for deployment is crucial for cost saving.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€‰æ‹©æ­£ç¡®çš„éƒ¨ç½²ç­–ç•¥å¯¹èŠ‚çœæˆæœ¬è‡³å…³é‡è¦ã€‚
- en: We can make our models faster and smaller using the quantization techniques.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨é‡åŒ–æŠ€æœ¯ä½¿æ¨¡å‹æ›´å¿«æ›´å°ã€‚
- en: Serverless deployment with quantized model is a great strategy and can easily
    serve many requests without using costly GPU instances.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é‡åŒ–æ¨¡å‹çš„æ— æœåŠ¡å™¨éƒ¨ç½²æ˜¯ä¸€ç§å¾ˆå¥½çš„ç­–ç•¥ï¼Œå¯ä»¥è½»æ¾å¤„ç†è®¸å¤šè¯·æ±‚è€Œä¸ä½¿ç”¨æ˜‚è´µçš„ GPU å®ä¾‹ã€‚
- en: Serverless takes away the hastle of scaling.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ— æœåŠ¡å™¨æ¶æ„æ¶ˆé™¤äº†æ‰©å±•çš„éº»çƒ¦ã€‚
- en: Thanks for your time and patience, happy learning â¤. Follow me for more of such
    awesome content.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢ä½ çš„æ—¶é—´å’Œè€å¿ƒï¼Œç¥å­¦ä¹ æ„‰å¿« â¤ã€‚å…³æ³¨æˆ‘ï¼Œè·å–æ›´å¤šè¿™æ ·çš„ç²¾å½©å†…å®¹ã€‚
- en: '**Hereâ€™s my reading list for MLOps, discussing several other key concepts and
    strategies:**'
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**è¿™æ˜¯æˆ‘çš„ MLOps é˜…è¯»æ¸…å•ï¼Œè®¨è®ºäº†å‡ ä¸ªå…¶ä»–å…³é”®æ¦‚å¿µå’Œç­–ç•¥ï¼š**'
- en: '![Vishal Rajput](../Images/a96ad11c1783cef37c01eb5b36ffbe0d.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![Vishal Rajput](../Images/a96ad11c1783cef37c01eb5b36ffbe0d.png)'
- en: '[Vishal Rajput](https://vishal-ai.medium.com/?source=post_page-----b4cd84f86de1--------------------------------)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[Vishal Rajput](https://vishal-ai.medium.com/?source=post_page-----b4cd84f86de1--------------------------------)'
- en: MLOps
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLOps
- en: '[View list](https://vishal-ai.medium.com/list/mlops-ff7f2453a835?source=post_page-----b4cd84f86de1--------------------------------)10
    stories![](../Images/f43ac8226531c0d15a3fdd2c774f939c.png)![](../Images/f6516179d1713c465b1fff8a3017344f.png)![](../Images/65ceb6ecc46ce9f7e4159979b811b308.png)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹åˆ—è¡¨](https://vishal-ai.medium.com/list/mlops-ff7f2453a835?source=post_page-----b4cd84f86de1--------------------------------)10
    ä¸ªæ•…äº‹![](../Images/f43ac8226531c0d15a3fdd2c774f939c.png)![](../Images/f6516179d1713c465b1fff8a3017344f.png)![](../Images/65ceb6ecc46ce9f7e4159979b811b308.png)'
