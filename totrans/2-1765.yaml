- en: Random Numbers in Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/random-numbers-in-machine-learning-9cb5d83d078e](https://towardsdatascience.com/random-numbers-in-machine-learning-9cb5d83d078e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All about pseudo-random numbers, seeding, and reproducibility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@caroline.arnold_63207?source=post_page-----9cb5d83d078e--------------------------------)[![Caroline
    Arnold](../Images/2560e106ba9deda7889c7d253792d814.png)](https://medium.com/@caroline.arnold_63207?source=post_page-----9cb5d83d078e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9cb5d83d078e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9cb5d83d078e--------------------------------)
    [Caroline Arnold](https://medium.com/@caroline.arnold_63207?source=post_page-----9cb5d83d078e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9cb5d83d078e--------------------------------)
    ·8 min read·Oct 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cede17b2b0c90191d192b48d7de7c688.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Riho Kroll](https://unsplash.com/@rihok?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning relies on statistics, and random numbers are important to the
    performance of many steps in the data processing and model training pipeline.
    Modern machine learning frameworks provide abstractions and functions that implement
    randomness under the hood, and for us as data scientists and machine learning
    engineers, the details of random number generation often remain obscure.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, I want to shed some light on random numbers in machine learning.
    You will read about:'
  prefs: []
  type: TYPE_NORMAL
- en: 3 examples of the use of random numbers in machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating (pseudo-)random numbers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fixing random numbers by seeding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reproducible machine learning: necessary lines of code for scikit-learn, tensorflow,
    and pytorch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this article, you will know what happens when you use random numbers
    in your machine learning pipeline, and you will learn the necessary lines of code
    to ensure reproducibility of your machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Examples for the use of random numbers in machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To illustrate the importance of random numbers, we discuss three examples where
    they are relevant along the machine learning pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Creating train/test splits of a dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Weight initialization in a neural network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choosing minibatches during training
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Train/test split** Splitting your dataset into training and test data is
    one of the most important steps in evaluating the performance of a machine learning
    algorithm. We are interested in creating models that generalize well to data not
    used during training. To this end, a collection of data samples is divided into
    at least two disjoint sets.'
  prefs: []
  type: TYPE_NORMAL
- en: The training data is used to train the algorithm, i.e. to iteratively fix the
    model parameters. The test data is used to validate the algorithm by applying
    a trained model to test data and reporting appropriate metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9a5a0010d6e5ee72993ca18b135788a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Random split of a dataset into train and test data. Image: Author.'
  prefs: []
  type: TYPE_NORMAL
- en: The popular scikit-learn function `sklearn.model_selection.train_test_split`
    uses random numbers. Under the hood, it creates an array of indices corresponding
    to the length of the data set. The indices are then randomly assigned to the train
    and test data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Weight initialization**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The layers of a neural network contain parameters that are adjusted during
    training. For linear layers, these parameters are weights and biases. They are
    initially assigned random values. Consider the example of a linear layer with
    4 neurons connected to an output layer with 3 potential classes: This layer corresponds
    to a matrix *W* of 4 x 3 weights.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/59fffcc5ca648fe4d7673e49fed816f5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Three different random initializations for the 4 x 3 matrix W. Image: Author.'
  prefs: []
  type: TYPE_NORMAL
- en: The initialization of the weights is crucial for the convergence of the model
    training. If all weights had the same value, the backpropagation algorithm would
    see no reason to treat each neuron differently when updating the weights.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, when the model is first instantiated, the weights are chosen randomly.
    Typically, the random numbers have a reasonable statistical distribution behind
    them. Xavier Glorot found in 2010 that training a feed-forward neural network
    of layer dimension n x m is improved when the network weights are initialized
    from the uniform distribution [1].
  prefs: []
  type: TYPE_NORMAL
- en: '**Choosing training batches** In general, it is neither computationally feasible
    nor advisable to use the entire training dataset to update the model parameters
    in one go. Therefore, the training dataset is divided into minibatches of fixed
    size. The dataloader creates these minibatches and can randomly shuffle the data.
    This is to prevent data from entering training in a biased way, e.g. in a temporal
    order because of the way it was collected.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9245d04a92db4b56d02c9df60e9965fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Minibatches of size 4 formed from a dataset with 12 samples. Top: Dataset is
    not shuffled. Bottom: Dataset is shuffled randomly. Image: Author.'
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic gradient descent relies on the randomness of these minibatches. By
    presenting random subsets of the training data to the algorithm, each backpropagation
    step emphasizes a slightly different aspect of the training data. This avoids
    getting stuck in local minima during training.
  prefs: []
  type: TYPE_NORMAL
- en: Pseudo-random numbers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Modern programming languages and machine learning frameworks provide tools for
    the developer to generate random numbers without worrying about the underlying
    algorithm. To generate a sequence of 100 uniform random numbers between 0 and
    1 in Python, simply type
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: But what actually happens under the hood when you execute this line of code?
    Enter the Random Number Generators (RNGs).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A standard computer algorithm produces predictable results, which is exactly
    the opposite of what we want— a random sequence of numbers. What we are looking
    for is an algorithm that produces sequences of numbers that are *hard to predict*.
    Note that *hard* does not mean *impossible*! So we need a source of entropy and
    a cryptographic algorithm. For a true random number generator, the source of entropy
    could be informed by the environment, e.g. by taking a sensor temperature or the
    decay of a radioactive particle as input [3].
  prefs: []
  type: TYPE_NORMAL
- en: For machine learning, we do not need the same high-end randomness that would
    be required for a cryptographic application. Python — specifically the numpy library
    — implements an RNG to generate pseudo-random numbers.
  prefs: []
  type: TYPE_NORMAL
- en: In everyday language, a pseudo-random number sequence is close enough to a truly
    random sequence that the lack of complete randomness does not affect the purpose
    of the algorithm. The correlation length of the sequence must be sufficient for
    the application.
  prefs: []
  type: TYPE_NORMAL
- en: From an initial seed, a function *f* is applied to generate a state. Another
    function *g* is applied to generate a random number. This process is repeated
    as many times as necessary. The functions *f, g* should not be invertible. Also,
    the different states must provide enough variability to really generate a lot
    of different random numbers, the so-called sequence length.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/58d59cfb5bde0f3915d838c17a967198.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Pseudo-random number generator following [4]. Image: Author.'
  prefs: []
  type: TYPE_NORMAL
- en: The underlying algorithm used in Python is called Mersenne Twister [4]. It provides
    sequences of random numbers for each state and is computationally more efficient
    than the simple implementation shown above. It is not cryptographically secure,
    but it is fast and sufficiently powerful.
  prefs: []
  type: TYPE_NORMAL
- en: Random number distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often we are interested in drawing random numbers that follow a certain statistical
    distribution. For example, the uniform distribution assigns equal probability
    to all numbers in a given interval. The normal distribution follows a Gaussian
    curve with a fixed mean and variance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7055ffbb5bf73a5f41c0b228d5808f2a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Normal distribution (left) and uniform distribution (right) of 1000 random
    numbers. Image: Author.'
  prefs: []
  type: TYPE_NORMAL
- en: Distributions of random numbers can be transformed into each other by their
    cumulative distribution function (CDF). Thus, it is sufficient to be able to draw
    uniform numbers, and any other distribution can be generated from them. The details
    are beyond the scope of this article, for a start we recommend [5].
  prefs: []
  type: TYPE_NORMAL
- en: Fixing random numbers by seeding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As explained above, the seed is an integer that serves as input for the pseudo-random
    number generator.
  prefs: []
  type: TYPE_NORMAL
- en: For a fixed seed, the sequence of pseudo-random numbers generated by the presented
    algorithm is always the same.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We can use this insight to achieve two different objectives.
  prefs: []
  type: TYPE_NORMAL
- en: '**Choose a new random seed** When you choose a new random seed, all aspects
    of the machine learning pipeline that rely on random numbers are initialized with
    different values. While the underlying distributions and dataset sizes remain
    unchanged, the training process can be affected.'
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, your training procedure should not be too dependent on the choice of
    random seed. You want to make sure that the training converges regardless of the
    initialization, and not rely on golden runs that magically produce great results
    without ever being reproducible.
  prefs: []
  type: TYPE_NORMAL
- en: '**Fix the random seed** Fixing the random seed is particularly popular for
    teaching purposes. When I create a simple model for a small dataset, the probability
    that my results depend on the random seed is quite high. Whenever I prepare teaching
    materials, such as a Jupyter notebook, for students, I fix the random seed so
    that they get the same results. This reduces confusion and allows students to
    focus on the new concepts rather than the less important digits of the metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c12caaf7bcc716e1731fce5a3656db6.png)'
  prefs: []
  type: TYPE_IMG
- en: Clones look exactly the same. Photo by [Phil Shaw](https://unsplash.com/@phillshaw?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Reproducibility in machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have now learned that random numbers are critical to the performance of the
    machine learning process. However, if we want exact reproducibility of our algorithm,
    we need to fix the random seeds.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to realize that the random seed must be fixed separately for
    each library that uses random numbers. For example, even if we fix the `numpy`
    random seed by `np.random.seed(789123)` , the `torch` random seed will not be
    affected by that and training will not be reproducible.
  prefs: []
  type: TYPE_NORMAL
- en: In the following, we summarize the necessary calls to fix random seeds for different
    popular machine learning frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scikit-Learn** uses numpy’s random number generator throughout. To fix the
    random seed for this framework, it is sufficent to set'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The popular framework offers another function to access random number generators,
    [*check_random_state*](https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_random_state.html#sklearn.utils.check_random_state).
    To generate 1000 random numbers quickly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Tensorflow 2** provides its own random number generator module in `tf.random`
    . According to the [documentation](https://www.tensorflow.org/guide/random_numbers),
    fixing random seeds there ensures reproducibility. However, it is not guaranteed
    across different versions of tensorflow. To generate the same pseudo-random numbers,
    use the `stateless_XXX` functions from the `tf.random` module, e.g., for a series
    of 1000 random numbers following a normal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Pytorch** [controls the random number generation](https://pytorch.org/docs/stable/notes/randomness.html)
    in a way that is very similar to the numpy statements. The following will set
    the seed for all procedures that rely on this random number generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: However, they point out that some functions may depend on the numpy random number
    generator, so it is advisable to fix that seed as well.
  prefs: []
  type: TYPE_NORMAL
- en: The Pytorch documentation points out a problem with full reproducibility. Not
    only is it impossible to reproduce the random numbers exactly between versions,
    but the numbers may also depend on the hardware. Using different GPUs with different
    CUDA toolkits or CPUs can lead to different random number sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Random numbers are critical to the performance of machine learning algorithms.
    They ensure that datasets are partitioned in an unbiased manner, improve the generalizability
    of the algorithm, and increase the convergence of training procedures.
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, sequences of random numbers are generated by so-called RNGs,
    established algorithms that are able to provide random numbers following different
    distributions.
  prefs: []
  type: TYPE_NORMAL
- en: The random seed can be used to fix the sequence of random numbers. This makes
    the machine learning algorithm reproducible, which is especially useful for teaching
    purposes and for academic papers. Finally, remember that if you are using a mix
    of machine learning frameworks in your project, you may need to set the random
    seed for all of them!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Xavier Glorot and Yoshua Bengio, “Understanding the difficulty of training deep
    feedforward neural networks”, *Proceedings of the Thirteenth International Conference
    on Artificial Intelligence and Statistics*, PMLR 9:249–256, 2010\. [[paper](http://proceedings.mlr.press/v9/glorot10a.html)]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://numpy.org/doc/stable/reference/random/index.html](https://numpy.org/doc/stable/reference/random/index.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.redhat.com/en/blog/understanding-random-number-generators-and-their-limitations-linux](https://www.redhat.com/en/blog/understanding-random-number-generators-and-their-limitations-linux)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mersenne Twister in detail: [https://www.cryptologie.net/article/331/how-does-the-mersennes-twister-work/](https://www.cryptologie.net/article/331/how-does-the-mersennes-twister-work/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.wikiwand.com/en/Inverse_transform_sampling](https://www.wikiwand.com/en/Inverse_transform_sampling)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Tensorflow: [https://www.tensorflow.org/guide/random_numbers](https://www.tensorflow.org/guide/random_numbers)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pytorch: [https://pytorch.org/docs/stable/notes/randomness.html](https://pytorch.org/docs/stable/notes/randomness.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
