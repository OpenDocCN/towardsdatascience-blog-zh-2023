- en: 'Avoid Overfitting in Neural Networks: a Deep Dive'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 避免神经网络过拟合：深度探讨
- en: 原文：[https://towardsdatascience.com/avoid-overfitting-in-neural-networks-a-deep-dive-b4615a2d9507](https://towardsdatascience.com/avoid-overfitting-in-neural-networks-a-deep-dive-b4615a2d9507)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/avoid-overfitting-in-neural-networks-a-deep-dive-b4615a2d9507](https://towardsdatascience.com/avoid-overfitting-in-neural-networks-a-deep-dive-b4615a2d9507)
- en: Learn how to implement regularization techniques to boost performances and prevent
    Neural Network overfitting
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何实施正则化技术以提升性能并防止神经网络过拟合
- en: '[](https://medium.com/@riccardo.andreoni?source=post_page-----b4615a2d9507--------------------------------)[![Riccardo
    Andreoni](../Images/5e22581e419639b373019a809d6e65c1.png)](https://medium.com/@riccardo.andreoni?source=post_page-----b4615a2d9507--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b4615a2d9507--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b4615a2d9507--------------------------------)
    [Riccardo Andreoni](https://medium.com/@riccardo.andreoni?source=post_page-----b4615a2d9507--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@riccardo.andreoni?source=post_page-----b4615a2d9507--------------------------------)[![Riccardo
    Andreoni](../Images/5e22581e419639b373019a809d6e65c1.png)](https://medium.com/@riccardo.andreoni?source=post_page-----b4615a2d9507--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b4615a2d9507--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b4615a2d9507--------------------------------)
    [Riccardo Andreoni](https://medium.com/@riccardo.andreoni?source=post_page-----b4615a2d9507--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b4615a2d9507--------------------------------)
    ·10 min read·Nov 30, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b4615a2d9507--------------------------------)
    ·10分钟阅读·2023年11月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/9b99bb683b9b10973ea4a001da99621b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b99bb683b9b10973ea4a001da99621b.png)'
- en: 'Image source: [unsplash.com](https://unsplash.com/photos/multicolored-illustration-gpiKdZmDQig).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[unsplash.com](https://unsplash.com/photos/multicolored-illustration-gpiKdZmDQig).
- en: 'When training a deep neural network, it’s often troublesome to achieve the
    same performances on **both the training and validation sets**. A considerably
    higher error on the validation set is a **clear flag for overfitting**: the network
    has become too specialized in the training data. In this article, I provide a
    comprehensive guide on how to bypass this issue.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练深度神经网络时，往往很难在**训练集和验证集上获得相同的性能**。验证集上明显较高的误差是**过拟合的明显标志**：网络在训练数据上过于专业化。本文提供了绕过这一问题的综合指南。
- en: Overfitting Neural Networks
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的过拟合
- en: When dealing with any machine learning application, it’s important to have a
    clear understanding of the **bias and variance of the model**. In traditional
    machine learning algorithms, we talk about the [**bias vs. variance tradeoff**](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#:~:text=In%20statistics%20and%20machine%20learning,bias%20in%20the%20estimated%20parameters.),
    which consists of the struggle of minimizing both the **variance** and the **bias**
    of a model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理任何机器学习应用时，清楚了解**模型的偏差和方差**是非常重要的。在传统机器学习算法中，我们讨论[**偏差与方差权衡**](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#:~:text=In%20statistics%20and%20machine%20learning,bias%20in%20the%20estimated%20parameters.)，即在最小化模型的**方差**和**偏差**之间的斗争。
- en: In order to reduce the bias of a model (i.e. its error from erroneous assumptions),
    we need a **more complex model**. On the contrary, reducing the model’s variance
    (the sensitivity of the model in capturing the variations of the training data),
    implies a **more simple model**. It is straightforward that the bias vs. variance
    tradeoff, in traditional machine learning, derives from the conflict of necessitating
    both a **more complex and a simpler model at the same time**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少模型的偏差（即模型由于错误假设产生的误差），我们需要一个**更复杂的模型**。相反，减少模型的方差（模型捕捉训练数据变化的敏感度）则意味着**更简单的模型**。显然，在传统机器学习中，偏差与方差的权衡源于同时需要一个**更复杂和更简单的模型**的冲突。
- en: In the **Deep Learning era,** we have tools to reduce just the model’s variance
    without hurting the model’s bias or, on the contrary, to reduce the bias without
    increasing the variance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在**深度学习时代**，我们拥有工具来仅仅减少模型的方差，而不影响模型的偏差，或者反过来，在不增加方差的情况下减少偏差。
- en: Before exploring the different techniques used to prevent the overfitting of
    a neural network, it’s important to clarify what high variance or high bias means.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在探讨用于防止神经网络过拟合的不同技术之前，明确什么是高方差或高偏差是很重要的。
- en: 'Consider a common neural network task such as image recognition, and think
    over a neural network that recognizes the presence of pandas in a picture. We
    can confidently assess that a human can carry out this task with a near 0% error.
    As a consequence, this is a reasonable benchmark for the accuracy of the image
    recognition network. After training the neural network on the training set and
    evaluating its performances on both the training and validation sets, we may come
    up with these different results:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以图像识别为例，考虑一个识别图片中是否有熊猫的神经网络。我们可以自信地评估，一个人可以以接近0%的误差完成这个任务。因此，这对图像识别网络的准确率来说是一个合理的基准。在对训练集进行训练并在训练集和验证集上评估其性能后，我们可能会得到以下不同的结果：
- en: Train Error = 20% and Validation Error = 22%
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练误差 = 20% 和 验证误差 = 22%
- en: Train Error = 1% and Validation Error = 15%
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练误差 = 1% 和 验证误差 = 15%
- en: Train Error = 0.5% and Validation Error = 1%
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练误差 = 0.5% 和 验证误差 = 1%
- en: Train Error = 20% and Validation Error = 30%
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练误差 = 20% 和 验证误差 = 30%
- en: 'The first example is a typical instance of **high bias**: the error is large
    on **both the train and validation sets**. Conversely, the second example suffers
    from **high variance**, having a much lower accuracy when dealing with data **the
    model didn’t learn from**. The third result represents **low variance and bias**,
    the model can be considered valid. Finally, the fourth example shows a case of
    **both high bias and variance**: not only the training error is large when compared
    with the benchmark, but the validation error is also higher.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个示例是**高偏差**的典型实例：**训练集和验证集上的误差都很大**。相反，第二个示例则遭遇了**高方差**，当处理**模型未学习到的数据**时准确率要低得多。第三个结果代表了**低方差和偏差**，模型可以被认为是有效的。最后，第四个示例展示了**高偏差和高方差**的情况：不仅训练误差在与基准比较时很大，而且验证误差也更高。
- en: '![](../Images/f8e21e27da658ab098e20e6dc05b9597.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8e21e27da658ab098e20e6dc05b9597.png)'
- en: Image by the author.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: From now on, I will present several techniques of regularization, used to reduce
    the model’s overfitting to the training data. They are beneficial for cases 2\.
    and 4\. of the previous example.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在起，我将介绍几种正则化技术，用于减少模型对训练数据的过拟合。这些技术对前面例子中的第2和第4种情况很有帮助。
- en: L1 and L2 Regularization
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L1和L2正则化
- en: Similar to classical regression algorithms (linear, logistic, polynomial, etc.),
    [**L1 and L2 regularizations**](/l1-and-l2-regularization-methods-ce25e7fc831c)
    are also employed to prevent overfitting in high-variance neural networks. In
    order to keep this article short and on-point, I won’t recall how L1 and L2 regularizations
    work on regression algorithms, but you can check [this article](https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization)
    for additional information.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于经典的回归算法（线性、逻辑、分式等），[**L1和L2正则化**](/l1-and-l2-regularization-methods-ce25e7fc831c)也被用来防止高方差神经网络的过拟合。为了使本文简洁明了，我不会回顾L1和L2正则化在回归算法中的工作原理，但你可以查看[这篇文章](https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization)获取更多信息。
- en: The idea behind the L1 and L2 regularization techniques is to **constrain the
    model’s weights** to be smaller or to **shrink some of them** to 0.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: L1和L2正则化技术的理念是**约束模型的权重**使其变小或**将一些权重**缩小到0。
- en: 'Consider the cost function J of a classical deep neural network:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个经典深度神经网络的成本函数J：
- en: '![](../Images/74851e40e220fbd23aa00087890900a7.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74851e40e220fbd23aa00087890900a7.png)'
- en: The cost function J is, of course, a function of the weights and biases of each
    layer 1, …, L. m is the number of training examples and ℒ is the loss function.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数J显然是每层1, …, L的权重和偏差的函数。m是训练样本的数量，ℒ是损失函数。
- en: L1 Regularization
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L1正则化
- en: 'In L1 Regularization we add the following term to the cost function J:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在L1正则化中，我们将以下项添加到成本函数J中：
- en: '![](../Images/6708fd5d805a36ef04965644cd8919aa.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6708fd5d805a36ef04965644cd8919aa.png)'
- en: 'where the matrix norm is the sum of the absolute value of the weights for each
    layer 1, …, L of the network:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中矩阵范数是网络中每层1, …, L的权重绝对值的总和：
- en: '![](../Images/e0840263ec9997187da46ac6795bb596.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0840263ec9997187da46ac6795bb596.png)'
- en: 'λ is the **regularization term**. It’s a hyperparameter that must be **carefully
    tuned**. λ directly controls the impact of the regularization: as λ increases,
    the effects on the weights shrinking are more severe.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: λ 是**正则化项**。它是一个必须**仔细调整**的超参数。λ 直接控制正则化的影响：随着 λ 的增加，对权重收缩的影响会更严重。
- en: 'The complete cost function under L1 Regularization becomes:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: L1 正则化下的完整成本函数变为：
- en: '![](../Images/8696d7d55275f3fef2b4ca9d1585bfb8.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8696d7d55275f3fef2b4ca9d1585bfb8.png)'
- en: For λ=0, the effects of L1 Regularization are null. Instead, choosing a value
    of λ which is too big, will over-simplify the model, probably resulting in an
    underfitting network.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 λ=0，L1 正则化的效果为零。相反，选择一个过大的 λ 值将过度简化模型，可能导致欠拟合网络。
- en: L1 Regularization can be considered as a sort of neuron selection process because
    it would bring to zero the weights of some hidden neurons.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: L1 正则化可以被视为一种神经元选择过程，因为它会将一些隐藏神经元的权重变为零。
- en: L2 Regularization
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L2 正则化
- en: 'In L2 Regularization, the term we add to the cost function is the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在 L2 正则化中，我们添加到成本函数中的项如下：
- en: '![](../Images/b08ca79a0cf0042384850dd21cca9cd4.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b08ca79a0cf0042384850dd21cca9cd4.png)'
- en: 'In this case, the regularization term is the **squared norm of the weights**
    of each network’s layer. This matrix norm is called [Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm)
    and, explicitly, it’s computed as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，正则化项是每个网络层的**权重的平方范数**。这个矩阵范数被称为 [Frobenius 范数](https://en.wikipedia.org/wiki/Matrix_norm)，显式地计算方法如下：
- en: '![](../Images/771b8017371792e5895c8f3b36983455.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/771b8017371792e5895c8f3b36983455.png)'
- en: Please note that the weight matrix relative to layer l has n^{[l]} rows and
    n^{[l-1]} columns.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，相对于第 l 层的权重矩阵有 n^{[l]} 行和 n^{[l-1]} 列。
- en: 'Finally, the complete cost function under L2 Regularization becomes:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，L2 正则化下的完整成本函数变为：
- en: '![](../Images/20df25a8f8630e67332916e96f232a1e.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20df25a8f8630e67332916e96f232a1e.png)'
- en: Again, λ is the **regularization term** and for λ=0 the effects of L2 Regularization
    are null.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，λ 是**正则化项**，当 λ=0 时，L2 正则化的效果为零。
- en: L2 Regularization brings towards zero the values of the weights, resulting in
    a more simple model.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: L2 正则化使权重的值趋近于零，从而得到一个更简单的模型。
- en: How do L1 and L2 Regularization reduce overfitting?
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L1 和 L2 正则化如何减少过拟合？
- en: 'L1 and L2 Regularization techniques have positive effects on overfitting to
    the training data for two reasons:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: L1 和 L2 正则化技术对训练数据的过拟合有积极的影响，原因有二：
- en: The weights of some hidden units become closer (or equal) to 0\. As a consequence,
    their effect is weakened and the resulting network is simpler because it’s closer
    to a **smaller network**. As stated in the introduction, a simpler network is
    less prone to overfitting.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些隐藏单元的权重变得更接近（或等于）0。结果是，它们的影响被削弱，最终的网络更简单，因为它更接近于**较小的网络**。正如引言中所述，较简单的网络更不容易过拟合。
- en: For smaller weights, also the input z of the activation function of a hidden
    neuron becomes smaller. For values close to 0, many activation functions behave
    **linearly**.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于较小的权重，隐藏神经元激活函数的输入 z 也变得更小。对于接近 0 的值，许多激活函数表现为**线性**。
- en: 'The second reason is not trivial and deserves an expansion. Consider a hyperbolic
    tangent (tanh) activation function, whose graph is the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原因并不简单，需要进一步展开。考虑一个双曲正切（tanh）激活函数，其图形如下：
- en: '![](../Images/8a77bb53e829a09ca28df59431b20cab.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a77bb53e829a09ca28df59431b20cab.png)'
- en: Image by the author.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: 'From the function plot we can see that if the input value x is small, the function
    tanh(x) behaves **almost linearly**. When tanh is used as the activation function
    of a neural network’s hidden layers, the input value is:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从函数图中我们可以看到，如果输入值 x 很小，函数 tanh(x) 的表现是**几乎线性的**。当 tanh 被用作神经网络隐藏层的激活函数时，输入值为：
- en: '![](../Images/fef52ba778f11544a14bbc260189c706.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fef52ba778f11544a14bbc260189c706.png)'
- en: which for small weights w is also **close to zero**.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于小权重 w 也**接近零**。
- en: If each layer of the neural network is linear, we can prove that the whole network
    behaves linearly. Thus, constraining some of the hidden units to mimic linear
    functions, leads to a simpler network and, as a consequence, helps to prevent
    overfitting.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果神经网络的每一层都是线性的，我们可以证明整个网络表现为线性。因此，约束一些隐藏单元以模拟线性函数，会导致网络更简单，从而有助于防止过拟合。
- en: A simpler model often is [not capable to capture the noise](/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036)
    in the training data and therefore, **overfitting is less frequent**.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 更简单的模型通常[无法捕捉训练数据中的噪声](/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036)，因此，**过拟合的情况较少**。
- en: Dropout
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dropout
- en: The idea of [**dropout regularization**](https://www.analyticsvidhya.com/blog/2022/08/dropout-regularization-in-deep-learning/#:~:text=In%20machine%20learning%2C%20%E2%80%9Cdropout%E2%80%9D,are%20codependent%20with%20one%20another.)
    is to **randomly remove some nodes** in the network. Before the training process,
    we set a probability (suppose p = 50%) for each node of the network. During the
    training phase, each node has a p **probability of being turned off**. The dropout
    process is random, and it is performed separately for each training example. As
    a consequence, each training example might be trained on a different network.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[**dropout正则化**](https://www.analyticsvidhya.com/blog/2022/08/dropout-regularization-in-deep-learning/#:~:text=In%20machine%20learning%2C%20%E2%80%9Cdropout%E2%80%9D,are%20codependent%20with%20one%20another.)的理念是**随机移除网络中的某些节点**。在训练过程开始之前，我们为网络中的每个节点设置一个概率（假设
    p = 50%）。在训练阶段，每个节点有 p **被关闭的概率**。dropout过程是随机的，并且是为每个训练样本单独执行的。因此，每个训练样本可能会在不同的网络上进行训练。'
- en: As for L2 Regularization, the result of dropout regularization is a simpler
    network, and a simpler network leads to a less complex model.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 与L2正则化类似，dropout正则化的结果是一个更简单的网络，而更简单的网络会导致一个更简单的模型。
- en: '![](../Images/a1949c771af16c058a1b49153f38e052.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1949c771af16c058a1b49153f38e052.png)'
- en: The effect of dropout on a simple network. Image by the author.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 简单网络上dropout的效果。图像由作者提供。
- en: Dropout in practice
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际中的dropout
- en: In this brief section, I show how to implement **Dropout regularization in practice**.
    I will go through a few simple lines of code (python). If you are only interested
    in the general theory of regularization, you can easily skip this section.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一简短的部分中，我展示了如何**在实践中实现Dropout正则化**。我将通过几行简单的代码（python）进行说明。如果你只对正则化的一般理论感兴趣，可以轻松跳过这一部分。
- en: 'Suppose we have stored the activation values of the network’s layer 4 in the
    [NumPy](https://numpy.org/) array `a4`. First, we create the auxiliary vector
    `d4`:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经将网络第4层的激活值存储在[NumPy](https://numpy.org/)数组`a4`中。首先，我们创建辅助向量`d4`：
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The vector `d4` has the same dimensions as `a4` and contains the values `True`
    or `False` based on the probability `keep_prob`. If we set a keeping probability
    of 70%, that is the probability that a given hidden unit is kept, and therefore,
    the probability of having `True` value on a given element of `d4`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 向量`d4`的维度与`a4`相同，并根据概率`keep_prob`包含`True`或`False`的值。如果我们设置了70%的保留概率，这就是给定隐藏单元被保留的概率，因此，在`d4`的某个元素上具有`True`值的概率。
- en: 'We apply the auxiliary vector `d4` to the activations `a4`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将辅助向量`d4`应用于激活`a4`：
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, we need to scale the modified vector `a4` by the `keep_prob` value:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要通过`keep_prob`值来缩放修改后的向量`a4`：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This last operation is needed to compensate for the reduction of units in the
    layer. Carrying out this operation during the training process allows us not to
    apply dropout during the test phase.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最后的操作是为了补偿层中单位的减少。在训练过程中执行此操作可以让我们在测试阶段不应用dropout。
- en: How does dropout reduce overfitting?
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dropout是如何减少过拟合的？
- en: Dropout has the effect of temporarily **transforming the network into a smaller
    one**, and we know that smaller networks are **less complex and less prone to
    overfitting**.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout的效果是暂时**将网络转变为一个更小的网络**，我们知道较小的网络**更简单且不易过拟合**。
- en: Consider the network illustrated above, and focus on the first unit of the second
    layer. Because some of its inputs may be temporarily shut down due to dropout,
    the unit **can’t always rely on them** during the training phase. As a consequence,
    the hidden unit is encouraged to spread its weights across its inputs. Spreading
    the weights has the effect of decreasing the squared norm of the weight matrix,
    resulting in a **sort of L2 regularization**.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以上图所示的网络为例，关注第二层的第一个单元。由于某些输入可能由于dropout而被临时关闭，这个单元**不能总是依赖它们**。因此，隐藏单元被鼓励将其权重分散到各个输入上。权重的分散效果是降低权重矩阵的平方范数，从而产生一种**类似于L2正则化**的效果。
- en: Setting the keeping probability is a fundamental step for an effective dropout
    regularization. Typically, the keeping probability is set separately for each
    layer of the neural network. For layers with a large weight matrix, we usually
    set a smaller keeping probability because, at each step, we want to conserve proportionally
    fewer weights with respect to smaller layers.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 设置保留概率是有效 dropout 正则化的一个基本步骤。通常，保留概率是为神经网络的每一层单独设置的。对于权重矩阵较大的层，我们通常设置较小的保留概率，因为在每一步中，我们希望相对于较小的层保留比例较少的权重。
- en: Other Regularization Techniques
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他正则化技术
- en: In addition to L1/L2 regularization and dropout, there exist **other regularization
    techniques**. Two of them are **data augmentation** and **early stopping**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 L1/L2 正则化和 dropout，还有**其他正则化技术**。其中两种是**数据增强**和**早停**。
- en: From the theory, we know that training a network on more data has positive effects
    on reducing high variance. As getting more data is often a demanding task, data
    augmentation is a technique that, for some applications, allows machine learning
    practitioners to get more data almost for free. In computer vision, data augmentation
    provides a **larger training set** by flipping, zooming, and translating the original
    images. In the case of digit recognition, we can also impose distortion on the
    images.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论上讲，我们知道在更多数据上训练网络对减少高方差有积极影响。由于获取更多数据通常是一项艰巨的任务，因此数据增强是一种技术，它在某些应用中几乎可以“免费”获得更多数据。在计算机视觉中，数据增强通过翻转、缩放和移动原始图像提供了**更大的训练集**。在数字识别的情况下，我们还可以对图像施加扭曲。
- en: '**Early stopping**, as the name suggests, involves **stopping the training
    phase** before the initially defined number of iterations. If we plot the cost
    function on both the training set and the validation set as a function of the
    iterations, we can experience that, for an overfitting model, the training error
    always keeps decreasing but the validation error might start to increase after
    a certain number of iterations. When the validation error stops decreasing, that
    is exactly the time to stop the training process. By stopping the training process
    earlier, we force the model to be simpler, thus reducing overfitting.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**早停**，顾名思义，涉及**在最初定义的迭代次数之前停止训练阶段**。如果我们将成本函数绘制在训练集和验证集上，并以迭代次数为函数进行观察，我们会发现，对于过拟合模型，训练误差总是持续减少，但验证误差可能在某些迭代次数后开始增加。当验证误差停止减少时，这正是停止训练过程的时机。通过更早地停止训练，我们迫使模型变得更简单，从而减少过拟合。'
- en: '**Conclusion**'
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: In conclusion, the L1 and L2 regularization techniques are indispensable tools
    in addressing overfitting within neural networks.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，L1 和 L2 正则化技术是解决神经网络中过拟合问题的不可或缺的工具。
- en: '![](../Images/661c006d92710389459291d28fb756aa.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/661c006d92710389459291d28fb756aa.png)'
- en: Image by the author.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像。
- en: L1 regularization, with its ability to induce sparsity by **penalizing irrelevant
    features**, proves effective in **simplifying models**. On the other hand, L2
    regularization, by penalizing the squared magnitude of weights, **promotes smoother
    models**, reducing the risk of extreme values.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: L1 正则化通过**惩罚无关特征**来引入稀疏性，这在**简化模型**方面表现有效。另一方面，L2 正则化通过惩罚权重的平方大小，**促进了更平滑的模型**，从而降低了极端值的风险。
- en: Dropout, a dynamic technique involving randomly deactivating neurons during
    the training phase, enhances model generalization. It prevents over-reliance on
    specific neurons, promoting a more robust network.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 是一种在训练阶段随机停用神经元的动态技术，它增强了模型的泛化能力。它防止对特定神经元的过度依赖，从而促进了更强大的网络。
- en: However, these techniques come with tradeoffs. While regularization methods
    effectively mitigate overfitting, they may inadvertently restrict the model’s
    ability to capture intricate patterns in the data. The real challenge is choosing
    the right balance between regularization strength and model complexity.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些技术也有其权衡之处。虽然正则化方法能有效缓解过拟合，但它们可能会无意中限制模型捕捉数据中复杂模式的能力。真正的挑战在于选择正则化强度和模型复杂性之间的正确平衡。
- en: For further insights, I will fill the Reference section with amazing resources.
    I highly suggest checking them out to refine your understanding of the subject.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步了解，我会在参考部分添加一些精彩资源。我强烈建议查看这些资源，以便深化对主题的理解。
- en: If you liked this story, consider following me to be notified of my upcoming
    projects and articles!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这个故事，考虑关注我，以便及时了解我即将发布的项目和文章！
- en: 'Here are some of my past projects:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我过去的一些项目：
- en: '[](/advanced-dimensionality-reduction-models-made-simple-639fca351528?source=post_page-----b4615a2d9507--------------------------------)
    [## Advanced Dimensionality Reduction Models Made Simple'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/advanced-dimensionality-reduction-models-made-simple-639fca351528?source=post_page-----b4615a2d9507--------------------------------)
    [## 先进的降维模型简单化'
- en: Learn how to efficiently apply state-of-the-art Dimensionality Reduction methods
    and boost your Machine Learning…
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习如何高效地应用最先进的降维方法，并提升你的机器学习…
- en: 'towardsdatascience.com](/advanced-dimensionality-reduction-models-made-simple-639fca351528?source=post_page-----b4615a2d9507--------------------------------)
    [](/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----b4615a2d9507--------------------------------)
    [## Use Deep Learning to Generate Fantasy Names: Build a Language Model from Scratch'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/advanced-dimensionality-reduction-models-made-simple-639fca351528?source=post_page-----b4615a2d9507--------------------------------)
    [](/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----b4615a2d9507--------------------------------)
    [## 使用深度学习生成奇幻名字：从零开始构建语言模型
- en: Can a language model invent unique fantasy character names? Let’s build it from
    scratch
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语言模型能否创造出独特的奇幻角色名字？让我们从零开始构建它。
- en: 'towardsdatascience.com](/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----b4615a2d9507--------------------------------)
    [](/outlier-detection-with-scikit-learn-and-matplotlib-a-practical-guide-382d1411b8ec?source=post_page-----b4615a2d9507--------------------------------)
    [## Outlier Detection with Scikit-Learn and Matplotlib: a Practical Guide'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----b4615a2d9507--------------------------------)
    [](/outlier-detection-with-scikit-learn-and-matplotlib-a-practical-guide-382d1411b8ec?source=post_page-----b4615a2d9507--------------------------------)
    [## 使用深度学习生成奇幻名字：从零开始构建语言模型
- en: Learn how visualizations, algorithms, and statistics help you to identify anomalies
    for your machine learning tasks.
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解如何将先进的降维方法高效应用于机器学习任务。
- en: towardsdatascience.com](/outlier-detection-with-scikit-learn-and-matplotlib-a-practical-guide-382d1411b8ec?source=post_page-----b4615a2d9507--------------------------------)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/outlier-detection-with-scikit-learn-and-matplotlib-a-practical-guide-382d1411b8ec?source=post_page-----b4615a2d9507--------------------------------)
- en: References
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville](https://www.deeplearningbook.org/)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[《深度学习》作者：伊恩·古德费洛、约书亚·本吉奥、亚伦·库尔维尔](https://www.deeplearningbook.org/)'
- en: '[Dropout: A Simple Way to Prevent Neural Networks from Overfitting by Nitish
    Srivastava et al. (2014)](https://jmlr.org/papers/v15/srivastava14a.html)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[《Dropout：防止神经网络过拟合的简单方法》作者：Nitish Srivastava 等（2014）](https://jmlr.org/papers/v15/srivastava14a.html)'
- en: '[Regularization Techniques in Deep Learning: A Survey and Practical Guide by
    Navid Pustokhina, et al.](https://arxiv.org/abs/2201.03299)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[《深度学习中的正则化技术：调查与实践指南》作者：Navid Pustokhina 等](https://arxiv.org/abs/2201.03299)'
- en: '[Understanding the Disharmony between Dropout and Batch Normalization by Variance
    Shift by Shibani Santurkar, et al. (2018)](https://www.researchgate.net/publication/327434121_Understanding_Regularization_in_Batch_Normalization)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[《理解批归一化中的 dropout 与方差偏移的失调》作者：Shibani Santurkar 等（2018）](https://www.researchgate.net/publication/327434121_Understanding_Regularization_in_Batch_Normalization)'
- en: '[Neural Networks and Deep Learning: A Textbook by Charu Aggarwal](https://link.springer.com/book/10.1007/978-3-319-94463-0)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[《神经网络与深度学习：一本教科书》作者：Charu Aggarwal](https://link.springer.com/book/10.1007/978-3-319-94463-0)'
