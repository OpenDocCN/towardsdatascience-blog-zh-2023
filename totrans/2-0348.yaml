- en: 'Avoid Overfitting in Neural Networks: a Deep Dive'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/avoid-overfitting-in-neural-networks-a-deep-dive-b4615a2d9507](https://towardsdatascience.com/avoid-overfitting-in-neural-networks-a-deep-dive-b4615a2d9507)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn how to implement regularization techniques to boost performances and prevent
    Neural Network overfitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@riccardo.andreoni?source=post_page-----b4615a2d9507--------------------------------)[![Riccardo
    Andreoni](../Images/5e22581e419639b373019a809d6e65c1.png)](https://medium.com/@riccardo.andreoni?source=post_page-----b4615a2d9507--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b4615a2d9507--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b4615a2d9507--------------------------------)
    [Riccardo Andreoni](https://medium.com/@riccardo.andreoni?source=post_page-----b4615a2d9507--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b4615a2d9507--------------------------------)
    ·10 min read·Nov 30, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b99bb683b9b10973ea4a001da99621b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [unsplash.com](https://unsplash.com/photos/multicolored-illustration-gpiKdZmDQig).'
  prefs: []
  type: TYPE_NORMAL
- en: 'When training a deep neural network, it’s often troublesome to achieve the
    same performances on **both the training and validation sets**. A considerably
    higher error on the validation set is a **clear flag for overfitting**: the network
    has become too specialized in the training data. In this article, I provide a
    comprehensive guide on how to bypass this issue.'
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When dealing with any machine learning application, it’s important to have a
    clear understanding of the **bias and variance of the model**. In traditional
    machine learning algorithms, we talk about the [**bias vs. variance tradeoff**](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#:~:text=In%20statistics%20and%20machine%20learning,bias%20in%20the%20estimated%20parameters.),
    which consists of the struggle of minimizing both the **variance** and the **bias**
    of a model.
  prefs: []
  type: TYPE_NORMAL
- en: In order to reduce the bias of a model (i.e. its error from erroneous assumptions),
    we need a **more complex model**. On the contrary, reducing the model’s variance
    (the sensitivity of the model in capturing the variations of the training data),
    implies a **more simple model**. It is straightforward that the bias vs. variance
    tradeoff, in traditional machine learning, derives from the conflict of necessitating
    both a **more complex and a simpler model at the same time**.
  prefs: []
  type: TYPE_NORMAL
- en: In the **Deep Learning era,** we have tools to reduce just the model’s variance
    without hurting the model’s bias or, on the contrary, to reduce the bias without
    increasing the variance.
  prefs: []
  type: TYPE_NORMAL
- en: Before exploring the different techniques used to prevent the overfitting of
    a neural network, it’s important to clarify what high variance or high bias means.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a common neural network task such as image recognition, and think
    over a neural network that recognizes the presence of pandas in a picture. We
    can confidently assess that a human can carry out this task with a near 0% error.
    As a consequence, this is a reasonable benchmark for the accuracy of the image
    recognition network. After training the neural network on the training set and
    evaluating its performances on both the training and validation sets, we may come
    up with these different results:'
  prefs: []
  type: TYPE_NORMAL
- en: Train Error = 20% and Validation Error = 22%
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train Error = 1% and Validation Error = 15%
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train Error = 0.5% and Validation Error = 1%
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train Error = 20% and Validation Error = 30%
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The first example is a typical instance of **high bias**: the error is large
    on **both the train and validation sets**. Conversely, the second example suffers
    from **high variance**, having a much lower accuracy when dealing with data **the
    model didn’t learn from**. The third result represents **low variance and bias**,
    the model can be considered valid. Finally, the fourth example shows a case of
    **both high bias and variance**: not only the training error is large when compared
    with the benchmark, but the validation error is also higher.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8e21e27da658ab098e20e6dc05b9597.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: From now on, I will present several techniques of regularization, used to reduce
    the model’s overfitting to the training data. They are beneficial for cases 2\.
    and 4\. of the previous example.
  prefs: []
  type: TYPE_NORMAL
- en: L1 and L2 Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to classical regression algorithms (linear, logistic, polynomial, etc.),
    [**L1 and L2 regularizations**](/l1-and-l2-regularization-methods-ce25e7fc831c)
    are also employed to prevent overfitting in high-variance neural networks. In
    order to keep this article short and on-point, I won’t recall how L1 and L2 regularizations
    work on regression algorithms, but you can check [this article](https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization)
    for additional information.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind the L1 and L2 regularization techniques is to **constrain the
    model’s weights** to be smaller or to **shrink some of them** to 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the cost function J of a classical deep neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/74851e40e220fbd23aa00087890900a7.png)'
  prefs: []
  type: TYPE_IMG
- en: The cost function J is, of course, a function of the weights and biases of each
    layer 1, …, L. m is the number of training examples and ℒ is the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: L1 Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In L1 Regularization we add the following term to the cost function J:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6708fd5d805a36ef04965644cd8919aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where the matrix norm is the sum of the absolute value of the weights for each
    layer 1, …, L of the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0840263ec9997187da46ac6795bb596.png)'
  prefs: []
  type: TYPE_IMG
- en: 'λ is the **regularization term**. It’s a hyperparameter that must be **carefully
    tuned**. λ directly controls the impact of the regularization: as λ increases,
    the effects on the weights shrinking are more severe.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete cost function under L1 Regularization becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8696d7d55275f3fef2b4ca9d1585bfb8.png)'
  prefs: []
  type: TYPE_IMG
- en: For λ=0, the effects of L1 Regularization are null. Instead, choosing a value
    of λ which is too big, will over-simplify the model, probably resulting in an
    underfitting network.
  prefs: []
  type: TYPE_NORMAL
- en: L1 Regularization can be considered as a sort of neuron selection process because
    it would bring to zero the weights of some hidden neurons.
  prefs: []
  type: TYPE_NORMAL
- en: L2 Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In L2 Regularization, the term we add to the cost function is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b08ca79a0cf0042384850dd21cca9cd4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the regularization term is the **squared norm of the weights**
    of each network’s layer. This matrix norm is called [Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm)
    and, explicitly, it’s computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/771b8017371792e5895c8f3b36983455.png)'
  prefs: []
  type: TYPE_IMG
- en: Please note that the weight matrix relative to layer l has n^{[l]} rows and
    n^{[l-1]} columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the complete cost function under L2 Regularization becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/20df25a8f8630e67332916e96f232a1e.png)'
  prefs: []
  type: TYPE_IMG
- en: Again, λ is the **regularization term** and for λ=0 the effects of L2 Regularization
    are null.
  prefs: []
  type: TYPE_NORMAL
- en: L2 Regularization brings towards zero the values of the weights, resulting in
    a more simple model.
  prefs: []
  type: TYPE_NORMAL
- en: How do L1 and L2 Regularization reduce overfitting?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'L1 and L2 Regularization techniques have positive effects on overfitting to
    the training data for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The weights of some hidden units become closer (or equal) to 0\. As a consequence,
    their effect is weakened and the resulting network is simpler because it’s closer
    to a **smaller network**. As stated in the introduction, a simpler network is
    less prone to overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For smaller weights, also the input z of the activation function of a hidden
    neuron becomes smaller. For values close to 0, many activation functions behave
    **linearly**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second reason is not trivial and deserves an expansion. Consider a hyperbolic
    tangent (tanh) activation function, whose graph is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8a77bb53e829a09ca28df59431b20cab.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the function plot we can see that if the input value x is small, the function
    tanh(x) behaves **almost linearly**. When tanh is used as the activation function
    of a neural network’s hidden layers, the input value is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fef52ba778f11544a14bbc260189c706.png)'
  prefs: []
  type: TYPE_IMG
- en: which for small weights w is also **close to zero**.
  prefs: []
  type: TYPE_NORMAL
- en: If each layer of the neural network is linear, we can prove that the whole network
    behaves linearly. Thus, constraining some of the hidden units to mimic linear
    functions, leads to a simpler network and, as a consequence, helps to prevent
    overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: A simpler model often is [not capable to capture the noise](/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036)
    in the training data and therefore, **overfitting is less frequent**.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea of [**dropout regularization**](https://www.analyticsvidhya.com/blog/2022/08/dropout-regularization-in-deep-learning/#:~:text=In%20machine%20learning%2C%20%E2%80%9Cdropout%E2%80%9D,are%20codependent%20with%20one%20another.)
    is to **randomly remove some nodes** in the network. Before the training process,
    we set a probability (suppose p = 50%) for each node of the network. During the
    training phase, each node has a p **probability of being turned off**. The dropout
    process is random, and it is performed separately for each training example. As
    a consequence, each training example might be trained on a different network.
  prefs: []
  type: TYPE_NORMAL
- en: As for L2 Regularization, the result of dropout regularization is a simpler
    network, and a simpler network leads to a less complex model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a1949c771af16c058a1b49153f38e052.png)'
  prefs: []
  type: TYPE_IMG
- en: The effect of dropout on a simple network. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout in practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this brief section, I show how to implement **Dropout regularization in practice**.
    I will go through a few simple lines of code (python). If you are only interested
    in the general theory of regularization, you can easily skip this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have stored the activation values of the network’s layer 4 in the
    [NumPy](https://numpy.org/) array `a4`. First, we create the auxiliary vector
    `d4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The vector `d4` has the same dimensions as `a4` and contains the values `True`
    or `False` based on the probability `keep_prob`. If we set a keeping probability
    of 70%, that is the probability that a given hidden unit is kept, and therefore,
    the probability of having `True` value on a given element of `d4`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We apply the auxiliary vector `d4` to the activations `a4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need to scale the modified vector `a4` by the `keep_prob` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This last operation is needed to compensate for the reduction of units in the
    layer. Carrying out this operation during the training process allows us not to
    apply dropout during the test phase.
  prefs: []
  type: TYPE_NORMAL
- en: How does dropout reduce overfitting?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dropout has the effect of temporarily **transforming the network into a smaller
    one**, and we know that smaller networks are **less complex and less prone to
    overfitting**.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the network illustrated above, and focus on the first unit of the second
    layer. Because some of its inputs may be temporarily shut down due to dropout,
    the unit **can’t always rely on them** during the training phase. As a consequence,
    the hidden unit is encouraged to spread its weights across its inputs. Spreading
    the weights has the effect of decreasing the squared norm of the weight matrix,
    resulting in a **sort of L2 regularization**.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the keeping probability is a fundamental step for an effective dropout
    regularization. Typically, the keeping probability is set separately for each
    layer of the neural network. For layers with a large weight matrix, we usually
    set a smaller keeping probability because, at each step, we want to conserve proportionally
    fewer weights with respect to smaller layers.
  prefs: []
  type: TYPE_NORMAL
- en: Other Regularization Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to L1/L2 regularization and dropout, there exist **other regularization
    techniques**. Two of them are **data augmentation** and **early stopping**.
  prefs: []
  type: TYPE_NORMAL
- en: From the theory, we know that training a network on more data has positive effects
    on reducing high variance. As getting more data is often a demanding task, data
    augmentation is a technique that, for some applications, allows machine learning
    practitioners to get more data almost for free. In computer vision, data augmentation
    provides a **larger training set** by flipping, zooming, and translating the original
    images. In the case of digit recognition, we can also impose distortion on the
    images.
  prefs: []
  type: TYPE_NORMAL
- en: '**Early stopping**, as the name suggests, involves **stopping the training
    phase** before the initially defined number of iterations. If we plot the cost
    function on both the training set and the validation set as a function of the
    iterations, we can experience that, for an overfitting model, the training error
    always keeps decreasing but the validation error might start to increase after
    a certain number of iterations. When the validation error stops decreasing, that
    is exactly the time to stop the training process. By stopping the training process
    earlier, we force the model to be simpler, thus reducing overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, the L1 and L2 regularization techniques are indispensable tools
    in addressing overfitting within neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/661c006d92710389459291d28fb756aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: L1 regularization, with its ability to induce sparsity by **penalizing irrelevant
    features**, proves effective in **simplifying models**. On the other hand, L2
    regularization, by penalizing the squared magnitude of weights, **promotes smoother
    models**, reducing the risk of extreme values.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout, a dynamic technique involving randomly deactivating neurons during
    the training phase, enhances model generalization. It prevents over-reliance on
    specific neurons, promoting a more robust network.
  prefs: []
  type: TYPE_NORMAL
- en: However, these techniques come with tradeoffs. While regularization methods
    effectively mitigate overfitting, they may inadvertently restrict the model’s
    ability to capture intricate patterns in the data. The real challenge is choosing
    the right balance between regularization strength and model complexity.
  prefs: []
  type: TYPE_NORMAL
- en: For further insights, I will fill the Reference section with amazing resources.
    I highly suggest checking them out to refine your understanding of the subject.
  prefs: []
  type: TYPE_NORMAL
- en: If you liked this story, consider following me to be notified of my upcoming
    projects and articles!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of my past projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/advanced-dimensionality-reduction-models-made-simple-639fca351528?source=post_page-----b4615a2d9507--------------------------------)
    [## Advanced Dimensionality Reduction Models Made Simple'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to efficiently apply state-of-the-art Dimensionality Reduction methods
    and boost your Machine Learning…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/advanced-dimensionality-reduction-models-made-simple-639fca351528?source=post_page-----b4615a2d9507--------------------------------)
    [](/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----b4615a2d9507--------------------------------)
    [## Use Deep Learning to Generate Fantasy Names: Build a Language Model from Scratch'
  prefs: []
  type: TYPE_NORMAL
- en: Can a language model invent unique fantasy character names? Let’s build it from
    scratch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----b4615a2d9507--------------------------------)
    [](/outlier-detection-with-scikit-learn-and-matplotlib-a-practical-guide-382d1411b8ec?source=post_page-----b4615a2d9507--------------------------------)
    [## Outlier Detection with Scikit-Learn and Matplotlib: a Practical Guide'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how visualizations, algorithms, and statistics help you to identify anomalies
    for your machine learning tasks.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/outlier-detection-with-scikit-learn-and-matplotlib-a-practical-guide-382d1411b8ec?source=post_page-----b4615a2d9507--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville](https://www.deeplearningbook.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Dropout: A Simple Way to Prevent Neural Networks from Overfitting by Nitish
    Srivastava et al. (2014)](https://jmlr.org/papers/v15/srivastava14a.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Regularization Techniques in Deep Learning: A Survey and Practical Guide by
    Navid Pustokhina, et al.](https://arxiv.org/abs/2201.03299)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding the Disharmony between Dropout and Batch Normalization by Variance
    Shift by Shibani Santurkar, et al. (2018)](https://www.researchgate.net/publication/327434121_Understanding_Regularization_in_Batch_Normalization)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Neural Networks and Deep Learning: A Textbook by Charu Aggarwal](https://link.springer.com/book/10.1007/978-3-319-94463-0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
