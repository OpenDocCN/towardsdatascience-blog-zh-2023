- en: 'Pipeline Dreams: Automating ML Training on AWS'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Pipeline Dreams: 在 AWS 上自动化机器学习训练'
- en: 原文：[https://towardsdatascience.com/pipeline-dreams-automating-ml-training-on-aws-8e90a33061fd](https://towardsdatascience.com/pipeline-dreams-automating-ml-training-on-aws-8e90a33061fd)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/pipeline-dreams-automating-ml-training-on-aws-8e90a33061fd](https://towardsdatascience.com/pipeline-dreams-automating-ml-training-on-aws-8e90a33061fd)
- en: '[](https://medium.com/@raicik.zach?source=post_page-----8e90a33061fd--------------------------------)[![Zachary
    Raicik](../Images/860760b53fcc75013007067190e8ca65.png)](https://medium.com/@raicik.zach?source=post_page-----8e90a33061fd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8e90a33061fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8e90a33061fd--------------------------------)
    [Zachary Raicik](https://medium.com/@raicik.zach?source=post_page-----8e90a33061fd--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@raicik.zach?source=post_page-----8e90a33061fd--------------------------------)[![Zachary
    Raicik](../Images/860760b53fcc75013007067190e8ca65.png)](https://medium.com/@raicik.zach?source=post_page-----8e90a33061fd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8e90a33061fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8e90a33061fd--------------------------------)
    [Zachary Raicik](https://medium.com/@raicik.zach?source=post_page-----8e90a33061fd--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8e90a33061fd--------------------------------)
    ·11 min read·Oct 25, 2023
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----8e90a33061fd--------------------------------)
    ·阅读时间11分钟·2023年10月25日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c696ca663854fee564e07b9749427f82.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c696ca663854fee564e07b9749427f82.png)'
- en: Photo by [Arnold Francisca](https://unsplash.com/@clark_fransa?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Arnold Francisca](https://unsplash.com/@clark_fransa?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)。
- en: In the world of machine learning, automated training pipelines streamline the
    journey from data to insight. They automate various parts of the machine learning
    life cycle such as data ingestion, preprocessing, model training, evaluation and
    deployment. Amazon Web Services (“AWS”) provides various tools to develop an automated
    training pipeline. In this article, we will walk through setting up a basic automated
    training pipeline using the classic iris dataset.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的世界里，自动化训练管道简化了从数据到洞察的过程。它们自动化了机器学习生命周期的各个部分，如数据摄取、预处理、模型训练、评估和部署。亚马逊网络服务（“AWS”）提供了多种工具来开发自动化训练管道。在本文中，我们将通过使用经典的鸢尾花数据集来讲解如何设置一个基本的自动化训练管道。
- en: 'Setting the Stage: Requirements and AWS Toolkit'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置舞台：需求和 AWS 工具包
- en: In this section, we will cover some high level requirements as well as a brief
    overview of the AWS tools we will use.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将介绍一些高级需求以及我们将使用的 AWS 工具的简要概述。
- en: Requirements
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 需求
- en: If you choose to follow along by building your own training pipeline, you will
    need the following.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择通过构建自己的训练管道来跟随，您将需要以下内容。
- en: An active AWS account (you can sign up [here](https://aws.amazon.com/free/?trk=78b916d7-7c94-4cab-98d9-0ce5e648dd5f&sc_channel=ps&ef_id=CjwKCAjws9ipBhB1EiwAccEi1K7ugfy2KiyndD6pN_A9_pVTNfC-K3Xp41WNOcLEEDoPj1Lyu_7WHRoCtjEQAvD_BwE%3AG%3As&s_kwcid=AL%214422%213%21432339156165%21e%21%21g%21%21aws+account%219572385111%21102212379047))
    with Administrator Access
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要一个活跃的 AWS 账户（你可以在[这里](https://aws.amazon.com/free/?trk=78b916d7-7c94-4cab-98d9-0ce5e648dd5f&sc_channel=ps&ef_id=CjwKCAjws9ipBhB1EiwAccEi1K7ugfy2KiyndD6pN_A9_pVTNfC-K3Xp41WNOcLEEDoPj1Lyu_7WHRoCtjEQAvD_BwE%3AG%3As&s_kwcid=AL%214422%213%21432339156165%21e%21%21g%21%21aws+account%219572385111%21102212379047)注册），并具有管理员权限。
- en: Basic knowledge of **AWS CLI (**We will explore alternatives to the AWS CLI
    in future posts)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS CLI**的基本知识（我们将在未来的帖子中探索AWS CLI的替代方案）'
- en: Setting up your AWS account and connecting to AWS via the CLI is beyond the
    scope of this post, however- feel free to reach out directly if you need help.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 设置你的 AWS 账户并通过 CLI 连接到 AWS 超出了本文的范围，但如果你需要帮助，请随时直接联系我。
- en: Toolkit
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工具包
- en: Setting up the automated training pipeline will require the use of the following
    AWS products.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 设置自动化训练管道将需要使用以下 AWS 产品。
- en: '**S3**: Scalable object storage service designed to store and retrieve any
    amount of data from anywhere on the web'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**S3**：可扩展的对象存储服务，旨在存储和检索来自网络任何位置的任意数量的数据。'
- en: '**Lambda**: Serverless compute service that automatically runs your code in
    response to events, such as changes to data in an Amazon S3 bucket'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Lambda**：无服务器计算服务，自动响应事件运行你的代码，例如对 Amazon S3 桶中数据的更改。'
- en: '**Docker:** Docker is a platform that packages, distributes, and manages applications
    inside lightweight, portable containers'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Docker**：Docker 是一个平台，它在轻量级、可移植的容器中打包、分发和管理应用程序。'
- en: '**Sagemaker**: Fully managed service that provides developers and data scientists
    with the ability to build, train, and deploy machine learning models quickly and
    easily'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sagemaker**：完全托管的服务，提供开发人员和数据科学家快速、轻松地构建、训练和部署机器学习模型的能力。'
- en: '**Step Functions**: Serverless workflow service that lets you coordinate distributed
    applications and microservices using visual workflows, enabling you to build,
    run, and visualize complex processes at scale'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Step Functions**：无服务器工作流服务，让你通过可视化工作流协调分布式应用程序和微服务，使你能够大规模地构建、运行和可视化复杂的流程。'
- en: Implementing the Automated Training Pipeline
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现自动化训练管道
- en: Assuming you made it past the requirements, we can switch our focus to building
    our automated training pipeline. For this simple example, we will focus on the
    following steps.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经通过了要求，我们可以将注意力转向构建我们的自动化训练管道。对于这个简单的例子，我们将专注于以下步骤。
- en: Create AWS S3 bucket to store data and artifacts related to our training pipeline.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 AWS S3 存储桶来存储与我们的训练管道相关的数据和工件。
- en: Create AWS Lambda function data ingestion, preprocessing, and training.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 AWS Lambda 函数用于数据摄取、预处理和训练。
- en: Create an AWS Step Functions state machine to orchestrate the execution of your
    pipeline stages.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 AWS Step Functions 状态机来协调执行你的管道阶段。
- en: We take a deep dive into each of these steps below.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将深入探讨每个步骤。
- en: 1\. S3 Bucket
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. S3 存储桶
- en: The first thing we will do is create a new S3 bucket to store data and artifacts
    using the AWS CLI.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用 AWS CLI 创建一个新的 S3 存储桶来存储数据和工件。
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If this command runs successfully, it should output something like this in your
    terminal.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个命令成功运行，你的终端应该会输出类似的信息。
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Taking a look at our management console, we can see the S3 bucket was succesfully
    created.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下我们的管理控制台，我们可以看到 S3 存储桶已经成功创建。
- en: '![](../Images/d4be38174ded05e2d492ad20c7e6c28f.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4be38174ded05e2d492ad20c7e6c28f.png)'
- en: 2\. Lambda Functions
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. Lambda 函数
- en: 'For our automated training pipeline, we will rely heavily on the use of lambda
    functions to execute and trigger certain parts of our process. Specifically, we
    will use lambdas for:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的自动化训练管道，我们将大量依赖 lambda 函数来执行和触发过程的某些部分。具体来说，我们将使用 lambda 函数来：
- en: Data ingestion
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据摄取
- en: Data preprocessing
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Model training
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练
- en: '***2a. Data Ingestion***'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '***2a. 数据摄取***'
- en: Lambda functions require lambda handlers. The Lambda handler is a user-defined
    function in a Lambda deployment package that the AWS Lambda service can invoke
    when the service executes the Lambda function. The handler function receives and
    processes the event data from the invoker.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 函数需要 lambda 处理程序。Lambda 处理程序是 Lambda 部署包中的用户定义函数，当 AWS Lambda 服务执行 Lambda
    函数时，服务可以调用该处理程序函数。处理程序函数接收并处理来自调用者的事件数据。
- en: With that in mind, let’s define our `lambda_handler` for data ingestion. The
    `lambda_handler` function serves as the entry point for AWS Lambda to execute
    the code, which in this case retrieves a CSV file from a specified URL and uploads
    it to an Amazon S3 bucket. This file has been stored in `./src/data`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这一点，让我们定义我们的 `lambda_handler` 用于数据摄取。`lambda_handler` 函数作为 AWS Lambda 执行代码的入口点，在这种情况下，它从指定的
    URL 中检索一个 CSV 文件，并将其上传到 Amazon S3 存储桶。该文件已存储在 `./src/data` 中。
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We will package and deploy your AWS Lambda function as a container image using
    docker. To do so, we first need to create an ECR repository to house our image.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 docker 将你的 AWS Lambda 函数打包和部署为容器镜像。为此，我们首先需要创建一个 ECR 仓库来存放我们的镜像。
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have our ECR repository stood up, we create our `Dockerfile` in
    the root of our project directory.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们的 ECR 仓库已经搭建好了，我们就在项目目录的根目录中创建我们的 `Dockerfile`。
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We use this Dockerfile to build our image and push it to the ECR registry.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用这个 Dockerfile 来构建我们的镜像并将其推送到 ECR 注册表。
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We need to authenticate Docker to the ECR registry.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要对 Docker 进行身份验证以访问 ECR 注册表。
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Next, we tag the image to match the repository.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为镜像打上标签以匹配仓库。
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, we can push our image to the ECR registry.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将我们的镜像推送到 ECR 注册表。
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Finally, we will create the lambda function and attach the container.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将创建 lambda 函数并附加容器。
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You will now be able to see the lambda function in your AWS management console.
    I can invoke the function and confirm the file was downloaded to the S3 bucket
    correctly.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以在 AWS 管理控制台中看到 lambda 函数。我可以调用该函数并确认文件已正确下载到 S3 存储桶中。
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '***2b. Preprocessing***'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '***2b. 预处理***'
- en: Now, we have the data we need for our modeling task. For most machine learning
    tasks, we will want to process that data in some way. In the file below, we define
    a script that will process our Iris data.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经有了建模任务所需的数据。对于大多数机器学习任务，我们希望以某种方式处理这些数据。在下面的文件中，我们定义了一个脚本来处理我们的鸢尾花数据。
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We upload our preprocessing script to S3\. Run this command in the same directory
    as `preprocess.py`
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将预处理脚本上传到 S3。在与 `preprocess.py` 相同的目录中运行此命令。
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Similar to how we built an image for the data ingestion step, we will build
    an image for the preprocessing step. Like before, let’s start by creating our
    repository.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们为数据获取步骤构建镜像的方式，我们将为预处理步骤构建镜像。像之前一样，我们先创建我们的仓库。
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now that we have our ECR repository stood up, we create our `Dockerfile` in
    the root of our project directory.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经建立了 ECR 仓库，我们在项目目录的根目录中创建我们的 `Dockerfile`。
- en: '[PRE14]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We build the image.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建镜像。
- en: '[PRE15]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Let’s authenticate Docker to the ECR registry. You might not have to do this
    if your access hasn’t expired.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对 Docker 进行 ECR 注册表的认证。如果你的访问权限没有过期，你可能不需要这样做。
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Again, we tag the image to match the repository.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将镜像标记以匹配仓库。
- en: '[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now, we can push our image to the ECR registry.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将镜像推送到 ECR 注册表。
- en: '[PRE18]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Like before, we will create a lambda function that will kick off a SageMaker
    processing job. We define our lambda handler below.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 像之前一样，我们将创建一个 lambda 函数来启动一个 SageMaker 处理作业。我们在下面定义了我们的 lambda 处理程序。
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We zip up our Lambda handler and store in S3.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 lambda 处理程序打包，并存储在 S3 中。
- en: '[PRE20]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Finally, we create our lambda function.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建我们的 lambda 函数。
- en: '[PRE21]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Since we haven’t strung the Lambda functions together yet, we can manually invoke
    the lambda and check the contents of S3 to ensure our training and testing datasets
    are there.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们还没有将 Lambda 函数串联起来，我们可以手动调用 Lambda 函数，并检查 S3 的内容以确保我们的训练和测试数据集存在。
- en: '[PRE22]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '***2c. Model training***'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '***2c. 模型训练***'
- en: Instead of creating our own image for train, we will use a built SageMaker image
    for logistic regression. You can create your own image if you want a customized
    training process. Consequently, we write our lambda handler right away.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个内置的 SageMaker 镜像来进行逻辑回归，而不是为训练创建我们自己的镜像。如果你想要一个定制化的训练过程，你可以创建自己的镜像。因此，我们会立即编写我们的
    lambda 处理程序。
- en: '[PRE23]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Before we can create our lambda function, we have to zip up our handler and
    send it to S3.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们创建 lambda 函数之前，我们必须将处理程序打包并发送到 S3。
- en: '[PRE24]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now we can create our lambda function.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建我们的 lambda 函数了。
- en: '[PRE25]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Again, we can manually invoke our training lambda to ensure it works correctly.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以手动调用我们的训练 lambda 函数，以确保其正常工作。
- en: '[PRE26]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 3\. Step Functions
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 步骤函数
- en: Now, we have a very basic pipeline consisting of data ingestion, preprocessing,
    and model training. However, each of these components exist on their own — how
    do we string them together to create an automated pipeline. Step functions!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个非常基本的流程，包括数据获取、预处理和模型训练。然而，这些组件各自存在——我们如何将它们串联起来以创建一个自动化的流程？步骤函数！
- en: To string our lambda functions together, the first thing we will do is define
    a state machine using Amazon States Language and save it in a `.json` file so
    we can push it to AWS using the CLI. For our purposes, we will define our state
    machine in the following way.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们的 lambda 函数串联起来，我们首先将使用 Amazon States Language 定义一个状态机，并将其保存在 `.json` 文件中，以便我们可以通过
    CLI 将其推送到 AWS。对于我们的目的，我们将以以下方式定义我们的状态机。
- en: '[PRE27]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We can deploy our step function to AWS.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将步骤函数部署到 AWS。
- en: '[PRE28]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Once the state machine has been created, we can visualize it in the management
    console.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦状态机创建完成，我们可以在管理控制台中可视化它。
- en: '![](../Images/b434ffa1f6a9df02f39e5a150c01294c.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b434ffa1f6a9df02f39e5a150c01294c.png)'
- en: We can manually invoke the pipeline as follows.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以手动调用流程，如下所示。
- en: '[PRE29]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Additional Considerations & Conclusion
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附加考虑事项与结论
- en: The pipeline we built in this example is extremely basic — we don’t even consider
    model evaluation! In a real world setting, this approach can be expanded to cover
    additional parts of the model building lifecycle including model evaluation &
    deployment.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个例子中构建的流程非常基础——我们甚至没有考虑模型评估！在实际应用中，这种方法可以扩展到涵盖模型构建生命周期的其他部分，包括模型评估和部署。
- en: 'Additionally, we built our pipeline but we have to trigger it manually. In
    the real world, you can trigger the execution of a Step Functions state machine
    based on various events. Using triggers provides a more event-driven architecture.
    Here are some common ways to set up triggers for your state machine:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们已经构建了我们的管道，但必须手动触发。在实际应用中，你可以根据各种事件触发Step Functions状态机的执行。使用触发器提供了更具事件驱动的架构。以下是为状态机设置触发器的一些常见方法：
- en: '**Amazon S3 Events**: If you want to run your state machine when a new file
    is uploaded to an S3 bucket, you can set up an event within the bucket to trigger
    a Lambda function, which in turn starts the execution of your state machine.'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Amazon S3 Events**：如果你希望在将新文件上传到S3桶时运行状态机，可以在桶内设置事件来触发Lambda函数，从而启动状态机的执行。'
- en: '**AWS CodeCommit**: If you’re using AWS CodeCommit as your repository, you
    can use AWS Lambda and Amazon CloudWatch Events to trigger the state machine whenever
    there’s a new commit.'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**AWS CodeCommit**：如果你使用AWS CodeCommit作为代码库，可以使用AWS Lambda和Amazon CloudWatch
    Events在有新的提交时触发状态机。'
- en: '**GitHub or Other Repositories**: If you’re using a service like GitHub, you
    can use webhooks to notify an AWS service of a new commit or a pull request merge.
    Typically, you’d set up an API Gateway endpoint to receive the webhook, trigger
    a Lambda function from the API Gateway call, and then have the Lambda function
    start your state machine.'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**GitHub或其他代码库**：如果你使用类似GitHub的服务，可以使用webhooks通知AWS服务有关新的提交或拉取请求合并。通常，你会设置一个API
    Gateway端点来接收webhook，触发API Gateway调用的Lambda函数，然后让Lambda函数启动你的状态机。'
- en: '**Amazon DynamoDB Streams**: If you want to trigger your state machine based
    on changes in a DynamoDB table, you can use DynamoDB Streams. Whenever there’s
    a change in the table, the stream can trigger a Lambda function, which then starts
    the state machine.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Amazon DynamoDB Streams**：如果你想根据DynamoDB表中的变化触发状态机，可以使用DynamoDB Streams。每当表中发生变化时，流可以触发一个Lambda函数，然后启动状态机。'
- en: '**AWS EventBridge (previously CloudWatch Events)**: AWS EventBridge allows
    you to create rules based on a wide range of AWS service events. You can target
    a Lambda function with these rules. Then, like in other scenarios, the Lambda
    function would start your state machine.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**AWS EventBridge（之前的CloudWatch Events）**：AWS EventBridge允许你基于各种AWS服务事件创建规则。你可以使用这些规则来指定Lambda函数。然后，像其他场景一样，Lambda函数将启动你的状态机。'
- en: 'In conclusion, an automated training pipeline in the context of machine learning
    and data science offers multiple advantages. Here’s a concise list of pros for
    your conclusion:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，机器学习和数据科学领域的自动化训练管道提供了多种优势。以下是你结论部分的简洁优点列表：
- en: '**Consistency and Reproducibility**: Automation ensures that the training process
    remains consistent across different runs. This aids in reproducing results and
    eliminates variations due to manual interventions.'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**一致性和可重复性**：自动化确保训练过程在不同运行中保持一致。这有助于重现结果，并消除了由于人工干预而导致的变化。'
- en: '**Efficiency**: Automated pipelines can streamline processes, reducing the
    time required for training and retraining models. This can be especially beneficial
    when iterating over different model architectures or parameters.'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**效率**：自动化管道可以简化流程，减少训练和重新训练模型所需的时间。这在迭代不同的模型架构或参数时尤其有利。'
- en: '**Scalability**: As your data or model complexity grows, an automated pipeline
    can scale up resources and processes without the need for manual oversight.'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**可扩展性**：随着数据或模型复杂性的增加，自动化管道可以扩大资源和流程，而无需人工监督。'
- en: '**Resource Optimization**: Automation can manage resources more effectively,
    potentially leading to cost savings. For instance, cloud resources can be automatically
    scaled down when not in use.'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**资源优化**：自动化可以更有效地管理资源，从而可能节省成本。例如，云资源可以在不使用时自动缩减。'
- en: Feel free to reach out with any questions!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如有任何问题，请随时联系！
