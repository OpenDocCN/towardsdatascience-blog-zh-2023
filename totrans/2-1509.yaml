- en: Meta AI’s Another Revolutionary Large Scale Model — DINOv2 for Image Feature
    Extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/meta-ais-another-revolutionary-large-scale-model-dinov2-for-image-feature-extraction-1114b287eadd](https://towardsdatascience.com/meta-ais-another-revolutionary-large-scale-model-dinov2-for-image-feature-extraction-1114b287eadd)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: DINOv2 is one of the best self-supervised ViT-based DL models for image feature
    extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@gkeretchashvili?source=post_page-----1114b287eadd--------------------------------)[![Gurami
    Keretchashvili](../Images/4da78f113a0046c2deb8224e09dd9e3d.png)](https://medium.com/@gkeretchashvili?source=post_page-----1114b287eadd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1114b287eadd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1114b287eadd--------------------------------)
    [Gurami Keretchashvili](https://medium.com/@gkeretchashvili?source=post_page-----1114b287eadd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1114b287eadd--------------------------------)
    ·8 min read·Jun 26, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mete AI introduces a new version of the image feature extraction model called
    DINOv2 which automatically extracts visual features from the image. This is another
    revolutionary step in the AI field especially in the computer vision domain in
    terms of data and model scaling.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b5509b6f4b08541f8e6df946fd86e4c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Demo of DINOv2 by ai.facebook.com
  prefs: []
  type: TYPE_NORMAL
- en: Motivation — Why should we care?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DINOv2 is a self-supervised modelthatdoes not require fine-tuning and has a
    good performance. In addition, it can be used as a backbone for many different
    computer vision tasks such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification, fine-grained classification — e.g cat vs dog or e.g dog breed
    identifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image retrieval —e.g. find a bag that is similar to yours from a large amount
    of images on the internet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic image Segmentation — Associates a label or category with every pixel
    in an image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Video understanding — automatically recognizes and interpret various aspects
    of videos, such as objects, actions, events, scenes, and even higher-level concepts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monocular Depth Estimation — predict if an object is in the foreground or in
    the background in an image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image clustering —group images into clusters such that the images within the
    same clusters are similar to each other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Content-based recommendation system — recommend items to a user-based representation
    of an image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DINOv2 complements another recent computer vision research, including Segment
    Anything. Segment Anything is a promptable segmentation system focused on zero-shot
    generalization to a diverse set of segmentation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: You can have a look at Segment Anything Model (SAM) in my previous post.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/meta-ai-introduces-revolutionary-image-segmentation-model-trained-on-1-billion-masks-8f13c86a13a2?source=post_page-----1114b287eadd--------------------------------)
    [## Meta AI Introduces Revolutionary Image Segmentation Model Trained on 1 Billion
    Masks'
  prefs: []
  type: TYPE_NORMAL
- en: Segment Anything Model (SAM) - Best DL Model for Image Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/meta-ai-introduces-revolutionary-image-segmentation-model-trained-on-1-billion-masks-8f13c86a13a2?source=post_page-----1114b287eadd--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Methodology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main contribution of the [paper](https://arxiv.org/pdf/2304.07193.pdf)
    can be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a large and curated training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve the training algorithm and implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design a functional distillation pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a large and curated training dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large deep learning models need large amounts of data for training. That is
    why, the authors have created an automatic pipeline shown in the figure below
    to get a curated training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3df39205426167a23ecc1e2221efa155.png)'
  prefs: []
  type: TYPE_IMG
- en: The workflow of curated image creation [1]
  prefs: []
  type: TYPE_NORMAL
- en: They selected a set of seed images from a collection of about 25 third-party
    datasets and their goal was to augment this seed images. Here is how it works.
    They scraped a large amount of uncurated images about (1.2 Billion unique images)
    from the internet. After that they created image embeddings using a self-supervised
    ViT-H/16 network pre-trained on ImageNet-22k. Second, they used cosine-similarity
    as a distance measure between images to filter out duplicated images in order
    to reduce redundancy and increase diversity. As duplicated images was removed
    from large uncurated datasets, then they performed image retrieval stage, where
    images that was similar to curated images were extracted. In the end, this approach
    enabled them to create a 142 million curated images out of 1.2 billion uncurated
    database of images.
  prefs: []
  type: TYPE_NORMAL
- en: Improved training algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DINOv2 uses student-teacher mechanism that is a training technique where a smaller
    neural network, known as the student, learns to mimic the behavior of a larger
    or more complex neural network, known as the teacher. Studen and teacher are based
    on Vision Tranformer architecture (ViT) [2]. As for the loss they use cross-entropy
    loss for student-teacher feature similarities. In order to learn both local and
    global features on the image they used different level of learning. For the global
    learning, they used is image leveling learning — randomly crop data augmentation
    on the same image. As for the local feature learning, they used patch level learning
    — randomly applying Mask to the input patches to the student, but not to the teacher.
    In addition they performed different normalization techniques such as Sinkhorn-knop
    batch normalization etc. You can find more details in the paper.
  prefs: []
  type: TYPE_NORMAL
- en: Distillation pipeline.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Powerful hardware are required when making prediction (inference) using large
    models. To overcome this limitation, they also compressed large model into a smaller
    one. Knowledge distillation [5] aims at reproducing the output of a large model
    with a smaller model by minimizing some distance between both outputs for a set
    of given inputs. The training algorithm is based on self-distillation that makes
    it straightforward to compress our large models into smaller ones.
  prefs: []
  type: TYPE_NORMAL
- en: Result
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: They have evaluated the model performance on eight different computer vision
    tasks and compared to other methods.
  prefs: []
  type: TYPE_NORMAL
- en: In the graph bellow, the result from DINOv2 models are dark blue, other self-supervised
    methods are pale orange and weakly-supervised methods are highlighted as dark
    pink. the dashed horizontal line is best-performing weakly- supervised model.
  prefs: []
  type: TYPE_NORMAL
- en: As the result shows, DINOv2 models drastically improves over the previous state
    of the art in self-supervised learning and reaches performance comparable with
    weakly-supervised features.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c5d15921f5e14d84d51bd56da95e158a.png)'
  prefs: []
  type: TYPE_IMG
- en: DINOv2 vs other SOTA models [1]
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In summary, DINOv2 is another revolutionary model from Meta AI team. It does
    not require fine-tuning and can be used as a backbone for many different computer
    vision models. DINOv2 uses self-supervision mechanism and it can learn from any
    collection of images.
  prefs: []
  type: TYPE_NORMAL
- en: DINOv2 Demo — Fine-grained Image classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, I will try to demonstrate how DINOv2 works in a real-case scenario.
    I will create fine-grained image classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Classification workflow:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Download the Food101 dataset from PyTorch datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract features from train and test datasets using small DINOv2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train ML classifier models (SVM, XGBoost and KNN) using extracted features from
    training dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make a prediction on extracted features from test dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate each ML model’s accuracy and F1score.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data**: [Food 101](https://pytorch.org/vision/stable/generated/torchvision.datasets.Food101.html)
    is a challenging data set of 101 food categories with 101,000 images. For each
    class, 250 manually reviewed test images are provided as well as 750 training
    images.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model**: small [DINOv2 model](https://github.com/facebookresearch/dinov2)
    (ViT-S/14 distilled)'
  prefs: []
  type: TYPE_NORMAL
- en: '**ML models**: SVM, XGBoost, KNN.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1 — Set up (You can use** [**Google Colab**](http://colab.research.google.com)
    **to run the code and turn GPU on)**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2 — Create Transformation, download and create Food101 Pytorch datasets,
    create train and test dataloader objects.**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[out] 75750 25250'
  prefs: []
  type: TYPE_NORMAL
- en: '[out] 9469 3157'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3 (Optional) — Visualize training dataloader batch**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1857142179e4aec23c17f5c169af578a.png)'
  prefs: []
  type: TYPE_IMG
- en: batch of images
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4 — load small DINOv2 model and extract features from training and test
    dataloaders.**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[out] ((75750, 384), (75750,), (25250, 384), (25250,))'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5 — Build a function for SVM, XGBoost and KNN classifiers.**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Results**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/ae8b127c16e2a83d9169141b17b323ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Result of small DINOv2 + SVM/XGBoost/KNN (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Wow, the results are great! As demonstrated, SVM model trained on small DINOv2
    extracted features outperformed other ML models and achieved almost 90% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though we used small DINOv2 model to extract features, ML models (especially
    SVM) trained on extracted features demonstrated great performance on the fine
    grained classification task. The model can classify objects with almost 90% accuracy
    out of 101 different classes.
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy would improve if it was used big, large or giant DINOv2 models.
    You just need to change the *dinov2_vits14* in step 4 with *dinov2_vitb14*, *dinov2_vitl14
    or dinov2_vitg14*. You can have a try and feel free to share the accuracy result
    in the comment section :)
  prefs: []
  type: TYPE_NORMAL
- en: I hope you enjoyed it. If you have any questions or would like to share your
    thoughts about this article, feel free to comment, I will be happy to answer.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to support my work directly and also get unlimited access on Medium
    articles, become a Medium member using my [referral link](https://medium.com/@gkeretchashvili/membership)
    here. Thank you a million times and have a nice day!
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@gkeretchashvili/membership?source=post_page-----1114b287eadd--------------------------------)
    [## Join Medium with my referral link - Gurami Keretchashvili'
  prefs: []
  type: TYPE_NORMAL
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@gkeretchashvili/membership?source=post_page-----1114b287eadd--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov,
    V., … & Bojanowski, P. (2023). Dinov2: Learning robust visual features without
    supervision. *arXiv preprint arXiv:2304.07193*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
    Unterthiner, T., … & Houlsby, N. (2020). An image is worth 16x16 words: Transformers
    for image recognition at scale. *arXiv preprint arXiv:2010.11929*.ISO 690'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] The DINOv2 team, [DINOv2: State-of-the-art computer vision models with
    self-supervised learning](https://ai.facebook.com/blog/dino-v2-computer-vision-self-supervised-learning/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [DINOv2 Github](https://github.com/facebookresearch/dinov2)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in
    a neural network. *arXiv preprint arXiv:1503.02531*.'
  prefs: []
  type: TYPE_NORMAL
