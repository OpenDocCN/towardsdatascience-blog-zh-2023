["```py\nfor epoch in range(epochs):\n    # forward pass\n    predictions = model(features)\n    # calculate the loss\n    loss = criterion(predictions, labels)\n    # propagate the error backward to calculate the gradients\n    loss.backward()\n    # update the model's weights\n    optimizer.step()\n\n    # xero gradients\n    optimizer.zero_grad()\n```", "```py\nimport evaluate\n\nmetric = evaluate.load(\"accuracy\")\n\ndef preprocess_logits_for_metrics(logits, labels):\n    mlm_logits = logits[0]\n    nsp_logits = logits[1]\n    return mlm_logits.argmax(-1), nsp_logits.argmax(-1)\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n\n    mlm_preds = preds[0]\n    nsp_preds = preds[1]\n\n    mlm_labels = labels[0]\n    nsp_labels = labels[1]\n\n    mask = mlm_labels != -100\n    mlm_labels = mlm_labels[mask]\n    mlm_preds = mlm_preds[mask]\n\n    mlm_accuracy =  metric.compute(\n        predictions=mlm_preds, references=mlm_labels)[\"accuracy\"]\n    nsp_accuracy = metric.compute(\n        predictions=nsp_preds, references=nsp_labels)[\"accuracy\"]\n\n    return {\"Masked ML Accuracy\": mlm_accuracy, \"NSP Accuracy\": nsp_accuracy}\n```", "```py\nfrom transformers import BertForPreTraining, BertConfig\n\nmodel_config = BertConfig()\nmodel = BertForPreTraining(model_config)\n```", "```py\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    \"bert-pretrained-wikitext-2-raw-v1\",\n    logging_first_step=True,\n    evaluation_strategy=\"epoch\",\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    push_to_hub=True,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    fp16=True,\n    save_strategy=\"epoch\",\n    num_train_epochs=20\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=nsp_train_dataset,\n    eval_dataset=nsp_validation_dataset,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    preprocess_logits_for_metrics=preprocess_logits_for_metrics\n)\n```"]