# 混合模型、潜变量和期望最大化算法

> 原文：[https://towardsdatascience.com/mixture-models-latent-variables-and-the-expectation-maximization-algorithm-e5b18e15faa](https://towardsdatascience.com/mixture-models-latent-variables-and-the-expectation-maximization-algorithm-e5b18e15faa)

[](https://reoneo.medium.com/?source=post_page-----e5b18e15faa--------------------------------)[![Reo Neo](../Images/a3c192dafc1222b06b2e7fcf4d35cb27.png)](https://reoneo.medium.com/?source=post_page-----e5b18e15faa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e5b18e15faa--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e5b18e15faa--------------------------------) [Reo Neo](https://reoneo.medium.com/?source=post_page-----e5b18e15faa--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----e5b18e15faa--------------------------------) ·阅读时长 8 分钟·2023年3月19日

--

![](../Images/eae2fea5e6268b616a5acfcaca0178b9.png)

图片由[Sung Shin](https://unsplash.com/ko/@ironstagram?utm_source=medium&utm_medium=referral)提供，发布在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

无监督学习一直让我着迷。这是一种无需手动标记的数据学习方式，并且可以识别数据集中的模式。在各种无监督学习技术中，最简单的就是聚类。从本质上讲，聚类算法旨在找到彼此相似的数据点。通过将数据点进行聚类，我们可以获得关于数据集的有价值见解，以及各个聚类所代表的内容。

本文旨在深入探讨高斯混合模型聚类算法，它如何建模数据，更重要的是如何使用期望最大化来将模型拟合到新数据集上。

# **什么是混合模型？**

实质上，混合模型（或混合分布）是**将多个概率分布组合成一个单一的分布**。

![](../Images/a74bf32579fb4ddda706f9cbdcbde8e5.png)

**混合模型的 PDF**

为了将这些分布组合在一起，我们为每个组件分布分配一个**权重**，以使分布下的总概率和为 1。一个简单的例子是由 2 个高斯分布组成的混合分布。我们可以有 2 个均值和方差不同的分布，并使用不同的权重将它们结合起来。

![](../Images/31a44cd2b833fc89d86614e39ecaf2e1.png)![](../Images/d3812ae7c15398d04fc9df4fd09221db.png)

2 个均值相同但 π 值不同的 GMMs — 图片由作者提供

具体来说，我们可以将这种分布视为由一个两步生成过程产生。在这个过程中，一个数据点可以从 n 个不同的概率分布中生成。首先，我们确定它来自哪个概率分布。这个概率就是权重 π_i。一旦选择了组件概率分布，数据点可以通过模拟组件概率分布本身来生成。

# **GMM**

高斯混合模型本质上是一个混合模型，其中 **所有组件分布都是高斯分布。**

![](../Images/25c24b86e8715bae51890cf334bdd409.png)

GMM 模型的概率密度函数

现在让我们尝试理解为什么使用高斯分布来建模混合组件。当查看数据集时，我们希望将相似的点聚集在一起。这些聚类通常是球形或椭圆形的，因为我们希望相近的点被聚集在一起。

因此，正态分布是聚类的一个良好模型。分布的均值将是 **聚类的中心**，而 **形状** 和 **分布** 可以通过 **分布的协方差** 来很好地建模。

聚类的第二个变量将是 **不同聚类的相对大小**。在一个真实数据集中，我们通常不期望聚类的大小相同，这意味着一些聚类将比其他聚类包含更多的点。聚类的大小将由聚类权重 π_i 决定。

在聚类的背景下，我们假设有 *k* 个影响因素影响数据的生成。每个影响因素有一个 **不同的权重**，对应于聚类权重 π。

# **定义术语**

在深入模型推导之前，我们首先定义一些关键术语。

![](../Images/1d1d0584c23cf379e660fa695b1bf071.png)

聚类责任的数学函数

+   θ：这是指 GMM 模型的参数，包括聚类的均值和协方差 θ = {μ, Σ,π}。因为每个聚类都有相应的参数，所以有 *k* 个 θ 值。

+   *y_n* — 这是指数据集中的第 n 个数据点

+   *z_n* — 这是指第 n 个数据点所属的聚类

+   *r_nk* — 聚类责任（在下一节中解释）

## **拟合 GMM 模型**

既然我们已经有了模型，我们希望利用它来洞察数据。对于一个聚类问题，这将是预测的聚类分配 ***z_n^***。与其求解确定的聚类分配，GMM 模型允许我们计算聚类概率。

这个值***r_nk***是聚类*k*对第n个数据点的影响，被称为聚类责任。这种方法被称为软聚类，其中每个数据点**按比例分配**到每个聚类，而不是**直接分配到一个单一聚类**。作为概率分布，向量***r_nk***对所有点的总和为1。在数学上，这个值定义为：

![](../Images/2da31cc2dee72f76ca49ca59b5fc6314.png)

聚类责任定义

从定义中，我们可以看到这是给定数据y_n和参数θ的z_n的后验概率。

这种方法的好处在于**识别模型对数据点的不确定性**。或者，如果我们想找到最可能的聚类，我们可以通过将每个数据点分配到具有最高责任的聚类来将结果转换回**硬聚类**。

![](../Images/b5087a6590f16e0305846428a919fd82.png)

针对硬聚类的聚类责任的Argmax函数

使用贝叶斯规则，我们可以进一步细化后验分布：

![](../Images/1d1d0584c23cf379e660fa695b1bf071.png)

使用贝叶斯规则扩展聚类责任

在分子中，后验分布被分解为先验***p(z_n=k|θ)***和似然***p(y_n|z_n=k, θ)***。分母是归一化常数，它考虑了**所有可能的聚类**。

在这里，我们也遇到了解决后验分布的问题。我们无法在首先不知道参数估计θ的情况下解决责任问题。同时，在不知道聚类责任的情况下，也无法得出良好的责任估计。因此，我们使用**期望最大化算法**，它用于解决潜在变量模型。

# EM算法

潜在变量是数据集中**从未被明确观察到**的变量。在这种情况下，潜在变量是聚类分配***z_n***。

该算法是一个**迭代的2步算法**。它从随机初始化的参数*θ*值开始，然后在每次迭代中逐步优化对潜在变量*z_n*和参数*θ*的估计。每次迭代有2个步骤：

+   **E步**：在E步中，我们尝试估计给定当前对*θ*的估计的潜在变量*z_n*的概率分布。这等同于构造**给定Z和*θ*的条件对数似然的期望**。

![](../Images/b5a0cdcfe0f49ad03cbf201b8e545880.png)

给定当前参数的期望条件对数似然

+   **M步**：在M步中，**我们生成一组新的*θ*估计值，给定当前期望的Z**（来自E步）。这是通过最大化M步中相对于参数*θ*的项来完成的。

如果上述步骤听起来很混乱，不用担心！让我们在GMM的背景下看这个例子。

## **GMM的E步**

![](../Images/1d1d0584c23cf379e660fa695b1bf071.png)

让我们尝试求解之前推导的集群责任方程。

对于分子，我们有第一个项***p(z_n=k|θ)***，这是数据点属于集群*k*的概率。这个项不考虑似然，因此将只是*π_k*。第二个项***p(y_n|z_n=k,θ)***是**在属于集群k的情况下的似然**，这是高斯似然。实质上，这个概率变为：

![](../Images/052b0e47abf7e506b86bb968bbbf9845.png)

简化集群责任的推导

对于数据集中的每个点，我们计算***r_n***，一个大小为*k*的向量，表示点属于每个集群的概率。由于我们有随机初始化的μ和Σ，这个问题是可以解决的。一旦E步完成，我们可以进入M步。

## **GMM的M-Step**

在M步中，我们使用**E步中的集群责任**来**获得集群参数的更好估计**。有3个参数**{π, μ, Σ}**，我们可以进行这种优化。

第一个参数***π_k***代表一个分类分布，并具有简单的MLE估计。

![](../Images/4d16ed55bf765a13a9b6796ca934a3d2.png)

GMM的π的MLE估计

对于数据集中的每个点，我们有点属于集群*k*的概率。如果我们在数据集中取平均，这将给我们提供给定数据的最可能的*π*值。

对于μ和Σ，它们可以以类似于高斯MLE的方式进行优化。不同之处在于*r_nk*作为计算的加权参数。

![](../Images/bfeae7e63c5d0faf3924cef1918e470a.png)

GMM的μ和Σ的MLE估计

对于参数μ，我们对与集群*k*相关联的特征取加权平均。我们对方差执行相同的过程，取加权平均，这给我们提供了Σ的估计。

使用我们改进的参数，我们完成了EM算法的1次迭代。我们可以重复相同的步骤，直到算法收敛，参数变化很小。

# 与K-Means的区别

尽管GMM模型可能更难实现，但它们比更常用的K-Means算法具有两个独特的优势。

1.  椭圆形集群

+   K-Means使用L2距离，这导致集群在本质上是球形的。

+   GMM允许更多的灵活性。首先，不同的方差（对角线条目）可以解释特征扩展的相对差异。

+   允许协方差矩阵具有非对角条目也可以允许

2\. 软聚类

+   如前所述，GMM模型允许软聚类，其中每个数据点以部分的方式分配到不同的集群。

有趣的是，K-Means 算法可以看作是 GMM 的一种特殊形式。如果我们将算法限制为仅具有单位协方差矩阵（Σ = *I*），则参数优化将是最小化 L2 损失函数。这与 K-Means 相同，其中最小化数据点与簇之间的平方距离。

同时，为了将软聚类转换为硬聚类，我们可以将簇设置为最可能的簇。通过取*argmax*，在 E 步骤中，每个数据点只被分配到一个簇。这与 K-Means 相同，其中每次迭代都将数据点重新分配到最近的簇。

在未来的博客文章中，我将详细讲解一个“从头开始”的 Python 实现 EM 算法，并探索拟合 GMM 模型的一些特征。
