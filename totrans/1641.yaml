- en: PCA vs Autoencoders for a Small Dataset in Dimensionality Reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/pca-vs-autoencoders-for-a-small-dataset-in-dimensionality-reduction-67b15318dea0](https://towardsdatascience.com/pca-vs-autoencoders-for-a-small-dataset-in-dimensionality-reduction-67b15318dea0)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Neural Networks and Deep Learning Course: Part 45'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://rukshanpramoditha.medium.com/?source=post_page-----67b15318dea0--------------------------------)[![Rukshan
    Pramoditha](../Images/b80426aff64ff186cb915795644590b1.png)](https://rukshanpramoditha.medium.com/?source=post_page-----67b15318dea0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----67b15318dea0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----67b15318dea0--------------------------------)
    [Rukshan Pramoditha](https://rukshanpramoditha.medium.com/?source=post_page-----67b15318dea0--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----67b15318dea0--------------------------------)
    ·8 min read·Feb 16, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/40244380e27388a744474a8be2aa482a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Robert Katzki](https://unsplash.com/@ro_ka?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/jbtfM0XBeRc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: '***Can general machine learning algorithms outperform neural networks with
    small datasets?***'
  prefs: []
  type: TYPE_NORMAL
- en: In general, deep learning algorithms such as neural networks require a massive
    amount of data to achieve reasonable performance. So, neural networks like autoencoders
    can benefit from very large datasets that we use to train the models.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, general machine learning algorithms can outperform neural network
    algorithms when they are trained with very small datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders can also be used in dimensionality reduction applications, even
    though they are widely used in other popular applications such as image denoising,
    image generation, image colorization, image compression, image super-resolution,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier, we compared the performance of autoencoders in dimensionality reduction
    against PCA by training the models on the *very large* MNIST dataset. There, the
    autoencoder model easily outperformed the PCA model [ref¹] because the MNIST data
    is large and non-linear.
  prefs: []
  type: TYPE_NORMAL
- en: 'ref¹: [*How Autoencoders Outperform PCA in Dimensionality Reduction*](/how-autoencoders-outperform-pca-in-dimensionality-reduction-1ae44c68b42f)'
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders work well with large and non-linear data.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Even though autoencoders are a type of neural network, it is still possible
    to use them with smaller datasets to achieve some performance with the correct
    model architecture and hyperparameter values.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders are powerful and flexible enough to capture complex and non-linear
    patterns in data.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Today, we will compare the performance of an autoencoder (the neural network
    model) in dimensionality reduction against PCA (the general machine learning algorithm)
    by training the models on the *very small* Winedataset.
  prefs: []
  type: TYPE_NORMAL
- en: The Wine dataset has 178 samples and 13 features. This dataset is very small
    when we compare it with the MNIST dataset which has 60,000 samples and 784 features!
  prefs: []
  type: TYPE_NORMAL
- en: Load the Wine dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Wine dataset comes preloaded with Sciki-learn and can be loaded as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6b5b070277c2b9ca837b575f18bdc5a6.png)'
  prefs: []
  type: TYPE_IMG
- en: '**The shape of the Wine dataset** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: Run PCA with Wine data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we will need to select the best number of components by running the PCA
    with all components [ref²].
  prefs: []
  type: TYPE_NORMAL
- en: 'ref²: [*3 Easy Steps to Perform Dimensionality Reduction Using Principal Component
    Analysis (PCA)*](https://rukshanpramoditha.medium.com/3-easy-steps-to-perform-dimensionality-reduction-using-principal-component-analysis-pca-79121998b991)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5e27b552287a6852bfbd89582152b636.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Cumulative explained variance plot** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: By analyzing the above cumulative explained variance plot, I’ve decided to keep
    the first seven components which capture about 90% variance in the data. So, those
    seven components will accurately represent the original Wine dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Run PCA again with selected components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To apply dimensionality reduction to the Wine dataset, we need to run PCA again
    with the selected components and apply the transformation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ef1bd7818c895ecf64c547c39298fb86.png)'
  prefs: []
  type: TYPE_IMG
- en: '**The shape of the reduced Wine dataset after applying PCA** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: Now, there are only seven components (features) in the dataset. So, the dimensionality
    of the data has been reduced!
  prefs: []
  type: TYPE_NORMAL
- en: Perform dimensionality reduction with an autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following code defines an autoencoder in which the encoder part can be used
    to obtain the lower dimensional (encoded) representation of the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0a10b14840a711dc24bfdc8682bb472e.png)'
  prefs: []
  type: TYPE_IMG
- en: '**The shape of the reduced Wine dataset after auto-encoding data** (Image by
    author)'
  prefs: []
  type: TYPE_NORMAL
- en: The autoencoder model has an input layer, encoding layers and decoding layers.
    The input dimension is the number of input features in the Wine dataset, which
    is 13\. The latent vector dimension is 7 which is equal to the number of components
    that we selected earlier in PCA.
  prefs: []
  type: TYPE_NORMAL
- en: All layers are connected using the Keras Functional API method [ref³]. Then,
    the whole autoencoder model is created by connecting the input layer and decoder
    part.
  prefs: []
  type: TYPE_NORMAL
- en: 'ref³: [*Two Different Ways to Build Keras Models: Sequential API and Functional
    API*](https://rukshanpramoditha.medium.com/two-different-ways-to-build-keras-models-sequential-api-and-functional-api-868e64594820)'
  prefs: []
  type: TYPE_NORMAL
- en: Then, we compile the whole autoencoder with the Adam optimizer and the mean
    squared error (mse) loss function.
  prefs: []
  type: TYPE_NORMAL
- en: The model is trained on the standardized (scaled) Wine data for 100 epochs with
    a batch size of 16.
  prefs: []
  type: TYPE_NORMAL
- en: The latent vector represents the most important features of the input data in
    a lower-dimensional form [ref⁴]. Therefore, after training the whole autoencoder,
    we can use its encoder part to obtain the lower dimensional (encoded) representation
    of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'ref⁴: [*An Introduction to Autoencoders in Deep Learning*](https://rukshanpramoditha.medium.com/an-introduction-to-autoencoders-in-deep-learning-ab5a5861f81e)'
  prefs: []
  type: TYPE_NORMAL
- en: The `encoder_model`is created by connecting the input layer and the encoder
    part. Then, we can call its predict() method on the scaled Wine data to obtain
    the lower dimensional (encoded) representation of the Wine dataset which is represented
    by the variable, `X_encoded`. Since the **“latent vector dimension”** is set to
    7, the encoded representation has 7 features and the dimensionality of the data
    has been reduced!
  prefs: []
  type: TYPE_NORMAL
- en: Compare both PCA and autoencoder models by visualizing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Visualization of high-dimensional data can be achieved through dimensionality
    reduction [ref⁵]. So, dimensionality reduction is extremely useful for data visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'ref⁵: [*11 Different Uses of Dimensionality Reduction*](/11-different-uses-of-dimensionality-reduction-4325d62b4fa6)'
  prefs: []
  type: TYPE_NORMAL
- en: By using only two components (dimensions), we plot the Wine dataset outputs
    returned by both PCA and autoencoder models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/958a23c84db65d7e1e3acbfd732c3734.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Two-dimensional representations of Wine data: PCA vs Autoencoder** (Image
    by author)'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the two-dimensional representation of Wine data obtained using
    PCA shows a clear separation between the three classes of wine, but the separation
    is not good enough in autoencoder output.
  prefs: []
  type: TYPE_NORMAL
- en: PCA works well with small datasets like the Wine dataset.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Note:** The output of the autoencoder (right plot) may vary significantly
    due to the stochastic nature of the algorithm and the values of hyperparameters
    such as the number of hidden layers and hidden units, type of activation function
    used in each layer, type of loss function, type of optimizer, number of epochs
    and the batch size! But, the separation of the three classes may not be as good
    as the PCA output.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'PCA vs Autoencoder: Which is better in dimensionality reduction?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The choice of PCA and autoencoder for dimensionality reduction depends on the
    following factors.
  prefs: []
  type: TYPE_NORMAL
- en: Size of the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The complexity of the dataset (linear or non-linear, image or numerical data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goals of the analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Availability of computational resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpretability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA works well with small datasets. It can also be used with larger datasets.
    However, autoencoders work really well with very large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: PCA works well with linear data as it is a linear dimensionality reduction technique.
    It is not effective with non-linear data. In contrast, autoencoders can easily
    capture complex and non-linear patterns in the data. So, they work well with non-linear
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Autoecnder works well with image data. PCA works well with numerical data.
  prefs: []
  type: TYPE_NORMAL
- en: There is no way to determine the importance of each component (feature) in the
    latent vector of an autoencoder model. But in PCA, we can create the cumulative
    explained variance plot for that.
  prefs: []
  type: TYPE_NORMAL
- en: Since an autoencoder is a neural network, its architecture may become complex.
    In addition to that, it requires a massive amount of data. Therefore, autoencoders
    require more computational resources than PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Autoecndoers are black-box models. So, they are hard to interpret. We don’t
    know how they select important features from the data we provide. So, the interpretation
    of those models is very difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both PCA and autoencoders can be used to perform dimensionality reduction. PCA
    is a general machine-learning algorithm while autoencoders are a type of neural
    network architecture that require large datasets and a lot of computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders work well with large and non-linear data. They are powerful and
    flexible enough to capture complex and non-linear patterns in data, but may fail
    to outperform general machine learning algorithms like PCA with smaller datasets!
  prefs: []
  type: TYPE_NORMAL
- en: If you really want to consider class separability while performing dimensionality
    reduction, LDA (Linear Discriminant Analysis) is the best choice. Read the complete
    guide below.
  prefs: []
  type: TYPE_NORMAL
- en: '[*LDA Is More Effective than PCA for Dimensionality Reduction in Classification
    Datasets*](/lda-is-highly-effective-than-pca-for-dimensionality-reduction-in-classification-datasets-4489eade632)'
  prefs: []
  type: TYPE_NORMAL
- en: This is the end of today’s article.
  prefs: []
  type: TYPE_NORMAL
- en: '**Please let me know if you’ve any questions or feedback.**'
  prefs: []
  type: TYPE_NORMAL
- en: How about an AI course?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[![](../Images/61d87653a43e302c4f2a839ae046ac31.png)](https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Join my Neural Networks and Deep Learning Course, the first ever on Medium**
    (Screenshot by author)'
  prefs: []
  type: TYPE_NORMAL
- en: Support me as a writer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*I hope you enjoyed reading this article. If you’d like to support me as a
    writer, kindly consider* [***signing up for a membership***](https://rukshanpramoditha.medium.com/membership)
    *to get unlimited access to Medium. It only costs $5 per month and I will receive
    a portion of your membership fee.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://rukshanpramoditha.medium.com/membership?source=post_page-----67b15318dea0--------------------------------)
    [## Join Medium with my referral link - Rukshan Pramoditha'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Rukshan Pramoditha (and thousands of other writers on
    Medium). Your membership fee directly…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: rukshanpramoditha.medium.com](https://rukshanpramoditha.medium.com/membership?source=post_page-----67b15318dea0--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Join my private list of emails
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Never miss a great story from me again. By* [***subscribing to my email list***](https://rukshanpramoditha.medium.com/subscribe)*,
    you will directly receive my stories as soon as I publish them.*'
  prefs: []
  type: TYPE_NORMAL
- en: Thank you so much for your continuous support! See you in the next article.
    Happy learning to everyone!
  prefs: []
  type: TYPE_NORMAL
- en: Wine dataset info
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Dataset source:** You can download the original dataset [here](https://archive.ics.uci.edu/ml/datasets/Wine).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dataset license:** This dataset is available under the [*CC BY 4.0*](https://creativecommons.org/licenses/by/4.0/)
    (*Creative Commons Attribution 4.0*) license.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Citation:** Lichman, M. (2013). UCI Machine Learning Repository [[https://archive.ics.uci.edu/ml](https://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Rukshan Pramoditha](https://medium.com/u/f90a3bb1d400?source=post_page-----67b15318dea0--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: '**2023–02–16**'
  prefs: []
  type: TYPE_NORMAL
