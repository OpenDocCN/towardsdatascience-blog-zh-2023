- en: 'The Long and Short of It: Proportion-Based Relevance to Capture Document Semantics
    End-to-End'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长短期结合：基于比例的相关性以捕捉文档语义端到端
- en: 原文：[https://towardsdatascience.com/the-long-and-short-of-it-proportion-based-relevance-to-capture-document-semantics-end-to-end-f5a755e5a82f](https://towardsdatascience.com/the-long-and-short-of-it-proportion-based-relevance-to-capture-document-semantics-end-to-end-f5a755e5a82f)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-long-and-short-of-it-proportion-based-relevance-to-capture-document-semantics-end-to-end-f5a755e5a82f](https://towardsdatascience.com/the-long-and-short-of-it-proportion-based-relevance-to-capture-document-semantics-end-to-end-f5a755e5a82f)
- en: '[](https://medium.com/@alcarazanthony1?source=post_page-----f5a755e5a82f--------------------------------)[![Anthony
    Alcaraz](../Images/6a71a1752677bd07c384246fb0c7f7e8.png)](https://medium.com/@alcarazanthony1?source=post_page-----f5a755e5a82f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f5a755e5a82f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f5a755e5a82f--------------------------------)
    [Anthony Alcaraz](https://medium.com/@alcarazanthony1?source=post_page-----f5a755e5a82f--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@alcarazanthony1?source=post_page-----f5a755e5a82f--------------------------------)[![Anthony
    Alcaraz](../Images/6a71a1752677bd07c384246fb0c7f7e8.png)](https://medium.com/@alcarazanthony1?source=post_page-----f5a755e5a82f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f5a755e5a82f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f5a755e5a82f--------------------------------)
    [安东尼·阿尔卡拉斯](https://medium.com/@alcarazanthony1?source=post_page-----f5a755e5a82f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f5a755e5a82f--------------------------------)
    ·5 min read·Nov 25, 2023
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f5a755e5a82f--------------------------------)
    ·阅读时间5分钟·2023年11月25日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*Artificial intelligence software was used to enhance the grammar, flow, and
    readability of this article’s text.*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文的语法、流畅性和可读性由人工智能软件增强。*'
- en: Dominant search methods today typically rely on keywords matching or vector
    space similarity to estimate relevance between a query and documents. However,
    these techniques struggle when it comes to searching corpora using entire files,
    papers or even books as search queries.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当前主流的搜索方法通常依赖于关键词匹配或向量空间相似性来估算查询与文档之间的相关性。然而，当使用整个文件、论文甚至书籍作为搜索查询时，这些技术会遇到困难。
- en: '![](../Images/88c8318af5222375bec03ff04d3b5364.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88c8318af5222375bec03ff04d3b5364.png)'
- en: Some fun with Dall-E 3
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 与Dall-E 3的趣味互动
- en: '**Keyword-based Retrieval**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于关键词的检索**'
- en: While keywords searches excel for short look up, they fail to capture semantics
    critical for long-form content. A document correctly discussing “cloud platforms”
    may be completely missed by a query seeking expertise in “AWS”. Exact term matches
    face vocabulary mismatch issues frequently in lengthy texts.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然关键词搜索在短时间查询中表现出色，但它们未能捕捉到对长篇内容至关重要的语义。一个正确讨论“云平台”的文档可能会完全被一个寻求“AWS”专业知识的查询遗漏。确切的术语匹配在长篇文本中经常面临词汇不匹配问题。
- en: '**Vector Similarity Search**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**向量相似性搜索**'
- en: Modern vector embedding models like BERT condensed meaning into hundreds of
    numerical dimensions accurately estimating semantic similarity. However, transformer
    architectures with self-attention don’t scale beyond 512–1024 tokens due to exploding
    computation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现代向量嵌入模型如BERT将意义压缩为数百个数值维度，准确估算语义相似性。然而，具有自注意力的变换器架构因计算量激增而无法扩展到512–1024个标记以上。
- en: Without the capacity to fully ingest documents, the resulting “bag-of-words”
    partial embeddings lose the nuances of meaning interspersed across sections. The
    context gets lost in abstraction.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 没有完全处理文档的能力，结果的“词袋”部分嵌入丢失了在各部分间交织的意义细微差别。背景在抽象中丢失了。
- en: The prohibitive compute complexity also restricts fine-tuning on most real-world
    corpora limiting accuracy. Unsupervised learning provides one alternative but
    solid techniques are lacking.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 巨大的计算复杂度也限制了在大多数实际语料库上的微调，从而限制了准确性。无监督学习提供了一个替代方案，但仍缺乏可靠的技术。
- en: In a [recent paper](https://arxiv.org/pdf/2303.01200.pdf), researchers address
    exactly these pitfalls by re-imagining relevance for ultra-long queries and documents.
    Their innovations unlock new potential for AI document search.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇 [最新论文](https://arxiv.org/pdf/2303.01200.pdf) 中，研究人员通过重新构思超长查询和文档的相关性，解决了这些问题。他们的创新解锁了AI文档搜索的新潜力。
- en: The Trouble with Long Documents
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长文档的问题
- en: 'Dominant search paradigms today are ineffective for queries that run into thousands
    of words as input text. Key issues faced include:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 目前主流的搜索范式对于输入文本达到数千字的查询效果不佳。面临的主要问题包括：
- en: '**Transformers like BERT** have quadratic self-attention complexity, making
    them infeasible for sequences beyond 512–1024 tokens. Their sparse attention alternatives
    compromise on accuracy.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**像 BERT 这样的变换器**具有二次自注意力复杂度，使得处理超出 512–1024 令牌的序列变得不可行。它们的稀疏注意力替代方案在准确性上有所妥协。'
- en: '**Lexical models** matching based on exact term overlaps cannot infer semantic
    similarity critical for long-form text.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词汇模型**基于精确的术语重叠进行匹配，无法推断长文本中至关重要的语义相似性。'
- en: Lack of labelled training data for most domain collections necessitates **unsupervised
    or minimally-tuned approaches**.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数领域集合缺乏标注训练数据，需采用 **无监督或最少调优的方法**。
- en: Long documents covering multiple sub-topics require models that can **factor
    document structure into relevance judgment**.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 涵盖多个子主题的长文档需要能够 **将文档结构因素纳入相关性判断** 的模型。
- en: The RPRS method aims to tackle these weaknesses in current retrieval architectures.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: RPRS 方法旨在解决当前检索架构中的这些弱点。
- en: Introducing the RPRS Model
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 RPRS 模型
- en: The RPRS model computes relevance between a long query document and candidate
    documents using the proportional matches across their sentences.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: RPRS 模型通过计算长查询文档与候选文档之间句子的比例匹配来计算相关性。
- en: The critical insight is that documents containing a relatively higher proportion
    of sentences similar to sentences from the query are likely more pertinent overall.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 关键洞察是，包含与查询中句子相似的句子比例较高的文档通常更相关。
- en: 'The approach consists of 3 key stages:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法包含 3 个关键阶段：
- en: 1\. Sentence Encoding
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 句子编码
- en: Sentences from queries and candidate documents are encoded into vectors using
    SBERT — an efficient transformer architecture for sentence embeddings.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SBERT 将查询和候选文档中的句子编码为向量 —— 一种高效的变换器架构用于句子嵌入。
- en: SBERT avoids quadratic complexity allowing incorporation of full document lengths.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SBERT 避免了二次复杂度，使得可以处理完整的文档长度。
- en: 2\. Most Relevant Sentence Sets
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 最相关的句子集
- en: For each query sentence, find the k most similar candidate document sentences
    based on vector embeddings.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个查询句子，基于向量嵌入找到最相似的 k 个候选文档句子。
- en: Determine sets of the most relevant document sentences for every query sentence.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定每个查询句子最相关的文档句子集。
- en: 3\. Proportion-based Relevance Scoring
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 基于比例的相关性评分
- en: Define Query Proportion (QP) — the relative proportion of query sentences that
    have similarity to document sentences
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义查询比例 (QP) —— 查询句子与文档句子相似的相对比例
- en: Define Document Proportion (DP) — the relative proportion of document sentences
    that are similar to query sentences
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义文档比例 (DP) —— 与查询句子相似的文档句子的相对比例
- en: Combine QP and DP to compute a final relevance score estimating the inter-relatedness
    of the texts.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合 QP 和 DP 来计算最终的相关性评分，估计文本之间的相互关联性。
- en: The proportional relevance concept intrinsically accounts for document structure
    within long-form text.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 比例相关性的概念内在地考虑了长文本中的文档结构。
- en: An extension called RPRS w/freq additionally factors *term frequency* and *length
    normalization* inspired by BM25 to handle repetition and length bias.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个扩展名为 RPRS w/freq 额外考虑了 *术语频率* 和 *长度归一化*，受到 BM25 的启发，以处理重复和长度偏差。
- en: The Proportional Relevance Formulation
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比例相关性公式
- en: 'At the heart of the RPRS method lies a simple yet powerful relevance scoring
    formula between a query document `q` and candidate document `d`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: RPRS 方法的核心是一个简单而强大的相关性评分公式，用于计算查询文档 `q` 和候选文档 `d` 之间的相关性：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Where:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '`QP(q, d)` is the **Query Proportion**'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`QP(q, d)` 是 **查询比例**'
- en: '`DP(q, d)` is the **Document Proportion**'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DP(q, d)` 是 **文档比例**'
- en: '`RPRS_q(d)` is the **Proportional Relevance Score**'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RPRS_q(d)` 是 **比例相关性评分**'
- en: These proportion factors aim to quantify the inter-relatedness between the query
    and candidate document texts from both perspectives.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这些比例因素旨在从两个角度量化查询与候选文档文本之间的相互关联性。
- en: Query Proportion
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询比例
- en: The Query Proportion determines what percentage of the query content is similar
    to some part of the document.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 查询比例确定查询内容中有多少百分比与文档的某些部分相似。
- en: 'For each sentence `q_s` in query `q`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于查询 `q` 中的每个句子 `q_s`：
- en: Retrieve top `n` most similar sentences `d_si` from document `d`
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文档 `d` 中检索前 `n` 个最相似的句子 `d_si`
- en: 'Count # query sentences that have at least 1 similar `d_si`'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '计算 # 查询句子中至少有 1 个相似的 `d_si`'
- en: 'Divide by total # query sentences'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除以查询句子的总数
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A higher QP indicates more of the query finds matches in the document.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 更高的 QP 表示查询中的更多内容在文档中找到了匹配。
- en: Document Proportion
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文档比例
- en: The Document Proportion conversely determines what percentage of the document
    is similar to some part of the query.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 文档比例反过来决定了文档中有多少百分比与查询的某个部分相似。
- en: 'For each sentence `d_s` in document `d`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文档 `d` 中的每个句子 `d_s`：
- en: Check if `d_s` occurs in the top `n` matches `d_si` for any `q_s`
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查 `d_s` 是否出现在任何 `q_s` 的前 `n` 个匹配 `d_si` 中
- en: 'Count # document sentences that match some `q_s`'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算与某些 `q_s` 匹配的文档句子数量
- en: 'Divide by total # document sentences'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除以文档句子的总数
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: A higher DP indicates more of the document content matches the query.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 更高的 DP 表示文档内容中有更多的内容与查询匹配。
- en: Combining the Factors
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结合因素
- en: 'The Proportional Relevance Score is then simply the product of Query Proportion
    and Document Proportion:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 比例相关性评分简单地是查询比例和文档比例的乘积：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The net effect is rewarding documents that maximally cover the query as well
    as get maximally covered by the query — indicating comprehensive semantic similarity
    from both angles.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 净效果是奖励最大限度地覆盖查询的文档，同时被查询最大限度地覆盖——表明从两个角度来看都具有全面的语义相似性。
- en: Results on Legal, Patent and Wikipedia Datasets
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在法律、专利和维基百科数据集上的结果
- en: The researchers comprehensively evaluated RPRS on five long-document datasets
    spanning legal case retrieval, patent search and Wikipedia document similarity
    tasks containing thousands of words.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员在五个长文档数据集上全面评估了 RPRS，这些数据集涵盖了法律案件检索、专利搜索和维基百科文档相似性任务，包含数千个单词。
- en: On all datasets, RPRS significantly outperformed previous state-of-the-art techniques
    as well as lexical and neural baselines while using just 3 tuned parameters demonstrating
    its effectiveness. Component importance analysis further validated the proportional
    scoring approach.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有数据集上，RPRS 显著超越了之前的最先进技术以及词汇和神经基线，同时仅使用了 3 个调整参数，展示了其有效性。组件重要性分析进一步验证了比例评分方法。
- en: The method combines semantic matching capability through vector embeddings with
    an intuitive notion of topical relevance across sentences providing interpretable
    high accuracy retrieval.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法结合了通过向量嵌入进行的语义匹配能力和跨句子的主题相关性的直观概念，提供了可解释的高准确性检索。
- en: Addressing Long Standing Limitations
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决长期存在的局限性
- en: The RPRS model highlights that key ideas from classic retrieval augmented with
    modern NLP representations can push boundaries on challenging domains like searching
    legal corpora and scientific literature which have resisted high-performance automation
    so far.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: RPRS 模型突显了经典检索中的关键思想与现代 NLP 表示相结合，可以在挑战性领域（如搜索法律文献和科学文献）推动边界，这些领域迄今为止一直抵抗高性能自动化。
- en: In doing so, it also expands the scope of neural search paradigms to ultra-long
    text where most mature models face limitations today. More broadly, designing
    architectures around basic principles of relevance tailored for complex document
    collections remains a fertile area for innovation in search technology.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做也将神经搜索范式的范围扩展到超长文本，其中大多数成熟模型目前面临限制。更广泛地说，围绕复杂文档集合设计基于相关性的基本原则架构仍然是搜索技术中一个富有创新性的领域。
- en: The paper provides a compelling blueprint for adaptation, but much room remains
    for integration with large language models, explainability as well as user experience
    advances in commercial document search solutions by learning from this approach.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 论文提供了一个有说服力的适应蓝图，但仍有很大空间可供与大型语言模型、可解释性以及商业文档搜索解决方案的用户体验进步进行整合。
