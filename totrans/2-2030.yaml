- en: 'The Long and Short of It: Proportion-Based Relevance to Capture Document Semantics
    End-to-End'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-long-and-short-of-it-proportion-based-relevance-to-capture-document-semantics-end-to-end-f5a755e5a82f](https://towardsdatascience.com/the-long-and-short-of-it-proportion-based-relevance-to-capture-document-semantics-end-to-end-f5a755e5a82f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@alcarazanthony1?source=post_page-----f5a755e5a82f--------------------------------)[![Anthony
    Alcaraz](../Images/6a71a1752677bd07c384246fb0c7f7e8.png)](https://medium.com/@alcarazanthony1?source=post_page-----f5a755e5a82f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f5a755e5a82f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f5a755e5a82f--------------------------------)
    [Anthony Alcaraz](https://medium.com/@alcarazanthony1?source=post_page-----f5a755e5a82f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f5a755e5a82f--------------------------------)
    ·5 min read·Nov 25, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*Artificial intelligence software was used to enhance the grammar, flow, and
    readability of this article’s text.*'
  prefs: []
  type: TYPE_NORMAL
- en: Dominant search methods today typically rely on keywords matching or vector
    space similarity to estimate relevance between a query and documents. However,
    these techniques struggle when it comes to searching corpora using entire files,
    papers or even books as search queries.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88c8318af5222375bec03ff04d3b5364.png)'
  prefs: []
  type: TYPE_IMG
- en: Some fun with Dall-E 3
  prefs: []
  type: TYPE_NORMAL
- en: '**Keyword-based Retrieval**'
  prefs: []
  type: TYPE_NORMAL
- en: While keywords searches excel for short look up, they fail to capture semantics
    critical for long-form content. A document correctly discussing “cloud platforms”
    may be completely missed by a query seeking expertise in “AWS”. Exact term matches
    face vocabulary mismatch issues frequently in lengthy texts.
  prefs: []
  type: TYPE_NORMAL
- en: '**Vector Similarity Search**'
  prefs: []
  type: TYPE_NORMAL
- en: Modern vector embedding models like BERT condensed meaning into hundreds of
    numerical dimensions accurately estimating semantic similarity. However, transformer
    architectures with self-attention don’t scale beyond 512–1024 tokens due to exploding
    computation.
  prefs: []
  type: TYPE_NORMAL
- en: Without the capacity to fully ingest documents, the resulting “bag-of-words”
    partial embeddings lose the nuances of meaning interspersed across sections. The
    context gets lost in abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: The prohibitive compute complexity also restricts fine-tuning on most real-world
    corpora limiting accuracy. Unsupervised learning provides one alternative but
    solid techniques are lacking.
  prefs: []
  type: TYPE_NORMAL
- en: In a [recent paper](https://arxiv.org/pdf/2303.01200.pdf), researchers address
    exactly these pitfalls by re-imagining relevance for ultra-long queries and documents.
    Their innovations unlock new potential for AI document search.
  prefs: []
  type: TYPE_NORMAL
- en: The Trouble with Long Documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dominant search paradigms today are ineffective for queries that run into thousands
    of words as input text. Key issues faced include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformers like BERT** have quadratic self-attention complexity, making
    them infeasible for sequences beyond 512–1024 tokens. Their sparse attention alternatives
    compromise on accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lexical models** matching based on exact term overlaps cannot infer semantic
    similarity critical for long-form text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lack of labelled training data for most domain collections necessitates **unsupervised
    or minimally-tuned approaches**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long documents covering multiple sub-topics require models that can **factor
    document structure into relevance judgment**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RPRS method aims to tackle these weaknesses in current retrieval architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the RPRS Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The RPRS model computes relevance between a long query document and candidate
    documents using the proportional matches across their sentences.
  prefs: []
  type: TYPE_NORMAL
- en: The critical insight is that documents containing a relatively higher proportion
    of sentences similar to sentences from the query are likely more pertinent overall.
  prefs: []
  type: TYPE_NORMAL
- en: 'The approach consists of 3 key stages:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Sentence Encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sentences from queries and candidate documents are encoded into vectors using
    SBERT — an efficient transformer architecture for sentence embeddings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SBERT avoids quadratic complexity allowing incorporation of full document lengths.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2\. Most Relevant Sentence Sets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For each query sentence, find the k most similar candidate document sentences
    based on vector embeddings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine sets of the most relevant document sentences for every query sentence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3\. Proportion-based Relevance Scoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Define Query Proportion (QP) — the relative proportion of query sentences that
    have similarity to document sentences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define Document Proportion (DP) — the relative proportion of document sentences
    that are similar to query sentences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combine QP and DP to compute a final relevance score estimating the inter-relatedness
    of the texts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The proportional relevance concept intrinsically accounts for document structure
    within long-form text.
  prefs: []
  type: TYPE_NORMAL
- en: An extension called RPRS w/freq additionally factors *term frequency* and *length
    normalization* inspired by BM25 to handle repetition and length bias.
  prefs: []
  type: TYPE_NORMAL
- en: The Proportional Relevance Formulation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the heart of the RPRS method lies a simple yet powerful relevance scoring
    formula between a query document `q` and candidate document `d`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '`QP(q, d)` is the **Query Proportion**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DP(q, d)` is the **Document Proportion**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RPRS_q(d)` is the **Proportional Relevance Score**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These proportion factors aim to quantify the inter-relatedness between the query
    and candidate document texts from both perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: Query Proportion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Query Proportion determines what percentage of the query content is similar
    to some part of the document.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each sentence `q_s` in query `q`:'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve top `n` most similar sentences `d_si` from document `d`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Count # query sentences that have at least 1 similar `d_si`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Divide by total # query sentences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: A higher QP indicates more of the query finds matches in the document.
  prefs: []
  type: TYPE_NORMAL
- en: Document Proportion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Document Proportion conversely determines what percentage of the document
    is similar to some part of the query.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each sentence `d_s` in document `d`:'
  prefs: []
  type: TYPE_NORMAL
- en: Check if `d_s` occurs in the top `n` matches `d_si` for any `q_s`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Count # document sentences that match some `q_s`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Divide by total # document sentences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A higher DP indicates more of the document content matches the query.
  prefs: []
  type: TYPE_NORMAL
- en: Combining the Factors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Proportional Relevance Score is then simply the product of Query Proportion
    and Document Proportion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The net effect is rewarding documents that maximally cover the query as well
    as get maximally covered by the query — indicating comprehensive semantic similarity
    from both angles.
  prefs: []
  type: TYPE_NORMAL
- en: Results on Legal, Patent and Wikipedia Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The researchers comprehensively evaluated RPRS on five long-document datasets
    spanning legal case retrieval, patent search and Wikipedia document similarity
    tasks containing thousands of words.
  prefs: []
  type: TYPE_NORMAL
- en: On all datasets, RPRS significantly outperformed previous state-of-the-art techniques
    as well as lexical and neural baselines while using just 3 tuned parameters demonstrating
    its effectiveness. Component importance analysis further validated the proportional
    scoring approach.
  prefs: []
  type: TYPE_NORMAL
- en: The method combines semantic matching capability through vector embeddings with
    an intuitive notion of topical relevance across sentences providing interpretable
    high accuracy retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing Long Standing Limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The RPRS model highlights that key ideas from classic retrieval augmented with
    modern NLP representations can push boundaries on challenging domains like searching
    legal corpora and scientific literature which have resisted high-performance automation
    so far.
  prefs: []
  type: TYPE_NORMAL
- en: In doing so, it also expands the scope of neural search paradigms to ultra-long
    text where most mature models face limitations today. More broadly, designing
    architectures around basic principles of relevance tailored for complex document
    collections remains a fertile area for innovation in search technology.
  prefs: []
  type: TYPE_NORMAL
- en: The paper provides a compelling blueprint for adaptation, but much room remains
    for integration with large language models, explainability as well as user experience
    advances in commercial document search solutions by learning from this approach.
  prefs: []
  type: TYPE_NORMAL
