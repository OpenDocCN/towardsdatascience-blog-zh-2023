["```py\n\"\"\"Downloading the BLIP-2 Architecture\nloading as an 8 bit integer to save on GPU memory. This may have some impact on\nperformance.\n\"\"\"\n\nfrom transformers import AutoProcessor, Blip2ForConditionalGeneration\nimport torch\n\nprocessor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", device_map=\"auto\", load_in_8bit=True) # load in int8\n```", "```py\n\"\"\"Loading and displaying a sample image\n\"\"\"\n\nimport requests\nfrom PIL import Image\n\nurl = 'https://github.com/DanielWarfield1/MLWritingAndResearch/blob/main/Assets/Images/pexels-thuany-marcante-1805053.jpg?raw=true'\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\n\nprint(f'Image dimensions: {image.width}px X {image.height}px')\n\ndsfact = 15\ndisplay(image.resize((int(image.width/dsfact), int(image.height/dsfact))))\n```", "```py\n\"\"\"Exploring the outputs of the processor\n\"\"\"\nprocessor_result = processor(image, text='a prompt from the user about the image', return_tensors=\"pt\").to(\"cpu\", torch.float16)\nprocessor_result.keys()\n```", "```py\n\"\"\"Understanding resolution and plotting one of the color channels\n\"\"\"\n\nimport matplotlib.pyplot as plt\n\n#printing the processed image shape\nprint(f'processed image shape: {processor_result[\"pixel_values\"].numpy().shape}')\n\n#extracting one of the color channels from the processed image\nprint('single color channel:')\nprocessed_im_c0 = processor_result['pixel_values'].numpy()[0,0]\n\n#rendering\nplt.imshow(processed_im_c0, interpolation='nearest')\nplt.show()\n```", "```py\n\"\"\"Understanding the distribution of values allong each color channel,\nboth in the processed image and in the original image\n\"\"\"\n\nbins = 100\n\n#extracting all color channels from the processed image\nprocessed_im_c1 = processor_result['pixel_values'].numpy()[0,1]\nprocessed_im_c2 = processor_result['pixel_values'].numpy()[0,2]\n\n#plotting modified pixel value distributions\n\nplt.figure()\nplt.hist([processed_im_c0.flatten(),\n          processed_im_c1.flatten(),\n          processed_im_c2.flatten()], bins, stacked=True, density = True)\nplt.title('processed image value distribution')\nplt.show()\n\n#plotting original pixel value distributions\nimport numpy as np\nimage_np = np.array(image)\n\nplt.figure()\nplt.hist([image_np[:,:,0].flatten(),\n          image_np[:,:,1].flatten(),\n          image_np[:,:,2].flatten()], bins, stacked=True, density = True)\nplt.title('raw image value distribution')\nplt.show()\n```", "```py\n\"\"\"Exploring the input_ids from the processor given a variety of prompts\n\"\"\"\n\nprint('input_ids for \"a short prompt\":')\nsampres = processor(image, text='a short prompt', return_tensors=\"pt\").to(\"cpu\", torch.float16)\nprint(sampres['input_ids'])\n\nprint('input_ids for \"a much much much much longer prompt\":')\nsampres = processor(image, text='a much much much much longer prompt', return_tensors=\"pt\").to(\"cpu\", torch.float16)\nprint(sampres['input_ids'])\n\nprint('input_ids for \"alongcompoundword\":')\nsampres = processor(image, text='alongcompoundword', return_tensors=\"pt\").to(\"cpu\", torch.float16)\nprint(sampres['input_ids'])\n```", "```py\n\"\"\"Understanding the mask from the processor\n\"\"\"\n\nprint('input_ids for \"a short prompt\":')\nsampres = processor(image, text='a short prompt', return_tensors=\"pt\").to(\"cpu\", torch.float16)\nprint(sampres['input_ids'])\nprint('mask for \"a short prompt\":')\nprint(sampres['attention_mask'])\n```", "```py\n\"\"\"Getting BLIP-2 to describe the image, unprompted\nthis is done by only passing the image, not the text\n\"\"\"\ninputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=20)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\nprint(generated_text)\n```", "```py\n\"\"\"Prompted caption example 1\n\"\"\"\n\nprompt = \"this is a picture of\"\n\ninputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=20)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\nprint(generated_text)\n```", "```py\n\"\"\"Prompted caption example 2\n\"\"\"\n\nprompt = \"the weather looks\"\n\ninputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=20)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\nprint(generated_text)\n```", "```py\nprompt = \"Question: what season is it? Answer:\"\n\ninputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=20)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\nprint(generated_text)\n```", "```py\n\"\"\"Visual Conversation\nconversing about the image\n\"\"\"\n\n#imagine these are generated by a person as a response to output, rather than pre-defined.\nquestions = [\n    \"What's in this photo?\",\n    \"What is vernacular architecture?\"\n]\n\n#defining the state of the conversation as it progresses, to be passed to the model\nconv_state = ''\n\n#asking all questions in order\nfor question in questions:\n    #updating the conversational state with the question\n    conv_state = conv_state+' Question: ' + question + ' Answer: '\n\n    #passing the state thus far to the model\n    inputs = processor(image, text=conv_state, return_tensors=\"pt\").to(device, torch.float16)\n\n    #generating a response\n    generated_ids = model.generate(**inputs, max_new_tokens=40)\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n\n    #rendering conversation\n    print('Question: '+question)\n    print('Answer: ' + generated_text)\n\n    #updating the conversational state with the answer\n    conv_state = conv_state + generated_text + '\\n'\n```"]