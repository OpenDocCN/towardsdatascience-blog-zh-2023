- en: Forward Pass & Backpropagation In Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/forward-pass-backpropagation-neural-networks-101-3a75996ada3b](https://towardsdatascience.com/forward-pass-backpropagation-neural-networks-101-3a75996ada3b)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explaining how neural networks “train” and “learn” patterns in data by hand
    and in code using PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@egorhowell?source=post_page-----3a75996ada3b--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----3a75996ada3b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3a75996ada3b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3a75996ada3b--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----3a75996ada3b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3a75996ada3b--------------------------------)
    ·10 min read·Nov 4, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a894bd9e34501e8413c8bf96a918f1e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Neural network icons created by juicy_fish — Flaticon. [https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network).
  prefs: []
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In my past two articles, we dived into the origins of the neural network from
    a single [***perceptron***](https://en.wikipedia.org/wiki/Perceptron)to a large
    interconnected ([***multi-layer perceptron***](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    ***(MLP)***) non-linear optimisation engine. I highly recommend you check my previous
    posts if you are unfamiliar with the perceptron, MLP, and activation functions
    as we will discuss quite a bit in this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----3a75996ada3b--------------------------------)
    [## Intro, Perceptron & Architecture: Neural Networks 101'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to Neural Networks and their building blocks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----3a75996ada3b--------------------------------)
    [](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----3a75996ada3b--------------------------------)
    [## Activation Functions & Non-Linearity: Neural Networks 101'
  prefs: []
  type: TYPE_NORMAL
- en: Explaining why neural networks can learn (nearly) anything and everything
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----3a75996ada3b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it’s time to understand how these neural networks get “trained” and “learn”
    the patterns in the data that you pass into it. There are two key components:
    [***forward pass***](https://theneuralblog.com/forward-pass-backpropagation-example/)
    and [***backpropagation***](https://en.wikipedia.org/wiki/Backpropagation). Let’s
    get into it!'
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s quickly recap the general structure of a neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94cdac9897a5e852d9cd6edd425fcba0.png)'
  prefs: []
  type: TYPE_IMG
- en: A basic two-hidden multi-layer perceptron. Diagram by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Where each hidden neuron is carrying out the following process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57f8921c8f7e374d7f075b2f4ae32e6b.png)'
  prefs: []
  type: TYPE_IMG
- en: The process carried out inside each neuron. Diagram by author.
  prefs: []
  type: TYPE_NORMAL
- en: '***Inputs:*** *These are the features of our data.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Weights:*** *Some coefficients we multiply the inputs by. The goal of the
    algorithm is to find the most optimal weights.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Linear Weighted Sum:*** *Sum up the products of the inputs and weights and
    add a bias/offset term,* ***b****.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Hidden Layer:*** *This is where the multiple neurons are stored to learn
    patterns in the data. The superscript refers to the layer and the subscript to
    the neuron/perceptron in that layer.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Edges/Arrows:*** *These are the weights for the network from the corresponding
    inputs, whether it be the features or the hidden layer outputs. I have omitted
    them for a cleaner plot.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[***ReLU Activation Function***](https://en.wikipedia.org/wiki/Heaviside_step_function)***:***
    *The most popular* [***activation function***](https://en.wikipedia.org/wiki/Activation_function)
    *as it is computationally efficient and intuitive.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forward Pass
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first part of training a neural network is getting it to generate a prediction.
    This is called a **forward pass** and is where the data is traversed through all
    the neurons from the first to the last layer (also known as the output layer).
  prefs: []
  type: TYPE_NORMAL
- en: For this article, we will do the forward pass by hand. In reality, we would
    use a package such as [***PyTorch***](https://pytorch.org/) or [***TensorFlow***](https://www.tensorflow.org/)***.***
    But this will give us a great intuition behind the process.
  prefs: []
  type: TYPE_NORMAL
- en: Example Neural Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The neural network that we will perform the forward pass is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07e85418b0df7415133c2c8500b7e803.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple neural network. Diagram by author.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, it is quite simple using two inputs, two neurons in the hidden
    layer (with a *ReLU* activation function), and a single prediction (output layer).
  prefs: []
  type: TYPE_NORMAL
- en: Weights & Biases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now create some arbitrary weights, biases, and inputs for this simple
    network:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input values:** *[0.9, 1.0]*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Target/True value:** *[2.0]*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input to hidden layer weights, *W_1:***'
  prefs: []
  type: TYPE_NORMAL
- en: '*Neuron 1: W_{1,1} = [0.2, 0.3]*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Neuron 2: W_{1,2} = [0.4, 0.5]*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden layer biases, *b_1*:** *[0.1, 0.2]*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hidden to output layer weights,** ***W_2***: *[0.5, 0.6]*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output layer bias, *b_2***: *[0.4]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s important to note that I have randomly generated the initial weights and
    biases in this case. This is not a bad thing and you can achieve good results
    with a completely random initialization. However, there are more sophisticated
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Xavier**](https://365datascience.com/tutorials/machine-learning-tutorials/what-is-xavier-initialization/#h_75113286374001686829810816):
    *Suitable for* [***sigmoid***](https://en.wikipedia.org/wiki/Sigmoid_function)
    *and* [***tanh***](https://en.wikipedia.org/wiki/Hyperbolic_functions) *activation
    functions. It generates random weights from a uniform distribution that utilizes
    the number of inputs to that node to set the distribution range.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**He**](https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/):
    *Suitable for the ReLU activation function. It generates random weights from a
    normal distribution that utilizes the number of inputs to that node to set the
    standard deviation.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to learn more about these initialization methods, then check out
    the links tagged in the list above.
  prefs: []
  type: TYPE_NORMAL
- en: One last important note about weight initialization is to ensure they are different
    so we ‘[**break symmetry.**](/neural-network-breaking-the-symmetry-e04f963395dd)’
    If the neurons in the same layer start with the same weights, then they will most
    likely be updated exactly equally. Therefore, the network will not converge and
    fail to learn anything.
  prefs: []
  type: TYPE_NORMAL
- en: First Forward Pass
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the data listed above, we are now in a position to carry out our first
    forward pass! This looks like as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The linear weighted sum, *z¹_{1}* and *z¹_{2},* from the inputs to the hidden
    layers is:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/adbe828addffb5cfa63bd36204d1787f.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: Remember the superscript denotes the layer and subscript the neuron in that
    layer. The first layer is considered to be the one straight after the input layer,
    this is because the input layer doesn’t do any calculations. So, in our case,
    we have a 2-layer network.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Here, we have utilized the [***dot product***](https://en.wikipedia.org/wiki/Dot_product)
    to simplify and condense the calculation. **Now, we can perform the ReLU activation
    function, *a¹_{1,1}* and *a¹_{1,2},* in the hidden layer:**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/90e70a103b4f3eba781e350a935ae01f.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: '**And the final bit we need to do is generate the output,** *z²_{1*}**, this
    doesn’t have an activation function associated with it:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e78c0ef07eced4faaa31e59a9eb6501a.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: Voila, we have just done our first forward pass!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Note: I have condensed the expression by writing the weights as a vector/matrix.
    This is how it’s frequently done in literature to make the workflow neater.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The forward pass is also visualized below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72afd49009b5268e207f6ab02e502288.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple neural network with its weights, biases and outputs. Diagram by author.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After doing our forward pass, we are now in a position to start updating our
    weights and biases to minimise the error of the prediction from the network. The
    way the weights and biases are updated is through the *backpropagation* algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Compute Graph, Chain Rule & Gradient Descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s now gain some intuition behind this algorithm. Backpropagation aims to
    get the partial derivative of every weight and bias with respect to the error
    (loss). Then update each parameter using [***gradient descent***](/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c),
    so that we minimise the error (loss) caused by each parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Right, this may make sense on paper, but I appreciate it can still seem a bit
    arbitrary. I want to go through a really “simple” example using [***compute graphs***](https://www.tutorialspoint.com/python_deep_learning/python_deep_learning_computational_graphs.htm).
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8cb287c88da22f545f8c2a619bdff87.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can plot it as a compute graph, which is just another way of visualising
    the calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4047cf68f641963e1b2e1eeb0d893a1e.png)'
  prefs: []
  type: TYPE_IMG
- en: Compute graph example. Diagram by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s basically a flowchart on how we calculate ***f(x,y,z).*** I have also
    expressed ***p=x-y***. Let’s now plug in some numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/deb6bcaa4c35868c02d2abd5978aa53e.png)'
  prefs: []
  type: TYPE_IMG
- en: Compute graph example with numbers. Diagram by author.
  prefs: []
  type: TYPE_NORMAL
- en: '*This all looks pretty good and intuitive so far!*'
  prefs: []
  type: TYPE_NORMAL
- en: The way we calculate the minimum of ***f(x,y,z)*** is using calculus. Particularly,
    we need to know the partial derivative of ***f(x,y,z)*** with respect to all three
    of its variables ***x, y,*** and ***z***.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can start by calculating the partial derivatives for ***p=x-y*** and ***f=pz***:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/faa5838ed3085ddda4d76e6f939804bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: '*But, how do we go about getting?*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52687bd4c8162d292dd4b26310773380.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, we use the[***chain rule!***](https://en.wikipedia.org/wiki/Chain_rule)This
    is an example for ***x***:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95137bfc8b2891a2d09180e6b82b98d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: 'By combining different partial derivatives, we can get our desired expression.
    So, for the example above:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7568e02dcb2b420c7e448156b669dda2.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: The gradient of the output, ***f***, with respect to ***x*** is ***z***. This
    makes sense as ***z*** is the only value we multiply ***x.***
  prefs: []
  type: TYPE_NORMAL
- en: 'Repeating for ***y*** and ***z***:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f56f6e7e20f83bf96d3cd44d9cbb159b.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can write these gradients and their corresponding values on the compute
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/578b82e9ed62175c510b6eada2f180ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Compute graph example with numbers and gradients. Diagram by author.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent works by updating the values (***x,y,z***) by a small amount
    in the opposite direction of the gradient. The goal of gradient descent is to
    try and minimise the output function. For example, for ***x:***
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5866f8f21dd278964bc6c1743a9eeed7.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: Where ***h*** is called the [***learning rate***](https://deepchecks.com/glossary/learning-rate-in-machine-learning/#:~:text=The%20learning%20rate%2C%20denoted%20by,network%20concerning%20the%20loss%20gradient%3E.)
    and decides how much the parameter will get updated. For this case, let’s define
    ***h=0.1***, so ***x=3.7***. What is the output now?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95a57e60032255539a1cbb03d242367b.png)'
  prefs: []
  type: TYPE_IMG
- en: Compute graph example with numbers and gradients after performing gradient descent.
    Diagram by author.
  prefs: []
  type: TYPE_NORMAL
- en: The output got smaller, in other words, it’s getting minimised!
  prefs: []
  type: TYPE_NORMAL
- en: This example was inspired by [Andrej Karpathy](https://www.youtube.com/@andrejkarpathy4906)
    [video on YouTube](https://www.youtube.com/watch?v=i94OvYb6noo&t=358s). I highly
    recommend you check it out to get a thorough understanding of this process.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Applying Backpropagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*How does all that stuff above link into training neural networks?*'
  prefs: []
  type: TYPE_NORMAL
- en: Well, a neural network is just a compute graph! The inputs are analogous to
    ***x, y***, and ***z*** above and the operations gates are analogous to the activation
    functions inside the neurons.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now apply this process to the simple example neural network we had above.
    Remember, our prediction was ***1.326,*** and let’s say the target is ***2.0***.
    The loss function (error) in this prediction could be the [***mean squared error***](https://en.wikipedia.org/wiki/Mean_squared_error):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ad5582ee7f5a12428e564dfcdc0e90b.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: Remember the 2 attached to z is the layer number, it's not a power term!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The next step is to calculate the gradient of the loss with respect to the
    prediction/output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ae7e2e12dc0717c6c9301f931e3ed3b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to calculate the gradient of the loss with respect to the output
    layers’ bias and weights: ***W_2*** and ***b_2:***'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02746a72459aeb1efc366008137b9b49.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t worry if the above expressions seem scary. All we have done is partially
    differentiated and applied the chain rule several times.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab1122a7115e3527d82642c02ee69d5a.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is to update the parameters using gradient descent, where I have
    set the learning rate, ***h=0.1***:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/99bebba4589568a8cea2d113f6dbea10.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: Voila, we have updated the output layers’ weights and bias!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The next thing is to replicate this process for the hidden layers’ weights
    and biases. To start, we need to find the loss with respect to the activation
    function outputs, ***a¹_{1,1}* and *a¹_{1,2}***:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1e00c4f35f203e86ec6f6eee5487b668.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we find the derivatives for **z*¹_{1}* and z*¹_{2}*:**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cb7bdd6f67b656c6e9762ea7dda02a5a.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: 'For some context:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8f36e173b6bc3c4622c411947510c85e.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next bit is finding the loss with respect to ***W_{1,1}***and ***W_{1,2}.***
    Before that, I want to redefine some notation just to make it clear which weight
    we are working with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6289f16900a84c4fed7ebcbe97d64af5.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/80cbe028c9930838a828747967d7f811.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: 'And so, the partial derivative of the loss with respect to the weights and
    biases are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b9cfe0f15de03a04cd2da7ce0ed431d.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the last step is to update these values using gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd22b10d72a82642a99a80a0cfce5973.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation generated by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: WOW. We have just together completed one whole iteration of backpropagation!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: One cycle of a forward pass and backward pass (backpropagation) is known as
    an [**epoch**](https://www.baeldung.com/cs/epoch-neural-networks).
  prefs: []
  type: TYPE_NORMAL
- en: The next step would be to carry out another forward pass using the updated weights
    and biases. The result of this forward pass with the new weights and biases would
    be **1.618296**. This is closer to the target value of **2**, so the network has
    ‘learned’ better weights and biases. This is ‘machine learning’ in action. Truly
    amazing!
  prefs: []
  type: TYPE_NORMAL
- en: In practice, this algorithm would run for 100 or 1000s of epochs. Luckily, there
    exist packages for this process, as doing it by hand would be very tedious!
  prefs: []
  type: TYPE_NORMAL
- en: Extra Detail
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One can see why this process is called backpropagation as we are propagating
    the error (derivatives) backward at each layer of the network. Once you get the
    hang of it, it’s pretty simple to understand. So, I highly recommend you go through
    this process slowly, ideally by hand yourself and I promise it will just click!
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Doing by hand, as we did above, is quite exhaustive and this was for a tiny
    network. However, we can leverage PyTorch (a deep learning library) to do all
    this heavy lifting for us:'
  prefs: []
  type: TYPE_NORMAL
- en: GitHub Gist by author.
  prefs: []
  type: TYPE_NORMAL
- en: So, the whole forward pass and backpropagation process that we wrote out by
    hand can be done in ~50 lines of code! Power of Python!
  prefs: []
  type: TYPE_NORMAL
- en: However, due to randomization, floating point precision, and other computer
    factors the weights, biases, and predictions may not match exactly to the hand
    calculations. This is not a problem, but it’s important to be aware of it.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about PyTorch, check out the [tutorials on their site](https://pytorch.org/tutorials/).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Summary & Further Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we went over how neural networks generate predictions and learn
    from their errors. The process revolves around updating the network’s parameter
    using partial differential against the loss error. The algorithm is backpropagation
    as it propagates the error backward through each layer using the chain rule. Backpropagation
    is quite intuitive once you gain the hang of it, but very tedious particularly
    for large neural networks. This is where we use deep learning libraries such as
    PyTorch that do most of the heavy lifting for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code used in this article is available at my GitHub here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Neural%20Networks/basic_foward_backward_pass.py?source=post_page-----3a75996ada3b--------------------------------)
    [## Medium-Articles/Neural Networks/basic_foward_backward_pass.py at main · egorhowell/Medium-Articles'
  prefs: []
  type: TYPE_NORMAL
- en: Code I use in my medium blog/articles. Contribute to egorhowell/Medium-Articles
    development by creating an account on…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Neural%20Networks/basic_foward_backward_pass.py?source=post_page-----3a75996ada3b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References & Further Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Andrej Karpathy Neural Network Course*](https://www.youtube.com/watch?v=i94OvYb6noo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*PyTorch site*](https://pytorch.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Another example of training a neural network by hand*](/training-a-neural-network-by-hand-1bcac4d82a6e)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another Thing!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I have a free newsletter, [**Dishing the Data**](https://dishingthedata.substack.com/),
    where I share weekly tips for becoming a better Data Scientist. There is no “fluff”
    or “clickbait,” just pure actionable insights from a practicing Data Scientist.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://newsletter.egorhowell.com/?source=post_page-----3a75996ada3b--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
  prefs: []
  type: TYPE_NORMAL
- en: How To Become A Better Data Scientist. Click to read Dishing The Data, by Egor
    Howell, a Substack publication with…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----3a75996ada3b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Connect With Me!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[**YouTube**](https://www.youtube.com/@egorhowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Twitter**](https://twitter.com/EgorHowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**GitHub**](https://github.com/egorhowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
