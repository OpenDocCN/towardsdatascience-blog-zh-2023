- en: Breaking Linearity With ReLU
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用 ReLU 打破线性
- en: 原文：[https://towardsdatascience.com/breaking-linearity-with-relu-d2cfa7ebf264](https://towardsdatascience.com/breaking-linearity-with-relu-d2cfa7ebf264)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/breaking-linearity-with-relu-d2cfa7ebf264](https://towardsdatascience.com/breaking-linearity-with-relu-d2cfa7ebf264)
- en: Explaining how and why the ReLU activation function is non-linear
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释 ReLU 激活函数如何以及为何是非线性的
- en: '[](https://medium.com/@egorhowell?source=post_page-----d2cfa7ebf264--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----d2cfa7ebf264--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d2cfa7ebf264--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d2cfa7ebf264--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----d2cfa7ebf264--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@egorhowell?source=post_page-----d2cfa7ebf264--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----d2cfa7ebf264--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d2cfa7ebf264--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d2cfa7ebf264--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----d2cfa7ebf264--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d2cfa7ebf264--------------------------------)
    ·4 min read·Mar 1, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [数据科学前沿](https://towardsdatascience.com/?source=post_page-----d2cfa7ebf264--------------------------------)
    ·阅读时间4分钟·2023年3月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e9eb0239dd7d7e0b58720e6193479978.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9eb0239dd7d7e0b58720e6193479978.png)'
- en: Photo by [Alina Grubnyak](https://unsplash.com/@alinnnaaaa?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Alina Grubnyak](https://unsplash.com/@alinnnaaaa?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '[***Neural networks***](https://en.wikipedia.org/wiki/Artificial_neural_network)
    and [***deep learning***](https://en.wikipedia.org/wiki/Deep_learning) are assumably
    one of the most popular reasons people transition into data science. However,
    this excitement can lead to overlooking the core concepts that make neural networks
    tick. In this post, I want to go over probably the most key feature of neural
    networks, which I think most practitioners should be aware of to fully understand
    what is happening under the hood.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[***神经网络***](https://en.wikipedia.org/wiki/Artificial_neural_network) 和 [***深度学习***](https://en.wikipedia.org/wiki/Deep_learning)
    是人们转行进入数据科学的最受欢迎的原因之一。然而，这种兴奋可能会导致忽视神经网络的核心概念。在这篇文章中，我想讨论神经网络的一个关键特性，我认为大多数从业者应该了解，以充分理解其内部运作。'
- en: Why We Need Activation Functions?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们为什么需要激活函数？
- en: '[***Activation functions***](https://en.wikipedia.org/wiki/Activation_function)
    are ubiquitous in data science and machine learning. They typically refer to the
    transformation that’s applied to the [***linear***](https://en.wikipedia.org/wiki/Linear_equation)
    input of a neuron in a neural network***:***'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[***激活函数***](https://en.wikipedia.org/wiki/Activation_function) 在数据科学和机器学习中无处不在。它们通常指的是应用于神经网络中神经元的
    [***线性***](https://en.wikipedia.org/wiki/Linear_equation) 输入的变换***：***'
- en: '![](../Images/60e9f3aa07b7135e6c0eadb2b8a184ca.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60e9f3aa07b7135e6c0eadb2b8a184ca.png)'
- en: Equation by author in LaTeX.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在 LaTeX 中的方程。
- en: Where ***f*** is the activation function, ***y*** is the output, ***b*** is
    the bias, ***and w_i*** and ***x_i*** are the [***weights***](https://en.wikipedia.org/wiki/Weighting)
    and their corresponding feature values.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ***f*** 是激活函数，***y*** 是输出，***b*** 是偏置，***w_i*** 和 ***x_i*** 是 [***权重***](https://en.wikipedia.org/wiki/Weighting)
    和它们对应的特征值。
- en: But, why do we need activation functions?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我们为什么需要激活函数呢？
- en: The simple answer is that they allow us to model complex patterns and they do
    this by making the neural network [***non-linear***](https://en.wikipedia.org/wiki/Nonlinear_system).
    If there are no non-linear activation functions in the network, the whole model
    just becomes a [***linear regression***](https://en.wikipedia.org/wiki/Linear_regression)
    model!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的答案是，它们使我们能够建模复杂的模式，且通过使神经网络变得 [***非线性***](https://en.wikipedia.org/wiki/Nonlinear_system)
    来实现。如果网络中没有非线性激活函数，那么整个模型就会变成一个 [***线性回归***](https://en.wikipedia.org/wiki/Linear_regression)
    模型！
- en: Non-linear is a change to the input that is not proportional to the change in
    the corresponding output.
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 非线性是指输入的变化与相应输出的变化不成比例。
- en: 'For example, consider a feed-forward two-layer neural network with two neurons
    in the middle layer (ignoring the bias terms):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个前馈的两层神经网络，中间层有两个神经元（忽略偏置项）：
- en: '![](../Images/7e743bf6079fe4879b43c69e4dc07d27.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e743bf6079fe4879b43c69e4dc07d27.png)'
- en: Equation by author in LaTeX.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作者用 LaTeX 写的方程。
- en: We have managed to condense our 2-layer network into a single-layer network!
    The final equation, in the above derivation, is just simply a linear regression
    model with features ***x_1*** and ***x_2*** and their corresponding coefficients***.***
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功将我们的两层网络简化为单层网络！上述推导中的最终方程仅仅是一个具有特征 ***x_1*** 和 ***x_2*** 及其对应系数的线性回归模型***。***
- en: So our ‘deep neural network’ would collapse to a single layer and become the
    good old linear regression model! This is not good as the neural network won’t
    be able to model or fit complex functions to the data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的“深度神经网络”将会简化为单层，变成传统的线性回归模型！这不好，因为神经网络将无法对数据建模或拟合复杂函数。
- en: Neural networks can compute any function due to something called the [**Universal
    Approximation Theorem**](https://en.wikipedia.org/wiki/Universal_approximation_theorem).
    Check out this post [here](http://mcneela.github.io/machine_learning/2017/03/21/Universal-Approximation-Theorem.html)
    if you want to learn more!
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 神经网络能够计算任何函数，这要归功于所谓的 [**通用近似定理**](https://en.wikipedia.org/wiki/Universal_approximation_theorem)。如果你想了解更多，可以查看[这篇文章](http://mcneela.github.io/machine_learning/2017/03/21/Universal-Approximation-Theorem.html)！
- en: 'The formal mathematical definition for a linear function is:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 线性函数的正式数学定义是：
- en: '![](../Images/5ff20f8ab0a88e38e57c203934156124.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ff20f8ab0a88e38e57c203934156124.png)'
- en: Equation by author in LaTeX.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 作者用 LaTeX 写的方程。
- en: 'And this is a very simple example:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的例子：
- en: '![](../Images/56782595607096e97dd3459e60dcc2a3.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56782595607096e97dd3459e60dcc2a3.png)'
- en: Equation by author in LaTeX.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作者用 LaTeX 写的方程。
- en: So the function ***f(x) = 10x*** is linear!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 所以函数 ***f(x) = 10x*** 是线性的！
- en: Note if we added a bias term to the above equation it’s no longer be a linear
    function but rather an [**affine function**](https://en.wikipedia.org/wiki/Affine_transformation).
    See [this statexchange thread](https://math.stackexchange.com/questions/275310/what-is-the-difference-between-linear-and-affine-function)
    discussing why this is the case.
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，如果我们在上面的方程中添加一个偏置项，它就不再是线性函数，而是一个 [**仿射函数**](https://en.wikipedia.org/wiki/Affine_transformation)。请参阅[这个状态交换讨论](https://math.stackexchange.com/questions/275310/what-is-the-difference-between-linear-and-affine-function)讨论为什么会这样。
- en: ReLU
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ReLU
- en: The [***rectified linear unit (ReLU)***](https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29)
    is the most popular activation function as it is computationally efficient and
    removes the issues with the [***vanishing gradient problem***](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[***整流线性单元 (ReLU)***](https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29)
    是最流行的激活函数，因为它计算高效，并且解决了[***梯度消失问题***](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)。'
- en: 'Mathematically the function reads:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，该函数表达为：
- en: '![](../Images/de16e655145b6cd29f0b6efa2cacdf22.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de16e655145b6cd29f0b6efa2cacdf22.png)'
- en: Equation by author in LaTeX.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 作者用 LaTeX 写的方程。
- en: 'We can visualise it graphically in Python:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用 Python 进行图形化展示：
- en: '![](../Images/0adbc408b3159c3e3806b4ed21036aab.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0adbc408b3159c3e3806b4ed21036aab.png)'
- en: Plot generated by author in Python.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者用 Python 生成的图。
- en: Why Is ReLU Non-Linear?
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么 ReLU 是非线性的？
- en: The ReLU function may appear to be linear, due to the two straight lines. In
    fact, it is actually [***piece-wise linear***](https://en.wikipedia.org/wiki/Piecewise_linear_function).
    However, it is precisely these two different straight lines that make it non-linear.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 函数可能看起来是线性的，因为有两条直线。实际上，它是[***分段线性***](https://en.wikipedia.org/wiki/Piecewise_linear_function)的。然而，正是这两条不同的直线使其成为非线性。
- en: 'We can show it is non-linear by carrying out the same example as above but
    with the ReLU function:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过执行与上述相同的示例但使用 ReLU 函数来证明它是非线性的：
- en: '![](../Images/b661a3f776feedd95e1badcbbf009673.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b661a3f776feedd95e1badcbbf009673.png)'
- en: Equation by author in LaTeX.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 作者用 LaTeX 写的方程。
- en: 'Lets break it down:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一下：
- en: '![](../Images/5856308135b9c0d14654e9cd613b35c6.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5856308135b9c0d14654e9cd613b35c6.png)'
- en: Equation by author in LaTeX.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 作者用 LaTeX 写的方程。
- en: Therefore, ReLU is non-linear!
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因此，ReLU 是非线性的！
- en: I have linked a good article [here](https://www.linkedin.com/pulse/rectified-linear-unit-non-linear-mukesh-manral/?trk=pulse-article_more-articles_related-content-card)
    that showcases how you can create any function using ReLU.
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我已经在[这里](https://www.linkedin.com/pulse/rectified-linear-unit-non-linear-mukesh-manral/?trk=pulse-article_more-articles_related-content-card)链接了一篇很好的文章，展示了如何使用
    ReLU 创建任何函数。
- en: Summary and Further Thoughts
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结和进一步的思考
- en: Non-linearity is essential in neural networks as it allows the algorithm to
    deduce complex patterns in the data. Non-linearity is accomplished by activation
    functions and the most famous one is the ReLU for computational efficiency and
    improving known issues when training neural networks. The ReLU function is piece-wise
    linear which is what causes it to be non-linear as we showed mathematically above.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性在神经网络中是至关重要的，因为它允许算法推断数据中的复杂模式。非线性是通过激活函数来实现的，其中最著名的是 ReLU，它在计算效率和解决训练神经网络时已知的问题方面表现优异。ReLU
    函数是分段线性的，这就是它如上所述在数学上表现为非线性的原因。
- en: 'The full code can be found on my GitHub here:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可以在我的 GitHub 上找到：
- en: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/relu.py?source=post_page-----d2cfa7ebf264--------------------------------)
    [## Medium-Articles/relu.py at main · egorhowell/Medium-Articles'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/relu.py?source=post_page-----d2cfa7ebf264--------------------------------)
    [## Medium-Articles/relu.py at main · egorhowell/Medium-Articles'
- en: You can't perform that action at this time. You signed in with another tab or
    window. You signed out in another tab or…
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你现在不能执行该操作。你在另一个标签页或窗口中登录了。你在另一个标签页或窗口中登出了……
- en: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/relu.py?source=post_page-----d2cfa7ebf264--------------------------------)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/relu.py?source=post_page-----d2cfa7ebf264--------------------------------)
- en: Another Thing!
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另一个事项！
- en: I have a free newsletter, [**Dishing the Data**](https://dishingthedata.substack.com/),
    where I share weekly tips for becoming a better Data Scientist. There is no “fluff”
    or “clickbait,” just pure actionable insights from a practicing Data Scientist.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一个免费的通讯，[**Dishing the Data**](https://dishingthedata.substack.com/)，在其中我每周分享成为更好的数据科学家的技巧。没有“虚浮内容”或“点击诱饵”，只有来自实际数据科学家的纯粹可操作的见解。
- en: '[](https://newsletter.egorhowell.com/?source=post_page-----d2cfa7ebf264--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://newsletter.egorhowell.com/?source=post_page-----d2cfa7ebf264--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
- en: How To Become A Better Data Scientist. Click to read Dishing The Data, by Egor
    Howell, a Substack publication with…
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何成为更好的数据科学家。点击阅读《Dishing The Data》，作者为 Egor Howell，这是一个 Substack 出版物……
- en: newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----d2cfa7ebf264--------------------------------)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----d2cfa7ebf264--------------------------------)'
- en: Connect With Me!
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与我联系！
- en: '[**YouTube**](https://www.youtube.com/@egorhowell)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**YouTube**](https://www.youtube.com/@egorhowell)'
- en: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
- en: '[**Twitter**](https://twitter.com/EgorHowell)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Twitter**](https://twitter.com/EgorHowell)'
- en: '[**GitHub**](https://github.com/egorhowell)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**GitHub**](https://github.com/egorhowell)'
