- en: Breaking Linearity With ReLU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/breaking-linearity-with-relu-d2cfa7ebf264](https://towardsdatascience.com/breaking-linearity-with-relu-d2cfa7ebf264)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explaining how and why the ReLU activation function is non-linear
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@egorhowell?source=post_page-----d2cfa7ebf264--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----d2cfa7ebf264--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d2cfa7ebf264--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d2cfa7ebf264--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----d2cfa7ebf264--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d2cfa7ebf264--------------------------------)
    ·4 min read·Mar 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e9eb0239dd7d7e0b58720e6193479978.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Alina Grubnyak](https://unsplash.com/@alinnnaaaa?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[***Neural networks***](https://en.wikipedia.org/wiki/Artificial_neural_network)
    and [***deep learning***](https://en.wikipedia.org/wiki/Deep_learning) are assumably
    one of the most popular reasons people transition into data science. However,
    this excitement can lead to overlooking the core concepts that make neural networks
    tick. In this post, I want to go over probably the most key feature of neural
    networks, which I think most practitioners should be aware of to fully understand
    what is happening under the hood.'
  prefs: []
  type: TYPE_NORMAL
- en: Why We Need Activation Functions?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[***Activation functions***](https://en.wikipedia.org/wiki/Activation_function)
    are ubiquitous in data science and machine learning. They typically refer to the
    transformation that’s applied to the [***linear***](https://en.wikipedia.org/wiki/Linear_equation)
    input of a neuron in a neural network***:***'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/60e9f3aa07b7135e6c0eadb2b8a184ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: Where ***f*** is the activation function, ***y*** is the output, ***b*** is
    the bias, ***and w_i*** and ***x_i*** are the [***weights***](https://en.wikipedia.org/wiki/Weighting)
    and their corresponding feature values.
  prefs: []
  type: TYPE_NORMAL
- en: But, why do we need activation functions?
  prefs: []
  type: TYPE_NORMAL
- en: The simple answer is that they allow us to model complex patterns and they do
    this by making the neural network [***non-linear***](https://en.wikipedia.org/wiki/Nonlinear_system).
    If there are no non-linear activation functions in the network, the whole model
    just becomes a [***linear regression***](https://en.wikipedia.org/wiki/Linear_regression)
    model!
  prefs: []
  type: TYPE_NORMAL
- en: Non-linear is a change to the input that is not proportional to the change in
    the corresponding output.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For example, consider a feed-forward two-layer neural network with two neurons
    in the middle layer (ignoring the bias terms):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e743bf6079fe4879b43c69e4dc07d27.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: We have managed to condense our 2-layer network into a single-layer network!
    The final equation, in the above derivation, is just simply a linear regression
    model with features ***x_1*** and ***x_2*** and their corresponding coefficients***.***
  prefs: []
  type: TYPE_NORMAL
- en: So our ‘deep neural network’ would collapse to a single layer and become the
    good old linear regression model! This is not good as the neural network won’t
    be able to model or fit complex functions to the data.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks can compute any function due to something called the [**Universal
    Approximation Theorem**](https://en.wikipedia.org/wiki/Universal_approximation_theorem).
    Check out this post [here](http://mcneela.github.io/machine_learning/2017/03/21/Universal-Approximation-Theorem.html)
    if you want to learn more!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The formal mathematical definition for a linear function is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ff20f8ab0a88e38e57c203934156124.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: 'And this is a very simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56782595607096e97dd3459e60dcc2a3.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: So the function ***f(x) = 10x*** is linear!
  prefs: []
  type: TYPE_NORMAL
- en: Note if we added a bias term to the above equation it’s no longer be a linear
    function but rather an [**affine function**](https://en.wikipedia.org/wiki/Affine_transformation).
    See [this statexchange thread](https://math.stackexchange.com/questions/275310/what-is-the-difference-between-linear-and-affine-function)
    discussing why this is the case.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ReLU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The [***rectified linear unit (ReLU)***](https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29)
    is the most popular activation function as it is computationally efficient and
    removes the issues with the [***vanishing gradient problem***](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically the function reads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de16e655145b6cd29f0b6efa2cacdf22.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can visualise it graphically in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0adbc408b3159c3e3806b4ed21036aab.png)'
  prefs: []
  type: TYPE_IMG
- en: Plot generated by author in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Why Is ReLU Non-Linear?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ReLU function may appear to be linear, due to the two straight lines. In
    fact, it is actually [***piece-wise linear***](https://en.wikipedia.org/wiki/Piecewise_linear_function).
    However, it is precisely these two different straight lines that make it non-linear.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can show it is non-linear by carrying out the same example as above but
    with the ReLU function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b661a3f776feedd95e1badcbbf009673.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lets break it down:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5856308135b9c0d14654e9cd613b35c6.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, ReLU is non-linear!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I have linked a good article [here](https://www.linkedin.com/pulse/rectified-linear-unit-non-linear-mukesh-manral/?trk=pulse-article_more-articles_related-content-card)
    that showcases how you can create any function using ReLU.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Summary and Further Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Non-linearity is essential in neural networks as it allows the algorithm to
    deduce complex patterns in the data. Non-linearity is accomplished by activation
    functions and the most famous one is the ReLU for computational efficiency and
    improving known issues when training neural networks. The ReLU function is piece-wise
    linear which is what causes it to be non-linear as we showed mathematically above.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code can be found on my GitHub here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/relu.py?source=post_page-----d2cfa7ebf264--------------------------------)
    [## Medium-Articles/relu.py at main · egorhowell/Medium-Articles'
  prefs: []
  type: TYPE_NORMAL
- en: You can't perform that action at this time. You signed in with another tab or
    window. You signed out in another tab or…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/relu.py?source=post_page-----d2cfa7ebf264--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Another Thing!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I have a free newsletter, [**Dishing the Data**](https://dishingthedata.substack.com/),
    where I share weekly tips for becoming a better Data Scientist. There is no “fluff”
    or “clickbait,” just pure actionable insights from a practicing Data Scientist.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://newsletter.egorhowell.com/?source=post_page-----d2cfa7ebf264--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
  prefs: []
  type: TYPE_NORMAL
- en: How To Become A Better Data Scientist. Click to read Dishing The Data, by Egor
    Howell, a Substack publication with…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----d2cfa7ebf264--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Connect With Me!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[**YouTube**](https://www.youtube.com/@egorhowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Twitter**](https://twitter.com/EgorHowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**GitHub**](https://github.com/egorhowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
