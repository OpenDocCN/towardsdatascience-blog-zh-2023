- en: Python To SQL — I Can Now Load Data 20X Faster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/fast-load-data-to-sql-from-python-2d67aea946c0](https://towardsdatascience.com/fast-load-data-to-sql-from-python-2d67aea946c0)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The good, bad, and ugly ways of uploading large batches of data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://thuwarakesh.medium.com/?source=post_page-----2d67aea946c0--------------------------------)[![Thuwarakesh
    Murallie](../Images/44f1a14a899426592bbd8c7f73ce169d.png)](https://thuwarakesh.medium.com/?source=post_page-----2d67aea946c0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2d67aea946c0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2d67aea946c0--------------------------------)
    [Thuwarakesh Murallie](https://thuwarakesh.medium.com/?source=post_page-----2d67aea946c0--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2d67aea946c0--------------------------------)
    ·6 min read·Mar 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8276c9e69e0172eae178b74760c9fee1.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Matheo JBT](https://unsplash.com/@matheo_jbt?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Speed matters! In data pipelines, as in anywhere else.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with massive datasets is an everyday thing for most data professionals.
    There would be no issue if they streamed into a database or a warehouse.
  prefs: []
  type: TYPE_NORMAL
- en: But there are instances where we need to upload batch data that are heavy enough
    to keep our workstation useless for hours. Take a walk, and have some water. It’s
    good for you!
  prefs: []
  type: TYPE_NORMAL
- en: But if you really want to keep this task shorter, we need the most optimal way
    to load data to databases.
  prefs: []
  type: TYPE_NORMAL
- en: If it’s a pre-formatted file, I’d prefer to use the client libraries to do this.
    For instance, the following shell command can upload your CSV file to a remote
    Postgres database.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/streamlit-openai-gpt3-example-app-b333da955ceb?source=post_page-----2d67aea946c0--------------------------------)
    [## Create GPT3 Powered Apps in Minutes With Streamlit'
  prefs: []
  type: TYPE_NORMAL
- en: Learn to build intelligent apps without worrying too much about software development.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/streamlit-openai-gpt3-example-app-b333da955ceb?source=post_page-----2d67aea946c0--------------------------------)
    [](https://levelup.gitconnected.com/3-ways-of-web-scraping-in-python-e953c4a96ec2?source=post_page-----2d67aea946c0--------------------------------)
    [## The Serene Symphony of Python Web Scraping — in 3 Movements
  prefs: []
  type: TYPE_NORMAL
- en: The easiest, the most flexible, and the most comprehensive ways to do web scraping
    in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/3-ways-of-web-scraping-in-python-e953c4a96ec2?source=post_page-----2d67aea946c0--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: But this is rarely the case, at least in my case.
  prefs: []
  type: TYPE_NORMAL
- en: Since Python is my primary programming language, I have to upload them from
    Python, possibly after some pre-processing. Therefore I did a little experiment
    to see the fastest.
  prefs: []
  type: TYPE_NORMAL
- en: I found the fastest, and it’s not what I regularly use.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/etl-github-actions-cron-383f618704b6?source=post_page-----2d67aea946c0--------------------------------)
    [## How to Build Simple ETL Pipelines With GitHub Actions'
  prefs: []
  type: TYPE_NORMAL
- en: ETLs don’t have to be complex. If that’s the case, use GitHub Actions.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/etl-github-actions-cron-383f618704b6?source=post_page-----2d67aea946c0--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'The ugly: How not to upload data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although I’m now confident I should never use this method, I thought this might
    help initially.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s what we do. I’ve got a dataset that’s about 500MB on the disk. First,
    I tried to load it with the insert command in the psycopg2 module.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: I use the timing decorator to measure how long it takes to load. It’s one of
    my five favorite python decorators.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/python-decorators-for-data-science-6913f717669a?source=post_page-----2d67aea946c0--------------------------------)
    [## 5 Python Decorators I Use in Almost All My Data Science Projects'
  prefs: []
  type: TYPE_NORMAL
- en: Decorators provide a new and convenient way for everything from caching to sending
    notifications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/python-decorators-for-data-science-6913f717669a?source=post_page-----2d67aea946c0--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Further, I intentionally kept my database in my local host. Thus, bandwidth
    is out of the equation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier, I genuinely thought this might be the fastest way to load data. That’s
    because we use a cursor to insert data and commit simultaneously. But the whole
    process took this much time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Okay, how do we know this is too slow without a reference?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try out the most popular way.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/5-advanced-sql-techniques-for-real-life-projects-f2db9b6680e2?source=post_page-----2d67aea946c0--------------------------------)
    [## These 5 SQL Techniques Cover ~80% of Real-Life Projects'
  prefs: []
  type: TYPE_NORMAL
- en: Speed up your SQL learning curve.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/5-advanced-sql-techniques-for-real-life-projects-f2db9b6680e2?source=post_page-----2d67aea946c0--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'The bad: Using Pandas to_sql to load massive datasets.'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you use Pandas and it’s `to_sql` API frequently, this might surprise you.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve been using this, and I’m continuing to use it. But this is still not the
    best way to go about for large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s my code. I’m using the same database and CSV as before. I’ve truncated
    the table before I start loading the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the above code, I’m not using the popular `method` argument. Setting them
    to `[multi](https://pandas.pydata.org/docs/user_guide/io.html#io-sql-method)`
    [would speed up data loading to an analytical database](https://pandas.pydata.org/docs/user_guide/io.html#io-sql-method)
    such as Redshift. But in our case, we use a transactional database.
  prefs: []
  type: TYPE_NORMAL
- en: Returning to the main point, this method took us 376 seconds to load.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This makes Pandas way better than using a cursor to load data. It’s about 3X
    gain.
  prefs: []
  type: TYPE_NORMAL
- en: But what makes it not the fastest? After all, this is my favorite and most used
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The good: Using the COPY method'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There’s a native way to upload text data to SQL tables. And libraries such as
    psycopg2 offer this out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: Yes, we can copy a file as it is to SQL tables in Python.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `copy_from` method in the cursor uses the `COPY` method in SQL client API
    and directly uploads the file to SQL. Along with it, we pass additional arguments.
  prefs: []
  type: TYPE_NORMAL
- en: Here, in this case, we specify that a comma separates the columns. We also used
    the `next` method to skip the first row, which is the table header.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: An astonishing 50 seconds — 20X faster than using a cursor and almost 8X faster
    than the `to_sql` Pandas method.
  prefs: []
  type: TYPE_NORMAL
- en: But wait!
  prefs: []
  type: TYPE_NORMAL
- en: I said I use Python because often do data pre-processing. The other two methods
    are straightforward. But how will we upload an existing dataset in Python runtime
    in this method?
  prefs: []
  type: TYPE_NORMAL
- en: '[](/junior-developers-write-multi-page-sql-queries-seniors-use-window-functions-f82dfeb8e378?source=post_page-----2d67aea946c0--------------------------------)
    [## Junior Developers Write Multi-Page SQL Queries; Seniors Use Window Functions'
  prefs: []
  type: TYPE_NORMAL
- en: An elegant way to perform computation within the context of a record
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/junior-developers-write-multi-page-sql-queries-seniors-use-window-functions-f82dfeb8e378?source=post_page-----2d67aea946c0--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Writing a dataset existing in the memory using COPY
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is where we can benefit from the buffer method. Here’s the code fast load
    an existing Pandas dataframe to a SQL database.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This code creates an `StringIO` object called `csv_buffer`, which is a file-like
    object that behaves like a CSV file. The DataFrame is written to this object using
    the `to_csv()` method with `index=False` and `header=False` to exclude the index
    and header from the CSV output.
  prefs: []
  type: TYPE_NORMAL
- en: The `seek(0)` method is then called on the `csv_buffer` object to move the file
    pointer back to the beginning of the file-like object.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Working with large datasets is not the same as working with an ordinary dataset.
    And one of the challenging tasks is to load such gigantic datasets into databases.
  prefs: []
  type: TYPE_NORMAL
- en: Unless no pre-processing is required, I’d almost always use the Pandas’ to_sql
    method. Yet, my experiment suggests this isn’t the best way for large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The COPY method is the fastest I’ve seen so far. While I suggest this for batch
    loading in a controlled environment, for day-to-day tasks, `to_sql` gives a fantastic
    interface for tweaking several upload behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading, friend! If you enjoyed my article, let’s keep in touch on
    [**LinkedIn**](https://www.linkedin.com/in/thuwarakesh/), [**Twitter**](https://twitter.com/Thuwarakesh),
    and [**Medium**](https://thuwarakesh.medium.com/).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Not a Medium member yet? Please use this link to [**become a member**](https://thuwarakesh.medium.com/membership)
    because, at no extra cost for you, I earn a small commission for referring you.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
