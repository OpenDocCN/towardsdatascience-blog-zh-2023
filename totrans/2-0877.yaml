- en: Python To SQL — I Can Now Load Data 20X Faster
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python 到 SQL — 我现在可以以 20 倍的速度加载数据
- en: 原文：[https://towardsdatascience.com/fast-load-data-to-sql-from-python-2d67aea946c0](https://towardsdatascience.com/fast-load-data-to-sql-from-python-2d67aea946c0)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fast-load-data-to-sql-from-python-2d67aea946c0](https://towardsdatascience.com/fast-load-data-to-sql-from-python-2d67aea946c0)
- en: The good, bad, and ugly ways of uploading large batches of data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上传大量数据的好方法、坏方法和丑陋的方法
- en: '[](https://thuwarakesh.medium.com/?source=post_page-----2d67aea946c0--------------------------------)[![Thuwarakesh
    Murallie](../Images/44f1a14a899426592bbd8c7f73ce169d.png)](https://thuwarakesh.medium.com/?source=post_page-----2d67aea946c0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2d67aea946c0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2d67aea946c0--------------------------------)
    [Thuwarakesh Murallie](https://thuwarakesh.medium.com/?source=post_page-----2d67aea946c0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://thuwarakesh.medium.com/?source=post_page-----2d67aea946c0--------------------------------)[![Thuwarakesh
    Murallie](../Images/44f1a14a899426592bbd8c7f73ce169d.png)](https://thuwarakesh.medium.com/?source=post_page-----2d67aea946c0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2d67aea946c0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2d67aea946c0--------------------------------)
    [Thuwarakesh Murallie](https://thuwarakesh.medium.com/?source=post_page-----2d67aea946c0--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2d67aea946c0--------------------------------)
    ·6 min read·Mar 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2d67aea946c0--------------------------------)
    ·6 分钟阅读·2023年3月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8276c9e69e0172eae178b74760c9fee1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8276c9e69e0172eae178b74760c9fee1.png)'
- en: Photo by [Matheo JBT](https://unsplash.com/@matheo_jbt?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[Matheo JBT](https://unsplash.com/@matheo_jbt?utm_source=medium&utm_medium=referral)
    摄影，[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
- en: Speed matters! In data pipelines, as in anywhere else.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 速度很重要！在数据管道中，和在其他地方一样。
- en: Dealing with massive datasets is an everyday thing for most data professionals.
    There would be no issue if they streamed into a database or a warehouse.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 处理大量数据集是大多数数据专业人员的日常工作。如果它们流入数据库或仓库，这不会成为问题。
- en: But there are instances where we need to upload batch data that are heavy enough
    to keep our workstation useless for hours. Take a walk, and have some water. It’s
    good for you!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但有时我们需要上传足够重的批量数据，以至于使我们的工作站无用数小时。散步一下，喝点水，对你有好处！
- en: But if you really want to keep this task shorter, we need the most optimal way
    to load data to databases.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果您真的想缩短这个任务，我们需要最优的方法将数据加载到数据库中。
- en: If it’s a pre-formatted file, I’d prefer to use the client libraries to do this.
    For instance, the following shell command can upload your CSV file to a remote
    Postgres database.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是一个预格式化的文件，我会更倾向于使用客户端库来完成。例如，以下 shell 命令可以将您的 CSV 文件上传到远程 Postgres 数据库。
- en: '[](https://levelup.gitconnected.com/streamlit-openai-gpt3-example-app-b333da955ceb?source=post_page-----2d67aea946c0--------------------------------)
    [## Create GPT3 Powered Apps in Minutes With Streamlit'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/streamlit-openai-gpt3-example-app-b333da955ceb?source=post_page-----2d67aea946c0--------------------------------)
    [## 用 Streamlit 在几分钟内创建 GPT3 驱动的应用程序'
- en: Learn to build intelligent apps without worrying too much about software development.
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习构建智能应用程序，而不必过多担心软件开发。
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/streamlit-openai-gpt3-example-app-b333da955ceb?source=post_page-----2d67aea946c0--------------------------------)
    [](https://levelup.gitconnected.com/3-ways-of-web-scraping-in-python-e953c4a96ec2?source=post_page-----2d67aea946c0--------------------------------)
    [## The Serene Symphony of Python Web Scraping — in 3 Movements
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/streamlit-openai-gpt3-example-app-b333da955ceb?source=post_page-----2d67aea946c0--------------------------------)
    [](https://levelup.gitconnected.com/3-ways-of-web-scraping-in-python-e953c4a96ec2?source=post_page-----2d67aea946c0--------------------------------)
    [## Python 网络抓取的宁静交响曲 — 3 移动篇章'
- en: The easiest, the most flexible, and the most comprehensive ways to do web scraping
    in Python
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Python 中进行网络抓取的最简单、最灵活和最全面的方法
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/3-ways-of-web-scraping-in-python-e953c4a96ec2?source=post_page-----2d67aea946c0--------------------------------)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/3-ways-of-web-scraping-in-python-e953c4a96ec2?source=post_page-----2d67aea946c0--------------------------------)'
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: But this is rarely the case, at least in my case.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但这在我的情况中很少发生。
- en: Since Python is my primary programming language, I have to upload them from
    Python, possibly after some pre-processing. Therefore I did a little experiment
    to see the fastest.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: I found the fastest, and it’s not what I regularly use.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[](/etl-github-actions-cron-383f618704b6?source=post_page-----2d67aea946c0--------------------------------)
    [## How to Build Simple ETL Pipelines With GitHub Actions'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: ETLs don’t have to be complex. If that’s the case, use GitHub Actions.
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/etl-github-actions-cron-383f618704b6?source=post_page-----2d67aea946c0--------------------------------)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'The ugly: How not to upload data'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although I’m now confident I should never use this method, I thought this might
    help initially.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Here’s what we do. I’ve got a dataset that’s about 500MB on the disk. First,
    I tried to load it with the insert command in the psycopg2 module.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: I use the timing decorator to measure how long it takes to load. It’s one of
    my five favorite python decorators.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[](/python-decorators-for-data-science-6913f717669a?source=post_page-----2d67aea946c0--------------------------------)
    [## 5 Python Decorators I Use in Almost All My Data Science Projects'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Decorators provide a new and convenient way for everything from caching to sending
    notifications
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/python-decorators-for-data-science-6913f717669a?source=post_page-----2d67aea946c0--------------------------------)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Further, I intentionally kept my database in my local host. Thus, bandwidth
    is out of the equation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier, I genuinely thought this might be the fastest way to load data. That’s
    because we use a cursor to insert data and commit simultaneously. But the whole
    process took this much time:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Okay, how do we know this is too slow without a reference?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try out the most popular way.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[](/5-advanced-sql-techniques-for-real-life-projects-f2db9b6680e2?source=post_page-----2d67aea946c0--------------------------------)
    [## These 5 SQL Techniques Cover ~80% of Real-Life Projects'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Speed up your SQL learning curve.
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/5-advanced-sql-techniques-for-real-life-projects-f2db9b6680e2?source=post_page-----2d67aea946c0--------------------------------)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'The bad: Using Pandas to_sql to load massive datasets.'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you use Pandas and it’s `to_sql` API frequently, this might surprise you.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: I’ve been using this, and I’m continuing to use it. But this is still not the
    best way to go about for large datasets.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Here’s my code. I’m using the same database and CSV as before. I’ve truncated
    the table before I start loading the dataset.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the above code, I’m not using the popular `method` argument. Setting them
    to `[multi](https://pandas.pydata.org/docs/user_guide/io.html#io-sql-method)`
    [would speed up data loading to an analytical database](https://pandas.pydata.org/docs/user_guide/io.html#io-sql-method)
    such as Redshift. But in our case, we use a transactional database.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Returning to the main point, this method took us 376 seconds to load.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This makes Pandas way better than using a cursor to load data. It’s about 3X
    gain.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得 Pandas 比使用游标加载数据要好得多，效率提升约 3 倍。
- en: But what makes it not the fastest? After all, this is my favorite and most used
    method.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 那么是什么让它不是最快的呢？毕竟，这是我最喜欢和最常用的方法。
- en: 'The good: Using the COPY method'
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 好处：使用 COPY 方法
- en: There’s a native way to upload text data to SQL tables. And libraries such as
    psycopg2 offer this out of the box.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种本地方式可以将文本数据上传到 SQL 表中。像 psycopg2 这样的库可以直接提供这种功能。
- en: Yes, we can copy a file as it is to SQL tables in Python.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我们可以直接将文件复制到 Python 中的 SQL 表。
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `copy_from` method in the cursor uses the `COPY` method in SQL client API
    and directly uploads the file to SQL. Along with it, we pass additional arguments.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 游标中的 `copy_from` 方法使用 SQL 客户端 API 中的 `COPY` 方法，并直接将文件上传到 SQL。我们还传递了额外的参数。
- en: Here, in this case, we specify that a comma separates the columns. We also used
    the `next` method to skip the first row, which is the table header.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们指定用逗号分隔列。我们还使用 `next` 方法跳过第一行，即表头。
- en: 'Here are the results:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: An astonishing 50 seconds — 20X faster than using a cursor and almost 8X faster
    than the `to_sql` Pandas method.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 惊人的 50 秒 — 比使用游标快 20 倍，比 `to_sql` Pandas 方法快近 8 倍。
- en: But wait!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 但等一下！
- en: I said I use Python because often do data pre-processing. The other two methods
    are straightforward. But how will we upload an existing dataset in Python runtime
    in this method?
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我提到我使用 Python，因为通常会进行数据预处理。其他两种方法很直接。但我们如何在 Python 运行时上传现有数据集呢？
- en: '[](/junior-developers-write-multi-page-sql-queries-seniors-use-window-functions-f82dfeb8e378?source=post_page-----2d67aea946c0--------------------------------)
    [## Junior Developers Write Multi-Page SQL Queries; Seniors Use Window Functions'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 初级开发者编写多页 SQL 查询；高级开发者使用窗口函数](https://junior-developers-write-multi-page-sql-queries-seniors-use-window-functions-f82dfeb8e378?source=post_page-----2d67aea946c0--------------------------------)'
- en: An elegant way to perform computation within the context of a record
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在记录的上下文中执行计算的优雅方式
- en: towardsdatascience.com](/junior-developers-write-multi-page-sql-queries-seniors-use-window-functions-f82dfeb8e378?source=post_page-----2d67aea946c0--------------------------------)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](https://junior-developers-write-multi-page-sql-queries-seniors-use-window-functions-f82dfeb8e378?source=post_page-----2d67aea946c0--------------------------------)'
- en: Writing a dataset existing in the memory using COPY
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 COPY 写入内存中存在的数据集
- en: This is where we can benefit from the buffer method. Here’s the code fast load
    an existing Pandas dataframe to a SQL database.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们可以从缓冲区方法中受益。下面的代码可以快速加载现有 Pandas 数据框到 SQL 数据库。
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This code creates an `StringIO` object called `csv_buffer`, which is a file-like
    object that behaves like a CSV file. The DataFrame is written to this object using
    the `to_csv()` method with `index=False` and `header=False` to exclude the index
    and header from the CSV output.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码创建了一个名为 `csv_buffer` 的 `StringIO` 对象，它是一个类似于 CSV 文件的文件类对象。使用 `to_csv()`
    方法将 DataFrame 写入该对象，`index=False` 和 `header=False` 以排除 CSV 输出中的索引和标题。
- en: The `seek(0)` method is then called on the `csv_buffer` object to move the file
    pointer back to the beginning of the file-like object.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在 `csv_buffer` 对象上调用 `seek(0)` 方法，将文件指针移动回文件类对象的开头。
- en: Conclusion
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Working with large datasets is not the same as working with an ordinary dataset.
    And one of the challenging tasks is to load such gigantic datasets into databases.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 处理大数据集与处理普通数据集不同。一个具有挑战性的任务是将这些巨大的数据集加载到数据库中。
- en: Unless no pre-processing is required, I’d almost always use the Pandas’ to_sql
    method. Yet, my experiment suggests this isn’t the best way for large datasets.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 除非不需要预处理，我几乎总是使用 Pandas 的 to_sql 方法。然而，我的实验表明，这对大数据集来说不是最好的方法。
- en: The COPY method is the fastest I’ve seen so far. While I suggest this for batch
    loading in a controlled environment, for day-to-day tasks, `to_sql` gives a fantastic
    interface for tweaking several upload behaviors.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: COPY 方法是我见过的最快的方法。虽然我建议在受控环境中批量加载时使用这个方法，但对于日常任务，`to_sql` 提供了一个很棒的接口来调整多个上传行为。
- en: Thanks for reading, friend! If you enjoyed my article, let’s keep in touch on
    [**LinkedIn**](https://www.linkedin.com/in/thuwarakesh/), [**Twitter**](https://twitter.com/Thuwarakesh),
    and [**Medium**](https://thuwarakesh.medium.com/).
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 感谢阅读，朋友！如果你喜欢我的文章，让我们在 [**LinkedIn**](https://www.linkedin.com/in/thuwarakesh/)、[**Twitter**](https://twitter.com/Thuwarakesh)
    和 [**Medium**](https://thuwarakesh.medium.com/) 保持联系。
- en: ''
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Not a Medium member yet? Please use this link to [**become a member**](https://thuwarakesh.medium.com/membership)
    because, at no extra cost for you, I earn a small commission for referring you.
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 还不是 Medium 会员？请使用这个链接来[**成为会员**](https://thuwarakesh.medium.com/membership)，因为这样你无需额外付费，我可以通过推荐你获得少量佣金。
