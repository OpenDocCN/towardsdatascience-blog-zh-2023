- en: Machine Learning in a Non-Euclidean Space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/machine-learning-in-a-non-euclidean-space-99b0a776e92e](https://towardsdatascience.com/machine-learning-in-a-non-euclidean-space-99b0a776e92e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/33e8dc96ecf4a66659e0c47e33bde03e.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Greg Rosenke](https://unsplash.com/@greg_rosenke?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Chapter I. Why you should learn about non-Euclidean ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mastafa.foufa?source=post_page-----99b0a776e92e--------------------------------)[![Mastafa
    Foufa](../Images/2e0b26ed83f04e943438afa1aab462a8.png)](https://medium.com/@mastafa.foufa?source=post_page-----99b0a776e92e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----99b0a776e92e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----99b0a776e92e--------------------------------)
    [Mastafa Foufa](https://medium.com/@mastafa.foufa?source=post_page-----99b0a776e92e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----99b0a776e92e--------------------------------)
    ·10 min read·Jun 16, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '“Is our comfortable and familiar Euclidean space and its linear structure always
    the right place for machine learning? Recent research argues otherwise: it is
    not always needed and sometimes harmful, as demonstrated by a wave of exciting
    work. Starting with the notion of hyperbolic representations for hierarchical
    data two years ago, a major push has resulted in new ideas for representations
    in non-Euclidean spaces, new algorithms and models with non-Euclidean data and
    operations, and new perspectives on the underlying functionality of non-Euclidean
    ML.” *by* [*Fred Sala*](http://stanford.edu/~fredsala)*,* [*Ines Chami*](http://web.stanford.edu/~chami/)*,*
    [*Adva Wolf*](https://mathematics.stanford.edu/people/adva-wolf)*,* [*Albert Gu*](http://web.stanford.edu/~albertgu/)*,*
    [*Beliz Gunel*](http://web.stanford.edu/~bgunel/) *and* [*Chris Ré*](https://cs.stanford.edu/people/chrismre/),
    [2019](https://dawn.cs.stanford.edu/2019/10/10/noneuclidean/)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What you will learn in this article.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Distortion measures how well distance is preserved when representing data in
    another space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For some data, Euclidean space implies high distortion, so non-Euclidean spaces
    like spherical or hyperbolic spaces are used instead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Riemannian geometry tools like manifolds and Riemannian metric are used for
    non-Euclidean Machine Learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manifolds are curved spaces that are locally Euclidean.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exponential and logarithmic maps are used to go from a manifold to its tangent
    space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Riemannian metric allows to compute shortest distances on the manifold.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before going further in this series about non-Euclidean geometry applied to
    Machine Learning (*ML*), I had to answer an important question. **Is it worth
    learning more about non-Euclidean ML?**
  prefs: []
  type: TYPE_NORMAL
- en: To answer such a question, I started by researching non-Euclidean ML. I quickly
    ended up finding a couple of resources. The very first one is from Stanford and
    the citation above is extracted from it. The authors argue that Machine Learning
    was designed with a certain geometry, namely the Euclidean geometry, more by tradition
    or convenience, than by rational thinking.
  prefs: []
  type: TYPE_NORMAL
- en: So far, the choice of the Euclidean geometry doesn’t seem like a major problem.
    But the authors trigger our attention by citing Bronstein et al. in their [seminal
    description of the geometric deep learning](https://arxiv.org/pdf/1611.08097.pdf)
    paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: “[m]any scientific fields study data with an underlying structure that is a
    non-Euclidean space.” [Bronstein et al.](https://arxiv.org/pdf/1611.08097.pdf)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'As I continued reading the article, I stumbled upon an aspect that I wasn’t
    familiar with: the notion of space **flatness**.'
  prefs: []
  type: TYPE_NORMAL
- en: “We have chosen to work with Euclidean space, with all of its inherent properties,
    among the most critical of which is its **flatness**.” [*Fred Sala*](http://stanford.edu/~fredsala)*,*
    [*Ines Chami*](http://web.stanford.edu/~chami/)*,* [*Adva Wolf*](https://mathematics.stanford.edu/people/adva-wolf)*,*
    [*Albert Gu*](http://web.stanford.edu/~albertgu/)*,* [*Beliz Gunel*](http://web.stanford.edu/~bgunel/)
    *and* [*Chris Ré*](https://cs.stanford.edu/people/chrismre/), [2019](https://dawn.cs.stanford.edu/2019/10/10/noneuclidean/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The authors of the Stanford article mention the impact of flatness. Below are
    the three points raised and you should read our series to get more intuition:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Better representations** — they argue that Euclidean spaces are not fit for
    certain datasets like hierarchical datasets that can be described by trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unlocking the full potential of models** — they argue that to push the barriers
    in terms of model performance, we could improve the space the data lie in, by
    moving from Euclidean geometry to non-Euclidean geometry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**More flexible operations** — they argue that operations in non-Euclidean
    spaces are more flexible and require less dimensions. The authors explain that
    later in their article. But we will try to simplify that in our Medium series.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Representing a non-flat entity into a flat space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is important to have an appropriate geometry, conditionally to the input
    data. Below, we show an example of non-Euclidean data that is “forced” to fit
    in a 2-dimensional Euclidean space. This is our **well known spherical planet**
    that is flattened into a plane. However, this is done with **non negligible distortions**.
    By distortion, we mean that distances are not preserved, from the original space
    [*Earth — sphere*] to the space where the data is represented [*Maps — plane*].
  prefs: []
  type: TYPE_NORMAL
- en: For example, below Mexico has ***in reality*** almost the same surface as Greenland
    (*right*) but appears much smaller in the actual projection (*left*).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a79caf56683cbffe3194b997a903c659.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Resource:** From the authors. Note that the World map (left) is using the
    Mercator projection of our spherical planet. The Mercator map is defined by the
    formula (x, y) = λ, log tan(π/4 + φ/2). *Adapted from* [*Wikipedia*](https://commons.wikimedia.org/wiki/File:World_map_-_low_resolution_chain_test.svg)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to represent our earth that all involve some degree of distortion.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/74da35a7c4dad90e88628d6619c531b5.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Resource:** Projecting the globe, with distortion. From the authors, adapted
    from [Wikipedia](https://en.wikipedia.org/wiki/Mercator_projection).'
  prefs: []
  type: TYPE_NORMAL
- en: For example, distortion is naturally observed in the famous Mercator projection.
    The Greenland problem showcases the loss of information when moving from a spherical
    representation to a plane representation with such projection. This projection
    is not **area preserving,** a core property expected in this case. Indeed, Greenland,
    with an area of about 2.2 million square kilometers, looks larger than South America,
    with an area of about 17.8 million square kilometers. This Mercator projection
    preserves angles but not areas, hence making it non-perfect.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, other datasets, are also forced to lie in an Euclidean space whereby we
    observe distortions. This is the case of **graphs**: in a Euclidean space, we
    cannot embed large classes of graphs without low distortion or without loss of
    information.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distortion** has several more rigorous mathematical definitions. Essentially,
    we want distortion to measure the quality of the embedding by evaluating how well
    distances are preserved. Here, we define it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Distortion ~ AVG {Graph distance / Embedding distance }
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Example.**'
  prefs: []
  type: TYPE_NORMAL
- en: In the figure below, we can prove through the Poincare-type inequalities that
    we cannot embed the two cycles (*square*, *circle*) in a Euclidean space without
    distortion. Note that a distortion of 1 is a perfect distortion — graph distances
    are exactly matching the embedding space distances. **Any distortion different
    than 1 means we are not preserving the graph distances.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/42350b592503cae1e5375925361599e4.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Resource:** From the authors. Optimal embedding of a cycle of length 4 [left],
    and optimal embedding of the 3-star K(1,3) [right]. Adapted from Octavian Ganea’s
    lecture at ETH Zurich.'
  prefs: []
  type: TYPE_NORMAL
- en: On the square above, the two opposite nodes on the diagonal have a distance
    of 2 in terms of **graph distance.** However, the shortest path in the Euclidean
    embedding has a distance of **√2.**
  prefs: []
  type: TYPE_NORMAL
- en: This concept of distortion is really important as Euclidean geometry does not
    allow having the ideal “projection” of graph data. In particular, for hierachical
    graph data, to minimize distortion, **a solution is to use a hyperbolic space**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**. We will learn more about this example of non-Euclidean space in the
    next chapter.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Representing data in a non-Euclidean space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is hard understanding how we can represent data in any other way than with
    vectors in ***Rn***. Plus, how can we move away from the Euclidean distance we
    know so well to compare two vector representations?
  prefs: []
  type: TYPE_NORMAL
- en: A solution is described by manifolds, in Riemannian geometry. Manifolds are
    objects that look like ***Rn but only locally.*** *That means we can locally use
    vectors for representing our data points. But only locally!*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9cba2f31402dd14b9d086d73a44c3a96.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Resource**: The tangent space [Light grey, TxM] at a point x from the manifol
    M [Dark grey] and its tangent vector v. The vector x from the Manifold can be
    represented locally in the Euclidean tangent space. From [Wikipedia.](https://en.wikipedia.org/wiki/Tangent_space#)'
  prefs: []
  type: TYPE_NORMAL
- en: The notion of similarity or distances is key in Machine Learning. If we are
    building say an NLP model, we want to preserve the notion of similarity in semantics
    within the embedding space that represents textual input. In other words, we want
    two words that are similar in meaning to also be similar in the Euclidean space,
    i.e. with a low Euclidean distance. Similarly, two words that are dissimilar in
    meaning should be far away in the Euclidean space, i.e. with a high Euclidean
    distance.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, there needs to be an equivalent approach when escaping Euclidean geometry.
    This approach is described by a *Riemannian metric.* The **Riemannian metric allows
    us to compare two entities in the non-Euclidean space and preserve this intuitive
    notion of distance**.
  prefs: []
  type: TYPE_NORMAL
- en: 👀 I remember.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we need to remember that in this non-Euclidean framework we can perform
    operations locally on our data representations and we have a metric to measure
    distances. Thus, we are equipped to do ML in non-Euclidean spaces.
  prefs: []
  type: TYPE_NORMAL
- en: 🙌🏻 Why should I learn more about ML in a non-Euclidean space?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we know that ML without the genius [Euclid](https://en.wikipedia.org/wiki/Euclid)
    is actually something. There are actual projects that exist and approach our traditional
    machine learning problems with a different geometry framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, a question naturally emerges: **is it worth our time learning more than
    the existence of this field?**'
  prefs: []
  type: TYPE_NORMAL
- en: It is a rather scary space that involves non-trivial mathematics. But my friend,
    Aniss Medbouhi, [ML Doctoral Researcher at KTH](https://www.linkedin.com/in/aniss-medbouhi/),
    will help us get past the inherent complexity of this space.
  prefs: []
  type: TYPE_NORMAL
- en: The other reason I wasn’t convinced about this space is that I read that it
    was mostly fit for hierarchical data that can be described by trees. At a first
    glance, it doesn’t involve the data I work with on a daily basis.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the abstracts below give us an idea of relevant datasets of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: “However, **recent work has shown that the appropriate isometric space for embedding
    complex networks is not the flat Euclidean space**, **but negatively curved, hyperbolic
    space**. We present a new concept that exploits these recent insights and propose
    learning neural embeddings of graphs in hyperbolic space. We provide experimental
    evidence that embedding graphs in their natural geometry signicantly improves
    performance on downstream tasks for several **real-world public datasets**.” [Chamberlain
    et al.](https://arxiv.org/pdf/1705.10359.pdf)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “However, while **complex symbolic datasets often exhibit a latent hierarchical
    structure, state-of-the-art methods typically learn embeddings in Euclidean vector
    spaces, which do not account for this property**. For this purpose, we introduce
    a new approach for learning hierarchical representations of symbolic data by **embedding
    them into hyperbolic space** — or more precisely into an n-dimensional Poincaré
    ball.” [Nickel and Kiela](https://arxiv.org/pdf/1705.08039.pdf)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The datasets mentioned above are listed as follows, by [Chamberlain et al.](https://arxiv.org/pdf/1705.10359.pdf)
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '(1) Karate: Zachary’s karate club contains 34 vertices divided into two factions.
    [4]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '(2) Polbooks: A network of books about US politics published around the time
    of the 2004 presidential election and sold by the online bookseller Amazon.com.
    Edges between books represent frequent co-purchasing of books by the same buyers.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '(3) Football: A network of American football games between Division IA colleges
    during regular season Fall 2000\. [2]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '(4) Adjnoun: Adjacency network of common adjectives and nouns in the novel
    David Coppereld by Charles Dickens. [3]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '(5) Polblogs: A network of hyperlinks between weblogs on US politics, recorded
    in 2005\. [1]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Additionally, in biology, we find this reference dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Biology: Evolutionary data like proteins.* [5]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/2af3a2d5ffcf1f54c748242329d868cc.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Resource**: A network representation of social relationships among the 34
    individuals in the karate club studied by Zachary. The population is divided into
    two fractions based on an event [4]. Adapted from [Wikipedia](https://en.wikipedia.org/wiki/Zachary%27s_karate_club).'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, NLP data, i.e. textual data, is another type of hierarchical data.
    As a result, a lot of domains may benefit from understanding the advancements
    in non-Euclidean Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to better represent certain datasets, it is key to tie
    it back to Machine Learning. Any downstream ML tasks require ingesting data first.
    A lot of time is spent in cleaning our underlying data and representing it accurately.
    The quality of the data representation is essential as it directly impacts the
    performance of our models. For example, in NLP, I advise my students to focus
    on architectures that provide good embeddings, e.g. contextual embeddings. There
    has been extensive research in improving embeddings, moving from shallow neural
    networks (*fasttext, word2vec*) to deep neural networks and transformers *(sentence-transformers,
    BERT, RoBERTa, XLM)*. However, it is also worth noting that data representation
    is very much linked to the task at hand, and research shows that certain shallow
    neural networks provide better results than deep neural networks, for certain
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article, we saw that we can leverage non-Euclidean geometry to tackle
    existing problems specific to spherical data and hierarchical datasets like graphs.
    When embedding such datasets into a Euclidean space, the price to pay is a distortion
    that does not permit preserving distances from the original space to the embedding
    space. Such distortion is intuitive in our earth representation where we have
    many ways to represent our globe, some of which do not preserve core properties
    expected such as **area preserving.** Similarly for graphs, core properties need
    to be preserved and distorting the underlying space may result in poorer performance
    for downstream Machine Learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn more about spherical and hyperbolic geometries.
    We will focus more on the latter and give intuition on how models in such space
    can better embed hierarchical data.
  prefs: []
  type: TYPE_NORMAL
- en: Connect with the contributors.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/3aeef9a3a6d5979f8293050b0b599f88.png)'
  prefs: []
  type: TYPE_IMG
- en: ML Doctoral Researcher at KTH Royal Institute of Technology.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Linkedin*. [https://www.linkedin.com/in/aniss-medbouhi/](https://www.linkedin.com/in/aniss-medbouhi/)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc4860e542b8410558a3768c6a07eaf3.png)'
  prefs: []
  type: TYPE_IMG
- en: Data Scientist at Microsoft and Teacher at EPITA Paris.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Linkedin*. [https://www.linkedin.com/in/mastafa-foufa/](https://www.linkedin.com/in/mastafa-foufa/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Lada A. Adamic and Natalie Glance. The political blogosphere and the 2004
    U.S. election. Proceedings of the 3rd international workshop on Link discovery
    — LinkKDD ’05, pages 36–43, 2005.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Michelle Girvan and Mark E. J. Newman. Community structure in social and
    biological networks. In Proceedings of the national academy of sciences, 99:7821–7826,
    2002.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Mark E. J. Newman. Finding community structure in networks using the eigenvectors
    of matrices. Physical Review E — Statistical, Nonlinear, and Soft Matter Physics,
    74(3):1–19, 2006.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Wayne W. Zachary. An information ow model for conict and ssion in small
    groups. Journal of anthropological research, 33:452–473, 1977.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] AlQuraishi, Mohammed. “ProteinNet: a standardized data set for machine
    learning of protein structure.” BMC bioinformatics 20.1 (2019): 1–10.'
  prefs: []
  type: TYPE_NORMAL
