- en: 'Support Vector Machine with Scikit-Learn: A Friendly Introduction'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/support-vector-machine-with-scikit-learn-a-friendly-introduction-a2969f2ff00d](https://towardsdatascience.com/support-vector-machine-with-scikit-learn-a-friendly-introduction-a2969f2ff00d)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Every data scientist should have SVM in their toolbox. Learn how to master this
    versatile model with a hands-on introduction.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@riccardo.andreoni?source=post_page-----a2969f2ff00d--------------------------------)[![Riccardo
    Andreoni](../Images/5e22581e419639b373019a809d6e65c1.png)](https://medium.com/@riccardo.andreoni?source=post_page-----a2969f2ff00d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a2969f2ff00d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a2969f2ff00d--------------------------------)
    [Riccardo Andreoni](https://medium.com/@riccardo.andreoni?source=post_page-----a2969f2ff00d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a2969f2ff00d--------------------------------)
    ·9 min read·Oct 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ca089d89ca86dd60ffd60783581574e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [unsplash.com](https://unsplash.com/photos/3DkouQeZjp4).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Among the available Machine Learning models, there exists one whose versatility
    makes it a must-have tool for **every data scientist toolbox**: [**Support Vector
    Machine**](https://en.wikipedia.org/wiki/Support_vector_machine) ([SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'SVM is a powerful and versatile algorithm, which, at its core, can delineate
    **optimal hyperplanes** in a high-dimensional space, effectively **segregating
    the different classes** of a dataset. But it doesn’t stop here! Its effectiveness
    is not limited to **classification** tasks: SVM is well-suited even for **regression**
    and **outlier detection** tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: One feature makes the SVM approach particularly effective. Instead of processing
    the entire dataset, as [KNN does](https://levelup.gitconnected.com/optical-character-recognition-with-knn-classifier-a-step-by-step-guide-b6d7b90e1122),
    SVM strategically focuses only on the subset of data points located near the decision
    boundaries. These points are called [*support vectors*](https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for-beginners/#:~:text=Support%20Vectors%3A%20These%20are%20the,is%20considered%20a%20good%20margin.),
    and the mathematics behind this unique idea will be simply explained in the upcoming
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: By doing so, SVM Algorithm is **computationally conservative** and ideal for
    tasks involving medium or even medium-large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: As I do in all my articles, I won't just explain the theoretical concepts, but
    I will also provide you with **coding examples** to familiarize yourself with
    the [**Scikit-Learn**](https://scikit-learn.org/) (sklearn) [**Python**](https://www.python.org/)
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s analyze Support Vector Machine (SVM) algorithms, and explore Machine Learning
    techniques, Python programming, and Data Science applications.
  prefs: []
  type: TYPE_NORMAL
- en: Linear SVM Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At its core, SVM classification resembles the elegant simplicity of Linear Algebra.
    Imagine a dataset in two-dimensional space, with two distinct classes to be separated.
    Linear SVM tries to separate the two classes with the best possible straight line.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c3a48ffc42b897621e5195ca0b7126a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'What does it mean “best” in this context? SVM searches for the optimal separation
    line: a line that not only separates the classes, but does it with the maximum
    possible distance from the closest training instances of each classes. That distance
    is called **margin**. The data points that lay on the margin edge are the key
    element of the linear SVM classifier, and are called **Support Vectors**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69aaa61db77873bc703ca9c4bf91ea0e.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation of the decision boundary to be optimized.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the separator is defined exclusively by the Support
    Vectors. As a consequence, adding more “off-the-street” training instances has
    no impact on the decision boundary. When more training instances that are not
    Support Vectors are added into the training set, the decision boundary doesn’t
    move and those instances are “ignored”. This feature is a great advantage of SVM,
    as it is not required to memorize the entire training set.
  prefs: []
  type: TYPE_NORMAL
- en: For an m-dimensional dataset, the separation line becomes a **separation hyperplane**,
    but the inspiring idea still holds.
  prefs: []
  type: TYPE_NORMAL
- en: Since the SVM model is based on distances, it is extremely **sensitive to feature
    scales**. For this reason it is always a good idea to normalize the features’
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Scikit-Learn we can instantiate a `LinearSVC()` object from the `.svm` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting accuracy is a mere 66%: not a remarkable result. In the next
    section, we will understand why the Linear Support Vector Machine classifier didn’t
    perform better and what to do to cope with this.'
  prefs: []
  type: TYPE_NORMAL
- en: Soft Margin Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you can imagine, having a dataset whose classes can be perfercty separated
    by linear surfaces is a luxury. In a real-world scenario, it doesn’t happen and
    data only needs one outlier to **make it impossible** for a Linear SVM to find
    a viable decision boundary.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1430d7521724daaf61b6e79826ae6359.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the image above, in this case, there doesn’t exist any line capable
    of separating the two classes.
  prefs: []
  type: TYPE_NORMAL
- en: To cope with real-world scenarios we need to introduce the Soft Margin Support
    Vector Machine Classification.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike traditional Linear SVM (also called Hard Margin SVM), Soft Margin SVM
    doesn’t require a rigid separation between the classes, allowing some **elements
    of flexibility**. By using a Soft Margin SVM, we acknowledge the existence of
    noisy data points and outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'More practically, we can specify an hyperparameter `C` which serves as a regularization
    parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: setting `C` to a large value, we enforce a stricter margin, reducing misclassifications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: setting `C` to a small value, we encourage a wider margin, allowing a greater
    number of misclassifications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/3ffe5e0ccf465f762fd7e6a0cbae1f06.png)'
  prefs: []
  type: TYPE_IMG
- en: Increasing C value reduces margin violations.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to implement the Soft Margin Linear SVM classifier in Scikit-Learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Running these lines of code for the same datasets, we obtain a slighter higher
    accuracy than the one we obtained for the Hard Margin LinearSVM. Although linear
    SVM classifiers are efficient and perform well in many scenarios, most datasets’
    classes are not linearly separable. In these cases, a linear decision boundary
    yields poor results.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately for us, SVM classifiers are not limited to linear decision boundaries.
    Thanks to the [*kernel trick*](/the-kernel-trick-c98cdbcaeb3f), they can learn
    even the most complex separation shapes. The next section will focus on that.
  prefs: []
  type: TYPE_NORMAL
- en: Non-Linear SVM Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As I anticipated, many datasets are **not linearly separable**. Even if we account
    some flexibility, results obtained through a linear separation line are not optimal.
    To handle this issue, we can **add more features**, such as polynomial features.
    Adding new features will transform the original dataset into a higher-dimensional
    one, where it may be separated by a line or hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/24d97585963430cd93a00ccc2411bb64.png)'
  prefs: []
  type: TYPE_IMG
- en: Non-linearly separable data. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Data have only one feature and it is not separable by any straight line. If
    we add an artificial feature, computed as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/59799aed154bf5939e96dea66eefd910.png)'
  prefs: []
  type: TYPE_IMG
- en: 'we obtain the following dataset, which can be separated by a linear boundary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e62ed4bcb50b84bae69211782aedd6cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Linearly separable data. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: However, adding polynomial features, like we did above, is **not feasible for
    large and complex datasets**. The resulting number of features would be too high.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there exists a technique called **kernel trick** that makes it
    possible even with high-degree polynomials. The mathematics behind the kernel
    trick is not complex, but as I want to focus on the practical implementation,
    I will leave [this guide](https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/lectures/lec3.pdf)
    explaining it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement a SVM classifier with a polynomial kernel in Python, we simply
    use the `SVC()` class and specify the type of kernel we intend to use and its
    degree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `coef0` hyperparameter acts as a regularization parameter, allowing to control
    how the model is influenced by high-degree polynomials.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the correct balance between the `degree`, `C` and`coef0` hyperparameters
    is not a straightforward task. It is typically recommended to use a Grid Search
    approach to find some viable values, and later perform a finer manual search to
    tune them precisely.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarity Features: Gaussian Radial Basis Function'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Polynomial kernels often work for a variety of machine learning problems, however,
    there exists a notable technique that often works even better: Similarity Features.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of adding artificial polynomial features based on the original features’
    values, we place several *landmarks* in our high-dimensional feature space and
    measure the distance from them to each data point. These distance measures become
    the **new model’s features**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a training set with two independent features that look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/396662f8494f869be53ae7034d77d92d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: By adding a certain number of landmarks in specific locations, we can create
    additional features computed as the distance from the data point to each landmark.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2fa21b729b109a5ce564920018b6c59c.png)'
  prefs: []
  type: TYPE_IMG
- en: In this example, we can place two landmarks in those precise locations. The
    SVM classifier can then learn to predict as class A the instances close to the
    landmarks and as class B the ones far away from the landmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Any distance measure can be used, however, it is proven to be convenient to
    implement the [**Gaussian Kernel function**](https://pages.stat.wisc.edu/~mchung/teaching/MIA/reading/diffusion.gaussian.kernel.pdf.pdf):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d5f0622a2fd929365149e4b74c87e1e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x* is the vector containing the data point coordinates'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*l* is the vector containing the landmark coordinates'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*gamma* acts as a regularization hyperparameter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The question now is: “*Where do I place the landmarks?*”. The most diffused
    approach is to place a landmark at each training example location. Having *m*
    training examples means creating *m* landmarks, and, as a consequence, it will
    result in *m* new features.'
  prefs: []
  type: TYPE_NORMAL
- en: The downside of this approach is that for large training sets, we will end up
    with an equally large number of distance features.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Scikit-Learn a SVM classifier with a Gaussian kernel is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, we detailed the theory of this versatile and powerful model, and
    we understood how easy it is to implement it in Python through the Scikit-Learn
    library.
  prefs: []
  type: TYPE_NORMAL
- en: I conclude this introductory guide by delineating the pros and cons of Support
    Vector Machine model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without any question, the strength of SVM coincide with its **accuracy in high-dimensional
    spaces**, making it ideal for data with numerous features, like images or genetic
    data. Additionally, I must remark SVM **versatility**: beyond classification,
    it seamlessly adapts to regression and anomaly detection tasks. Finally, on the
    pros side, I must point out its **flexibility**. SVM ability to handle non-linear
    relationships through different kernel functions, grants it a vast application
    spectrum.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we all know the mantra “*there is no free meal in Machine Learning*”, we
    must acknowledge SVM cons. First of all, SVM can easily be **affected by noisy
    data**, as we saw in the **Linear SVM** section. **Scalability** is also an Achilles
    heel for SVM: training large datasets with SVM can be computationally intensive
    and demanding.'
  prefs: []
  type: TYPE_NORMAL
- en: As I reach the end of this guide, I need to point out that, despite having touched
    all the most relevant aspects of SVM, its field extends far beyond the insight
    provided here. For this reason, I recommend digging into the resources and references
    attached to this article.
  prefs: []
  type: TYPE_NORMAL
- en: If you liked this story, consider following me to be notified of my upcoming
    projects and articles!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of my past projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/social-network-analysis-with-networkx-a-gentle-introduction-6123eddced3?source=post_page-----a2969f2ff00d--------------------------------)
    [## Social Network Analysis with NetworkX: A Gentle Introduction'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how companies like Facebook and LinkedIn extract insights from networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/social-network-analysis-with-networkx-a-gentle-introduction-6123eddced3?source=post_page-----a2969f2ff00d--------------------------------)
    [](/ensemble-learning-with-scikit-learn-a-friendly-introduction-5dd64650de6c?source=post_page-----a2969f2ff00d--------------------------------)
    [## Ensemble Learning with Scikit-Learn: A Friendly Introduction'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning algorithms like XGBoost or Random Forests are among the top-performing
    models in Kaggle competitions…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/ensemble-learning-with-scikit-learn-a-friendly-introduction-5dd64650de6c?source=post_page-----a2969f2ff00d--------------------------------)
    [](/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----a2969f2ff00d--------------------------------)
    [## Use Deep Learning to Generate Fantasy Names: Build a Language Model from Scratch'
  prefs: []
  type: TYPE_NORMAL
- en: Can a language model invent unique fantasy character names? Let’s build it from
    scratch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----a2969f2ff00d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Machine Learning with Python](https://www.coursera.org/learn/machine-learning-with-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition
    — Aurélien Géron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Scikit-Learn SVM documentation](https://scikit-learn.org/stable/modules/svm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
