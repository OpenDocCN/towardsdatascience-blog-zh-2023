- en: Model Optimization with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/model-optimization-with-tensorflow-629342d1a96f](https://towardsdatascience.com/model-optimization-with-tensorflow-629342d1a96f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: MLOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reduce your models' latency, storage, and inference costs with quantization
    and pruning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://michaloleszak.medium.com/?source=post_page-----629342d1a96f--------------------------------)[![Michał
    Oleszak](../Images/61b32e70cec4ba54612a8ca22e977176.png)](https://michaloleszak.medium.com/?source=post_page-----629342d1a96f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----629342d1a96f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----629342d1a96f--------------------------------)
    [Michał Oleszak](https://michaloleszak.medium.com/?source=post_page-----629342d1a96f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----629342d1a96f--------------------------------)
    ·9 min read·Apr 17, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff833d1ba8f3475a8a12f0187b1f2a2c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Over the last few years, machine learning models have seen two seemingly opposing
    trends. On the one hand, the models tend to get bigger and bigger, culminating
    in what’s all the rage these days: the [large language models](https://pub.towardsai.net/forget-about-chatgpt-f17a7f5089c3).
    Nvidia’s [Megatron-Turing Natural Language Generation](https://developer.nvidia.com/megatron-turing-natural-language-generation)
    model has 530 billion parameters! On the other hand, these models are being deployed
    onto smaller and smaller devices, such as smartwatches or drones, whose memory
    and computing power are naturally limited by their size.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we squeeze ever larger models into increasingly smaller devices? The
    answer is model optimization: the process of compressing the model in size and
    reducing its latency. In this article, we will see how it works and how to implement
    two popular model optimization methods — quantization and pruning — in TensorFlow.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b2d87f1fce5973b8a06955590df36b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Baseline model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we jump to model optimization techniques, we need a toy model to be
    optimized. Let’s train a simple binary classifier to differentiate between Paris’
    two famous landmarks: the Eiffel Tower and the Mona Lisa, as drawn by the players
    of [Google’s game called “Quick, Draw!”](https://quickdraw.withgoogle.com/). The
    [QuickDraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) consists
    of 28x28 grayscale images.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07437054e5fb8dc5ce7357e75cde82ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Examples from the Quickdraw dataset: Eiffel Tower (top) and Mona Lisa (bottom).'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s train a simple convolutional network to classify the two landmarks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We will now save the model in the TensorFlow Lite format. It’s a smaller and
    more efficient file format compared to the traditional `.h5` file, designed specifically
    with mobile and edge deployments in mind.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To assess our model’s size and accuracy, we will need two simple utility functions.
    First, `evaluate_tflite_model()` sets up a TF Lite interpreter to pass test examples
    to a saved TF Lite model and calculates the accuracy of its predictions. Second,
    `get_gzipped_model_size()` creates a temporary zipped version of the `.tflite`
    model file to compress it further and returns its size on disc in bytes. The implementations
    of both functions we use here are based on similar utilities from Coursera’a [Machine
    Learning Modeling Pipelines in Production course](https://www.coursera.org/learn/machine-learning-modeling-pipelines-in-production?specialization=machine-learning-engineering-for-production-mlops).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let’s get some baseline metrics from our unoptimized model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The model scores an accuracy of 98.524% while taking over 14k bytes of space.
    Let’s see whether we can compress its size without sacrificing much of the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b2d87f1fce5973b8a06955590df36b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Quantization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first optimization technique we will look at is model quantization. Its
    goal is to decrease the precision of objects in which the model’s parameters are
    stored.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization decreases the precision of objects in which the model’s parameters
    are stored.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: By default, TensorFlow stores model biases, weights, and activations as 32-bit
    floating points. A quick check with `np.finfo(np.float32).max` will tell you that
    this data type allows for storing values as large as 3³⁸. But do we need it?
  prefs: []
  type: TYPE_NORMAL
- en: Typically, most model weights and activations are not that far away from zero;
    otherwise, the gradients would explode preventing us from training a model in
    the first place. Converting these `float32`s into a lighter data structure such
    as `int8` could go a long way toward reducing the model size, while not necessarily
    impacting its accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization is very convenient to use since it operates on an already trained
    model and only converts its internal data structures. In TensorFlow, it can be
    done while converting the model to the TF Lite format by setting the `optimizations`
    attribute in the converter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This implements the default quantization variant called dynamic range quantization.
    This is a good starting point in most cases. Other, more advanced strategies are
    available such as full integer quantization which tries to estimate the range
    of values in each model’s tensor, but to do so, it requires a representative dataset
    to calibrate its estimates.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s stick with the default option and check the size and accuracy of the quantized
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The model size was almost halved, from more than 14k to over 7k bytes! At the
    same time, the accuracy did not degrade. Actually, it even went up a little bit
    from 98.524% to 98.526%. This is a rare case, as one would typically observe a
    slight decrease in accuracy as the result of quantization.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b2d87f1fce5973b8a06955590df36b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Pruning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another popular model optimization technique is weight pruning. In large neural
    networks, it is very unlikely that each weight plays a crucial role in the model’s
    performance. Identifying the not-so-important weights and removing them from the
    network allows us to reduce the space needed to store the model and save on float
    multiplications during inference, reducing its latency.
  prefs: []
  type: TYPE_NORMAL
- en: Pruning identifies and removes unimportant weights from the model, reducing
    its size and inference time.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The simplest way to identify unimportant weights is by their magnitude. The
    weights with values close to zero are the least likely to contribute much to the
    network’s output and are the best candidates for pruning.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e33263b610445e20e05d33ac9e2b6e45.png)'
  prefs: []
  type: TYPE_IMG
- en: A dense network (left) and a sparse network after pruning two nodes (right).
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, setting some weights to zero in a trained network breaks the
    flow of information inside it with tends to result in sharp accuracy drops. An
    obvious remedy would be to retrain the model after pruning, but this proved to
    be a challenging task.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, pruning and retraining can be performed concurrently. This allows us
    to explicitly teach the model not to use some of the network’s weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement the in-retraining pruning in TensorFlow, we need to define a pruning
    schedule. The schedule governs two processes during retraining:'
  prefs: []
  type: TYPE_NORMAL
- en: How much sparsity to enforce throughout the training;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When (in which training steps) to enforce it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A popular choice for the rate of sparsity increase that proved to work in practice
    is a polynomial decay function. It will sharply increase the sparsity in the first
    training steps to slow the increase down in later steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0f45c7cc8f3d6346c010c4b9e39da4e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Sparsity increase rate.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will start by requiring 50% of the weights to be zero at the beginning
    of training and this percentage will increase to reach 80% at the end. We will
    also start introducing sparsity immediately as the training starts, and the process
    will last for all the training steps.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Having the schedule set up, we pass it alongside the trained baseline model
    to the `prune_low_magnitude` method. Next, we recompile and retrain the model,
    passing it the `UpdatePruningStep` callback. This callback is called after each
    batch of training data is processed, and it updates the pruning step to keep it
    aligned with the pre-defined schedule. It also adds wrappers around layers to
    be pruned, so to get the original model architecture, we need to strip these wrappers
    off after retraining with the `strip_pruning` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now we can convert the model to the TFLite format and check its size and accuracy
    as we have done before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Pruning has compressed our model even more than quantization: from 14k to less
    than 6k bytes. The accuracy took a slight hit, however; it dropped from 98.524%
    in the baseline model to 98.402% in the pruned version.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b2d87f1fce5973b8a06955590df36b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Quantization & pruning come together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since quantization and pruning work independently and in different ways, there
    is nothing prohibiting us from using both these methods at the same time. We can
    prune some of the model’s weights and then quantize the ones that are left to
    achieve an even stronger compression.
  prefs: []
  type: TYPE_NORMAL
- en: We already have our pruned model, so all we need to do is to quantize it while
    converting to TFLite, just like we did before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Quantization did not decrease the accuracy of the pruned model, but it compressed
    its size by roughly a third, from less than 6k to 4k bytes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b2d87f1fce5973b8a06955590df36b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overall, we started from a baseline model of 14k bytes and ended up with a compressed
    version of 4k bytes, a reduction of 70%.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e9bdcde57ad095b8355fe9a7686fa270.png)'
  prefs: []
  type: TYPE_IMG
- en: At the same time, the model’s accuracy saw barely any degradation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e66a19208a5fc0c2baeb68837ab0cbe.png)'
  prefs: []
  type: TYPE_IMG
- en: As machine learning models continue to grow in size and are being deployed on
    increasingly smaller devices, model optimization techniques such as quantization
    and pruning become essential to reduce the model size and increase its efficiency.
    By implementing these techniques in TensorFlow, we can compress the model without
    losing much accuracy. Optimizing models can lead to faster inference times, smaller
    memory footprints, and improved power efficiency, making it easier to deploy them
    on a wider range of devices.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b2d87f1fce5973b8a06955590df36b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: If you liked this post, why don’t you [**subscribe for email updates**](https://michaloleszak.medium.com/subscribe)
    on my new articles? And by [**becoming a Medium member**](https://michaloleszak.medium.com/membership),
    you can support my writing and get unlimited access to all stories by other authors
    and yours truly.
  prefs: []
  type: TYPE_NORMAL
- en: Want to always keep your finger on the pulse of the increasingly faster-developing
    field of machine learning and AI? Check out my new newsletter, [**AI Pulse**](https://pulseofai.substack.com/).
    Need consulting? You can ask me anything or book me for a 1:1 [**here**](https://topmate.io/michaloleszak).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also try one of [my other articles](https://michaloleszak.github.io/blog/).
    Can’t choose? Pick one of these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pub.towardsai.net/forget-about-chatgpt-f17a7f5089c3?source=post_page-----629342d1a96f--------------------------------)
    [## Forget About ChatGPT'
  prefs: []
  type: TYPE_NORMAL
- en: Bard, Sparrow, and multimodal chatbots will render it obsolete soon, and here
    is why.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pub.towardsai.net](https://pub.towardsai.net/forget-about-chatgpt-f17a7f5089c3?source=post_page-----629342d1a96f--------------------------------)
    [](/self-supervised-learning-in-computer-vision-fd43719b1625?source=post_page-----629342d1a96f--------------------------------)
    [## Self-Supervised Learning in Computer Vision
  prefs: []
  type: TYPE_NORMAL
- en: How to train models with only a few labeled examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/self-supervised-learning-in-computer-vision-fd43719b1625?source=post_page-----629342d1a96f--------------------------------)
    [](/monte-carlo-dropout-7fd52f8b6571?source=post_page-----629342d1a96f--------------------------------)
    [## Monte Carlo Dropout
  prefs: []
  type: TYPE_NORMAL
- en: Improve your neural network for free with one small trick, getting model uncertainty
    estimate as a bonus.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/monte-carlo-dropout-7fd52f8b6571?source=post_page-----629342d1a96f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: All images, unless otherwise noted, are by the author.
  prefs: []
  type: TYPE_NORMAL
