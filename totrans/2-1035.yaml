- en: 'Harnessing the Power of Knowledge Graphs: Enriching an LLM with Structured
    Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/harnessing-the-power-of-knowledge-graphs-enriching-an-llm-with-structured-data-997fabc62386](https://towardsdatascience.com/harnessing-the-power-of-knowledge-graphs-enriching-an-llm-with-structured-data-997fabc62386)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e32186b4eaa2e36dee4c8c50e5110b8f.png)'
  prefs: []
  type: TYPE_IMG
- en: A step-by-step guide to creating a knowledge graph and exploring its potential
    to enhance an LLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://stevehedden.medium.com/?source=post_page-----997fabc62386--------------------------------)[![Steve
    Hedden](../Images/af7bec4a191ab857eccd885dd89e88b4.png)](https://stevehedden.medium.com/?source=post_page-----997fabc62386--------------------------------)[](https://towardsdatascience.com/?source=post_page-----997fabc62386--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----997fabc62386--------------------------------)
    [Steve Hedden](https://stevehedden.medium.com/?source=post_page-----997fabc62386--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----997fabc62386--------------------------------)
    ·20 min read·Jul 10, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*For accompanying code, see notebook* [*here.*](https://github.com/SteveHedden/kg_llm/blob/main/SDKG.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, [large language models](https://snorkel.ai/large-language-models-llms/)
    (LLMs), have become ubiquitous. Perhaps the most famous LLM is ChatGPT, which
    was released by OpenAI in November 2022\. ChatGPT is able to [generate ideas](https://www.linkedin.com/pulse/generate-100-content-ideas-chat-gpt-mfon-akpan/),
    [give personalized recommendations](https://bootcamp.uxdesign.cc/how-to-use-chatgpt-for-personalized-recommendations-840e01dcad89),
    [understand complicated topics](https://medium.com/101-innovation-hacks/using-chatgpt-to-explain-complex-concepts-2ea6aba97cf3),
    [act as a writing assistant](https://chatgptwriter.ai/), or [help you build a
    model to predict the Academy Awards.](https://medium.com/design-bootcamp/using-chatgpt-to-predict-the-oscars-c6d8cdb6b3a0)
    Meta has announced their own LLM called [LLaMA](https://ai.meta.com/blog/large-language-model-llama-meta-ai/),
    Google has [LaMDA](https://blog.google/technology/ai/lamda/), and there is even
    an open-source alternative, [BLOOM.](https://huggingface.co/bigscience/bloom)
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs have excelled in natural language processing (NLP) tasks like the ones
    listed above because LLMs have historically focused on [unstructured data](https://en.wikipedia.org/wiki/Unstructured_data)
    — data that does not have a pre-defined structure, and is usually text-heavy.
    I asked ChatGPT, “why have LLMs historically focused on unstructured data?” The
    reply was:'
  prefs: []
  type: TYPE_NORMAL
- en: “LLMs have historically focused on unstructured data due to its abundance, availability,
    and the challenges it poses. Unstructured data provides a vast source for training
    language models, allowing them to learn patterns, context, and semantics. LLMs
    excel in handling the complexities and nuances of unstructured data, capturing
    the subtleties of language and leveraging the rich context, opinions, and creative
    expressions it contains. While efforts are underway to bridge the gap between
    LLMs and structured data, the historical emphasis on unstructured data stems from
    its unique characteristics and the valuable information it offers, allowing LLMs
    to provide more human-like responses.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Structured data](https://www.ibm.com/cloud/blog/structured-vs-unstructured-data)
    is usually quantitative and well-organized, usually into rows and columns, and,
    as ChatGPT notes, there is still a gap between LLMs and structured data.'
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge graphs (KG), on the other hand, are excellent at querying structured
    data. A knowledge graph is,
  prefs: []
  type: TYPE_NORMAL
- en: “directed labeled graph in which domain specific meanings are associated with
    nodes and edges. A node could represent any real-world entity, for example, people,
    company, computer, etc. An edge label captures the relationship of interest between
    the two nodes, for example, a friendship relationship between two people, a customer
    relationship between a company and person, or a network connection between two
    computers, etc.” [(Chaudhri et al., 2022)](https://onlinelibrary.wiley.com/doi/10.1002/aaai.12033).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: KGs allow one to integrate heterogenous data sources, including combinations
    of structured, semi-structured, and unstructured data. KGs are used for organizing
    data, drawing inferences, [creating recommendations](/introduction-to-knowledge-graph-based-recommender-systems-34254efd1960),
    and [semantic search](https://www.stardog.com/blog/how-to-build-a-semantic-search-engine-using-a-knowledge-graph/).
  prefs: []
  type: TYPE_NORMAL
- en: 'As Shirui Pan et al. point out in their paper, “[Unifying Large Language Models
    and Knowledge Graphs: A Roadmap](https://arxiv.org/abs/2306.08302),” the two models
    can be complementary. Some of the main weaknesses of LLMs, that they are black-box
    models and struggle with factual knowledge, are some of the KGs’ greatest strengths.
    KGs are, essentially, collections of facts, and they are fully interpretable.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b67d913513771a3e703d2426924b8a5.png)'
  prefs: []
  type: TYPE_IMG
- en: How LLMs and KGs can be complementary. From Shirui Pan et al., 2023\. <[https://arxiv.org/abs/2306.08302](https://arxiv.org/abs/2306.08302)>
  prefs: []
  type: TYPE_NORMAL
- en: Shirui et al. propose many potential ways LLMs and KGs can complement each other.
    In this tutorial, I will show how to create a KG from structured data, and then
    use that KG as part of the input prompt to the LLM, something called in-context
    learning. I will compare the LLM’s responses when using the KG as part of the
    input with the LLM’s responses when using the original structured data as part
    of the input prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'The methodology for this tutorial is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Download some metadata on World Bank documents using the [World Bank API](https://documents.worldbank.org/en/publication/documents-reports/api)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build an ontology using the metadata for the documents
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Populate the ontology with instances of documents
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pull in additional entities and relationships into the graph using [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Query the KG directly using [SPARQL](https://www.ontotext.com/knowledgehub/fundamentals/what-is-sparql/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compare the different ways of interacting with data: using SPARQL queries on
    the RDF, putting raw metadata into [LlamaIndex](https://www.llamaindex.ai/), and
    putting the RDF data into LlamaIndex'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you don’t want to read any more, my overall takeaways are that:'
  prefs: []
  type: TYPE_NORMAL
- en: 'While it is possible to use structured data (in the form of CSV or RDF files)
    directly, using in-context learning, to enhance an LLM, the results are not great.
    The LLM suffers the same problems: it sometimes gives correct answers but sometimes
    suffers from hallucinations (incorrect facts) and it is impossible to know how/why
    these are occurring.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Turning your structured data into a knowledge graph by building out an ontology
    and assigning each instance of data appropriate classes and properties can improve
    the results, but hallucinations and inexplicable inaccuracies persist.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are interested in specific queries across your personal structured dataset
    and need accurate and verifiable answers, you should use formal queries either
    using SPARQL or something else. A SPARQL query can answer a question like, “what
    are all of the projects associated with documents that this author has written?”
    much better than an LLM, even when enhanced through in-context learning with the
    KG.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An LLM can help write and refine the SPARQL query, however. If an LLM can translate
    a prompt into a SPARQL query, then a user can still ‘chat’ directly with structured
    data without writing their own code/queries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are great at understanding and interpreting unstructured data. This capability
    extends even to structured data *when unstructured information is embedded within
    it.* For instance, if the structured data includes a column labeled ‘abstracts’
    containing unstructured text, the LLM can leverage that data to generate insightful
    results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using both the raw metadata and the KG to enhance the LLM did not improve results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some other ways Shirui et al. suggest KGs and LLMs can work together: use LLMs
    to translate prompt into formal query, use KGs to validate the LLMs responses,
    [use LLMs to build KGs](https://gpt-index.readthedocs.io/en/latest/reference/indices/kg.html),
    and use KGs to train LLMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1\. Download some metadata on World Bank documents using the World Bank API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First we need the metadata for some World Bank docs. For full documentation
    on the World Bank Documents and Reports API, go [here.](https://documents.worldbank.org/en/publication/documents-reports/api)
  prefs: []
  type: TYPE_NORMAL
- en: I selected World Bank document metadata as the foundation for our knowledge
    graph for several reasons. Firstly, the World Bank provides an API that enables
    access to their data. Additionally, the metadata associated with World Bank documents
    is comprehensive and offers valuable information. Lastly, my familiarity with
    this type of data ensures a better understanding of its structure and attributes.
    It’s important to note that the flexibility of building a knowledge graph applies
    to any data source, as long as one possesses sufficient domain knowledge to structure
    it effectively.
  prefs: []
  type: TYPE_NORMAL
- en: The following code gives us the metadata for the most recent 20 reports that
    have the phrase “sustainable development” in their title.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now we have our metadata saved as a pandas dataframe (df).
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Build an ontology using the metadata from the documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we can set up our ontology. What even is an ontology?
  prefs: []
  type: TYPE_NORMAL
- en: '*“An ontology defines a common vocabulary for researchers who need to share
    information in a domain. It includes machine-interpretable definitions of basic
    concepts in the domain and relations among them,”* [*(Noy and McGuinness, 2001).*](https://protege.stanford.edu/publications/ontology_development/ontology101.pdf)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: By adopting an ontology, we gain the ability to connect diverse data sources.
    In this tutorial, our main focus lies in World Bank data, however, if we were
    to integrate UN data, we would face challenges such as varying document type categories,
    date formats, and country names. An ontology serves as a valuable tool in bridging
    these differences and establishing connections between the datasets. Moreover,
    we can expand the scope of our knowledge graph by incorporating data from Wikidata,
    a widely used public knowledge graph closely related to Wikipedia. Aligning our
    data ontology with Wikidata’s ontology enables seamless integration of information
    from Wikidata into our own knowledge graph.
  prefs: []
  type: TYPE_NORMAL
- en: The following code sets up our graph
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: There is a column in the metadata called, “count”, which represents the country
    of origin for a given document. We want to use this column to create a ‘country’
    class in our ontology, along with subclasses for each unique country in this column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: I use [protégé](https://protege.stanford.edu/), a free, open-source ontology
    editor, to view and sometimes manually adjust my ontologies. If you want to view
    the ontology as you create it, just save the graph as a ttl file and open with
    protégé.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you open the file in protégé, it should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1be512ad9ab9802089b5314d6efcffc7.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot from Protégé. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the unique countries in the ‘count’ column of the World Bank data will
    have created a unique subclass in our ontology under the ‘country’ class. You
    can see that each country also has a label and a Wikidata URI. In this case, Argentina’s
    Wikidata URI is: [https://www.wikidata.org/entity/Q414](https://www.wikidata.org/wiki/Q414).'
  prefs: []
  type: TYPE_NORMAL
- en: That is also the link to the Wikidata page for the country Argentina.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have to create classes and subclasses for everything else. I made classes
    for document type, World Bank project, World Bank trustfund, country, and authors.
    I’m not putting all of that code in this tutorial, but see my [notebook](https://github.com/SteveHedden/kg_llm/blob/main/SDKG.ipynb)
    for the code to create all of it.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Populate the ontology with instances of documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ontology is the backbone of the KG, but now we need to populate it with
    data. The ontology so far defines classes, subclasses, properties of classes and
    subclasses, and the relations between them.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we now have a class for World Bank documents and a subclass for
    working papers. Our ontology establishes that a working paper is a subclass of
    (or a type of) a World Bank document. Our ontology establishes labels and other
    properties for these entities. For example, [working paper](https://www.wikidata.org/wiki/Q1228945)
    is an entity in Wikidata, and so we include the Wikidata URI in our ontology as
    a property.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/925aa55e53d0d825ac2f393d27d1b9a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Conceptualization of several entities in an ontology. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: In the example above, however, there are no actual World Bank working papers
    i.e. there are no instances of working papers. We have established the class,
    not the instances.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the code to go through our DataFrame row by row, and for each row, create
    an instance of a document, and assign that instance appropriate properties. Note
    that to run this code you will need to have created all of these classes already
    (again, see the notebook for full code).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have an actual knowledge graph. An instance of a working paper, with
    its associated properties, can be visualized the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e32186b4eaa2e36dee4c8c50e5110b8f.png)'
  prefs: []
  type: TYPE_IMG
- en: Conceptualization of ontology populated with some instances of data. Image by
    author.
  prefs: []
  type: TYPE_NORMAL
- en: You can save this new file and open in protégé to ensure that all of the entities
    have been incorporated appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Pull in additional entities and relationships into the graph using Wikidata
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because many of the entities in our KG have Wikidata URIs associated with them,
    we can import additional data from Wikidata into our KG. For this demo, I have
    only done this for countries. In the following code, we iterate over the country
    column, query Wikidata for the country entity, and import all properties and their
    values for the given country. *Note that this code can take a while to run — it
    has to query Wikidata for all of the properties for each country, and their values,
    and put them all into our graph.*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now we have an ontology populated with data from the World Bank and additional
    data from Wikidata! If you open your KG in protégé you can explore all of the
    properties we have imported. Below is a screenshot of some of the properties we’ve
    imported for Argentina. All of this data can also be viewed directly on Wikidata
    [here](https://www.wikidata.org/wiki/Q414).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5b517e5545d10f0f0b76a12c538b25e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot from Protégé. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Wikidata has data like: form of government, head of state, diplomatic relations,
    life expectancy (and many other development indicators) over time, subregions/territories,
    and many more.'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Query the KG directly using SPARQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can query this KG directly using [SPARQL](https://www.ontotext.com/knowledgehub/fundamentals/what-is-sparql/)
    queries, the standard query language for RDF databases (what we have created).
    There is a SPARQL wrapper in Python that we can use.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if want to see all documents about Brazil, we can run the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This will output all World Bank documents in our KG that have ‘Brazil’ listed
    as the countryOfOrigin.
  prefs: []
  type: TYPE_NORMAL
- en: We can make SPARQL queries as complicated as we like and include any properties
    from either the World Bank metadata or the Wikidata that we imported. For example,
    what if we want to know which author has written the most documents about countries
    that are federal republics?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2067ed15f4ae4e05dce8921af8c7fb46.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the author “World Bank” has written five documents about countries
    that Wikidata has listed as federal republics. In this case, the countries are
    Brazil, Comoros, and Ethiopia. This is significant because the World Bank data
    does not tell us the form of government for any country, that came from Wikidata.
    Since we built an ontology that aligns with Wikidata, we can incorporate additional
    data from Wikidata easily.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Compare the different ways of interacting with data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We know that we can use SPARQL to query the KG and get accurate results. But
    that requires that we write SPARQL queries, which requires some technical capabilities.
    Can we combine this accuracy with the ease of use of an LLM so that we can ‘chat’
    directly with the data? [LlamaIndex](https://www.llamaindex.ai/) is a powerful
    tool that allows you to enhance an LLM using your own data, what they call context
    information. [Here](https://gpt-index.readthedocs.io/en/latest/getting_started/starter_example.html)
    is a LlamaIndex tutorial to get started. I will use LlamaIndex to incorporate
    this World Bank data into an LLM in two ways: using the raw CSV file that we got
    directly from the World Bank API, and using the KG that we’ve built and populated
    using the World Bank data.'
  prefs: []
  type: TYPE_NORMAL
- en: Use raw CSV file we got directly from World Bank API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a starting point, let’s load the raw metadata from the World Bank into LlamaIndex.
    This will serve as a benchmark to which we can compare the results after incorporating
    the KG. The raw data that comes from the World Bank is in CSV format and does
    not have any ontology associated with it. Here is all the code you need to get
    LlamaIndex set up. You’ll need an OpenAI API key, which you can get from the OpenAI
    website. This code reads data directly from a local data folder, I have named
    my folder ‘data’. You can just put a csv file directly into this folder and LlamaIndex
    will index it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now we can ask it some questions, using basic human English, just like we would
    ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Brazil — LATIN AMERICA AND CARIBBEAN — P126452 — Rio Grande do Norte: Regional
    Development and Governance — Audited Financial Statement Brazil — LATIN AMERICA
    AND CARIBBEAN- P158000- Amazon Sustainable Landscapes Project — Procurement Plan`'
  prefs: []
  type: TYPE_NORMAL
- en: These documents are, in fact, World Bank documents about Brazil that are in
    the context information. There are, however, many other documents that are not
    listed here.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s try to get all of the documents that Anna Corsi has written.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The response is:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Corsi,Anna has not written any documents based on the context information.`'
  prefs: []
  type: TYPE_NORMAL
- en: This is incorrect. Anna Corsi is an author in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try one more time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'And the response is:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Anna Corsi is not mentioned in the given context information`'
  prefs: []
  type: TYPE_NORMAL
- en: Again, this is incorrect. Anna Corsi is one of the World Bank authors in our
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Use the knowledge graph as input
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now the big question — does using a knowledge graph as our context information
    improve these results? Rather than load the RDF data into LlamaIndex the same
    way, we will use the [RDFReader](https://github.com/emptycrown/llama-hub/tree/main/llama_hub/file/rdf).
    I have found this method of loading data into LlamaIndex to be a bit more problematic
    and it takes longer, but it is recommended way of incorporating RDF data into
    your input prompt. RDFReader requires that every entity in the KG have a label.
    So even if you add a comment on an entity you also need to add a label to the
    comment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '***Note:*** *that I am still using the GPTVectorStoreIndex for the RDF data,
    rather than the KnowledgeGraphIndex. I didn’t find that the KnowledgeGraphIndex
    produced any better results but it took a lot more time and was a lot more costly.*
    [*Others*](https://www.mikulskibartosz.name/llama-index-which-index-should-you-use/)
    *seem to have had similar experiences. My understanding is the the RDFReader is
    for using a pre-made KG and the KGIndex is for creating a KG with LlamaIndex.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once initialized, we can ask this LLM the same questions to see if the answers
    are any better. For the first prompt, “Show me all of the World Bank documents
    in the context information about Brazil,” the response is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This is partially correct. Two of these entities are documents about Brazil,
    but two are about Argentina, and one is about Ethiopia. There are also additional
    documents about Brazil that are not listed here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s ask about a specific author again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The response is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This is partially correct. Anna Corsi did write this document, but she wrote
    three other documents in the context information that are not listed here.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The response is:'
  prefs: []
  type: TYPE_NORMAL
- en: '`‘Anna Corsi is an author of the document “Concept Project Information Document
    (PID) — Land administration infrastructure for green and sustainable development
    — P179217”. This document is about a project to support the development of a mass
    valuation system in Turkey and generate market values for individual property
    units. Corsi is also the author of “The Time is Now : How Can Uzbekistan Leverage
    Urbanization as a Driver of Sustainable Development?”, a document about how Uzbekistan
    can use urbanization to promote sustainable development.’`'
  prefs: []
  type: TYPE_NORMAL
- en: This is partially correct. The first part, about the land administration project
    in Turkey, is correct. Anna Corsi was not, however, an author of the Uzbekistan
    report.
  prefs: []
  type: TYPE_NORMAL
- en: 'The LLM clearly struggles with specific structured data questions. But, as
    we said at the beginning, LLMs excel at understanding and generating human-like
    text, capturing intricate language patterns, and producing contextually relevant
    responses. Here is an example of how we can use the strengths of the LLM on this
    dataset. Suppose we already know that the World Bank has a land management infrastructure
    project in Turkey. We can ask the LLM to describe the project to us using the
    following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The response is:'
  prefs: []
  type: TYPE_NORMAL
- en: '`The objective of the Land Management Infrastructure for Green and Sustainable
    Development Project is to improve the accuracy and accessibility of land administration
    information in Turkiye. There are three components to the project, the first component
    being creating 3D city models and updating cadastre data. This component will
    support: (i) the creation of 3D city models based on the proven approach tested
    in the Amasya pilot; and (ii) the completion of the update and verification of
    cadastral data for 6 million parcels (out of the total remaining 11 million parcels19
    not covered by LRCMP), in both urban and rural areas. As part of the cadastre
    updating activities, capacity building programs for addressing challenges concerning
    women’s land rights and ownership will be discussed with TKGM to determine how
    to better address these issues as part of the public consultation step during
    the surveying process. While activities on the update and verification of cadastral
    data will be carried out in both urban and rural areas, for the creation of 3D
    city models, the Project will cover major urban areas (approximately 40,000 km,
    almost all urban areas in the country) in all 81 provinces in Turkiye. As part
    of these activities, the project will finance the completion and renewal`'
  prefs: []
  type: TYPE_NORMAL
- en: This is an accurate summary of the document in our database. This information
    largely comes from the ‘abstract’ field in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, incorporating structured data directly into LLMs through in-context
    learning yields suboptimal results, and challenges such as occasional hallucinations
    and lack of transparency persist. While transforming structured data into knowledge
    graphs improves LLM performance to some extent, formal query languages like SPARQL
    outperform LLMs in specific queries across structured datasets. However, LLMs
    can assist in formulating SPARQL queries, providing a way for users to ‘chat’
    with structured data without writing formal queries.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have demonstrated their prowess in understanding and interpreting unstructured
    data, allowing them to provide relevant responses. This capability extends even
    to structured data when unstructured information is embedded within it. For instance,
    if the structured data includes a column labeled ‘abstracts’ containing unstructured
    text, the LLM can leverage that data to generate insightful results.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative synergies between LLMs and knowledge graphs, such as using LLMs
    to translate prompts into formal queries or leveraging knowledge graphs for LLM
    validation, offer potential avenues for exploration and advancement in the field.
  prefs: []
  type: TYPE_NORMAL
