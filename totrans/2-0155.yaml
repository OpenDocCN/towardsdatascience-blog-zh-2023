- en: A Beginner-Friendly Introduction to Applied Science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-beginner-friendly-introduction-to-applied-science-dd60741a9b17](https://towardsdatascience.com/a-beginner-friendly-introduction-to-applied-science-dd60741a9b17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn the basics of features and statistical analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://emmaccode.medium.com/?source=post_page-----dd60741a9b17--------------------------------)[![Emma
    Boudreau](../Images/f7201d012b733643d6e97957f73fd1fa.png)](https://emmaccode.medium.com/?source=post_page-----dd60741a9b17--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dd60741a9b17--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dd60741a9b17--------------------------------)
    [Emma Boudreau](https://emmaccode.medium.com/?source=post_page-----dd60741a9b17--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dd60741a9b17--------------------------------)
    ·12 min read·Jun 6, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e91a2f1a3c4434bcfa619eddf242623.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Edge2Edge Media](https://unsplash.com/@edge2edgemedia?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Empiricism is likely the most important aspect to formulating a real-world understanding
    of just about anything. While anecdotal observation might go a long way for more
    basic things, it is important that things we actually want to be proven and accepted
    as fact are handled with empiricism. The backbone of such empiricism is of course
    data. Data is great because it gives us some insight into the world around us.
    However, the big shortcoming to data is that data must be interpreted, and such
    interpretation does require some level of expertise — meaning that there are likely
    many people who would not be able to draw some empirical conclusions based on
    data. The discretion that separates these from the people that can is of course
    statistical expertise.
  prefs: []
  type: TYPE_NORMAL
- en: Statistics allow us to break down different things about our data and make observations
    on different things about that data. Statistical applications to data can be incredibly
    powerful and often motivate some of the most important decisions that are made
    globally on a daily basis. Taking this into consideration, it is easy to see why
    such a skill might be desirable, important, and potentially even overwhelming.
    Fortunately, math is just math — numbers are just numbers — things might get confusing,
    but there is always an explanation. When it comes to statistics, some relatively
    rudimentary knowledge can actually go a long way. All of this comes to be very
    important for many modern disciplines; from Data Science to Analytics and Marketing,
    simple statistics are often a prominent excercise, so knowing such statistics
    in these fields can also be very valuable.
  prefs: []
  type: TYPE_NORMAL
- en: '[notebook](https://github.com/emmettgb/Emmetts-DS-NoteBooks/blob/master/Julia/Basic%20Applied%20Science.ipynb)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can really understand statistics, Data Science, machine learning,
    or anything of this nature, we first need to understand data. Data is comprised
    of features and their subsequent observations that comprise a **population**.
    Population generally means “ all of our clean data,” as opposed to **sample**
    which may infer that we are working with some data under the context of a hypothesis
    test or something of that nature. That might sound confusing, but it is simple
    to remember; **your population is all of your data and the sample is some of your
    data**. A feature is simply a column of this data, and an observation a row. For
    example, a feature of a human would be hair color, and an observation would be
    blonde. Our features are outlines for how the data is structured and what is measured
    in the data. The observations are the actual data that corresponds to that outline.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three main types of features:'
  prefs: []
  type: TYPE_NORMAL
- en: Labels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: labels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Labels are the least used out of all features, however can often be used to
    distinguish different observations. These can be used in machine-learning or statistical
    contexts, but rarely are. An example of this being used as a feature in a statistical
    context would be if we wanted to test past a certain year for some statistical
    significance. For example, if we wanted to test if people were taller in 2021
    than 2001 we would grab a sample from both and test the statistical significance.
    In a machine-learning context, there certainly are instances where dates or names
    might be used to predict certain features, and I have seen this be the case. A
    real world example I have had experience with was training a machine-learning
    algorithm to predict market values for different items at different dates.
  prefs: []
  type: TYPE_NORMAL
- en: categorical
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Categorical data is a lot like label data. While label data does not really
    tell us a lot about the observation, however, categorical data does often actually
    represent an actual measurement. In some ways, categorical data is just a label
    for a section of continuous space — this will make a lot more sense as one gets
    to know categorical data… But to understand what I mean I will need to cite an
    example.
  prefs: []
  type: TYPE_NORMAL
- en: We could consider that someone who is 300+ pounds is very overweight, someone
    who is between 200–300 is pretty overweight, 100–200 is generally average, and
    below 100 is underweight for a specific demographic of people. These are all categorical
    labels placed over continuous space, but in a lot of ways can tell us more about
    the data while simplifying this feature — making it easier to interpret and simplified.
    We might not need to necessarily know an exact weight if our statistics only need
    to know if someone is overweight. This helps relieve some level of bias, prevent
    fit issues on models, and can also be helpful to obscure personal information.
  prefs: []
  type: TYPE_NORMAL
- en: continuous
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Continuous features are very much the quintessential feature type. These could
    be ratings, measurements from instruments, temperature, anything that is numerical.
    Every continuous feature then seemingly forms its own numerical space. With this
    comes a mean, a standard deviation, and all of the other things we conventionally
    use for statistical analysis. The same can be said for categorical data, but in
    some capacity such features must be translated into language that statistics and
    computers can understand — numbers. More often, we instead take the avenue of
    using categorical features to distinguish different observations for our sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'In many cases, there is going to be a good chance that you will have to perform
    some processing on your features. This is usually based on what type of features
    you are dealing with and it certainly can be very tricky. If you would like an
    overview of processing these different types of features, here are some articles
    I wrote on that topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/features-101-an-introduction-to-analyzing-feature-sets-92e19d09ebd2?source=post_page-----dd60741a9b17--------------------------------)
    [## Features 101: An Introduction To Analyzing Feature-Sets'
  prefs: []
  type: TYPE_NORMAL
- en: A small guide on how to work with and understand features better
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/features-101-an-introduction-to-analyzing-feature-sets-92e19d09ebd2?source=post_page-----dd60741a9b17--------------------------------)
    [](https://chifi.dev/categorical-data-vs-continuous-data-everything-you-need-to-know-36c2a0dbf6c?source=post_page-----dd60741a9b17--------------------------------)
    [## Categorical Data Vs. Continuous Data: Everything You Need To Know'
  prefs: []
  type: TYPE_NORMAL
- en: The rundown on the two most common feature types, and some important things
    to remember about them.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: chifi.dev](https://chifi.dev/categorical-data-vs-continuous-data-everything-you-need-to-know-36c2a0dbf6c?source=post_page-----dd60741a9b17--------------------------------)
    [](/encoders-how-to-write-them-how-to-use-them-d8dd70f45e39?source=post_page-----dd60741a9b17--------------------------------)
    [## Encoders — How To Write Them, How To Use Them
  prefs: []
  type: TYPE_NORMAL
- en: Discover the many uses of encoders for the purpose of machine-learning with
    this “ from-scratch” walkthrough!
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/encoders-how-to-write-them-how-to-use-them-d8dd70f45e39?source=post_page-----dd60741a9b17--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have a basic grasp on data and the different types of features
    we might encounter on our travels, we can finally start analyzing some data. In
    order to make this as comprehensible as possible, I will be taking on some data
    in order to demonstrate these different concepts. The data I will be using is
    the `trees.csv` data distributed by Florida State University available [here](https://people.sc.fsu.edu/~jburkardt/data/csv/csv.html)
    (GNU LGPL license). I will be using the Julia programming language, but the process
    is generally the same for most different solutions, the method calls or technique
    of change might vary depending on what is used, but generally the techniques and
    ultimate changes we are making are identical. We will start this process by reading
    in the data and doing some basic processing. This will require some packages in
    Julia:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s import, and I will use a sink argument in order to read in our DataFrame.
    A sink argument just allows for our data format module to infer how to read some
    data into a given type; in our case, DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In most cases, this would be where we would clean our data and remove any missing
    values. Luckily, this data is incredibly clean and doesn’t even have missing values.
    The shape remains the same after a drop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: That being said, we can go straight to looking at our features.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/752963623be0e0175f4088f951b500b0.png)'
  prefs: []
  type: TYPE_IMG
- en: We have four different features, Index, Girth, Height, Volume. Girth, Height,
    and Volume are all continuous features. Index is just a representation of the
    row, a label feature. In some cases, it can be confusing whether or not we are
    looking at a label or a feature. A great way to figure this out is to see how
    many unique values there are. Especially in cases where this is the same as the
    length of the data, this is an obvious indicator for a label.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'There are no categorical features in this data, but this gives us a great opportunity
    to create one! As stated prior, categorical features are often just a measurement
    of continuous features. For example, we could classify our trees by how tall they
    are. Using a simple comprehension, I will create a new feature which will determine
    a category based on the mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: And now we have a new feature!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now we have a basic relationship with our data where we could actually find
    some insights. For example, it might be interesting to see if height correlates
    to width. For this, we now need to structure a test.
  prefs: []
  type: TYPE_NORMAL
- en: hypothesis testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have established our features and recognize our data in the format
    it is presented, we might begin to formulate a hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/76b2932ae6b8593026b8a4702302e54f.png)'
  prefs: []
  type: TYPE_IMG
- en: Considering our features, we will form the following hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: “ If the height of a tree is higher, then it is likely girth will also be higher.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is an interesting hypothesis because the answer is not immediately obvious.
    Of course, there is a lot of genetic diversity between trees and some trees grow
    tall while some trees grow wide. This data also only has 31 observations, which
    probably means we do not have a good enough generalization to accept this hypothesis.
    That being said, we can still perform a test! To start, we will create the sample
    we discussed earlier. We will use our new category for this, separating trees
    by whether or not they are taller or shorter. With Julia, I like to make my own
    neat little dispatch for this that just makes conditional masking a lot easier.
    Of course, it can still be done with `filter!` .
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now I will separate our the taller df:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now we can perform a test to check if the girth inside of this new separated
    sample is statistically significant. If this were the case, we could potentially
    accept or reject our null hypothesis and recieve an answer. There is more to this;
    we have a certain level of confidence to any test, but for this brief overview
    we will only touch lightly on this. Now I will get our two samples out, note that
    they will need to be of the same length, so we will randomly subsample to population
    up to the length of our sample.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Our `grow_samp` is our group that is categorized as greater in height. Our `samples`
    are random samples from our population. Using these two, we can determine if there
    is any difference between the trees labeled as taller and the trees that are not
    in terms of girth. For this, I will use a One Sample T-test, also known as an
    independent T test. This is a standard hypothesis test and likely the easiest
    to learn. In statistics, everything starts with a parabolla. The majority of the
    population in this parabola lies in the center. Things which are statistically
    significant lie in abnormality; away from the rest of the data, to the sides —
    or, tails — of the parabola. In the example of the normal distribution, our mean
    sits center and two standard deviations from the mean set on each end. In either
    direction — negative or positive standard deviations from the mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'The parabola itself is called a distribution, as it describes how our data
    was distributed. In most cases, given we live in 2023, we are likely going to
    be working with software that has already done the formulas for you. As a beginner,
    I would recommend getting familiar with Probability Density Functions, PDFs, and
    cumulative distribution functions, CDFs, but not necessarily learning the formulas
    for the functions. We are going to go into some detail here, but not much, but
    here are some articles that go more in depth on the topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/what-is-a-probability-density-function-d9b4b8bea121?source=post_page-----dd60741a9b17--------------------------------)
    [## What Is A Probability Density Function?'
  prefs: []
  type: TYPE_NORMAL
- en: towardsdatascience.com](/what-is-a-probability-density-function-d9b4b8bea121?source=post_page-----dd60741a9b17--------------------------------)
    [](/what-is-a-cumulative-distribution-function-2e0540ec2a60?source=post_page-----dd60741a9b17--------------------------------)
    [## What Is A Cumulative Distribution Function?
  prefs: []
  type: TYPE_NORMAL
- en: An overview of CDF’s and their application in Data Science
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/what-is-a-cumulative-distribution-function-2e0540ec2a60?source=post_page-----dd60741a9b17--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: One notable PDF one might want to get familiar with is that of the normal distribution.
    This is a simple formula that makes a lot of sense. In order to encourage thinking,
    let’s work our way backwards; what do we want in return?
  prefs: []
  type: TYPE_NORMAL
- en: We want a distribution function that will lay our data out based on probability
    where each input is scaled to its standard deviations from the mean. If we were
    to take a value from some data, how would we find out how many standard deviations
    from the mean we are? It takes 5 packing peanuts to ship our boxes, we had 200
    but used 125, how many more boxes can we fill? This is a similar problem; first
    we get the difference between our value and the mean, and then we see how many
    standard deviations it is from the mean just as we subtract the used peanuts and
    divide by how many per box to see how many we might fill.
  prefs: []
  type: TYPE_NORMAL
- en: x̄-µ/σ
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Where
  prefs: []
  type: TYPE_NORMAL
- en: xbar is an observation from our sample,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mu is the sample mean,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and lowercase sigma is the standard deviation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you would like to learn more about what these symbols mean in the context
    of statistics, here is an article I wrote all about it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/the-statistical-greek-alphabet-in-python-65295526146?source=post_page-----dd60741a9b17--------------------------------)
    [## The Statistical Greek Alphabet In Python'
  prefs: []
  type: TYPE_NORMAL
- en: Get familiar with what each Greek letter represents in statistics.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/the-statistical-greek-alphabet-in-python-65295526146?source=post_page-----dd60741a9b17--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'What we are testing for whenever we test for correlation is whether or not
    our data is abnormal; whether or not it lies in those tails that are two standard
    deviations from the mean. It is important to remember exactly how these tests
    work; we are not testing for cause, we are testing for correlation. This does
    not mean that these trees are this wide because they are tall, they just are wider
    when they are taller. Another thing to consider here is our hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: If a given tree is taller, then it also likely has more girth.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Whenever we perform this test, we are not testing to see if something is true.
    We are testing to see if the opposite is true; our real question here is entirely
    different.
  prefs: []
  type: TYPE_NORMAL
- en: If a given tree is taller, then it still has normal girth.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Whenever we refuse to reject our hypothesis, we are not even refusing to reject
    what we originally set out to prove; we are just proving that the opposite is
    not true. This is called the null hypothesis, and should be in the back of your
    mind during an experiment. Now back to our expirement. As I briefly mentioned,
    there is a lot of math involved with the different functions for different distributions.
    The most approachable of testing distributions is probably the T distribution,
    this distributioin still has a rather complicated CDF, which is typically used
    for this type of one-sample test. That being said, we can use software libraries
    and the Julia ecosystem to our advantage. In languages like Python or R, there
    are similar packages which are capable of performing this sort of test. For this,
    I will use the `OneSampleTTest` from `HypothesisTests` .
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: While it is unfortunate that we do not have many observations here to work with,
    from this data there is not much statistical significance to this hypothesis.
    We have found that the opposite could potentially be true, even — though we would
    certainly need more data to find out. Our P value came out to a mere 0.225\. Generally
    speaking, a P value below .05 is considered statistically significant. Our value
    certainly indicates that this is not the case, at least not in this data.
  prefs: []
  type: TYPE_NORMAL
- en: Hypothesis testing and applied sciences can not only be a valuable asset for
    Data Scientists, programmers, analysts, and more, but can also play a vital role
    in life and understanding the world around you. Mathematics and science are incredibly
    particular and scrutinizing, but at the same time can often be very rewarding
    and allow us to learn new things from simple observations. Applied science really
    is a superpower, and statistics are infinitely valuable in many different cases.
    Thank you for reading, and I hope that this article provided plenty of valuable
    information!
  prefs: []
  type: TYPE_NORMAL
