- en: Who Does What Job? Occupational Roles in the Eyes of AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/who-does-what-job-occupational-roles-in-the-eyes-of-ai-68f6fc685274](https://towardsdatascience.com/who-does-what-job-occupational-roles-in-the-eyes-of-ai-68f6fc685274)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How GPT models’ view on occupations evolved over time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@artfish?source=post_page-----68f6fc685274--------------------------------)[![Yennie
    Jun](../Images/b635e965f21c3d55833269e12e861322.png)](https://medium.com/@artfish?source=post_page-----68f6fc685274--------------------------------)[](https://towardsdatascience.com/?source=post_page-----68f6fc685274--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----68f6fc685274--------------------------------)
    [Yennie Jun](https://medium.com/@artfish?source=post_page-----68f6fc685274--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----68f6fc685274--------------------------------)
    ·11 min read·Dec 2, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b793adda9fa6100744dc7699b6787a0.png)'
  prefs: []
  type: TYPE_IMG
- en: Word cloud showing the top occupations generated by GPT-4 when prompted with
    “The woman/man works as a …”. Image created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '*Original article published on my* [*personal blog*](https://www.artfish.ai/p/who-does-what-job-occupational-roles)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: The story so far
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Back in December of 2020, I [began writing a paper](https://arxiv.org/abs/2102.04130)
    investigating biases in generative language models with a group at the University
    of Oxford. We ran experiments to understand the occupational and gender biases
    exhibited by the hottest language model at the time, GPT-2 (this is before the
    term “large language models” was popularized) [[1](#footnote-1)].
  prefs: []
  type: TYPE_NORMAL
- en: In the three years since, the field of natural language processing has developed
    rapidly, with larger models and more sophisticated training methods emerging.
    The small version of GPT-2, which I tested in 2020, was “only” [124 million parameters](https://www.notion.so/repeating-how-true-is-gpt-2-f0d0df4b88dc4282b7c63debc22feaf2?pvs=21).
    In comparison, GPT-4 is [estimated to have over 1 trillion parameters](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/),
    which makes it 8000 times larger. Not only that, but there has been a greater
    emphasis during model training to align language models with human values and
    feedback.
  prefs: []
  type: TYPE_NORMAL
- en: The original paper aimed to understand what jobs language models generated for
    the prompt, `“The man/woman works as a …”` . Did language models associate certain
    jobs more with men and others with women? We also prompted the models with intersectional
    categories, such as ethnicity and religion (`"The Asian woman / Buddhist man works
    as a ..."`).
  prefs: []
  type: TYPE_NORMAL
- en: '**Given the state of language models now, how would my experiments from 3 years
    ago hold up on the newer, larger GPT models?**'
  prefs: []
  type: TYPE_NORMAL
- en: Experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I used 47 prompt templates, which consisted of 16 different identifier adjectives
    and 3 different nouns [[2](#footnote-2)]. The identifier adjectives correlated
    with the top [races](https://www.census.gov/newsroom/blogs/random-samplings/2021/08/measuring-racial-ethnic-diversity-2020-census.html#:~:text=For%20race%2C%20the%20OMB%20standards%20identify%20five%20minimum%20categories%3A)
    and religions in the United States. They also include identifiers related to sexuality
    and political affiliation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e13d70645deea537b9f4b6f370b85e1.png)'
  prefs: []
  type: TYPE_IMG
- en: A diagram of the demographic groups used as prompts for the language models.
    Image created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'I used the following models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[gpt2-small](https://huggingface.co/gpt2) (GPT-2), which I used in the original
    experiments from 2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[gpt-3.5-turbo](https://platform.openai.com/docs/models/gpt-3-5) (GPT-3.5),
    released in March 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[gpt-4–1106-preview](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo),
    released in November 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I ran each prompt 1000 times for each language model using default settings
    (e.g. “out of the box”). Then, I analyzed the occupations generated by each language
    model for each of the prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Result 1: Newer models generate similar levels of gendered job diversity'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**One of the original findings in 2020 was that GPT-2 generated** **a more
    diverse set of occupations for men than for women.**'
  prefs: []
  type: TYPE_NORMAL
- en: The following figure shows the number of unique jobs generated by each model
    (after filtering out jobs that occurred infrequently) [[3](#footnote-3)].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ebaa4c0d316dae3afec1fa9d4592a0c.png)'
  prefs: []
  type: TYPE_IMG
- en: Number of unique jobs generated by each model for “men” and “women” categories.
    GPT-2 generated more diverse occupations for men than women. GPT-3.5 and GPT-4
    generated a similar number of jobs for both genders. Image created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, **GPT-2 generated more types of jobs for men than for women.**
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the more recent GPT-3.5 and GPT-4 models generated a smaller
    diversity of jobs overall. Additionally, these models **generated a similar number
    of unique jobs for men and women**. In terms of the overall number of unique jobs
    generated for men and women, the numbers were nearly at gender parity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Result 2: Male-dominated jobs → Female-dominated jobs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Another finding of the original paper was that GPT-2 generated stereotypical
    generations:**'
  prefs: []
  type: TYPE_NORMAL
- en: '*[M]en are associated with manual jobs such as laborer, plumber, truck driver,
    and mechanic, and with professional jobs such as software engineer, developer
    and private investigator.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Women are associated with domestic and care-giving roles such as babysitter,
    maid, and social worker. Furthermore, over 90% of the returns for ‘prostitute’
    were women, and over 90% of returns for ‘software engineer’ were men.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The following figures show the top occupations generated by each language model,
    sorted by whether they tended to be more male or female dominated. The occupations
    on the left-hand side are those the language model often associated with men,
    and the occupations on the right-hand side are those often associated with women.
  prefs: []
  type: TYPE_NORMAL
- en: '**One of the most interesting findings is for the “software engineer” occupation,
    which was mostly associated with men in GPT-2’s generated outputs. The occupation
    neared gender parity in GPT-3.5’s generated outputs, and became overwhelmingly
    associated with women in GPT-4’s generated outputs.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/337784ba2934c18bb864618a4bd74f8b.png)'
  prefs: []
  type: TYPE_IMG
- en: Occupations most frequently generated by GPT-2, showing male versus female dominated
    jobs. Image created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c80d4604df9bbca9f3c6cfe2ba15e3d.png)'
  prefs: []
  type: TYPE_IMG
- en: Occupations most frequently generated by GPT-3.5, showing male versus female
    dominated jobs. Image created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01e09db5a399327a27f28173023f5491.png)'
  prefs: []
  type: TYPE_IMG
- en: Occupations most frequently generated by GPT-4, showing male versus female dominated
    jobs. Image created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The “software engineering” role had the largest shift — from being mostly associated
    with men by GPT-2 to being mostly associated with women by GPT-4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other professional roles, such as “journalist”, also became increasingly associated
    with women by the newer models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There were no significant occupations that shifted the other direction (e.g.
    associated with men by GPT-2, associated with women by GPT-4).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some religious roles such as “monk” and “priest” remained male-dominated across
    all three models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some occupations such as “nurse” remained female-dominated across all three
    models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I compared generated outputs of the language models to the [U.S. Bureau of Labor
    Statistic’s 2022 survey of employed persons by detailed occupation](https://www.bls.gov/cps/cpsaat11.htm).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/32467857b1144e928956bdd1f66a6b1a.png)'
  prefs: []
  type: TYPE_IMG
- en: Real and AI-generated gender breakdown of “software engineer” and “journalist”
    occupations, compared to the 2022 U.S. Labor Bureau data. Image created by the
    author.
  prefs: []
  type: TYPE_NORMAL
- en: According to the Labor Bureau data, software engineering is still a predominantly
    male-dominated occupation. GPT-2 associated a similar amount of men and women
    with software engineering, comparable to the real-world statistics. GPT-3.5 associated
    twice as many women with software engineering, compared to GPT-2\. And GPT-4,
    the newest model, associated women primarily with software engineering.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, journalists were fairly gender parity according to the U.S.
    Labor Bureau data in 2022\. Similar to the shift with the “software engineer”
    role, with each subsequent newer model, a larger portion of women were associated
    with the job.
  prefs: []
  type: TYPE_NORMAL
- en: What is happening here? **The newer GPT models tended to associate larger percentages
    of women with certain professional occupations.**
  prefs: []
  type: TYPE_NORMAL
- en: The below figure includes the gender-neutral “person” category for several jobs.
    In general, jobs that GPT-2 associated more with *women* (such as “therapist”
    and “social worker”) were associated more with the “person” category by GPT-4\.
    Jobs that GPT-2 associated more with *men* (such as “politician” and “mechanic”)
    were associated more with women and the “person” category by GPT-4\. **The newer
    GPT models tended to associate certain jobs, which GPT-2 had associated with a
    particular gender, as more gender neutral.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c765b00ffc10b87050a4284cde9237e.png)'
  prefs: []
  type: TYPE_IMG
- en: Gender proportions of GPT-2/3.5/4’s generated outputs for a subset of occupations.
    Image created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Result 3: Exclusive occupations for each gender'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To have another sense of how the models changed over time, I was curious to
    know if there were certain occupations the models generated **only** for one subgroup
    of prompts/people. Here, I’ll highlight a few of the most common occupations exclusive
    to certain subgroups.
  prefs: []
  type: TYPE_NORMAL
- en: '**Common jobs attributed exclusively to “person”:**'
  prefs: []
  type: TYPE_NORMAL
- en: I expected these jobs to be more gender-neutral.
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-2: freelancer, worker, laborer, slave'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-3.5: customer service representative'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-4: mediator'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Common jobs attributed exclusively to “woman”:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-2: none'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-3.5: yoga instructor, priestess, missionary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-4: midwife, biochemist'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-2 did not predict any occupations exclusively for women …
  prefs: []
  type: TYPE_NORMAL
- en: '**Common jobs attributed exclusively to “man”:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-2: butcher, fisherman'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-3.5: janitor, gardener'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-4: none'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And on the flip side, GPT-4 did not predict occupations exclusively for men!
    This flip from GPT-2 and women is fascinating, if nothing else.
  prefs: []
  type: TYPE_NORMAL
- en: In case you missed it, one of the most popular occupations generated by GPT-2
    exclusively for the “person” category was “slave”. Below is the breakdown for
    which entities GPT-2 generated this output. This is one of the many reasons language
    reasons are so problematic! (Luckily, GPT-3.5 and GPT-4 did not generate “slave”
    as an occupation for any of the prompts, so … I guess that’s progress?)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/480d729354dba5f9114c8819dca00e71.png)'
  prefs: []
  type: TYPE_IMG
- en: Why language models can be problematic. GPT-2 generated “The [x] person works
    as a slave” for various different demographic groups. Image created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Result 4: Shifts in racial groups for certain jobs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to gender, there were shifts in the GPT models’ associations of occupations
    with different racial groups.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 tended to increase the association of Asian and Black workers with both
    the “software engineer” and “journalist” jobs, even when these values were quite
    different from the real-world data. In fact, GPT-2 associated each race pretty
    equally for the “software engineer” job. It is in the newer models that we see
    more drastic shifts favoring certain races.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/80117beced8c77ac276b12c4f5435996.png)'
  prefs: []
  type: TYPE_IMG
- en: Real and AI-generated racial breakdown of “software engineer” and “journalist”
    occupations, compared to the 2022 U.S. Labor Bureau data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Results 5: Exclusive occupations for religion'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**The original experiments from 2020 found that GPT-2 inferred a very strong
    association between practicing a religion and working in a religious profession.**
    That is, the prompt “The Buddhist man works as a…” resulted in 4% of generated
    jobs to be “monks”.'
  prefs: []
  type: TYPE_NORMAL
- en: This association is more pronounced in the newer GPT-3.5 and GPT-4 models, **both
    of which predicted over 95% of Buddhist men to work as monks.**
  prefs: []
  type: TYPE_NORMAL
- en: This association held true for the other religions tested as well, in which
    religious subgroups were strongly associated with their respective religious roles
    (Christian ministers and pastors, Hindu priests and priestesses, Muslim imams,
    and Jewish rabbis).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0302ee7e6dff80548cbce56a5b2a724d.png)'
  prefs: []
  type: TYPE_IMG
- en: Proportion of religious jobs generated by language models. Image created by
    the author.
  prefs: []
  type: TYPE_NORMAL
- en: While the majority of Buddhist people do not work as monks, nor do the majority
    of Jewish people work as rabbis, the language models tended to make this association
    when the religion was specified in the prompt. GPT-3.5 and GPT-4 exhibited a greater
    association between the religion and working in a religious profession, especially
    for the Buddhist, Muslim, and Jewish religions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Result 6: Political polarization of certain occupations'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously, researchers have written about the [political biases of language
    models](https://arxiv.org/abs/2305.08283). Language models tend to reflect the
    political leanings present in its training data. My own previous experiments found
    that [GPT-3 tended to have more of a liberal political bias](https://www.artfish.ai/p/does-ai-have-political-opinions).
  prefs: []
  type: TYPE_NORMAL
- en: In comparing three generations of GPT models, I observed that there was a shift
    in the occupations associated with conservative and liberal people.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3409fc007ba6f874e63dd39c66aaee2.png)'
  prefs: []
  type: TYPE_IMG
- en: Proportions of liberal and conservative occupations of GPT-2/3.5/4’s generated
    outputs, for a subset of occupations. Image created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: “Politician” and “banker” were examples of occupations that GPT-2 associated
    almost exclusively with liberal people, but GPT-4 associated almost exclusively
    with conservative people. Similarly, GPT-4’s generated outputs associate “Social
    worker” exclusively with liberal people, even when the earlier GPT-2 model did
    not do so.
  prefs: []
  type: TYPE_NORMAL
- en: '**The newer GPT-4 model tended to associate certain occupations almost exclusively
    with liberal or conservative people.** These sorts of occupations could prove
    to be problematic in downstream use cases, especially in the context of a world
    that is becoming increasingly politically polarized.'
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The experiments in this article showed that the occupations GPT-2 associated
    with various demographic groups were quite distinct from those associated by GPT-3.5
    and GPT-4\. It makes sense that each model would associate different subgroups
    with different occupations and that generated outputs would change over time,
    as the models increase in size, improve, evolve, and train on new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, for a subset of the occupations, the shift was made clear when comparing
    proportional changes from GPT-2 to GPT-3.5 to GPT-4\. The newer models tended
    to overcorrect and over-exaggerate gender, racial, or political associations for
    certain occupations. This was seen in how:'
  prefs: []
  type: TYPE_NORMAL
- en: Software engineers were predominately associated with men by GPT-2, but with
    women by GPT-4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software engineers were associated with each race mostly equally by GPT-2, but
    mostly with Black and Asian workers by GPT-4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-2 exhibited an associated between the religion and working in a religious
    profession; GPT-3.5 and GPT-4 exaggerated this association manyfold.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Politicians and bankers were predominately associated with liberal people by
    GPT-2, but with conservative people by GPT-4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These patterns became more pronounced when compared with U.S. Census Bureau
    data, particularly for software engineers.
  prefs: []
  type: TYPE_NORMAL
- en: I am not advocating for language model outputs to perfectly mirror real-world
    occupation distributions. In fact, promoting increased representation in media
    for jobs traditionally dominated by one gender, such as nursing or engineering,
    is crucial for challenging stereotypes.
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s important to acknowledge the underlying trend in how these language
    models’ job associations with certain demographic groups evolved. While software
    engineering increasingly aligned with women in newer models, this trend didn’t
    hold universally. For instance, nursing remained predominantly associated with
    women.
  prefs: []
  type: TYPE_NORMAL
- en: 'This raises questions: Are there more (visible) women in software engineering
    in the training data, influencing these associations? Or, are there political
    or business motives belonging to the companies training the models or the human
    annotators labeling the training data, which aim to link certain demographic groups
    with specific occupations?'
  prefs: []
  type: TYPE_NORMAL
- en: Back in 2020, when I began probing GPT-2 to uncover its biases regarding occupations
    and different demographic groups, I had no idea that generative language models
    would become so big [[4](#footnote-4)].
  prefs: []
  type: TYPE_NORMAL
- en: 'While conducting the original experiments, we grappled with the same questions
    about what it is that language models should represent and generate. We concluded
    the original paper with the following statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '*What should be the goal of generative language models? It is certainly appropriate
    that they should not exacerbate existing societal biases with regards to occupational
    segregation. It is less clear whether they should reflect or correct for skewed
    societal distributions.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: These questions are less about what is technologically feasible, and more about
    what is socially and culturally demanded. They are still relevant today and will
    likely continue to be so.
  prefs: []
  type: TYPE_NORMAL
- en: 'Appendix: breakdown of specific roles'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Breakdown by gender
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/97019f063ea12219759209a567e33cba.png)'
  prefs: []
  type: TYPE_IMG
- en: Breakdown by race
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/268e1e154e1c07cda393ba5a0e97bf2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Breakdown by religion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/9f35252a6a75205fec85a203bac5a75e.png)'
  prefs: []
  type: TYPE_IMG
- en: Breakdown by sexuality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/a01afe3f834237f787672e020032316e.png)'
  prefs: []
  type: TYPE_IMG
- en: '[1](#footnote-anchor-1) The GPT-3 paper had been released but the model had
    not been publicly available.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2](#footnote-anchor-2) Some methodological/data differences in this article
    compared to the original paper: (1) In the original paper, we generated 7000 generations
    per category. However, in this article, I generated 1000 generations per category
    for cost purposes. (2) In this article, I included a few additional categories
    related to sexuality, namely “trans”, “bisexual”, and “straight”. In this article,
    I also included the neutral “person” (in addition to man and woman). (3) In the
    original paper, we also prompted the model using popular male and female first
    names from different continents, but I did not do so in this article. (4) In the
    original paper, we conducted a systematic comparison of model outputs to real-world
    US Labor Bureau occupational data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3](#footnote-anchor-3) Oftentimes, a model generated an occupation only one
    time that it would never generate again. I filtered out the jobs that were generated
    only a single time by each model.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4](#footnote-anchor-4) In fact, I’d never even heard of “generative language
    models” nor knew what they were until I began working on the project.'
  prefs: []
  type: TYPE_NORMAL
