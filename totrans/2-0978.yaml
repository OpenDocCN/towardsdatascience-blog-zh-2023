- en: 'Getting Started with LangChain: A Beginner’s Guide to Building LLM-Powered
    Applications'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用LangChain：构建LLM驱动应用程序的初学者指南
- en: 原文：[https://towardsdatascience.com/getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications-95fc8898732c](https://towardsdatascience.com/getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications-95fc8898732c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications-95fc8898732c](https://towardsdatascience.com/getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications-95fc8898732c)
- en: A LangChain tutorial to build anything with large language models in Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用大型语言模型在Python中构建任何东西的LangChain教程
- en: '[](https://medium.com/@iamleonie?source=post_page-----95fc8898732c--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----95fc8898732c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----95fc8898732c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----95fc8898732c--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----95fc8898732c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@iamleonie?source=post_page-----95fc8898732c--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----95fc8898732c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----95fc8898732c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----95fc8898732c--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----95fc8898732c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----95fc8898732c--------------------------------)
    ·11 min read·Apr 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----95fc8898732c--------------------------------)
    ·11分钟阅读·2023年4月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ba0a0432f474352f0aac6ee6c0e9eeea.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba0a0432f474352f0aac6ee6c0e9eeea.png)'
- en: “What did the stochastic parrot say to the other?” (Image drawn by the author)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: “随机的鹦鹉对其他的说了什么？”（作者绘制的图像）
- en: 'Since the release of ChatGPT, large language models (LLMs) have gained a lot
    of popularity. Although you probably don’t have enough money and computational
    resources to train an LLM from scratch in your basement, you can still use pre-trained
    LLMs to build something cool, such as:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自ChatGPT发布以来，大型语言模型（LLMs）已经获得了极大的关注。尽管你可能没有足够的资金和计算资源在地下室从头训练一个LLM，但你仍然可以使用预训练的LLM来构建一些很酷的东西，比如：
- en: '[Personal assistants](https://python.langchain.com/en/latest/use_cases/personal_assistants.html)
    that can interact with the outside world based on your data'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[可以根据您的数据与外部世界互动的个人助手](https://python.langchain.com/en/latest/use_cases/personal_assistants.html)'
- en: '[Chatbots](https://python.langchain.com/en/latest/use_cases/chatbots.html)
    customized for your purpose'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为您的目的定制的聊天机器人](https://python.langchain.com/en/latest/use_cases/chatbots.html)'
- en: '[Analysis](https://python.langchain.com/en/latest/use_cases/question_answering.html)
    or [summarization](https://python.langchain.com/en/latest/use_cases/summarization.html)
    of your documents or [code](https://python.langchain.com/en/latest/use_cases/code.html)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分析](https://python.langchain.com/en/latest/use_cases/question_answering.html)或[总结](https://python.langchain.com/en/latest/use_cases/summarization.html)您的文档或[代码](https://python.langchain.com/en/latest/use_cases/code.html)'
- en: LLMs are changing how we build AI-powered products
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: LLM正在改变我们构建AI驱动产品的方式
- en: With their weird APIs and prompt engineering, LLMs are changing how we build
    AI-powered products. That’s why new developer tools are emerging everywhere under
    the term [“LLMOps”](https://wandb.ai/iamleonie/Articles/reports/Understanding-LLMOps-Large-Language-Model-Operations--Vmlldzo0MDgyMDc2).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通过其奇特的API和提示工程，LLM正在改变我们构建AI驱动产品的方式。这就是为什么新的开发者工具在[“LLMOps”](https://wandb.ai/iamleonie/Articles/reports/Understanding-LLMOps-Large-Language-Model-Operations--Vmlldzo0MDgyMDc2)的名义下不断涌现的原因。
- en: One of these new tools is [LangChain](https://github.com/hwchase17/langchain).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些新工具之一是[LangChain](https://github.com/hwchase17/langchain)。
- en: '[](https://github.com/hwchase17/langchain?source=post_page-----95fc8898732c--------------------------------)
    [## GitHub - hwchase17/langchain: ⚡ Building applications with LLMs through composability
    ⚡'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/hwchase17/langchain?source=post_page-----95fc8898732c--------------------------------)
    [## GitHub - hwchase17/langchain: ⚡ 通过组合性构建LLM应用 ⚡'
- en: '⚡ Building applications with LLMs through composability ⚡ Production Support:
    As you move your LangChains into…'
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ⚡ 通过组合性构建LLM应用 ⚡ 生产支持：当你将LangChains迁移到…
- en: github.com](https://github.com/hwchase17/langchain?source=post_page-----95fc8898732c--------------------------------)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/hwchase17/langchain?source=post_page-----95fc8898732c--------------------------------)
- en: What is LangChain?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LangChain 是什么？
- en: 'LangChain is a framework built to help you build LLM-powered applications more
    easily by providing you with the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 是一个框架，旨在通过提供以下内容来帮助你更轻松地构建 LLM 驱动的应用程序：
- en: a generic interface to a variety of different foundation models (see [Models](#bd03)),
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种通用接口，用于各种不同的基础模型（见 [Models](#bd03)），
- en: a framework to help you manage your prompts (see [Prompts](#b1a5)), and
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个帮助你管理提示的框架（见 [Prompts](#b1a5)），以及
- en: a central interface to long-term memory (see [Memory](#3bc6)), external data
    (see [Indexes](#806f)), other LLMs (see [Chains](#fcac)), and other agents for
    tasks an LLM is not able to handle (e.g., calculations or search) (see [Agents](#b842)).
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个中央接口，用于长期记忆（见 [Memory](#3bc6)）、外部数据（见 [Indexes](#806f)）、其他 LLMs（见 [Chains](#fcac)）以及其他
    LLM 无法处理的任务（例如计算或搜索）（见 [Agents](#b842)）。
- en: It is an open-source project ([GitHub repository](https://github.com/hwchase17/langchain))
    created by [Harrison Chase](https://twitter.com/hwchase17).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个开源项目（[GitHub 仓库](https://github.com/hwchase17/langchain)），由 [哈里森·查斯](https://twitter.com/hwchase17)
    创建。
- en: Because LangChain has a lot of different functionalities, it may be challenging
    to understand what it does at first. That’s why we will go over the (currently)
    six key modules of LangChain in this article to give you a better understanding
    of its capabilities.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 LangChain 具有许多不同的功能，因此一开始可能很难理解它的作用。这就是为什么我们将在本文中介绍（目前的）六个关键模块，以便让你更好地理解其功能。
- en: Prerequisites
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 先决条件
- en: To follow along in this tutorial, you will need to have the `langchain` Python
    package installed and all relevant API keys ready to use.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随本教程，你需要安装 `langchain` Python 包，并准备好所有相关的 API 密钥。
- en: Installing LangChain
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 LangChain
- en: Before installing the `langchain` package, ensure you have a Python version
    of ≥ 3.8.1 and <4.0.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装 `langchain` 包之前，确保你有 Python 版本 ≥ 3.8.1 且 <4.0。
- en: To install the `langchain` Python package, you can `pip` install it.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 `langchain` Python 包，你可以使用 `pip` 进行安装。
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this tutorial, we are using version 0.0.147\. The [GitHub repository](https://github.com/hwchase17/langchain)
    is very active; thus, ensure you have a current version.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们使用的是版本 0.0.147。由于 [GitHub 仓库](https://github.com/hwchase17/langchain)
    非常活跃，请确保你使用的是最新版本。
- en: Once you are all setup, import the `langchain` Python package.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你完成所有设置，导入 `langchain` Python 包。
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: API keys
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: API 密钥
- en: Building an application with LLMs requires API keys for some services you want
    to use, and some APIs have associated costs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LLM 构建应用程序需要一些服务的 API 密钥，并且一些 API 可能会产生费用。
- en: '**LLM provider (required)** — You will first need an API key for the LLM provider
    you want to use. We are currently experiencing [“AI’s Linux moment](https://hazyresearch.stanford.edu/blog/2023-01-30-ai-linux)”
    where developers must **choose between proprietary or open-source foundation models**
    based on a trade-off mainly between performance and cost.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLM 提供者（必需）** — 你首先需要一个用于所需 LLM 提供者的 API 密钥。我们目前正经历[“AI 的 Linux 时刻](https://hazyresearch.stanford.edu/blog/2023-01-30-ai-linux)”，在这一时刻，开发者必须**在性能和成本之间权衡，选择专有或开源基础模型**。'
- en: '![](../Images/00b94afaccb89deaf33f4d10262f2f94.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00b94afaccb89deaf33f4d10262f2f94.png)'
- en: 'LLM Providers: Proprietary and open-source foundation models (Image by the
    author, inspired by [Fiddler.ai](https://www.fiddler.ai/blog/the-missing-link-in-generative-ai),
    first published on [W&B’s blog](https://wandb.ai/iamleonie/Articles/reports/Understanding-LLMOps-Large-Language-Model-Operations--Vmlldzo0MDgyMDc2))'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 提供者：专有和开源基础模型（图像由作者提供，灵感来源于 [Fiddler.ai](https://www.fiddler.ai/blog/the-missing-link-in-generative-ai)，首次发布在
    [W&B 的博客](https://wandb.ai/iamleonie/Articles/reports/Understanding-LLMOps-Large-Language-Model-Operations--Vmlldzo0MDgyMDc2)）
- en: '**Proprietary models** are closed-source foundation models owned by companies
    with large expert teams and big AI budgets. They usually are larger than open-source
    models and thus have better performance, but they also have expensive APIs. Examples
    of proprietary model providers are [OpenAI](https://python.langchain.com/en/latest/modules/models/llms/integrations/openai.html),
    [co:here](https://python.langchain.com/en/latest/modules/models/llms/integrations/cohere.html),
    [AI21 Labs](https://python.langchain.com/en/latest/modules/models/llms/integrations/ai21.html),
    or [Anthropic](https://python.langchain.com/en/latest/modules/models/llms/integrations/anthropic_example.html).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**专有模型** 是由拥有大型专家团队和大额 AI 预算的公司拥有的封闭源基础模型。它们通常比开源模型大，因此性能更好，但它们的 API 也很昂贵。专有模型提供商的例子包括
    [OpenAI](https://python.langchain.com/en/latest/modules/models/llms/integrations/openai.html)、[co:here](https://python.langchain.com/en/latest/modules/models/llms/integrations/cohere.html)、[AI21
    Labs](https://python.langchain.com/en/latest/modules/models/llms/integrations/ai21.html)
    或 [Anthropic](https://python.langchain.com/en/latest/modules/models/llms/integrations/anthropic_example.html)。'
- en: Most available LangChain tutorials use OpenAI but note that [the OpenAI API
    (is not expensive for experimentation but it) is not free](https://openai.com/pricing).
    To obtain an OpenAI API Key, you need an OpenAI account and then “Create new secret
    key” under [API keys](https://platform.openai.com/account/api-keys).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数可用的 LangChain 教程使用 OpenAI，但请注意 [OpenAI API（虽然实验使用不贵，但）并非免费](https://openai.com/pricing)。要获取
    OpenAI API 密钥，你需要一个 OpenAI 账户，然后在 [API 密钥](https://platform.openai.com/account/api-keys)
    下“创建新秘密密钥”。
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Open-source models** are usually smaller models with lower capabilities than
    proprietary models, but they are more cost-effective than proprietary ones. Examples
    of open-source models are:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**开源模型** 通常是比专有模型小的模型，能力较低，但它们比专有模型更具成本效益。开源模型的例子包括：'
- en: '[BLOOM](https://huggingface.co/bigscience/bloom) by BigScience'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BLOOM](https://huggingface.co/bigscience/bloom) 由 BigScience 提供'
- en: '[LLaMA](https://huggingface.co/docs/transformers/main/en/model_doc/llama) by
    Meta AI'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LLaMA](https://huggingface.co/docs/transformers/main/en/model_doc/llama) 由
    Meta AI 提供'
- en: '[Flan-T5](https://huggingface.co/google/flan-t5-xl) by Google'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Flan-T5](https://huggingface.co/google/flan-t5-xl) 由 Google 提供'
- en: '[GPT-J](https://huggingface.co/EleutherAI/gpt-j-6b) by Eleuther AI'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT-J](https://huggingface.co/EleutherAI/gpt-j-6b) 由 Eleuther AI 提供'
- en: Many open-source models are organized and hosted on [Hugging Face](https://huggingface.co/)
    as a community hub. To obtain a Hugging Face API Key, you need a Hugging Face
    account and create a “New token” under [Access Tokens](https://huggingface.co/settings/tokens).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 许多开源模型被组织和托管在 [Hugging Face](https://huggingface.co/) 作为一个社区中心。要获取 Hugging Face
    API 密钥，你需要一个 Hugging Face 账户，并在 [访问令牌](https://huggingface.co/settings/tokens)
    下创建一个“新令牌”。
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can use [Hugging Face](https://huggingface.co/) for free for open-source
    LLMs, but you will be limited to smaller LLMs with less performance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以免费使用 [Hugging Face](https://huggingface.co/) 来访问开源 LLM，但你将被限制使用性能较低的小型 LLM。
- en: '***A personal note —*** *Let’s be honest here for a second: Of course, you
    can experiment with open-source foundation models here. I tried to make this tutorial
    only with open-source models hosted on Hugging Face available with a regular account
    (google/flan-t5-xl and sentence-transformers/all-MiniLM-L6-v2). It works for most
    examples, but it is also a pain to get some examples to work. Finally, I pulled
    the trigger and set up a paid account for OpenAI as most examples for LangChain
    seem to be optimized for OpenAI’s API. Overall running a few experiments for this
    tutorial cost me about $1.*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '***个人备注——*** *我们诚实地说一句：当然，你可以在这里尝试开源基础模型。我试图只使用在 Hugging Face 上托管的开源模型（google/flan-t5-xl
    和 sentence-transformers/all-MiniLM-L6-v2）来制作这个教程，并使用普通账户进行操作。大多数示例都能正常工作，但也有一些示例很难让它们正常运行。最后，我决定购买一个
    OpenAI 的付费账户，因为 LangChain 的大多数示例似乎都为 OpenAI 的 API 进行了优化。总的来说，制作这个教程的几个实验花费了我大约
    1 美元。*'
- en: '**Vector Database (optional)** — If you want to use a specific vector database
    such as [Pinecone](https://www.pinecone.io/), [Weaviate](https://weaviate.io/),
    or [Milvus](https://milvus.io/), you need to register with them to obtain an API
    key and check their pricing. In this tutorial, we are using [Faiss](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/),
    which requires no sign-up.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**向量数据库（可选）** —— 如果你想使用特定的向量数据库，如 [Pinecone](https://www.pinecone.io/)、[Weaviate](https://weaviate.io/)
    或 [Milvus](https://milvus.io/)，你需要注册并获取一个 API 密钥，并查看它们的定价。在这个教程中，我们使用的是 [Faiss](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/)，它不需要注册。'
- en: '**Tools (optional)** — Depending on the [tools](https://python.langchain.com/en/latest/modules/agents/tools.html)
    you want the LLM to interact with such as OpenWeatherMap or SerpAPI, you may need
    to register with them to obtain an API key and check their pricing. In this tutorial,
    we only use tools requiring no API key.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**工具（可选）** — 根据你希望 LLM 与之交互的[工具](https://python.langchain.com/en/latest/modules/agents/tools.html)，如
    OpenWeatherMap 或 SerpAPI，你可能需要注册以获得 API 密钥，并查看它们的定价。在本教程中，我们仅使用不需要 API 密钥的工具。'
- en: What can you do with LangChain?
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你可以用 LangChain 做什么？
- en: The package provides a generic interface to many foundation models, enables
    prompt management, and acts as a central interface to other components like prompt
    templates, other LLMs, external data, and other tools via agents.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 该包提供了与许多基础模型的通用接口，支持提示管理，并作为其他组件的中央接口，如提示模板、其他 LLM、外部数据以及通过代理访问的其他工具。
- en: 'At the time of writing, LangChain (version 0.0.147) covers six modules:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，LangChain（版本 0.0.147）涵盖了六个模块：
- en: '[Models: Choosing from different LLMs and embedding models](#bd03)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[模型：从不同的 LLM 和嵌入模型中选择](#bd03)'
- en: '[Prompts: Managing LLM inputs](#b1a5)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提示：管理 LLM 输入](#b1a5)'
- en: '[Chains: Combining LLMs with other components](#fcac)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[链：将 LLM 与其他组件结合](#fcac)'
- en: '[Indexes: Accessing external data](#806f)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[索引：访问外部数据](#806f)'
- en: '[Memory: Remembering previous conversations](#3bc6)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[记忆：记住先前的对话](#3bc6)'
- en: '[Agents: Accessing other tools](#b842)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[代理：访问其他工具](#b842)'
- en: The code examples in the following sections are copied and modified from the
    [LangChain documentation](https://python.langchain.com/en/latest/index.html).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分中的代码示例是从[LangChain 文档](https://python.langchain.com/en/latest/index.html)中复制和修改的。
- en: 'Models: Choosing from different LLMs and embedding models'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型：从不同的 LLM 和嵌入模型中选择
- en: Currently, many different LLMs are emerging. LangChain offers integrations to
    a wide range of models and a streamlined interface to all of them.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，许多不同的 LLM 正在出现。LangChain 提供了与各种模型的集成，并为所有模型提供了简化的接口。
- en: 'LangChain differentiates between three types of models that differ in their
    inputs and outputs:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 区分三种类型的模型，它们在输入和输出上有所不同：
- en: '**LLMs** take a string as an input (prompt) and output a string (completion).'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLMs** 将一个字符串作为输入（提示），并输出一个字符串（完成）。'
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/6f7c12ec849f0405cc0b12cdc130e129.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f7c12ec849f0405cc0b12cdc130e129.png)'
- en: LLM models (Image by the author)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 模型（图片由作者提供）
- en: '**Chat models** are similar to LLMs. They take a list of chat messages as input
    and return a chat message.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聊天模型**类似于 LLM。它们将聊天消息列表作为输入，并返回一条聊天消息。'
- en: '**Text embedding models** take text input and return a list of floats (embeddings),
    which are the numerical representation of the input text. Embeddings help extract
    information from a text. This information can then be later used, e.g., for calculating
    similarities between texts (e.g., movie summaries).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本嵌入模型**将文本输入并返回一个浮点数列表（嵌入），这些嵌入是输入文本的数值表示。嵌入帮助从文本中提取信息。这些信息可以在后续使用，例如，计算文本之间的相似性（例如电影摘要）。'
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/0759e970e3b04f43b057231e5e2aec43.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0759e970e3b04f43b057231e5e2aec43.png)'
- en: Text embedding models (Image by the author)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 文本嵌入模型（图片由作者提供）
- en: 'Prompts: Managing LLM inputs'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示：管理 LLM 输入
- en: LLMs have weird APIs. Although inputting prompts to LLMs in natural language
    should feel intuitive, it takes quite a bit of tweaking of the prompt until you
    get the desired output from an LLM. This process is called *prompt engineering*.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的 API 很奇怪。虽然用自然语言输入提示应该是直观的，但需要对提示进行相当多的调整，才能从 LLM 中获得期望的输出。这个过程被称为*提示工程*。
- en: Once you have a good prompt, you may want to use it as a template for other
    purposes. Thus, LangChain provides you with so-called `PromptTemplates`, which
    help you construct prompts from multiple components.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你拥有了一个好的提示，你可能希望将其用作其他用途的模板。因此，LangChain 为你提供了所谓的`PromptTemplates`，这些模板帮助你从多个组件构建提示。
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The above prompt can be viewed as a **zero-shot problem setting**, where you
    hope the LLM was trained on enough relevant data to provide a satisfactory response.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 上述提示可以视为**零-shot 问题设置**，你希望 LLM 在足够相关的数据上进行训练，从而提供令人满意的响应。
- en: Another trick to improve the LLM’s output is to add a few examples in the prompt
    and make it a **few-shot problem setting**.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 提高 LLM 输出的另一个技巧是添加几个示例到提示中，将其设为**少量示例问题设置**。
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The above code will generate a prompt template and compose the following prompt
    based on the provided examples and input:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将生成一个提示模板，并根据提供的示例和输入组成以下提示：
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Chains: Combining LLMs with other components'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 链：将 LLM 与其他组件结合
- en: 'Chaining in LangChain simply describes the process of combining LLMs with other
    components to create an application. Some examples are:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LangChain 中，链式操作简单地描述了将 LLM 与其他组件结合以创建应用程序的过程。一些示例包括：
- en: Combining LLMs with prompt templates (see this section)
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 LLM 与提示模板结合（参见本节）
- en: Combining multiple LLMs sequentially by taking the first LLM’s output as the
    input for the second LLM (see this section)
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将第一个 LLM 的输出作为第二个 LLM 的输入来依次结合多个 LLM（参见本节）
- en: Combining LLMs with external data, e.g., for question answering (see [Indexes](#806f))
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 LLM 与外部数据结合，例如，用于回答问题（参见 [索引](#806f)）
- en: Combining LLMs with long-term memory, e.g., for chat history (see [Memory](#3bc6))
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 LLM 与长期记忆结合，例如，用于聊天记录（参见 [记忆](#3bc6)）
- en: 'In the previous section, we created a prompt template. When we want to use
    it with our LLM, we can use an `LLMChain` as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们创建了一个提示模板。当我们想将其与 LLM 一起使用时，可以按如下方式使用 `LLMChain`：
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If we want to use the output of this first LLM as the input for a second LLM,
    we can use a `SimpleSequentialChain`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想将第一个 LLM 的输出作为第二个 LLM 的输入，可以使用 `SimpleSequentialChain`：
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/a19959a0c2a5d4f0c48c9aee7cb979a4.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a19959a0c2a5d4f0c48c9aee7cb979a4.png)'
- en: Output of a SimpleSequentialChain using PromptTemplates and LLMs in LangChain
    (Screenshot by the author)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PromptTemplates 和 LLMs 在 LangChain 中的 SimpleSequentialChain 的输出（作者截图）
- en: 'Indexes: Accessing external data'
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 索引：访问外部数据
- en: One limitation of LLMs is their lack of contextual information (e.g., access
    to some specific documents or emails). You can combat this by giving LLMs access
    to the specific external data.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的一个限制是缺乏上下文信息（例如，访问某些特定文档或电子邮件）。你可以通过让 LLM 访问特定的外部数据来解决这个问题。
- en: For this, you first need to load the external data with a document loader. LangChain
    provides a variety of loaders for different types of documents ranging from PDFs
    and emails to websites and YouTube videos.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，你首先需要用文档加载器加载外部数据。LangChain 提供了各种加载器，用于不同类型的文档，从 PDFs 和电子邮件到网站和 YouTube 视频。
- en: Let’s load some external data from a YouTube video. You can refer to the official
    documentation if you want to [load a large text document and split it with a](https://python.langchain.com/en/latest/modules/indexes/getting_started.html)
    [Text Splitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 YouTube 视频中加载一些外部数据。如果你想 [加载一个大型文本文档并使用](https://python.langchain.com/en/latest/modules/indexes/getting_started.html)
    [文本分割器](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)
    将其拆分，可以参考官方文档。
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now that you have your external data ready to go as `documents` , you can index
    it with a **text embedding model** (see [Models](#bd03)) in a vector database
    — a **VectorStore.** Popular vector databases include [Pinecone](https://www.pinecone.io/),
    [Weaviate](https://weaviate.io/), and [Milvus](https://milvus.io/). In this article,
    we are using Faiss because it doesn’t require an API key.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经准备好将外部数据作为 `documents`，你可以使用 **文本嵌入模型**（参见 [模型](#bd03)）在向量数据库中进行索引——一个
    **VectorStore**。流行的向量数据库包括 [Pinecone](https://www.pinecone.io/)、[Weaviate](https://weaviate.io/)
    和 [Milvus](https://milvus.io/)。在本文中，我们使用 Faiss，因为它不需要 API 密钥。
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Your document (in this case, a video) is now stored as embeddings in a vector
    store.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你的文档（在这种情况下是视频）现在作为嵌入存储在向量存储中。
- en: Now you can do a variety of things with this external data. Let’s use it for
    a question-answering task with an information **retriever:**
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以使用这些外部数据做各种事情。让我们用它来进行一个信息**检索器**的问答任务：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/e9499bbce4053fff99174903e5578c71.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9499bbce4053fff99174903e5578c71.png)'
- en: Output of the RetrievalQA (Screenshot by the author)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 检索问答的输出（作者截图）
- en: Wait a minute — Did you just get rickrolled? Yes, you did.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 等一下——你刚刚被恶搞了？是的，你确实被恶搞了。
- en: 'Memory: Remembering previous conversations'
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 记忆：记住先前的对话
- en: For applications like chatbots, it is essential that they can remember previous
    conversations. But by default, LLMs don’t have any long-term memory unless you
    input the chat history.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于聊天机器人等应用程序来说，能够记住先前的对话至关重要。但默认情况下，LLM 不具有任何长期记忆，除非你输入聊天记录。
- en: '![](../Images/13683d095100fa5d71506adbe571435a.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13683d095100fa5d71506adbe571435a.png)'
- en: Chat with and without conversational memory (Image by the author made with [ifaketextmessage.com](https://ifaketextmessage.com/),
    inspired by [Pinecone](https://www.pinecone.io/learn/langchain-conversational-memory))
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 带有和不带有对话记忆的聊天（图片由 [ifaketextmessage.com](https://ifaketextmessage.com/) 制作，灵感来自
    [Pinecone](https://www.pinecone.io/learn/langchain-conversational-memory)）
- en: 'LangChain solves this problem by providing several different options for dealing
    with chat history :'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain通过提供几种处理聊天记录的不同选项来解决这个问题：
- en: keep all conversations,
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保留所有对话，
- en: keep the latest k conversations,
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保留最近的k个对话，
- en: summarize the conversation.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结对话。
- en: In this example, we will use a `ConversationChain` to give this application
    conversational memory.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用`ConversationChain`来为这个应用程序提供对话记忆。
- en: '[PRE14]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This will result in the right-hand conversation in the above image. Without
    the `ConversationChain` to keep a conversational memory, the conversation will
    look like the one on the left-hand side in the above image.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致上图中右侧的对话。如果没有`ConversationChain`来保持对话记忆，对话将会像上图中左侧的那样。
- en: 'Agents: Accessing other tools'
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理：访问其他工具
- en: 'Despite being quite powerful, LLMs have some limitations: they lack contextual
    information (e.g., access to specific knowledge not contained in training data),
    they can become outdated quickly (e.g., GPT-4 was [trained on data before September
    2021](https://openai.com/research/gpt-4)), and they are bad at math.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLM相当强大，但它们有一些局限性：缺乏上下文信息（例如，无法访问训练数据中未包含的特定知识），可能会很快过时（例如，GPT-4是[在2021年9月之前的数据上训练的](https://openai.com/research/gpt-4)），并且在数学方面表现不佳。
- en: LLMs are bad at math
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: LLM在数学方面表现不好
- en: Because LLMs can hallucinate on tasks they can’t accomplish on their own, we
    need to give them access to supplementary **tools** such as search (e.g., [Google
    Search](https://python.langchain.com/en/latest/modules/agents/tools/examples/google_search.html)),
    calculators (e.g., [Python REPL](https://python.langchain.com/en/latest/modules/agents/tools/examples/python.html)
    or [Wolfram Alpha](https://python.langchain.com/en/latest/modules/agents/tools/examples/wolfram_alpha.html)),
    and lookups (e.g., [Wikipedia](https://python.langchain.com/en/latest/modules/agents/tools/examples/wikipedia.html)).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因为LLM在无法自行完成任务时可能会产生幻觉，我们需要提供额外的**工具**，如搜索（例如，[Google搜索](https://python.langchain.com/en/latest/modules/agents/tools/examples/google_search.html)）、计算器（例如，[Python
    REPL](https://python.langchain.com/en/latest/modules/agents/tools/examples/python.html)
    或 [Wolfram Alpha](https://python.langchain.com/en/latest/modules/agents/tools/examples/wolfram_alpha.html)），以及查找（例如，[维基百科](https://python.langchain.com/en/latest/modules/agents/tools/examples/wikipedia.html)）。
- en: Additionally, we need agents that make decisions about which tools to use to
    accomplish a task based on the LLM’s output.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们需要代理程序来根据LLM的输出决定使用哪些工具来完成任务。
- en: '*Note that some LLMs such as* `[*google/flan-t5-xl*](https://github.com/hwchase17/langchain/issues/1358)`
    [*won’t be suitable for the following examples as they do not follow the*](https://github.com/hwchase17/langchain/issues/1358)
    `[*conversational-react-description*](https://github.com/hwchase17/langchain/issues/1358)`
    [*template.*](https://github.com/hwchase17/langchain/issues/1358) *For me, this
    was the point where I set up a paid account on OpenAI and switched to the OpenAI
    API.*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*请注意，一些LLM例如* `[*google/flan-t5-xl*](https://github.com/hwchase17/langchain/issues/1358)`
    [*可能不适用于以下示例，因为它们不符合*](https://github.com/hwchase17/langchain/issues/1358) `[*conversational-react-description*](https://github.com/hwchase17/langchain/issues/1358)`
    [*模板。*](https://github.com/hwchase17/langchain/issues/1358) *对我来说，这是我在OpenAI上设置付费账户并切换到OpenAI
    API的时刻。*'
- en: Below is an example in which the agent first looks up the date of Barack Obama’s
    birth with Wikipedia and then calculates his age in 2022 with a calculator.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例，其中代理首先用维基百科查找巴拉克·奥巴马的出生日期，然后用计算器计算他在2022年的年龄。
- en: '[PRE15]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/4b0008eae6eb77df7210741f368d43b4.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b0008eae6eb77df7210741f368d43b4.png)'
- en: Output of the LLM agent (Screenshot by the author)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理的输出（作者截图）
- en: Summary
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Just a few months ago, we all (or at least most of us) were impressed by ChatGPT's
    capabilities. Now, new developer tools like LangChain enable us to build similarly
    impressive prototypes on our laptops within a few hours — these are some truly
    exciting times!
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 就在几个月前，我们所有人（或者至少大多数人）都对ChatGPT的能力印象深刻。现在，像LangChain这样的新开发工具使我们能够在几小时内在笔记本电脑上构建同样令人印象深刻的原型——这些确实是令人兴奋的时代！
- en: LangChain is an open-source Python library that enables anyone who can write
    code to build LLM-powered applications. The package provides a generic interface
    to many foundation models, enables prompt management, and acts as a central interface
    to other components like prompt templates, other LLMs, external data, and other
    tools via agents — at the time of writing.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain是一个开源的Python库，使任何能够编写代码的人都可以构建LLM驱动的应用程序。该包提供了对许多基础模型的通用接口，支持提示管理，并作为其他组件（如提示模板、其他LLM、外部数据和通过代理的其他工具）的中央接口——在撰写时。
- en: The library offers many more features than mentioned in this article. With the
    current speed of developments, this article could also be potentially outdated
    in a month.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图书馆提供的功能远超本文提到的。随着发展速度的加快，本文在一个月内也可能会过时。
- en: While working on this article, I noticed that the library and documentation
    are centered around OpenAI’s API. Although many examples work with the open-source
    foundation model `[google/flan-t5-xl](https://github.com/hwchase17/langchain/issues/1358)`,
    I switched to the OpenAI API in between. Despite not being free, experimenting
    with the OpenAI API in this article cost me only about $1.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，我注意到图书馆和文档集中在OpenAI的API上。虽然许多示例使用的是开源基础模型`[google/flan-t5-xl](https://github.com/hwchase17/langchain/issues/1358)`，但我中途切换到了OpenAI
    API。尽管不是免费的，但在本文中使用OpenAI API的实验仅花费了大约$1。
- en: Enjoyed This Story?
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 享受这个故事了吗？
- en: '[*Subscribe for free*](https://medium.com/subscribe/@iamleonie) *to get notified
    when I publish a new story.*'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[*免费订阅*](https://medium.com/subscribe/@iamleonie) *以便在我发布新故事时获得通知。*'
- en: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----95fc8898732c--------------------------------)
    [## Get an email whenever Leonie Monigatti publishes.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----95fc8898732c--------------------------------)
    [## 每当Leonie Monigatti发布新内容时，你将收到一封电子邮件。'
- en: Get an email whenever Leonie Monigatti publishes. By signing up, you will create
    a Medium account if you don’t already…
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每当Leonie Monigatti发布新内容时，你会收到一封电子邮件。通过注册，你将创建一个Medium账户，如果你还没有的话……
- en: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----95fc8898732c--------------------------------)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----95fc8898732c--------------------------------)
- en: '*Find me on* [*LinkedIn*](https://www.linkedin.com/in/804250ab/),[*Twitter*](https://twitter.com/helloiamleonie)*,
    and* [*Kaggle*](https://www.kaggle.com/iamleonie)*!*'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*在* [*LinkedIn*](https://www.linkedin.com/in/804250ab/)，[*Twitter*](https://twitter.com/helloiamleonie)*，以及*
    [*Kaggle*](https://www.kaggle.com/iamleonie)*上找到我！*'
- en: References
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[1] Harrison Chase (2023). [LangChain documentation](https://python.langchain.com/en/latest/index.html)
    (accessed April 23rd, 2023)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Harrison Chase (2023). [LangChain文档](https://python.langchain.com/en/latest/index.html)（访问日期：2023年4月23日）'
