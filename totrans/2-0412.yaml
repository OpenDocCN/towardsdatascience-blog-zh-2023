- en: 'Bridging Domains: Infusing Financial, Privacy, and Software Best Practices
    into ML Risk Management'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/bridging-domains-infusing-financial-privacy-and-software-best-practices-into-ml-risk-management-3de1fa1e6dd2](https://towardsdatascience.com/bridging-domains-infusing-financial-privacy-and-software-best-practices-into-ml-risk-management-3de1fa1e6dd2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Responsible AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Understanding strategies that go beyond traditional Model Risk Management*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://pandeyparul.medium.com/?source=post_page-----3de1fa1e6dd2--------------------------------)[![Parul
    Pandey](../Images/760b72a4feacfad6fc4224835c2e1f19.png)](https://pandeyparul.medium.com/?source=post_page-----3de1fa1e6dd2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3de1fa1e6dd2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3de1fa1e6dd2--------------------------------)
    [Parul Pandey](https://pandeyparul.medium.com/?source=post_page-----3de1fa1e6dd2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3de1fa1e6dd2--------------------------------)
    ¬∑10 min read¬∑Oct 10, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/872f9cbcc173a12dcd6d51d3b8c3db1c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author | Icons: [Flaticon](https://www.flaticon.com/) (Free for Private
    and Commercial use)'
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúAviation laws were written in blood. Let‚Äôs not reproduce that methodology with
    AI‚Äù ‚Äî [Sim√©on Campos](https://oecd.ai/en/community/simeon-campos)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In 2018, Bloomberg‚Äôs story "[***Zillow's Algorithm-Fueled Buying Spree Doomed
    Its Home-Flipping Experiment***](https://www.bloomberg.com/news/articles/2021-11-08/zillow-z-home-flipping-experiment-doomed-by-tech-algorithms)"
    made quite a headline. It outlined Zillow's daring entry into the **iBuying**
    world, betting on its ML-powered **Zestimate** algorithm to revolutionize home
    flipping for profit. Despite a carefully structured start, incorporating local
    real estate experts to authenticate the algorithm's pricing, Zillow shifted to
    a fully algorithmic approach in the quest for faster offers. This move, however,
    did not pay off.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac73695203f37b309a79ebbe5434f2e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Tierra Mallorca](https://unsplash.com/@tierramallorca?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The Zestimate struggled to adapt to the swift inflation in the 2021 real estate
    market, prompting Zillow to take action to enhance the appeal of its offers. The
    company embarked on an ambitious buying spree, reportedly acquiring as many as
    10,000 homes per quarter. However, the human workforce struggled to keep up with
    the sheer scale and speed of these acquisitions, a challenge exacerbated by the
    concurrent outbreak of the pandemic. In the face of mounting difficulties, including
    a backlog of unsold properties, Zillow decided to halt its offers in October 2021\.
    Subsequent months witnessed homes being resold at a loss, leading to a substantial
    inventory write-down exceeding $500 million.
  prefs: []
  type: TYPE_NORMAL
- en: In *addition to the huge monetary loss of its failed venture, Zillow announced
    that it would lay off about 2,000 employees ‚Äî a full quarter of the company.*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We initiate our discussion with a rather unfortunate incident, as the fall of
    Zillow's iBuying venture is embedded within a complex framework of causes. Although
    it's impossible to extricate this incident from the global pandemic of 2020 that
    disrupted the housing market, it certainly paves the way for a rich analysis.
    In this article, we'll use this as an example and shine a light on how the principles
    of governance and risk management discussed in our series could possibly avert
    such unfortunate debacles in the future.
  prefs: []
  type: TYPE_NORMAL
- en: '***Before you read further***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before proceeding, know that this is the third article in our AI Risk Management
    series. It‚Äôs recommended to read the first two articles for a complete understanding.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ‚Ä¢ The first article unfurls the **Cultural Competencies for Machine Learning
    Risk Management**, exploring the human dimensions required to navigate this intricate
    domain.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](/cultural-competencies-for-machine-learning-risk-management-c38616c2ccdf?source=post_page-----3de1fa1e6dd2--------------------------------)
    [## Cultural Competencies for Machine Learning Risk Management'
  prefs: []
  type: TYPE_NORMAL
- en: An organization's culture is an essential aspect of responsible AI.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/cultural-competencies-for-machine-learning-risk-management-c38616c2ccdf?source=post_page-----3de1fa1e6dd2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '‚Ä¢ The second article pivots the focus to another vital element within the context
    of ML systems: **Organizational Processes**. Embark on this enlightening journey
    with us for a robust grasp on managing the intertwined realms of AI and risk management.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](/organizational-processes-for-machine-learning-risk-management-14f4444dd07f?source=post_page-----3de1fa1e6dd2--------------------------------)
    [## Organizational Processes for Machine Learning Risk Management'
  prefs: []
  type: TYPE_NORMAL
- en: Organizational processes are a key nontechnical determinant of reliability in
    ML systems.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/organizational-processes-for-machine-learning-risk-management-14f4444dd07f?source=post_page-----3de1fa1e6dd2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*Going beyond Model Risk Management*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the [previous article](/organizational-processes-for-machine-learning-risk-management-14f4444dd07f),
    we discussed in detail how **Machine Learning Risk Management (MRM)** constitutes
    a comprehensive framework, along with a series of procedures aimed at identifying,
    assessing, mitigating, and monitoring risks associated with the development, deployment,
    and operation of machine learning systems. In this part, we will explore various
    strategies and practices beyond the realm of traditional Model Risk Management
    that prove to be exceptionally beneficial, especially concerning ML safety. We
    will commence by discussing the AI incident response.
  prefs: []
  type: TYPE_NORMAL
- en: üìùAI Incident Response plan
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Zillow fiasco underscores an AI Incident of Failure, illustrating how a
    well-crafted algorithm could not keep pace with the fast-changing real estate
    market, leading to significant financial and reputational damage. Despite the
    best model training and validation tests, as noted even in the [SR-11 guidelines](https://www.federalreserve.gov/supervisionreg/srletters/sr1107a1.pdf),
    eliminating model risk is impossible, highlighting the pressing need for a solid
    Incident Response plan.
  prefs: []
  type: TYPE_NORMAL
- en: An **AI incident plan** is a preplanned strategy for quickly and effectively
    addressing AI issues, helping organizations swiftly identify, contain, and eliminate
    AI incidents and prevent costly or hazardous situations, especially crucial for
    smaller or new organizations. It is a well-regarded practice in computer security,
    with organizations like [NIST](https://www.nist.gov/) and [SANS](https://www.sans.org/in_en/)
    underscoring its importance in managing ML and AI complexities. Like computer
    incident response, the AI incident plan operates in six clear phases, each vital
    for reducing risk in AI.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f231f528802bc60bd7a6c8f8deab3a9a.png)'
  prefs: []
  type: TYPE_IMG
- en: Six phases of AI incident plan| Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Phase 1: Preparation**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To effectively prepare for AI incidents, organizations should define the incident
    parameters, allocate response budgets, develop communication plans, and implement
    technical safeguards. Practicing scenarios through tabletop exercises with key
    personnel enhances readiness.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7ca2754bfa19e454b918ddb6a253493.png)'
  prefs: []
  type: TYPE_IMG
- en: Starter questions for an AI Incident Preparation phase | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Phase 2: Identification**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identification involves detecting system failures, attacks, or abuses. It combines
    general security methods with specialized AI monitoring, like detecting concept
    drift or algorithmic discrimination. Once an issue is identified, relevant stakeholders,
    including management, are alerted.
  prefs: []
  type: TYPE_NORMAL
- en: '**Phase 3: Containment**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containment refers to mitigating the immediate harm caused by an incident, with
    the goal of reducing the initial damage. Incidents can have a tendency to spread
    beyond their point of origin, impacting other aspects of a business and its customers.
    The approach to addressing such issues may differ depending on their root cause,
    whether it is an external attack, an internal error, or misuse of the AI system.
    When deemed necessary, initiating communication with the public during the containment
    phase is advisable.
  prefs: []
  type: TYPE_NORMAL
- en: '**Phase 4: Eradication**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eradication means fixing the affected systems to stop the problem. This could
    be by blocking attacked systems to prevent further damage or shutting down a faulty
    AI system and temporarily using a trusted, simpler system instead. After eradication,
    the incident should not cause any more harm.
  prefs: []
  type: TYPE_NORMAL
- en: '**Phase 5: Recovery**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recovery involves fixing affected systems, preventing future issues, and possibly
    reviewing or improving technical procedures, especially if a mistake or malicious
    act caused the problem.
  prefs: []
  type: TYPE_NORMAL
- en: '**Phase 6: Lessons Learned**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lessons learned mean making changes or betterments to how we respond to AI incidents
    based on what worked well and what didn't within the current problem. These improvements
    can be related to either the process or the technology used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Learnings from the Zillow iBuying Incident: Insights for AI Incident Response'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After reviewing our AI incident response plan, let‚Äôs dive back into the Zillow
    iBuying saga. What insights can we gather from this chapter about the Zillow iBuying
    situation? Based on the public reports on this topic, it becomes apparent that
    there were potential red flags üö©. These include Zillow''s lack of human oversight,
    insufficient assessment of financial risks, and the absence of appropriate governance
    structures. While the specific answers regarding Zillow remain uncertain, it underscores
    the significance of extracting valuable lessons from this case to enhance our
    readiness and response to AI-related challenges within our own organizations,
    including:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Lesson 1: Validate with domain experts.***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Lesson 2: Anticipate failure modes.***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Lesson 3: Governance is crucial.***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Lesson 4: AI incidents can scale rapidly.***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Lesson 5: Emerging technologies always entail risks.***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional Practices for Enhanced Risk Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Besides the AI incident response discussed above, practices adopted from financial
    audits, data privacy, software development best practices, and IT security bring
    significant value to the field.
  prefs: []
  type: TYPE_NORMAL
- en: üîç*Model audits and assessments*
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model audits are a formal evaluation exercise focusing on an ML system, ensuring
    compliance with specific policies, regulations, or laws. These formal evaluations,
    usually conducted by independent third parties, prioritize transparency and exhaustive
    testing. Model assessment is a similar but more informal check, possibly done
    by internal or external groups, checking for various issues such as bias, safety,
    data privacy harms, and security vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a deeper dive into model audits and assessments, two papers, [*Algorithmic
    Bias and Risk Assessments: Lessons from Practice*](https://link.springer.com/content/pdf/10.1007/s44206-022-00017-z.pdf)
    and [*Closing the AI Accountability Gap: Defining an End-to-End Framework for
    Internal Algorithmic Auditing*](https://dl.acm.org/doi/abs/10.1145/3351095.3372873)
    provide excellent insights and frameworks for conducting these audits and assessments.'
  prefs: []
  type: TYPE_NORMAL
- en: üìà*Impact assessments*
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Impact assessments are gaining traction in ML policies and proposed laws for
    anticipating and documenting potential system challenges. These assessments make
    it easier for AI designers and operators to understand and be responsible for
    the possible problems their systems could cause. However, they are just a beginning
    step. They should be done regularly and taken into account along with other factors
    to get a full picture of the risks. It‚ÄôsIt'so important for them to be done by
    people who are not part of the ML teams being assessed to avoid any bias and ensure
    a thorough check.
  prefs: []
  type: TYPE_NORMAL
- en: While impact assessments play a pivotal role in risk management and governance
    strategies, their execution by independent professionals and integration with
    other risk factors is essential for overall efficacy.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ‚öñÔ∏è*Appeal, override, and opt-out*
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Have you seen the [**Report inappropriate predictions**](https://support.google.com/websearch/answer/106230?hl=en&co=GENIE.Platform%3DDesktop)
    function in Google's search bar? It‚Äôs a basic way for users to point out problems.
    This feature allows users to challenge or correct ML systems decisions. This idea,
    also known as actionable recourse or redress, can vary in complexity. Another
    approach is the **opt-out** option, letting users skip automated processing. Both
    these options, recognized by many data privacy and US consumer finance laws, are
    crucial in defending consumer rights against automated ML errors. However, many
    ML systems still lack these features due to the planning and resources needed
    to integrate them from the onset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/17b6daf65c5a8c80ee108e2a676d9c8a.png)'
  prefs: []
  type: TYPE_IMG
- en: Reporting inappropriate predictions through Google | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: üë©‚Äçüíªüë®‚Äçüíª*Pair and double programming*
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine learning algorithms can be complex and unpredictable, making it hard
    to ensure they work correctly. Some top ML organizations double-check their work
    using two main methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pair Programming**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‚Äî Two experts code the same algorithm separately.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ‚Äî They then collaborate to sort out any differences and ensure both versions
    work the same way.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Interestingly, Large Language Models (LLMs) are now being incorporated into
    pair programming. A recent course titled [Pair Programming with a Large Language
    Model](https://www.deeplearning.ai/short-courses/pair-programming-llm/?utm_content=teaser&utm_source=youtube&utm_medium=video&utm_campaign=palm-launch)
    delves into the nuances of collaborating with LLMs in real-time coding scenarios.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Double Programming**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‚Äî One person writes the same algorithm twice, each time using a different programming
    language.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ‚Äî They then compare and reconcile any differences between the two versions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Both methods help in finding and fixing bugs early, making sure the algorithm
    is solid before being used in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: üîí*Security permissions for model deployment*
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a concept in IT security called the **least privilege**, which emphasizes
    that no system user should have excess permissions. Despite its significance,
    this is often neglected in ML systems, potentially causing safety and performance
    issues. It is a recognized practice that diverse roles, like product managers
    or executives, should make the final call on software release to avoid bias and
    ensure a thorough assessment.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cbda9c7e77ee2c7dda43c3115517ed48.png)'
  prefs: []
  type: TYPE_IMG
- en: '**The principle of least privilege demonstrated by privilege rings for the**
    [**Intel x86**](https://en.wikipedia.org/wiki/Intel_x86)| [By Hertzsprung at English
    Wikipedia, CC BY-SA 3.0,](https://commons.wikimedia.org/w/index.php?curid=8950144)'
  prefs: []
  type: TYPE_NORMAL
- en: During development sprints, it is essential for data scientists and engineers
    to have full control over their environments. However, as significant releases
    or reviews approach, the IT permissions to make changes to user-facing products
    should shift to other roles within the organization. This transition of control
    serves as a checkpoint, ensuring that unapproved or faulty code does not get deployed,
    thereby enhancing the security and reliability of the system.
  prefs: []
  type: TYPE_NORMAL
- en: üí∞Bug Bounties
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bug bounties are rewards given by organizations to people who find problems
    in their software, including machine learning systems. They are not just for finding
    security issues but also for problems related to things like safety, privacy,
    and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: By offering money or other rewards, organizations encourage people to give feedback
    and find issues in their ML systems, making them more reliable and secure. If
    organizations are worried about making their bug bounties public, they can hold
    internal events where different teams search for problems in their ML systems.
    The key is to provide good incentives to get the best results.
  prefs: []
  type: TYPE_NORMAL
- en: '*Through bug bounties, we use monetary rewards to incentivize community feedback
    in a standardized process.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Many companies have launched Bug Bounty programs to detect and rectify vulnerabilities
    in their systems. Here are a few examples:'
  prefs: []
  type: TYPE_NORMAL
- en: In 2021, **Twitter** (Now known as **X**) announced its inaugural [**algorithmic
    bias bounty challenge**](https://hackerone.com/twitter-algorithmic-bias?type=team)
    to explore potential biases in its image-cropping algorithm. This algorithm utilized
    an XAI technique known as a saliency map to determine which part of a user-uploaded
    image was most engaging.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/924aa830b27b034d6fb91dfa63fabc80.png)'
  prefs: []
  type: TYPE_IMG
- en: '[*Image Cropping Algorithm*](/what-you-see-is-what-youll-get-twitter-s-new-strategy-for-displaying-images-on-the-timeline-3ddc2040c728)
    *used by Twitter (Now Known as X)* for displaying images on feed. The algorithm
    was later dropped| Image by Author'
  prefs: []
  type: TYPE_NORMAL
- en: Some users observed that the ML-based image cropper seemed to favor images of
    white individuals and disproportionately zoomed in on areas like women‚Äôs chests
    and legs, suggesting a **male gaze** bias. Moreover, there was no mechanism for
    users to alter the automated cropping when such issues were highlighted. The aim
    of the challenge was to identify the potential harm such an algorithm might introduce.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/what-you-see-is-what-youll-get-twitter-s-new-strategy-for-displaying-images-on-the-timeline-3ddc2040c728?source=post_page-----3de1fa1e6dd2--------------------------------)
    [## What you see is what you''ll get ‚Äî Twitter‚Äôs new strategy for displaying images
    on the timeline'
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing Twitter‚Äôs paper on their Image Cropping Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/what-you-see-is-what-youll-get-twitter-s-new-strategy-for-displaying-images-on-the-timeline-3ddc2040c728?source=post_page-----3de1fa1e6dd2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 2\. On April 11, 2023, [**OpenAI announced a bug bounty program**](https://openai.com/blog/bug-bounty-program),
    inviting the security research community to participate. Rewards for findings
    range from $200 for low-severity issues to up to $20,000 for exceptional discoveries.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. **Meta** has a history of running bug bounty programs for its platforms.
    However, when they introduced **LLaMA 2** ‚Äî their open-source large language model
    in February 2023, they also released a Responsible Use Guide. This guide included
    options for [reporting bugs and security issues](https://www.facebook.com/whitehat/info).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article emphasizes the importance of governance, incident response, and
    expert validation in responsible AI development. As we explore strategies that
    go beyond the usual Model Risk Management, including AI incident response plans
    and borrowing practices from financial audits, data privacy, software development,
    and IT security, it becomes clear that a multifaceted approach is crucial to address
    the ever-changing challenges of AI in a responsible and secure way. The lessons
    learned from Zillow‚Äôs experience remind us of the need for strong risk management
    in AI, leading us toward the creation of more reliable and ethical AI systems
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Machine Learning For High-Risk Application, Chapter 1 ‚Äî Beyond Model Risk
    Management](https://www.amazon.in/Machine-Learning-High-Risk-Applications-Responsible/dp/1098102436)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
