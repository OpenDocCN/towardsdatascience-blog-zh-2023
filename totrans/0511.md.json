["```py\n**Table of Contents:**\n\nWhat is a classification problem?\nDealing with class imbalance\nWhat a classification algorithm actually does\nAccuracy\nPrecision and recall\nF1-score\nThe confusion matrix\nSensitivity and specificity\nLog loss (cross-entropy)\nCategorical crossentropy\nAUC/ROC curve\nPrecision-recall curve\nBONUS: KDE and learning curves\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate data\nnum_samples = 1000\nX = np.random.rand(num_samples, 2) * 10 - 5\ny = np.zeros(num_samples)\ny[np.sum(X ** 2, axis=1) < 5] = 1\n\n# Plot data\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm')\nplt.xlabel('Feature')\nplt.ylabel('Label')\nplt.title('Binary Classification Dataset')\nplt.show()\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate data\nnum_samples = 1000\nX = np.random.rand(num_samples, 2) * 10 - 5\ny = np.zeros(num_samples, dtype=int)\ny[np.sum(X ** 2, axis=1) < 2.5] = 1\ny[np.logical_and(X[:, 0] > 2, np.abs(X[:, 1]) < 1)] = 2\ny[np.logical_and(X[:, 0] < -2, np.abs(X[:, 1]) < 1)] = 3\n\n# Plot data\nplt.scatter(X[y==0, 0], X[y==0, 1], c='blue', label='Class 1')\nplt.scatter(X[y==1, 0], X[y==1, 1], c='red', label='Class 2')\nplt.scatter(X[y==2, 0], X[y==2, 1], c='green', label='Class 3')\nplt.scatter(X[y==3, 0], X[y==3, 1], c='purple', label='Class 4')\nplt.xlabel('Feature')\nplt.ylabel('Label')\nplt.title('Multiclass Classification Dataset')\nplt.legend()\nplt.show()\n```", "```py\nNOTE:\n\nin the case of a binary classification,classes can be named as 0-1.\nBut they can also be named as 1-2\\. So, there is no convention that\ntells us we need to start from 0.\n\nThis is the same for the multi-class case. Classe can be named 0,1,2,3 as\nwell as 1,2,3,4.\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Class 1: blue\nmean1 = [0, 0]\ncov1 = [[1, 0], [0, 1]]\nnum_points1 = 7000\nX1 = np.random.multivariate_normal(mean1, cov1, num_points1)\n\n# Class 2: green\nmean2 = [3, 3]\ncov2 = [[0.5, 0], [0, 0.5]]\nnum_points2 = 2700\nX2 = np.random.multivariate_normal(mean2, cov2, num_points2)\n\n# Class 3: red\nmean3 = [-3, 3]\ncov3 = [[0.5, 0], [0, 0.5]]\nnum_points3 = 300\nX3 = np.random.multivariate_normal(mean3, cov3, num_points3)\n\n# Plot the data\nplt.scatter(X1[:, 0], X1[:, 1], color='blue', s=1, label='Class 1')\nplt.scatter(X2[:, 0], X2[:, 1], color='green', s=1, label='Class 2')\nplt.scatter(X3[:, 0], X3[:, 1], color='red', s=1, label='Class 3')\nplt.xlabel('Feature')\nplt.ylabel('Label')\nplt.title('Imbalanced Multiclass Classification Dataset')\nplt.legend()\nplt.show()\n```", "```py\nimport pandas as pd\nimport numpy as np\n\n# Create a list of labels\nlabels = ['1', '2', '3']\n\n# Create a list of features\nfeatures = ['feature_1', 'feature_2', 'feature_3']\n\n# Set the number of samples\nnum_samples = 1000\n\n# Create an empty Pandas DataFrame to store the data\ndata = pd.DataFrame()\n\n# Add the features to the DataFrame\nfor feature in features:\n    data[feature] = np.random.rand(num_samples)\n\n# Add the labels to the DataFrame\ndata['label'] = np.random.choice(labels, num_samples)\n```", "```py\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Plot histogram\nsns.histplot(data=data, x='label')\n\n# Write title and axis labels\nplt.title('CLASSES FREQUENCIES', fontsize=14) #plot TITLE\nplt.xlabel('Our labels (our classes)', fontsize=12) #x-axis label\nplt.ylabel('Frequencies of the three classes', fontsize=12) #y-axis label\n```", "```py\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a list of labels with class imbalance\nlabels = ['1'] * 500 + ['2'] * 450 + ['3'] * 50\n\n# Create a list of features\nfeatures = ['feature_1', 'feature_2', 'feature_3']\n\n# Shuffle the labels\nnp.random.shuffle(labels)\n\n# Create an empty Pandas DataFrame to store the data\ndata = pd.DataFrame()\n\n# Add the features to the DataFrame\nfor feature in features:\n    data[feature] = np.random.rand(len(labels))\n\n# Add the labels to the DataFrame\ndata['label'] = labels\n\n# Plot histogram\nsns.histplot(data=data, x='label')\n\n# Write title and axis labels\nplt.title('CLASSES FREQUENCIES', fontsize=14) #plot TITLE\nplt.xlabel('Our labels (our classes)', fontsize=12) #x-axis label\nplt.ylabel('Frequencies of the three classes', fontsize=12) #y-axis label\n```", "```py\nimport numpy as np\nimport pandas as pd\n\n# Random seed for reproducibility\nnp.random.seed(42)\n\n# Create samples\nn_samples = 1000\nfraud_percentage = 0.05 # Fraudolent percentage\n\n# Create classes \nX = np.random.rand(n_samples, 10)\ny = np.random.binomial(n=1, p=fraud_percentage, size=n_samples)\n\n# Create data frame\ndf = pd.DataFrame(X)\ndf['fraudulent'] = y\n```", "```py\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Split the dataset \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit logistic regression model to train set\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Calculate and print accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy:', accuracy)\n\n>>>\n    Accuracy: 0.95\n```", "```py\nfrom sklearn.metrics import precision_score\n\n# Calculate and print precision\nprecision = precision_score(y_test, y_pred)\n\nprint('Precision:', precision)\n\n>>>\n\n    Precision: 0.0\n```", "```py\nfrom sklearn.metrics import recall_score\n\n# Calculate and print recall\nrecall = recall_score(y_test, y_pred)\n\nprint('Recall:', recall)\n\n>>>\n\n    Recall: 0.0\n```", "```py\nfrom sklearn.metrics import f1_score\n\n# Calculate and print f1-score\nf1 = f1_score(y_test, y_pred)\n\nprint('F1 score:', f1)\n\n>>>\n\n    F1 score: 0.0\n```", "```py\nfrom sklearn.metrics import confusion_matrix\n\n# Calculate and print confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\nprint('Confusion matrix:\\n', cm)\n\n>>>\n\n    Confusion matrix:\n       [[285   0]\n       [ 15   0]]\n```", "```py\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n# Calculate confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Plot confusion matrix\ncmd = ConfusionMatrixDisplay(cm)\ncmd.plot()\n```", "```py\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Generate random data with 3 classes\nX, y = make_classification(n_samples=1000, n_classes=3, n_features=10,\n                            n_clusters_per_class=1, n_informative=5,\n                            class_sep=0.5, random_state=42)\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                    random_state=42)\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Train a logistic regression model on the training data\nclf = LogisticRegression(random_state=42).fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = clf.predict(X_test)\n\n# Calculate the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Display the confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n        display_labels=['Class 0', 'Class 1', 'Class 2'])\ndisp.plot()\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\n\n# Invoke logistic regression model\nmodel = LogisticRegression()\n# Fit the data on the train set\nmodel.fit(X_train, y_train) \n\n# Calculate probabilities\ny_prob = model.predict_proba(X_new)\n```", "```py\nfrom sklearn.metrics import log_loss\n\n# Invoke & print Log Loss\nlog_loss_score = log_loss(y_test, y_pred)\n\nprint(\"Log loss score:\", log_loss_score)\n\n>>>\n\n    Log loss score: 1.726938819745535\n```", "```py\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib.pyplot as plt\n\n# Generate a random binary classification dataset\nX, y = make_classification(n_samples=1000, n_features=10, n_classes=2,\n       random_state=42)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                  test_size=0.2, random_state=42)\n\n# Fit a logistic regression model on the training data\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict probabilities for the testing data\nprobs = model.predict_proba(X_test)\n\n# Compute the ROC curve and AUC score\nfpr, tpr, thresholds = roc_curve(y_test, probs[:, 1])\nauc_score = roc_auc_score(y_test, probs[:, 1])\n\n# Plot the ROC curve\nplt.plot(fpr, tpr, label='AUC = {:.2f}'.format(auc_score))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc='lower right')\nplt.show()\n```", "```py\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib.pyplot as plt\n\n# Generate a random binary classification dataset\nX, y = make_classification(n_samples=1000, n_features=10, n_classes=2,\n       random_state=42)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                  random_state=42)\n\n# Fit three different classifiers on the training data\nclf1 = LogisticRegression()\nclf2 = RandomForestClassifier(n_estimators=100)\nclf3 = KNeighborsClassifier(n_neighbors=5)\nclfs = [clf1, clf2, clf3]\n\n# Predict probabilities for the testing data\nplt.figure(figsize=(8,6))\nfor clf in clfs:\n    clf.fit(X_train, y_train)\n    probs = clf.predict_proba(X_test)\n    fpr, tpr, _ = roc_curve(y_test, probs[:,1])\n    auc_score = roc_auc_score(y_test, probs[:,1])\n    plt.plot(fpr, tpr, label='{} (AUC = {:.2f})'.format(clf.__class__.__name__,\n    auc_score))\n\n# Plot the ROC/AUC curves for each classifier\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve Comparison')\nplt.legend(loc=\"lower right\")\nplt.show()\n```", "```py\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\n# Generate a random binary classification dataset\nX, y = make_classification(n_samples=1000, n_features=10, n_classes=2,\n      random_state=42)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                  random_state=42)\n\n# Fit a logistic regression model on the training data\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\n\n# Predict probabilities for the testing data. Compute precision-recall curve\nprobs = clf.predict_proba(X_test)\nprecision, recall, thresholds = precision_recall_curve(y_test, probs[:,1])\n\n# Plot the precision-recall curve\nplt.plot(recall, precision)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.show()\n```", "```py\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\n# Generate a random binary classification dataset\nX, y = make_classification(n_samples=1000, n_features=10, n_classes=2,\n      random_state=42)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                  random_state=42)\n\n# Fit a logistic regression model on the training data\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\n\n# Predict probabilities for the testing data. Compute precision-recall curve\nprobs = clf.predict_proba(X_test)\nprecision, recall, thresholds = precision_recall_curve(y_test, probs[:,1])\n\n# Plot precision and recall as thresholds change\nplt.plot(thresholds, precision[:-1], label='Precision')\nplt.plot(thresholds, recall[:-1], label='Recall')\nplt.xlabel('Threshold')\nplt.ylabel('Precision & Recall')\nplt.legend()\nplt.title('Precision and Recall as Thresholds Change')\nplt.show()\n```"]