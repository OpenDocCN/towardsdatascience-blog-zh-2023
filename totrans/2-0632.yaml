- en: Data Augmentation Techniques for Audio Data in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/data-augmentation-techniques-for-audio-data-in-python-15505483c63c](https://towardsdatascience.com/data-augmentation-techniques-for-audio-data-in-python-15505483c63c)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to augment audio in waveform (time domain) and as spectrograms (frequency
    domain) with librosa, numpy, and PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@iamleonie?source=post_page-----15505483c63c--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----15505483c63c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----15505483c63c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----15505483c63c--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----15505483c63c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----15505483c63c--------------------------------)
    ·7 min read·Mar 28, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09de7dc6b33b345123ccb71209ef5b2b.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image drawn by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning models are data-hungry. If you don’t have a sufficient amount
    of data, generating synthetic data from the available dataset can help improve
    the generalization capabilities of your Deep Learning model. While you might already
    be familiar with data augmentation techniques for images (e.g., flipping an image
    horizontally), data augmentation techniques for audio data are often lesser known.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/audio-classification-with-deep-learning-in-python-cf752b22ba07?source=post_page-----15505483c63c--------------------------------)
    [## Audio Classification with Deep Learning in Python'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning image models to tackle domain shift and class imbalance with PyTorch
    and torchaudio in audio data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/audio-classification-with-deep-learning-in-python-cf752b22ba07?source=post_page-----15505483c63c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'This article will review popular data augmentation techniques for audio data.
    You can apply data augmentations for audio data in the waveform and in the spectrogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Audio Data Augmentations for Waveform (Time Domain)](#d071)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ∘ [Noise injection](#bd92)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ∘ [Shifting time](#4f84)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ∘ [Changing speed](#f531)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ∘ [Changing pitch](#76d6)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ∘ [Changing volume (not recommended)](#c212)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Audio Data Augmentations for Spectrograms (Frequency Domain)](#f7f8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ∘ [Mixup](#6c42)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ∘ [SpecAugment](#23ca)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For the data augmentations, we will use `[librosa](https://librosa.org/doc/main/index.html)`,
    which is a popular library for audio processing, and `numpy`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you are already working with PyTorch, you could also use `torchaudio` as
    an alternative.
  prefs: []
  type: TYPE_NORMAL
- en: Audio Data Augmentations for Waveform (Time Domain)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will discuss popular data augmentation techniques you can apply
    to the audio data in the waveform. You can use the `load()` method from the `[librosa](https://librosa.org/doc/main/index.html)`
    library to load the audio file as a waveform.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/cf21435e81ab505cacb8d357ddf21a01.png)'
  prefs: []
  type: TYPE_IMG
- en: Original audio data of the word “stop” in waveform from the “Speech Commands”
    dataset [1] (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: The following code implementations are referenced from Kaggle Notebooks by [kaerururu](https://www.kaggle.com/kaerunantoka)
    [7] and [CVxTz](https://www.kaggle.com/CVxTz) [5].
  prefs: []
  type: TYPE_NORMAL
- en: Noise injection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A popular data augmentation technique is to inject some sort of noise into the
    original audio data.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can choose from many different types of noise:'
  prefs: []
  type: TYPE_NORMAL
- en: White noise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Colored noise (e.g., pink, brown, etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Background noise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Once you have defined the type of noise you want to inject, you add the noise
    to your original waveform audio. Of course, you can use all different types of
    noises for your data augmentations. Below, you can see an example of noise injection
    with white noise.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9c47320d4a1db694faa565c3ef2b6611.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Data Augmentation for Audio: White Noise (Image by the author)'
  prefs: []
  type: TYPE_NORMAL
- en: Shifting time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the `roll()` function from the `numpy` library, you can shift the audio
    in time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/082e112ad5cfff1f02568331ff1d125a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Data Augmentation for Audio: Shifting time (Image by the author)'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the audio will wrap around if you don’t have enough trailing silence.
    Depending on your sound type, this data augmentation might not be recommended
    in this case (e.g., human speech).
  prefs: []
  type: TYPE_NORMAL
- en: Changing speed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can also increase (`rate>1`) or decrease (`rate<1`) the speed of the audio
    with the `time_stretch()` method from the `[librosa](https://librosa.org/doc/main/index.html)`
    library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f17f337abe6296ca32c3d88c2c677696.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Data Augmentation for Audio: Time stretch / changing speed (Image by the author)'
  prefs: []
  type: TYPE_NORMAL
- en: Changing pitch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Or you can modify the pitch of the audio with the `pitch_shift()` method from
    the `[librosa](https://librosa.org/doc/main/index.html)` library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/70f8b751d84e8f408be6f6692e0903be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Data Augmentation for Audio: Pitch shift (Image by the author)'
  prefs: []
  type: TYPE_NORMAL
- en: Changing volume (not recommended)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You could also augment the waveform in terms of volume. However, if you are
    going to convert the waveform into spectrograms later on, **volume augmentation
    will be ineffective** as the amplitude is not considered in the frequency domain.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Audio Data Augmentations for Spectrograms (Frequency Domain)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When modeling audio data with a Deep Learning model, it is a popular method
    to convert the audio classification problem into an image classification problem.
    For this, the waveform audio data is converted to a Mel spectrogram. If you need
    a refresher on Mel spectrograms, I recommend the following article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/audio-deep-learning-made-simple-part-2-why-mel-spectrograms-perform-better-aad889a93505?source=post_page-----15505483c63c--------------------------------)
    [## Audio Deep Learning Made Simple (Part 2): Why Mel Spectrograms perform better'
  prefs: []
  type: TYPE_NORMAL
- en: A Gentle Guide to processing audio in Python. What are Mel Spectrograms and
    how to generate them, in Plain English.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/audio-deep-learning-made-simple-part-2-why-mel-spectrograms-perform-better-aad889a93505?source=post_page-----15505483c63c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: You can convert audio in waveform into a Mel spectrogram with the `melspectrogram()`
    and `power_to_db()` methods from the `[librosa](https://librosa.org/doc/main/index.html)`
    library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/cf21435e81ab505cacb8d357ddf21a01.png)'
  prefs: []
  type: TYPE_IMG
- en: Original audio data of the word “stop” in waveform from the “Speech Commands”
    dataset [1] (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e6dbf4ece82dbf01e814922b47b5c9e.png)'
  prefs: []
  type: TYPE_IMG
- en: Original audio data of the word “stop” as Mel spectrogram from the “Speech Commands”
    dataset [1] (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Although you now have an image classification problem, you must be careful when
    selecting image augmentation techniques for spectrograms. E.g., horizontally flipping
    the spectrogram would substantially change the information contained in the spectrogram
    and thus is not recommended.
  prefs: []
  type: TYPE_NORMAL
- en: This section will discuss popular data augmentation techniques you can apply
    to the audio data as a Mel spectrogram.
  prefs: []
  type: TYPE_NORMAL
- en: The following code implementations are referenced from Kaggle Notebooks by [kaerururu](https://www.kaggle.com/kaerunantoka)
    [7] and [DavidS](https://www.kaggle.com/davids1992) [6].
  prefs: []
  type: TYPE_NORMAL
- en: Mixup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Simply put, Mixup [4] combines two samples by overlaying them and giving the
    new sample two labels.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3761f4efb8f2f79422bcd29acd222255.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Data Augmentation for Spectrogram: Mixup [4] (Image by the author)'
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you could also try other data augmentation techniques used in
    computer vision, like cutmix [3].
  prefs: []
  type: TYPE_NORMAL
- en: SpecAugment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SpecAugment [2] is to spectrograms what cutout is to regular images. While cutout
    blocks out random areas in an image, SpecAugment [2] masks random frequencies
    and time periods.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/cf7b42d71e651da842088d30b328d256.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Data Augmentation for Spectrogram: SpecAugment [2] (Image by the author)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, if you are using Pytorch, you could also use the `TimeMasking`
    and `FrequencyMasking` augmentations from `torchaudio`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data augmentations for audio data can be applied to the audio data in the time
    domain (waveform) as well as the frequency domain (Mel spectrogram). To successfully
    apply data augmentations to the audio data in a Deep Learning setting, you have
    to consider the following processing steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Load audio file as waveform (time domain)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply data augmentation to the waveform
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert audio from waveform to spectrogram (frequency domain)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply data augmentations to the spectrogram
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This article has covered different data augmentation techniques for audio data
    in the waveform. It is important to note that some data augmentation techniques,
    such as augmenting the volume, are ineffective when converting the waveform to
    spectrograms later on because the amplitude is not considered in the frequency
    domain.
  prefs: []
  type: TYPE_NORMAL
- en: Although you could technically apply all image augmentation techniques to spectrograms,
    not all of them will make sense. E.g., flipping them vertically or horizontally
    would change the meaning of the spectrograms. Additionally, a variant of the popular
    cutout image augmentation tailored for spectrograms masks whole timestamps and
    frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: Enjoyed This Story?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Subscribe for free*](https://medium.com/subscribe/@iamleonie) *to get notified
    when I publish a new story.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----15505483c63c--------------------------------)
    [## Get an email whenever Leonie Monigatti publishes.'
  prefs: []
  type: TYPE_NORMAL
- en: Get an email whenever Leonie Monigatti publishes. By signing up, you will create
    a Medium account if you don’t already…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----15505483c63c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*Find me on* [*LinkedIn*](https://www.linkedin.com/in/804250ab/),[*Twitter*](https://twitter.com/helloiamleonie)*,
    and* [*Kaggle*](https://www.kaggle.com/iamleonie)*!*'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Warden P. Speech Commands: A public dataset for single-word speech recognition,
    2017\. Available from [http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz](http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz)'
  prefs: []
  type: TYPE_NORMAL
- en: 'License: CC-BY-4.0'
  prefs: []
  type: TYPE_NORMAL
- en: Literature
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[2] Park, D. S., Chan, W., Zhang, Y., Chiu, C. C., Zoph, B., Cubuk, E. D.,
    & Le, Q. V. (2019). Specaugment: A simple data augmentation method for automatic
    speech recognition. *arXiv preprint arXiv:1904.08779*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Yun, S., Han, D., Oh, S. J., Chun, S., Choe, J., & Yoo, Y. (2019). Cutmix:
    Regularization strategy to train strong classifiers with localizable features.
    In *Proceedings of the IEEE/CVF international conference on computer vision* (pp.
    6023–6032).'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Zhang, H., Cisse, M., Dauphin, Y. N., & Lopez-Paz, D. (2017) mixup: Beyond
    empirical risk minimization. arXiv preprint arXiv:1710.09412.'
  prefs: []
  type: TYPE_NORMAL
- en: Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[5] [CVxTz](https://www.kaggle.com/CVxTz) (2018). [Audio data augmentation](https://www.kaggle.com/code/CVxTz/audio-data-augmentation/notebook)
    in Kaggle Notebooks (accessed March 24, 2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] [DavidS](https://www.kaggle.com/davids1992) (2019). [SpecAugment quick
    implementation](https://www.kaggle.com/code/davids1992/specaugment-quick-implementation)
    in Kaggle Notebooks (accessed March 24, 2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] [kaerururu](https://www.kaggle.com/kaerunantoka) (2022). [BirdCLEF2022
    : use 2nd label f0](https://www.kaggle.com/code/kaerunantoka/birdclef2022-use-2nd-label-f0)
    in Kaggle Notebooks (accessed March 24, 2023).'
  prefs: []
  type: TYPE_NORMAL
