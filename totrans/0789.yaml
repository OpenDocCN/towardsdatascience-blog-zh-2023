- en: 'Efficient Deep Learning: Unleashing the Power of Model Compression'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/efficient-deep-learning-unleashing-the-power-of-model-compression-7b5ea37d4d06](https://towardsdatascience.com/efficient-deep-learning-unleashing-the-power-of-model-compression-7b5ea37d4d06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e3f104a88962263b5a88baa0f644362f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Author
  prefs: []
  type: TYPE_NORMAL
- en: Accelerate model inference speed in production
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)
    ¬∑9 min read¬∑Sep 3, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a Machine Learning model is deployed into production there are often requirements
    to be met that are not taken into account in a prototyping phase of the model.
    For example, the model in production will have to handle lots of requests from
    different users running the product. So you will want to optimize for instance
    latency and/o throughput.
  prefs: []
  type: TYPE_NORMAL
- en: '**Latency**: is the time it takes for a task to get done, like how long it
    takes to load a webpage after you click a link. It‚Äôs the waiting time between
    starting something and seeing the result.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Throughput**: is how much requests a system can handle in a certain time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This means that the Machine Learning model has to be very fast at making its
    predictions, and for this there are various techniques that serve to increase
    the speed of model inference, let‚Äôs look at the most important ones in this article.
  prefs: []
  type: TYPE_NORMAL
- en: Model Compression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are techniques that aim to make **models smaller**, which is why they
    are called **model compression** techniques, while others that focus on making
    models **faster at inference** and thus fall under the field of **model optimization**.
  prefs: []
  type: TYPE_NORMAL
- en: But often making models smaller also helps with inference speed, so it is a
    very blurred line that separates these two fields of study.
  prefs: []
  type: TYPE_NORMAL
- en: Low Rank Factorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the first method we see, and it is being studied a lot, in fact many
    papers have recently come out concerning it.
  prefs: []
  type: TYPE_NORMAL
- en: '**The basic idea is to replace the matrices of a neural network** (the matrices
    representing the layers of the network) **with matrices that have a lower dimensionality**,
    although it would be more correct to talk about tensors, because we can often
    have matrices of more than 2 dimensions. In this way we will have fewer network
    parameters and faster inference.'
  prefs: []
  type: TYPE_NORMAL
- en: A trivial case is in a CNN network of replacing 3x3 convolutions with 1x1 convolutions.
    Such techniques are used by networks such as [SqueezeNet](https://arxiv.org/abs/1602.07360).
  prefs: []
  type: TYPE_NORMAL
- en: Lately, similar ideas are being applied for other purposes, such as allowing
    fine-tuning of large language models with limited resources.
  prefs: []
  type: TYPE_NORMAL
- en: When fine-tuning a pretrained model for a downstreamtask, one still has to train
    the model on all the parameters of the pretrained model, which can be very expensive.
  prefs: []
  type: TYPE_NORMAL
- en: So the idea of the method called ‚Äú[Low Rank Adaptation Of Large Language Models](https://arxiv.org/abs/2106.09685)‚Äù,
    or LoRA, is to replace matrices from the original model with pairs of smaller
    matrices (using matrix decomposition) that have a smaller size. This way only
    these new matrices need to be retrained to fit the pretrained model to more downstream
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b21d32443e3691de45e54500a439b131.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Matric Decomposition in LoRA( src: https://arxiv.org/pdf/2106.09685.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Lets now see how to implement fine tuning using LoRA with the [PEFT](https://huggingface.co/docs/peft/index)
    library from Hugging Face ü§ó.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we want to fine-tune the `[bigscience/mt0-large](https://huggingface.co/bigscience/mt0-large)`
    using LoRA. We must first take care of importing what we will need.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The next step will be to create a configuration for LoRA to be applied during
    fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We now instantiate the model using the base model of the Transformers library
    and the configuration object we created for LoRA.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Knowledge Distillation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is another method that allows us to put a ‚Äúsmall‚Äù and therefore fast model
    into production.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to have a **large model called the teacher**, and a s**maller model
    called the student**, and we will **use the** **teacher‚Äôs knowledge to teach the
    student what to predict**. That way we can put only the student into production.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e3f862323f2d9b3c00b3c9ae076faab1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Knowledge Distillation (src: [https://www.analyticsvidhya.com/blog/2022/01/knowledge-distillation-theory-and-end-to-end-case-study/](https://www.analyticsvidhya.com/blog/2022/01/knowledge-distillation-theory-and-end-to-end-case-study/))'
  prefs: []
  type: TYPE_NORMAL
- en: A classic example of a model developed in this way is [DistillBERT](https://huggingface.co/docs/transformers/model_doc/distilbert),
    which is the student model of [BERT](https://arxiv.org/abs/1810.04805). DistilBERT
    is 40% smaller than BERT, but retains 97% of the language comprehension capabilities
    and is 60% faster in inference.
  prefs: []
  type: TYPE_NORMAL
- en: One downside of this method is that you still need to have the large techer
    model available in order to train the student, and you may not have the resources
    to train a model like the teacher.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs look at a simple example of knowledge distillation in Python. A key concept
    to understand is [Kullback‚ÄìLeibler divergenc](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)e,
    which is a mathematical concept for understanding the difference between two distributions,
    and in fact in our case we want to understand the difference between the predictions
    of the two models, so the loss function of the training will be based on this
    mathematical concept.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Pruning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pruning is a model compression method that I studied for my graduate thesis,
    and in fact I have previously published an article on how to implement Pruning
    in Julia: [Iterative Pruning Methods for Artificial Neural Networks in Julia](/iterative-pruning-methods-for-artificial-neural-networks-in-julia-c605f547a485).'
  prefs: []
  type: TYPE_NORMAL
- en: Pruning was born to address overfitting in Decision Trees, in fact branches
    were cut off to decrease tree depth. The concept was later used in Neural Networks
    in which edges and/or nodes in the network are removed ( depending on whether
    unstructured pruning or structured pruning is performed).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a5512b117100950116c5961541a85284.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Neural Network Pruning (src: [https://towardsdatascience.com/pruning-neural-networks-1bb3ab5791f9](/pruning-neural-networks-1bb3ab5791f9))'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose to remove entire nodes from the network, the matrices representing the
    layers will become smaller along with your model, and thus also faster.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, if we remove individual edges, the size of the matrices will remain
    the same, but we will place zeros in correspondence with the removed edges, and
    thus we will have very sparse matrices. In unstructured pruning therefore the
    advantage lies not in increased speed, but in memory, because saving sparse matrices
    in memory takes up much less space than saving dense matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 'But what are the nodes or edges that we want to prune? The most unnecessary
    ones‚Ä¶ There are a lot of research about this, and I‚Äôd like to link you to two
    papers in particular:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Optimal Brain Damage](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=17c0a7de3c17d31f79589d245852b57d083d386e)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Optimal Brain Surgeon and general network pruning](https://ieeexplore.ieee.org/document/298572)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs see a simple Python script on how to implement Pruning for a simple MNIST
    Model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I don‚Äôt think I am wrong in saying that quantization is probably the most widely
    used comrpession technique at the moment. Again, the basic idea is simple. **We
    generally represent of parameters of our neural network using 32bit float numbers.
    But what if we used less than that? We could use 16bit, 8, bit, 4 bit, or even
    1bit and have binary networks!**
  prefs: []
  type: TYPE_NORMAL
- en: What does this imply? By using lower precision numbers, the model will weigh
    less and be smaller, however it will also lose precision giving more approximate
    results than the original model. This is a technique used a lot when we need to
    deploy an on edge devices, on some particular hardware like a smartphone, because
    it allows us to shrink the size of the network a lot. Many frameworks allow to
    easily apply quantization such as [TensorFlow Lite](https://www.tensorflow.org/lite),
    [PyTorch](https://pytorch.org/) or [TensorRT](https://developer.nvidia.com/tensorrt).
  prefs: []
  type: TYPE_NORMAL
- en: Quantization can be applied pre-training, so we directly transect a network
    whose parameters can only take parameters in a certain range, or post-training,
    so at the end we round up the value of the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Here again we quickly see how to apply quantization in Python.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we looked at several model compression methodologies in order
    to speed up the model inference phase, which can be a critical requirement for
    models in production. In particular, we focused on Low Rank Factorization, Knowledge
    Distillation, Pruning and Quantization, explaining the basic ideas and showing
    a simple implementation in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Model compression is also particularly useful for deploying models on particular
    hardware such as smartphones that have few resources (RAM, GPU, etc‚Ä¶).
  prefs: []
  type: TYPE_NORMAL
- en: One use case that I am very passionate about is also to use model compression
    to deploy models on satellites and spacecraft, which is very useful particularly
    in the field of earth observation, for example to allow the satellite to recognize
    autonomously which data or images to discard so as not to have too much traffic
    when this data is then sent to the ground segment on the for the data analysis.
    I hope this article has been helpful to you in better understanding this topic.
  prefs: []
  type: TYPE_NORMAL
- en: If you liked this article, follow me on Medium! üòÑ
  prefs: []
  type: TYPE_NORMAL
- en: üíº [Linkedin](https://www.linkedin.com/in/marcello-politi/) Ô∏è| üê¶ [Twitter](https://twitter.com/_March08_)
    | [üíª](https://emojiterra.com/laptop-computer/) [Website](https://marcello-politi.super.site/)
  prefs: []
  type: TYPE_NORMAL
