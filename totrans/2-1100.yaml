- en: How t-SNE Outperforms PCA in Dimensionality Reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-t-sne-outperforms-pca-in-dimensionality-reduction-7a3975e8cbdb](https://towardsdatascience.com/how-t-sne-outperforms-pca-in-dimensionality-reduction-7a3975e8cbdb)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: PCA vs t-SNE for visualizing high-dimensional data in a lower-dimensional space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://rukshanpramoditha.medium.com/?source=post_page-----7a3975e8cbdb--------------------------------)[![Rukshan
    Pramoditha](../Images/b80426aff64ff186cb915795644590b1.png)](https://rukshanpramoditha.medium.com/?source=post_page-----7a3975e8cbdb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7a3975e8cbdb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7a3975e8cbdb--------------------------------)
    [Rukshan Pramoditha](https://rukshanpramoditha.medium.com/?source=post_page-----7a3975e8cbdb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7a3975e8cbdb--------------------------------)
    ·15 min read·May 23, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3b7e20bb0c5cf927b95d2c4314e06247.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Pat Whelen](https://unsplash.com/@patwhelen?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/Rxt252RzQlY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, **dimensionality reduction** refers to reducing the number
    of input variables in the dataset. The number of input variables refers to the
    **dimensionality** of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dimensionality reduction techniques are mainly divided into two main categories:
    *Linear* and *Non-linear (Manifold)*.'
  prefs: []
  type: TYPE_NORMAL
- en: Under linear methods, we have discussed [*Principal Component Analysis (PCA)*](https://medium.com/data-science-365/3-easy-steps-to-perform-dimensionality-reduction-using-principal-component-analysis-pca-79121998b991),
    [*Factor Analysis (FA)*](/factor-analysis-on-women-track-records-data-with-r-and-python-6731a73cd2e0),
    [*Linear Discriminant Analysis (LDA)*](/lda-is-highly-effective-than-pca-for-dimensionality-reduction-in-classification-datasets-4489eade632)
    and [*Non-Negative Matrix Factorization (NMF)*](/non-negative-matrix-factorization-nmf-for-dimensionality-reduction-in-image-data-8450f4cae8fa).
  prefs: []
  type: TYPE_NORMAL
- en: Under non-linear methods, we have discussed [*Autoencoders (AEs)*](/how-autoencoders-outperform-pca-in-dimensionality-reduction-1ae44c68b42f)
    and [*Kernel PCA*](/dimensionality-reduction-for-linearly-inseparable-data-5030f0dc0f5e).
  prefs: []
  type: TYPE_NORMAL
- en: '**t-Distributed Stochastic Neighbor Embedding (t-SNE)** is also a *non-linear*
    dimensionality reduction method used for visualizing high-dimensional data in
    a lower-dimensional space to find important clusters or groups in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: All dimensionality reduction techniques fall under the category of unsupervised
    machine learning in which we can reveal hidden patterns and important relationships
    in the data without requiring labels.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So, dimensionality reduction algorithms deal with unlabeled data. When training
    such algorithms, the **fit()** method only needs the feature matrix, **X** as
    the input and it does not require the label column, **y** — **Source:** [Non-Negative
    Matrix Factorization (NMF) for Dimensionality Reduction in Image Data](/non-negative-matrix-factorization-nmf-for-dimensionality-reduction-in-image-data-8450f4cae8fa)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Advantages of t-SNE over PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are mainly two advantages of t-SNE over PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE can preserve the spatial relationship between data points after reducing
    the dimensionality of the data. It means that the nearby data (points with similar
    characteristics) in the original dimension will still be nearby in the lower dimension!
    That is why t-SNE is mostly used to find clusters in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: t-SNE can handle non-linear data which is very common in real-world applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA tries to reduce dimensionality by maximizing variance in the data while
    t-SNE tries to do the same by keeping similar data points together (and dissimilar
    data points apart) in both higher and lower dimensions.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Because of these reasons, t-SNE can easily outperform PCA in dimensionality
    reduction. Today, you will see this in action with an actual implementation of
    both t-SNE and PCA on the same dataset. You can compare the outputs of both methods
    and verify the fact!
  prefs: []
  type: TYPE_NORMAL
- en: Disadvantages of t-SNE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The major disadvantage of t-SNE is that it requires a lot of computation resources
    to execute the algorithm on large datasets. So, it is time-consuming to execute
    t-SNE when the dimensionality of data is very high. To avoid this, we will discuss
    a [very special trick](#44b0). You can get almost similar results but with significantly
    less time!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another disadvantage is that you will get unstable (different) results with
    t-SNE if you use random initialization even with the same hyperparameter values.
    You will [learn more about this at the end](#3301).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will also learn how to tune some of the most important hyperparameters that
    come with Scikit-learn’s t-SNE algorithm for obtaining even better results!
  prefs: []
  type: TYPE_NORMAL
- en: So, just continue reading!
  prefs: []
  type: TYPE_NORMAL
- en: How t-SNE works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two probability distributions involved in t-SNE calculations. So,
    the algorithm is stochastic by nature as its name implies.
  prefs: []
  type: TYPE_NORMAL
- en: In the high-dimensional space, we use **Gaussian (normal) distribution** to
    convert pairwise distances between data points into conditional probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the low-dimensional space, we use **Student’s t-distribution** to convert
    pairwise distances between data points into conditional probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The KL divergence measures the divergence (difference) between these two probability
    distributions. It is the cost function that we try to minimize during the training
    of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: So, t-SNE aims to locate the data points in the low-dimensional space in a way
    that it can minimize the KL divergence between the two probability distributions
    as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE requires heavy computational resources and too much time to do these probability
    calculations, especially with large datasets. That’s why t-SNE is extremely slow
    when compared to PCA. As a solution for this, we will discuss a [very special
    trick](#44b0).
  prefs: []
  type: TYPE_NORMAL
- en: Python implementation of t-SNE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Python, t-SNE is implemented by using Scikit-learn’s **TSNE()** class. Scikit-learn
    is the Python machine-learning library.
  prefs: []
  type: TYPE_NORMAL
- en: First, you need to import and create an instance of the **TSNE()** class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Important arguments of TSNE() class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many arguments in the **TSNE()** class. If we do not specify them
    directly, they take their default values when calling the **TSNE()** function.
    To learn more about those arguments, refer to the [Scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#examples-using-sklearn-manifold-tsne).
  prefs: []
  type: TYPE_NORMAL
- en: The following list contains detailed explanations of the most important arguments
    in the **TSNE()** class.
  prefs: []
  type: TYPE_NORMAL
- en: '**n_components:** An integer that defines the number of dimensions of the embedded
    space. The default is 2\. The most common values are 2 and 3 which are used to
    visualize data in 2D and 3D spaces respectively as t-SNE is mostly used for data
    visualization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**perplexity:** The number of nearest neighbors to consider when visualizing
    data. This is the most important argument in the TSNE() class. The default is
    30.0, but trying out a value between 5 and 50 is highly recommended as different
    values can result in significantly different results. You need to plot the KL
    divergence for different perplexity values and choose the right value. This is
    technically called *hyperparameter tuning*. In general, larger datasets require
    larger perplexity values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**learning_rate:** The TSNE() function uses stochastic gradient descent to
    minimize the KL divergence cost function. The stochastic gradient descent optimizer
    requires a proper learning rate value to execute the process. The learning rate
    determines how fast or slow the optimizer minimizes the loss function. A larger
    value may fail to converge the model on a solution while a smaller value may take
    too much time to converge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**n_iter:** The maximum number of iterations you set for gradient descent.
    The default is 1000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**init:** An initialization method. There are two options: *“random”* or *“pca”*.
    The default is PCA initialization which is more stable than random initialization
    and produces the same results across different executions. If we choose random
    initialization, the algorithm will generate different results at different executions
    as TSNE has a non-convex cost function and the GD optimizer may stick at a local
    minimum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**random_state:** An integer to get the same results across different executions
    when using the random initialization which generates significantly different results
    each time we run the algorithm. Popular integers are 0, 1, 2 and 42\. Learn more
    [here](/why-do-we-set-a-random-state-in-machine-learning-models-bb2dc68d8431).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important methods of TSNE() class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**fit(X):** Learns a TSNE model from the feature matrix, **X**. No transformation
    is applied here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**fit_transform(X):** Learns a TSNE model from the feature matrix, **X** and
    returns the TSNE transformed data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Important attributes of TSNE() class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**kl_divergence_:** Returns the Kullback-Leibler divergence after optimization.
    The GD optimizer tries to minimize the KL divergence during training. Analyzing
    this divergence by setting different values for perplexityis a great way of choosing
    the right value for perplexity. More on this shortly!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python implementation of PCA (Optional)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article also includes the use of the PCA algorithm for visualizing MNIST
    data in a lower-dimensional space. Therefore, it is worth discussing the Python
    implementation of the PCA algorithm although it is optional here. I have already
    published in-detail articles for PCA, which can be found in the following links.
  prefs: []
  type: TYPE_NORMAL
- en: '**PCA articles:** [3 Easy Steps to Perform Dimensionality Reduction Using Principal
    Component Analysis (PCA)](https://medium.com/data-science-365/3-easy-steps-to-perform-dimensionality-reduction-using-principal-component-analysis-pca-79121998b991),
    [Principal Component Analysis (PCA) with Scikit-learn](https://medium.com/data-science-365/principal-component-analysis-pca-with-scikit-learn-1e84a0c731b0),
    [An In-depth Guide to PCA with NumPy](https://medium.com/data-science-365/an-in-depth-guide-to-pca-with-numpy-1fb128535b3e).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The MNIST data in tabular format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are different ways of loading the MNIST dataset which contains 70,000
    grayscale images of handwritten digits under 10 categories (0 to 9).
  prefs: []
  type: TYPE_NORMAL
- en: '**Using Scikit-learn API:** We get the shape of (70000, 784) which is the required
    tabular format for the TSNE and PCA algorithms. The dataset will be loaded as
    a Pandas data frame. There are 70000 observations (images) in the dataset. Each
    observation has 784 (28 x 28) features (pixel values). The size of an image is
    28 x 28.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Using Keras API:** We get the shape of (60000, 28, 28) for the train test
    and (10000, 28, 28) for the test set. The data will be loaded as three-dimensional
    NumPy arrays. This format cannot be directly used in the TSNE and PCA algorithms.
    We need to [reshape the MNIST data](https://medium.com/data-science-365/acquire-understand-and-prepare-the-mnist-dataset-3d71a84e07e7#a69a).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Note:** To learn more about the differences between the two APIs, click [here](/exploring-the-mnist-digits-dataset-7ff62631766a#879a).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For now, we will load the MNIST dataset using the Scikit-learn API. To speed
    up the computation process when using TSNE and PCA, we only load a part (the first
    10,000 instances) of the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4cd875bf670b1e87a54f3312f45ca023.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: We also normalize pixel values to use with PCA and t-SNE.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Visualizing MNIST data using PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see in the above output, the original dimensionality of MNIST data
    is 784 which cannot be plotted in a 2D plot. So, we need to reduce the number
    of dimensions to 2 by applying PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/be836666523b5da3f7a99997e9ba29a5.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The PCA-transformed MNIST data has the shape of (10000, 2) which can now be
    plotted in a 2D plot.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23073b5f97c0a7c83c4d1aaea6b48967.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: It is clear that all data points are in a single cluster and we cannot see different
    clusters for each class label. This is not the representation we need.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s fix this by applying TSNE to the MNIST data.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing MNIST data using t-SNE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will apply t-SNE on the same dataset. When applying t-SNE, we will use
    default values for all arguments (hyperparameters) in the **TSNE()** class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0a3afd960bfb45b76fd0a47c9a29f3c7.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The TSNE-transformed MNIST data has the shape of (10000, 2) which can now be
    plotted in a 2D plot same as before.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f7271b1ec59c6a0659d010559a211f1.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: It is clear that data points are separated as clusters according to their class
    labels. The nearby data points of the same class in the original dimension will
    still be nearby in the lower dimension!
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE is more effective than PCA for dimensionality reduction as it keeps similar
    data points together (and dissimilar data points apart) in both higher and lower
    dimensions. and works well with non-linear data.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: PCA before t-SNE — Combining both methods (Very special trick)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PCA can’t maintain the spatial relationship between data points after dimensionality
    reduction although it executes really fast compared to t-SNE.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, t-SNE is really slow with larger datasets, but it can preserve
    the spatial relationship between data points after dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: To speed up the computation process of t-SNE, we apply PCA before t-SNE and
    combine both methods as in the following diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61b75660ca3bd1ca2710b13b9bd15347.png)'
  prefs: []
  type: TYPE_IMG
- en: '**PCA before t-SNE workflow** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: First, we apply PCA to the MNIST data and reduce dimensionality to 100 (keep
    only 100 dimensions/components/features). Then, we apply t-SNE to the PCA-transformed
    MNIST data. This time, t-SNE only sees 100 features instead of 784 features and
    does not want to perform much computation. Now, t-SNE executes really fast but
    still manages to generate the same or even better results!
  prefs: []
  type: TYPE_NORMAL
- en: By applying PCA before t-SNE, you will get the following benefits.
  prefs: []
  type: TYPE_NORMAL
- en: PCA removes noise in the data and keeps only the most important features in
    the data. By feeding PCA-transformed data into t-SNE, you will get an even better
    output!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA removes multicollinearity between the input features. The PCA-transformed
    data has uncorrelated variables which are fed into t-SNE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As I already said, PCA reduces the number of features significantly. The PCA-transformed
    data will be fed into t-SNE ad you will get results very fast!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c928cbb4f5e73ed9f6f8b131c18d300b.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: It took 100 seconds to run TSNE with all 784 features. After applying PCA, it
    only took 20 seconds to run TSNE with PCA-transformed data which has 100 features.
  prefs: []
  type: TYPE_NORMAL
- en: The PCA-transformed data accurately represents the original MNIST data as the
    first 100 components capture about 90% variance in the original data. We can confirm
    it by looking at the following plot. Therefore, it is reasonable to feed the PCA-transformed
    data to t-SNE in place of the original data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/cf459c8709b07f79200f312ca684e15f.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right value for perplexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perplexity determines the number of nearest neighbors to consider when visualizing
    data. It is the most important hyperparameter in t-SNE. Therefore, you need to
    tune it properly.
  prefs: []
  type: TYPE_NORMAL
- en: One option is that you can plot the KL divergences for different perplexity
    values and analyze how Kl divergence behaves when increasing the value of perplexity.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3827d9ccb25d6fc552f4f980ba8a0332.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The KL divergence is decreasing continuously when the perplexity value is increased!
    Therefore, we can't decide the right value for perplexity by only analyzing this
    plot.
  prefs: []
  type: TYPE_NORMAL
- en: As the second option, we need to run the TSNE algorithm several times with different
    perplexity values and visualize the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56c410280a45f1740bf8f74efe35eb5e.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: At perplexity = 5 (a lower value), clear clusters are formed. But, there is
    not enough space between the clusters and the points within clusters are well
    separated.
  prefs: []
  type: TYPE_NORMAL
- en: At perplexity = 30, 50 and 100 (right values), the space between the clusters
    increases and the points within clusters concentrate well.
  prefs: []
  type: TYPE_NORMAL
- en: At perplexity = 210 and 500 (excessive values), clusters tend to overlap each
    other, which is not required for us.
  prefs: []
  type: TYPE_NORMAL
- en: For the MNIST dataset, any perplexity value between 30 and 50 will be good enough.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** The optimal value of perplexity highly depends on the nature and
    volume of the data. In general, larger datasets require larger perplexity values.
    It is always good practice to start with the default value which is 30.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Changing the right number of iterations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will visualize the effect of the number of iterations used in the gradient
    descent optimization algorithm. TSNE uses stochastic gradient descent to minimize
    the KL divergence cost function.
  prefs: []
  type: TYPE_NORMAL
- en: The number of iterations determines how many times the optimizer performs gradient
    updates during optimization. The default is 1000\. The minimum should be 250.
  prefs: []
  type: TYPE_NORMAL
- en: The number of iterations is specified in the ***n_iter*** argument.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In this experiment, we will try out three different values which are 250, 500
    and 1000.
  prefs: []
  type: TYPE_NORMAL
- en: '**250 iterations**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/63463b84b0308fba48446629185ea641.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: '**500 iterations**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36d0989f401e7372a30c5ad5be36bcc9.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: '**1000 iterations**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23f0298b955d036ac5674eabbd56e505.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: After running for 250 iterations, clusters are not clearly formed because the
    cost function is not fully minimized. So, we need to give the algorithm more time
    to run for further optimization. After running for 500 iterations, the separation
    of clusters is very clear. Once optimized, increasing the number of iterations
    will have little to no effect on changing the cluster formation! It will only
    increase the training time! In this case, 500 iterations are sufficient to optimize
    the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** The selection of the number of iterations highly depends on the nature
    and volume of the data.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Randomness of initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two types of initialization for TSNE:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Random initialization**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**PCA initialization (default)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: PCA initialization is more stable than random initialization and produces the
    same results across different executions.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, random initialization will generate different results at different
    executions as **TSNE has a non-convex cost function** and the GD optimizer may
    stick at a local minimum as in the following diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e899f18df4def4d415788ddca9c5106e.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: If the initialization occurs near point **A**, the optimizer may stick at the
    local minimum and the cost function will not be fully minimized.
  prefs: []
  type: TYPE_NORMAL
- en: If the initialization occurs near point **B**, the optimizer may stick at the
    global minimum and this time, TSNE will generate a completely different output
    than in the previous case.
  prefs: []
  type: TYPE_NORMAL
- en: There may be many local minima in the cost functions. Therefore, different initializations
    may tend the algorithm to be stuck at a local minimum. This is the case for random
    initialization and PCA initialization will not suffer from this!
  prefs: []
  type: TYPE_NORMAL
- en: Using a random state in random initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you still want to get stable results at different executions of the TSNE
    algorithm with the random initialization, specify the ***random_state*** argument
    with an integer like 0, 1, or 42.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: If you run this many times, you will get the same results. However, if you specify
    0 for ***random_state***, you will get different results than in the previous
    case.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see two outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Random state = 42**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/99b9b3e66709180e5d1afa2cc547ea80.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: '**Random state = 0**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e775ac7ed760f42084be02a0195f5501.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: You will get completely different outputs with different random state values!
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** Specifying a random state will **not** prevent the algorithm from
    being stuck at a local minimum.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is the end of today’s article.
  prefs: []
  type: TYPE_NORMAL
- en: '**Please let me know if you’ve any questions or feedback.**'
  prefs: []
  type: TYPE_NORMAL
- en: Other dimensionality reduction methods you might be interested in
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[**Principal Component Analysis (PCA)**](https://medium.com/data-science-365/3-easy-steps-to-perform-dimensionality-reduction-using-principal-component-analysis-pca-79121998b991)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Factor Analysis (FA)**](/factor-analysis-on-women-track-records-data-with-r-and-python-6731a73cd2e0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Linear Discriminant Analysis (LDA)**](/lda-is-highly-effective-than-pca-for-dimensionality-reduction-in-classification-datasets-4489eade632)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Non-Negative Matrix Factorization (NMF)**](/non-negative-matrix-factorization-nmf-for-dimensionality-reduction-in-image-data-8450f4cae8fa)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Autoencoders (AEs)**](/how-autoencoders-outperform-pca-in-dimensionality-reduction-1ae44c68b42f)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Kernel PCA**](/dimensionality-reduction-for-linearly-inseparable-data-5030f0dc0f5e)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read next (Recommended)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[**PCA and Dimensionality Reduction Special Collection**](https://rukshanpramoditha.medium.com/list/pca-and-dimensionality-reduction-special-collection-146045a5acb5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How about an AI course?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[**Neural Networks and Deep Learning Course**](https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support me as a writer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*I hope you enjoyed reading this article. If you’d like to support me as a
    writer, kindly consider* [***signing up for a membership***](https://rukshanpramoditha.medium.com/membership)
    *to get unlimited access to Medium. It only costs $5 per month and I will receive
    a portion of your membership fee.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://rukshanpramoditha.medium.com/membership?source=post_page-----7a3975e8cbdb--------------------------------)
    [## Join Medium with my referral link - Rukshan Pramoditha'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Rukshan Pramoditha (and thousands of other writers on
    Medium). Your membership fee directly…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: rukshanpramoditha.medium.com](https://rukshanpramoditha.medium.com/membership?source=post_page-----7a3975e8cbdb--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Join my private list of emails
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Never miss a great story from me again. By* [***subscribing to my email list***](https://rukshanpramoditha.medium.com/subscribe)*,
    you will directly receive my stories as soon as I publish them.*'
  prefs: []
  type: TYPE_NORMAL
- en: Thank you so much for your continuous support! See you in the next article.
    Happy learning to everyone!
  prefs: []
  type: TYPE_NORMAL
- en: MNIST dataset info
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Citation:** Deng, L., 2012\. The mnist database of handwritten digit images
    for machine learning research. **IEEE Signal Processing Magazine**, 29(6), pp.
    141–142.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Source:** [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**License:** *Yann LeCun* (Courant Institute, NYU) and *Corinna Cortes* (Google
    Labs, New York) hold the copyright of the MNIST dataset which is available under
    the *Creative Commons Attribution-ShareAlike 4.0 International License* ([**CC
    BY-SA**](https://creativecommons.org/licenses/by-sa/4.0/)). You can learn more
    about different dataset license types [here](https://rukshanpramoditha.medium.com/dataset-and-software-license-types-you-need-to-consider-d20965ca43dc#6ade).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Designed and written by:** [Rukshan Pramoditha](https://medium.com/u/f90a3bb1d400?source=post_page-----7a3975e8cbdb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: '**2023–05–23**'
  prefs: []
  type: TYPE_NORMAL
