- en: How t-SNE Outperforms PCA in Dimensionality Reduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: t-SNE 如何在降维中优于 PCA
- en: 原文：[https://towardsdatascience.com/how-t-sne-outperforms-pca-in-dimensionality-reduction-7a3975e8cbdb](https://towardsdatascience.com/how-t-sne-outperforms-pca-in-dimensionality-reduction-7a3975e8cbdb)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-t-sne-outperforms-pca-in-dimensionality-reduction-7a3975e8cbdb](https://towardsdatascience.com/how-t-sne-outperforms-pca-in-dimensionality-reduction-7a3975e8cbdb)
- en: PCA vs t-SNE for visualizing high-dimensional data in a lower-dimensional space
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA 与 t-SNE 在低维空间中可视化高维数据的比较
- en: '[](https://rukshanpramoditha.medium.com/?source=post_page-----7a3975e8cbdb--------------------------------)[![Rukshan
    Pramoditha](../Images/b80426aff64ff186cb915795644590b1.png)](https://rukshanpramoditha.medium.com/?source=post_page-----7a3975e8cbdb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7a3975e8cbdb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7a3975e8cbdb--------------------------------)
    [Rukshan Pramoditha](https://rukshanpramoditha.medium.com/?source=post_page-----7a3975e8cbdb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://rukshanpramoditha.medium.com/?source=post_page-----7a3975e8cbdb--------------------------------)[![Rukshan
    Pramoditha](../Images/b80426aff64ff186cb915795644590b1.png)](https://rukshanpramoditha.medium.com/?source=post_page-----7a3975e8cbdb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7a3975e8cbdb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7a3975e8cbdb--------------------------------)
    [Rukshan Pramoditha](https://rukshanpramoditha.medium.com/?source=post_page-----7a3975e8cbdb--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7a3975e8cbdb--------------------------------)
    ·15 min read·May 23, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7a3975e8cbdb--------------------------------)
    ·15 分钟阅读·2023年5月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3b7e20bb0c5cf927b95d2c4314e06247.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b7e20bb0c5cf927b95d2c4314e06247.png)'
- en: Photo by [Pat Whelen](https://unsplash.com/@patwhelen?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/Rxt252RzQlY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Pat Whelen](https://unsplash.com/@patwhelen?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/photos/Rxt252RzQlY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: In machine learning, **dimensionality reduction** refers to reducing the number
    of input variables in the dataset. The number of input variables refers to the
    **dimensionality** of the dataset.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，**降维**是指减少数据集中的输入变量数量。输入变量的数量即数据集的**维度**。
- en: 'Dimensionality reduction techniques are mainly divided into two main categories:
    *Linear* and *Non-linear (Manifold)*.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 降维技术主要分为两大类：*线性*和*非线性（流形）*。
- en: Under linear methods, we have discussed [*Principal Component Analysis (PCA)*](https://medium.com/data-science-365/3-easy-steps-to-perform-dimensionality-reduction-using-principal-component-analysis-pca-79121998b991),
    [*Factor Analysis (FA)*](/factor-analysis-on-women-track-records-data-with-r-and-python-6731a73cd2e0),
    [*Linear Discriminant Analysis (LDA)*](/lda-is-highly-effective-than-pca-for-dimensionality-reduction-in-classification-datasets-4489eade632)
    and [*Non-Negative Matrix Factorization (NMF)*](/non-negative-matrix-factorization-nmf-for-dimensionality-reduction-in-image-data-8450f4cae8fa).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性方法下，我们讨论了 [*主成分分析（PCA）*](https://medium.com/data-science-365/3-easy-steps-to-perform-dimensionality-reduction-using-principal-component-analysis-pca-79121998b991)，[*因子分析（FA）*](/factor-analysis-on-women-track-records-data-with-r-and-python-6731a73cd2e0)，[*线性判别分析（LDA）*](/lda-is-highly-effective-than-pca-for-dimensionality-reduction-in-classification-datasets-4489eade632)
    和 [*非负矩阵分解（NMF）*](/non-negative-matrix-factorization-nmf-for-dimensionality-reduction-in-image-data-8450f4cae8fa)。
- en: Under non-linear methods, we have discussed [*Autoencoders (AEs)*](/how-autoencoders-outperform-pca-in-dimensionality-reduction-1ae44c68b42f)
    and [*Kernel PCA*](/dimensionality-reduction-for-linearly-inseparable-data-5030f0dc0f5e).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在非线性方法下，我们讨论了 [*自编码器（AEs）*](/how-autoencoders-outperform-pca-in-dimensionality-reduction-1ae44c68b42f)
    和 [*核 PCA*](/dimensionality-reduction-for-linearly-inseparable-data-5030f0dc0f5e)。
- en: '**t-Distributed Stochastic Neighbor Embedding (t-SNE)** is also a *non-linear*
    dimensionality reduction method used for visualizing high-dimensional data in
    a lower-dimensional space to find important clusters or groups in the data.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**t-分布随机邻域嵌入（t-SNE）** 也是一种*非线性*降维方法，用于在低维空间中可视化高维数据，以寻找数据中的重要集群或群体。'
- en: All dimensionality reduction techniques fall under the category of unsupervised
    machine learning in which we can reveal hidden patterns and important relationships
    in the data without requiring labels.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 所有的降维技术都属于无监督机器学习的范畴，我们可以在不需要标签的情况下揭示数据中的隐藏模式和重要关系。
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So, dimensionality reduction algorithms deal with unlabeled data. When training
    such algorithms, the **fit()** method only needs the feature matrix, **X** as
    the input and it does not require the label column, **y** — **Source:** [Non-Negative
    Matrix Factorization (NMF) for Dimensionality Reduction in Image Data](/non-negative-matrix-factorization-nmf-for-dimensionality-reduction-in-image-data-8450f4cae8fa)
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因此，降维算法处理的是未标记的数据。在训练这样的算法时，**fit()**方法只需要特征矩阵**X**作为输入，而不需要标签列**y** — **来源：**
    [用于图像数据降维的非负矩阵分解（NMF）](/non-negative-matrix-factorization-nmf-for-dimensionality-reduction-in-image-data-8450f4cae8fa)
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Advantages of t-SNE over PCA
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: t-SNE相对于PCA的优点
- en: 'There are mainly two advantages of t-SNE over PCA:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE相对于PCA有两个主要优点：
- en: t-SNE can preserve the spatial relationship between data points after reducing
    the dimensionality of the data. It means that the nearby data (points with similar
    characteristics) in the original dimension will still be nearby in the lower dimension!
    That is why t-SNE is mostly used to find clusters in the data.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: t-SNE可以在减少数据的维度后保持数据点之间的空间关系。这意味着在原始维度中相邻的数据（具有相似特征的点）在低维中仍将保持相邻！这就是为什么t-SNE主要用于发现数据中的聚类。
- en: t-SNE can handle non-linear data which is very common in real-world applications.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: t-SNE可以处理在实际应用中非常常见的非线性数据。
- en: PCA tries to reduce dimensionality by maximizing variance in the data while
    t-SNE tries to do the same by keeping similar data points together (and dissimilar
    data points apart) in both higher and lower dimensions.
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: PCA尝试通过最大化数据的方差来减少维度，而t-SNE则通过在高维和低维中将相似的数据点保持在一起（并将不相似的数据点分开）来实现相同的目标。
- en: Because of these reasons, t-SNE can easily outperform PCA in dimensionality
    reduction. Today, you will see this in action with an actual implementation of
    both t-SNE and PCA on the same dataset. You can compare the outputs of both methods
    and verify the fact!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，t-SNE在降维方面往往能超越PCA。今天，你将看到对同一数据集上t-SNE和PCA的实际实现。你可以比较这两种方法的输出，并验证这一事实！
- en: Disadvantages of t-SNE
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: t-SNE的缺点
- en: The major disadvantage of t-SNE is that it requires a lot of computation resources
    to execute the algorithm on large datasets. So, it is time-consuming to execute
    t-SNE when the dimensionality of data is very high. To avoid this, we will discuss
    a [very special trick](#44b0). You can get almost similar results but with significantly
    less time!
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: t-SNE的主要缺点是它在大数据集上执行算法时需要大量计算资源。因此，当数据的维度非常高时，执行t-SNE是非常耗时的。为了解决这个问题，我们将讨论一个[非常特别的技巧](#44b0)。你可以在显著减少时间的同时获得几乎相似的结果！
- en: Another disadvantage is that you will get unstable (different) results with
    t-SNE if you use random initialization even with the same hyperparameter values.
    You will [learn more about this at the end](#3301).
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个缺点是，如果使用随机初始化，即使在相同的超参数值下，t-SNE也会得到不稳定（不同）的结果。你可以在[最后学习更多关于这个问题的信息](#3301)。
- en: You will also learn how to tune some of the most important hyperparameters that
    come with Scikit-learn’s t-SNE algorithm for obtaining even better results!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你还将学习如何调整Scikit-learn的t-SNE算法中的一些最重要的超参数，以获得更好的结果！
- en: So, just continue reading!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，继续阅读吧！
- en: How t-SNE works
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: t-SNE的工作原理
- en: There are two probability distributions involved in t-SNE calculations. So,
    the algorithm is stochastic by nature as its name implies.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE计算中涉及两个概率分布。因此，正如其名称所暗示的，算法本质上是随机的。
- en: In the high-dimensional space, we use **Gaussian (normal) distribution** to
    convert pairwise distances between data points into conditional probabilities.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在高维空间中，我们使用**Gaussian (normal) distribution**将数据点之间的成对距离转换为条件概率。
- en: In the low-dimensional space, we use **Student’s t-distribution** to convert
    pairwise distances between data points into conditional probabilities.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在低维空间中，我们使用**Student’s t-distribution**将数据点之间的成对距离转换为条件概率。
- en: The KL divergence measures the divergence (difference) between these two probability
    distributions. It is the cost function that we try to minimize during the training
    of the algorithm.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: KL散度测量这两个概率分布之间的差异（不同）。这是我们在算法训练过程中尝试最小化的成本函数。
- en: So, t-SNE aims to locate the data points in the low-dimensional space in a way
    that it can minimize the KL divergence between the two probability distributions
    as much as possible.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，t-SNE 的目标是将数据点定位到低维空间中，以便尽可能最小化两个概率分布之间的 KL 散度。
- en: t-SNE requires heavy computational resources and too much time to do these probability
    calculations, especially with large datasets. That’s why t-SNE is extremely slow
    when compared to PCA. As a solution for this, we will discuss a [very special
    trick](#44b0).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE 需要大量计算资源和时间来进行这些概率计算，尤其是在处理大型数据集时。这就是为什么 t-SNE 比 PCA 慢得多。对此，我们将讨论一个[非常特别的技巧](#44b0)。
- en: Python implementation of t-SNE
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: t-SNE 的 Python 实现
- en: In Python, t-SNE is implemented by using Scikit-learn’s **TSNE()** class. Scikit-learn
    is the Python machine-learning library.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，t-SNE 是通过使用 Scikit-learn 的 **TSNE()** 类来实现的。Scikit-learn 是 Python
    机器学习库。
- en: First, you need to import and create an instance of the **TSNE()** class.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要导入并创建 **TSNE()** 类的实例。
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Important arguments of TSNE() class
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TSNE() 类的重要参数
- en: There are many arguments in the **TSNE()** class. If we do not specify them
    directly, they take their default values when calling the **TSNE()** function.
    To learn more about those arguments, refer to the [Scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#examples-using-sklearn-manifold-tsne).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**TSNE()** 类中有许多参数。如果我们没有直接指定这些参数，它们在调用 **TSNE()** 函数时会取其默认值。要了解更多关于这些参数的信息，请参阅
    [Scikit-learn 文档](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#examples-using-sklearn-manifold-tsne)。'
- en: The following list contains detailed explanations of the most important arguments
    in the **TSNE()** class.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表包含了 **TSNE()** 类中最重要参数的详细解释。
- en: '**n_components:** An integer that defines the number of dimensions of the embedded
    space. The default is 2\. The most common values are 2 and 3 which are used to
    visualize data in 2D and 3D spaces respectively as t-SNE is mostly used for data
    visualization.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**n_components:** 一个整数，定义嵌入空间的维度数量。默认值为 2。最常用的值是 2 和 3，分别用于在 2D 和 3D 空间中可视化数据，因为
    t-SNE 主要用于数据可视化。'
- en: '**perplexity:** The number of nearest neighbors to consider when visualizing
    data. This is the most important argument in the TSNE() class. The default is
    30.0, but trying out a value between 5 and 50 is highly recommended as different
    values can result in significantly different results. You need to plot the KL
    divergence for different perplexity values and choose the right value. This is
    technically called *hyperparameter tuning*. In general, larger datasets require
    larger perplexity values.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**perplexity:** 在可视化数据时考虑的最近邻居的数量。这是 TSNE() 类中最重要的参数。默认值为 30.0，但强烈建议尝试 5 到
    50 之间的值，因为不同的值可能会产生显著不同的结果。你需要绘制不同困惑度值的 KL 散度，并选择合适的值。这在技术上称为 *超参数调优*。一般来说，较大的数据集需要较大的困惑度值。'
- en: '**learning_rate:** The TSNE() function uses stochastic gradient descent to
    minimize the KL divergence cost function. The stochastic gradient descent optimizer
    requires a proper learning rate value to execute the process. The learning rate
    determines how fast or slow the optimizer minimizes the loss function. A larger
    value may fail to converge the model on a solution while a smaller value may take
    too much time to converge.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**learning_rate:** TSNE() 函数使用随机梯度下降来最小化 KL 散度成本函数。随机梯度下降优化器需要一个合适的学习率值来执行此过程。学习率决定了优化器最小化损失函数的快慢。较大的值可能会导致模型无法收敛，而较小的值可能需要太多时间才能收敛。'
- en: '**n_iter:** The maximum number of iterations you set for gradient descent.
    The default is 1000.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**n_iter:** 为梯度下降设置的最大迭代次数。默认值为 1000。'
- en: '**init:** An initialization method. There are two options: *“random”* or *“pca”*.
    The default is PCA initialization which is more stable than random initialization
    and produces the same results across different executions. If we choose random
    initialization, the algorithm will generate different results at different executions
    as TSNE has a non-convex cost function and the GD optimizer may stick at a local
    minimum.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**init:** 初始化方法。有两个选项：*“random”* 或 *“pca”*。默认是 PCA 初始化，这比随机初始化更稳定，并且在不同执行之间产生相同的结果。如果选择随机初始化，由于
    TSNE 具有非凸成本函数，且 GD 优化器可能会停留在局部最小值，算法会在不同的执行中生成不同的结果。'
- en: '**random_state:** An integer to get the same results across different executions
    when using the random initialization which generates significantly different results
    each time we run the algorithm. Popular integers are 0, 1, 2 and 42\. Learn more
    [here](/why-do-we-set-a-random-state-in-machine-learning-models-bb2dc68d8431).'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**random_state：** 一个整数，用于在使用随机初始化时获取相同的结果，以避免每次运行算法时生成显著不同的结果。常用的整数有 0、1、2
    和 42。了解更多信息 [这里](/why-do-we-set-a-random-state-in-machine-learning-models-bb2dc68d8431)。'
- en: Important methods of TSNE() class
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TSNE() 类的重要方法
- en: '**fit(X):** Learns a TSNE model from the feature matrix, **X**. No transformation
    is applied here.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**fit(X):** 从特征矩阵 **X** 学习一个 TSNE 模型。这里不进行任何转换。'
- en: '**fit_transform(X):** Learns a TSNE model from the feature matrix, **X** and
    returns the TSNE transformed data.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**fit_transform(X):** 从特征矩阵 **X** 学习一个 TSNE 模型，并返回 TSNE 转换后的数据。'
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Important attributes of TSNE() class
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TSNE() 类的重要属性
- en: '**kl_divergence_:** Returns the Kullback-Leibler divergence after optimization.
    The GD optimizer tries to minimize the KL divergence during training. Analyzing
    this divergence by setting different values for perplexityis a great way of choosing
    the right value for perplexity. More on this shortly!'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kl_divergence_:** 返回优化后的 Kullback-Leibler 散度。GD 优化器在训练过程中尝试最小化 KL 散度。通过设置不同的困惑度值来分析这一散度是选择正确困惑度值的好方法。稍后将详细介绍！'
- en: Python implementation of PCA (Optional)
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA 的 Python 实现（可选）
- en: This article also includes the use of the PCA algorithm for visualizing MNIST
    data in a lower-dimensional space. Therefore, it is worth discussing the Python
    implementation of the PCA algorithm although it is optional here. I have already
    published in-detail articles for PCA, which can be found in the following links.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本文还包括了使用 PCA 算法在低维空间中可视化 MNIST 数据。因此，虽然在这里是可选的，但讨论 PCA 算法的 Python 实现仍然是有意义的。我已经发布了详细的
    PCA 文章，相关链接如下。
- en: '**PCA articles:** [3 Easy Steps to Perform Dimensionality Reduction Using Principal
    Component Analysis (PCA)](https://medium.com/data-science-365/3-easy-steps-to-perform-dimensionality-reduction-using-principal-component-analysis-pca-79121998b991),
    [Principal Component Analysis (PCA) with Scikit-learn](https://medium.com/data-science-365/principal-component-analysis-pca-with-scikit-learn-1e84a0c731b0),
    [An In-depth Guide to PCA with NumPy](https://medium.com/data-science-365/an-in-depth-guide-to-pca-with-numpy-1fb128535b3e).'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**PCA 相关文章：** [使用主成分分析（PCA）进行降维的 3 个简单步骤](https://medium.com/data-science-365/3-easy-steps-to-perform-dimensionality-reduction-using-principal-component-analysis-pca-79121998b991)，[使用
    Scikit-learn 进行主成分分析（PCA）](https://medium.com/data-science-365/principal-component-analysis-pca-with-scikit-learn-1e84a0c731b0)，[使用
    NumPy 进行主成分分析（PCA）的深入指南](https://medium.com/data-science-365/an-in-depth-guide-to-pca-with-numpy-1fb128535b3e)。'
- en: The MNIST data in tabular format
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表格格式的 MNIST 数据
- en: There are different ways of loading the MNIST dataset which contains 70,000
    grayscale images of handwritten digits under 10 categories (0 to 9).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 加载 MNIST 数据集的方式有很多，这个数据集包含 70,000 张灰度手写数字图像，涵盖 10 个类别（0 到 9）。
- en: '**Using Scikit-learn API:** We get the shape of (70000, 784) which is the required
    tabular format for the TSNE and PCA algorithms. The dataset will be loaded as
    a Pandas data frame. There are 70000 observations (images) in the dataset. Each
    observation has 784 (28 x 28) features (pixel values). The size of an image is
    28 x 28.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 Scikit-learn API：** 我们得到的形状为 (70000, 784)，这是 TSNE 和 PCA 算法所需的表格格式。数据集将被加载为
    Pandas 数据框。数据集中有 70000 个观测（图像）。每个观测有 784 个特征（像素值）。图像的大小是 28 x 28。'
- en: '**Using Keras API:** We get the shape of (60000, 28, 28) for the train test
    and (10000, 28, 28) for the test set. The data will be loaded as three-dimensional
    NumPy arrays. This format cannot be directly used in the TSNE and PCA algorithms.
    We need to [reshape the MNIST data](https://medium.com/data-science-365/acquire-understand-and-prepare-the-mnist-dataset-3d71a84e07e7#a69a).'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 Keras API：** 训练集的形状为 (60000, 28, 28)，测试集的形状为 (10000, 28, 28)。数据将被加载为三维的
    NumPy 数组。这种格式不能直接用于 TSNE 和 PCA 算法。我们需要 [重新调整 MNIST 数据的形状](https://medium.com/data-science-365/acquire-understand-and-prepare-the-mnist-dataset-3d71a84e07e7#a69a)。'
- en: '**Note:** To learn more about the differences between the two APIs, click [here](/exploring-the-mnist-digits-dataset-7ff62631766a#879a).'
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 要了解这两个 API 之间的差异，请点击 [这里](/exploring-the-mnist-digits-dataset-7ff62631766a#879a)。'
- en: For now, we will load the MNIST dataset using the Scikit-learn API. To speed
    up the computation process when using TSNE and PCA, we only load a part (the first
    10,000 instances) of the MNIST dataset.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们将使用 Scikit-learn API 加载 MNIST 数据集。为了加快使用 TSNE 和 PCA 时的计算过程，我们只加载 MNIST
    数据集的一部分（前 10,000 个实例）。
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/4cd875bf670b1e87a54f3312f45ca023.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4cd875bf670b1e87a54f3312f45ca023.png)'
- en: (Image by author)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源：作者）
- en: We also normalize pixel values to use with PCA and t-SNE.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将像素值进行标准化，以便与 PCA 和 t-SNE 一起使用。
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Visualizing MNIST data using PCA
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PCA 可视化 MNIST 数据
- en: As you can see in the above output, the original dimensionality of MNIST data
    is 784 which cannot be plotted in a 2D plot. So, we need to reduce the number
    of dimensions to 2 by applying PCA.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如上面的输出所示，MNIST 数据的原始维度是 784，无法在 2D 图中绘制。因此，我们需要通过应用 PCA 将维度减少到 2。
- en: Let’s see the output.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看输出结果。
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/be836666523b5da3f7a99997e9ba29a5.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be836666523b5da3f7a99997e9ba29a5.png)'
- en: (Image by author)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源：作者）
- en: The PCA-transformed MNIST data has the shape of (10000, 2) which can now be
    plotted in a 2D plot.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 转换后的 MNIST 数据形状为 (10000, 2)，现在可以在 2D 图中绘制。
- en: '![](../Images/23073b5f97c0a7c83c4d1aaea6b48967.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23073b5f97c0a7c83c4d1aaea6b48967.png)'
- en: (Image by author)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源：作者）
- en: It is clear that all data points are in a single cluster and we cannot see different
    clusters for each class label. This is not the representation we need.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，所有数据点都在一个簇中，我们无法看到每个类别标签的不同簇。这不是我们需要的表示方式。
- en: Let’s fix this by applying TSNE to the MNIST data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过对 MNIST 数据应用 TSNE 来解决这个问题。
- en: Visualizing MNIST data using t-SNE
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 t-SNE 可视化 MNIST 数据
- en: Now, we will apply t-SNE on the same dataset. When applying t-SNE, we will use
    default values for all arguments (hyperparameters) in the **TSNE()** class.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将在相同的数据集上应用 t-SNE。应用 t-SNE 时，我们将使用 **TSNE()** 类中的所有参数（超参数）的默认值。
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/0a3afd960bfb45b76fd0a47c9a29f3c7.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a3afd960bfb45b76fd0a47c9a29f3c7.png)'
- en: (Image by author)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源：作者）
- en: The TSNE-transformed MNIST data has the shape of (10000, 2) which can now be
    plotted in a 2D plot same as before.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: TSNE 转换后的 MNIST 数据形状为 (10000, 2)，现在可以像以前一样在 2D 图中绘制。
- en: '![](../Images/2f7271b1ec59c6a0659d010559a211f1.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f7271b1ec59c6a0659d010559a211f1.png)'
- en: (Image by author)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源：作者）
- en: It is clear that data points are separated as clusters according to their class
    labels. The nearby data points of the same class in the original dimension will
    still be nearby in the lower dimension!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，数据点根据其类别标签被分隔成不同的簇。原始维度中同一类别的附近数据点在较低维度中仍将保持接近！
- en: t-SNE is more effective than PCA for dimensionality reduction as it keeps similar
    data points together (and dissimilar data points apart) in both higher and lower
    dimensions. and works well with non-linear data.
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: t-SNE 在降维方面比 PCA 更有效，因为它在高维和低维中都能将相似的数据点保持在一起（而将不相似的数据点分开），并且在处理非线性数据时表现良好。
- en: PCA before t-SNE — Combining both methods (Very special trick)
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA 之后的 t-SNE — 结合这两种方法（非常特别的技巧）
- en: PCA can’t maintain the spatial relationship between data points after dimensionality
    reduction although it executes really fast compared to t-SNE.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 PCA 执行得非常快，但它无法在降维后保持数据点之间的空间关系。
- en: On the other hand, t-SNE is really slow with larger datasets, but it can preserve
    the spatial relationship between data points after dimensionality reduction.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，t-SNE 在处理较大数据集时确实很慢，但它可以在降维后保持数据点之间的空间关系。
- en: To speed up the computation process of t-SNE, we apply PCA before t-SNE and
    combine both methods as in the following diagram.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加快 t-SNE 的计算过程，我们在 t-SNE 之前应用 PCA，并将两种方法结合，如下图所示。
- en: '![](../Images/61b75660ca3bd1ca2710b13b9bd15347.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61b75660ca3bd1ca2710b13b9bd15347.png)'
- en: '**PCA before t-SNE workflow** (Image by author)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**PCA 之前的 t-SNE 工作流程**（图片来源：作者）'
- en: First, we apply PCA to the MNIST data and reduce dimensionality to 100 (keep
    only 100 dimensions/components/features). Then, we apply t-SNE to the PCA-transformed
    MNIST data. This time, t-SNE only sees 100 features instead of 784 features and
    does not want to perform much computation. Now, t-SNE executes really fast but
    still manages to generate the same or even better results!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们对 MNIST 数据应用 PCA，将维度降至 100（仅保留 100 个维度/成分/特征）。然后，我们对 PCA 转换后的 MNIST 数据应用
    t-SNE。这次，t-SNE 仅看到 100 个特征而不是 784 个特征，因此不需要进行大量计算。现在，t-SNE 执行速度非常快，但仍能生成相同或更好的结果！
- en: By applying PCA before t-SNE, you will get the following benefits.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在 t-SNE 之前应用 PCA，你将获得以下好处。
- en: PCA removes noise in the data and keeps only the most important features in
    the data. By feeding PCA-transformed data into t-SNE, you will get an even better
    output!
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA 去除数据中的噪声，只保留数据中最重要的特征。将 PCA 转换后的数据输入 t-SNE，你将获得更好的输出！
- en: PCA removes multicollinearity between the input features. The PCA-transformed
    data has uncorrelated variables which are fed into t-SNE.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA 去除了输入特征之间的多重共线性。PCA 转换后的数据具有不相关的变量，这些变量被输入到 t-SNE 中。
- en: As I already said, PCA reduces the number of features significantly. The PCA-transformed
    data will be fed into t-SNE ad you will get results very fast!
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如我之前所说，PCA 显著减少了特征的数量。PCA 转换后的数据将输入 t-SNE，你将能非常快速地得到结果！
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/c928cbb4f5e73ed9f6f8b131c18d300b.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c928cbb4f5e73ed9f6f8b131c18d300b.png)'
- en: (Image by author)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: (图片来源：作者)
- en: It took 100 seconds to run TSNE with all 784 features. After applying PCA, it
    only took 20 seconds to run TSNE with PCA-transformed data which has 100 features.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用所有 784 个特征运行 t-SNE 花费了 100 秒。在应用 PCA 后，只需 20 秒就可以用具有 100 个特征的 PCA 转换数据运行 t-SNE。
- en: The PCA-transformed data accurately represents the original MNIST data as the
    first 100 components capture about 90% variance in the original data. We can confirm
    it by looking at the following plot. Therefore, it is reasonable to feed the PCA-transformed
    data to t-SNE in place of the original data.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 转换后的数据准确地表示了原始 MNIST 数据，因为前 100 个组件捕获了原始数据中约 90% 的方差。我们可以通过查看以下图表来确认这一点。因此，用
    PCA 转换后的数据替代原始数据输入 t-SNE 是合理的。
- en: '[PRE8]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/cf459c8709b07f79200f312ca684e15f.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf459c8709b07f79200f312ca684e15f.png)'
- en: (Image by author)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: (图片来源：作者)
- en: Choosing the right value for perplexity
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择合适的困惑度值
- en: Perplexity determines the number of nearest neighbors to consider when visualizing
    data. It is the most important hyperparameter in t-SNE. Therefore, you need to
    tune it properly.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度决定了在可视化数据时要考虑的最近邻居数量。它是 t-SNE 中最重要的超参数。因此，你需要正确地调整它。
- en: One option is that you can plot the KL divergences for different perplexity
    values and analyze how Kl divergence behaves when increasing the value of perplexity.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一种选择是你可以绘制不同困惑度值的 KL 散度，并分析当困惑度值增加时 KL 散度的变化情况。
- en: '[PRE9]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/3827d9ccb25d6fc552f4f980ba8a0332.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3827d9ccb25d6fc552f4f980ba8a0332.png)'
- en: (Image by author)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: (图片来源：作者)
- en: The KL divergence is decreasing continuously when the perplexity value is increased!
    Therefore, we can't decide the right value for perplexity by only analyzing this
    plot.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当困惑度值增加时，KL 散度持续减少！因此，我们不能仅通过分析这个图来决定困惑度的正确值。
- en: As the second option, we need to run the TSNE algorithm several times with different
    perplexity values and visualize the results.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第二种选择，我们需要多次运行 t-SNE 算法，使用不同的困惑度值并可视化结果。
- en: '![](../Images/56c410280a45f1740bf8f74efe35eb5e.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56c410280a45f1740bf8f74efe35eb5e.png)'
- en: (Image by author)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: (图片来源：作者)
- en: At perplexity = 5 (a lower value), clear clusters are formed. But, there is
    not enough space between the clusters and the points within clusters are well
    separated.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在困惑度 = 5（较低值）时，会形成清晰的簇。但簇之间的空间不足，簇内的点分离得很好。
- en: At perplexity = 30, 50 and 100 (right values), the space between the clusters
    increases and the points within clusters concentrate well.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在困惑度 = 30、50 和 100（合适值）时，簇之间的间距增加，而簇内的点集中得很好。
- en: At perplexity = 210 and 500 (excessive values), clusters tend to overlap each
    other, which is not required for us.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在困惑度 = 210 和 500（过高值）时，簇之间往往会重叠，这对我们来说是不需要的。
- en: For the MNIST dataset, any perplexity value between 30 and 50 will be good enough.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 MNIST 数据集，任何介于 30 和 50 之间的困惑度值都足够好。
- en: '**Note:** The optimal value of perplexity highly depends on the nature and
    volume of the data. In general, larger datasets require larger perplexity values.
    It is always good practice to start with the default value which is 30.'
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 困惑度的最佳值高度依赖于数据的性质和数量。通常，较大的数据集需要较大的困惑度值。最好从默认值 30 开始。'
- en: Changing the right number of iterations
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更改合适的迭代次数
- en: Now, we will visualize the effect of the number of iterations used in the gradient
    descent optimization algorithm. TSNE uses stochastic gradient descent to minimize
    the KL divergence cost function.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将可视化梯度下降优化算法中使用的迭代次数的效果。t-SNE 使用随机梯度下降来最小化 KL 散度成本函数。
- en: The number of iterations determines how many times the optimizer performs gradient
    updates during optimization. The default is 1000\. The minimum should be 250.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代次数决定了优化器在优化过程中进行梯度更新的次数。默认值为1000。最小值应为250。
- en: The number of iterations is specified in the ***n_iter*** argument.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代次数在***n_iter***参数中指定。
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In this experiment, we will try out three different values which are 250, 500
    and 1000.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，我们将尝试三种不同的值：250、500和1000。
- en: '**250 iterations**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**250次迭代**'
- en: '![](../Images/63463b84b0308fba48446629185ea641.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63463b84b0308fba48446629185ea641.png)'
- en: (Image by author)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源于作者）
- en: '**500 iterations**'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**500次迭代**'
- en: '![](../Images/36d0989f401e7372a30c5ad5be36bcc9.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36d0989f401e7372a30c5ad5be36bcc9.png)'
- en: (Image by author)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源于作者）
- en: '**1000 iterations**'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**1000次迭代**'
- en: '![](../Images/23f0298b955d036ac5674eabbd56e505.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23f0298b955d036ac5674eabbd56e505.png)'
- en: (Image by author)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源于作者）
- en: After running for 250 iterations, clusters are not clearly formed because the
    cost function is not fully minimized. So, we need to give the algorithm more time
    to run for further optimization. After running for 500 iterations, the separation
    of clusters is very clear. Once optimized, increasing the number of iterations
    will have little to no effect on changing the cluster formation! It will only
    increase the training time! In this case, 500 iterations are sufficient to optimize
    the algorithm.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 运行250次迭代后，簇尚未明显形成，因为成本函数未被完全最小化。因此，我们需要给算法更多时间以进一步优化。运行500次迭代后，簇的分离非常明显。一旦优化，增加迭代次数对改变簇形成几乎没有影响！它只会增加训练时间！在这种情况下，500次迭代足以优化算法。
- en: '**Note:** The selection of the number of iterations highly depends on the nature
    and volume of the data.'
  id: totrans-139
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 迭代次数的选择高度依赖于数据的性质和量。'
- en: Randomness of initialization
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化的随机性
- en: 'There are two types of initialization for TSNE:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: TSNE有两种初始化方式：
- en: '**Random initialization**'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机初始化**'
- en: '[PRE11]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**PCA initialization (default)**'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PCA初始化（默认）**'
- en: '[PRE12]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: PCA initialization is more stable than random initialization and produces the
    same results across different executions.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: PCA初始化比随机初始化更稳定，并且在不同执行中产生相同的结果。
- en: In contrast, random initialization will generate different results at different
    executions as **TSNE has a non-convex cost function** and the GD optimizer may
    stick at a local minimum as in the following diagram.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，随机初始化会在不同执行中生成不同的结果，因为**TSNE具有非凸的成本函数**，并且GD优化器可能会陷入局部最小值，如下图所示。
- en: '![](../Images/e899f18df4def4d415788ddca9c5106e.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e899f18df4def4d415788ddca9c5106e.png)'
- en: (Image by author)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源于作者）
- en: If the initialization occurs near point **A**, the optimizer may stick at the
    local minimum and the cost function will not be fully minimized.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果初始化发生在点**A**附近，优化器可能会停留在局部最小值，成本函数将无法完全最小化。
- en: If the initialization occurs near point **B**, the optimizer may stick at the
    global minimum and this time, TSNE will generate a completely different output
    than in the previous case.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果初始化发生在点**B**附近，优化器可能会停留在全局最小值，这时，TSNE将生成与之前完全不同的输出。
- en: There may be many local minima in the cost functions. Therefore, different initializations
    may tend the algorithm to be stuck at a local minimum. This is the case for random
    initialization and PCA initialization will not suffer from this!
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数中可能存在许多局部最小值。因此，不同的初始化可能会导致算法陷入局部最小值。随机初始化就是这种情况，而PCA初始化不会遭受这种问题！
- en: Using a random state in random initialization
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在随机初始化中使用随机状态
- en: If you still want to get stable results at different executions of the TSNE
    algorithm with the random initialization, specify the ***random_state*** argument
    with an integer like 0, 1, or 42.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仍然想在不同的TSNE算法执行中获得稳定的结果，可以将***random_state***参数指定为整数，如0、1或42。
- en: '[PRE13]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If you run this many times, you will get the same results. However, if you specify
    0 for ***random_state***, you will get different results than in the previous
    case.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行多次，你将会得到相同的结果。然而，如果你将***random_state***指定为0，你会得到不同于之前的结果。
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s see two outputs:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看两个输出：
- en: '**Random state = 42**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机状态 = 42**'
- en: '![](../Images/99b9b3e66709180e5d1afa2cc547ea80.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99b9b3e66709180e5d1afa2cc547ea80.png)'
- en: (Image by author)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源于作者）
- en: '**Random state = 0**'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机状态 = 0**'
- en: '![](../Images/e775ac7ed760f42084be02a0195f5501.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e775ac7ed760f42084be02a0195f5501.png)'
- en: (Image by author)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源于作者）
- en: You will get completely different outputs with different random state values!
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你将会得到完全不同的输出，取决于不同的随机状态值！
- en: '**Note:** Specifying a random state will **not** prevent the algorithm from
    being stuck at a local minimum.'
  id: totrans-166
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 指定随机状态**不会**阻止算法陷入局部最小值。'
- en: This is the end of today’s article.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的文章到此结束。
- en: '**Please let me know if you’ve any questions or feedback.**'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你有任何问题或反馈，请告诉我。**'
- en: Other dimensionality reduction methods you might be interested in
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你可能感兴趣的其他降维方法
- en: '[**Principal Component Analysis (PCA)**](https://medium.com/data-science-365/3-easy-steps-to-perform-dimensionality-reduction-using-principal-component-analysis-pca-79121998b991)'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**主成分分析（PCA）**](https://medium.com/data-science-365/3-easy-steps-to-perform-dimensionality-reduction-using-principal-component-analysis-pca-79121998b991)'
- en: '[**Factor Analysis (FA)**](/factor-analysis-on-women-track-records-data-with-r-and-python-6731a73cd2e0)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**因子分析（FA）**](/factor-analysis-on-women-track-records-data-with-r-and-python-6731a73cd2e0)'
- en: '[**Linear Discriminant Analysis (LDA)**](/lda-is-highly-effective-than-pca-for-dimensionality-reduction-in-classification-datasets-4489eade632)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**线性判别分析（LDA）**](/lda-is-highly-effective-than-pca-for-dimensionality-reduction-in-classification-datasets-4489eade632)'
- en: '[**Non-Negative Matrix Factorization (NMF)**](/non-negative-matrix-factorization-nmf-for-dimensionality-reduction-in-image-data-8450f4cae8fa)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**非负矩阵分解（NMF）**](/non-negative-matrix-factorization-nmf-for-dimensionality-reduction-in-image-data-8450f4cae8fa)'
- en: '[**Autoencoders (AEs)**](/how-autoencoders-outperform-pca-in-dimensionality-reduction-1ae44c68b42f)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**自编码器（AEs）**](/how-autoencoders-outperform-pca-in-dimensionality-reduction-1ae44c68b42f)'
- en: '[**Kernel PCA**](/dimensionality-reduction-for-linearly-inseparable-data-5030f0dc0f5e)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**核主成分分析**](/dimensionality-reduction-for-linearly-inseparable-data-5030f0dc0f5e)'
- en: Read next (Recommended)
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 继续阅读（推荐）
- en: '[**PCA and Dimensionality Reduction Special Collection**](https://rukshanpramoditha.medium.com/list/pca-and-dimensionality-reduction-special-collection-146045a5acb5)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**PCA 和维度降维特别合集**](https://rukshanpramoditha.medium.com/list/pca-and-dimensionality-reduction-special-collection-146045a5acb5)'
- en: How about an AI course?
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 怎么样，来一门 AI 课程？
- en: '[**Neural Networks and Deep Learning Course**](https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**神经网络与深度学习课程**](https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75)'
- en: Support me as a writer
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持我作为一名写作者
- en: '*I hope you enjoyed reading this article. If you’d like to support me as a
    writer, kindly consider* [***signing up for a membership***](https://rukshanpramoditha.medium.com/membership)
    *to get unlimited access to Medium. It only costs $5 per month and I will receive
    a portion of your membership fee.*'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*希望你喜欢阅读这篇文章。如果你愿意支持我作为一名写作者，请考虑* [***注册会员***](https://rukshanpramoditha.medium.com/membership)
    *以获得无限访问 Medium 的权限。每月只需 $5，我将获得部分会员费用。*'
- en: '[](https://rukshanpramoditha.medium.com/membership?source=post_page-----7a3975e8cbdb--------------------------------)
    [## Join Medium with my referral link - Rukshan Pramoditha'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://rukshanpramoditha.medium.com/membership?source=post_page-----7a3975e8cbdb--------------------------------)
    [## 通过我的推荐链接加入 Medium - Rukshan Pramoditha'
- en: Read every story from Rukshan Pramoditha (and thousands of other writers on
    Medium). Your membership fee directly…
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读 Rukshan Pramoditha 的每一篇故事（以及 Medium 上的其他成千上万的作者）。你的会员费直接…
- en: rukshanpramoditha.medium.com](https://rukshanpramoditha.medium.com/membership?source=post_page-----7a3975e8cbdb--------------------------------)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[rukshanpramoditha.medium.com](https://rukshanpramoditha.medium.com/membership?source=post_page-----7a3975e8cbdb--------------------------------)'
- en: Join my private list of emails
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我的私人邮件列表
- en: '*Never miss a great story from me again. By* [***subscribing to my email list***](https://rukshanpramoditha.medium.com/subscribe)*,
    you will directly receive my stories as soon as I publish them.*'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*不要再错过我的精彩故事。通过* [***订阅我的邮件列表***](https://rukshanpramoditha.medium.com/subscribe)*，你将直接在我发布故事时第一时间收到。*'
- en: Thank you so much for your continuous support! See you in the next article.
    Happy learning to everyone!
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢你的持续支持！下篇文章见。祝大家学习愉快！
- en: MNIST dataset info
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MNIST 数据集信息
- en: '**Citation:** Deng, L., 2012\. The mnist database of handwritten digit images
    for machine learning research. **IEEE Signal Processing Magazine**, 29(6), pp.
    141–142.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**引用：** Deng, L., 2012\. 用于机器学习研究的手写数字图像 MNIST 数据库。**IEEE 信号处理杂志**，29(6)，第
    141–142 页。'
- en: '**Source:** [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**来源：** [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
- en: '**License:** *Yann LeCun* (Courant Institute, NYU) and *Corinna Cortes* (Google
    Labs, New York) hold the copyright of the MNIST dataset which is available under
    the *Creative Commons Attribution-ShareAlike 4.0 International License* ([**CC
    BY-SA**](https://creativecommons.org/licenses/by-sa/4.0/)). You can learn more
    about different dataset license types [here](https://rukshanpramoditha.medium.com/dataset-and-software-license-types-you-need-to-consider-d20965ca43dc#6ade).'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**许可：** *Yann LeCun*（纽约大学Courant Institute）和 *Corinna Cortes*（谷歌实验室，纽约）持有MNIST数据集的版权，该数据集采用*Creative
    Commons Attribution-ShareAlike 4.0 International License*（[**CC BY-SA**](https://creativecommons.org/licenses/by-sa/4.0/)）。你可以在[这里](https://rukshanpramoditha.medium.com/dataset-and-software-license-types-you-need-to-consider-d20965ca43dc#6ade)了解更多关于不同数据集许可类型的信息。'
- en: '**Designed and written by:** [Rukshan Pramoditha](https://medium.com/u/f90a3bb1d400?source=post_page-----7a3975e8cbdb--------------------------------)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**设计及编写者：** [Rukshan Pramoditha](https://medium.com/u/f90a3bb1d400?source=post_page-----7a3975e8cbdb--------------------------------)'
- en: '**2023–05–23**'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**2023–05–23**'
