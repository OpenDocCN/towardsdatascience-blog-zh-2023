["```py\nimport numpy as np\n\ndef f(x):\n    '''Objective function'''\n    return 0.5*(x[0] - 4.5)**2 + 2.5*(x[1] - 2.3)**2\n\ndef df(x):\n    '''Gradient of the objective function'''\n    return np.array([x[0] - 4.5, 5*(x[1] - 2.3)])\n```", "```py\nfrom scipy.optimize import minimize\n\nresult = minimize(\n    f, np.zeros(2), method='trust-constr', jac=df)\n\nresult.x\n```", "```py\narray([4.5, 2.3])\n```", "```py\nimport matplotlib.pyplot as plt\n\n# Prepare the objective function between -10 and 10\nX, Y = np.meshgrid(np.linspace(-10, 10, 20), np.linspace(-10, 10, 20))\nZ = f(np.array([X, Y]))\n\n# Minimizer\nmin_x0, min_x1 = np.meshgrid(result.x[0], result.x[1])   \nmin_z = f(np.stack([min_x0, min_x1]))\n\n# Plot\nfig = plt.figure(figsize=(15, 20))\n\n# First subplot\nax = fig.add_subplot(1, 2, 1, projection='3d')\nax.contour3D(X, Y, Z, 60, cmap='viridis')\nax.scatter(min_x0, min_x1, min_z, marker='o', color='red', linewidth=10)\nax.set_xlabel('$x_{0}$')\nax.set_ylabel('$x_{1}$')\nax.set_zlabel('$f(x)$')\nax.view_init(40, 20)\n\n# Second subplot\nax = fig.add_subplot(1, 2, 2, projection='3d')\nax.contour3D(X, Y, Z, 60, cmap='viridis')\nax.scatter(min_x0, min_x1, min_z, marker='o', color='red', linewidth=10)\nax.set_xlabel('$x_{0}$')\nax.set_ylabel('$x_{1}$')\nax.set_zlabel('$f(x)$')\nax.axes.zaxis.set_ticklabels([])\nax.view_init(90, -90);\n```", "```py\ndef steepest_descent(gradient, x0 = np.zeros(2), alpha = 0.01, max_iter = 10000, tolerance = 1e-10): \n    '''\n    Steepest descent with constant step size alpha.\n\n    Args:\n      - gradient: gradient of the objective function\n      - alpha: line search parameter (default: 0.01)\n      - x0: initial guess for x_0 and x_1 (default values: zero) <numpy.ndarray>\n      - max_iter: maximum number of iterations (default: 10000)\n      - tolerance: minimum gradient magnitude at which the algorithm stops (default: 1e-10)\n\n    Out:\n      - results: <numpy.ndarray> of size (n_iter, 2) with x_0 and x_1 values at each iteration\n      - number of steps: <int>\n    '''\n\n    # Prepare list to store results at each iteration \n    results = np.array([])\n\n    # Evaluate the gradient at the starting point \n    gradient_x = gradient(x0)\n\n    # Initialize the steps counter \n    steps_count = 0\n\n    # Set the initial point \n    x = x0 \n    results = np.append(results, x, axis=0)\n\n    # Iterate until the gradient is below the tolerance or maximum number of iterations is reached\n    # Stopping criterion: inf norm of the gradient (max abs)\n    while any(abs(gradient_x) > tolerance) and steps_count < max_iter:\n\n        # Update the step size through the Armijo condition\n        # Note: the first value of alpha is commonly set to 1\n        #alpha = line_search(1, x, gradient_x)\n\n        # Update the current point by moving in the direction of the negative gradient \n        x = x - alpha * gradient_x\n\n        # Store the result\n        results = np.append(results, x, axis=0)\n\n        # Evaluate the gradient at the new point \n        gradient_x = gradient(x) \n\n        # Increment the iteration counter \n        steps_count += 1 \n\n    # Return the steps taken and the number of steps\n    return results.reshape(-1, 2), steps_count\n```", "```py\n# Steepest descent\npoints, iters = steepest_descent(\n  df, x0 = np.array([-9, -9]), alpha=0.30)\n\n# Found minimizer\nminimizer = points[-1].round(1)\n\n# Print results\nprint('- Final results: {}'.format(minimizer))\nprint('- N° steps: {}'.format(iters))\n```", "```py\nFinal results: [4.5 2.3]\nN° steps: 72\n```", "```py\n# Steepest descent steps\nX_estimate, Y_estimate = points[:, 0], points[:, 1] \nZ_estimate = f(np.array([X_estimate, Y_estimate]))\n\n# Plot\nfig = plt.figure(figsize=(20, 20))\n\n# First subplot\nax = fig.add_subplot(1, 2, 1, projection='3d')\nax.contour3D(X, Y, Z, 60, cmap='viridis')\nax.plot(X_estimate, Y_estimate, Z_estimate, color='red', linewidth=3)\nax.scatter(min_x0, min_x1, min_z, marker='o', color='red', linewidth=10)\nax.set_xlabel('$x_{0}$')\nax.set_ylabel('$x_{1}$')\nax.set_zlabel('$f(x)$')\nax.view_init(20, 20)\n\n# Second subplot\nax = fig.add_subplot(1, 2, 2, projection='3d')\nax.contour3D(X, Y, Z, 60, cmap='viridis')\nax.plot(X_estimate, Y_estimate, Z_estimate, color='red', linewidth=3)\nax.scatter(min_x0, min_x1, min_z, marker='o', color='red', linewidth=10)\nax.set_xlabel('$x_{0}$')\nax.set_ylabel('$x_{1}$')\nax.set_zlabel('$f(x)$')\nax.axes.zaxis.set_ticklabels([])\nax.view_init(90, -90);\n```", "```py\n# Step sizes to be tested\nalphas = [0.01, 0.25, 0.3, 0.35, 0.4]\n\n# Store the iterations for each step size\nX_estimates, Y_estimates, Z_estimates = [], [], []\n\n# Plot f(x) at each iteration for different step sizes\nfig, ax = plt.subplots(len(alphas), figsize=(8, 9))\nfig.suptitle('$f(x)$ at each iteration for different $α$')\n\n# For each step size\nfor i, alpha in enumerate(alphas):\n\n    # Steepest descent\n    estimate, iters = steepest_descent(\n      df, x0 = np.array([-5, -5]), alpha=alpha, max_iter=3000)\n\n    # Print results\n    print('Input alpha: {}'.format(alpha))\n    print('\\t- Final results: {}'.format(estimate[-1].round(1)))\n    print('\\t- N° steps: {}'.format(iters))\n\n    # Store for 3D plots\n    X_estimates.append(estimate[:, 0])\n    Y_estimates.append(estimate[:, 1])  \n    Z_estimates.append(f(np.array([estimate[:, 0], estimate[:, 1]])))\n\n    # Subplot of f(x) at each iteration for current alpha\n    ax[i].plot([f(var) for var in estimate], label='alpha: '+str(alpha))\n    ax[i].axhline(y=0, color='r', alpha=0.7, linestyle='dashed')\n    ax[i].set_xlabel('Number of iterations')\n    ax[i].set_ylabel('$f(x)$')\n    ax[i].set_ylim([-10, 200])\n    ax[i].legend(loc='upper right')\n```", "```py\nInput alpha: 0.01\n - Final results: [4.5 2.3]\n - N° steps: 2516\nInput alpha: 0.25\n - Final results: [4.5 2.3]\n - N° steps: 88\nInput alpha: 0.3\n - Final results: [4.5 2.3]\n - N° steps: 71\nInput alpha: 0.35\n - Final results: [4.5 2.3]\n - N° steps: 93\nInput alpha: 0.4\n - Final results: [ 4.5 -5\\. ]\n - N° steps: 3000\n```", "```py\nfig = plt.figure(figsize=(25, 60))\n\n# For each step size\nfor i in range(0, len(alphas)):\n\n    # First subplot\n    ax = fig.add_subplot(5, 2, (i*2)+1, projection='3d')\n    ax.contour3D(X, Y, Z, 60, cmap='viridis')\n    ax.plot(X_estimates[i], Y_estimates[i], Z_estimates[i], color='red', label='alpha: '+str(alphas[i]) , linewidth=3)\n    ax.scatter(min_x0, min_x1, min_z, marker='o', color='red', linewidth=10)\n    ax.set_xlabel('$x_{0}$')\n    ax.set_ylabel('$x_{1}$')\n    ax.set_zlabel('$f(x)$')\n    ax.view_init(20, 20)\n    plt.legend(prop={'size': 15})\n\n    # Second third\n    ax = fig.add_subplot(5, 2, (i*2)+2, projection='3d')\n    ax.contour3D(X, Y, Z, 60, cmap='viridis')\n    ax.plot(X_estimates[i], Y_estimates[i], Z_estimates[i], color='red', label='alpha: '+str(alphas[i]) , linewidth=3)\n    ax.scatter(min_x0, min_x1, min_z, marker='o', color='red', linewidth=10)\n    ax.set_xlabel('$x_{0}$')\n    ax.set_ylabel('$x_{1}$')\n    ax.set_zlabel('$f(x)$')\n    ax.axes.zaxis.set_ticklabels([])\n    ax.view_init(90, -90)\n    plt.legend(prop={'size': 15})\n```", "```py\ndef line_search(step, x, gradient_x, c = 1e-4, tol = 1e-8):\n    '''\n    Inexact line search where the step length is updated through the Armijo condition:\n    $ f (x_k + α * p_k ) ≤ f ( x_k ) + c * α * ∇ f_k^T * p_k $\n\n    Args:\n      - step: starting alpha value\n      - x: current point\n      - gradient_x: gradient of the current point\n      - c: constant value (default: 1e-4)\n      - tol: tolerance value (default: 1e-6)\n    Out:\n      - New value of step: the first value found respecting the Armijo condition\n    '''\n    f_x = f(x)\n    gradient_square_norm = np.linalg.norm(gradient_x)**2\n\n    # Until the sufficient decrease condition is met \n    while f(x - step * gradient_x) >= (f_x - c * step * gradient_square_norm):\n\n        # Update the stepsize (backtracking)\n        step /= 2\n\n        # If the step size falls below a certain tolerance, exit the loop\n        if step < tol:\n            break\n\n    return step\n\ndef steepest_descent(gradient, x0 = np.zeros(2), max_iter = 10000, tolerance = 1e-10): \n    '''\n    Steepest descent with alpha updated through line search (Armijo condition).\n\n    Args:\n      - gradient: gradient of the objective function\n      - x0: initial guess for x_0 and x_1 (default values: zero) <numpy.ndarray>\n      - max_iter: maximum number of iterations (default: 10000)\n      - tolerance: minimum gradient magnitude at which the algorithm stops (default: 1e-10)\n\n    Out:\n      - results: <numpy.ndarray> with x_0 and x_1 values at each iteration\n      - number of steps: <int>\n    '''\n\n    # Prepare list to store results at each iteration \n    results = np.array([])\n\n    # Evaluate the gradient at the starting point \n    gradient_x = gradient(x0)\n\n    # Initialize the steps counter \n    steps_count = 0\n\n    # Set the initial point \n    x = x0 \n    results = np.append(results, x, axis=0)\n\n    # Iterate until the gradient is below the tolerance or maximum number of iterations is reached\n    # Stopping criterion: inf norm of the gradient (max abs)\n    while any(abs(gradient_x) > tolerance) and steps_count < max_iter:\n\n        # Update the step size through the Armijo condition\n        # Note: the first value of alpha is commonly set to 1\n        alpha = line_search(1, x, gradient_x)\n\n        # Update the current point by moving in the direction of the negative gradient \n        x = x - alpha * gradient_x\n\n        # Store the result\n        results = np.append(results, x, axis=0)\n\n        # Evaluate the gradient at the new point \n        gradient_x = gradient(x) \n\n        # Increment the iteration counter \n        steps_count += 1 \n\n    # Return the steps taken and the number of steps\n    return results.reshape(-1, 2), steps_count\n```", "```py\n# Steepest descent\npoints, iters = steepest_descent(\n  df, x0 = np.array([-9, -9]))\n\n# Found minimizer\nminimizer = points[-1].round(1)\n\n# Print results\nprint('- Final results: {}'.format(minimizer))\nprint('- N° steps: {}'.format(iters))\n\n# Steepest descent steps\nX_estimate, Y_estimate = points[:, 0], points[:, 1] \nZ_estimate = f(np.array([X_estimate, Y_estimate]))\n\n# Plot\nfig = plt.figure(figsize=(20, 20))\n\n# First subplot\nax = fig.add_subplot(1, 2, 1, projection='3d')\nax.contour3D(X, Y, Z, 60, cmap='viridis')\nax.plot(X_estimate, Y_estimate, Z_estimate, color='red', linewidth=3)\nax.scatter(min_x0, min_x1, min_z, marker='o', color='red', linewidth=10)\nax.set_xlabel('$x_{0}$')\nax.set_ylabel('$x_{1}$')\nax.set_zlabel('$f(x)$')\nax.view_init(20, 20)\n\n# Second subplot\nax = fig.add_subplot(1, 2, 2, projection='3d')\nax.contour3D(X, Y, Z, 60, cmap='viridis')\nax.plot(X_estimate, Y_estimate, Z_estimate, color='red', linewidth=3)\nax.scatter(min_x0, min_x1, min_z, marker='o', color='red', linewidth=10)\nax.set_xlabel('$x_{0}$')\nax.set_ylabel('$x_{1}$')\nax.set_zlabel('$f(x)$')\nax.axes.zaxis.set_ticklabels([])\nax.view_init(90, -90);\n```", "```py\n- Final results: [4.5 2.3]\n- N° steps: 55\n```"]