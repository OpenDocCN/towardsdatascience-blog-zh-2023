- en: 'TSMixer: The Latest Forecasting Model by Google'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'TSMixer: è°·æ­Œæ¨å‡ºçš„æœ€æ–°é¢„æµ‹æ¨¡å‹'
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/tsmixer-the-latest-forecasting-model-by-google-2fd1e29a8ccb](https://towardsdatascience.com/tsmixer-the-latest-forecasting-model-by-google-2fd1e29a8ccb)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/tsmixer-the-latest-forecasting-model-by-google-2fd1e29a8ccb](https://towardsdatascience.com/tsmixer-the-latest-forecasting-model-by-google-2fd1e29a8ccb)
- en: Explore the architecture of TSMixer and implement it in Python for a long-horizon
    multivariate forecasting task
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¢ç´¢TSMixerçš„æ¶æ„ï¼Œå¹¶åœ¨Pythonä¸­å®ç°å®ƒï¼Œç”¨äºé•¿æœŸå¤šå˜é‡é¢„æµ‹ä»»åŠ¡ã€‚
- en: '[](https://medium.com/@marcopeixeiro?source=post_page-----2fd1e29a8ccb--------------------------------)[![Marco
    Peixeiro](../Images/7cf0a81d87281d35ff47f51e3026a3e9.png)](https://medium.com/@marcopeixeiro?source=post_page-----2fd1e29a8ccb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2fd1e29a8ccb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2fd1e29a8ccb--------------------------------)
    [Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page-----2fd1e29a8ccb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcopeixeiro?source=post_page-----2fd1e29a8ccb--------------------------------)[![Marco
    Peixeiro](../Images/7cf0a81d87281d35ff47f51e3026a3e9.png)](https://medium.com/@marcopeixeiro?source=post_page-----2fd1e29a8ccb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2fd1e29a8ccb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2fd1e29a8ccb--------------------------------)
    [Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page-----2fd1e29a8ccb--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2fd1e29a8ccb--------------------------------)
    Â·12 min readÂ·Nov 14, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----2fd1e29a8ccb--------------------------------)
    Â·é˜…è¯»æ—¶é—´12åˆ†é’ŸÂ·2023å¹´11æœˆ14æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e73873566af0876cf840a6284a7f2f21.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e73873566af0876cf840a6284a7f2f21.png)'
- en: Photo by [ZdenÄ›k MachÃ¡Äek](https://unsplash.com/@zmachacek?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±[ZdenÄ›k MachÃ¡Äek](https://unsplash.com/@zmachacek?utm_source=medium&utm_medium=referral)æ‹æ‘„çš„ç…§ç‰‡ï¼Œæ¥è‡ª[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The field of time series forecasting continues to be in effervescence, with
    many important recent contributions like N-HiTS, PatchTST, TimesNet and of course
    TimeGPT.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸä»åœ¨è“¬å‹ƒå‘å±•ï¼Œæœ€è¿‘æœ‰è®¸å¤šé‡è¦çš„è´¡çŒ®ï¼Œå¦‚N-HiTSã€PatchTSTã€TimesNetä»¥åŠå½“ç„¶è¿˜æœ‰TimeGPTã€‚
- en: In the meantime, the Transformer architecture unlocked unprecedented performance
    in the field of natural language processing (NLP), but that is not true for time
    series forecasting.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ­¤åŒæ—¶ï¼ŒTransformeræ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå®ç°äº†å‰æ‰€æœªæœ‰çš„æ€§èƒ½ï¼Œä½†åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­å¹¶éå¦‚æ­¤ã€‚
- en: In fact, many Transformer-based model were proposed like Autoformer, Informer,
    FEDformer, and more. Those models are often very long to train and it turns out
    that simple linear models outperform them on many benchmark datasets (see [Zheng
    et al., 2022](https://arxiv.org/pdf/2205.13504.pdf)).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: äº‹å®ä¸Šï¼Œè®¸å¤šåŸºäºTransformerçš„æ¨¡å‹å¦‚Autoformerã€Informerã€FEDformerç­‰è¢«æå‡ºã€‚è¿™äº›æ¨¡å‹é€šå¸¸è®­ç»ƒæ—¶é—´éå¸¸é•¿ï¼Œè€Œç®€å•çš„çº¿æ€§æ¨¡å‹åœ¨è®¸å¤šåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°æ›´å¥½ï¼ˆè§[Zheng
    et al., 2022](https://arxiv.org/pdf/2205.13504.pdf)ï¼‰ã€‚
- en: To that point, in September 2023, researchers from Google Cloud AI Research
    proposed **TSMixer**, a Multi-layer Perceptron (MLP) based model that focuses
    on mixing time and feature dimensions to make better predictions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: äº‹å®ä¸Šï¼Œåœ¨2023å¹´9æœˆï¼Œè°·æ­Œäº‘AIç ”ç©¶äººå‘˜æå‡ºäº†**TSMixer**ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰çš„æ¨¡å‹ï¼Œä¸“æ³¨äºæ··åˆæ—¶é—´å’Œç‰¹å¾ç»´åº¦ï¼Œä»¥æä¾›æ›´å¥½çš„é¢„æµ‹ã€‚
- en: 'In their paper [TSMixer: An All-MLP Architecture for Time Series Forecasting](https://arxiv.org/pdf/2303.06053.pdf),
    the authors demonstrate that this model achieves state-of-the-art performance
    on many benchmark datasets, while remaining simple to implement.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨ä»–ä»¬çš„è®ºæ–‡[TSMixer: An All-MLP Architecture for Time Series Forecasting](https://arxiv.org/pdf/2303.06053.pdf)ä¸­ï¼Œä½œè€…å±•ç¤ºäº†è¯¥æ¨¡å‹åœ¨è®¸å¤šåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†å®ç°çš„ç®€å•æ€§ã€‚'
- en: In this article, we first explore the architecture of TSMixer to understand
    its inner workings. Then, we implement the model in Python and run our own experiment
    to compare its performance to N-HiTS.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæ¢ç´¢TSMixerçš„æ¶æ„ï¼Œä»¥ç†è§£å…¶å†…éƒ¨å·¥ä½œåŸç†ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨Pythonä¸­å®ç°è¯¥æ¨¡å‹ï¼Œå¹¶è¿è¡Œè‡ªå·±çš„å®éªŒï¼Œå°†å…¶æ€§èƒ½ä¸N-HiTSè¿›è¡Œæ¯”è¾ƒã€‚
- en: For more details on TSMixer, make sure to read the [original paper](https://arxiv.org/pdf/2303.06053.pdf).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ¬²äº†è§£æœ‰å…³TSMixerçš„æ›´å¤šç»†èŠ‚ï¼Œè¯·åŠ¡å¿…é˜…è¯»[åŸå§‹è®ºæ–‡](https://arxiv.org/pdf/2303.06053.pdf)ã€‚
- en: '**Learn the latest time series analysis techniques with my** [**free time series
    cheat sheet**](https://www.datasciencewithmarco.com/pl/2147608294) **in Python!
    Get the implementation of statistical and deep learning techniques, all in Python
    and TensorFlow!**'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**é€šè¿‡æˆ‘çš„** [**å…è´¹æ—¶é—´åºåˆ—å¤‡å¿˜å•**](https://www.datasciencewithmarco.com/pl/2147608294)
    **ï¼Œ** **å­¦ä¹ æœ€æ–°çš„æ—¶é—´åºåˆ—åˆ†ææŠ€æœ¯ï¼è·å–ç»Ÿè®¡å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„å®ç°ï¼Œå…¨éƒ¨ç”¨Pythonå’ŒTensorFlowï¼**'
- en: Letâ€™s get started!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¼€å§‹å§ï¼
- en: Explore TSMixer
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¢ç´¢TSMixer
- en: When it comes to forecasting, we intuitively know that using cross-variate information
    can help make better predictions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é¢„æµ‹æ–¹é¢ï¼Œæˆ‘ä»¬ç›´è§‚åœ°çŸ¥é“ï¼Œä½¿ç”¨äº¤å‰å˜é‡ä¿¡æ¯å¯ä»¥å¸®åŠ©åšå‡ºæ›´å¥½çš„é¢„æµ‹ã€‚
- en: For example, weather and precipitation are likely to have an impact on the number
    of visitors to an amusement park. Likewise, the days of the week and holidays
    will also have an impact.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¤©æ°”å’Œé™æ°´é‡å¯èƒ½ä¼šå½±å“æ¸¸ä¹å›­çš„è®¿å®¢æ•°é‡ã€‚åŒæ ·ï¼Œæ˜ŸæœŸå‡ å’ŒèŠ‚å‡æ—¥ä¹Ÿä¼šäº§ç”Ÿå½±å“ã€‚
- en: Therefore, it would make sense to have models that can leverage information
    from covariates and other features to make predictions.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ‹¥æœ‰èƒ½å¤Ÿåˆ©ç”¨åå˜é‡å’Œå…¶ä»–ç‰¹å¾ä¿¡æ¯è¿›è¡Œé¢„æµ‹çš„æ¨¡å‹æ˜¯æœ‰æ„ä¹‰çš„ã€‚
- en: This is what motivated the creation of TSMixer. As simple univariate linear
    models were shown to outperform more complex architectures (see [Zheng et al.,
    2022](https://arxiv.org/pdf/2205.13504.pdf)), TSMixer now extends the capabilities
    of linear models by adding cross-variate feed-forward layers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ¿€å‘äº†TSMixerçš„åˆ›å»ºã€‚ç”±äºç®€å•çš„å•å˜é‡çº¿æ€§æ¨¡å‹è¢«è¯æ˜ä¼˜äºæ›´å¤æ‚çš„æ¶æ„ï¼ˆè§ [Zheng et al., 2022](https://arxiv.org/pdf/2205.13504.pdf)ï¼‰ï¼ŒTSMixerç°åœ¨é€šè¿‡æ·»åŠ äº¤å‰å˜é‡å‰é¦ˆå±‚æ¥æ‰©å±•çº¿æ€§æ¨¡å‹çš„èƒ½åŠ›ã€‚
- en: Therefore, we now get a linear model capable of handling multivariate forecasting
    that can leverage information from covariates and other static features.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬ç°åœ¨å¾—åˆ°ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†å¤šå˜é‡é¢„æµ‹çš„çº¿æ€§æ¨¡å‹ï¼Œå®ƒå¯ä»¥åˆ©ç”¨åå˜é‡å’Œå…¶ä»–é™æ€ç‰¹å¾çš„ä¿¡æ¯ã€‚
- en: Architecture of TSMixer
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TSMixerçš„æ¶æ„
- en: The general architecture is shown in the figure below.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¶æ„å¦‚ä¸‹é¢çš„å›¾æ‰€ç¤ºã€‚
- en: '![](../Images/d91af3dd374db62e24ad58bc3635b890.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d91af3dd374db62e24ad58bc3635b890.png)'
- en: 'Architecture of TSMixer. Image by S. Chen, C. Li, N. Yoder, S. Arik and T.
    Pfister from [TSMixer: An All-MLP Architecture for Time Series Forecasting](https://arxiv.org/pdf/2303.06053.pdf)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 'TSMixerçš„æ¶æ„ã€‚å›¾åƒæ¥æºï¼šS. Chenã€C. Liã€N. Yoderã€S. Arik å’Œ T. Pfisterï¼Œæ¥è‡ª [TSMixer: An
    All-MLP Architecture for Time Series Forecasting](https://arxiv.org/pdf/2303.06053.pdf)'
- en: Since TSMixer is simply extending linear models, its architecture is fairly
    straightforward, since it is entirely MLP-based.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºTSMixeråªæ˜¯æ‰©å±•çº¿æ€§æ¨¡å‹ï¼Œå®ƒçš„æ¶æ„ç›¸å½“ç®€å•ï¼Œå› ä¸ºå®ƒå®Œå…¨åŸºäºMLPã€‚
- en: 'From the figure above, we can see that the model mainly consists of two steps:
    a mixer layer and a temporal projection.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šå›¾å¯ä»¥çœ‹å‡ºï¼Œè¯¥æ¨¡å‹ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ­¥éª¤ï¼šæ··åˆå±‚å’Œæ—¶é—´æŠ•å½±ã€‚
- en: Letâ€™s explore each step in more detail.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ›´è¯¦ç»†åœ°æ¢è®¨æ¯ä¸€æ­¥ã€‚
- en: Mixer layer
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ··åˆå±‚
- en: This is where time mixing and feature mixing occurs, hence the name TSMixer.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æ—¶é—´æ··åˆå’Œç‰¹å¾æ··åˆå‘ç”Ÿçš„åœ°æ–¹ï¼Œå› æ­¤å‘½åä¸ºTSMixerã€‚
- en: '![](../Images/b06f726222fad431f6ffb69063c8e599.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b06f726222fad431f6ffb69063c8e599.png)'
- en: 'Focusing on the mixing layer. Image by S. Chen, C. Li, N. Yoder, S. Arik and
    T. Pfister from [TSMixer: An All-MLP Architecture for Time Series Forecasting](https://arxiv.org/pdf/2303.06053.pdf)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸“æ³¨äºæ··åˆå±‚ã€‚å›¾åƒæ¥æºï¼šS. Chenã€C. Liã€N. Yoderã€S. Arik å’Œ T. Pfisterï¼Œæ¥è‡ª [TSMixer: An All-MLP
    Architecture for Time Series Forecasting](https://arxiv.org/pdf/2303.06053.pdf)'
- en: In the figure above, we see that for time mixing, the MLP consists of a fully
    connected layer, followed by the ReLU activation function, and a dropout layer.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°å¯¹äºæ—¶é—´æ··åˆï¼ŒMLPåŒ…æ‹¬ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼Œéšåæ˜¯ReLUæ¿€æ´»å‡½æ•°å’Œä¸€ä¸ªdropoutå±‚ã€‚
- en: The input, where rows represent time and columns represent features, is transposed
    so the MLP is applied on the time domain and shared across all features. This
    unit is responsible for learning temporal patterns.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥æ•°æ®ä¸­ï¼Œè¡Œä»£è¡¨æ—¶é—´ï¼Œåˆ—ä»£è¡¨ç‰¹å¾ï¼Œæ•°æ®è¢«è½¬ç½®ï¼Œä»¥ä¾¿MLPåº”ç”¨äºæ—¶é—´åŸŸï¼Œå¹¶åœ¨æ‰€æœ‰ç‰¹å¾ä¸­å…±äº«ã€‚è¿™ä¸ªå•å…ƒè´Ÿè´£å­¦ä¹ æ—¶é—´æ¨¡å¼ã€‚
- en: Before leaving the time mixing unit, the matrix is transposed again, and it
    is sent to the feature mixing unit.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¦»å¼€æ—¶é—´æ··åˆå•å…ƒä¹‹å‰ï¼ŒçŸ©é˜µä¼šå†æ¬¡è½¬ç½®ï¼Œç„¶åå‘é€åˆ°ç‰¹å¾æ··åˆå•å…ƒã€‚
- en: The feature mixing unit, then consists of two MLPs. Since it is applied in the
    feature domain, it is shared across all time steps. Here, there is no need to
    transpose, since the features are already on the horizontal axis.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾æ··åˆå•å…ƒåŒ…æ‹¬ä¸¤ä¸ªMLPã€‚ç”±äºå®ƒåœ¨ç‰¹å¾åŸŸä¸­åº”ç”¨ï¼Œå› æ­¤åœ¨æ‰€æœ‰æ—¶é—´æ­¥é•¿ä¸­å…±äº«ã€‚åœ¨è¿™é‡Œï¼Œæ²¡æœ‰å¿…è¦è½¬ç½®ï¼Œå› ä¸ºç‰¹å¾å·²ç»åœ¨æ°´å¹³è½´ä¸Šã€‚
- en: Notice that in both mixers, we have normalization layers and residual connections.
    The latter help the model learn deeper representations of the data while keeping
    the computational cost reasonable, while normalization is a common technique to
    improve the training of deep learning models.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œåœ¨ä¸¤ä¸ªæ··åˆå™¨ä¸­ï¼Œæˆ‘ä»¬éƒ½æœ‰å½’ä¸€åŒ–å±‚å’Œæ®‹å·®è¿æ¥ã€‚åè€…å¸®åŠ©æ¨¡å‹å­¦ä¹ æ•°æ®çš„æ›´æ·±å±‚æ¬¡è¡¨ç¤ºï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æˆæœ¬åˆç†ï¼Œè€Œå½’ä¸€åŒ–æ˜¯æé«˜æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒçš„å¸¸ç”¨æŠ€æœ¯ã€‚
- en: Once that mixing is done, the output is sent to the temporal projection step.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ··åˆå®Œæˆï¼Œè¾“å‡ºå°†å‘é€åˆ°æ—¶é—´æŠ•å½±æ­¥éª¤ã€‚
- en: Temporal projection
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ—¶é—´æŠ•å½±
- en: The temporal projection step is what generates the predictions in TSMixer.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¶é—´æŠ•å½±æ­¥éª¤æ˜¯ç”ŸæˆTSMixeré¢„æµ‹çš„éƒ¨åˆ†ã€‚
- en: '![](../Images/001d4503e1f393a0b44601e994fe95ac.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/001d4503e1f393a0b44601e994fe95ac.png)'
- en: 'Focusing on the temporal projection layer. Image by S. Chen, C. Li, N. Yoder,
    S. Arik and T. Pfister from [TSMixer: An All-MLP Architecture for Time Series
    Forecasting](https://arxiv.org/pdf/2303.06053.pdf)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 'é‡ç‚¹å…³æ³¨æ—¶é—´æŠ•å½±å±‚ã€‚å›¾åƒæ¥è‡ª**S. Chen**ã€**C. Li**ã€**N. Yoder**ã€**S. Arik**å’Œ**T. Pfister**ï¼Œè§[TSMixer:
    An All-MLP Architecture for Time Series Forecasting](https://arxiv.org/pdf/2303.06053.pdf)'
- en: Here, the matrix is again transposed and sent through a fully connected layer
    to generate predictions. The final step is then to transpose that matrix again
    to have the features on the horizontal axis, and the time steps on the vertical
    axis.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼ŒçŸ©é˜µå†æ¬¡è¢«è½¬ç½®ï¼Œå¹¶é€šè¿‡å…¨è¿æ¥å±‚ç”Ÿæˆé¢„æµ‹ã€‚æœ€ç»ˆæ­¥éª¤æ˜¯å†æ¬¡è½¬ç½®è¯¥çŸ©é˜µï¼Œä½¿ç‰¹å¾ä½äºæ°´å¹³è½´ä¸Šï¼Œæ—¶é—´æ­¥ä½äºå‚ç›´è½´ä¸Šã€‚
- en: Now that we understand how TSMixer works, letâ€™s implement it in Python and test
    it ourselves.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬äº†è§£äº†TSMixerçš„å·¥ä½œåŸç†ï¼Œè®©æˆ‘ä»¬åœ¨Pythonä¸­å®ç°å®ƒå¹¶è¿›è¡Œæµ‹è¯•ã€‚
- en: Implement TSMixer
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®ç°TSMixer
- en: To my knowledge, TSMixer is not implemented in common time series libraries
    in Python, like Darts or Neuralforecast. Therefore, I will adapt the original
    implementation to my experiment for this article.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ®æˆ‘äº†è§£ï¼ŒTSMixeråœ¨Pythonä¸­çš„å¸¸ç”¨æ—¶é—´åºåˆ—åº“ä¸­å°šæœªå®ç°ï¼Œå¦‚Dartsæˆ–Neuralforecastã€‚å› æ­¤ï¼Œæˆ‘å°†è°ƒæ•´åŸå§‹å®ç°ä»¥é€‚åº”æˆ‘çš„å®éªŒã€‚
- en: The original implementation is available in the [repository of Google Research](https://github.com/google-research/google-research/tree/master/tsmixer/tsmixer_basic).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹å®ç°å¯åœ¨[Google Researchçš„ä»£ç åº“](https://github.com/google-research/google-research/tree/master/tsmixer/tsmixer_basic)ä¸­æ‰¾åˆ°ã€‚
- en: The complete code for this experiment is available on [GitHub](https://github.com/marcopeix/time-series-analysis/blob/master/TSMixer.ipynb).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬å®éªŒçš„å®Œæ•´ä»£ç å¯åœ¨[GitHub](https://github.com/marcopeix/time-series-analysis/blob/master/TSMixer.ipynb)ä¸Šè·å–ã€‚
- en: Read and format the data
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯»å–å’Œæ ¼å¼åŒ–æ•°æ®
- en: The hardest part in applying deep learning models for time series forecasting
    is arguably formatting the dataset to be fed into the neural network.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åº”ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹ä¸­æœ€å›°éš¾çš„éƒ¨åˆ†æ— ç–‘æ˜¯æ ¼å¼åŒ–æ•°æ®é›†ä»¥è¾“å…¥ç¥ç»ç½‘ç»œã€‚
- en: So, the first step is create a `DataLoader` class that handles all the transformations
    of the dataset. This class is initialized with the batch size, the input sequence
    length, the output sequence length (the horizon) and a slice object for the targets.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œç¬¬ä¸€æ­¥æ˜¯åˆ›å»ºä¸€ä¸ª`DataLoader`ç±»æ¥å¤„ç†æ•°æ®é›†çš„æ‰€æœ‰è½¬æ¢ã€‚è¿™ä¸ªç±»é€šè¿‡æ‰¹å¤„ç†å¤§å°ã€è¾“å…¥åºåˆ—é•¿åº¦ã€è¾“å‡ºåºåˆ—é•¿åº¦ï¼ˆè§†é‡ï¼‰ä»¥åŠç›®æ ‡çš„åˆ‡ç‰‡å¯¹è±¡è¿›è¡Œåˆå§‹åŒ–ã€‚
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then, we add a method to read and scale the data. Here, we use the electricity
    transformer dataset Etth1, publicly [available on GitHub](https://github.com/zhouhaoyi/ETDataset/blob/main/ETT-small/ETTh1.csv)
    under the Creative Commons Attribute Licence.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æ·»åŠ ä¸€ä¸ªæ–¹æ³•æ¥è¯»å–å’Œç¼©æ”¾æ•°æ®ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ç”µåŠ›å˜å‹å™¨æ•°æ®é›†Etth1ï¼Œå…¬å¼€[åœ¨GitHubä¸Šè·å–](https://github.com/zhouhaoyi/ETDataset/blob/main/ETT-small/ETTh1.csv)ï¼Œæ ¹æ®åˆ›ä½œå…±ç”¨ç½²åè®¸å¯ã€‚
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the code block above, note that it is crucial to scale our data to improve
    the modelâ€™s training time. Also note that we fit the scaler on the training set
    only to avoid data leakage in the validation and test sets.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šé¢çš„ä»£ç å—ä¸­ï¼Œè¯·æ³¨æ„ç¼©æ”¾æ•°æ®ä»¥æé«˜æ¨¡å‹è®­ç»ƒæ—¶é—´æ˜¯è‡³å…³é‡è¦çš„ã€‚è¿˜è¦æ³¨æ„ï¼Œæˆ‘ä»¬åªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆç¼©æ”¾å™¨ï¼Œä»¥é¿å…åœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­å‡ºç°æ•°æ®æ³„æ¼ã€‚
- en: Then, we create two methods for splitting the windows of data into inputs and
    labels, and then create a dataset that can be fed to a Keras neural network.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬åˆ›å»ºä¸¤ä¸ªæ–¹æ³•ï¼Œå°†æ•°æ®çª—å£æ‹†åˆ†ä¸ºè¾“å…¥å’Œæ ‡ç­¾ï¼Œç„¶ååˆ›å»ºä¸€ä¸ªå¯ä»¥è¾“å…¥åˆ°Kerasç¥ç»ç½‘ç»œçš„æ•°æ®é›†ã€‚
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Finally, we complete the `DataLoader` class with methods to inverse transform
    the predictions and to generate the training, validation and test sets.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å®Œæˆ`DataLoader`ç±»ï¼Œæ·»åŠ æ–¹æ³•ä»¥é€†å‘è½¬æ¢é¢„æµ‹ç»“æœï¼Œå¹¶ç”Ÿæˆè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†ã€‚
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The complete `DataLoader` class is shown below:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæ•´çš„`DataLoader`ç±»å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Then, we can simply initialize an instance of the `DataLoader` class to read
    our dataset and create the relevant sets of data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°åˆå§‹åŒ–`DataLoader`ç±»çš„ä¸€ä¸ªå®ä¾‹ï¼Œä»¥è¯»å–æˆ‘ä»¬çš„æ•°æ®é›†å¹¶åˆ›å»ºç›¸å…³çš„æ•°æ®é›†ã€‚
- en: Here, we use a horizon of 96, an input sequence length of 512, and a batch size
    of 32.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨äº† 96 çš„é¢„æµ‹èŒƒå›´ã€512 çš„è¾“å…¥åºåˆ—é•¿åº¦å’Œ 32 çš„æ‰¹é‡å¤§å°ã€‚
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that the data is ready, we can build the TSMixer model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ•°æ®å·²ç»å‡†å¤‡å¥½ï¼Œæˆ‘ä»¬å¯ä»¥æ„å»º TSMixer æ¨¡å‹äº†ã€‚
- en: Build TSMixer
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ„å»º TSMixer
- en: Building TSMixer is fairly easy, as the model consists only of MLPs. Letâ€™s bring
    back its architecture so we can reference it as we build the model.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»º TSMixer éå¸¸ç®€å•ï¼Œå› ä¸ºè¯¥æ¨¡å‹ä»…ç”± MLP ç»„æˆã€‚è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹å…¶æ¶æ„ï¼Œä»¥ä¾¿åœ¨æ„å»ºæ¨¡å‹æ—¶å‚è€ƒã€‚
- en: '![](../Images/d91af3dd374db62e24ad58bc3635b890.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d91af3dd374db62e24ad58bc3635b890.png)'
- en: 'Architecture of TSMixer. Image by S. Chen, C. Li, N. Yoder, S. Arik and T.
    Pfister from [TSMixer: An All-MLP Architecture for Time Series Forecasting](https://arxiv.org/pdf/2303.06053.pdf)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 'TSMixer çš„æ¶æ„ã€‚å›¾ç‰‡æ¥è‡ª [TSMixer: An All-MLP Architecture for Time Series Forecasting](https://arxiv.org/pdf/2303.06053.pdf)
    çš„ S. Chenã€C. Liã€N. Yoderã€S. Arik å’Œ T. Pfisterã€‚'
- en: 'First, we must take care of the Mixer Layer, which consists of:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å¿…é¡»å¤„ç† Mixer Layerï¼Œå®ƒåŒ…æ‹¬ï¼š
- en: batch normalization
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰¹é‡å½’ä¸€åŒ–
- en: transpose the matrix
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è½¬ç½®çŸ©é˜µ
- en: feed to a fully connected layer with a ReLu activation
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾“å…¥åˆ°å…·æœ‰ ReLu æ¿€æ´»çš„å…¨è¿æ¥å±‚
- en: transpose again
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å†æ¬¡è½¬ç½®
- en: dropout layer
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: dropout å±‚
- en: and add the residuals at the end
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¹¶åœ¨æœ€åæ·»åŠ æ®‹å·®
- en: 'This is translated to code like so:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯ä»¥ç¿»è¯‘æˆå¦‚ä¸‹ä»£ç ï¼š
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, we add the portion for feature mixing which has:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æ·»åŠ ç‰¹å¾æ··åˆéƒ¨åˆ†ï¼Œå…¶ä¸­åŒ…æ‹¬ï¼š
- en: batch normalization
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰¹é‡å½’ä¸€åŒ–
- en: a dense layer
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª dense å±‚
- en: a dropout layer
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: dropout å±‚
- en: another dense layer
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ª dense å±‚
- en: another dropout layer
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ª dropout å±‚
- en: and add the residuals to make the residual connection
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¹¶æ·»åŠ æ®‹å·®ä»¥å½¢æˆæ®‹å·®è¿æ¥
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Thatâ€™s it! The full function for the Mixer Layer is shown below:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™æ ·ï¼Mixer Layer çš„å®Œæ•´å‡½æ•°å¦‚ä¸‹ï¼š
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, we simply write a function to build the model. We include a for loop to
    create as many Mixer Layers as we want, and we add the final temporal projection
    step.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬ç®€å•åœ°ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥æ„å»ºæ¨¡å‹ã€‚æˆ‘ä»¬åŒ…æ‹¬ä¸€ä¸ª for å¾ªç¯ä»¥åˆ›å»ºæˆ‘ä»¬éœ€è¦çš„ Mixer å±‚ï¼Œå¹¶æ·»åŠ æœ€ç»ˆçš„æ—¶é—´æŠ•å½±æ­¥éª¤ã€‚
- en: 'From the figure above, the temporal projection step is simply:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šé¢çš„å›¾ä¸­å¯ä»¥çœ‹åˆ°ï¼Œæ—¶é—´æŠ•å½±æ­¥éª¤åªæ˜¯ï¼š
- en: a transpose
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªè½¬ç½®
- en: a pass through a dense layer
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»è¿‡ dense å±‚å¤„ç†
- en: a final transpose
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€åçš„è½¬ç½®
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, we can run the function to build the model. In this case, we use eight
    blocks in the Mixer Layer.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œå‡½æ•°æ¥æ„å»ºæ¨¡å‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åœ¨ Mixer Layer ä¸­ä½¿ç”¨äº†å…«ä¸ªå—ã€‚
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Train TSMixer
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒ TSMixer
- en: We are now ready to train the model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å‡†å¤‡å¥½è®­ç»ƒæ¨¡å‹äº†ã€‚
- en: We use the Adam optimizer with a learning rate of 1e-4\. We also implement checkpoints
    to save the best model and early stopping to stop training if there is no improvements
    for three consecutive epochs.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨å­¦ä¹ ç‡ä¸º 1e-4 çš„ Adam ä¼˜åŒ–å™¨ã€‚æˆ‘ä»¬è¿˜å®ç°äº†æ£€æŸ¥ç‚¹ä»¥ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œå¹¶é€šè¿‡æ—©åœæœºåˆ¶åœ¨è¿ç»­ä¸‰æ¬¡è®­ç»ƒè½®æ¬¡æ²¡æœ‰æ”¹è¿›æ—¶åœæ­¢è®­ç»ƒã€‚
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that it took 15 minutes to train the model using the CPU only.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œä»…ä½¿ç”¨ CPU è®­ç»ƒæ¨¡å‹èŠ±è´¹äº† 15 åˆ†é’Ÿã€‚
- en: Once the model is trained, we can load the best model that was saved by the
    checkpoint callback.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥åŠ è½½é€šè¿‡æ£€æŸ¥ç‚¹å›è°ƒä¿å­˜çš„æœ€ä½³æ¨¡å‹ã€‚
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Then, letâ€™s access the predictions for the last window of 96 time steps. Note
    that the predictions are scaled right now.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œè®©æˆ‘ä»¬è®¿é—®æœ€åä¸€ä¸ª 96 æ—¶é—´æ­¥çš„é¢„æµ‹ã€‚æ³¨æ„ï¼Œé¢„æµ‹ç°åœ¨æ˜¯ç»è¿‡ç¼©æ”¾çš„ã€‚
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Finally, we store both the scaled and inverse transformed predictions in DataFrames
    to evaluate the performance and plot the predictions later on.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å°†ç¼©æ”¾åçš„é¢„æµ‹å’Œé€†å˜æ¢åçš„é¢„æµ‹å­˜å‚¨åœ¨ DataFrame ä¸­ï¼Œä»¥ä¾¿åç»­è¯„ä¼°æ€§èƒ½å’Œç»˜åˆ¶é¢„æµ‹ç»“æœã€‚
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Predict with N-HiTS
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ N-HiTS è¿›è¡Œé¢„æµ‹
- en: To assess the performance of TSMixer, we perform the same training protocol
    with N-HiTS, as they also support multivariate forecasting.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯„ä¼° TSMixer çš„æ€§èƒ½ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸ N-HiTS ç›¸åŒçš„è®­ç»ƒåè®®ï¼Œå› ä¸ºå®ƒä»¬ä¹Ÿæ”¯æŒå¤šå˜é‡é¢„æµ‹ã€‚
- en: As a reminder, you can access the full code of this experiment on [GitHub](https://github.com/marcopeix/time-series-analysis/blob/master/TSMixer.ipynb).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: æé†’ä¸€ä¸‹ï¼Œä½ å¯ä»¥åœ¨ [GitHub](https://github.com/marcopeix/time-series-analysis/blob/master/TSMixer.ipynb)
    ä¸Šè®¿é—®æ­¤å®éªŒçš„å®Œæ•´ä»£ç ã€‚
- en: For this section, we use the library NeuralForecast. So the natural first step
    is to read the data and format it accordingly.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä½¿ç”¨ NeuralForecast åº“ã€‚å› æ­¤ï¼Œè‡ªç„¶çš„ç¬¬ä¸€æ­¥æ˜¯è¯»å–æ•°æ®å¹¶æŒ‰éœ€æ ¼å¼åŒ–ã€‚
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Then, we can initialize N-HiTS and fit on the data.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥åˆå§‹åŒ– N-HiTS å¹¶åœ¨æ•°æ®ä¸Šè¿›è¡Œæ‹Ÿåˆã€‚
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Then, we extract the predictions for the last 96 time steps only.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬ä»…æå–è¿‡å» 96 ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹ã€‚
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: At this point, we have the predictions for each column over the last 96 time
    steps, as shown below.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ—¶ï¼Œæˆ‘ä»¬å·²ç»å¾—åˆ°äº†æ¯ä¸€åˆ—åœ¨è¿‡å» 96 ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/993ea50dd61bab4b5aa76d2ec22b0747.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/993ea50dd61bab4b5aa76d2ec22b0747.png)'
- en: Predictions of each series by N-HiTS over the last 96 time steps. Image by the
    author.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: N-HiTS å¯¹è¿‡å» 96 ä¸ªæ—¶é—´æ­¥çš„æ¯ä¸ªåºåˆ—çš„é¢„æµ‹ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Now, we are ready to visualize and measure the performance of our models.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å‡†å¤‡å¥½å¯è§†åŒ–å’Œè¡¡é‡æˆ‘ä»¬æ¨¡å‹çš„è¡¨ç°äº†ã€‚
- en: Evaluation
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°
- en: First, letâ€™s visualize the predictions.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬å¯è§†åŒ–é¢„æµ‹ç»“æœã€‚
- en: For simplicity, we plot only the forecast for the first four series in the dataset.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€ä¾¿èµ·è§ï¼Œæˆ‘ä»¬åªç»˜åˆ¶äº†æ•°æ®é›†ä¸­å‰å››ä¸ªåºåˆ—çš„é¢„æµ‹ã€‚
- en: '[PRE18]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/c3cba66a8a601f65922c0bea1bc9ab92.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3cba66a8a601f65922c0bea1bc9ab92.png)'
- en: Predictions of N-HiTS and TSMixer for the first four series in the dataset.
    We see that TSMixer had trouble generalizing for MULL and HULL, while N-HiTS seems
    to be following the actual curve fairly well. Image by the author.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: N-HiTS å’Œ TSMixer å¯¹æ•°æ®é›†ä¸­å‰å››ä¸ªåºåˆ—çš„é¢„æµ‹ã€‚æˆ‘ä»¬çœ‹åˆ° TSMixer åœ¨ MULL å’Œ HULL ä¸Šéš¾ä»¥æ³›åŒ–ï¼Œè€Œ N-HiTS ä¼¼ä¹å¾ˆå¥½åœ°è·Ÿéšäº†å®é™…æ›²çº¿ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: From the figure above, we can see that TSMixer does a pretty good job at forecasting
    HUFL and MUFL, but it struggles with MULL and HULL. However, N-HiTS seems to be
    doing fairly good on all series.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° TSMixer åœ¨é¢„æµ‹ HUFL å’Œ MUFL ä¸Šè¡¨ç°å¾—ç›¸å½“ä¸é”™ï¼Œä½†åœ¨ MULL å’Œ HULL ä¸Šè¡¨ç°ä¸ä½³ã€‚ç„¶è€Œï¼ŒN-HiTS
    ä¼¼ä¹åœ¨æ‰€æœ‰åºåˆ—ä¸Šçš„è¡¨ç°éƒ½å¾ˆä¸é”™ã€‚
- en: Still, the best way to assess the performance is by measuring an error metric.
    Here, we compute the MAE and MSE.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿‡ï¼Œè¯„ä¼°è¡¨ç°çš„æœ€ä½³æ–¹å¼è¿˜æ˜¯é€šè¿‡æµ‹é‡è¯¯å·®æŒ‡æ ‡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è®¡ç®— MAE å’Œ MSEã€‚
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/64fc5b5d4d677632dd5a2459780d4404.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64fc5b5d4d677632dd5a2459780d4404.png)'
- en: MAE and MSE of N-HiTS and TSMixer for the task of multivariate forecasting on
    a horizon of 96 time steps. Image by the author.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: N-HiTS å’Œ TSMixer åœ¨ 96 ä¸ªæ—¶é—´æ­¥é•¿çš„å¤šå˜é‡é¢„æµ‹ä»»åŠ¡ä¸­çš„ MAE å’Œ MSEã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: From the figure above, we can see that TSMixer outperforms N-HiTS on the task
    of multivariate forecasting on a horizon of 96 time steps, since it achieved the
    lowest MAE and MSE.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° TSMixer åœ¨ 96 ä¸ªæ—¶é—´æ­¥é•¿çš„å¤šå˜é‡é¢„æµ‹ä»»åŠ¡ä¸­ä¼˜äº N-HiTSï¼Œå› ä¸ºå®ƒå®ç°äº†æœ€ä½çš„ MAE å’Œ MSEã€‚
- en: While this is not the most extensive experiment, it is interesting to see that
    kind of performance coming from a rather simple model architecture.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™ä¸æ˜¯æœ€å…¨é¢çš„å®éªŒï¼Œä½†çœ‹åˆ°è¿™ç§æ€§èƒ½æ¥è‡ªç›¸å½“ç®€å•çš„æ¨¡å‹æ¶æ„ä»ç„¶å¾ˆæœ‰è¶£ã€‚
- en: Conclusion
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: TSMixer is an an all-MLP model specifically designed for multivariate time series
    forecasting.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: TSMixer æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºå¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹è®¾è®¡çš„å…¨ MLP æ¨¡å‹ã€‚
- en: It extends the capacity of linear models by adding cross-variate feed-forward
    layers, enabling the model to achieve state-of-the-art performances on long horizon
    multivariate forecasting tasks.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒé€šè¿‡æ·»åŠ äº¤å‰å˜é‡å‰é¦ˆå±‚æ‰©å±•äº†çº¿æ€§æ¨¡å‹çš„èƒ½åŠ›ï¼Œä½¿æ¨¡å‹åœ¨é•¿æ—¶é—´è·¨åº¦çš„å¤šå˜é‡é¢„æµ‹ä»»åŠ¡ä¸­è¾¾åˆ°æœ€å…ˆè¿›çš„è¡¨ç°ã€‚
- en: While there is no out-of-the-box implementation yet, you now have the knowledge
    and skills to implement it yourself, as its simple architecture makes it easy
    for us to do so.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶ç›®å‰è¿˜æ²¡æœ‰ç°æˆçš„å®ç°ï¼Œä½†ä½ ç°åœ¨æ‹¥æœ‰äº†è‡ªå·±å®ç°å®ƒçš„çŸ¥è¯†å’ŒæŠ€èƒ½ï¼Œå› ä¸ºå®ƒç®€å•çš„æ¶æ„ä½¿å¾—æˆ‘ä»¬å¯ä»¥è½»æ¾åšåˆ°è¿™ä¸€ç‚¹ã€‚
- en: As always, each forecasting problem requires a unique approach and a specific
    model, so make sure to test TSMixer as well as other models.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€å¦‚æ—¢å¾€ï¼Œæ¯ä¸ªé¢„æµ‹é—®é¢˜éƒ½éœ€è¦ç‹¬ç‰¹çš„æ–¹æ³•å’Œç‰¹å®šçš„æ¨¡å‹ï¼Œå› æ­¤è¯·ç¡®ä¿æµ‹è¯• TSMixer ä»¥åŠå…¶ä»–æ¨¡å‹ã€‚
- en: Thanks for reading! I hope that you enjoyed it and that you learned something
    new!
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼å¸Œæœ›ä½ å–œæ¬¢ï¼Œå¹¶ä¸”å­¦åˆ°äº†ä¸€äº›æ–°ä¸œè¥¿ï¼
- en: Looking to master time series forecasting? The check out my course [Applied
    Time Series Forecasting in Python](https://www.datasciencewithmarco.com/offers/zTAs2hi6/checkout?coupon_code=ATSFP10).
    This is the only course that uses Python to implement statistical, deep learning
    and state-of-the-art models in 16 guided hands-on projects.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è¦æŒæ¡æ—¶é—´åºåˆ—é¢„æµ‹å—ï¼Ÿé‚£å°±çœ‹çœ‹æˆ‘çš„è¯¾ç¨‹ [Applied Time Series Forecasting in Python](https://www.datasciencewithmarco.com/offers/zTAs2hi6/checkout?coupon_code=ATSFP10)ã€‚è¿™æ˜¯å”¯ä¸€ä¸€ä¸ªä½¿ç”¨
    Python å®ç°ç»Ÿè®¡ã€æ·±åº¦å­¦ä¹ å’Œæœ€å…ˆè¿›æ¨¡å‹çš„ 16 ä¸ªå¼•å¯¼æ€§å®è·µé¡¹ç›®çš„è¯¾ç¨‹ã€‚
- en: Cheers ğŸ»
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å¹²æ¯ ğŸ»
- en: Support me
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ”¯æŒæˆ‘
- en: Enjoying my work? Show your support with [Buy me a coffee](http://buymeacoffee.com/dswm),
    a simple way for you to encourage me, and I get to enjoy a cup of coffee! If you
    feel like it, just click the button below ğŸ‘‡
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: å–œæ¬¢æˆ‘çš„å·¥ä½œå—ï¼Ÿé€šè¿‡ [Buy me a coffee](http://buymeacoffee.com/dswm) æ”¯æŒæˆ‘ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„æ–¹å¼æ¥é¼“åŠ±æˆ‘ï¼ŒåŒæ—¶æˆ‘å¯ä»¥äº«å—ä¸€æ¯å’–å•¡ï¼å¦‚æœä½ æ„¿æ„ï¼Œåªéœ€ç‚¹å‡»ä¸‹é¢çš„æŒ‰é’®
    ğŸ‘‡
- en: '![](../Images/d8286ff73e873a825293e2ae8e2a7da3.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8286ff73e873a825293e2ae8e2a7da3.png)'
- en: References
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: 'Si-An Chen, Chun-Liang Li, Nate Yoder, Sercan O. Arik, Tomas Pfister â€” [TSMixer:
    An All-MLP Architecture for Time Series Forecasting](https://arxiv.org/abs/2303.06053)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 'Si-An Chen, Chun-Liang Li, Nate Yoder, Sercan O. Arik, Tomas Pfister â€” [TSMixer:
    An All-MLP Architecture for Time Series Forecasting](https://arxiv.org/abs/2303.06053)'
- en: Original implementation of TSMixer by the researchers at Googleâ€” [GitHub](https://github.com/google-research/google-research/tree/master/tsmixer/tsmixer_basic)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Google ç ”ç©¶äººå‘˜çš„ TSMixer åŸå§‹å®ç°â€” [GitHub](https://github.com/google-research/google-research/tree/master/tsmixer/tsmixer_basic)
