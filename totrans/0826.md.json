["```py\nclass PlatformSim:\n    def __init__(self, alpha=0.05, beta=0.80, delta=0.1):\n        self.alpha, self.beta, self.delta = alpha, beta, delta\n        self.z_crit = stats.norm.isf(self.alpha/2)\n        self.n_s = int(np.ceil(delta**(-2) * (stats.norm.isf(alpha/2) - stats.norm.isf(beta))**2))\n        self.theta_crit = self.z_crit / np.sqrt(self.n_s)\n\n    def run(self, n, lambda_=0.2, kappa=0.5, rho=0.2, shape=3):\n        self.n_null = int(n * (1 - lambda_))\n        self.n_pos = int(n * lambda_ * kappa)\n        self.n_neg = n - self.n_null - self.n_pos \n        # compute thetas\n        thetas_null = np.zeros(self.n_null)\n        scale = self.delta / scipy.stats.gamma.isf(rho, shape)\n        thetas_pos = +np.random.gamma(shape, scale=scale, size=self.n_pos)\n        thetas_neg = -np.random.gamma(shape, scale=scale, size=self.n_neg)\n        self.thetas = np.concatenate((thetas_null, thetas_pos, thetas_neg))\n        # for each theta, simulate result of an experiment        \n        np.random.shuffle(self.thetas)\n        self.x_values, self.z_values, self.p_values = self.simulate_results(self.thetas)\n        self.results = (self.z_values > self.z_crit).astype(int)\n\n    def simulate_results(self, thetas):\n        X = np.random.normal(loc=thetas, size=(self.n_s, len(thetas)))\n        self.X = X\n        x_values = X.mean(axis=0)\n        z_values = x_values * np.sqrt(self.n_s)\n        p_values = 2 * stats.norm.sf(np.abs(z_values))\n        return x_values, z_values, p_values\n```", "```py\nP = PlatformSim(alpha=0.05, beta=0.8, delta=0.1)\nP.run(10000, lambda_= 0.2, kappa=0.5, rho=0.3, null_type=null_type, pos_type=pos_type, shape=3)\ndf = DataFrame({'theta': P.thetas, 'x': P.x_values, 'z': P.z_values, 'p': P.p_values, 'result': P.results})\ndf.sort_values('p', inplace=True, ignore_index=True)\ndfs = df[df.result==1]\neffect = dfs.theta.mean()\nprint(f\"Average Effect (Actual): {round(effect, 6)}\")  \nprint(f\"Average Effect (Observed): {round(dfs.x.mean(), 6)}     REL: {100*round(dfs.x.mean() / effect - 1, 2)}%\")  \nprint(f\"False Discovery Rate: {round(100*len(dfs[dfs.theta <= 0]) / len(dfs), 2)}%\") \nsig_i = 1 / np.sqrt(P.n_s)\nimp_b = (dfs.x.sum() - sig_i * scipy.stats.norm.pdf((P.theta_crit - df.x) / sig_i).sum()) / len(dfs)\nimp_0 = (dfs.x.sum() - sig_i * scipy.stats.norm.pdf((P.theta_crit - df.theta) / sig_i).sum()) / len(dfs)\nprint(f\"Bias-Corrected True      Estimate: {round(imp_0, 6)}     REL: {round(100* imp_0 / dfs.theta.mean() - 100, 2)}%\")\nprint(f\"Bias-Corrected Empirical Estimate: {round(imp_b, 6)}     REL: {round(100* imp_b / dfs.theta.mean() - 100, 2)}%\")\n# Output ~\n# Average Effect (Actual): 0.079171\n# Average Effect (Observed):  0.113245    REL: 43.0%\n# False Discovery Rate:  27.04%\n# Bias-Corrected True      Estimate:  0.079023     REL: -0.19%\n# Bias-Corrected Empirical Estimate:  0.062471     REL: -21.09%\n```", "```py\n def knn(x, k, X):\n    \"\"\" Returns the k nearest neighbors to x from the set X \"\"\"\n    n = len(X)\n    X = np.sort(X)\n    z = np.abs(X - x)\n    start = np.argmin(z)\n    stop = start * 1\n    k_count = 1\n    while k_count < k:\n        if start == 0:\n            stop += (k - k_count)\n            k_count = k\n            break\n        if stop == n-1:\n            start -= (k - k_count)\n            k_count = k\n            break\n        if z[start-1] < z[stop+1]:\n            start -= 1 \n        else:\n            stop += 1\n        k_count += 1\n    return X[start:stop+1]\n# KNN-density estimate \nk, n_b = 200, 10000 # k, number of bins\nh = 1 / n_b # bin size \nt = np.linspace(0, 1, n_b+1).round(4)\nfp_knn = np.zeros(n_b+1)\nfor i in range(n_b+1):\n    xnn = knn(t[i], k, df.p.values)\n    fp_knn[i] = k / len(df) / (np.max(xnn) - np.min(xnn))\nfp_base = fp_knn[t > 0.5].mean()\np_map = {p:f for p, f in zip(t, fp_knn)}\ndf.loc[:, 'f_p'] = df.p.apply(lambda x: p_map[round(x, 4)])\ndf.loc[:, 'p_prob'] = df.f_p.apply(lambda x: max(0, 1 - fp_base / x))\ndfs = df[df.result==1] # positive significant results (for later)\ndfs = dfs.reset_index(drop=True)\n```", "```py\ndf.loc[:, 'power'] = stats.norm.sf(P.z_crit, np.abs(df.z))\nbeta_ = np.average(df[df.x > 0].power, weights=df[df.x > 0].p_prob) # average power of positive results\nkappa_hat = np.average((df.x>0), weights=df.p_prob)\nfrac_sig = (np.abs(df.x) > 0.07).mean()\nbeta_0 = np.average(df.power, weights=df.p_prob) ## Two-sided average prob. of sig result\nlambda_hat = (frac_sig - alpha) / (beta_0 - alpha)\nFDR_hat = alpha*(1-lambda_hat) / (alpha*(1-lambda_hat) + 2 * beta_ * lambda_hat * kappa_hat)\n# Outputs\n# lambda     : 0.164780\n# beta_      : 0.666541\n# kappa      : 0.498272\n# FDR true   : 0.270448\n# FDR_hat    : 0.276170\n```", "```py\nbias_sum = np.average(scipy.stats.norm.pdf((P.theta_crit - df.x) / sig_i), weights=df.p_prob) * df.p_prob.sum()\nbias_sum += scipy.stats.norm.pdf((P.theta_crit - 0) / sig_i) * (len(df) - df.p_prob.sum())\nimp_pcor = (dfs.x.sum() - sig_i * bias_sum) / len(dfs)\nprint(f\"P-Corrected Average Effect : {round((dfs.x * dfs.p_prob).mean(), 6)}     REL: {round(100* (dfs.x * dfs.p_prob).mean() / dfs.theta.mean() - 100, 2)}%\")\nprint(f\"P-Corrected Bias Estimate  : {round(imp_pcor, 6)}     REL: {round(100* imp_pcor / dfs.theta.mean() - 100, 2)}%\")\n# Output\n# P-Corrected Average Effect : 0.089668     REL: 13.26%\n# P-Corrected Bias Estimate  : 0.082046     REL: 3.63%\n```", "```py\nclass EffectDistribution:\n\n    def __init__(self, x, p_prob, sort=True):\n        x, f = list(x), list(p_prob)\n        if (0 not in x):\n            x.append(0)\n            f.append(len(p_prob) - p_prob.sum())\n        pmfmat = np.array([x, f])\n        if sort:\n            pmfmat = pmfmat[:, pmfmat[0, :].argsort()]\n        self.x, self.pmf = pmfmat[0, :], pmfmat[1, :]\n        self.zero_loc = np.argmin(np.abs(self.x)) \n        self.normalize()\n\n    def __len__(self):\n        return len(self.x)\n\n    def normalize(self):\n        self.pmf /= self.pmf.sum()\n\n    def getProbNull(self):\n        return self.pmf[self.zero_loc]  \n\n    def getSample(self, n, pmf=None, prob=1):\n        pmf = self.pmf if pmf is None else pmf\n        sample = np.random.choice(self.x, size=n, replace=True, p=pmf)\n        mask = np.random.choice([0, 1], size=n, replace=True, p=[1-prob, prob])\n        return mask * sample\n\n    def getUpdate(self, x_mean, n, sigma=1, inplace=False):\n        pmf = self.pmf * np.exp(-n * self.x * (self.x - 2 * x_mean) / (2 * sigma**2))\n        if inplace:\n            self.pmf = pmf \n            self.normalize()\n            return\n        return pmf / pmf.sum()\n```", "```py\nn_boot = 1000\nB = np.zeros((len(dfs), n_boot))\nE = EffectDistribution(df.x, df.p_prob)\nfor i, row in dfs.iterrows():\n    pmf_ = E.getUpdate(row.x, P.n_s)\n    dfs.loc[i, 'theta_hat'] = np.average(E.x, weights=pmf_)\n    B[i, :] = E.getSample(n_boot, pmf=pmf_) #, prob=row.p_prob)\nbootstrap_means = B.mean(axis=0)\nprint(f\"POSTERIOR CORRECTED Estimate       : {round(dfs.theta_hat.mean(), 6)}     REL: {round(100*dfs.theta_hat.mean() / dfs.theta.mean() - 100, 2)}%\")\n# POSTERIOR CORRECTED Estimate       : 0.079478     REL: 0.39%\n```"]