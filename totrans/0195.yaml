- en: A Gentle Introduction To Generative AI For Beginners
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-gentle-introduction-to-generative-ai-for-beginners-8c8752085900](https://towardsdatascience.com/a-gentle-introduction-to-generative-ai-for-beginners-8c8752085900)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s understand the big picture behind generative AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://federicotrotta.medium.com/?source=post_page-----8c8752085900--------------------------------)[![Federico
    Trotta](../Images/e997e3a96940c16ab5071629016d82fd.png)](https://federicotrotta.medium.com/?source=post_page-----8c8752085900--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8c8752085900--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8c8752085900--------------------------------)
    [Federico Trotta](https://federicotrotta.medium.com/?source=post_page-----8c8752085900--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8c8752085900--------------------------------)
    ·8 min read·Jun 29, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/738f387e4050427ddb87dbad2b2f05f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Susan Cipriano](https://pixabay.com/it/users/susan-lu4esm-7009216/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=8015423)
    on [Pixabay](https://pixabay.com/it//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=8015423)
  prefs: []
  type: TYPE_NORMAL
- en: The last months have seen the rise of the so-called “Generative AI”, which is
    a sub-field of Artificial Intelligence (AI). Tools like ChatGPT have become one
    of the most spoken words and are becoming fundamental tools for everyday tasks
    in many jobs ([even to learn to code](/how-to-effectively-start-coding-in-the-era-of-chatgpt-cfc5151e1c42)).
  prefs: []
  type: TYPE_NORMAL
- en: Words like “[DALL-E](https://medium.com/mlearning-ai/can-i-sell-ai-generated-images-a5d4619c8e1b)”,
    “ChatGPT” and “Generative AI” have pervaded socials, media, chats with colleagues,
    and everything related to our world over the last few months. Literally, everyone
    is talking about that.
  prefs: []
  type: TYPE_NORMAL
- en: But what is generative AI? Why is that any different from “normal” AI?
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we’ll clarify the big picture behind generative AI. So, if
    you’ve participated in discussions but don’t have clear ideas on this topic, this
    article is definitely for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a discursive explanation to understand the basics of what’s behind
    the scene of generative AI. So, don’t worry: you won’t find any code here. Just
    ideas and descriptions and these will be presented in a very short and concise
    way. In particular, we’ll focus on Large Language Models and Image Generation
    Models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a summary of what you’ll learn here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: What is generative AI and how does it differ from traditional AI?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI is a subfield of AI that involves creating algorithms that can
    generate new data such as images, text, code, and music.
  prefs: []
  type: TYPE_NORMAL
- en: The big difference between generative AI and “traditional AI” is that the former
    generates new data based on the training data. Also, it works with types of data
    that “traditional AI” can’t.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say it a little more technically:'
  prefs: []
  type: TYPE_NORMAL
- en: “Traditional AI” can be defined as discriminative AI. In this case, in fact,
    we train Machine Learning models so that they can make predictions or classifications
    on new, unseen data. These ML models can work only with numbers, and sometimes
    with text (for example, in the case of Natural Language Processing).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In generative AI, we train an ML model and it creates an output that is similar
    to the data it has been trained on. These kinds of ML models can work with different
    kinds of data like numbers, text, images, and audio.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s visualize the processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d56bfd8216ebc2e01e106b7576454ee.png)'
  prefs: []
  type: TYPE_IMG
- en: The process behind traditional AI. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: So, in traditional AI, we train an ML model to learn from data. Then, we feed
    it with new and unseen data and it can discriminate, making predictions or classifications.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the presented example, we’ve trained an ML model to recognize dogs
    from images. Then, we feed the trained ML model with new and unseen pictures of
    dogs and it will be able to classify whether these new images are representing
    dogs or not.
  prefs: []
  type: TYPE_NORMAL
- en: This is the typical task for a Deep Learning algorithm, in the case of a classification
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d3e30bae09a261503bfdf3b1d4e00ec.png)'
  prefs: []
  type: TYPE_IMG
- en: The process behind generative AI. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of generative AI, instead, we train an ML model with data from various
    sources using a vast quantity of data. Then, thanks to a prompt (a query in natural
    language inserted by a user), the model gives us an output that is similar to
    the data it’s been trained on.
  prefs: []
  type: TYPE_NORMAL
- en: To stick to the example, our model has been trained on a massive amount of (text)
    data that, among others, explains what a dog is. Then, if a user queries the model
    asking what a dog is, the model will describe what a dog is in natural language.
  prefs: []
  type: TYPE_NORMAL
- en: This is the typical task performed by tools like ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see some types of generative AI models.
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s start to dive into the various kinds of generative AI subfields by starting
    with Large Language Models (LLMs). [An LLM is](https://en.wikipedia.org/wiki/Large_language_model#:~:text=A%20large%20language%20model%20(LLM,learning%20or%20semi%2Dsupervised%20learning.)
    (from Wikipedia):'
  prefs: []
  type: TYPE_NORMAL
- en: a computerized language model consisting of an artificial neural network with
    many parameters (tens of millions to billions), trained on large quantities of
    unlabeled text using self-supervised learning or semi-supervised learning.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Though the term large language model has no formal definition, it often refers
    to deep learning models with millions or even billions of parameters, that have
    been “pre-trained” on a large corpus.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'So, LLMs are Deep Learning (DL) models (aka, Neural Networks) trained with
    millions of parameters on a huge amount of text (this is why we call them “large”)
    and are useful to solve some language problems like:'
  prefs: []
  type: TYPE_NORMAL
- en: Text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Question & Answering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document summarization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, another important difference between standard ML models is that, in this
    case, we can train a DL algorithm that can be used for different tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Let me explain better.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we need to develop a system that can recognize dogs in images as we’ve seen
    before, we need to train a DL algorithm to [solve a classification task](/classification-metrics-the-complete-guide-for-aspiring-data-scientists-9f02eab796ae)
    that is: tell us if new, unseen images are representing dogs or not. Nothing more.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead, training an LLM can help us in all the tasks we’ve described above.
    So, this also justifies the amount of computing power (and money!) needed to train
    an LLM (which requires petabytes of data!).
  prefs: []
  type: TYPE_NORMAL
- en: 'As we know, LLMs are queried by users thanks to prompts. Now, we have to spot
    the difference between prompt design and prompt engineering:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt design**. This is the art of creating a prompt that is specifically
    suitable for the specific task that the system is performing. For example, if
    we want to ask our LLM to translate a text from English to Italian, we have to
    write a specific prompt in English asking the model to translate the text we’re
    pasting into Italian.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt engineering**. This is the process of creating prompts to improve
    the performance of our LLM. This means using our domain knowledge to add details
    to the prompt like specific keywords, specific context and examples, and the desired
    output if necessary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, when we’re prompting, sometimes we use a mix of both. For example,
    we may want a translation from English to Italian that interests a particular
    domain of knowledge, like mechanics.
  prefs: []
  type: TYPE_NORMAL
- en: So, for example, a prompt may be:” *Translate in Italian the following:*
  prefs: []
  type: TYPE_NORMAL
- en: '*the beam is subject to normal stress.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Consider that we’re in the field of mechanics, so ‘normal stress’ must be
    related to it*”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because, you know: “normal” and “stress” may be misunderstood by the model
    (but even by humans!).'
  prefs: []
  type: TYPE_NORMAL
- en: The three types of LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are three types of LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generic Language Models**. These are able to predict a word (or a phrase)
    based on the language in the training data. Think, for example, of your email
    auto-completion feature to understand this type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Instruction Tuned Models**. These kinds of models are trained to predict
    a response to the instructions given in the input. Summarizing a given text is
    a typical example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dialog Tuned Models**. These are trained to have a dialogue with the user,
    using the subsequent responses. An AI-powered chatbot is a typical example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anyway, consider that the models that are actually distributed have mixed features.
    Or, at least, they can perform actions that are typical of more than one of these
    types.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we think of ChatGPT we can clearly say that it:'
  prefs: []
  type: TYPE_NORMAL
- en: Can predict a response to the instructions, given an input. In fact, for example,
    it can summarize texts, give insights on a certain argument we provide via prompts,
    etc… So, it has features like an Instruction Tuned Model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is trained to have a dialog with the users. And this is very clear, as it works
    with consequent prompts until we’re happy with its answer. So, it has also features
    like a Dialog Tuned Model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image generation has been around for quite some time, contrary to what one might
    believe.
  prefs: []
  type: TYPE_NORMAL
- en: Anyway, in recent times it gained popularity, especially with tools like “DALL-E”
    or “stable diffusion” that have cleared their use, making this technology accessible
    to the masses worldwide.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can say that image generation can be divided into four categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Variational Autoencoders** (VAEs). [Variational autoencoders are](https://en.wikipedia.org/wiki/Variational_autoencoder#:~:text=Variational%20autoencoders%20are%20probabilistic%20generative,first%20and%20second%20component%20respectively.)
    “*probabilistic generative models that require neural networks as only a part
    of their overall structure*”. In operational words, they encode images to a compressed
    size and decode them to the original size. During this process, they learn the
    distribution of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generative Adversarial Models** (GANs). These are generally the most known,
    at least as a word that resonates in the field of generative AI. [A GAN is](https://en.wikipedia.org/wiki/Generative_adversarial_network)
    “*a class of ML framework in which two Neural Networks are pith against each other
    where the gain of one is the loss of the other*”. This means that one Neural Network
    creates the image while the other predicts if it is real or fake.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autoregressive models**. In statistics, an autoregressive model is the representation
    of a random process. In the context of generative images, these kinds of models
    generate images by treating images as a sequence of pixels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diffusion models**. Diffusion models have been inspired by thermodynamics
    and are definitely the most promising and interesting kinds of models in the subfield
    of image generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is the process working under the hood of diffusion models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Forward distribution process**. We have an initial, iterative, process where
    the structure of the image is “destroyed” in a data distribution. In simple words,
    is like we iteratively add noise to the image, until all the pixels become pure
    noise and the image is not recognizable (by the human eye).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reverse diffusion process**. Then, there is a reverse diffusion process which
    is the actual learning process: this restores the structure of the data. It is
    like our model learns how to “de-noise” the pixels to recreate the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The power of connecting it all
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you maintained the attention until now, a question should naturally pop
    up in your mind:” *Ok, Federico, it’s clear. But I’m missing something: when I
    use “DALL-E” I insert a prompt and it outputs an image: we haven''t talked about
    that, do we?!*”.'
  prefs: []
  type: TYPE_NORMAL
- en: No, we haven’t.
  prefs: []
  type: TYPE_NORMAL
- en: Above we made a brief description of the most promising (and currently, the
    most used) model for generating images, but the missing part is the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve discussed, in fact, how they work at a high level. Meaning: we gave a
    short explanation of how their learning process works.'
  prefs: []
  type: TYPE_NORMAL
- en: But the real power of these models arrives when they are coupled with LLMs.
    This coupling, in fact, gives us the possibility to combine the power of prompt
    engineering to ask our models for outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words: we’ve combined the possibility to use natural language as inputs
    to models that can actually understand it and can generate images according to
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: Isn’t it a superpower?!?
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concluding, we can say that generative AI is a subfield of AI that generates
    new data similar to the train data.
  prefs: []
  type: TYPE_NORMAL
- en: While, on the one hand, LLMs can generate text based on training data and image
    generation models can generate new images based on the training images, the real
    power of generative AI, at least in the case of images, relies on the combination
    of LLM and models for image generation. This gives us the possibility to create
    images according to prompts as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '***NOTE****: this article has been freely inspired by the Generative AI course
    provided by Google, and some references are taken from it. I suggest* [*taking
    this course*](https://www.cloudskillsboost.google/journeys/118)*, for a better
    understanding of generative AI.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5079e3af9eda458328cb258c452fb935.png)'
  prefs: []
  type: TYPE_IMG
- en: Federico Trotta
  prefs: []
  type: TYPE_NORMAL
- en: Hi, I’m Federico Trotta and I’m a freelance Technical Writer.
  prefs: []
  type: TYPE_NORMAL
- en: Want to collaborate with me? [Contact me](https://bio.link/federicotrotta).
  prefs: []
  type: TYPE_NORMAL
