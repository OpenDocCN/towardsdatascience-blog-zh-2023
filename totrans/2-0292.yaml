- en: An Example of Sequence Modelling with Transformer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Transformer的序列建模示例
- en: 原文：[https://towardsdatascience.com/an-example-of-sequence-modelling-with-transformer-7d4c7dc85fc3](https://towardsdatascience.com/an-example-of-sequence-modelling-with-transformer-7d4c7dc85fc3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/an-example-of-sequence-modelling-with-transformer-7d4c7dc85fc3](https://towardsdatascience.com/an-example-of-sequence-modelling-with-transformer-7d4c7dc85fc3)
- en: NLP Deep Learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NLP深度学习
- en: Some concepts about transformer and an example on how to build a sequence model
    with transformer
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些关于transformer的概念以及如何使用transformer构建序列模型的示例
- en: '[](https://medium.com/@jhwang1992m?source=post_page-----7d4c7dc85fc3--------------------------------)[![Jiahui
    Wang](../Images/91350774d661092f429d1b0591af95f4.png)](https://medium.com/@jhwang1992m?source=post_page-----7d4c7dc85fc3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7d4c7dc85fc3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7d4c7dc85fc3--------------------------------)
    [Jiahui Wang](https://medium.com/@jhwang1992m?source=post_page-----7d4c7dc85fc3--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jhwang1992m?source=post_page-----7d4c7dc85fc3--------------------------------)[![Jiahui
    Wang](../Images/91350774d661092f429d1b0591af95f4.png)](https://medium.com/@jhwang1992m?source=post_page-----7d4c7dc85fc3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7d4c7dc85fc3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7d4c7dc85fc3--------------------------------)
    [Jiahui Wang](https://medium.com/@jhwang1992m?source=post_page-----7d4c7dc85fc3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7d4c7dc85fc3--------------------------------)
    ·5 min read·Feb 10, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----7d4c7dc85fc3--------------------------------)
    ·5分钟阅读·2023年2月10日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7516b578384db3e4826b2bf637a5ba5f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7516b578384db3e4826b2bf637a5ba5f.png)'
- en: Photo by [Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Users’ behaviour sequence data can be used to predict users’ profile and preferences.
    For instance, we can learn a user’s gender from the user’s historical purchase
    records, or we can predict whether a user will resolve a loan based on this user’s
    repayment history.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 用户行为序列数据可以用来预测用户的个人信息和偏好。例如，我们可以通过用户的历史购买记录来了解用户的性别，或者根据用户的还款历史来预测用户是否会偿还贷款。
- en: In this post, we will try to predict users’ financial risk level by modelling
    the app instalment sequence on their phone. Here, we take the app instalment sequence
    modelling task as an NLP problem. Apps installed by a group of users form a corpus.
    Each app is analogous to a word, while each app instalment sequence by a user
    is analogous to a sentence in the NLP problem. To solve this app instalment sequence
    modelling task, we choose to use a neural network with transformers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将尝试通过对手机上应用安装序列建模来预测用户的财务风险水平。在这里，我们将应用安装序列建模任务视为一个NLP问题。用户安装的应用形成一个语料库。每个应用类似于一个词，而每个用户的应用安装序列类似于NLP问题中的一个句子。为了解决这个应用安装序列建模任务，我们选择使用带有transformers的神经网络。
- en: 'In this post, the followings will be covered:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，将涵盖以下内容：
- en: Transformers and Why Transformers Work
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers及其有效性
- en: Neural Network with Transformers Development and Deployment on Spark
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络与Transformers在Spark上的开发和部署
- en: 1\. Transformers and Why Transformers Work
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. Transformers及其有效性
- en: Transformers have been widely used in NLP problems, since Google published the
    paper [***Attention Is All You Need***](https://arxiv.org/abs/1706.03762). The
    attention module differentiates the transformers from the RNN structures. Ketan
    Doshi has [**a wonderful series of Medium posts**](/transformers-explained-visually-not-just-how-but-why-they-work-so-well-d840bd61a9d3)
    that well explains transformers and attention mechanism, both mathematically and
    visually. If you are interested, don’t forget to check out his series of posts.
    Here, I will summarise some key take-aways that I find particularly useful to
    understand transformers and attention mechanism.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 Google 发布了论文[***Attention Is All You Need***](https://arxiv.org/abs/1706.03762)以来，变压器在
    NLP 问题中得到了广泛应用。注意力模块使得变压器与 RNN 结构有所不同。Ketan Doshi 在[**Medium 上有一系列精彩的帖子**](/transformers-explained-visually-not-just-how-but-why-they-work-so-well-d840bd61a9d3)很好地解释了变压器和注意力机制，包括数学和视觉方面。如果你感兴趣，不要忘了查看他的系列帖子。在这里，我将总结一些我认为特别有助于理解变压器和注意力机制的关键要点。
- en: In each attention module, the embedding input goes through three linear transformations
    to form Query, Key, and Value. These three linear transformations are achieved
    by matrix multiplication, where the transformation matrix contains trainable parameters.
    In the training process, these trainable parameters will be tuned to align the
    input embedding for optimised prediction output.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个注意力模块中，嵌入输入经过三个线性变换形成 Query、Key 和 Value。这三个线性变换通过矩阵乘法实现，其中变换矩阵包含可训练的参数。在训练过程中，这些可训练的参数将被调整，以使输入嵌入与优化的预测输出对齐。
- en: The attention score in the attention module is dot product between Query and
    Key to get Query-Key score, and then dot product between the Query-Key and Value.
    By using attention score, each word pays attention to all the other words in the
    input sequence. For each word, the attention scores to all the other words in
    the input sequence are calculated, and larger attention score means higher relevance.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意力模块中的注意力分数是 Query 和 Key 之间的点积得到 Query-Key 分数，然后再计算 Query-Key 和 Value 之间的点积。通过使用注意力分数，每个词都会关注输入序列中的所有其他词。对于每个词，计算到输入序列中所有其他词的注意力分数，较大的注意力分数意味着更高的相关性。
- en: There are many variations of transformer structures. A transformer containing
    stacks of encoders and stacks of decoders can handle sequence-to-sequence NLP
    problem, while a transformer containing stacks of encoders followed by classification
    layers can be used for classification tasks.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变压器结构有许多变种。包含编码器堆栈和解码器堆栈的变压器可以处理序列到序列的 NLP 问题，而包含编码器堆栈后接分类层的变压器可以用于分类任务。
- en: Before an input sequence is passed to a transformer, the input sequence will
    go through an embedding layer and a position encoding layer. The embedding layer
    incorporates the meaning of the input, while the position encoding layer retrieves
    the position information of the input words. Different from RNN, where the input
    sequence position information is retained, attention mechanism parallelly calculates
    the attention scores without considering the position information. Thus, the position
    encoding layer helps to add back the position information.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在输入序列传递给变压器之前，输入序列将经过嵌入层和位置编码层。嵌入层包含了输入的语义，而位置编码层提取了输入词的位置信息。与 RNN 不同的是，RNN
    中保留了输入序列的位置信息，而注意力机制并行计算注意力分数，而不考虑位置信息。因此，位置编码层有助于重新添加位置信息。
- en: 2\. Neural Network with Transformers Code Example
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 使用变压器的神经网络代码示例
- en: In this session, we will go through the entire process of app sequence modelling.
    The transformer used here takes a reference from [**Transformer Model for Language
    Understanding tutorial**](https://www.tensorflow.org/text/tutorials/transformer)
    published by TensorFlow and [**Text Classification with Transformer tutorial**](https://keras.io/examples/nlp/text_classification_with_transformer/)
    published by Keras.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次会议中，我们将详细讲解应用序列建模的整个过程。这里使用的变压器参考了由 TensorFlow 发布的[**语言理解的变压器模型教程**](https://www.tensorflow.org/text/tutorials/transformer)和由
    Keras 发布的[**使用变压器进行文本分类教程**](https://keras.io/examples/nlp/text_classification_with_transformer/)。
- en: 'The input used in this example is a Pandas DataFrame df_app_sequence, and the
    DataFrame has three columns: userid, label, and app_sequence.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例中使用的输入是 Pandas DataFrame df_app_sequence，该 DataFrame 有三列：userid、label 和
    app_sequence。
- en: The following code goes through the entire process of the app sequence modelling.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了应用序列建模的整个过程。
- en: 1\. Tokenization and Sequence Padding
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The app_sequence is a list of installed app_id in the form of strings. Since
    a neural network does not take string input, the first thing we need to do is
    to tokenize the app_sequence and turn each app_id from a string into an integer.
    Here NUM_APPS is the number of apps in the corpus.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: After tokenization, we will need to pad each user’s installed app sequence to
    the same length. Here MAX_LENGTH is a hyperparameter. App sequences longer than
    MAX_LENGTH will be truncated, while app sequences shorter than MAX_LENGTH will
    be padded with zero at the end.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Define the Encoder Layer
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By the time this post publishes, `tensorflow.keras.layers.MultiHeadAttention`
    is only supported in TensorFlow versions higher than 2.4.0\. Here, we will implement
    the MultiHeadAttention given by the [Transformer Model for Language Understanding
    tutorial](https://www.tensorflow.org/text/tutorials/transformer).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'The scaled dot product function:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'The MultiHeadAttention function:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'Point-wise feed-forward layer:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'Encoder layer:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Encoder:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Define the Neural Network
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the neural network, we implement stacks of transformer encoders, followed
    by a series of classification layers.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'The neural network structure is:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ecc0c0fca986b6ca6043cce360752d72.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: The Neural Network Structure
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Training and Evaluation
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An Adam optimizer is used. Training can be examined after epochs of training.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Saving and Reloading the Model
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the model has custom object (the encoder layer), we chose to save the
    model as TensorFlow SavedModel format. Both TensorFlow SavedModel format and Keras
    model work for saving model with custom object, however we found TensorFlow SavedModel
    format is more easy and straightforward. If you are interested to learn the difference
    in saving model with custom object as TensorFlow SavedModel and Keras model, I
    would recommend you to check out [**Murat Karakaya’s post**](https://medium.com/deep-learning-with-keras/save-load-keras-models-with-custom-layers-8f55ba9183d2).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Here, the model is saved as TensorFlow SavedModel by using `tf.function`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: The model can be reloaded and used for prediction.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Deployment on Spark
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During the model training, we were using Pandas DataFrame on a single GPU server.
    However, when it comes to model deployment for batch inference, we hope to deploy
    the trained TensorFlow model on Spark and make inference on large-scale datasets
    on distributed Spark environment.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: To deploy the TensorFlow model with custom object (the encoder layer) on Spark,
    the key challenge is how to serialise the trained model. I found [**a useful post**](https://github.com/tensorflow/tensorflow/issues/31421)
    on solving this issue.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: The solution is to upload the trained model repository from local path to HDFS
    first.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Then, use SparkFiles to upload the HDFS model repository to every working node
    in the cluster. By setting recursive=True, the entire repository gets uploaded.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Thirdly, wrap the prediction function.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: In the end, call get_model_prediction function in the Pandas UDF function, and
    inference on Spark is done!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，在 Pandas UDF 函数中调用 get_model_prediction 函数，Spark 上的推断就完成了！
- en: Summary
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this post, we covered:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们涵盖了：
- en: Some key concepts about transformer.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关于 Transformer 的一些关键概念。
- en: An example of how to build sequence model with transformer.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个使用 Transformer 构建序列模型的示例。
- en: How to deploy the sequence model on spark for batch inference.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何在 Spark 上部署序列模型进行批量推断。
- en: Hope you enjoyed reading!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你喜欢阅读！
