- en: An Example of Sequence Modelling with Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/an-example-of-sequence-modelling-with-transformer-7d4c7dc85fc3](https://towardsdatascience.com/an-example-of-sequence-modelling-with-transformer-7d4c7dc85fc3)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: NLP Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some concepts about transformer and an example on how to build a sequence model
    with transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jhwang1992m?source=post_page-----7d4c7dc85fc3--------------------------------)[![Jiahui
    Wang](../Images/91350774d661092f429d1b0591af95f4.png)](https://medium.com/@jhwang1992m?source=post_page-----7d4c7dc85fc3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7d4c7dc85fc3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7d4c7dc85fc3--------------------------------)
    [Jiahui Wang](https://medium.com/@jhwang1992m?source=post_page-----7d4c7dc85fc3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7d4c7dc85fc3--------------------------------)
    ·5 min read·Feb 10, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7516b578384db3e4826b2bf637a5ba5f.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Users’ behaviour sequence data can be used to predict users’ profile and preferences.
    For instance, we can learn a user’s gender from the user’s historical purchase
    records, or we can predict whether a user will resolve a loan based on this user’s
    repayment history.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we will try to predict users’ financial risk level by modelling
    the app instalment sequence on their phone. Here, we take the app instalment sequence
    modelling task as an NLP problem. Apps installed by a group of users form a corpus.
    Each app is analogous to a word, while each app instalment sequence by a user
    is analogous to a sentence in the NLP problem. To solve this app instalment sequence
    modelling task, we choose to use a neural network with transformers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this post, the followings will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Transformers and Why Transformers Work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural Network with Transformers Development and Deployment on Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1\. Transformers and Why Transformers Work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformers have been widely used in NLP problems, since Google published the
    paper [***Attention Is All You Need***](https://arxiv.org/abs/1706.03762). The
    attention module differentiates the transformers from the RNN structures. Ketan
    Doshi has [**a wonderful series of Medium posts**](/transformers-explained-visually-not-just-how-but-why-they-work-so-well-d840bd61a9d3)
    that well explains transformers and attention mechanism, both mathematically and
    visually. If you are interested, don’t forget to check out his series of posts.
    Here, I will summarise some key take-aways that I find particularly useful to
    understand transformers and attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: In each attention module, the embedding input goes through three linear transformations
    to form Query, Key, and Value. These three linear transformations are achieved
    by matrix multiplication, where the transformation matrix contains trainable parameters.
    In the training process, these trainable parameters will be tuned to align the
    input embedding for optimised prediction output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The attention score in the attention module is dot product between Query and
    Key to get Query-Key score, and then dot product between the Query-Key and Value.
    By using attention score, each word pays attention to all the other words in the
    input sequence. For each word, the attention scores to all the other words in
    the input sequence are calculated, and larger attention score means higher relevance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are many variations of transformer structures. A transformer containing
    stacks of encoders and stacks of decoders can handle sequence-to-sequence NLP
    problem, while a transformer containing stacks of encoders followed by classification
    layers can be used for classification tasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before an input sequence is passed to a transformer, the input sequence will
    go through an embedding layer and a position encoding layer. The embedding layer
    incorporates the meaning of the input, while the position encoding layer retrieves
    the position information of the input words. Different from RNN, where the input
    sequence position information is retained, attention mechanism parallelly calculates
    the attention scores without considering the position information. Thus, the position
    encoding layer helps to add back the position information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. Neural Network with Transformers Code Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this session, we will go through the entire process of app sequence modelling.
    The transformer used here takes a reference from [**Transformer Model for Language
    Understanding tutorial**](https://www.tensorflow.org/text/tutorials/transformer)
    published by TensorFlow and [**Text Classification with Transformer tutorial**](https://keras.io/examples/nlp/text_classification_with_transformer/)
    published by Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input used in this example is a Pandas DataFrame df_app_sequence, and the
    DataFrame has three columns: userid, label, and app_sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: The following code goes through the entire process of the app sequence modelling.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Tokenization and Sequence Padding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The app_sequence is a list of installed app_id in the form of strings. Since
    a neural network does not take string input, the first thing we need to do is
    to tokenize the app_sequence and turn each app_id from a string into an integer.
    Here NUM_APPS is the number of apps in the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: After tokenization, we will need to pad each user’s installed app sequence to
    the same length. Here MAX_LENGTH is a hyperparameter. App sequences longer than
    MAX_LENGTH will be truncated, while app sequences shorter than MAX_LENGTH will
    be padded with zero at the end.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Define the Encoder Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By the time this post publishes, `tensorflow.keras.layers.MultiHeadAttention`
    is only supported in TensorFlow versions higher than 2.4.0\. Here, we will implement
    the MultiHeadAttention given by the [Transformer Model for Language Understanding
    tutorial](https://www.tensorflow.org/text/tutorials/transformer).
  prefs: []
  type: TYPE_NORMAL
- en: 'The scaled dot product function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The MultiHeadAttention function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Point-wise feed-forward layer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Encoder layer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Define the Neural Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the neural network, we implement stacks of transformer encoders, followed
    by a series of classification layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The neural network structure is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ecc0c0fca986b6ca6043cce360752d72.png)'
  prefs: []
  type: TYPE_IMG
- en: The Neural Network Structure
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is:'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Training and Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An Adam optimizer is used. Training can be examined after epochs of training.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Saving and Reloading the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the model has custom object (the encoder layer), we chose to save the
    model as TensorFlow SavedModel format. Both TensorFlow SavedModel format and Keras
    model work for saving model with custom object, however we found TensorFlow SavedModel
    format is more easy and straightforward. If you are interested to learn the difference
    in saving model with custom object as TensorFlow SavedModel and Keras model, I
    would recommend you to check out [**Murat Karakaya’s post**](https://medium.com/deep-learning-with-keras/save-load-keras-models-with-custom-layers-8f55ba9183d2).
  prefs: []
  type: TYPE_NORMAL
- en: Here, the model is saved as TensorFlow SavedModel by using `tf.function`.
  prefs: []
  type: TYPE_NORMAL
- en: The model can be reloaded and used for prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Deployment on Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During the model training, we were using Pandas DataFrame on a single GPU server.
    However, when it comes to model deployment for batch inference, we hope to deploy
    the trained TensorFlow model on Spark and make inference on large-scale datasets
    on distributed Spark environment.
  prefs: []
  type: TYPE_NORMAL
- en: To deploy the TensorFlow model with custom object (the encoder layer) on Spark,
    the key challenge is how to serialise the trained model. I found [**a useful post**](https://github.com/tensorflow/tensorflow/issues/31421)
    on solving this issue.
  prefs: []
  type: TYPE_NORMAL
- en: The solution is to upload the trained model repository from local path to HDFS
    first.
  prefs: []
  type: TYPE_NORMAL
- en: Then, use SparkFiles to upload the HDFS model repository to every working node
    in the cluster. By setting recursive=True, the entire repository gets uploaded.
  prefs: []
  type: TYPE_NORMAL
- en: Thirdly, wrap the prediction function.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, call get_model_prediction function in the Pandas UDF function, and
    inference on Spark is done!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this post, we covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Some key concepts about transformer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An example of how to build sequence model with transformer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to deploy the sequence model on spark for batch inference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hope you enjoyed reading!
  prefs: []
  type: TYPE_NORMAL
