- en: 'XGBoost: Theory and Hyperparameter Tuning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost：理论与超参数调优
- en: 原文：[https://towardsdatascience.com/xgboost-theory-and-hyperparameter-tuning-bc4068aba95e](https://towardsdatascience.com/xgboost-theory-and-hyperparameter-tuning-bc4068aba95e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/xgboost-theory-and-hyperparameter-tuning-bc4068aba95e](https://towardsdatascience.com/xgboost-theory-and-hyperparameter-tuning-bc4068aba95e)
- en: A complete guide with examples in Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个包含Python示例的完整指南
- en: '[](https://jorgemartinlasaosa.medium.com/?source=post_page-----bc4068aba95e--------------------------------)[![Jorge
    Martín Lasaosa](../Images/21b4e500b7d14204ea76f579c3e2433f.png)](https://jorgemartinlasaosa.medium.com/?source=post_page-----bc4068aba95e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bc4068aba95e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bc4068aba95e--------------------------------)
    [Jorge Martín Lasaosa](https://jorgemartinlasaosa.medium.com/?source=post_page-----bc4068aba95e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jorgemartinlasaosa.medium.com/?source=post_page-----bc4068aba95e--------------------------------)[![Jorge
    Martín Lasaosa](../Images/21b4e500b7d14204ea76f579c3e2433f.png)](https://jorgemartinlasaosa.medium.com/?source=post_page-----bc4068aba95e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bc4068aba95e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bc4068aba95e--------------------------------)
    [Jorge Martín Lasaosa](https://jorgemartinlasaosa.medium.com/?source=post_page-----bc4068aba95e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bc4068aba95e--------------------------------)
    ·17 min read·Feb 16, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bc4068aba95e--------------------------------)
    ·17分钟阅读·2023年2月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c1e8bc4dcd14576319bd1f517bfeaa2b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1e8bc4dcd14576319bd1f517bfeaa2b.png)'
- en: Photo by [Joanne Francis](https://unsplash.com/@nipawinnews?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Joanne Francis](https://unsplash.com/@nipawinnews?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In a few months, I will have been working as a Data Scientist for 3 years. I
    know it is not a long career yet, but together with my academic experience, I
    have been able to work on several machine learning projects for different sectors
    (energy, customer experience…). All of them were fed by *tabular data,* which
    means structured data (organised in rows and columns). In contrast, there are
    projects fed by unstructured data such as images or text which are more related
    to machine learning fields such as Computer Vision or Natural Language Processing
    (NLP).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 几个月后，我将从事数据科学工作满3年。我知道这还不算长的职业生涯，但结合我的学术经验，我已经能够参与多个不同行业的机器学习项目（能源、客户体验等）。所有这些项目都使用了
    *表格数据*，即结构化数据（按行和列组织）。相比之下，使用图像或文本等非结构化数据的项目则更多地与计算机视觉或自然语言处理（NLP）等机器学习领域相关。
- en: Based on my experience, XGBoost *usually* performs well with *tabular data*
    projects. Although the No Free Lunch Theorem [1] states that any two algorithms
    are equivalent when their performances are averaged across all possible problems,
    on Bojan Tunguz’s Twitter [2] you can read frequent discussions with other professionals
    about why tree-based models (and specially XGBoost) are often the best candidates
    for tackling *tabular data* projects, even with the growing research into the
    use of Deep Learning techniques for this type of data. [3]
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我的经验，XGBoost *通常* 在 *表格数据* 项目中表现良好。尽管“无免费午餐定理” [1] 表明当将两种算法的表现平均到所有可能的问题上时，它们是等效的，但在Bojan
    Tunguz的Twitter [2] 上，你可以阅读到与其他专业人士的频繁讨论，关于为何基于树的模型（尤其是XGBoost）通常是解决 *表格数据* 项目最佳候选者，即便随着对深度学习技术应用于这种数据的研究不断增加。
    [3]
- en: Also, it is quite funny to see how a Kaggle Grandmaster [4] jokes about being
    an XGBoost evangelist.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，看到一位Kaggle大师 [4] 玩笑说自己是XGBoost的宣传者也挺有趣的。
- en: Bojan Tunguz’s pinned tweet.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Bojan Tunguz的置顶推文。
- en: Despite the great success of XGBoost, when I wanted to get the best performance
    out of the model in the past, I did not find complete guides where all the needed
    knowledge is centralised. There are great theoretical and practical explanations
    that I will be referencing throughout the reading, but I have not found any complete
    guide that gives a holistic view. That is why I have decided to write this article.
    Furthermore, I am going to apply what I have collected here to a well-known Kaggle
    competition called [House Prices — Advanced Regression Techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 XGBoost 取得了巨大的成功，但在过去当我想从模型中获得最佳性能时，我没有找到集中所有必要知识的完整指南。虽然有很多理论和实践上的解释，我将在阅读中参考这些内容，但我没有找到任何完整的指南提供整体视角。这就是我决定写这篇文章的原因。此外，我将把我在这里收集到的内容应用到一个著名的
    Kaggle 比赛中，[房价预测 — 高级回归技术](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)。
- en: 'The rest of the article is divided into two sections:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 文章的其余部分分为两个部分：
- en: '**Theory:** after a short introduction, we will dive into the original paper
    to understand the theory behind this great model. Then, a brief visual explanation
    will help us to better understand the theory.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理论：** 在简短的介绍后，我们将深入原始论文，以理解这个伟大模型背后的理论。接着，简要的视觉解释将帮助我们更好地理解理论。'
- en: '**Practice:** after an overview of the XGBoost parameters, I will present a
    step-by-step guide for tuning the hyperparameters.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实践：** 在概述 XGBoost 参数后，我将提供一个逐步指南来调整超参数。'
- en: All images unless otherwise noted are by the author.
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 除非另有说明，否则所有图片均由作者提供。
- en: Theory
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理论
- en: '**XGBoost** stands for e**X**treme **G**radient **Boos**ting and was officially
    published by Tianqi Chen and Carlos Guestrin in 2016 [5]. Even before its publication,
    it was established as one of the best algorithms to use in Kaggle competitions.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**XGBoost** 代表 e**X**treme **G**radient **Boos**ting，由 Tianqi Chen 和 Carlos
    Guestrin 于 2016 年正式发布 [5]。在发布之前，它已经被确立为 Kaggle 比赛中最优秀的算法之一。'
- en: Nevertheless, and despite the fact that the rise of Deep Learning is collecting
    incredible success in fields such as Computer Vision and NLP, XGBoost and other
    tree-based models (CatBoost [6] or LightGBM [7]) continue to be one of the best
    options for predicting *tabular data* [8]. All these tree-based algorithms are
    based on Gradient Boosting, and if you want to understand how this technique works,
    I recommend you to check my article on tree ensembles. [9]
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习在计算机视觉和自然语言处理等领域取得了巨大成功，XGBoost 和其他基于树的模型（CatBoost [6] 或 LightGBM [7]）仍然是预测*表格数据*
    [8] 的最佳选项之一。所有这些基于树的算法都基于梯度提升，如果你想了解这种技术是如何工作的，我建议你查看我关于树集成的文章。[9]
- en: '[](/tree-ensembles-theory-and-practice-1cf9eb27781?source=post_page-----bc4068aba95e--------------------------------)
    [## Tree Ensembles: Bagging, Boosting and Gradient Boosting'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[## Tree Ensembles: Bagging, Boosting and Gradient Boosting](https://towardsdatascience.com/tree-ensembles-theory-and-practice-1cf9eb27781?source=post_page-----bc4068aba95e--------------------------------)'
- en: Theory and practice explained in detail
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理论与实践详细解释
- en: towardsdatascience.comf](/tree-ensembles-theory-and-practice-1cf9eb27781?source=post_page-----bc4068aba95e--------------------------------)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](https://towardsdatascience.com/tree-ensembles-theory-and-practice-1cf9eb27781?source=post_page-----bc4068aba95e--------------------------------)'
- en: 'Do you want to know what makes XGBoost special? I will explain it in two different
    subsections: **Original Paper** and **Visual Explanation.** Let’s get started!'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你想知道 XGBoost 有什么特别之处吗？我将通过两个不同的子部分来解释：**原始论文**和**视觉解释**。让我们开始吧！
- en: Original Paper
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原始论文
- en: 'As mentioned before, XGBoost is based on Gradient Boosting, so several trees
    are trained sequentially on the residuals of the previous trees. However, there
    is a combination of some **minor improvements** that allowed XGBoost to outperform
    the existing tree ensembles models by **preventing overfitting**:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，XGBoost 基于梯度提升，因此多个树是依次在前一棵树的残差上进行训练的。然而，有一些**小的改进**使得 XGBoost 能够通过**防止过拟合**超越现有的树集成模型：
- en: '**Regularized Learning Objective** (similar to *RGF* [10]). In gradient boosting,
    each tree is trained in the best possible way to achieve the learning objective:
    reduce the difference between the predictions and the target. This learning objective
    is replaced in XGBoost by a regularised learning objective, which adds a regularisation
    term to the calculation of the difference. In plain English, this term *adds*
    noise when each tree learns and is intended to reduce the prediction’s sensitivity
    to individual observations. If the regularization term is set to zero, the objective
    falls back to the traditional gradient boosting.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化学习目标**（类似于*RGF* [10]）。在梯度提升中，每棵树都以最佳方式训练，以实现学习目标：减少预测与目标之间的差异。在XGBoost中，这个学习目标被一个正则化学习目标取代，该目标在差异计算中添加了一个正则化项。用简单的话说，这个项*增加*了每棵树学习时的噪音，并旨在减少预测对单个观察值的敏感性。如果将正则化项设置为零，则目标会回到传统的梯度提升。'
- en: '**Shrinkage** (borrowed from *Random Forests* [11]).A technique that limits
    the weight that each trained tree has in the final prediction. Thus, the influence
    of each tree is reduced and there is more space for future trees to improve the
    predictions. It is similar to the learning rate (and also specified as it in the
    parameters section).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**收缩**（借鉴自*随机森林* [11]）。一种限制每棵训练树在最终预测中权重的技术。因此，每棵树的影响被减少，未来的树有更多空间来改善预测。这类似于学习率（在参数部分也如此指定）。'
- en: '**Column Subsampling** (borrowed from *Random Forests* [11]). It allows to
    randomly select a subsample of features for each tree, tree level and/or tree
    node. Therefore, a very important feature for the first tree/level/node, could
    not be available for the second one, pushing it to use other features.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**列子采样**（借鉴自*随机森林* [11]）。它允许为每棵树、树级别和/或树节点随机选择一个特征子样本。因此，对第一棵树/级别/节点非常重要的特征，可能在第二棵树中不可用，从而推动使用其他特征。'
- en: 'One of the biggest problems in tree learning is finding the best split. If
    there is a continuous feature that varies from 0 to 100, what value should be
    used for doing the split? 20? 32.5? 70? In order to find the best split, XGBoost
    can apply different algorithms:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 树学习中的最大问题之一是找到最佳分裂。如果有一个从0到100变化的连续特征，那么应该使用什么值来进行分裂？20？32.5？70？为了找到最佳分裂，XGBoost可以应用不同的算法：
- en: '**Exact greedy algorithm.** It is the most commonly used algorithm in older
    tree-boosting implementations which consists of testing all possible splits of
    all features. Doing it for continuous features is computationally demanding and
    therefore requires more time as the data sample increases.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确贪心算法**。这是旧版树提升实现中最常用的算法，它包括测试所有特征的所有可能分裂。对连续特征进行这种操作计算量大，因此随着数据样本的增加，需要更多时间。'
- en: '**Weighted quantile sketch algorithm**: proposes candidate splitting points
    according to percentiles of feature distributions. The algorithm then maps the
    continuous features into buckets split by these candidate points, aggregates the
    statistics and finds the best solution among proposals based on aggregated statistics.
    Furthermore, it can handle *weighted* data and includes a default direction in
    each tree node to make the algorithm aware of the sparsity patterns in the data.
    The sparsity may be due to the presence of missing values, frequent zero entries
    or the consequences of feature engineering (e.g. use of One Hot Encoding).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加权分位数草图算法**：根据特征分布的百分位数提出候选分裂点。该算法将连续特征映射到这些候选点划分的桶中，聚合统计数据，并根据聚合统计数据在提议中找到最佳解决方案。此外，它可以处理*加权*数据，并在每个树节点中包含一个默认方向，使算法能够识别数据中的稀疏模式。稀疏性可能由于缺失值的存在、频繁的零条目或特征工程的后果（例如使用独热编码）造成。'
- en: In addition, the algorithm is designed to interact with the system efficiently.
    Without going into detail, I can point out that the data is stored in units in
    memory called blocks to help the algorithm sort the data. This technique allows
    parallelisation. Moreover, in the case of large datasets, cache-aware access is
    defined to avoid slowing down the search for splits when some computations do
    not fit in the CPU cache. Finally, it uses block compression and block fragmentation
    to handle data that does not fit in the main memory.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，该算法设计用于高效地与系统交互。无需详细说明，我可以指出数据存储在内存中的称为块的单元中，以帮助算法对数据进行排序。这种技术允许并行化。此外，在大数据集的情况下，定义了缓存感知访问，以避免在某些计算无法适应
    CPU 缓存时搜索分裂的速度变慢。最后，它使用块压缩和块碎片化来处理不适合主内存的数据。
- en: Visual Explanation
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉解释
- en: This section is highly inspired by [Josh Starmer’s Youtube channel](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw)
    and [Shreya Rao’s article](/xgboost-regression-explain-it-to-me-like-im-10-2cf324b0bbdb).
    My goal is to explain with a little example how XGBoost works and link it with
    the paper theory seen before. To do this, let’s follow the steps below.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本节高度受到[Josh Starmer 的 YouTube 频道](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw)和[Shreya
    Rao 的文章](/xgboost-regression-explain-it-to-me-like-im-10-2cf324b0bbdb)的启发。我的目标是通过一个小示例解释
    XGBoost 的工作原理，并将其与之前看到的论文理论联系起来。为此，让我们按照以下步骤进行。
- en: '***Step 1: Create synthetic data***'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '***步骤 1：创建合成数据***'
- en: A small synthetic dataset about house pricing is used. It has two features (*Square
    Meters* and *Has Garage?*) and a target (*Price*).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用了一个关于房价的小型合成数据集。它有两个特征（*平方米数*和*是否有车库？*）和一个目标（*价格*）。
- en: '![](../Images/e049b46744eb2c330927b0f775fbb4ed.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e049b46744eb2c330927b0f775fbb4ed.png)'
- en: Synthetic data about house pricing used for the visual explanation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 用于视觉解释的关于房价的合成数据。
- en: '***Step 2: Calculate the residuals***'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '***步骤 2：计算残差***'
- en: As you have already seen, residuals have been calculated in the table shown
    above. The calculation is simple, you only have to subtract the predicted price
    of the previous tree from the real price (remember that XGBoost train several
    trees sequentially). However, this is the first tree, so we do not have predicted
    prices. In that case, a mean of the price is calculated.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，残差已经在上表中计算出来。计算方法很简单，你只需将前一个树的预测价格从实际价格中减去（记住，XGBoost 会顺序训练多个树）。然而，这还是第一棵树，所以我们没有预测价格。在这种情况下，计算了价格的平均值。
- en: '![](../Images/50502b9ce92a913892bd8cc736b602a5.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50502b9ce92a913892bd8cc736b602a5.png)'
- en: Calculation of the first residuals.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 计算第一个残差。
- en: '***Step 3: Build the first tree of XGBoost***'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '***步骤 3：构建 XGBoost 的第一棵树***'
- en: The first tree is going to be trained with all the residuals as the target.
    So the first thing to do is to calculate the **similarity score** for all the
    residuals. This is the score that the tree splits intend to augment.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 第一棵树将用所有的残差作为目标进行训练。因此，首先需要计算所有残差的**相似性得分**。这是树分裂旨在增加的得分。
- en: '![](../Images/fee2e131b782c121f1d337d6001d0bb1.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fee2e131b782c121f1d337d6001d0bb1.png)'
- en: Calculation of the Similarity Score for the first tree.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 计算第一个树的相似性得分。
- en: Do you see the lambda (λ) character? As explained in the paper theory section,
    this is the **regularisation term**that helps to prevent overfitting by *adding*
    noise. Thus, the XGBoost learning objective (augment the similarity score) is
    actually a regularized learning objective. The default value for the regularisation
    term in XGBoost is 1\. Now, let’s see how the feature *Has Garage?* influence
    in the similarity score and the gain.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你看到那个 lambda (λ) 字符了吗？如论文理论部分所述，这是一个**正则化项**，通过*添加*噪声来帮助防止过拟合。因此，XGBoost 的学习目标（增强相似性得分）实际上是一个正则化学习目标。XGBoost
    中正则化项的默认值是 1。现在，让我们看看特征*是否有车库？*对相似性得分和增益的影响。
- en: '![](../Images/eb9adc9d4791d5570f2558d2deec27e5.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb9adc9d4791d5570f2558d2deec27e5.png)'
- en: Similarity scores calculated for the split with the g*arage* feature.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 计算了带有*车库*特征的分裂的相似性得分。
- en: '![](../Images/b370f375828e78c5996d10f2332d60e9.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b370f375828e78c5996d10f2332d60e9.png)'
- en: Final gain for garage split.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 车库分裂的最终增益。
- en: Once the similarity scores are calculated, the gain for the feature *Has Garage?*
    is 7448,58\. If the final gain is positive, then it is a good split, otherwise,
    it is not. As it is positive, we can conclude that is a good split.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算了相似性得分，*是否有车库？*的特征增益为 7448.58。如果最终增益为正，则这是一个好的分裂，否则不是。由于它是正的，我们可以得出这是一个好的分裂。
- en: For a continuous feature like *Square Meters,* the process is slightly different.
    First, we need to find the best split for the continuous feature. For that purpose,
    the **exact greedy algorithm** explained in the paper theory section will be used.
    That is to say, every possible split is going to be tested.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像*平方米*这样的连续特征，过程略有不同。首先，我们需要找到连续特征的最佳分裂。为此，将使用论文理论部分中解释的**精确贪婪算法**。也就是说，将测试每一个可能的分裂。
- en: '![](../Images/4c350744c5d2c5b9f541e791d4c1e63b.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c350744c5d2c5b9f541e791d4c1e63b.png)'
- en: Sorting the feature and finding all the possible splits (greedy).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对特征进行排序并找到所有可能的分裂（贪婪）。
- en: Sorting shown in the figure above can be computationally expensive when datasets
    are big. As commented in the paper theory section, XGBoost uses **block units**
    that allow parallelization and help with this problem. Also, remember that XGBoost
    can use the **weighted quantile sketch algorithm** to propose candidate splitting
    points according to percentiles of feature distributions. This is not going to
    be explained here, but it is one of the main advantages of XGBoost. That being
    said, we are going to calculate the similarity scores and gain for each possible
    split.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示的排序在数据集很大的时候可能会计算成本高。正如论文理论部分评论的那样，XGBoost使用**块单元**来允许并行化，并帮助解决这个问题。同时，请记住，XGBoost可以使用**加权分位数草图算法**根据特征分布的百分位数来提议候选分裂点。这里不会详细解释，但这是XGBoost的主要优势之一。话虽如此，我们将计算每个可能分裂的相似度得分和增益。
- en: '![](../Images/d2c4ca8c8a970a9e0fb11a22a44daba2.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2c4ca8c8a970a9e0fb11a22a44daba2.png)'
- en: Calculations of the gain for each Square Meters split.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 每个平方米分裂的增益计算。
- en: The split of the *Square Meters* feature by 175 has the greatest gain (even
    greater than the *Has Garage?* split), so it should be the first split. Only one
    residual is left in the right leaf (156). So let’s focus on the left leaf and
    look for another split. After all the calculations, the best split is done again
    with *Square Meters* feature.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*平方米*特征通过175的分裂具有最大的增益（甚至比*是否有车库？*的分裂还要大），因此它应该是第一个分裂。右叶子中只剩下一个残差（156）。所以我们来关注左叶子，寻找另一个分裂。经过所有计算，最佳的分裂再次使用*平方米*特征。'
- en: '![](../Images/1e39a1c0e9383f32de1a31f2c30d54cc.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e39a1c0e9383f32de1a31f2c30d54cc.png)'
- en: Best second split on garage feature.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳第二次分裂在车库特征上。
- en: After new calculations, the right leaf can be split again.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 经过新计算，右叶子可以再次分裂。
- en: '![](../Images/8afd0f0a292e55f79beefabf79c71e53.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8afd0f0a292e55f79beefabf79c71e53.png)'
- en: Final tree with every possible split done.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 完成所有可能分裂的最终树。
- en: '***Step 4: Pruning the tree***'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '***步骤4：剪枝树***'
- en: 'In order to avoid overfitting, there is another process called tree pruning
    that XGBoost does. From bottom to top every gain is validated. How? If an XGBoost
    parameter called **gamma** (γ) is greater than the gain, then the split is removed.
    The default value of γ in XGBoost is 0, but a 200 value is set to represent a
    case in which a split is removed. Why? Because in that case, 200 (γ) is greater
    than the last gain (198). So, the tree is pruned and the final result is:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免过拟合，还有一个叫做树剪枝的过程是XGBoost执行的。从下往上验证每个增益。怎么做？如果一个叫做**gamma**（γ）的XGBoost参数大于增益，则会移除分裂。XGBoost中γ的默认值是0，但设置为200以表示移除分裂的情况。为什么？因为在这种情况下，200（γ）大于最后的增益（198）。因此，树被剪枝，最终结果是：
- en: '![](../Images/5dca48ca940139086b47e228507545bc.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5dca48ca940139086b47e228507545bc.png)'
- en: Last version of the first tree after pruning.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝后的第一棵树的最终版本。
- en: '***Step 5: Make predictions with the tree***'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '***步骤5：使用树进行预测***'
- en: 'In order to do predictions, the first step is to have a single value (output
    value) in each final leaf. For that, we use the following formula:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行预测，第一步是每个最终叶子中都有一个单一的值（输出值）。为此，我们使用以下公式：
- en: '![](../Images/c7154226da6092d01863b1721421d288.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7154226da6092d01863b1721421d288.png)'
- en: Calculation of the output value (value of each leaf).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 计算输出值（每个叶子的值）。
- en: 'The result is:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是：
- en: '![](../Images/ffb55aeba472b69a3e2ab8e795478229.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ffb55aeba472b69a3e2ab8e795478229.png)'
- en: First tree with an output value for each leaf.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 每个叶子都有一个输出值的第一棵树。
- en: Finally, the output values of the final leaves can be used for doing new predictions
    with the formula shown below. Being *i* the observation that we want to predict,
    *prediction_t0* the first prediction (mean of observed price), *ɛ* the learning
    rate and *leaf_i* the value for the observation *i* if we follow the tree rules.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，最终叶子的输出值可以用下面显示的公式进行新预测。*i* 是我们想要预测的观察值，*prediction_t0* 是第一次预测（观察价格的均值），*ɛ*
    是学习率，*leaf_i* 是按照树的规则得到的观察值 *i* 的值。
- en: '![](../Images/50906e985cf106da45eaba4b89833934.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50906e985cf106da45eaba4b89833934.png)'
- en: 'Note that the learning rate (*ɛ*), whose default value is 0.3, is the **shrinkage**
    explanation of the paper theory section. That being said, let’s predict all the
    observations:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，学习率（*ɛ*），其默认值为 0.3，是**收缩**理论部分的解释。话虽如此，让我们预测所有观察值：
- en: '![](../Images/7eb869512d15da5b3343876a4db6126b.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7eb869512d15da5b3343876a4db6126b.png)'
- en: Predictions for each value.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 每个值的预测。
- en: 'Now, we use the predictions to calculate new residuals (Residuals_1):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用预测来计算新的残差（Residuals_1）：
- en: '![](../Images/cd24f341ebaa64cb1f2437178c347f5d.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd24f341ebaa64cb1f2437178c347f5d.png)'
- en: Residuals of the first tree.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 第一棵树的残差。
- en: As we can see above, every residual (except the first one) is closer to zero.
    That means that the first tree gives information that improves the first prediction
    (the mean). The next step would be to create another tree that gives another step
    in the direction of reducing the residuals.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上面看到的，每个残差（除了第一个）都更接近零。这意味着第一棵树提供的信息改善了第一次预测（均值）。下一步是创建另一棵树，使残差减少的方向再进一步。
- en: '***Step 6: Train new trees***'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '***步骤 6：训练新树***'
- en: 'If we want to create a new tree, the only thing to do is to redo steps 3–5
    with the new residuals (Residuals_1) as the target. Then, the final prediction
    would be:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想创建一棵新树，唯一需要做的就是用新的残差（Residuals_1）作为目标重新执行步骤 3-5。然后，最终预测将是：
- en: '![](../Images/a9d8fd8988703141e2d96c31c1b4faf6.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9d8fd8988703141e2d96c31c1b4faf6.png)'
- en: Calculation of the final predictions with two trees.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用两棵树计算最终预测。
- en: XGBoost can sequentially train trees using these steps. Personally, I find that
    the visual explanation is an effective way to comprehend the model and its theory.
    I hope it was helpful for you as well. Be that as it may, now it’s time to proceed
    with the practical section.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 可以通过这些步骤顺序训练树。就我个人而言，我发现视觉解释是理解模型及其理论的有效方法。我希望这对你也有帮助。不过，现在是时候进入实际部分了。
- en: Practice
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践
- en: First, I would like to emphasize that while I am not an expert in the practical
    side of XGBoost, I have used certain techniques that have proven to be successful.
    As a result, I think that I can provide a brief guide to help you tune the hyperparameters.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我想强调的是，尽管我不是 XGBoost 实践方面的专家，但我使用了一些证明成功的技术。因此，我认为我可以提供一个简要指南，帮助你调整超参数。
- en: To achieve this goal, it is essential to divide the practice into two subsections.
    The first one will focus on defining the **hyperparameters** of the algorithm
    and compiling them into a concise cheat sheet. The second subsection, called **Hands-On
    Implementation**, will provide a step-by-step guide that will walk you through
    the process of training XGBoost.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，必须将实践分为两个子部分。第一个部分将专注于定义算法的**超参数**并将其编译成一个简明的备忘单。第二个子部分称为**实际操作**，将提供一个逐步指南，带你完成
    XGBoost 的训练过程。
- en: Hyperparameters
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数
- en: 'All the information explained here can be found in the official XGBoost documentation
    [12]. One of the general parameters is known as **booster**. As I explained in
    my previous article [9], gradient boosting is a technique that is not restricted
    to using decision trees, but any model. That is why XGBoost accepts three values
    for the **booster** parameter:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 所有在这里解释的信息都可以在官方 XGBoost 文档 [12] 中找到。一个通用的参数被称为**booster**。正如我在之前的文章 [9] 中解释的那样，梯度提升是一种不限于使用决策树的技术，它可以应用于任何模型。这就是为什么
    XGBoost 接受**booster**参数的三种值：
- en: '*gbtree*: a gradient boosting with decision trees (default value)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*gbtree*：使用决策树的梯度提升（默认值）'
- en: '*dart*: a gradient boosting with decision trees that uses a method proposed
    by Vinayak and Gilad-Bachrach (2015) [13] that adds dropout techniques from the
    deep neural net community to boosted trees.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*dart*：一种使用 Vinayak 和 Gilad-Bachrach（2015）[13] 提出的将深度神经网络社区中的 dropout 技术添加到提升树中的方法的决策树梯度提升。'
- en: '*gblinear*: a gradient boosting with linear functions.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*gblinear*：使用线性函数的梯度提升。'
- en: While *gblinear* is the best option to catch linear links between predictors
    and the outcome, boosters based on decision trees (*gbtree* and *dart*) are much
    better to catch non-linear links. **As g*btree* is the most used value, the rest
    of the article is going to use it**. If you are interested in using the others,
    check the documentation [12] because depending on your choice the parameters change.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然*gblinear*是捕捉预测变量和结果之间线性关系的最佳选择，但基于决策树的提升器（*gbtree*和*dart*）在捕捉非线性关系方面要好得多。**由于*gbtree*是最常用的值，本文余下部分将使用它**。如果你有兴趣使用其他方法，请查看文档[12]，因为根据你的选择，参数会有所不同。
- en: 'Once we set *gbtree* as the **booster**, there are several parameters that
    we can tune in order to get the best model. The most important ones are explained
    here:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将*gbtree*设置为**booster**，我们可以调整多个参数以获得最佳模型。这里解释了最重要的几个：
- en: '**eta** (a.k.a learning rate): shown in the visual explanation section as ***ɛ,***
    it limits the weight each trained tree has in the final prediction to make the
    boosting process more conservative.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**eta**（又名学习率）：在视觉解释部分显示为***ɛ***，它限制每棵训练树在最终预测中的权重，以使提升过程更具保守性。'
- en: '**gamma:** shown in the visual explanation section as **γ**, it marks the minimum
    gain required to make a further partition on a leaf node of the tree.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**gamma:** 在视觉解释部分显示为**γ**，它标记了在树的叶节点上进行进一步划分所需的最小增益。'
- en: '**max_depth:** sets the maximum depth of a tree.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_depth:** 设置树的最大深度。'
- en: '**n_estimators**: number of trees trained.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**n_estimators**: 训练的树的数量。'
- en: '**min_child_weight:** sets the minimum number of instance weights (sum of residuals)
    needed in a child for doing a split.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**min_child_weight:** 设置进行拆分时子节点所需的最小实例权重（残差之和）。'
- en: '**subsample:** sets the percentage of the sample got (randomly) before training
    each tree.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**subsample:** 设置在训练每棵树之前获得的样本百分比（随机）。'
- en: '**colsample_by[]:** a collection of parameters of subsampling columns. The
    subsampling can be done for each tree (**colsample_bytree**), for each depth level
    reached in a tree (**colsample_bylevel**) or for every time a new split is evaluated
    (**colsample_bynode**). Note that these parameters can work simultaneously: if
    every parameter has 0.5 value and you have 32 columns, then each split would use
    4 columns (32/ 2³)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**colsample_by[]:** 一组用于列抽样的参数。抽样可以在每棵树（**colsample_bytree**）、每棵树中达到的深度级别（**colsample_bylevel**）或每次评估新拆分时（**colsample_bynode**）进行。注意，这些参数可以同时工作：如果每个参数的值为0.5，并且你有32列，那么每个拆分将使用4列（32/
    2³）。'
- en: '**lambda** (L2 regularization):shown in the visual explanation as **λ**. It
    decreases the output value (step 5 in the visual explanation) smoothly as it increases
    the denominator. [14]'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**lambda**（L2 正则化）：在视觉解释中显示为**λ**。它通过增加分母平滑地减少输出值（视觉解释中的第5步）。[14]'
- en: '**alpha** (L1 regularization): It decreases the output value and promotes sparsity
    by forcing output values to be 0\. [14]'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**alpha**（L1 正则化）：它通过强制输出值为0来减少输出值并促进稀疏性。[14]'
- en: '**tree_method:** it sets the construction algorithm used by the tree for finding
    the splits. As discussed in the paper theory section, exact or approximate algorithms
    can be used. In practice, there are 5possible values for this parameter: *auto*
    for letting a heuristic select the fastest option among the next ones, *exact*
    for applying the exact greedy algorithm that enumerates all split candidates,
    *approx* for applying an approximate greedy algorithm that uses quantile sketch
    and gradient histogram, *hist* for using a histogram-optimized version of the
    approximate algorithm and *gpu_hist* for using the GPU implementation of *hist.*'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tree_method:** 设置树用于查找拆分的构建算法。正如论文理论部分所讨论的，可以使用精确或近似算法。在实践中，这个参数有5个可能的值：*auto*
    让启发式算法从下列选项中选择最快的选项，*exact* 应用枚举所有拆分候选项的精确贪婪算法，*approx* 应用使用分位数草图和梯度直方图的近似贪婪算法，*hist*
    使用近似算法的直方图优化版本，*gpu_hist* 使用*hist*的GPU实现。'
- en: '**scale_pos_weight**: it controls the balance of positive and negative weights,
    which can be useful for unbalanced classes. A typical value to consider is *sum(negative
    instances) / sum(positive instances).*'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**scale_pos_weight**: 它控制正负权重的平衡，这对于类别不平衡的情况非常有用。一个典型的值是*sum(negative instances)
    / sum(positive instances)*。'
- en: '**max_leaves**: it sets the maximum number of nodes to be added only when the
    *exact* value for the **tree_method** parameter is not chosen.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_leaves**: 设置仅在未选择**tree_method**参数的*exact*值时要添加的最大节点数。'
- en: 'There are more parameters to tune: **updater**, **refresh_leaf**, **process_type**,
    **grow_policy**, **max_bin**, **predictor**, **num_parallel_tree**, **monotone_constraints**
    or **interaction_constraints**. However, the former ones are enough to make the
    most of your XGBoost model and therefore, those were explained.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 还有更多参数需要调整：**updater**、**refresh_leaf**、**process_type**、**grow_policy**、**max_bin**、**predictor**、**num_parallel_tree**、**monotone_constraints**或**interaction_constraints**。然而，前面的参数已经足以充分发挥XGBoost模型的作用，因此这些参数得到了说明。
- en: That said, how does tuning the explained features affect the model? The **cheat
    sheet** shown below helps us to understand the effect of tuning each parameter
    in different ways. Also, it shows how the specific tuning affects the results,
    either understanding how the variance and the bias are improved or worsened.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，调整解释的特征如何影响模型？下面显示的**备忘单**帮助我们理解以不同方式调整每个参数的效果。此外，它展示了特定调整如何影响结果，无论是理解方差和偏差如何改善还是恶化。
- en: '![](../Images/cba278fdc1841fcce5ec64e4b6b2b260.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cba278fdc1841fcce5ec64e4b6b2b260.png)'
- en: Cheat sheet of main parameters and their effects.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 主要参数及其效果的备忘单。
- en: '**Disclaimer**: decreasing or increasing the value means doing it from the
    perfect bias-variance balance of the specific feature, not from the default value.
    Also, there are a lot of interactions between the parameters so decrasing some
    value and increasing others may result different than explained in the table.'
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**免责声明**：减少或增加值意味着从特定特征的完美偏差-方差平衡出发，而不是从默认值开始。此外，参数之间有很多相互作用，因此减少某些值并增加其他值可能会导致与表中解释的不同结果。'
- en: Hands-on Implementation
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践实施
- en: For the implementation, we are going to use two resources. First, we are going
    to use the **data** from the Kaggle competition called House Prices [15]. Secondly,
    and given that the focus of the article is XGBoost and not the rest of the tasks
    related to a machine learning project, we will borrow code from Nanashi’s notebook
    [16] to perform the **data processing**.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实施，我们将使用两个资源。首先，我们将使用Kaggle竞赛中**数据**，该竞赛名为房价[15]。其次，由于本文的重点是XGBoost，而不是机器学习项目中的其他任务，我们将借用Nanashi的笔记本[16]中的代码来进行**数据处理**。
- en: Although you will be able to see the most relevant code fragments throughout
    the reading, all the code can be seen in my [GitHub Repository](https://github.com/jomartla/medium_articles/tree/main/XGBoost)
    dedicated to Medium articles.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管你将在阅读过程中看到最相关的代码片段，但所有代码可以在我专门为Medium文章创建的[GitHub仓库](https://github.com/jomartla/medium_articles/tree/main/XGBoost)中查看。
- en: As mentioned before, I am not a great connoisseur of XGBoost training, so I
    encourage you to share your tricks and techniques in the comments section, or
    even criticise this way of training if you really think it is wrong. In my experience
    though, following the next steps is a good way to iteratively improve the model.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我并不是XGBoost训练方面的专家，所以我鼓励你在评论区分享你的技巧和方法，甚至批评这种训练方式，如果你真的认为它有问题的话。然而，根据我的经验，遵循以下步骤是迭代改进模型的好方法。
- en: '***Step 0: Data read and processing***'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '***步骤0：数据读取和处理***'
- en: 'As mentioned before, the data is read and processed. It is divided into two
    parts:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，数据已经读取和处理。它被分为两部分：
- en: Train set (85% of the data) which contains X_train and y_train. Steps 1–4 will
    use this data set to build the best possible XGBoost model. During the building
    process, the mean test score will be discussed, it is necessary to say that this
    “test” score does not really correspond to the test set, but to a validation test
    that is generated in the cross-validation process.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集（数据的85%），包含X_train和y_train。步骤1–4将使用此数据集来构建最佳的XGBoost模型。在构建过程中，将讨论平均测试得分，需要说明的是，这个“测试”得分实际上并不对应于测试集，而是对应于在交叉验证过程中生成的验证测试。
- en: Test set (15% of the data) which contains X_test and y_test. It is commonly
    known as the holdout. When we get the best possible XGBoost model, we should check
    that the model behaves in the same way with the test set as with the train set.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试集（数据的15%），包含X_test和y_test。它通常被称为保留集。当我们得到最佳的XGBoost模型时，我们应该检查模型在测试集上的表现是否与在训练集上的表现一致。
- en: Code for reading and processing data from House Prices Kaggle competition.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 读取和处理来自房价Kaggle竞赛的数据的代码。
- en: If you want to see the processing details, check the GitHub repository.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看处理细节，请查看GitHub仓库。
- en: '***Step 1: Create a baseline***'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '***步骤1：创建基准***'
- en: The first step is to create a baseline. For that purpose, a simple XGBoost model
    is created without tuning any parameter. In order to better compare the results,
    cross-validation [17] with 5 folds is used. The metric used is the Mean Absolute
    Error (MAE).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建基准。为此，创建一个简单的 XGBoost 模型而不调整任何参数。为了更好地比较结果，使用了 5 折交叉验证[17]。所用的度量标准是平均绝对误差（MAE）。
- en: Code for creating an XGBoost baseline
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 XGBoost 基准的代码
- en: That gives us a mean test score of 0.0961 and a main train score of 0.0005\.
    It seems that the bias is low but the variance is high, which can be translated
    as the presence of overfitting. However, the baseline is only a reference, let’s
    move on and see what’s next in the second step.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们提供了 0.0961 的平均测试分数和 0.0005 的主要训练分数。看起来偏差较低但方差较高，这可以解释为存在过拟合。然而，基准只是一个参考，让我们继续进行，看看第二步接下来会有什么。
- en: '***Step 2: Use GridSearchCV for improving the baseline***'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '***第 2 步：使用 GridSearchCV 改进基准***'
- en: 'Now, we will use GridSearchCV [18] to search for a good combination of parameters
    in a parameter grid. On the first try, we will use parameter values that are close
    to those used by XGBoost by default:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用 GridSearchCV [18] 来搜索参数网格中的良好参数组合。在第一次尝试中，我们将使用接近 XGBoost 默认值的参数值：
- en: Code for using GridSearchCV with XGBoost
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GridSearchCV 和 XGBoost 的代码
- en: 'The best combinations are:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳组合是：
- en: '![](../Images/af2a9be594f2859f3b8441d9c44a4a15.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af2a9be594f2859f3b8441d9c44a4a15.png)'
- en: Best parameters combinations found by GridSearchCV.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: GridSearchCV 找到的最佳参数组合。
- en: The best test mean obtained is 0.0840\. It is slightly better than the obtained
    with the first model (0.0961), so now it will be considered as the metric to improve.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 获得的最佳测试平均值为 0.0840。这比第一个模型获得的 0.0961 稍好，因此现在将其作为改进的指标。
- en: '***Step 3: Tune parameters individually***'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '***第 3 步：单独调整参数***'
- en: 'Once we have beaten our previous baseline score, we can try to understand how
    tuning the parameters **individually** affects the results. With the code provided,
    several parameters can be tested at once. Let’s see, for example, how these parameter
    values affect the results:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们超越了之前的基准分数，我们可以尝试了解如何**单独**调整参数影响结果。使用提供的代码，可以一次测试多个参数。例如，我们来看看这些参数值如何影响结果：
- en: '**n_estimators**: [125, 150, 175, 200, 225]'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**n_estimators**: [125, 150, 175, 200, 225]'
- en: Each possible value is used in a cross-validation train. The results returned
    are compiled and shown in a plot that can be seen below.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 每个可能的值都用于交叉验证训练。返回的结果被编译并显示在下面的图中。
- en: Code for tuning parameters individually and plotting the scores.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 单独调整参数和绘制分数的代码。
- en: '![](../Images/ee501d2ea5c24b6080fef72f0a9630ae.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee501d2ea5c24b6080fef72f0a9630ae.png)'
- en: Plot how the number of estimators changes affect the results.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制估计器数量变化如何影响结果。
- en: The graph shows that the mean test score can be lower if n_estimators is higher.
    However, the mean train score is getting close to zero, so overfitting is knocking
    on the door.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示，如果 n_estimators 较高，平均测试分数可能会降低。然而，平均训练分数接近零，所以过拟合即将到来。
- en: '***Step 4: Repeat steps 2 and 3.***'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '***第 4 步：重复第 2 步和第 3 步。***'
- en: With the information gathered in step 3, we can redefine the parameters grid
    in GridSearchCV and try to get a better model. In this case, higher n_estimators
    are tested. This means that the complexity of the model is going to be higher,
    so in the parameter grid, we have also included some parameter values that can
    help to avoid overfitting (lower learning_rate, higher lambda, higher gamma…).
    The cheat sheet defined in the hyperparameters section can be very useful here.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 利用第 3 步中收集的信息，我们可以重新定义 GridSearchCV 中的参数网格，尝试获得更好的模型。在这种情况下，测试了更高的 n_estimators。这意味着模型的复杂度将更高，因此在参数网格中，我们还包括了一些可以帮助避免过拟合的参数值（较低的
    learning_rate、更高的 lambda、更高的 gamma……）。超参数部分定义的备忘单在这里可能会非常有用。
- en: Code of the second GridSearchCV with XGBoost.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次 GridSearchCV 与 XGBoost 的代码。
- en: 'Using this code we achieve a new best mean test score of 0.0814 (previous:
    0.0840).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这段代码，我们获得了新的最佳平均测试分数 0.0814（之前为 0.0840）。
- en: '![](../Images/d3c9bf0adac31897ff0a7cc00e2aaaa5.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3c9bf0adac31897ff0a7cc00e2aaaa5.png)'
- en: Best parameters combinations found by the second GridSearchCV.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次 GridSearchCV 找到的最佳参数组合。
- en: Steps 2–3 should be repeated as many times as needed in order to improve the
    model.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 第 2 步至第 3 步应根据需要重复进行，以改进模型。
- en: '***Step 5: Use the test set to validate the model.***'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '***第 5 步：使用测试集验证模型。***'
- en: Finally, we need to validate the model built with the test set (holdout test,
    no validation test) of step 0.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要使用步骤0中的测试集（留出测试，无验证测试）来验证所构建的模型。
- en: Code for calculating the holdout predictions MAE.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 计算留出预测 MAE 的代码。
- en: The MAE obtained is 0.0808\. As it is lower than our best MAE (0.0814), we can
    say that our model generalises well and has been trained properly.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 获得的 MAE 为 0.0808。由于低于我们最好的 MAE（0.0814），我们可以说我们的模型泛化良好，并且已经得到了很好的训练。
- en: Conclusion
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, I aim to provide a comprehensive guide for using XGBoost with
    complete proficiency. Through hours of research on papers, guides, posts, and
    articles, I believe that I have produced a complete article that can aid in comprehending
    and using XGBoost in one go.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我旨在提供一个使用 XGBoost 的全面指南。经过数小时对论文、指南、帖子和文章的研究，我相信我已完成一篇完整的文章，可以帮助全面理解和使用
    XGBoost。
- en: I hope you have found the reading useful and enjoyable. And above all, I am
    happy to receive any kind of feedback. So feel free to share your thoughts!
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你觉得阅读有用且愉快。最重要的是，我很高兴收到任何形式的反馈。请随时分享你的想法！
- en: References
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Wolpert, D. H., & Macready, W. G. «No free lunch theorems for optimization».
    *IEEE transactions on evolutionary computation* (1997). [https://ieeexplore.ieee.org/abstract/document/585893](https://ieeexplore.ieee.org/abstract/document/585893)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Wolpert, D. H., & Macready, W. G. «优化的无免费午餐定理». *IEEE Transactions on Evolutionary
    Computation* (1997). [https://ieeexplore.ieee.org/abstract/document/585893](https://ieeexplore.ieee.org/abstract/document/585893)'
- en: '[2] Bojan Tunguz Twitter Account. [https://twitter.com/tunguz](https://twitter.com/tunguz)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Bojan Tunguz Twitter 账户。 [https://twitter.com/tunguz](https://twitter.com/tunguz)'
- en: '[3] Raschka, Sebastian (2022). Deep Learning for Tabular Data. [https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html](https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Raschka, Sebastian (2022). 针对表格数据的深度学习。 [https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html](https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html)'
- en: '[4] Bojan Tunguz Kaggle Profile. [https://www.kaggle.com/tunguz](https://www.kaggle.com/tunguz)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Bojan Tunguz Kaggle 个人资料。 [https://www.kaggle.com/tunguz](https://www.kaggle.com/tunguz)'
- en: '[5] Chen, T., & Guestrin, C. (2016, August). [Xgboost: A scalable tree boosting
    system](https://dl.acm.org/doi/abs/10.1145/2939672.2939785). In *Proceedings of
    the 22nd acm sigkdd international conference on knowledge discovery and data mining*
    (pp. 785–794).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Chen, T., & Guestrin, C. (2016年8月). [Xgboost: 一个可扩展的树提升系统](https://dl.acm.org/doi/abs/10.1145/2939672.2939785)。在
    *第22届ACM SIGKDD国际知识发现与数据挖掘大会论文集*（第785–794页）。'
- en: '[6] Dorogush, A.V.; Ershov, V.; Gulin. A. CatBoost: Gradient Boosting with
    Categorical Features Support». *ArXiv:1810.11363*, 24 (2018). [http://arxiv.org/abs/1810.11363](http://arxiv.org/abs/1810.11363).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Dorogush, A.V.; Ershov, V.; Gulin, A. CatBoost: 支持类别特征的梯度提升». *ArXiv:1810.11363*,
    24 (2018). [http://arxiv.org/abs/1810.11363](http://arxiv.org/abs/1810.11363).'
- en: '[7] Ke, G.; Meng, Q.; Finley, T; Wang, T; Chen, W; Ma, W; Ye, Q; Liu, T. «LightGBM:
    A Highly Efficient Gradient Boosting Decision Tree». *Advances in Neural Information
    Processing Systems*, 20 (2017). [https://proceedings.neurips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Ke, G.; Meng, Q.; Finley, T; Wang, T; Chen, W; Ma, W; Ye, Q; Liu, T. «LightGBM:
    一种高效的梯度提升决策树». *神经信息处理系统进展*, 20 (2017). [https://proceedings.neurips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html).'
- en: '[8] Tunguz, Bojan. [https://twitter.com/tunguz/status/1620048813686923266?s=20&t=BxzKnvn7G0ieo1I7SfDnSQ](https://twitter.com/tunguz/status/1620048813686923266?s=20&t=BxzKnvn7G0ieo1I7SfDnSQ)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Tunguz, Bojan. [https://twitter.com/tunguz/status/1620048813686923266?s=20&t=BxzKnvn7G0ieo1I7SfDnSQ](https://twitter.com/tunguz/status/1620048813686923266?s=20&t=BxzKnvn7G0ieo1I7SfDnSQ)'
- en: '[9]Martín Lasaosa, Jorge. «Tree Ensembles: Bagging, Boosting and Gradient Boosting».
    In *Towards Data Science (Medium)*. (2022) [https://medium.com/r/?url=https%3A%2F%2Ftowardsdatascience.com%2Ftree-ensembles-theory-and-practice-1cf9eb27781](/tree-ensembles-theory-and-practice-1cf9eb27781)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Martín Lasaosa, Jorge. «树集成：Bagging、Boosting 和梯度提升». 在 *Towards Data Science
    (Medium)*。 (2022) [https://medium.com/r/?url=https%3A%2F%2Ftowardsdatascience.com%2Ftree-ensembles-theory-and-practice-1cf9eb27781](/tree-ensembles-theory-and-practice-1cf9eb27781)'
- en: '[10] T. Zhang and R. Johnson. Learning nonlinear functions using regularized
    greedy forest. IEEE Transactions on Pattern Analysis and Machine Intelligence,
    36(5), 2014\. [https://ieeexplore.ieee.org/abstract/document/6583153](https://ieeexplore.ieee.org/abstract/document/6583153)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] T. Zhang 和 R. Johnson. 使用正则化贪婪森林学习非线性函数。IEEE 计算机学会模式分析与机器智能汇刊，36(5)，2014\.
    [https://ieeexplore.ieee.org/abstract/document/6583153](https://ieeexplore.ieee.org/abstract/document/6583153)'
- en: '[11] Breiman, L. «Random Forests». *Machine Learning* 45, (2001): 5–32\. [https://doi.org/10.1023/A:1010933404324](https://doi.org/10.1023/A:1010933404324).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Breiman, L. «随机森林». *机器学习* 45, (2001): 5–32\. [https://doi.org/10.1023/A:1010933404324](https://doi.org/10.1023/A:1010933404324).'
- en: '[12] XGBoost Documentation (Parameters section) [https://xgboost.readthedocs.io/en/stable/parameter.html](https://xgboost.readthedocs.io/en/stable/parameter.html)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] XGBoost 文档（参数部分） [https://xgboost.readthedocs.io/en/stable/parameter.html](https://xgboost.readthedocs.io/en/stable/parameter.html)'
- en: '[13] Vinayak, R. K.; Gilad-Bachrach, R. «Dart: Dropouts meet multiple additive
    regression trees.» In *Artificial Intelligence and Statistics*. PMLR. (2015) [https://proceedings.mlr.press/v38/korlakaivinayak15.html](https://proceedings.mlr.press/v38/korlakaivinayak15.html)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Vinayak, R. K.; Gilad-Bachrach, R. «Dart: Dropouts 遇见多重加法回归树». 收录于 *人工智能与统计*.
    PMLR. (2015) [https://proceedings.mlr.press/v38/korlakaivinayak15.html](https://proceedings.mlr.press/v38/korlakaivinayak15.html)'
- en: '[14] Um, Albert. «L1, L2 Regularization in XGBoost Regression» In *Medium*
    (2021) [https://albertum.medium.com/l1-l2-regularization-in-xgboost-regression-7b2db08a59e0](https://albertum.medium.com/l1-l2-regularization-in-xgboost-regression-7b2db08a59e0)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Um, Albert. «XGBoost 回归中的 L1、L2 正则化» 收录于 *Medium* (2021) [https://albertum.medium.com/l1-l2-regularization-in-xgboost-regression-7b2db08a59e0](https://albertum.medium.com/l1-l2-regularization-in-xgboost-regression-7b2db08a59e0)'
- en: '[15] House Prices Kaggle Competition. [https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] 房价 Kaggle 竞赛。 [https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)'
- en: '[16] Nanashi’s Notebook in Kaggle [https://www.kaggle.com/code/jesucristo/1-house-prices-solution-top-1](https://www.kaggle.com/code/jesucristo/1-house-prices-solution-top-1)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Nanashi 在 Kaggle 上的笔记本 [https://www.kaggle.com/code/jesucristo/1-house-prices-solution-top-1](https://www.kaggle.com/code/jesucristo/1-house-prices-solution-top-1)'
- en: '[17] Yiu, Tony. «Understanding Cross Validation» In *Towards Data Science (Medium)*
    (2020) [https://towardsdatascience.com/understanding-cross-validation-419dbd47e9bd](/understanding-cross-validation-419dbd47e9bd)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Yiu, Tony. «理解交叉验证» 收录于 *Towards Data Science (Medium)* (2020) [https://towardsdatascience.com/understanding-cross-validation-419dbd47e9bd](/understanding-cross-validation-419dbd47e9bd)'
- en: '[18] «Hyperparameter Tuning with GridSearchCV» In *Great Learning* (2022) [https://www.mygreatlearning.com/blog/gridsearchcv/](https://www.mygreatlearning.com/blog/gridsearchcv/)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] «使用 GridSearchCV 调优超参数» 收录于 *Great Learning* (2022) [https://www.mygreatlearning.com/blog/gridsearchcv/](https://www.mygreatlearning.com/blog/gridsearchcv/)'
