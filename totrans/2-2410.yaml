- en: 'XGBoost: Theory and Hyperparameter Tuning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/xgboost-theory-and-hyperparameter-tuning-bc4068aba95e](https://towardsdatascience.com/xgboost-theory-and-hyperparameter-tuning-bc4068aba95e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A complete guide with examples in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://jorgemartinlasaosa.medium.com/?source=post_page-----bc4068aba95e--------------------------------)[![Jorge
    Martín Lasaosa](../Images/21b4e500b7d14204ea76f579c3e2433f.png)](https://jorgemartinlasaosa.medium.com/?source=post_page-----bc4068aba95e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bc4068aba95e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bc4068aba95e--------------------------------)
    [Jorge Martín Lasaosa](https://jorgemartinlasaosa.medium.com/?source=post_page-----bc4068aba95e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bc4068aba95e--------------------------------)
    ·17 min read·Feb 16, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1e8bc4dcd14576319bd1f517bfeaa2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Joanne Francis](https://unsplash.com/@nipawinnews?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a few months, I will have been working as a Data Scientist for 3 years. I
    know it is not a long career yet, but together with my academic experience, I
    have been able to work on several machine learning projects for different sectors
    (energy, customer experience…). All of them were fed by *tabular data,* which
    means structured data (organised in rows and columns). In contrast, there are
    projects fed by unstructured data such as images or text which are more related
    to machine learning fields such as Computer Vision or Natural Language Processing
    (NLP).
  prefs: []
  type: TYPE_NORMAL
- en: Based on my experience, XGBoost *usually* performs well with *tabular data*
    projects. Although the No Free Lunch Theorem [1] states that any two algorithms
    are equivalent when their performances are averaged across all possible problems,
    on Bojan Tunguz’s Twitter [2] you can read frequent discussions with other professionals
    about why tree-based models (and specially XGBoost) are often the best candidates
    for tackling *tabular data* projects, even with the growing research into the
    use of Deep Learning techniques for this type of data. [3]
  prefs: []
  type: TYPE_NORMAL
- en: Also, it is quite funny to see how a Kaggle Grandmaster [4] jokes about being
    an XGBoost evangelist.
  prefs: []
  type: TYPE_NORMAL
- en: Bojan Tunguz’s pinned tweet.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the great success of XGBoost, when I wanted to get the best performance
    out of the model in the past, I did not find complete guides where all the needed
    knowledge is centralised. There are great theoretical and practical explanations
    that I will be referencing throughout the reading, but I have not found any complete
    guide that gives a holistic view. That is why I have decided to write this article.
    Furthermore, I am going to apply what I have collected here to a well-known Kaggle
    competition called [House Prices — Advanced Regression Techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques).
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the article is divided into two sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Theory:** after a short introduction, we will dive into the original paper
    to understand the theory behind this great model. Then, a brief visual explanation
    will help us to better understand the theory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Practice:** after an overview of the XGBoost parameters, I will present a
    step-by-step guide for tuning the hyperparameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All images unless otherwise noted are by the author.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**XGBoost** stands for e**X**treme **G**radient **Boos**ting and was officially
    published by Tianqi Chen and Carlos Guestrin in 2016 [5]. Even before its publication,
    it was established as one of the best algorithms to use in Kaggle competitions.'
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, and despite the fact that the rise of Deep Learning is collecting
    incredible success in fields such as Computer Vision and NLP, XGBoost and other
    tree-based models (CatBoost [6] or LightGBM [7]) continue to be one of the best
    options for predicting *tabular data* [8]. All these tree-based algorithms are
    based on Gradient Boosting, and if you want to understand how this technique works,
    I recommend you to check my article on tree ensembles. [9]
  prefs: []
  type: TYPE_NORMAL
- en: '[](/tree-ensembles-theory-and-practice-1cf9eb27781?source=post_page-----bc4068aba95e--------------------------------)
    [## Tree Ensembles: Bagging, Boosting and Gradient Boosting'
  prefs: []
  type: TYPE_NORMAL
- en: Theory and practice explained in detail
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.comf](/tree-ensembles-theory-and-practice-1cf9eb27781?source=post_page-----bc4068aba95e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Do you want to know what makes XGBoost special? I will explain it in two different
    subsections: **Original Paper** and **Visual Explanation.** Let’s get started!'
  prefs: []
  type: TYPE_NORMAL
- en: Original Paper
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned before, XGBoost is based on Gradient Boosting, so several trees
    are trained sequentially on the residuals of the previous trees. However, there
    is a combination of some **minor improvements** that allowed XGBoost to outperform
    the existing tree ensembles models by **preventing overfitting**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regularized Learning Objective** (similar to *RGF* [10]). In gradient boosting,
    each tree is trained in the best possible way to achieve the learning objective:
    reduce the difference between the predictions and the target. This learning objective
    is replaced in XGBoost by a regularised learning objective, which adds a regularisation
    term to the calculation of the difference. In plain English, this term *adds*
    noise when each tree learns and is intended to reduce the prediction’s sensitivity
    to individual observations. If the regularization term is set to zero, the objective
    falls back to the traditional gradient boosting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shrinkage** (borrowed from *Random Forests* [11]).A technique that limits
    the weight that each trained tree has in the final prediction. Thus, the influence
    of each tree is reduced and there is more space for future trees to improve the
    predictions. It is similar to the learning rate (and also specified as it in the
    parameters section).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Column Subsampling** (borrowed from *Random Forests* [11]). It allows to
    randomly select a subsample of features for each tree, tree level and/or tree
    node. Therefore, a very important feature for the first tree/level/node, could
    not be available for the second one, pushing it to use other features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One of the biggest problems in tree learning is finding the best split. If
    there is a continuous feature that varies from 0 to 100, what value should be
    used for doing the split? 20? 32.5? 70? In order to find the best split, XGBoost
    can apply different algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exact greedy algorithm.** It is the most commonly used algorithm in older
    tree-boosting implementations which consists of testing all possible splits of
    all features. Doing it for continuous features is computationally demanding and
    therefore requires more time as the data sample increases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weighted quantile sketch algorithm**: proposes candidate splitting points
    according to percentiles of feature distributions. The algorithm then maps the
    continuous features into buckets split by these candidate points, aggregates the
    statistics and finds the best solution among proposals based on aggregated statistics.
    Furthermore, it can handle *weighted* data and includes a default direction in
    each tree node to make the algorithm aware of the sparsity patterns in the data.
    The sparsity may be due to the presence of missing values, frequent zero entries
    or the consequences of feature engineering (e.g. use of One Hot Encoding).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, the algorithm is designed to interact with the system efficiently.
    Without going into detail, I can point out that the data is stored in units in
    memory called blocks to help the algorithm sort the data. This technique allows
    parallelisation. Moreover, in the case of large datasets, cache-aware access is
    defined to avoid slowing down the search for splits when some computations do
    not fit in the CPU cache. Finally, it uses block compression and block fragmentation
    to handle data that does not fit in the main memory.
  prefs: []
  type: TYPE_NORMAL
- en: Visual Explanation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section is highly inspired by [Josh Starmer’s Youtube channel](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw)
    and [Shreya Rao’s article](/xgboost-regression-explain-it-to-me-like-im-10-2cf324b0bbdb).
    My goal is to explain with a little example how XGBoost works and link it with
    the paper theory seen before. To do this, let’s follow the steps below.
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 1: Create synthetic data***'
  prefs: []
  type: TYPE_NORMAL
- en: A small synthetic dataset about house pricing is used. It has two features (*Square
    Meters* and *Has Garage?*) and a target (*Price*).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e049b46744eb2c330927b0f775fbb4ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Synthetic data about house pricing used for the visual explanation.
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 2: Calculate the residuals***'
  prefs: []
  type: TYPE_NORMAL
- en: As you have already seen, residuals have been calculated in the table shown
    above. The calculation is simple, you only have to subtract the predicted price
    of the previous tree from the real price (remember that XGBoost train several
    trees sequentially). However, this is the first tree, so we do not have predicted
    prices. In that case, a mean of the price is calculated.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/50502b9ce92a913892bd8cc736b602a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculation of the first residuals.
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 3: Build the first tree of XGBoost***'
  prefs: []
  type: TYPE_NORMAL
- en: The first tree is going to be trained with all the residuals as the target.
    So the first thing to do is to calculate the **similarity score** for all the
    residuals. This is the score that the tree splits intend to augment.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fee2e131b782c121f1d337d6001d0bb1.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculation of the Similarity Score for the first tree.
  prefs: []
  type: TYPE_NORMAL
- en: Do you see the lambda (λ) character? As explained in the paper theory section,
    this is the **regularisation term**that helps to prevent overfitting by *adding*
    noise. Thus, the XGBoost learning objective (augment the similarity score) is
    actually a regularized learning objective. The default value for the regularisation
    term in XGBoost is 1\. Now, let’s see how the feature *Has Garage?* influence
    in the similarity score and the gain.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb9adc9d4791d5570f2558d2deec27e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarity scores calculated for the split with the g*arage* feature.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b370f375828e78c5996d10f2332d60e9.png)'
  prefs: []
  type: TYPE_IMG
- en: Final gain for garage split.
  prefs: []
  type: TYPE_NORMAL
- en: Once the similarity scores are calculated, the gain for the feature *Has Garage?*
    is 7448,58\. If the final gain is positive, then it is a good split, otherwise,
    it is not. As it is positive, we can conclude that is a good split.
  prefs: []
  type: TYPE_NORMAL
- en: For a continuous feature like *Square Meters,* the process is slightly different.
    First, we need to find the best split for the continuous feature. For that purpose,
    the **exact greedy algorithm** explained in the paper theory section will be used.
    That is to say, every possible split is going to be tested.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4c350744c5d2c5b9f541e791d4c1e63b.png)'
  prefs: []
  type: TYPE_IMG
- en: Sorting the feature and finding all the possible splits (greedy).
  prefs: []
  type: TYPE_NORMAL
- en: Sorting shown in the figure above can be computationally expensive when datasets
    are big. As commented in the paper theory section, XGBoost uses **block units**
    that allow parallelization and help with this problem. Also, remember that XGBoost
    can use the **weighted quantile sketch algorithm** to propose candidate splitting
    points according to percentiles of feature distributions. This is not going to
    be explained here, but it is one of the main advantages of XGBoost. That being
    said, we are going to calculate the similarity scores and gain for each possible
    split.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d2c4ca8c8a970a9e0fb11a22a44daba2.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculations of the gain for each Square Meters split.
  prefs: []
  type: TYPE_NORMAL
- en: The split of the *Square Meters* feature by 175 has the greatest gain (even
    greater than the *Has Garage?* split), so it should be the first split. Only one
    residual is left in the right leaf (156). So let’s focus on the left leaf and
    look for another split. After all the calculations, the best split is done again
    with *Square Meters* feature.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1e39a1c0e9383f32de1a31f2c30d54cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Best second split on garage feature.
  prefs: []
  type: TYPE_NORMAL
- en: After new calculations, the right leaf can be split again.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8afd0f0a292e55f79beefabf79c71e53.png)'
  prefs: []
  type: TYPE_IMG
- en: Final tree with every possible split done.
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 4: Pruning the tree***'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to avoid overfitting, there is another process called tree pruning
    that XGBoost does. From bottom to top every gain is validated. How? If an XGBoost
    parameter called **gamma** (γ) is greater than the gain, then the split is removed.
    The default value of γ in XGBoost is 0, but a 200 value is set to represent a
    case in which a split is removed. Why? Because in that case, 200 (γ) is greater
    than the last gain (198). So, the tree is pruned and the final result is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5dca48ca940139086b47e228507545bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Last version of the first tree after pruning.
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 5: Make predictions with the tree***'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to do predictions, the first step is to have a single value (output
    value) in each final leaf. For that, we use the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7154226da6092d01863b1721421d288.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculation of the output value (value of each leaf).
  prefs: []
  type: TYPE_NORMAL
- en: 'The result is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ffb55aeba472b69a3e2ab8e795478229.png)'
  prefs: []
  type: TYPE_IMG
- en: First tree with an output value for each leaf.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the output values of the final leaves can be used for doing new predictions
    with the formula shown below. Being *i* the observation that we want to predict,
    *prediction_t0* the first prediction (mean of observed price), *ɛ* the learning
    rate and *leaf_i* the value for the observation *i* if we follow the tree rules.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/50906e985cf106da45eaba4b89833934.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that the learning rate (*ɛ*), whose default value is 0.3, is the **shrinkage**
    explanation of the paper theory section. That being said, let’s predict all the
    observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7eb869512d15da5b3343876a4db6126b.png)'
  prefs: []
  type: TYPE_IMG
- en: Predictions for each value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we use the predictions to calculate new residuals (Residuals_1):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cd24f341ebaa64cb1f2437178c347f5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Residuals of the first tree.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see above, every residual (except the first one) is closer to zero.
    That means that the first tree gives information that improves the first prediction
    (the mean). The next step would be to create another tree that gives another step
    in the direction of reducing the residuals.
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 6: Train new trees***'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to create a new tree, the only thing to do is to redo steps 3–5
    with the new residuals (Residuals_1) as the target. Then, the final prediction
    would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a9d8fd8988703141e2d96c31c1b4faf6.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculation of the final predictions with two trees.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost can sequentially train trees using these steps. Personally, I find that
    the visual explanation is an effective way to comprehend the model and its theory.
    I hope it was helpful for you as well. Be that as it may, now it’s time to proceed
    with the practical section.
  prefs: []
  type: TYPE_NORMAL
- en: Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, I would like to emphasize that while I am not an expert in the practical
    side of XGBoost, I have used certain techniques that have proven to be successful.
    As a result, I think that I can provide a brief guide to help you tune the hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this goal, it is essential to divide the practice into two subsections.
    The first one will focus on defining the **hyperparameters** of the algorithm
    and compiling them into a concise cheat sheet. The second subsection, called **Hands-On
    Implementation**, will provide a step-by-step guide that will walk you through
    the process of training XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All the information explained here can be found in the official XGBoost documentation
    [12]. One of the general parameters is known as **booster**. As I explained in
    my previous article [9], gradient boosting is a technique that is not restricted
    to using decision trees, but any model. That is why XGBoost accepts three values
    for the **booster** parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*gbtree*: a gradient boosting with decision trees (default value)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*dart*: a gradient boosting with decision trees that uses a method proposed
    by Vinayak and Gilad-Bachrach (2015) [13] that adds dropout techniques from the
    deep neural net community to boosted trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*gblinear*: a gradient boosting with linear functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While *gblinear* is the best option to catch linear links between predictors
    and the outcome, boosters based on decision trees (*gbtree* and *dart*) are much
    better to catch non-linear links. **As g*btree* is the most used value, the rest
    of the article is going to use it**. If you are interested in using the others,
    check the documentation [12] because depending on your choice the parameters change.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we set *gbtree* as the **booster**, there are several parameters that
    we can tune in order to get the best model. The most important ones are explained
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**eta** (a.k.a learning rate): shown in the visual explanation section as ***ɛ,***
    it limits the weight each trained tree has in the final prediction to make the
    boosting process more conservative.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gamma:** shown in the visual explanation section as **γ**, it marks the minimum
    gain required to make a further partition on a leaf node of the tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_depth:** sets the maximum depth of a tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**n_estimators**: number of trees trained.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**min_child_weight:** sets the minimum number of instance weights (sum of residuals)
    needed in a child for doing a split.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**subsample:** sets the percentage of the sample got (randomly) before training
    each tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**colsample_by[]:** a collection of parameters of subsampling columns. The
    subsampling can be done for each tree (**colsample_bytree**), for each depth level
    reached in a tree (**colsample_bylevel**) or for every time a new split is evaluated
    (**colsample_bynode**). Note that these parameters can work simultaneously: if
    every parameter has 0.5 value and you have 32 columns, then each split would use
    4 columns (32/ 2³)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**lambda** (L2 regularization):shown in the visual explanation as **λ**. It
    decreases the output value (step 5 in the visual explanation) smoothly as it increases
    the denominator. [14]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**alpha** (L1 regularization): It decreases the output value and promotes sparsity
    by forcing output values to be 0\. [14]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tree_method:** it sets the construction algorithm used by the tree for finding
    the splits. As discussed in the paper theory section, exact or approximate algorithms
    can be used. In practice, there are 5possible values for this parameter: *auto*
    for letting a heuristic select the fastest option among the next ones, *exact*
    for applying the exact greedy algorithm that enumerates all split candidates,
    *approx* for applying an approximate greedy algorithm that uses quantile sketch
    and gradient histogram, *hist* for using a histogram-optimized version of the
    approximate algorithm and *gpu_hist* for using the GPU implementation of *hist.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scale_pos_weight**: it controls the balance of positive and negative weights,
    which can be useful for unbalanced classes. A typical value to consider is *sum(negative
    instances) / sum(positive instances).*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_leaves**: it sets the maximum number of nodes to be added only when the
    *exact* value for the **tree_method** parameter is not chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are more parameters to tune: **updater**, **refresh_leaf**, **process_type**,
    **grow_policy**, **max_bin**, **predictor**, **num_parallel_tree**, **monotone_constraints**
    or **interaction_constraints**. However, the former ones are enough to make the
    most of your XGBoost model and therefore, those were explained.'
  prefs: []
  type: TYPE_NORMAL
- en: That said, how does tuning the explained features affect the model? The **cheat
    sheet** shown below helps us to understand the effect of tuning each parameter
    in different ways. Also, it shows how the specific tuning affects the results,
    either understanding how the variance and the bias are improved or worsened.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cba278fdc1841fcce5ec64e4b6b2b260.png)'
  prefs: []
  type: TYPE_IMG
- en: Cheat sheet of main parameters and their effects.
  prefs: []
  type: TYPE_NORMAL
- en: '**Disclaimer**: decreasing or increasing the value means doing it from the
    perfect bias-variance balance of the specific feature, not from the default value.
    Also, there are a lot of interactions between the parameters so decrasing some
    value and increasing others may result different than explained in the table.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hands-on Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the implementation, we are going to use two resources. First, we are going
    to use the **data** from the Kaggle competition called House Prices [15]. Secondly,
    and given that the focus of the article is XGBoost and not the rest of the tasks
    related to a machine learning project, we will borrow code from Nanashi’s notebook
    [16] to perform the **data processing**.
  prefs: []
  type: TYPE_NORMAL
- en: Although you will be able to see the most relevant code fragments throughout
    the reading, all the code can be seen in my [GitHub Repository](https://github.com/jomartla/medium_articles/tree/main/XGBoost)
    dedicated to Medium articles.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, I am not a great connoisseur of XGBoost training, so I
    encourage you to share your tricks and techniques in the comments section, or
    even criticise this way of training if you really think it is wrong. In my experience
    though, following the next steps is a good way to iteratively improve the model.
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 0: Data read and processing***'
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned before, the data is read and processed. It is divided into two
    parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Train set (85% of the data) which contains X_train and y_train. Steps 1–4 will
    use this data set to build the best possible XGBoost model. During the building
    process, the mean test score will be discussed, it is necessary to say that this
    “test” score does not really correspond to the test set, but to a validation test
    that is generated in the cross-validation process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test set (15% of the data) which contains X_test and y_test. It is commonly
    known as the holdout. When we get the best possible XGBoost model, we should check
    that the model behaves in the same way with the test set as with the train set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code for reading and processing data from House Prices Kaggle competition.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to see the processing details, check the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 1: Create a baseline***'
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to create a baseline. For that purpose, a simple XGBoost model
    is created without tuning any parameter. In order to better compare the results,
    cross-validation [17] with 5 folds is used. The metric used is the Mean Absolute
    Error (MAE).
  prefs: []
  type: TYPE_NORMAL
- en: Code for creating an XGBoost baseline
  prefs: []
  type: TYPE_NORMAL
- en: That gives us a mean test score of 0.0961 and a main train score of 0.0005\.
    It seems that the bias is low but the variance is high, which can be translated
    as the presence of overfitting. However, the baseline is only a reference, let’s
    move on and see what’s next in the second step.
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 2: Use GridSearchCV for improving the baseline***'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will use GridSearchCV [18] to search for a good combination of parameters
    in a parameter grid. On the first try, we will use parameter values that are close
    to those used by XGBoost by default:'
  prefs: []
  type: TYPE_NORMAL
- en: Code for using GridSearchCV with XGBoost
  prefs: []
  type: TYPE_NORMAL
- en: 'The best combinations are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af2a9be594f2859f3b8441d9c44a4a15.png)'
  prefs: []
  type: TYPE_IMG
- en: Best parameters combinations found by GridSearchCV.
  prefs: []
  type: TYPE_NORMAL
- en: The best test mean obtained is 0.0840\. It is slightly better than the obtained
    with the first model (0.0961), so now it will be considered as the metric to improve.
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 3: Tune parameters individually***'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have beaten our previous baseline score, we can try to understand how
    tuning the parameters **individually** affects the results. With the code provided,
    several parameters can be tested at once. Let’s see, for example, how these parameter
    values affect the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '**n_estimators**: [125, 150, 175, 200, 225]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each possible value is used in a cross-validation train. The results returned
    are compiled and shown in a plot that can be seen below.
  prefs: []
  type: TYPE_NORMAL
- en: Code for tuning parameters individually and plotting the scores.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee501d2ea5c24b6080fef72f0a9630ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Plot how the number of estimators changes affect the results.
  prefs: []
  type: TYPE_NORMAL
- en: The graph shows that the mean test score can be lower if n_estimators is higher.
    However, the mean train score is getting close to zero, so overfitting is knocking
    on the door.
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 4: Repeat steps 2 and 3.***'
  prefs: []
  type: TYPE_NORMAL
- en: With the information gathered in step 3, we can redefine the parameters grid
    in GridSearchCV and try to get a better model. In this case, higher n_estimators
    are tested. This means that the complexity of the model is going to be higher,
    so in the parameter grid, we have also included some parameter values that can
    help to avoid overfitting (lower learning_rate, higher lambda, higher gamma…).
    The cheat sheet defined in the hyperparameters section can be very useful here.
  prefs: []
  type: TYPE_NORMAL
- en: Code of the second GridSearchCV with XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this code we achieve a new best mean test score of 0.0814 (previous:
    0.0840).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d3c9bf0adac31897ff0a7cc00e2aaaa5.png)'
  prefs: []
  type: TYPE_IMG
- en: Best parameters combinations found by the second GridSearchCV.
  prefs: []
  type: TYPE_NORMAL
- en: Steps 2–3 should be repeated as many times as needed in order to improve the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 5: Use the test set to validate the model.***'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we need to validate the model built with the test set (holdout test,
    no validation test) of step 0.
  prefs: []
  type: TYPE_NORMAL
- en: Code for calculating the holdout predictions MAE.
  prefs: []
  type: TYPE_NORMAL
- en: The MAE obtained is 0.0808\. As it is lower than our best MAE (0.0814), we can
    say that our model generalises well and has been trained properly.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, I aim to provide a comprehensive guide for using XGBoost with
    complete proficiency. Through hours of research on papers, guides, posts, and
    articles, I believe that I have produced a complete article that can aid in comprehending
    and using XGBoost in one go.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you have found the reading useful and enjoyable. And above all, I am
    happy to receive any kind of feedback. So feel free to share your thoughts!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Wolpert, D. H., & Macready, W. G. «No free lunch theorems for optimization».
    *IEEE transactions on evolutionary computation* (1997). [https://ieeexplore.ieee.org/abstract/document/585893](https://ieeexplore.ieee.org/abstract/document/585893)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Bojan Tunguz Twitter Account. [https://twitter.com/tunguz](https://twitter.com/tunguz)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Raschka, Sebastian (2022). Deep Learning for Tabular Data. [https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html](https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Bojan Tunguz Kaggle Profile. [https://www.kaggle.com/tunguz](https://www.kaggle.com/tunguz)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Chen, T., & Guestrin, C. (2016, August). [Xgboost: A scalable tree boosting
    system](https://dl.acm.org/doi/abs/10.1145/2939672.2939785). In *Proceedings of
    the 22nd acm sigkdd international conference on knowledge discovery and data mining*
    (pp. 785–794).'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Dorogush, A.V.; Ershov, V.; Gulin. A. CatBoost: Gradient Boosting with
    Categorical Features Support». *ArXiv:1810.11363*, 24 (2018). [http://arxiv.org/abs/1810.11363](http://arxiv.org/abs/1810.11363).'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Ke, G.; Meng, Q.; Finley, T; Wang, T; Chen, W; Ma, W; Ye, Q; Liu, T. «LightGBM:
    A Highly Efficient Gradient Boosting Decision Tree». *Advances in Neural Information
    Processing Systems*, 20 (2017). [https://proceedings.neurips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Tunguz, Bojan. [https://twitter.com/tunguz/status/1620048813686923266?s=20&t=BxzKnvn7G0ieo1I7SfDnSQ](https://twitter.com/tunguz/status/1620048813686923266?s=20&t=BxzKnvn7G0ieo1I7SfDnSQ)'
  prefs: []
  type: TYPE_NORMAL
- en: '[9]Martín Lasaosa, Jorge. «Tree Ensembles: Bagging, Boosting and Gradient Boosting».
    In *Towards Data Science (Medium)*. (2022) [https://medium.com/r/?url=https%3A%2F%2Ftowardsdatascience.com%2Ftree-ensembles-theory-and-practice-1cf9eb27781](/tree-ensembles-theory-and-practice-1cf9eb27781)'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] T. Zhang and R. Johnson. Learning nonlinear functions using regularized
    greedy forest. IEEE Transactions on Pattern Analysis and Machine Intelligence,
    36(5), 2014\. [https://ieeexplore.ieee.org/abstract/document/6583153](https://ieeexplore.ieee.org/abstract/document/6583153)'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] Breiman, L. «Random Forests». *Machine Learning* 45, (2001): 5–32\. [https://doi.org/10.1023/A:1010933404324](https://doi.org/10.1023/A:1010933404324).'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] XGBoost Documentation (Parameters section) [https://xgboost.readthedocs.io/en/stable/parameter.html](https://xgboost.readthedocs.io/en/stable/parameter.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[13] Vinayak, R. K.; Gilad-Bachrach, R. «Dart: Dropouts meet multiple additive
    regression trees.» In *Artificial Intelligence and Statistics*. PMLR. (2015) [https://proceedings.mlr.press/v38/korlakaivinayak15.html](https://proceedings.mlr.press/v38/korlakaivinayak15.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[14] Um, Albert. «L1, L2 Regularization in XGBoost Regression» In *Medium*
    (2021) [https://albertum.medium.com/l1-l2-regularization-in-xgboost-regression-7b2db08a59e0](https://albertum.medium.com/l1-l2-regularization-in-xgboost-regression-7b2db08a59e0)'
  prefs: []
  type: TYPE_NORMAL
- en: '[15] House Prices Kaggle Competition. [https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)'
  prefs: []
  type: TYPE_NORMAL
- en: '[16] Nanashi’s Notebook in Kaggle [https://www.kaggle.com/code/jesucristo/1-house-prices-solution-top-1](https://www.kaggle.com/code/jesucristo/1-house-prices-solution-top-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[17] Yiu, Tony. «Understanding Cross Validation» In *Towards Data Science (Medium)*
    (2020) [https://towardsdatascience.com/understanding-cross-validation-419dbd47e9bd](/understanding-cross-validation-419dbd47e9bd)'
  prefs: []
  type: TYPE_NORMAL
- en: '[18] «Hyperparameter Tuning with GridSearchCV» In *Great Learning* (2022) [https://www.mygreatlearning.com/blog/gridsearchcv/](https://www.mygreatlearning.com/blog/gridsearchcv/)'
  prefs: []
  type: TYPE_NORMAL
