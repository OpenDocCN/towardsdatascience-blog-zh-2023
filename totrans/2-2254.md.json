["```py\nimport pandas as pd\nimport numpy as np\n\n#read the dataset\ndata = np.load('x20data.npz')\n\ndf = pd.DataFrame(data['data'])\ndf['target'] = data['label']\ndf['target'] = df['target'].replace(-1,0)\n\n#lets plot class distribution\ndf['target']\\\n.value_counts(normalize=True)\\\n.plot\\\n.bar(figsize=(10,6),alpha=0.6)\n\nplt.xlabel(\"Label\")\nplt.ylabel(\"Distribution %\")\nplt.yticks(list(np.linspace(0,1,11)))\nplt.grid(True)\nplt.show()\n```", "```py\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom imblearn.over_sampling import SMOTE\n\n# Set the font family for matplotlib\nplt.rcParams['font.family'] = 'Verdana'\n\n# Split your data into features and labels\nX = df.drop('target', axis=1)\ny = df['target']\n\n# Define the SMOTE resampling method\nsmote = SMOTE(random_state=42)\n\n# Resample the data using SMOTE\nX_resampled, y_resampled = smote.fit_resample(X, y)\n\n# Use TSNE for dimensionalilty reduction\nX_tsne = TSNE(n_components=2).fit_transform(X)\nX_resampled_tsne = TSNE(n_components=2).fit_transform(X_resampled)\n\n# Get the class ratios\noriginal_class_ratio = np.bincount(y) / len(y)\nresampled_class_ratio = np.bincount(y_resampled) / len(y_resampled)\n\n# Create subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n# Plot the original data\nax1.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='rainbow',alpha=0.4)\nax1.set_title(f'Original Data\\nClass Ratio: {original_class_ratio[1] * 100:.1f} : {original_class_ratio[0] * 100:.1f}')\n\n# Plot the resampled data\nax2.scatter(X_resampled_tsne[:, 0], X_resampled_tsne[:, 1], c=y_resampled, cmap='rainbow',alpha=0.4)\nax2.set_title(f'Resampled Data\\nClass Ratio: {resampled_class_ratio[1] * 100:.0f} : {resampled_class_ratio[0] * 100:.0f}')\n\nplt.show()\n```", "```py\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, ConfusionMatrixDisplay\n\ndef plot_confusion_matrix_and_roc(model,df):\n\n    \"\"\"Helper function for plotting model performances\"\"\"    \n\n    # Make predictions on the holdout set\n\n    X,y = df.drop('target',axis=1), df['target']\n\n    y_pred = model.predict(X)\n\n    # Create the confusion matrix\n    cm = confusion_matrix(y, y_pred)\n\n    # Calculate the AUC ROC score\n    auc = roc_auc_score(y, y_pred)\n\n    # Get the FPR and TPR for the ROC curve\n    fpr, tpr, thresholds = roc_curve(y, y_pred)\n\n    # Create a figure with two subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n    # Plot the confusion matrix\n    ax1.matshow(cm, cmap='Blues')\n    ax1.set_title('Confusion Matrix')\n    ax1.set_xlabel('Predicted')\n    ax1.set_ylabel('Actual')\n\n        # Add labels to the confusion matrix\n    # for i in range(cm.shape[0]):\n    #     for j in range(cm.shape[1]):\n    #         ax1.text(j, i, f'{cm[i, j]:n}', ha='center', va='center')\n\n    # Add labels for the TP, FP, TN, and FN cells\n    ax1.text(0, 0, f'TN: {cm[0, 0]:n}', ha='center', va='center', color='w')\n    ax1.text(0, 1, f'FN: {cm[1, 0]:n}', ha='center', va='center', color='k')\n    ax1.text(1, 0, f'FP: {cm[0, 1]:n}', ha='center', va='center', color='k')\n    ax1.text(1, 1, f'TP: {cm[1, 1]:n}', ha='center', va='center', color='k')\n\n    # Plot the ROC curve\n    ax2.plot(fpr, tpr, label='AUC ROC = %0.2f' % auc)\n    ax2.plot([0, 1], [0, 1], 'k--')\n    ax2.set_title('ROC Curve')\n    ax2.set_xlabel('FPR')\n    ax2.set_ylabel('TPR')\n    ax2.legend()\n\n    plt.show()\n\n#train the model\n\nfrom xgboost import XGBClassifier\n\nmodel = XGBClassifier()\nmodel.fit(X_resampled,y_resampled)\n\nplot_confusion_matrix_and_roc(model,df_holdout)\n```", "```py\nclass_weights = {0: 1, 1: 10}\n```", "```py\n# this will compute sample weight for us\nfrom sklearn.utils import compute_sample_weight\n\n#coputing class weights, this will be (num_samples_in_y,)\nsample_weights = compute_sample_weight(class_weight='balanced',y=y)\n\nmodel = XGBClassifier()\n\n# just plug sample weight parameter into your model\nmodel.fit(X,y,sample_weight=sample_weights)\n```"]