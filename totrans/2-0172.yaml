- en: A Complete Tutorial on Off-Policy Evaluation for Recommender Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐系统离线评估的完整教程
- en: 原文：[https://towardsdatascience.com/a-complete-tutorial-on-off-policy-evaluation-for-recommender-systems-e92085018afe](https://towardsdatascience.com/a-complete-tutorial-on-off-policy-evaluation-for-recommender-systems-e92085018afe)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-complete-tutorial-on-off-policy-evaluation-for-recommender-systems-e92085018afe](https://towardsdatascience.com/a-complete-tutorial-on-off-policy-evaluation-for-recommender-systems-e92085018afe)
- en: How to reduce the offline-online evaluation gap
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何缩小离线与在线评估之间的差距
- en: '[](https://biarnes-adrien.medium.com/?source=post_page-----e92085018afe--------------------------------)[![Adrien
    Biarnes](../Images/105f2bb62cb2bf825d74ea85b14eabfc.png)](https://biarnes-adrien.medium.com/?source=post_page-----e92085018afe--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e92085018afe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e92085018afe--------------------------------)
    [Adrien Biarnes](https://biarnes-adrien.medium.com/?source=post_page-----e92085018afe--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://biarnes-adrien.medium.com/?source=post_page-----e92085018afe--------------------------------)[![Adrien
    Biarnes](../Images/105f2bb62cb2bf825d74ea85b14eabfc.png)](https://biarnes-adrien.medium.com/?source=post_page-----e92085018afe--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e92085018afe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e92085018afe--------------------------------)
    [Adrien Biarnes](https://biarnes-adrien.medium.com/?source=post_page-----e92085018afe--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e92085018afe--------------------------------)
    ·18 min read·Mar 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e92085018afe--------------------------------)
    ·18分钟阅读·2023年3月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/601eef605ca244782eb4bd4437802193.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/601eef605ca244782eb4bd4437802193.png)'
- en: A recommender system jumping through the offline-online evaluation gap (AI-generated
    image)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统跨越离线与在线评估差距（AI生成的图像）
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In this tutorial, I will give some rationales about why one should care about
    off-policy evaluation. Then I’ll introduce the right amount of theory on the matter.
    I will not dive too deep into the latest methods and stop on what works well empirically.
    It will be sufficient for the following part describing the practical aspects
    of its implementation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我将解释为什么应该关注离线评估。接着我会介绍适量的理论。我不会深入探讨最新的方法，而是停留在经验上效果良好的方法。这对于接下来的实际实施部分将足够。
- en: Evaluating a recommender system
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估推荐系统
- en: Let’s say you have an idea to increase the performance of your recommendation
    system. Let’s simply say you got access to new metadata about your items, like
    the main topic of the items, which should really help in predicting user engagement
    on the platform more accurately. So how do you leverage that new feature? After
    a rounded exploration phase of the new data, you perform the necessary modifications
    in your data pipeline to transform the feature and feed it to the model along
    with the other ones. You can now trigger the learning phase to produce the new
    version of the model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个提高推荐系统性能的想法。假设你获得了关于项目的新元数据，例如项目的主要主题，这应该有助于更准确地预测用户在平台上的参与度。那么你如何利用这个新特性呢？在对新数据进行全面探索后，你需要对数据管道进行必要的修改，以转换特性并将其与其他特性一起输入模型。现在你可以启动学习阶段以生成新版本的模型。
- en: Great! Now, how do you evaluate the performance of the new model? How can you
    conclude whether the new feature helps the recommendation and will drive more
    engagement on your platform?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！那么，你如何评估新模型的表现？你如何确定新特性是否有助于推荐，并能驱动平台上的更多参与？
- en: Well, you could directly put the new version in production alongside the old
    one, redirect a portion of the traffic to the new model, and compare both performances
    using the metrics you defined as the most important for your business (Click-Through-Rate,
    Watch time, Dwelve time, Subscriptions, etc…). That is indeed called an AB test.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，你可以直接将新版本与旧版本一起投入生产，将一部分流量重定向到新模型，并使用你为业务定义的最重要的指标（点击率、观看时间、深入时间、订阅等）比较两者的表现。这确实被称为AB测试。
- en: '![](../Images/d1cc27bebc72de642b75dd187447df9a.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1cc27bebc72de642b75dd187447df9a.png)'
- en: Image by author
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: But what if the new version is terrible and incurs a huge drop in revenue or
    badly deteriorates the quality of your service? That could be very bad!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: In general, as a machine learning practitioner, you’ll first want to validate
    that the new version is better than the old one in an offline manner, meaning
    without having to show anything to real users (I sense that this is pretty trivial
    for most of you — but bear with me, I am coming to it).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding a drop in revenue is very important but it is not the only concern.
    Performing an online AB test takes time, especially if you don’t have a lot of
    traffic. In such cases, it might take up to several weeks for you to be able to
    conclude with sufficient statistical confidence. Also, in such cases, you might
    need to rewrite your model into a different stack in order for it to scale and
    match the latency requirements. This could incur additional weeks of work (even
    months). Hence, having a robust offline evaluation strategy allows one to iterate
    faster on different ideas or hyper-parameters to tune. One can assess thousands
    of ideas offline and only select the best promising candidates for online AB testing.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eefa71428ccffddbc3257b1b9659fef8.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: Building multiple candidate policies offline using logging data D_0
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: But offline, we can’t directly measure the click-through rates or watch times
    because we don’t know in advance whether future users will click or for how long
    they will consume the recommended items. We operate in a counterfactual scenario.
    That is, we don’t observe the rewards of the actions we did not take but only
    the rewards of taken actions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: In an online setting, this major evaluation drawback is not an issue anymore.
    We collect feedback based on what the user really does on the platform. On the
    contrary, an offline evaluation will always be strongly biased towards measuring
    past behaviors and is limited in terms of evaluating what will really happen online.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: So are we to accept this fact as granted or can we do something about it?
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What is off-policy evaluation and why should you care?
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s first introduce some important pieces of vocabulary that one will encounter
    when reading about off-policy evaluation. When a user enters your platform, the
    following happens:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d81f203062ab5014f4e20acbb4f6b78.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: Image by author
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'Off-policy evaluation is about evaluating a new policy with data collected
    from a different policy. We want to come up with an estimator **V_hat** that will
    estimate the performance of the new recommender **π_e** given the log data **D_0**
    collected and used to train the old policy **π_0**:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4c28e58a28f67d9f37fc8bb0a86bb29.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: The direct method
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As I said before when evaluating our recommender systems, we operate in a counterfactual
    setting. We do not know what would have been the outcomes of the actions we did
    not take. So basically we have a missing labels problem.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f9faa94f62f265c1b07873ba47df656c.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: Image by Author — Missing labels problem for the newly recommended items
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像 — 新推荐项目的缺失标签问题
- en: In the picture above, for the first recommendation, we managed to recommend
    the clicked item seen in the logged data so we can say that we have a positive
    prediction and the other ones are negatives. But for the rest of the evaluation
    dataset, we recommended completely different items so we have no clue what would
    have been the outcome.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，对于第一次推荐，我们成功推荐了记录数据中看到的点击项，因此我们可以说我们有一个积极的预测，而其他项是负面的。但对于其余的评估数据集，我们推荐了完全不同的项目，因此我们不知道结果会是什么。
- en: 'Given that we have logged interaction data **D_0,** a straightforward idea,
    that most ML practitioners can think of, is to use this data and produce a machine-learning
    model that will directly estimate the missing rewards (eg. replacing the question
    marks above with our best guesses). Then we can perform the evaluation as usual.
    More formally, we are interested in producing a reward model **r_hat** that will
    give us the expected value of the reward given a context and a specific action:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们已经记录了交互数据**D_0，** 一个简单的想法是，许多机器学习从业者可能会想到，利用这些数据来生成一个机器学习模型，该模型将直接估计缺失的奖励（例如，用我们的最佳猜测替换上面的问号）。然后我们可以像往常一样进行评估。更正式地说，我们希望生成一个奖励模型**r_hat**，该模型将给出在给定上下文和特定行动时奖励的期望值：
- en: '![](../Images/f68a59c13b7453f26c58793a33579fa3.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f68a59c13b7453f26c58793a33579fa3.png)'
- en: Many ML models can be used for this task. This is nice but in practice, it doesn’t
    work very well! Let’s see why…
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习模型可以用于这个任务。这虽然很好，但实际上效果并不理想！让我们来看看原因……
- en: 'It appears that the logged data and the data we want to predict do not come
    from the same distribution. There is a big covariate shift. We have logged data
    for some specific actions and we try to predict the reward for other ones (possibly
    completely different). Let’s say we are exploiting 2 features to try to minimize
    the regret of the reward model. We can then plot the reward space like so:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎记录的数据和我们想要预测的数据不来自相同的分布。存在较大的协变量偏移。我们记录了某些特定行动的数据，而尝试预测其他（可能完全不同）行动的奖励。假设我们利用了2个特征来尽量减少奖励模型的遗憾。我们可以如下绘制奖励空间：
- en: '![](../Images/3a716c9a3162282989cb876ca2c89fe2.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a716c9a3162282989cb876ca2c89fe2.png)'
- en: Image by author — Picturing the reward space
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像 — 奖励空间的可视化
- en: The green dots are positive observations (observations the user was interested
    in) and the red dots are negatives. What happens is that the reward space is mostly
    empty. There are vast regions for which we do not have any observations. But our
    new policy will probably have to decide in these unknown spaces. This means that
    it really matters how the reward model will extrapolate. It really matters where
    the decision boundaries will be located in the above plot. And this will not be
    reflected in the regret function the reward model will try to minimize. Hence,
    there is a very high chance that the extrapolation will be wrong. In the end,
    this method displays a low variance but can be very biased.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 绿色点是正观察（用户感兴趣的观察），红色点是负观察。发生的情况是，奖励空间大部分是空的。存在广阔的区域我们没有任何观察数据。但我们的新政策可能需要在这些未知空间中做出决策。这意味着奖励模型如何外推非常重要。决策边界在上述图中的位置也非常重要。而这不会在奖励模型尝试最小化的遗憾函数中反映出来。因此，外推出现错误的可能性非常高。最终，这种方法表现出低方差但可能非常偏倚。
- en: So using a model to predict the rewards does not seem like a working method.
    What else can we do? This is where the model-free approaches come in.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用模型来预测奖励似乎不是一个有效的方法。我们还能做什么？这时模型无关的方法就派上用场了。
- en: Inverse Propensity Score (IPS)
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逆倾向评分（IPS）
- en: The goal of this estimator is **to reweight the rewards in order to give more
    or less importance to actions according to the propensity of the old and new policies
    to take them.** We’ll want to lower the importance of an action that was much
    more taken by the old policy than by the new one (and vice-versa).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个估计器的目标是**重新加权奖励，以便根据旧政策和新政策采取行动的倾向来赋予行动更多或更少的重要性。** 我们希望降低旧政策比新政策更多采取的行动的重要性（反之亦然）。
- en: Let’s visualize this concept. Below is the probability space of the old policy
    **π_0** taking actions given the context**:**
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化一下这个概念。下面是旧政策**π_0**在给定上下文的情况下采取行动的概率空间**：**
- en: '![](../Images/141795fd44e8a1476fd804c2ea0f0b34.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/141795fd44e8a1476fd804c2ea0f0b34.png)'
- en: Image by author
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: The red and green dots are still the observed rewards. The dark area is where
    the concentration of the conditional action probability mass is located.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 红色和绿色的点仍然是观察到的奖励。暗区是条件动作概率质量的集中区域。
- en: 'Now the new policy **π_e** will take actions in a different part of the space,
    like so:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，新策略**π_e**将在空间的不同部分采取行动，如下所示：
- en: '![](../Images/19f0ec163ca996a16b4e64c9514ae3b8.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19f0ec163ca996a16b4e64c9514ae3b8.png)'
- en: Image by author
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'The goal of the IPS estimator will be to reweight the rewards in regard to
    these action probabilities by applying the following formula:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: IPS估计器的目标是通过应用以下公式来重新加权奖励：
- en: '![](../Images/1e7ff73f992a42c82790303e140f0a09.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e7ff73f992a42c82790303e140f0a09.png)'
- en: 'The ratio between the two action probabilities is called the importance weight.
    This gives the following result:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 两个动作概率之间的比率称为重要性权重。结果如下：
- en: '![](../Images/bc9527b38bd2944e8b8506606ee78b8b.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc9527b38bd2944e8b8506606ee78b8b.png)'
- en: Image by author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: It can be proved that this method is unbiased meaning that the more data we
    collect the more the IPS estimator will converge to the true value of the online
    reward.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明，这种方法是无偏的，意味着我们收集的数据越多，IPS估计器将越趋近于在线奖励的真实值。
- en: 'If you still have a hard time wrapping your head around this concept, let’s
    simplify it even more. Let’s say that way only have 2 recommendable items. We
    have collected the below amount of user interactions on these items (as seen in
    the evaluation dataset):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还是很难理解这个概念，让我们进一步简化。假设我们只有2个推荐的项目。我们已经收集了这些项目的用户互动数据（如评估数据集中所见）：
- en: '![](../Images/38ed19196a67a515c2f9abdf61cee8ee.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38ed19196a67a515c2f9abdf61cee8ee.png)'
- en: Image by author
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'Surely, if your new policy **π_e** always predicts item 1, the overall precision
    of your recommender will be higher than a policy that always predicts item 2\.
    Or should it? Now let’s add to this data, the associated amounts of displays:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果你的新策略**π_e**总是预测项目1，那么你的推荐系统的整体精度将高于一个总是预测项目2的策略。或者会这样吗？现在让我们将显示数据加入其中：
- en: '![](../Images/61dc46b0f4b90ae6794be879f3db4e86.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61dc46b0f4b90ae6794be879f3db4e86.png)'
- en: Image by author
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Now seeing things like you surely understand the added value of the off-policy
    evaluation. Taking into account the displays, we see that the old policy **π_0,**
    which produced the above logs, highly favored item 1\. And this was probably not
    the best possible recommendation. Favoring item 2 seems like a better option,
    but in order to realize that we’ll need to take the displays into account during
    the evaluation (eg. the propensity of policies to recommend items).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，从这些角度来看，你肯定能理解离线评估的附加价值。考虑到显示数据，我们看到旧策略**π_0**（生成了上述日志）高度偏爱项目1。而这可能不是最佳推荐。偏爱项目2似乎是一个更好的选择，但为了实现这一点，我们需要在评估过程中考虑显示数据（例如，策略推荐项目的倾向）。
- en: But this method can exhibit a strong variance in the sense that some reward
    estimations might be completely off. And this is because we are computing the
    ratio of two action probabilities which might sometimes take tiny values. For
    example, if the action probability of the old policy for a given observation is
    super small then the importance weight might become huge giving way too much importance
    to this specific observation.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 但这种方法可能会表现出很强的方差，因为某些奖励估计可能完全偏离。这是因为我们在计算两个动作概率的比率，而这些比率有时可能非常小。例如，如果给定观察的旧策略的动作概率非常小，则重要性权重可能变得很大，使得这个特定观察的权重过高。
- en: Clipped Inverse Propensity Score (CIPS)
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 裁剪反向倾向评分（CIPS）
- en: 'This estimator is a variation of the vanilla IPS that tries to address the
    challenge we just mentioned. It does so by clipping the importance weight so that
    it can not get higher than a maximal value:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个估计器是经典IPS的一个变体，试图解决我们刚刚提到的挑战。它通过裁剪重要性权重来实现这一点，以使其不会超过最大值：
- en: '![](../Images/af6dcbdf90e8172f55174fc3ac7c2ee6.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af6dcbdf90e8172f55174fc3ac7c2ee6.png)'
- en: The problem is that this modification breaks the unbiasedness of the IPS estimator
    but most importantly it introduces a hyper-parameter that can be very impractical
    to tune.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，这种修改破坏了IPS估计器的无偏性，但更重要的是，它引入了一个可能非常不实用的超参数。
- en: Self-Normalized Inverse Propensity Score (SNIPS)
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自归一化反向倾向评分（SNIPS）
- en: '![](../Images/6786ff3c6048a5d559608cbd95dea425.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6786ff3c6048a5d559608cbd95dea425.png)'
- en: This time we reweight the vanilla IPS by the inverse of the empirical mean of
    the importance weights. It simply rescales the results of the vanilla IPS. As
    I said, the problem with IPS is that sometimes we might get huge importance weights
    because of a tiny old policy action probability. The SNIPS estimator takes care
    of this problem by “canceling” this huge importance weight because it will be
    included in the denominator as well.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们通过重要性权重的经验均值的倒数来重新加权普通的IPS。这只是简单地调整了普通IPS的结果。正如我所说，IPS的问题在于有时我们可能会因为一个微小的旧策略动作概率而获得巨大的重要性权重。SNIPS估计器通过将这种巨大的重要性权重“取消”来解决这个问题，因为它也会包含在分母中。
- en: In the end, the estimator works very well empirically and has no hyper-parameter
    to tune which makes it very appealing for most ML practitioners.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，估计器在经验上表现非常好，并且没有超参数需要调整，这使得它对大多数机器学习从业者非常有吸引力。
- en: How to implement a reweighting off-policy evaluation method?
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现重新加权的离线策略评估方法？
- en: When going through the literature on the subject we can see that the vast majority
    of the efforts deployed by machine learning researchers focus on finding the best
    method to get an unbiased and low variance off-policy estimator of the online
    reward. But almost no one talks about the practicalities of implementing such
    strategies. What is worse, most researchers play along with simulated data to
    demonstrate the robustness of their approach. This is great from a theoretical
    standpoint but when it comes to implementing the method in real life, one shall
    struggle.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究相关文献时，我们可以看到，大多数机器学习研究者的努力都集中在寻找最佳方法，以获得一个无偏差且方差低的在线奖励离线策略估计器。但几乎没有人讨论实现这些策略的实际问题。更糟糕的是，大多数研究者使用模拟数据来展示他们方法的稳健性。这在理论上很棒，但在实际应用中，实施方法时会遇到困难。
- en: We have seen that the Self-Normalized Inverse Propensity Score (SNIPS) estimator
    works well empirically, has no hyperparameter to tune, has a low variance, and
    is consistent (although biased).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，自我标准化逆倾向评分（SNIPS）估计器在经验上表现良好，没有需要调整的超参数，方差低且一致（尽管有偏）。
- en: So are we ready to implement this estimator? Well, no just quite yet.
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 那么我们准备好实现这个估计器了吗？嗯，还没有完全准备好。
- en: The large part of whether the off-policy evaluation will work or not is mostly
    due to our ability to compute the action probabilities **π(a_i|x_i)** as accurately
    as possible. This is, in fact, **the most important part**!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 离线策略评估是否有效很大程度上取决于我们能多准确地计算动作概率**π(a_i|x_i)**。这实际上是**最重要的部分**！
- en: In order to do so, we are first going to take the necessary time to properly
    define the said probabilities. Let’s first think about the definition of the probability
    **π** to “select” an action **a_i** given a context **x_i**.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们首先要花时间正确地定义这些概率。让我们首先思考概率**π**在给定上下文**x_i**的情况下“选择”动作**a_i**的定义。
- en: One has to remember that **our goal is to reweight the rewards in order to give
    less importance to actions collected by the old logging policy π_0 when the new
    policy π_e is very much less likely to take them** (and vice versa).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 人们必须记住，**我们的目标是重新加权奖励，以便在新策略π_e很不可能采取这些动作时，减少对由旧日志策略π_0收集的动作的重视**（反之亦然）。
- en: So what is an action **a_i**? Here, it is important to make the distinction
    between the probability of the policy to output an item to recommend and the probability
    of the item being displayed to the user. These probabilities are vastly different.
    Indeed, an algorithm can output 1000 items but only a handful will be displayed
    to the user because of UI constraints (we can’t display 1000 items on a mobile
    device). And as I just said before, we are interested in the probabilities that
    the taken actions will end up in the logging data. So what we are interested in
    are the display probabilities and not the recommender output probabilities. And
    this fact is very important because it changes the source of the data one has
    to consider if one wants to estimate the action probabilities (eg. infer the display
    and not the output).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，什么是动作**a_i**？在这里，区分策略输出推荐项的概率和项被显示给用户的概率是很重要的。这些概率差异很大。确实，一个算法可以输出1000个项目，但由于UI限制（我们不能在移动设备上显示1000个项目），只有一小部分会显示给用户。正如我刚才所说，我们关注的是所采取的动作将最终出现在日志数据中的概率。因此，我们关心的是显示概率，而不是推荐系统的输出概率。这一点非常重要，因为它改变了如果一个人想要估计动作概率（例如，推断显示而不是输出）时需要考虑的数据源。
- en: Now, what is the context **x_i**? Well, in reality, it can be anything. You,
    as the modeler, are the one in charge of defining it. In theory, the more relevant
    and precise the context, that is, the more relevant information we have, the better
    we should able to estimate the display probabilities accurately. But in practice,
    it also makes the task more difficult. The bigger the feature space, the more
    data you need to produce robust estimates. And having robust estimates is critically
    important. If your display probabilities are completely wrong you’ll end up giving
    way too much or way too less importance to specific rewards which completely defeats
    the purpose of the off-policy evaluation. As I will explain briefly, a good practice
    is to simplify the context and only use the most important features.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，**x_i**的背景是什么？实际上，它可以是任何东西。作为建模者，你负责定义它。从理论上讲，背景越相关和精确，也就是说，我们拥有的信息越相关，我们应该能够更准确地估计展示概率。但在实践中，这也使得任务更加困难。特征空间越大，你需要的数据就越多以产生稳健的估计。而拥有稳健的估计至关重要。如果你的展示概率完全错误，你将会给特定奖励过多或过少的重视，这完全违背了离线评估的目的。正如我将简要说明的那样，一个好的做法是简化背景，仅使用最重要的特征。
- en: Display probability estimation
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 展示概率估计
- en: First, a method that fails
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 首先，一个失败的方法
- en: When I was first introduced to off-policy evaluation, I was to understand the
    implementation performed by one of my predecessors. It was a fancy method where
    the display probabilities were directly estimated by a deep-learning model. Just
    to clarify, we were not using the direct method here. The model was not used to
    directly estimate the unknown rewards. It was used to estimate the display probabilities
    **π(a|x)** of the old and new policies. Equipped with the said probabilities we
    could then compute the importance weights and apply one of the estimators like
    IPS, CIPS, or SNIPS.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当我第一次接触离线评估时，我需要了解我的前任之一实施的方法。这是一种高级方法，其中展示概率由深度学习模型直接估计。澄清一下，我们在这里没有使用直接方法。模型没有用于直接估计未知的奖励。它用于估计旧政策和新政策的展示概率**π(a|x)**。凭借这些概率，我们可以计算重要性权重并应用如
    IPS、CIPS 或 SNIPS 等估计器。
- en: 'Without getting into too many details, we can simply explain it with a picture.
    What was done here was to go from a model that looked (from a very broad angle)
    like this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 不深入细节，我们可以用一张图片来简单解释。这里做的是从一个从非常广角看起来像这样的模型：
- en: '![](../Images/07baf95e511223dab9523445ab69909c.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07baf95e511223dab9523445ab69909c.png)'
- en: Image by author — Broad overview of the deep recommender architecture
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像 — 深度推荐系统架构的广泛概述
- en: 'To something like this:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 变成这样：
- en: '![](../Images/e64a176a7ff8380d4ca72fb40e0b2fcd.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e64a176a7ff8380d4ca72fb40e0b2fcd.png)'
- en: Image by author — Modified architecture to predict the display probabilities
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像 — 修改后的架构以预测展示概率
- en: In theory, this was very neat because we would get probabilities given a very
    precise set of inputs. We could estimate the display probabilities on a very granular
    level. This was an implementation of an idea found in [this paper from the Google
    YouTube team](https://arxiv.org/pdf/1812.02353.pdf).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，这非常整洁，因为我们可以在给定非常精确的输入集合时获得概率。我们可以在非常细粒度的层面上估计展示概率。这是[谷歌 YouTube 团队的这篇论文](https://arxiv.org/pdf/1812.02353.pdf)中找到的一个想法的实现。
- en: 'But in practice it failed for the following reasons:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 但在实践中，它失败了，原因如下：
- en: 'We now had 2 targets instead of one (the next video selected by the user **and**
    the sequence of videos displayed as recommendations). This introduced additional
    questions that were not properly managed like: What if we only have the next video
    in the sequence but no associated displays (only 1 of the targets)? Indeed, in
    the training data, we were using sequences of videos coming from other sources
    (external) than the platform where we were not displaying any recommendations.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们现在有了两个目标而不是一个（用户**和**展示作为推荐的序列的下一个视频）。这引入了额外的问题，这些问题没有得到妥善管理，比如：如果我们只有序列中的下一个视频但没有相关的展示（只有一个目标）怎么办？确实，在训练数据中，我们使用的是来自其他来源（外部）的序列视频，而不是我们没有展示任何推荐的平台。
- en: Having 2 targets means having two different loss functions. How do you combine
    them? If one of the losses leaves in a completely different range of values (which
    is what was happening) then it might simply be insignificant when you combine
    them (which would make one of the branches not learn anything).
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有两个目标意味着有两个不同的损失函数。你如何将它们结合起来？如果其中一个损失在完全不同的值范围内（这就是发生的情况），那么在你将它们结合时，它可能会变得不重要（这会导致其中一个分支没有学习到任何东西）。
- en: The overall codebase gets much more complex.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整体代码库变得复杂得多。
- en: In theory, introducing auxiliary tasks can be beneficial for the main task but
    it requires them to be well aligned. In this case, the added complexity made the
    new display probability task deteriorate the performance of the main task.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理论上，引入辅助任务对主要任务是有利的，但需要它们之间良好的对齐。在这种情况下，增加的复杂性使得新的显示概率任务使主要任务的性能下降。
- en: The outputted probabilities were not real probabilities. Indeed the model was
    either way too confident or not confident at all. This is a pretty common scenario
    with deep learning models that are known to be overconfident given their tendency
    to overfit.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出的概率并不是实际的概率。实际上，模型要么过于自信，要么完全没有自信。这是深度学习模型中的一种常见情况，因为这些模型倾向于过拟合而表现出过度自信。
- en: 'This is what a kernel density plot of the display probabilities would look
    like (simulated with fake data):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是显示概率的内核密度图（使用虚假数据模拟的）会是什么样的。
- en: '![](../Images/1d0cc6be9f6be4165ea09891d6f75e05.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d0cc6be9f6be4165ea09891d6f75e05.png)'
- en: Image by author — Estimated display probability distribution
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片 — 估计的显示概率分布
- en: Here it is important to note that it is not just because you use a sigmoïd or
    a softmax as the last activation of your neural network that you will get back
    proper probabilities. The output of the network will effectively live in the space
    [0,1] but the probabilities will, most likely, not be well calibrated. When a
    well-calibrated model outputs 0.6 for a given observation, we shall find the associated
    target 60 percent of the time in the long run. Here is [a Wikipedia article](https://en.wikipedia.org/wiki/Calibration_(statistics))
    on the matter. Using well-calibrated probabilities is extra important in the context
    of off-policy importance weights computation because we are using the ratio of
    two display probabilities. When a model outputs 0.4 probability it should mean
    that the target is to be found twice as much as when a model outputs 0.2 probability.
    The ratio of 0.2 and 0.4 should hold the same importance as the ratio between
    0.4 and 0.8.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里需要注意的是，仅仅因为你在神经网络的最后激活函数中使用了sigmoïd或softmax，并不意味着你会得到正确的概率。网络的输出确实会位于[0,1]的范围内，但概率很可能不会得到良好的校准。当一个良好校准的模型对一个给定的观察值输出0.6时，我们在长期内应找到60%的目标。这里有[一篇维基百科文章](https://en.wikipedia.org/wiki/Calibration_(statistics))讨论这个问题。在离策略重要性权重计算的背景下，使用良好校准的概率非常重要，因为我们使用的是两个显示概率的比率。当模型输出0.4的概率时，应该意味着目标被找到的次数是模型输出0.2概率的两倍。0.2和0.4的比率应具有与0.4和0.8之间的比率相同的重要性。
- en: A simpler and more accurate method?
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更简单、更准确的方法？
- en: 'Given my first experience and considering that it was not a success, my question
    then was: do we really need to use a machine-learning model to estimate the display
    probabilities? Why not just simply use the available data and estimate it directly?'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我第一次的经历，并且考虑到它并不成功，我的问题是：我们真的需要使用机器学习模型来估计显示概率吗？为什么不直接使用现有的数据进行估计呢？
- en: If you want to perform off-policy evaluation you need to log the displayed items.
    So you should have that information in your database (whatever it is). If your
    logging system is sufficiently well thought out, you also have the context in
    the logs.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想进行离策略评估，你需要记录显示的项目。因此，你应该在你的数据库中拥有这些信息（无论是什么）。如果你的日志系统经过充分考虑，你也会在日志中拥有上下文。
- en: When performing the direct estimation of the display probabilities using the
    logged data we need to simplify things. The simpler the context, the more you’ll
    be able to produce robust estimates. For example, if you are creating a recommender
    that is trying to recommend a set of items to consume after a specific item, then
    the most important feature is the input item. Your goal will then be to estimate
    the probability that an item (let’s call it **i1**) is being displayed (action
    **a_i**) when a specific item (let’s call it **i2**) is being consumed (context
    **x_i**).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用日志数据进行展示概率的直接估计时，我们需要简化问题。上下文越简单，你将能够生成更稳健的估计。例如，如果你正在创建一个推荐系统，试图在特定项目后推荐一组项目，那么最重要的特征就是输入项。你的目标将是估计当特定项目（我们称之为**i2**）被消费（上下文**x_i**）时，某个项目（我们称之为**i1**）被展示（动作**a_i**）的概率。
- en: 'In order to estimate this probability you then have 2 choices. Either you go
    for:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计这个概率，你有两种选择。你可以选择：
- en: the frequentist estimate which is the number of times **i1** was displayed when
    **i2** was consumed divided by the total number of times **i2** was consumed.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 频率估计，即**i1**在**i2**被消费时被展示的次数除以**i2**被消费的总次数。
- en: 'the bayesian estimate: frequentist estimates can be unreliable when the number
    of trials (displays in this case) is too low. Stating that the display probability
    is 1.0 when we have 2 displays out of 2 trials does not seem like a robust estimate.
    We’ll explain how this works in a dedicated section at the end.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯估计：当试验次数（在此情况下为展示次数）过少时，频率估计可能不可靠。当我们在2次试验中有2次展示时，将展示概率声明为1.0似乎不是一个稳健的估计。我们将在最后的专门部分解释其工作原理。
- en: 'Because we are computing these estimates using the logged data that was produced
    by the old policy **π_0** we are effectively computing the display probabilities
    of the old policy. In other words, we are answering the question: how likely it
    is that the old policy chooses to display this specific item in this specific
    context?'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们使用的是由旧策略**π_0**产生的日志数据来计算这些估计，所以实际上我们是在计算旧策略的展示概率。换句话说，我们是在回答这样一个问题：旧策略在这种特定情况下展示这个特定项目的可能性有多大？
- en: Now, how do we estimate the display probability of the new policy **π_e** given
    that the said policy has never been put in production and we never collected display
    logs with it?
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们如何估计新策略**π_e**的展示概率，考虑到该策略从未投入生产，我们也没有收集过与之相关的展示日志？
- en: 'Well, we do have the training data that was used to produce the policy **π_e**
    so we have the context (the input item per observation in this case) and we can
    record the predictions of **π_e** on the training data. So we have the input and
    the output of the policy. The only thing that we are missing is whether the output
    of the policy will be displayed to the user and up to what rank. So here is the
    trick. We know that items on the top of the output of the recommender will have
    more chances to be displayed to the user. We can even determine the overall probability
    that an item will be displayed at specific ranks. It could for example look something
    like this:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们确实拥有用于生成策略**π_e**的训练数据，因此我们有上下文（在此情况下为每次观察的输入项），并且我们可以记录**π_e**在训练数据上的预测。所以我们拥有了策略的输入和输出。我们唯一缺少的就是策略的输出是否会展示给用户以及展示的排名。因此，这里有一个技巧。我们知道推荐系统输出顶部的项目更有可能展示给用户。我们甚至可以确定项目在特定排名下被展示的总体概率。例如，它可能看起来像这样：
- en: '![](../Images/b1c05be0c87adc704f667eec681650b6.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1c05be0c87adc704f667eec681650b6.png)'
- en: So the trick is to state that an item (outputted by the policy) being displayed
    to the user is a Bernoulli trial of probability p. Then we can sample the displayed
    outcome according to p.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，技巧在于将一个项目（由策略输出）展示给用户视为具有概率p的伯努利试验。然后我们可以根据p进行样本展示结果。
- en: In the end, will have everything we need to compute the display probabilities
    of the new policy **π_e** exactly in the same way we did for **π_0** (in a frequentist
    or bayesian fashion).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们将拥有所有需要的东西，以相同的方式精确计算新策略**π_e**的展示概率，就像我们对**π_0**所做的那样（无论是频率方法还是贝叶斯方法）。
- en: Equipped with the display probabilities we can then compute the importance weights
    and perform an off-policy evaluation with the SNIPS estimator.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有展示概率后，我们可以计算重要性权重，并使用SNIPS估计器进行离策略评估。
- en: I employed this method and it worked very effectively. Indeed, without off-policy
    evaluation, the evaluation pipeline was giving a 10% decrease in the main metrics
    but with off-policy evaluation, it transformed into a 2% increase. It was then
    decided to perform an AB test which was indeed positive.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我采用了这种方法，并且效果非常好。事实上，没有离线策略评估时，评估流程的主要指标下降了10%，但有了离线策略评估，它转变为2%的增长。于是决定进行一次AB测试，结果确实是积极的。
- en: Side note on bayesian estimation of display probabilities
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于展示概率的贝叶斯估计的附注
- en: As explained above, the display probabilities can be estimated with a Bayesian
    approach to get more robust results.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，展示概率可以通过贝叶斯方法来估计，以获得更稳健的结果。
- en: In the frequentist paradigm, the parameter we want to estimate has a truly unique
    value that we will get closer to by running more and more trials. It is a fixed
    quantity, and there is no probability associated with it. On the contrary, in
    the bayesian perspective, the parameter of interest has no true unique value but
    is rather described with a probability distribution.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在频率主义范式中，我们想要估计的参数有一个真实唯一的值，我们通过不断增加实验次数来逐步接近它。它是一个固定的量，与其无关的是概率。相反，在贝叶斯视角下，感兴趣的参数没有真实的唯一值，而是通过概率分布来描述。
- en: '**What is the problem with the frequentist approach in our case?**'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**在我们的情况下，频率主义方法有什么问题？**'
- en: Our parameter of interest is the display probability conditioned on the input
    item. So we are computing the ratio of 2 count variables. The issue comes from
    the fact that in some cases, the dividend of the ratio is too low. When the number
    of displays is really low, the uncertainty in our estimate is very high. The variance
    of the error of the estimator is really high.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的参数是基于输入项的展示概率。因此，我们正在计算两个计数变量的比率。问题在于，在某些情况下，比率的分子过低。当展示次数非常少时，我们的估计不确定性非常高。估计器的误差方差非常大。
- en: '**How can we remedy this?**'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们如何解决这个问题？**'
- en: Count variables follow a binomial distribution. The number of displays follows
    a binomial of parameter p (the probability of a display). There is a p probability
    to obtain a display and a 1-p probability to obtain a no-display event. The ratio
    between the two counts follows a beta distribution with parameter alpha = (number
    of successes + 1) and beta = (number of failures + 1).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 计数变量遵循二项分布。展示次数遵循参数p（二项展示的概率）的二项分布。有p的概率获得展示，1-p的概率获得不展示事件。两个计数之间的比率遵循一个参数为alpha
    =（成功次数+1）和beta =（失败次数+1）的贝塔分布。
- en: '**What are we going to compute?**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们要计算什么？**'
- en: 'In the bayesian paradigm, we define a prior belief P(Θ) about the parameter
    distribution. We also define a likelihood function P(X|Θ) that tells us how likely
    it is to observe the data given our belief of the parameter distribution. Everything
    then stands on the shoulder of the Bayes formula:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯范式中，我们定义了一个关于参数分布的先验信念P(Θ)。我们还定义了一个似然函数P(X|Θ)，它告诉我们在给定对参数分布的信念的情况下观察到数据的可能性。所有的工作都基于贝叶斯公式：
- en: '![](../Images/61e682b46f572aaf1f392bf704767f86.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61e682b46f572aaf1f392bf704767f86.png)'
- en: The Bayes theorem
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理
- en: The posterior distribution P(Θ|X) is equal to the likelihood function P(X|Θ)
    multiplied by the prior P(Θ) and divided by the evidence P(X), the probability
    of observing the data. Now computing the probability of observing the data is
    generally intractable. If we really need to obtain the exact probability estimates
    along with credible intervals, etc…, we need to rely on advanced methods of Bayesian
    inference like Markov Chain Monte Carlo - MCMC (and we get into a totally different
    ball game).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 后验分布P(Θ|X)等于似然函数P(X|Θ)乘以前验P(Θ)，再除以证据P(X)，即观察数据的概率。现在计算观察数据的概率通常是不可处理的。如果我们确实需要获得精确的概率估计以及可信区间等，我们需要依赖先进的贝叶斯推断方法，如马尔可夫链蒙特卡洛
    - MCMC（这就涉及到完全不同的领域）。
- en: Now the good thing in our case, is that the prior follows a well-known distribution
    and so does the likelihood. The likelihood models the probability of observing
    a success (a click) for a given trial (a display) so it also follows a binomial
    law. The parameter of interest is the ratio of two count variables that both follow
    a binomial so the prior follows a beta distribution. And the good thing about
    the binomial and the beta distributions is that they are conjugate priors. This
    means that we have a proven theoretical result that directly gives us the form
    of the posterior without computing the evidence. In this case, the posterior also
    follows a beta distribution with parameters alpha_posterior = (alpha_prior + nb_success)
    and beta_posterior = (beta_prior + nb_failures).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在我们的情况下，优点是先验遵循一个知名分布，似乎似然也遵循一个知名分布。似然模型描述了在给定试验（显示）中观察到成功（点击）的概率，因此它也遵循二项分布。感兴趣的参数是两个计数变量的比率，这两个变量都遵循二项分布，因此先验遵循beta分布。而二项分布和beta分布的优点是它们是共轭先验。这意味着我们有一个经过验证的理论结果，可以直接给出后验分布的形式，而无需计算证据。在这种情况下，后验分布也遵循beta分布，参数为alpha_posterior
    = (alpha_prior + nb_success)和beta_posterior = (beta_prior + nb_failures)。
- en: Now that we know that the posterior follows a beta distribution, applying the
    formula of the expected value of the beta distribution we can directly compute
    the display probability which is simply the ratio between alpha_posterior and
    beta_posterior. See [Wikipedia](https://en.wikipedia.org/wiki/Beta_distribution)
    for more information.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道后验分布遵循beta分布，应用beta分布的期望值公式，我们可以直接计算显示概率，该概率只是alpha_posterior和beta_posterior的比率。有关更多信息，请参见[维基百科](https://en.wikipedia.org/wiki/Beta_distribution)。
- en: Also, an additional benefit of opting for a Bayesian approach in computing the
    display probability is that we know the exact distribution of the posterior. Instead
    of only using the expected value to perform the ranking, we could also have performed
    a stochastic ranking procedure by sampling into the posterior to obtain the ranking
    scores. This would favor exploration at the expense of exploitation but it would
    probably result in more diversified recommendations.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，选择贝叶斯方法计算显示概率的额外好处是我们知道后验的确切分布。除了使用期望值进行排序，我们还可以通过对后验分布进行抽样来执行随机排序程序，以获得排序分数。这将有利于探索，但会牺牲开发，但可能会导致更为多样化的推荐。
- en: Conclusion
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, off-policy evaluation is a very effective method to reduce the
    offline-online gap and avoid the disappointment of getting inclusive results with
    online tests when your offline results were positive (or the contrary).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，离线策略评估是一种非常有效的方法，用于减少离线与在线之间的差距，并避免当离线结果是积极的（或相反）时在在线测试中获得包容性结果的失望。
- en: When you start off with your recommendation endeavors it might not be the absolute
    most important priority to set up an off-policy evaluation strategy but once you
    start to have a model in production and you start iterating towards better models
    it might be a good time to seriously think about it.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始进行推荐工作时，设置离线策略评估策略可能不是绝对最重要的优先事项，但一旦你开始有了生产中的模型并开始迭代改进模型时，这可能是一个认真考虑的好时机。
- en: Also, one important lesson that one could draw from my experience is that one
    should always start with simple methods and iterate from there.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，从我的经验中可以得出一个重要的教训，即应始终从简单的方法开始，并从那里迭代。
