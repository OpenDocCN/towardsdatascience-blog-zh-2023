- en: 'Inspecting Data Science Predictions: Individual + Negative Case Analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/inspecting-data-science-predictions-individual-negative-case-analysis-d2e4ddbcf830](https://towardsdatascience.com/inspecting-data-science-predictions-individual-negative-case-analysis-d2e4ddbcf830)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to inspect specific predictions and conduct negative case analyses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://adamrossnelson.medium.com/?source=post_page-----d2e4ddbcf830--------------------------------)[![Adam
    Ross Nelson](../Images/030b86a8c8bbd40c6acf60d1e387950c.png)](https://adamrossnelson.medium.com/?source=post_page-----d2e4ddbcf830--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d2e4ddbcf830--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d2e4ddbcf830--------------------------------)
    [Adam Ross Nelson](https://adamrossnelson.medium.com/?source=post_page-----d2e4ddbcf830--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d2e4ddbcf830--------------------------------)
    ·14 min read·Jul 21, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'Somewhere around 40 to 43% of the time when I am showing new learners how to
    use the `.predict()` methods I get the following question:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Where are the predictions?**'
  prefs: []
  type: TYPE_NORMAL
- en: I wish this was a question learners would ask more often. It is an insightful
    question, especially for folks who are newer to Python, data science, and who
    may be seeing the `.predict()` method for the first time.
  prefs: []
  type: TYPE_NORMAL
- en: For sure the number of groups who ask this question is less than half, but possibly,
    the proportion is lower than 30 or 20%. I don’t keep precise track.
  prefs: []
  type: TYPE_NORMAL
- en: In part one of this deep dive, this article will first show how to build a simple
    predictive model, second how to generate predictions, and third cover how to inspect
    predictions more closely.
  prefs: []
  type: TYPE_NORMAL
- en: For part two of this deep dive this article will also show why it is useful
    to know how to inspect individual predictions plus also why it is necessary to
    inspect individual predictions. Having the ability to inspect individual predictions
    opens a range of analytical avenues, for example not the least of which is the
    negative case analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part One: Predict Methods'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are not yet familiar with building predictive model I suggest you consider
    reading one or more other articles that cover this topic. Chapter 11 of [*Confident
    Data Science: Discovering The Essential Skills of Data Science*](https://a.co/d/hS2cbou)
    (by, Me) shows how to build predictive models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in [Fake Birds & Machine Learning: Using the popular bird variety
    data to demonstrate nearest neighbors classification](/fake-birds-machine-learning-acc553312a8f)
    I shared code that trained a machine learning model that can predict bird species
    variety based on a bird’s weight, length, location, and color. This fake birds
    example demonstrated predictive modeling with the [fake bird species data](/how-to-make-fictional-data-7a79ce86a350).'
  prefs: []
  type: TYPE_NORMAL
- en: A Simple Predictive Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To help us focus on inspecting specific individual predictions this subsection
    will speed through the creation of a predictive model. To be speedy this subsection
    skips optimizing hyper parameters and also skips a few data preparation steps.
  prefs: []
  type: TYPE_NORMAL
- en: Also to speed things along we look at evaluation through alternate methods aside
    from the more traditional `accuracy_score`, `classification_report`, and `confusion_matrix`
    functions from SciKit Learn.
  prefs: []
  type: TYPE_NORMAL
- en: For this simple, speedy, predictive model, lets take a look at automobile data.
    Speedy pun intended here!
  prefs: []
  type: TYPE_NORMAL
- en: Usually we would like a set of data that consists of more than a few hundred
    observations. However, I am fond of using this automobile data, from Seaborn,
    because almost everyone understands or knows a few basics about automobiles.
  prefs: []
  type: TYPE_NORMAL
- en: '**First, open the data.** As is customary for many boot camp demonstrations,
    or demonstration articles online you first open the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Second, prepare the data.** After that you prepare a target variable. For
    this demonstration we’ll create a target variable to predict that will equal 1
    when the vehicle was manufactured in the United States and then 0 when manufactured
    elsewhere.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Third, train, test, and split.** Another customary step is to also train,
    test, and split (`train, test, split`) the data as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Fourth, train the model.** Now the data are ready for training.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Generating Predictions With the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Fifth, generate predictions from the trained model.** Once trained the model
    is ready to generate predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Sixth, begin the exploration/evaluation process.** Once generated, the predictions
    are ready for inspection. Here I will indulge in less-common inspection options.
    In the bootcamp context, of course, I share the standard `accuracy_score`, `classification_report`,
    and `confusion_matrix` functions from SciKit Learn (along with the equivalents
    for mean square errors and root mean squared errors). However, I also believe
    it is important to show alternate methods. I find learners appreciate seeing the
    alternate methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Showing alternate methods help leaners understand what `accuracy_score`, `classification_report`,
    and `confusion_matrix` do, how they do it, and how to explain what they do. It
    is important to be able to explain this to others when there are questions about
    the work.
  prefs: []
  type: TYPE_NORMAL
- en: More fundamentally, learning, knowing, and using alternate methods let data
    scientists replicate (and verify) more traditional approaches. In other words,
    this empowers learners to do more than merely just trust the traditional out-of-the-box
    options. No need to rely on trust when you can also independently verify the results.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have the predictions you can begin inspecting the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The beauty of this code is that it is much more complex than it looks. Consider
    how firstly, it compares each prediction (found in `pred`) with the corresponding
    actual values (all found in `y_test`). The comparison `pred == y_test` will return
    a Boolean array (a series of `True` and `False` values), where `True` indicates
    that the prediction matches the actual value, and `False` indicates a mismatch.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the magic. . . then, the `.mean()` function applied to this array essentially
    finds the proportion of correct predictions. Here is how. In Python, when calculating
    the mean of a Boolean array, `True` values are considered as 1 and `False` values
    as 0\. This effectively calculates the proportion of correct predictions (or the
    accuracy of the model), as it divides the number of `True` values (the number
    correct predictions are the sum of all values True = 1 while False = 0) by the
    total number of predictions. In this way the math works out such that the formula
    for the mean is equivalent to the formula for proportion True.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, this code also multiplies by 100 to make a readable percentage. And
    an f-string reports the results. If you’re following along, with the same random
    states, you should find an accuracy rate of about 84%.
  prefs: []
  type: TYPE_NORMAL
- en: Another option is to use `pd.crosstab()` to show what would also be a confusion
    matrix as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Which then produces a version of the following.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: From these results we see the model correctly identified/predicted 40 vehicles
    as having been manufactured in the United States, correctly identified/predicted
    27 vehicles manufactured abroad, and incorrectly identified/predicted around 12
    other vehicles (9 false negatives + 3 false positives).
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting Individual Predictions More Closely
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The core of this article is to explain how to inspect individual predictions.
    This article answers the question I often get from bootcamp learners who are seeing
    the `.predict()` method for the first time.
  prefs: []
  type: TYPE_NORMAL
- en: '**Where are the predictions?**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/997982370f4880b2de5569a68ff0e0e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image Credit: Author’s illustration created in Canva using Canva stock images.'
  prefs: []
  type: TYPE_NORMAL
- en: A good first step in conducting this closer examination is to append the true
    target values, plus the predicted target values, to the original training data.
    When you do this, you can compare results side-by-side and in context along with
    each predictor variable.
  prefs: []
  type: TYPE_NORMAL
- en: An interesting side note for folks coming from other programming languages.
    Stata’s equivalent to SciKit Learn’s `.predict()` method automatically appends
    the predictions to the original data. In Python, that merger / concatenation requires
    some additional work.
  prefs: []
  type: TYPE_NORMAL
- en: The following code can accomplish this merger / concatenation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: For a result that will resemble the following.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d16c635db3add5b48e0219d27e75ab4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image Credit: Screen grab from Jupyter Notebook generated with the code shown
    here.'
  prefs: []
  type: TYPE_NORMAL
- en: This output shows the first and last five observations along with a column called
    `isUS` (which show the true values 1 = manufactured in the United States and 0
    = manufactured abroad) and another column called `preds` (which show the predicted
    values 1 = predicted to have been manufactured in the United States and 0 = predicted
    to have been manufactured in the United States).
  prefs: []
  type: TYPE_NORMAL
- en: Observations numbers 4 and 77 show mismatched actual values and predicted values.
    Observation number 4 is a false negative, which means the model incorrectly predicted
    the vehicle to have been manufactured abroad. While observation number 77 is a
    false positive, which means the model incorrectly predicted the vehicle to have
    been manufactured in the United States.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part Two: Utility of Detailed Inspections'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Questions to Ask During Inspection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inspecting individual predictions from a model is not just about confirming
    whether or not they were correct. It’s also a process of inquiry that can yield
    important insights and inform further model refinement. When inspecting individual
    predictions, it’s beneficial to ask several key questions.
  prefs: []
  type: TYPE_NORMAL
- en: Would a domain expert agree with the (wrong) predictions?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How would a causal observer have predicted the outcome? Would the casual observer
    agree with the (wrong) prediction?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are there other features or predictors, not included in the model, that might
    have helped the model produce a better result in this specific (wrong) case?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Would subject matter experts agree with feature importances? Could different
    or more thorough data preparation resulted in different feature importances with
    which subject matter might better agree?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What features may have contributed to this particular (wrong) prediction?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are there any patterns among incorrect predictions?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How confident was the model in this prediction?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All of these questions can help in evaluating the existing model and inform
    further development and revision ahead.
  prefs: []
  type: TYPE_NORMAL
- en: Negative Case + Other Data Sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The process of digging into the questions proposed above is the process of conducting
    an individual negative case analysis. For the first question, whether domain experts
    would agree, your work could involve preparing an export of the data for you to
    share with domain experts.
  prefs: []
  type: TYPE_NORMAL
- en: This kind of export can let domain experts review the results on their own time
    and then report back to you their thoughts, observations, and ideas. The following
    code produces an HTML file, consisting of only the incorrect predictions, that
    anyone can review in any web browser.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This same export could be shared with other trusted advisors or colleagues who
    are not necessarily subject matter experts. A lay person’s input may be valuable
    also.
  prefs: []
  type: TYPE_NORMAL
- en: For the third question on whether other features or predictors, not included
    in the model, that might have helped the model produce a better result in specific
    (wrong) cases, most models come with at least one method that can assist in evaluating
    feature importance.
  prefs: []
  type: TYPE_NORMAL
- en: However as it turns out, the K-Nearest Neighbors (KNN) algorithm does not directly
    provide a feature importances metric or function. This is not an oversight. The
    KNN approach makes predictions based solely on the similarity between feature
    vectors without weighting individual features. This means we need an indirect
    method of evaluating feature importance, some kind of proxy. I provide such a
    proxy below.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, another model that supports feature importance, such as a decision
    tree or random forest, could be used. Absent building another model however, it’s
    possible to gauge the importance of features by seeing how the model performance
    changes when rotating through feature holdouts.
  prefs: []
  type: TYPE_NORMAL
- en: Rotating through each feature, to hold that feature out during training, is
    a procedure that requires a simple for loop as shown below. Any reduction in performance,
    while a given feature is absent, can be taken as a meaningful measure of feature
    importance. Here is code that can help in the process of measuring feature importance
    by rotating through multiple models while iterating through and in turn holding
    out each feature.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In this example, if you are following along you will see results that show displacement
    and weight as the most important features / predictors. When excluded, weight
    reduces model accuracy by 3.8%. However when excluded, displacement reduces the
    model accuracy by 2.5%.
  prefs: []
  type: TYPE_NORMAL
- en: Further Enriching Opportunities for Feedback
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Related to, and perhaps in addition to sharing the sheer outcomes with others
    as described above, it can also be helpful to add columns that show prediction
    probabilities. Building on the code above, this next code block provides outputs
    with the addition of two columns containing the prediction probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2382a73cbc708c83ba466ed15133673a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image Credit: Screen grab from Jupyter Notebook generated with the code shown
    here.'
  prefs: []
  type: TYPE_NORMAL
- en: If you have not studied KNN here is a smidge more on how KNN works. The K-Nearest
    Neighbors (KNN) algorithm determines class probabilities based on the majority
    vote of the nearest neighbors within the training data set’s hyper space. Essentially,
    it counts the number of data points belonging to each class among the ‘k’ nearest
    neighbors. The class with the most neighbors is the prediction for the new point.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, when determining probabilities, KNN calculates the fraction of the ‘k’
    nearest neighbors that belong to each class. For example, if you set k=7 as we
    have here and three of the seven nearest neighbors to a test point belong to Class
    A, and four belong to Class B, then the KNN algorithm would assign a probability
    of 0.4285 (3 out of 7) to the test point being of Class A and a probability of
    0.5714 (4 out of 7) to Class B.
  prefs: []
  type: TYPE_NORMAL
- en: From this output we see that the false negative in observation number 4 was
    off by a high margin. While the false positive in observation number 77 was a
    close call. While no model is perfect you might consider the misclassification
    in observation number 77 as a lost cause. However, the misclassification in observation
    number 4 might be worth a closer look as a significant opportunity to improve
    model performance.
  prefs: []
  type: TYPE_NORMAL
- en: We can also take the descriptive statistics from our probability columns as
    another look at model performance. The following code will show these statistics.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Which will show a version of the following output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f193bff6c5e2d74926853c18f3a7e2c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image Credit: Screen grab from Jupyter Notebook generated with the code shown
    here.'
  prefs: []
  type: TYPE_NORMAL
- en: This result shows that for at least one of the incorrect results the model was
    100% certain of its incorrect predictions. It will be worth exploring that observation
    more closely. It would seem that there may be an opportunity to improve a prediction
    that was not only wrong, but could have been no further wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion + Review
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In part one of this deep dive, I first showed how to build a simple predictive
    model, second how to generate predictions, and third I covered how to inspect
    individual specific predictions more closely.
  prefs: []
  type: TYPE_NORMAL
- en: For part two of this deep dive this article I also showed why it is useful to
    know how to inspect individual predictions. Having the ability to inspect individual
    predictions opens a range of analytical avenues.
  prefs: []
  type: TYPE_NORMAL
- en: On some views, this article did what many books, video courses, and bootcamp
    educational programs skip over. They tend to skip over and overly simplify evaluation
    of model performance following `.predict()` methods.
  prefs: []
  type: TYPE_NORMAL
- en: In this article we learned and explored the need to question and inspect model
    predictions. The article provides and discusses multiple example code blocks and
    strategies that effectively explore prediction results in depth.
  prefs: []
  type: TYPE_NORMAL
- en: To set things up we sped through (pun intended) the setup for a predictive model
    involving automobile data from Seaborn. Along the way we also explored the concepts
    of prediction confidence, feature importance, reporting results, and sharing results
    with others for an opportunity to get feedback from lay people and domain or subject
    matter experts.
  prefs: []
  type: TYPE_NORMAL
- en: Stepping back, this deep, deep, deep, dive serves as a testament to the incredible
    power of machine learning and data science. The complex interplay between — the
    data, the models, the predictions, the scientists, the code, the notebooks, the
    domain experts, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Most prominently this article also prepared readers in ways that can help connect
    the tech with human understanding and intuition. The ability to explore predictions
    at a high level of detail, such as those shown here, also show the fascinating
    landscape of data science.
  prefs: []
  type: TYPE_NORMAL
- en: In our work we can uncover insights hidden in vast pools of data, and it is
    here that we can harness those insights to drive decisions, innovations, and progress.
    As this article has emphasized, effective utilization of machine learning goes
    beyond simply developing and deploying algorithms at speed or scale.
  prefs: []
  type: TYPE_NORMAL
- en: Instead our work requires a curious and critical mind that is willing to question,
    inspect, and understand all of the details. All of them. I’m the kind of data
    scientist who strives to leave no stone unturned. This journey through `.predict()`
    method results is just one instance of that wider quest.
  prefs: []
  type: TYPE_NORMAL
- en: This article is an invitation to dive deeper into the heart of your data, to
    seek out its truths, and to question its narratives. The code blocks and strategies
    presented here, and in all of my articles, are more than just instructions — they
    are catalysts for your learning and creativity.
  prefs: []
  type: TYPE_NORMAL
- en: I encourage you to take this code, experiment with it, and apply it to new datasets.
    Break it, fix it, improve it. Each dataset has a different story to tell, and
    each exploration will yield different insights. The world of data is a vast, complex,
    and beautiful place. Dive in, explore, and let the data guide your journey.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks For Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Are you ready to learn more about careers in data science? I perform one-on-one
    career coaching and have a weekly email list that helps data professional job
    candidates. [Contact me to learn more](https://coaching.adamrossnelson.com/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks for reading. Send me your thoughts and ideas. You can write just to
    say hey. And if you really need to tell me how I got it wrong I look forward to
    chatting soon. Twitter: [@adamrossnelson](https://twitter.com/adamrossnelson)
    LinkedIn: [Adam Ross Nelson](https://www.linkedin.com/in/arnelson/).'
  prefs: []
  type: TYPE_NORMAL
