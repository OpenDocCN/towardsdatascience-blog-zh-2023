- en: 'Deep GPVAR: Upgrading DeepAR For Multi-Dimensional Forecasting'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Deep GPVAR：升级 DeepAR 实现多维度预测
- en: 原文：[https://towardsdatascience.com/deep-gpvar-upgrading-deepar-for-multi-dimensional-forecasting-e39204d90af3](https://towardsdatascience.com/deep-gpvar-upgrading-deepar-for-multi-dimensional-forecasting-e39204d90af3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deep-gpvar-upgrading-deepar-for-multi-dimensional-forecasting-e39204d90af3](https://towardsdatascience.com/deep-gpvar-upgrading-deepar-for-multi-dimensional-forecasting-e39204d90af3)
- en: Amazon’s new Time-Series Forecasting model
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon 的新时间序列预测模型
- en: '[](https://medium.com/@nikoskafritsas?source=post_page-----e39204d90af3--------------------------------)[![Nikos
    Kafritsas](../Images/de965cfcd8fbd8e1baf849017d365cbb.png)](https://medium.com/@nikoskafritsas?source=post_page-----e39204d90af3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e39204d90af3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e39204d90af3--------------------------------)
    [Nikos Kafritsas](https://medium.com/@nikoskafritsas?source=post_page-----e39204d90af3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@nikoskafritsas?source=post_page-----e39204d90af3--------------------------------)[![Nikos
    Kafritsas](../Images/de965cfcd8fbd8e1baf849017d365cbb.png)](https://medium.com/@nikoskafritsas?source=post_page-----e39204d90af3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e39204d90af3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e39204d90af3--------------------------------)
    [Nikos Kafritsas](https://medium.com/@nikoskafritsas?source=post_page-----e39204d90af3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e39204d90af3--------------------------------)
    ·19 min read·Mar 24, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e39204d90af3--------------------------------)
    ·19 分钟阅读·2023年3月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c5191da2e60488dbedbc2c1d45651487.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5191da2e60488dbedbc2c1d45651487.png)'
- en: Created with DALLE [1]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DALLE 创建 [1]
- en: '**Warning**: The information about this model, including the tutorial below
    is not up-to-date. Read the updated version [here](https://aihorizonforecast.substack.com/p/deep-gpvar-upgrading-deepar-for-multi)'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**警告：** 关于该模型的信息，包括下面的教程，可能不是最新的。阅读更新版本 [这里](https://aihorizonforecast.substack.com/p/deep-gpvar-upgrading-deepar-for-multi)'
- en: 'What is the most enjoyable thing when you read a new paper? For me, this is
    the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读新论文时最令人愉快的事情是什么？对我而言，就是以下内容：
- en: Imagine a popular model suddenly getting upgraded — with just a few elegant
    tweaks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，一个流行的模型突然升级——只需几个优雅的调整。
- en: Three years after [*DeepAR*](https://medium.com/towards-data-science/deepar-mastering-time-series-forecasting-with-deep-learning-bc717771ce85)[1],
    Amazon engineers published its revamped version, known as ***Deep GPVAR* [2] *(D****eep*
    ***G****aussian-****P****rocess* ***V****ector* ***A****uto-****r****egressive)*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 三年后，Amazon 工程师发布了其改版版本，称为 ***Deep GPVAR* [2] *(D****eep* ***G****aussian-****P****rocess*
    ***V****ector* ***A****uto-****r****egressive)*。
- en: 'This is a much-improved model of the original version. Plus, it is open-source.
    In this article, we discuss:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对原始版本的显著改进版。此外，它是开源的。本文中，我们讨论：
- en: '**How *Deep GPVAR* works in depth.**'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深入了解 *Deep GPVAR* 的工作原理。**'
- en: '**How DeepAR and *Deep GPVAR* are different.**'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DeepAR 和 *Deep GPVAR* 的不同之处。**'
- en: '**What problems does *Deep GPVAR* solve and why it’s better than DeepAR?**'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '** *Deep GPVAR* 解决了哪些问题，以及为何它比 DeepAR 更好？**'
- en: '**A hands-on tutorial on energy demand forecasting.**'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有关能源需求预测的实践教程。**'
- en: I’ve launched [**AI Horizon Forecast**](https://aihorizonforecast.substack.com/)**,**
    a newsletter focusing on time-series and innovative AI research. Subscribe [here](https://aihorizonforecast.substack.com/)
    to broaden your horizons!
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我推出了 [**AI Horizon Forecast**](https://aihorizonforecast.substack.com/)**，**
    这是一个专注于时间序列和创新 AI 研究的新闻通讯。订阅 [这里](https://aihorizonforecast.substack.com/) 扩展你的视野！
- en: '*What is Deep GPVAR?*'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*什么是 Deep GPVAR？*'
- en: Deep GPVAR is an autoregressive DL model that leverages low-rank Gaussian Processes
    to model thousands of time-series jointly, by considering their interdependencies.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Deep GPVAR 是一种自回归深度学习模型，利用低秩高斯过程共同建模数千个时间序列，考虑它们之间的相互依赖性。
- en: Let’s briefly review the advantages of using *Deep GPVAR :*
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 简要回顾 *Deep GPVAR* 的优点：
- en: '**Multiple time-series support:** The model uses multiple time-series data
    to learn global characteristics, improving its ability to accurately forecast.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多时间序列支持：** 该模型使用多个时间序列数据来学习全局特征，从而提高其准确预测能力。'
- en: '**Extra covariates:** *Deep GPVAR* allows extra features (covariates).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**额外协变量：** *Deep GPVAR* 允许额外特征（协变量）。'
- en: '**Scalability:** The model leverages *low-rank gaussian distribution* to scale
    training to multiple time series **simultaneously.**'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性：** 该模型利用*低秩高斯分布*将训练扩展到多个时间序列**同时进行**。'
- en: '**Multi-dimensional modeling:** Compared to other global forecasting models,*Deep
    GPVAR* models time series together, rather than individually. This allows for
    improved forecasting by considering their interdependencies.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多维建模：** 与其他全球预测模型相比，*Deep GPVAR*模型将时间序列一起建模，而不是单独建模。这通过考虑它们的相互依赖性来提高预测准确性。'
- en: The last part is what differentiates *Deep GPVAR from* DeepAR. We will discuss
    this more in the next section.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一部分是*Deep GPVAR*与DeepAR的区别所在。我们将在下一节中深入探讨这一点。
- en: Global Forecasting Mechanics
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全球预测机制
- en: A global model trained on multiple time series is not a new concept. But why
    the need for a global model?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个时间序列上训练的全球模型并不是一个新概念。但是为什么需要全球模型？
- en: 'At my previous company, where clients were interested in time-series forecasting
    projects, the main request was something like this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前的公司，客户对时间序列预测项目感兴趣，主要需求如下：
- en: “We have 10,000 time-series, and we would like to create a single model, instead
    of 10,000 individual models.”
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们有10,000个时间序列，我们希望创建一个单一模型，而不是10,000个单独的模型。”
- en: The time series could represent product sales, option prices, atmospheric pollution,
    etc. — it doesn't matter. What’s important here is that a company needs a lot
    of resources to train, evaluate, deploy, and monitor (for **concept drift**) *10,000*
    time series in production.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列可以代表产品销售、期权价格、气候污染等——这并不重要。重要的是，公司需要大量资源来训练、评估、部署和监控（**概念漂移**）*10,000*个生产中的时间序列。
- en: So, that is a good reason. Also, at that time, there was no [*N-BEATS*](https://medium.com/towards-data-science/n-beats-time-series-forecasting-with-neural-basis-expansion-af09ea39f538)
    or [Temporal Fusion Transformer](https://medium.com/towards-data-science/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这是一个很好的理由。此外，那时没有[*N-BEATS*](https://medium.com/towards-data-science/n-beats-time-series-forecasting-with-neural-basis-expansion-af09ea39f538)或[Temporal
    Fusion Transformer](https://medium.com/towards-data-science/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91)。
- en: However, if we are to create a global model, what should it learn? Should the
    model just learn a clever mapping that conditions each time series based on the
    input? But, this assumes that time series are **independent**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们要创建一个全球模型，它应该学习什么？模型是否仅仅学习一个聪明的映射，根据输入条件处理每个时间序列？但这假设时间序列是**独立的**。
- en: '**Or should the model learn global temporal patterns that apply to all time
    series in the dataset?**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型是否应该学习适用于数据集中所有时间序列的全球时间模式？**'
- en: Interdependencies Of Time Series
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列的相互依赖性
- en: '*Deep GPVAR* builds upon DeepAR by seeking a more advanced way to utilize the
    dependencies between multiple time series for improved forecasting.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*Deep GPVAR*在DeepAR的基础上，寻求一种更先进的方法来利用多个时间序列之间的依赖关系，从而提高预测效果。'
- en: For many tasks, this makes sense.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多任务，这种做法是有意义的。
- en: A model that considers time series of a global dataset as independent loses
    the ability to effectively utilize their relationships in applications such as
    finance and retail. For instance, risk-minimizing portfolios require a forecast
    of the covariance of assets, and a probabilistic forecast for different sellers
    must consider competition and cannibalization effects.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一个将全球数据集的时间序列视为独立的模型会失去在金融和零售等应用中有效利用它们关系的能力。例如，风险最小化的投资组合需要预测资产的协方差，而不同卖家的概率预测必须考虑竞争和侵蚀效应。
- en: '**Therefore, a robust global forecasting model cannot assume the underlying
    time series are independent.**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**因此，一个强大的全球预测模型不能假设潜在的时间序列是独立的。**'
- en: '*Deep GPVAR is differentiated from DeepAR* in two things:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*Deep GPVAR与DeepAR的区别*在于两个方面：'
- en: '**High-dimensional estimation:** *Deep GPVAR* models time series together,
    factoring in their relationships. For this purpose, the model estimates their
    **covariance matrix** using a **low-rank Gaussian approximation**.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高维估计：** *Deep GPVAR*将时间序列一起建模，考虑它们的关系。为此，模型使用**低秩高斯近似**来估计它们的**协方差矩阵**。'
- en: '**Scaling:** *Deep GPVAR*does not simply normalize each time series, like its
    predecessor. Instead, the model learns how to scale each time series by transforming
    them first using **Gaussian Copulas.**'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩展性：** *Deep GPVAR*并不像其前身那样仅仅对每个时间序列进行标准化。相反，模型学习如何通过使用**高斯Copulas**首先转换每个时间序列来缩放它们。'
- en: The following sections describe how these two concepts work in detail.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分详细描述了这两个概念的工作原理。
- en: Low-Rank Gaussian Approximation — Introduction
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 低秩高斯近似——简介
- en: As we said earlier, one of the best ways to study the relationships of multiple
    time series is to estimate the covariate matrix.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所说，研究多个时间序列之间关系的最佳方法之一是估计协方差矩阵。
- en: However, scaling this task for thousands of time series is not easily accomplished
    — due to memory and numerical stability limitations. Plus, covariance matrix estimation
    is a time-consuming process — the covariance should be estimated for every time
    window during training.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于内存和数值稳定性限制，将此任务扩展到数千个时间序列并不容易实现。此外，协方差矩阵估计是一个耗时的过程——在训练期间，需要为每个时间窗口估计协方差。
- en: To address this issue, the authors simplify the covariance estimation using
    **low-rank approximation**.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，作者使用**低秩近似**来简化协方差估计。
- en: Let’s start with the basics. Below is the matrix form of a Multivariate normal`N**∼**(μ,Σ)`with
    mean `**μ** **∈** (k,1)`and covariance `**Σ** **∈** (k,k)`
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从基础开始。下面是多变量正态分布`N**∼**(μ,Σ)`的矩阵形式，其中均值`**μ** **∈** (k,1)`，协方差`**Σ** **∈**
    (k,k)`
- en: '![](../Images/c316b88c85d984a5ce6a287c1b09c5d1.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c316b88c85d984a5ce6a287c1b09c5d1.png)'
- en: '**Equation 1:** Multivariate Gaussian distribution in matrix form'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**方程1：** 矩阵形式的多变量高斯分布'
- en: The problem here is the size of the covariance matrix `**Σ**`that is quadratic
    with respect to `N`, the number of time series in the dataset.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题是协方差矩阵`**Σ**`的大小是与数据集中时间序列的数量`N`平方相关的。
- en: We can address this challenge using an approximated form, called the **low-rank
    Gaussian approximation.** This method has its roots in factor analysis and is
    closely related to [SVD (Singular Value Decomposition)](https://medium.com/towards-data-science/how-to-use-singular-value-decomposition-svd-for-image-classification-in-python-20b1b2ac4990).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用一种近似形式来解决这个挑战，称为**低秩高斯近似**。这种方法源于因子分析，并与[SVD（奇异值分解）](https://medium.com/towards-data-science/how-to-use-singular-value-decomposition-svd-for-image-classification-in-python-20b1b2ac4990)密切相关。
- en: 'Instead of computing the full covariance matrix of size `**(**N,N**)**`**,**
    we can approximate by computing instead:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过计算以下内容来近似，而不是计算大小为`**(**N,N**)**`**的完整协方差矩阵：
- en: '![](../Images/33f2c0aeb9ff6e8ae5d9334c88aefe18.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33f2c0aeb9ff6e8ae5d9334c88aefe18.png)'
- en: '**Equation 2:** Low-rank covariance matrix formula'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**方程2：** 低秩协方差矩阵公式'
- en: where `**D** **∈** R(N,N)`is a diagonal matrix and `**V** **∈** R(N,r)`.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`**D** **∈** R(N,N)`是对角矩阵，`**V** **∈** R(N,r)`。
- en: But why do we represent the covariance matrix using the low-rank format? Because
    since`r<<N`, it is proved that the Gaussian likelihood can be computed using `O(Nr²
    + r³)` operations instead of `O(N³)` (the proof can be found in the paper’s Appendix).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么我们使用低秩格式来表示协方差矩阵呢？因为`r<<N`，证明了高斯似然可以使用`O(Nr² + r³)`操作来计算，而不是`O(N³)`（证明可以在论文的附录中找到）。
- en: 'The low-rank normal distribution is part of PyTorch’s **distributions** module.
    Feel free to experiment and see how it works:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩正态分布是PyTorch的**分布**模块的一部分。可以随意尝试，看看它是如何工作的：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Deep GPVAR Architecture
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度GPVAR架构
- en: '**Notation:** From now on, all variables in bold are considered either vectors
    or matrices.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**符号：** 从现在开始，所有粗体变量都被视为向量或矩阵。'
- en: '**Note:** Find a hands-on project for Deep GPVAR in the [AI Projects folder](https://aihorizonforecast.substack.com/p/ai-projects)
    — which is constantly updated with fresh tutorials on the latest time-series models!'
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 在[AI项目文件夹](https://aihorizonforecast.substack.com/p/ai-projects)中找到一个关于Deep
    GPVAR的动手项目，该文件夹会不断更新最新时间序列模型的教程！'
- en: Now that we have seen how low-rank normal approximation works, we delve deeper
    into *Deep GPVAR’s* architecture.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了低秩正态近似的工作原理，接下来我们将深入探讨*Deep GPVAR*的架构。
- en: First, *Deep GPVAR* is similar to DeepAR *—* the model also uses an LSTM network.Let’s
    assumeour dataset contains `N` time series, indexed from `i= [1…N]`
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，*Deep GPVAR*类似于DeepAR——该模型也使用LSTM网络。假设我们的数据集中包含`N`个时间序列，索引从`i= [1…N]`
- en: 'At each time step `t` we have:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步`t`，我们有：
- en: An LSTM cell takes as input the target variable `z_t-1` of the previous time
    step `t-1` **for a subset of time series**. Also, the LSTM receives the hidden
    state `**h_t-1**` of the previous time step.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个LSTM单元将前一时间步`t-1`的目标变量`z_t-1` **用于部分时间序列**作为输入。此外，LSTM接收前一时间步的隐藏状态`**h_t-1**`。
- en: The model uses the LSTM to compute its hidden vector `**h_t**`.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型使用LSTM来计算其隐藏向量`**h_t**`。
- en: The hidden vector `**h_t**` will now be used to compute the`**μ**, **Σ**` parameters
    of a multivariate Gaussian distribution `N∼(μ,Σ)`. This is a special kind of normal
    distribution called **Gaussian copula (**More to that later**).**
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 隐藏向量`**h_t**`将被用来计算多变量高斯分布`N∼(μ,Σ)`的`**μ**`和`**Σ**`参数。这是一种特别的正态分布，称为**高斯 copula（**稍后会详细介绍**）**。
- en: This process is shown in **Figure 1:**
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程如**图 1**所示：
- en: '![](../Images/81fdb9b2b6e49070894141d5b932ef8b.png)![](../Images/8668ca3908cd12d4d4738e286e9ee156.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81fdb9b2b6e49070894141d5b932ef8b.png)![](../Images/8668ca3908cd12d4d4738e286e9ee156.png)'
- en: '**Figure 1:** Two training steps of Deep GPVAR. The covariance matrix Σ is
    expressed as Σ=D+V*V^T ([Source](https://arxiv.org/abs/1910.03002))'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1：**Deep GPVAR 的两个训练步骤。协方差矩阵 Σ 表示为 Σ=D+V*V^T ([来源](https://arxiv.org/abs/1910.03002))'
- en: 'At each time step `t`, *Deep GPVAR* randomly chooses`B << N` time-series to
    implement the low-rank parameterization: On the left, the model chooses from (1,2
    and 4) time series and on the right, the model chooses from (1,3 and 4).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步`t`，*Deep GPVAR* 随机选择`B << N`个时间序列来实现低秩参数化：左侧，模型从（1、2 和 4）时间序列中选择，右侧，模型从（1、3
    和 4）时间序列中选择。
- en: The authors in their experiments have configured `B = 20`. With a dataset containing
    potentially over `N= 1000` time series, the benefit of this approach becomes clear.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在实验中将`B`配置为20。对于可能包含超过`N=1000`个时间序列的数据集，这种方法的好处变得明显。
- en: 'There are 3 parameters that our model estimates:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型估计了3个参数：
- en: The **means** `**μ**` of the normal distribution.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正态分布的**均值**`**μ**`。
- en: The covariance matrix parameters are `d` and `**v**`according to **Equation
    2.**
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协方差矩阵参数是`d`和`**v**`，根据**方程 2**。
- en: 'They are all calculated from the `**h_t**` LSTM hidden vector. **Figure 2**
    shows the low-rank covariance parameters:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 它们都从`**h_t**` LSTM 隐藏向量中计算得出。**图 2**展示了低秩协方差参数：
- en: '![](../Images/a2d204f15dea0ffd4de84b0df258bbb8.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2d204f15dea0ffd4de84b0df258bbb8.png)'
- en: '**Figure 2:** Parameterization of the low-rank covariance matrix, as expressed
    in **Equation 2**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2：**低秩协方差矩阵的参数化，如**方程 2**所示'
- en: 'Careful with notation: `μ_i`, `d_i`, `**v_i**` refer to the`i-th` time series
    in our dataset, where `i ∈ [1..N]`.'
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意符号：`μ_i`、`d_i`、`**v_i**`指的是我们数据集中第`i-th`个时间序列，其中`i ∈ [1..N]`。
- en: 'For each time-series `i`, we create the `**y_i**` vector, which concatenates
    `**h_i**` with `**e_i**` — the `**e_i**` vector contains the features/covariates
    of the `i-th` time series. Hence we have:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个时间序列`i`，我们创建`**y_i**`向量，它将`**h_i**`与`**e_i**`连接在一起——`**e_i**`向量包含第`i-th`时间序列的特征/协变量。因此，我们有：
- en: '![](../Images/58073208219d430b9f51f8f59c8e433a.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58073208219d430b9f51f8f59c8e433a.png)'
- en: '**Figure 3** displays a training snapshot for a time step `t`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 3**展示了时间步`t`的训练快照：'
- en: '![](../Images/f553782e64272b650bb415d98b3e6562.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f553782e64272b650bb415d98b3e6562.png)'
- en: '**Figure 3:** A detailed architecture of low-rank parameterization'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 3：**低秩参数化的详细架构'
- en: 'Notice that:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：
- en: '`μ` and `d` are scalars, while `**v**` is vector. For example, `μ = **w_μ**^T
    * **y**` with dimensions `(1,p)`*`(p,1`), therefore `μ` is a scalar.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`μ`和`d`是标量，而`**v**`是向量。例如，`μ = **w_μ**^T * **y**`，维度为`(1,p)`*`(p,1)`，因此`μ`是标量。'
- en: The same LSTM is used for all time series, including the dense layer projections
    `**w_μ**` ,`**w_d**` ,`**w_u**`.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有时间序列都使用相同的 LSTM，包括密集层投影`**w_μ**`、`**w_d**`和`**w_u**`。
- en: Hence, the neural network parameters `**w_μ**` ,`**w_d**` ,`**w_u**`and`**y**`are
    used to compute the `**μ**` and `**Σ**`parameterswhich are shown in **Figure 3.**
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，神经网络参数`**w_μ**`、`**w_d**`、`**w_u**`和`**y**`被用来计算`**μ**`和`**Σ**`参数，这些参数在**图
    3**中展示。
- en: The Gaussian Copula Function
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯 Copula 函数
- en: 'Question: What does the `**μ**` and `**Σ**`parameterize?'
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问题：`**μ**`和`**Σ**`参数化什么？
- en: They parameterize a special kind of multivariate Gaussian distribution, called
    **Gaussian Copula.**
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 它们参数化了一种特殊的多变量高斯分布，称为**高斯 Copula**。
- en: But why does Deep GPVAR needs a Gaussian Copula?
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 但为什么 Deep GPVAR 需要高斯 Copula？
- en: Remember, *Deep GPVAR* does multi-dimensional forecasting, so we cannot use
    a simple univariate Gaussian, like in DeepAR.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，*Deep GPVAR* 进行多维预测，因此我们不能像在 DeepAR 中那样使用简单的单变量高斯分布。
- en: Ok. So why not use our familiar multivariate Gaussian distribution instead of
    a Copula function — like the one shown in **Equation 1?**
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 好吧。那么为什么不使用我们熟悉的多变量高斯分布，而是使用像**方程 1**中那样的 Copula 函数呢？
- en: '2 reasons:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 2 个原因：
- en: '**1)** **Because a multivariate Gaussian distribution requires gaussian random
    variables as marginals.** We could also use mixtures, but they are too complex
    and not applicable in every situation. Conversely, Gaussian Copulas are easier
    to use and can work with any input distribution — and by input distribution, we
    mean an individual time series from our dataset.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Hence, the copula learns to estimate the underlying data distributions without
    making assumptions about the data.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '**2) The Gaussian Copula can model the dependency structure among these different
    distributions by controlling the parameterization of the covariance matrix** `***Σ***`***.***
    That’s how *Deep GPVAR* learns to consider the interdependencies among input time
    series, something other global forecasting models don’t do.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '**Remember: time series can have unpredictable interdependencies.** For example,
    in retail, we have product cannibalization: a successful product pulls demand
    away from similar items in its category. So, we should also factor in this phenomenon
    when we forecast product sales. With copulas, Deep GPVAR learns those interdependencies
    automatically.'
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What are copulas?
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A copula is a mathematical function that describes the correlation between multiple
    random variables.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** If you are completely unfamiliar with copulas, [this article](https://medium.com/towards-data-science/copulas-an-essential-guide-applications-in-time-series-forecasting-f5c93dcd6e99)
    explains in-depth what copulas are and how we construct them.'
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Copulas are heavily used in quantitative finance for portfolio risk assessment.
    [Their misuse also played a significant role in the 2008 recession.](http://samueldwatts.com/wp-content/uploads/2016/08/Watts-Gaussian-Copula_Financial_Crisis.pdf)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'More formally, a copula function `**C**` is a CDF of `N` random variables,
    where each random variable (marginal) is uniformly distributed:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b6dbbc8c39b1b59ab8718f0a16d712e.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4** belowshows the plot of a bivariate copula, consisting of 2 marginal
    distributions. The copula is defined in the [0–1]² domain (x, y-axis) and outputs
    values in [0–1] (z-axis):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/30a5e70fb487c96888bb47d5d6c3904a.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4:** A Gaussian copula CDF function consisting of 2 beta distributions
    as marginals'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: A popular choice for `**C**`is the Gaussian Copula — the copula of **Figure
    4** is also Gaussian.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: How we construct copulas
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We won’t delve into much detail here, but let’s give a brief overview.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Initially, we have a random vector — a collection of random variables. In our
    case, each random variable `z_i` represents the observation of a time series `i`
    at a time step `t`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/664fdec17126ab7f2f266ca7fcf9a9f6.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: 'Then, we make our variables **uniformly distributed** using the [*probability
    integral transform*](https://medium.com/towards-data-science/copulas-an-essential-guide-applications-in-time-series-forecasting-f5c93dcd6e99)*:*
    The CDF output of any continuous random variable is uniformly distributed,`F(z)=U`
    :'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过使用[*概率积分变换*](https://medium.com/towards-data-science/copulas-an-essential-guide-applications-in-time-series-forecasting-f5c93dcd6e99)来使我们的变量**均匀分布**：任何连续随机变量的累积分布函数（CDF）输出是均匀分布的，`F(z)=U`：
- en: '![](../Images/db326ee56efd02d5ce456390b83aa496.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db326ee56efd02d5ce456390b83aa496.png)'
- en: 'And finally, we apply our Gaussian Copula:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们应用我们的高斯 copula：
- en: '![](../Images/7321552dd36eea752dadccde0a581061.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7321552dd36eea752dadccde0a581061.png)'
- en: where **Φ**^-1 is the inverse standard gaussian CDF `N∼(0,1)`, and **φ** (lowercase
    letter) is a gaussian distribution parameterized with `*μ*` and `*Σ*` *.* Note
    that `Φ^-1[F(z)] = x`, where `x~(-Inf, Inf)` because we use the standard inverse
    CDF.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 其中**Φ**^-1是标准高斯 CDF `N∼(0,1)`的逆函数，**φ**（小写字母）是一个用`*μ*`和`*Σ*`参数化的高斯分布*。注意`Φ^-1[F(z)]
    = x`，其中`x~(-Inf, Inf)`，因为我们使用的是标准逆 CDF。
- en: '**So, what happens here?**'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**那么，这里发生了什么？**'
- en: 'We can take any continuous random variable, marginalize it as uniform, and
    then transform it into a gaussian. The chain of operations is the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以取任何连续随机变量，将其边际化为均匀分布，然后将其转化为高斯分布。操作链如下：
- en: '![](../Images/8188b43a463a65d9f50c0fb1093e1f8c.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8188b43a463a65d9f50c0fb1093e1f8c.png)'
- en: 'There are 2 transformations here:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有 2 种变换：
- en: '`F(z) = U`, known as *probability integral transform*. Simply put, this transformation
    converts any continuous random variable to uniform.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`F(z) = U`，被称为*概率积分变换*。简单来说，这种变换将任何连续随机变量转换为均匀分布。'
- en: '`Φ^-1(U)=x`, known as [*inverse sampling*](https://en.wikipedia.org/wiki/Inverse_transform_sampling).
    Simply put, this transformation converts any uniform random variable to the distribution
    of our choice — here `Φ` is gaussian, so `x` also becomes a gaussian random variable.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Φ^-1(U)=x`，被称为[*逆采样*](https://en.wikipedia.org/wiki/Inverse_transform_sampling)。简单来说，这种变换将任何均匀随机变量转换为我们选择的分布——在这里，`Φ`
    是高斯分布，因此 `x` 也成为高斯随机变量。'
- en: In our case, `**z**`are the past observations of a time series in our dataset.
    Because our model makes no assumptions about how the past observations are distributed,
    we use the **empirical CDF —** a special function that calculates the CDF of any
    distribution non-parametrically (empirically).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，`**z**`是我们数据集中时间序列的过去观察值。由于我们的模型对过去观察值的分布没有假设，我们使用**经验 CDF**——一种非参数（经验）计算任何分布
    CDF 的特殊函数。
- en: '**Note:** If you are not familiar with the **empirical CDF**, refer to my [article](https://medium.com/towards-data-science/copulas-an-essential-guide-applications-in-time-series-forecasting-f5c93dcd6e99)
    for a detailed explanation.'
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 如果你不熟悉**经验 CDF**，请参考我的[文章](https://medium.com/towards-data-science/copulas-an-essential-guide-applications-in-time-series-forecasting-f5c93dcd6e99)以获取详细解释。'
- en: In other words, at the `F(z) = U` transformation, `F` is the empirical CDF and
    not the actual gaussian CDF of the variable `z` . The authors use `m=100` past
    observations throughout their experiments to calculate the empirical CDF.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，在`F(z) = U`变换中，`F`是经验 CDF，而不是变量`z`的实际高斯 CDF。作者在实验中使用了`m=100`个过去的观察值来计算经验
    CDF。
- en: '**Recap of copulas**'
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**回顾 copulas**'
- en: To sum up, the Gaussian copula is a multivariate function that uses `μ` and
    `Σ` to directly paremeterize the correlation of two or more random variables.
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 总结来说，高斯 copula 是一个多变量函数，使用`μ`和`Σ`直接参数化两个或更多随机变量的相关性。
- en: But how a Gaussian copula differs from a Gaussian multivariate probability distribution(PDF)?
    Besides, a Gaussian Copula is just a multivariate CDF.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 但高斯 copula 如何与高斯多变量概率分布（PDF）不同？此外，高斯 copula 只是一个多变量 CDF。
- en: The Gaussian Copula can use any random variable as a marginal, not just a Gaussian.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高斯 copula 可以使用任何随机变量作为边际分布，而不仅仅是高斯分布。
- en: The original distribution of the data does not matter — using the probability
    integral transform and the empirical CDF, we can transform the original data to
    gaussian, **no matter how they are distributed**.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据的原始分布无关紧要——通过使用概率积分变换和经验 CDF，我们可以将原始数据转化为高斯分布，**无论它们如何分布**。
- en: Gaussian Copulas in Deep GPVAR
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度 GPVAR 中的高斯 copula
- en: Now that we have seen how copulas work, it’s time to see how *Deep GPVAR* uses
    them.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 copula 的工作原理，是时候看看*Deep GPVAR*如何使用它们了。
- en: 'Let’s go back to **Figure 3\.** Using the LSTM, we have computed the `*μ*`
    and `*Σ*`parameters. What we do is the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 回到**图 3**。使用 LSTM，我们已经计算了`*μ*`和`*Σ*`参数。我们所做的是以下操作：
- en: '**Step 1: Transform our observations to Gaussian**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1：将我们的观察值转换为高斯分布**'
- en: 'Using the copula function, we transform our observed time-series datapoints
    `**z**` to gaussian `**x**` using the copula function. The transformation is expressed
    as:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 copula 函数，我们将观察到的时间序列数据点 `**z**` 转换为高斯 `**x**`。转换公式为：
- en: '![](../Images/da801c95a5e88dd990a01bb6453e9a13.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da801c95a5e88dd990a01bb6453e9a13.png)'
- en: where `f(z_i,t)` is actually the marginal transformation `Φ^-1(F(z_i))` of the
    time-series `i`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `f(z_i,t)` 实际上是时间序列 `i` 的边际转换 `Φ^-1(F(z_i))`。
- en: In practice, our model makes no assumptions about how our past observations
    `z` are distributed. Therefore, no matter how the original observations are distributed,
    our model can effectively learn their behavior, including their correlation —
    all these thanks to the power of Gaussian Copulas.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，我们的模型对过去观察值 `z` 的分布没有假设。因此，无论原始观察值的分布如何，我们的模型都可以有效地学习它们的行为，包括它们的相关性——这一切都归功于高斯
    copula 的强大功能。
- en: '**Step 2: Use the computed parameters for the Gaussian.**'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2：使用计算出的高斯参数。**'
- en: I mentioned that we should transform our observations to Gaussian, but what
    are the parameters of the Gaussian? In other words, when I said that `f(z_i) =
    Φ^-1(F(z_i))`, what are the parameters of `Φ` ?
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我提到我们应该将观察值转换为高斯分布，但高斯分布的参数是什么？换句话说，当我说 `f(z_i) = Φ^-1(F(z_i))` 时，`Φ` 的参数是什么？
- en: The answer is the `*μ*` and `*Σ*` parameters — these are calculated from the
    dense layers and the LSTM shown in **Figure 3.**
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是 `*μ*` 和 `*Σ*` 参数——这些参数是从**图 3**中所示的密集层和 LSTM 计算得到的。
- en: '**Step 3: Calculate the loss and update our network parameters**'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3：计算损失并更新我们的网络参数**'
- en: 'To recap, we transformed our observations to Gaussian and we assume those observations
    are parameterized by a low-rank normal Gaussian. Thus, we have:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们将观察值转换为高斯分布，并假设这些观察值由低秩正态高斯分布参数化。因此，我们有：
- en: '![](../Images/453c8a175f78d70f9cd5fa72ec5c354a.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/453c8a175f78d70f9cd5fa72ec5c354a.png)'
- en: where `f1(z1)` is the transformed observed prediction for the first time series,
    `f2(z2)` refers to the second one and `f_n(z_n)` refers to the N-th time series
    of our dataset.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `f1(z1)` 是第一个时间序列的转换后的预测，`f2(z2)` 指的是第二个时间序列，而 `f_n(z_n)` 指的是我们数据集中的第 N 个时间序列。
- en: 'Finally, we train our model by maximizing the **multivariate** **gaussian log-likelihood
    function.** The paper uses the convention of minimizing the loss function **—**
    the gaussian log-likelihood preceded with a minus:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过最大化**多元** **高斯对数似然函数**来训练我们的模型。论文采用了最小化损失函数**—**即前面带有负号的高斯对数似然。
- en: '![](../Images/a125c011c6b9b865364db9162d3bf096.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a125c011c6b9b865364db9162d3bf096.png)'
- en: Using the gaussian log-likelihood loss, *Deep GPVAR* updates its LSTM and the
    shared dense layer weights displayed in **Figure 3.**
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 使用高斯对数似然损失，*Deep GPVAR* 更新其 LSTM 和**图 3**中显示的共享密集层权重。
- en: 'Also, notice:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请注意：
- en: The `**z**`is not a single observation, but the vector of observations from
    all `N` time-series at time `t`. The summation loops until `T`, the maximum lookup
    window — upon which the gaussian log-likelihood is evaluated.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**z**` 不是单个观察值，而是所有 `N` 个时间序列在时间 `t` 的观察向量。求和会循环到 `T`，即最大查找窗口——在此之后计算高斯对数似然。'
- en: And since **z** is a vector of observations, the gaussian log-likelihood is
    actually **multivariate**. In contrast, DeepAR uses a univariate gaussian likelihood.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 **z** 是观察向量，高斯对数似然实际上是**多元**的。相比之下，DeepAR 使用的是单变量高斯似然。
- en: '**Deep GPVAR: The Big Picture**'
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**Deep GPVAR: 大致概况**'
- en: I recommend going over the article several times to grasp how the model works.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议多次阅读文章，以掌握模型的工作原理。
- en: Generally speaking, you can view Deep GPVAR as a Gaussian stochastic process.
  id: totrans-156
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一般来说，你可以将 Deep GPVAR 视为一个高斯随机过程。
- en: For those unfamiliar with **stochastic processes**, you can think of a stochastic
    process as a collection of random variables — each variable represents the outcome
    of a random event at a given time.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些不熟悉**随机过程**的人，你可以将随机过程视为一组随机变量——每个变量代表某一时刻随机事件的结果。
- en: The random variables are indexed by some set, usually time, and the values of
    the random variables depend on each other. Hence the outcome of one event can
    influence the outcome of future events. This interdependence also explains the
    temporal nature of a stochastic process.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量由某个集合（通常是时间）索引，这些随机变量的值彼此依赖。因此，一个事件的结果可以影响未来事件的结果。这种相互依赖性也解释了随机过程的时间性。
- en: 'In our case, each datapoint `y_i` of a time-series `i` can be viewed as a random
    variable. Hence, *Deep GPVAR* can be considered a Gaussian process `GP`, evaluated
    at every data point `y` as:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，时间序列`y_i`的每个数据点可以视为一个随机变量。因此，*Deep GPVAR*可以被视为一个高斯过程`GP`，在每个数据点`y`上评估如下：
- en: '![](../Images/7c8d35297e83da00fabfd26cde061e2c.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c8d35297e83da00fabfd26cde061e2c.png)'
- en: Now, the parameters of a gaussian process are functions, not variables. In the
    equation above, the `**μ**`, `**d**`, `**v**` **(with tilde)** are time functions,
    indexed by `**y**` — where `**y**` is an observation of vectors at a certain time
    step.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，高斯过程的参数是函数，而不是变量。在上述方程中，`**μ**`、`**d**`、`**v**` **（带有波浪线）** 是时间函数，由`**y**`索引——其中`**y**`是在某个时间步的向量观察值。
- en: At each time step, the functions `**μ**`, `**d**`, `**v**` **(with tilde)**
    which essentially are the LSTM and the Dense layers have a realization `**μ**`,
    `**d**`, `**v**` **(without tilde) .** This realization parameterizes the `*μ*`
    and `*Σ*` of the copula function at a time step `*t*`. Hence during training,
    at each time-step, `*μ*` and `*Σ*` change, such that they optimally explain our
    observations.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步，函数`**μ**`、`**d**`、`**v**` **（带有波浪线）** 本质上是LSTM和Dense层，有一个实现 `**μ**`、`**d**`、`**v**`
    **（不带波浪线）**。这个实现参数化了时间步`*t*`下copula函数的`*μ*`和`*Σ*`。因此，在训练过程中，在每个时间步，`*μ*`和`*Σ*`会变化，以最佳地解释我们的观察结果。
- en: Consequently, the notion that *Deep GPVAR* works as a Gaussian stochastic process
    is entirely justified.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*Deep GPVAR*作为高斯随机过程的概念是完全合理的。
- en: Deep GPVAR Variants
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Deep GPVAR变体
- en: From the current paper, Amazon created 2 models, **Deep GPVAR** (which we describe
    in this article) and **DeepVAR**.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 根据当前论文，亚马逊创建了2个模型，**Deep GPVAR**（我们在本文中描述）和**DeepVAR**。
- en: DeepVARis similar to *Deep GPVAR.* The difference is that DeepVAR uses a global
    multivariate LSTM that receives and predicts all time series at once. On the other
    hand, *Deep GPVAR* unrolls the LSTM on each time series separately.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: DeepVAR类似于*Deep GPVAR*。不同之处在于DeepVAR使用一个全球多变量LSTM，一次接收和预测所有时间序列。另一方面，*Deep GPVAR*在每个时间序列上分别展开LSTM。
- en: In their experiments, the authors refer to the DeepVAR as **Vec-LSTM** and *Deep
    GPVAR as* **GP***.*
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的实验中，作者将DeepVAR称为**Vec-LSTM**，而*Deep GPVAR*称为**GP***。
- en: The *Vec-LSTM*and*GP*terms are mentioned in **Table 1** of the original paper.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Vec-LSTM*和*GP*项在原始论文的**表1**中提到。'
- en: The *Deep GPVAR* and *DeepVAR* terms are mentioned in Amazon’s Forecasting library
    [Gluon TS](https://ts.gluon.ai/stable/getting_started/models.html).
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Deep GPVAR*和*DeepVAR*术语在亚马逊的预测库[Gloun TS](https://ts.gluon.ai/stable/getting_started/models.html)中提到。'
- en: This article describes the ***Deep GPVAR* variant**, which is better on average
    and has fewer parameters than *DeepVAR.* Feel free to read the original paper
    and learn more about the experimental process.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 本文描述了***Deep GPVAR*变体**，该变体在平均情况下表现更好，并且比*DeepVAR*具有更少的参数。可以阅读原始论文以了解更多实验过程。
- en: Project Tutorial — Demand Energy Forecasting
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目教程 — 需求能源预测
- en: 'This tutorial uses the **ElectricityLoadDiagrams20112014** [4]dataset from
    UCI. The notebook for this example can be downloaded [here](https://aihorizonforecast.substack.com/p/ai-projects):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程使用了来自UCI的**ElectricityLoadDiagrams20112014** [4]数据集。该示例的笔记本可以从[这里](https://aihorizonforecast.substack.com/p/ai-projects)下载：
- en: '**Note :** Currently, the PyTorch Forecasting library provides the DeepVAR
    version. That’s the variant we show in this tutorial. You can also try all DeepAR
    variants, including GPVAR from Amazon’s open-souce [Gluon TS library](https://github.com/awslabs/gluonts/tree/dev/src/gluonts/mx/model).'
  id: totrans-173
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 目前，PyTorch Forecasting库提供了DeepVAR版本。这就是我们在本教程中展示的变体。你还可以尝试所有DeepAR变体，包括亚马逊开源的[Gluon
    TS库](https://github.com/awslabs/gluonts/tree/dev/src/gluonts/mx/model)中的GPVAR。'
- en: Download Data
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载数据
- en: '[PRE1]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Data Preprocessing
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: '[PRE2]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/8d4ffbf8701559ff5f1bd0f8ce6eb654.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d4ffbf8701559ff5f1bd0f8ce6eb654.png)'
- en: Each column represents a consumer. The values in each cell are the electrical
    power usages in each quarter-of-an-hour.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 每一列代表一个消费者。每个单元格中的值是每15分钟的电力使用量。
- en: Also, we aggregate into hourly data. Due to the model’s size and complexity,
    we train our model using 5 consumers only (considering only non-zero values).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将数据汇总为每小时数据。由于模型的大小和复杂性，我们仅使用5个消费者进行模型训练（仅考虑非零值）。
- en: '[PRE3]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, we convert our dataset to a special format that PyTorch Forecasting understands
    — called ***TimeSeriesDataset****.* This format is handy because it lets us specify
    the nature of our features (e.g. if they are time-varying, or static).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将数据集转换为PyTorch Forecasting理解的特殊格式——称为***TimeSeriesDataset***。这种格式非常方便，因为它允许我们指定特征的性质（例如，它们是时间变化的还是静态的）。
- en: '**Note:** If you want to learn more about the ***TimeSeriesDataset*** format,
    check my [Temporal Fusion Transformer article](https://medium.com/towards-data-science/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91)
    that explains in detail how this format works.'
  id: totrans-183
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 如果你想了解更多关于***TimeSeriesDataset***格式的信息，请查看我的[Temporal Fusion Transformer文章](https://medium.com/towards-data-science/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91)，该文章详细解释了该格式的工作原理。'
- en: To modify our dataset for the TimeSeriesDataset format, we stack all time series
    vertically instead of horizontally. In other words, we convert our dataframe from
    “wide” to “long” format.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将数据集修改为TimeSeriesDataset格式，我们将所有时间序列垂直堆叠，而不是水平堆叠。换句话说，我们将数据框从“宽”格式转换为“长”格式。
- en: Also, we create new features from the date column such `day` and `month` -those
    will help our model capture the seasonality dynamics better
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们从日期列创建新的特征，如`day`和`month`—这些特征将帮助我们的模型更好地捕捉季节性动态。
- en: '[PRE4]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The final preprocessed dataframe is called `time_df`. Let’s print its contents:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最终预处理的数据框称为`time_df`。让我们打印其内容：
- en: '![](../Images/ce0186cef0e4fa8d3a621ab395d58320.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce0186cef0e4fa8d3a621ab395d58320.png)'
- en: The `time_df` is now in the proper format for the *TimeSeriesDataset*. Moreover,
    the *TimeSeriesDataset* format requires a **time index** for our data. In this
    case, we use the `hours_from_start` variable. Also, the `power_usage` is the **target
    variable** our model will try to predict.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`time_df`现在已转换为*TimeSeriesDataset*的正确格式。此外，*TimeSeriesDataset*格式要求我们的数据具有**时间索引**。在这种情况下，我们使用`hours_from_start`变量。此外，`power_usage`是我们模型将尝试预测的**目标变量**。'
- en: Create DataLoaders
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建数据加载器
- en: 'In this step, we pass our `time_df` to the *TimeSeriesDataSet* format. We do
    this because:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，我们将`time_df`转换为*TimeSeriesDataSet*格式。我们这样做的原因是：
- en: It spares us from writing our own Dataloader.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这使我们免去了编写自己的数据加载器的麻烦。
- en: We can specify how our model will handle the features.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以指定模型如何处理特征。
- en: We can normalize our dataset with ease. In our case, normalization is mandatory
    because all time sequences differ in magnitude. Thus, we use the **GroupNormalizer**
    to normalize each time series individually.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以轻松地对数据集进行归一化。在我们的例子中，归一化是强制性的，因为所有时间序列在幅度上有所不同。因此，我们使用**GroupNormalizer**来单独归一化每个时间序列。
- en: Our model uses a lookback window of one week (7*24) to predict the power usage
    of the next 24 hours.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型使用一周（7*24）的回溯窗口来预测接下来的24小时的功率使用。
- en: 'Also, notice that the `hours_from_start` is both the time index and a time-varying
    feature. For the sake of demonstration, our validation set is the last day:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请注意，`hours_from_start`既是时间索引，又是时间变化特征。为了演示，我们的验证集是最后一天：
- en: '[PRE5]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Baseline Model
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基线模型
- en: Also, remember to create a baseline model.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，记得创建一个基线模型。
- en: 'We create a naive baseline that predicts the power usage curve of the previous
    day:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个朴素的基线模型，用于预测前一天的功率使用曲线：
- en: '[PRE6]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Building and Training our Model
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建和训练我们的模型
- en: We are now ready to train our model. We will use the *Trainer* interface from
    the PyTorch Lightning library.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始训练我们的模型了。我们将使用来自PyTorch Lightning库的*Trainer*接口。
- en: 'First, we have to configure hyperparameters and callbacks:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要配置超参数和回调函数：
- en: The **EarlyStopping** callback to monitor the validation loss.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**EarlyStopping**回调函数来监控验证损失。
- en: '**Tensorboard** to log our training and validation metrics.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tensorboard**用于记录我们的训练和验证指标。'
- en: The `hidden_size` and `rnn_layers` refer to the number of LSTN cells and the
    number of LSTM layers respectively.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`和`rnn_layers`分别指代LSTN单元的数量和LSTM层的数量。'
- en: We also use `gradient_clip_val` (gradient clipping) to prevent overfitting.
    Also, the model has a default dropout of `0.1` — we leave the default value.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还使用`gradient_clip_val`（梯度裁剪）来防止过拟合。此外，模型的默认dropout为`0.1`——我们保留默认值。
- en: 'Our loss function is the`**MultivariateNormalDistributionLoss**(rank : int)`
    . The function takes the **rank** parameter as an argument — this is the `R` value
    we explained in the beginning.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的损失函数是`**MultivariateNormalDistributionLoss**(rank : int)`。该函数将**rank**参数作为一个参数——这是我们在开始时解释的`R`值。'
- en: 'Remember, the lower the value, the biggest the speedup during training. The
    authors in their experiments use **rank**=10 — we do the same here:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，值越低，训练期间的加速效果越大。作者在他们的实验中使用**rank**=10——我们在这里也做了相同的处理：
- en: '[PRE7]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: After 6 epochs, EarlyStopping kicks in the training finishes.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在6个周期后，EarlyStopping触发，训练结束。
- en: '**Note:** If you want to run DeepAR instead of DeepVAR, use the [NormalDistributionLoss](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.metrics.distributions.NormalDistributionLoss.html)
    instead.'
  id: totrans-213
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 如果你想运行DeepAR而不是DeepVAR，请使用[NormalDistributionLoss](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.metrics.distributions.NormalDistributionLoss.html)。'
- en: Load and Save the Best Model
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载和保存最佳模型
- en: '[PRE8]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Don’t forget to download your model:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记下载你的模型：
- en: '[PRE9]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This is how you load the model again — you only have to remember the best model
    path:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是如何重新加载模型——你只需要记住最佳模型路径：
- en: '[PRE10]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Tensorboard Logs
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Tensorboard日志
- en: 'Take a closer look at training and validation curves with Tensorboard. You
    can start it by executing:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Tensorboard仔细查看训练和验证曲线。你可以通过执行以下命令启动它：
- en: '[PRE11]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Model Evaluation
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估
- en: 'Get predictions on the validation set and calculate the average **P50** (quantile
    median) **loss**:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 获取验证集上的预测并计算平均**P50**（分位数中位数）**损失**：
- en: '[PRE12]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Notice that we got a slightly worse score compared to the [TFT implementation](https://medium.com/towards-data-science/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91).
    The loss function we used is probabilistic — thus, each time you will get a slightly
    different score.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们获得的分数比[TFT实现](https://medium.com/towards-data-science/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91)略差。我们使用的损失函数是概率性的——因此，每次得到的分数会略有不同。
- en: The last two time series have a slightly higher loss because their relative
    magnitude is high compared to the others.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两个时间序列的损失略高，因为它们的相对幅度较大。
- en: Plot Predictions on Validation Data
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制验证数据上的预测
- en: '[PRE13]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/06810a5dfe9056de790441ad79a59212.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06810a5dfe9056de790441ad79a59212.png)'
- en: '**Figure 5:** Predictions on validation data for MT_002'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5：** MT_002的验证数据预测'
- en: '![](../Images/8221abe2eee378543f06a92b5646d8a0.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8221abe2eee378543f06a92b5646d8a0.png)'
- en: '**Figure 6:** Predictions on validation data for MT_004'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**图6：** MT_004的验证数据预测'
- en: '![](../Images/296881231e8ac8fec22168bf2b83e217.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/296881231e8ac8fec22168bf2b83e217.png)'
- en: '**Figure 7:** Predictions on validation data for MT_005'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**图7：** MT_005的验证数据预测'
- en: '![](../Images/0613e83a4a18e6e829ee64dff68c5f59.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0613e83a4a18e6e829ee64dff68c5f59.png)'
- en: '**Figure 8:** Predictions on validation data for MT_006'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**图8：** MT_006的验证数据预测'
- en: '![](../Images/c8b86c21ff8afddd84a8e0f09c9bf96b.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8b86c21ff8afddd84a8e0f09c9bf96b.png)'
- en: '**Figure 9:** Predictions on validation data for MT_008'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**图9：** MT_008的验证数据预测'
- en: The predictions on the validation set are quite impressive.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集上的预测相当令人印象深刻。
- en: Also, notice that we did not perform any hyperparameter tuning, meaning we could
    get even better results.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，请注意我们没有进行任何超参数调整，这意味着我们可以获得更好的结果。
- en: Plot Predictions For A Specific Time Series
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制特定时间序列的预测
- en: 'Previously, we plotted predictions on the validation data using the `idx` argument,
    which iterates over all time series in our dataset. We can be more specific and
    output predictions of a specific time series:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们使用`idx`参数绘制了验证数据上的预测，该参数遍历了数据集中的所有时间序列。我们可以更具体地输出特定时间序列的预测：
- en: '[PRE14]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/267d7a183f429edd937291ab15e065d4.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/267d7a183f429edd937291ab15e065d4.png)'
- en: '**Figure 10:** Day ahead predictions for MT_004 on the training set'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '**图10：** MT_004在训练集上的第二天预测'
- en: In **Figure 10,** we plot the day-ahead of the **MT_004** consumer for time
    index=26512.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在**图10**中，我们绘制了时间索引=26512的**MT_004**消费者的第二天预测。
- en: Remember, our time-indexing column `hours_from_start` starts from 26304 and
    we can get predictions from 26388 onwards (because we set earlier `min_encoder_length=max_encoder_length
    // 2` which equals `26304 + 168//2=26388`
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们的时间索引列`hours_from_start`从26304开始，我们可以从26388开始获取预测（因为我们之前设置了`min_encoder_length=max_encoder_length
    // 2`，即`26304 + 168//2=26388`）。
- en: Plotting the Covariance Matrix
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制协方差矩阵
- en: The biggest advantage of *Deep (GP)VAR* is the estimation of the covariance
    matrix — which provides some insight about the interdepencies of the time series
    in our dataset.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '*Deep (GP)VAR*最大的优势在于协方差矩阵的估计——这提供了对数据集中时间序列相互依赖关系的一些洞察。'
- en: 'In our case, it makes no sense to do this calculation because we only have
    5 time series in our dataset. Nevertheless, here is the code that shows how to
    do it if you have a dataset with many time series:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，进行此计算没有意义，因为我们的数据集中只有5个时间序列。然而，以下是展示如何在有多个时间序列的数据集上进行计算的代码：
- en: '[PRE15]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Finally, the PyTorch Forecasting library provides additional features such as
    **out-of-sample forecasts and hyperparameter tuning with Optuna**. You can learn
    more about this in the [TFT tutorial](/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91),
    where they are explained in-depth.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，PyTorch Forecasting库提供了额外的功能，如**样本外预测和Optuna的超参数调优**。你可以在[TFT教程](/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91)中深入了解这些内容。
- en: Closing Remarks
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语
- en: '*Deep GPVAR* and its variants are a powerful family of TS forecasting, bearing
    the expertise of Amazon’s research.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '*Deep GPVAR*及其变体是一种强大的时间序列预测家族，具备了亚马逊研究的专业知识。'
- en: The model addresses multivariate forecasting by considering the interdependencies
    among thousands of time series.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型通过考虑数千个时间序列之间的相互依赖性来处理多变量预测。
- en: Also, if you want to learn more about the initial architecture of [DeepAR](https://medium.com/p/bc717771ce85),
    feel free to check this companion article.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你想了解更多关于[DeepAR](https://medium.com/p/bc717771ce85)初始架构的信息，欢迎查阅这篇相关文章。
- en: Thank you for reading!
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: Follow me on [Linkedin](https://www.linkedin.com/in/nikos-kafritsas-b3699180/)!
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[Linkedin](https://www.linkedin.com/in/nikos-kafritsas-b3699180/)上关注我吧！
- en: Subscribe to my [newsletter](https://aihorizonforecast.substack.com/welcome),
    AI Horizon Forecast!
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 订阅我的[新闻通讯](https://aihorizonforecast.substack.com/welcome)，AI Horizon Forecast！
- en: '[](https://aihorizonforecast.substack.com/p/autogluon-timeseries-creating-powerful?source=post_page-----e39204d90af3--------------------------------)
    [## AutoGluon-TimeSeries : Creating Powerful Ensemble Forecasts - Complete Tutorial'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[## AutoGluon-TimeSeries : 创建强大的集成预测 - 完整教程](https://aihorizonforecast.substack.com/p/autogluon-timeseries-creating-powerful?source=post_page-----e39204d90af3--------------------------------)'
- en: Amazon's framework for time-series forecasting has it all.
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 亚马逊的时间序列预测框架应有尽有。
- en: aihorizonforecast.substack.com](https://aihorizonforecast.substack.com/p/autogluon-timeseries-creating-powerful?source=post_page-----e39204d90af3--------------------------------)
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[aihorizonforecast.substack.com](https://aihorizonforecast.substack.com/p/autogluon-timeseries-creating-powerful?source=post_page-----e39204d90af3--------------------------------)'
- en: References
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Created with Stable Diffusion, CreativeML Open RAIL-M license. Text prompt:
    “a nebula traveling through space, digital art, illustration”, to rg'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 使用Stable Diffusion创建，CreativeML Open RAIL-M许可证。文本提示：“在太空中旅行的星云，数字艺术，插图”，到rg'
- en: '[2] D. Salinas et al. [*DeepAR: Probabilistic forecasting with autoregressive
    recurrent networks*](https://arxiv.org/pdf/1704.04110.pdf), International Journal
    of Forecasting (2019)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] D. Salinas等人，[*DeepAR: Probabilistic forecasting with autoregressive recurrent
    networks*](https://arxiv.org/pdf/1704.04110.pdf)，《国际预测期刊》（2019）'
- en: '[3] D. Salinas et al. [High-Dimensional Multivariate Forecasting with Low-Rank
    Gaussian Copula Processes](https://arxiv.org/abs/1910.03002)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] D. Salinas等人，[高维多变量预测与低秩高斯Copula过程](https://arxiv.org/abs/1910.03002)'
- en: '[4] [ElectricityLoadDiagrams20112014](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014)
    dataset by UCI, CC BY 4.0.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [ElectricityLoadDiagrams20112014](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014)
    数据集，由UCI提供，CC BY 4.0。'
- en: '*All images are created by the author, unless stated otherwise*'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有图片均由作者创建，除非另有说明*'
