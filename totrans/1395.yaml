- en: Large Language Models as Zero-shot Labelers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/large-language-models-as-zero-shot-labelers-d26aa2642c88](https://towardsdatascience.com/large-language-models-as-zero-shot-labelers-d26aa2642c88)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Using LLMs to obtain labels for supervised models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@devins?source=post_page-----d26aa2642c88--------------------------------)[![Devin
    Soni](../Images/ad0e4be26a861f2584abc643daf52203.png)](https://medium.com/@devins?source=post_page-----d26aa2642c88--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d26aa2642c88--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d26aa2642c88--------------------------------)
    [Devin Soni](https://medium.com/@devins?source=post_page-----d26aa2642c88--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d26aa2642c88--------------------------------)
    ·5 min read·Mar 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4517fd2ff5ae2bea5a595bff399475b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [h heyerlein](https://unsplash.com/es/@heyerlein?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Labeling data is a critical step in building supervised machine learning models,
    as the quantity and quality of labels is often the main factor that determines
    model performance.
  prefs: []
  type: TYPE_NORMAL
- en: However, labeling data can be very time-consuming and expensive, especially
    for complex tasks that involve domain knowledge or reading large amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, large language models (LLMs) have emerged as a powerful solution
    for obtaining labels on text data. Through zero-shot learning, we can obtain labels
    on unlabeled data using only the output of the LLM, rather than having to ask
    a human to obtain the labels. This can significantly lower the cost of obtaining
    labels, and makes the process far more salable.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will further explore the concept of zero-shot learning and
    how LLMs can be used for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: What is Zero-Shot Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/e6b0b7b78d31241482486c84fdd7e723.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Alex Knight](https://unsplash.com/@agk42?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot learning (ZSL) is a a problem setup in machine learning in which the
    model is asked to solve a prediction task that it was not trained on. This often
    involves recognizing or classifying data into concepts it had not explicitly seen
    during training.
  prefs: []
  type: TYPE_NORMAL
- en: In traditional supervised learning, this is not possible, as the model can only
    output predictions for tasks it was trained on (i.e. had labels for). However,
    in the ZSL paradigm, models can generalize to an arbitrary unseen task, and perform
    at a reasonable level. Note that in most cases, a supervised model trained on
    a given task will still outperform a model using ZSL, so ZSL is more often used
    before supervised labels are readily available.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most promising applications of ZSL is in data labeling, where it
    can significantly reduce the cost of obtaining labels. If a model is able to automatically
    classify data into categories without having been trained on that task, it can
    be used to generate labels for a downstream supervised model. These labels can
    be used to bootstrap a supervised model, in a paradigm similar to active learning
    or human-in-the-loop machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Shot Learning with Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/a125bdc1b8fe07f54ee9edc4871db98a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Arseny Togulev](https://unsplash.com/@tetrakiss?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: LLMs like GPT-3 are powerful tools for ZSL because their robust pre-training
    process allows them to have a holistic understanding of natural language that
    is not based on a certain supervised task’s labels.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The contextual embeddings of a LLM are able to capture the semantic concepts
    in a given piece of text, which make them very useful for ZSL.
  prefs: []
  type: TYPE_NORMAL
- en: Libraries such as [sentence-transformers](https://github.com/UKPLab/sentence-transformers)
    offer LLMs that have been trained in such a way that semantically similar pieces
    of text will have embeddings that have a small distance from each other.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/UKPLab/sentence-transformers?source=post_page-----d26aa2642c88--------------------------------)
    [## GitHub - UKPLab/sentence-transformers: Multilingual Sentence & Image Embeddings
    with BERT'
  prefs: []
  type: TYPE_NORMAL
- en: This framework provides an easy method to compute dense vector representations
    for sentences, paragraphs, and images…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/UKPLab/sentence-transformers?source=post_page-----d26aa2642c88--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: If we have the embeddings for a few labeled pieces of data, we can use a nearest-neighbor
    search to find pieces of unlabeled data with similar embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: If two pieces of text are very close to each other in the embedding space, then
    they likely have the same label.
  prefs: []
  type: TYPE_NORMAL
- en: In-context learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[In-context learning](http://ai.stanford.edu/blog/understanding-incontext/)
    is an emergent ability of LLMs that allows them to learn to solve new tasks simply
    by seeing input-output pairs. No parameter updates of the model are needed for
    it to be able to learn arbitrary new tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](http://ai.stanford.edu/blog/understanding-incontext/?source=post_page-----d26aa2642c88--------------------------------)
    [## How does in-context learning work? A framework for understanding the differences
    from traditional…'
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we provide a Bayesian inference framework for in-context learning
    in large language models like GPT-3 and…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ai.stanford.edu](http://ai.stanford.edu/blog/understanding-incontext/?source=post_page-----d26aa2642c88--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: We can use this ability to obtain labels by simply providing a few input-output
    pairs for our downstream task, and allow the model to provide labels for unlabeled
    data points.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of ZSL, this means that we can provide a few handcrafted examples
    of text with their associated supervised labels, and have the model learn the
    labeling function on-the-fly.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad50a16d50b0f93dee71ca00397fbfb1.png)'
  prefs: []
  type: TYPE_IMG
- en: In this trivial case, we train ChatGPT to classify whether or not a sentence
    is about frogs via in-context learning.
  prefs: []
  type: TYPE_NORMAL
- en: Generative models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent advances in alignment methods such as [RLHF (Reinforcement Learning from
    Human Feedback)](https://huggingface.co/blog/rlhf) in generative LLMs have made
    it possible to simply ask the model to label data for you.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://huggingface.co/blog/rlhf?source=post_page-----d26aa2642c88--------------------------------)
    [## Illustrating Reinforcement Learning from Human Feedback (RLHF)'
  prefs: []
  type: TYPE_NORMAL
- en: This article has been translated to Chinese 简体中文. Interested in translating
    to another language? Contact nathan at…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: huggingface.co](https://huggingface.co/blog/rlhf?source=post_page-----d26aa2642c88--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Models such as ChatGPT are able to provide labels for input data by simply replying
    (in language) with the desired label. Their vast knowledge of the world obtained
    through pre-training on such large amounts of data have endowed these models with
    the ability to solve novel tasks using only their semantic understanding of the
    question being asked.
  prefs: []
  type: TYPE_NORMAL
- en: This process can be automated using open-sourced models such as [FLAN-T5](https://huggingface.co/google/flan-t5-xxl)
    by asking the model to respond with only items in your label set (e.g. “Respond
    with ‘Yes’ or ‘No’”), and checking which option has the highest output probability
    after asking the model for labels.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1548eb1c4ec958355a2c58b1d51dd1d7.png)'
  prefs: []
  type: TYPE_IMG
- en: ChatGPT is able to not only provide a label, but also explain its logic for
    obtaining said label.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Labeling data is a critical step in supervised machine learning, but it can
    be costly to obtain large amounts of labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: With zero-shot learning and LLMs, we can significantly reduce the cost of label
    acquisition.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs pre-trained on huge amounts of data encode a semantic understanding of
    the world’s information that allow them to have high performance on arbitrary,
    unseen tasks. These models can automatically label data for us with high accuracy,
    allowing us to bootstrap supervised models at a low cost.
  prefs: []
  type: TYPE_NORMAL
