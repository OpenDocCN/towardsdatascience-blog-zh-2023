- en: Unleashing the ChatGPT Tokenizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/chatgpt-tokenizer-chatgpt3-chatgpt4-artificial-intelligence-python-ai-27f78906ea54](https://towardsdatascience.com/chatgpt-tokenizer-chatgpt3-chatgpt4-artificial-intelligence-python-ai-27f78906ea54)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hands-On! How ChatGPT Manages Tokens?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@andvalenzuela?source=post_page-----27f78906ea54--------------------------------)[![Andrea
    Valenzuela](../Images/ddfc1534af92413fd91076f826cc49b6.png)](https://medium.com/@andvalenzuela?source=post_page-----27f78906ea54--------------------------------)[](https://towardsdatascience.com/?source=post_page-----27f78906ea54--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----27f78906ea54--------------------------------)
    [Andrea Valenzuela](https://medium.com/@andvalenzuela?source=post_page-----27f78906ea54--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----27f78906ea54--------------------------------)
    ·9 min read·Jul 6, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6d00dc92aaafbfe984040c4f28c2509.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-made gif.
  prefs: []
  type: TYPE_NORMAL
- en: '**Have you ever wondered which are the key components behind ChatGPT?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We all have been told the same: ChatGPT predicts the next word. But actually,
    there is a bit of a lie in this statement. **It does not predict the next word,
    ChatGPT predicts the next token**.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Token?* Yes, a token is the unit of text for Large Language Models (LLMs).'
  prefs: []
  type: TYPE_NORMAL
- en: Indeed **one of the first steps that ChatGPT does when processing any prompt
    is splitting the user input into tokens**. And that is the job of the so-called
    **tokenizer**.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will uncover how the ChatGPT tokenizer works with hands-on
    practice with the original library used by OpenAI, the `tiktoken` library.
  prefs: []
  type: TYPE_NORMAL
- en: '*TikTok-en… Funny enough :)*'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive deep and comprehend the actual steps performed by the tokenizer,
    and how its behavior really impacts the quality of the ChatGPT output.
  prefs: []
  type: TYPE_NORMAL
- en: How the Tokenizer Works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the article [Mastering ChatGPT: Effective Summarization with LLMs](/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce)
    we already saw some of the mysteries behind the ChatGPT tokenizer, but let’s start
    from scratch.'
  prefs: []
  type: TYPE_NORMAL
- en: The tokenizer appears at the first step in the process of text generation. **It
    is responsible for breaking down the piece of text that we input to ChatGPT into
    individual elements**, the tokens, which are then processed by the language model
    to generate new text.
  prefs: []
  type: TYPE_NORMAL
- en: When the tokenizer breaks down a piece of text into tokens, it does so based
    on a set of rules that are designed to identify the meaningful units of the target
    language.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, when the words that appear in a given sentence are fairly common
    words, chances are that each token corresponds to one word. But if we use a prompt
    with less frequently used words, like in the sentence *“Prompting as powerful
    developer tool”,* we might not get a one-to-one mapping. In this case, the word
    *prompting* is still not that common in the English language, so it is actually
    broken down to three tokens: *“‘prom”*, *“pt”*, and *“ing”* because those three
    are commonly occurring sequences of letters.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Let’s see another example!*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following sentence: *“I want to eat a peanut butter sandwich”.*
    If the tokenizer is configured to split tokens based on spaces and punctuation,
    it may break this sentence down into the following tokens with a total word count
    of 8, equal to the token count.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/372b106853250883ddbc4f64d78a113e.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-made image.
  prefs: []
  type: TYPE_NORMAL
- en: However, if the tokenizer treats *“peanut butter”* as a compound word due to
    the fact that the components appear together quite often, it may break the sentence
    down into the following tokens, **with a total word count of 8, but a token count
    of 7**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e7d6a87f22c074d29ae976ca8071d29.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-made image.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of ChatGPT and the management of tokens, the terms **encoding
    and decoding refer to the processes of converting text into tokens** that the
    model can understand (encoding) and converting the model’s completion back into
    human-readable text (decoding).
  prefs: []
  type: TYPE_NORMAL
- en: Tiktoken Library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Knowing the theory behind the ChatGPT tokenizer is necessary, but in this article,
    I would like to focus on some hands-on revelations too.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ChatGPT implementation uses the `tiktoken` library for managing tokens.
    We can get it up a running like any other Python library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once it is installed, it is very simple to get the same encoding model that
    ChatGPT uses since there is a `encoding_for_model()` method. As inferred by the
    name, this method automatically loads the correct encoding for a given model name.
  prefs: []
  type: TYPE_NORMAL
- en: The first time it runs for a given model, it requires an internet connection
    to download the encoding model. Later runs won’t need internet since the encoding
    is already pre-cached.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the widely used `gpt-3.5-turbo` model, we can simply run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The output `encoding` is a tokenizer object that we can use to visualize how
    ChatGPT actually sees our prompt.
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, the `tiktoken.encoding_for_model` function initializes a
    tokenization pipeline specifically for the `gpt-3.5-turbo` model. This pipeline
    handles the tokenization and encoding of the text, preparing it for the model’s
    consumption.
  prefs: []
  type: TYPE_NORMAL
- en: One important aspect to consider is that the tokens are numerical representations.
    In our *“Prompting as powerful developer tool”* example, the tokens associated
    with the word *prompting* were *“‘prom”*, *“pt”*, and *“ing”*, but what the model
    actually receives is the numerical representation of those sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '*No worries!* We will see what this looks like in the hands-on section.'
  prefs: []
  type: TYPE_NORMAL
- en: Encoding types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `tiktoken` library supports multiple encoding types. In fact, different
    `gpt` models use different encodings. Here is a table with the most common ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cb5e491500e9b4c16f5132985f4b63d5.png)'
  prefs: []
  type: TYPE_IMG
- en: Encoding — Hands-On!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s move forward and try to encode our first prompt. Given the prompt *“tiktoken
    is great!”* and the already loaded `encoding`, we can use the method `encoding.encode`
    to split the prompt into tokens and visualize their numerical representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*Yes, that is true.* The output `[83, 1609, 5963, 374, 2294, 0]` does not seem
    very meaningful. But actually, there is something one can guess at first glance.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Got it?*'
  prefs: []
  type: TYPE_NORMAL
- en: '*The length!* We can quickly see that our prompt *“tiktoken is great!”* is
    split into 6 tokens. In this case, **ChatGPT is not splitting this sample prompt
    based on blank spaces, but on the most frequent sequences of letters.**'
  prefs: []
  type: TYPE_NORMAL
- en: In our example, each coordinate in the output list corresponds to a specific
    token in the tokenized sequence, the so-called token IDs. **The token IDs are
    integers that uniquely identify each token according to the vocabulary used by
    the model**. IDs are typically mapped to words or subword units in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s just decode the list of coordinates back to double check it corresponds
    to our original prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `.decode()` method converts a list of token integers to a string. Although
    the `.decode()` method can be applied to single tokens, **be aware that it can
    be lossy for tokens that aren’t on** `**utf-8**` **boundaries**.
  prefs: []
  type: TYPE_NORMAL
- en: And now you might wonder, *is there a way to see the individual tokens?*
  prefs: []
  type: TYPE_NORMAL
- en: '*Let’s go for it!*'
  prefs: []
  type: TYPE_NORMAL
- en: 'For single tokens, the `.decode_single_token_bytes()` method safely converts
    a single integer token to the bytes it represents. For our sample prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `b` in front of the strings indicates that the strings are byte
    strings. For the English language, one token roughly on average, corresponds to
    about four characters or about three-quarters of a word.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing how the text is split into tokens is useful because GPT models see text
    in the form of tokens. Knowing how many tokens are in a text string can give you
    useful information like whether the string is too long for a text model to process,
    or how much an OpenAI API call will cost as usage is priced by token, among others.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Encoding Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/b567bf0e70a07178a6d46f1b1c2ac962.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-made gif.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen, **different models use different encodings types**. Sometimes,
    there can be a huge difference in the token management between models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Different encodings vary in how they split words, group spaces, and handle
    non-English characters**. Using the methods above, we can compare different encodings
    for the different `gpt` models available on a few example strings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s compare the encodings of the table above (`gpt2`, `p50k_base`, and `cl100k_base`).
    To do so, we can use the following function that contains all the bits and pieces
    we have seen so far:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `compare_encodings` function takes an `example_string` as input and compares
    the encodings of that string using three different encoding schemes: `gpt2`, `p50k_base`,
    and `cl100k_base`. Finally, it prints various information about the encodings,
    including the number of tokens, the token integers, and the token bytes.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Let’s try some examples!*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96a06fc1407200dc733968cc15adec56.png)'
  prefs: []
  type: TYPE_IMG
- en: In this first example, although the `gpt2` and `p50k_base` models agree on the
    encoding by merging together the math symbols with the blank spaces, the `cl100k_base`
    encoding considers them separate entities.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab5e9520a298e6044dff60495731c058.png)'
  prefs: []
  type: TYPE_IMG
- en: In this example, the way of tokenizing the word *Prompting* also depends on
    the selected encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizer Limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/a56c3bd7ed2bfe51aa7feb3d52553ff3.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-made gif.
  prefs: []
  type: TYPE_NORMAL
- en: '**This way of tokenizing the input prompts is sometimes the cause of some ChatGPT
    completion errors**. For example, if we ask ChatGPT to write the word lollipop
    in reversed order, it does it wrong!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f049427e75ab89b1f45fee021a553ded.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-made screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: 'What is happening here is that the tokenizer actually breaks the given word
    down into three tokens: *“l”* ,*“oll”* and *“ipop”*. Therefore, **ChatGPT does
    not see the individual letters, instead it sees these three tokens** making it
    more difficult to print out individual letters in reverse order correctly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Being aware of the limitations can make you find workarounds to avoid them.
    In this case, if we add dashes to the word between the individual letters, we
    can force the tokenizer to split the text based on those symbols. By slightly
    modifying the input prompt, it actually does a much better job:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e16850a86e10532d2a9bce6c756642d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-made screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: '**By using dashes, it is easier for the model to see the individual letters
    and print them out in reverse order**. So keep that in mind: If you ever want
    to use ChatGPT to play a word game, like *Word* or *Scrabble*, or build an application
    around those principles, this nifty trick helps it to better see the individual
    letters of the words.'
  prefs: []
  type: TYPE_NORMAL
- en: '**This is just a simple example where the ChatGPT tokenizer causes the model
    to fail in a very simple task**. *Have you encountered any other similar cases?*'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we have explored how **ChatGPT sees user prompts and processes
    them to generate a completion output** based on statistical patterns learned from
    vast amounts of language data during its training.
  prefs: []
  type: TYPE_NORMAL
- en: By using the `tiktoken` library, **we are now able to evaluate any prompt**
    before feeding it into ChatGPT. This can help us to **debug ChatGPT errors** since
    it can happen that by slightly modifying our prompt, we can make ChatGPT better
    complete the task.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also an extra take-home message: **Some design decisions made can
    turn into technical debts in the future**. As we have seen in the simple *lollipop*
    example, while the model succeeds in mind-blowing tasks, it cannot complete simple
    exercises. And the reason behind this is not on the model capabilities, but on
    the first tokenization step!'
  prefs: []
  type: TYPE_NORMAL
- en: That is all! Many thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: I hope this article helps you when **builiding ChatGPT applications!**
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also subscribe to my [**Newsletter**](https://towardsdatascience.com/@andvalenzuela/subscribe)
    to stay tuned for new content. **Especially**, **if you are interested in articles
    about ChatGPT**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce?source=post_page-----27f78906ea54--------------------------------)
    [## Mastering ChatGPT: Effective Summarization with LLMs'
  prefs: []
  type: TYPE_NORMAL
- en: How to Prompt ChatGPT to get High-Quality Summaries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce?source=post_page-----27f78906ea54--------------------------------)
    [](https://medium.com/geekculture/prompt-engineering-course-openai-inferring-transforming-expanding-chatgpt-chatgpt4-e5f63132f422?source=post_page-----27f78906ea54--------------------------------)
    [## Prompt Engineering Course by OpenAI — Inferring, Transforming, and Expanding
    with ChatGPT
  prefs: []
  type: TYPE_NORMAL
- en: Maximize ChatGPT’s Potential in your Custom Application
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'medium.com](https://medium.com/geekculture/prompt-engineering-course-openai-inferring-transforming-expanding-chatgpt-chatgpt4-e5f63132f422?source=post_page-----27f78906ea54--------------------------------)
    [](/chatgpt-text-to-speech-artificial-intelligence-python-data-science-52456f51fad6?source=post_page-----27f78906ea54--------------------------------)
    [## Unlocking a New Dimension of ChatGPT: Text-to-Speech Integration'
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing User Experience in ChatGPT Interactions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/chatgpt-text-to-speech-artificial-intelligence-python-data-science-52456f51fad6?source=post_page-----27f78906ea54--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Feel free to forward any questions** you may have to *forcodesake.hello@gmail.com*
    :)'
  prefs: []
  type: TYPE_NORMAL
