- en: Multi-Layer Perceptrons Explained and Illustrated
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/multi-layer-perceptrons-8d76972afa2b](https://towardsdatascience.com/multi-layer-perceptrons-8d76972afa2b)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understand the first fully-functional model of neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@roiyeho?source=post_page-----8d76972afa2b--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----8d76972afa2b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8d76972afa2b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8d76972afa2b--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----8d76972afa2b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8d76972afa2b--------------------------------)
    ·13 min read·Apr 2, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: In [the previous article](https://medium.com/@roiyeho/perceptrons-the-first-neural-network-model-8b3ee4513757)
    we talked about perceptrons as one of the earliest models of neural networks.
    As we have seen, single perceptrons are limited in their computational power since
    they can solve only linearly separable problems.
  prefs: []
  type: TYPE_NORMAL
- en: In this article we will discuss multi-layer perceptrons (MLPs), which are networks
    consisting of multiple layers of perceptrons and are much more powerful than single-layer
    perceptrons. We will see how these networks operate and how to use them to solve
    complex tasks such as image classification.
  prefs: []
  type: TYPE_NORMAL
- en: Definitions and Notations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A **multi-layer perceptron** (MLP) is a neural network that has at least three
    layers: an input layer, an hidden layer and an output layer. Each layer operates
    on the outputs of its preceding layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a1eb1fbb0ed55fbf175252ae54f14c61.png)'
  prefs: []
  type: TYPE_IMG
- en: The MLP architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following notations:'
  prefs: []
  type: TYPE_NORMAL
- en: '*aᵢˡ* is the activation (output) of neuron *i* in layer *l*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*wᵢⱼˡ* is the weight of the connection from neuron *j* in layer *l*-1 to neuron
    *i* in layer *l*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*bᵢˡ* is the bias term of neuron *i* in layer *l*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The intermediate layers between the input and the output are called **hidden
    layers** since they are not visible outside of the network (they form the “internal
    brain” of the network).
  prefs: []
  type: TYPE_NORMAL
- en: The input layer is typically not counted in the number of layers in the network.
    For example, a 3-layer network has one input layer, two hidden layers, and an
    output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Forward Propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Forward propagation is the process where the input data is fed through the network
    in a forward direction, layer-by-layer, until it generates the output.
  prefs: []
  type: TYPE_NORMAL
- en: The activations of the neurons during the forward propagation phase are computed
    similar to how the activation of a single perceptron is computed.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s look at neuron *i* in layer *l*. The activation of this
    neuron is computed in two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first compute the net input of the neuron as the weighted sum of its incoming
    inputs plus its bias:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/c1bdf5f6f5c68e0d2e90012c8360fb73.png)'
  prefs: []
  type: TYPE_IMG
- en: The net input of neuron i in layer l
  prefs: []
  type: TYPE_NORMAL
- en: '2\. We now apply the activation function to the net input to get the neuron’s
    activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cb4bcad164ee6ecf2463d814e5fd5155.png)'
  prefs: []
  type: TYPE_IMG
- en: The activation of neuron i in layer l
  prefs: []
  type: TYPE_NORMAL
- en: By definition, the activations of the neurons in the input layer are equal to
    the feature values of the example currently presented to the network, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b5717c3d1ced500889945d444a0d6255.png)'
  prefs: []
  type: TYPE_IMG
- en: The activations of the input neurons
  prefs: []
  type: TYPE_NORMAL
- en: where *m* is the number of features in the data set.
  prefs: []
  type: TYPE_NORMAL
- en: Vectorized Form
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To make the computations more efficient (especially when using numerical libraries
    like NumPy), we typically use the vectorized form of the above equations.
  prefs: []
  type: TYPE_NORMAL
- en: We first define the vector **a***ˡ* as the vector containing the activations
    of all the neurons in layer *l*, and the vector**b***ˡ* as the vector with the
    biases of all the neurons in layer *l*.
  prefs: []
  type: TYPE_NORMAL
- en: We also define *Wˡ* as the matrix of connection weights from all the neurons
    in layer *l* — 1 to all the neurons in layer *l*. For example, *W*¹₂₃ is the weight
    of the connection between neuron no. 2 in layer 0 (the input layer) and neuron
    no. 3 in layer 1 (the first hidden layer).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now write the forward propagation equations in vector form. For each
    layer *l* we compute:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ce5b15f01e65393f47a2048d9ebba24.png)'
  prefs: []
  type: TYPE_IMG
- en: The vectorized form of the forward propagation equations
  prefs: []
  type: TYPE_NORMAL
- en: Solving the XOR Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first demonstration of the power of MLPs over single perceptrons has shown
    that they are capable of solving the XOR problem. The XOR problem is not linearly
    separable, thus a single perceptron cannot solve it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37e6380514099aaec8d2ef89be3a9642.png)'
  prefs: []
  type: TYPE_IMG
- en: The XOR problem
  prefs: []
  type: TYPE_NORMAL
- en: 'However, an MLP with a single hidden layer can easily solve this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf692a0df09ba84762cf196f2cc00473.png)'
  prefs: []
  type: TYPE_IMG
- en: An MLP that solves the XOR problem. The bias terms are written inside the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s analyze how this MLP works. The MLP has three hidden neurons and one output
    neuron in addition to the two input neurons. We assume here that all the neurons
    use the step activation function (i.e., the function whose value is 1 for all
    non-negative inputs, and 0 for all negative inputs).
  prefs: []
  type: TYPE_NORMAL
- en: The top hidden neuron is connected only to the first input *x*₁ with a connection
    weight of 1, and it has a bias of -1\. Therefore, this neuron fires only when
    *x*₁ = 1 (in which case its net input is 1 × 1 + (-1) = 0, and *f*(0) = 1, where
    *f* is the step function).
  prefs: []
  type: TYPE_NORMAL
- en: The middle hidden neuron is connected to both inputs with connection weights
    of 1, and it has a bias of -2\. Therefore, this neuron fires only when both inputs
    are 1.
  prefs: []
  type: TYPE_NORMAL
- en: The bottom hidden neuron is connected only to the second input *x*₂ with a connection
    weight of 1, and it has a bias of -1\. Therefore, this neuron fires only when
    *x*₂ = 1.
  prefs: []
  type: TYPE_NORMAL
- en: The output neuron is connected to the top and the bottom hidden neurons with
    a weight of 1, and to the middle hidden neuron with a weight of -2, and it has
    a bias of -1\. Therefore, it fires only when either the top or the bottom hidden
    neuron fire, but not when both of them fire together. In other words, it fires
    only when *x*₁ = 1 or *x*₂ = 1 but not when both inputs are 1, which is exactly
    what we expect the output of the XOR function to be.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s compute the forward propagation of this MLP for the inputs
    *x*₁ = 1 and *x*₂ = 0\. The activations of the hidden neurons in this case are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04c4f48e457481f6411dae3a8eb2ba51.png)'
  prefs: []
  type: TYPE_IMG
- en: The activations of the hidden neurons for x1 = 1 and x2 = 0
  prefs: []
  type: TYPE_NORMAL
- en: We can see that only the top hidden neuron fires in this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'The activation of the output neuron is therefore:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/414f961379e3a54e7d06cf06e737f52d.png)'
  prefs: []
  type: TYPE_IMG
- en: The output of the MLP for x1 = 1 and x2 = 0
  prefs: []
  type: TYPE_NORMAL
- en: The output neuron fires in this case, which is what we expect the output of
    XOR to be for the inputs *x*₁ = 1 and *x*₂ = 0.
  prefs: []
  type: TYPE_NORMAL
- en: Verify that you understand how the MLP computes the other three cases of the
    XOR function as well!
  prefs: []
  type: TYPE_NORMAL
- en: MLP Construction Exercise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As another example, consider the following data set that contains points from
    three different classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7dfb7d4a25f24acdf00bcbaec13823d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Build an MLP that correctly classifies all the points in this data set.
  prefs: []
  type: TYPE_NORMAL
- en: '*Hint*: Use the hidden neurons to identify the three classification areas.'
  prefs: []
  type: TYPE_NORMAL
- en: The solution can be found at the bottom of this article.
  prefs: []
  type: TYPE_NORMAL
- en: The Universal Approximation Theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the remarkable facts about MLPs is that they can compute any arbitrary
    function (even though each neuron in the network computes a very simple function
    such as the step function).
  prefs: []
  type: TYPE_NORMAL
- en: The [universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)
    states that an MLP with one hidden layer (with a sufficient number of neurons)
    can approximate any continuous function of the inputs arbitrarily well. With two
    hidden layers, it can even approximate discontinuous functions. This means that
    even very simple network architectures can be extremely powerful.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the proof of the theorem is non-constructive, i.e., it does not
    tell us how to build a network to compute a specific function but only shows that
    such a network exists.
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning in MLPs: Backpropagation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although MLPs have proven to be computationally powerful, for a long time it
    was not clear how to train them on a specific data set. While single perceptrons
    have a simple weight update rule, it was not clear how to apply this rule to the
    weights of the hidden layers, since these do not directly affect the output of
    the network (and hence its training loss).
  prefs: []
  type: TYPE_NORMAL
- en: It took the AI community more than 30 years to solve this problem when in 1986
    Rumelhart et al. introduced their groundbreaking **backpropagation** algorithm
    for training MLPs.
  prefs: []
  type: TYPE_NORMAL
- en: The main idea of backpropagation is to first compute the gradients of the error
    function of the network with respect to each one of its weights, and then use
    gradient descent to minimize the error. It is called backpropagation because we
    propagate the gradients of the error from the output layer back to the input layer
    using the **chain rule of derivatives**.
  prefs: []
  type: TYPE_NORMAL
- en: The backpropagation algorithm is thoroughly explained in [this article](https://medium.com/towards-data-science/backpropagation-step-by-step-derivation-99ac8fbdcc28).
  prefs: []
  type: TYPE_NORMAL
- en: Activation Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In single-layer perceptrons we have used either the step or the sign functions
    for the neuron’s activation. The issue with these functions is that their gradient
    is 0 almost everywhere (since they are equal to a constant value for *x* > 0 and
    for *x* < 0). This means that we cannot use them in gradient descent to find the
    minimum error of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in MLPs we need to use other activation functions. These functions
    should be both differentiable and non-linear (if all the neurons in an MLP use
    a linear activation function then the MLP behaves like a single-layer perceptron).
  prefs: []
  type: TYPE_NORMAL
- en: 'For the hidden layers, the three most common activation functions are:'
  prefs: []
  type: TYPE_NORMAL
- en: The sigmoid function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/626faaeee07320671c208244f8fc9d07.png)'
  prefs: []
  type: TYPE_IMG
- en: 2\. The hyperpoblic tangent function
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/722972c5d254f95a20b77ace39781619.png)'
  prefs: []
  type: TYPE_IMG
- en: 3\. The ReLU (rectified linear unit) function
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6abf1e2e4d040e3f101274fac04c0b11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The activation function in the output layer depends on the problem the network
    is trying to solve:'
  prefs: []
  type: TYPE_NORMAL
- en: For regression problems we use the identity function *f*(*x*) = *x*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For binary classification problems we use the sigmoid function (shown above).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For multi-class classification problems we use the softmax function, which
    converts a vector of *k* real numbers into a probability distribution of *k* possible
    outcomes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/55e040a35e031692c9b4ee9159a4cb44.png)'
  prefs: []
  type: TYPE_IMG
- en: The softmax function
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason why we use the softmax function in multi-class problems is explained
    in depth in this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/deep-dive-into-softmax-regression-62deea103cb8?source=post_page-----8d76972afa2b--------------------------------)
    [## Deep Dive into Softmax Regression'
  prefs: []
  type: TYPE_NORMAL
- en: Understand the math behind softmax regression and how to use it to solve an
    image classification task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/deep-dive-into-softmax-regression-62deea103cb8?source=post_page-----8d76972afa2b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: MLPs in Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scikit-Learn provides two classes that implement MLPs in the sklearn.neural_network
    module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)
    is used for classification problems.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)
    is used for regression problems.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The important hyperparameters in these classes are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*hidden_layer_sizes* — a tuple that defines the number of neurons in each hidden
    layer. The default is (100,), i.e., a single hidden layer with 100 neurons. For
    many problems, using just one or two hidden layers should be enough. For more
    complex problems, you can gradually increase the number of hidden layers, until
    the network starts overfitting the training set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*activation* — the activation function to use in the hidden layers. The options
    are ‘identity’, ‘logistic’, ‘tanh’, and ‘relu’ (the default).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*solver* — the solver to use for the weight optimization. The default is ‘adam’,
    which works well on most data sets. The behavior of the various optimizers will
    be explained in a future article.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*alpha* — the L2 regularization coefficient (defaults to 0.0001)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*batch_size* — the size of the mini-batches used for training (defaults to
    200).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*learning_rate* — learning rate schedule for weight updates (defaults to ‘constant’).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*learning_rate_init* — the initial learning rate used (defaults to 0.001).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*early_stopping* — whether to stop the training when the validation score is
    not improving (defaults to False).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*validation_fraction* — the proportion of the training set to set aside for
    validation (defaults to 0.1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We normally use grid search and cross-validation to tune these hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Training an MLP on MNIST
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For example, let’s train an MLP on the [MNIST data set](https://en.wikipedia.org/wiki/MNIST_database),
    which is a widely used data set for image classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The data set contains 60,000 training images and 10,000 testing images of handwritten
    digits. Each image is 28 × 28 pixels in size, and is typically represented by
    a vector of 784 numbers in the range [0, 255]. The task is to classify these images
    into one of the ten digits (0–9).
  prefs: []
  type: TYPE_NORMAL
- en: 'We first fetch the MNIST data set using the [fetch_openml()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html)
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The *as_frame* parameter specifies that we want to get the data and the labels
    as NumPy arrays instead of DataFrames (the default of this parameter has changed
    in Scikit-Learn 0.24 from False to ‘auto’).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine the shape of *X*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: That is, *X* consists of 70,000 flat vectors of 784 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s display the first 50 digits in the data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8cf5f43a5f3f214df8bf8277c60c3530.png)'
  prefs: []
  type: TYPE_IMG
- en: The first 50 digits from the MNIST data set
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check how many samples we have from each digit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The data set is fairly balanced between the 10 classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now scale the inputs to be within the range [0, 1] instead of [0, 255]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Feature scaling makes the training of neural networks faster and prevents them
    from getting stuck in local optima.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now split the data into training and test sets. Note that the first 60,000
    images in MNIST are already designated for training, so we can just use simple
    slicing for the split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We now create an MLP classifier with a single hidden layer with 300 neurons.
    We will keep all the other hyperparameters with their default values, except for
    *early_stopping* which we will change to True. We will also set verbose=True in
    order to track the progress of the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s fit the classifier to the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output we get during training is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The training stopped after 31 iterations, since the validation score has not
    improved during the previous 10 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check the accuracy of the MLP on the training and the test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: These are great results, but networks with more complex architectures such as
    convolutional neural networks (CNNs) can achieve even better results on this data
    set (up to 99.91% accuracy on the test!). You can find the state-of-the-art results
    on MNIST with links to the relevant papers [here](https://paperswithcode.com/sota/image-classification-on-mnist).
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand better the errors of our model, let’s display its confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/565ee83303ba8410af3687aa5c8e7f26.png)'
  prefs: []
  type: TYPE_IMG
- en: The confusion matrix on the test set
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the main confusions of the model are between the digits 4⇔9,
    7⇔9 and 2⇔8\. This makes sense since these digits often resemble each other when
    written by hand. To help our model distinguish between these digits, we can add
    more examples from these digits (e.g., by using data augmentation) or extract
    additional features from the images (e.g., the number of closed loops in the digit).
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the MLP Weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although neural networks are generally considered to be “black-box” models,
    in simple networks that consist of one or two hidden layers, we can visualize
    the learned weights and occasionally gain some insight into how these networks
    work internally.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can plot the weights between the input and the hidden layers
    of our MLP classifier. The weight matrix has a shape of (784, 300), and is stored
    in a variable called mlp.coefs_[0]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Column *i* of this matrix represents the weights of the incoming inputs to hidden
    neuron *i*. We can display this column as a 28 × 28 pixel image, in order to examine
    which input neurons have a stronger influence on this neuron’s activation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following plot displays the weights of the first 20 hidden neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ec1a1c851e805c9f1ec63ced661523f7.png)'
  prefs: []
  type: TYPE_IMG
- en: The weights of the first 20 hidden neurons
  prefs: []
  type: TYPE_NORMAL
- en: We can see that each hidden neuron focuses on different segments of the image.
  prefs: []
  type: TYPE_NORMAL
- en: MLPs in Other Libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although the MLP classifier in Scikit-Learn is easy to use, in practical applications
    you are more likely to use a deep learning library such as TensorFlow or PyTorch
    to build MLPs. These libraries can take advantage of faster GPU processing and
    they also provide many additional options, such as additional activation functions
    and optimizers. You can find an example of how to use these libraries in [this
    post](https://medium.com/@roiyeho/pytorch-2-0-or-tensorflow-2-10-which-one-is-better-52669cec994).
  prefs: []
  type: TYPE_NORMAL
- en: Solution to the MLP Construction Exercise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following MLP classifies correctly all the points in the data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/78fe6af3673e411edf25fde4e044d6bc.png)'
  prefs: []
  type: TYPE_IMG
- en: MLP for solving the classification problem. The bias terms are written inside
    the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: The left hidden neuron fires only when *x*₁ ≤ 3, the middle hidden neuron fires
    only when *x*₂ ≥ 4, and the right hidden neuron fires only when *x*₂ ≤ 0.
  prefs: []
  type: TYPE_NORMAL
- en: The left output neuron performs an OR between the left and the middle hidden
    neurons, therefore it fires only if *x*₁ ≤ 3 OR *x*₂ ≥ 4, i.e., only when the
    point is blue.
  prefs: []
  type: TYPE_NORMAL
- en: The middle output neuron performs a NOR (Not OR) between all the hidden neurons,
    therefore it fires only when NOT (*x*₁ ≤ 3 OR *x*₂ ≥ 4 OR *x*₂ ≤ 0). In other
    words, it fires only when *x*₁ > 3 AND 0 < *x*₂ < 4, i.e., only when the point
    is red.
  prefs: []
  type: TYPE_NORMAL
- en: The right output neuron fires only when the right hidden neuron fires, i.e.,
    only when *x*₂ ≤ 0, which is true only for the purple points.
  prefs: []
  type: TYPE_NORMAL
- en: Final Notes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find the code examples of this article on my github: [https://github.com/roiyeho/medium/tree/main/mlp](https://github.com/roiyeho/medium/tree/main/mlp)'
  prefs: []
  type: TYPE_NORMAL
- en: All images unless otherwise noted are by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'MNIST Dataset Info:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Citation:** Deng, L., 2012\. The mnist database of handwritten digit images
    for machine learning research. *IEEE Signal Processing Magazine*, 29(6), pp. 141–142.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**License:** Yann LeCun and Corinna Cortes hold the copyright of the MNIST
    dataset which is available under the *Creative Commons Attribution-ShareAlike
    4.0 International License* ([CC BY-SA](https://creativecommons.org/licenses/by-sa/4.0/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
