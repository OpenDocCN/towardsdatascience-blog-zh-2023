- en: How to Create Valuable Data Tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-create-valuable-data-tests-850e778718e1](https://towardsdatascience.com/how-to-create-valuable-data-tests-850e778718e1)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What matters is not the quantity, but the quality.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@xiaoxugao?source=post_page-----850e778718e1--------------------------------)[![Xiaoxu
    Gao](../Images/8712a7e5f3bad0d2abd7e04792fad66f.png)](https://medium.com/@xiaoxugao?source=post_page-----850e778718e1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----850e778718e1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----850e778718e1--------------------------------)
    [Xiaoxu Gao](https://medium.com/@xiaoxugao?source=post_page-----850e778718e1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----850e778718e1--------------------------------)
    ·9 min read·Jul 3, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a6433b06eebe7358fd99a93d34ff62f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Shubham Dhage](https://unsplash.com/@theshubhamdhage) on [Unsplash](https://unsplash.com/)
  prefs: []
  type: TYPE_NORMAL
- en: Data quality has been widely discussed over the past year. The increasing adoption
    of data contracts, data products, and data observability tools certainly shows
    data practitioners’ commitment to providing high-quality data to their consumers.
    We all love to see this!
  prefs: []
  type: TYPE_NORMAL
- en: One essential building block in data solutions is data tests. It’s one of the
    most fundamental and practical ways to validate data quality and is explicitly
    or implicitly embedded in many data solutions.
  prefs: []
  type: TYPE_NORMAL
- en: While its effectiveness has yielded significant benefits for data teams, it
    also raises questions regarding how to maximize its potential values because having
    more tests doesn’t necessarily mean having higher data quality. In this article,
    I want to show you some approaches to designing data tests. Hopefully, they can
    shed some light here.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that you are recommended to combine these approaches and find
    a balance that works best for you.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Quality > Quantity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/50cbde299cf98bb64a915f7b5a14d136.png)'
  prefs: []
  type: TYPE_IMG
- en: Quality > Quantity (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: I’m one of those who love creating tests because they give me increased confidence
    in my solutions. With a background in Software Engineering, I once lived by the
    motto “The more tests, the merrier”. I was always excited about data frameworks
    offering simple data test creation methods.
  prefs: []
  type: TYPE_NORMAL
- en: However, I underestimated the side effects of having an excessive number of
    data tests. (Is there even a side effect? YES!) Let’s first understand the distinction
    between data tests and unit tests (i.e. logic tests). In short, a unit test is
    meant to validate the correctness of the code’s logic that we’ve written. The
    more unit tests we have, the more confident we are in handling edge cases. But
    a data test goes beyond the code logic, it also examines the quality of the source
    data, data pipeline configurations, upstream dependencies, and so on. The metrics
    are endless and can be overwhelming. It’s tempting to create numerous tests just
    in case, but they don’t always bring value and might introduce unnecessary noise.
    For example, let’s face it, not every data pipeline needs a daily freshness test
    if they are only used by users once in a while, and not all stages in the data
    pipeline require the same test. Duplicated data tests only will result in redundant
    alerts.
  prefs: []
  type: TYPE_NORMAL
- en: At one point, I was lost in creating data tests and ended up with many redundant
    tests that led to false alerts. **I learned that in order to ensure data quality,
    we first need to ensure the quality of data tests.** Quantity doesn’t necessarily
    correlate with quality.
  prefs: []
  type: TYPE_NORMAL
- en: Fitness for use
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/dbb105199c04b434c3b2eaa313b13ccf.png)'
  prefs: []
  type: TYPE_IMG
- en: Capture the full voice of customer (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Data can be treated as a product. Data pipelines can be viewed as data manufacturing
    systems taking raw data input and producing data products. Although data consumers
    don’t purchase data, they can decide whether or not to use it and they can provide
    requirements and feedback accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of “fitness for use” is widely adopted in measuring quality. It
    emphasizes the importance of **capturing the full voice of customers** because,
    in the end, it is consumers who will decide the success of the product. For example,
    when building a university library, it’s crucial to keep students and teachers
    in mind, meaning that it should have a range of books and resources covering diverse
    interests and educational requirements.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to data products, a good example is reporting data. To be considered
    valuable, engineers should work closely with stakeholders to understand regulatory
    requirements on accuracy, reliability, integrity, timeliness, etc, and then create
    data tests accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: This approach is a good start for the test creation journey, especially when
    data consumers possess a clear vision of their requirements and have a good understanding
    of data quality. It can make data engineers’ life a lot easier. However, relying
    solely on stakeholder requirements is often not enough since they assess data
    from an external view without taking upstream dependencies, and technical errors
    into consideration. In that case, a data quality framework in the next section
    will help us cover most of the scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Data Quality dimensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Taking a consumer viewpoint of data quality is undoubtedly a valuable initial
    step. But it might not cover the completeness of the test scope. Extensive literature
    reviews have addressed this issue for us, offering [a range of data quality dimensions](https://dl.acm.org/doi/pdf/10.1145/240455.240479)
    that are relevant to most use cases. It’s advisable to review the list with data
    consumers and collectively determine which dimensions are applicable and create
    tests accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You might find this list too long and wonder how to start with it. Data products
    or any information system can be observed or analyzed from two perspectives: external
    view and internal view.'
  prefs: []
  type: TYPE_NORMAL
- en: '***External view***'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52918b43295ed070325b0cea5f1198cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Dimensions of external view (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The external view is about the use of the data and its relation with the organization.
    It’s often considered a “black box” with functionality to represent the real-world
    system. The dimensions that fall into the external view are highly business-driven.
    Sometimes, the evaluation of those dimensions can be subjective, so it’s not always
    easy to create automated tests for them. But let’s check out a few well-known
    dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Relevancy**: The extent to which data are applicable and helpful for the
    analysis. Considering a market campaign aimed at promoting a new product. All
    data attributes should directly contribute to the success of the campaign such
    as customer demographic data and purchase data. Data like city weather or stock
    market prices are irrelevant data in this case. Another example is the level of
    detail (granularity). If the business wants the market data to be on the day level,
    but it’s delivered on the weekly level, then it’s not relevant and useful.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Representation:** The extent to which data is interpretable for data consumers
    and the data format is consistent and descriptive. The importance of the representation
    layer is often overlooked when accessing data quality. It includes the format
    of the data — being consistent and user-friendly, and the meaning of the data
    — being understandable. For instance, consider a scenario where data is anticipated
    to be available in a CSV file with descriptive column descriptions, and the values
    are expected to be in EUR currency rather than in cents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Timeliness:** The extent to which data is fresh for data consumers. For example,
    the business needs the sales transaction data with a maximum delay of 1 hour from
    the point of sale. It indicates that the data pipeline should be refreshed frequently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accuracy:** The extent to which data is compliant with business rules. Data
    metrics are often associated with complicated business rules such as data mapping,
    rounding modes, etc. Automated tests on data logic are highly recommended and
    the more, the better.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Out of the four dimensions, when it comes to creating data tests, timeliness
    and accuracy are more straightforward. Timeliness is achieved by comparing the
    timestamp column with the current timestamp. Accuracy tests are feasible through
    customer queries.
  prefs: []
  type: TYPE_NORMAL
- en: '***Internal view***'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb14c702bc68ab045979558b81807d50.png)'
  prefs: []
  type: TYPE_IMG
- en: Dimensions of internal view (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, the internal view is concerned with the operation that remains
    independent of specific requirements. They are essential regardless of the use
    cases at hand. Dimensions in the internal view are more technical-driven as opposed
    to business-driven dimensions in the external view. It also means that data tests
    are less dependent on consumers and can be automated most of the time. Here are
    a few key perspectives:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Quality of data source:** The quality of the data source significantly impacts
    the overall quality of the final data. The data contract is a great initiative
    to ensure source data quality. As data consumers of the source, we can employ
    a similar approach to monitor the source data as data stakeholders do when evaluating
    the data products.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Completeness:** The extent to which information is retained in its entirety.
    As the complexity of the data pipeline increases, there is a higher likelihood
    of information loss occurring within the intermediate stages. Let’s consider a
    financial system that stores customer transaction data. The completeness test
    ensures that all transactions successfully traverse the entire lifecycle without
    being omitted or left out. For example, the final account balance should accurately
    mirror the real-world situation, capturing every transaction without any omissions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Uniqueness:** This dimension goes hand-in-hand with the completeness test.
    While completeness guarantees that nothing is lost, uniqueness ensures that no
    duplication occurs within the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consistency:** The extent to which data is consistent across internal systems
    on a daily basis. The discrepancy is a common data issue that often stems from
    data silos or inconsistent metric calculation methods. Another aspect of the consistency
    issue occurs between days when data is anticipated to have a steady growth pattern.
    Any deviation should raise a flag for further investigation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s worth noting that each dimension can be associated with one or more data
    tests. What’s crucial is understanding the appropriate application of dimensions
    to specific tables or metrics. Only then, the more tests employed, the better.
  prefs: []
  type: TYPE_NORMAL
- en: Thus far, we’ve discussed the dimensions of external views and internal views.
    In future data test designs, it’s important to consider both the external and
    internal perspectives. By asking the right questions to the right people, we can
    enhance efficiency and reduce miscommunication.
  prefs: []
  type: TYPE_NORMAL
- en: More tips on data tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the last section, I want to share a few practical tips for creating data
    tests. They come from my day-to-day work, but you are more than welcome to share
    more in the comments.
  prefs: []
  type: TYPE_NORMAL
- en: '**Wrong data v.s. No data:** In many data solutions, data tests are typically
    conducted after the model has been updated. This implies that by the time we recognize
    the issue, the data has already become “corrupted”. If you prefer “no data” over
    “wrong data”, you can first materialize the table in a temporary location, subsequently
    conducting the tests. Only if the tests are successfully passed would the pipeline
    proceeds to copy the table to the original destination; otherwise the process
    will be halted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reconciliation test:** A reconciliation test is a data validation process
    that compares the consistency and accuracy of data between two or more systems,
    usually it’s between the source and the destination data set. For example, two
    pipelines are designed to process transactions, it’s worth comparing the total
    transaction amount from both systems and the source. The presence of any discrepancy
    could potentially indicate flaws within the data pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tests with margin:** We might get this statement from the stakeholders: “Having
    0 in this column is acceptable, but an excessive amount should be avoided.” It
    means that they want to capture a deviation from a consistent pattern. Many modern
    data monitoring tools provide anomaly detection features, but if it’s not an option
    for you, you can start with creating tests with margin. For instance, no more
    than 5% of the total rows should have value 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As always, I hope you find this article useful and inspiring. The key takeaway
    is to avoid focusing too much on the quantity of data tests you’ve created. There
    is no point to add the same test on every single column because it will just create
    a lot of noise and decrease your productivity.
  prefs: []
  type: TYPE_NORMAL
- en: Try to have the data quality framework in mind before talking to data consumers
    and data providers. Be smart with your questions and use the framework to inspire
    stakeholders to consider additional perspectives of the data. Cheers!
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Anchoring data quality dimensions in ontological foundations](https://dl.acm.org/doi/pdf/10.1145/240455.240479)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Beyond Accuracy: What Data Quality Means to Data Consumers](http://mitiq.mit.edu/documents/publications/tdqmpub/14_beyond_accuracy.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
