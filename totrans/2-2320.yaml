- en: What are Multimodal models?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯å¤šæ¨¡æ€æ¨¡å‹ï¼Ÿ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/what-are-multimodal-models-fe118f3ef963](https://towardsdatascience.com/what-are-multimodal-models-fe118f3ef963)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/what-are-multimodal-models-fe118f3ef963](https://towardsdatascience.com/what-are-multimodal-models-fe118f3ef963)
- en: Give LLMs the ability to see!
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ‹äºˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§†è§‰èƒ½åŠ›ï¼
- en: '[](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)[![Omer
    Mahmood](../Images/0c87da4134bea397c77bc4ba6640e34b.png)](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)
    [Omer Mahmood](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)[![Omer
    Mahmood](../Images/0c87da4134bea397c77bc4ba6640e34b.png)](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)
    [Omer Mahmood](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)
    Â·6 min readÂ·Oct 16, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)
    Â·é˜…è¯»æ—¶é—´6åˆ†é’ŸÂ·2023å¹´10æœˆ16æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4b7fb0fc89f35adbb6b9d33055d2f0d0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b7fb0fc89f35adbb6b9d33055d2f0d0.png)'
- en: Screenshot of [Mecari text & image embeddings demo](https://atlas.nomic.ai/map/vertex-mercari)
    running on Atlas by Nomic.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Atlas by Nomic ä¸Šè¿è¡Œçš„ [Mecari æ–‡æœ¬ä¸å›¾åƒåµŒå…¥æ¼”ç¤º](https://atlas.nomic.ai/map/vertex-mercari)
    æˆªå›¾ã€‚
- en: Who is this post for?
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿™ç¯‡æ–‡ç« é€‚åˆè°ï¼Ÿ
- en: '**Reader Audience [ğŸŸ¢âšªï¸âšªï¸]:** AI beginners, familiar with popular concepts,
    models and their applications'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¯»è€…ç¾¤ä½“ [ğŸŸ¢âšªï¸âšªï¸]ï¼š** AI åˆå­¦è€…ï¼Œç†Ÿæ‚‰æµè¡Œæ¦‚å¿µã€æ¨¡å‹åŠå…¶åº”ç”¨'
- en: '**Level [ğŸŸ¢ğŸŸ¢ï¸âšªï¸]:** Intermediate topic'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç­‰çº§ [ğŸŸ¢ğŸŸ¢ï¸âšªï¸]ï¼š** ä¸­çº§è¯é¢˜'
- en: '**Complexity [ğŸŸ¢âšªï¸âšªï¸]:** Easy to digest, no mathematical formulas or complex
    theory here'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤æ‚åº¦ [ğŸŸ¢âšªï¸âšªï¸]ï¼š** å®¹æ˜“ç†è§£ï¼Œæ²¡æœ‰æ•°å­¦å…¬å¼æˆ–å¤æ‚ç†è®º'
- en: â“Why It Matters
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: â“ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦
- en: Foundational large language models (LLMs), pre-trained on huge datasets are
    pretty efficient at handling generic, multi-tasking via prompts through zero-shot,
    few-shot or transfer learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºç¡€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç»è¿‡åœ¨åºå¤§æ•°æ®é›†ä¸Šçš„é¢„è®­ç»ƒï¼Œåœ¨é€šè¿‡é›¶-shotã€few-shot æˆ–è¿ç§»å­¦ä¹ çš„æç¤ºä¸‹å¤„ç†é€šç”¨ã€å¤šä»»åŠ¡æ—¶ç›¸å½“é«˜æ•ˆã€‚
- en: Indeed, examples of these models like [PaLM2](https://ai.google/discover/palm2/)
    and [GPT4](https://openai.com/research/gpt-4) have revolutionised the way we interact
    with computers **using text as an input**, butâ€¦
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®å®ï¼Œè¿™äº›æ¨¡å‹çš„ä¾‹å­å¦‚ [PaLM2](https://ai.google/discover/palm2/) å’Œ [GPT4](https://openai.com/research/gpt-4)
    å·²ç»å½»åº•æ”¹å˜äº†æˆ‘ä»¬ä¸è®¡ç®—æœºäº¤äº’çš„æ–¹å¼ï¼Œ**ä½¿ç”¨æ–‡æœ¬ä½œä¸ºè¾“å…¥**ï¼Œä½†æ˜¯â€¦â€¦
- en: What if there was a way to extend the intelligence of these models, by enabling
    them to use different modalities of input, such as photos, audio, and video? **Or
    in other words, make them Multimodal!**
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæœ‰ä¸€ç§æ–¹æ³•å¯ä»¥é€šè¿‡ä½¿è¿™äº›æ¨¡å‹ä½¿ç”¨ä¸åŒçš„è¾“å…¥æ¨¡æ€ï¼Œå¦‚ç…§ç‰‡ã€éŸ³é¢‘å’Œè§†é¢‘ï¼Œæ¥æ‰©å±•å®ƒä»¬çš„æ™ºèƒ½ä¼šæ€æ ·ï¼Ÿ**æ¢å¥è¯è¯´ï¼Œè®©å®ƒä»¬å˜æˆå¤šæ¨¡æ€çš„ï¼**
- en: It could greatly improve how we search for things on the web, or even understand
    the world around us for example in real world applications such as medicine and
    pathology.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™å¯èƒ½ä¼šå¤§å¤§æ”¹å–„æˆ‘ä»¬åœ¨ç½‘ç»œä¸Šæœç´¢ä¿¡æ¯çš„æ–¹å¼ï¼Œç”šè‡³åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­ï¼Œä¾‹å¦‚åŒ»å­¦å’Œç—…ç†å­¦ï¼Œå¸®åŠ©æˆ‘ä»¬ç†è§£å‘¨å›´çš„ä¸–ç•Œã€‚
- en: There is a solution! Multimodal deep learning models can combine the embeddings
    from different types of input, enabling, for example, an LLM to â€œseeâ€ what you
    are asking for, and return relevant results.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ‰è§£å†³æ–¹æ¡ˆï¼å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¨¡å‹å¯ä»¥ç»“åˆä¸åŒç±»å‹è¾“å…¥çš„åµŒå…¥ï¼Œä½¿å¾—ä¾‹å¦‚ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿâ€œçœ‹åˆ°â€ä½ æ‰€è¯¢é—®çš„å†…å®¹ï¼Œå¹¶è¿”å›ç›¸å…³ç»“æœã€‚
- en: '**âš¡ï¸Stick around if** you want to learn more about how this all works and play
    around with a working demo!'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**âš¡ï¸ç»§ç»­å…³æ³¨** å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºè¿™ä¸€åˆ‡å¦‚ä½•è¿ä½œçš„çŸ¥è¯†ï¼Œå¹¶è¯•ç”¨ä¸€ä¸ªå®é™…çš„æ¼”ç¤ºï¼'
- en: ğŸ”¥ How does it work?
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ”¥ è¿™å¦‚ä½•è¿ä½œï¼Ÿ
- en: '**It starts with embeddings**'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**å®ƒä»åµŒå…¥å¼€å§‹**'
- en: One of the most powerful building blocks of training deep learning models is
    the creation of embedding vectors.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹çš„æœ€å¼ºå¤§æ„å»ºæ¨¡å—ä¹‹ä¸€æ˜¯åˆ›å»ºåµŒå…¥å‘é‡ã€‚
- en: During training, the model encodes the different categories (for example, people,
    foods, and toys) it encounters into their numerical representation aka. an Embedding,
    that is stored as a vector of numbers.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å°†é‡åˆ°çš„ä¸åŒç±»åˆ«ï¼ˆä¾‹å¦‚ï¼Œäººã€é£Ÿç‰©å’Œç©å…·ï¼‰ç¼–ç æˆå…¶æ•°å€¼è¡¨ç¤ºï¼Œå³åµŒå…¥ï¼Œè¿™äº›åµŒå…¥å­˜å‚¨ä¸ºæ•°å­—å‘é‡ã€‚
- en: Embeddings are useful when we want to move from a sparse representation of category
    (or class) for example a long string of text or an image, to something that is
    more compact, and can be reused across other models too.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åµŒå…¥åœ¨æˆ‘ä»¬æƒ³ä»ç±»åˆ«ï¼ˆæˆ–ç±»ï¼‰çš„ç¨€ç–è¡¨ç¤ºï¼ˆä¾‹å¦‚ä¸€é•¿ä¸²æ–‡æœ¬æˆ–å›¾åƒï¼‰è½¬å˜ä¸ºæ›´ç´§å‡‘çš„å½¢å¼ï¼Œå¹¶ä¸”å¯ä»¥åœ¨å…¶ä»–æ¨¡å‹ä¸­é‡ç”¨æ—¶éå¸¸æœ‰ç”¨ã€‚
- en: Put simply, Embeddings provide a way for us to store the meaning of things in
    a shorthand form, that machines can use to quickly compare and search against!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼ŒåµŒå…¥ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç§ä»¥ç®€å†™å½¢å¼å­˜å‚¨äº‹ç‰©å«ä¹‰çš„æ–¹æ³•ï¼Œæœºå™¨å¯ä»¥ç”¨æ¥å¿«é€Ÿæ¯”è¾ƒå’Œæœç´¢ï¼
- en: To be a bit more concrete, with vision models trained on different images, images
    with a similar appearance and meaning will be placed closely together in the embedding
    space it creates.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å…·ä½“åœ°è¯´ï¼Œé€šè¿‡å¯¹ä¸åŒå›¾åƒè¿›è¡Œè®­ç»ƒçš„è§†è§‰æ¨¡å‹ï¼Œå…·æœ‰ç›¸ä¼¼å¤–è§‚å’Œå«ä¹‰çš„å›¾åƒå°†åœ¨å®ƒåˆ›å»ºçš„åµŒå…¥ç©ºé—´ä¸­è¢«ç´§å¯†æ”¾ç½®åœ¨ä¸€èµ·ã€‚
- en: '![](../Images/c7ec53cf44032053dd9980ef256f49a2.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7ec53cf44032053dd9980ef256f49a2.png)'
- en: '*Figure 1: Visualisation of embeddings that represent different categories
    of images, and similar images in the same embedding space. Illustration by the
    author.*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 1ï¼šè¡¨ç¤ºä¸åŒç±»åˆ«å›¾åƒå’ŒåŒä¸€åµŒå…¥ç©ºé—´ä¸­ç›¸ä¼¼å›¾åƒçš„åµŒå…¥å¯è§†åŒ–ã€‚ç”±ä½œè€…æ’å›¾ã€‚*'
- en: â€œThe model can map an image to an embedding, which is a location in the space.
    Therefore, if you look around the embedding, you can find other images with similar
    appearance and meaning. This is how image similarity search works!â€[1]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: â€œæ¨¡å‹å¯ä»¥å°†å›¾åƒæ˜ å°„åˆ°åµŒå…¥ï¼Œè¿™æ˜¯ç©ºé—´ä¸­çš„ä¸€ä¸ªä½ç½®ã€‚å› æ­¤ï¼Œå¦‚æœä½ åœ¨åµŒå…¥å‘¨å›´æŸ¥çœ‹ï¼Œä½ å¯ä»¥æ‰¾åˆ°å…¶ä»–å…·æœ‰ç›¸ä¼¼å¤–è§‚å’Œå«ä¹‰çš„å›¾åƒã€‚è¿™å°±æ˜¯å›¾åƒç›¸ä¼¼æ€§æœç´¢çš„å·¥ä½œåŸç†ï¼â€[1]
- en: '**Training on image and text pairs**'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**å›¾åƒå’Œæ–‡æœ¬å¯¹çš„è®­ç»ƒ**'
- en: 'In a similar fashion to the previous example, deep learning models can also
    be trained using related pairings of text and images. This happens using a combination
    of other models, as illustrated in the Figure below:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äºå‰é¢çš„ç¤ºä¾‹ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹ä¹Ÿå¯ä»¥é€šè¿‡ç›¸å…³çš„æ–‡æœ¬å’Œå›¾åƒé…å¯¹è¿›è¡Œè®­ç»ƒã€‚è¿™æ˜¯é€šè¿‡ç»“åˆå…¶ä»–æ¨¡å‹æ¥å®ç°çš„ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
- en: '![](../Images/4fd63d2d4fcf69eae568d4383c13edc2.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fd63d2d4fcf69eae568d4383c13edc2.png)'
- en: '*Figure 2: Using a combination of models to train on image and text pairs aka
    â€œCoCaâ€. Illustration by the author.*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 2ï¼šä½¿ç”¨æ¨¡å‹ç»„åˆå¯¹å›¾åƒå’Œæ–‡æœ¬å¯¹è¿›è¡Œè®­ç»ƒï¼Œå³â€œCoCaâ€ã€‚ç”±ä½œè€…æ’å›¾ã€‚*'
- en: 'Letâ€™s breakdown whatâ€™s happening above, when we pass in an image and text pair:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥è§£æä¸€ä¸‹å½“æˆ‘ä»¬ä¼ å…¥ä¸€ä¸ªå›¾åƒå’Œæ–‡æœ¬å¯¹æ—¶å‘ç”Ÿäº†ä»€ä¹ˆï¼š
- en: '**Image Encoder:** a model to obtain image embeddings,'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å›¾åƒç¼–ç å™¨ï¼š** ç”¨äºè·å–å›¾åƒåµŒå…¥çš„æ¨¡å‹ï¼Œ'
- en: '**Unimodel Text Encoder:** a text model to obtain text embeddings. This model
    is fed by the Image Encoder model, taking source image embeddings as input and
    to produce a representation of the sequence of images and text pairs aka. **Self
    Attention**.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å•æ¨¡æ€æ–‡æœ¬ç¼–ç å™¨ï¼š** ä¸€ç§ç”¨äºè·å–æ–‡æœ¬åµŒå…¥çš„æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç”±å›¾åƒç¼–ç å™¨æ¨¡å‹è¾“å…¥ï¼Œæ¥å—æºå›¾åƒåµŒå…¥ä½œä¸ºè¾“å…¥ï¼Œäº§ç”Ÿå›¾åƒå’Œæ–‡æœ¬å¯¹çš„åºåˆ—è¡¨ç¤ºï¼Œå³**è‡ªæ³¨æ„åŠ›**ã€‚'
- en: '**Multimodal Text Encoder:** a model to learn the relationships between them.
    More formally, the technique is called **Cross Attention**: an attention mechanism
    in Transformer architecture that mixes two different embedding sequences, or in
    our case images and the text that describes them.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤šæ¨¡æ€æ–‡æœ¬ç¼–ç å™¨ï¼š** ä¸€ç§å­¦ä¹ å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´å…³ç³»çš„æ¨¡å‹ã€‚æ›´æ­£å¼åœ°è¯´ï¼Œè¿™ç§æŠ€æœ¯ç§°ä¸º**äº¤å‰æ³¨æ„åŠ›**ï¼šä¸€ç§åœ¨ Transformer æ¶æ„ä¸­æ··åˆä¸¤ç§ä¸åŒåµŒå…¥åºåˆ—çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œæˆ–è€…åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­æ˜¯å›¾åƒå’Œæè¿°å®ƒä»¬çš„æ–‡æœ¬ã€‚'
- en: This design, or one way of combining models is known as Contrastive Captioning
    or simply â€˜CoCaâ€™ see [here](https://arxiv.org/abs/2205.01917) if you want to delve
    deeper!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è®¾è®¡ï¼Œæˆ–ç»“åˆæ¨¡å‹çš„ä¸€ç§æ–¹å¼ï¼Œè¢«ç§°ä¸ºå¯¹æ¯”å­—å¹•æˆ–ç®€å•åœ°â€˜CoCaâ€™ï¼Œå¦‚æœä½ æƒ³æ·±å…¥äº†è§£ï¼Œå¯ä»¥æŸ¥çœ‹[è¿™é‡Œ](https://arxiv.org/abs/2205.01917)ï¼
- en: '**â€œThe result: a foundational â€˜Vision Language Model (VLM)â€™â€¦**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**â€œç»“æœï¼šä¸€ä¸ªåŸºç¡€çš„â€˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰â€™â€¦**'
- en: Where we have built a shared embedding space for images and texts with a fixed
    dimension, organized by their meanings. In this space, images and text with similar
    meanings are placed close together.â€[1]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå…·æœ‰å›ºå®šç»´åº¦çš„å…±äº«åµŒå…¥ç©ºé—´ï¼Œç”¨äºå›¾åƒå’Œæ–‡æœ¬ï¼ŒæŒ‰ç…§å®ƒä»¬çš„å«ä¹‰ç»„ç»‡ã€‚åœ¨è¿™ä¸ªç©ºé—´ä¸­ï¼Œå«ä¹‰ç›¸ä¼¼çš„å›¾åƒå’Œæ–‡æœ¬è¢«æ”¾åœ¨ä¸€èµ·ã€‚â€[1]
- en: '![](../Images/a6598a08a9054ae15d9ab90bf549733c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a6598a08a9054ae15d9ab90bf549733c.png)'
- en: '*Figure 3: Visualisation of embeddings for related images and text. Illustration
    by the author.*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 3ï¼šç›¸å…³å›¾åƒå’Œæ–‡æœ¬çš„åµŒå…¥å¯è§†åŒ–ã€‚ç”±ä½œè€…æ’å›¾ã€‚*'
- en: This means that you can search for images based on text (text-to-image search)
    or search for text based on images (image-to-text search).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€ä½ å¯ä»¥æ ¹æ®æ–‡æœ¬æœç´¢å›¾åƒï¼ˆæ–‡æœ¬åˆ°å›¾åƒæœç´¢ï¼‰ï¼Œæˆ–è€…æ ¹æ®å›¾åƒæœç´¢æ–‡æœ¬ï¼ˆå›¾åƒåˆ°æ–‡æœ¬æœç´¢ï¼‰ã€‚
- en: This is the basic idea behind how Google Search finds relevant results across
    images and text.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ Google æœç´¢å¦‚ä½•åœ¨å›¾åƒå’Œæ–‡æœ¬ä¸­æ‰¾åˆ°ç›¸å…³ç»“æœçš„åŸºæœ¬æ€è·¯ã€‚
- en: ğŸ” Try a quick demo!
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ” è¯•ä¸€ä¸‹å¿«é€Ÿæ¼”ç¤ºï¼
- en: Want to see how text and image embeddings work in a real world application?
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æƒ³çœ‹çœ‹æ–‡æœ¬å’Œå›¾åƒåµŒå…¥åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æ•ˆæœå—ï¼Ÿ
- en: You can play around for free using a demo running on Altas by Nomic.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥é€šè¿‡ä½¿ç”¨åœ¨ Nomic æä¾›çš„ Altas ä¸Šè¿è¡Œçš„æ¼”ç¤ºå…è´¹è¿›è¡Œä½“éªŒã€‚
- en: â€œAtlas enables you to Interact, discover insights and build with unstructured
    text, image and audio data.â€[2]
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: â€œAtlas ä½¿ä½ èƒ½å¤Ÿä¸éç»“æ„åŒ–æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘æ•°æ®è¿›è¡Œäº’åŠ¨ã€å‘ç°æ´å¯Ÿå¹¶è¿›è¡Œæ„å»ºã€‚â€[2]
- en: Here is a search demo that brings together text and image embeddings based on
    items available on the Mercari website (a Japanese e-commerce retailer).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€ä¸ªæœç´¢æ¼”ç¤ºï¼Œå°† Mercari ç½‘ç«™ï¼ˆä¸€ä¸ªæ—¥æœ¬ç”µå­å•†åŠ¡é›¶å”®å•†ï¼‰ä¸Šå¯ç”¨çš„é¡¹ç›®çš„æ–‡æœ¬å’Œå›¾åƒåµŒå…¥ç»“åˆåœ¨ä¸€èµ·ã€‚
- en: '1\. Navigate to: [https://atlas.nomic.ai/map/vertex-mercari](https://atlas.nomic.ai/map/vertex-mercari)
    (you might need to create a free account to gain access).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. å¯¼èˆªè‡³ï¼š[https://atlas.nomic.ai/map/vertex-mercari](https://atlas.nomic.ai/map/vertex-mercari)ï¼ˆä½ å¯èƒ½éœ€è¦åˆ›å»ºä¸€ä¸ªå…è´¹è´¦æˆ·ä»¥è·å¾—è®¿é—®æƒé™ï¼‰ã€‚
- en: '![](../Images/ce2cc59b9cacf30d4972defb136e1780.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce2cc59b9cacf30d4972defb136e1780.png)'
- en: 'Figure 4: Screenshot of Atlas by Nomic user interface, running Mecari embeddings
    demo.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4ï¼šNomic çš„ Atlas ç”¨æˆ·ç•Œé¢çš„æˆªå›¾ï¼Œè¿è¡Œ Mecari åµŒå…¥æ¼”ç¤ºã€‚
- en: 2\. You can move your mouse cursor around the â€˜dot-cloudâ€™ vizualisation that
    represents the embedding space you can search against.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. ä½ å¯ä»¥åœ¨è¡¨ç¤ºåµŒå…¥ç©ºé—´çš„â€œç‚¹äº‘â€å¯è§†åŒ–ä¸Šç§»åŠ¨é¼ æ ‡å…‰æ ‡ï¼Œä»¥è¿›è¡Œæœç´¢ã€‚
- en: 3\. Or in the â€œSelection Toolsâ€ in the top-right corner of the interface you
    can type a text search query, filter by certain attributes or â€˜lassoâ€™ your own
    selection.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç•Œé¢å³ä¸Šè§’çš„â€œé€‰æ‹©å·¥å…·â€ä¸­ï¼Œä½ å¯ä»¥è¾“å…¥æ–‡æœ¬æœç´¢æŸ¥è¯¢ï¼ŒæŒ‰æŸäº›å±æ€§è¿›è¡Œç­›é€‰æˆ–ä½¿ç”¨â€˜å¥—ç´¢â€™å·¥å…·è¿›è¡Œè‡ªå®šä¹‰é€‰æ‹©ã€‚
- en: It really is that simple! If you want to learn more about how it was put together,
    take a look [here](https://ai-demos.dev/).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: çœŸçš„å°±è¿™ä¹ˆç®€å•ï¼å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºå®ƒçš„ç»„è£…è¿‡ç¨‹ï¼Œå¯ä»¥æŸ¥çœ‹ [è¿™é‡Œ](https://ai-demos.dev/)ã€‚
- en: ğŸ± Multimodal LLMs
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ± å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹
- en: Yes, thereâ€™s more to it than what we have discussed. In the case of LLMs, complex
    [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    architectures are used to convert text and other data to vectors and back using
    tokenization, positional encoding, and embedding layers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œè¿™æ¯”æˆ‘ä»¬è®¨è®ºçš„å†…å®¹è¿˜è¦å¤šã€‚å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¤æ‚çš„ [å˜æ¢å™¨](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    æ¶æ„ç”¨äºé€šè¿‡æ ‡è®°åŒ–ã€ä½ç½®ç¼–ç å’ŒåµŒå…¥å±‚å°†æ–‡æœ¬å’Œå…¶ä»–æ•°æ®è½¬æ¢ä¸ºå‘é‡å¹¶è¿›è¡Œåå‘è½¬æ¢ã€‚
- en: But broadly speaking, the same principle weâ€™ve discussed earlier, enables us
    to use images to complement the text prompts we pass to LLMs aka. Multimodal LLMs,
    which let the user specify any vision or language task.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬ä¹‹å‰è®¨è®ºè¿‡çš„ç›¸åŒåŸç†ï¼Œä½¿å¾—æˆ‘ä»¬å¯ä»¥åˆ©ç”¨å›¾åƒæ¥è¡¥å……ä¼ é€’ç»™å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå³å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼‰çš„æ–‡æœ¬æç¤ºï¼Œè¿™å…è®¸ç”¨æˆ·æŒ‡å®šä»»ä½•è§†è§‰æˆ–è¯­è¨€ä»»åŠ¡ã€‚
- en: Multimodal LLMs are a recent and powerful development, examples such [GPT-4V](https://openai.com/research/gpt-4)
    and [MedPalm M](https://medika.life/a-new-era-in-medical-ai-the-power-of-med-palm-m/)
    are pushing the boundaries of how we use LLMs because they can provide rich responses
    based on image and text inputs.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æ˜¯æœ€è¿‘çš„ä¸€ä¸ªå¼ºå¤§å‘å±•ï¼Œåƒ [GPT-4V](https://openai.com/research/gpt-4) å’Œ [MedPalm M](https://medika.life/a-new-era-in-medical-ai-the-power-of-med-palm-m/)
    è¿™æ ·çš„ä¾‹å­æ­£åœ¨æ¨åŠ¨æˆ‘ä»¬ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è¾¹ç•Œï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥æ ¹æ®å›¾åƒå’Œæ–‡æœ¬è¾“å…¥æä¾›ä¸°å¯Œçš„å“åº”ã€‚
- en: For example, MedPalm M makes it possible to ask the model to diagnose a patient
    based on an X-ray image, with results that would rival a qualified physician!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼ŒMedPalm M ä½¿å¾—åŸºäº X å°„çº¿å›¾åƒè®©æ¨¡å‹å¯¹æ‚£è€…è¿›è¡Œè¯Šæ–­æˆä¸ºå¯èƒ½ï¼Œç»“æœå¯ä¸åˆæ ¼çš„åŒ»ç”Ÿç›¸åª²ç¾ï¼
- en: It is important to note that Multimodal LLMs are not perfect and can still be
    susceptible to the limitations of earlier models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦çš„æ˜¯è¦æ³¨æ„ï¼Œå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å¹¶ä¸å®Œç¾ï¼Œä»ç„¶å¯èƒ½å—åˆ°æ—©æœŸæ¨¡å‹å±€é™æ€§çš„å½±å“ã€‚
- en: They are still not fully reliable (i.e. they can â€œhallucinateâ€ facts and make
    reasoning errors). Great care should be taken when using language model outputs,
    particularly in high-stakes contexts, such as medical diagnosis.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬ä»ç„¶ä¸æ˜¯å®Œå…¨å¯é çš„ï¼ˆå³ï¼Œå®ƒä»¬å¯èƒ½â€œå¹»æƒ³â€äº‹å®å¹¶äº§ç”Ÿæ¨ç†é”™è¯¯ï¼‰ã€‚åœ¨ä½¿ç”¨è¯­è¨€æ¨¡å‹è¾“å‡ºæ—¶ï¼Œå°¤å…¶æ˜¯åœ¨é«˜é£é™©æƒ…å¢ƒä¸‹ï¼ˆå¦‚åŒ»ç–—è¯Šæ–­ï¼‰ï¼Œåº”ç‰¹åˆ«å°å¿ƒã€‚
- en: Ensuring that appropriate safeguards and processes are in place is critical
    (such as human review, grounding with additional context, or even avoiding high-stakes
    use cases altogether).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿é€‚å½“çš„ä¿æŠ¤æªæ–½å’Œæµç¨‹åˆ°ä½æ˜¯è‡³å…³é‡è¦çš„ï¼ˆä¾‹å¦‚äººå·¥å®¡æ ¸ã€é™„åŠ èƒŒæ™¯ä¿¡æ¯çš„åŸºç¡€ï¼Œæˆ–ç”šè‡³å®Œå…¨é¿å…é«˜é£é™©ç”¨ä¾‹ï¼‰ã€‚
- en: This is an exciting space representing the next evolution of LLMs!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªä»¤äººå…´å¥‹çš„é¢†åŸŸï¼Œä»£è¡¨äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸‹ä¸€æ¬¡è¿›åŒ–ï¼
- en: ğŸ The wrap up
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ æ€»ç»“
- en: 'In this post we introduced:'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ï¼š
- en: How embeddings are used to enable models to capture (encode) input data they
    are trained on.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åµŒå…¥æ˜¯å¦‚ä½•è¢«ç”¨äºä½¿æ¨¡å‹æ•æ‰ï¼ˆç¼–ç ï¼‰å®ƒä»¬è®­ç»ƒæ—¶è¾“å…¥çš„æ•°æ®çš„ã€‚
- en: The concepts of Multimodal deep learning models that are typically composed
    of multiple unimodal neural networks, which process each input modality separately.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ¦‚å¿µé€šå¸¸ç”±å¤šä¸ªå•æ¨¡æ€ç¥ç»ç½‘ç»œç»„æˆï¼Œè¿™äº›ç½‘ç»œåˆ†åˆ«å¤„ç†æ¯ç§è¾“å…¥æ¨¡æ€ã€‚
- en: A simple demo incorporating text and image embeddings to illustrate the Multimodal
    Search use case when searching for products on a website.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç®€å•çš„æ¼”ç¤ºï¼Œç»“åˆæ–‡æœ¬å’Œå›¾åƒåµŒå…¥ï¼Œä»¥è¯´æ˜åœ¨ç½‘ç«™ä¸Šæœç´¢äº§å“æ—¶çš„å¤šæ¨¡æ€æœç´¢ç”¨ä¾‹ã€‚
- en: Finally we also discussed some potential use cases and indeed real world examples
    of Multimodal LLMs.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†ä¸€äº›æ½œåœ¨çš„ç”¨ä¾‹ä»¥åŠå®é™…çš„å¤šæ¨¡æ€LLMåº”ç”¨å®ä¾‹ã€‚
- en: '**ğŸ‘‹ğŸ¼Thanks for stopping by, I hope you enjoyed this post, and Iâ€™ll see you
    on the next!**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ‘‹ğŸ¼æ„Ÿè°¢ä½ çš„å…‰ä¸´ï¼Œå¸Œæœ›ä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œæˆ‘ä»¬ä¸‹æ¬¡è§ï¼**'
- en: ğŸ“š References & Further reading
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ“š å‚è€ƒèµ„æ–™åŠè¿›ä¸€æ­¥é˜…è¯»
- en: '[1] â€œWhat is Multimodal Search: â€˜LLMs with visionâ€™ change businessesâ€ -[https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search](https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] â€œä»€ä¹ˆæ˜¯å¤šæ¨¡æ€æœç´¢ï¼šâ€˜å…·æœ‰è§†è§‰èƒ½åŠ›çš„LLMâ€™å¦‚ä½•æ”¹å˜ä¸šåŠ¡â€ - [https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search](https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search)'
- en: '[2] [https://atlas.nomic.ai/](https://atlas.nomic.ai/)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://atlas.nomic.ai/](https://atlas.nomic.ai/)'
- en: '**Publications:**'
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**å‡ºç‰ˆç‰©ï¼š**'
- en: 'â€œTowards Generalist Biomedical AIâ€ aka. The MedPalm M paper, arXiv:2307.14334v1
    [cs.CL] 26 Jul 2023: [https://arxiv.org/pdf/2307.14334.pdf?ref=maginative.com](https://arxiv.org/pdf/2307.14334.pdf?ref=maginative.com)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'â€œé¢å‘é€šç”¨ç”Ÿç‰©åŒ»å­¦AIâ€åˆåMedPalm Mè®ºæ–‡ï¼ŒarXiv:2307.14334v1 [cs.CL] 2023å¹´7æœˆ26æ—¥: [https://arxiv.org/pdf/2307.14334.pdf?ref=maginative.com](https://arxiv.org/pdf/2307.14334.pdf?ref=maginative.com)'
- en: 'â€œCoCa: Contrastive Captioners are Image-Text Foundation Modelsâ€, arXiv:2205.01917v2
    [cs.CV] 14 Jun 2022: [https://arxiv.org/pdf/2205.01917.pdf](https://arxiv.org/pdf/2205.01917.pdf)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'â€œCoCa: å¯¹æ¯”å¼å­—å¹•ç”Ÿæˆå™¨æ˜¯å›¾åƒ-æ–‡æœ¬åŸºç¡€æ¨¡å‹â€ï¼ŒarXiv:2205.01917v2 [cs.CV] 2022å¹´6æœˆ14æ—¥: [https://arxiv.org/pdf/2205.01917.pdf](https://arxiv.org/pdf/2205.01917.pdf)'
