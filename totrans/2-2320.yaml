- en: What are Multimodal models?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/what-are-multimodal-models-fe118f3ef963](https://towardsdatascience.com/what-are-multimodal-models-fe118f3ef963)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Give LLMs the ability to see!
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)[![Omer
    Mahmood](../Images/0c87da4134bea397c77bc4ba6640e34b.png)](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)
    [Omer Mahmood](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)
    Â·6 min readÂ·Oct 16, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b7fb0fc89f35adbb6b9d33055d2f0d0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Screenshot of [Mecari text & image embeddings demo](https://atlas.nomic.ai/map/vertex-mercari)
    running on Atlas by Nomic.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Who is this post for?
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Reader Audience [ğŸŸ¢âšªï¸âšªï¸]:** AI beginners, familiar with popular concepts,
    models and their applications'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Level [ğŸŸ¢ğŸŸ¢ï¸âšªï¸]:** Intermediate topic'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complexity [ğŸŸ¢âšªï¸âšªï¸]:** Easy to digest, no mathematical formulas or complex
    theory here'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: â“Why It Matters
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Foundational large language models (LLMs), pre-trained on huge datasets are
    pretty efficient at handling generic, multi-tasking via prompts through zero-shot,
    few-shot or transfer learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, examples of these models like [PaLM2](https://ai.google/discover/palm2/)
    and [GPT4](https://openai.com/research/gpt-4) have revolutionised the way we interact
    with computers **using text as an input**, butâ€¦
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: What if there was a way to extend the intelligence of these models, by enabling
    them to use different modalities of input, such as photos, audio, and video? **Or
    in other words, make them Multimodal!**
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It could greatly improve how we search for things on the web, or even understand
    the world around us for example in real world applications such as medicine and
    pathology.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a solution! Multimodal deep learning models can combine the embeddings
    from different types of input, enabling, for example, an LLM to â€œseeâ€ what you
    are asking for, and return relevant results.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**âš¡ï¸Stick around if** you want to learn more about how this all works and play
    around with a working demo!'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ”¥ How does it work?
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**It starts with embeddings**'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most powerful building blocks of training deep learning models is
    the creation of embedding vectors.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: During training, the model encodes the different categories (for example, people,
    foods, and toys) it encounters into their numerical representation aka. an Embedding,
    that is stored as a vector of numbers.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings are useful when we want to move from a sparse representation of category
    (or class) for example a long string of text or an image, to something that is
    more compact, and can be reused across other models too.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Put simply, Embeddings provide a way for us to store the meaning of things in
    a shorthand form, that machines can use to quickly compare and search against!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: To be a bit more concrete, with vision models trained on different images, images
    with a similar appearance and meaning will be placed closely together in the embedding
    space it creates.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7ec53cf44032053dd9980ef256f49a2.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1: Visualisation of embeddings that represent different categories
    of images, and similar images in the same embedding space. Illustration by the
    author.*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: â€œThe model can map an image to an embedding, which is a location in the space.
    Therefore, if you look around the embedding, you can find other images with similar
    appearance and meaning. This is how image similarity search works!â€[1]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '**Training on image and text pairs**'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a similar fashion to the previous example, deep learning models can also
    be trained using related pairings of text and images. This happens using a combination
    of other models, as illustrated in the Figure below:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4fd63d2d4fcf69eae568d4383c13edc2.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2: Using a combination of models to train on image and text pairs aka
    â€œCoCaâ€. Illustration by the author.*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s breakdown whatâ€™s happening above, when we pass in an image and text pair:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '**Image Encoder:** a model to obtain image embeddings,'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unimodel Text Encoder:** a text model to obtain text embeddings. This model
    is fed by the Image Encoder model, taking source image embeddings as input and
    to produce a representation of the sequence of images and text pairs aka. **Self
    Attention**.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multimodal Text Encoder:** a model to learn the relationships between them.
    More formally, the technique is called **Cross Attention**: an attention mechanism
    in Transformer architecture that mixes two different embedding sequences, or in
    our case images and the text that describes them.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This design, or one way of combining models is known as Contrastive Captioning
    or simply â€˜CoCaâ€™ see [here](https://arxiv.org/abs/2205.01917) if you want to delve
    deeper!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '**â€œThe result: a foundational â€˜Vision Language Model (VLM)â€™â€¦**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Where we have built a shared embedding space for images and texts with a fixed
    dimension, organized by their meanings. In this space, images and text with similar
    meanings are placed close together.â€[1]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a6598a08a9054ae15d9ab90bf549733c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3: Visualisation of embeddings for related images and text. Illustration
    by the author.*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: This means that you can search for images based on text (text-to-image search)
    or search for text based on images (image-to-text search).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: This is the basic idea behind how Google Search finds relevant results across
    images and text.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ” Try a quick demo!
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Want to see how text and image embeddings work in a real world application?
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æƒ³çœ‹çœ‹æ–‡æœ¬å’Œå›¾åƒåµŒå…¥åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æ•ˆæœå—ï¼Ÿ
- en: You can play around for free using a demo running on Altas by Nomic.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥é€šè¿‡ä½¿ç”¨åœ¨ Nomic æä¾›çš„ Altas ä¸Šè¿è¡Œçš„æ¼”ç¤ºå…è´¹è¿›è¡Œä½“éªŒã€‚
- en: â€œAtlas enables you to Interact, discover insights and build with unstructured
    text, image and audio data.â€[2]
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: â€œAtlas ä½¿ä½ èƒ½å¤Ÿä¸éç»“æ„åŒ–æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘æ•°æ®è¿›è¡Œäº’åŠ¨ã€å‘ç°æ´å¯Ÿå¹¶è¿›è¡Œæ„å»ºã€‚â€[2]
- en: Here is a search demo that brings together text and image embeddings based on
    items available on the Mercari website (a Japanese e-commerce retailer).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€ä¸ªæœç´¢æ¼”ç¤ºï¼Œå°† Mercari ç½‘ç«™ï¼ˆä¸€ä¸ªæ—¥æœ¬ç”µå­å•†åŠ¡é›¶å”®å•†ï¼‰ä¸Šå¯ç”¨çš„é¡¹ç›®çš„æ–‡æœ¬å’Œå›¾åƒåµŒå…¥ç»“åˆåœ¨ä¸€èµ·ã€‚
- en: '1\. Navigate to: [https://atlas.nomic.ai/map/vertex-mercari](https://atlas.nomic.ai/map/vertex-mercari)
    (you might need to create a free account to gain access).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. å¯¼èˆªè‡³ï¼š[https://atlas.nomic.ai/map/vertex-mercari](https://atlas.nomic.ai/map/vertex-mercari)ï¼ˆä½ å¯èƒ½éœ€è¦åˆ›å»ºä¸€ä¸ªå…è´¹è´¦æˆ·ä»¥è·å¾—è®¿é—®æƒé™ï¼‰ã€‚
- en: '![](../Images/ce2cc59b9cacf30d4972defb136e1780.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce2cc59b9cacf30d4972defb136e1780.png)'
- en: 'Figure 4: Screenshot of Atlas by Nomic user interface, running Mecari embeddings
    demo.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4ï¼šNomic çš„ Atlas ç”¨æˆ·ç•Œé¢çš„æˆªå›¾ï¼Œè¿è¡Œ Mecari åµŒå…¥æ¼”ç¤ºã€‚
- en: 2\. You can move your mouse cursor around the â€˜dot-cloudâ€™ vizualisation that
    represents the embedding space you can search against.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. ä½ å¯ä»¥åœ¨è¡¨ç¤ºåµŒå…¥ç©ºé—´çš„â€œç‚¹äº‘â€å¯è§†åŒ–ä¸Šç§»åŠ¨é¼ æ ‡å…‰æ ‡ï¼Œä»¥è¿›è¡Œæœç´¢ã€‚
- en: 3\. Or in the â€œSelection Toolsâ€ in the top-right corner of the interface you
    can type a text search query, filter by certain attributes or â€˜lassoâ€™ your own
    selection.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç•Œé¢å³ä¸Šè§’çš„â€œé€‰æ‹©å·¥å…·â€ä¸­ï¼Œä½ å¯ä»¥è¾“å…¥æ–‡æœ¬æœç´¢æŸ¥è¯¢ï¼ŒæŒ‰æŸäº›å±æ€§è¿›è¡Œç­›é€‰æˆ–ä½¿ç”¨â€˜å¥—ç´¢â€™å·¥å…·è¿›è¡Œè‡ªå®šä¹‰é€‰æ‹©ã€‚
- en: It really is that simple! If you want to learn more about how it was put together,
    take a look [here](https://ai-demos.dev/).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: çœŸçš„å°±è¿™ä¹ˆç®€å•ï¼å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºå®ƒçš„ç»„è£…è¿‡ç¨‹ï¼Œå¯ä»¥æŸ¥çœ‹ [è¿™é‡Œ](https://ai-demos.dev/)ã€‚
- en: ğŸ± Multimodal LLMs
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ± å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹
- en: Yes, thereâ€™s more to it than what we have discussed. In the case of LLMs, complex
    [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    architectures are used to convert text and other data to vectors and back using
    tokenization, positional encoding, and embedding layers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œè¿™æ¯”æˆ‘ä»¬è®¨è®ºçš„å†…å®¹è¿˜è¦å¤šã€‚å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¤æ‚çš„ [å˜æ¢å™¨](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    æ¶æ„ç”¨äºé€šè¿‡æ ‡è®°åŒ–ã€ä½ç½®ç¼–ç å’ŒåµŒå…¥å±‚å°†æ–‡æœ¬å’Œå…¶ä»–æ•°æ®è½¬æ¢ä¸ºå‘é‡å¹¶è¿›è¡Œåå‘è½¬æ¢ã€‚
- en: But broadly speaking, the same principle weâ€™ve discussed earlier, enables us
    to use images to complement the text prompts we pass to LLMs aka. Multimodal LLMs,
    which let the user specify any vision or language task.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬ä¹‹å‰è®¨è®ºè¿‡çš„ç›¸åŒåŸç†ï¼Œä½¿å¾—æˆ‘ä»¬å¯ä»¥åˆ©ç”¨å›¾åƒæ¥è¡¥å……ä¼ é€’ç»™å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå³å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼‰çš„æ–‡æœ¬æç¤ºï¼Œè¿™å…è®¸ç”¨æˆ·æŒ‡å®šä»»ä½•è§†è§‰æˆ–è¯­è¨€ä»»åŠ¡ã€‚
- en: Multimodal LLMs are a recent and powerful development, examples such [GPT-4V](https://openai.com/research/gpt-4)
    and [MedPalm M](https://medika.life/a-new-era-in-medical-ai-the-power-of-med-palm-m/)
    are pushing the boundaries of how we use LLMs because they can provide rich responses
    based on image and text inputs.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æ˜¯æœ€è¿‘çš„ä¸€ä¸ªå¼ºå¤§å‘å±•ï¼Œåƒ [GPT-4V](https://openai.com/research/gpt-4) å’Œ [MedPalm M](https://medika.life/a-new-era-in-medical-ai-the-power-of-med-palm-m/)
    è¿™æ ·çš„ä¾‹å­æ­£åœ¨æ¨åŠ¨æˆ‘ä»¬ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è¾¹ç•Œï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥æ ¹æ®å›¾åƒå’Œæ–‡æœ¬è¾“å…¥æä¾›ä¸°å¯Œçš„å“åº”ã€‚
- en: For example, MedPalm M makes it possible to ask the model to diagnose a patient
    based on an X-ray image, with results that would rival a qualified physician!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼ŒMedPalm M ä½¿å¾—åŸºäº X å°„çº¿å›¾åƒè®©æ¨¡å‹å¯¹æ‚£è€…è¿›è¡Œè¯Šæ–­æˆä¸ºå¯èƒ½ï¼Œç»“æœå¯ä¸åˆæ ¼çš„åŒ»ç”Ÿç›¸åª²ç¾ï¼
- en: It is important to note that Multimodal LLMs are not perfect and can still be
    susceptible to the limitations of earlier models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦çš„æ˜¯è¦æ³¨æ„ï¼Œå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å¹¶ä¸å®Œç¾ï¼Œä»ç„¶å¯èƒ½å—åˆ°æ—©æœŸæ¨¡å‹å±€é™æ€§çš„å½±å“ã€‚
- en: They are still not fully reliable (i.e. they can â€œhallucinateâ€ facts and make
    reasoning errors). Great care should be taken when using language model outputs,
    particularly in high-stakes contexts, such as medical diagnosis.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬ä»ç„¶ä¸æ˜¯å®Œå…¨å¯é çš„ï¼ˆå³ï¼Œå®ƒä»¬å¯èƒ½â€œå¹»æƒ³â€äº‹å®å¹¶äº§ç”Ÿæ¨ç†é”™è¯¯ï¼‰ã€‚åœ¨ä½¿ç”¨è¯­è¨€æ¨¡å‹è¾“å‡ºæ—¶ï¼Œå°¤å…¶æ˜¯åœ¨é«˜é£é™©æƒ…å¢ƒä¸‹ï¼ˆå¦‚åŒ»ç–—è¯Šæ–­ï¼‰ï¼Œåº”ç‰¹åˆ«å°å¿ƒã€‚
- en: Ensuring that appropriate safeguards and processes are in place is critical
    (such as human review, grounding with additional context, or even avoiding high-stakes
    use cases altogether).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿é€‚å½“çš„ä¿æŠ¤æªæ–½å’Œæµç¨‹åˆ°ä½æ˜¯è‡³å…³é‡è¦çš„ï¼ˆä¾‹å¦‚äººå·¥å®¡æ ¸ã€é™„åŠ èƒŒæ™¯ä¿¡æ¯çš„åŸºç¡€ï¼Œæˆ–ç”šè‡³å®Œå…¨é¿å…é«˜é£é™©ç”¨ä¾‹ï¼‰ã€‚
- en: This is an exciting space representing the next evolution of LLMs!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªä»¤äººå…´å¥‹çš„é¢†åŸŸï¼Œä»£è¡¨äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸‹ä¸€æ¬¡è¿›åŒ–ï¼
- en: ğŸ The wrap up
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ æ€»ç»“
- en: 'In this post we introduced:'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ï¼š
- en: How embeddings are used to enable models to capture (encode) input data they
    are trained on.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åµŒå…¥æ˜¯å¦‚ä½•è¢«ç”¨äºä½¿æ¨¡å‹æ•æ‰ï¼ˆç¼–ç ï¼‰å®ƒä»¬è®­ç»ƒæ—¶è¾“å…¥çš„æ•°æ®çš„ã€‚
- en: The concepts of Multimodal deep learning models that are typically composed
    of multiple unimodal neural networks, which process each input modality separately.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ¦‚å¿µé€šå¸¸ç”±å¤šä¸ªå•æ¨¡æ€ç¥ç»ç½‘ç»œç»„æˆï¼Œè¿™äº›ç½‘ç»œåˆ†åˆ«å¤„ç†æ¯ç§è¾“å…¥æ¨¡æ€ã€‚
- en: A simple demo incorporating text and image embeddings to illustrate the Multimodal
    Search use case when searching for products on a website.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç®€å•çš„æ¼”ç¤ºï¼Œç»“åˆæ–‡æœ¬å’Œå›¾åƒåµŒå…¥ï¼Œä»¥è¯´æ˜åœ¨ç½‘ç«™ä¸Šæœç´¢äº§å“æ—¶çš„å¤šæ¨¡æ€æœç´¢ç”¨ä¾‹ã€‚
- en: Finally we also discussed some potential use cases and indeed real world examples
    of Multimodal LLMs.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†ä¸€äº›æ½œåœ¨çš„ç”¨ä¾‹ä»¥åŠå®é™…çš„å¤šæ¨¡æ€LLMåº”ç”¨å®ä¾‹ã€‚
- en: '**ğŸ‘‹ğŸ¼Thanks for stopping by, I hope you enjoyed this post, and Iâ€™ll see you
    on the next!**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ‘‹ğŸ¼æ„Ÿè°¢ä½ çš„å…‰ä¸´ï¼Œå¸Œæœ›ä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œæˆ‘ä»¬ä¸‹æ¬¡è§ï¼**'
- en: ğŸ“š References & Further reading
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ“š å‚è€ƒèµ„æ–™åŠè¿›ä¸€æ­¥é˜…è¯»
- en: '[1] â€œWhat is Multimodal Search: â€˜LLMs with visionâ€™ change businessesâ€ -[https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search](https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] â€œä»€ä¹ˆæ˜¯å¤šæ¨¡æ€æœç´¢ï¼šâ€˜å…·æœ‰è§†è§‰èƒ½åŠ›çš„LLMâ€™å¦‚ä½•æ”¹å˜ä¸šåŠ¡â€ - [https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search](https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search)'
- en: '[2] [https://atlas.nomic.ai/](https://atlas.nomic.ai/)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://atlas.nomic.ai/](https://atlas.nomic.ai/)'
- en: '**Publications:**'
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**å‡ºç‰ˆç‰©ï¼š**'
- en: 'â€œTowards Generalist Biomedical AIâ€ aka. The MedPalm M paper, arXiv:2307.14334v1
    [cs.CL] 26 Jul 2023: [https://arxiv.org/pdf/2307.14334.pdf?ref=maginative.com](https://arxiv.org/pdf/2307.14334.pdf?ref=maginative.com)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'â€œé¢å‘é€šç”¨ç”Ÿç‰©åŒ»å­¦AIâ€åˆåMedPalm Mè®ºæ–‡ï¼ŒarXiv:2307.14334v1 [cs.CL] 2023å¹´7æœˆ26æ—¥: [https://arxiv.org/pdf/2307.14334.pdf?ref=maginative.com](https://arxiv.org/pdf/2307.14334.pdf?ref=maginative.com)'
- en: 'â€œCoCa: Contrastive Captioners are Image-Text Foundation Modelsâ€, arXiv:2205.01917v2
    [cs.CV] 14 Jun 2022: [https://arxiv.org/pdf/2205.01917.pdf](https://arxiv.org/pdf/2205.01917.pdf)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'â€œCoCa: å¯¹æ¯”å¼å­—å¹•ç”Ÿæˆå™¨æ˜¯å›¾åƒ-æ–‡æœ¬åŸºç¡€æ¨¡å‹â€ï¼ŒarXiv:2205.01917v2 [cs.CV] 2022å¹´6æœˆ14æ—¥: [https://arxiv.org/pdf/2205.01917.pdf](https://arxiv.org/pdf/2205.01917.pdf)'
