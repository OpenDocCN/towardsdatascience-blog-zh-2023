- en: What are Multimodal models?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是多模态模型？
- en: 原文：[https://towardsdatascience.com/what-are-multimodal-models-fe118f3ef963](https://towardsdatascience.com/what-are-multimodal-models-fe118f3ef963)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/what-are-multimodal-models-fe118f3ef963](https://towardsdatascience.com/what-are-multimodal-models-fe118f3ef963)
- en: Give LLMs the ability to see!
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 赋予大型语言模型（LLM）视觉能力！
- en: '[](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)[![Omer
    Mahmood](../Images/0c87da4134bea397c77bc4ba6640e34b.png)](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)
    [Omer Mahmood](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)[![Omer
    Mahmood](../Images/0c87da4134bea397c77bc4ba6640e34b.png)](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)
    [Omer Mahmood](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)
    ·6 min read·Oct 16, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)
    ·阅读时间6分钟·2023年10月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4b7fb0fc89f35adbb6b9d33055d2f0d0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b7fb0fc89f35adbb6b9d33055d2f0d0.png)'
- en: Screenshot of [Mecari text & image embeddings demo](https://atlas.nomic.ai/map/vertex-mercari)
    running on Atlas by Nomic.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Atlas by Nomic 上运行的 [Mecari 文本与图像嵌入演示](https://atlas.nomic.ai/map/vertex-mercari)
    截图。
- en: Who is this post for?
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这篇文章适合谁？
- en: '**Reader Audience [🟢⚪️⚪️]:** AI beginners, familiar with popular concepts,
    models and their applications'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**读者群体 [🟢⚪️⚪️]：** AI 初学者，熟悉流行概念、模型及其应用'
- en: '**Level [🟢🟢️⚪️]:** Intermediate topic'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**等级 [🟢🟢️⚪️]：** 中级话题'
- en: '**Complexity [🟢⚪️⚪️]:** Easy to digest, no mathematical formulas or complex
    theory here'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂度 [🟢⚪️⚪️]：** 容易理解，没有数学公式或复杂理论'
- en: ❓Why It Matters
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ❓为什么这很重要
- en: Foundational large language models (LLMs), pre-trained on huge datasets are
    pretty efficient at handling generic, multi-tasking via prompts through zero-shot,
    few-shot or transfer learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 基础的大型语言模型（LLM），经过在庞大数据集上的预训练，在通过零-shot、few-shot 或迁移学习的提示下处理通用、多任务时相当高效。
- en: Indeed, examples of these models like [PaLM2](https://ai.google/discover/palm2/)
    and [GPT4](https://openai.com/research/gpt-4) have revolutionised the way we interact
    with computers **using text as an input**, but…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，这些模型的例子如 [PaLM2](https://ai.google/discover/palm2/) 和 [GPT4](https://openai.com/research/gpt-4)
    已经彻底改变了我们与计算机交互的方式，**使用文本作为输入**，但是……
- en: What if there was a way to extend the intelligence of these models, by enabling
    them to use different modalities of input, such as photos, audio, and video? **Or
    in other words, make them Multimodal!**
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果有一种方法可以通过使这些模型使用不同的输入模态，如照片、音频和视频，来扩展它们的智能会怎样？**换句话说，让它们变成多模态的！**
- en: It could greatly improve how we search for things on the web, or even understand
    the world around us for example in real world applications such as medicine and
    pathology.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这可能会大大改善我们在网络上搜索信息的方式，甚至在现实世界应用中，例如医学和病理学，帮助我们理解周围的世界。
- en: There is a solution! Multimodal deep learning models can combine the embeddings
    from different types of input, enabling, for example, an LLM to “see” what you
    are asking for, and return relevant results.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有解决方案！多模态深度学习模型可以结合不同类型输入的嵌入，使得例如一个大型语言模型（LLM）能够“看到”你所询问的内容，并返回相关结果。
- en: '**⚡️Stick around if** you want to learn more about how this all works and play
    around with a working demo!'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**⚡️继续关注** 如果你想了解更多关于这一切如何运作的知识，并试用一个实际的演示！'
- en: 🔥 How does it work?
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🔥 这如何运作？
- en: '**It starts with embeddings**'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**它从嵌入开始**'
- en: One of the most powerful building blocks of training deep learning models is
    the creation of embedding vectors.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度学习模型的最强大构建模块之一是创建嵌入向量。
- en: During training, the model encodes the different categories (for example, people,
    foods, and toys) it encounters into their numerical representation aka. an Embedding,
    that is stored as a vector of numbers.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，模型将遇到的不同类别（例如，人、食物和玩具）编码成其数值表示，即嵌入，这些嵌入存储为数字向量。
- en: Embeddings are useful when we want to move from a sparse representation of category
    (or class) for example a long string of text or an image, to something that is
    more compact, and can be reused across other models too.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入在我们想从类别（或类）的稀疏表示（例如一长串文本或图像）转变为更紧凑的形式，并且可以在其他模型中重用时非常有用。
- en: Put simply, Embeddings provide a way for us to store the meaning of things in
    a shorthand form, that machines can use to quickly compare and search against!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，嵌入为我们提供了一种以简写形式存储事物含义的方法，机器可以用来快速比较和搜索！
- en: To be a bit more concrete, with vision models trained on different images, images
    with a similar appearance and meaning will be placed closely together in the embedding
    space it creates.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，通过对不同图像进行训练的视觉模型，具有相似外观和含义的图像将在它创建的嵌入空间中被紧密放置在一起。
- en: '![](../Images/c7ec53cf44032053dd9980ef256f49a2.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7ec53cf44032053dd9980ef256f49a2.png)'
- en: '*Figure 1: Visualisation of embeddings that represent different categories
    of images, and similar images in the same embedding space. Illustration by the
    author.*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1：表示不同类别图像和同一嵌入空间中相似图像的嵌入可视化。由作者插图。*'
- en: “The model can map an image to an embedding, which is a location in the space.
    Therefore, if you look around the embedding, you can find other images with similar
    appearance and meaning. This is how image similarity search works!”[1]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: “模型可以将图像映射到嵌入，这是空间中的一个位置。因此，如果你在嵌入周围查看，你可以找到其他具有相似外观和含义的图像。这就是图像相似性搜索的工作原理！”[1]
- en: '**Training on image and text pairs**'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**图像和文本对的训练**'
- en: 'In a similar fashion to the previous example, deep learning models can also
    be trained using related pairings of text and images. This happens using a combination
    of other models, as illustrated in the Figure below:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于前面的示例，深度学习模型也可以通过相关的文本和图像配对进行训练。这是通过结合其他模型来实现的，如下图所示：
- en: '![](../Images/4fd63d2d4fcf69eae568d4383c13edc2.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fd63d2d4fcf69eae568d4383c13edc2.png)'
- en: '*Figure 2: Using a combination of models to train on image and text pairs aka
    “CoCa”. Illustration by the author.*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2：使用模型组合对图像和文本对进行训练，即“CoCa”。由作者插图。*'
- en: 'Let’s breakdown what’s happening above, when we pass in an image and text pair:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来解析一下当我们传入一个图像和文本对时发生了什么：
- en: '**Image Encoder:** a model to obtain image embeddings,'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像编码器：** 用于获取图像嵌入的模型，'
- en: '**Unimodel Text Encoder:** a text model to obtain text embeddings. This model
    is fed by the Image Encoder model, taking source image embeddings as input and
    to produce a representation of the sequence of images and text pairs aka. **Self
    Attention**.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单模态文本编码器：** 一种用于获取文本嵌入的模型。该模型由图像编码器模型输入，接受源图像嵌入作为输入，产生图像和文本对的序列表示，即**自注意力**。'
- en: '**Multimodal Text Encoder:** a model to learn the relationships between them.
    More formally, the technique is called **Cross Attention**: an attention mechanism
    in Transformer architecture that mixes two different embedding sequences, or in
    our case images and the text that describes them.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多模态文本编码器：** 一种学习图像和文本之间关系的模型。更正式地说，这种技术称为**交叉注意力**：一种在 Transformer 架构中混合两种不同嵌入序列的注意力机制，或者在我们的案例中是图像和描述它们的文本。'
- en: This design, or one way of combining models is known as Contrastive Captioning
    or simply ‘CoCa’ see [here](https://arxiv.org/abs/2205.01917) if you want to delve
    deeper!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计，或结合模型的一种方式，被称为对比字幕或简单地‘CoCa’，如果你想深入了解，可以查看[这里](https://arxiv.org/abs/2205.01917)！
- en: '**“The result: a foundational ‘Vision Language Model (VLM)’…**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**“结果：一个基础的‘视觉语言模型（VLM）’…**'
- en: Where we have built a shared embedding space for images and texts with a fixed
    dimension, organized by their meanings. In this space, images and text with similar
    meanings are placed close together.”[1]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建立了一个具有固定维度的共享嵌入空间，用于图像和文本，按照它们的含义组织。在这个空间中，含义相似的图像和文本被放在一起。”[1]
- en: '![](../Images/a6598a08a9054ae15d9ab90bf549733c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a6598a08a9054ae15d9ab90bf549733c.png)'
- en: '*Figure 3: Visualisation of embeddings for related images and text. Illustration
    by the author.*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3：相关图像和文本的嵌入可视化。由作者插图。*'
- en: This means that you can search for images based on text (text-to-image search)
    or search for text based on images (image-to-text search).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着你可以根据文本搜索图像（文本到图像搜索），或者根据图像搜索文本（图像到文本搜索）。
- en: This is the basic idea behind how Google Search finds relevant results across
    images and text.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Google 搜索如何在图像和文本中找到相关结果的基本思路。
- en: 🔍 Try a quick demo!
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🔍 试一下快速演示！
- en: Want to see how text and image embeddings work in a real world application?
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 想看看文本和图像嵌入在现实世界应用中的效果吗？
- en: You can play around for free using a demo running on Altas by Nomic.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用在 Nomic 提供的 Altas 上运行的演示免费进行体验。
- en: “Atlas enables you to Interact, discover insights and build with unstructured
    text, image and audio data.”[2]
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: “Atlas 使你能够与非结构化文本、图像和音频数据进行互动、发现洞察并进行构建。”[2]
- en: Here is a search demo that brings together text and image embeddings based on
    items available on the Mercari website (a Japanese e-commerce retailer).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个搜索演示，将 Mercari 网站（一个日本电子商务零售商）上可用的项目的文本和图像嵌入结合在一起。
- en: '1\. Navigate to: [https://atlas.nomic.ai/map/vertex-mercari](https://atlas.nomic.ai/map/vertex-mercari)
    (you might need to create a free account to gain access).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 导航至：[https://atlas.nomic.ai/map/vertex-mercari](https://atlas.nomic.ai/map/vertex-mercari)（你可能需要创建一个免费账户以获得访问权限）。
- en: '![](../Images/ce2cc59b9cacf30d4972defb136e1780.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce2cc59b9cacf30d4972defb136e1780.png)'
- en: 'Figure 4: Screenshot of Atlas by Nomic user interface, running Mecari embeddings
    demo.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：Nomic 的 Atlas 用户界面的截图，运行 Mecari 嵌入演示。
- en: 2\. You can move your mouse cursor around the ‘dot-cloud’ vizualisation that
    represents the embedding space you can search against.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 你可以在表示嵌入空间的“点云”可视化上移动鼠标光标，以进行搜索。
- en: 3\. Or in the “Selection Tools” in the top-right corner of the interface you
    can type a text search query, filter by certain attributes or ‘lasso’ your own
    selection.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在界面右上角的“选择工具”中，你可以输入文本搜索查询，按某些属性进行筛选或使用‘套索’工具进行自定义选择。
- en: It really is that simple! If you want to learn more about how it was put together,
    take a look [here](https://ai-demos.dev/).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 真的就这么简单！如果你想了解更多关于它的组装过程，可以查看 [这里](https://ai-demos.dev/)。
- en: 🍱 Multimodal LLMs
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🍱 多模态语言模型
- en: Yes, there’s more to it than what we have discussed. In the case of LLMs, complex
    [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    architectures are used to convert text and other data to vectors and back using
    tokenization, positional encoding, and embedding layers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这比我们讨论的内容还要多。对于大型语言模型，复杂的 [变换器](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    架构用于通过标记化、位置编码和嵌入层将文本和其他数据转换为向量并进行反向转换。
- en: But broadly speaking, the same principle we’ve discussed earlier, enables us
    to use images to complement the text prompts we pass to LLMs aka. Multimodal LLMs,
    which let the user specify any vision or language task.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 但总的来说，我们之前讨论过的相同原理，使得我们可以利用图像来补充传递给大型语言模型（即多模态语言模型）的文本提示，这允许用户指定任何视觉或语言任务。
- en: Multimodal LLMs are a recent and powerful development, examples such [GPT-4V](https://openai.com/research/gpt-4)
    and [MedPalm M](https://medika.life/a-new-era-in-medical-ai-the-power-of-med-palm-m/)
    are pushing the boundaries of how we use LLMs because they can provide rich responses
    based on image and text inputs.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态语言模型是最近的一个强大发展，像 [GPT-4V](https://openai.com/research/gpt-4) 和 [MedPalm M](https://medika.life/a-new-era-in-medical-ai-the-power-of-med-palm-m/)
    这样的例子正在推动我们使用大型语言模型的边界，因为它们可以根据图像和文本输入提供丰富的响应。
- en: For example, MedPalm M makes it possible to ask the model to diagnose a patient
    based on an X-ray image, with results that would rival a qualified physician!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，MedPalm M 使得基于 X 射线图像让模型对患者进行诊断成为可能，结果可与合格的医生相媲美！
- en: It is important to note that Multimodal LLMs are not perfect and can still be
    susceptible to the limitations of earlier models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，多模态语言模型并不完美，仍然可能受到早期模型局限性的影响。
- en: They are still not fully reliable (i.e. they can “hallucinate” facts and make
    reasoning errors). Great care should be taken when using language model outputs,
    particularly in high-stakes contexts, such as medical diagnosis.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 它们仍然不是完全可靠的（即，它们可能“幻想”事实并产生推理错误）。在使用语言模型输出时，尤其是在高风险情境下（如医疗诊断），应特别小心。
- en: Ensuring that appropriate safeguards and processes are in place is critical
    (such as human review, grounding with additional context, or even avoiding high-stakes
    use cases altogether).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 确保适当的保护措施和流程到位是至关重要的（例如人工审核、附加背景信息的基础，或甚至完全避免高风险用例）。
- en: This is an exciting space representing the next evolution of LLMs!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个令人兴奋的领域，代表了大型语言模型的下一次进化！
- en: 🏁 The wrap up
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🏁 总结
- en: 'In this post we introduced:'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们介绍了：
- en: How embeddings are used to enable models to capture (encode) input data they
    are trained on.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入是如何被用于使模型捕捉（编码）它们训练时输入的数据的。
- en: The concepts of Multimodal deep learning models that are typically composed
    of multiple unimodal neural networks, which process each input modality separately.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多模态深度学习模型的概念通常由多个单模态神经网络组成，这些网络分别处理每种输入模态。
- en: A simple demo incorporating text and image embeddings to illustrate the Multimodal
    Search use case when searching for products on a website.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个简单的演示，结合文本和图像嵌入，以说明在网站上搜索产品时的多模态搜索用例。
- en: Finally we also discussed some potential use cases and indeed real world examples
    of Multimodal LLMs.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们还讨论了一些潜在的用例以及实际的多模态LLM应用实例。
- en: '**👋🏼Thanks for stopping by, I hope you enjoyed this post, and I’ll see you
    on the next!**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**👋🏼感谢你的光临，希望你喜欢这篇文章，我们下次见！**'
- en: 📚 References & Further reading
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 📚 参考资料及进一步阅读
- en: '[1] “What is Multimodal Search: ‘LLMs with vision’ change businesses” -[https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search](https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] “什么是多模态搜索：‘具有视觉能力的LLM’如何改变业务” - [https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search](https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search)'
- en: '[2] [https://atlas.nomic.ai/](https://atlas.nomic.ai/)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://atlas.nomic.ai/](https://atlas.nomic.ai/)'
- en: '**Publications:**'
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**出版物：**'
- en: '“Towards Generalist Biomedical AI” aka. The MedPalm M paper, arXiv:2307.14334v1
    [cs.CL] 26 Jul 2023: [https://arxiv.org/pdf/2307.14334.pdf?ref=maginative.com](https://arxiv.org/pdf/2307.14334.pdf?ref=maginative.com)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“面向通用生物医学AI”又名MedPalm M论文，arXiv:2307.14334v1 [cs.CL] 2023年7月26日: [https://arxiv.org/pdf/2307.14334.pdf?ref=maginative.com](https://arxiv.org/pdf/2307.14334.pdf?ref=maginative.com)'
- en: '“CoCa: Contrastive Captioners are Image-Text Foundation Models”, arXiv:2205.01917v2
    [cs.CV] 14 Jun 2022: [https://arxiv.org/pdf/2205.01917.pdf](https://arxiv.org/pdf/2205.01917.pdf)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“CoCa: 对比式字幕生成器是图像-文本基础模型”，arXiv:2205.01917v2 [cs.CV] 2022年6月14日: [https://arxiv.org/pdf/2205.01917.pdf](https://arxiv.org/pdf/2205.01917.pdf)'
