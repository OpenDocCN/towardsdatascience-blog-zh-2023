- en: What are Multimodal models?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/what-are-multimodal-models-fe118f3ef963](https://towardsdatascience.com/what-are-multimodal-models-fe118f3ef963)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Give LLMs the ability to see!
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)[![Omer
    Mahmood](../Images/0c87da4134bea397c77bc4ba6640e34b.png)](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)
    [Omer Mahmood](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)
    ·6 min read·Oct 16, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b7fb0fc89f35adbb6b9d33055d2f0d0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Screenshot of [Mecari text & image embeddings demo](https://atlas.nomic.ai/map/vertex-mercari)
    running on Atlas by Nomic.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Who is this post for?
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Reader Audience [🟢⚪️⚪️]:** AI beginners, familiar with popular concepts,
    models and their applications'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Level [🟢🟢️⚪️]:** Intermediate topic'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complexity [🟢⚪️⚪️]:** Easy to digest, no mathematical formulas or complex
    theory here'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ❓Why It Matters
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Foundational large language models (LLMs), pre-trained on huge datasets are
    pretty efficient at handling generic, multi-tasking via prompts through zero-shot,
    few-shot or transfer learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, examples of these models like [PaLM2](https://ai.google/discover/palm2/)
    and [GPT4](https://openai.com/research/gpt-4) have revolutionised the way we interact
    with computers **using text as an input**, but…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: What if there was a way to extend the intelligence of these models, by enabling
    them to use different modalities of input, such as photos, audio, and video? **Or
    in other words, make them Multimodal!**
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It could greatly improve how we search for things on the web, or even understand
    the world around us for example in real world applications such as medicine and
    pathology.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a solution! Multimodal deep learning models can combine the embeddings
    from different types of input, enabling, for example, an LLM to “see” what you
    are asking for, and return relevant results.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**⚡️Stick around if** you want to learn more about how this all works and play
    around with a working demo!'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 🔥 How does it work?
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**It starts with embeddings**'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most powerful building blocks of training deep learning models is
    the creation of embedding vectors.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: During training, the model encodes the different categories (for example, people,
    foods, and toys) it encounters into their numerical representation aka. an Embedding,
    that is stored as a vector of numbers.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings are useful when we want to move from a sparse representation of category
    (or class) for example a long string of text or an image, to something that is
    more compact, and can be reused across other models too.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Put simply, Embeddings provide a way for us to store the meaning of things in
    a shorthand form, that machines can use to quickly compare and search against!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: To be a bit more concrete, with vision models trained on different images, images
    with a similar appearance and meaning will be placed closely together in the embedding
    space it creates.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7ec53cf44032053dd9980ef256f49a2.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1: Visualisation of embeddings that represent different categories
    of images, and similar images in the same embedding space. Illustration by the
    author.*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: “The model can map an image to an embedding, which is a location in the space.
    Therefore, if you look around the embedding, you can find other images with similar
    appearance and meaning. This is how image similarity search works!”[1]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '**Training on image and text pairs**'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a similar fashion to the previous example, deep learning models can also
    be trained using related pairings of text and images. This happens using a combination
    of other models, as illustrated in the Figure below:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4fd63d2d4fcf69eae568d4383c13edc2.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2: Using a combination of models to train on image and text pairs aka
    “CoCa”. Illustration by the author.*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s breakdown what’s happening above, when we pass in an image and text pair:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '**Image Encoder:** a model to obtain image embeddings,'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unimodel Text Encoder:** a text model to obtain text embeddings. This model
    is fed by the Image Encoder model, taking source image embeddings as input and
    to produce a representation of the sequence of images and text pairs aka. **Self
    Attention**.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multimodal Text Encoder:** a model to learn the relationships between them.
    More formally, the technique is called **Cross Attention**: an attention mechanism
    in Transformer architecture that mixes two different embedding sequences, or in
    our case images and the text that describes them.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This design, or one way of combining models is known as Contrastive Captioning
    or simply ‘CoCa’ see [here](https://arxiv.org/abs/2205.01917) if you want to delve
    deeper!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '**“The result: a foundational ‘Vision Language Model (VLM)’…**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Where we have built a shared embedding space for images and texts with a fixed
    dimension, organized by their meanings. In this space, images and text with similar
    meanings are placed close together.”[1]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a6598a08a9054ae15d9ab90bf549733c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3: Visualisation of embeddings for related images and text. Illustration
    by the author.*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: This means that you can search for images based on text (text-to-image search)
    or search for text based on images (image-to-text search).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: This is the basic idea behind how Google Search finds relevant results across
    images and text.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 🔍 Try a quick demo!
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Want to see how text and image embeddings work in a real world application?
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 想看看文本和图像嵌入在现实世界应用中的效果吗？
- en: You can play around for free using a demo running on Altas by Nomic.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用在 Nomic 提供的 Altas 上运行的演示免费进行体验。
- en: “Atlas enables you to Interact, discover insights and build with unstructured
    text, image and audio data.”[2]
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: “Atlas 使你能够与非结构化文本、图像和音频数据进行互动、发现洞察并进行构建。”[2]
- en: Here is a search demo that brings together text and image embeddings based on
    items available on the Mercari website (a Japanese e-commerce retailer).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个搜索演示，将 Mercari 网站（一个日本电子商务零售商）上可用的项目的文本和图像嵌入结合在一起。
- en: '1\. Navigate to: [https://atlas.nomic.ai/map/vertex-mercari](https://atlas.nomic.ai/map/vertex-mercari)
    (you might need to create a free account to gain access).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 导航至：[https://atlas.nomic.ai/map/vertex-mercari](https://atlas.nomic.ai/map/vertex-mercari)（你可能需要创建一个免费账户以获得访问权限）。
- en: '![](../Images/ce2cc59b9cacf30d4972defb136e1780.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce2cc59b9cacf30d4972defb136e1780.png)'
- en: 'Figure 4: Screenshot of Atlas by Nomic user interface, running Mecari embeddings
    demo.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：Nomic 的 Atlas 用户界面的截图，运行 Mecari 嵌入演示。
- en: 2\. You can move your mouse cursor around the ‘dot-cloud’ vizualisation that
    represents the embedding space you can search against.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 你可以在表示嵌入空间的“点云”可视化上移动鼠标光标，以进行搜索。
- en: 3\. Or in the “Selection Tools” in the top-right corner of the interface you
    can type a text search query, filter by certain attributes or ‘lasso’ your own
    selection.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在界面右上角的“选择工具”中，你可以输入文本搜索查询，按某些属性进行筛选或使用‘套索’工具进行自定义选择。
- en: It really is that simple! If you want to learn more about how it was put together,
    take a look [here](https://ai-demos.dev/).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 真的就这么简单！如果你想了解更多关于它的组装过程，可以查看 [这里](https://ai-demos.dev/)。
- en: 🍱 Multimodal LLMs
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🍱 多模态语言模型
- en: Yes, there’s more to it than what we have discussed. In the case of LLMs, complex
    [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    architectures are used to convert text and other data to vectors and back using
    tokenization, positional encoding, and embedding layers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这比我们讨论的内容还要多。对于大型语言模型，复杂的 [变换器](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    架构用于通过标记化、位置编码和嵌入层将文本和其他数据转换为向量并进行反向转换。
- en: But broadly speaking, the same principle we’ve discussed earlier, enables us
    to use images to complement the text prompts we pass to LLMs aka. Multimodal LLMs,
    which let the user specify any vision or language task.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 但总的来说，我们之前讨论过的相同原理，使得我们可以利用图像来补充传递给大型语言模型（即多模态语言模型）的文本提示，这允许用户指定任何视觉或语言任务。
- en: Multimodal LLMs are a recent and powerful development, examples such [GPT-4V](https://openai.com/research/gpt-4)
    and [MedPalm M](https://medika.life/a-new-era-in-medical-ai-the-power-of-med-palm-m/)
    are pushing the boundaries of how we use LLMs because they can provide rich responses
    based on image and text inputs.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态语言模型是最近的一个强大发展，像 [GPT-4V](https://openai.com/research/gpt-4) 和 [MedPalm M](https://medika.life/a-new-era-in-medical-ai-the-power-of-med-palm-m/)
    这样的例子正在推动我们使用大型语言模型的边界，因为它们可以根据图像和文本输入提供丰富的响应。
- en: For example, MedPalm M makes it possible to ask the model to diagnose a patient
    based on an X-ray image, with results that would rival a qualified physician!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，MedPalm M 使得基于 X 射线图像让模型对患者进行诊断成为可能，结果可与合格的医生相媲美！
- en: It is important to note that Multimodal LLMs are not perfect and can still be
    susceptible to the limitations of earlier models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，多模态语言模型并不完美，仍然可能受到早期模型局限性的影响。
- en: They are still not fully reliable (i.e. they can “hallucinate” facts and make
    reasoning errors). Great care should be taken when using language model outputs,
    particularly in high-stakes contexts, such as medical diagnosis.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 它们仍然不是完全可靠的（即，它们可能“幻想”事实并产生推理错误）。在使用语言模型输出时，尤其是在高风险情境下（如医疗诊断），应特别小心。
- en: Ensuring that appropriate safeguards and processes are in place is critical
    (such as human review, grounding with additional context, or even avoiding high-stakes
    use cases altogether).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 确保适当的保护措施和流程到位是至关重要的（例如人工审核、附加背景信息的基础，或甚至完全避免高风险用例）。
- en: This is an exciting space representing the next evolution of LLMs!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个令人兴奋的领域，代表了大型语言模型的下一次进化！
- en: 🏁 The wrap up
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🏁 总结
- en: 'In this post we introduced:'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们介绍了：
- en: How embeddings are used to enable models to capture (encode) input data they
    are trained on.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入是如何被用于使模型捕捉（编码）它们训练时输入的数据的。
- en: The concepts of Multimodal deep learning models that are typically composed
    of multiple unimodal neural networks, which process each input modality separately.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多模态深度学习模型的概念通常由多个单模态神经网络组成，这些网络分别处理每种输入模态。
- en: A simple demo incorporating text and image embeddings to illustrate the Multimodal
    Search use case when searching for products on a website.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个简单的演示，结合文本和图像嵌入，以说明在网站上搜索产品时的多模态搜索用例。
- en: Finally we also discussed some potential use cases and indeed real world examples
    of Multimodal LLMs.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们还讨论了一些潜在的用例以及实际的多模态LLM应用实例。
- en: '**👋🏼Thanks for stopping by, I hope you enjoyed this post, and I’ll see you
    on the next!**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**👋🏼感谢你的光临，希望你喜欢这篇文章，我们下次见！**'
- en: 📚 References & Further reading
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 📚 参考资料及进一步阅读
- en: '[1] “What is Multimodal Search: ‘LLMs with vision’ change businesses” -[https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search](https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] “什么是多模态搜索：‘具有视觉能力的LLM’如何改变业务” - [https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search](https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search)'
- en: '[2] [https://atlas.nomic.ai/](https://atlas.nomic.ai/)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://atlas.nomic.ai/](https://atlas.nomic.ai/)'
- en: '**Publications:**'
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**出版物：**'
- en: '“Towards Generalist Biomedical AI” aka. The MedPalm M paper, arXiv:2307.14334v1
    [cs.CL] 26 Jul 2023: [https://arxiv.org/pdf/2307.14334.pdf?ref=maginative.com](https://arxiv.org/pdf/2307.14334.pdf?ref=maginative.com)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“面向通用生物医学AI”又名MedPalm M论文，arXiv:2307.14334v1 [cs.CL] 2023年7月26日: [https://arxiv.org/pdf/2307.14334.pdf?ref=maginative.com](https://arxiv.org/pdf/2307.14334.pdf?ref=maginative.com)'
- en: '“CoCa: Contrastive Captioners are Image-Text Foundation Models”, arXiv:2205.01917v2
    [cs.CV] 14 Jun 2022: [https://arxiv.org/pdf/2205.01917.pdf](https://arxiv.org/pdf/2205.01917.pdf)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“CoCa: 对比式字幕生成器是图像-文本基础模型”，arXiv:2205.01917v2 [cs.CV] 2022年6月14日: [https://arxiv.org/pdf/2205.01917.pdf](https://arxiv.org/pdf/2205.01917.pdf)'
