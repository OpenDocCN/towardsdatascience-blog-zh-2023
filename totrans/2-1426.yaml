- en: Linear Regression — Occam’s Razor of Predictive Machine Learning Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/linear-regression-occams-razor-of-predictive-machine-learning-modeling-f2ba5b144a2b](https://towardsdatascience.com/linear-regression-occams-razor-of-predictive-machine-learning-modeling-f2ba5b144a2b)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Machine learning modeling using linear regression in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@fmnobar?source=post_page-----f2ba5b144a2b--------------------------------)[![Farzad
    Mahmoodinobar](../Images/2d75209693b712300e6f0796bd2487d0.png)](https://medium.com/@fmnobar?source=post_page-----f2ba5b144a2b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f2ba5b144a2b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f2ba5b144a2b--------------------------------)
    [Farzad Mahmoodinobar](https://medium.com/@fmnobar?source=post_page-----f2ba5b144a2b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f2ba5b144a2b--------------------------------)
    ·16 min read·Jan 3, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2494e60058d5a0eb46decef5f125cf20.png)'
  prefs: []
  type: TYPE_IMG
- en: crystal ball, by [DALL.E 2](https://openai.com/dall-e-2/)
  prefs: []
  type: TYPE_NORMAL
- en: Are you familiar with Occam’s Razor? I remember a mention of it in the Big Bang
    Theory TV series! The idea behind Occam’s Razor is that all other things being
    equal, the simplest explanation for a phenomenon is more likely to be true than
    a more complex one (i.e. the simplest solution is almost always the best solution).
    I’d like to think that Occam’s Razor of predictive modeling in machine learning
    is linear regression, which is almost the simplest modeling methodology to use
    and can be the best solution for certain tasks. This post will cover an introduction
    and implementation of linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to my other posts, learning will be achieved through practice questions
    and answers. I will include hints and explanations in the questions as needed
    to make the journey easier. Lastly, the notebook that I used to create this exercise
    is also linked in the bottom of the post, which you can download, run and follow
    along.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: '*(All images, unless otherwise noted, are by the author.)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@fmnobar/membership?source=post_page-----f2ba5b144a2b--------------------------------)
    [## Join Medium with my referral link - Farzad Mahmoodinobar'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Farzad (and other writers on Medium). Your membership
    fee directly supports Farzad and other…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@fmnobar/membership?source=post_page-----f2ba5b144a2b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Data Set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to practice linear regression, we will use a data set of car prices
    from [UCI Machine Learning Repository](https://archive-beta.ics.uci.edu/dataset/10/automobile)
    (CC BY 4.0). I have cleaned up parts of the data for our use and it can be downloaded
    from [this link](https://gist.github.com/fmnobar/c9b4029e08e97978a9a53f4eb034b16f).
  prefs: []
  type: TYPE_NORMAL
- en: I will explain some of the math behind the linear regression model we will be
    using in the exercise. Understanding the math is not required to be able to successfully
    understand the content of this post but I do recommend going through it to get
    a better sense of what is happening behind the scene, when we create a linear
    regression model.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Fundamentals of Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear regression is when linear predictors (or independent variables) are
    used to predict a dependent variable. One simple example is the formula for a
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/681b884e40b7a6b0db5530abb337ed9b.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, `y` is the dependent variable and `x` is the independent variable
    (`c` is a constant). The goal of a linear regression model is to determine the
    best coefficient (`a` in the example above) for `x` to most accurately predict
    `y`.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s generalize that example, which is also known as mulitple linear regression.
    In a multiple linear regression model, the goal is to find the line of best fit
    that describes the relationship between the dependent variable and multiple independent
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/daebd57c67021726b1c7287621920276.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, we have multiple independent variables (or predictors) from `x_1`
    to `x_n` and each one is multiplied by its own coefficient to predict the dependent
    variable `y`. In a linear regression model, we will try to determine the values
    of coefficients `a_1` to `a_n` to have the best prediction for the dependent variable
    `y`.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand what a linear regression is, let’s move to Ordinary Least
    Squares (OLS) regression, which is a form of linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Ordinary Least Squares Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ordinary least squares regression model estimates the coefficients of a regression
    model by minimizing the sum of the squares of residuals. Residual is the vertical
    distance between the line (i.e. predicted values) vs. the actuals, as shown in
    the figure below. These residuals are squared so that errors do not cancel each
    other out (when one prediction is higher than actual and another prediction is
    lower than actual, these two are still errors and should not cancel each other
    out).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/814567d23e6731a092e28fec2bdddb61.png)'
  prefs: []
  type: TYPE_IMG
- en: Ordinary Least Squares Regression — Regression Line and Residuals
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the underlying concepts, we will start with exploring
    the data and the variables (or features) that we may be able to use to predict
    the car prices. Then we will split the data into train and test sets to build
    the regression model. We will then look at the performance of the regression model
    and finally will plot the results.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Exploratory Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s start by looking at the data, which can also be downloaded from [here](https://gist.github.com/fmnobar/c9b4029e08e97978a9a53f4eb034b16f).
    First we will import Pandas and NumPy. Then we will read the CSV file including
    our data set and look at the top five rows of the data set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb8ad12479cdd91d62cb8c73d320fd73.png)'
  prefs: []
  type: TYPE_IMG
- en: Column names are mostly self-explanatory so I will add only the ones that were
    not immediately obvious to me. You can ignore these for now and just refer to
    them during the course of the exercise if you need the definition of a column
    name.
  prefs: []
  type: TYPE_NORMAL
- en: 'symboling: A value assigned by insurance companies according to the car’s perceived
    riskiness. A value of +3 indicates that the car is risky, -3 indicates that it
    is safe'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'aspiration: Standard or turbo'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'drive-wheels: rwd for rear-wheel drive; fwd for front-wheel drive; 4wd for
    four-wheel drive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'wheel-base: The distance between the centres of the front and rear wheels in
    centimeters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'engine-type: dohc for Dual OverHead Cam; dohcv for Dual OverHead Cam and Valve;
    l for L engine; ohc for OverHead Cam; ohcf for OverHead Cam and Valve F engine;
    ohcv for OverHead Cam and Valve; rotor for Rotary engine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'bore: Inner diameter of the cylinder in centimeters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'stroke: Movement of the cylinder'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Question 1:**'
  prefs: []
  type: TYPE_NORMAL
- en: Are there any missing values in the dataframe?
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d3e3b7d855ecea3cbdd3e6f348f436b.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, there are 25 columns (note column numbers start from 0 to 24)
    and 193 rows. There are no null values in the columns.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Feature Selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature selection is the process of identifying and selecting a subset of relevant
    features (also known as “predictors,” “inputs,” or “attributes”) for building
    a machine learning model. The goal of feature selection is to improve the model’s
    accuracy and interpretability by reducing the complexity of the model and eliminating
    irrelevant, redundant, or noisy features.
  prefs: []
  type: TYPE_NORMAL
- en: '**Question 2:**'
  prefs: []
  type: TYPE_NORMAL
- en: Create a table showing the correlation among the columns in the dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to use `pandas.DataFrame.corr`, which calculates pairwise correlation
    of columns. There are two points to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas.DataFrame.corr` will exclude null values. We confirmed our data set
    does not include any null values but this might be important in exercises with
    null values.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will be limiting the correlation to numerical values only and will discuss
    categorical values later in the exercise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As a refresher, let’s review what categorical and numerical variables are before
    we continue.
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, categorical variables are variables that can take on a
    limited number of values. These values represent different categories and the
    values themselves have no inherent order or numerical meaning. Examples of categorical
    variables include gender (male or female), marital status (married, single, divorced,
    etc.).
  prefs: []
  type: TYPE_NORMAL
- en: Numeric variables are variables that can take on any numerical value within
    a certain range. These variables can be either continuous (meaning they can take
    on any value within a certain range) or discrete (meaning they can only take on
    specific, predetermined values). Examples of numeric variables include age, height,
    weight, etc.
  prefs: []
  type: TYPE_NORMAL
- en: With those out of the way, let’s calculate the correlations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0b403d5f49deaa35f582b0961ea6feb2.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Question 3:**'
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of correlation values generated in the last question. We care
    more about the correlation with the car prices. Show the correlation with the
    car prices and order that from the largest to the smallest.
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e8d1690d750aa81648fbc0a35675b7eb.png)'
  prefs: []
  type: TYPE_IMG
- en: This is quite interesting. For example, “engine-size” seems to have the highest
    correlation with the price, which is expected, while “compression-ratio” does
    not seem to be as highly correlated with the price. On the other hand, “symboling”,
    which we recall is a measure of riskiness of the car, is negatively-correlated
    with the car price, which again makes intuitive sense.
  prefs: []
  type: TYPE_NORMAL
- en: '**Question 4:**'
  prefs: []
  type: TYPE_NORMAL
- en: In order to focus more on the relevant features to build a car price model,
    filter out columns that have a weaker correlation with price, which we are going
    to define as any feature with correlation less than an absolute value of 0.2 (this
    is an arbitrarily-selected value for this exercise).
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aea2051e3164f7de810698a44ff777cd.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that as a result of this, we are now left with 19 features (there are
    20 columns but one of them is the price itself so there are 19 features or predictors).
  prefs: []
  type: TYPE_NORMAL
- en: '**Question 5:**'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a more manageable number of features, take another look at
    them and see if we need to drop any of them.
  prefs: []
  type: TYPE_NORMAL
- en: '***Hint:*** *Some features might be very similar and arguably redundant. And
    some might not really matter.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer:**'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the dataframe and then at the correlation among the features left.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00d5ea313f493625011f16024e2576ea.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b5ac276e4dab731b663e69f57182034d.png)'
  prefs: []
  type: TYPE_IMG
- en: “wheel-base” (distance between the front and rear wheels) and “length” (total
    lenght of the car) are highly-correlated and seem to convey the same information.
    Additionally, “city-mpg” and “highway-mpg” are highly-correlated, so we can consider
    dropping one of them. Let’s go ahead and drop “wheel-base” and “city-mpg” and
    then look at the top five rows of the dataframe again.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d2eaea36a671a657462c276b6c803e81.png)'
  prefs: []
  type: TYPE_IMG
- en: As we see above, the new dataframe is smaller and does not include the two columns
    that we just removed. Next, we will talk about categorical variables.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Dummy Coding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s look more closely at the values of columns “make” and “fuel-type”.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c5e9096f7d2a55e7c573d50e607d725.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c920a9e96fd9e2516ccb31cc3eaff17.png)'
  prefs: []
  type: TYPE_IMG
- en: These two columns are categorical values (e.g. Toyota or Diesel), and not numerical.
  prefs: []
  type: TYPE_NORMAL
- en: In order to include these categorical variables in our regression model, we
    are going to create “dummy codes” for these categorical variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dummy coding is where the categorical variables (or predictors) in one column,
    are replaced by multiple binary columns. For example, let’s assume we had a categorical
    variable as shown in this table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be2d6ff2c74f37d7aa906bc1d8c006b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Categorical Variable — Before Dummy Coding
  prefs: []
  type: TYPE_NORMAL
- en: 'As we see in the above table, “random_categorical_variable” can have three
    categorical values of A, B and C. We would like to transform the categorical variable
    into a format that we can more easily use in our regression model using dummy
    coding, which will transform it into three separate columns of A, B and C, with
    binary values, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/745f3d431c3ab9d3debc858990d29472.png)'
  prefs: []
  type: TYPE_IMG
- en: Categorical Variable — After Dummy Coding
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how dummy coding can be implemented in Python.
  prefs: []
  type: TYPE_NORMAL
- en: '**Question 6:**'
  prefs: []
  type: TYPE_NORMAL
- en: Dummy code the categorical columns of our dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first look at what the dataframe looks like before dummy coding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e06f558283eea02f5eacd756842ecd35.png)'
  prefs: []
  type: TYPE_IMG
- en: We know from the previous question that column “fuel-type” can take 2 distinct
    values (i.e. gas and diesel). Therefore, after dummy coding, we expect to replace
    “fuel-type” column with 2 separate columns. The same applies to other categorical
    columns, depending on how many unique values each has.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first only dummy code the “fuel-type” column as an example and look at
    how the dataframe changes, then we can go ahead and dummy code other categorical
    columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0be5e2605762055a81e9e8b81816af61.png)'
  prefs: []
  type: TYPE_IMG
- en: As expected, we now have 2 columns for the original “fuel-type”, named “fuel-type_gas”
    and “fuel-type_diesel”.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s identify all the categorical columns and dummy code them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/477e0a07ebf3588fbfc63241c71f8ab6.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the above snapshot does not cover all the columns after dummy coding,
    since now we have 63 columns, which would become too small to demonstrate in a
    snapshot.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly and now that we have created all these new columns, let’s recreate the
    correlation between price and all the other columns and sort them from the highest
    to the lowest.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9c7563699f6719c7abc2c85654efa98c.png)'
  prefs: []
  type: TYPE_IMG
- en: As we see above, some of the categorical variables have a high correlation with
    the price such as “drive-wheels” and “num-of-cylinders”.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have familiarized ourselves with the data and cleaned up the
    data to a certain extent, now let’s continue with the main goal of creating a
    model to predict the price of the car, based on these attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Splitting the Data Into Train and Test Sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we are going to first break down the data into dependent and
    independent variables. Dependent variable or “y” is what we are going to predict,
    which is “price” in this exercise. It is called the dependent variable because
    its value depends on the values of the independent variables. Independent variables
    or “X” are all other variables or features that we have left in our data frame
    at this point, which includes “engine-size”, “horsepower”, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will break down the data into Train and Test sets. As the names suggest,
    Train data set will be used to train our regression model and then we will test
    the performance of the model using the Test set. We split the data to ensure that
    model does not see the Test set during its training process so that the Test set
    can be a good representative of how well the model performs. It is important to
    split the data into a training set and a test set because using the same data
    to fit the model and evaluate its performance can lead to overfitting. Overfitting
    occurs when the model is too complex and has learned the noise and random fluctuations
    in the data, rather than the underlying pattern. As a result, the model may perform
    well on the training data but poorly on new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Question 7:**'
  prefs: []
  type: TYPE_NORMAL
- en: Assign the dependent variable (target) to y and the independent variables (or
    features) to X.
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**Question 8:**'
  prefs: []
  type: TYPE_NORMAL
- en: Break down the data into a train and test set. Use 30% of the data for the test
    set, and use a `random_state` of 1234.
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**Question 9:**'
  prefs: []
  type: TYPE_NORMAL
- en: Train a linear regression model using the training set.
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f621a2e7175b2d4c43945becfb948c12.png)'
  prefs: []
  type: TYPE_IMG
- en: We will discuss what happened here, but let’s first look at how we can evaluate
    a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Model Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 4.1\. R²
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Question 10:**'
  prefs: []
  type: TYPE_NORMAL
- en: What is the score of the trained model?
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this purpose, we can use LinearRegression’s “score()”, that returns the
    coefficient of determination of the prediction, or R²$, which is calculated as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6dc564ec022cd3d147dcff7c4bdea2bc.png)![](../Images/11dffd9b28f0fd2a5421f3642d067a80.png)'
  prefs: []
  type: TYPE_IMG
- en: The best possible score is 1.0\. A constant model that always predicts the expected
    value of “y”, regardless of the input features, would get R² score of 0.0.
  prefs: []
  type: TYPE_NORMAL
- en: With that knowledge, let’s look at the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/24e8597820edac349384c602fbe2d4f3.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Question 11:**'
  prefs: []
  type: TYPE_NORMAL
- en: Predict the values of the test set and then evaluate the performance of the
    trained model on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e762acf16b46983385cf997279802de.png)'
  prefs: []
  type: TYPE_IMG
- en: 4.2\. Mean Squared Error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Mean Squared Error (MSE) is the average of the squared errors and is calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/80dbaace5ecde5a193981630d22b1075.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Question 12:**'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the Mean Squared Error and R² for the predicted results of the test
    sets.
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38284a52fe093e63223b6a7b1948926b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Question 13:**'
  prefs: []
  type: TYPE_NORMAL
- en: How do you interpret the results of the previous question? What are your recommendations
    for the next steps?
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer:**'
  prefs: []
  type: TYPE_NORMAL
- en: R² is relatively high but the MSE is pretty high too, which can suggest the
    error may be too high — note this really depends on the business needs and what
    this model is being used for. There might be a case where R² of 90.6% is good
    enough for the business needs and there might be cases where this number is just
    not good enough. This performance level can be driven by some features that are
    not strong predictors of price. Let’s see if we can identify which ones are not
    strong predictors and eliminate them. Then we can retrain and look at the scores
    again to see if we were able to make improvements to our model.
  prefs: []
  type: TYPE_NORMAL
- en: For this step and in order to try something new, we are going to use the ordinary
    least squares (OLS) from statsmodels library. The steps of training and then predicting
    the values of the test set is the same as before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/072aed0efd18d7c8f0abafb182807484.png)'
  prefs: []
  type: TYPE_IMG
- en: This one provides a nice presentation of the features and a measurement of p-value
    for that specific feature’s significance. For example, if we use a 0.05 or 5%
    significance level (or 95% confidence level), we can eliminate the features where
    “P > |t|” is larger than 0.05.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: There are 43 such columns. Let’s drop these columns and see if the results improve.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ec99628d8067f6c8c8e04199dadefb4.png)'
  prefs: []
  type: TYPE_IMG
- en: The overall performance, as judged by the R-squared value, improved from 0.967
    to 0.972 and we reduced the number of columns, which makes our model and analysis
    more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Plot of the Predictions vs. Actuals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Question 14:**'
  prefs: []
  type: TYPE_NORMAL
- en: Create a scatter plot of predictions vs. actuals. We would expect all the points
    to lay across a straight line such as `f(x) = x` if all the predictions matched
    the actuals. Add such a straight line in red for comparison.
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bcee29cf4f4b942a17d73866cf3a019a.png)'
  prefs: []
  type: TYPE_IMG
- en: Scatter Plot of the Trained Model’s Predictions vs. Actuals
  prefs: []
  type: TYPE_NORMAL
- en: As we expected, the values are scattered around the straight line, demonstrating
    a good level of prediction generatd by the model. Where the dots are to the right
    side of the red line, it means that the model predicted a larger price compared
    to the actual, while the dots in the left side of the red line indicate the reverse.
  prefs: []
  type: TYPE_NORMAL
- en: Notebook with Practice Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Below is the notebook with both questions and answers that you can download
    and practice.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, we talked about how in some cases the simplest solution can be
    the most appropriate solution and introduced and implemented Linear Regression
    as such a solution in predictive machine learning tasks. We started by learning
    about the math behind linear regression and then implemented a model to predict
    car prices based on existing car attributes. We then measured the model’s performance
    and took certain measures to improve our model’s performance and finally visualized
    the comparison of the trained model’s predictions to the actuals using a scatterplot.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for Reading!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you found this post helpful, please [follow me on Medium](https://medium.com/@fmnobar)
    and subscribe to receive my latest posts!
  prefs: []
  type: TYPE_NORMAL
