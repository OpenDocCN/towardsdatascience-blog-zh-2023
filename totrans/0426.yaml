- en: Build Deployable Machine Learning Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/build-deployable-machine-learning-pipelines-a6d7035816a6](https://towardsdatascience.com/build-deployable-machine-learning-pipelines-a6d7035816a6)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Leverage Kedro to build production-ready machine learning pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://johnadeojo.medium.com/?source=post_page-----a6d7035816a6--------------------------------)[![John
    Adeojo](../Images/f6460fae462b055d36dce16fefcd142c.png)](https://johnadeojo.medium.com/?source=post_page-----a6d7035816a6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a6d7035816a6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a6d7035816a6--------------------------------)
    [John Adeojo](https://johnadeojo.medium.com/?source=post_page-----a6d7035816a6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a6d7035816a6--------------------------------)
    ·8 min read·Jun 30, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec434ef42ac9cdfcc488fd5ed7688ab9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Generated with Midjourney'
  prefs: []
  type: TYPE_NORMAL
- en: Background — Notebooks are not “Deployable”
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many data scientists’ initial encounters with coding take place through notebook-style
    user interfaces. Notebooks are indispensable for exploration — a critical aspect
    of our workflow. However, they’re not designed to be production-ready. This is
    a key issue I’ve observed among numerous clients, some of whom inquire about ways
    to productionise their notebooks. Rather than productionising your notebooks,
    the optimal route to production readiness is to craft modular, maintainable, and
    reproducible code.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, I present an example of a modular ML pipeline for training
    a model to classify fraudulent credit card transactions. By the conclusion of
    this article, I hope that you will:'
  prefs: []
  type: TYPE_NORMAL
- en: Gain an appreciation and understanding of modular ML pipelines.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feel inspired to build one for yourself.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you want to reap the benefits of deploying your machine learning models for
    maximum effect, writing modular code is an important step to take.
  prefs: []
  type: TYPE_NORMAL
- en: First a quick definition of [modular](https://en.wikipedia.org/wiki/Modular_programming)
    code. Modular code is software design paradigm that emphasizes separating a program
    out into independent, interchangeable modules. We should aim to approach this
    state with our machine learning pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Quick Detour — The project, the Data, and Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The machine learning project is sourced from [Kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud).
    The dataset consists of 284,807 anonymised credit card transactions for which
    492 are fraudulent. The task is to build a classifier to detect the fraudulent
    transactions.
  prefs: []
  type: TYPE_NORMAL
- en: The Data for this project is licensed for any purpose including commercial use
    under the [Open Data Commons](https://opendatacommons.org/licenses/dbcl/1-0/).
  prefs: []
  type: TYPE_NORMAL
- en: I have used a deep learning approach leveraging the [Ludwig](https://ludwig.ai/latest/)
    , an open-source framework for declarative deep learning. I won’t go into the
    details of Ludwig here, however I have previously written an article on the [framework](https://medium.com/towards-data-science/ludwig-a-friendlier-deep-learning-framework-946ee3d3b24).
  prefs: []
  type: TYPE_NORMAL
- en: The Ludwig deep neural network is configured with a **.yaml** file. For those
    that are curious you can find this in the [model registry GitHub](https://github.com/john-adeojo/Credit-Card-Fraud-Model-Registry/blob/main/model%20yaml%20files/model_1a.yaml).
  prefs: []
  type: TYPE_NORMAL
- en: Building Modular Pipelines with Kedro
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building modular machine learning pipelines has been made easier with open-source
    tools, my favourite of these is [Kedro](https://kedro.org/). Not only because
    I have seen this used successfully in industry, but also because it helped me
    develop my software engineering skills.
  prefs: []
  type: TYPE_NORMAL
- en: Kedro is an open-source framework (licensed under Apache 2.0) for creating reproducible,
    maintainable and modular data science code. I came across it while I was developing
    the AI strategy for a bank, considering which tools my team could utilise to build
    production-ready code.
  prefs: []
  type: TYPE_NORMAL
- en: '*Disclaimer: I have no affiliation with Kedro or McKinsey’s QuantumBlack, the
    creators of this open-source tool.*'
  prefs: []
  type: TYPE_NORMAL
- en: The Model Training Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/521cb43a198ee7b9415263cf30f40f77.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: End-to-end model training pipeline generated with Kedro viz'
  prefs: []
  type: TYPE_NORMAL
- en: Kedro conveniently allows you to visualise your pipelines, a fantastic feature
    that can help bring clarity to your code. The pipeline is standard for machine
    learning so I will only briefly touch on each aspect.
  prefs: []
  type: TYPE_NORMAL
- en: '**Import Data**: Import the credit card transaction data from an external source.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Split Data**: Use random split to split the data into training and holdout
    sets.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Run Experiment**: Uses the Ludwig framework to train a deep neural network
    on the train data set. The Ludwig experiment API conveniently saves model artefacts
    for every experiment run.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Run Predictions**: Uses the model trained in the previous step to run predictions
    on the holdout dataset.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model Diagnostics**: Produces two diagnostic charts. First, the tracking
    the cross-entropy loss over each epoch. Second, the [ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)
    measuring the model’s performance on the holdout dataset.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/5b057563d48b3bc1b69a199f33954e1d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Loss Curve from model training process'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9023e2e090e82f08b52fe22fbc91d82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: ROC Curve from model evaluation on the holdout dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Core Components of the Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have established a high-level view, let’s get into some of the core
    components of this pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Project Structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Kedro provides a templated directory structure that is established when you
    initiate a project. From this base, you can programmatically add more pipelines
    to your directory structure. This standardised structure ensures that every machine
    learning project is identical and easy to document, thereby facilitating ease
    of maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: Data Management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data plays a crucial role in machine learning. The ability to track your data
    becomes even more essential when employing machine learning models in a commercial
    setting. You often find yourself facing audits, or the necessity to productionise
    or reproduce your pipeline on someone else’s machine.
  prefs: []
  type: TYPE_NORMAL
- en: Kedro offers two methods for enforcing best practices in data management. The
    first is a directory structure, designed for machine learning workloads, providing
    distinct locations for the intermediate tables produced during data transformation
    and the model artefacts. The second method is the [data catalogue](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/conf/base/catalog.yml).
    As part of the Kedro workflow, you are required to register datasets within a
    .yaml configuration file, thereby enabling you to leverage these datasets in your
    pipelines. This approach may initially seem unusual, but it allows you and others
    working on your pipeline to track data with ease.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestration — Nodes and Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is really where the magic happens. Kedro provides you with pipeline functionality
    straight out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: The initial building block of your pipeline is the [nodes](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/src/fraud_detection_model/pipelines/train_model/nodes.py).
    Each executable piece of code can be encapsulated within a Node, which is simply
    a Python function that accepts an input and yields an output. You can then structure
    a [pipeline](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/src/fraud_detection_model/pipelines/train_model/pipeline.py)
    as a series of nodes. Pipelines are easily constructed by invoking the node and
    specifying the inputs and outputs. Kedro determines the execution order.
  prefs: []
  type: TYPE_NORMAL
- en: Once pipelines are constructed, they are registered in the provided **pipeline_registry.py**
    file. The beauty of this approach is that you can create multiple pipelines. This
    is particularly beneficial in machine learning, where you might have a data processing
    pipeline, a model training pipeline, an inference pipeline, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: Once set up, it’s straightforward enough to modify aspects of your pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Code snippet showing example of nodes.py script
  prefs: []
  type: TYPE_NORMAL
- en: Code snippet showing example of Pipeline script
  prefs: []
  type: TYPE_NORMAL
- en: Configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kedro’s best practices stipulate that all configurations should be handled through
    the provided [**parameters.yml**](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/conf/base/parameters.yml)
    file. From a machine learning perspective, hyperparameters fall into this category.
    This approach streamlines experimentation, as you can simply substitute one **parameters.yml**
    file with a set of hyperparameters for another, which is also much easier to track.
  prefs: []
  type: TYPE_NORMAL
- en: I have also included the locations of my Ludwig deep neural network [**model.yaml**](https://github.com/john-adeojo/Credit-Card-Fraud-Model-Registry/blob/main/model%20yaml%20files/model_1a.yaml)
    and the data source within the **parameters.yml** configuration. Should the model
    or the location of the data change — for instance, when moving between developers’
    machines — it would be incredibly straightforward to adjust these settings.
  prefs: []
  type: TYPE_NORMAL
- en: Code snippet showing the contents of the parameters.yml file
  prefs: []
  type: TYPE_NORMAL
- en: Reproducibility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kedro includes a [**requirements.txt**](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/src/requirements.txt)
    file as part of the templated structure. This makes it really straightforward
    to monitor your environment and exact library versions. However, should you prefer,
    you can employ other environment management methods, such as an **environment.yml**
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Establishing a Workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you’re developing machine learning pipelines and considering using Kedro,
    it can initially present a steep learning curve, but adopting a standard workflow
    will simplify the process. Here’s my suggested workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Establish your working environment**: I prefer using Anaconda for this task.
    I typically use an [**environment.yml**](https://github.com/john-adeojo/kedro-project/blob/main/environment.yml)
    file, containing all the dependencies needed for my environment, and employ the
    Anaconda Powershell command line to [create](https://gist.github.com/john-adeojo/fa22082328e65d0260fefd9b149e4b69)
    my environment from this.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create a Kedro project**: Once you have Kedro installed — which should hopefully
    be declared in your **environment.yml** — you can [create](https://docs.kedro.org/en/stable/get_started/new_project.html)
    a Kedro project through the [Anaconda](https://www.anaconda.com/) command line
    interface.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Explore in Jupyter Notebooks**: I construct an initial pipeline in Jupyter
    notebooks, a process familiar to most data scientists. The only difference is
    that, once your pipeline is built, you should tidy it up so that each cell could
    serve as one Node in your Kedro pipeline.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Register your data**: Record the input and outputs for each data processing
    or data ingestion step in the data [catalogue](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/conf/base/catalog.yml).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Add your pipeline**: After conducting your exploration in notebooks, you’ll
    want to [create a pipeline](https://docs.kedro.org/en/stable/tutorial/create_a_pipeline.html#:~:text=Note,the%20starter%20project.).
    This is achieved through the command line interface. Running this command will
    add an additional folder to ‘pipelines’, bearing the name of the pipeline you’ve
    just created. It’s within this folder that you’ll construct your nodes and pipelines.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Define your pipeline**: This is the stage where you start transferring the
    code from your Jupyter notebooks into the [**node.py**](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/src/fraud_detection_model/pipelines/train_model/nodes.py)
    file in your pipeline folder, ensuring that nodes you intend to be part of a pipeline
    have inputs and outputs. Once the nodes are set up, proceed to define your pipeline
    in the [**pipeline.py**](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/src/fraud_detection_model/pipelines/train_model/pipeline.py)
    file.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Register your pipelines**: The [**pipeline_registry.py**](https://github.com/john-adeojo/kedro-project/blob/main/fraud-detection/src/fraud_detection_model/pipeline_registry.py)
    file offers a template for you to register your newly created pipeline.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Run your project**: Once established, you can [run](https://docs.kedro.org/en/stable/nodes_and_pipelines/run_a_pipeline.html)
    any pipeline through the CLI and also [visualise](https://docs.kedro.org/en/stable/visualisation/index.html)
    your project.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Production-ready pipelines fit into a wider ecosystem of machine learning operations.
    Read my article on MLOps for a deeper dive.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/building-machine-learning-operations-for-businesses-6d0bfbbf2139?source=post_page-----a6d7035816a6--------------------------------)
    [## Building Machine Learning Operations for Businesses'
  prefs: []
  type: TYPE_NORMAL
- en: A Blueprint for Effective MLOps to Support Your AI Strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/building-machine-learning-operations-for-businesses-6d0bfbbf2139?source=post_page-----a6d7035816a6--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kedro is an excellent framework for delivering production-ready machine learning
    pipelines. Beyond the functionality discussed in this article, there are numerous
    integrations with other open-source libraries, as well as packages for documentation
    and testing. Kedro doesn’t solve every issue related to model deployment — for
    instance, model versioning is likely better handled by another tool such as DVC.
    However, it will assist data scientists in a commercial setting to produce more
    maintainable, modular, and reproducible code that is ready for production. There
    is a relatively steep learning curve for complete novices, but the documentation
    is clear and includes guided tutorials. As with any of these packages, the best
    way to learn is to dive in and experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Link to the full [GitHub repo](https://github.com/john-adeojo/kedro-project/tree/main)
  prefs: []
  type: TYPE_NORMAL
- en: '*Follow me on* [*LinkedIn*](https://www.linkedin.com/in/john-adeojo/)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Subscribe to medium to get more insights from me:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://johnadeojo.medium.com/membership?source=post_page-----a6d7035816a6--------------------------------)
    [## Join Medium with my referral link — John Adeojo'
  prefs: []
  type: TYPE_NORMAL
- en: I share data science projects, experiences, and expertise to assist you on your
    journey You can sign up to medium via…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: johnadeojo.medium.com](https://johnadeojo.medium.com/membership?source=post_page-----a6d7035816a6--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*Should you be interested in integrating AI or data science into your business
    operations, we invite you to schedule a complimentary initial consultation with
    us:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.data-centric-solutions.com/book-online?source=post_page-----a6d7035816a6--------------------------------)
    [## Book Online | Data-Centric Solutions'
  prefs: []
  type: TYPE_NORMAL
- en: Discover our expertise in helping businesses achieve ambitious goals with a
    free consultation. Our data scientists and…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.data-centric-solutions.com](https://www.data-centric-solutions.com/book-online?source=post_page-----a6d7035816a6--------------------------------)
  prefs: []
  type: TYPE_NORMAL
