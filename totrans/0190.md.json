["```py\nfrom transformers import ViTForImageClassification\nimport torch\nimport numpy as np\n\nmodel = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n```", "```py\nViTForImageClassification(\n  (vit): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): PatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0): ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n\n        .......\n\n        (11): ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  )\n  (classifier): Linear(in_features=768, out_features=1000, bias=True)\n)\n```", "```py\nproj = model.vit.embeddings.patch_embeddings.projection\ntorch.allclose(torch.sum(image[0, :, 0:16, 0:16] * w[0]) + b[0],\n               proj(image)[0][0][0, 0], atol=1e-6)\n# True\n\ntorch.allclose(torch.sum(image[0, :, 16:32, 0:16] * w[0]) + b[0],\n                 proj(image)[0][0][1, 0], atol=1e-6)\n\n# True\n```", "```py\nembeddings = model.vit.embeddings.patch_embeddings.projection(image)\n# shape (batch_size, 196, 768)\nembeddings = embeddings.flatten(2).transpose(1, 2)\n```", "```py\n batch_size = 1 \nF = 768 # number of filters\nH1 = 14 # output dimension hight - 224/16\nW1 = 14 # output dimension width - 224/16\nstride = 16\nHH = 16 # patch hight\nWW = 16 # patch width\nw = model.vit.embeddings.patch_embeddings.projection.weight\nb = model.vit.embeddings.patch_embeddings.projection.bias\n\nout = np.zeros((N, F, H1, W1))\nchunks = []\nfor n in range(batch_size):\n    for f in range(F):\n        for i in range(H1):\n            for j in range(W1):\n                # perform convolution operation\n                out[n, f, i, j] = torch.sum( image[n, :, i*stride:i*stride+HH, j*stride : j*stride + WW] * w[f] ) + b[f]\n\nnp.allclose(out[0], embeddings[0].detach().numpy(), atol=1e-5)\n# True\n```", "```py\ncls_token = nn.Parameter(torch.randn(1, 1, 768))\ncls_tokens = cls_token.expand(batch_size, -1, -1)\n# append [CLS] token\nembeddings = torch.cat((cls_tokens, embeddings), dim=1)\n```", "```py\nembeddings = embeddings + model.vit.embeddings.position_embeddings\n```", "```py\n# compute the embedding\nembeddings = model.vit.embeddings.patch_embeddings.projection(image)\nembeddings = embeddings.flatten(2).transpose(1, 2)\n# append [CLS] token\ncls_token = model.vit.embeddings.cls_token\nembeddings = torch.cat((cls_tokens, embeddings), dim=1)\n# positional embedding\nembeddings = embeddings + self.position_embeddings\n# droput\nembeddings = model.vit.embeddings.dropout(embeddings) \n```", "```py\n(0): ViTLayer(\n  (attention): ViTAttention(\n    (attention): ViTSelfAttention(\n      (query): Linear(in_features=768, out_features=768, bias=True)\n      (key): Linear(in_features=768, out_features=768, bias=True)\n      (value): Linear(in_features=768, out_features=768, bias=True)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (output): ViTSelfOutput(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n  )\n  (intermediate): ViTIntermediate(\n    (dense): Linear(in_features=768, out_features=3072, bias=True)\n  )\n  (output): ViTOutput(\n    (dense): Linear(in_features=3072, out_features=768, bias=True)\n    (dropout): Dropout(p=0.0, inplace=False)\n  )\n  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n)\n```", "```py\nimport math \nimport torch.nn as nn\n\ntorch.manual_seed(0)\n\nhidden_size = 768\nnum_attention_heads = 12\nattention_head_size = hidden_size // num_attention_heads # 64\n\nhidden_states = embeddings\n\n# apply LayerNorm to the embeddings\nhidden_states = model.vit.encoder.layer[0].layernorm_before(hidden_states)\n\n# take first layer of the Transformer\nlayer_0 = model.vit.encoder.layer[0]\n\n# shape (768, 64) \nkey_matrix = layer_0.attention.attention.key.weight.T[:, :attention_head_size]\nkey_bias = layer_0.attention.attention.key.bias[:attention_head_size]\n\nquery_matrix = layer_0.attention.attention.query.weight.T[:, :attention_head_size] \nquery_bias = layer_0.attention.attention.query.bias[:attention_head_size]\n\nvalue_matrix = layer_0.attention.attention.value.weight.T[:, :attention_head_size]\nvalue_bias = layer_0.attention.attention.value.bias[:attention_head_size]\n\n# compute key, query and value for the first head attention\n# all of shape (b_size, 197, 64)\nkey_1head = hidden_states @ key_matrix + key_bias\nquery_1head = hidden_states @ query_matrix + query_bias\nvalue_1head = hidden_states @ value_matrix + value_bias\n```", "```py\n# shape (b_size, 197, 197)\n# compute the attention scores by dot product of query and key\nattention_scores_1head = torch.matmul(query_1head, key_1head.transpose(-1, -2))\n\nattention_scores_1head = attention_scores_1head / math.sqrt(attention_head_size)\nattention_probs_1head = nn.functional.softmax(attention_scores_1head, dim=-1)\n\n# contextualized embedding for this layer\ncontext_layer_1head = torch.matmul(attention_probs_1head, value_1head)\n```", "```py\npatch_n = 1\n# shape (, 197)\nprint(attention_probs_1head[0, patch_n])\n[2.4195e-01, 7.3293e-01, ..,\n        2.6689e-06, 4.6498e-05, 1.1380e-04, 5.1591e-06, 2.1265e-05], \n```", "```py\n# shape (, 197)\n[2.6356e-01, 1.2783e-03, 2.6888e-01, ... , 1.8458e-02]\n```", "```py\ndef transpose_for_scores(x: torch.Tensor) -> torch.Tensor:\n    new_x_shape = x.size()[:-1] + (num_attention_heads, attention_head_size)\n    x = x.view(new_x_shape)\n    return x.permute(0, 2, 1, 3)\n\nmixed_query_layer = layer_0.attention.attention.query(hidden_states)\n\nkey_layer = transpose_for_scores(layer_0.attention.attention.key(hidden_states))\nvalue_layer = transpose_for_scores(layer_0.attention.attention.value(hidden_states))\nquery_layer = transpose_for_scores(mixed_query_layer)\n\n# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\nattention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\nattention_scores = attention_scores / math.sqrt(attention_head_size)\n\n# Normalize the attention scores to probabilities.\nattention_probs = nn.functional.softmax(attention_scores, dim=-1)\n\n# This is actually dropping out entire tokens to attend to, which might\n# seem a bit unusual, but is taken from the original Transformer paper.\nattention_probs = layer_0.attention.attention.dropout(attention_probs)\n\ncontext_layer = torch.matmul(attention_probs, value_layer)\n\ncontext_layer = context_layer.permute(0, 2, 1, 3).contiguous()\nnew_context_layer_shape = context_layer.size()[:-2] + (hidden_size,)\ncontext_layer = context_layer.view(new_context_layer_shape)\n```", "```py\noutput_weight = layer_0.attention.output.dense.weight\noutput_bias = layer_0.attention.output.dense.bias\n\nattention_output = context_layer @ output_weight.T + output_bias\nattention_output = layer_0.attention.output.dropout(attention_output)\n```", "```py\nfirst_patch_embed = embeddings[0][0]\n# compute first patch mean\nfirst_patch_mean = first_patch_embed.mean()\n# compute first patch variance\nfirst_patch_std = (first_patch_embed - first_patch_mean).pow(2).mean()\n# standardize the first patch\nfirst_patch_standardized = (first_patch_embed - first_patch_mean) / torch.sqrt(first_patch_std + 1e-12)\n# apply trained weight and bias vectors\nfirst_patch_norm = layer_0.layernorm_before.weight * first_patch_standardized + layer_0.layernorm_before.bias\n```", "```py\ntransformations = nn.Sequential([nn.Linear(), nn.ReLU(), nn.Linear()])\noutput = input + transformations(input)\n```", "```py\n# first residual connection - NOTE the hidden_states are the \n# `embeddings` here\nhidden_states = attention_output + hidden_states\n\n# in ViT, layernorm is also applied after self-attention\nlayer_output = layer_0.layernorm_after(hidden_states)\n```", "```py\nlayer_output_intermediate = layer_0.intermediate.dense(layer_output)\nlayer_output_intermediate = layer_0.intermediate.intermediate_act_fn(layer_output_intermediate)\n```", "```py\n# linear projection\noutput_dense = layer_0.output.dense(layer_output_intermediate)\n# dropout\noutput_drop = layer_0.output.dropout(output_dense)\n# residual connection - NOTE these hidden_states are computed in \n# Intermediate \noutput_res = output_drop + hidden_states # shape (b_size, 197, 768)\n```", "```py\ntorch.manual_seed(0)\n# masking heads in a given layer\nlayer_head_mask = None\n# output attention probabilities\noutput_attentions = False\n\nembeddings = model.vit.embeddings(image)\nhidden_states = embeddings\nfor l in range(12):\n    hidden_states = model.vit.encoder.layer[l](hidden_states, layer_head_mask, output_attentions)[0]\n\noutput = model.vit.layernorm(sequence_output)\n```", "```py\npooled_output = output[:, 0, :] # shape (b_size, 768)\n```", "```py\nlogits = model.classifier(pooled_output) # shape (b_size, num_classes)\n```"]