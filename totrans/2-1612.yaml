- en: 'Other ML Jargons: Sparse and Dense Representations of Texts for Machine Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他机器学习术语：文本的稀疏和密集表示
- en: 原文：[https://towardsdatascience.com/other-ml-jargons-sparse-and-dense-representations-of-texts-for-machine-learning-21fcd7a01410](https://towardsdatascience.com/other-ml-jargons-sparse-and-dense-representations-of-texts-for-machine-learning-21fcd7a01410)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/other-ml-jargons-sparse-and-dense-representations-of-texts-for-machine-learning-21fcd7a01410](https://towardsdatascience.com/other-ml-jargons-sparse-and-dense-representations-of-texts-for-machine-learning-21fcd7a01410)
- en: OTHER ML JARGONS
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他机器学习术语
- en: A Brief Introduction to Vectorization and its Importance in the Context of NLP
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量化的简要介绍及其在自然语言处理中的重要性
- en: '[](https://nroy0110.medium.com/?source=post_page-----21fcd7a01410--------------------------------)[![Nabanita
    Roy](../Images/83ab7766a28c79371ebf9517e1f273d2.png)](https://nroy0110.medium.com/?source=post_page-----21fcd7a01410--------------------------------)[](https://towardsdatascience.com/?source=post_page-----21fcd7a01410--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----21fcd7a01410--------------------------------)
    [Nabanita Roy](https://nroy0110.medium.com/?source=post_page-----21fcd7a01410--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://nroy0110.medium.com/?source=post_page-----21fcd7a01410--------------------------------)[![Nabanita
    Roy](../Images/83ab7766a28c79371ebf9517e1f273d2.png)](https://nroy0110.medium.com/?source=post_page-----21fcd7a01410--------------------------------)[](https://towardsdatascience.com/?source=post_page-----21fcd7a01410--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----21fcd7a01410--------------------------------)
    [Nabanita Roy](https://nroy0110.medium.com/?source=post_page-----21fcd7a01410--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----21fcd7a01410--------------------------------)
    ·9 min read·Feb 15, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----21fcd7a01410--------------------------------)
    ·9 分钟阅读·2023年2月15日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a2550ebb25c16994eb499f8e089b474a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2550ebb25c16994eb499f8e089b474a.png)'
- en: Photo by [Compare Fibre](https://unsplash.com/@comparefibre?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Compare Fibre](https://unsplash.com/@comparefibre?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '**Introduction**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: Matrices and vectors are quantified information that Machine Learning(ML) algorithms
    require for learning patterns and making predictions. For applying these techniques
    to textual data as well, numeric representations of the texts are engineered to
    form matrices that hold the relevant information from those texts. The concepts
    of “Sparsity” and “Density” arrive at efficiently designing and constructing these
    matrices for all high-dimensional data processing use-cases in the world of Artificial
    Intelligence.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵和向量是机器学习（ML）算法进行模式学习和预测所需的量化信息。将这些技术应用于文本数据时，文本的数值表示被工程化为矩阵，以包含文本中的相关信息。“稀疏性”和“密集性”概念有助于高效设计和构建这些矩阵，用于人工智能领域中的所有高维数据处理用例。
- en: Significance of Vector Representations for NLP
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量表示在自然语言处理中的重要性
- en: 'Representing text data as vectors are necessary for applying Machine Learning
    techniques to make predictions, recommendations, or clusters. In NLP, the concept
    of “*similar words occur in similar contexts”* is fundamental. Let’s see how:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本数据表示为向量是应用机器学习技术进行预测、推荐或聚类的必要条件。在自然语言处理（NLP）中，“*相似的单词出现在相似的上下文中*”这一概念是基础的。让我们来看看：
- en: In **Text Classification** use-cases like categorizing support tickets, spam
    detection, fake news detection, and feedback sentiment analysis, texts having
    similar words are classified into a particular category.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**文本分类**用例中，如支持票据的分类、垃圾邮件检测、虚假新闻检测和反馈情感分析，具有相似单词的文本会被分类到特定类别中。
- en: In R**ecommendation Systems**, people with similar profile details, browsing
    history, and past orders indicate similar choices or tastes in products. This
    information is used to make recommendations.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**推荐系统**中，具有相似个人资料信息、浏览历史和过去订单的人表示对产品有类似的选择或口味。这些信息用于生成推荐。
- en: '**Unsupervised Clustering** looks for patterns and similar words in the texts
    to group documents and articles. Typical applications include segregating news
    articles, trend analysis, and customer segmentation.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**无监督聚类**在文本中寻找模式和相似的单词，以便对文档和文章进行分组。典型应用包括新闻文章的分类、趋势分析和客户细分。'
- en: In **Information Retrieval** systems, indexed documents are matched with queries,
    sometimes in a “fuzzy” way and the collection of matched documents is returned
    to the user. Besides, the measure of similarity is used to rank the search.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**信息检索**系统中，索引文档会与查询进行匹配，有时以“模糊”的方式进行，然后将匹配的文档集合返回给用户。此外，相似度度量用于对搜索结果进行排序。
- en: Hence, capturing similarities in these vectors is a primary research area in
    the NLP domain. These vectors are projected in an N-dimensional plane and then
    patterns in these vectors in the N-dimensional space are extracted to categorize
    the texts. Sometimes, dimensionality reduction techniques are applied, like PCA
    or t-SNE. The design of the vectors controls the overall performance of text-based
    ML models and is, hence, crucial.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，捕捉这些向量中的相似性是自然语言处理领域的一个主要研究方向。这些向量被投影到一个N维平面中，然后从N维空间中的这些向量中提取模式以对文本进行分类。有时会应用降维技术，如PCA或t-SNE。向量的设计控制了基于文本的机器学习模型的整体性能，因此至关重要。
- en: The vector designs are broadly classified as “Sparse” (meaning scarcely populated)
    and “Dense” (meaning densely populated) vectors. In this article, I have recalled
    the concepts of matrices and vectors from a mathematical perspective, and then
    discussed these two classes of vectorization techniques — sparse vector representations
    and dense vector representations. including a demo using Scikit Learn and Gensim,
    respectively. I have also concluded this article with an overview of the applications
    and usability of these representations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 向量设计大致分为“稀疏”（意指很少填充）和“密集”（意指密集填充）两类。在本文中，我从数学的角度回顾了矩阵和向量的概念，然后讨论了这两种向量化技术——稀疏向量表示和密集向量表示，包括分别使用
    Scikit Learn 和 Gensim 的演示。我还在文章末尾总结了这些表示的应用和实用性。
- en: A Primer to Matrices and Vectors
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵和向量的入门
- en: Mathematically, *a matrix is defined as a 2-dimensional rectangular array of
    numbers.* If the array has *m* rows and *n* columns, then it is a matrix of size
    *m × n*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上，*矩阵定义为一个二维矩形数字数组。* 如果数组有 *m* 行和 *n* 列，那么它是一个 *m × n* 的矩阵。
- en: 'If a matrix has only one row OR only one column it is called a vector. A *1×n*
    matrix or vector is a row vector (where there are *n* columns but only *1* row)
    and an *m × 1* matrix or vector is a column vector (where there are *m* rows but
    only *1* column). Here’s an image that clearly demonstrates this:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个矩阵只有一行或只有一列，它被称为向量。*1×n* 矩阵或向量是行向量（其中有 *n* 列但只有 *1* 行），而 *m × 1* 矩阵或向量是列向量（其中有
    *m* 行但只有 *1* 列）。这里有一张清晰演示这一点的图片：
- en: '![](../Images/a79c68118010432cfdd21fbc389d8c03.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a79c68118010432cfdd21fbc389d8c03.png)'
- en: 'Image source: [Linear Algebra for Data Science Ep1 — Introduction to Vectors
    and Matrices using Python](/introduction-to-vectors-and-matrices-using-python-for-data-science-e836e014eb12)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[数据科学线性代数 Ep1 — 使用 Python 介绍向量和矩阵](/introduction-to-vectors-and-matrices-using-python-for-data-science-e836e014eb12)
- en: Here’s a [primer for scalars, vectors, and matrices](https://www.mathsisfun.com/algebra/scalar-vector-matrix.html)
    and an [Introduction to Vectors and Matrices using Python](/introduction-to-vectors-and-matrices-using-python-for-data-science-e836e014eb12)
    for Data Science.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个 [标量、向量和矩阵的入门](https://www.mathsisfun.com/algebra/scalar-vector-matrix.html)
    和 [数据科学中使用 Python 介绍向量和矩阵](/introduction-to-vectors-and-matrices-using-python-for-data-science-e836e014eb12)。
- en: Sparse Representations | Matrices | Vectors
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏表示 | 矩阵 | 向量
- en: '![](../Images/3b14d8e5f8e6f8b1c9e99ed0eceaadf1.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b14d8e5f8e6f8b1c9e99ed0eceaadf1.png)'
- en: Photo by [Henning Witzel](https://unsplash.com/@henning?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Henning Witzel](https://unsplash.com/@henning?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In almost all real-world cases, the count-based quantified numeric representation
    of information is **sparse in nature**, in other words, ***the numeric representation*
    *contains only a fraction that is useful to you in an ocean of numbers.***
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在几乎所有实际应用中，基于计数的量化数值表示信息通常是**稀疏的**，换句话说，***数值表示* *只包含在海量数字中对你有用的一小部分。***
- en: It is because, intuitively, in a collection of documents, only words that are
    articles, prepositions, conjunctions, and pronouns are overtly used and therefore,
    have a higher frequency of occurrence. However, in a collection of sports news
    articles, the terms ‘soccer’ or ‘basketball’, occurrences of which would help
    us determine which sport is the article associated with, occurs only a few times
    but is not of a very high frequency.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为，从直观上看，在文档集合中，只有作为文章、介词、连词和代词的单词被明显使用，因此具有较高的出现频率。然而，在一组体育新闻文章中，诸如‘soccer’或‘basketball’的术语，其出现次数有助于确定文章关联的运动类型，尽管出现次数较少，但频率并不很高。
- en: Now, if we construct a vector per new article, assuming there are 50 words per
    article, the word ‘soccer’ would occur about 5 times. Hence, 45 out of 50 times,
    the elements in the vector will be zero, which indicates the absence of the word
    we are focusing on. Therefore, 90% of the vector of length 50 is redundant. This
    is an example of a [**one-hot vector**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)generation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们为每个新文章构造一个向量，假设每篇文章有50个单词，那么‘soccer’会出现约5次。因此，在50次中，45次向量元素将为零，这表示我们关注的单词的缺失。因此，长度为50的向量中90%是冗余的。这是[**one-hot向量**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)生成的一个例子。
- en: Another typical example of sparse matrix generation is the [**Count Vectorizer**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)which
    determines how many times a word has occurred in a document. It generates a matrix
    of “count vectors” per document to constitute a matrix of size *d × v* where d
    is the number of documents and v is the number of words or vocabulary in the collection
    of documents.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏矩阵生成的另一个典型例子是[**Count Vectorizer**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)，它确定一个单词在文档中出现的次数。它为每个文档生成一个“计数向量”矩阵，从而构成一个*d
    × v*的矩阵，其中d是文档的数量，v是文档集合中的单词或词汇的数量。
- en: '**Here’s a demonstration of how a Count Vectorizer works:**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**这里演示了Count Vectorizer的工作原理：**'
- en: Below are [four different meanings of the word ‘demo’](https://www.google.com/search?q=demo&oq=demo&aqs=chrome..69i57j69i61.775j0j4&sourceid=chrome&ie=UTF-8),
    each of which represent one document ~
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是[‘demo’一词的四种不同含义](https://www.google.com/search?q=demo&oq=demo&aqs=chrome..69i57j69i61.775j0j4&sourceid=chrome&ie=UTF-8)，每种含义代表一个文档
    ~
- en: '**Document 1:** a demonstration of a product or technique'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**文档1：**演示一个产品或技术'
- en: '**Document 2:** a public meeting or march protesting against something or expressing
    views on a political issue'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**文档2：**公开会议或游行，抗议某事或表达对政治问题的观点'
- en: '**Document 3:** record a song or piece of music to demonstrate the capabilities
    of a musical group or performer or as preparation for a full recording'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**文档3：**录制一首歌曲或音乐作品，以展示音乐团队或表演者的能力，或作为完整录音的准备'
- en: '**Document 4:** demonstrate the capabilities of software or another product'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**文档4：**演示软件或其他产品的功能'
- en: I used [**Scikit Learn’s CountVectorizer**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)
    implementation to generate this sparse matrix for these four “documents”. Below
    is the code I have used 👩‍💻
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了[**Scikit Learn的CountVectorizer**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)实现来生成这四个“文档”的稀疏矩阵。以下是我使用的代码👩‍💻
- en: 'Image Source: Author'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: 'The output of *_cv.toarray()* is of the numeric representation of the words
    in a **4** *×* **34 array** (*converted using .toarray() from the vector*)as in
    the screenshot below:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*_cv.toarray()*的输出是**4** *×* **34数组**中单词的数字表示（*使用.toarray()从向量转换*），如下图所示：'
- en: '![](../Images/977ab67300d896810371527907ee6841.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/977ab67300d896810371527907ee6841.png)'
- en: 'Find this example in this Jupiter Notebook | Image Source: Author'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个Jupiter Notebook中找到这个例子 | 图片来源：作者
- en: 'In the matrix:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在矩阵中：
- en: zero represents no occurrence at all (basically no information)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: zero表示完全没有出现（基本上没有信息）
- en: anything more than zero is the number of times the word has occurred in the
    four documents (some useful and some redundant information).
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何大于零的数值是单词在四个文档中出现的次数（一些有用的信息和一些冗余的信息）。
- en: 34 is the size of the vocabulary (or the total number of unique words in the
    documents), hence the shape is 4 x 34.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 34是词汇表的大小（或文档中唯一单词的总数），因此形状为4 x 34。
- en: The simplest measure of sparsity is the fraction of the total number of zeroes
    over the total number of elements, in this case ~
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏性的最简单衡量标准是零元素占总元素的比例，在这种情况下约为~
- en: Number of zeros = 93
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零的数量 = 93
- en: Number of elements in the array = 4 *×* 34 *=* 136
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数组中的元素数量 = 4 *×* 34 *=* 136
- en: Therefore, more than 50% of the array has no information at all (93 out of 136),
    yet it is a high-dimensional matrix that needs more memory (increased [**Space
    complexity**](https://en.wikipedia.org/wiki/Space_complexity)) and computation
    time (increased [**Time complexity**](https://en.wikipedia.org/wiki/Time_complexity)).
    If a machine learning model is fed with this high-dimensional data, it will find
    it difficult to
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数组中超过50%的元素完全没有信息（93个中的136个），但这是一个高维矩阵，需要更多的内存（增加的 [**空间复杂度**](https://en.wikipedia.org/wiki/Space_complexity)）和计算时间（增加的
    [**时间复杂度**](https://en.wikipedia.org/wiki/Time_complexity)）。如果机器学习模型使用这个高维数据，它将发现很难处理。
- en: find patterns
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找模式
- en: will be far too expensive to tune weights for all the dimensions
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为所有维度调整权重将花费过高。
- en: it will eventually lead to latency issues during prediction time
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它最终会导致预测时的延迟问题。
- en: Sparsity is analogous to the concept of the curse of dimensionality. Recommender
    systems and collaborative filtering techniques, count-based data representations
    including the famous TF-IDF are prone to issues related to the sparsity of textual
    data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏性类似于维度诅咒的概念。推荐系统和协同过滤技术、计数数据表示，包括著名的TF-IDF，都容易受到文本数据稀疏性相关的问题。
- en: Dense Representations | Matrices | Vectors
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稠密表示 | 矩阵 | 向量
- en: '![](../Images/c64852af08fb0022efd2856c916bbdfc.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c64852af08fb0022efd2856c916bbdfc.png)'
- en: Photo by [Mike L](https://unsplash.com/@wheremikeat?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [Mike L](https://unsplash.com/@wheremikeat?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 拍摄
- en: The way out? A representation that has more information and less redundancy,
    is mathematically defined as a matrix or a vector where most elements are non-zero.
    Such data representations are called dense matrices or dense vectors. They are
    also usually smaller in size than sparse matrices.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 出路？一种包含更多信息且冗余更少的表示，数学上定义为大多数元素非零的矩阵或向量。这种数据表示被称为稠密矩阵或稠密向量。它们通常比稀疏矩阵的尺寸更小。
- en: Advantages of using Dense Vectors for Machine Learning
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用稠密向量进行机器学习的优点
- en: Smaller the dimension, the faster and easier to optimize weights for ML models
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 维度越小，优化机器学习模型的权重就越快、越容易。
- en: Even though dense vectors could have smaller dimensions than sparse matrices,
    they could also be large enough to challenge computational infrastructures (imagine
    Word2Vec or BERT-based representations), but still would contain rich and useful
    information like syntactical, semantical, or morphological relationships.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽管稠密向量的维度可能比稀疏矩阵小，但它们也可能足够大以挑战计算基础设施（想象一下Word2Vec或基于BERT的表示），但仍然包含丰富且有用的信息，如句法、语义或形态关系。
- en: They generalize relationships between textual elements better which is clearly
    demonstrated by the success of word2vec algorithms
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们对文本元素之间关系的概括能力更强，这一点通过word2vec算法的成功得到了明确证明。
- en: 'Common NLP techniques for transforming sparse to dense representations:'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将稀疏表示转换为稠密表示的常见自然语言处理技术：
- en: '[**Word2Vec**](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html):
    It is one of the most popular schools of algorithms that learns dense representation
    using shallow Neural Networks (NN) while trying to predict the probable word(s)
    and capture semantic relations.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**Word2Vec**](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html):
    这是最受欢迎的算法之一，使用浅层神经网络学习稠密表示，同时尝试预测可能的词汇并捕捉语义关系。'
- en: '[**FastText**](https://fasttext.cc/): Another algorithm with the same objective
    using shallow NNs, except that FastText character-level n-grams. However, Word2Vec
    has been noted to work best for English but FastText is better for morphologically
    rich languages like Arabic, German, and Russian. Besides, it captures syntactic
    features better than semantic ones.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**FastText**](https://fasttext.cc/): 另一种使用浅层神经网络实现相同目标的算法，只是FastText使用了字符级n-grams。然而，Word2Vec被认为对英语效果最好，但FastText对形态丰富的语言如阿拉伯语、德语和俄语更为适用。此外，它在捕捉句法特征方面优于语义特征。'
- en: '[**GloVe**](https://nlp.stanford.edu/projects/glove/): Again same! Learns dense
    representation but based on probability of co-occurrence.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**GloVe**](https://nlp.stanford.edu/projects/glove/): 同样！学习密集表示，但基于共现的概率。'
- en: Here is a compact demo of how to obtain dense representations using [Gensim’s
    word2vec](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html)
    algorithm.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简明的演示，展示如何使用[**Gensim的word2vec**](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html)算法获取密集表示。
- en: Below is the dense representation of the word ‘demonstration’ of length 10 (note
    the value of *vector_size* argument for *word2vec* model is 10). Note how there
    are no zeros in this numeric representation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是词汇‘demonstration’的长度为10的密集表示（请注意*vector_size*参数对于*word2vec*模型的值是10）。注意此数值表示中没有零。
- en: '![](../Images/ee568a02c0a6725fdbadd9a3e87f1a71.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee568a02c0a6725fdbadd9a3e87f1a71.png)'
- en: 'Image Source: Author'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Usually, the higher the vector size, the better the knowledge captured, especially
    semantic information. These are extremely useful for assessing text similarities
    and in unsupervised techniques for text processing.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，向量的尺寸越大，捕捉的知识就越好，特别是语义信息。这些对评估文本相似性和在无监督的文本处理技术中非常有用。
- en: 'Also, below is a snapshot of the dense vectors obtained for each word in our
    first document “*a demonstration of a product or technique”* appended in a list,
    corresponding to the sequence of occurrence within the document*.* This time I
    did not clean and kept the texts as is, hence there are seven word-vectors in
    the list:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，以下是我们第一个文档“*产品或技术的演示*”中每个单词获得的密集向量的快照，这些向量按文档中出现的顺序排列。这次我没有清理文本，而是保持原样，因此列表中有七个词向量：
- en: '![](../Images/89598c769571754a487e49dc67430f96.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89598c769571754a487e49dc67430f96.png)'
- en: 'Dense vectors of length 10 were obtained for the document ‘*a demonstration
    of a product or technique’ showing that this numeric representation contains no
    non-zero elements |* Image Source: Author'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对文档‘*产品或技术的演示*’获得的长度为10的密集向量显示该数值表示中没有非零元素 |* 图片来源：作者
- en: '**Dense vectors like these could also be pre-trained** on a large corpus, usually
    made available to be accessed online. Imagine a dictionary with, as usual, an
    index of all the unique words but their lexical meanings are replaced by pre-trained
    word vectors containing their numeric representations, quite like a **“Quantified
    Dictionary”**. Here are two popular “Quantified Dictionaries” ready to be downloaded
    and applied to text processing tasks:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**像这样的密集向量也可以在大型语料库上进行预训练**，这些语料库通常可以在线访问。想象一下一个字典，像往常一样列出所有唯一的词汇，但其词汇意义被包含其数值表示的预训练词向量替代，颇似一个**“量化字典”**。这里有两个流行的“量化字典”可以下载并应用于文本处理任务：'
- en: '[**Google**](https://code.google.com/archive/p/word2vec/)**: Trained using
    Google News dataset** containing about 100 billion words. The word vectors of
    size 300 are available to download from [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing).
    The vocabulary size is 3 million.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Google**](https://code.google.com/archive/p/word2vec/)**：使用Google新闻数据集训练**，包含约1000亿个词汇。300维的词向量可以从[这里](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)下载。词汇表的大小为300万。'
- en: '[**GloVe**](https://nlp.stanford.edu/projects/glove/)**: Trained using Wikipedia
    articles** and contains 50, 100, 300, and 500-dimensional word vectors, ready
    to download from [here](https://nlp.stanford.edu/data/glove.6B.zip). The vocabulary
    size is 400k.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**GloVe**](https://nlp.stanford.edu/projects/glove/)**：使用Wikipedia文章训练**，包含50、100、300和500维的词向量，可以从[这里](https://nlp.stanford.edu/data/glove.6B.zip)下载。词汇表的大小是400k。'
- en: '**Conclusion**'
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: Traditionally, one-hot vectors, word-frequency matrices (or count vectors),
    and TF-IDF scores (Sparse representations) were used for text analytics. They
    do not preserve any information about semantics. However, the modern approach
    of obtaining word embeddings using Neural Networks(like Word2Vec) or more sophisticated
    statistical approaches with normalization techniques (like GloVe) does a better
    job of retaining the “meanings” of words and enables us to cluster words occurring
    in similar contexts. But, they also come with complexities in terms of higher
    computation times which makes them expensive to scale (depending on the learning
    hyperparameters, especially with higher vector representation size). Besides,
    they are also harder to explain.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，一维热编码向量、词频矩阵（或计数向量）和TF-IDF评分（稀疏表示）被用于文本分析。它们并不保留任何语义信息。然而，现代方法通过神经网络（如Word2Vec）或更复杂的统计方法和归一化技术（如GloVe）来获取词嵌入，更好地保留了词的“意义”，并使我们能够对出现于类似上下文中的词进行聚类。但这些方法也带来了计算时间更长的复杂性，这使得它们在扩展时成本较高（取决于学习超参数，特别是更高的向量表示大小）。此外，它们也更难以解释。
- en: '![](../Images/481dda7eb5f0a946abc59bd48fe1787e.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/481dda7eb5f0a946abc59bd48fe1787e.png)'
- en: Photo by [Michał Parzuchowski](https://unsplash.com/@mparzuchowski?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Michał Parzuchowski](https://unsplash.com/@mparzuchowski?utm_source=medium&utm_medium=referral)拍摄，发布在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)。
- en: In all real-world cases, both approaches are applicable. For example, in the
    famous [Spam Classification Dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset/code),
    sparse representations yield near-perfect model performances. In such cases, we
    do not need to compute dense vector embeddings to achieve better performance since
    we successfully achieve our objectives with simple and transparent approaches.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有现实世界的案例中，这两种方法都是适用的。例如，在著名的[垃圾邮件分类数据集](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset/code)中，稀疏表示方法可以获得接近完美的模型表现。在这种情况下，我们无需计算稠密向量嵌入即可获得更好的性能，因为我们用简单透明的方法就能成功实现目标。
- en: Hence the recommended starting point for a text processing task is with [frequency-based
    Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) models which produce
    sparse vectors. For such pipelines, cleaning and wrangling the text data is pivotal.
    With the right feature and vocabulary engineering, they could be the most efficient
    in terms of speed and performance, especially when semantic information is not
    required.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，推荐的文本处理任务起点是使用[基于频率的词袋模型](https://en.wikipedia.org/wiki/Bag-of-words_model)，该模型生成稀疏向量。对于这样的管道，清理和整理文本数据至关重要。通过合适的特征和词汇工程，它们在速度和性能方面可能是最有效的，特别是当不需要语义信息时。
- en: '[**Here’s the Jupyter Notebook**](https://github.com/royn5618/Medium_Blog_Codes/blob/master/Sparsity_and_Density.ipynb)
    **with the complete Pythonic demonstration of obtaining Sparse and Dense vectors
    (Word2Vec using Gensim) for the same example.**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[**这是 Jupyter Notebook**](https://github.com/royn5618/Medium_Blog_Codes/blob/master/Sparsity_and_Density.ipynb)
    **，其中包含完整的 Python 演示，展示了如何为相同示例获取稀疏和稠密向量（使用 Gensim 的 Word2Vec）。**'
- en: '**💡 Want to know more about Matrix Designs for NLP? Here is an article for
    you to learn further:**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**💡 想了解更多关于自然语言处理的矩阵设计吗？这里有一篇文章可以进一步学习：**'
- en: '[](/matrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399?source=post_page-----21fcd7a01410--------------------------------)
    [## Matrix Design for Vector Space Models in Natural Language Processing'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/matrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399?source=post_page-----21fcd7a01410--------------------------------)
    [## 自然语言处理中的向量空间模型矩阵设计'
- en: A brief philosophy of knowledge representation for NLP and a concise guide to
    matrix design for distributed word…
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自然语言处理中的知识表示简要哲学和分布式词矩阵设计的简明指南……
- en: towardsdatascience.com](/matrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399?source=post_page-----21fcd7a01410--------------------------------)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/matrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399?source=post_page-----21fcd7a01410--------------------------------)
- en: '**💡Want to implement Word2Vec for Text Classification? Here is a hands-on tutorial
    on Multiclass Text Classification by learning dense representations using Keras’s
    Embedding Layer as well as Gensim’s Word2Vec algorithm:**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**💡 想要实现 Word2Vec 用于文本分类？这里有一个关于多类文本分类的实用教程，讲解了如何使用 Keras 的嵌入层以及 Gensim 的 Word2Vec
    算法学习稠密表示：**'
- en: '[](/multiclass-text-classification-using-keras-to-predict-emotions-a-comparison-with-and-without-word-5ef0a5eaa1a0?source=post_page-----21fcd7a01410--------------------------------)
    [## Multiclass Text Classification Using Keras to Predict Emotions: A Comparison
    with and without Word…'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/multiclass-text-classification-using-keras-to-predict-emotions-a-comparison-with-and-without-word-5ef0a5eaa1a0?source=post_page-----21fcd7a01410--------------------------------)
    [## 使用 Keras 进行多类文本分类：有词嵌入与无词嵌入的比较…'
- en: Do word embeddings add value to text classification models? Let’s find out in
    this multiclass prediction task for…
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词嵌入是否为文本分类模型增加价值？让我们在这个多类预测任务中来探讨一下…
- en: towardsdatascience.com](/multiclass-text-classification-using-keras-to-predict-emotions-a-comparison-with-and-without-word-5ef0a5eaa1a0?source=post_page-----21fcd7a01410--------------------------------)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/multiclass-text-classification-using-keras-to-predict-emotions-a-comparison-with-and-without-word-5ef0a5eaa1a0?source=post_page-----21fcd7a01410--------------------------------)
- en: '**References:**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考资料：**'
- en: '[https://machinelearningmastery.com/sparse-matrices-for-machine-learning/](https://machinelearningmastery.com/sparse-matrices-for-machine-learning/)'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://machinelearningmastery.com/sparse-matrices-for-machine-learning/](https://machinelearningmastery.com/sparse-matrices-for-machine-learning/)'
- en: '[https://kavita-ganesan.com/fasttext-vs-word2vec/#.Y-OP7XbP02w](https://kavita-ganesan.com/fasttext-vs-word2vec/#.Y-OP7XbP02w)'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://kavita-ganesan.com/fasttext-vs-word2vec/#.Y-OP7XbP02w](https://kavita-ganesan.com/fasttext-vs-word2vec/#.Y-OP7XbP02w)'
- en: '*Thanks for visiting!*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢访问！*'
- en: '**My Links:** [Medium](https://medium.com/@nroy0110) | [LinkedIn](https://www.linkedin.com/in/nabanita-roy/)
    | [GitHub](https://github.com/royn5618)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**我的链接：** [Medium](https://medium.com/@nroy0110) | [LinkedIn](https://www.linkedin.com/in/nabanita-roy/)
    | [GitHub](https://github.com/royn5618)'
