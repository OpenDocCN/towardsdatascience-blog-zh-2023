- en: 'ONNX: The Standard for Interoperable Deep Learning Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/onnx-the-standard-for-interoperable-deep-learning-models-a47dfbdf9a09](https://towardsdatascience.com/onnx-the-standard-for-interoperable-deep-learning-models-a47dfbdf9a09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/889af96423dbf877991bb0f406706a90.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jonny Caspari](https://unsplash.com/@jonnyuiux?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Learn about the benefits of using the ONNX standard for deploying models across
    frameworks and hardware platforms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----a47dfbdf9a09--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----a47dfbdf9a09--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a47dfbdf9a09--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a47dfbdf9a09--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----a47dfbdf9a09--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a47dfbdf9a09--------------------------------)
    ·5 min read·Jan 24, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: The first time I heard about ONNX was during my internship at INRIA. I was working
    to develop Neural Network Pruning algorithms in the Julia language. There weren’t
    many pre-trained models yet that I could use, so utilizing ONNX to import models
    developed with other languages and frameworks might have been a solution.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I want to introduce ONNX and explain its enormous potential
    by also seeing a practical example.
  prefs: []
  type: TYPE_NORMAL
- en: What is ONNX?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ONNX, or Open Neural Network Exchange, is an open-source standard for representing
    deep learning models. It was developed by Facebook and Microsoft in order to make
    it easier for researchers and engineers to move models between different deep-learning
    frameworks and hardware platforms.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main advantages of ONNX is that it allows models to be easily **exported
    from one framework, such as PyTorch, and imported into another framework, such
    as TensorFlow**. This can be especially useful for researchers who want to try
    out different frameworks for training and deploying their models, or for engineers
    who need to deploy models on different hardware platforms.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c9e402a5fd921c63034240ae17e04d24.png)'
  prefs: []
  type: TYPE_IMG
- en: Frameworks Interoperability (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: ONNX also provides a set of tools for optimizing and quantizing models, which
    can help to reduce the memory and computational requirements of the model. This
    can be especially useful for deploying models on edge devices and other resource-constrained
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: Another important feature of ONNX is that it is supported by a wide range of
    companies and organizations. This includes not only Facebook and Microsoft, but
    also companies like Amazon, NVIDIA, and Intel. This wide range of support ensures
    that ONNX will continue to be actively developed and maintained, making it a robust
    and stable standard for representing deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: ONNX Runtime
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**ONNX Runtime is an open-source inference engine for executing ONNX** (Open
    Neural Network Exchange) **models**. It is designed to be **high-performance**
    and lightweight, making it well-suited for **deployment on a wide range of hardware
    platforms**, including edge devices, servers, and cloud services.'
  prefs: []
  type: TYPE_NORMAL
- en: The ONNX Runtime provides a C++ API, a C# API, and a Python API for executing
    ONNX models. It also provides support for multiple backends, including CUDA and
    OpenCL, which allows it to run on a wide range of hardware platforms, such as
    NVIDIA GPUs and Intel CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: ONNX Runtime can be very useful since you can use models in inference with a
    single framework no matter what hardware you are going to use. So without having
    to actually rewrite the code depending on whether we want to use a CPU, GPU, FPGA
    or whatever!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/28c0d0696ea2096db955d905348c868c.png)'
  prefs: []
  type: TYPE_IMG
- en: ONNX Runtime (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: One of the main advantages of ONNX Runtime is its performance. It uses various
    techniques such as Just-In-Time (JIT) compilation, kernel fusion and subgraph
    partitioning to optimize the performance of the model. It also supports thread
    pooling and inter-node communication for distributed deployment which makes it
    a suitable choice for large-scale deployment.
  prefs: []
  type: TYPE_NORMAL
- en: I will explain all these advanced features in future articles!
  prefs: []
  type: TYPE_NORMAL
- en: ONNX Runtime also provides support for a wide range of models, including both
    traditional machine learning models and deep learning models. This makes it a
    versatile inference engine that can be used in a wide range of applications, from
    computer vision and natural language processing to speech recognition and autonomous
    vehicles.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s code!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s look at an example now, where we **create a Machine Learning model using**
    the classic **scikit-learn,** and then **convert** this model **to ONNX** format
    so that we can **use it with ONNX Runtime**.
  prefs: []
  type: TYPE_NORMAL
- en: First, we import the necessary libraries, pull a model into sklearn and export
    to the classic pickle format. We will use the iris dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have trained and saved the model, we can reimport it and convert
    it to an ONNX model. **Each framework will have its own conversion library**.
    So you will have to use another one if you developed your model in PyTorch or
    TensorFlow for example. In this case the library is called **skl2onnx**.
  prefs: []
  type: TYPE_NORMAL
- en: So we import the necessary libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now we can finally convert. We should specify the inital_type though, and then
    we could create a file called *model.onnx* where we will save the onnx model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the model in ONNX format we can import it and use it on some
    data to make inferences.
  prefs: []
  type: TYPE_NORMAL
- en: We then install ONNX Runtime.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now we create data, and import the model, thus creating a session. We specify
    the input and output name (label), and run the session on the data!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Well, you got your results by leveraging ONNX Runtime. It only took a few simple
    commands!
  prefs: []
  type: TYPE_NORMAL
- en: This is just an introduction to ONNX you can certainly do much more, but I hope
    you found this example useful.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ONNX is an open-source standard that makes it easy to move deep learning models
    between different frameworks and hardware platforms. It provides a set of tools
    for optimizing and quantizing models, and it is supported by a wide range of companies
    and organizations. As a result, ONNX is becoming an important standard for deep
    learning, making it easy to share models and deploy them across different platforms.
  prefs: []
  type: TYPE_NORMAL
- en: The End
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Marcello Politi*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Linkedin](https://www.linkedin.com/in/marcello-politi/), [Twitter](https://twitter.com/_March08_),
    [CV](https://march-08.github.io/digital-cv/)'
  prefs: []
  type: TYPE_NORMAL
