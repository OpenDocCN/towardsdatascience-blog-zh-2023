["```py\ntext_chunks = [\n    \"The sky is blue.\",\n    \"The grass is green.\",\n    \"The sun is shining.\",\n    \"I love chocolate.\",\n    \"Pizza is delicious.\",\n    \"Coding is fun.\",\n    \"Roses are red.\",\n    \"Violets are blue.\",\n    \"Water is essential for life.\",\n    \"The moon orbits the Earth.\",\n]\n```", "```py\nanyio==4.0.0\nbackoff==2.2.1\nbcrypt==4.0.1\ncertifi==2023.7.22\ncharset-normalizer==3.2.0\nchroma-hnswlib==0.7.3\nchromadb==0.4.10\nclick==8.1.7\ncolorama==0.4.6\ncoloredlogs==15.0.1\ncontourpy==1.1.1\ncycler==0.11.0\nexceptiongroup==1.1.3\nfastapi==0.99.1\nfilelock==3.12.4\nflatbuffers==23.5.26\nfonttools==4.42.1\nfsspec==2023.9.1\nh11==0.14.0\nhttptools==0.6.0\nhuggingface-hub==0.16.4\nhumanfriendly==10.0\nidna==3.4\nimportlib-resources==6.0.1\njoblib==1.3.2\nkiwisolver==1.4.5\nmatplotlib==3.8.0\nmonotonic==1.6\nmpmath==1.3.0\nnumpy==1.26.0\nonnxruntime==1.15.1\noverrides==7.4.0\npackaging==23.1\npandas==2.1.0\nPillow==10.0.1\nposthog==3.0.2\nprotobuf==4.24.3\npulsar-client==3.3.0\npydantic==1.10.12\npyparsing==3.1.1\nPyPika==0.48.9\npyreadline3==3.4.1\npython-dateutil==2.8.2\npython-dotenv==1.0.0\npytz==2023.3.post1\nPyYAML==6.0.1\nrequests==2.31.0\nscikit-learn==1.3.0\nscipy==1.11.2\nsix==1.16.0\nsniffio==1.3.0\nstarlette==0.27.0\nsympy==1.12\nthreadpoolctl==3.2.0\ntokenizers==0.14.0\ntqdm==4.66.1\ntyping_extensions==4.7.1\ntzdata==2023.3\nurllib3==2.0.4\nuvicorn==0.23.2\nwatchfiles==0.20.0\nwebsockets==11.0.3\n```", "```py\n##########################################################################################################\n'''\nThe following script translates the list of strings \"text_chunks\" into vector embeddings and\nsaves the DataFrame including \"text_chunks\" and \"embeddings\" in the csv file \"embeddings_df.csv\"\n'''\n##########################################################################################################\n\nimport os\nimport requests\nimport pandas as pd\nimport numpy as np\n\n# hugging face token\nos.environ['hf_token'] = os.environ.get('HF_TOKEN')\n# os.environ['hf_token'] = 'testtoken123'\n\n# example text snippets we want to translate into vector embeddings\ntext_chunks = [\n    \"The sky is blue.\",\n    \"The grass is green.\",\n    \"The sun is shining.\",\n    \"I love chocolate.\",\n    \"Pizza is delicious.\",\n    \"Coding is fun.\",\n    \"Roses are red.\",\n    \"Violets are blue.\",\n    \"Water is essential for life.\",\n    \"The moon orbits the Earth.\",\n]\n\ndef _get_embeddings(text_chunk):\n    '''\n    Use embedding model from hugging face to calculate embeddings for the text snippets provided\n\n    Parameters:\n        - text_chunk (string): the sentence or text snippet you want to translate into embeddings\n\n    Returns:\n        - embedding(list): list with all embedding dimensions\n    '''\n    # define the embedding model you want to use\n    model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n\n    # you can find the token to the hugging face api in your settings page https://huggingface.co/settings/tokens\n    hf_token = os.environ.get('hf_token')\n\n    # API endpoint for embedding model\n    api_url = f\"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}\"\n    headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n\n    # call API\n    response = requests.post(api_url, headers=headers, json={\"inputs\": text_chunk, \"options\":{\"wait_for_model\":True}})\n\n    # load response from embedding model into json format\n    embedding = response.json()\n\n    return embedding\n\ndef from_text_to_embeddings(text_chunks):\n    '''\n    Translate sentences into vector embeddings\n\n    Attributes:\n        - text_chunks (list): list of example strings\n\n    Returns:\n        - embeddings_df (DataFrame): data frame with the columns \"text_chunk\" and \"embeddings\"\n    '''\n    # create new data frame using text chunks list\n    embeddings_df = pd.DataFrame(text_chunks).rename(columns={0:\"text_chunk\"})\n\n    # use the _get_embeddings function to retrieve the embeddings for each of the sentences\n    embeddings_df[\"embeddings\"] = embeddings_df[\"text_chunk\"].apply(_get_embeddings)\n\n    # split the embeddings column into individuell columns for each vector dimension\n    embeddings_df = embeddings_df['embeddings'].apply(pd.Series)\n    embeddings_df[\"text_chunk\"] = text_chunks\n\n    return embeddings_df\n\n# get embeddings for each of the text chunks\nembeddings_df = from_text_to_embeddings(text_chunks)\n\n# save data frame with text chunks and embeddings to csv\nembeddings_df.to_csv('../02_Data/embeddings_df.csv', index=False)\n```", "```py\n##########################################################################################################\n# Create PCA plot using embeddings df\n##########################################################################################################\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef create_pca_plot(embeddings_df):\n    '''\n    The function performs a principal component analysis to reduce the dimensions to 2 so we can print them in a plot\n\n    Parameters\n        - embeddings_df (DataFrame): data frame with the columns \"text_chunk\" and \"embeddings\"\n\n    Returns\n        - df_reduced (DataFrame): data frame with the 2 most relevant Principal Components\n    '''\n    # Perform PCA with 2 components\n    pca = PCA(n_components=2)\n\n    # apply principal component analysis to the embeddings table\n    df_reduced = pca.fit_transform(embeddings_df[embeddings_df.columns[:-2]])\n\n    # Create a new DataFrame with reduced dimensions\n    df_reduced = pd.DataFrame(df_reduced, columns=['PC1', 'PC2'])\n\n    ############################################################################################\n    # Create a scatter plot\n    ############################################################################################\n    def create_scatter_plot(df_reduced):\n        plt.scatter(df_reduced['PC1'], df_reduced['PC2'], label=df_reduced['PC2'])\n\n        # Add labels and title\n        plt.xlabel('X')\n        plt.ylabel('Y')\n        plt.title('Scatter Plot')\n\n        # Add labels to each dot\n        for i, label in enumerate(embeddings_df.iloc[: , -1].to_list()):\n            plt.text(df_reduced['PC1'][i], df_reduced['PC2'][i], label)\n\n        # Save and display the plot\n        plt.savefig('../02_Data/principal_component_plot.png', format='png')\n\n    # create and save scatter plot\n    create_scatter_plot(df_reduced=df_reduced)\n\n    return df_reduced\n\n# Load embeddings_df.csv into data frame\nembeddings_df = pd.read_csv('../02_Data/embeddings_df.csv')\n\n# use the function create_pca_plot to\ndf_reduced = create_pca_plot(embeddings_df)\n```", "```py\n##########################################################################################################\n# Calculate cosine similarity between the query vector and all other embedding vectors\n##########################################################################################################\nimport numpy as np\nfrom numpy.linalg import norm\nimport time\nimport pandas as pd\nimport os\nimport requests\n\ndef _get_embeddings(text_chunk):\n    '''\n    Use embedding model from hugging face to calculate embeddings for the text snippets provided\n\n    Parameters:\n        - text_chunk (string): the sentence or text snippet you want to translate into embeddings\n\n    Returns:\n        - embedding(list): list with all embedding dimensions\n    '''\n    # define the embedding model you want to use\n    model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n\n    # you can find the token to the hugging face api in your settings page https://huggingface.co/settings/tokens\n    hf_token = os.environ.get('HF_TOKEN')\n\n    # API endpoint for embedding model\n    api_url = f\"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}\"\n    headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n\n    # call API\n    response = requests.post(api_url, headers=headers, json={\"inputs\": text_chunk, \"options\":{\"wait_for_model\":True}})\n\n    # load response from embedding model into json format\n    embedding = response.json()\n\n    return embedding\n\ndef calculate_cosine_similarity(text_chunk, embeddings_df):\n    '''\n    Calculate the cosine similarity between the query sentence and every other sentence\n    1\\. Get the embeddings for the text chunk\n    2\\. Calculate the cosine similarity between the embeddings of our text chunk und every other entry in the data frame\n\n    Parameters:\n        - text_chunk (string): the text snippet we want to use to look for similar entries in our database (embeddings_df)\n        - embeddings_df (DataFrame): data frame with the columns \"text_chunk\" and \"embeddings\"\n    Returns:\n        -\n    '''\n\n    # use the _get_embeddings function the retrieve the embeddings for the text chunk\n    sentence_embedding = _get_embeddings(text_chunk)\n\n    # combine all dimensions of the vector embeddings to one array\n    embeddings_df['embeddings_array'] = embeddings_df.apply(lambda row: row.values[:-1], axis=1)\n\n    # start the timer\n    start_time = time.time()\n    print(start_time)\n\n    # create a list to store the calculated cosine similarity\n    cos_sim = []\n\n    for index, row in embeddings_df.iterrows():\n        A = row.embeddings_array\n        B = sentence_embedding\n\n        # calculate the cosine similarity\n        cosine = np.dot(A,B)/(norm(A)*norm(B))\n\n        cos_sim.append(cosine)\n\n    embeddings_cosine_df = embeddings_df\n    embeddings_cosine_df[\"cos_sim\"] = cos_sim\n    embeddings_cosine_df.sort_values(by=[\"cos_sim\"], ascending=False)\n\n    # stop the timer\n    end_time = time.time()\n\n    # calculate the time needed to calculate the similarities\n    elapsed_time = (end_time - start_time)\n    print(\"Execution Time: \", elapsed_time, \"seconds\")\n\n    return embeddings_cosine_df\n\n# Load embeddings_df.csv into data frame\nembeddings_df = pd.read_csv('../02_Data/embeddings_df.csv')\n\n# test query sentence\ntext_chunk = \"Lilies are white.\"\n\n# calculate cosine similarity\nembeddings_cosine_df = calculate_cosine_similarity(text_chunk, embeddings_df)\n\n# save data frame with text chunks and embeddings to csv\nembeddings_cosine_df.to_csv('../02_Data/embeddings_cosine_df.csv', index=False)\n\n# rank based on similarity\nsimilarity_ranked_df = embeddings_cosine_df[[\"text_chunk\", \"cos_sim\"]].sort_values(by=[\"cos_sim\"], ascending=False)\n```", "```py\npip install chromadb\n```", "```py\nimport chromadb\nfrom chromadb.config import Settings\nimport pandas as pd\n\n# vector store settings\nVECTOR_STORE_PATH = r'../02_Data/00_Vector_Store'\nCOLLECTION_NAME = 'my_collection'\n\n# Load embeddings_df.csv into data frame\nembeddings_df = pd.read_csv('../02_Data/embeddings_df.csv')\n\ndef get_or_create_client_and_collection(VECTOR_STORE_PATH, COLLECTION_NAME):\n    # get/create a chroma client\n    chroma_client = chromadb.PersistentClient(path=VECTOR_STORE_PATH)\n\n    # get or create collection\n    collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)\n\n    return collection\n\n# get or create collection\ncollection = get_or_create_client_and_collection(VECTOR_STORE_PATH, COLLECTION_NAME)\n```", "```py\n# Load embeddings_df.csv into data frame\nembeddings_df = pd.read_csv('../02_Data/embeddings_df.csv')\n\ndef add_to_collection(embeddings_df):\n    # add a sample entry to collection\n    # collection.add(\n    #     documents=[\"This is a document\", \"This is another document\"],\n    #     metadatas=[{\"source\": \"my_source\"}, {\"source\": \"my_source\"}],\n    #     ids=[\"id1\", \"id2\"]\n    # )\n\n    # combine all dimensions of the vector embeddings to one array\n    embeddings_df['embeddings_array'] = embeddings_df.apply(lambda row: row.values[:-1], axis=1)\n    embeddings_df['embeddings_array'] = embeddings_df['embeddings_array'].apply(lambda x: x.tolist())\n\n    # add data frame to collection\n    collection.add(\n        embeddings=embeddings_df.embeddings_array.to_list(),\n        documents=embeddings_df.text_chunk.to_list(),\n        # create a list of string as index\n        ids=list(map(str, embeddings_df.index.tolist()))\n    )\n\n# add the embeddings_df to our vector store collection\nadd_to_collection(embeddings_df)\n```", "```py\ndef get_all_entries(collection):\n    # query collection\n    existing_docs = pd.DataFrame(collection.get()).rename(columns={0: \"ids\", 1:\"embeddings\", 2:\"documents\", 3:\"metadatas\"})\n    existing_docs.to_excel(r\"..//02_Data//01_vector_stores_export.xlsx\")\n    return existing_docs\n\n# extract all entries in vector store collection\nexisting_docs = get_all_entries(collection)\n```", "```py\ndef query_vector_database(VECTOR_STORE_PATH, COLLECTION_NAME, query, n=2):\n    # query collection\n    results = collection.query(\n        query_texts=query,\n        n_results=n\n    )\n\n    print(f\"Similarity Search: {n} most similar entries:\")\n    print(results[\"documents\"])\n    return results\n\n# similarity search\nsimilar_vector_entries = query_vector_database(VECTOR_STORE_PATH, COLLECTION_NAME, query=[\"Lilies are white.\"])\n```", "```py\n##########################################################################################################\n'''\nIncludes some functions to create a new vector store collection, fill it and query it\n'''\n##########################################################################################################\nimport chromadb\nfrom chromadb.config import Settings\nimport pandas as pd\n\n# vector store settings\nVECTOR_STORE_PATH = r'../02_Data/00_Vector_Store'\nCOLLECTION_NAME = 'my_collection'\n\n# Load embeddings_df.csv into data frame\nembeddings_df = pd.read_csv('../02_Data/embeddings_df.csv')\n\ndef get_or_create_client_and_collection(VECTOR_STORE_PATH, COLLECTION_NAME):\n    # get/create a chroma client\n    chroma_client = chromadb.PersistentClient(path=VECTOR_STORE_PATH)\n\n    # get or create collection\n    collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)\n\n    return collection\n\n# get or create collection\ncollection = get_or_create_client_and_collection(VECTOR_STORE_PATH, COLLECTION_NAME)\n\ndef add_to_collection(embeddings_df):\n    # add a sample entry to collection\n    # collection.add(\n    #     documents=[\"This is a document\", \"This is another document\"],\n    #     metadatas=[{\"source\": \"my_source\"}, {\"source\": \"my_source\"}],\n    #     ids=[\"id1\", \"id2\"]\n    # )\n\n    # combine all dimensions of the vector embeddings to one array\n    embeddings_df['embeddings_array'] = embeddings_df.apply(lambda row: row.values[:-1], axis=1)\n    embeddings_df['embeddings_array'] = embeddings_df['embeddings_array'].apply(lambda x: x.tolist())\n\n    # add data frame to collection\n    collection.add(\n        embeddings=embeddings_df.embeddings_array.to_list(),\n        documents=embeddings_df.text_chunk.to_list(),\n        # create a list of string as index\n        ids=list(map(str, embeddings_df.index.tolist()))\n    )\n\n# add the embeddings_df to our vector store collection\nadd_to_collection(embeddings_df)\n\ndef get_all_entries(collection):\n    # query collection\n    existing_docs = pd.DataFrame(collection.get()).rename(columns={0: \"ids\", 1:\"embeddings\", 2:\"documents\", 3:\"metadatas\"})\n    existing_docs.to_excel(r\"..//02_Data//01_vector_stores_export.xlsx\")\n    return existing_docs\n\n# extract all entries in vector store collection\nexisting_docs = get_all_entries(collection)\n\ndef query_vector_database(VECTOR_STORE_PATH, COLLECTION_NAME, query, n=2):\n    # query collection\n    results = collection.query(\n        query_texts=query,\n        n_results=n\n    )\n\n    print(f\"Similarity Search: {n} most similar entries:\")\n    print(results[\"documents\"])\n    return results\n\n# similarity search\nsimilar_vector_entries = query_vector_database(VECTOR_STORE_PATH, COLLECTION_NAME, query=[\"Lilies are white.\"])\n```"]