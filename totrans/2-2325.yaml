- en: What does Entropy Measure? An Intuitive Explanation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/what-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421](https://towardsdatascience.com/what-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://tim-lou.medium.com/?source=post_page-----a7f7e5d16421--------------------------------)[![Tim
    Lou, PhD](../Images/e4931bb6d59e27730529ceaf00a23822.png)](https://tim-lou.medium.com/?source=post_page-----a7f7e5d16421--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a7f7e5d16421--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a7f7e5d16421--------------------------------)
    [Tim Lou, PhD](https://tim-lou.medium.com/?source=post_page-----a7f7e5d16421--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a7f7e5d16421--------------------------------)
    ·11 min read·Jan 4, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'Entropy may seem abstract, but it has an intuitive side: as the probability
    of seeing certain patterns in data. Here’s how it works.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4daae7bdad8d3ebc360bb1ef70760dd8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Background Credit: Joe Maldonado [@unsplash](https://unsplash.com/@joesracingteam)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In data science, there are many concepts linked to the notion of entropy. The
    most basic one is Shannon’s information entropy, defined for any distribution,
    *P*(*x*), through the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a8588251ff381e4720e63406deb3205a.png)'
  prefs: []
  type: TYPE_IMG
- en: Where the sum is over all the possible categories in *C*.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other related concepts that have similarly looking formulae:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence):
    for comparing two distributions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mutual information](https://en.wikipedia.org/wiki/Mutual_information): for
    capturing general relationships between two variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cross entropy](https://en.wikipedia.org/wiki/Cross_entropy): for training
    classification models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Despite the ubiquity of entropy-like formulae, there are rarely discussions
    on the intuitions behind the formula: Why is the logarithm involved? Why are we
    multiplying *P*(*x*) and log *P*(*x*)? While many articles mention terms like
    “information”, “expected surprise”, the intuitions behind them are missing.'
  prefs: []
  type: TYPE_NORMAL
- en: It turns out, just like probabilities, entropy can be understood through a counting
    exercise, and it can be linked to a sort of log-likelihood for distributions.
    Furthermore, this counting can be linked to the literal number of bytes in a computer.
    These interpretations will enable us to demystify the many facts about entropy.
    Curious? Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Counting Entropy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/d167237368b07ac265e675b3f71352a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Probability’s counting definition is what makes it so intuitive (Photo by [Ibrahim
    Rifath](https://unsplash.com/@photoripey?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral))
  prefs: []
  type: TYPE_NORMAL
- en: 'Probability can be defined operationally: When we say a coin has a 50% chance
    to land a head, it means that if we were to flip the coin a million times, the
    number of heads will get pretty close to half a million. This fraction will get
    closer to the 50% probability as the number of trials increase. This definition
    is what makes probabilities so intuitive.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Is there a similar interpretation for entropy? There is, although the counting
    is a little bit tricker: it will require some basic combinatorics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How many ways are there to arrange *N* different balls? There are *N* choices
    for the first one, *N* − 1 for the second one… etc. The answer is *N*!, or the
    factorial symbol:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/344ab819c57f5948cf5f1ea8dbec506d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Just like in the definition of probabilities, we’ll be working with very large
    numbers. So it is helpful to approximate this object through [Sterling’s approximation](https://en.wikipedia.org/wiki/Stirling%27s_approximation):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3f6508c3690964cbfbdaba2108afa70a.png)'
  prefs: []
  type: TYPE_IMG
- en: Where log indicates the natural logarithm; analogous formulae also exist if
    we use alternative bases like log₂ and log₁₀ (this will determine the units in
    which we measure entropy). The big-O notation indicates the validity of the approximation
    as *N* gets large. The term *N* log *N* will be the origin of *p* log *p* in entropy’s
    definition.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to derive what entropy is counting. Imagine there is a large
    number of distinguishable objects, or distinguishable data points. These *N* data
    points are grouped into say *c* categories, like in the figure below
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/921883f6e44aed334b172acfe55f503c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'What is the total number of ways this can be done? Keeping in mind that we
    don’t care about the ordering of our data in any category, the answer is captured
    by the classic multinomial coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/620970d66f7f69c551ca919e9ef76f60.png)'
  prefs: []
  type: TYPE_IMG
- en: Where we have used the Ω symbol to denote the number of configurations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like the case for probability, we are only interested in the large *N*
    behaviors. When dealing with such large numbers, it is helpful to take the logarithm,
    so we can use Sterling’s approximation to make things more manageable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1206496dc3bce7cf30dd9693b6719e8a.png)'
  prefs: []
  type: TYPE_IMG
- en: The formula can be simplified by utilizing the fact that the sum of all the
    *nᵢ* equals *N*,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da8eae2a138667b729bad6985415f186.png)'
  prefs: []
  type: TYPE_IMG
- en: 'if we substitute *nᵢ*/*N* → *P*(*i*), we get exactly the entropy formula. Alternatively,
    we can write (for large *N*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5736092920f8617b70665370c19d02b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So we arrived at an operational definition of entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Entropy counts the # of ways of categorizing large amounts of data that resemble
    a given probability distribution (in logarithmic units and per number of data
    point)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This counting exercise lies at the heart of information theory, which we’ll
    turn to next.
  prefs: []
  type: TYPE_NORMAL
- en: Entropy as Information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So how does our concept of entropy relate to literal bits of 0s and 1s in a
    computer?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb3f9ddc43602f73e10cc1f8d7bc3c23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The concept of entropy can linked to information when we apply it to patterns
    of 1s and 0s (credit: [Gerd Altmann](https://pixabay.com/users/geralt-9301/))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine a binary sequence of some fix length *N*. Intuitively we know that
    it contains *N* bits of information: because it literally takes *N* bits to store
    the sequence in a hard-drive or memory.'
  prefs: []
  type: TYPE_NORMAL
- en: But what if the sequence has some interesting patterns like the following?
  prefs: []
  type: TYPE_NORMAL
- en: '000000000000000000000000000'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '010101010101010101010101010'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '000000010000000000000000000'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In these cases, the binary sequence representation would be very inefficient.
    We intuitively know that there are more efficient ways to store these sequence:
    we can specify the patterns as opposed to all the literal bits, and the amount
    of meaningful information in these sequences should be smaller.'
  prefs: []
  type: TYPE_NORMAL
- en: So if we ignore the subtle patterns of digit repetitions, and just look at the
    basic statistical properties of the digits (proportions of 0s and 1s), how much
    better can we do in terms of storing those sequences?
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where our entropy counting formula can help us: it can count the total
    number of sequence given some fixed proportions of 0s and 1s.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case where the proportions of 0s and 1s are 50/50, the total number
    of possibilities is (in the large *N* limit):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1937ff9ce3a6eafbcb7d21f4a9b01a56.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We see that this just approximately yields the number of all the possible binary
    sequences 2*ᴺ.* So the number of bits required to store the sequence is still
    *N*. This isn’t surprising, as we know that random sequences should be impossible
    to compress: it has the maximum *N* bits of information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But what if the proportions are no longer 50/50? We should expect some potential
    savings. In this case, the total number of bits required to store one sequence
    would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/749c4409803ad01dca62ec7a2f948886.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s sanity check the case when the number of 0s is much smaller than the
    number of 1s, say *n* ≪ *N*. In this case the *P*₁ term can be ignored, and the
    number of bits is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/635d4229fdbbfed25583b61af4459693.png)'
  prefs: []
  type: TYPE_IMG
- en: So the amount of information is proportional to *n* instead of *N.* This is
    because we now only need to store the position of each 0 instead of the whole
    sequence.
  prefs: []
  type: TYPE_NORMAL
- en: This illustrate the power of entropy as it relates to physical bits and bytes
    in a computer. In summary,
  prefs: []
  type: TYPE_NORMAL
- en: The information entropy specifies the expected number of bit per length that
    is required to store a sequence generated by a given probability distribution.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In other words, entropy is a sort of optimal compression ratio for a fixed proportion
    of characters in a sequence. This is the way that entropy is linked to information.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond thinking of sequence as our subject of interest, we can turn our attention
    to the distributions themselves. This view point allows us to interpret entropy
    as a sort of probability (or log-likelihood).
  prefs: []
  type: TYPE_NORMAL
- en: Entropy as Log-Likelihood
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Entropy counts the number of possibilities. We want to convert this to a sort
    of probability. To do this we simply need to normalize the counts.
  prefs: []
  type: TYPE_NORMAL
- en: 'What is the total number of ways to categorize *N* data points into *c* categories?
    The answer is simple, because each data point has *c* choices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05935d2b317cfd14412fca4652c4ceca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now divide the entropy’s count by the total to obtain a probability
    (substituting *nᵢ*/*N* → *P*(*i*)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/84eb6451eccba793aacbcb92ccba2105.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This way, entropy becomes the probability (asymptotic because of large *N*)
    of observing a particular distribution from randomly categorizing data points:'
  prefs: []
  type: TYPE_NORMAL
- en: Entropy can be viewed as the log-likelihood of observing a given distribution
    (per data point)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There is a hidden assumption in our discussion though, as we are treating every
    configuration as equally likely in our calculations. What happens if some categories
    are more favored over others?
  prefs: []
  type: TYPE_NORMAL
- en: 'We can consider some reference distribution *Q*(*x*). If each data point has
    a chance *Q*(*x*) to be in a particular category *x*, the probability of observing
    *n*₁ in category 1, *n*₂ in category 2 and so on is given by the multinomial probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed03a064964e21cd2f066d52f36d2090.png)'
  prefs: []
  type: TYPE_IMG
- en: Once again, we can go through Sterling’s approximation. The calculation is very
    similar to the previous ones, except we have a extra *Q*(*i*) in the end
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f838ff7e0bc302f2cb063fe56450c4c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Substituting *nᵢ*/*N* → *P*(*i*), the term inside the exponential becomes the
    [Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).
    So our equation can be summarized as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be46304139b1eb70476e9d8888b08426.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where we have used the common notation of KL-divergence inside the exponential.
    The KL-divergence is a generalization of Shannon’s information entropy, and our
    equations make our interpretation even more precise:'
  prefs: []
  type: TYPE_NORMAL
- en: The Kullback-Leibler divergence of P on Q is the negative log-likelihood (per
    data point) of observing P when sampling data according to Q
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Once again, all this is assuming *N* is very large.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few facts about KL-divergence now become obvious:'
  prefs: []
  type: TYPE_NORMAL
- en: 'KL-divergence is always non-negative: this is because a probability can never
    be larger than 1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'KL-divergence can be infinite: this happens when two distributions have no
    overlap, and so the counting exercise yields 0 = exp[–∞].'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'KL-divergence is zero if and only if *P* = *Q*: when we sample data according
    to *Q*, we expect the resulting distributions to look like *Q —* this expectation
    is exact at large *N*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Armed with this new understanding, we are now ready to reinterpret facts about
    various entropic concepts in data science!
  prefs: []
  type: TYPE_NORMAL
- en: An Entropic Sampler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Below we’ll discuss the intuitions behind some common entropy-like variables
    in data science. We’ll once again remind the reader that the large *N* limit is
    implicitly assumed.
  prefs: []
  type: TYPE_NORMAL
- en: Cross Entropy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is useful for training categorical variables. It is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a8485c1c02e027374c126a6355c55ec3.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that we’ve rewritten the definition as a sum of the KL-divergence and Shannon’s
    information entropy. This may look a little unfamiliar, since when we train machine
    learning model, we only compute an estimate of it through our samples (say *S*)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/868e4a17287e8aae05b7ce6098e40899.png)'
  prefs: []
  type: TYPE_IMG
- en: Using our counting intuition, we conclude that
  prefs: []
  type: TYPE_NORMAL
- en: Minimizing cross-entropy is equivalent to maximizing the log-likelihood of observing
    the same statistics as those from our sampled data, if we sample our data from
    a distribution Q that is being trained
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This puts the cross-entropy loss on a similar conceptual ground as the L2 loss
    in regressions: they are both some sort of log-likelihood functions.'
  prefs: []
  type: TYPE_NORMAL
- en: Mutual Information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mutual information can be thought of as a generalized sort of correlation between
    two variables. Denoted by *I*, it is defined through the KL-divergence
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/53ddc49f96d4672862b76e741c435412.png)'
  prefs: []
  type: TYPE_IMG
- en: Where in the KL-divergence computation, we are comparing a distribution of two
    variables, against the distribution of considering each variable separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our counting intuitions give us a very nice interpretation:'
  prefs: []
  type: TYPE_NORMAL
- en: Mutual information is the negative log-likelihood (per data point) of obtaining
    a given distribution on two variables, when we sample the two variables independently
    based on their marginalized distributions
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This explains why mutual information is a powerful tool that can capture nonlinear
    correlations between variables.
  prefs: []
  type: TYPE_NORMAL
- en: The Inevitable Increase In Entropy?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, we are ready to address one of the most well-known facts about entropy:
    the laws of thermodynamics and the inevitable increase in entropy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to keep in mind though that there are two concepts of entropy
    at play here:'
  prefs: []
  type: TYPE_NORMAL
- en: Shannon’s Information Entropy in Data Science
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Entropy in Thermal Physics
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The increase in Entropy is a physical law that only applies in the second case.
    However, the entropy in physics can be viewed as a special case of Shannon’s entropy
    when applied to physical systems, so there is a connection there.
  prefs: []
  type: TYPE_NORMAL
- en: What this means in terms of the counting exercise then, is that the number of
    possibilities will inevitably increase. This makes intuitive sense, because when
    a physical (chaotic) system is not constrained, it should eventually sample all
    the possibilities. It’s a bit similar to the famous Murphy’s law that stats “anything
    that can go wrong will go wrong”.
  prefs: []
  type: TYPE_NORMAL
- en: 'From a data science perspective, if we believe that our data are results of
    some dynamical systems, then it might makes sense to maximize entropy: because
    if we believe all the variables have been taken into account, there is no reason
    to think that our data would not explore all the possibilities. In other words,
    we want to consider all the possibilities/combinations — even ones that are not
    present in our data. This is perhaps what grants entropic concepts their super
    powers in data science, that'
  prefs: []
  type: TYPE_NORMAL
- en: By counting all the possibilities, entropy is a conservative measure of our
    ignorances
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This viewpoint was explored in another one of my articles on [entropy](https://medium.com/swlh/entropy-is-not-disorder-a-physicists-perspective-c0dccfea67f1).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By interpreting the formula for entropy as counting possibilities, we are able
    to understand entropy’s role in information theory and think of entropy as a sort
    of probability. This interpretation is ultimately what makes various entropic
    concepts meaningful and useful.
  prefs: []
  type: TYPE_NORMAL
- en: Please share your thoughts and feedback if you have any, happy reading! 👋
  prefs: []
  type: TYPE_NORMAL
- en: 'If you enjoy this article, you might be interested in some of my other related
    articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/swlh/entropy-is-not-disorder-a-physicists-perspective-c0dccfea67f1?source=post_page-----a7f7e5d16421--------------------------------)
    [## Entropy Is Not Disorder: A Physicist’s Perspective'
  prefs: []
  type: TYPE_NORMAL
- en: Entropy is often treated as synonymous to chaos. But what is it really? In this
    article, we explore how entropy is more…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'medium.com](https://medium.com/swlh/entropy-is-not-disorder-a-physicists-perspective-c0dccfea67f1?source=post_page-----a7f7e5d16421--------------------------------)
    [](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----a7f7e5d16421--------------------------------)
    [## A Physicist’s View of Machine Learning: The Thermodynamics of Machine Learning'
  prefs: []
  type: TYPE_NORMAL
- en: Complex systems in nature can be successfully studied using thermodynamics.
    What about Machine Learning?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----a7f7e5d16421--------------------------------)
    [](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----a7f7e5d16421--------------------------------)
    [## Why We Don’t Live in a Simulation
  prefs: []
  type: TYPE_NORMAL
- en: Describing reality as a simulation vastly understates the complexities of our
    world. Here’s why the simulation…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----a7f7e5d16421--------------------------------)
  prefs: []
  type: TYPE_NORMAL
