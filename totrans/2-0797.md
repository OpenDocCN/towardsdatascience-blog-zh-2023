# Elliot 激活函数：它是什么，它有效吗？

> 原文：[https://towardsdatascience.com/elliot-activation-function-what-is-it-and-is-it-effective-59b63ec1fd8a](https://towardsdatascience.com/elliot-activation-function-what-is-it-and-is-it-effective-59b63ec1fd8a)

## 什么是 Elliot 激活函数，它是否是神经网络中其他激活函数的良好替代方案？

[](https://ben-mccloskey20.medium.com/?source=post_page-----59b63ec1fd8a--------------------------------)[![本杰明·麦克洛斯基](../Images/7118f5933f2affe2a7a4d3375452fa4c.png)](https://ben-mccloskey20.medium.com/?source=post_page-----59b63ec1fd8a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----59b63ec1fd8a--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----59b63ec1fd8a--------------------------------) [本杰明·麦克洛斯基](https://ben-mccloskey20.medium.com/?source=post_page-----59b63ec1fd8a--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----59b63ec1fd8a--------------------------------) ·阅读时间 7 分钟·2023年2月4日

--

![](../Images/39a7ac7751ff21380e8ecc69c64edfb1.png)

Elliot 激活函数（图片来源：作者）

# 介绍

你是否在创建新的机器学习模型时，不确定应该使用什么激活函数？

**但等一下，*什么是激活函数？***

激活函数使机器学习模型能够理解和解决*非线性*问题。在神经网络中使用激活函数特别有助于将每个神经元传递给下一个神经元的最重要信息。今天，ReLU 激活函数通常用于神经网络的架构中，但这并不一定意味着它总是最佳选择。（请查看我下面关于 ReLU 和 LReLU 激活函数的文章）。

[](/leaky-relu-vs-relu-activation-functions-which-is-better-1a1533d0a89f?source=post_page-----59b63ec1fd8a--------------------------------) [## Leaky ReLU 与 ReLU 激活函数：哪一个更好？

### 一项实验调查在使用 ReLU 激活函数时模型性能是否存在明显差异……

towardsdatascience.com](/leaky-relu-vs-relu-activation-functions-which-is-better-1a1533d0a89f?source=post_page-----59b63ec1fd8a--------------------------------)

我最近发现了**Elliot 激活函数**，它被赞誉为可能替代各种激活函数的选择，包括 Sigmoid 和双曲正切函数。今天我们将进行一个实验来测试 Elliot 激活函数的性能。

**实验 1：** *测试 Elliot 激活函数与 Sigmoid 激活函数和双曲正切激活函数的性能。*

**实验2:** *测试Elliot激活函数与ReLU激活函数的性能。*

目标是回答这个问题：*Elliot激活函数是否有效？*

# Elliot激活函数

![](../Images/77e52168c1320307627e828d342e28a2.png)

Elliot激活函数将产生一个相对接近Sigmoid和双曲正切激活函数的近似结果。有些人发现Elliot比Sigmoid激活函数**快2倍** [3]。就像Sigmoid激活函数一样，Elliot激活函数被限制在0到1之间。

![](../Images/8d6d284c49d6dfde0ab3cd3d7f19a2f7.png)

Elliot激活函数（图像来自作者）

# 实验

**问题:** Keras目前在其库中没有Elliot激活函数。

**解决方案:** 我们可以使用Keras后端自行创建它！

[PRE0]

对于这个实验，让我们看看Elliot激活函数与类似激活函数以及ReLU激活函数的比较，ReLU是今天神经网络中使用的基本激活函数。

## 数据集和设置

所有Python项目的第一步是导入你的包。

[PRE1]

今天使用的数据集是鸢尾花数据集，可以在[这里](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data)找到。这个数据集是公开的，允许公开使用（可以通过*sklearn*加载到Python中）。

[PRE2]

接下来，我们创建四个模型。这些将是相当简单的模型。每个模型将有一层8个神经元和一个激活函数。最终层将有3个神经元并使用Softmax激活函数。

[PRE3]

接下来，简单地训练模型并分析结果。

[PRE4]

# 结果

结果实际上是……令人惊讶的。正如预期的那样，Elliot激活函数生成的模型在性能上与采用Sigmoid和双曲正切激活函数的模型相似。

**1周期**

+   *Sigmoid @ 1*: 准确率: 0.3109 | 损失: 2.0030

+   *Elliot @ 1*: 准确率: 0.3361 | 损失: 1.0866

在1个周期下，Elliot激活函数模型的准确率比Sigmoid激活函数模型高2.61%，并且*损失量减少了近100%。*

**10周期**

+   *Sigmoid @ 10*: 准确率: 0.3529 | 损失: 1.0932

+   *Elliot @ 10*: 准确率: 0.6891 | 损失: 0.9434

在10个周期下，使用Elliot激活函数的模型比使用Sigmoid激活函数的模型准确率提高了近30%，同时损失较低。

**100周期**

+   *Sigmoid @ 100*: 准确率: 0.9496 | 损失: 0.4596

+   *Elliot @ 100*: 准确率: 0.9580 | 损失: 0.5485

虽然Sigmoid模型的表现优于Elliot模型，但需要注意的是它们的表现几乎完全相同。

**1000周期**

+   *Sigmoid @ 1000*: 准确率: 0.9832 | 损失: 0.0584

+   *Elliot @ 1000*: 准确率: 0.9832 | 损失: 0.0433

在1000个周期下，两种不同模型的表现几乎完全相同。

**总体而言，使用Elliot激活函数的模型表现略优于使用Sigmoid激活函数的模型。**

## Elliot与双曲正切

**1个周期**

+   *tanh @ 1*: 准确率: 0.3361 | 损失: 1.1578

+   *Elliot @ 1*: 准确率: 0.3361 | 损失: 1.0866

在1个周期时，Elliot激活函数模型与双曲正切激活函数模型的表现相同。我原本预期这些函数会产生相似的模型，因为它们都以类似的方式限制了传递到神经网络下一层的值。

**10个周期**

+   *tanh @ 10*: 准确率: 0.3277 | 损失: 0.9981

+   *Elliot @ 10*: 准确率: 0.6891 | 损失: 0.9434

使用Elliot激活函数的模型明显优于使用双曲正切激活函数的模型，就像Elliot模型在与Sigmoid模型比较时在10个周期的表现一样。

**100个周期**

+   *tanh @ 100:* 准确率: 0.9916 | 损失: 0.2325

+   *Elliot @ 100*: 准确率: 0.9580 | 损失: 0.5485

在100个周期时，双曲正切模型的表现明显优于Elliot模型。在更高的周期下，Elliot激活函数似乎表现不如*tanh*激活函数，但让我们看看在1000个周期时它们的表现有何不同。

**1000个周期**

+   *tanh @ 1000:* 准确率: 0.9748 | 损失: 0.0495

+   *Elliot @ 1000*: 准确率: 0.9832 | 损失: 0.0433

好吧，在1000个周期时，Elliot激活函数模型略微超越了双曲正切激活函数模型。

总的来说，我认为双曲正切和Elliot激活函数模型在神经网络的层次中几乎表现相同。**训练模型所需的时间可能会有所不同**，然而这些模型非常简单，随着数据量的增加以及网络规模的扩大，时间可能会成为一个更重要的因素。

## Elliot与ReLU

**1个周期**

+   *ReLU @ 1:* 准确率: 0.6639 | 损失: 1.0221

+   *Elliot @ 1*: 准确率: 0.3361 | 损失: 1.0866

在1个周期时，ReLU激活函数模型的表现*更好*，这表明Elliot激活函数使模型训练速度较慢。

**10个周期**

+   *ReLU @ 10*: 准确率: 0.6471 | 损失: 0.9413

+   *Elliot @ 10*: 准确率: 0.6891 | 损失: 0.9434

哇！包含Elliot激活函数的模型实际上表现*更好*，准确率高出4.2%，损失低了0.21%。

**100个周期**

+   *ReLU @ 100*: 准确率: 0.9160 | 损失: 0.4749

+   *Elliot @ 100*: 准确率: 0.9580 | 损失: 0.5485

尽管采用Elliot激活函数的模型损失较高，但它能够实现4.2%的更高准确率。*这再次*展示了Elliot激活函数在神经网络中的强大优势。

**1000个周期**

+   *ReLU @ 1000*: 准确率: 0.9916 | 损失: 0.0494

+   *Elliot @ 1000*: 准确率: 0.9832 | 损失: 0.0433

尽管采用 Elliot 激活函数的模型在准确性方面表现不佳，但损失较低，我对结果仍然感到满意。如 1000 次迭代所示，Elliot 激活函数几乎与 ReLU 激活函数一样好，并且在正确的问题和超参数调整下，Elliot 激活函数可能是更优的选择。

# 结论

今天，我们探讨了一种较不为人知的激活函数：**Elliot 激活函数**。为了测试它的性能，我们将其与形状相似的两个激活函数进行比较：Sigmoid 和双曲正切激活函数。结果显示，Elliot 函数的表现与这两个函数相当，甚至更好。接下来，我们将 Elliot 激活函数的性能与现代神经网络中使用的标准 ReLU 激活函数进行了比较。在四次试验中，采用 Elliot 激活函数的模型有 50% 的时间表现更好。在其他试验中，尽管表现不如前者，但其性能与使用 ReLU 激活函数的模型几乎一致。我建议在下一个神经网络中尝试 Elliot 激活函数，因为它可能表现更佳！

**如果你喜欢今天的阅读，请关注我，并告诉我是否有其他话题你希望我深入探讨！如果你没有 Medium 账户，可以通过我的链接** [**这里**](https://ben-mccloskey20.medium.com/membership)**注册！使用我的链接我将获得少量佣金。此外，也可以在** [**LinkedIn**](https://www.linkedin.com/in/benjamin-mccloskey-169975a8/) **添加我，或者随时联系我！感谢阅读！**

1.  Dubey, Shiv Ram, Satish Kumar Singh, 和 Bidyut Baran Chaudhuri. “深度学习中激活函数的全面调查与性能分析。” *arXiv 预印本 arXiv:2109.14545* (2021)。

1.  Sharma, Sagar, Simone Sharma, 和 Anidhya Athaiya. “神经网络中的激活函数。” *towards data science* 6.12 (2017): 310–316。

1.  [https://www.gallamine.com/2013/01/a-sigmoid-function-without-exponential_31.html](https://www.gallamine.com/2013/01/a-sigmoid-function-without-exponential_31.html)
