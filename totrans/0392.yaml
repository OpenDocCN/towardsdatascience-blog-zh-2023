- en: 'BigQuery Best Practices: Unleash the Full Potential of Your Data Warehouse'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://towardsdatascience.com/bigquery-best-practices-unleash-the-full-potential-of-your-data-warehouse-334a0a9adef2](https://towardsdatascience.com/bigquery-best-practices-unleash-the-full-potential-of-your-data-warehouse-334a0a9adef2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Supercharge your BigQuery experience with these 6 best practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://niczky12.medium.com/?source=post_page-----334a0a9adef2--------------------------------)[![Bence
    Komarniczky](../Images/d4de94667bcac6d9001390515592eab9.png)](https://niczky12.medium.com/?source=post_page-----334a0a9adef2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----334a0a9adef2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----334a0a9adef2--------------------------------)
    [Bence Komarniczky](https://niczky12.medium.com/?source=post_page-----334a0a9adef2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----334a0a9adef2--------------------------------)
    Â·5 min readÂ·Jun 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/597078e5659914a67c73196ce9a38721.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [charlesdeluvio](https://unsplash.com/@charlesdeluvio?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Google BigQuery, a powerful serverless data warehouse, has become a cornerstone
    of data analysis and machine learning pipelines for many organizations. To harness
    its full potential and ensure an efficient, cost-effective experience, itâ€™s crucial
    to follow some best practices. In this article, weâ€™ll delve into **six essential
    best practices that will help you optimize your BigQuery performance and usage**.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Keep an Eye on Cost Estimates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BigQueryâ€™s pricing model is based on the amount of data processed by your queries.
    Before running complex or large-scale queries, itâ€™s wise to review the cost estimates
    to ensure you stay within your budget. By understanding the cost implications
    of your queries, you can make informed decisions and avoid unexpected billing
    surprises.
  prefs: []
  type: TYPE_NORMAL
- en: Never select more columns than needed. BigQuery will push down your selections
    to the very beginning of computing if it can, but if you do `select *` youâ€™re
    shooting yourself in the foot as BigQuery would need to take all the columns for
    your query and that costs money.
  prefs: []
  type: TYPE_NORMAL
- en: Before you press `RUN`, take a peak at the top right corner and save yourself
    some ðŸ’°.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/489339e51fadd22e5c2fed735b0ef4c8.png)'
  prefs: []
  type: TYPE_IMG
- en: Youâ€™ll be looking for the number of GB/MB processed by the query â€” screenshot
    by author
  prefs: []
  type: TYPE_NORMAL
- en: 2 Sampling for Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Learning in BigQuery costs 50x more ($250/TB vs $5/TB) than querying
    so it is especially important to **reduce the amount of data youâ€™re working with**.
    By sampling your data, you can significantly reduce the amount of data processed,
    leading to faster model training and reduced costs. Use BigQueryâ€™s `FARM_FINGERPRINT()`
    function to generate reproducible random samples for your machine-learning pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out this article on how to do ML in BigQuery and do reproducible sampling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/end-to-end-bigquery-machine-learning-e7e6e2e83b34?source=post_page-----334a0a9adef2--------------------------------)
    [## End-to-End BigQuery Machine Learning'
  prefs: []
  type: TYPE_NORMAL
- en: Use Google Cloud BigQuery to compete in a Kaggle competition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/end-to-end-bigquery-machine-learning-e7e6e2e83b34?source=post_page-----334a0a9adef2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 3 Utilize Parquet for Data Uploads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When uploading data to BigQuery, consider using the Parquet file format. Parquet
    is a columnar storage format optimized for analytical workloads, offering improved
    compression and encoding schemes. Uploading data in Parquet format can lead to
    **reduced headaches and faster load times** as all your column information is
    embedded in your files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hereâ€™s how you can load files into BigQuery and why parquet is faster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/loading-files-into-bigquery-6de1ff63df35?source=post_page-----334a0a9adef2--------------------------------)
    [## Loading files into BigQuery'
  prefs: []
  type: TYPE_NORMAL
- en: Using Python to ingest Parquet and CSV files into GCP BigQuery
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/loading-files-into-bigquery-6de1ff63df35?source=post_page-----334a0a9adef2--------------------------------)
    [](/load-files-faster-into-bigquery-94355c4c086a?source=post_page-----334a0a9adef2--------------------------------)
    [## Load files faster into BigQuery
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking CSV, GZIP, AVRO and PARQUET file types for ingestion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/load-files-faster-into-bigquery-94355c4c086a?source=post_page-----334a0a9adef2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 4 Generate Arrays for More Powerful Experimentations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BigQuery allows you to **generate arrays for quick and cheap experimentation
    by querying your data multiple times**. Use the `GENERATE_ARRAY()` and `UNNEST()`
    functions to cross-join your data for testing parameters.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you have a TB of data, but you want to test 100 different parameters
    for an experiment, donâ€™t query your data 100 times as that would mean you query
    100TB resulting in a bill of $500! Instead, **if you cross-join your data with
    your 100 parameters, youâ€™ll only incur costs for querying that single TB**, so
    your bill is $5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hereâ€™s a made-up example for finding the optimal values for the `min_price`,
    `min_quantity`, and `max_delivery_time` parameters that maximize the profit margin
    while meeting customer expectations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'And if you want a more sophisticated example, check this article out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/advanced-random-sampling-in-bigquery-sql-7d4483b580bb?source=post_page-----334a0a9adef2--------------------------------)
    [## Advanced Random Sampling in BigQuery SQL'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to sample rows from BigQuery tables in a reproducible manner
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/advanced-random-sampling-in-bigquery-sql-7d4483b580bb?source=post_page-----334a0a9adef2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 5 Harness the Power of WITH Statements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BigQueryâ€™s `WITH` statement, also known as **Common Table Expressions** (CTEs),
    can make your queries more readable and maintainable. By using `WITH` statements,
    you can **break down complex queries into smaller, reusable components, making
    it easier to understand and debug your queries**. Additionally, CTEs can improve
    performance by preventing the same subquery from being executed multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use as many of these as you like, and you can even create new CTEs
    from already-defined CTEs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 6 Leverage Arguments in Custom Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When creating custom functions in BigQuery, make use of arguments to create
    reusable and flexible functions. **You can build modular and easily maintainable
    code by passing arguments to your functions**. This practice leads to cleaner
    and more efficient code, streamlining your development process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Want to learn about BigQuery custom functions? Glad you asked, check this out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/bigquery-udfs-complete-guide-181cbdaea55b?source=post_page-----334a0a9adef2--------------------------------)
    [## BigQuery UDFs Complete Guide'
  prefs: []
  type: TYPE_NORMAL
- en: Everything you need to know about Google Cloud BigQueryâ€™s User-Defined Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/bigquery-udfs-complete-guide-181cbdaea55b?source=post_page-----334a0a9adef2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Want to deploy a bunch of these using CI/CD pipelines on GitHub? Look no more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/deploying-bigquery-custom-functions-with-github-actions-d76c118e0abf?source=post_page-----334a0a9adef2--------------------------------)
    [## Deploying BigQuery Custom Functions with GitHub Actions'
  prefs: []
  type: TYPE_NORMAL
- en: Streamline your BigQuery custom functions deployment with the power of GitHub
    Actions automation.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/deploying-bigquery-custom-functions-with-github-actions-d76c118e0abf?source=post_page-----334a0a9adef2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Happy Querying!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mastering these best practices will help you take full advantage of BigQueryâ€™s
    features while maintaining cost efficiency and improving performance. By monitoring
    cost estimates, using data sampling with `FARM_FINGERPRINT()`, embracing Parquet,
    generating arrays, utilizing `WITH` statements, and employing arguments in custom
    functions, you'll be well-equipped to tackle any BigQuery challenge with confidence.
    Happy querying! ðŸ’»
  prefs: []
  type: TYPE_NORMAL
