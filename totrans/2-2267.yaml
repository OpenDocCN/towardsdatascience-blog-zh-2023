- en: Using enums and functools to Upgrade Your Pandas Data Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/using-enums-and-functools-to-upgrade-your-pandas-data-pipelines-d51ca1418fe2](https://towardsdatascience.com/using-enums-and-functools-to-upgrade-your-pandas-data-pipelines-d51ca1418fe2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: PROGRAMMING
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A look at more efficient programming for your data processing with two step-by-step
    examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://byrondolon.medium.com/?source=post_page-----d51ca1418fe2--------------------------------)[![Byron
    Dolon](../Images/9ff32138c7b1913be24cc7ab971752b0.png)](https://byrondolon.medium.com/?source=post_page-----d51ca1418fe2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d51ca1418fe2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d51ca1418fe2--------------------------------)
    [Byron Dolon](https://byrondolon.medium.com/?source=post_page-----d51ca1418fe2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d51ca1418fe2--------------------------------)
    ·12 min read·Jun 9, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4628e4600a99a70c1aab75fbd4986ddd.png)'
  prefs: []
  type: TYPE_IMG
- en: Image used with permission by my talented sister [ohmintyartz](https://www.instagram.com/ohmintyartz/)
  prefs: []
  type: TYPE_NORMAL
- en: You’ve likely worked with Pandas before when creating a pipeline to process
    your raw data. Writing code to filter, group, and perform calculations on your
    data is just the first step in building data pipelines and ETL processes.
  prefs: []
  type: TYPE_NORMAL
- en: Working with data at scale means that in addition to this, we should write code
    that’s **functional** and **easy to read and maintain**.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of different things you can do to improve your existing data
    pipelines like [adding efficient logging](/why-you-need-to-write-dry-code-with-decorators-in-python-3930ea23f569),
    [including data validation](/how-to-do-data-validation-on-your-data-on-pandas-with-pytest-d5dda51ad0e4),
    and even by using new libraries besides Pandas like PySpark and Polars.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, you can also structure the actual code you use for processing data
    differently. This means not doing something to necessarily improve the performance
    of your pipelines, but rather focusing on writing code that is easy to modify
    and iterate on over time.
  prefs: []
  type: TYPE_NORMAL
- en: In this piece, let’s take a look at how you can do just that with two simple
    examples using some native Python, specifically by using **enums** and **functools.**
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to follow along in a notebook or IDE of your choice! You can download
    the dataset from Kaggle [here](https://www.kaggle.com/datasets/deepanshuverma0154/sales-dataset-of-ecommerce-electronic-products?resource=download),
    available free for use under the CC0 1.0 Universal (CC0 1.0) Public Domain Dedication
    license. Then import and run the following and we can get started!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Using enums to better structure your data pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A quick introduction to enums
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You might first be wondering “What’s an enum”?
  prefs: []
  type: TYPE_NORMAL
- en: An enum, short for enumeration, is a “set of symbolic names (members) bound
    to unique values” ([Python docs, 2023](https://docs.python.org/3/library/enum.html)).
    Practically speaking, this means that you can define and use a set of related
    variables under one main “class”.
  prefs: []
  type: TYPE_NORMAL
- en: A simple example would be having an enum class “Color”, and having names like
    “Red”, “Green”, and “Blue” that you could use whenever you wanted to refer to
    specific colors.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you’re probably wondering what’s the point of defining some variables
    in a separate enum class when you could just call the names you need directly
    in your data processing pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enums have a few key benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining enums lets you have related constants organized in one (or many) classes
    that can act as a source of truth for dimensions, measures, and other constants
    you need to call in your pipelines;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using enums will allow you to avoid passing invalid values in your data pipelines,
    assuming you correctly define and maintain the enum class;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enums allow users to work with a standardized set of data points and constants,
    which is helpful when multiple people are aggregating or creating models based
    on one main source of data (to help avoid having multiple definitions or aliases
    for the same column in the raw data source).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This sounds a little abstract, so let’s take a look at how you can practically
    apply enums when working with data in an example of a standard pre-processing
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Using enums in your data processing pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We already have our initial DataFrame, so let’s begin by creating a function
    to add a few more columns to our data by splitting the purchase address.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, we can apply this to our existing table by using the native Pandas `pipe`
    method like so, where we call pipe on the DataFrame and pass the function name
    as an argument.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e85b5ba7816b44f6e634fa0b2ceee827.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, you’ll see that the data we have is still on a very granular level, with
    the Order ID as the primary key of the table. When we want to aggregate the data
    for further analysis, we can use the `groupby` method in Pandas to do so.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some code you might see in Pandas to group data on a set of columns and then
    do an aggregate count on one of the dimensions (in this case we’ll use the Order
    ID) can look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of this is a new DataFrame that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e18a7e934db461347542d91f149e2c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this simple example, grouping by just six columns is not too difficult and
    we can pass a list of these columns directly to the `groupby` method. However,
    this has a few drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: What if we were working with a larger dataset and wanted to group by 20 columns?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What if we had new requirements from end users come in and we needed to tweak
    the specific columns to group by?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What if the underlying table changes and the names or aliases of the columns
    changed?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can in part address some of these issues by defining the columns in an enum
    class. Specifically for this case, we can define these group by columns pertaining
    to our sales table in a new class `SalesGroupByColumns` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: What we’re doing here is ultimately just defining the columns as constants inside
    a new Enum class (which is taken from the import before `from enum import Enum`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have these new enum values defined, we can access individual members
    of the enum like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3915711c7889df5bb3122a851ee8a381.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/73ecc590411f9f6fc11c6df48cfd56b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Just calling the enum name will return the enum member, and calling `value`
    on the target enum lets us access the string value of the enum member directly.
    Now, to get all members of the enum into a list we can pass to the `groupby`,
    we can use a list comprehension like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/41eda42e5cf90a47f8fca043a4a67193.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Having gotten them into a list, we can assign this output to a variable and
    then pass this variable to our `groupby` method instead of passing the raw list
    of strings directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b952f64a8f81cf5324ab3562cb9c97ca.png)'
  prefs: []
  type: TYPE_IMG
- en: We arrive at the same table as before, but with code, that’s a little cleaner
    to look at. The benefit of this for maintainability can be seen if you’re working
    on the processing pipeline over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, you might find that you want to add by a few new columns, like
    for example if you also wanted to do a little more feature engineering and create
    a house number and product category column to then add to the group by. You could
    update your enum class like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Then, you wouldn’t need to change your existing processing code, because the
    list comprehension would automatically grab all the values in the `SalesGroupByColumns`
    class and apply that to your aggregation logic.
  prefs: []
  type: TYPE_NORMAL
- en: A good note here is that all of this will only work if you know exactly what
    you’re defining in the enum class and use them only as intended. If you make changes
    here and you’re grabbing all these columns to group by in a few different tables,
    it’s important that that’s what you’re intending to do.
  prefs: []
  type: TYPE_NORMAL
- en: Otherwise, you could instead define the set of enums you need to use for a specific
    table either in separate classes or if it makes sense in a separate list of columns
    (so you still avoid passing a raw list of strings to the `groupby` method.
  prefs: []
  type: TYPE_NORMAL
- en: Using enums for your data aggregation in Pandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For another example, say we had a different case where we apply a few additional
    transformations to our data by changing some columns'' data types and creating
    a new total cost column. We can add to our previous pipeline like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/37f58c1379c96c134a5c7196496b7210.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that our DataFrame is transformed on an Order ID level, let’s next perform
    another group by on a new set of columns, but this time aggregate on a few different
    measures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/37022fbdd3e0e7378448172b00aa9c58.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are a few key points to note here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve defined a new set of enum classes: `AddressColumns` and `SalesMeasureColumns`.
    Now for a different table where we want to group specifically on address fields,
    we can instead define the `groupby_columns` list to include those columns to later
    pass to the `groupby` method on the transformed DataFrame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `SalesMeasureColumns` class includes the measures that we want to be aggregating.
    Putting column names from the raw table in the class means that if other people
    also want to sum the cost and quantity ordered they call the proper columns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We could additionally add to our pipeline from before that chained pipes and
    the functions we defined from before and put this code to collect the list of
    columns and aggregate the table in a new function. Then the final code becomes
    easier to read and potentially debug and log over time.
  prefs: []
  type: TYPE_NORMAL
- en: For the aggregation, it’s also possible that total_cost and quantity_ordered
    are defined differently for different tables, teams, and end users. Defining it
    in the enum for `SalesMeasuresColumns` means that for the Sales table and measures
    all users can do the aggregation on these columns with the same definitions.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging the reduce method from functools to clear up your data filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let’s take a look at how we can use the reduce method from functools to
    improve how we filter data.
  prefs: []
  type: TYPE_NORMAL
- en: In Pandas, a common way to filter your raw data is to use the `loc` method.
    As an example, let’s write some code to filter our data on a street that contains
    “North”.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c7c29163ced8d7bfbdde0f79581465f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our output DataFrame now only includes the columns according to the filter.
    In general, you’d likely want to apply multiple filters to the DataFrame when
    doing either analysis on a specific problem or tweaking your data set to how you
    want for a machine learning model. You can apply multiple filters with the `loc`
    method like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/83f150b7ccbd0dff1c8e92d9895eab67.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, just like before when we were optimizing our code with enums, what if
    we wanted to add, edit, or remove some of the filters for this process? Changing
    requirements from your end users or new insights you find from some exploratory
    data analysis can mean you need to tweak how you filter your data over time.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of continuing to add more lines of code in the `loc` method directly,
    we can define a list of filters in a variable to then pass to `loc` later on.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: However, we can’t just pass this list into `loc`. The format that multiple conditions
    in `loc` accepts is the boolean masks from the DataFrame separated by the `&`
    operator.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This is where the `functools.reduce` method comes in. How we can then implement
    the correct format can be seen in an example here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/47b54dee9dadefd3241998c1d625315b.png)'
  prefs: []
  type: TYPE_IMG
- en: In this simple example, we’re just combining a bunch of strings together to
    output what we’d need to write in our `loc` method later on.
  prefs: []
  type: TYPE_NORMAL
- en: The `reduce` method from functools allows you to pass a function and an iterable
    as arguments. The `reduce` method then applies the function to the elements in
    the iterable cumulatively. This means that it will perform the function in sequence
    for the set and combination of elements in the iterable.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we have an anonymous `lambda` function that takes the two arguments
    `x` and `y` and combines them using an f-string and separating them with an `&`
    operator. This means first the function would output `condition_1 & condition_2`,
    then it would add `condition_3` to the first accumulated value, resulting in `condition_1
    & condition_2 & condition_3`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can pass this reduce method with the function and filter conditions
    to the `loc` method, instead of passing all the raw filter conditions to `loc`
    one by one.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2a352c24c6f9b077932a20741cd3cf93.png)'
  prefs: []
  type: TYPE_IMG
- en: Our final DataFrame looks like this with all the filters applied. Now, if you
    wanted to include another filter, you could just add them to the existing `filter_conditions`
    variable and leave the rest of the code unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: The focus of this piece was to present a few different ways of structuring your
    code differently to improve readability and maintainability over time. While these
    changes won’t necessarily speed up your pipelines or improve memory usage, it
    is important to think about how easy your code is to work with.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be helpful for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: If you have multiple people working with the same raw data over time, defining
    column names and filters in a centralized place means that everyone can refer
    back to a single source of truth and avoid having different names and logic to
    refer to ultimately the same thing;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When working with data over time that includes changing requirements, you only
    need to fix column names and logic in one place to speed up your development time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I hope you found these tips for improving your data pipelines helpful. Give
    this structure for improving your data processing a go and see if it works for
    you!
  prefs: []
  type: TYPE_NORMAL
- en: If you enjoy my content consider following me and **signing up to be a Medium
    member** using my referral link below. It costs only $5 a month and you’ll get
    unlimited access to everything on Medium. Signing up using my link lets me earn
    a small commission. And if you’re already signed up to follow me, thanks a bunch
    for your support!
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://byrondolon.medium.com/membership?source=post_page-----d51ca1418fe2--------------------------------)
    [## Join Medium with my referral link — Byron Dolon'
  prefs: []
  type: TYPE_NORMAL
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: byrondolon.medium.com](https://byrondolon.medium.com/membership?source=post_page-----d51ca1418fe2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*M****ore by me:*** *-* [*3 Efficient Ways to Filter a Pandas DataFrame Column
    by Substring*](https://medium.com/towards-artificial-intelligence/3-efficient-ways-to-filter-a-pandas-dataframe-column-by-substring-fc2993692141)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*-* [*5 Practical Tips for Aspiring Data Analysts*](https://byrondolon.medium.com/5-practical-tips-for-aspiring-data-analysts-9917006d4dae?sk=019edbddaca4d313665caafe4b747d26)
    *-* [*Improving Your Data Visualizations with Stacked Bar Charts in Python*](/improving-your-data-visualizations-with-stacked-bar-charts-in-python-f18e2b2b9b70)
    *- C*[*onditional Selection and Assignment With .loc in Pandas*](/conditional-selection-and-assignment-with-loc-in-pandas-2a5d17c7765b?sk=e5672d859a3964c1453a1c09edca22cf)
    *-* [*5 (and a half) Lines of Code for Understanding Your Data with Pandas*](/5-and-a-half-lines-of-code-for-understanding-your-data-with-pandas-aedd3bec4c89?sk=7007a1ae248cf7ea4ef5fcd4af7ae72b)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
