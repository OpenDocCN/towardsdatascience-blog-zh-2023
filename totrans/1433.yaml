- en: 'LLM Economics: ChatGPT vs Open-Source'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/llm-economics-chatgpt-vs-open-source-dfc29f69fec1](https://towardsdatascience.com/llm-economics-chatgpt-vs-open-source-dfc29f69fec1)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How much does it cost to deploy LLMs like ChatGPT? Are open-source LLMs cheaper
    to deploy? What are the tradeoffs?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://skanda-vivek.medium.com/?source=post_page-----dfc29f69fec1--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----dfc29f69fec1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dfc29f69fec1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dfc29f69fec1--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----dfc29f69fec1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dfc29f69fec1--------------------------------)
    ·6 min read·Apr 26, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa975d94faf37f22c807696c80da0324.png)'
  prefs: []
  type: TYPE_IMG
- en: Cartoon schematic for comparing LLM costs | Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: 'TLDR: For lower usage in the 1000’s of requests per day range ChatGPT works
    out cheaper than using open-sourced LLMs deployed to AWS. For millions of requests
    per day, open-sourced models deployed in AWS work out cheaper. (As of writing
    this article on April 24th, 2023.)'
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models are taking the world by storm. Transformers were introduced
    in 2017, followed by breakthrough models like BERT, GPT, and BART — 100’s of millions
    of parameters; and capable to performing multiple Language tasks like sentiment
    analysis, Q&A, classification, etc.
  prefs: []
  type: TYPE_NORMAL
- en: A couple of years ago — researchers from OpenAI and Google documented multiple
    papers showing that large language models with more than 10’s of Billions of parameters
    started showing emergent capabilities where they seemingly understand complex
    aspects of language and are almost human-like in their responses.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f39567219981d5f1f495ab8bd9f9174f.png)'
  prefs: []
  type: TYPE_IMG
- en: '[GPT-3 paper](https://arxiv.org/abs/2005.14165) showing the impressive learning
    capabilities of Large Language Models'
  prefs: []
  type: TYPE_NORMAL
- en: The GPT-3 paper showed that models > 10-100 Billion parameters show impressive
    learning capabilities with just a few tens of prompts.
  prefs: []
  type: TYPE_NORMAL
- en: However, these LLMs are so resource intensive that they were economically challenging
    to deploy at scale. That is, until the recent arrival of ChatGPT. Soon after the
    ChatGPT interface was released, OpenAI made the ChatGPT API accessible, so developers
    could use ChatGPT in their applications. Let’s see how much these cost at scale
    and the economic viability.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT API costs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ChatGPT API is priced by usage. It costs $0.002/1K tokens. Each token is
    3/4th a word roughly — and the number of tokens in a single request is the sum
    of prompt + generated output tokens. Let’s say you process 1000 small chunks of
    text per day, each chunk being a page of text — so 500 words or 667 tokens. **This
    comes to $0.002/1000x667*1000= ~$1.3 a day.** Not too bad!
  prefs: []
  type: TYPE_NORMAL
- en: But what happens if you are processing a million such documents a day? **Then
    it is $1,300 per day or ~0.5 Million$ per year!** ChatGPT goes from being a cool
    toy to being a major expense (and consequently one hopes — a major source of revenue)
    in a multi-million dollar business!
  prefs: []
  type: TYPE_NORMAL
- en: Opensource Generative Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After ChatGPT came out, there were a bunch of open source initiatives. Meta
    came out with [LLaMA — a 10’s of billions of parameters LLM model](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
    that outperforms GPT-3\. Stanford then fine-tuned the 7B version of LLaMA on 52K
    instruction-following demos and found that [their Alpaca model](https://crfm.stanford.edu/2023/03/13/alpaca.html)
    outperformed GPT-3.
  prefs: []
  type: TYPE_NORMAL
- en: '[A team of researchers also recently showed a 13B parameter fine-tuned LLaMA
    model called Vicuna acheived >90% of ChatGPT quality](https://vicuna.lmsys.org/).
    There are multiple reasons why companies might choose to use opensource generative
    models instead of the GPT family of models from OpenAI. These include less susceptibility
    to OpenAI outages, easier to customize, and possibly cheaper.'
  prefs: []
  type: TYPE_NORMAL
- en: While open source models are free to use, the infrastructure to host and deploy
    them is not. And while earlier transformer models like BERT could easily be run
    and fine-tuned on personal computers with a good CPU and basic GPUs, LLMs are
    more resource intensive. A common solution is to use cloud providers like AWS
    to host and deploy such models.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive into AWS costs for hosting open source models.
  prefs: []
  type: TYPE_NORMAL
- en: AWS costs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s discuss the standard architecture for deploying models in AWS
    and serving them as APIs. Usually there are three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the model as an endpoint usingAWS Sagemaker.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connecting the Sagemaker endpoint to AWS Lambda.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Serving the Lambda function as an API through API Gateway
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/f6023397d6b436e188b548f74e4fc97a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Invoking a Sagemaker model endpoint using API Gateway and Lambda](https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/)'
  prefs: []
  type: TYPE_NORMAL
- en: When the client makes an API call to the API Gateway, it triggers the lambda
    function, that parses the function and sends it to the Sagemaker endpoint. The
    model endpoint then does the prediction, and sends the information to Lambda.
    Lambda parses this and sends it to the API, and eventually back to the client.
  prefs: []
  type: TYPE_NORMAL
- en: The Sagemaker costs are sensitive to the type of computing instance for hosting
    the model. LLMs use rather large computing instance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b32a9a0d869317c427aa9307917cd4f7.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Sagemaker instance pricing](https://aws.amazon.com/sagemaker/pricing/?p=pm&c=sm&z=2)'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, this article written by someone from AWS details how to deploy
    Flan UL2 — a 20 Billion parameter model, on AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://betterprogramming.pub/deploy-flan-ul2-on-a-single-gpu-1778dac605f3?source=post_page-----dfc29f69fec1--------------------------------)
    [## Deploy Flan-UL2 on a Single GPU With Amazon SageMaker'
  prefs: []
  type: TYPE_NORMAL
- en: The Hugging Face + AWS partnership makes it easier than ever to experiment with
    open-source state-of-the-art language…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: betterprogramming.pub](https://betterprogramming.pub/deploy-flan-ul2-on-a-single-gpu-1778dac605f3?source=post_page-----dfc29f69fec1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The article used the ml.g5.4xlarge instance. While the Sagemaker pricing above
    didn’t list the cost for this specific instance price, it seems like it would
    be in the ballpark of ~5$ per hour. Which comes to 150$ per day! And this is just
    for instance hosting, we haven’t yet come to the Lambda and API Gateway costs.
  prefs: []
  type: TYPE_NORMAL
- en: Below details the AWS lambda pricing — which is a function of memory usage and
    frequency of requests.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/76c3c94e61983612142278210fbaa61c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[AWS Lambda Pricing](https://aws.amazon.com/lambda/pricing/)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say it takes 5s for obtaining a response. 128 MB is plenty given we are
    routing the data to the AWS Sagemaker endpoint. So this would cost 5*.128*1000*$0.0000166667=
    0.01$ for 1000 requests, or 10$ for 1M requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'And the final cost is for the API gateway:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bae2fc46e6a3b3c1ae8ba8e38e0c60e5.png)'
  prefs: []
  type: TYPE_IMG
- en: '[AWS API Gateway Pricing](https://aws.amazon.com/api-gateway/pricing/)'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the API Gateway is pretty cheap — 1$ per million requests.
  prefs: []
  type: TYPE_NORMAL
- en: So ultimately **the cost of hosting an open-source LLM like Flan-UL2 on AWS
    is 150$ for 1000 requests a day, and 160$ for 1 M requests a day.**
  prefs: []
  type: TYPE_NORMAL
- en: But do we always need such costly computing instances? For smaller language
    models like BERT that are 100’s of millions of parameters — you can get away with
    using cheaper instances like ml.m5.xlarge that is **$0.23/hour and ~5$ a day.**
    These models are also pretty powerful, and are more task and training data specific
    as compared to LLMs that seem to understand the complex nuances of language.
  prefs: []
  type: TYPE_NORMAL
- en: Closing Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So which is better? Using paid service LLMs like OpenAI’s GPT series? Or open-access
    LLMs? It depends on the use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8da5d346bdd3bdd5e95c61253fdf48cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Pros and Cons of Paid Service LLMs | Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/edbb5072fea5e8b9d77b42a82d1b4cb9.png)'
  prefs: []
  type: TYPE_IMG
- en: Pros and Cons of Open-Access LLMs | Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Since this is such a rapidly advancing field, it is quite possible that
    due to the large-scale demands, in the relatively near future, deployment costs
    drastically reduce. (Keep in mind that while hosting open-source LLMs is a challenge,
    smaller language models like BERT that have 100’s of millions of parameters are
    still a great option for specific tasks. I’ve written articles on how fine-tuning
    BERT based models on tasks like [question answering](/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80)
    and [spam detection](/transformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1)
    can yield near human performance.)'
  prefs: []
  type: TYPE_NORMAL
- en: But which model is better? The responses from ChatGPT and GPT-4 are more relevant
    than those from open-source LLMs. However, open-source models are catching up
    quickly. And there might be very good reasons for using open source models rather
    than closed APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Companies want to fine-tune open source models on their specific data sources.
    ChatGPT and subsequent OpenAI models might not perform as well as open sourced
    models fine-tuned on domain specific data; due to the generic nature of such models.
    Already, we are seeing domain specific models like BloombergGPT making powerful
    moves in Generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Oh — and let’s all pray that OpenAI does not hike the price of the ChatGPT API.
    When the ChatGPT API came out it was pleasantly surprising that the API was priced
    10X cheaper than the earlier GPT-3 API.
  prefs: []
  type: TYPE_NORMAL
- en: We live in exciting times!
  prefs: []
  type: TYPE_NORMAL
- en: '*If you like this post, follow me — I write on topics related to applying state-of-the-art
    NLP in real-world applications and, more generally, on the intersections between
    data and society.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Feel free to connect with me on* [*LinkedIn*](https://www.linkedin.com/in/skanda-vivek-01619311b/)*!*'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you are not yet a Medium member and want to support writers like me, feel
    free to sign-up through my referral link:* [*https://skanda-vivek.medium.com/membership*](https://skanda-vivek.medium.com/membership)'
  prefs: []
  type: TYPE_NORMAL
