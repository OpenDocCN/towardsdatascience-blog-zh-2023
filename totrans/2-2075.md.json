["```py\nfrom datasets import load_dataset\n\ndatasets = load_dataset('wikitext', 'wikitext-2-raw-v1')\n```", "```py\ndatasets[\"train\"]\n```", "```py\nDataset({\n    features: ['text'],\n    num_rows: 36718\n})\n```", "```py\nfrom transformers import AutoTokenizer\n\nmodel_checkpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n```", "```py\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], add_special_tokens=False)\n\ntokenized_datasets = datasets.map(\n    tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])\n```", "```py\ndef group_texts(examples, block_size=255):\n    # Concatenate all texts.\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    # We drop the small remainder. We can use padding later.\n    total_length = (total_length // block_size) * block_size\n    # Split by chunks of `block_size`.\n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    return result\n```", "```py\nlm_datasets = tokenized_datasets.map(\n    group_texts, batched=True, batch_size=1000, num_proc=4)\n```", "```py\ndef create_nsp_entry(example, idx, dataset, total_examples):\n    \"\"\"\n    Create a Next Sentence Prediction entry using the given example and its index.\n    \"\"\"\n    import random\n\n    first_sentence = example['input_ids']\n    attention_mask = [1] * 512\n    next_sentence_label = 0\n\n    # Decide the second sequence based on the index\n    if idx % 4 < 2:  # Use subsequent sequences half of the time\n        next_idx = idx + 1\n        try:\n            next_sentence = dataset[next_idx]['input_ids']\n        except IndexError:\n            # If the index is out of bounds\n            # (e.g., the example is the last in the dataset),\n            # wrap around to the start.\n            # In this case, set the label to 1.\n            next_idx = next_idx % total_examples\n            next_sentence = dataset[next_idx]['input_ids']\n            next_sentence_label = 1  # Indicate that the sentences are not consecutive.\n        # Set attention mask accordingly.\n        attention_mask = [1] + example['attention_mask'] + [1] + dataset[next_idx]['attention_mask']\n    else:\n        # Get a random sentence to create a negative example.\n        rand_idx = random.randint(0, total_examples - 1)\n        while rand_idx == idx:  # Ensure we don't pick the same example.\n            rand_idx = random.randint(0, total_examples - 1)\n        next_sentence = dataset[rand_idx]['input_ids']\n        next_sentence_label = 1\n        attention_mask = [1] + example['attention_mask'] + [1] + dataset[rand_idx]['attention_mask']\n\n    # Create combined input IDs\n    combined_input_ids = [tokenizer.cls_token_id] + first_sentence + [tokenizer.sep_token_id] + next_sentence\n\n    # Create token type IDs - the first sentences + the [SEP] are set to 0\n    token_type_ids = [0] * (257) + [1] * (255)\n\n    return {\n        'input_ids': combined_input_ids,\n        'token_type_ids': token_type_ids,\n        'attention_mask': attention_mask,\n        'next_sentence_label': next_sentence_label\n    }\n```", "```py\ndef forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,  # Different segments\n        position_ids: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None,\n        next_sentence_label: Optional[torch.Tensor] = None,  # labels for NSP\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], BertForPreTrainingOutput]:\n   ...\n```", "```py\ntrain_dataset = lm_datasets['train']\nvalidation_dataset = lm_datasets['validation']\ntest_dataset = lm_datasets['test']\n\nnsp_train_dataset = train_dataset.map(\n    lambda example, idx: create_nsp_entry(example, idx, train_dataset, total_examples=len(train_dataset)),\n    with_indices=True)\n\nnsp_validation_dataset = validation_dataset.map(\n    lambda example, idx: create_nsp_entry(example, idx, validation_dataset, total_examples=len(validation_dataset)),\n    with_indices=True)\n\nnsp_test_dataset = test_dataset.map(\n    lambda example, idx: create_nsp_entry(example, idx, test_dataset, total_examples=len(test_dataset)),\n    with_indices=True)\n```", "```py\ndef torch_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None) -> Tuple[Any, Any]:\n    \"\"\"\n    Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n    \"\"\"\n    import torch\n\n    labels = inputs.clone()\n    # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n    probability_matrix = torch.full(labels.shape, self.mlm_probability)\n    if special_tokens_mask is None:\n        special_tokens_mask = [\n            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n        ]\n        special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n    else:\n        special_tokens_mask = special_tokens_mask.bool()\n\n    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n    masked_indices = torch.bernoulli(probability_matrix).bool()\n    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n\n    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n    inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n\n    # 10% of the time, we replace masked input tokens with random word\n    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n    random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n    inputs[indices_random] = random_words[indices_random]\n\n    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n    return inputs, labels\n```", "```py\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15) \n```"]