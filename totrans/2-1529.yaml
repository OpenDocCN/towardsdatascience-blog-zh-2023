- en: 'Model Selection with Imbalance Data: Only AUC may Not Save you'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/model-selection-with-imbalance-data-only-auc-may-not-save-you-5aed73c5efed](https://towardsdatascience.com/model-selection-with-imbalance-data-only-auc-may-not-save-you-5aed73c5efed)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Are you Searching Parameters Efficiently?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@cerlymarco?source=post_page-----5aed73c5efed--------------------------------)[![Marco
    Cerliani](../Images/48a07a024349bac3c8e397bf5a0372e2.png)](https://medium.com/@cerlymarco?source=post_page-----5aed73c5efed--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5aed73c5efed--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5aed73c5efed--------------------------------)
    [Marco Cerliani](https://medium.com/@cerlymarco?source=post_page-----5aed73c5efed--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5aed73c5efed--------------------------------)
    ·6 min read·Feb 22, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0fd9065c148b0219514a0be4d198178b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Mpho Mojapelo](https://unsplash.com/@mpho_mojapelo?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'Most data scientists, who attend meetings to present ML results to business
    stakeholders, usually answer questions like these:'
  prefs: []
  type: TYPE_NORMAL
- en: AUC? What is it? Could you please elaborate?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Terms and concepts standard in data science daily routine may be unfamiliar
    to most**. This frequently happens when artificial intelligence products are developed
    to solve real-world problems. In this scenario, data scientists work together
    and collaborate with domain experts to understand field dynamics and accordingly
    incorporate them into automated solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Having a critical view, of the added value that artificial intelligence provides
    in solving a business problem, is crucial**. In a lot of situations, the adoption
    of machine learning may be useless since the tasks can be solved with simple automation
    rules, or there is no evidence in the available data that justifies the usage
    of artificial intelligence techniques. That said, **choosing the most appropriate
    metrics to evaluate the effectiveness of proposed solutions, it’s a very important
    step**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The choice of proper metrics is domain-dependent and changes according to
    the needs**. Choosing AUC as a metric, to present to business stakeholders the
    goodness/strengths of the machine learning approach adopted, may be risky. Firstly
    because the definition of AUC may not be clear to all. Secondly, **it isn’t easy
    to give an economic meaning to AUC**. Business people are money-oriented. If they
    don’t understand that the proposed solutions make them save time or money, they
    likely reject them.'
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we don’t suggest a method to choose the correct business metric.
    Instead, we focus on a more technical problem that is strictly correlated with
    the metric definition. We are referring to model selection. We want to **test
    the effectiveness of model selection in an imbalanced binary classification context**.
    The scope is to investigate how simple decisions (like metric selection or threshold
    tuning) influence the final results and how these relate to the business goals.
  prefs: []
  type: TYPE_NORMAL
- en: EXPERIMENT SETUP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We start by simulating an unbalanced tabular dataset with 90% negative and 10%
    positive target samples.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/313f7fec937b48e91d68b228a18088ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Target distribution of simulated data [image by the author]
  prefs: []
  type: TYPE_NORMAL
- en: We can imagine the minority class (10% of the sample in our case) as the customers
    that churned in a fixed temporal range, as the failures that happened in an engine
    system, or also as the number of frauds occurred. **For data scientists, working
    with unbalanced data in the real world is the normality**.
  prefs: []
  type: TYPE_NORMAL
- en: Unbalancing is difficult to deal with. Instead of fighting with extreme unbalance,
    a better approach, which is simple and works in most cases, consists in leveraging
    it during the learning phase. In other words, **it’s better to not engage in experimenting
    with oversampling methodologies, but it’s best to simply downsample the majority
    class or leave it as is**. Applying a reasonable undersampling ratio, it’s possible
    to make the models learn from the data. Furthermore, the unbalanced nature of
    the phenomena is preserved and replicable at inference time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c58909b178b150978d7d87a088198efc.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparing techniques to handle target imbalance [image by the author]
  prefs: []
  type: TYPE_NORMAL
- en: With this simple modeling strategy in mind, we are ready to deep dive into model
    selection.
  prefs: []
  type: TYPE_NORMAL
- en: MODEL SELECTION
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/715a2cbb1411667d96ff80ec13419c93.png)'
  prefs: []
  type: TYPE_IMG
- en: Machine Learning use cases lifecycle [image by the author]
  prefs: []
  type: TYPE_NORMAL
- en: '**When searching for the best model or set of parameters in unbalance scenarios,
    the choice of the suited metrics falls on scoring-based ones**. We are referring
    to all the metrics which evaluate the goodness of fit using the predicted probabilities.
    In a binary classification context, the most known scoring metrics are [AUC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score),
    [average precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score),
    or [cross-entropy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Using a scoring metric in this situation seems a reasonable solution. We are
    evaluating the goodness of fit independently from a hard threshold, like one used
    to compute [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html),
    [precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score),
    [recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score),
    or [Fbeta](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score).
  prefs: []
  type: TYPE_NORMAL
- en: Coming back to our simulated use case… Supposing one of the requirements, defined
    by the business stakeholder, is to obtain a high precision on the minority class.
    How can we carry out model selection and parameter tuning to satisfy this request?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We set up a randomized search with a random forest, searching for the optimal
    number of trees. We register cross-validated scoring for AUC, average precision,
    and Fbeta. We choose **Fbeta with a low beta value (0.1) as an approximation for
    precision** (what we are trying to optimize). The trial results are reported in
    the plots below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29a56bd68514c5cec13ee05be0493aa6.png)'
  prefs: []
  type: TYPE_IMG
- en: Fbeta as a function of AUC (on the left) and average precision (on the right)
    [image by the author]
  prefs: []
  type: TYPE_NORMAL
- en: As expected, there is no clear relation between AUC/average precision and Fbeta.
    **Choosing the model with the best AUC doesn’t guarantee the choice of the model
    with the best Fbeta**.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, with our “optimal” parameter configuration selected according
    to AUC, we have to operate an additional fine-tuning, on a newer set of data,
    to select a hard threshold to maximize precision and make our stakeholders happy.
  prefs: []
  type: TYPE_NORMAL
- en: Nothing bad in doing this but, is there a more efficient approach? **Can we
    make the threshold tuning related to the choice of parameters?**
  prefs: []
  type: TYPE_NORMAL
- en: Embedding the threshold searching inside the model training is straightforward.
    **With the *ThresholdClassifier* estimator, it’s possible to tune a binary classification
    threshold while optimizing a defined scoring function** (Fbeta in our case). This
    is done automatically on a validation set derived by splitting the received training
    data. The predicted classes are obtained by discretizing the probabilities according
    to the tuned threshold.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The ***ThresholdClassifier*** estimator is model agnostic and can be used with
    any binary classifier that outputs probabilities. In our example, we apply it
    to our random forest allowing, as before, the search for optimal parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/43f99d171f0503304442cf37fde39d4a.png)'
  prefs: []
  type: TYPE_IMG
- en: Fbeta as a function of AUC (on the left) and average precision (on the right)
    [image by the author]
  prefs: []
  type: TYPE_NORMAL
- en: Not surprisingly, there is no relationship between AUC/average precision and
    Fbeta. Comparing the scores, obtained by the raw random forest and the random
    forest with threshold tuning, we observe a difference in the value of Fbeta.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/60f6dbe9023c9a6bba28742436fa1f39.png)'
  prefs: []
  type: TYPE_IMG
- en: Fbeta obtained w/ (red) and w/o threshold tuning (blue) for the same set of
    parameters [image by the author]
  prefs: []
  type: TYPE_NORMAL
- en: '**The search for an optimal value of the classification threshold provides
    better precision on the minority class for the same set of parameters**. The results
    don’t affect the produced probabilities. Scoring metrics, like AUC or average
    precision, remain unaltered.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/927330c1514cb30dbb1ece538c378612.png)'
  prefs: []
  type: TYPE_IMG
- en: Fbeta, as a function of AUC (on the left) and average precision (on the right),
    obtained w/ (red) and w/o threshold tuning (blue) [image by the author]
  prefs: []
  type: TYPE_NORMAL
- en: We are not here for claiming the models with the best performances by looking
    at cents improvements of validation metrics. We must pursue business goals. In
    our simulated scenario, it’s evident that with simple tricks **we can obtain better
    precision**. The notable point is that we get these findings **without the need
    for additional validation data and combining parameter search with threshold tuning**.
  prefs: []
  type: TYPE_NORMAL
- en: SUMMARY
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, we outlined the main differences between scoring metrics and accuracy-based
    ones. We saw how these behave in an unbalance binary classification context to
    solve real business problems. If our scope is to measure how good we are at detecting
    churned customers, identifying frauds, or finding failed engine components, using
    only AUC may produce incomplete/suboptimal solutions. As always, we must deeply
    understand the business logic from the beginning and try to satisfy them.
  prefs: []
  type: TYPE_NORMAL
- en: '[**CHECK MY GITHUB REPO**](https://github.com/cerlymarco/MEDIUM_NoteBook)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep in touch: [Linkedin](https://www.linkedin.com/in/marco-cerliani-b0bba714b/)'
  prefs: []
  type: TYPE_NORMAL
