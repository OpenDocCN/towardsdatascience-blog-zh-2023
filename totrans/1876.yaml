- en: 'Sklearn Tutorial: Module 2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/sklearn-tutorial-module-2-0739c44f595a](https://towardsdatascience.com/sklearn-tutorial-module-2-0739c44f595a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I took the official sklearn MOOC tutorial. Here are my takeaways.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mocquin.medium.com/?source=post_page-----0739c44f595a--------------------------------)[![Yoann
    Mocquin](../Images/b30a0f70c56972aabd2bc0a74baa90bb.png)](https://mocquin.medium.com/?source=post_page-----0739c44f595a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----0739c44f595a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----0739c44f595a--------------------------------)
    [Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----0739c44f595a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----0739c44f595a--------------------------------)
    ·14 min read·Nov 25, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: After years of playing with the Python scientific stack (NumPy, Matplotlib,
    SciPy, Pandas, and Seaborn), it became obvious to me that the next step was scikit-learn,
    or “sklearn”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/12ce198b8d409170b76046f45bd4f12c.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Nick Morrison](https://unsplash.com/@nickmorrison?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: This second module focuses on the concept of models scores, including the test
    score and train score. Those scores are then used to define overfitting and underfitting,
    as well as the concepts of bias and variance.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll also see how to inspect model’s performance with respect to their complexity
    and the number of input samples.
  prefs: []
  type: TYPE_NORMAL
- en: '*All images by author.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you didn’t catch it, I strongly recommend my first post of this series —
    it’ll be way easier to follow along:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/sklearn-tutorial-module-1-f31b3964a3b4?source=post_page-----0739c44f595a--------------------------------)
    [## Sklearn Tutorial: Module 1'
  prefs: []
  type: TYPE_NORMAL
- en: I took the official sklearn MOOC tutorial. Here are my takeaways.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/sklearn-tutorial-module-1-f31b3964a3b4?source=post_page-----0739c44f595a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Score: train score and test score**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first concept I want to talk about are **train score and test score**.
    **The score is a way to numericaly express the performance of a model**. To compute
    such performance, we use a score function, that aggregates the “distance” or “error”
    between what the model predicted versus what the ground truth is. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In sklearn, all models (also called estimators) provide an even quicker way
    to compute a score using the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**The actual score function of the model depends on the model and the kind
    of problem it is designed to solve**. For example a linear regressor is the R²
    coefficient (numerical regression) while a support-verctor classifier (classication)
    will use the accuracy which is basicaly the number of good class-prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the default score of the model doesn’t fit your need, you can also import
    score function from sklearn’s metrics. Numerous score functions can be used to
    compute the score of a model, each with their pros and cons. They are available
    in sklearn.metrics module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'So remember for what follows: from a dataset, we create a train set and a test
    set. After training the model, we can compute a score both ot the train set and
    test set to estimate the performance of the fitted model.'
  prefs: []
  type: TYPE_NORMAL
- en: Given a fixed input dataset, these scores depend on the choice of model, the
    parameters of that model (like the degree for a polynomial fit for example), the
    way we split that dataset (which sample goes into which set) and the choice of
    the score function.
  prefs: []
  type: TYPE_NORMAL
- en: It was important to introduce the test and train scores, because those concepts
    are usefull to inspect the “fitting state” - over or under-fitting- of a model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Relation between over/under-fitting and train/test-score**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Remember from the previous the rationale behind splitting and cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**splitting**: allows to esimate the generalization performances'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cross-validation**: estimate the robustness of the generalization, evens
    out the luck/no-luck in a single split'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also, remember that in the process of cross-validation, different splits are
    used, but the rest of the process is the same: once split, train the model on
    the train set, and we can then compute the scores of that model (train score and
    test score).'
  prefs: []
  type: TYPE_NORMAL
- en: That being said, let’s define what over-fitting and under-fitting mean. As their
    name suggest, they correspond to opposite state of your model, relative to the
    dataset at stake.
  prefs: []
  type: TYPE_NORMAL
- en: '**We say a trained model is over-fitted** if it learned to much the dataset
    it was trained on, and hence lacks generalization abilities. This can be seen
    when the train score is very good (the model make very little error on the data
    it was trained on) but the test score is bad, so it is not good at generazlization.
    This can happen if the model is too complex/flexible (a very high degree polynom
    for example), if the trainset was too small or is very noisy. In this case, small
    changes in the train set imply big changes in the test predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: '**On the other hand, a trained model is under-fitted** if it only focuses on
    very general global trend and not enough on details. This can be seen when the
    train score is not good enough, meaning that the model didn’t have the flexibility
    to learn the complexity of the data. This happens when the model is not flexible
    enough (we also say the model is too “constrained”), which can be a consequence
    of the choice of the model or of its parameters (like setting a 1-degree polynom
    to fit a 10-degree problem).'
  prefs: []
  type: TYPE_NORMAL
- en: Our job is to find the sweet-middle spot, where there’s the best trade-off between
    over and underfitting, by tuning the model — in the very general way, including
    model choice, preprocessing choice, and all associated parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'So to summarize, the relation between train error, test error and model complexity
    (with a fixed input dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Underfit**: at very low complexity, the model will underfit the train set
    (because it does not have the flexibility to bear the actual complexity in the
    data) and lead to errors on both the trainset and testset (train set and test
    set should have more or less the same complexity/noise, since they are drawn from
    the same population/dataset)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sweetspot**: as the model complexity increases away from heavy underfitting,
    the train error and test error will decrease.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfit**: if the complexity increases too much, the train error will decrease
    (since we give the model more flexibility to learn the train set), but the test
    error will increase exponentially (because the model “learned too much” the train
    set, it performs less on the new data of the test set)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see a quick example: we will modify the model complexity by changing
    the degree of a polynomial fit. The truth model is 0.5 * X**2 + X + 2, and we
    try various degrees: 0, 1, 2, 10 and 25\. Since the model has 0-th, 1-st and 2-nd
    order, we now that 0 degree will probably underfit, and 25 degrees will probably
    overfit.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The result looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f08840caf44ad4fb78d835bc73290701.png)'
  prefs: []
  type: TYPE_IMG
- en: From left to right, the complexity is increased using the degree. Both scores
    start low, and increase up to degree 2/10\. After that the test score strongly
    decreases indicating lack of generalization and over-fitting.
  prefs: []
  type: TYPE_NORMAL
- en: '**On the extreme lowest-complexity, 0 degree**, the model underfits. Both the
    train and test scores are pretty low (for a linear regression, the optimum score
    is 1).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Increasing the degree to 1** brings significant improvements of both scores,
    but we can still see visually that the model is too simple to fit the data trend.'
  prefs: []
  type: TYPE_NORMAL
- en: '**With a degree of 2**, the model seems to approach an optimum. The scores
    are again way better, and we get a visual accordance. **Compared to degree 5**
    for which the scores are a bit better, we can see a misfit bump (actually overfit).
    It is likely that for another split (like done in cross-validation), that degree
    5 would be quite different.'
  prefs: []
  type: TYPE_NORMAL
- en: '**With a degree of 25**, we see a clear decrease of the test score and the
    train score keeps improving: this is a clear sign of over-fitting. At this point,
    our model is memorizing the train set and cannot generalize on new data.'
  prefs: []
  type: TYPE_NORMAL
- en: This dependence of train/test scores with model complexity shows how over-fitting
    and under-fitting occurs. We will inspect this further with validation curve below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note another important term: ‘**inductive bias**’: this is the bias introduced
    by the choice/kind of model. It is builtin the model itself, not the hyperparameters
    or number of samples (as opposed to bias introduced by the different hyperparameters
    like the degree of a polynomial regression). Remember that a model’s complexity
    depend both on the kind of model and its parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model performance with number of samples**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While in most cases, we have to work with fixed size input data, another way
    to look at the model performance and the overall ML exercice, is to inspect how
    the scores change with the number of sample data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we have more or less 3 zones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**with few samples**, both train and test errors are important (there is not
    enough data for the model to understand what is happening, whatever its flexibility)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**as the number of samples increases**, the train error will increase (since
    the model complexity is fixed), but the test error will decrease (adding more
    samples allowed the model to learn better)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**if the number of samples increases a lot**, the train and test error will
    almost converge together: the model has reached its potential. The train error
    stopped increasing because the model itself (what it learns) is not changed by
    any new data point, and the test error is limited by the model complexity and
    cannot decrease anymore.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this last case of very high number of samples, we say the model approaches
    the **Bayes error rate**: this is the error of the best model trained on unlimited
    data, when predictions are just limited by noise in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Visualizing scores as function of complexity and number of samples**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, it can be a good practice to visualize those concepts using what are
    called “Validation curve” and “Learning curve”:'
  prefs: []
  type: TYPE_NORMAL
- en: '**validation curve**: plot the test score and train score as a funtion of model
    complexity (like the degree of polynomial fit): `score=f(complexity)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**learning curve**: plot the test score and train score as a function of input
    size (for a given input data matrix, we can use just a portion of the total available
    data): `score=f(#samples)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Both curves can be generated pretty simply with sklearn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5b50e6efba9ed51c4c28ddebb50ced07.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, both the train score and test increase quickly in the [0–2]
    range. For higher degrees, the test score starts decreasing, indicating a loss
    of generalization of. the model. Obviously, the training keeps increasing when
    the model complexity increases.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/cee4cf2b13557f60fa45e9919b41ef16.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, with a fixed complexity of degree=2, the train score and test
    scores converge toward the same value for very high number of samples. This kind
    of curve can help you analyse if the input data you’re working with is enough
    for your model to approach its Bayes error rate.
  prefs: []
  type: TYPE_NORMAL
- en: '**The bias-variance tradeoff**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The concepts of bias, variance and the bias-variance is strongly related to
    the concepts of over-fitting and underfitting. We already covered overfitting
    and underfitting, so I’ll make this quick:'
  prefs: []
  type: TYPE_NORMAL
- en: '**variance** refers to the variation a model’s reponse changes with the train
    set. In other words, a model can exihbits strong variance when the trainset is
    very small and/or the model complexity is very high. Overfitting is often associated
    with high variance because the model is sensitive to the specifics of the training
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**bias** refers to how the fitted model will be “biased” compared to the perfect
    model, and will be pretty much the same whatever the input its fed. This happens
    especially if the model complexity/flexibility is very low compared to what we
    want it to learn. In other words, the model is biased towards its assumptions
    and might not adapt well to the complexities of the data. Underfitting is associated
    with high bias because the model is not flexible enough to adapt to the complexities
    of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **bias-variance tradeoff** is then a key concept in ML: it suggests that
    there is a tradeoff between bias and variance. Increasing model complexity tends
    to decrease bias but increase variance, and vice versa. The goal is to find the
    right balance that minimizes both bias and variance, leading to a model that generalizes
    well to new, unseen data. So our job as data-scientist is to tune the model and
    find this sweet spot.'
  prefs: []
  type: TYPE_NORMAL
- en: The example below shows how a simple polynomial fit of a single variable, so
    y=p(x) can have either high bias or high variance, depending on the degree allowed
    for the model (the code is available further down, below the figure).
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is the following: we create a toy dataset y with a known polynomial
    function p(x)=0.5X³ + X + 2 + noise. So the true coefficients we would like the
    polynomial fit to learn are 2 for the constant coefficient, 1 for the X coefficient,
    and 0.5 fort the X³ coefficient. We test 2 models: a very low degree polynom with
    degree=1, and a high degree polynom with degree=15.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Once the data is generated, we split it 50–50 into a first train/test set. Both
    low and high degree models are fitted on this split, and their learned coefs,
    preditcions and score are computed. Then we just switch the train set and test
    set, so for the second split, we are training on what was the test set and testing
    on what was the train set. This is not common practice but rather a pedagogical
    trick. Again, both low and high degree models are fitted, and again their learned
    coefs, predictions and scores are computed.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The results are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/333de7b5faf21a84790d24aa13fd9b45.png)'
  prefs: []
  type: TYPE_IMG
- en: '**As you can see for the low degree plots (middle column plots)**, both splits
    lead to pretty much the same coefficients solution. The predictions are “biased”
    from the true data, there is this kind of constant “offset” in the learned coefs.'
  prefs: []
  type: TYPE_NORMAL
- en: '**On the other hand for the high degree plots (right column plots)**, both
    splits lead to very different coefficients. There is a high variance in their
    response.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Wrapup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To summarize, remember these important concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Score/Train Score/Test Score:** Scores quantify the performance of a model;
    Train Score reflects its accuracy on the training data, Test Score on unseen data.
    Balancing high train score with a comparable test score is crucial for a model
    that generalizes well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Underfitting/Overfitting:** Underfitting occurs when a model is too simplistic,
    missing data complexities; Overfitting arises when a model overly tailors itself
    to training data, hindering generalization. Finding balance in model complexity
    is essential to avoid both underfitting’s simplicity and overfitting’s memorization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Performance as a Function of Complexity and Number of Samples:** Examining
    how a model’s scores change with complexity (e.g., polynomial degree) or dataset
    size provides insights into its behavior. Assessing performance across varied
    complexities and sample sizes aids in identifying the optimal model characteristics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias and Variance in Model Evaluation:** Bias refers to a model’s tendency
    to deviate from the true data pattern; low flexibility yields high bias. Variance
    captures a model’s sensitivity to dataset changes; high complexity leads to high
    variance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, achieving the bias-variance balance is often the key point in a good
    ML exercise to tune a model to its best possible performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might like some of my other posts, make sure to check them out:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Yoann Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----0739c44f595a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Sklearn tutorial
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://mocquin.medium.com/list/sklearn-tutorial-2e46a0e06b39?source=post_page-----0739c44f595a--------------------------------)9
    stories![](../Images/4ffe6868fb22c241a959bd5d5a9fd5d7.png)![](../Images/8aa32b00faa0ef7376e121ba9c9ffdb7.png)![](../Images/9f986423d7983bc08fc2073534603c35.png)![Yoann
    Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----0739c44f595a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Scientific/numerical python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://mocquin.medium.com/list/scientificnumerical-python-9ce115122ab6?source=post_page-----0739c44f595a--------------------------------)3
    stories![Ironicaly, an array of containers](../Images/4ecd0326a3efdda93947f60872018d41.png)![](../Images/f11076a724463f7b11d819d95bcf0ea4.png)![](../Images/e340b22f444d2bd311537341cf1a105a.png)![Yoann
    Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----0739c44f595a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Data science and Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://mocquin.medium.com/list/data-science-and-machine-learning-ba3fb2206051?source=post_page-----0739c44f595a--------------------------------)3
    stories![](../Images/c078e74fd67e0141c2b54b82823c78d4.png)![](../Images/79988eda04a078da9373f03d7db51c51.png)![](../Images/6a5966e529bf4ba9b16c592fec7b591a.png)![Yoann
    Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----0739c44f595a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Fourier-transforms for time-series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://mocquin.medium.com/list/fouriertransforms-for-timeseries-ed423e3f38ad?source=post_page-----0739c44f595a--------------------------------)4
    stories![](../Images/86efd63d329650eb9b6d7c33625d6884.png)![](../Images/c693e4e596df5c1a8ef1b0fb3777d7ac.png)![](../Images/b6bc5330fb2d92bc3aad36f5bbc950da.png)'
  prefs: []
  type: TYPE_NORMAL
