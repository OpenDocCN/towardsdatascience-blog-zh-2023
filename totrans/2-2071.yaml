- en: The Two Faces of AI Alignment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-two-faces-of-ai-alignment-e58b0c11cc01](https://towardsdatascience.com/the-two-faces-of-ai-alignment-e58b0c11cc01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Misaligned Models and Misaligned Agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@maxhilsdorf?source=post_page-----e58b0c11cc01--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page-----e58b0c11cc01--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e58b0c11cc01--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e58b0c11cc01--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page-----e58b0c11cc01--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e58b0c11cc01--------------------------------)
    ·12 min read·Jul 18, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95518eb41d8df5b41fc07d61b3b3ce53.png)'
  prefs: []
  type: TYPE_IMG
- en: Image adapted from [Tara Winstead](https://www.pexels.com/de-de/foto/hande-verbindung-zukunft-roboter-8386434/).
  prefs: []
  type: TYPE_NORMAL
- en: What is AI Alignment?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Artificial Intelligence (AI) is no longer just a buzzword; it is a rapidly evolving
    field where intelligent systems are becoming increasingly integrated into our
    daily lives. From recommendation algorithms on Netflix to automating creative
    workflows with ChatGPT or Midjourney, AI is transforming various aspects of our
    world. However, this remarkable progress also presents significant challenges,
    with AI alignment being one of the most critical issues to address.
  prefs: []
  type: TYPE_NORMAL
- en: AI alignment is the process of ensuring that AI systems operate in a way that
    aligns with what humans consider desirable behavior. It’s like trying to teach
    a toddler to behave appropriately — just as you’d want the child to understand
    and respect your values, we need to hold AI systems to the same standard. However,
    as it turns out, we’re not always as good at this task as we think — neither for
    toddlers, nor for AIs.
  prefs: []
  type: TYPE_NORMAL
- en: AI Alignment in the Current Discourse
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI models like ChatGPT or Midjourney have been caught generating
    biased, offensive, or harmful content. These systems learn from the data they
    are fed, and if this data contains biases or harmful patterns, the system may
    unwittingly replicate them. The same applies to self-driving cars. While they
    hold the potential to save lives by reducing accidents caused by human error,
    there are some major ethical hurdles to overcome before we put the lives of ourselves
    and our loved ones into an AI’s hands.
  prefs: []
  type: TYPE_NORMAL
- en: AI alignment is even more critical in the discourse about the potential threat
    of AI achieving artificial general intelligence (AGI), i.e., human-like intelligence
    generalizable to any kind of task. Just one year ago, AGI seemed like an unattainable
    sci-fi story to most data scientists, including me. Now, here we are, only a few
    months after the release of ChatGPT, with accomplished AI figures [leaving big
    tech research teams](https://apnews.com/article/ai-godfather-google-geoffery-hinton-fa98c6a6fddab1d7c27560f6fcbad0ad),
    [testifying for AI regulation in front of the U.S. Senate](https://www.youtube.com/watch?v=TO0J2Yw7usM),
    or [speaking up for a 6-month halt](https://www.dw.com/en/tech-experts-call-for-6-month-pause-on-ai-development/a-65174081)
    in the development of new AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: Whatever opinion you may have about these figures or their arguments, it seems
    inevitable to delve deeper into the topic of AI alignment. In this post, I argue
    that concerns about AI alignment need to be addressed from two different angles.
    Misalignment can affect either an AI model itself or an agent developed on the
    basis of an AI model. These two types of misalignment have drastically different
    implications and risks that require different strategies to overcome.
  prefs: []
  type: TYPE_NORMAL
- en: Misaligned Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A misaligned model occurs when an AI system’s understanding of the world is
    not in sync with reality. It’s like a GPS system that has outdated maps — it may
    lead you down a road that no longer exists or miss a new, more efficient route.
    This could happen if the AI is trained on inaccurate, incomplete, or outdated
    data. For example, a medical diagnosis AI trained mostly on data from male patients
    might struggle to accurately diagnose diseases in women.
  prefs: []
  type: TYPE_NORMAL
- en: Misaligned Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Misaligned agents are AI systems that take actions not aligned with human values
    or goals. It’s like a well-trained dog that fetches the newspaper every morning
    but chews it up before you can read it. The dog is doing what it was trained to
    do (fetch the newspaper), but the outcome is not what you want. It’s a misaligned
    agent.
  prefs: []
  type: TYPE_NORMAL
- en: Why the Distinction Matters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s important to distinguish between misaligned models and misaligned agents
    because both require different methods for detecting and addressing misalignment.
    To illustrate this, suppose you ask ChatGPT a question, and it generates an incorrect
    fact, i.e., “hallucinates,” to answer your question. I would argue that at the
    core of this issue lies both a misaligned model and a misaligned agent.
  prefs: []
  type: TYPE_NORMAL
- en: If ChatGPT is a misaligned model, it makes errors due to a lack of information
    or a misinterpretation of observations made about the real world. We know that
    hallucination usually occurs when ChatGPT is asked very specific questions that
    may not have prominent answers in its training data. If ChatGPT hallucinates due
    to a lack of knowledge, it is clearly an issue with the misalignment of the model.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is more to consider. If ChatGPT doesn’t know the answer to a
    question, hallucination is not the only possible response. It would be much better
    for the user if ChatGPT stated that it cannot answer the question due to a lack
    of knowledge. It can be presumed that ChatGPT’s tendency to wildly hallucinate
    can be partially attributed to its training process, where it was trained to provide
    answers perceived as helpful by the end user, but not necessarily truthful. In
    other words, ChatGPT simply doesn’t prioritize being truthful. If that is the
    case, it is a clear case of a misaligned agent.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen that even a single incident of misalignment between AI
    and humans can reveal both kinds of misalignment, let’s discuss what we can or
    cannot do to address these problems.
  prefs: []
  type: TYPE_NORMAL
- en: How We Can Address Misaligned Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Detecting Misaligned Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To detect a misaligned model, we need methods that make the behavior and flaws
    of the model transparent. One common approach for this is to conduct an extensive
    qualitative and quantitative evaluation of the model to detect flaws in its decision-making.
    Through this process, you can assess not only the model’s accuracy but also check
    whether the model is biased toward or less accurate for certain minorities. For
    deeper insights, Explainable AI methods can be employed to enhance the interpretability
    and transparency of the AI system, allowing for a better understanding of its
    inner workings.
  prefs: []
  type: TYPE_NORMAL
- en: Data Quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we detect hints of misalignment in the model, we can attempt to address
    this by crafting a more diverse and accurate training dataset. For instance, if
    a recruiting AI discriminates against women, we can adjust the training data to
    ensure that women are no longer underrepresented. However, tackling the misalignment
    issue when ChatGPT generates “hallucinations” or made-up facts is much more challenging.
    While we could address specific untruthful statements by fine-tuning ChatGPT with
    accurate texts about the relevant topic in the training data, this would only
    solve the misalignment for that particular topic. It would not solve the problem
    of hallucination as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: Transparency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Addressing misaligned models on a larger scale involves rethinking the current
    machine learning paradigm, which often relies on teaching large black box models
    to identify correlations between features in the training data. Black box models
    pose challenges because assessing the alignment of a model based solely on its
    outputs is difficult. If ChatGPT were capable of truthfully and reliably communicating
    the facts it used and how it combined them to reach its conclusions, it would
    be much easier to verify its alignment with truth and human values.
  prefs: []
  type: TYPE_NORMAL
- en: Causality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another issue is that most state-of-the-art AI models lack the ability to think
    causally. Their functioning is based on learning correlations between features
    in the training data, which is fundamentally different from how humans think.
    For example, when it’s hot outside and we get sunburnt, we understand that the
    sun is the cause of the burn. An AI typically doesn’t distinguish between whether
    the sun causes the burn or the burn causes the sun. Surprisingly, AI systems can
    reliably solve complex problems without this kind of causal awareness. However,
    I would argue that a model lacking causal thinking can never fully comprehend
    the underlying mechanisms of the world, thus remaining inevitably misaligned.
  prefs: []
  type: TYPE_NORMAL
- en: How to Address Misaligned Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Detecting Misaligned Agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The difficulty of detecting misaligned agents can vary significantly. An illustrative
    example is when OpenAI researchers trained an AI to play the boat racing game
    “Coast Runner,” with the goal of maximizing the number of points collected. After
    a few days, they were astonished to find that the boat had completely ignored
    the race and instead circled around three “turbo” power-ups. The reason was clear:
    the power-ups appeared to yield more points than finishing the race.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c57b5a9ff3856e1f02196ae0233cc7e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Misaligned agent in “Coast Runner”. Screenshot from [this video](https://www.youtube.com/watch?v=tlOIHko8ySg).
  prefs: []
  type: TYPE_NORMAL
- en: However, not all cases of misaligned agents are as obvious. The challenge lies
    in the fact that, as long as we primarily deal with black-box models, we cannot
    fully understand or trust their intentions and values. Often, our understanding
    is derived from the training data, the reward/loss function the model optimizes,
    and qualitative evaluation post-training. Nonetheless, as AI systems grow increasingly
    powerful, even minor misalignments can lead to significant real-world harm. The
    truth is that we are still far from reliably detecting or preventing misalignment
    in AI agents.
  prefs: []
  type: TYPE_NORMAL
- en: Why this Problem is so Hard to Tackle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Addressing misaligned agents is a complex task that requires a nuanced understanding
    of values and consequences. There are two main reasons why this challenge is particularly
    difficult:'
  prefs: []
  type: TYPE_NORMAL
- en: Specifying human values in a manner that an AI system can comprehend and incorporate
    is a challenging endeavor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Humans exhibit a wide range of diverse values, and there is no universal set
    of values that every individual subscribes to.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Consequently, not only do we lack a clear consensus on how to teach an AI system
    to adhere to a specific set of values, but we also struggle to agree on which
    values should be imparted. As a result, AI systems need to be designed in a way
    that allows for the adoption of different value sets, ensuring alignment with
    the diverse range of users worldwide.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement-Learning with Human Feedback (RLHF)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One explanation for the misalignment of an agent is exemplified by the well-known
    paperclip maximizer problem. Consider training an AI with the sole objective of
    maximizing paperclip production. As it becomes more competent at this task, it
    may view humans as obstacles to its objective. Humans could potentially turn it
    off, resulting in fewer paperclips. More importantly, humans are made of atoms,
    which can be used to produce even more paperclips.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement-Learning with Human Feedback (RLHF) aims to address this problem
    by linking the AI’s objective to its friendliness toward humans. Let’s take ChatGPT
    as an example, which was also trained using RLHF. OpenAI had thousands of humans
    rank ChatGPT’s outputs based on their perceived “helpfulness.” They then trained
    an AI model, ironically, to take on the role of the human raters by learning from
    their responses. ChatGPT was subsequently trained to produce outputs that the
    (non-human) ranking AI would deem helpful.
  prefs: []
  type: TYPE_NORMAL
- en: 'RLHF is not without its flaws. Even when executed perfectly, it still faces
    the challenge that there is no universally agreed-upon value system among humans.
    One potential solution is to utilize RLHF to train AI models that are specifically
    tailored to a particular target group, such as a specific country or culture.
    However, there is another method for customizing an AI’s value system to specific
    needs: the system message.'
  prefs: []
  type: TYPE_NORMAL
- en: The System Message
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Did you know that ChatGPT doesn’t show you everything that is in its prompt?
    If you have ever used the ChatGPT web app through the OpenAI API, you have probably
    already experimented with the system message. The system message is a part of
    the prompt that is positioned at the beginning of the conversation history and
    typically not disclosed to the user. The default system message on the OpenAI
    website is “You are a helpful assistant.” It’s interesting to observe the changes
    when I inform ChatGPT that it is Donald Duck and deeply in love with his girlfriend,
    Daisy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61dc8b1087d957cbeb11bf6d09f2f3c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Example how how the system message can be used to control the AI’s outputs.
  prefs: []
  type: TYPE_NORMAL
- en: While the system message is great for generating amusing conversations, it also
    serves as a powerful alignment tool. For instance, when I asked ChatGPT about
    its preference between beer and wine, using the system message “You are a German”
    resulted in a clear preference for beer, while the message “You are a Frenchman”
    led to a favoring of wine. As a German, I cannot deny that the former ChatGPT
    variant is more aligned with my personal beliefs. Beyond this trivial example,
    these two distinct ChatGPT “personalities” would likely respond differently to
    important political or ethical questions as well.
  prefs: []
  type: TYPE_NORMAL
- en: In Lex Fridman’s [interview](https://www.youtube.com/watch?v=L_Guz73e6fw) with
    Sam Altman, the CEO of OpenAI, Altman mentioned that GPT-4 was trained with a
    greater focus on the impact of the system message compared to GPT-3.5\. Additionally,
    he believes that leveraging the system message to tailor AI responses is a more
    effective solution for addressing alignment issues than training separate large
    models for different cultures or value systems. However, it is not surprising
    to hear the CEO of OpenAI recommend utilizing their models instead of building
    your own.
  prefs: []
  type: TYPE_NORMAL
- en: Uncertainty-Awareness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you were to ask me to provide you with 10 fun facts about Michael Jackson,
    I would probably tell you that I don’t feel confident giving you this answer due
    to my limited knowledge on the topic. However, ChatGPT, in contrast, can respond
    to a question it doesn’t know the answer to with the same level of confidence
    and persuasiveness as it would to a simple question. In my view, this fundamental
    characteristic makes ChatGPT a misaligned agent.
  prefs: []
  type: TYPE_NORMAL
- en: To understand this, let’s review how ChatGPT was trained. During the RLHF stage,
    the model is encouraged to generate responses that sound helpful to the (non-human)
    ranking system. However, this system does not consider the accuracy of the answers,
    but only their perceived helpfulness. Consequently, ChatGPT may respond with unwarranted
    confidence in order to appear more helpful, even when its knowledge or facts are
    incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: To qualify as a misaligned agent, a model must pursue objectives that are not
    aligned with human goals or values. It is evident that ChatGPT prioritizes appearing
    confident over being truthful. This represents an obvious and concerning case
    of misalignment. Building complex AI systems that are uncertainty-aware is not
    an easy task, but if we aim to collaborate effectively with AI systems, it is
    crucial to teach them to recognize and communicate their uncertainty about the
    accuracy of their responses.
  prefs: []
  type: TYPE_NORMAL
- en: Future Perspectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Are we truly ready for the AI transformation? I think that the answer to this
    question is strongly connected to our ability to build aligned AI systems. As
    we let AI deeper and deeper into our lives, jobs, and economies, our claim should
    be to make it safer along the way. However, it is not clear to me that this has
    been happening in the last 10 years. Of course, people are shocked when they find
    out that a recruitment system discriminates against women or that ChatGPT hallucinates
    false statements and conveys them with unmatched confidence. What I am missing,
    however, is a systematic debate about calling this by its name: the alignment
    problem.'
  prefs: []
  type: TYPE_NORMAL
- en: The alignment problem involves two different kinds of alignment. While misaligned
    models present themselves through bias, false claims, and wrong predictions, misaligned
    agents are caught when their actions hint towards a conflict between their configuration
    and our values as humans. However, as we can learn from the most popular AI of
    all time, ChatGPT, both kinds of alignment are not mutually exclusive but can
    occur at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: '[Eliezer Yudkowsky](https://www.youtube.com/watch?v=AaTRHFaaPG8), a well-known
    AI skeptic, argues that solving the alignment problem is the only way humans can
    avert their annihilation through their electronic “pets.” He thinks that we are
    in deep trouble because we might only get one shot at solving the alignment problem.
    Once a superhuman intelligence was developed, we would no longer be able to control
    or align it. However, Yudkowsky’s opinions are controversial, to say the least.
    Neither do we know whether superhuman intelligence can even be developed, nor
    is it obvious that any small degree of misalignment would automatically have catastrophic
    consequences.'
  prefs: []
  type: TYPE_NORMAL
- en: While I believe that demanding a temporary halt in the development of powerful
    AI systems is not feasible, I understand where this idea is coming from. We have
    seen a major increase in the capabilities of modern AI systems within the last
    10 years that AI alignment research has not been able to catch up to. However,
    solving the alignment problem is, in my estimation, a much safer bet on a positive
    return than building larger and larger black-box AI models. I’m not saying that
    we shouldn’t develop GPT-5, -6, and -7\. I’m saying we should actively engage
    in research and discourse around AI alignment to prepare ourselves for the future,
    regardless of how it may unfold.
  prefs: []
  type: TYPE_NORMAL
- en: '**I hope found this post enjoyable and helpful!**'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you did, you might also like some of my other work about music AI or just
    AI in general:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chatbots are About to Disrupt Music Search](/chatbots-are-about-to-disrupt-music-search-1e4a4cd7ba01)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3 Things Decision Makers Should Know about AI Fairness](https://medium.datadriveninvestor.com/3-things-decision-makers-should-know-about-ai-fairness-f925fda0af0b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How Meta’s AI Generates Music Based on a Reference Melody](/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
