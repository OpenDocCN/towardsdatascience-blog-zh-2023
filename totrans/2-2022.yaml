- en: The Hidden Linearity in Polynomial Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-hidden-linearity-in-polynomial-regression-333f1bed7aa5](https://towardsdatascience.com/the-hidden-linearity-in-polynomial-regression-333f1bed7aa5)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From two perspectives to gain a better understanding of the linear models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@angela.shi?source=post_page-----333f1bed7aa5--------------------------------)[![Angela
    and Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----333f1bed7aa5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----333f1bed7aa5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----333f1bed7aa5--------------------------------)
    [Angela and Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----333f1bed7aa5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----333f1bed7aa5--------------------------------)
    ·9 min read·Mar 17, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we will discuss this viewpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial regression is a linear regression.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/7a33640069803e36d823adefaa7d337a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Denny Müller](https://unsplash.com/de/@redaquamedia?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, this statement may seem absurd. Polynomial regression is known
    as non-linear regression, whereas linear regression is, well, linear, so how can
    these two models be considered the same?
  prefs: []
  type: TYPE_NORMAL
- en: 'Like many things in life, you can see the same thing in different ways, and
    two persons can come to apparently different conclusions. However, what we tend
    to neglect is that the answer is not always the most important part, but it is
    the way, the methodology, or the framework that you use to achieve the conclusion.
    I recently published [an article about the debate of whether logistic regression
    is a regressor or a classifier](https://medium.com/towards-data-science/is-logistic-regression-a-regressor-or-a-classifier-lets-end-the-debate-a01b024f7f65).
    And I mention the two perspectives: statistics vs. machine learning. In this article,
    we will also explore these two different perspectives on the understanding of
    polynomial regression.'
  prefs: []
  type: TYPE_NORMAL
- en: Oh yes, some may say that polynomial regression is just some theoretical model,
    not easy to put into practice since raising one feature to high powers is not
    something meaningful. With this article, you will discover that the principle
    is more used than you think, and the degree of the polynomial should not be the
    only hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Definitions of linear regression and polynomial regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s first have a look at the definitions of linear regression and polynomial
    regression. Now, I know that you probably already know the definitions very well,
    but when you read through these definitions, try to find something unusual about
    the inputs for both models.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Defining Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear regression typically refers to Ordinary Least Squares (OLS) regression,
    and it involves minimizing the sum of the squared differences between the predicted
    values of the output and its actual values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation of linear regression can be expressed as y = w X + b. Since X
    is a matrix that represents multiple variables, we can develop the function in
    this way:'
  prefs: []
  type: TYPE_NORMAL
- en: y = w1x1 + w2x2 + … + wpxp + b
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: y is the dependent variable or output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: x1, x2, …, xp are the independent variables or input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: b is the intercept
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: w1, w2, …, wp are the coefficients for the dependent variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1.2 Defining Polynomial Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Polynomial regression models the relationship between the independent variable
    x and dependent variable y as an nth-degree polynomial. The result is a curve
    that best fits the data points, rather than a straight line.
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation for polynomial regression can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: y = b0 + b1x + b2x² + … + bn*x^n
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: again, y is the dependent variable,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: x is the independent variable,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: b0, b1, b2, …, bn are the coefficients, and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: n is the degree of the polynomial.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, polynomial regression allows for a nonlinear relationship between x and
    y to be modeled.
  prefs: []
  type: TYPE_NORMAL
- en: '1.3 One Important Aspect: the Number of Features'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, do you notice a difference in the previous definitions? I searched for
    these definitions on the internet with various sources such as Wikipedia and university
    courses, and this difference is always present.
  prefs: []
  type: TYPE_NORMAL
- en: linear regression is usually defined using multiple variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: polynomial regression typically has only one feature variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In my opinion, the main reason is that polynomial regression is usually seen
    through the statisticians’ perspective. That is why the fitting algorithm for
    polynomial regression, polyfit, from the package numpy only allows one feature.
    And what about scikit learn, we will see later.
  prefs: []
  type: TYPE_NORMAL
- en: Anyway, in today’s “machine learning” applications, taking only one feature
    for the model is just not enough for real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is of course possible to input multiple features for polynomial
    regression. It is then named **multivariate polynomial regression**, the features
    will each be raised to different powers and with interactions between different
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 'If there are p features, the generalized equation for polynomial regression
    of degree n becomes (I will write the equation in Latex so that you can better
    read it):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29bbccf597227e217a772bfe20833317.png)'
  prefs: []
  type: TYPE_IMG
- en: You don’t see this often, maybe because it is complex, and not practical to
    use. However, you can see that this idea is very important since it is key to
    modeling more complex relationships between the variables. We will see later in
    the article how we can translate this idea in a more elegant way.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Demystifying and Visualizing the Linearity of Polynomial Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To simplify the explanation, we will use polynomial regression with only one
    feature. However, you can easily imagine that the analysis can be generalized
    to polynomial regression with multiple features.
  prefs: []
  type: TYPE_NORMAL
- en: So in this section, we will refer to the feature variable as x.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 What the Linearity is Referred to
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Is polynomial regression a linear model or not? The answer lies in the question
    of which features we consider this linearity is referred to.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial regression is non-linear regarding the variable x.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But polynomial regression is linear if we consider the variables x, x^2, x^3,
    etc. as feature variables. To simplify the abstraction, we can use the notion
    x, x_2, x_3, etc. for these features. And then we can fit a linear regression
    to these features. So in the end, it is a multivariate linear regression!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to clarify this consideration, we can consider polynomial regression
    with two separate steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature engineering: in this step, we create the polynomial features independently
    of the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linear regression: the proper model of linear regression will take the features
    above and find the coefficients for each feature'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I find this way of breaking complex models down into small pieces very helpful
    to understand them. That is why in scikit learn, we don’t have an estimator called
    PolynomialRegression, but it is done in two steps
  prefs: []
  type: TYPE_NORMAL
- en: PolynomialFeatures in the module preprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LinearRegression in the module linear_model … oh wait, do we have to use this
    estimator only? You will discover more in the following sections.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, in short, there are two perspectives:'
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial regression is ONE model, and it takes x as input. Then it is a non-linear
    model to x
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polynomial regression is NOT a standalone model, but it is built by transforming
    x into polynomial features first, and then a linear regression is applied.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latter will unlock some very interesting potentials as we will see later
    in this article. But first, let’s create a plot to better convince ourselves of
    this linearity. Because seeing is believing.
  prefs: []
  type: TYPE_NORMAL
- en: '2.2 Visualizing Polynomial Regression: From Curves to Planes'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since polynomial regression is a linear regression, so we first have to know
    to visualize linear regressions. I wrote [this article to visualize linear regression](/linear-regression-visualized-and-better-understood-c8f7b9c69810)
    with different numbers and types of variables. Considering the following examples
    of visualization, how would you use one of them, to be adapted for the case of
    polynomial regression?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/20698b15e54fdab9c0ce87e530a62dc0.png)'
  prefs: []
  type: TYPE_IMG
- en: To visualize polynomial regression, we will only add a quadratic term to make
    a linear regression with two continuous variables (x, x²), so we will see a 3D
    plot. We will not able to visualize more variables but you can imagine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider this simple example of a set of data points (x, y) that follows
    the equation y = 2 + x — 0.5 * x², we usually will create the following plot to
    represent x and y:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6369a1bd08052171cf5d61d9f1daec3.png)'
  prefs: []
  type: TYPE_IMG
- en: Polynomial regression x, y plot — image by author
  prefs: []
  type: TYPE_NORMAL
- en: The red line represents the model, and the blue dots the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s imagine that we first plot some values of x and x²:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37805247c58350a0274c8346e1284dd6.png)'
  prefs: []
  type: TYPE_IMG
- en: Polynomial regression plot x and x^2— image by author
  prefs: []
  type: TYPE_NORMAL
- en: Then we create a 3D plot by adding the values of y for each (x, x²). The linear
    regression model using x and x² as inputs is then a plane that also can be represented
    in the 3D plot as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61c8d572eda196b560061b4d63781995.png)'
  prefs: []
  type: TYPE_IMG
- en: Polynomial regression is linear regression 3D plot — image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'And we can make a gif out the several images by changing the angle of view.
    If you want to get the code for the creation of this gif, among other useful codes,
    please support me on Ko-fi with the following link: [https://ko-fi.com/s/4cc6555852](https://ko-fi.com/s/4cc6555852).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1463b2496ed7c9cc75c659e6211b1ba4.png)'
  prefs: []
  type: TYPE_IMG
- en: Polynomial regression is linear regression 3d plot — image by author
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Machine Learning Perspective for polynomial regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 3.1 Practical application of polynomial regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fellow data scientists, be honest, do you really build polynomial regression
    for real-world applications? When do you really encounter polynomial regression?
    Try to remember… yes, that’s right, when the teacher tries to explain overfitting
    for regression problems. Here is an example below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0bf51defcce25bb280ac2a886c02f96e.png)'
  prefs: []
  type: TYPE_IMG
- en: Polynomial regression overfitting — image by author
  prefs: []
  type: TYPE_NORMAL
- en: From a statistician’s viewpoint, with numpy, we already said only one feature
    is allowed. So it is not realistic to use this polyfit to create real-world models.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Machine learning perspective with scaling and regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From a machine learning perspective, creating a polynomial regression is more
    than raising one feature to various powers.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, for the overall creation of polynomial regression, we already mentioned
    that it is done in two steps: PolynomialFeatures with preprocessing and LinearRegression
    with the proper model part.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we also have to mention some specific technics like scaling. In fact, when
    the features are raised to very high degrees, then the numbers become so big that
    scikit learn can’t handle them anymore. So, it is better to perform scaling, not
    for the theoretical part, but for the practical aspect.
  prefs: []
  type: TYPE_NORMAL
- en: 'I wrote an article to talk about this point: [Polynomial Regression with Scikit
    learn: What You Should Know](/polynomial-regression-with-scikit-learn-what-you-should-know-bed9d3296f2).'
  prefs: []
  type: TYPE_NORMAL
- en: One crucial aspect of polynomial regression is the degree of the polynomial,
    usually considered as the hyperparameter of the model. However, we should not
    forget that the degree is not the only possible hyperparameter. In fact, it is
    easier to think when you do polynomial regression in two steps
  prefs: []
  type: TYPE_NORMAL
- en: 'Polynomial features: we can customize the features, and even manually add some
    interactions between certain variables. Then we can scale the features or use
    other technics such as QuantileTransformer or KBinsDiscretizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Models: then for the model part, we also can choose a model such as Ridge,
    Lasso, or even SVR, instead of Linear regression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here is an example to illustrate the impact of the hyperparameter alpha on the
    model with ridge regression.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fcae649d77160d04e356712508bd0cca.png)'
  prefs: []
  type: TYPE_IMG
- en: Polynomial regression overfitting with alpha ridge — image by author
  prefs: []
  type: TYPE_NORMAL
- en: Now, you may be happy that you just learned how to create some new and more
    effective models. But they are still not practical to use. Maybe some of you can
    see that it is what we already do, but with a different approach… yes, kernels!
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Poly kernels and more
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The usual form of polynomial regression is not practical to implement, but in
    theory, this is one effective way to create a non-linear model based on mathematical
    functions. The other one is creating neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: One reason is that when creating polynomial features, the number of features
    can be huge. And fitting a linear regression can be time-consuming. That is why
    SVR or SVM comes into play since the hinge loss function allows to “drop” many
    data points by keeping only the said “support vectors”.
  prefs: []
  type: TYPE_NORMAL
- en: Yes, the idea of kernel function is often associated with SVM or SVR, but we
    should not forget that it is a separate theory and that is why we also have [KernelRidge](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html)
    in scikit learn. And in theory, we also could have KernelLasso or KernelElasticNet.
    KernelLogisticRegression is another example for classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, we have explored the model of polynomial regression and its relationship
    with linear regression. While it is usually considered ONE model for one feature
    from a statistical perspective, it can be viewed as a form of linear regression
    preceded by a feature engineering part which consists namely of creating polynomial
    features but they should be scaled. Moreover, we can also apply other models such
    as ridge, lasso or SVM to these polynomial features which will improve their performance.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, polynomial regression serves also as a prime example for the feature
    mapping technics in the case of mathematical function-based models. And this leads
    to kernel functions that allow the modeling of non-linear relationships between
    the features and the target variable.
  prefs: []
  type: TYPE_NORMAL
