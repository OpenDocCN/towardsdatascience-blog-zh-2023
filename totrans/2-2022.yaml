- en: The Hidden Linearity in Polynomial Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多项式回归中的隐藏线性
- en: 原文：[https://towardsdatascience.com/the-hidden-linearity-in-polynomial-regression-333f1bed7aa5](https://towardsdatascience.com/the-hidden-linearity-in-polynomial-regression-333f1bed7aa5)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-hidden-linearity-in-polynomial-regression-333f1bed7aa5](https://towardsdatascience.com/the-hidden-linearity-in-polynomial-regression-333f1bed7aa5)
- en: From two perspectives to gain a better understanding of the linear models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从两个角度更好地理解线性模型
- en: '[](https://medium.com/@angela.shi?source=post_page-----333f1bed7aa5--------------------------------)[![Angela
    and Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----333f1bed7aa5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----333f1bed7aa5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----333f1bed7aa5--------------------------------)
    [Angela and Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----333f1bed7aa5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@angela.shi?source=post_page-----333f1bed7aa5--------------------------------)[![Angela
    and Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----333f1bed7aa5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----333f1bed7aa5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----333f1bed7aa5--------------------------------)
    [Angela and Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----333f1bed7aa5--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----333f1bed7aa5--------------------------------)
    ·9 min read·Mar 17, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[数据科学前沿](https://towardsdatascience.com/?source=post_page-----333f1bed7aa5--------------------------------)
    ·阅读时间9分钟·2023年3月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'In this article, we will discuss this viewpoint:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将讨论这种观点：
- en: Polynomial regression is a linear regression.
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 多项式回归是线性回归的一种。
- en: '![](../Images/7a33640069803e36d823adefaa7d337a.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a33640069803e36d823adefaa7d337a.png)'
- en: Photo by [Denny Müller](https://unsplash.com/de/@redaquamedia?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自[丹尼·穆勒](https://unsplash.com/de/@redaquamedia?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: At first glance, this statement may seem absurd. Polynomial regression is known
    as non-linear regression, whereas linear regression is, well, linear, so how can
    these two models be considered the same?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 初看之下，这个说法可能显得荒谬。多项式回归被称为非线性回归，而线性回归显然是线性的，这两个模型怎么可能被认为是相同的呢？
- en: 'Like many things in life, you can see the same thing in different ways, and
    two persons can come to apparently different conclusions. However, what we tend
    to neglect is that the answer is not always the most important part, but it is
    the way, the methodology, or the framework that you use to achieve the conclusion.
    I recently published [an article about the debate of whether logistic regression
    is a regressor or a classifier](https://medium.com/towards-data-science/is-logistic-regression-a-regressor-or-a-classifier-lets-end-the-debate-a01b024f7f65).
    And I mention the two perspectives: statistics vs. machine learning. In this article,
    we will also explore these two different perspectives on the understanding of
    polynomial regression.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 就像生活中的许多事物一样，你可以用不同的方式看待相同的事物，两个人可能会得出看似不同的结论。然而，我们往往忽略了答案并不是最重要的部分，而是你用来得出结论的方法、方法论或框架。我最近发表了[一篇关于逻辑回归是回归模型还是分类模型的争论的文章](https://medium.com/towards-data-science/is-logistic-regression-a-regressor-or-a-classifier-lets-end-the-debate-a01b024f7f65)。我提到了两个观点：统计学与机器学习。在这篇文章中，我们还将探讨这两种不同的多项式回归理解视角。
- en: Oh yes, some may say that polynomial regression is just some theoretical model,
    not easy to put into practice since raising one feature to high powers is not
    something meaningful. With this article, you will discover that the principle
    is more used than you think, and the degree of the polynomial should not be the
    only hyperparameter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，有些人可能会说多项式回归只是一个理论模型，不容易付诸实践，因为将一个特征提升到高次幂并没有实际意义。通过这篇文章，你会发现这个原则的使用比你想象的要广泛，而且多项式的次数不应是唯一的超参数。
- en: 1\. Definitions of linear regression and polynomial regression
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. 线性回归和多项式回归的定义
- en: Let’s first have a look at the definitions of linear regression and polynomial
    regression. Now, I know that you probably already know the definitions very well,
    but when you read through these definitions, try to find something unusual about
    the inputs for both models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来看一下线性回归和多项式回归的定义。现在，我知道你可能已经非常了解这些定义，但当你阅读这些定义时，尝试找出两个模型的输入中有什么不寻常的地方。
- en: 1.1 Defining Linear Regression
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 定义线性回归
- en: Linear regression typically refers to Ordinary Least Squares (OLS) regression,
    and it involves minimizing the sum of the squared differences between the predicted
    values of the output and its actual values.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归通常指的是普通最小二乘法（OLS）回归，它涉及最小化输出的预测值与实际值之间的平方差之和。
- en: 'The equation of linear regression can be expressed as y = w X + b. Since X
    is a matrix that represents multiple variables, we can develop the function in
    this way:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的方程可以表示为y = w X + b。由于X是表示多个变量的矩阵，我们可以以这种方式展开函数：
- en: y = w1x1 + w2x2 + … + wpxp + b
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: y = w1x1 + w2x2 + … + wpxp + b
- en: 'where:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: y is the dependent variable or output
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: y是因变量或输出
- en: x1, x2, …, xp are the independent variables or input
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: x1, x2, …, xp 是自变量或输入
- en: b is the intercept
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: b是截距
- en: w1, w2, …, wp are the coefficients for the dependent variables
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: w1, w2, …, wp 是因变量的系数
- en: 1.2 Defining Polynomial Regression
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 定义多项式回归
- en: Polynomial regression models the relationship between the independent variable
    x and dependent variable y as an nth-degree polynomial. The result is a curve
    that best fits the data points, rather than a straight line.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式回归将自变量x和因变量y之间的关系建模为n次多项式。结果是一个最适合数据点的曲线，而不是一条直线。
- en: 'The equation for polynomial regression can be written as:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式回归的方程可以写作：
- en: y = b0 + b1x + b2x² + … + bn*x^n
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: y = b0 + b1x + b2x² + … + bn*x^n
- en: 'where:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: again, y is the dependent variable,
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 再次强调，y是因变量，
- en: x is the independent variable,
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: x是自变量，
- en: b0, b1, b2, …, bn are the coefficients, and
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: b0, b1, b2, …, bn 是系数，以及
- en: n is the degree of the polynomial.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: n是多项式的次数。
- en: So, polynomial regression allows for a nonlinear relationship between x and
    y to be modeled.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，多项式回归允许x和y之间建模非线性关系。
- en: '1.3 One Important Aspect: the Number of Features'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 一个重要的方面：特征的数量
- en: Now, do you notice a difference in the previous definitions? I searched for
    these definitions on the internet with various sources such as Wikipedia and university
    courses, and this difference is always present.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你注意到之前的定义中有什么不同吗？我在互联网上搜索了这些定义，包括维基百科和大学课程，这种差异总是存在的。
- en: linear regression is usually defined using multiple variables
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归通常使用多个变量进行定义
- en: polynomial regression typically has only one feature variable.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式回归通常只有一个特征变量。
- en: In my opinion, the main reason is that polynomial regression is usually seen
    through the statisticians’ perspective. That is why the fitting algorithm for
    polynomial regression, polyfit, from the package numpy only allows one feature.
    And what about scikit learn, we will see later.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，主要原因是多项式回归通常从统计学家的角度来看待。这就是为什么numpy包中的多项式回归拟合算法polyfit只允许一个特征。而scikit learn如何，我们稍后会看到。
- en: Anyway, in today’s “machine learning” applications, taking only one feature
    for the model is just not enough for real-world applications.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，在今天的“机器学习”应用中，只使用一个特征来进行模型构建对实际应用而言是远远不够的。
- en: However, it is of course possible to input multiple features for polynomial
    regression. It is then named **multivariate polynomial regression**, the features
    will each be raised to different powers and with interactions between different
    features.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，可以为多项式回归输入多个特征。这时称为**多变量多项式回归**，特征将分别提高到不同的幂，并且不同特征之间会有交互。
- en: 'If there are p features, the generalized equation for polynomial regression
    of degree n becomes (I will write the equation in Latex so that you can better
    read it):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有p个特征，多项式回归的广义方程为n次方程（我将用Latex写出这个方程，以便你更好地阅读）：
- en: '![](../Images/29bbccf597227e217a772bfe20833317.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29bbccf597227e217a772bfe20833317.png)'
- en: You don’t see this often, maybe because it is complex, and not practical to
    use. However, you can see that this idea is very important since it is key to
    modeling more complex relationships between the variables. We will see later in
    the article how we can translate this idea in a more elegant way.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法并不常见，也许是因为它复杂且不易使用。然而，你可以看到这个想法非常重要，因为它是建模变量之间更复杂关系的关键。我们将在文章后面看到如何以更优雅的方式转化这个想法。
- en: 2\. Demystifying and Visualizing the Linearity of Polynomial Regression
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 揭示和可视化多项式回归的线性
- en: To simplify the explanation, we will use polynomial regression with only one
    feature. However, you can easily imagine that the analysis can be generalized
    to polynomial regression with multiple features.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化解释，我们将使用只有一个特征的多项式回归。然而，你可以很容易地想象这个分析可以推广到多个特征的多项式回归。
- en: So in this section, we will refer to the feature variable as x.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本节中，我们将特征变量称为 x。
- en: 2.1 What the Linearity is Referred to
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 线性性所指的内容
- en: Is polynomial regression a linear model or not? The answer lies in the question
    of which features we consider this linearity is referred to.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式回归是否是线性模型？答案在于我们认为这种线性是指哪些特征。
- en: Polynomial regression is non-linear regarding the variable x.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式回归在变量 x 上是非线性的。
- en: But polynomial regression is linear if we consider the variables x, x^2, x^3,
    etc. as feature variables. To simplify the abstraction, we can use the notion
    x, x_2, x_3, etc. for these features. And then we can fit a linear regression
    to these features. So in the end, it is a multivariate linear regression!
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 但是如果我们将变量 x、x^2、x^3 等视为特征变量，则多项式回归是线性的。为了简化抽象，我们可以使用 x、x_2、x_3 等表示这些特征。然后，我们可以对这些特征进行线性回归。因此，最终它是一个多变量线性回归！
- en: 'In order to clarify this consideration, we can consider polynomial regression
    with two separate steps:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了澄清这一点，我们可以将多项式回归视为两个独立的步骤：
- en: 'Feature engineering: in this step, we create the polynomial features independently
    of the model'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程：在这一步中，我们独立于模型创建多项式特征。
- en: 'Linear regression: the proper model of linear regression will take the features
    above and find the coefficients for each feature'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归：线性回归的适当模型将接受上述特征并为每个特征找到系数。
- en: I find this way of breaking complex models down into small pieces very helpful
    to understand them. That is why in scikit learn, we don’t have an estimator called
    PolynomialRegression, but it is done in two steps
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现将复杂模型拆解成小块以便理解非常有帮助。这就是为什么在 scikit learn 中，我们没有一个叫做 PolynomialRegression
    的估计器，而是通过两步来完成。
- en: PolynomialFeatures in the module preprocessing
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PolynomialFeatures 在模块 preprocessing 中
- en: LinearRegression in the module linear_model … oh wait, do we have to use this
    estimator only? You will discover more in the following sections.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LinearRegression 在模块 linear_model 中……哦等等，我们只需使用这个估计器吗？你将在接下来的部分中发现更多信息。
- en: 'So, in short, there are two perspectives:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，有两个视角：
- en: Polynomial regression is ONE model, and it takes x as input. Then it is a non-linear
    model to x
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式回归是一个模型，它以 x 为输入。然后它是对 x 的非线性模型。
- en: Polynomial regression is NOT a standalone model, but it is built by transforming
    x into polynomial features first, and then a linear regression is applied.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式回归不是一个独立的模型，而是通过将 x 转换为多项式特征，然后应用线性回归来构建的。
- en: The latter will unlock some very interesting potentials as we will see later
    in this article. But first, let’s create a plot to better convince ourselves of
    this linearity. Because seeing is believing.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 后者将解锁一些非常有趣的潜力，我们将在本文章中看到。但首先，让我们创建一个图来更好地说服自己这种线性。因为眼见为实。
- en: '2.2 Visualizing Polynomial Regression: From Curves to Planes'
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 可视化多项式回归：从曲线到平面
- en: Since polynomial regression is a linear regression, so we first have to know
    to visualize linear regressions. I wrote [this article to visualize linear regression](/linear-regression-visualized-and-better-understood-c8f7b9c69810)
    with different numbers and types of variables. Considering the following examples
    of visualization, how would you use one of them, to be adapted for the case of
    polynomial regression?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于多项式回归是一种线性回归，因此我们首先需要了解如何可视化线性回归。我写了[这篇文章来可视化线性回归](/linear-regression-visualized-and-better-understood-c8f7b9c69810)，其中包含了不同数量和类型的变量。考虑以下可视化示例，你会如何使用其中之一来适应多项式回归的情况？
- en: '![](../Images/20698b15e54fdab9c0ce87e530a62dc0.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20698b15e54fdab9c0ce87e530a62dc0.png)'
- en: To visualize polynomial regression, we will only add a quadratic term to make
    a linear regression with two continuous variables (x, x²), so we will see a 3D
    plot. We will not able to visualize more variables but you can imagine.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化多项式回归，我们只会添加一个二次项，以使线性回归具有两个连续变量(x, x²)，因此我们将看到一个3D图。我们无法可视化更多变量，但你可以想象。
- en: 'Let’s consider this simple example of a set of data points (x, y) that follows
    the equation y = 2 + x — 0.5 * x², we usually will create the following plot to
    represent x and y:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的数据点集合(x, y)，其遵循方程y = 2 + x — 0.5 * x²，我们通常会创建以下图表来表示x和y：
- en: '![](../Images/d6369a1bd08052171cf5d61d9f1daec3.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6369a1bd08052171cf5d61d9f1daec3.png)'
- en: Polynomial regression x, y plot — image by author
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式回归x, y图 — 图片由作者提供
- en: The red line represents the model, and the blue dots the training dataset.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 红线代表模型，蓝点代表训练数据集。
- en: 'Now, let’s imagine that we first plot some values of x and x²:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们假设我们首先绘制一些x和x²的值：
- en: '![](../Images/37805247c58350a0274c8346e1284dd6.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37805247c58350a0274c8346e1284dd6.png)'
- en: Polynomial regression plot x and x^2— image by author
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式回归图x和x^2 — 图片由作者提供
- en: Then we create a 3D plot by adding the values of y for each (x, x²). The linear
    regression model using x and x² as inputs is then a plane that also can be represented
    in the 3D plot as shown below.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过为每个(x, x²)添加y值来创建一个3D图。使用x和x²作为输入的线性回归模型随后是一个平面，也可以在3D图中表示如下。
- en: '![](../Images/61c8d572eda196b560061b4d63781995.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61c8d572eda196b560061b4d63781995.png)'
- en: Polynomial regression is linear regression 3D plot — image by author
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式回归是线性回归的3D图 — 图片由作者提供
- en: 'And we can make a gif out the several images by changing the angle of view.
    If you want to get the code for the creation of this gif, among other useful codes,
    please support me on Ko-fi with the following link: [https://ko-fi.com/s/4cc6555852](https://ko-fi.com/s/4cc6555852).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过改变视角，将几张图片合成一个gif。如果你想获取创建此gif的代码及其他有用的代码，请通过以下链接在Ko-fi上支持我：[https://ko-fi.com/s/4cc6555852](https://ko-fi.com/s/4cc6555852)。
- en: '![](../Images/1463b2496ed7c9cc75c659e6211b1ba4.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1463b2496ed7c9cc75c659e6211b1ba4.png)'
- en: Polynomial regression is linear regression 3d plot — image by author
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式回归是线性回归的3D图 — 图片由作者提供
- en: 3\. Machine Learning Perspective for polynomial regression
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 多项式回归的机器学习视角
- en: 3.1 Practical application of polynomial regression
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 多项式回归的实际应用
- en: Fellow data scientists, be honest, do you really build polynomial regression
    for real-world applications? When do you really encounter polynomial regression?
    Try to remember… yes, that’s right, when the teacher tries to explain overfitting
    for regression problems. Here is an example below.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 各位数据科学家，坦白说，你们真的在实际应用中构建多项式回归吗？你们什么时候真正遇到多项式回归？试着回忆一下…对了，就是老师尝试解释回归问题中的过拟合时。下面是一个例子。
- en: '![](../Images/0bf51defcce25bb280ac2a886c02f96e.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0bf51defcce25bb280ac2a886c02f96e.png)'
- en: Polynomial regression overfitting — image by author
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式回归过拟合 — 图片由作者提供
- en: From a statistician’s viewpoint, with numpy, we already said only one feature
    is allowed. So it is not realistic to use this polyfit to create real-world models.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 从统计学家的观点来看，使用numpy时，我们已经提到只允许一个特征。因此，使用这个polyfit来创建实际模型是不现实的。
- en: 3.2 Machine learning perspective with scaling and regularization
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 从机器学习角度看缩放和正则化
- en: From a machine learning perspective, creating a polynomial regression is more
    than raising one feature to various powers.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从机器学习的角度看，创建多项式回归不仅仅是将一个特征提升到不同的幂次。
- en: 'First, for the overall creation of polynomial regression, we already mentioned
    that it is done in two steps: PolynomialFeatures with preprocessing and LinearRegression
    with the proper model part.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，对于多项式回归的整体创建，我们已经提到这是分两步完成的：使用PolynomialFeatures进行预处理和使用LinearRegression进行模型部分。
- en: Now, we also have to mention some specific technics like scaling. In fact, when
    the features are raised to very high degrees, then the numbers become so big that
    scikit learn can’t handle them anymore. So, it is better to perform scaling, not
    for the theoretical part, but for the practical aspect.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们还需要提到一些特定的技术，比如缩放。实际上，当特征提升到非常高的次数时，数字变得非常大，以至于scikit learn无法处理。因此，进行缩放是更好的选择，这不仅是理论上的问题，更是实践中的需要。
- en: 'I wrote an article to talk about this point: [Polynomial Regression with Scikit
    learn: What You Should Know](/polynomial-regression-with-scikit-learn-what-you-should-know-bed9d3296f2).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我写了一篇文章讨论这一点：[使用Scikit-learn的多项式回归：你应该知道的事项](/polynomial-regression-with-scikit-learn-what-you-should-know-bed9d3296f2)。
- en: One crucial aspect of polynomial regression is the degree of the polynomial,
    usually considered as the hyperparameter of the model. However, we should not
    forget that the degree is not the only possible hyperparameter. In fact, it is
    easier to think when you do polynomial regression in two steps
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式回归的一个关键方面是多项式的次数，通常被视为模型的超参数。然而，我们不应忘记次数并不是唯一可能的超参数。实际上，当你分两步进行多项式回归时，更容易理解。
- en: 'Polynomial features: we can customize the features, and even manually add some
    interactions between certain variables. Then we can scale the features or use
    other technics such as QuantileTransformer or KBinsDiscretizer.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式特征：我们可以自定义特征，甚至手动添加某些变量之间的交互。然后我们可以缩放特征或使用其他技术，如QuantileTransformer或KBinsDiscretizer。
- en: 'Models: then for the model part, we also can choose a model such as Ridge,
    Lasso, or even SVR, instead of Linear regression.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型：在模型部分，我们还可以选择像岭回归、套索回归，甚至支持向量回归这样的模型，而不是线性回归。
- en: Here is an example to illustrate the impact of the hyperparameter alpha on the
    model with ridge regression.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示例，说明超参数alpha对岭回归模型的影响。
- en: '![](../Images/fcae649d77160d04e356712508bd0cca.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fcae649d77160d04e356712508bd0cca.png)'
- en: Polynomial regression overfitting with alpha ridge — image by author
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式回归的过拟合与alpha岭回归 — 作者提供的图像
- en: Now, you may be happy that you just learned how to create some new and more
    effective models. But they are still not practical to use. Maybe some of you can
    see that it is what we already do, but with a different approach… yes, kernels!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能很高兴你刚刚学会了如何创建一些新的、更有效的模型。但它们仍然不够实用。也许你们中的一些人会发现这实际上是我们已经在做的事情，只不过方法不同……没错，就是核方法！
- en: 3.3 Poly kernels and more
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 多项式核及其他
- en: The usual form of polynomial regression is not practical to implement, but in
    theory, this is one effective way to create a non-linear model based on mathematical
    functions. The other one is creating neural networks.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式回归的常见形式在实际中并不方便实现，但理论上，这是一种基于数学函数创建非线性模型的有效方法。另一种方法是创建神经网络。
- en: One reason is that when creating polynomial features, the number of features
    can be huge. And fitting a linear regression can be time-consuming. That is why
    SVR or SVM comes into play since the hinge loss function allows to “drop” many
    data points by keeping only the said “support vectors”.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一个原因是，当创建多项式特征时，特征数量可能会非常庞大。拟合线性回归可能非常耗时。这就是为什么支持向量回归或支持向量机会发挥作用，因为铰链损失函数允许通过仅保留所称的“支持向量”来“丢弃”许多数据点。
- en: Yes, the idea of kernel function is often associated with SVM or SVR, but we
    should not forget that it is a separate theory and that is why we also have [KernelRidge](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html)
    in scikit learn. And in theory, we also could have KernelLasso or KernelElasticNet.
    KernelLogisticRegression is another example for classification tasks.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，核函数的概念通常与支持向量机或支持向量回归相关，但我们不应忘记这是一个独立的理论，这也是为什么在scikit-learn中我们有[KernelRidge](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html)。理论上，我们也可以有KernelLasso或KernelElasticNet。KernelLogisticRegression是分类任务中的另一个例子。
- en: Conclusion
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, we have explored the model of polynomial regression and its relationship
    with linear regression. While it is usually considered ONE model for one feature
    from a statistical perspective, it can be viewed as a form of linear regression
    preceded by a feature engineering part which consists namely of creating polynomial
    features but they should be scaled. Moreover, we can also apply other models such
    as ridge, lasso or SVM to these polynomial features which will improve their performance.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们探讨了多项式回归模型及其与线性回归的关系。虽然从统计学角度来看，它通常被视为针对一个特征的单一模型，但它也可以被看作是一个形式上的线性回归，其前面有一个特征工程部分，主要包括创建多项式特征，但这些特征需要进行缩放。此外，我们还可以将其他模型如岭回归、套索回归或支持向量机应用于这些多项式特征，以提升其性能。
- en: Finally, polynomial regression serves also as a prime example for the feature
    mapping technics in the case of mathematical function-based models. And this leads
    to kernel functions that allow the modeling of non-linear relationships between
    the features and the target variable.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，多项式回归也作为数学函数模型中特征映射技术的一个典型例子。*这* 进而引出了核函数，这些函数可以建模特征与目标变量之间的非线性关系。
