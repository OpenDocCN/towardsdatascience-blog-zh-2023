- en: GPT — Intuitively and Exhaustively Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/gpt-intuitively-and-exhaustively-explained-c70c38e87491](https://towardsdatascience.com/gpt-intuitively-and-exhaustively-explained-c70c38e87491)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Natural Language Processing | Machine Learning | Chat GPT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exploring the architecture of OpenAI’s Generative Pre-trained Transformers.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@danielwarfield1?source=post_page-----c70c38e87491--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----c70c38e87491--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c70c38e87491--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c70c38e87491--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----c70c38e87491--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c70c38e87491--------------------------------)
    ·16 min read·Dec 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4411a465ee908df05347a8808740ba13.png)'
  prefs: []
  type: TYPE_IMG
- en: “Mixture Expert” by the author using MidJourney. All images by the author unless
    otherwise specified.
  prefs: []
  type: TYPE_NORMAL
- en: In this article we’ll be exploring the evolution of OpenAI’s GPT models. We’ll
    briefly cover the transformer, describe variations of the transformer which lead
    to the first GPT model, then we’ll go through GPT1, GPT2, GPT3, and GPT4 to build
    a complete conceptual understanding of the state of the art.
  prefs: []
  type: TYPE_NORMAL
- en: '**Who is this useful for?** Anyone interested in natural language processing
    (NLP), or cutting edge AI advancements.'
  prefs: []
  type: TYPE_NORMAL
- en: '**How advanced is this post?** This post should be accessible to all experience
    levels.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-requisites:** I’ll briefly cover transformers in this article, but you
    can refer to my dedicated article on the subject for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----c70c38e87491--------------------------------)
    [## Transformers — Intuitively and Exhaustively Explained'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exploring the modern wave of machine learning: taking apart the transformer
    step by step'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----c70c38e87491--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: A Brief Introduction to Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we get into GPT I want to briefly go over the transformer. In its most
    basic sense, the transformer is an encoder-decoder style model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4432448b3d91010af8bf9da54683b03c.png)'
  prefs: []
  type: TYPE_IMG
- en: A transformer working in a translation task. The input (I am a manager) is compressed
    to some abstract representation that encodes the meaning of the entire input.
    The decoder works recurrently, by feeding into itself, to construct the output.
    From [my article on transformers](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
  prefs: []
  type: TYPE_NORMAL
- en: The encoder converts an input into an abstract representation which the decoder
    uses to iteratively generate output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd4d304239e39c72240c88fd9ce84ec0.png)'
  prefs: []
  type: TYPE_IMG
- en: high level representation of how the output of the encoder relates to the decoder.
    the decoder references the encoded input for every recursive loop of the output.
    From [my article on transformers](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
  prefs: []
  type: TYPE_NORMAL
- en: both the encoder and decoder employ an abstract representations of text which
    is created using multi headed self attention.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/055a140c1f0692597fb98dfc81c1bfbb.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi Headed self attention, in a nutshell. The mechanism mathematically combines
    the vectors for different words, creating a matrix which encodes a deeper meaning
    of the entire input. From [my article on transformers](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
  prefs: []
  type: TYPE_NORMAL
- en: There’s a few steps which multiheaded self attention employs to construct this
    abstract representation. In a nutshell, a dense neural network constructs three
    representations, usually referred to as the query, key, and value, based on the
    input.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/51ee47ff765f5366ae85cb45804948f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Turning the embedded input into the query, key, and value. The query, key, and
    value all have the same dimensions as the input, and can be thought of as the
    input which has been passed through a filter. From [my article on transformers](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
  prefs: []
  type: TYPE_NORMAL
- en: The query and key are multiplied together. Thus, some representation of every
    word is combined with a representation of every other word.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba6755db58b9ed709ff2cd18f7c8e03d.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculating the attention matrix with the query and key. The attention matrix
    is then used, in combination with the value, to generate the final output of the
    attention mechanism. From [my article on transformers](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
  prefs: []
  type: TYPE_NORMAL
- en: The value is then multiplied by this abstract combination of the query and key,
    constructing the final output of multi headed self attention.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/adf0f25a0b677c8c6977f565c7e99601.png)'
  prefs: []
  type: TYPE_IMG
- en: The attention matrix (which is the matrix multiplication of the query and key)
    multiplied by the value matrix to yield the final result of the attention mechanism.
    Because of the shape of the attention matrix, the result is the same shape as
    the value matrix. From [my article on transformers](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
  prefs: []
  type: TYPE_NORMAL
- en: The encoder uses multi-headed self attention to create abstract representations
    of the input, and the decoder uses multi-headed self attention to create abstract
    representations of the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ed71b9401cae258d706bbfd0103184a.png)'
  prefs: []
  type: TYPE_IMG
- en: The Transformer Architecture, with the encoder on the left and the decoder on
    the right. There’s a lot of details I skimmed over for brevity, but if you look
    at it from a high level it’s not too complicated. The encoder uses multi-headed
    self attention to encode the input, and the decoder uses multi-headed self attention
    twice; once to encode the previously constructed outputs, and once to combine
    the input representation with the previous outputs. image [source](https://arxiv.org/pdf/1706.03762v2.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4432448b3d91010af8bf9da54683b03c.png)'
  prefs: []
  type: TYPE_IMG
- en: Recall that this is what the transformer is doing, in essence.
  prefs: []
  type: TYPE_NORMAL
- en: That was a super quick rundown on transformers. I tried to cover the high points
    without getting too in the weeds, feel free to refer to my [article on transformers](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
    for more information. Now that we vaguely understand the essentials we can start
    talking about GPT.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-1 (Released June 2018)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The paper [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
    introduced the GPT style model. This is a fantastic paper, with a lot of cool
    details. We’ll summarize this paper into the following key concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: A decoder-only style architecture
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unsupervised pre-training
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Supervised fine tuning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Task-specific input transformation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s unpack each of these ideas one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder Only Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we previously discussed, the transformer is an encoder-decoder style architecture.
    The encoder converts some input into an abstract representation, and the decoder
    iteratively generates the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ed71b9401cae258d706bbfd0103184a.png)'
  prefs: []
  type: TYPE_IMG
- en: The Transformer Architecture, with the encoder on the left and the decoder on
    the right. Image [source](https://arxiv.org/pdf/1706.03762v2.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: You might notice that both the encoder and the decoder are remarkably similar.
    Since the transformer was published back in 2017, researchers have played around
    with each of these subcomponents, and have found that both of them are phenomenally
    good at language representation. Models which use only an encoder, or only a decoder,
    have been popular ever since.
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, encoder-only style models are good at extracting information
    from text for tasks like classification and regression, while decoder-only style
    models focus on generating text. GPT, being a model focused on text generation,
    is a decoder only style model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62b283a03f81874c3b020565d9e0df9c.png)'
  prefs: []
  type: TYPE_IMG
- en: The model architecture of GPT-1, a decoder-only style model. [source](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: The decoder-only style of model used in GPT has very similar components to the
    traditional transformer, but also some important and subtle distinctions. Let’s
    run through the key ideas of the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-1 uses a text and position embedding, which converts a given input word
    into a vector which encodes both the words general meaning and the position of
    the word within the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/caff7020e503ff415e9d2d938a146fc3.png)'
  prefs: []
  type: TYPE_IMG
- en: A conceptual diagram of GPT’s input embedding. Each word has two vectors, one
    for the word and one for the location, which are added together to represent each
    word. These are used to construct the input to the model. [source](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: Like the original transformer, GPT uses a “learned word embedding.” Essentially,
    a vector for each word in the vocabulary of the model is randomly initialized,
    then updated throughout model training.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the original transformer, GPT uses a “learned positional encoding.” GPT
    learns a vector for each input location, which it adds to the learned vector embedding.
    This results in an embedding which contains information about the word, and where
    the word is in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: These embedded words, with positional information, get passed through masked
    multi-headed self attention. For the purposes of this article we’ll stick with
    the simplification that this mechanism combines every word vector with every other
    word vector to create an abstract matrix encoding the whole input in some meaningful
    way.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41f2cc815b20a269864c806b24689d3c.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi Headed self attention, in a nutshell. The mechanism mathematically combines
    the vectors for different words, creating a matrix which encodes a deeper meaning
    of the entire input. From [my article on transformers](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
    and [source](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: A lot of math happens in self attention, which could result in super big or
    super small values. This has a tendency to make models perform poorly, so all
    the values are squashed down into a reasonable range with layer normalization.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2dce43f9f6975106aac078bf84751776.png)'
  prefs: []
  type: TYPE_IMG
- en: Layer Normalization squashes numbers into a reasonable output, which is important
    in machine learning for models to learn effectively. [source](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: The data is then passed through a dense network (your classic neural network)
    then passed through another layer normalization. This all happens in several decoder
    blocks which are stacked on top of eachother, allowing the GPT model to do some
    pretty complex manipulations to the input text.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd4fae7ac5a14583a38dd84dadc2ec4f.png)'
  prefs: []
  type: TYPE_IMG
- en: The entire GPT-1 model. 12 decoder layers stacked on top of each other. [source](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: I wanted to take a moment to describe how decoder style models actually go about
    making inferences; a topic which, for whatever reason, a lot of people don’t seem
    to take the time to explain. The typical encoder-decoder style transformer encodes
    the entire input, then recurrently constructs the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df14d38b4ec854c7339ac1b31e787102.png)'
  prefs: []
  type: TYPE_IMG
- en: The traditional encoder-decoder style transformer in action. The encoder compresses
    the input into an abstract representation, then the decoder uses previously predicted
    outputs to guess the next word in the output sequence. Thus, the transformer iteratively
    constructing a response.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder only transformers, like GPT, don’t have an encoder to work with. Instead,
    they simply concatenate the previous output to the input sequence and pass the
    entire input and all previous outputs for every inference, a style referred to
    in the literature as “autoregressive generation”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d7b7f7bd71cc624c21bf8c57d38cd90.png)'
  prefs: []
  type: TYPE_IMG
- en: A decoder only style model, like GPT, iteratively constructing an output with
    autoregressive generation.
  prefs: []
  type: TYPE_NORMAL
- en: This idea of treating outputs similarly to inputs is critical in one of the
    main deviations GPT took from transformers to reach cutting edge performance.
  prefs: []
  type: TYPE_NORMAL
- en: Semi-Supervised Pre-Training, Then Supervised Fine Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you research GPT, or language modeling as a whole, you might find the term
    “language modeling objective.” This term refers to the act of predicting the next
    word given an input sequence, which, essentially models textual language. The
    idea is, if you can get really really good at predicting the next word in a sequence
    of text, in theory you can keep predicting the next word over and over until you’ve
    output a whole book.
  prefs: []
  type: TYPE_NORMAL
- en: Because the traditional transformer requires the concept of an explicit “input”
    and “output” (an input for the encoder and an output from the decoder), the idea
    of next word prediction doesn’t really make a tone of sense. Decoder models, like
    GPT, *only* do next word prediction, so the idea of training them on language
    modeling makes a tone of sense.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/291d3fab63f0be88da5d32b607cbcc09.png)'
  prefs: []
  type: TYPE_IMG
- en: An example of the data a model might see when being trained on language modeling.
    The model is provided some intput (in red) and asked to predict the next word
    (in blue). Of course, for a model like GPT, this data would be extracted from
    large corpus of documents.
  prefs: []
  type: TYPE_NORMAL
- en: This opens up a tone of options in terms of training strategies. I talk a bit
    about pre-training and fine tuning in my [article on LoRA](https://medium.com/towards-data-science/lora-intuitively-and-exhaustively-explained-e944a6bff46b).
    In a nutshell, pre-training is the idea of training on a bulk dataset to get a
    general understanding of some domain, then that model can be fine tuned on a specific
    task.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/24e3be015597ecbc43818286e6fc85fb.png)'
  prefs: []
  type: TYPE_IMG
- en: A diagram depicting what pre-training and fine tuning might look like. A language
    model might be trained on bulk data to understand language, then be fine tuned
    on a specific task. From my [article on LoRA](https://medium.com/towards-data-science/lora-intuitively-and-exhaustively-explained-e944a6bff46b)
  prefs: []
  type: TYPE_NORMAL
- en: GPT is an abbreviation of “Generative Pre-Trained Transformer” for a reason.
    GPT is pre-trained on a vast amount of text using language modeling (next word
    prediction). It essentially learns “given an input sequence X, the next word should
    be Y.”
  prefs: []
  type: TYPE_NORMAL
- en: 'This form of training falls under the broader umbrella of “semi-supervision”.
    Here’s a quote from [another article](/self-supervised-learning-using-projection-heads-b77af3911d33)
    which highlights how semi-supervised learning deviates from other training strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised Learning** is the process of training a model based on **labeled**
    information. When training a model to predict if images contain cats or dogs,
    for instance, one curates a set of images which are labeled as having a cat or
    a dog, then trains the model (using [gradient descent](https://medium.com/@danielwarfield1/what-are-gradients-and-why-do-they-explode-add23264d24b))
    to understand the difference between images with cats and dogs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised Learning** is the process of giving some sort of model **unlabeled**
    information, and extracting useful inferences through some sort of transformation
    of the data. A classic example of unsupervised learning is clustering; where groups
    of information are extracted from un-grouped data based on local position.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-supervised learning is somewhere in between. **Self-supervision uses**
    **labels** **that are generated programmatically, not by humans.** In some ways
    it’s supervised because the model learns from labeled data, but in other ways
    it’s unsupervised because no labels are provided to the training algorithm. Hence
    self-supervised.
  prefs: []
  type: TYPE_NORMAL
- en: Using a semi-supervised approach, in the form of language modeling, allows GPT
    to be trained on a previously unprecedented volume of training data, allowing
    the model to create robust linguistic representations. This model, with a firm
    understanding of language, can then be fine-tuned on more specific datasets for
    more specific tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We use the BooksCorpus dataset [71] for training the language model. It contains
    over 7,000 unique unpublished books from a variety of genres including Adventure,
    Fantasy, and Romance. — [The GPT paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
    on pre-training
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We perform experiments on a variety of supervised tasks including natural language
    inference, question answering, semantic similarity, and text classification. —
    [The GPT paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
    on fine tuning
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Task-Specific Input Transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Language models, like GPT, are currently known to be incredibly powerful “in
    context learners”; you can give them information in their input as context, then
    they can use that context to construct better responses. I used this concept in
    my [article on retreival augmented generation](https://medium.com/towards-data-science/retrieval-augmented-generation-intuitively-and-exhaustively-explain-6a39d6fe6fc9),
    my [article on visual question answering](https://medium.com/towards-data-science/visual-question-answering-with-frozen-large-language-models-353d42791054),
    and my [article on parsing the output from language models](https://medium.com/towards-data-science/conversations-as-directed-graphs-with-lang-chain-46d70e1a846c).
    It’s a pretty powerful concept.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e689a3ff3fd290d6998a23a996211dfa.png)'
  prefs: []
  type: TYPE_IMG
- en: An example of ChatGPT succeeding at a task with the help of in context learning.
    From my [RAG article](https://medium.com/towards-data-science/retrieval-augmented-generation-intuitively-and-exhaustively-explain-6a39d6fe6fc9)
  prefs: []
  type: TYPE_NORMAL
- en: At the time of the first GPT paper in context learning was not nearly so well
    understood. The paper dedicates an entire section to “Task-specific input transformations”.
    Basically, instead of adding special components to the model to make it work for
    a specific task, the authors of GPT opted to format those tasks textually.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, for textual entailment (the process of predicting if a piece
    of text directly relates with another piece of text) the GPT paper simply concatenated
    both pieces of text together, with a dollar sign in between. This allowed them
    to fine tune GPT on textual entailment without adding any new parameters to the
    model. Other objectives, like text similarity, question answering, and commonsense
    reasoning, were all fine tuned in a similar way.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2 (Released February 2019)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The paper [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
    introduced the world to GPT-2, which is essentially identical to GPT-1 save two
    key differences:'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2 is way bigger than GPT-1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GPT-2 doesn’t use any fine tuning, only pre-training
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Also, as a brief note, the GPT-2 architecture is ever so slightly different
    from the GPT-1 architecture. In GPT-1 each block consists of `[Attention, Norm,
    Feed Forward, Norm]` , where GPT-2 consists of `[Norm, Attention, Norm, Feed Forward]`.
    However, this difference is so minor it’s hardly worth mentioning.
  prefs: []
  type: TYPE_NORMAL
- en: Bigger is Better
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/32b61edd83117e0746d7c016bbde6250.png)'
  prefs: []
  type: TYPE_IMG
- en: The performance of GPT-2 on several objectives, as a function of parameter count.
    As can be seen, the more parameters a language model is, the better it generally
    performs. From the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the findings of the GPT-2 paper is that larger models are better. Specifically,
    they theorize that language model performance scales “log-linearly”, so something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8734649b954ad678df9f89b2f7cdc520.png)'
  prefs: []
  type: TYPE_IMG
- en: The relationship between how good a model is, and how big it is, seems to be
    log-linear, based on the findings of GPT-2
  prefs: []
  type: TYPE_NORMAL
- en: As a result GPT-2 contains 1.5 billion parameters, which is roughly ten times
    larger than GPT-1\. GPT-2 was also trained on roughly ten times the data.
  prefs: []
  type: TYPE_NORMAL
- en: Language Understanding is General
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In GPT-1 they focused on using the language modeling objective to create a solid
    baseline of textual understanding, and then used that baseline to fine tune models
    for specific tasks. In GPT-2 they got rid of fine tuning completely, operating
    under the assumption that a sufficiently pre-trained model can perform well on
    specific problems without being explicitly trained on them.
  prefs: []
  type: TYPE_NORMAL
- en: Our suspicion is that the prevalence of single task training on single domain
    datasets is a major contributor to the lack of generalization observed in current
    systems. Progress towards robust systems with current architectures is likely
    to require training and measuring performance on a wide range of domains and tasks…
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When conditioned on a document plus questions, the answers generated by the
    language model reach 55 F1 on the CoQA dataset — matching or exceeding the performance
    of 3 out of 4 baseline systems without using the 127,000+ training examples…
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state
    of the art results on 7 out of 8 tested language modeling datasets in a zero-shot
    setting — [The GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: They achieved this generalized performance by using a super big model and feeding
    it a whole bunch of high quality data.
  prefs: []
  type: TYPE_NORMAL
- en: Our approach motivates building as large and diverse a dataset as possible in
    order to collect natural language demonstrations of tasks in as varied of domains
    and contexts as possible…
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: we created a new web scrape which emphasizes document quality. To do this we
    only scraped web pages which have been curated/filtered by humans. Manually filtering
    a full web scrape would be exceptionally expensive so as a starting point, we
    scraped all outbound links from Reddit, a social media platform, which received
    at least 3 karma. This can be thought of as a heuristic indicator for whether
    other users found the link interesting, educational, or just funny. — [The GPT-2
    paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: GPT 3 (Released June 2020)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The paper [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)
    introduced the world to GPT-3, which essentially said “bigger was good, so why
    not even bigger?” Instead of a measly 1.5 billion parameters in GPT-2, GPT-3 employs
    175 billion parameters, and was trained on 45 Terabytes of text data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ae7e2df876bb511aac2d20674fec6d7c.png)'
  prefs: []
  type: TYPE_IMG
- en: A graph of model size vs performance. From the [GPT-3 paper](https://arxiv.org/pdf/2005.14165.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: From a data science perspective not much really changed. Similar model, similar
    core architecture, similar language modeling objective. From an engineering perspective,
    however, things were certainly different. GPT-2 is roughly 5 Gigabytes in size,
    whereas GPT-3 is around 800 Gigabytes (I’ve seen a lot of variation in this number
    online, but it’s definitely big). The Nvidia H100 GPU, which retails at a cool
    $30,000, holds 80GB of VRAM. Keep in mind, to train a machine learning model you
    essentially need double the model size in VRAM to hold the parameters as well
    as the gradients (as I discussed in my [LoRA article](https://medium.com/towards-data-science/lora-intuitively-and-exhaustively-explained-e944a6bff46b)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/247fb6f7e4b422a3b39b2edf476c57d0.png)'
  prefs: []
  type: TYPE_IMG
- en: A table from the [GPT-3 paper](https://arxiv.org/pdf/2005.14165.pdf), stating
    the accuracy of humans in predicting if a news article was written by GPT-3 or
    not. As you can see, for the full sized 175B parameter model, only 52% of people
    guessed correctly. 50% would be 50/50 shot in the dark.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve seen actual cost estimations for GPT-3 which deviate wildly, mostly as
    a result of how much VRAM the model actually required to train. One thing is certain;
    you need a cluster of super expensive computers, working in parallel, to train
    a model like GPT-3, and you need a similar cluster to serve that model to users.
  prefs: []
  type: TYPE_NORMAL
- en: The GPT-3 paper is titled “Language Models are Few-Shot Learners.” By “Few-Shot
    Learners” the authors mean that GPT-3 can get pretty good at most tasks without
    training the model. Instead, you can just experiment with the input to the model
    to find the right input format for a particular task. And thus, prompt engineering
    was born.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 (Released March 2023)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point clarity ends. OpenAI has shifted from a research non-profit to
    a seriously heavy hitting corporate machine, facing off against big names like
    Microsoft and Google. There is no peer reviewed and transparent paper on GPT-4\.
    As you might know, OpenAI has been experiencing a bit of friction internally,
    this is part of the reason why.
  prefs: []
  type: TYPE_NORMAL
- en: That said, is there information? Absolutely.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 is “Safe” To Use in Products
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First of all, we can glean a lot of information from the [100 page technical
    report](https://arxiv.org/pdf/2303.08774.pdf) by OpenAI, which hides a lot of
    the specifics in terms of modeling, but exhaustively details performance and capability
    of the model. This paper reads a lot more like an advertisement than anything
    seriously academic; the term “safety”, for instance, has a much stronger bias
    towards safety to use within a product rather than the traditional usage of the
    word in research which focuses on safety for humans.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 Can Understand Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the big changes in GPT-4 is “multi-modality”; allowing GPT-4 to converse
    about both images and text.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b0113700ca80d01837e9fcd814135c3.png)'
  prefs: []
  type: TYPE_IMG
- en: GPT-4 conversing about an image. [Source](https://arxiv.org/pdf/2303.08774.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: This has all sorts of cool uses. In a pretty famous example, OpenAI demoed how
    this functionality could be used to turn a crude sketch into a (still fairly crude)
    website written in HTML, CSS, and JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 Is Fine Tuned on Human Feedback
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Also, GPT-4 uses “Reinforcement Learning with Human Feedback”, commonly abbreviated
    as RLHF. You know those little thumbs up and thumbs down on the ChatGPT website?
    Those allow OpenAI to further improve the model based on in-context human feedback.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69c862ec441682296957f375c3b7b214.png)'
  prefs: []
  type: TYPE_IMG
- en: This strategy of learning can have dubious results, and can actually degrade
    the models performance. However, it does help with getting GPT-4 into a better
    style as a sort of “AI Helper”
  prefs: []
  type: TYPE_NORMAL
- en: Our evaluations suggest RLHF does not significantly affect the base GPT-4 model’s
    capability — [GPT-4 Technical Report](https://arxiv.org/pdf/2303.08774.pdf)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: GPT-4 is Really Big
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There was an [infamous leak](https://archive.is/2RQ8X#selection-833.1-873.202)
    about four months ago which seemed to have drop some credible evidence about some
    of the behind the scenes details on GPT-4\. This information was released behind
    a paywall, which was then leaked again on Twitter, which is now… X? I guess?
  prefs: []
  type: TYPE_NORMAL
- en: Anyway, GPT-4 seems like it hits the 1.8 Trillion parameter mark, which is around
    ten times the size of GPT-3.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 uses Mixture of Experts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I plan on covering Mixture of Experts (MOE) in a future article. In a nutshell
    MOE is the process of dividing a large model into smaller sub models, and creating
    a tiny model which serves to route data to individual components.
  prefs: []
  type: TYPE_NORMAL
- en: The whole idea is, the bigger your model gets, the more costly it becomes to
    run. If instead you can have a portion of your model dedicated to art, another
    dedicated to science, and another to telling jokes, you can route queries to individual
    sub-components. In reality these groupings are much more abstract, but the logic
    is the same; the model can use different sub-components on every query, allowing
    it to run much leaner than if it were to use all parameters for every query.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the [GPT-4 leak](https://archive.is/2RQ8X#selection-833.1-873.202),
    by using MOE, the model only needs to use 280 billion parameters of the models
    1.8 Trillion (a mere 15%) on any given inference.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 uses 16 experts (the model is divided into 16 parts which are then routed
    to). This is much less than the current state of research, which suggests larger
    sets of 64 or 128 experts.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 Training Cost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on the [GPT-4 leak](https://archive.is/2RQ8X#selection-833.1-873.202),
    the model cost around $63 million to train on A100 GPUs. Using more modern H100
    GPUs would probably cost around $21.5 million. Such is the cost of being an early
    mover, I guess.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 Uses Some Bells and Whistles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPT-4 uses “multi-query attention”, which is a style of attention which improves
    model throughput. I’ll be covering it in an upcoming article.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-4 uses “continuous batching” which I will probably cover in yet another
    article.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-4 uses “speculative decoding”. My goodness, the pace of AI research is keeping
    me busy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-4 Probably Uses Something Like Flamingo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GPT-4 uses a style of multimodal modeling similar to “Flamingo”, a multimodal
    model by Deepmind. I’ll cover Flamingo specifically in the future, but I do have
    an article on a similar approach by the SalesForce research team, called Q-Fromer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/visual-question-answering-with-frozen-large-language-models-353d42791054?source=post_page-----c70c38e87491--------------------------------)
    [## Visual Question Answering with Frozen Large Language Models'
  prefs: []
  type: TYPE_NORMAL
- en: Talking with LLMs about images, without training LLMs on images.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/visual-question-answering-with-frozen-large-language-models-353d42791054?source=post_page-----c70c38e87491--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, both Q-Former and Flamingo use frozen models for both language
    and images, and trains some glue that joins them together to create a model which
    can talk about images.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 Uses High Quality Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It seems like, based on the [GPT-4 leak](https://archive.is/2RQ8X#selection-833.1-873.202),
    OpenAI used college textbooks to improve the model’s knowledge in numerous intellectual
    domains. The legality and ethicality of this is definitely in a gray area, which
    might also explain some of the current in-fighting and secrecy at OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well, that got kind of dramatic in the end, huh? First we briefly went over
    transformers, then described how GPT-1 adopted the decoder component of the transformer
    to improve text generation through self-supervised pre-training using the language
    modeling objective. We then discussed how GPT-2 and GPT-3 both doubled down on
    this approach, producing language models which were, to a large degree, Identical
    to GPT-1 just way bigger. We then discussed what we know of GPT-4; the model size,
    some architectural changes, and even some saucy controversy.
  prefs: []
  type: TYPE_NORMAL
- en: Follow For More!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I describe papers and concepts in the ML space, with an emphasis on practical
    and intuitive explanations.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@danielwarfield1/subscribe?source=post_page-----c70c38e87491--------------------------------)
    [## Get an email whenever Daniel Warfield publishes'
  prefs: []
  type: TYPE_NORMAL
- en: Get an email whenever Daniel Warfield publishes By signing up, you will create
    a Medium account if you don't already…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@danielwarfield1/subscribe?source=post_page-----c70c38e87491--------------------------------)
    [![](../Images/1f6f4c8a07d69cf53e055e0130a85b03.png)](https://www.buymeacoffee.com/danielwarfield)
  prefs: []
  type: TYPE_NORMAL
- en: Never expected, always appreciated. By donating you allow me to allocate more
    time and resources towards more frequent and higher quality articles. [Link](https://www.buymeacoffee.com/danielwarfield)
  prefs: []
  type: TYPE_NORMAL
- en: '**Attribution:** All of the resources in this document were created by Daniel
    Warfield, unless a source is otherwise provided. You can use any resource in this
    post for your own non-commercial purposes, so long as you reference this article,
    [https://danielwarfield.dev](https://danielwarfield.dev/), or both. An explicit
    commercial license may be granted upon request.'
  prefs: []
  type: TYPE_NORMAL
