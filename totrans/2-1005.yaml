- en: GPT — Intuitively and Exhaustively Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT — 直观而全面的解释
- en: 原文：[https://towardsdatascience.com/gpt-intuitively-and-exhaustively-explained-c70c38e87491](https://towardsdatascience.com/gpt-intuitively-and-exhaustively-explained-c70c38e87491)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gpt-intuitively-and-exhaustively-explained-c70c38e87491](https://towardsdatascience.com/gpt-intuitively-and-exhaustively-explained-c70c38e87491)
- en: Natural Language Processing | Machine Learning | Chat GPT
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理 | 机器学习 | Chat GPT
- en: Exploring the architecture of OpenAI’s Generative Pre-trained Transformers.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索 OpenAI 生成式预训练变换器的架构。
- en: '[](https://medium.com/@danielwarfield1?source=post_page-----c70c38e87491--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----c70c38e87491--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c70c38e87491--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c70c38e87491--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----c70c38e87491--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@danielwarfield1?source=post_page-----c70c38e87491--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----c70c38e87491--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c70c38e87491--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c70c38e87491--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----c70c38e87491--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c70c38e87491--------------------------------)
    ·16 min read·Dec 1, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c70c38e87491--------------------------------)
    ·阅读时间 16 分钟·2023年12月1日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4411a465ee908df05347a8808740ba13.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4411a465ee908df05347a8808740ba13.png)'
- en: “Mixture Expert” by the author using MidJourney. All images by the author unless
    otherwise specified.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: “Mixture Expert” 由作者使用 MidJourney 制作。所有图像由作者提供，除非另有说明。
- en: In this article we’ll be exploring the evolution of OpenAI’s GPT models. We’ll
    briefly cover the transformer, describe variations of the transformer which lead
    to the first GPT model, then we’ll go through GPT1, GPT2, GPT3, and GPT4 to build
    a complete conceptual understanding of the state of the art.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨 OpenAI 的 GPT 模型的演变。我们将简要介绍变换器，描述导致第一个 GPT 模型的变换器变体，然后我们将逐步了解 GPT1、GPT2、GPT3
    和 GPT4，以建立对最新技术的全面概念。
- en: '**Who is this useful for?** Anyone interested in natural language processing
    (NLP), or cutting edge AI advancements.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**这对谁有用？** 任何对自然语言处理（NLP）或前沿人工智能进展感兴趣的人。'
- en: '**How advanced is this post?** This post should be accessible to all experience
    levels.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**这篇文章的先进程度如何？** 这篇文章应该对所有经验水平的人都易于理解。'
- en: '**Pre-requisites:** I’ll briefly cover transformers in this article, but you
    can refer to my dedicated article on the subject for more information.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**前置条件：** 我将在这篇文章中简要介绍变换器，但你可以参考我关于该主题的专门文章以获取更多信息。'
- en: '[](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----c70c38e87491--------------------------------)
    [## Transformers — Intuitively and Exhaustively Explained'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----c70c38e87491--------------------------------)
    [## 变换器 — 直观而全面的解释'
- en: 'Exploring the modern wave of machine learning: taking apart the transformer
    step by step'
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索机器学习的现代浪潮：逐步拆解变换器
- en: towardsdatascience.com](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----c70c38e87491--------------------------------)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----c70c38e87491--------------------------------)
- en: A Brief Introduction to Transformers
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变换器简介
- en: Before we get into GPT I want to briefly go over the transformer. In its most
    basic sense, the transformer is an encoder-decoder style model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入 GPT 之前，我想简要介绍一下变换器。在最基本的意义上，变换器是一个编码器-解码器风格的模型。
- en: '![](../Images/4432448b3d91010af8bf9da54683b03c.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4432448b3d91010af8bf9da54683b03c.png)'
- en: A transformer working in a translation task. The input (I am a manager) is compressed
    to some abstract representation that encodes the meaning of the entire input.
    The decoder works recurrently, by feeding into itself, to construct the output.
    From [my article on transformers](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个在翻译任务中工作的变压器。输入（我是一名经理）被压缩成某种抽象表示，这种表示编码了整个输入的意义。解码器通过递归地将自身输入构建输出。来自[我关于变压器的文章](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
- en: The encoder converts an input into an abstract representation which the decoder
    uses to iteratively generate output.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器将输入转换为一种抽象表示，解码器使用这种表示来迭代生成输出。
- en: '![](../Images/bd4d304239e39c72240c88fd9ce84ec0.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd4d304239e39c72240c88fd9ce84ec0.png)'
- en: high level representation of how the output of the encoder relates to the decoder.
    the decoder references the encoded input for every recursive loop of the output.
    From [my article on transformers](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器输出与解码器的高层次表示。解码器在每次递归循环中都参考编码的输入。来自[我关于变压器的文章](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
- en: both the encoder and decoder employ an abstract representations of text which
    is created using multi headed self attention.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器都使用通过多头自注意力创建的文本抽象表示。
- en: '![](../Images/055a140c1f0692597fb98dfc81c1bfbb.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/055a140c1f0692597fb98dfc81c1bfbb.png)'
- en: Multi Headed self attention, in a nutshell. The mechanism mathematically combines
    the vectors for different words, creating a matrix which encodes a deeper meaning
    of the entire input. From [my article on transformers](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 多头自注意力，简而言之。该机制在数学上结合了不同单词的向量，创建了一个编码了整个输入更深层含义的矩阵。来自[我关于变压器的文章](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
- en: There’s a few steps which multiheaded self attention employs to construct this
    abstract representation. In a nutshell, a dense neural network constructs three
    representations, usually referred to as the query, key, and value, based on the
    input.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 多头自注意力在构建这种抽象表示时有几个步骤。简而言之，一个密集神经网络基于输入构建三个表示，通常称为查询、键和值。
- en: '![](../Images/51ee47ff765f5366ae85cb45804948f3.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51ee47ff765f5366ae85cb45804948f3.png)'
- en: Turning the embedded input into the query, key, and value. The query, key, and
    value all have the same dimensions as the input, and can be thought of as the
    input which has been passed through a filter. From [my article on transformers](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 将嵌入的输入转换为查询、键和值。查询、键和值都与输入具有相同的维度，可以被认为是经过过滤的输入。来自[我关于变压器的文章](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
- en: The query and key are multiplied together. Thus, some representation of every
    word is combined with a representation of every other word.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 查询和键被相乘。因此，每个单词的某种表示与每个其他单词的表示结合在一起。
- en: '![](../Images/ba6755db58b9ed709ff2cd18f7c8e03d.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba6755db58b9ed709ff2cd18f7c8e03d.png)'
- en: Calculating the attention matrix with the query and key. The attention matrix
    is then used, in combination with the value, to generate the final output of the
    attention mechanism. From [my article on transformers](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用查询和键计算注意力矩阵。然后将注意力矩阵与值结合，生成注意力机制的最终输出。来自[我关于变压器的文章](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
- en: The value is then multiplied by this abstract combination of the query and key,
    constructing the final output of multi headed self attention.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将值与查询和键的这种抽象组合相乘，构建多头自注意力的最终输出。
- en: '![](../Images/adf0f25a0b677c8c6977f565c7e99601.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adf0f25a0b677c8c6977f565c7e99601.png)'
- en: The attention matrix (which is the matrix multiplication of the query and key)
    multiplied by the value matrix to yield the final result of the attention mechanism.
    Because of the shape of the attention matrix, the result is the same shape as
    the value matrix. From [my article on transformers](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力矩阵（即查询和键的矩阵乘法）乘以值矩阵以产生注意力机制的最终结果。由于注意力矩阵的形状，结果与值矩阵的形状相同。来自[我关于变压器的文章](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
- en: The encoder uses multi-headed self attention to create abstract representations
    of the input, and the decoder uses multi-headed self attention to create abstract
    representations of the output.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器使用多头自注意力来创建输入的抽象表示，解码器使用多头自注意力来创建输出的抽象表示。
- en: '![](../Images/9ed71b9401cae258d706bbfd0103184a.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ed71b9401cae258d706bbfd0103184a.png)'
- en: The Transformer Architecture, with the encoder on the left and the decoder on
    the right. There’s a lot of details I skimmed over for brevity, but if you look
    at it from a high level it’s not too complicated. The encoder uses multi-headed
    self attention to encode the input, and the decoder uses multi-headed self attention
    twice; once to encode the previously constructed outputs, and once to combine
    the input representation with the previous outputs. image [source](https://arxiv.org/pdf/1706.03762v2.pdf)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器架构，左侧是编码器，右侧是解码器。为了简洁，我略过了很多细节，但从高层次来看，它并不复杂。编码器使用多头自注意力对输入进行编码，解码器使用多头自注意力两次；一次对之前构建的输出进行编码，一次将输入表示与之前的输出结合。图片[来源](https://arxiv.org/pdf/1706.03762v2.pdf)
- en: '![](../Images/4432448b3d91010af8bf9da54683b03c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4432448b3d91010af8bf9da54683b03c.png)'
- en: Recall that this is what the transformer is doing, in essence.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆一下，这就是变压器的本质。
- en: That was a super quick rundown on transformers. I tried to cover the high points
    without getting too in the weeds, feel free to refer to my [article on transformers](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
    for more information. Now that we vaguely understand the essentials we can start
    talking about GPT.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是对变压器的一个非常快速的概述。我尝试覆盖重点而不涉及过多细节，欢迎参考我的[变压器文章](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)以获取更多信息。现在我们大致理解了要点，可以开始讨论GPT了。
- en: GPT-1 (Released June 2018)
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-1（发布于2018年6月）
- en: 'The paper [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
    introduced the GPT style model. This is a fantastic paper, with a lot of cool
    details. We’ll summarize this paper into the following key concepts:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 论文[通过生成预训练提高语言理解](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)介绍了GPT风格的模型。这是一篇很棒的论文，包含许多有趣的细节。我们将总结这篇论文的以下关键概念：
- en: A decoder-only style architecture
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅解码器风格的架构
- en: Unsupervised pre-training
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 无监督预训练
- en: Supervised fine tuning
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有监督微调
- en: Task-specific input transformation
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任务特定的输入转换
- en: Let’s unpack each of these ideas one by one.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一拆解这些想法。
- en: Decoder Only Transformers
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 仅解码器变压器
- en: As we previously discussed, the transformer is an encoder-decoder style architecture.
    The encoder converts some input into an abstract representation, and the decoder
    iteratively generates the output.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，变压器是一种编码器-解码器风格的架构。编码器将输入转换为抽象表示，而解码器则迭代生成输出。
- en: '![](../Images/9ed71b9401cae258d706bbfd0103184a.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ed71b9401cae258d706bbfd0103184a.png)'
- en: The Transformer Architecture, with the encoder on the left and the decoder on
    the right. Image [source](https://arxiv.org/pdf/1706.03762v2.pdf)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器架构，左侧是编码器，右侧是解码器。图片[来源](https://arxiv.org/pdf/1706.03762v2.pdf)
- en: You might notice that both the encoder and the decoder are remarkably similar.
    Since the transformer was published back in 2017, researchers have played around
    with each of these subcomponents, and have found that both of them are phenomenally
    good at language representation. Models which use only an encoder, or only a decoder,
    have been popular ever since.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到编码器和解码器非常相似。自2017年发布以来，研究人员对这些子组件进行了多次实验，发现它们在语言表示方面都非常出色。仅使用编码器或仅使用解码器的模型从那时起就非常流行。
- en: Generally speaking, encoder-only style models are good at extracting information
    from text for tasks like classification and regression, while decoder-only style
    models focus on generating text. GPT, being a model focused on text generation,
    is a decoder only style model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，仅编码器风格的模型擅长从文本中提取信息，用于分类和回归等任务，而仅解码器风格的模型则专注于生成文本。GPT作为一个专注于文本生成的模型，是一种仅解码器风格的模型。
- en: '![](../Images/62b283a03f81874c3b020565d9e0df9c.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62b283a03f81874c3b020565d9e0df9c.png)'
- en: The model architecture of GPT-1, a decoder-only style model. [source](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-1的模型架构，采用仅解码器风格。 [source](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- en: The decoder-only style of model used in GPT has very similar components to the
    traditional transformer, but also some important and subtle distinctions. Let’s
    run through the key ideas of the architecture.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: GPT中使用的仅解码器风格模型与传统变换器有非常相似的组件，但也有一些重要的细微差别。我们来了解一下架构的关键思想。
- en: GPT-1 uses a text and position embedding, which converts a given input word
    into a vector which encodes both the words general meaning and the position of
    the word within the sequence.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-1使用文本和位置嵌入，将给定的输入词转换为一个向量，该向量同时编码词汇的整体含义和词在序列中的位置。
- en: '![](../Images/caff7020e503ff415e9d2d938a146fc3.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/caff7020e503ff415e9d2d938a146fc3.png)'
- en: A conceptual diagram of GPT’s input embedding. Each word has two vectors, one
    for the word and one for the location, which are added together to represent each
    word. These are used to construct the input to the model. [source](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: GPT输入嵌入的概念图。每个词有两个向量，一个表示词语本身，一个表示位置，这两个向量相加以表示每个词。这些向量用于构建模型的输入。 [source](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- en: Like the original transformer, GPT uses a “learned word embedding.” Essentially,
    a vector for each word in the vocabulary of the model is randomly initialized,
    then updated throughout model training.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始变换器类似，GPT使用“学习到的词嵌入”。本质上，模型词汇表中的每个词都有一个随机初始化的向量，然后在模型训练过程中不断更新。
- en: Unlike the original transformer, GPT uses a “learned positional encoding.” GPT
    learns a vector for each input location, which it adds to the learned vector embedding.
    This results in an embedding which contains information about the word, and where
    the word is in the sequence.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始变换器不同，GPT使用“学习到的位置信息编码。”GPT学习每个输入位置的向量，并将其添加到学习到的词嵌入向量中。这会生成一个包含词汇信息以及词在序列中位置的嵌入。
- en: These embedded words, with positional information, get passed through masked
    multi-headed self attention. For the purposes of this article we’ll stick with
    the simplification that this mechanism combines every word vector with every other
    word vector to create an abstract matrix encoding the whole input in some meaningful
    way.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 带有位置信息的嵌入词通过掩码多头自注意力机制传递。为了简化说明，我们将假设这个机制将每个词向量与每个其他词向量结合，形成一个抽象矩阵，以某种有意义的方式编码整个输入。
- en: '![](../Images/41f2cc815b20a269864c806b24689d3c.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41f2cc815b20a269864c806b24689d3c.png)'
- en: Multi Headed self attention, in a nutshell. The mechanism mathematically combines
    the vectors for different words, creating a matrix which encodes a deeper meaning
    of the entire input. From [my article on transformers](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
    and [source](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 多头自注意力机制，简而言之。该机制在数学上将不同词的向量结合，创建一个编码整个输入深层含义的矩阵。来自 [我的变换器文章](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
    和 [source](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- en: A lot of math happens in self attention, which could result in super big or
    super small values. This has a tendency to make models perform poorly, so all
    the values are squashed down into a reasonable range with layer normalization.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制中会发生大量的数学运算，这可能导致超大或超小的值。这往往会使模型表现不佳，因此所有的值都通过层归一化被压缩到一个合理的范围内。
- en: '![](../Images/2dce43f9f6975106aac078bf84751776.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2dce43f9f6975106aac078bf84751776.png)'
- en: Layer Normalization squashes numbers into a reasonable output, which is important
    in machine learning for models to learn effectively. [source](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化将数字压缩到合理的输出，这在机器学习中对模型有效学习非常重要。[source](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- en: The data is then passed through a dense network (your classic neural network)
    then passed through another layer normalization. This all happens in several decoder
    blocks which are stacked on top of eachother, allowing the GPT model to do some
    pretty complex manipulations to the input text.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 数据然后通过一个密集网络（你的经典神经网络）传递，然后通过另一个层归一化。所有这些都发生在几个堆叠在一起的解码器块中，使GPT模型能够对输入文本进行一些相当复杂的操作。
- en: '![](../Images/fd4fae7ac5a14583a38dd84dadc2ec4f.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd4fae7ac5a14583a38dd84dadc2ec4f.png)'
- en: The entire GPT-1 model. 12 decoder layers stacked on top of each other. [source](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 整个GPT-1模型。12层解码器堆叠在一起。[source](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- en: I wanted to take a moment to describe how decoder style models actually go about
    making inferences; a topic which, for whatever reason, a lot of people don’t seem
    to take the time to explain. The typical encoder-decoder style transformer encodes
    the entire input, then recurrently constructs the output.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我想花点时间描述解码器风格模型实际上如何进行推断；这是一个出于某种原因，很多人似乎不愿意花时间解释的话题。典型的编码器-解码器风格变换器对整个输入进行编码，然后递归地构建输出。
- en: '![](../Images/df14d38b4ec854c7339ac1b31e787102.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df14d38b4ec854c7339ac1b31e787102.png)'
- en: The traditional encoder-decoder style transformer in action. The encoder compresses
    the input into an abstract representation, then the decoder uses previously predicted
    outputs to guess the next word in the output sequence. Thus, the transformer iteratively
    constructing a response.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的编码器-解码器风格变换器的运作。编码器将输入压缩为抽象表示，然后解码器使用先前预测的输出来猜测输出序列中的下一个词。因此，变换器迭代地构建响应。
- en: Decoder only transformers, like GPT, don’t have an encoder to work with. Instead,
    they simply concatenate the previous output to the input sequence and pass the
    entire input and all previous outputs for every inference, a style referred to
    in the literature as “autoregressive generation”.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 像GPT这样的仅解码器变换器没有编码器可用。相反，它们只是将先前的输出连接到输入序列，并将整个输入和所有先前的输出传递给每次推断，这种风格在文献中被称为“自回归生成”。
- en: '![](../Images/4d7b7f7bd71cc624c21bf8c57d38cd90.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d7b7f7bd71cc624c21bf8c57d38cd90.png)'
- en: A decoder only style model, like GPT, iteratively constructing an output with
    autoregressive generation.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 像GPT这样的仅解码器模型，通过自回归生成迭代地构建输出。
- en: This idea of treating outputs similarly to inputs is critical in one of the
    main deviations GPT took from transformers to reach cutting edge performance.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这种将输出处理类似于输入的想法在GPT从变换器中脱离以达到前沿性能的主要偏离之一中至关重要。
- en: Semi-Supervised Pre-Training, Then Supervised Fine Tuning
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 半监督预训练，然后监督微调
- en: If you research GPT, or language modeling as a whole, you might find the term
    “language modeling objective.” This term refers to the act of predicting the next
    word given an input sequence, which, essentially models textual language. The
    idea is, if you can get really really good at predicting the next word in a sequence
    of text, in theory you can keep predicting the next word over and over until you’ve
    output a whole book.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你研究GPT或整体语言建模，可能会遇到“语言建模目标”这个术语。这个术语指的是在给定输入序列的情况下预测下一个词的行为，这本质上是对文本语言的建模。其想法是，如果你能非常非常擅长预测文本序列中的下一个词，理论上你可以不断预测下一个词，直到你输出了整本书。
- en: Because the traditional transformer requires the concept of an explicit “input”
    and “output” (an input for the encoder and an output from the decoder), the idea
    of next word prediction doesn’t really make a tone of sense. Decoder models, like
    GPT, *only* do next word prediction, so the idea of training them on language
    modeling makes a tone of sense.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由于传统变换器需要明确的“输入”和“输出”（编码器的输入和解码器的输出）的概念，下一个词预测的想法实际上并没有太多意义。像GPT这样的解码器模型*仅*进行下一个词预测，因此对它们进行语言建模训练是非常合理的。
- en: '![](../Images/291d3fab63f0be88da5d32b607cbcc09.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/291d3fab63f0be88da5d32b607cbcc09.png)'
- en: An example of the data a model might see when being trained on language modeling.
    The model is provided some intput (in red) and asked to predict the next word
    (in blue). Of course, for a model like GPT, this data would be extracted from
    large corpus of documents.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在进行语言建模训练时可能看到的数据示例。模型会提供一些输入（用红色标记），并要求预测下一个词（用蓝色标记）。当然，对于像 GPT 这样的模型，这些数据将从大量的文档语料库中提取。
- en: This opens up a tone of options in terms of training strategies. I talk a bit
    about pre-training and fine tuning in my [article on LoRA](https://medium.com/towards-data-science/lora-intuitively-and-exhaustively-explained-e944a6bff46b).
    In a nutshell, pre-training is the idea of training on a bulk dataset to get a
    general understanding of some domain, then that model can be fine tuned on a specific
    task.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这为训练策略开辟了大量选项。我在我的[LoRA文章](https://medium.com/towards-data-science/lora-intuitively-and-exhaustively-explained-e944a6bff46b)中简要讨论了预训练和微调。简而言之，预训练是指在一个大数据集上进行训练以获得对某个领域的整体理解，然后可以在具体任务上对该模型进行微调。
- en: '![](../Images/24e3be015597ecbc43818286e6fc85fb.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24e3be015597ecbc43818286e6fc85fb.png)'
- en: A diagram depicting what pre-training and fine tuning might look like. A language
    model might be trained on bulk data to understand language, then be fine tuned
    on a specific task. From my [article on LoRA](https://medium.com/towards-data-science/lora-intuitively-and-exhaustively-explained-e944a6bff46b)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一个图示，展示了预训练和微调可能是什么样的。语言模型可能在大数据上进行训练以理解语言，然后在具体任务上进行微调。来自我的[LoRA文章](https://medium.com/towards-data-science/lora-intuitively-and-exhaustively-explained-e944a6bff46b)
- en: GPT is an abbreviation of “Generative Pre-Trained Transformer” for a reason.
    GPT is pre-trained on a vast amount of text using language modeling (next word
    prediction). It essentially learns “given an input sequence X, the next word should
    be Y.”
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 是“生成预训练变换器”的缩写。GPT 在大量文本上通过语言建模（下一个词预测）进行预训练。它基本上学习“给定一个输入序列 X，下一个词应该是 Y。”
- en: 'This form of training falls under the broader umbrella of “semi-supervision”.
    Here’s a quote from [another article](/self-supervised-learning-using-projection-heads-b77af3911d33)
    which highlights how semi-supervised learning deviates from other training strategies:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这种形式的训练属于“半监督”的更广泛范畴。以下是[另一篇文章](/self-supervised-learning-using-projection-heads-b77af3911d33)中的一句话，突出了半监督学习与其他训练策略的不同：
- en: '**Supervised Learning** is the process of training a model based on **labeled**
    information. When training a model to predict if images contain cats or dogs,
    for instance, one curates a set of images which are labeled as having a cat or
    a dog, then trains the model (using [gradient descent](https://medium.com/@danielwarfield1/what-are-gradients-and-why-do-they-explode-add23264d24b))
    to understand the difference between images with cats and dogs.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**是基于**标记**信息训练模型的过程。例如，当训练一个模型来预测图像中是否包含猫或狗时，首先需要策划一组标记为猫或狗的图像，然后训练模型（使用[梯度下降法](https://medium.com/@danielwarfield1/what-are-gradients-and-why-do-they-explode-add23264d24b)）以理解猫和狗图像之间的差异。'
- en: '**Unsupervised Learning** is the process of giving some sort of model **unlabeled**
    information, and extracting useful inferences through some sort of transformation
    of the data. A classic example of unsupervised learning is clustering; where groups
    of information are extracted from un-grouped data based on local position.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**是将某种模型用于**未标记**的信息，并通过某种数据变换提取有用的推论。无监督学习的一个经典例子是聚类；它从未分组的数据中根据局部位置提取信息组。'
- en: Self-supervised learning is somewhere in between. **Self-supervision uses**
    **labels** **that are generated programmatically, not by humans.** In some ways
    it’s supervised because the model learns from labeled data, but in other ways
    it’s unsupervised because no labels are provided to the training algorithm. Hence
    self-supervised.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习介于两者之间。**自监督使用** **程序生成的** **标签，而不是由人工生成。** 在某些方面它是监督的，因为模型从标记的数据中学习，但在其他方面它是无监督的，因为训练算法没有提供标签。因此称为自监督。
- en: Using a semi-supervised approach, in the form of language modeling, allows GPT
    to be trained on a previously unprecedented volume of training data, allowing
    the model to create robust linguistic representations. This model, with a firm
    understanding of language, can then be fine-tuned on more specific datasets for
    more specific tasks.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用半监督的方法，通过语言建模的形式，使得 GPT 能够在前所未有的大量训练数据上进行训练，从而使模型能够创建强大的语言表示。这个模型在对语言有了扎实理解之后，可以在更具体的数据集上进行微调，以应对更具体的任务。
- en: We use the BooksCorpus dataset [71] for training the language model. It contains
    over 7,000 unique unpublished books from a variety of genres including Adventure,
    Fantasy, and Romance. — [The GPT paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
    on pre-training
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们使用 BooksCorpus 数据集[71]来训练语言模型。它包含了来自各种类型的超过7000本独特的未出版书籍，包括冒险、奇幻和浪漫等。— [GPT论文](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)关于预训练
- en: ''
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We perform experiments on a variety of supervised tasks including natural language
    inference, question answering, semantic similarity, and text classification. —
    [The GPT paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
    on fine tuning
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们在各种监督任务上进行实验，包括自然语言推理、问答、语义相似性和文本分类。— [GPT论文](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)关于微调
- en: Task-Specific Input Transformation
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务特定输入转换
- en: Language models, like GPT, are currently known to be incredibly powerful “in
    context learners”; you can give them information in their input as context, then
    they can use that context to construct better responses. I used this concept in
    my [article on retreival augmented generation](https://medium.com/towards-data-science/retrieval-augmented-generation-intuitively-and-exhaustively-explain-6a39d6fe6fc9),
    my [article on visual question answering](https://medium.com/towards-data-science/visual-question-answering-with-frozen-large-language-models-353d42791054),
    and my [article on parsing the output from language models](https://medium.com/towards-data-science/conversations-as-directed-graphs-with-lang-chain-46d70e1a846c).
    It’s a pretty powerful concept.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型，如 GPT，目前被认为是非常强大的“上下文学习者”；你可以在输入中提供信息作为上下文，然后它们可以利用这些上下文构建更好的回应。我在我的[增强生成的文章](https://medium.com/towards-data-science/retrieval-augmented-generation-intuitively-and-exhaustively-explain-6a39d6fe6fc9)、我的[视觉问答的文章](https://medium.com/towards-data-science/visual-question-answering-with-frozen-large-language-models-353d42791054)和我的[解析语言模型输出的文章](https://medium.com/towards-data-science/conversations-as-directed-graphs-with-lang-chain-46d70e1a846c)中使用了这一概念。这是一个相当强大的概念。
- en: '![](../Images/e689a3ff3fd290d6998a23a996211dfa.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e689a3ff3fd290d6998a23a996211dfa.png)'
- en: An example of ChatGPT succeeding at a task with the help of in context learning.
    From my [RAG article](https://medium.com/towards-data-science/retrieval-augmented-generation-intuitively-and-exhaustively-explain-6a39d6fe6fc9)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 在上下文学习的帮助下成功完成任务的一个例子。来自我的[RAG文章](https://medium.com/towards-data-science/retrieval-augmented-generation-intuitively-and-exhaustively-explain-6a39d6fe6fc9)
- en: At the time of the first GPT paper in context learning was not nearly so well
    understood. The paper dedicates an entire section to “Task-specific input transformations”.
    Basically, instead of adding special components to the model to make it work for
    a specific task, the authors of GPT opted to format those tasks textually.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次 GPT 论文发表时，上下文学习并没有被很好地理解。论文专门为“任务特定输入转换”分配了一个部分。基本上，GPT 的作者选择将这些任务以文本形式格式化，而不是向模型中添加特殊组件以使其适应特定任务。
- en: As an example, for textual entailment (the process of predicting if a piece
    of text directly relates with another piece of text) the GPT paper simply concatenated
    both pieces of text together, with a dollar sign in between. This allowed them
    to fine tune GPT on textual entailment without adding any new parameters to the
    model. Other objectives, like text similarity, question answering, and commonsense
    reasoning, were all fine tuned in a similar way.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于文本蕴涵（预测一段文本是否与另一段文本直接相关的过程），GPT论文简单地将这两段文本拼接在一起，中间用美元符号分隔。这使他们能够在文本蕴涵上微调
    GPT，而无需向模型中添加任何新参数。其他目标，如文本相似性、问答和常识推理，也以类似的方式进行微调。
- en: GPT-2 (Released February 2019)
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-2（发布于2019年2月）
- en: 'The paper [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
    introduced the world to GPT-2, which is essentially identical to GPT-1 save two
    key differences:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 论文[语言模型是无监督多任务学习者](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)向世界介绍了
    GPT-2，它本质上与 GPT-1 相同，只有两个关键区别：
- en: GPT-2 is way bigger than GPT-1
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPT-2 远大于 GPT-1
- en: GPT-2 doesn’t use any fine tuning, only pre-training
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPT-2 不使用任何微调，仅使用预训练
- en: Also, as a brief note, the GPT-2 architecture is ever so slightly different
    from the GPT-1 architecture. In GPT-1 each block consists of `[Attention, Norm,
    Feed Forward, Norm]` , where GPT-2 consists of `[Norm, Attention, Norm, Feed Forward]`.
    However, this difference is so minor it’s hardly worth mentioning.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，简要说明一下，GPT-2 架构与 GPT-1 架构略有不同。在 GPT-1 中，每个块由 `[Attention, Norm, Feed Forward,
    Norm]` 组成，而 GPT-2 由 `[Norm, Attention, Norm, Feed Forward]` 组成。然而，这种差异微乎其微，几乎不值得一提。
- en: Bigger is Better
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更大更好
- en: '![](../Images/32b61edd83117e0746d7c016bbde6250.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32b61edd83117e0746d7c016bbde6250.png)'
- en: The performance of GPT-2 on several objectives, as a function of parameter count.
    As can be seen, the more parameters a language model is, the better it generally
    performs. From the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 在几个目标上的表现，作为参数数量的函数。如图所示，语言模型的参数越多，通常表现越好。来自 [GPT-2 论文](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- en: 'One of the findings of the GPT-2 paper is that larger models are better. Specifically,
    they theorize that language model performance scales “log-linearly”, so something
    like this:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 论文的发现之一是更大的模型表现更好。具体来说，他们理论化语言模型性能呈“对数线性”扩展，如下图所示：
- en: '![](../Images/8734649b954ad678df9f89b2f7cdc520.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8734649b954ad678df9f89b2f7cdc520.png)'
- en: The relationship between how good a model is, and how big it is, seems to be
    log-linear, based on the findings of GPT-2
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的好坏与其大小之间的关系，似乎是对数线性的，这基于 GPT-2 的发现
- en: As a result GPT-2 contains 1.5 billion parameters, which is roughly ten times
    larger than GPT-1\. GPT-2 was also trained on roughly ten times the data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 结果 GPT-2 包含 15 亿个参数，这大约是 GPT-1 的十倍。GPT-2 还在大约十倍的数据上进行训练。
- en: Language Understanding is General
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言理解是通用的
- en: In GPT-1 they focused on using the language modeling objective to create a solid
    baseline of textual understanding, and then used that baseline to fine tune models
    for specific tasks. In GPT-2 they got rid of fine tuning completely, operating
    under the assumption that a sufficiently pre-trained model can perform well on
    specific problems without being explicitly trained on them.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPT-1 中，他们专注于使用语言建模目标创建坚实的文本理解基准，然后利用该基准对模型进行微调，以适应特定任务。在 GPT-2 中，他们完全去除了微调，假设一个足够预训练的模型可以在没有明确训练的情况下，在特定问题上表现良好。
- en: Our suspicion is that the prevalence of single task training on single domain
    datasets is a major contributor to the lack of generalization observed in current
    systems. Progress towards robust systems with current architectures is likely
    to require training and measuring performance on a wide range of domains and tasks…
  id: totrans-115
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们的怀疑是，单任务训练在单一领域数据集上的普遍性是当前系统缺乏通用性的主要原因。要实现当前架构下的鲁棒系统，可能需要在广泛的领域和任务上进行训练和性能测量…
- en: ''
  id: totrans-116
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When conditioned on a document plus questions, the answers generated by the
    language model reach 55 F1 on the CoQA dataset — matching or exceeding the performance
    of 3 out of 4 baseline systems without using the 127,000+ training examples…
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当条件是文档加问题时，语言模型生成的答案在 CoQA 数据集上达到了 55 F1 —— 与 4 个基准系统中的 3 个系统的表现相匹配或超越，而无需使用
    127,000+ 的训练样本…
- en: ''
  id: totrans-118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state
    of the art results on 7 out of 8 tested language modeling datasets in a zero-shot
    setting — [The GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
  id: totrans-119
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们最大的模型 GPT-2 是一个 15 亿参数的 Transformer，在 8 个测试语言建模数据集中中有 7 个达到了最先进的结果，在零-shot
    设置下 — [GPT-2 论文](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- en: They achieved this generalized performance by using a super big model and feeding
    it a whole bunch of high quality data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 他们通过使用一个超级大的模型，并为其提供大量高质量数据，实现了这种通用性能。
- en: Our approach motivates building as large and diverse a dataset as possible in
    order to collect natural language demonstrations of tasks in as varied of domains
    and contexts as possible…
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们的方法激励着建立尽可能大且多样化的数据集，以收集尽可能多领域和上下文的自然语言任务示例…
- en: ''
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: we created a new web scrape which emphasizes document quality. To do this we
    only scraped web pages which have been curated/filtered by humans. Manually filtering
    a full web scrape would be exceptionally expensive so as a starting point, we
    scraped all outbound links from Reddit, a social media platform, which received
    at least 3 karma. This can be thought of as a heuristic indicator for whether
    other users found the link interesting, educational, or just funny. — [The GPT-2
    paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们创建了一个新的网页抓取，强调文档质量。为此，我们只抓取了经过人工策划/过滤的网页。手动过滤整个网页抓取会异常昂贵，所以作为起点，我们抓取了所有从Reddit这个社交媒体平台中获得至少3点karma的外部链接。这可以被视为其他用户是否认为该链接有趣、教育性或仅仅有趣的启发式指标。
    — [GPT-2 论文](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- en: GPT 3 (Released June 2020)
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT 3（发布于2020年6月）
- en: The paper [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)
    introduced the world to GPT-3, which essentially said “bigger was good, so why
    not even bigger?” Instead of a measly 1.5 billion parameters in GPT-2, GPT-3 employs
    175 billion parameters, and was trained on 45 Terabytes of text data.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 论文[语言模型是少样本学习者](https://arxiv.org/pdf/2005.14165.pdf)向世界介绍了GPT-3，实际上它说的是“更大更好，那为什么不再更大呢？”GPT-2仅有的15亿参数相比，GPT-3使用了1750亿参数，并且在45TB的文本数据上进行了训练。
- en: '![](../Images/ae7e2df876bb511aac2d20674fec6d7c.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae7e2df876bb511aac2d20674fec6d7c.png)'
- en: A graph of model size vs performance. From the [GPT-3 paper](https://arxiv.org/pdf/2005.14165.pdf)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型大小与性能的图表。来自于[GPT-3 论文](https://arxiv.org/pdf/2005.14165.pdf)。
- en: From a data science perspective not much really changed. Similar model, similar
    core architecture, similar language modeling objective. From an engineering perspective,
    however, things were certainly different. GPT-2 is roughly 5 Gigabytes in size,
    whereas GPT-3 is around 800 Gigabytes (I’ve seen a lot of variation in this number
    online, but it’s definitely big). The Nvidia H100 GPU, which retails at a cool
    $30,000, holds 80GB of VRAM. Keep in mind, to train a machine learning model you
    essentially need double the model size in VRAM to hold the parameters as well
    as the gradients (as I discussed in my [LoRA article](https://medium.com/towards-data-science/lora-intuitively-and-exhaustively-explained-e944a6bff46b)).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据科学的角度来看，变化不大。模型相似，核心架构相似，语言建模目标相似。然而，从工程学的角度来看，情况显然不同。GPT-2的大小约为5GB，而GPT-3约为800GB（我在网上看到的这个数字有很多变化，但肯定很大）。售价30000美元的Nvidia
    H100 GPU拥有80GB的VRAM。请记住，要训练一个机器学习模型，你实际上需要双倍于模型大小的VRAM来存储参数和梯度（正如我在我的[LoRA 文章](https://medium.com/towards-data-science/lora-intuitively-and-exhaustively-explained-e944a6bff46b)中所讨论的）。
- en: '![](../Images/247fb6f7e4b422a3b39b2edf476c57d0.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/247fb6f7e4b422a3b39b2edf476c57d0.png)'
- en: A table from the [GPT-3 paper](https://arxiv.org/pdf/2005.14165.pdf), stating
    the accuracy of humans in predicting if a news article was written by GPT-3 or
    not. As you can see, for the full sized 175B parameter model, only 52% of people
    guessed correctly. 50% would be 50/50 shot in the dark.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[GPT-3 论文](https://arxiv.org/pdf/2005.14165.pdf)的一个表格，显示了人们在预测新闻文章是否由GPT-3编写的准确率。正如你所见，对于完整的175B参数模型，只有52%的人猜对了。50%就是50/50的瞎猜。
- en: I’ve seen actual cost estimations for GPT-3 which deviate wildly, mostly as
    a result of how much VRAM the model actually required to train. One thing is certain;
    you need a cluster of super expensive computers, working in parallel, to train
    a model like GPT-3, and you need a similar cluster to serve that model to users.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我见过对GPT-3的实际成本估算存在很大偏差，这主要是由于模型训练所需的VRAM量不同。可以确定的是；训练像GPT-3这样的模型需要一群超昂贵的计算机并行工作，而且需要类似的集群来为用户提供该模型。
- en: The GPT-3 paper is titled “Language Models are Few-Shot Learners.” By “Few-Shot
    Learners” the authors mean that GPT-3 can get pretty good at most tasks without
    training the model. Instead, you can just experiment with the input to the model
    to find the right input format for a particular task. And thus, prompt engineering
    was born.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 论文的标题是“语言模型是少样本学习者。”作者所说的“少样本学习者”是指GPT-3可以在没有训练模型的情况下在大多数任务上表现得相当好。相反，你可以通过实验输入模型的输入格式来找到特定任务的正确输入格式。因此，提示工程应运而生。
- en: GPT-4 (Released March 2023)
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-4（发布于2023年3月）
- en: At this point clarity ends. OpenAI has shifted from a research non-profit to
    a seriously heavy hitting corporate machine, facing off against big names like
    Microsoft and Google. There is no peer reviewed and transparent paper on GPT-4\.
    As you might know, OpenAI has been experiencing a bit of friction internally,
    this is part of the reason why.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，清晰度结束了。OpenAI 已经从一个研究型非营利组织转变为一个真正的重型企业，与微软和谷歌等大名鼎鼎的公司抗衡。关于 GPT-4 的论文没有经过同行评审和透明发布。正如你可能知道的，OpenAI
    内部经历了一些摩擦，这也是部分原因。
- en: That said, is there information? Absolutely.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，有信息吗？绝对有。
- en: GPT-4 is “Safe” To Use in Products
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-4 在产品中使用是“安全的”
- en: First of all, we can glean a lot of information from the [100 page technical
    report](https://arxiv.org/pdf/2303.08774.pdf) by OpenAI, which hides a lot of
    the specifics in terms of modeling, but exhaustively details performance and capability
    of the model. This paper reads a lot more like an advertisement than anything
    seriously academic; the term “safety”, for instance, has a much stronger bias
    towards safety to use within a product rather than the traditional usage of the
    word in research which focuses on safety for humans.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以从 OpenAI 的 [100 页技术报告](https://arxiv.org/pdf/2303.08774.pdf) 中获得大量信息，该报告隐藏了许多建模方面的具体细节，但详尽地描述了模型的性能和能力。这篇论文读起来更像是一种广告，而不是严肃的学术作品；例如，“安全”一词的使用更倾向于在产品中使用的安全性，而不是传统研究中关注人类安全的使用。
- en: GPT-4 Can Understand Images
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-4 可以理解图像
- en: One of the big changes in GPT-4 is “multi-modality”; allowing GPT-4 to converse
    about both images and text.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 的一个重大变化是“多模态”；允许 GPT-4 讨论图像和文本。
- en: '![](../Images/9b0113700ca80d01837e9fcd814135c3.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b0113700ca80d01837e9fcd814135c3.png)'
- en: GPT-4 conversing about an image. [Source](https://arxiv.org/pdf/2303.08774.pdf)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 讨论图像。 [来源](https://arxiv.org/pdf/2303.08774.pdf)
- en: This has all sorts of cool uses. In a pretty famous example, OpenAI demoed how
    this functionality could be used to turn a crude sketch into a (still fairly crude)
    website written in HTML, CSS, and JavaScript.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这有各种酷炫的用途。在一个相当著名的例子中，OpenAI 演示了如何将这种功能用于将粗略的草图转换为用 HTML、CSS 和 JavaScript 编写的（仍然相当粗糙的）网站。
- en: GPT-4 Is Fine Tuned on Human Feedback
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-4 在人类反馈下进行了微调
- en: Also, GPT-4 uses “Reinforcement Learning with Human Feedback”, commonly abbreviated
    as RLHF. You know those little thumbs up and thumbs down on the ChatGPT website?
    Those allow OpenAI to further improve the model based on in-context human feedback.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，GPT-4 使用了“基于人类反馈的强化学习”，通常缩写为 RLHF。你知道 ChatGPT 网站上的那些小拇指点赞和点踩吗？这些允许 OpenAI
    根据上下文中的人类反馈进一步改进模型。
- en: '![](../Images/69c862ec441682296957f375c3b7b214.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69c862ec441682296957f375c3b7b214.png)'
- en: This strategy of learning can have dubious results, and can actually degrade
    the models performance. However, it does help with getting GPT-4 into a better
    style as a sort of “AI Helper”
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这种学习策略可能会有可疑的结果，实际上可能会降低模型的性能。然而，它确实有助于将 GPT-4 以某种“AI助手”的风格提升到更好的水平。
- en: Our evaluations suggest RLHF does not significantly affect the base GPT-4 model’s
    capability — [GPT-4 Technical Report](https://arxiv.org/pdf/2303.08774.pdf)
  id: totrans-147
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们的评估表明，RLHF 不会显著影响基础 GPT-4 模型的能力 — [GPT-4 技术报告](https://arxiv.org/pdf/2303.08774.pdf)
- en: GPT-4 is Really Big
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-4 实在是太大了
- en: There was an [infamous leak](https://archive.is/2RQ8X#selection-833.1-873.202)
    about four months ago which seemed to have drop some credible evidence about some
    of the behind the scenes details on GPT-4\. This information was released behind
    a paywall, which was then leaked again on Twitter, which is now… X? I guess?
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 大约四个月前有一个 [臭名昭著的泄密事件](https://archive.is/2RQ8X#selection-833.1-873.202)，似乎透露了一些关于
    GPT-4 的幕后细节的可信证据。这些信息在一个付费墙后发布，随后在 Twitter 上被泄露，现在是… X？我猜？
- en: Anyway, GPT-4 seems like it hits the 1.8 Trillion parameter mark, which is around
    ten times the size of GPT-3.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，GPT-4 似乎达到了 1.8 万亿参数的规模，大约是 GPT-3 的十倍。
- en: GPT-4 uses Mixture of Experts
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-4 使用专家混合
- en: I plan on covering Mixture of Experts (MOE) in a future article. In a nutshell
    MOE is the process of dividing a large model into smaller sub models, and creating
    a tiny model which serves to route data to individual components.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我计划在未来的文章中讨论专家混合（MOE）。简而言之，MOE 是将大型模型划分为较小子模型的过程，并创建一个小型模型来将数据路由到各个组件。
- en: The whole idea is, the bigger your model gets, the more costly it becomes to
    run. If instead you can have a portion of your model dedicated to art, another
    dedicated to science, and another to telling jokes, you can route queries to individual
    sub-components. In reality these groupings are much more abstract, but the logic
    is the same; the model can use different sub-components on every query, allowing
    it to run much leaner than if it were to use all parameters for every query.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 整体思路是，模型越大，运行成本越高。如果你能将模型的一部分专门用于艺术，另一部分用于科学，再另一部分用于讲笑话，你可以将查询路由到各个子组件。实际上这些分组更为抽象，但逻辑是相同的；模型可以在每次查询时使用不同的子组件，使其运行比每次查询都使用所有参数要更精简。
- en: Based on the [GPT-4 leak](https://archive.is/2RQ8X#selection-833.1-873.202),
    by using MOE, the model only needs to use 280 billion parameters of the models
    1.8 Trillion (a mere 15%) on any given inference.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 基于[GPT-4泄露信息](https://archive.is/2RQ8X#selection-833.1-873.202)，通过使用MOE，该模型在任何给定推理中只需使用2800亿参数中的15%（总共1.8万亿中的一部分）。
- en: GPT-4 uses 16 experts (the model is divided into 16 parts which are then routed
    to). This is much less than the current state of research, which suggests larger
    sets of 64 or 128 experts.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4使用16个专家（模型被分成16个部分然后路由到这些部分）。这比当前研究的状态要少，当前研究建议使用64个或128个专家的大规模集合。
- en: GPT-4 Training Cost
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-4训练成本
- en: Based on the [GPT-4 leak](https://archive.is/2RQ8X#selection-833.1-873.202),
    the model cost around $63 million to train on A100 GPUs. Using more modern H100
    GPUs would probably cost around $21.5 million. Such is the cost of being an early
    mover, I guess.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 基于[GPT-4泄露信息](https://archive.is/2RQ8X#selection-833.1-873.202)，训练该模型在A100 GPU上大约花费了6300万美元。使用更现代的H100
    GPU则可能花费约2150万美元。这就是成为早期采用者的代价，我想。
- en: GPT-4 Uses Some Bells and Whistles
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-4使用了一些附加功能
- en: GPT-4 uses “multi-query attention”, which is a style of attention which improves
    model throughput. I’ll be covering it in an upcoming article.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4使用“多查询注意力”，这是一种提升模型吞吐量的注意力风格。我将在即将发表的文章中讨论它。
- en: GPT-4 uses “continuous batching” which I will probably cover in yet another
    article.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4使用“连续批处理”，我可能会在另一篇文章中详细介绍。
- en: GPT-4 uses “speculative decoding”. My goodness, the pace of AI research is keeping
    me busy.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4使用了“推测解码”。天哪，AI研究的进展让我忙得不可开交。
- en: GPT-4 Probably Uses Something Like Flamingo
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-4可能使用了类似Flamingo的方法
- en: 'GPT-4 uses a style of multimodal modeling similar to “Flamingo”, a multimodal
    model by Deepmind. I’ll cover Flamingo specifically in the future, but I do have
    an article on a similar approach by the SalesForce research team, called Q-Fromer:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4使用了一种类似于Deepmind的多模态模型“Flamingo”的建模风格。我会在未来专门讨论Flamingo，但我确实有一篇关于SalesForce研究团队类似方法的文章，名为Q-Fromer：
- en: '[](/visual-question-answering-with-frozen-large-language-models-353d42791054?source=post_page-----c70c38e87491--------------------------------)
    [## Visual Question Answering with Frozen Large Language Models'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 视觉问答与冻结的大型语言模型](https://towardsdatascience.com/visual-question-answering-with-frozen-large-language-models-353d42791054?source=post_page-----c70c38e87491--------------------------------)'
- en: Talking with LLMs about images, without training LLMs on images.
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与LLM谈论图像，而不需要对LLM进行图像训练。
- en: towardsdatascience.com](/visual-question-answering-with-frozen-large-language-models-353d42791054?source=post_page-----c70c38e87491--------------------------------)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[视觉问答与冻结的大型语言模型](https://towardsdatascience.com/visual-question-answering-with-frozen-large-language-models-353d42791054?source=post_page-----c70c38e87491--------------------------------)'
- en: In a nutshell, both Q-Former and Flamingo use frozen models for both language
    and images, and trains some glue that joins them together to create a model which
    can talk about images.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，Q-Former和Flamingo都使用冻结的语言和图像模型，并训练一些将它们结合起来的“胶水”，以创建一个可以谈论图像的模型。
- en: GPT-4 Uses High Quality Data
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-4使用高质量数据
- en: It seems like, based on the [GPT-4 leak](https://archive.is/2RQ8X#selection-833.1-873.202),
    OpenAI used college textbooks to improve the model’s knowledge in numerous intellectual
    domains. The legality and ethicality of this is definitely in a gray area, which
    might also explain some of the current in-fighting and secrecy at OpenAI.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来，根据[GPT-4泄露信息](https://archive.is/2RQ8X#selection-833.1-873.202)，OpenAI使用了大学教科书来提高模型在多个智力领域的知识。这在法律和伦理上确实是一个灰色地带，这也可能解释了OpenAI目前的一些内部争斗和保密。
- en: Conclusion
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Well, that got kind of dramatic in the end, huh? First we briefly went over
    transformers, then described how GPT-1 adopted the decoder component of the transformer
    to improve text generation through self-supervised pre-training using the language
    modeling objective. We then discussed how GPT-2 and GPT-3 both doubled down on
    this approach, producing language models which were, to a large degree, Identical
    to GPT-1 just way bigger. We then discussed what we know of GPT-4; the model size,
    some architectural changes, and even some saucy controversy.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，最后有点戏剧化了，是吧？我们首先简要回顾了变换器，然后描述了 GPT-1 如何采用变换器的解码器组件，通过自监督预训练改进文本生成。接着我们讨论了
    GPT-2 和 GPT-3 如何在此基础上进一步发展，生成了与 GPT-1 大致相同但规模更大的语言模型。然后我们讨论了我们对 GPT-4 的了解；模型大小，一些架构变化，甚至一些有趣的争议。
- en: Follow For More!
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关注获取更多内容！
- en: I describe papers and concepts in the ML space, with an emphasis on practical
    and intuitive explanations.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我描述了机器学习领域的论文和概念，重点是实用和直观的解释。
- en: '[](https://medium.com/@danielwarfield1/subscribe?source=post_page-----c70c38e87491--------------------------------)
    [## Get an email whenever Daniel Warfield publishes'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@danielwarfield1/subscribe?source=post_page-----c70c38e87491--------------------------------)
    [## 每当 Daniel Warfield 发布文章时，获取邮件通知'
- en: Get an email whenever Daniel Warfield publishes By signing up, you will create
    a Medium account if you don't already…
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每当 Daniel Warfield 发布文章时，获取邮件通知。通过注册，如果你还没有 Medium 账户，你将创建一个账户…
- en: medium.com](https://medium.com/@danielwarfield1/subscribe?source=post_page-----c70c38e87491--------------------------------)
    [![](../Images/1f6f4c8a07d69cf53e055e0130a85b03.png)](https://www.buymeacoffee.com/danielwarfield)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@danielwarfield1/subscribe?source=post_page-----c70c38e87491--------------------------------)
    [![](../Images/1f6f4c8a07d69cf53e055e0130a85b03.png)](https://www.buymeacoffee.com/danielwarfield)
- en: Never expected, always appreciated. By donating you allow me to allocate more
    time and resources towards more frequent and higher quality articles. [Link](https://www.buymeacoffee.com/danielwarfield)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 从未预料到，总是感激。通过捐赠，你使我能够投入更多时间和资源，提供更频繁和更高质量的文章。 [链接](https://www.buymeacoffee.com/danielwarfield)
- en: '**Attribution:** All of the resources in this document were created by Daniel
    Warfield, unless a source is otherwise provided. You can use any resource in this
    post for your own non-commercial purposes, so long as you reference this article,
    [https://danielwarfield.dev](https://danielwarfield.dev/), or both. An explicit
    commercial license may be granted upon request.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**归属说明：** 本文档中的所有资源均由**Daniel Warfield**创建，除非另有来源提供。你可以将本文中的任何资源用于自己的非商业用途，只要你引用了这篇文章，[https://danielwarfield.dev](https://danielwarfield.dev/)，或两者兼具。可以根据要求授予明确的商业许可。'
