- en: Critical Tools for Ethical and Explainable AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/critical-tools-for-ethical-and-explainable-ai-ed0e336d82a](https://towardsdatascience.com/critical-tools-for-ethical-and-explainable-ai-ed0e336d82a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0b5a2e6b118073ceb236ed484cd914a3.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Wesley Tingey](https://unsplash.com/@wesleyphotography?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: A guide to essential Libraries and Toolkits that can help you create trustworthy
    yet robust models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@upadhyan?source=post_page-----ed0e336d82a--------------------------------)[![Nakul
    Upadhya](../Images/336cb21272e9b1f098177adbde50e92e.png)](https://medium.com/@upadhyan?source=post_page-----ed0e336d82a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ed0e336d82a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ed0e336d82a--------------------------------)
    [Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----ed0e336d82a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ed0e336d82a--------------------------------)
    ·8 min read·Jul 19, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning models have revolutionized numerous fields by delivering remarkable
    predictive capabilities. However, as these models become increasingly ubiquitous,
    the need to ensure fairness and interpretability has emerged as a critical concern.
    Building fair and transparent models is an ethical imperative for building trust,
    avoiding bias, and mitigating unintended consequences. Fortunately, Python offers
    a plethora of powerful tools and libraries that empower data scientists and machine
    learning practitioners to address these challenges head-on. In fact, the variety
    of tools and resources out there can make it daunting for data scientists and
    stakeholders to know which ones to use.
  prefs: []
  type: TYPE_NORMAL
- en: This article delves into fairness and interpretability by introducing a carefully
    curated selection of Python packages encompassing a wide range of interpretability
    tools. These tools enable researchers, developers, and stakeholders to gain deeper
    insights into model behaviour, understand the influence of features, and ensure
    fairness in their machine-learning endeavours.
  prefs: []
  type: TYPE_NORMAL
- en: '*Disclaimer: I will only focus on three different packages since these 3 contain
    a majority of the interpretability and fairness tools anyone may need. However,
    a list of honourable mentions can be found at the very end of the article.*'
  prefs: []
  type: TYPE_NORMAL
- en: InterpretML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*GitHub*: [https://github.com/interpretml/interpret](https://github.com/interpretml/interpret)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Documentation*: [https://interpret.ml/docs/getting-started.html](https://interpret.ml/docs/getting-started.html)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Interpretable models play a pivotal role in machine learning, promoting trust
    by shedding light on their decision-making mechanisms. This transparency is crucial
    for regulatory compliance, ethical considerations, and gaining user acceptance.
    InterpretML [1] is an open-source package developed by Microsoft's research team
    that incorporates many crucial machine-learning interpretability techniques in
    one library.
  prefs: []
  type: TYPE_NORMAL
- en: '***Post-Hoc Explanations***'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, InterpretML includes many post-hoc explanation algorithms to shed light
    on the internals of black-box models. These include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Shapley Additive Explanations (SHAP): A feature importance explanation approach
    based on game theory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Local Interpretable Model-agnostic Explanations (LIME): A local explanation
    method that fits a surrogate interpretable model to predict the result of the
    black-box model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Partial Dependence Plots (PDP): A perturbation-based interpretability method
    that helps show interactions between features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Morris Sensitivity Analysis: A method for quantifying input variables'' influence
    on a model''s output by systematically perturbing the variables and observing
    the resulting changes in the output (similar to PDP)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Almost all of the methods above can be found in other libraries, but InterpretML
    makes it easier for us by combining all of them into one package.
  prefs: []
  type: TYPE_NORMAL
- en: '***Glassbox Models***'
  prefs: []
  type: TYPE_NORMAL
- en: Besides providing post-hoc explainability, InterpretML also contains a few glass
    box (or inherently interpretable) models such as Linear Models, Decision Trees,
    and Decision Rules (or Oblivious Decision Trees).
  prefs: []
  type: TYPE_NORMAL
- en: '**InterpretML is also the only package that contains the Explainable Boosting
    Machine (EBM)**, a tree-based, gradient-boosting Generalized Additive Model. Internally,
    EBMs generate contribution functions based on the values of individual variables
    or variable interactions. These functions are then combined for the final prediction,
    and global explanations can be generated by visualizing the contribution values.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/167c06cc8d09c8d98f0575dc1a1b40cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Explanation of priors_count on the COMPAS Dataset. As the number of priors goes
    up, the model predicts higher recidivism rates (Figure by Author)
  prefs: []
  type: TYPE_NORMAL
- en: EBMs are often as accurate as other boosting models like LightGBM and XGBoost,
    making them a vital tool in any data scientist's toolbox. Please check Dr. Kubler's
    article for [a full explanation of the EBM.](/the-explainable-boosting-machine-f24152509ebb)
  prefs: []
  type: TYPE_NORMAL
- en: Captum
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*GitHub*: [https://github.com/pytorch/captum](https://github.com/pytorch/captum)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Documentation*: [https://captum.ai/docs/introduction](https://captum.ai/docs/introduction)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: While InterpretML focused mainly on “shallow” models, Captum [2] is PyTorch's
    go-to package for deep learning interpretability. This library contains many post-hoc
    interpretability algorithms that help provide both feature-importance and neuron/layer
    attributions (a full table can be found below).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55f4aaf140a6a67721213dcd5534928d.png)'
  prefs: []
  type: TYPE_IMG
- en: Captum Attribution Algorithms organized by explanation focus (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'These algorithms help with tabular interpretability, but their use cases extend
    beyond that. Ever wondered what BERT might be looking at for its predictions?
    Well, one of the [tutorials provided by Captum](https://captum.ai/tutorials/)
    shows how to use [Layer Integrated Gradients to explain question-answer pairs
    generated by BERT](https://captum.ai/tutorials/Bert_SQUAD_Interpret):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3edf85cdd0a3bf79e17f3737ec3f5103.png)'
  prefs: []
  type: TYPE_IMG
- en: Question Answering Interpretability(Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Captum can also be used to explain image predictions using algorithms such
    as Input x Gradient or Layerwise relevance propagation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/167f3632286fb4437cf493a9323ddacb.png)'
  prefs: []
  type: TYPE_IMG
- en: MNIST Prediction Explanation using Layerwise Relevance Propagation (Image by
    Author)
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this library is incredibly easy to use and extremely versatile, making
    it a must-know for any deep learning developer.
  prefs: []
  type: TYPE_NORMAL
- en: AIF360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*GitHub*: [https://github.com/Trusted-AI/AIF360](https://github.com/Trusted-AI/AIF360)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Documentation*: [https://aif360.readthedocs.io/en/stable/](https://aif360.readthedocs.io/en/stable/)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: While interpretability can go a long way in identifying potential bias in models,
    some dedicated tools and metrics can measure and, more importantly, *mitigate*
    unfairness in datasets and predictive tools. One of these is the AI Fairness 360
    toolkit (AIF360) [3], an open-source library developed by IBM for both Python
    and R. This toolkit covers almost all the fairness and mitigation methods one
    may need.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, AIF360 (like Captum) has a large number of [easy-to-approach tutorials](https://github.com/Trusted-AI/AIF360/tree/master/examples)
    on how to use this library.
  prefs: []
  type: TYPE_NORMAL
- en: '***Datasets***'
  prefs: []
  type: TYPE_NORMAL
- en: The first extremely useful feature AIF360 provides is a large number of sandbox
    datasets provided that are extremely useful when learning about Fairness and Interpretability.
    These include the Adult Census Income, Bank Marketing, COMPAS (the criminal recidivism
    dataset), MEPS (Medical Expenditure Panel Survey) Data for 2019–21, Law School
    GPA, and German Credit datasets. All of these are great starting points for examining
    fairness and systemic bias.
  prefs: []
  type: TYPE_NORMAL
- en: '***Fairness Metrics***'
  prefs: []
  type: TYPE_NORMAL
- en: AIF360 also provides a comprehensive set of tools that calculate metrics on
    representation and model performance conditioned on privileged and underprivileged
    groups. This makes it easy for users to calculate fairness scores like Equalized
    Odds (equal false positive and negative rates across groups) and Demographic Parity
    (identical predictions if we ignore a sensitive feature). For example, using `compute_num_TF_PN`
    can give a confusion matrix comparison between an underrepresented and privileged
    group.
  prefs: []
  type: TYPE_NORMAL
- en: '***Mitigation Methods***'
  prefs: []
  type: TYPE_NORMAL
- en: The crowning feature of AIF360 is the large number of mitigation algorithms
    the library contains. These algorithms can easily be integrated into a standard
    machine-learning pipeline without many changes and all of them are compatible
    with the sklearn interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first group of mitigation methods is pre-processing algorithms. These transform
    input data to help balance the fairness and representation of the data. AIF360
    contains four algorithms for this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Disparate Impact Removal: This edits the feature values across classes to increase
    overall fairness and reduce the impact of systemic biases on the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Learning Fair Representations (LFR): This algorithm finds a latent representation
    of the data which encodes important information but obfuscates information about
    protected attributes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Optimized Preprocessing: This technique learns a probabilistic transformation
    that edits the features and labels to ensure group fairness and data fidelity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reweighting: This algorithm simply reweights the samples to ensure fairness
    before classification tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AIF360 also provides a lot of “in-processing” methods that wrap around the training
    and hyperparameter search processes. These include methods like Grid Search reduction
    (finding hyperparameters that optimize performance and fairness), Adversarial
    Debiasing (learning a second model that aims to detect protected attributes using
    the results from the first model), and others.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, AIF360 offers multiple post-processing algorithms that take in a model's
    predictions and solve their optimization problems to modify predictions to be
    more fair. These include Calibrated Equalized Odds (modifying predictions to ensure
    equal positive and negative rates) and Reject Option Classifier (changing predictions
    to give more favourable outcomes to underprivileged groups).
  prefs: []
  type: TYPE_NORMAL
- en: Honorable Mentions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The three libraries listed above are incredible and will cover 80% of the interpretability
    and fairness needs of the beginner data scientist. However, there are some other
    packages and tools that deserve an honourable mention:'
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[SHAP](https://github.com/slundberg/shap) [4] / [LIME](https://github.com/marcotcr/lime)
    [5]: Dedicated implementations of the SHAP and LIME algorithms, respectively,
    along with related visualizations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ELI5](https://github.com/eli5-org/eli5) [6]: This package is similar to InterpretML
    and shares many white-box models and black-box explainers in the other packages.
    Unfortunately, this project is no longer updated anymore.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Yellowbrick](https://github.com/DistrictDataLabs/yellowbrick/tree/main) [7]:
    This package extends the sklearn API to provide a lot of visualization tools for
    your model internals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Alibi](https://github.com/SeldonIO/alibi) [8]: This package is similar to
    InterpretML and ELI5, providing many explainers and white box models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fairness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Fairlearn](https://github.com/fairlearn/fairlearn) [9]: Fairlearn is a library
    similar to AIF360, providing fairness-promoting tools. This package shares many
    of the algorithms found in AIF360.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Aequitas](https://github.com/dssg/aequitas) [10]: Aequitas is a bias audit
    toolkit that is a library and a [web application](http://aequitas.dssg.io/). Using
    this tool, you can generate reports on the systemic biases potentially present
    in your data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FairML](https://github.com/adebayoj/fairml) [11]: FairML is a library that
    quantifies a model''s inputs'' relative significance and predictive dependency.
    This tool can help audit predictive models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the end, the collective effort to embrace interpretability and fairness in
    machine learning will lead us toward a future where AI systems are accurate and
    powerful but also transparent, fair, and trustworthy, ultimately benefiting developers
    and end-users alike. By harnessing the capabilities of these Python packages and
    embracing a commitment to ethical AI, we can pave the way for a more inclusive
    and responsible AI-driven world.
  prefs: []
  type: TYPE_NORMAL
- en: Resources and References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are interested in interpretable machine learning and forecasting, consider
    giving me a follow: [https://medium.com/@upadhyan](https://medium.com/@upadhyan)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For other articles on Ethical and Interpretable AI, check out the reading list
    below:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Nakul Upadhya](../Images/e62aa67aa11cd0f9bcd1132257fc3773.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----ed0e336d82a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Interpretable and Ethical AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@upadhyan/list/interpretable-and-ethical-ai-f6ee1f0b476d?source=post_page-----ed0e336d82a--------------------------------)5
    stories![](../Images/3718151c0f72303f3d1c71f54229bc98.png)![](../Images/eddb4279ebae7fc1ba79cf6dcc6ebd5a.png)![](../Images/7def8e23dad656929857f2a488b1f547.png)'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Nori, H., Jenkins, S., Koch, P., & Caruana, R. (2019). InterpretML: A Unified
    Framework for Machine Learning Interpretability*. arXiv preprint arXiv:1909.09223*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh,
    Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi
    Yan, & Orion Reblitz-Richardson. (2020). Captum: A unified and generic model interpretability
    library for PyTorch.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Rachel K. E. Bellamy, Kuntal Dey, Michael Hind and Samuel C. Hoffman, Stephanie
    Houde, Kalapriya Kannan and Pranay Lohia, Jacquelyn Martino, Sameep Mehta and
    Aleksandra Mojsilovic, Seema Nagar, Karthikeyan Natesan Ramamurthy and John Richards,
    Diptikalyan Saha, Prasanna Sattigeri and Moninder Singh, Kush R. Varshney, & Yunfeng
    Zhang. (2018). AI Fairness 360: An Extensible Toolkit for Detecting, Understanding,
    and Mitigating Unwanted Algorithmic Bias.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Lundberg, S., & Lee, S.I. (2017). A Unified Approach to Interpreting Model
    Predictions*. Advances in Neural Information Processing Systems 30*, 4765–4774.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Marco Tulio Ribeiro, Sameer Singh, & Carlos Guestrin (2016). “Why Should
    I Trust You?”: Explaining the Predictions of Any Classifier. In *Proceedings of
    the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
    San Francisco, CA, USA, August 13–17, 2016* (pp. 1135–1144).'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] TeamHG-Memex (2019) ELI5\. *Github*'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Bengfort, B., & Bilbro, R. (2019). Yellowbrick: Visualizing the Scikit-Learn
    Model Selection Process*. The Journal of Open Source Software, 4(35).*'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Janis Klaise, Arnaud Van Looveren, Giovanni Vacanti, & Alexandru Coca (2021).
    Alibi Explain: Algorithms for Explaining Machine Learning Models*. Journal of
    Machine Learning Research, 22(181), 1–7.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Bird, S., Dudik, M., Edgar, R., Horn, B., Lutz, R., Milan, V., Sameki,
    M., Wallach, H., & Walker, K. (2020). *Fairlearn: A toolkit for assessing and
    improving fairness in AI* [White paper]. Microsoft.'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Saleiro, P., Kuester, B., Stevens, A., Anisfeld, A., Hinkson, L., London,
    J., & Ghani, R. (2018). Aequitas: A Bias and Fairness Audit Toolkit*. arXiv preprint
    arXiv:1811.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] Adebayo, J. A. (2016). *FairML: ToolBox for diagnosing bias in predictive
    modeling* (Doctoral dissertation, Massachusetts Institute of Technology).'
  prefs: []
  type: TYPE_NORMAL
