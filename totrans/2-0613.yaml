- en: Critical Tools for Ethical and Explainable AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/critical-tools-for-ethical-and-explainable-ai-ed0e336d82a](https://towardsdatascience.com/critical-tools-for-ethical-and-explainable-ai-ed0e336d82a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0b5a2e6b118073ceb236ed484cd914a3.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Photo by [Wesley Tingey](https://unsplash.com/@wesleyphotography?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: A guide to essential Libraries and Toolkits that can help you create trustworthy
    yet robust models
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@upadhyan?source=post_page-----ed0e336d82a--------------------------------)[![Nakul
    Upadhya](../Images/336cb21272e9b1f098177adbde50e92e.png)](https://medium.com/@upadhyan?source=post_page-----ed0e336d82a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ed0e336d82a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ed0e336d82a--------------------------------)
    [Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----ed0e336d82a--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ed0e336d82a--------------------------------)
    ·8 min read·Jul 19, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning models have revolutionized numerous fields by delivering remarkable
    predictive capabilities. However, as these models become increasingly ubiquitous,
    the need to ensure fairness and interpretability has emerged as a critical concern.
    Building fair and transparent models is an ethical imperative for building trust,
    avoiding bias, and mitigating unintended consequences. Fortunately, Python offers
    a plethora of powerful tools and libraries that empower data scientists and machine
    learning practitioners to address these challenges head-on. In fact, the variety
    of tools and resources out there can make it daunting for data scientists and
    stakeholders to know which ones to use.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: This article delves into fairness and interpretability by introducing a carefully
    curated selection of Python packages encompassing a wide range of interpretability
    tools. These tools enable researchers, developers, and stakeholders to gain deeper
    insights into model behaviour, understand the influence of features, and ensure
    fairness in their machine-learning endeavours.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '*Disclaimer: I will only focus on three different packages since these 3 contain
    a majority of the interpretability and fairness tools anyone may need. However,
    a list of honourable mentions can be found at the very end of the article.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: InterpretML
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*GitHub*: [https://github.com/interpretml/interpret](https://github.com/interpretml/interpret)'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Documentation*: [https://interpret.ml/docs/getting-started.html](https://interpret.ml/docs/getting-started.html)'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Interpretable models play a pivotal role in machine learning, promoting trust
    by shedding light on their decision-making mechanisms. This transparency is crucial
    for regulatory compliance, ethical considerations, and gaining user acceptance.
    InterpretML [1] is an open-source package developed by Microsoft's research team
    that incorporates many crucial machine-learning interpretability techniques in
    one library.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '***Post-Hoc Explanations***'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'First, InterpretML includes many post-hoc explanation algorithms to shed light
    on the internals of black-box models. These include:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'Shapley Additive Explanations (SHAP): A feature importance explanation approach
    based on game theory.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Local Interpretable Model-agnostic Explanations (LIME): A local explanation
    method that fits a surrogate interpretable model to predict the result of the
    black-box model.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Partial Dependence Plots (PDP): A perturbation-based interpretability method
    that helps show interactions between features.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Morris Sensitivity Analysis: A method for quantifying input variables'' influence
    on a model''s output by systematically perturbing the variables and observing
    the resulting changes in the output (similar to PDP)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Almost all of the methods above can be found in other libraries, but InterpretML
    makes it easier for us by combining all of them into one package.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '***Glassbox Models***'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Besides providing post-hoc explainability, InterpretML also contains a few glass
    box (or inherently interpretable) models such as Linear Models, Decision Trees,
    and Decision Rules (or Oblivious Decision Trees).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '**InterpretML is also the only package that contains the Explainable Boosting
    Machine (EBM)**, a tree-based, gradient-boosting Generalized Additive Model. Internally,
    EBMs generate contribution functions based on the values of individual variables
    or variable interactions. These functions are then combined for the final prediction,
    and global explanations can be generated by visualizing the contribution values.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/167c06cc8d09c8d98f0575dc1a1b40cf.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: Explanation of priors_count on the COMPAS Dataset. As the number of priors goes
    up, the model predicts higher recidivism rates (Figure by Author)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: EBMs are often as accurate as other boosting models like LightGBM and XGBoost,
    making them a vital tool in any data scientist's toolbox. Please check Dr. Kubler's
    article for [a full explanation of the EBM.](/the-explainable-boosting-machine-f24152509ebb)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Captum
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*GitHub*: [https://github.com/pytorch/captum](https://github.com/pytorch/captum)'
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Documentation*: [https://captum.ai/docs/introduction](https://captum.ai/docs/introduction)'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: While InterpretML focused mainly on “shallow” models, Captum [2] is PyTorch's
    go-to package for deep learning interpretability. This library contains many post-hoc
    interpretability algorithms that help provide both feature-importance and neuron/layer
    attributions (a full table can be found below).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55f4aaf140a6a67721213dcd5534928d.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: Captum Attribution Algorithms organized by explanation focus (Image By Author)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Captum归因算法按解释焦点组织（图片由作者提供）
- en: 'These algorithms help with tabular interpretability, but their use cases extend
    beyond that. Ever wondered what BERT might be looking at for its predictions?
    Well, one of the [tutorials provided by Captum](https://captum.ai/tutorials/)
    shows how to use [Layer Integrated Gradients to explain question-answer pairs
    generated by BERT](https://captum.ai/tutorials/Bert_SQUAD_Interpret):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法帮助处理表格数据的可解释性，但它们的使用案例不限于此。是否曾想过BERT可能在其预测中关注什么？好吧，[Captum提供的教程之一](https://captum.ai/tutorials/)展示了如何使用[层集成梯度来解释BERT生成的问答对](https://captum.ai/tutorials/Bert_SQUAD_Interpret)：
- en: '![](../Images/3edf85cdd0a3bf79e17f3737ec3f5103.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3edf85cdd0a3bf79e17f3737ec3f5103.png)'
- en: Question Answering Interpretability(Image by Author)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 问答系统可解释性（图片由作者提供）
- en: 'Captum can also be used to explain image predictions using algorithms such
    as Input x Gradient or Layerwise relevance propagation:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Captum还可以用于解释图像预测，使用如Input x Gradient或层相关传播等算法：
- en: '![](../Images/167f3632286fb4437cf493a9323ddacb.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/167f3632286fb4437cf493a9323ddacb.png)'
- en: MNIST Prediction Explanation using Layerwise Relevance Propagation (Image by
    Author)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST预测解释使用层相关传播（图片由作者提供）
- en: Overall, this library is incredibly easy to use and extremely versatile, making
    it a must-know for any deep learning developer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这个库非常易于使用且极其多才多艺，使其成为任何深度学习开发者必知的工具。
- en: AIF360
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AIF360
- en: '*GitHub*: [https://github.com/Trusted-AI/AIF360](https://github.com/Trusted-AI/AIF360)'
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*GitHub*: [https://github.com/Trusted-AI/AIF360](https://github.com/Trusted-AI/AIF360)'
- en: ''
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Documentation*: [https://aif360.readthedocs.io/en/stable/](https://aif360.readthedocs.io/en/stable/)'
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*文档*: [https://aif360.readthedocs.io/en/stable/](https://aif360.readthedocs.io/en/stable/)'
- en: While interpretability can go a long way in identifying potential bias in models,
    some dedicated tools and metrics can measure and, more importantly, *mitigate*
    unfairness in datasets and predictive tools. One of these is the AI Fairness 360
    toolkit (AIF360) [3], an open-source library developed by IBM for both Python
    and R. This toolkit covers almost all the fairness and mitigation methods one
    may need.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可解释性可以在识别模型中的潜在偏差方面发挥很大作用，但一些专门的工具和指标可以衡量并更重要的是，*缓解*数据集和预测工具中的不公平现象。其中之一是AI公平性360工具包（AIF360）[3]，这是IBM为Python和R开发的开源库。该工具包涵盖了几乎所有可能需要的公平性和缓解方法。
- en: Additionally, AIF360 (like Captum) has a large number of [easy-to-approach tutorials](https://github.com/Trusted-AI/AIF360/tree/master/examples)
    on how to use this library.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，AIF360（如Captum一样）提供了大量[易于入门的教程](https://github.com/Trusted-AI/AIF360/tree/master/examples)来指导如何使用这个库。
- en: '***Datasets***'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '***数据集***'
- en: The first extremely useful feature AIF360 provides is a large number of sandbox
    datasets provided that are extremely useful when learning about Fairness and Interpretability.
    These include the Adult Census Income, Bank Marketing, COMPAS (the criminal recidivism
    dataset), MEPS (Medical Expenditure Panel Survey) Data for 2019–21, Law School
    GPA, and German Credit datasets. All of these are great starting points for examining
    fairness and systemic bias.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: AIF360提供的第一个极其有用的功能是提供大量在学习公平性和可解释性时非常有用的沙箱数据集。这些数据集包括成人普查收入、银行营销、COMPAS（犯罪复发数据集）、MEPS（2019-21年医疗支出调查）数据、法学院GPA和德国信用数据集。所有这些都是检查公平性和系统性偏见的绝佳起点。
- en: '***Fairness Metrics***'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '***公平性指标***'
- en: AIF360 also provides a comprehensive set of tools that calculate metrics on
    representation and model performance conditioned on privileged and underprivileged
    groups. This makes it easy for users to calculate fairness scores like Equalized
    Odds (equal false positive and negative rates across groups) and Demographic Parity
    (identical predictions if we ignore a sensitive feature). For example, using `compute_num_TF_PN`
    can give a confusion matrix comparison between an underrepresented and privileged
    group.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: AIF360还提供了一整套工具，用于计算在特权和非特权群体上的表示和模型性能指标。这使得用户能够轻松计算公平性评分，如平等化机会（各组之间的假阳性和假阴性率相等）和人口统计均衡（忽略敏感特征时的预测相同）。例如，使用`compute_num_TF_PN`可以比较一个被低估群体和特权群体之间的混淆矩阵。
- en: '***Mitigation Methods***'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '***缓解方法***'
- en: The crowning feature of AIF360 is the large number of mitigation algorithms
    the library contains. These algorithms can easily be integrated into a standard
    machine-learning pipeline without many changes and all of them are compatible
    with the sklearn interface.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: AIF360的顶级特色是库中包含的大量缓解算法。这些算法可以轻松集成到标准机器学习管道中，几乎不需要做任何改动，并且所有算法都与sklearn接口兼容。
- en: 'The first group of mitigation methods is pre-processing algorithms. These transform
    input data to help balance the fairness and representation of the data. AIF360
    contains four algorithms for this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 第一组缓解方法是预处理算法。这些算法转换输入数据，以帮助平衡数据的公平性和表示。AIF360包含四种此类算法：
- en: 'Disparate Impact Removal: This edits the feature values across classes to increase
    overall fairness and reduce the impact of systemic biases on the dataset'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不平等影响去除：这项技术编辑各类特征值，以提高整体公平性，并减少系统性偏见对数据集的影响。
- en: 'Learning Fair Representations (LFR): This algorithm finds a latent representation
    of the data which encodes important information but obfuscates information about
    protected attributes.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习公平表示（LFR）：该算法找到数据的潜在表示，编码重要信息，同时隐藏关于受保护属性的信息。
- en: 'Optimized Preprocessing: This technique learns a probabilistic transformation
    that edits the features and labels to ensure group fairness and data fidelity'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化预处理：这项技术学习一种概率变换，编辑特征和标签以确保群体公平性和数据的真实性。
- en: 'Reweighting: This algorithm simply reweights the samples to ensure fairness
    before classification tasks.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新加权：该算法简单地重新加权样本，以确保分类任务前的公平性。
- en: AIF360 also provides a lot of “in-processing” methods that wrap around the training
    and hyperparameter search processes. These include methods like Grid Search reduction
    (finding hyperparameters that optimize performance and fairness), Adversarial
    Debiasing (learning a second model that aims to detect protected attributes using
    the results from the first model), and others.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: AIF360还提供了许多“处理中”方法，这些方法围绕训练和超参数搜索过程进行。这些方法包括网格搜索减少（寻找优化性能和公平性的超参数）、对抗性去偏（学习第二个模型，旨在使用第一个模型的结果检测受保护属性）等。
- en: Finally, AIF360 offers multiple post-processing algorithms that take in a model's
    predictions and solve their optimization problems to modify predictions to be
    more fair. These include Calibrated Equalized Odds (modifying predictions to ensure
    equal positive and negative rates) and Reject Option Classifier (changing predictions
    to give more favourable outcomes to underprivileged groups).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，AIF360提供了多个后处理算法，这些算法接受模型的预测结果并解决其优化问题，以使预测结果更公平。这些算法包括校准均衡赔率（修改预测以确保正负率相等）和拒绝选项分类器（改变预测以给予弱势群体更有利的结果）。
- en: Honorable Mentions
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 荣誉提及
- en: 'The three libraries listed above are incredible and will cover 80% of the interpretability
    and fairness needs of the beginner data scientist. However, there are some other
    packages and tools that deserve an honourable mention:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 上述三大库非常出色，将覆盖初学数据科学家80%的可解释性和公平性需求。然而，还有一些其他包和工具值得荣誉提及：
- en: Interpretability
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释性
- en: '[SHAP](https://github.com/slundberg/shap) [4] / [LIME](https://github.com/marcotcr/lime)
    [5]: Dedicated implementations of the SHAP and LIME algorithms, respectively,
    along with related visualizations.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SHAP](https://github.com/slundberg/shap) [4] / [LIME](https://github.com/marcotcr/lime)
    [5]：分别是SHAP和LIME算法的专用实现，以及相关可视化。'
- en: '[ELI5](https://github.com/eli5-org/eli5) [6]: This package is similar to InterpretML
    and shares many white-box models and black-box explainers in the other packages.
    Unfortunately, this project is no longer updated anymore.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ELI5](https://github.com/eli5-org/eli5) [6]：该包类似于InterpretML，提供了许多白盒模型和黑盒解释器。不幸的是，这个项目已不再更新。'
- en: '[Yellowbrick](https://github.com/DistrictDataLabs/yellowbrick/tree/main) [7]:
    This package extends the sklearn API to provide a lot of visualization tools for
    your model internals.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yellowbrick](https://github.com/DistrictDataLabs/yellowbrick/tree/main) [7]：这个包扩展了sklearn
    API，提供了许多用于模型内部可视化的工具。'
- en: '[Alibi](https://github.com/SeldonIO/alibi) [8]: This package is similar to
    InterpretML and ELI5, providing many explainers and white box models.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Alibi](https://github.com/SeldonIO/alibi) [8]：该包类似于InterpretML和ELI5，提供了许多解释器和白盒模型。'
- en: Fairness
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公平性
- en: '[Fairlearn](https://github.com/fairlearn/fairlearn) [9]: Fairlearn is a library
    similar to AIF360, providing fairness-promoting tools. This package shares many
    of the algorithms found in AIF360.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Aequitas](https://github.com/dssg/aequitas) [10]: Aequitas is a bias audit
    toolkit that is a library and a [web application](http://aequitas.dssg.io/). Using
    this tool, you can generate reports on the systemic biases potentially present
    in your data.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FairML](https://github.com/adebayoj/fairml) [11]: FairML is a library that
    quantifies a model''s inputs'' relative significance and predictive dependency.
    This tool can help audit predictive models.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the end, the collective effort to embrace interpretability and fairness in
    machine learning will lead us toward a future where AI systems are accurate and
    powerful but also transparent, fair, and trustworthy, ultimately benefiting developers
    and end-users alike. By harnessing the capabilities of these Python packages and
    embracing a commitment to ethical AI, we can pave the way for a more inclusive
    and responsible AI-driven world.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Resources and References
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are interested in interpretable machine learning and forecasting, consider
    giving me a follow: [https://medium.com/@upadhyan](https://medium.com/@upadhyan)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For other articles on Ethical and Interpretable AI, check out the reading list
    below:'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Nakul Upadhya](../Images/e62aa67aa11cd0f9bcd1132257fc3773.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: '[Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----ed0e336d82a--------------------------------)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Interpretable and Ethical AI
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@upadhyan/list/interpretable-and-ethical-ai-f6ee1f0b476d?source=post_page-----ed0e336d82a--------------------------------)5
    stories![](../Images/3718151c0f72303f3d1c71f54229bc98.png)![](../Images/eddb4279ebae7fc1ba79cf6dcc6ebd5a.png)![](../Images/7def8e23dad656929857f2a488b1f547.png)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Nori, H., Jenkins, S., Koch, P., & Caruana, R. (2019). InterpretML: A Unified
    Framework for Machine Learning Interpretability*. arXiv preprint arXiv:1909.09223*.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh,
    Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi
    Yan, & Orion Reblitz-Richardson. (2020). Captum: A unified and generic model interpretability
    library for PyTorch.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Rachel K. E. Bellamy, Kuntal Dey, Michael Hind and Samuel C. Hoffman, Stephanie
    Houde, Kalapriya Kannan and Pranay Lohia, Jacquelyn Martino, Sameep Mehta and
    Aleksandra Mojsilovic, Seema Nagar, Karthikeyan Natesan Ramamurthy and John Richards,
    Diptikalyan Saha, Prasanna Sattigeri and Moninder Singh, Kush R. Varshney, & Yunfeng
    Zhang. (2018). AI Fairness 360: An Extensible Toolkit for Detecting, Understanding,
    and Mitigating Unwanted Algorithmic Bias.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Lundberg, S., & Lee, S.I. (2017). A Unified Approach to Interpreting Model
    Predictions*. Advances in Neural Information Processing Systems 30*, 4765–4774.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Marco Tulio Ribeiro, Sameer Singh, & Carlos Guestrin (2016). “Why Should
    I Trust You?”: Explaining the Predictions of Any Classifier. In *Proceedings of
    the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
    San Francisco, CA, USA, August 13–17, 2016* (pp. 1135–1144).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[6] TeamHG-Memex (2019) ELI5\. *Github*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Bengfort, B., & Bilbro, R. (2019). Yellowbrick: Visualizing the Scikit-Learn
    Model Selection Process*. The Journal of Open Source Software, 4(35).*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Janis Klaise, Arnaud Van Looveren, Giovanni Vacanti, & Alexandru Coca (2021).
    Alibi Explain: Algorithms for Explaining Machine Learning Models*. Journal of
    Machine Learning Research, 22(181), 1–7.*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Bird, S., Dudik, M., Edgar, R., Horn, B., Lutz, R., Milan, V., Sameki,
    M., Wallach, H., & Walker, K. (2020). *Fairlearn: A toolkit for assessing and
    improving fairness in AI* [White paper]. Microsoft.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Saleiro, P., Kuester, B., Stevens, A., Anisfeld, A., Hinkson, L., London,
    J., & Ghani, R. (2018). Aequitas: A Bias and Fairness Audit Toolkit*. arXiv preprint
    arXiv:1811.*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[11] Adebayo, J. A. (2016). *FairML: ToolBox for diagnosing bias in predictive
    modeling* (Doctoral dissertation, Massachusetts Institute of Technology).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
