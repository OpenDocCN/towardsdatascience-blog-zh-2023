- en: Critical Tools for Ethical and Explainable AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/critical-tools-for-ethical-and-explainable-ai-ed0e336d82a](https://towardsdatascience.com/critical-tools-for-ethical-and-explainable-ai-ed0e336d82a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0b5a2e6b118073ceb236ed484cd914a3.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Photo by [Wesley Tingey](https://unsplash.com/@wesleyphotography?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: A guide to essential Libraries and Toolkits that can help you create trustworthy
    yet robust models
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@upadhyan?source=post_page-----ed0e336d82a--------------------------------)[![Nakul
    Upadhya](../Images/336cb21272e9b1f098177adbde50e92e.png)](https://medium.com/@upadhyan?source=post_page-----ed0e336d82a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ed0e336d82a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ed0e336d82a--------------------------------)
    [Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----ed0e336d82a--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ed0e336d82a--------------------------------)
    ·8 min read·Jul 19, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning models have revolutionized numerous fields by delivering remarkable
    predictive capabilities. However, as these models become increasingly ubiquitous,
    the need to ensure fairness and interpretability has emerged as a critical concern.
    Building fair and transparent models is an ethical imperative for building trust,
    avoiding bias, and mitigating unintended consequences. Fortunately, Python offers
    a plethora of powerful tools and libraries that empower data scientists and machine
    learning practitioners to address these challenges head-on. In fact, the variety
    of tools and resources out there can make it daunting for data scientists and
    stakeholders to know which ones to use.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: This article delves into fairness and interpretability by introducing a carefully
    curated selection of Python packages encompassing a wide range of interpretability
    tools. These tools enable researchers, developers, and stakeholders to gain deeper
    insights into model behaviour, understand the influence of features, and ensure
    fairness in their machine-learning endeavours.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '*Disclaimer: I will only focus on three different packages since these 3 contain
    a majority of the interpretability and fairness tools anyone may need. However,
    a list of honourable mentions can be found at the very end of the article.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: InterpretML
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*GitHub*: [https://github.com/interpretml/interpret](https://github.com/interpretml/interpret)'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Documentation*: [https://interpret.ml/docs/getting-started.html](https://interpret.ml/docs/getting-started.html)'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Interpretable models play a pivotal role in machine learning, promoting trust
    by shedding light on their decision-making mechanisms. This transparency is crucial
    for regulatory compliance, ethical considerations, and gaining user acceptance.
    InterpretML [1] is an open-source package developed by Microsoft's research team
    that incorporates many crucial machine-learning interpretability techniques in
    one library.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释模型在机器学习中扮演着关键角色，通过揭示决策机制来促进信任。这种透明性对于监管合规、伦理考虑和用户接受度至关重要。InterpretML [1]
    是微软研究团队开发的一个开源包，集成了许多关键的机器学习可解释性技术于一个库中。
- en: '***Post-Hoc Explanations***'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '***Post-Hoc Explanations***'
- en: 'First, InterpretML includes many post-hoc explanation algorithms to shed light
    on the internals of black-box models. These include:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，InterpretML 包含了许多后期解释算法，以揭示黑箱模型的内部。这些算法包括：
- en: 'Shapley Additive Explanations (SHAP): A feature importance explanation approach
    based on game theory.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shapley Additive Explanations (SHAP)：一种基于博弈论的特征重要性解释方法。
- en: 'Local Interpretable Model-agnostic Explanations (LIME): A local explanation
    method that fits a surrogate interpretable model to predict the result of the
    black-box model.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局部可解释模型无关解释（LIME）：一种局部解释方法，它通过拟合一个可解释的替代模型来预测黑箱模型的结果。
- en: 'Partial Dependence Plots (PDP): A perturbation-based interpretability method
    that helps show interactions between features.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部分依赖图（PDP）：一种基于扰动的可解释性方法，帮助展示特征之间的交互。
- en: 'Morris Sensitivity Analysis: A method for quantifying input variables'' influence
    on a model''s output by systematically perturbing the variables and observing
    the resulting changes in the output (similar to PDP)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 莫里斯敏感度分析：一种通过系统性地扰动输入变量并观察输出结果的变化来量化输入变量对模型输出影响的方法（类似于PDP）。
- en: Almost all of the methods above can be found in other libraries, but InterpretML
    makes it easier for us by combining all of them into one package.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有上述方法都可以在其他库中找到，但 InterpretML 通过将所有这些方法组合到一个包中，使我们更容易使用。
- en: '***Glassbox Models***'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '***Glassbox Models***'
- en: Besides providing post-hoc explainability, InterpretML also contains a few glass
    box (or inherently interpretable) models such as Linear Models, Decision Trees,
    and Decision Rules (or Oblivious Decision Trees).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 除了提供后期解释外，InterpretML 还包含一些玻璃盒（或固有可解释）模型，如线性模型、决策树和决策规则（或忽略决策树）。
- en: '**InterpretML is also the only package that contains the Explainable Boosting
    Machine (EBM)**, a tree-based, gradient-boosting Generalized Additive Model. Internally,
    EBMs generate contribution functions based on the values of individual variables
    or variable interactions. These functions are then combined for the final prediction,
    and global explanations can be generated by visualizing the contribution values.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**InterpretML 还是唯一一个包含解释性增强机器（EBM）**的包，EBM 是一种基于树的梯度提升广义加性模型。内部，EBM 根据单个变量或变量交互的值生成贡献函数。这些函数随后被组合以进行最终预测，并可以通过可视化贡献值来生成全局解释。'
- en: '![](../Images/167c06cc8d09c8d98f0575dc1a1b40cf.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/167c06cc8d09c8d98f0575dc1a1b40cf.png)'
- en: Explanation of priors_count on the COMPAS Dataset. As the number of priors goes
    up, the model predicts higher recidivism rates (Figure by Author)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: COMPAS 数据集上的 priors_count 解释。随着 priors 的增加，模型预测的再犯率更高（作者图示）
- en: EBMs are often as accurate as other boosting models like LightGBM and XGBoost,
    making them a vital tool in any data scientist's toolbox. Please check Dr. Kubler's
    article for [a full explanation of the EBM.](/the-explainable-boosting-machine-f24152509ebb)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: EBM 的准确性通常与 LightGBM 和 XGBoost 等其他提升模型相当，使其成为任何数据科学家工具箱中的重要工具。请参阅 Dr. Kubler
    的文章 [关于EBM的详细解释。](/the-explainable-boosting-machine-f24152509ebb)
- en: Captum
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Captum
- en: '*GitHub*: [https://github.com/pytorch/captum](https://github.com/pytorch/captum)'
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*GitHub*: [https://github.com/pytorch/captum](https://github.com/pytorch/captum)'
- en: ''
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Documentation*: [https://captum.ai/docs/introduction](https://captum.ai/docs/introduction)'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*文档*: [https://captum.ai/docs/introduction](https://captum.ai/docs/introduction)'
- en: While InterpretML focused mainly on “shallow” models, Captum [2] is PyTorch's
    go-to package for deep learning interpretability. This library contains many post-hoc
    interpretability algorithms that help provide both feature-importance and neuron/layer
    attributions (a full table can be found below).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 InterpretML 主要集中于“浅层”模型，Captum [2] 是 PyTorch 的深度学习可解释性首选包。该库包含许多后期可解释性算法，帮助提供特征重要性和神经元/层的归因（完整表格见下）。
- en: '![](../Images/55f4aaf140a6a67721213dcd5534928d.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55f4aaf140a6a67721213dcd5534928d.png)'
- en: Captum Attribution Algorithms organized by explanation focus (Image By Author)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Captum归因算法按解释焦点组织（图片由作者提供）
- en: 'These algorithms help with tabular interpretability, but their use cases extend
    beyond that. Ever wondered what BERT might be looking at for its predictions?
    Well, one of the [tutorials provided by Captum](https://captum.ai/tutorials/)
    shows how to use [Layer Integrated Gradients to explain question-answer pairs
    generated by BERT](https://captum.ai/tutorials/Bert_SQUAD_Interpret):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法帮助处理表格数据的可解释性，但它们的使用案例不限于此。是否曾想过BERT可能在其预测中关注什么？好吧，[Captum提供的教程之一](https://captum.ai/tutorials/)展示了如何使用[层集成梯度来解释BERT生成的问答对](https://captum.ai/tutorials/Bert_SQUAD_Interpret)：
- en: '![](../Images/3edf85cdd0a3bf79e17f3737ec3f5103.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3edf85cdd0a3bf79e17f3737ec3f5103.png)'
- en: Question Answering Interpretability(Image by Author)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 问答系统可解释性（图片由作者提供）
- en: 'Captum can also be used to explain image predictions using algorithms such
    as Input x Gradient or Layerwise relevance propagation:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Captum还可以用于解释图像预测，使用如Input x Gradient或层相关传播等算法：
- en: '![](../Images/167f3632286fb4437cf493a9323ddacb.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/167f3632286fb4437cf493a9323ddacb.png)'
- en: MNIST Prediction Explanation using Layerwise Relevance Propagation (Image by
    Author)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST预测解释使用层相关传播（图片由作者提供）
- en: Overall, this library is incredibly easy to use and extremely versatile, making
    it a must-know for any deep learning developer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这个库非常易于使用且极其多才多艺，使其成为任何深度学习开发者必知的工具。
- en: AIF360
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AIF360
- en: '*GitHub*: [https://github.com/Trusted-AI/AIF360](https://github.com/Trusted-AI/AIF360)'
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*GitHub*: [https://github.com/Trusted-AI/AIF360](https://github.com/Trusted-AI/AIF360)'
- en: ''
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Documentation*: [https://aif360.readthedocs.io/en/stable/](https://aif360.readthedocs.io/en/stable/)'
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*文档*: [https://aif360.readthedocs.io/en/stable/](https://aif360.readthedocs.io/en/stable/)'
- en: While interpretability can go a long way in identifying potential bias in models,
    some dedicated tools and metrics can measure and, more importantly, *mitigate*
    unfairness in datasets and predictive tools. One of these is the AI Fairness 360
    toolkit (AIF360) [3], an open-source library developed by IBM for both Python
    and R. This toolkit covers almost all the fairness and mitigation methods one
    may need.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可解释性可以在识别模型中的潜在偏差方面发挥很大作用，但一些专门的工具和指标可以衡量并更重要的是，*缓解*数据集和预测工具中的不公平现象。其中之一是AI公平性360工具包（AIF360）[3]，这是IBM为Python和R开发的开源库。该工具包涵盖了几乎所有可能需要的公平性和缓解方法。
- en: Additionally, AIF360 (like Captum) has a large number of [easy-to-approach tutorials](https://github.com/Trusted-AI/AIF360/tree/master/examples)
    on how to use this library.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，AIF360（如Captum一样）提供了大量[易于入门的教程](https://github.com/Trusted-AI/AIF360/tree/master/examples)来指导如何使用这个库。
- en: '***Datasets***'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '***数据集***'
- en: The first extremely useful feature AIF360 provides is a large number of sandbox
    datasets provided that are extremely useful when learning about Fairness and Interpretability.
    These include the Adult Census Income, Bank Marketing, COMPAS (the criminal recidivism
    dataset), MEPS (Medical Expenditure Panel Survey) Data for 2019–21, Law School
    GPA, and German Credit datasets. All of these are great starting points for examining
    fairness and systemic bias.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: AIF360提供的第一个极其有用的功能是提供大量在学习公平性和可解释性时非常有用的沙箱数据集。这些数据集包括成人普查收入、银行营销、COMPAS（犯罪复发数据集）、MEPS（2019-21年医疗支出调查）数据、法学院GPA和德国信用数据集。所有这些都是检查公平性和系统性偏见的绝佳起点。
- en: '***Fairness Metrics***'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '***公平性指标***'
- en: AIF360 also provides a comprehensive set of tools that calculate metrics on
    representation and model performance conditioned on privileged and underprivileged
    groups. This makes it easy for users to calculate fairness scores like Equalized
    Odds (equal false positive and negative rates across groups) and Demographic Parity
    (identical predictions if we ignore a sensitive feature). For example, using `compute_num_TF_PN`
    can give a confusion matrix comparison between an underrepresented and privileged
    group.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: AIF360还提供了一整套工具，用于计算在特权和非特权群体上的表示和模型性能指标。这使得用户能够轻松计算公平性评分，如平等化机会（各组之间的假阳性和假阴性率相等）和人口统计均衡（忽略敏感特征时的预测相同）。例如，使用`compute_num_TF_PN`可以比较一个被低估群体和特权群体之间的混淆矩阵。
- en: '***Mitigation Methods***'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '***缓解方法***'
- en: The crowning feature of AIF360 is the large number of mitigation algorithms
    the library contains. These algorithms can easily be integrated into a standard
    machine-learning pipeline without many changes and all of them are compatible
    with the sklearn interface.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: AIF360的顶级特色是库中包含的大量缓解算法。这些算法可以轻松集成到标准机器学习管道中，几乎不需要做任何改动，并且所有算法都与sklearn接口兼容。
- en: 'The first group of mitigation methods is pre-processing algorithms. These transform
    input data to help balance the fairness and representation of the data. AIF360
    contains four algorithms for this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 第一组缓解方法是预处理算法。这些算法转换输入数据，以帮助平衡数据的公平性和表示。AIF360包含四种此类算法：
- en: 'Disparate Impact Removal: This edits the feature values across classes to increase
    overall fairness and reduce the impact of systemic biases on the dataset'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不平等影响去除：这项技术编辑各类特征值，以提高整体公平性，并减少系统性偏见对数据集的影响。
- en: 'Learning Fair Representations (LFR): This algorithm finds a latent representation
    of the data which encodes important information but obfuscates information about
    protected attributes.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习公平表示（LFR）：该算法找到数据的潜在表示，编码重要信息，同时隐藏关于受保护属性的信息。
- en: 'Optimized Preprocessing: This technique learns a probabilistic transformation
    that edits the features and labels to ensure group fairness and data fidelity'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化预处理：这项技术学习一种概率变换，编辑特征和标签以确保群体公平性和数据的真实性。
- en: 'Reweighting: This algorithm simply reweights the samples to ensure fairness
    before classification tasks.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新加权：该算法简单地重新加权样本，以确保分类任务前的公平性。
- en: AIF360 also provides a lot of “in-processing” methods that wrap around the training
    and hyperparameter search processes. These include methods like Grid Search reduction
    (finding hyperparameters that optimize performance and fairness), Adversarial
    Debiasing (learning a second model that aims to detect protected attributes using
    the results from the first model), and others.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: AIF360还提供了许多“处理中”方法，这些方法围绕训练和超参数搜索过程进行。这些方法包括网格搜索减少（寻找优化性能和公平性的超参数）、对抗性去偏（学习第二个模型，旨在使用第一个模型的结果检测受保护属性）等。
- en: Finally, AIF360 offers multiple post-processing algorithms that take in a model's
    predictions and solve their optimization problems to modify predictions to be
    more fair. These include Calibrated Equalized Odds (modifying predictions to ensure
    equal positive and negative rates) and Reject Option Classifier (changing predictions
    to give more favourable outcomes to underprivileged groups).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，AIF360提供了多个后处理算法，这些算法接受模型的预测结果并解决其优化问题，以使预测结果更公平。这些算法包括校准均衡赔率（修改预测以确保正负率相等）和拒绝选项分类器（改变预测以给予弱势群体更有利的结果）。
- en: Honorable Mentions
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 荣誉提及
- en: 'The three libraries listed above are incredible and will cover 80% of the interpretability
    and fairness needs of the beginner data scientist. However, there are some other
    packages and tools that deserve an honourable mention:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 上述三大库非常出色，将覆盖初学数据科学家80%的可解释性和公平性需求。然而，还有一些其他包和工具值得荣誉提及：
- en: Interpretability
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释性
- en: '[SHAP](https://github.com/slundberg/shap) [4] / [LIME](https://github.com/marcotcr/lime)
    [5]: Dedicated implementations of the SHAP and LIME algorithms, respectively,
    along with related visualizations.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SHAP](https://github.com/slundberg/shap) [4] / [LIME](https://github.com/marcotcr/lime)
    [5]：分别是SHAP和LIME算法的专用实现，以及相关可视化。'
- en: '[ELI5](https://github.com/eli5-org/eli5) [6]: This package is similar to InterpretML
    and shares many white-box models and black-box explainers in the other packages.
    Unfortunately, this project is no longer updated anymore.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ELI5](https://github.com/eli5-org/eli5) [6]：该包类似于InterpretML，提供了许多白盒模型和黑盒解释器。不幸的是，这个项目已不再更新。'
- en: '[Yellowbrick](https://github.com/DistrictDataLabs/yellowbrick/tree/main) [7]:
    This package extends the sklearn API to provide a lot of visualization tools for
    your model internals.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yellowbrick](https://github.com/DistrictDataLabs/yellowbrick/tree/main) [7]：这个包扩展了sklearn
    API，提供了许多用于模型内部可视化的工具。'
- en: '[Alibi](https://github.com/SeldonIO/alibi) [8]: This package is similar to
    InterpretML and ELI5, providing many explainers and white box models.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Alibi](https://github.com/SeldonIO/alibi) [8]：该包类似于InterpretML和ELI5，提供了许多解释器和白盒模型。'
- en: Fairness
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公平性
- en: '[Fairlearn](https://github.com/fairlearn/fairlearn) [9]: Fairlearn is a library
    similar to AIF360, providing fairness-promoting tools. This package shares many
    of the algorithms found in AIF360.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Fairlearn](https://github.com/fairlearn/fairlearn) [9]: Fairlearn 是一个类似于 AIF360
    的库，提供促进公平的工具。这个包包含了许多 AIF360 中的算法。'
- en: '[Aequitas](https://github.com/dssg/aequitas) [10]: Aequitas is a bias audit
    toolkit that is a library and a [web application](http://aequitas.dssg.io/). Using
    this tool, you can generate reports on the systemic biases potentially present
    in your data.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Aequitas](https://github.com/dssg/aequitas) [10]: Aequitas 是一个偏见审计工具包，它既是一个库也是一个
    [web 应用程序](http://aequitas.dssg.io/)。使用这个工具，你可以生成关于数据中可能存在的系统性偏见的报告。'
- en: '[FairML](https://github.com/adebayoj/fairml) [11]: FairML is a library that
    quantifies a model''s inputs'' relative significance and predictive dependency.
    This tool can help audit predictive models.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[FairML](https://github.com/adebayoj/fairml) [11]: FairML 是一个量化模型输入相对重要性和预测依赖性的库。这个工具可以帮助审计预测模型。'
- en: Conclusion
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In the end, the collective effort to embrace interpretability and fairness in
    machine learning will lead us toward a future where AI systems are accurate and
    powerful but also transparent, fair, and trustworthy, ultimately benefiting developers
    and end-users alike. By harnessing the capabilities of these Python packages and
    embracing a commitment to ethical AI, we can pave the way for a more inclusive
    and responsible AI-driven world.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，拥抱可解释性和公平性的集体努力将引领我们迈向一个未来，在这个未来中，AI 系统不仅准确强大，而且透明、公正和可信，*从而最终惠及开发者和最终用户*。通过利用这些
    Python 包的能力并承诺于伦理 AI，我们可以为一个更加包容和负责任的 AI 驱动世界铺平道路。
- en: Resources and References
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源与参考文献
- en: 'If you are interested in interpretable machine learning and forecasting, consider
    giving me a follow: [https://medium.com/@upadhyan](https://medium.com/@upadhyan)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你对可解释的机器学习和预测感兴趣，可以关注我：[https://medium.com/@upadhyan](https://medium.com/@upadhyan)
- en: 'For other articles on Ethical and Interpretable AI, check out the reading list
    below:'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欲了解更多有关伦理和可解释 AI 的文章，请查看以下阅读列表：
- en: '![Nakul Upadhya](../Images/e62aa67aa11cd0f9bcd1132257fc3773.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![Nakul Upadhya](../Images/e62aa67aa11cd0f9bcd1132257fc3773.png)'
- en: '[Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----ed0e336d82a--------------------------------)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----ed0e336d82a--------------------------------)'
- en: Interpretable and Ethical AI
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释和伦理的 AI
- en: '[View list](https://medium.com/@upadhyan/list/interpretable-and-ethical-ai-f6ee1f0b476d?source=post_page-----ed0e336d82a--------------------------------)5
    stories![](../Images/3718151c0f72303f3d1c71f54229bc98.png)![](../Images/eddb4279ebae7fc1ba79cf6dcc6ebd5a.png)![](../Images/7def8e23dad656929857f2a488b1f547.png)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@upadhyan/list/interpretable-and-ethical-ai-f6ee1f0b476d?source=post_page-----ed0e336d82a--------------------------------)5
    个故事![](../Images/3718151c0f72303f3d1c71f54229bc98.png)![](../Images/eddb4279ebae7fc1ba79cf6dcc6ebd5a.png)![](../Images/7def8e23dad656929857f2a488b1f547.png)'
- en: References
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Nori, H., Jenkins, S., Koch, P., & Caruana, R. (2019). InterpretML: A Unified
    Framework for Machine Learning Interpretability*. arXiv preprint arXiv:1909.09223*.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Nori, H., Jenkins, S., Koch, P., & Caruana, R. (2019). InterpretML: 一个统一的机器学习可解释性框架*。arXiv
    预印本 arXiv:1909.09223*。'
- en: '[2] Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh,
    Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi
    Yan, & Orion Reblitz-Richardson. (2020). Captum: A unified and generic model interpretability
    library for PyTorch.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh,
    Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi
    Yan, & Orion Reblitz-Richardson. (2020). Captum: 一个统一且通用的 PyTorch 模型可解释性库。'
- en: '[3] Rachel K. E. Bellamy, Kuntal Dey, Michael Hind and Samuel C. Hoffman, Stephanie
    Houde, Kalapriya Kannan and Pranay Lohia, Jacquelyn Martino, Sameep Mehta and
    Aleksandra Mojsilovic, Seema Nagar, Karthikeyan Natesan Ramamurthy and John Richards,
    Diptikalyan Saha, Prasanna Sattigeri and Moninder Singh, Kush R. Varshney, & Yunfeng
    Zhang. (2018). AI Fairness 360: An Extensible Toolkit for Detecting, Understanding,
    and Mitigating Unwanted Algorithmic Bias.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Rachel K. E. Bellamy, Kuntal Dey, Michael Hind 和 Samuel C. Hoffman, Stephanie
    Houde, Kalapriya Kannan 和 Pranay Lohia, Jacquelyn Martino, Sameep Mehta 和 Aleksandra
    Mojsilovic, Seema Nagar, Karthikeyan Natesan Ramamurthy 和 John Richards, Diptikalyan
    Saha, Prasanna Sattigeri 和 Moninder Singh, Kush R. Varshney, & Yunfeng Zhang.
    (2018). AI Fairness 360: 一个可扩展的工具包，用于检测、理解和缓解不希望出现的算法偏见。'
- en: '[4] Lundberg, S., & Lee, S.I. (2017). A Unified Approach to Interpreting Model
    Predictions*. Advances in Neural Information Processing Systems 30*, 4765–4774.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Lundberg, S., & Lee, S.I. (2017). 解释模型预测的统一方法*。神经信息处理系统进展 30*, 4765–4774。'
- en: '[5] Marco Tulio Ribeiro, Sameer Singh, & Carlos Guestrin (2016). “Why Should
    I Trust You?”: Explaining the Predictions of Any Classifier. In *Proceedings of
    the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
    San Francisco, CA, USA, August 13–17, 2016* (pp. 1135–1144).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Marco Tulio Ribeiro, Sameer Singh, & Carlos Guestrin (2016). “我为什么应该相信你？”：解释任何分类器的预测。在
    *第22届 ACM SIGKDD 国际知识发现与数据挖掘大会论文集，旧金山，加州，美国，2016年8月13–17日*（第1135–1144页）。'
- en: '[6] TeamHG-Memex (2019) ELI5\. *Github*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] TeamHG-Memex (2019) ELI5\。*Github*'
- en: '[7] Bengfort, B., & Bilbro, R. (2019). Yellowbrick: Visualizing the Scikit-Learn
    Model Selection Process*. The Journal of Open Source Software, 4(35).*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Bengfort, B., & Bilbro, R. (2019). Yellowbrick: 可视化 Scikit-Learn 模型选择过程*。开放源软件期刊，4(35)。*'
- en: '[8] Janis Klaise, Arnaud Van Looveren, Giovanni Vacanti, & Alexandru Coca (2021).
    Alibi Explain: Algorithms for Explaining Machine Learning Models*. Journal of
    Machine Learning Research, 22(181), 1–7.*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Janis Klaise, Arnaud Van Looveren, Giovanni Vacanti, & Alexandru Coca (2021).
    Alibi Explain: 解释机器学习模型的算法*。机器学习研究期刊，22(181)，1–7。*'
- en: '[9] Bird, S., Dudik, M., Edgar, R., Horn, B., Lutz, R., Milan, V., Sameki,
    M., Wallach, H., & Walker, K. (2020). *Fairlearn: A toolkit for assessing and
    improving fairness in AI* [White paper]. Microsoft.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Bird, S., Dudik, M., Edgar, R., Horn, B., Lutz, R., Milan, V., Sameki,
    M., Wallach, H., & Walker, K. (2020). *Fairlearn: 评估和改进 AI 公平性的工具包* [白皮书]。微软。'
- en: '[10] Saleiro, P., Kuester, B., Stevens, A., Anisfeld, A., Hinkson, L., London,
    J., & Ghani, R. (2018). Aequitas: A Bias and Fairness Audit Toolkit*. arXiv preprint
    arXiv:1811.*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Saleiro, P., Kuester, B., Stevens, A., Anisfeld, A., Hinkson, L., London,
    J., & Ghani, R. (2018). Aequitas: 偏见和公平性审计工具包*。arXiv 预印本 arXiv:1811。*'
- en: '[11] Adebayo, J. A. (2016). *FairML: ToolBox for diagnosing bias in predictive
    modeling* (Doctoral dissertation, Massachusetts Institute of Technology).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Adebayo, J. A. (2016). *FairML: 诊断预测建模偏差的工具箱*（博士论文，麻省理工学院）。'
