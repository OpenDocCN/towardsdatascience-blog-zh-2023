["```py\n!pip install -qq nougat-ocr\n\ndef nougat_ocr(file_name):\n\n  # Command to run\n  cli_command = [\n      'nougat',\n      '--out', 'output',\n      'pdf', file_name,\n      '--checkpoint', CHECKPOINT,\n      '--markdown'\n  ]\n\n  # Run the command\n  subprocess.run(cli_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n\n  return\n```", "```py\nRecent language models are consists of an embedding layer, \\(N\\) stacked transformer layers, and an affine layer \\(\\phi(\\cdot)\\) for predicting the next-word distributution. Given a sequence of tokens \\(\\{x_{1},x_{2},\\ldots,x_{t-1}\\}\\), the embedding layer first embeds the tokens into a sequence of vectors \\(H_{0}=\\{h_{1}^{(0)},\\ldots,h_{t-1}^{(0)}\\}\\). Then \\(H_{0}\\) would be processed by each of the transformer layers successively. We denote the output of the \\(j\\)-th layer as \\(H_{j}\\). Then, the vocabulary head \\(\\phi(\\cdot)\\) predicts the probability of the next token \\(x_{t}\\)\n\n\\[p(x_{t}\\mid x_{<t})=\\mathrm{softmax}\\big{(}\\phi(h_{t}^{N})\\big{)}_{x_{t}}, \\quad x_{t}\\in\\mathcal{X},\\]\n\nwhere \\(\\mathcal{X}\\) is the vocabulary set.\n\nInstead of applying \\(\\phi\\) just on the final layer, our approach contrasts the higher-layer and lower-layer information to obtain the probability of next token. More specifically, for the lower layers, we also compute the probability of the next tokens using \\(\\phi(\\cdot)\\),\n\n\\[q_{j}(x_{t}\\mid x_{<t})=\\mathrm{softmax}\\big{(}\\phi(h_{t}^{j})\\big{)}_{x_{t}}, \\quad j=1,\\ldots,N.\\]\n\nThe idea of applying language heads directly to the hidden states of the middle layers, known as _early exit_(Teerapittayanon et al., 2016; Elbayad et al., 2020; Schuster et al., 2022), has proven to be an effective inference method even without special training process (Kao et al., 2020), as the residual connections (He et al., 2016) in transformer layers make the hidden representations gradually evolve without abrupt changes. Using \\(q_{j}(x_{t})\\) to represent \\(q_{j}(x_{t}\\mid x_{<t})\\) for notational brevity, we then compute the probability of the next token by,\n\n\\[\\hat{p}(x_{t}\\mid x_{<t}) =\\mathrm{softmax}\\big{(}\\mathcal{F}\\big{(}q_{N}(x_{t}),q_{M}(x_{t })\\big{)}\\big{)}_{x_{t}}, \\tag{1}\\] \\[\\text{where}\\quad M =\\operatorname*{arg\\,max}_{j\\in\\mathcal{J}}\\;d\\big{(}q_{N}(\\cdot ),q_{j}(\\cdot)\\big{)}.\\]\n\nHere, layer \\(M\\) is referred to as the _premature layer_, while the final layer is referred to as the _mature layer_. The operator \\(\\mathcal{F}(\\cdot,\\cdot)\\), to be elaborated further in Section 2.3, is used to contrast between the output distributions from the premature layer and the mature layer by computing the difference between two distributions in the log domain. The premature layer is dynamically selected in each decoding step using a distributional distance measure \\(d(\\cdot,\\cdot)\\) (we use the Jensen-Shannon Divergence) between the mature layer and all the candidate layers in \\(\\mathcal{J}\\). We discuss \\(d(\\cdot,\\cdot)\\) in more detail in Section 2.1 and Section 2.2\\. The motivation for selecting the layer with the highest distance \\(d(\\cdot,\\cdot)\\) as the premature layer is to maximize the difference between the mature/premature layers.\n```", "```py\n\\begin{table}\n\\begin{tabular}{l c c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{4}{c}{**TruthfulQA**} & \\multicolumn{2}{c}{**FACTOR**} \\\\ \\cline{2-6}  & **MC1** & **MC2** & **MC3** & **News** & **Wiki** \\\\ \\hline LLaMa-7B & 25.6 & 40.6 & 19.2 & 58.3 & 58.6 \\\\ + ITI (Li et al., 2023) & 25.9 & - & - & - & - \\\\ + DoLa & **32.2** & **63.8** & **32.1** & **62.0** & **62.2** \\\\ \\hline LLaMa-13B & 28.3 & 43.3 & 20.8 & 61.1 & 62.6 \\\\ + CD (Li et al., 2022) & 24.4 & 41.0 & 19.0 & 62.3 & 64.4 \\\\ + DoLa & **28.9** & **64.9** & **34.8** & **62.5** & **66.2** \\\\ \\hline LLaMa-33B & 31.7 & 49.5 & 24.2 & 63.8 & 69.5 \\\\ + CD (Li et al., 2022) & **33.0** & 51.8 & 25.7 & 63.3 & **71.3** \\\\ + DoLa & 30.5 & **62.3** & **34.0** & **65.4** & 70.3 \\\\ \\hline LLaMa-65B & 30.8 & 46.9 & 22.7 & 63.6 & 72.2 \\\\ + CD (Li et al., 2022) & 29.3 & 47.0 & 21.5 & 64.6 & 71.3 \\\\ + DoLa & **31.1** & **64.6** & **34.3** & **66.2** & **72.4** \\\\ \\hline \\hline \\end{tabular}\n\\end{table}\nTable 1: Multiple choices results on the TruthfulQA and FACTOR.\n```", "```py\nfrom langchain.prompts import (\n    ChatPromptTemplate,\n    PromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import LLMChain\n\ncontext_template=\"You are a helpful AI Researcher that specializes in analysing research paper outputs presented to you in Latex\"\n\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(context_template)\n\nhuman_template= \"\"\"\nPlease extract all tables referenced in this paper. The tables are in Latex format. Summarize the tables one by one. \nEach summary should be 4-5 sentences long. Include numbers in summary where you can. Make a dictionary with table number, table name and summary of that table. \n\n PAPER: {paper_content}\n\n \"\"\"\n\nhuman_message_prompt = HumanMessagePromptTemplate(\n            prompt=PromptTemplate(\n            template=human_template,\n            input_variables=[\"paper_content\"],))\n\nchat_prompt_template = ChatPromptTemplate.from_messages([system_message_prompt,\n                                                         human_message_prompt])\n\nchat = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\",\n                  temperature=0.2)\n\nsummary_chain = LLMChain(llm=chat, prompt=chat_prompt_template)\n\noutput = summary_chain.run(noref_content)\n\npprint.pprint(output)\n```", "```py\n('Dictionary of Tables:\\n'\n '\\n'\n 'Table 1:\\n'\n '- Table Number: 1\\n'\n '- Table Name: Multiple choices results on the TruthfulQA and FACTOR '\n 'datasets\\n'\n '- Summary: This table presents the results of multiple choice tasks on the '\n 'TruthfulQA and FACTOR datasets. It compares the performance of different '\n 'models, including the baseline, Inference Time Intervention (ITI), and DoLa. '\n 'The table shows that DoLa consistently outperforms the other methods, '\n 'improving the truthfulness and informativeness scores.\\n'\n '\\n'\n 'Table 2:\\n'\n '- Table Number: 2\\n'\n '- Table Name: Open-ended generation results on TruthfulQA, StrategyQA, and '\n 'GSM8K\\n'\n '- Summary: This table summarizes the results of open-ended generation tasks '\n 'on the TruthfulQA, StrategyQA, and GSM8K datasets. It compares the '\n 'performance of different models, including the baseline, Contrastive '\n 'Decoding (CD), and DoLa. The table shows that DoLa consistently enhances the '\n 'truthfulness and informativeness scores, outperforming the other methods.\\n'\n '\\n'\n 'Table 3:\\n'\n '- Table Number: 3\\n'\n '- Table Name: Multiple choices results on the FACTOR dataset\\n'\n '- Summary: This table presents the results of multiple choice tasks on the '\n 'FACTOR dataset. It compares the performance of different models, including '\n 'the baseline, DoLa with dynamic premature layer selection, and DoLa with '\n 'random layer selection. The table shows that DoLa with dynamic premature '\n 'layer selection performs better than the other methods, improving the '\n 'truthfulness and informativeness scores.\\n'\n '\\n'\n 'Table 4:\\n'\n '- Table Number: 4\\n'\n '- Table Name: Comparison of MPT-7B and modifications on TruthfulQA, FACTOR, '\n 'and CoT datasets\\n'\n '- Summary: This table compares the performance of the MPT-7B model and its '\n 'modifications on the TruthfulQA, FACTOR, and CoT datasets. It shows that '\n 'DoLa improves the truthfulness and truthfulness+informativeness scores on '\n 'most datasets, indicating the potential of DoLa to generalize across '\n 'different transformer models.\\n'\n '\\n'\n 'Table 5:\\n'\n '- Table Number: 5\\n'\n '- Table Name: Qualitative study for LLaMA-33B on TruthfulQA\\n'\n '- Summary: This table presents qualitative examples from the TruthfulQA '\n 'dataset, comparing the answers generated by the baseline and DoLa using the '\n 'LLaMA-33B model. It shows that DoLa produces more truthful and informative '\n 'answers compared to the baseline.\\n'\n '\\n'\n 'Table 6:\\n'\n '- Table Number: 6\\n'\n '- Table Name: Averaged decoding latency per token in milliseconds\\n'\n '- Summary: This table shows the average decoding latency per token in '\n 'milliseconds for the baseline and DoLa. It indicates that DoLa adds a small '\n 'additional latency to the decoding process, making it a practical and '\n 'efficient decoding strategy.')\n```"]