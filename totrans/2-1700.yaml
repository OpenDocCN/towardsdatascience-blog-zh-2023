- en: 'Prompt Engineering: How to Trick AI into Solving Your Problems'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æç¤ºå·¥ç¨‹ï¼šå¦‚ä½•è®©AIè§£å†³ä½ çš„é—®é¢˜
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f](https://towardsdatascience.com/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f](https://towardsdatascience.com/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)
- en: 7 prompting tricks, LangChain, and Python example code
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7ä¸ªæç¤ºæŠ€å·§ã€LangChainå’ŒPythonç¤ºä¾‹ä»£ç 
- en: '[](https://shawhin.medium.com/?source=post_page-----7ce1ed3b553f--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----7ce1ed3b553f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7ce1ed3b553f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7ce1ed3b553f--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----7ce1ed3b553f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shawhin.medium.com/?source=post_page-----7ce1ed3b553f--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----7ce1ed3b553f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7ce1ed3b553f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7ce1ed3b553f--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----7ce1ed3b553f--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7ce1ed3b553f--------------------------------)
    Â·14 min readÂ·Aug 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7ce1ed3b553f--------------------------------)
    Â·14åˆ†é’Ÿé˜…è¯»Â·2023å¹´8æœˆ25æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: This is the fourth article in a [series on using large language models](/a-practical-introduction-to-llms-65194dda1148)
    (LLMs) in practice. Here, I will discuss prompt engineering (PE) and how to use
    it to build LLM-enabled applications. I start by reviewing key PE techniques and
    then walk through Python example code of using LangChain to build an LLM-based
    application.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯[å¤§å‹è¯­è¨€æ¨¡å‹å®è·µç³»åˆ—]( /a-practical-introduction-to-llms-65194dda1148)ä¸­çš„ç¬¬å››ç¯‡æ–‡ç« ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘å°†è®¨è®ºæç¤ºå·¥ç¨‹ï¼ˆPEï¼‰ä»¥åŠå¦‚ä½•ä½¿ç”¨å®ƒæ¥æ„å»ºæ”¯æŒLLMçš„åº”ç”¨ç¨‹åºã€‚æˆ‘é¦–å…ˆå›é¡¾å…³é”®çš„PEæŠ€æœ¯ï¼Œç„¶åé€šè¿‡Pythonç¤ºä¾‹ä»£ç æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨LangChainæ„å»ºåŸºäºLLMçš„åº”ç”¨ç¨‹åºã€‚
- en: '![](../Images/51cee05215f7e6b279687f028ed20dcc.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51cee05215f7e6b279687f028ed20dcc.png)'
- en: Photo by [Jason Leung](https://unsplash.com/@ninjason?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Jason Leung](https://unsplash.com/@ninjason?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥è‡ª [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: When first hearing about prompt engineering, many technical people (including
    myself) tend to scoff at the idea. We might think, â€œ*Prompt engineering? Psssh,
    thatâ€™s lame. Tell me how to build an LLM from scratch.*â€
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å½“é¦–æ¬¡å¬è¯´æç¤ºå·¥ç¨‹æ—¶ï¼Œè®¸å¤šæŠ€æœ¯äººå‘˜ï¼ˆåŒ…æ‹¬æˆ‘è‡ªå·±ï¼‰å¾€å¾€å¯¹è¿™ä¸ªæƒ³æ³•å—¤ä¹‹ä»¥é¼»ã€‚æˆ‘ä»¬å¯èƒ½ä¼šæƒ³ï¼Œâ€œ*æç¤ºå·¥ç¨‹ï¼Ÿå™—ï¼Œè¿™å¤ªæ— èŠäº†ã€‚å‘Šè¯‰æˆ‘æ€ä¹ˆä»å¤´å¼€å§‹æ„å»ºä¸€ä¸ªLLMå§ã€‚*â€
- en: However, after diving into it more deeply, Iâ€™d caution developers against writing
    off prompt engineering automatically. Iâ€™ll go even further and say that **prompt
    engineering can realize 80% of the value** of most LLM use cases with (relatively)
    very low effort.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæ·±å…¥æ¢è®¨ä¹‹åï¼Œæˆ‘è¦æé†’å¼€å‘è€…ä¸è¦è‡ªåŠ¨å¿½è§†æç¤ºå·¥ç¨‹ã€‚æˆ‘ç”šè‡³å¯ä»¥è¯´ï¼Œ**æç¤ºå·¥ç¨‹å¯ä»¥å®ç°å¤§å¤šæ•°LLMä½¿ç”¨æ¡ˆä¾‹çš„80%ä»·å€¼**ï¼Œä¸”ï¼ˆç›¸å¯¹ï¼‰èŠ±è´¹çš„ç²¾åŠ›éå¸¸å°‘ã€‚
- en: My goal with this article is to convey this point via a practical review of
    prompt engineering and illustrative examples. While there are surely gaps in what
    prompt engineering can do, it opens the door to discovering simple and clever
    solutions to our problems.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å†™è¿™ç¯‡æ–‡ç« çš„ç›®æ ‡æ˜¯é€šè¿‡å¯¹æç¤ºå·¥ç¨‹çš„å®é™…å›é¡¾å’Œç¤ºä¾‹æ¥ä¼ è¾¾è¿™ä¸€è§‚ç‚¹ã€‚è™½ç„¶æç¤ºå·¥ç¨‹çš„åŠŸèƒ½ç¡®å®æœ‰ä¸€äº›ä¸è¶³ï¼Œä½†å®ƒä¸ºå‘ç°ç®€å•è€Œèªæ˜çš„è§£å†³æ–¹æ¡ˆæ‰“å¼€äº†å¤§é—¨ã€‚
- en: Supplemental Video.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: é™„åŠ è§†é¢‘ã€‚
- en: '**What is Prompt Engineering?**'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ä»€ä¹ˆæ˜¯æç¤ºå·¥ç¨‹ï¼Ÿ**'
- en: In the [first article of this series](/a-practical-introduction-to-llms-65194dda1148),
    I defined **prompt engineering** as **any use of an LLM out-of-the-box** (i.e.
    not training any internal model parameters). However, there is much more that
    can be said about it.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[æœ¬ç³»åˆ—çš„ç¬¬ä¸€ç¯‡æ–‡ç« ](/a-practical-introduction-to-llms-65194dda1148)ä¸­ï¼Œæˆ‘å°†**æç¤ºå·¥ç¨‹**å®šä¹‰ä¸º**ä»»ä½•å¼€ç®±å³ç”¨çš„LLMçš„ä½¿ç”¨**ï¼ˆå³ä¸è®­ç»ƒä»»ä½•å†…éƒ¨æ¨¡å‹å‚æ•°ï¼‰ã€‚ç„¶è€Œï¼Œè¿˜æœ‰æ›´å¤šå¯ä»¥è¯´çš„ã€‚
- en: Prompt Engineering is â€œ*the means by which LLMs are programmed with prompts.*â€
    [[1](https://arxiv.org/abs/2302.11382)]
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æç¤ºå·¥ç¨‹æ˜¯â€œ*é€šè¿‡æç¤ºç¼–ç¨‹LLMçš„æ–¹æ³•ã€‚*â€ [[1](https://arxiv.org/abs/2302.11382)]
- en: Prompt Engineering is â€œa*n empirical art of composing and formatting the prompt
    to maximize a modelâ€™s performance on a desired task.*â€ [[2](https://arxiv.org/abs/2106.09685)]
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æç¤ºå·¥ç¨‹æ˜¯â€œ*ä¸€ç§å°†æç¤ºè¿›è¡Œæ„å»ºå’Œæ ¼å¼åŒ–çš„ç»éªŒæ€§è‰ºæœ¯ï¼Œä»¥æœ€å¤§åŒ–æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°*ã€‚â€ [[2](https://arxiv.org/abs/2106.09685)]
- en: '*â€œlanguage modelsâ€¦ want to complete documents, and so you can trick them into
    performing tasks just by arranging fake documents*.â€ [[3](https://www.youtube.com/watch?v=bZQun8Y4L2A)]'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*â€œè¯­è¨€æ¨¡å‹â€¦å¸Œæœ›å®Œæˆæ–‡æ¡£ï¼Œå› æ­¤ä½ å¯ä»¥é€šè¿‡å®‰æ’è™šå‡çš„æ–‡æ¡£æ¥æ¬ºéª—å®ƒä»¬æ‰§è¡Œä»»åŠ¡*ã€‚â€ [[3](https://www.youtube.com/watch?v=bZQun8Y4L2A)]'
- en: The first definition conveys the key innovation coming from LLMs, which is that
    **computers can now be programmed using plain English**. The second point frames
    prompt engineering as a largely empirical endeavor, where practitioners, tinkerers,
    and builders are the key explorers of this new way of programming.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªå®šä¹‰ä¼ è¾¾äº†æ¥è‡ª LLM çš„å…³é”®åˆ›æ–°ï¼Œå³**è®¡ç®—æœºç°åœ¨å¯ä»¥ä½¿ç”¨ç®€å•çš„è‹±è¯­è¿›è¡Œç¼–ç¨‹**ã€‚ç¬¬äºŒç‚¹å°†æç¤ºå·¥ç¨‹æ¡†æ¶åŒ–ä¸ºä¸€ç§ä¸»è¦ç»éªŒæ€§çš„å·¥ä½œï¼Œå…¶ä¸­ä»ä¸šè€…ã€ä¿®è¡¥è€…å’Œæ„å»ºè€…æ˜¯è¿™ä¸€æ–°ç¼–ç¨‹æ–¹å¼çš„ä¸»è¦æ¢ç´¢è€…ã€‚
- en: The third point (from [Andrej Karpathy](https://medium.com/u/ac9d9a35533e?source=post_page-----7ce1ed3b553f--------------------------------))
    reminds us that **LLMs arenâ€™t explicitly trained to do almost anything we ask
    them to do**. Thus, in some sense, we are â€œtrickingâ€ these language models to
    solve problems. I feel this captures the essence of prompt engineering, which
    relies less on your technical skills and more on your creativity.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰ç‚¹ï¼ˆæ¥è‡ª [Andrej Karpathy](https://medium.com/u/ac9d9a35533e?source=post_page-----7ce1ed3b553f--------------------------------)ï¼‰æé†’æˆ‘ä»¬**LLM
    å¹¶æœªæ˜ç¡®è®­ç»ƒæ¥åšå‡ ä¹æˆ‘ä»¬è¦æ±‚çš„ä»»ä½•äº‹æƒ…**ã€‚å› æ­¤ï¼Œä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼Œæˆ‘ä»¬æ˜¯åœ¨â€œæ¬ºéª—â€è¿™äº›è¯­è¨€æ¨¡å‹ä»¥è§£å†³é—®é¢˜ã€‚æˆ‘è§‰å¾—è¿™æ•æ‰åˆ°äº†æç¤ºå·¥ç¨‹çš„æœ¬è´¨ï¼Œå®ƒä¾èµ–äºä½ çš„åˆ›é€ åŠ›è€ŒéæŠ€æœ¯æŠ€èƒ½ã€‚
- en: '**2 Levels of Prompt Engineering**'
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**æç¤ºå·¥ç¨‹çš„ä¸¤ä¸ªå±‚æ¬¡**'
- en: There are two distinct ways in which one can do prompt engineering, which I
    called the â€œ**easy way**â€ and the â€œ**less easy way**â€ in the [first article](/a-practical-introduction-to-llms-65194dda1148)
    of this series.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥é€šè¿‡ä¸¤ç§ä¸åŒçš„æ–¹å¼è¿›è¡Œæç¤ºå·¥ç¨‹ï¼Œæˆ‘åœ¨æœ¬ç³»åˆ—çš„ [ç¬¬ä¸€ç¯‡æ–‡ç« ](/a-practical-introduction-to-llms-65194dda1148)
    ä¸­å°†å…¶ç§°ä¸ºâ€œ**ç®€å•æ–¹æ³•**â€å’Œâ€œ**è¾ƒéš¾çš„æ–¹æ³•**â€ã€‚
- en: The Easy Way
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç®€å•çš„æ–¹æ³•
- en: This is how most of the world does prompt engineering, which is via ChatGPT
    (or something similar). It is an intuitive, no-code, and cost-free way to interact
    with an LLM.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¤§å¤šæ•°ä¸–ç•Œä¸Šäººä»¬è¿›è¡Œæç¤ºå·¥ç¨‹çš„æ–¹å¼ï¼Œå³é€šè¿‡ ChatGPTï¼ˆæˆ–ç±»ä¼¼çš„å·¥å…·ï¼‰ã€‚è¿™æ˜¯ä¸€ç§ç›´è§‚çš„ã€æ— éœ€ç¼–ç ä¸”æ— éœ€è´¹ç”¨çš„ä¸ LLM äº’åŠ¨çš„æ–¹å¼ã€‚
- en: While this is a great approach for something quick and simple, e.g. summarizing
    a page of text, rewriting an email, helping you brainstorm birthday party plans,
    etc., it has its downsides. A big one is that **itâ€™s not easy to integrate this
    approach into a larger automated process or software system**. To do this, we
    need to go one step further.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™ç§æ–¹æ³•é€‚åˆå¿«é€Ÿå’Œç®€å•çš„ä»»åŠ¡ï¼Œä¾‹å¦‚æ€»ç»“ä¸€é¡µæ–‡æœ¬ã€é‡å†™ä¸€å°é‚®ä»¶ã€å¸®åŠ©ä½ å¤´è„‘é£æš´ç”Ÿæ—¥æ´¾å¯¹è®¡åˆ’ç­‰ï¼Œä½†å®ƒä¹Ÿæœ‰å…¶ç¼ºç‚¹ã€‚ä¸€ä¸ªä¸»è¦é—®é¢˜æ˜¯**å°†è¿™ç§æ–¹æ³•æ•´åˆåˆ°æ›´å¤§çš„è‡ªåŠ¨åŒ–æµç¨‹æˆ–è½¯ä»¶ç³»ç»Ÿä¸­å¹¶ä¸å®¹æ˜“**ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦æ›´è¿›ä¸€æ­¥ã€‚
- en: The Less Easy Way
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¾ƒéš¾çš„æ–¹æ³•
- en: This resolves many of the drawbacks of the â€œeasy wayâ€ by interacting with LLMs
    programmatically i.e. using Python. We got a sense of how we can do this in the
    previous two articles of this series, where explored [OpenAIâ€™s Python API](/cracking-open-the-openai-python-api-230e4cae7971)
    and the [Hugging Face Transformers library](/cracking-open-the-hugging-face-transformers-library-350aa0ef0161).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é€šè¿‡ä»¥ç¼–ç¨‹æ–¹å¼ä¸ LLM äº’åŠ¨æ¥è§£å†³äº†â€œç®€å•æ–¹æ³•â€çš„è®¸å¤šç¼ºç‚¹ï¼Œå³ä½¿ç”¨ Pythonã€‚æˆ‘ä»¬åœ¨æœ¬ç³»åˆ—çš„å‰ä¸¤ç¯‡æ–‡ç« ä¸­äº†è§£äº†å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ï¼Œå…¶ä¸­æ¢ç´¢äº† [OpenAI
    çš„ Python API](/cracking-open-the-openai-python-api-230e4cae7971) å’Œ [Hugging Face
    Transformers åº“](/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)ã€‚
- en: While this requires more technical knowledge, **this is where the real power
    of prompt engineering lies** because it allows developers to integrate LLM-based
    modules into larger software systems.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™éœ€è¦æ›´å¤šçš„æŠ€æœ¯çŸ¥è¯†ï¼Œ**ä½†è¿™æ­£æ˜¯æç¤ºå·¥ç¨‹çš„çœŸæ­£åŠ›é‡æ‰€åœ¨**ï¼Œå› ä¸ºå®ƒå…è®¸å¼€å‘äººå‘˜å°†åŸºäº LLM çš„æ¨¡å—é›†æˆåˆ°æ›´å¤§çš„è½¯ä»¶ç³»ç»Ÿä¸­ã€‚
- en: A good (and perhaps ironic) example of this is ChatGPT. The core of this product
    is prompting a pre-trained model (i.e. GPT-3.5-turbo) to act like a chatbot and
    then wrapping it in an easy-to-use web interface.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¥½çš„ï¼ˆä¹Ÿè®¸æ˜¯å…·æœ‰è®½åˆºæ„å‘³çš„ï¼‰ä¾‹å­æ˜¯ ChatGPTã€‚è¿™ä¸ªäº§å“çš„æ ¸å¿ƒæ˜¯æç¤ºä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹ï¼ˆå³ GPT-3.5-turboï¼‰å……å½“èŠå¤©æœºå™¨äººï¼Œç„¶åå°†å…¶å°è£…åœ¨ä¸€ä¸ªæ˜“äºä½¿ç”¨çš„ç½‘é¡µç•Œé¢ä¸­ã€‚
- en: Of course, developing GPT-3.5-turbo is the hard part, **but thatâ€™s not something
    we need to worry about here**. With all the pre-trained LLMs we have at our fingertips,
    almost anyone with basic programming skills can create a powerful AI application
    like ChatGPT without being an AI researcher or a machine learning Ph.D.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œå¼€å‘GPT-3.5-turboæ˜¯å›°éš¾çš„ï¼Œ**ä½†è¿™ä¸æ˜¯æˆ‘ä»¬éœ€è¦æ‹…å¿ƒçš„äº‹æƒ…**ã€‚å€ŸåŠ©æˆ‘ä»¬æ‰‹å¤´çš„æ‰€æœ‰é¢„è®­ç»ƒLLMï¼Œå‡ ä¹ä»»ä½•å…·å¤‡åŸºæœ¬ç¼–ç¨‹æŠ€èƒ½çš„äººéƒ½å¯ä»¥åˆ›å»ºä¸€ä¸ªåƒChatGPTè¿™æ ·çš„å¼ºå¤§AIåº”ç”¨ç¨‹åºï¼Œè€Œä¸å¿…æ˜¯AIç ”ç©¶å‘˜æˆ–æœºå™¨å­¦ä¹ åšå£«ã€‚
- en: '**Building AI Apps with Prompt Engineering**'
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**åˆ©ç”¨æç¤ºå·¥ç¨‹æ„å»ºAIåº”ç”¨**'
- en: The less easy way unlocks a **new paradigm of programming and software development**.
    No longer are developers required to define every inch of logic in their software
    systems. They now have the option to offload a non-trivial portion to LLMs. Letâ€™s
    look at a concrete example of what this might look like.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å›°éš¾çš„æ–¹æ³•è§£é”äº†**ç¼–ç¨‹å’Œè½¯ä»¶å¼€å‘çš„æ–°èŒƒå¼**ã€‚å¼€å‘è€…ä¸å†éœ€è¦åœ¨è½¯ä»¶ç³»ç»Ÿä¸­å®šä¹‰æ¯ä¸€å¯¸é€»è¾‘ã€‚ä»–ä»¬ç°åœ¨å¯ä»¥é€‰æ‹©å°†éçç¢çš„éƒ¨åˆ†è½¬ç§»ç»™LLMã€‚è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªå…·ä½“çš„ä¾‹å­ï¼Œè¿™å¯èƒ½æ˜¯ä»€ä¹ˆæ ·çš„ã€‚
- en: Suppose you want to create an **automatic grader for a high school history class**.
    The trouble, however, is that all the questions have written responses, so there
    often can be multiple versions of a correct answer. For example, the following
    responses to â€œ*Who was the 35th president of the United States of America?*â€ could
    be correct.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ä½ æƒ³ä¸ºé«˜ä¸­å†å²è¯¾åˆ›å»ºä¸€ä¸ª**è‡ªåŠ¨è¯„åˆ†ç³»ç»Ÿ**ã€‚é—®é¢˜åœ¨äºæ‰€æœ‰é—®é¢˜éƒ½æœ‰ä¹¦é¢å›ç­”ï¼Œå› æ­¤é€šå¸¸ä¼šæœ‰å¤šä¸ªæ­£ç¡®ç­”æ¡ˆçš„ç‰ˆæœ¬ã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹å¯¹â€œ*è°æ˜¯ç¾å›½çš„ç¬¬35ä»»æ€»ç»Ÿï¼Ÿ*â€çš„å›ç­”å¯èƒ½éƒ½æ˜¯æ­£ç¡®çš„ã€‚
- en: John F. Kennedy
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çº¦ç¿°Â·FÂ·è‚¯å°¼è¿ª
- en: JFK
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JFK
- en: Jack Kennedy (a common nickname)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ°å…‹Â·è‚¯å°¼è¿ªï¼ˆä¸€ä¸ªå¸¸è§çš„æ˜µç§°ï¼‰
- en: John Fitzgerald Kennedy (probably trying to get extra credit)
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çº¦ç¿°Â·FÂ·è‚¯å°¼è¿ªï¼ˆå¯èƒ½è¯•å›¾è·å¾—é¢å¤–çš„å­¦åˆ†ï¼‰
- en: John F. Kenedy (misspelled last name)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çº¦ç¿°Â·FÂ·è‚¯å°¼è¿ªï¼ˆæ‹¼å†™é”™è¯¯çš„å§“æ°ï¼‰
- en: In the **traditional programming paradigm**, it was on the developer to figure
    out how to account for all these variations. To do this, they might list all possible
    correct answers and use an exact string-matching algorithm or maybe even use fuzzy
    matching to help with misspelled words.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨**ä¼ ç»Ÿç¼–ç¨‹èŒƒå¼**ä¸­ï¼Œå¼€å‘è€…éœ€è¦æ‰¾å‡ºå¦‚ä½•å¤„ç†æ‰€æœ‰è¿™äº›å˜ä½“ã€‚ä¸ºæ­¤ï¼Œä»–ä»¬å¯èƒ½ä¼šåˆ—å‡ºæ‰€æœ‰å¯èƒ½çš„æ­£ç¡®ç­”æ¡ˆï¼Œå¹¶ä½¿ç”¨ç²¾ç¡®çš„å­—ç¬¦ä¸²åŒ¹é…ç®—æ³•ï¼Œç”šè‡³å¯èƒ½ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…æ¥å¸®åŠ©å¤„ç†æ‹¼å†™é”™è¯¯çš„å•è¯ã€‚
- en: However, with this new **LLM-enabled paradigm**, **the problem can be solved
    through simple prompt engineering**. For instance, we could use the following
    prompt to evaluate student answers.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå€ŸåŠ©è¿™ç§æ–°çš„**LLMå¯ç”¨çš„èŒƒå¼**ï¼Œ**é—®é¢˜å¯ä»¥é€šè¿‡ç®€å•çš„æç¤ºå·¥ç¨‹æ¥è§£å†³**ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æç¤ºæ¥è¯„ä¼°å­¦ç”Ÿçš„ç­”æ¡ˆã€‚
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We can think of this prompt as a function, where given a ***question***, ***correct_answer***,
    and ***student_answer***, it generates the student's grade. This can then be integrated
    into a larger piece of software that implements the automatic grader.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªæç¤ºè§†ä¸ºä¸€ä¸ªå‡½æ•°ï¼Œç»™å®šä¸€ä¸ª***é—®é¢˜***ã€***æ­£ç¡®ç­”æ¡ˆ***å’Œ***å­¦ç”Ÿç­”æ¡ˆ***ï¼Œå®ƒç”Ÿæˆå­¦ç”Ÿçš„è¯„åˆ†ã€‚ç„¶åï¼Œè¿™å¯ä»¥é›†æˆåˆ°ä¸€ä¸ªæ›´å¤§çš„å®ç°è‡ªåŠ¨è¯„åˆ†ç³»ç»Ÿçš„è½¯ä»¶ä¸­ã€‚
- en: In terms of time-saving, this prompt took me about 2 minutes to write, while
    if I were to try to develop an algorithm to do the same thing, it would take me
    hours (if not days) and probably have worse performance. **So the time savings
    for tasks like this are 100â€“1000x**.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä»èŠ‚çœæ—¶é—´çš„è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ªæç¤ºæˆ‘èŠ±äº†å¤§çº¦2åˆ†é’Ÿæ¥ç¼–å†™ï¼Œè€Œå¦‚æœæˆ‘å°è¯•å¼€å‘ä¸€ä¸ªç®—æ³•æ¥åšåŒæ ·çš„äº‹æƒ…ï¼Œå®ƒå¯èƒ½éœ€è¦å‡ ä¸ªå°æ—¶ï¼ˆç”šè‡³å‡ å¤©ï¼‰ï¼Œè€Œä¸”æ€§èƒ½å¯èƒ½æ›´å·®ã€‚**å› æ­¤ï¼Œè¿™ç±»ä»»åŠ¡çš„æ—¶é—´èŠ‚çœæ˜¯100â€“1000å€**ã€‚
- en: Of course, there are many tasks in which LLMs do not provide any substantial
    benefit, and other existing methods are much better suited (e.g. predicting tomorrowâ€™s
    weather). In no way are LLMs the solution to every problem, but they do create
    a new set of solutions to tasks that require processing natural language effectivelyâ€”something
    that has been historically difficult for computers to do.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œæœ‰è®¸å¤šä»»åŠ¡ä¸­LLMå¹¶æ²¡æœ‰æä¾›å®è´¨æ€§çš„å¥½å¤„ï¼Œå…¶ä»–ç°æœ‰æ–¹æ³•æ›´é€‚åˆï¼ˆä¾‹å¦‚é¢„æµ‹æ˜å¤©çš„å¤©æ°”ï¼‰ã€‚LLMç»ä¸æ˜¯è§£å†³æ‰€æœ‰é—®é¢˜çš„æ–¹æ¡ˆï¼Œä½†å®ƒä»¬ç¡®å®åˆ›é€ äº†ä¸€å¥—æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºå¤„ç†éœ€è¦æœ‰æ•ˆå¤„ç†è‡ªç„¶è¯­è¨€çš„ä»»åŠ¡â€”â€”è¿™æ˜¯è®¡ç®—æœºå†å²ä¸Šä¸€ç›´å›°éš¾çš„ä»»åŠ¡ã€‚
- en: '**7 Tricks for Prompt Engineering**'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**æç¤ºå·¥ç¨‹çš„7ä¸ªæŠ€å·§**'
- en: While the prompt example from before may seem like a natural and obvious way
    to frame the automatic grading task, it deliberately employed specific prompt
    engineering heuristics (or â€œtricks,â€ as Iâ€™ll call them). These (and other) tricks
    have emerged as reliable ways to improve the quality of LLM responses.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶ä¹‹å‰çš„æç¤ºç¤ºä¾‹çœ‹èµ·æ¥åƒæ˜¯ä¸€ç§è‡ªç„¶ä¸”æ˜æ˜¾çš„è‡ªåŠ¨è¯„åˆ†ä»»åŠ¡æ¡†æ¶ï¼Œä½†å®ƒåˆ»æ„ä½¿ç”¨äº†ç‰¹å®šçš„æç¤ºå·¥ç¨‹å¯å‘å¼æ–¹æ³•ï¼ˆæˆ–è€…è¯´â€œæŠ€å·§â€ï¼Œå¦‚æˆ‘æ‰€ç§°ï¼‰ã€‚è¿™äº›ï¼ˆä»¥åŠå…¶ä»–ï¼‰æŠ€å·§å·²æˆä¸ºæé«˜LLMå“åº”è´¨é‡çš„å¯é æ–¹æ³•ã€‚
- en: Although there are many tips and tricks for writing good prompts, here I restrict
    the discussion to the ones that seem the most fundamental (IMO) based on a handful
    of references [1,3â€“5]. For a deeper dive, I recommend the reader explore the sources
    cited here.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æœ‰è®¸å¤šæ’°å†™è‰¯å¥½æç¤ºçš„æŠ€å·§å’Œçªé—¨ï¼Œä½†åœ¨è¿™é‡Œæˆ‘å°†è®¨è®ºé‚£äº›åŸºäºå°‘æ•°å‚è€ƒèµ„æ–™ï¼ˆIMOï¼‰çœ‹èµ·æ¥æœ€åŸºæœ¬çš„æŠ€å·§ã€‚å¯¹äºæ›´æ·±å…¥çš„äº†è§£ï¼Œæˆ‘å»ºè®®è¯»è€…æ¢ç´¢æ­¤å¤„å¼•ç”¨çš„æ¥æºã€‚
- en: '**Trick 1: Be Descriptive (More is Better)**'
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æŠ€å·§ 1ï¼šæè¿°æ€§å¼ºï¼ˆå¤šå¤šç›Šå–„ï¼‰**'
- en: A defining feature of LLMs is that they are trained on massive text corpora.
    This equips them with a vast knowledge of the world and the ability to perform
    an enormous variety of tasks. However, this impressive generality may hinder performance
    on a specific task if the proper context is not provided.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs çš„ä¸€ä¸ªå†³å®šæ€§ç‰¹å¾æ˜¯å®ƒä»¬åœ¨å¤§é‡æ–‡æœ¬è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¿™ä½¿å®ƒä»¬å…·å¤‡äº†å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†å’Œæ‰§è¡Œå„ç§ä»»åŠ¡çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ç§ä»¤äººå°è±¡æ·±åˆ»çš„æ™®éæ€§å¯èƒ½ä¼šåœ¨æ²¡æœ‰æä¾›é€‚å½“ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹ï¼Œå½±å“ç‰¹å®šä»»åŠ¡çš„è¡¨ç°ã€‚
- en: For example, letâ€™s compare two prompts for generating a birthday message for
    my dad.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬æ¯”è¾ƒä¸¤ä¸ªç”Ÿæˆæˆ‘çˆ¸çˆ¸ç”Ÿæ—¥ç¥ç¦çš„æç¤ºã€‚
- en: '***Without Trick***'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä¸ä½¿ç”¨æŠ€å·§***'
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***With Trick***'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä½¿ç”¨æŠ€å·§***'
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Trick 2: Give Examples**'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æŠ€å·§ 2ï¼šæä¾›ç¤ºä¾‹**'
- en: The next trick is to give the LLM example responses to improve its performance
    on a particular task. The technical term for this is **few-shot learning,** and
    has been shown to improve LLM performance significantly [6].
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€ä¸ªæŠ€å·§æ˜¯ç»™ LLM ç¤ºä¾‹å“åº”ï¼Œä»¥æé«˜å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¿™ä¸ªæŠ€æœ¯æœ¯è¯­æ˜¯**å°‘é‡å­¦ä¹ **ï¼Œå·²è¢«è¯æ˜èƒ½æ˜¾è‘—æé«˜ LLM çš„è¡¨ç° [6]ã€‚
- en: Letâ€™s look at a specific example. Say we want to write a subtitle for a Towards
    Data Science article. We can use existing examples to help guide the LLM completion.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªå…·ä½“çš„ä¾‹å­ã€‚å‡è®¾æˆ‘ä»¬æƒ³ä¸º Towards Data Science æ–‡ç« å†™ä¸€ä¸ªå‰¯æ ‡é¢˜ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç°æœ‰çš„ç¤ºä¾‹æ¥æŒ‡å¯¼ LLM å®Œæˆã€‚
- en: '***Without Trick***'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä¸ä½¿ç”¨æŠ€å·§***'
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '***With Trick***'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä½¿ç”¨æŠ€å·§***'
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Trick 3: Use Structured Text**'
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æŠ€å·§ 3ï¼šä½¿ç”¨ç»“æ„åŒ–æ–‡æœ¬**'
- en: Ensuring prompts follow an organized structure not only makes them easier to
    read and write, but also tends to help the model generate good completions. We
    employed this technique in the example for **Trick 2**, where we explicitly labeled
    the *title* and *subtitle* for each example.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿æç¤ºéµå¾ªæœ‰ç»„ç»‡çš„ç»“æ„ï¼Œä¸ä»…ä½¿å…¶æ›´æ˜“è¯»å’Œç¼–å†™ï¼Œè¿˜å¾€å¾€æœ‰åŠ©äºæ¨¡å‹ç”Ÿæˆè‰¯å¥½çš„å®Œæˆã€‚æˆ‘ä»¬åœ¨**æŠ€å·§ 2**çš„ç¤ºä¾‹ä¸­åº”ç”¨äº†è¿™ä¸€æŠ€æœ¯ï¼Œå…¶ä¸­æˆ‘ä»¬æ˜ç¡®æ ‡è®°äº†æ¯ä¸ªç¤ºä¾‹çš„*æ ‡é¢˜*å’Œ*å‰¯æ ‡é¢˜*ã€‚
- en: 'However, there are countless ways we can give our prompts structure. Here are
    a handful of examples: use ALL CAPS for emphasis, use delimiters like [PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæˆ‘ä»¬å¯ä»¥ä»¥æ— æ•°ç§æ–¹å¼ä¸ºæç¤ºæä¾›ç»“æ„ã€‚è¿™é‡Œæœ‰ä¸€äº›ä¾‹å­ï¼šä½¿ç”¨å…¨å¤§å†™æ¥å¼ºè°ƒï¼Œä½¿ç”¨åˆ†éš”ç¬¦å¦‚ [PRE5]
- en: Write me a recipe for chocolate chip cookies.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™æˆ‘ä¸€ä¸ªå·§å…‹åŠ›æ›²å¥‡é¥¼å¹²çš„é£Ÿè°±ã€‚
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Create a well-organized recipe for chocolate chip cookies. Use the following
    \
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªç»„ç»‡è‰¯å¥½çš„å·§å…‹åŠ›æ›²å¥‡é¥¼å¹²é£Ÿè°±ã€‚ä½¿ç”¨ä»¥ä¸‹\
- en: 'formatting elements:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¼å¼å…ƒç´ ï¼š
- en: '**Title**: Classic Chocolate Chip Cookies'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ ‡é¢˜**ï¼šç»å…¸å·§å…‹åŠ›æ›²å¥‡é¥¼å¹²'
- en: '**Ingredients**: List the ingredients with precise measurements and formatting.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**ææ–™**ï¼šåˆ—å‡ºé…æ–™åŠå…¶å‡†ç¡®çš„æµ‹é‡å’Œæ ¼å¼ã€‚'
- en: '**Instructions**: Provide step-by-step instructions in numbered format, detailing
    the baking process.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤**ï¼šä»¥ç¼–å·æ ¼å¼æä¾›é€æ­¥è¯´æ˜ï¼Œè¯¦ç»†è¯´æ˜çƒ˜ç„™è¿‡ç¨‹ã€‚'
- en: '**Tips**: Include a separate section with helpful baking tips and possible
    variations.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**æç¤º**ï¼šåŒ…æ‹¬ä¸€ä¸ªå•ç‹¬çš„éƒ¨åˆ†ï¼Œæä¾›æœ‰ç”¨çš„çƒ˜ç„™æç¤ºå’Œå¯èƒ½çš„å˜åŒ–ã€‚'
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Write me a LinkedIn post based on the following Medium blog.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®ä»¥ä¸‹ Medium åšå®¢å†™ä¸€ç¯‡ LinkedIn å¸–å­ã€‚
- en: 'Medium blog: {Medium blog text}'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Medium åšå®¢ï¼š{Medium åšå®¢æ–‡æœ¬}
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Write me a LinkedIn post based on the step-by-step process and Medium blog \
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®é€æ­¥è¿‡ç¨‹å’Œ Medium åšå®¢å†™ä¸€ç¯‡ LinkedIn å¸–å­\
- en: given below.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸‹æ‰€ç¤ºã€‚
- en: 'Step 1: Come up with a one line hook relevant to the blog.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 1 æ­¥ï¼šæƒ³å‡ºä¸€ä¸ªä¸åšå®¢ç›¸å…³çš„ä¸€å¥è¯å¼•å­ã€‚
- en: 'Step 2: Extract 3 key points from the article'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 2 æ­¥ï¼šä»æ–‡ç« ä¸­æå– 3 ä¸ªå…³é”®ç‚¹
- en: 'Step 3: Compress each point to less than 50 characters.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 3 æ­¥ï¼šå°†æ¯ä¸ªè¦ç‚¹å‹ç¼©åˆ° 50 ä¸ªå­—ç¬¦ä»¥å†…ã€‚
- en: 'Step 4: Combine the hook, compressed key points from Step 3, and a call to
    action \'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 4 æ­¥ï¼šå°†å¼•å­ã€ç¬¬ 3 æ­¥ä¸­çš„å‹ç¼©è¦ç‚¹å’Œè¡ŒåŠ¨å·å¬ç»“åˆèµ·æ¥\
- en: to generate the final output.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ç”Ÿæˆæœ€ç»ˆè¾“å‡ºã€‚
- en: 'Medium blog: {Medium blog text}'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Medium åšå®¢ï¼š{Medium åšå®¢æ–‡æœ¬}
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Make me a travel itinerary for a weekend in New York City.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™æˆ‘åˆ¶å®šä¸€ä¸ªåœ¨çº½çº¦å¸‚åº¦è¿‡å‘¨æœ«çš„æ—…è¡Œè®¡åˆ’ã€‚
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Act as an NYC native and cabbie who knows everything about the city. \
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å……å½“ä¸€ä½äº†è§£çº½çº¦å¸‚çš„ä¸€åˆ‡çš„çº½çº¦æœ¬åœ°äººå’Œå‡ºç§Ÿè½¦å¸æœºã€‚\
- en: Please make me a travel itinerary for a weekend in New York City based on \
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ ¹æ®\
- en: your experience. Don't forget to include your charming NY accent in your \
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çš„ç»å†ã€‚ä¸è¦å¿˜è®°åœ¨ä½ çš„\
- en: response.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å“åº”ã€‚
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: What is an idea for an LLM-based application?
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŸºäº LLM çš„åº”ç”¨ç¨‹åºçš„æƒ³æ³•æ˜¯ä»€ä¹ˆï¼Ÿ
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: I want you to ask me questions to help me come up with an LLM-based \
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›ä½ é—®æˆ‘é—®é¢˜ï¼Œä»¥å¸®åŠ©æˆ‘æå‡ºåŸºäº LLM çš„
- en: application idea. Ask me one question at a time to keep things conversational.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åº”ç”¨ç¨‹åºæƒ³æ³•ã€‚ä¸€æ¬¡é—®æˆ‘ä¸€ä¸ªé—®é¢˜ï¼Œä»¥ä¿æŒå¯¹è¯æ€§ã€‚
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Review your previous response, pinpoint areas for enhancement, and offer an
    \
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å¤æŸ¥ä½ ä¹‹å‰çš„å“åº”ï¼Œæ‰¾å‡ºæ”¹è¿›çš„åœ°æ–¹ï¼Œå¹¶æä¾›
- en: improved version. Then explain your reasoning for how you improved the response.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¹è¿›ç‰ˆã€‚ç„¶åè§£é‡Šä½ å¦‚ä½•æ”¹è¿›å“åº”çš„ç†ç”±ã€‚
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You are a high school history teacher grading homework assignments. \
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æ˜¯ä¸€åé«˜ä¸­å†å²è€å¸ˆï¼Œè´Ÿè´£è¯„åˆ†ä½œä¸šã€‚
- en: Based on the homework question indicated by "Q:" and the correct answer \
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºç”± "Q:" æŒ‡ç¤ºçš„ä½œä¸šé—®é¢˜å’Œæ­£ç¡®ç­”æ¡ˆ
- en: indicated by "A:", your task is to determine whether the student's answer is
    \
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ç”± "A:" æŒ‡ç¤ºï¼Œä½ çš„ä»»åŠ¡æ˜¯ç¡®å®šå­¦ç”Ÿçš„ç­”æ¡ˆæ˜¯å¦
- en: correct.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£ç¡®ã€‚
- en: Grading is binary; therefore, student answers can be correct or wrong.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„åˆ†æ˜¯äºŒå…ƒçš„ï¼Œå› æ­¤ï¼Œå­¦ç”Ÿçš„å›ç­”å¯ä»¥æ˜¯æ­£ç¡®çš„æˆ–é”™è¯¯çš„ã€‚
- en: Simple misspellings are okay.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€å•çš„æ‹¼å†™é”™è¯¯æ˜¯å¯ä»¥çš„ã€‚
- en: 'Q: {question}'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: {question}'
- en: 'A: {correct_answer}'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: {correct_answer}'
- en: 'Student Answer: {student_answer}'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 'å­¦ç”Ÿç­”æ¡ˆ: {student_answer}'
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: from langchain.chat_models import ChatOpenAI
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ä» langchain.chat_models å¯¼å…¥ ChatOpenAI
- en: from langchain.prompts import PromptTemplate
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ä» langchain.prompts å¯¼å…¥ PromptTemplate
- en: from langchain.chains import LLMChain
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ä» langchain.chains å¯¼å…¥ LLMChain
- en: from langchain.schema import BaseOutputParser
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ä» langchain.schema å¯¼å…¥ BaseOutputParser
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'from sk import my_sk #importing secret key from another python file'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä» sk å¯¼å…¥ my_sk #ä»å¦ä¸€ä¸ª Python æ–‡ä»¶å¯¼å…¥å¯†é’¥'
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: define LLM object
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®šä¹‰ LLM å¯¹è±¡
- en: chat_model = ChatOpenAI(openai_api_key=my_sk, temperature=0)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: chat_model = ChatOpenAI(openai_api_key=my_sk, temperature=0)
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: define prompt template
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®šä¹‰æç¤ºæ¨¡æ¿
- en: prompt_template_text = """You are a high school history teacher grading \
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: prompt_template_text = """ä½ æ˜¯ä¸€åé«˜ä¸­å†å²è€å¸ˆï¼Œè´Ÿè´£è¯„åˆ†
- en: homework assignments. Based on the homework question indicated by â€œ**Q:**â€ \
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸šã€‚åŸºäºç”± â€œ**Q:**â€ æŒ‡ç¤ºçš„ä½œä¸šé—®é¢˜
- en: and the correct answer indicated by â€œ**A:**â€, your task is to determine \
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥åŠç”± â€œ**A:**â€ æŒ‡ç¤ºçš„æ­£ç¡®ç­”æ¡ˆï¼Œä½ çš„ä»»åŠ¡æ˜¯ç¡®å®š
- en: whether the student's answer is correct. Grading is binary; therefore, \
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®å®šå­¦ç”Ÿçš„ç­”æ¡ˆæ˜¯å¦æ­£ç¡®ã€‚è¯„åˆ†æ˜¯äºŒå…ƒçš„ï¼Œå› æ­¤ï¼Œ
- en: student answers can be correct or wrong. Simple misspellings are okay.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ç”Ÿçš„å›ç­”å¯ä»¥æ˜¯æ­£ç¡®çš„æˆ–é”™è¯¯çš„ã€‚ç®€å•çš„æ‹¼å†™é”™è¯¯æ˜¯å¯ä»¥çš„ã€‚
- en: '**Q:** {question}'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q:** {question}'
- en: '**A:** {correct_answer}'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**A:** {correct_answer}'
- en: '**Student''s Answer:** {student_answer}'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**å­¦ç”Ÿçš„ç­”æ¡ˆ:** {student_answer}'
- en: '"""'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '"""'
- en: prompt = PromptTemplate(
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: prompt = PromptTemplate(
- en: input_variables=["question", "correct_answer", "student_answer"], \
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'input_variables=["question", "correct_answer", "student_answer"], '
- en: template = prompt_template_text)
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: template = prompt_template_text)
- en: '[PRE19]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: define chain
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®šä¹‰é“¾
- en: chain = LLMChain(llm=chat_model, prompt=prompt)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: chain = LLMChain(llm=chat_model, prompt=prompt)
- en: '[PRE20]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: define inputs
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®šä¹‰è¾“å…¥
- en: question = "Who was the 35th president of the United States of America?"
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ = "è°æ˜¯ç¾å›½ç¬¬35ä»»æ€»ç»Ÿï¼Ÿ"
- en: correct_answer = "John F. Kennedy"
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£ç¡®ç­”æ¡ˆ = "John F. Kennedy"
- en: student_answer =  "FDR"
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: student_answer = "FDR"
- en: run chain
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿è¡Œé“¾
- en: chain.run({'question':question, 'correct_answer':correct_answer, \
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: chain.run({'question':question, 'correct_answer':correct_answer,
- en: '''student_answer'':student_answer})'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '''student_answer'':student_answer})'
- en: 'output: Student''s Answer is wrong.'
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'è¾“å‡º: å­¦ç”Ÿçš„ç­”æ¡ˆæ˜¯é”™è¯¯çš„ã€‚'
- en: '[PRE21]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: define output parser
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®šä¹‰è¾“å‡ºè§£æå™¨
- en: 'class GradeOutputParser(BaseOutputParser):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 'class GradeOutputParser(BaseOutputParser):'
- en: '"""Determine whether grade was correct or wrong"""'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"""ç¡®å®šè¯„åˆ†æ˜¯å¦æ­£ç¡®æˆ–é”™è¯¯"""'
- en: 'def parse(self, text: str):'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def parse(self, text: str):'
- en: '"""Parse the output of an LLM call."""'
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"""è§£æ LLM è°ƒç”¨çš„è¾“å‡ºã€‚"""'
- en: return "wrong" not in text.lower()
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿”å› "wrong" ä¸åœ¨ text.lower() ä¸­
- en: '[PRE22]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: update chain
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ›´æ–°é“¾
- en: chain = LLMChain(
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: chain = LLMChain(
- en: llm=chat_model,
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: llm=chat_model,
- en: prompt=prompt,
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: prompt=prompt,
- en: output_parser=GradeOutputParser()
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: output_parser=GradeOutputParser()
- en: )
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: '[PRE23]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: run chain in for loop
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨ for å¾ªç¯ä¸­è¿è¡Œé“¾
- en: student_answer_list = ["John F. Kennedy", "JFK", "FDR", "John F. Kenedy", \
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: student_answer_list = ["John F. Kennedy", "JFK", "FDR", "John F. Kenedy",
- en: '"John Kennedy", "Jack Kennedy", "Jacquelin Kennedy", \'
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"John Kennedy", "Jack Kennedy", "Jacquelin Kennedy",'
- en: '"Robert F. Kenedy"]'
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"Robert F. Kenedy"]'
- en: 'for student_answer in student_answer_list:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¯¹äº student_answer_list ä¸­çš„ student_answer:'
- en: print(student_answer + " - " +
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: print(student_answer + " - " +
- en: str(chain.run({'question':question, 'correct_answer':correct_answer, \
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: str(chain.run({'question':question, 'correct_answer':correct_answer,
- en: '''student_answer'':student_answer})))'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '''student_answer'':student_answer})))'
- en: print('\n')
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: print('\n')
- en: 'Output:'
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'è¾“å‡º:'
- en: John F. Kennedy - True
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: John F. Kennedy - æ­£ç¡®
- en: JFK - True
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JFK - æ­£ç¡®
- en: FDR - False
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FDR - é”™è¯¯
- en: John F. Kenedy - True
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: John F. Kenedy - æ­£ç¡®
- en: John Kennedy - True
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: John Kennedy - æ­£ç¡®
- en: Jack Kennedy - True
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Jack Kennedy - æ­£ç¡®
- en: Jacqueline Kennedy - False
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Jacqueline Kennedy - é”™è¯¯
- en: Robert F. Kenedy - False
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Robert F. Kenedy - é”™è¯¯
- en: '```'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '```'
- en: '[](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/langchain-example?source=post_page-----7ce1ed3b553f--------------------------------)
    [## YouTube-Blog/LLMs/langchain-example at main Â· ShawhinT/YouTube-Blog'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/langchain-example?source=post_page-----7ce1ed3b553f--------------------------------)
    [## YouTube-Blog/LLMs/langchain-example at main Â· ShawhinT/YouTube-Blog'
- en: Codes to complement YouTube videos and blog posts on Medium. - YouTube-Blog/LLMs/langchain-example
    at main Â·â€¦
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä»£ç ç”¨äºè¡¥å…… YouTube è§†é¢‘å’Œ Medium åšå®¢æ–‡ç« ã€‚- YouTube-Blog/LLMs/langchain-example at main
    Â·â€¦
- en: github.com](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/langchain-example?source=post_page-----7ce1ed3b553f--------------------------------)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/langchain-example?source=post_page-----7ce1ed3b553f--------------------------------)
- en: Limitations
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é™åˆ¶
- en: Prompt Engineering is more than asking ChatGPT for help writing an email or
    learning about Quantum Computing. It is a ***new programming paradigm that changes
    how developers can build applications***.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Prompt Engineering ä¸ä»…ä»…æ˜¯å‘ ChatGPT æ±‚åŠ©å†™ç”µå­é‚®ä»¶æˆ–äº†è§£é‡å­è®¡ç®—ã€‚å®ƒæ˜¯ä¸€ä¸ª***æ”¹å˜å¼€å‘è€…æ„å»ºåº”ç”¨ç¨‹åºæ–¹å¼çš„æ–°ç¼–ç¨‹èŒƒå¼***ã€‚
- en: While this is a powerful innovation, it has its limitations. For one, optimal
    prompting strategies are LLM-dependent. For example, prompting GPT-3 to â€œthink
    step-by-stepâ€ resulted in significant performance gains on simple mathematical
    reasoning tasks [8]. However, for the latest version of ChatGPT, the same strategy
    doesnâ€™t seem helpful (it already thinks step-by-step).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„åˆ›æ–°ï¼Œä½†å®ƒä¹Ÿæœ‰å…¶å±€é™æ€§ã€‚ä¾‹å¦‚ï¼Œæœ€ä½³çš„æç¤ºç­–ç•¥ä¾èµ–äº LLMã€‚ä¾‹å¦‚ï¼Œæç¤º GPT-3 â€œé€æ­¥æ€è€ƒâ€ åœ¨ç®€å•çš„æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡
    [8]ã€‚ç„¶è€Œï¼Œå¯¹äºæœ€æ–°ç‰ˆæœ¬çš„ ChatGPTï¼Œç›¸åŒçš„ç­–ç•¥ä¼¼ä¹å¹¶æ²¡æœ‰å¸®åŠ©ï¼ˆå®ƒå·²ç»é€æ­¥æ€è€ƒï¼‰ã€‚
- en: Another limitation of Prompt Engineering is it requires large-scale general-purpose
    language models such as ChatGPT, which come at significant computational and financial
    costs. This may be overkill for many use cases that are more narrowly defined
    e.g. string matching, sentiment analysis, or text summarization.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Prompt Engineering çš„å¦ä¸€ä¸ªé™åˆ¶æ˜¯å®ƒéœ€è¦å¤§è§„æ¨¡çš„é€šç”¨è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚ ChatGPTï¼Œè¿™éœ€è¦æ˜¾è‘—çš„è®¡ç®—å’Œç»æµæˆæœ¬ã€‚è¿™å¯¹äºè®¸å¤šæ›´ç‹­ä¹‰çš„ç”¨ä¾‹ï¼Œä¾‹å¦‚å­—ç¬¦ä¸²åŒ¹é…ã€æƒ…æ„Ÿåˆ†ææˆ–æ–‡æœ¬æ‘˜è¦ï¼Œå¯èƒ½è¿‡äºå¤æ‚ã€‚
- en: We can overcome both these limitations via **fine-tuning** pre-trained language
    models. This is where we **take an existing language model and tweak it for a
    particular use case.** In the [next article](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)
    of this series, we will explore popular fine-tuning techniques supplemented with
    example Python code.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡**å¾®è°ƒ**é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ¥å…‹æœè¿™ä¸¤ç§é™åˆ¶ã€‚è¿™æ˜¯æˆ‘ä»¬**å¯¹ç°æœ‰è¯­è¨€æ¨¡å‹è¿›è¡Œè°ƒæ•´ä»¥é€‚åº”ç‰¹å®šç”¨ä¾‹çš„è¿‡ç¨‹**ã€‚åœ¨[ä¸‹ä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨æµè¡Œçš„å¾®è°ƒæŠ€æœ¯ï¼Œå¹¶é™„æœ‰ç¤ºä¾‹
    Python ä»£ç ã€‚
- en: 'ğŸ‘‰ **More on LLMs**: [Introduction](/a-practical-introduction-to-llms-65194dda1148)
    | [OpenAI API](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)
    | [Hugging Face Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    | [Fine-tuning](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)
    | [Build an LLM](/how-to-build-an-llm-from-scratch-8c477768f1f9) | [QLoRA](/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)
    | [RAG](/how-to-improve-llms-with-rag-abdc132f76ac) | [Text Embeddings](/text-embeddings-classification-and-semantic-search-8291746220be)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ‘‰ **å…³äº LLMs çš„æ›´å¤šä¿¡æ¯**: [ä»‹ç»](/a-practical-introduction-to-llms-65194dda1148) |
    [OpenAI API](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)
    | [Hugging Face Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    | [å¾®è°ƒ](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)
    | [æ„å»º LLM](/how-to-build-an-llm-from-scratch-8c477768f1f9) | [QLoRA](/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)
    | [RAG](/how-to-improve-llms-with-rag-abdc132f76ac) | [æ–‡æœ¬åµŒå…¥](/text-embeddings-classification-and-semantic-search-8291746220be)'
- en: '![Shaw Talebi](../Images/02eefb458c6eeff7cd29d40c212e3b22.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![Shaw Talebi](../Images/02eefb458c6eeff7cd29d40c212e3b22.png)'
- en: '[Shaw Talebi](https://shawhin.medium.com/?source=post_page-----7ce1ed3b553f--------------------------------)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[Shaw Talebi](https://shawhin.medium.com/?source=post_page-----7ce1ed3b553f--------------------------------)'
- en: Large Language Models (LLMs)
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰
- en: '[View list](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c?source=post_page-----7ce1ed3b553f--------------------------------)13
    stories![](../Images/82e865594c68f5307e75665842d197bb.png)![](../Images/b9436354721f807e0390b5e301be2119.png)![](../Images/59c8db581de77a908457dec8981f3c37.png)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹åˆ—è¡¨](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c?source=post_page-----7ce1ed3b553f--------------------------------)13
    ä¸ªæ•…äº‹![](../Images/82e865594c68f5307e75665842d197bb.png)![](../Images/b9436354721f807e0390b5e301be2119.png)![](../Images/59c8db581de77a908457dec8981f3c37.png)'
- en: Resources
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: èµ„æº
- en: '**Connect**: [My website](https://shawhintalebi.com/) | [Book a call](https://calendly.com/shawhintalebi)
    | [Ask me anything](https://shawhintalebi.com/contact/)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**è”ç³»**: [æˆ‘çš„ç½‘ç«™](https://shawhintalebi.com/) | [é¢„çº¦ç”µè¯](https://calendly.com/shawhintalebi)
    | [é—®æˆ‘ä»»ä½•é—®é¢˜](https://shawhintalebi.com/contact/)'
- en: '**Socials**: [YouTube ğŸ¥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)
    | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¤¾äº¤åª’ä½“**: [YouTube ğŸ¥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)
    | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)'
- en: '**Support**: [Buy me a coffee](https://www.buymeacoffee.com/shawhint) â˜•ï¸'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ”¯æŒ**: [è¯·æˆ‘å–å’–å•¡](https://www.buymeacoffee.com/shawhint) â˜•ï¸'
- en: '[](https://shawhin.medium.com/subscribe?source=post_page-----7ce1ed3b553f--------------------------------)
    [## Get FREE access to every new story I write'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shawhin.medium.com/subscribe?source=post_page-----7ce1ed3b553f--------------------------------)
    [## å…è´¹è·å–æˆ‘å†™çš„æ¯ä¸€ä¸ªæ–°æ•…äº‹'
- en: Get FREE access to every new story I write P.S. I do not share your email with
    anyone By signing up, you will create aâ€¦
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å…è´¹è·å–æˆ‘å†™çš„æ¯ä¸€ä¸ªæ–°æ•…äº‹ P.S. æˆ‘ä¸ä¼šå°†ä½ çš„ç”µå­é‚®ä»¶åˆ†äº«ç»™ä»»ä½•äºº é€šè¿‡æ³¨å†Œï¼Œä½ å°†åˆ›å»ºä¸€ä¸ªâ€¦
- en: shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----7ce1ed3b553f--------------------------------)
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----7ce1ed3b553f--------------------------------)
- en: '[1] [arXiv:2302.11382](https://arxiv.org/abs/2302.11382) **[cs.SE]**'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [arXiv:2302.11382](https://arxiv.org/abs/2302.11382) **[cs.SE]**'
- en: '[2] [arXiv:2106.09685](https://arxiv.org/abs/2106.09685) **[cs.CL]**'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [arXiv:2106.09685](https://arxiv.org/abs/2106.09685) **[cs.CL]**'
- en: '[3] [State of GPT](https://www.youtube.com/watch?v=bZQun8Y4L2A) by [Andrej
    Karpathy](https://medium.com/u/ac9d9a35533e?source=post_page-----7ce1ed3b553f--------------------------------)
    at Microsoft Build 2023'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [GPT çŠ¶æ€](https://www.youtube.com/watch?v=bZQun8Y4L2A) ç”± [Andrej Karpathy](https://medium.com/u/ac9d9a35533e?source=post_page-----7ce1ed3b553f--------------------------------)
    åœ¨ Microsoft Build 2023 ä¸Šæ¼”è®²'
- en: '[4] [arXiv:2206.07682](https://arxiv.org/abs/2206.07682) **[cs.CL]**'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [arXiv:2206.07682](https://arxiv.org/abs/2206.07682) **[cs.CL]**'
- en: '[5] [ChatGPT Prompt Engineering for Developers](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)
    by deeplearning.ai'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [ä¸ºå¼€å‘è€…è®¾è®¡çš„ ChatGPT æç¤ºå·¥ç¨‹](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)
    ç”± deeplearning.ai æä¾›'
- en: '[6] [arXiv:2005.14165](https://arxiv.org/abs/2005.14165) **[cs.CL]**'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [arXiv:2005.14165](https://arxiv.org/abs/2005.14165) **[cs.CL]**'
- en: '[7] [arXiv:2201.11903](https://arxiv.org/abs/2201.11903) **[cs.CL]**'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [arXiv:2201.11903](https://arxiv.org/abs/2201.11903) **[cs.CL]**'
- en: '[8] [arXiv:2210.03493](https://arxiv.org/abs/2210.03493) **[cs.CL]**'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [arXiv:2210.03493](https://arxiv.org/abs/2210.03493) **[cs.CL]**'
