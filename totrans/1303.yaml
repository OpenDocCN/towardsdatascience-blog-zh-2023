- en: Image Classification with Vision Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/image-classification-with-vision-transformer-8bfde8e541d4](https://towardsdatascience.com/image-classification-with-vision-transformer-8bfde8e541d4)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to classify images with the help of Transformer-based model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcellusruben?source=post_page-----8bfde8e541d4--------------------------------)[![Ruben
    Winastwan](../Images/15ad0dd03bf5892510abdf166a1e91e1.png)](https://medium.com/@marcellusruben?source=post_page-----8bfde8e541d4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8bfde8e541d4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8bfde8e541d4--------------------------------)
    [Ruben Winastwan](https://medium.com/@marcellusruben?source=post_page-----8bfde8e541d4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8bfde8e541d4--------------------------------)
    ·13 min read·Apr 13, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a404a96d42476c650eab290a3eb6734f.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [drmakete lab](https://unsplash.com/@drmakete?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/hsg538WrP0Y?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Since its introduction in 2017, Transformer has been widely recognized as a
    powerful encoder-decoder model to solve pretty much any language modeling task.
  prefs: []
  type: TYPE_NORMAL
- en: BERT, RoBERTa, and XLM-RoBERTa are a few examples of state-of-the-art models
    in language processing that use a stack of Transformer encoders as the backbone
    in their architecture. ChatGPT and the GPT family also use the decoder part of
    Transformer to generate texts. It’s safe to say that almost any state-of-the-art
    model in natural language processing incorporate Transformer in its architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformer performance is so good that it seems wasteful not to use it for
    tasks beyond natural language processing, like computer vision for example. However,
    the big question is: can we actually use it for computer vision tasks?'
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that Transformer also has a good potential to be applied to computer
    vision tasks. In 2020, Google Brain team introduced a Transformer-based model
    that can be used to solve an image classification task called Vision Transformer
    (ViT). Its performance is very competitive in comparison with conventional CNNs
    on several image classification benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in this article, we’re going to talk about this model. Specifically,
    we’re going to talk about how a ViT model works and how we can fine-tune it on
    our own custom dataset with the help of HuggingFace library for an image classification
    task.
  prefs: []
  type: TYPE_NORMAL
- en: So, as the first step, let’s get started with the dataset that we’re going to
    use in this article.
  prefs: []
  type: TYPE_NORMAL
- en: About the Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use a snack dataset that you can easily access from `dataset` library
    from HuggingFace. This dataset is listed as having a CC-BY 2.0 license, which
    means that you are free to share and use it, as long as you cite the dataset source
    in your work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a sneak peek of this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23a1eb2ca0482bdf2de55bfbd9de8b62.png)'
  prefs: []
  type: TYPE_IMG
- en: Subset of images in the dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'We only need a few lines of code to load the dataset, as you can see below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The dataset is a dictionary object that consists of 4898 training images, 955
    validation images, and 952 test images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each image comes with a label, which belongs to one of 20 snack classes. We
    can check these 20 different classes with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: And let’s create a mapping between each label and its corresponding index.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: One important thing that we need to know before we move on is the fact that
    each image has varying dimension. Therefore, we need to perform some image preprocessing
    steps before feeding the images into the model for fine-tuning purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know the dataset that we’re working with, let’s take a closer look
    at ViT architecture.
  prefs: []
  type: TYPE_NORMAL
- en: How ViT Works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before the introduction of ViT, the fact that a Transformer model relies on
    self-attention mechanism raised a big challenge for us to use it for computer
    vision tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The self-attention mechanism is the reason why Transformer-based models can
    differentiate the semantic meaning of a word used in different contexts. For example,
    a BERT model can distinguish the meaning of the word *‘park’* in sentences *‘They
    park their car in the basement’* and *‘She walks her dog in a park’* due to self-attention
    mechanism*.*
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there is one problem with self-attention: it’s a computationally expensive
    operation as it requires each token to attend every other token in a sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: Now if we use self-attention mechanism on image data, then each pixel in an
    image would need to attend and be compared to every other pixel. The problem is,
    if we increase the pixel value by one, then the computational cost would increase
    quadratically. This is simply not feasible if we have an image with a reasonably
    large resolution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f419838075b136290ec794459e282028.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to overcome this problem, ViT introduces the concept of splitting
    the input image into patches. Each patch has a dimension of 16 x 16 pixels. Let’s
    say that we have an image with the dimension of 48 x 48 pixels, then the patches
    of our image will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8e9020397bf23d2ba96145c42b30f364.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'In its application, there are two options for how ViT splits our image into
    patches:'
  prefs: []
  type: TYPE_NORMAL
- en: Reshape our input image that has a size of `height x width x channel` into a
    sequence of flattened 2D image patches with a size of `no.of patches x (patch_size^2.channel)`
    . Then, we project the flattened patches into a basic linear layer to get the
    embedding of each patch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Project our input image into a convolutional layer with the kernel size and
    stride equal to the patch size. Then, we flatten the output from that convolutional
    layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After testing the model performance on several datasets, it turns out that the
    second approach leads to the better performance. Therefore, in this article, we’re
    going to use the second approach.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use a toy example to demonstrate the splitting process of an input image
    into patches with a convolutional layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The next thing that the model will do is flatten the patches and put them sequentially
    as you can see in the image below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62ca9cce1423a569cb951956a041ac5b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do the flattening process with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: What we have after the flattening process is basically the vector embedding
    of each patch. This is similar to token embeddings in many Transformer-based language
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Next, similar to BERT, ViT will add a special vector embedding for the **[CLS]**
    token in the first position of our patches’ sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1b8b3805d82eb7dae37b8609b3019f3c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, by prepending the **[CLS]** token embedding in the beginning
    of our patch embedding, the length of the sequence increases by one. The final
    step after this would be adding the positional embedding into our sequence of
    patches. This step is important so that our ViT model can learn the sequence order
    of our patches.
  prefs: []
  type: TYPE_NORMAL
- en: This position embedding is a learnable parameter that will be updated by the
    model during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0957e29cfbbcebe7099774485b63c5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the position embedding plus vector embedding of each patch will be the
    input of a stack of Transformer encoders. The number of Transformer encoders depends
    on the type of ViT model that you use. Overall, there are three types of ViT model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ViT-base:** it has 12 layers, hidden size of 768, and the total of 86M parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ViT-large:** it has 24 layers, hidden size of 1024, and the total of 307M
    parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ViT-huge:** it has 32 layers, hidden size of 1280, and the total of 632M
    parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following code snippet, let’s say that we want to use **Vit-base**.
    This means that we have 12 layers of Transformer encoders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the stack of Transformer encoders will output the final vector representation
    of each image patch. The dimensionality of the final vector corresponds to the
    hidden size of the ViT model that we use.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89d85ae70d147091e3f4f9f257855730.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: And that’s basically it.
  prefs: []
  type: TYPE_NORMAL
- en: We can certainly build and train our own ViT model from scratch. However, as
    with other Transformer-based models, ViT requires training on a large amount of
    image data (14M-300M of images) in order for them to generalize well on unseen
    data.
  prefs: []
  type: TYPE_NORMAL
- en: If we want to use ViT on a custom dataset, the most common approach is to fine-tune
    a pretrained model. The easiest way to do this is by utilizing HuggingFace library.
    All we have to do is call `ViTModel.from_pretrained()` method and put the path
    to our pretrained model as an argument. The `VitModel()`class from HuggingFace
    will also act as a wrapper of all of steps that we’ve discussed above.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The output of the complete ViT model is a vector embedding representing each
    image patch plus the **[CLS]** token. It has the dimension of `[batch_size, image_patches+1,
    hidden_size]`.
  prefs: []
  type: TYPE_NORMAL
- en: To perform an image classification task, we follow the same approach as with
    the BERT model. We extract the output vector embedding of the **[CLS]** token
    and pass it through the final linear layer to determine the class of the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7eab045d0cbbcc51471c5add96bec3b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Fine-Tuning Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will fine-tune a **ViT-base** model that was pre-trained
    on the ImageNet-21K dataset, which consists of approximately 14 million images
    and 21,843 classes. Each image in the dataset has a dimension of 224 x 224 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: To begin, we need to define the checkpoint path for the pre-trained model and
    load the necessary libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Image Dataloader
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As previously mentioned, the ViT-base model has been pretrained on a dataset
    consisting of images with the dimension of 224 x 224 pixels. The images have also
    been normalized according to a particular mean and standard deviation in each
    of their color channels.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, before we can feed our own dataset into the ViT model for fine-tuning,
    we must first preprocess our images. This involves transforming each image into
    a tensor, resizing it to the appropriate dimensions, and then normalizing it using
    the same mean and standard deviation values as the dataset on which the model
    was pretrained.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: From the image dataloader above, we will then get a batch of preprocessed images
    with their corresponding label. We can use the ouput of image dataloader above
    as an input for our model during the fine-tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: Model Definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The architecture of our ViT model is straightforward. Since we’ll be fine-tuning
    a pretrained model, we can use the `VitModel.from_pretrained()` method and provide
    the checkpoint of the model as an argument.
  prefs: []
  type: TYPE_NORMAL
- en: We also need to add a linear layer at the end, which will act as the final classifier.
    The output of this layer should be equal to the number of distinct labels in our
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The above ViT model generates final vector embeddings for each image patch plus
    the **[CLS]** token. To classify images, as you can see above, we extract the
    final vector embedding of the **[CLS]** token and pass it to the final linear
    layer to obtain the final class prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Model Fine-Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have defined the model architecture and prepared the input images
    for batching process, we can start to fine-tune our ViT model. The training script
    is a standard Pytorch training script, as you can see below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Since our snack dataset has 20 distinct classes, then we’re dealing with a multiclass
    classification problem. Therefore, `CrossEntropyLoss()` would be the appropriate
    loss function. In the example above, we train our model for 10 epochs, learning
    rate is set to be 1e-4, with the batch size of 8\. You can play around with these
    hyperparameters to tune the performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'After you trained the model, you will get an output that looks similar as the
    one below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab1e13dc6653a70aed12426bb4094162.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Model Prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since we have fine-tuned our model, naturally we want to use it for prediction
    on the test data. To do so, first let’s create a function that encapsulate all
    of the necessary image preprocessing steps and the model inference process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As you can see above, the image preprocessing step during inference is exactly
    the same as the step that we did on the training data. Then, we use the transformed
    image as the input to our trained model, and finally we map its prediction to
    the corresponding label.
  prefs: []
  type: TYPE_NORMAL
- en: If we want to predict a specific image on the test data, we can just call the
    function above and we’ll get the prediction afterwards. Let’s try it out.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/056b21551628c53ba34c5f6d32abd3b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of test data from the dataset
  prefs: []
  type: TYPE_NORMAL
- en: Our model predicted our test image correctly. Let’s try another one.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/61fc4b4a831b31060af95251c8425697.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of test data from the dataset
  prefs: []
  type: TYPE_NORMAL
- en: And our model predicted the test data correctly again. By fine-tuning a ViT
    model, we can get a good performance on a custom dataset. You can also do the
    same process for any custom dataset in an image classification task.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we have seen how Transformer can be used not only for language
    modeling tasks, but also for computer vision tasks, which in this case is image
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: To do so, first the input image is decomposed into patches with a size of 16
    x 16 pixels. Then, the Vision Transformer model utilizes a stack of Transformer
    encoders to learn the vector representation of each image patch. Finally, we can
    use the final vector representation of the **[CLS]** token prepended at the beginning
    of image patch sequence to predict the label of our input image.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this article is useful for you to get started with Vision Transformer
    model. As always, you can find the code implementation presented in this article
    in [**this notebook**](https://github.com/marcellusruben/medium-resources/blob/main/ViT/Vision_Transformer.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://huggingface.co/datasets/Matthijs/snacks](https://huggingface.co/datasets/Matthijs/snacks)'
  prefs: []
  type: TYPE_NORMAL
