- en: Environmental Impact of Ubiquitous Generative AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 普及生成式 AI 的环境影响
- en: 原文：[https://towardsdatascience.com/environmental-impact-of-ubiquitous-generative-ai-9e061bac6800](https://towardsdatascience.com/environmental-impact-of-ubiquitous-generative-ai-9e061bac6800)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/environmental-impact-of-ubiquitous-generative-ai-9e061bac6800](https://towardsdatascience.com/environmental-impact-of-ubiquitous-generative-ai-9e061bac6800)
- en: What could happen to our environment if billions of people began to use generative
    AI technology on a daily basis?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如果数十亿人开始每天使用生成式 AI 技术，会对环境产生什么影响？
- en: '[](https://kaspergroesludvigsen.medium.com/?source=post_page-----9e061bac6800--------------------------------)[![Kasper
    Groes Albin Ludvigsen](../Images/3c31c9e54fae4fd1c8f1c441379d1f10.png)](https://kaspergroesludvigsen.medium.com/?source=post_page-----9e061bac6800--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9e061bac6800--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9e061bac6800--------------------------------)
    [Kasper Groes Albin Ludvigsen](https://kaspergroesludvigsen.medium.com/?source=post_page-----9e061bac6800--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaspergroesludvigsen.medium.com/?source=post_page-----9e061bac6800--------------------------------)[![Kasper
    Groes Albin Ludvigsen](../Images/3c31c9e54fae4fd1c8f1c441379d1f10.png)](https://kaspergroesludvigsen.medium.com/?source=post_page-----9e061bac6800--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9e061bac6800--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9e061bac6800--------------------------------)
    [Kasper Groes Albin Ludvigsen](https://kaspergroesludvigsen.medium.com/?source=post_page-----9e061bac6800--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9e061bac6800--------------------------------)
    ·15 min read·Jul 12, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9e061bac6800--------------------------------)
    ·15 分钟阅读·2023年7月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/aba978b066343f9a34e7fcee5c0e854d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aba978b066343f9a34e7fcee5c0e854d.png)'
- en: Photo by Johannes Plenio on Unsplash
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Johannes Plenio 拍摄，来源于 Unsplash
- en: 'This article ponders the question: What would be the environmental impact of
    large-scale adoption of generative AI like ChatGPT? That is, what might the environmental
    impact be if billions of people began to use generative AI extensively on a daily
    basis?'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了这样一个问题：大规模采用类似 ChatGPT 的生成式 AI 会对环境产生什么影响？也就是说，如果数十亿人开始每天广泛使用生成式 AI，那么环境影响可能是什么？
- en: The reason this question is interesting to contemplate is that we can use its
    answer to inform how worried we should or shouldn’t be about the speedy adoption
    of this new technology.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题值得思考的原因在于，我们可以通过其答案来决定我们对这项新技术快速普及的担忧程度。
- en: As AI models have grown larger and larger [1] and as they have been made widely
    accessible by companies like OpenAI and Google, the environmental impact of AI
    models — eg. carbon and water footprint — have become the subject of inquiry and
    debate. First in academia (e.g. [2] and [3]) and later in mainstream media (e.g.
    [4] and [5]).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 AI 模型越来越大[1]，并且像 OpenAI 和 Google 等公司使其广泛可用，AI 模型的环境影响——例如碳足迹和水足迹——已经成为学术界（如[2]和[3]）以及主流媒体（如[4]和[5]）讨论和研究的主题。
- en: With ChatGPT reportedly having hundreds of millions of users – if not billions
    [6] – and Google embedding generative AI into several products [7], generative
    AI is arguably the most widely adopted type of AI at the moment. Combined with
    the immense size of generative AI models like GPT-4 – rumored to be almost 6 times
    larger than its predecessor [8] – generative AI is likely also the type of AI
    to have the largest environmental impact for the foreseeable future.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 据报道，ChatGPT 的用户数量达到数亿——甚至可能是数十亿[6]——而 Google 将生成式 AI 嵌入到多个产品中[7]，因此生成式 AI 可以说是当前最广泛采用的
    AI 类型。结合生成式 AI 模型如 GPT-4 的巨大规模——传闻其规模是前代模型的近 6 倍[8]——生成式 AI 可能也是未来可预见的环境影响最大的
    AI 类型。
- en: This article is a thought experiment that contemplates what the environmental
    impact might be of large-scale adoption of generative AI. Will it lead to environmental
    catastrophe, will it be a drop in the ocean or somewhere in between? The purpose
    of this article is to provide a basis for beginning to shed light on that question.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是一个思想实验，考虑了大规模采用生成式 AI 可能带来的环境影响。它会导致环境灾难，还是只是一滴水中海洋，或者介于两者之间？本文的目的是为开始揭示这个问题提供基础。
- en: Many assumptions went into making the estimates presented in this article, and
    if you’d like to play around with your own assumptions, you can do so [in this
    spreadsheet](https://docs.google.com/spreadsheets/d/1fXtR85LTtUiZ25rcPXzHmL4vR03x_hB9GPEWMLFhhXc/edit?usp=sharing).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中做出的估算涉及许多假设，如果你想使用自己的假设进行实验，可以在 [这个电子表格](https://docs.google.com/spreadsheets/d/1fXtR85LTtUiZ25rcPXzHmL4vR03x_hB9GPEWMLFhhXc/edit?usp=sharing)
    中进行。
- en: 'If you’d like to get full access to my stories on the environmental impact
    of AI, become a Medium member through the link below:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想全面了解我关于 AI 环境影响的故事，请通过下面的链接成为 Medium 会员：
- en: '[](https://kaspergroesludvigsen.medium.com/membership?source=post_page-----9e061bac6800--------------------------------)
    [## Join Medium with my referral link - Kasper Groes Albin Ludvigsen'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaspergroesludvigsen.medium.com/membership?source=post_page-----9e061bac6800--------------------------------)
    [## 通过我的推荐链接加入 Medium - Kasper Groes Albin Ludvigsen'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为 Medium 会员，你的会员费的一部分会分配给你阅读的作者，你将能完全访问每一篇故事……
- en: kaspergroesludvigsen.medium.com](https://kaspergroesludvigsen.medium.com/membership?source=post_page-----9e061bac6800--------------------------------)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[kaspergroesludvigsen.medium.com](https://kaspergroesludvigsen.medium.com/membership?source=post_page-----9e061bac6800--------------------------------)'
- en: Stages in an AI system’s life cycle
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI 系统生命周期的阶段
- en: 'Even though this article does not analyze one specific model, it’s informative
    to distinguish between different stages of an AI model’s life cycle. We can think
    of an AI model’s life cycle as consisting of 6 distinct stages [9]:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本文并未分析特定模型，但区分 AI 模型生命周期的不同阶段仍具有参考价值。我们可以将 AI 模型的生命周期视为由 6 个不同阶段组成 [9]：
- en: Raw material extraction
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 原材料开采
- en: Materials manufacturing
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 材料制造
- en: Hardware manufacturing
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 硬件制造
- en: Model training
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练
- en: Model deployment
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型部署
- en: End-of-life
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生命周期结束
- en: In this article, I’ll focus on hardware manufacturing (stage 3), model training
    (stage 4) and model deployment (stage 5) and will therefore briefly describe these
    stages below.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将重点关注硬件制造（第 3 阶段）、模型训练（第 4 阶段）和模型部署（第 5 阶段），因此下面将简要描述这些阶段。
- en: Hardware manufacturing refers to the environmental impact from manufacturing
    the hardware on which the AI model runs. Model training is the stage in which
    the model is developed. Model deployment is the stage in which the model is “deployed”
    to a “production environment” where it can be used by users. This is also sometimes
    referred to as the inference stage or the production stage. The life cycle is
    often depicted as linear although many AI systems require their models to be re-trained
    or adjusted during the system’s lifetime.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件制造是指制造 AI 模型运行所需硬件的环境影响。模型训练是模型开发的阶段。模型部署是将模型“部署”到“生产环境”的阶段，用户可以使用模型。这有时也称为推理阶段或生产阶段。生命周期通常被描述为线性的，尽管许多
    AI 系统在其生命周期内需要对模型进行重新训练或调整。
- en: To estimate the environmental impact of either of the 3 above-mentioned stages,
    we need to get an idea of how much hardware large-scale adoption of generative
    AI would require. This is what we’ll consider in the following section.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估算上述三个阶段的环境影响，我们需要了解大规模采用生成式 AI 需要多少硬件。这是我们在接下来的部分将考虑的内容。
- en: '[](/chatgpts-electricity-consumption-7873483feac4?source=post_page-----9e061bac6800--------------------------------)
    [## ChatGPT’s Electricity Consumption'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/chatgpts-electricity-consumption-7873483feac4?source=post_page-----9e061bac6800--------------------------------)
    [## ChatGPT 的电力消耗'
- en: ChatGPT may have consumed as much electricity as 175,000 people in January 2023.
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ChatGPT 可能在 2023 年 1 月消耗了相当于 175,000 人的电量。
- en: towardsdatascience.com](/chatgpts-electricity-consumption-7873483feac4?source=post_page-----9e061bac6800--------------------------------)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/chatgpts-electricity-consumption-7873483feac4?source=post_page-----9e061bac6800--------------------------------)'
- en: How much hardware does large-scale generative AI adoption require?
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模生成式 AI 的采用需要多少硬件？
- en: To assess the potential environmental impact of large-scale adoption of generative
    AI, we need to know how much hardware would be required to handle billions of
    daily queries.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估大规模采用生成式 AI 的潜在环境影响，我们需要了解处理每日数十亿次查询所需的硬件数量。
- en: To figure out how much hardware is needed, we need to think about how many users
    the technology will have and how much they'll use it. The more users, the more
    hardware is needed.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定所需的硬件数量，我们需要考虑技术将有多少用户以及他们将使用多少。用户越多，需要的硬件就越多。
- en: So, what would large-scale generative AI adoption look like in terms of user
    numbers? Let’s assume that 3.5B people start to use ChatGPT or similar technology
    daily and they make 30 queries per day. That’s a total of 105B daily requests.
    In lieu of ChatGPT’s staggering user numbers and efforts by Google and other companies
    to integrate generative AI into various products, this shouldn’t be an unreasonable
    assumption.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，大规模生成性 AI 采用在用户数量方面会是什么样的呢？假设 35 亿人开始每天使用 ChatGPT 或类似技术，每天进行 30 次查询。那总共是
    105B 次日常请求。鉴于 ChatGPT 的惊人用户数量以及 Google 和其他公司将生成性 AI 集成到各种产品中的努力，这种假设并不不合理。
- en: Now we need to get a sense of what kind and how much hardware it takes to handle
    100B daily requests.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要了解处理 100B 日常请求所需的硬件种类和数量。
- en: Patel and Ahmad have previously estimated that it takes around 3,617 Nvidia
    HGX A100 servers containing 28,936 Nvidia A100 GPUs to handle 195,000,000 daily
    ChatGPT requests [10]. The A100 GPU is a piece of processing hardware designed
    for AI workloads. Let’s assume that those numbers are in the right ballpark and
    that they will generalize to other generative AI services. Let’s further assume
    that the number of GPUs increases linearly with the number of daily requests.
    This means that if 3,617 HGX servers can handle 195,000,000 daily requests, we
    need 538.46x more compute — ie 1,947,615 Nvidia HGX A100 servers with a total
    of 15,580,923 A100 GPUs — to handle 105B daily requests.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Patel 和 Ahmad 曾估算处理 195,000,000 次日常 ChatGPT 请求需要大约 3,617 台 Nvidia HGX A100 服务器，其中包含
    28,936 个 Nvidia A100 GPU[10]。A100 GPU 是为 AI 工作负载设计的处理硬件。我们假设这些数字是正确的，并且可以推广到其他生成性
    AI 服务。我们进一步假设 GPU 的数量与日常请求的数量呈线性增长。这意味着如果 3,617 台 HGX 服务器可以处理 195,000,000 次日常请求，我们需要
    538.46 倍的计算能力——即 1,947,615 台 Nvidia HGX A100 服务器，总共 15,580,923 个 A100 GPU——来处理
    105B 次日常请求。
- en: Now that we have an idea of how much hardware is needed to support large-scale
    adoption of generative AI, let’s look at the environmental impact of manufacturing
    it.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了支持大规模采用生成性 AI 所需的硬件数量，让我们来看看制造这些硬件的环境影响。
- en: Environmental impact of large-scale generative AI adoption in the hardware manufacturing
    stage
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模生成性 AI 在硬件制造阶段的环境影响
- en: In the previous section, we saw that large-scale adoption of generative AI technology
    may require 1,947,615 Nvidia HGX servers and 15,580,923 Nvidia A100 GPUs. Let’s
    look at the environmental impact from manufacturing this hardware.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到大规模采用生成性 AI 技术可能需要 1,947,615 台 Nvidia HGX 服务器和 15,580,923 个 Nvidia
    A100 GPU。让我们来看一下制造这些硬件的环境影响。
- en: Nvidia have not released any information about the carbon footprint of their
    products, so we’ll have to use some proxies here, which means that the numbers
    we arrive at are highly speculative, so take them with a grain of salt and please
    challenge them.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Nvidia 尚未发布其产品的碳足迹信息，因此我们必须使用一些代理数据，这意味着我们得到的数字高度推测，因此请谨慎对待并提出挑战。
- en: The embodied emissions of Hewlett-Packard’s ProLiant DL345 Gen10 Plus server
    is 2,500 kgCO2e, according to the company’s own estimate [11]. This the only reasonably
    similar server for which I’ve been able to find embodied emissions data, so we’ll
    use that as proxy like Luccioni et al have previously done [9].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 根据惠普公司自己的估算，ProLiant DL345 Gen10 Plus 服务器的体现排放为 2,500 kgCO2e[11]。这是我找到的唯一一个具有体现排放数据的相似服务器，因此我们将以此作为类比，就像
    Luccioni 等人以前所做的那样[9]。
- en: The ProLiant server does not contain any GPUs, so let’s add the embodied emissions
    of 8 A100 GPUs. Again, Nvidia have not disclosed this, but 150 kgCO2e per GPU
    has been used by others [9] [12].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ProLiant 服务器不包含任何 GPU，所以我们需要加上 8 个 A100 GPU 的体现排放。同样，Nvidia 并未公开这些数据，但其他人已使用了每个
    GPU 150 kgCO2e 的数据[9] [12]。
- en: We’re assuming that the Nvidia HGX with 8 GPU slots is used, so let’s add 8
    * 150 kgCO2e to the 2,500 kgCO2e. That’s a total of 3,700 kgCO2e per Nvidia HGX
    A100 server.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设使用的是带有 8 个 GPU 插槽的 Nvidia HGX，因此我们需要将 8 * 150 kgCO2e 加到 2,500 kgCO2e 中。这总共是每台
    Nvidia HGX A100 服务器 3,700 kgCO2e。
- en: Recall that we need 1,947,615 of these to handle 105B daily requests. The embodied
    emissions of the GPU hardware needed to accommodate large-scale adoption of generative
    AI is thus estimated to be 1,947,615 * 3,7 = 7,206,177 tons CO2e.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们需要 1,947,615 台这样的服务器来处理 105B 次日常请求。因此，满足大规模采用生成性 AI 所需的 GPU 硬件的体现排放估计为
    1,947,615 * 3.7 = 7,206,177 吨 CO2e。
- en: Let’s spread out those emissions evenly over the life span of the hardware.
    We’ll assume that the hardware has a life span of 5 years after which it is either
    worn out or replaced by newer technology [13].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这些排放均匀分布在硬件的使用寿命内。我们假设硬件的使用寿命为5年，之后要么磨损，要么被新技术替代 [13]。
- en: Based on this, the carbon footprint of manufacturing the hardware needed for
    large-scale adoption of generative AI is estimated to be 1,441,235.4 tons CO2e
    per year.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，大规模采用生成式AI所需硬件的碳足迹估计为每年1,441,235.4吨CO2e。
- en: '[](https://kaspergroesludvigsen.medium.com/subscribe?source=post_page-----9e061bac6800--------------------------------)
    [## Get an email whenever Kasper Groes Albin Ludvigsen publishes.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaspergroesludvigsen.medium.com/subscribe?source=post_page-----9e061bac6800--------------------------------)
    [## 订阅Kasper Groes Albin Ludvigsen的文章以获取电子邮件通知。'
- en: Get an email whenever Kasper Groes Albin Ludvigsen publishes. By signing up,
    you will create a Medium account if you…
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 订阅Kasper Groes Albin Ludvigsen的文章以获取电子邮件通知。通过注册，你将创建一个Medium账户，如果你…
- en: kaspergroesludvigsen.medium.com](https://kaspergroesludvigsen.medium.com/subscribe?source=post_page-----9e061bac6800--------------------------------)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: kaspergroesludvigsen.medium.com](https://kaspergroesludvigsen.medium.com/subscribe?source=post_page-----9e061bac6800--------------------------------)
- en: Environmental impact of large-scale generative AI adoption in the training stage
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模生成式AI采用的训练阶段的环境影响
- en: Now, let’s consider the environmental impact of training the AI models that
    will underpin generative AI.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑训练支撑生成式AI的AI模型的环境影响。
- en: The initial version of ChatGPT was based on the large language model (LLM) called
    GPT-3.5 which is a version of GPT-3\. The newest version of ChatGPT is likely
    based on OpenAI’s newest LLM called GPT-4, but OpenAI have not released any information
    about GPT-4 that can be used to estimate its training costs. We do however have
    reliable estimates of GPT-3's energy consumption during training. These come from
    a paper by Google and UC Berkeley researchers who estimate the energy consumption
    of training GPT-3 to be 1,287,000 KWh [14]. Let’s assume that models trained by
    other companies are in the same ballpark.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT的初始版本基于一个名为GPT-3.5的大型语言模型（LLM），它是GPT-3的一个版本。最新版本的ChatGPT可能基于OpenAI最新的LLM——GPT-4，但OpenAI尚未发布任何关于GPT-4的训练成本信息。不过，我们有关于GPT-3训练过程中的能耗的可靠估算。这些数据来源于谷歌和加州大学伯克利分校研究人员的一篇论文，他们估算训练GPT-3的能耗为1,287,000
    KWh [14]。我们可以假设其他公司训练的模型在同一范围内。
- en: To compute the carbon footprint of consuming 1,287,000 KWh, we need to get an
    idea of how much carbon is emitted when 1 KWh of electricity is produced. This
    is called the carbon intensity of electricity and varies between regions, because
    sources of electricity (wind, coal, solar etc.) vary between regions. For this
    thought experiment, let’s use the average carbon intensity of electricity used
    by Google’s, Amazon’s, and Microsoft’s data centers. Using data from the ML CO2
    Impact Calculator [18], the mean carbon intensity of electricity used by these
    three cloud providers is 484 gCO2e/KWh.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算1,287,000 KWh的碳足迹，我们需要了解生产1 KWh电力时排放了多少碳。这被称为电力的碳强度，并在不同地区之间有所不同，因为电力来源（风能、煤炭、太阳能等）因地区而异。在这个思考实验中，我们使用谷歌、亚马逊和微软数据中心使用的电力的平均碳强度。根据ML
    CO2 Impact Calculator [18]的数据，这三家云服务提供商的电力平均碳强度为484 gCO2e/KWh。
- en: 'Now, to obtain an estimate of the annual carbon footprint of training generative
    models, we’ll need to know how many companies offer such models and how often
    they are trained. Let assume that there will be 9 major players in generative
    AI: OpenAI/Microsoft, Google, Meta, Anthropic, Inflection, Character, Tencent,
    ByteDance, Baidu. Let’s further assume that they train one model each per year.
    That’s an annual training footprint of 6,229 tons CO2e.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了获得生成模型年度碳足迹的估算，我们需要知道有多少公司提供这种模型以及它们的训练频率。假设在生成式AI领域将有9个主要参与者：OpenAI/Microsoft、Google、Meta、Anthropic、Inflection、Character、腾讯、字节跳动、百度。再假设他们每年训练一个模型。这将导致每年的训练碳足迹为6,229吨CO2e。
- en: The assumptions that go into this part of the article are the most speculative.
    However, as we’ll see, they won’t affect the total picture much because the environmental
    impact of the training stage pales in comparison to the environmental impact of
    the hardware manufacturing and deployment stages.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本文这一部分的假设是最具推测性的。然而，正如我们将看到的，它们对整体图景的影响不大，因为训练阶段的环境影响与硬件制造和部署阶段的环境影响相比，显得微不足道。
- en: '**Environmental impact of large-scale generative AI adoption in the deployment
    stage**'
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**大规模生成性人工智能部署阶段的环境影响**'
- en: Let’s now consider the environmental impact of the deployment stage of the generative
    AI models. In other words, let’s look at how much electricity it takes to keep
    the 1,947,615 HGX servers running.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑生成性人工智能模型部署阶段的环境影响。换句话说，让我们看看运行1,947,615台HGX服务器所需的电力。
- en: One way to calculate this is to look at the so-called Thermal Design Power (TDP)
    of the server. TDP is often used to quantify how much power a piece of processing
    hardware requires to run. The TDP of the HGX server is unknown, but the TDP of
    a similar Nvidia server, the DGX, is 6.5 kW [15] which we’ll assume also applies
    to the HGX server. So, if the server runs at full power for one hour, it has used
    6.5 kilowatt hours (kWh). However, for this thought experiment, let’s assume that
    all the servers run at 75% of their TDP on average, in which case they’ll consume
    4.875 KWh per hour. That’s 1,947,615 servers each consuming 4.875 KWh per hour.
    That’s a total of 9,494,625 KWh per hour, 227,871,000 KWh per day and 83,172,915,000
    KWh per year.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一种计算方法是查看服务器的所谓热设计功率（TDP）。TDP通常用于量化一块处理硬件运行所需的电力。HGX服务器的TDP未知，但类似的Nvidia服务器DGX的TDP为6.5
    kW [15]，我们假设HGX服务器也适用这一数据。因此，如果服务器全功率运行一个小时，则消耗6.5千瓦时（kWh）。然而，在这个思维实验中，我们假设所有服务器平均运行在其TDP的75%，那么每小时将消耗4.875
    KWh。共有1,947,615台服务器，每台每小时消耗4.875 KWh。这总计为每小时9,494,625 KWh，每天227,871,000 KWh，每年83,172,915,000
    KWh。
- en: When calculating the electricity consumption from data center grade hardware,
    the electricity consumption of the hardware itself is often multiplied by the
    so-called Power Usage Effectiveness (PUE) of the data center in which the hardware
    is running. PUE is a metric used to express the energy efficiency of a data center.
    The more energy the data center uses on, say, cooling compared to the energy used
    to power the actual computer hardware, the higher the PUE. Microsoft’s global
    PUE is 1.18 [16] and Google’s is 1.10 [17], so let’s use the average of these
    two, i.e. 1.14.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 计算数据中心级硬件的电力消耗时，通常会将硬件自身的电力消耗乘以硬件所在数据中心的所谓电力使用效率（PUE）。PUE是用于表达数据中心能源效率的指标。数据中心在冷却等方面使用的能量相对于实际计算硬件的电力使用得越多，PUE值越高。微软的全球PUE为1.18
    [16]，谷歌的为1.10 [17]，所以我们使用这两个值的平均数，即1.14。
- en: If we multiply the HGX servers’ estimated annual electricity consumption of
    83,172,915,000 KWh by 1.14, we get an annual electricity consumption of 94,817,123,100
    KWh.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将HGX服务器估算的年电力消耗83,172,915,000 KWh乘以1.14，则年电力消耗为94,817,123,100 KWh。
- en: Next, to calculate carbon emissions, we’ll multiply by the carbon intensity
    of 484 g/KWh presented in the previous section. Based on this, we can estimate
    the annual carbon footprint of large-scale generative AI adoption to be 45,891,487
    tons in the deployment stage.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了计算碳排放，我们将乘以前一节中提到的484克/KWh的碳强度。基于此，我们可以估算大规模生成性人工智能部署阶段的年碳足迹为45,891,487吨。
- en: Combined environmental impact of ubiquitous adoption of generative AI
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 普遍采用生成性人工智能的综合环境影响
- en: Now that we have estimated the carbon footprint of the hardware manufacturing,
    training and deployment stages of large-scale adoption of generative AI, let’s
    combine those into the total annual carbon footprint of large-scale generative
    AI adoption.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经估算了硬件制造、训练和大规模部署生成性人工智能的阶段的碳足迹，让我们将这些数据结合起来，计算大规模生成性人工智能采用的年总碳足迹。
- en: Recall that we estimated the annual emissions from manufacturing the hardware
    to be 1,441,235 tons. Then, we estimated the annual CO2e emissions from the training
    stage to be 6,229 tons. Finally, we estimated the carbon footprint of the deployment
    stage of large-scale adoption of generative AI to be 45,891,487 tons annually.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们估算了硬件制造的年排放量为1,441,235吨。然后，我们估算了训练阶段的年CO2e排放量为6,229吨。最后，我们估算了大规模生成性人工智能部署阶段的碳足迹为每年45,891,487吨。
- en: So the total carbon footprint of ubiquitous adoption of generative AI can be
    estimated to be 47,338,952 tons CO2e per year.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，普遍采用生成性人工智能的总碳足迹可以估算为每年47,338,952吨CO2e。
- en: Figure 1 below depicts how much larger the carbon emissions from the model deployment
    stage are compared to the hardware manufacturing and model training stages. Consequently,
    research should address how to reduce the deployment stage emissions rather than
    the training stage emissions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 下图1展示了模型部署阶段的碳排放量比硬件制造和模型训练阶段大得多。因此，研究应关注如何减少部署阶段的排放，而非训练阶段的排放。
- en: '![](../Images/024865a779ffe8fa4734f1c77dcbe06d.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/024865a779ffe8fa4734f1c77dcbe06d.png)'
- en: 'Figure 1: Estimated annual carbon emissions from large-scale adoption of generative
    AI, by life cycle stage. Chart by Kasper Groes Albin Ludvigsen'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：按生命周期阶段估计的大规模采用生成性 AI 的年碳排放量。图表由Kasper Groes Albin Ludvigsen提供
- en: '**Putting the annual carbon footprint of large-scale adoption of generative
    AI into perspective**'
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**将大规模采用生成性 AI 的年碳足迹放入视角中**'
- en: 'Throughout the article, you may have wondered: “What do these numbers actually
    mean? Is it a lot or not?”'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在整篇文章中，你可能会想：“这些数字究竟意味着什么？这多还是少？”
- en: Above, we estimated the total annual carbon footprint of large-scale adoption
    of generative AI to be 47,338,952 tons CO2e across the hardware manufacturing,
    model training and model deployment life cycle stages.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 上面，我们估计大规模采用生成性 AI 的总年碳足迹为47,338,952吨 CO2e，包括硬件制造、模型训练和模型部署生命周期阶段。
- en: That’s equivalent to the annual emissions of 4,303,541 Danes. When put like
    that, it sounds like a lot in my opinion. On the other hand, 4.3M people is just
    a tiny fraction of the world’s population. So, let’s also compare the carbon footprint
    of ubiquitous generative AI to the entire world’s annual emissions.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当于4,303,541丹麦人的年排放量。这样看来，我认为这听起来很多。另一方面，430万人只是全球人口的微不足道的一部分。因此，让我们也将普遍生成性
    AI 的碳足迹与全球年排放量进行比较。
- en: Estimates of global annual CO2e emissions vary between sources, but IAE [19]
    estimate that in 2021, 40B metric tons of CO2e was emitted globally. Of 40B tons,
    47,338,952 tons is 0.12 %. Put differently, if 3.5B people made 30 daily queries
    to generative AI models like ChatGPT, this article estimates that it could increase
    global CO2e emissions by 0.12 %. I’ll let it be up to the readers to decide if
    they think that’s a lot or not.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 全球年 CO2e 排放量的估计因来源不同而有所差异，但IAE [19] 估计2021年全球排放了400亿吨 CO2e。在这400亿吨中，47,338,952吨占0.12%。换句话说，如果35亿人每天对生成性
    AI 模型如 ChatGPT 进行30次查询，这篇文章估计可能会使全球 CO2e 排放量增加0.12%。我将留给读者自己决定是否认为这很多。
- en: Water footprint of ubiquitous generative AI adoption
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 普遍生成性 AI 采用的水足迹
- en: So far we have analyzed the potential carbon footprint of large-scale adoption
    of generative AI. But carbon emissions are not the only environmental impact of
    digital technology. Another important consideration is the water consumption from
    cooling the data centers which host large AI models. Water consumption refers
    to water that is lost and cannot be reused.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经分析了大规模采用生成性 AI 的潜在碳足迹。但碳排放并不是数字技术的唯一环境影响。另一个重要的考量是冷却承载大 AI 模型的数据中心的水消耗。水消耗指的是那些丧失且无法再利用的水。
- en: 'A recent paper by Pengfei Li et al [20] analyzes the water consumption from
    large language models like the one underpinning ChatGPT. In their paper, the authors
    present a methodology for estimating the water consumption from data centers and
    they estimate that ChatGPT consumes 500 ml of water for every 20–50 queries. Data
    centers consume water in two primary ways:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Pengfei Li 等人 [20] 的一篇最新论文分析了类似 ChatGPT 的大型语言模型的水消耗。在他们的论文中，作者提出了一种估计数据中心水消耗的方法，他们估计
    ChatGPT 每进行20到50次查询会消耗500毫升水。数据中心的水消耗主要有两种方式：
- en: direct consumption which is when water evaporates and is flushed as data center
    hardware is cooled, and
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直接消耗，即水分蒸发并在数据中心硬件冷却时被冲洗掉，以及
- en: indirect water consumption which accounts for the water being used to produce
    the electricity that data centers need to run.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 间接水消耗，即用于生产数据中心所需电力的水。
- en: Let’s assume that the water consumption of ChatGPT will generalize to other
    AI services and that 50 queries require 500 ml of water, so that’s 10 ml of water
    per query. Recall that we assumed that 105B daily requests will be made to generative
    AI models. That’s 1,050,000,000 liters of water per day, or 383,250,000,000 liters
    in a year. For comparison, the annual recommended fluid intake for adults is 3.2
    liters [21], or an annual intake of 1,168 liters. This means that the water consumption
    from large scale adoption of generative AI could sustain the annual fluid intake
    of 328,135,000 adults.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 ChatGPT 的水消耗可以推广到其他人工智能服务，并且 50 个查询需要 500 毫升的水，即每个查询需要 10 毫升的水。请记住，我们假设生成性人工智能模型每天会收到
    105 亿个请求。这每天需要 1,050,000,000 升水，或者每年需要 383,250,000,000 升水。相比之下，成年人每年的推荐液体摄入量为
    3.2 升[21]，即每年的摄入量为 1,168 升。这意味着生成性人工智能的大规模应用产生的水消耗可以满足 328,135,000 成人的年液体摄入量。
- en: You may wonder why water consumption can be problematic. The main issue is when
    data centers that consume a lot of water are placed in regions affected by drought.
    This is an issue because we generally don’t have the infrastructure needed to
    move water across large distances. The paper by Pengfei Li et al presents this
    map which shows areas of the US affected by drought. Thousands of data centers
    are located in these areas.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道为什么水的消耗会成为一个问题。主要问题是，当大量用水的数据中心被放置在受旱灾影响的地区时。这是一个问题，因为我们通常没有将水跨大距离转移所需的基础设施。Pengfei
    Li 等人的论文展示了这张图，显示了美国受旱灾影响的地区。这些地区有成千上万的数据中心。
- en: '![](../Images/f875c35208a82c1e5c63c52779fc20d7.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f875c35208a82c1e5c63c52779fc20d7.png)'
- en: Image from [https://arxiv.org/pdf/2304.03271.pdf](https://arxiv.org/pdf/2304.03271.pdf)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源 [https://arxiv.org/pdf/2304.03271.pdf](https://arxiv.org/pdf/2304.03271.pdf)
- en: Discussion
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论
- en: Let’s now discuss some of the potential implications of the estimates presented
    above.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论一下上述估算的潜在影响。
- en: Can we power large-scale generative AI adoption with renewable energy?
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们能否用可再生能源为大规模的生成性人工智能应用提供动力？
- en: So how might we mitigate the environmental impact of large-scale adoption of
    generative AI?
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何减少生成性人工智能大规模应用的环境影响呢？
- en: '[](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----9e061bac6800--------------------------------)
    [## How to estimate and reduce the carbon footprint of machine learning models'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[如何估算和减少机器学习模型的碳足迹](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----9e061bac6800--------------------------------)
    [## 如何估算和减少机器学习模型的碳足迹'
- en: Two ways to easily estimate the carbon footprint of machine learning models
    and 17 ideas for how you might reduce it
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 两种简单估算机器学习模型碳足迹的方法以及减少碳足迹的 17 种方法
- en: towardsdatascience.com](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----9e061bac6800--------------------------------)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----9e061bac6800--------------------------------)'
- en: One idea that springs to mind is to power it all with renewable energy. Let’s
    consider that idea.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一个浮现在脑海中的想法是用可再生能源为这些数据中心供电。让我们考虑这个想法。
- en: In the following, I’ll assume that all queries made of generative AI will require
    additional energy. I.e. I assume the queries won’t substitute queries made to
    other existing services. You might argue that some ChatGPT queries currently replaces
    traditional search engine queries, but given that generative AI is being built
    into both Bing and Google, I’d venture that near all generative AI queries will
    be additional.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将假设对生成性人工智能的所有查询都需要额外的能源。即，我假设这些查询不会替代对其他现有服务的查询。你可能会争辩说，一些 ChatGPT 查询当前替代了传统搜索引擎查询，但考虑到生成性人工智能正在被整合到
    Bing 和 Google 中，我敢说几乎所有的生成性人工智能查询都会是额外的。
- en: In order to make large-scale adoption of generative AI sustainable, the energy
    would therefore have to be generated by *additional* renewable energy capacity
    — i.e. we’d have to install additional renewable energy capacity. Recall that
    ubiquitous generative AI could require just shy of 95 billion KWh of electricity
    per year. The average wind turbine can generate 6 million KWh in a year [22].
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使生成性人工智能的大规模应用具有可持续性，因此能源必须通过*额外的*可再生能源容量来生成——即，我们需要安装额外的可再生能源容量。请记住，普遍的生成性人工智能可能每年需要接近
    95 亿千瓦时的电力。平均风力发电机每年可以产生 600 万千瓦时的电力[22]。
- en: So to produce enough renewable energy with wind turbines, we would need to install
    around 15,800 new wind turbines. To put that into perspective, Denmark, a leading
    nation in wind energy, currently has 6,286 active wind turbines [23]
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: I therefore think it’s safe to say that establishing enough additional renewable
    energy to power ubiquitous generative AI adoption would be a massive and expensive
    undertaking.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: On a side note, even renewable energy is considered to have a carbon footprint
    because the emissions caused by producing and installing e.g. a wind turbine are
    spread out over the life time of the energy source. Thus, electricity from off-shore
    wind power is considered to have a median carbon intensity of 12 gCO2e/KWh [24].
    So even all generative AI was powered by wind energy, it would have an annual
    carbon footprint of 1,114,000 tons CO2e — roughly the same as 104k Danes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Do the benefits outweigh the costs?
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another interesting aspect to consider in this debate is whether the productivity
    gains we can achieve with generative AI justifies its environmental impact. In
    a yet to be peer reviewed article (i.e. take it with a grain of salt), MIT PhD
    students Shakked Noy and Whitney Zhang show that using ChatGPT improved both productivity
    and quality of work for a number of tasks in an experimental setting [25]. Noy
    and Zhang measured productivity on tasks such as writing press releases, short
    reports, analysis plans, and delicate emails. Quality is assessed by (blinded)
    experienced professionals working in the same occupations.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Whether productivity gains are worth their environmental costs is in essence
    a value judgement, but it would be a good start for the debate if we could ascertain
    whether generative AI actually does make us more productive. More research should
    therefore be conducted, and companies using generative AI should critically assess
    the effects on productivity.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Caveats
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The estimates put forward in this article should be considered educated guesswork.
    This is first and foremost because we’re trying to guess what will happen in the
    future. Secondly, any effort at estimating the environmental impact of these types
    of AI models is hampered by the fact that the providers don’t disclose the necessary
    information, which means we must make assumptions. By writing this, I hope to
    inspire others to challenge my assumptions or produce their own estimates.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Although generative AI is an umbrella term for AI products than can generate
    both text and/or images, this article focuses on models that generate text. The
    estimates made here do therefore not consider the adoption of image generation
    technology per se.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT in it’s original version (based on GPT-3.5) was the point of departure
    for this article because relevant data is unavailable for OpenAI’s latest model,
    GPT-4\. As written above, GPT-4 is larger than GPT-3, which could mean that it
    consumes more energy. However, it’s not necessarily the case.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: I assume that the environmental impact of one company’s generative AI product
    will be in the same ballpark as competing products offered by other companies,
    but it could be the case that some companies will offer smaller or more specialized
    models.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设一家公司的生成式人工智能产品的环境影响将与其他公司提供的竞争产品大致相同，但也可能有些公司提供较小或更专业化的模型。
- en: Conclusion
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this thought experiment we looked at what the environmental impact could
    be if a large part of the world’s population begins to use generative AI like
    ChatGPT on a daily basis. The purpose of this thought experiment is give the reader
    a basis for assessing the question: Should we worry about the environmental impact
    of large-scale adoption of generative AI?'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个思想实验中，我们探讨了如果世界上很大一部分人口开始每天使用像 ChatGPT 这样的生成式人工智能，可能对环境产生的影响。这个思想实验的目的是为读者提供一个评估问题的基础：我们是否应该担心大规模采用生成式人工智能的环境影响？
- en: We estimated that ubiquitous generative AI might consume 95B KWh of electricity
    annually, and producing this amount of electricity could cause emission of 47,338,952
    tons CO2e. That’s 0.12 % of global CO2e emissions. In other words, this article
    estimates that if 3.5B people make 30 queries per day to generative AI services,
    it could increase global CO2e emissions by 0.12 %. Another environmental impact
    to consider is water consumption. This article estimates that ubiquitous generative
    AI might consume 383,250,000,000 liters of water in a year. This amount of water
    is the same as the recommended annual fluid intake of 328,125,000 adults.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们估计，普及的生成式人工智能每年可能消耗 95B KWh 的电力，生产这些电力可能会排放 47,338,952 吨 CO2e。这占全球 CO2e 排放量的
    0.12%。换句话说，本文估计如果 35 亿人每天向生成式人工智能服务发出 30 个查询，全球 CO2e 排放量可能增加 0.12%。另一个需要考虑的环境影响是水消耗。本文估计普及的生成式人工智能每年可能消耗
    383,250,000,000 升水。这相当于 328,125,000 名成人的推荐年液体摄入量。
- en: That’s it! I hope you enjoyed the story. Let me know what you think!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了！希望你喜欢这个故事。告诉我你的想法吧！
- en: Follow me for more on AI and sustainability and [subscribe](https://kaspergroesludvigsen.medium.com/subscribe)
    to get my stories via email when I publish.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 关注我获取更多关于人工智能和可持续发展的内容，并[订阅](https://kaspergroesludvigsen.medium.com/subscribe)，以便在我发布时通过电子邮件获取我的故事。
- en: I also sometimes write about [time series forecasting](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我有时还会写关于[时间序列预测](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e)的文章。
- en: And feel free to connect on [LinkedIn](https://www.linkedin.com/in/kaspergroesludvigsen).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 也欢迎在[LinkedIn](https://www.linkedin.com/in/kaspergroesludvigsen)上与我联系。
- en: References
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] [https://huggingface.co/blog/large-language-model](https://huggingface.co/blog/large-language-models)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://huggingface.co/blog/large-language-model](https://huggingface.co/blog/large-language-models)'
- en: '[2] [https://arxiv.org/abs/1907.10597](https://arxiv.org/abs/1907.10597)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://arxiv.org/abs/1907.10597](https://arxiv.org/abs/1907.10597)'
- en: '[3] [https://arxiv.org/abs/1906.02243](https://arxiv.org/abs/1906.02243)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [https://arxiv.org/abs/1906.02243](https://arxiv.org/abs/1906.02243)'
- en: '[4] [https://www.forbes.com/sites/bernardmarr/2023/03/22/green-intelligence-why-data-and-ai-must-become-more-sustainable/](https://www.forbes.com/sites/bernardmarr/2023/03/22/green-intelligence-why-data-and-ai-must-become-more-sustainable/)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [https://www.forbes.com/sites/bernardmarr/2023/03/22/green-intelligence-why-data-and-ai-must-become-more-sustainable/](https://www.forbes.com/sites/bernardmarr/2023/03/22/green-intelligence-why-data-and-ai-must-become-more-sustainable/)'
- en: '[5] [https://www.standard.co.uk/tech/ai-chatgpt-water-usage-environment-study-b1073866.html](https://www.standard.co.uk/tech/ai-chatgpt-water-usage-environment-study-b1073866.html)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [https://www.standard.co.uk/tech/ai-chatgpt-water-usage-environment-study-b1073866.html](https://www.standard.co.uk/tech/ai-chatgpt-water-usage-environment-study-b1073866.html)'
- en: '[6] [https://www.similarweb.com/blog/insights/ai-news/chatgpt-growth-flattens/](https://www.similarweb.com/blog/insights/ai-news/chatgpt-growth-flattens/)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [https://www.similarweb.com/blog/insights/ai-news/chatgpt-growth-flattens/](https://www.similarweb.com/blog/insights/ai-news/chatgpt-growth-flattens/)'
- en: '[7] [https://nypost.com/2023/05/10/google-integrates-more-ai-into-products-in-battle-with-microsoft/](https://nypost.com/2023/05/10/google-integrates-more-ai-into-products-in-battle-with-microsoft/)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [https://nypost.com/2023/05/10/google-integrates-more-ai-into-products-in-battle-with-microsoft/](https://nypost.com/2023/05/10/google-integrates-more-ai-into-products-in-battle-with-microsoft/)'
- en: '[8] [https://the-decoder.com/gpt-4-has-a-trillion-parameters/](https://the-decoder.com/gpt-4-has-a-trillion-parameters/)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [https://the-decoder.com/gpt-4-has-a-trillion-parameters/](https://the-decoder.com/gpt-4-has-a-trillion-parameters/)'
- en: '[9] [https://arxiv.org/pdf/2211.02001.pdf](https://arxiv.org/pdf/2211.02001.pdf)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] [https://arxiv.org/pdf/2211.02001.pdf](https://arxiv.org/pdf/2211.02001.pdf)'
- en: '[10] [https://www.semianalysis.com/p/the-inference-cost-of-search-disruption](https://www.semianalysis.com/p/the-inference-cost-of-search-disruption)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[11] [https://www.hpe.com/psnow/doc/a50005151enw](https://www.hpe.com/psnow/doc/a50005151enw)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[12] [https://medium.com/teads-engineering/building-an-aws-ec2-carbon-emissions-dataset-3f0fd76c98ac](https://medium.com/teads-engineering/building-an-aws-ec2-carbon-emissions-dataset-3f0fd76c98ac)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[13] [https://cybersided.com/how-long-do-gpus-last/](https://cybersided.com/how-long-do-gpus-last/)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[14] [https://arxiv.org/ftp/arxiv/papers/2204/2204.05149.pdf](https://arxiv.org/ftp/arxiv/papers/2204/2204.05149.pdf)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[15] [https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-dgx-a100-datasheet.pdf](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-dgx-a100-datasheet.pdf)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[16] [https://azure.microsoft.com/en-us/blog/how-microsoft-measures-datacenter-water-and-energy-use-to-improve-azure-cloud-sustainability/](https://azure.microsoft.com/en-us/blog/how-microsoft-measures-datacenter-water-and-energy-use-to-improve-azure-cloud-sustainability/)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[17] [https://www.google.com/about/datacenters/efficiency/](https://www.google.com/about/datacenters/efficiency/)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[18] [https://github.com/mlco2/impact/blob/master/data/impact.csv](https://github.com/mlco2/impact/blob/master/data/impact.csv)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[19] [https://www.iea.org/reports/co2-emissions-in-2022](https://www.iea.org/reports/co2-emissions-in-2022)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[20] [https://arxiv.org/pdf/2304.03271.pdf](https://arxiv.org/pdf/2304.03271.pdf)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[21] [https://www.health.harvard.edu/staying-healthy/how-much-water-should-you-drink](https://www.health.harvard.edu/staying-healthy/how-much-water-should-you-drink)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[22] [https://www.ewea.org/wind-energy-basics/faq/](https://www.ewea.org/wind-energy-basics/faq/)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[23] [https://turbines.dk/](https://turbines.dk/)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[24] [https://en.wikipedia.org/wiki/Life-cycle_greenhouse_gas_emissions_of_energy_sources](https://en.wikipedia.org/wiki/Life-cycle_greenhouse_gas_emissions_of_energy_sources)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[25] [https://economics.mit.edu/sites/default/files/inline-files/Noy_Zhang_1.pdf](https://economics.mit.edu/sites/default/files/inline-files/Noy_Zhang_1.pdf)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
