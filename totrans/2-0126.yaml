- en: 5 Ways to Get Interesting Datasets for Your Next Data Project (Not Kaggle)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/5-ways-to-get-interesting-datasets-for-your-next-data-project-not-kaggle-71cf76eef64b](https://towardsdatascience.com/5-ways-to-get-interesting-datasets-for-your-next-data-project-not-kaggle-71cf76eef64b)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Bored of Kaggle and FiveThirtyEight? Here are the alternative strategies I use
    for getting high-quality and unique datasets
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mattchapmanmsc?source=post_page-----71cf76eef64b--------------------------------)[![Matt
    Chapman](../Images/7511deb8d9ed408ece21031f6614c532.png)](https://medium.com/@mattchapmanmsc?source=post_page-----71cf76eef64b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----71cf76eef64b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----71cf76eef64b--------------------------------)
    [Matt Chapman](https://medium.com/@mattchapmanmsc?source=post_page-----71cf76eef64b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----71cf76eef64b--------------------------------)
    ·7 min read·Jun 24, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e876fa791ac0f463d9091464b6651ba9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Image by [Efe Kurnaz](https://unsplash.com/@efekurnaz) on [Unsplash](https://unsplash.com/photos/RnCPiXixooY)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: The key to a great data science project is a great dataset, but finding great
    data is much easier said than done.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: I remember back when I was studying for my master’s in Data Science, a little
    over a year ago. Throughout the course, I found that coming up with project ideas
    was the easy part — it was *finding good datasets* that I struggled with the most.
    I would spend hours scouring the internet, pulling my hair out trying to find
    juicy data sources and getting nowhere.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Since then, I’ve come a long way in my approach, and in this article I want
    to share with you the 5 strategies that I use to find datasets. If you’re bored
    of standard sources like Kaggle and FiveThirtyEight, these strategies will enable
    you to get data that are unique and much more tailored to the specific use cases
    you have in mind.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Make your own data
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yep, believe it or not, this is actually a legit strategy. It’s even got a fancy
    technical name (“synthetic data generation”).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: If you’re trying out a new idea or have very specific data requirements, making
    synthetic data is a fantastic way to get original and tailored datasets.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s say that you’re trying to build a churn prediction model
    — a model that can predict how likely a customer is to leave a company. Churn
    is a pretty common “operational problem” faced by many companies, and tackling
    a problem like this is a great way to show recruiters that you can use ML to solve
    commercially-relevant problems, as I’ve argued previously:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-to-find-unique-data-science-project-ideas-that-make-your-portfolio-stand-out-1c2ddfdbefa6?source=post_page-----71cf76eef64b--------------------------------)
    [## How to Find Unique Data Science Project Ideas That Make Your Portfolio Stand
    Out'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Forget Titanic and MNIST: Pick a unique project that builds your skills and
    helps you stand out from the crowd'
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-find-unique-data-science-project-ideas-that-make-your-portfolio-stand-out-1c2ddfdbefa6?source=post_page-----71cf76eef64b--------------------------------)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if you search online for “churn datasets,” you’ll find that there
    are (at the time of writing) only two main datasets obviously available to the
    public: the [Bank Customer Churn Dataset](https://www.kaggle.com/datasets/gauravtopre/bank-customer-churn-dataset),
    and the [Telecom Churn Dataset](https://www.kaggle.com/mnassrib/telecom-churn-datasets).
    These datasets are a fantastic place to start, but might not reflect the kind
    of data required for modelling churn in other industries.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Instead, you could try creating synthetic data that’s more tailored to your
    requirements.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'If this sounds too good to be true, here’s an example dataset which I created
    with just a short prompt to that old chestnut, ChatGPT:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38fca05145d88471423f4ed5fb19d78b.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: Image by author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Of course, ChatGPT is limited in the speed and size of the datasets it can create,
    so if you want to upscale this technique I’d recommend using either the Python
    library `faker` or scikit-learn’s `sklearn.datasets.make_classification` and `sklearn.datasets.make_regression`
    functions. These tools are a fantastic way to programmatically generate huge datasets
    in the blink of an eye, and perfect for building proof-of-concept models without
    having to spend ages searching for the perfect dataset.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: In practice, I have rarely needed to use synthetic data creation techniques
    to generate *entire* datasets (and, as I will explain later, you’d be wise to
    exercise caution if you intend to do this). Instead, I find this is a really neat
    technique for generating adversarial examples or adding noise to your datasets,
    enabling me to test my models’ weaknesses and build more robust versions. But,
    regardless of how you use this technique, it’s an incredibly useful tool to have
    at your disposal.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Ask a company for their data (nicely)
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating synthetic data is a nice workaround for situations when you can’t find
    the type of data you’re looking for, but the obvious problem is that you’ve got
    no guarantee that the data are good representations of real-life populations.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: If you want to guarantee that your data are realistic, the best way to do that
    is, surprise surprise…
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: … to actually go and find some *real* data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: One way of doing this is to reach out to companies that might hold such data
    and ask if they’d be interested in sharing some with you. At risk of stating the
    obvious, no company is going to give you data that are highly sensitive or if
    you are planning to use them for commercial or unethical purposes. That would
    just be plain stupid.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: However, if you intend to use the data for research (e.g., for a university
    project), you might well find that companies are open to providing data if it’s
    in the context of a *quid pro quo* joint research agreement.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'What do I mean by this? It’s actually pretty simple: I mean an arrangement
    whereby they provide you with some (anonymised/de-sensitised) data and you use
    the data to conduct research which is of some benefit to them. For example, if
    you’re interested in studying churn modelling, you could put together a proposal
    for comparing different churn prediction techniques. Then, share the proposal
    with some companies and ask whether there’s potential to work together. If you’re
    persistent and cast a wide net, you will likely find a company that is willing
    to provide data for your project *as long as you share your findings with them*
    so that they can get a benefit out of the research.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: If that sounds too good to be true, you might be surprised to hear that [this
    is exactly what I did during my master’s degree](https://medium.com/towards-data-science/4-ways-to-get-the-most-out-of-your-data-science-degree-40815f6a311d).
    I reached out to a couple of companies with a proposal for how I could use their
    data for research that would benefit them, signed some paperwork to confirm that
    I wouldn’t use the data for any other purpose, and conducted a really fun project
    using some real-world data. It really can be done.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: The other thing I particularly like about this strategy is that it provides
    a way to exercise and develop quite a broad set of skills which are important
    in Data Science. You have to communicate well, show commercial awareness, and
    become a pro at managing stakeholder expectations — all of which are essential
    skills in the day-to-day life of a Data Scientist.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f5396a4ba057dbee6ac686705a4b161.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: Pleeeeeease let me have your data. I’ll be a good boy, I promise! Image by [Nayeli
    Rosales](https://unsplash.com/@nrosales) on [Unsplash](https://unsplash.com/photos/BbGxyxb2O3U)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Look in the repositories where academics store code for their journal articles
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lots of datasets used in academic studies aren’t published on platforms like
    Kaggle, but are still publicly available for use by other researchers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: One of the best ways to find datasets like these is by looking in the repositories
    associated with academic journal articles. Why? Because lots of journals require
    their contributors to make the underlying data publicly available. For example,
    two of the data sources I used during my master’s degree (the [Fragile Families](https://www.fragilefamilieschallenge.org/)
    dataset and the [Hate Speech Data](https://github.com/leondz/hatespeechdata) website)
    weren’t available on Kaggle; I found them through academic papers and their associated
    code repositories.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: How can you find these repositories? It’s actually surprisingly simple — I start
    by opening up [paperswithcode.com](https://paperswithcode.com/), search for papers
    in the area I’m interested in, and look at the available datasets until I find
    something that looks interesting. In my experience, this is a really neat way
    to find datasets which haven’t been done-to-death by the masses on Kaggle.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: BigQuery Public Datasets
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Honestly, I’ve no idea why more people don’t make use of BigQuery Public Datasets.
    There are literally *hundreds* of datasets covering everything from Google Search
    Trends to London Bicycle Hires to Genomic Sequencing of Cannabis.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: One of the things I especially like about this source is that lots of these
    datasets are incredibly commercially relevant. You can kiss goodbye to niche academic
    topics like flower classification and digit prediction; in BigQuery, there are
    datasets on real-world business issues like ad performance, website visits and
    economic forecasts.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Lots of people shy away from these datasets because they require SQL skills
    to load them. But, even if you don’t know SQL and only know a language like Python
    or R, I’d still encourage you to take an hour or two to learn some basic SQL and
    then start querying these datasets. It doesn’t take long to get up and running,
    and this truly is a treasure trove of high-value data assets.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: To use the datasets in BigQuery Public Datasets, you can sign up for a completely
    free account and create a sandbox project by following the instructions [here](https://cloud.google.com/bigquery/docs/sandbox).
    You don’t need to enter your credit card details or anything like that — just
    your name, your email, a bit of info about the project, and you’re good to go.
    If you need more computing power at a later date, you can upgrade the project
    to a paid one and access GCP’s compute resources and advanced BigQuery features,
    but I’ve personally never needed to do this and have found the sandbox to be more
    than adequate.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Try a dataset search engine
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'My final tip is to try using a dataset search engine. These are incredibly
    tools that have only emerged in the last few years, and they make it very easy
    to quickly see what’s out there. Three of my favourites are:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[Harvard Dataverse](https://dataverse.harvard.edu/)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Google Dataset Search](https://datasetsearch.research.google.com/)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Papers with Code](https://paperswithcode.com/datasets)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In my experience, searching with these tools can be a much more effective strategy
    than using generic search engines as you’re often provided with metadata about
    the datasets and you have the ability to rank them by how often they’ve been used
    and the publication date. Quite a nifty approach, if you ask me.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我的经验，使用这些工具进行搜索往往比使用通用搜索引擎更有效，因为你通常可以获得有关数据集的元数据，并且可以根据使用频率和发布日期对其进行排名。如果你问我，这是一种相当巧妙的方法。
- en: Thanks for reading! I hope you find these 5 strategies helpful, and please feel
    free to reach out if you have any feedback or questions :-)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！希望你觉得这 5 种策略有帮助，如果你有任何反馈或问题，请随时联系我 :-)
- en: One more thing — could you be in my 1%?
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有一件事——你能成为我那 1% 的一员吗？
- en: Less than 1% of my readers on Medium click my ‘Follow’ button, so it really
    means a lot when you do, whether here on Medium, [Twitter](https://twitter.com/matt_chapma)
    or [LinkedIn](https://www.linkedin.com/in/matt-chapman-ba8488118/).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Medium 上不到 1% 的读者点击我的“关注”按钮，因此无论是在 Medium、[Twitter](https://twitter.com/matt_chapma)
    还是 [LinkedIn](https://www.linkedin.com/in/matt-chapman-ba8488118/)，你做出这样的举动对我而言意义重大。
- en: If you’d like to get unlimited access to all of my stories (and the rest of
    Medium.com), you can sign up via my [referral link](https://medium.com/@mattchapmanmsc/membership)
    for $5 per month. It adds no extra cost to you vs. signing up via the general
    signup page, and helps to support my writing as I get a small commission.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望获得对我所有故事（以及 Medium.com 上其他内容）的无限访问权限，你可以通过我的 [推荐链接](https://medium.com/@mattchapmanmsc/membership)
    以每月 $5 注册。相比于通过普通注册页面注册，这不会额外增加你的费用，并且有助于支持我的写作，因为我会获得一小部分佣金。
