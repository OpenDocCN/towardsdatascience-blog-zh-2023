["```py\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport torch\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel.eval()\n\ntext = \"I have a dream\"\ninput_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n\noutputs = model.generate(input_ids, max_length=len(input_ids.squeeze())+5)\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(f\"Generated text: {generated_text}\")\n```", "```py\nGenerated text: I have a dream of being a doctor.\n```", "```py\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nimport time\n\ndef get_log_prob(logits, token_id):\n    # Compute the softmax of the logits\n    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n    log_probabilities = torch.log(probabilities)\n\n    # Get the log probability of the token\n    token_log_probability = log_probabilities[token_id].item()\n    return token_log_probability\n\ndef greedy_search(input_ids, node, length=5):\n    if length == 0:\n        return input_ids\n\n    outputs = model(input_ids)\n    predictions = outputs.logits\n\n    # Get the predicted next sub-word (here we use top-k search)\n    logits = predictions[0, -1, :]\n    token_id = torch.argmax(logits).unsqueeze(0)\n\n    # Compute the score of the predicted token\n    token_score = get_log_prob(logits, token_id)\n\n    # Add the predicted token to the list of input ids\n    new_input_ids = torch.cat([input_ids, token_id.unsqueeze(0)], dim=-1)\n\n    # Add node and edge to graph\n    next_token = tokenizer.decode(token_id, skip_special_tokens=True)\n    current_node = list(graph.successors(node))[0]\n    graph.nodes[current_node]['tokenscore'] = np.exp(token_score) * 100\n    graph.nodes[current_node]['token'] = next_token + f\"_{length}\"\n\n    # Recursive call\n    input_ids = greedy_search(new_input_ids, current_node, length-1)\n\n    return input_ids\n\n# Parameters\nlength = 5\nbeams = 1\n\n# Create a balanced tree with height 'length'\ngraph = nx.balanced_tree(1, length, create_using=nx.DiGraph())\n\n# Add 'tokenscore', 'cumscore', and 'token' attributes to each node\nfor node in graph.nodes:\n    graph.nodes[node]['tokenscore'] = 100\n    graph.nodes[node]['token'] = text\n\n# Start generating text\noutput_ids = greedy_search(input_ids, 0, length=length)\noutput = tokenizer.decode(output_ids.squeeze().tolist(), skip_special_tokens=True)\nprint(f\"Generated text: {output}\")\n```", "```py\nGenerated text: I have a dream of being a doctor.\n```", "```py\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport matplotlib.colors as mcolors\nfrom matplotlib.colors import LinearSegmentedColormap\n\ndef plot_graph(graph, length, beams, score):\n    fig, ax = plt.subplots(figsize=(3+1.2*beams**length, max(5, 2+length)), dpi=300, facecolor='white')\n\n    # Create positions for each node\n    pos = nx.nx_agraph.graphviz_layout(graph, prog=\"dot\")\n\n    # Normalize the colors along the range of token scores\n    if score == 'token':\n        scores = [data['tokenscore'] for _, data in graph.nodes(data=True) if data['token'] is not None]\n    elif score == 'sequence':\n        scores = [data['sequencescore'] for _, data in graph.nodes(data=True) if data['token'] is not None]\n    vmin = min(scores)\n    vmax = max(scores)\n    norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n    cmap = LinearSegmentedColormap.from_list('rg', [\"r\", \"y\", \"g\"], N=256) \n\n    # Draw the nodes\n    nx.draw_networkx_nodes(graph, pos, node_size=2000, node_shape='o', alpha=1, linewidths=4, \n                          node_color=scores, cmap=cmap)\n\n    # Draw the edges\n    nx.draw_networkx_edges(graph, pos)\n\n    # Draw the labels\n    if score == 'token':\n        labels = {node: data['token'].split('_')[0] + f\"\\n{data['tokenscore']:.2f}%\" for node, data in graph.nodes(data=True) if data['token'] is not None}\n    elif score == 'sequence':\n        labels = {node: data['token'].split('_')[0] + f\"\\n{data['sequencescore']:.2f}\" for node, data in graph.nodes(data=True) if data['token'] is not None}\n    nx.draw_networkx_labels(graph, pos, labels=labels, font_size=10)\n    plt.box(False)\n\n    # Add a colorbar\n    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n    sm.set_array([])\n    if score == 'token':\n        fig.colorbar(sm, ax=ax, orientation='vertical', pad=0, label='Token probability (%)')\n    elif score == 'sequence':\n        fig.colorbar(sm, ax=ax, orientation='vertical', pad=0, label='Sequence score')\n    plt.show()\n\n# Plot graph\nplot_graph(graph, length, 1.5, 'token')\n```", "```py\nfrom tqdm.notebook import tqdm\n\ndef greedy_sampling(logits, beams):\n    return torch.topk(logits, beams).indices\n\ndef beam_search(input_ids, node, bar, length, beams, sampling, temperature=0.1):\n    if length == 0:\n        return None\n\n    outputs = model(input_ids)\n    predictions = outputs.logits\n\n    # Get the predicted next sub-word (here we use top-k search)\n    logits = predictions[0, -1, :]\n\n    if sampling == 'greedy':\n        top_token_ids = greedy_sampling(logits, beams)\n    elif sampling == 'top_k':\n        top_token_ids = top_k_sampling(logits, temperature, 20, beams)\n    elif sampling == 'nucleus':\n        top_token_ids = nucleus_sampling(logits, temperature, 0.5, beams)\n\n    for j, token_id in enumerate(top_token_ids):\n        bar.update(1)\n\n        # Compute the score of the predicted token\n        token_score = get_log_prob(logits, token_id)\n        cumulative_score = graph.nodes[node]['cumscore'] + token_score\n\n        # Add the predicted token to the list of input ids\n        new_input_ids = torch.cat([input_ids, token_id.unsqueeze(0).unsqueeze(0)], dim=-1)\n\n        # Add node and edge to graph\n        token = tokenizer.decode(token_id, skip_special_tokens=True)\n        current_node = list(graph.successors(node))[j]\n        graph.nodes[current_node]['tokenscore'] = np.exp(token_score) * 100\n        graph.nodes[current_node]['cumscore'] = cumulative_score\n        graph.nodes[current_node]['sequencescore'] = 1/(len(new_input_ids.squeeze())) * cumulative_score\n        graph.nodes[current_node]['token'] = token + f\"_{length}_{j}\"\n\n        # Recursive call\n        beam_search(new_input_ids, current_node, bar, length-1, beams, sampling, 1)\n\n# Parameters\nlength = 5\nbeams = 2\n\n# Create a balanced tree with height 'length' and branching factor 'k'\ngraph = nx.balanced_tree(beams, length, create_using=nx.DiGraph())\nbar = tqdm(total=len(graph.nodes))\n\n# Add 'tokenscore', 'cumscore', and 'token' attributes to each node\nfor node in graph.nodes:\n    graph.nodes[node]['tokenscore'] = 100\n    graph.nodes[node]['cumscore'] = 0\n    graph.nodes[node]['sequencescore'] = 0\n    graph.nodes[node]['token'] = text\n\n# Start generating text\nbeam_search(input_ids, 0, bar, length, beams, 'greedy', 1)\n```", "```py\ndef get_best_sequence(G):\n    # Create a list of leaf nodes\n    leaf_nodes = [node for node in G.nodes() if G.out_degree(node)==0]\n\n    # Get the leaf node with the highest cumscore\n    max_score_node = None\n    max_score = float('-inf')\n    for node in leaf_nodes:\n        if G.nodes[node]['sequencescore'] > max_score:\n            max_score = G.nodes[node]['sequencescore']\n            max_score_node = node\n\n    # Retrieve the sequence of nodes from this leaf node to the root node in a list\n    path = nx.shortest_path(G, source=0, target=max_score_node)\n\n    # Return the string of token attributes of this sequence\n    sequence = \"\".join([G.nodes[node]['token'].split('_')[0] for node in path])\n\n    return sequence, max_score\n\nsequence, max_score = get_best_sequence(graph)\nprint(f\"Generated text: {sequence}\")\n```", "```py\nGenerated text: I have a dream. I have a dream\n```", "```py\n# Plot graph\nplot_graph(graph, length, beams, 'sequence')\n```", "```py\ndef plot_prob_distribution(probabilities, next_tokens, sampling, potential_nb, total_nb=50):\n    # Get top k tokens\n    top_k_prob, top_k_indices = torch.topk(probabilities, total_nb)\n    top_k_tokens = [tokenizer.decode([idx]) for idx in top_k_indices.tolist()]\n\n    # Get next tokens and their probabilities\n    next_tokens_list = [tokenizer.decode([idx]) for idx in next_tokens.tolist()]\n    next_token_prob = probabilities[next_tokens].tolist()\n\n    # Create figure\n    plt.figure(figsize=(0.4*total_nb, 5), dpi=300, facecolor='white')\n    plt.rc('axes', axisbelow=True)\n    plt.grid(axis='y', linestyle='-', alpha=0.5)\n    if potential_nb < total_nb:\n        plt.axvline(x=potential_nb-0.5, ls=':', color='grey', label='Sampled tokens')\n    plt.bar(top_k_tokens, top_k_prob.tolist(), color='blue')\n    plt.bar(next_tokens_list, next_token_prob, color='red', label='Selected tokens')\n    plt.xticks(rotation=45, ha='right', va='top')\n    plt.gca().spines['top'].set_visible(False)\n    plt.gca().spines['right'].set_visible(False)\n    if sampling == 'top_k':\n        plt.title('Probability distribution of predicted tokens with top-k sampling')\n    elif sampling == 'nucleus':\n        plt.title('Probability distribution of predicted tokens with nucleus sampling')\n    plt.legend()\n    plt.savefig(f'{sampling}_{time.time()}.png', dpi=300)\n    plt.close()\n\ndef top_k_sampling(logits, temperature, top_k, beams, plot=True):\n    assert top_k >= 1\n    assert beams <= top_k\n\n    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n    new_logits = torch.clone(logits)\n    new_logits[indices_to_remove] = float('-inf')\n\n    # Convert logits to probabilities\n    probabilities = torch.nn.functional.softmax(new_logits / temperature, dim=-1)\n\n    # Sample n tokens from the resulting distribution\n    next_tokens = torch.multinomial(probabilities, beams)\n\n    # Plot distribution\n    if plot:\n        total_prob = torch.nn.functional.softmax(logits / temperature, dim=-1)\n        plot_prob_distribution(total_prob, next_tokens, 'top_k', top_k)\n\n    return next_tokens\n\n# Start generating text\nbeam_search(input_ids, 0, bar, length, beams, 'top_k', 1)\n```", "```py\nsequence, max_score = get_best_sequence(graph)\nprint(f\"Generated text: {sequence}\")\n```", "```py\nGenerated text: I have a dream job and I want to\n```", "```py\n# Plot graph\nplot_graph(graph, length, beams, 'sequence')\n```", "```py\ndef nucleus_sampling(logits, temperature, p, beams, plot=True):\n    assert p > 0\n    assert p <= 1\n\n    # Sort the probabilities in descending order and compute cumulative probabilities\n    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n    probabilities = torch.nn.functional.softmax(sorted_logits / temperature, dim=-1)\n    cumulative_probabilities = torch.cumsum(probabilities, dim=-1)\n\n    # Create a mask for probabilities that are in the top-p\n    mask = cumulative_probabilities < p\n\n    # If there's not n index where cumulative_probabilities < p, we use the top n tokens instead\n    if mask.sum() > beams:\n        top_p_index_to_keep = torch.where(mask)[0][-1].detach().cpu().tolist()\n    else:\n        top_p_index_to_keep = beams\n\n    # Only keep top-p indices\n    indices_to_remove = sorted_indices[top_p_index_to_keep:]\n    sorted_logits[indices_to_remove] = float('-inf')\n\n    # Sample n tokens from the resulting distribution\n    probabilities = torch.nn.functional.softmax(sorted_logits / temperature, dim=-1)\n    next_tokens = torch.multinomial(probabilities, beams)\n\n    # Plot distribution\n    if plot:\n        total_prob = torch.nn.functional.softmax(logits / temperature, dim=-1)\n        plot_prob_distribution(total_prob, next_tokens, 'nucleus', top_p_index_to_keep)\n\n    return next_tokens\n\n# Start generating text\nbeam_search(input_ids, 0, bar, length, beams, 'nucleus', 1)\n```", "```py\nsequence, max_score = get_best_sequence(graph)\nprint(f\"Generated text: {sequence}\")\n```", "```py\nGenerated text: I have a dream. I'm going to\n```", "```py\n# Plot graph\nplot_graph(graph, length, beams, 'sequence')\n```"]