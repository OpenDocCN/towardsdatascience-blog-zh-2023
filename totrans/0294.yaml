- en: 'An Introduction to a Powerful Optimization Technique: Simulated Annealing'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/an-introduction-to-a-powerful-optimization-technique-simulated-annealing-87fd1e3676dd](https://towardsdatascience.com/an-introduction-to-a-powerful-optimization-technique-simulated-annealing-87fd1e3676dd)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/14be4f75d59bab24c155145334bc6078.png)'
  prefs: []
  type: TYPE_IMG
- en: It’s all about the temperature. Image by Dall-E 2.
  prefs: []
  type: TYPE_NORMAL
- en: Explanation, parameters, strengths, weaknesses and use cases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://hennie-de-harder.medium.com/?source=post_page-----87fd1e3676dd--------------------------------)[![Hennie
    de Harder](../Images/3e4f2cccd6cb976ca3f8bf15597daea8.png)](https://hennie-de-harder.medium.com/?source=post_page-----87fd1e3676dd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----87fd1e3676dd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----87fd1e3676dd--------------------------------)
    [Hennie de Harder](https://hennie-de-harder.medium.com/?source=post_page-----87fd1e3676dd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----87fd1e3676dd--------------------------------)
    ·9 min read·Mar 15, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '**Simulated annealing is an optimization technique that tries to find the global
    optimum for a mathematical optimization problem. It is a great technique and you
    can apply it to a wide range of problems. In this post we’ll dive into the definition,
    strengths, weaknesses and use cases of simulated annealing.**'
  prefs: []
  type: TYPE_NORMAL
- en: You are not supposed to have favorite machine learning models or optimization
    techniques. But people do have their favorites, take LightGBM for example. In
    optimization, one technique I like is simulated annealing. It’s easy to understand
    and works for many different types of problems.
  prefs: []
  type: TYPE_NORMAL
- en: This post requires knowledge about mathematical optimization problems and local
    search. If you don’t have this knowledge, [this post](https://medium.com/towards-data-science/mathematical-optimization-heuristics-every-data-scientist-should-know-b26de0bd43e6)
    is a good start.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What is Simulated Annealing?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simulated annealing (SA) is a stochastic optimization algorithm used to find
    the global minimum of a cost function. It is a meta-heuristic in local search
    to solve the problem of getting stuck in local minima. The algorithm is based
    on the physical process of [annealing in metallurgy](https://www.chemeurope.com/en/encyclopedia/Annealing_%28metallurgy%29.html),
    where metal is heated and then slowly cooled. By doing this, the metal is softened
    and prepared for further work. The algorithm was introduced [in 1983 in this paper](https://www.dcs.gla.ac.uk/~pat/ads2/java/TxSxP/papers/sa.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: In SA, the algorithm starts with an initial solution to the problem and a high
    initial temperature. The solution is perturbed by making a random change to it,
    and the resulting new solution is evaluated using an objective function. If the
    new solution is better (i.e., has a lower cost), it is accepted as the new current
    solution. If the new solution is worse (i.e., has a higher cost), it may still
    be accepted with a probability that depends on the temperature and the degree
    of worsening, this is called the acceptance value. The idea is to allow the algorithm
    to occasionally accept worse solutions early on in the process, in order to explore
    a wider range of solutions and avoid getting stuck in local minima.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *metropolis acceptance criterion* is used normally to calculate the acceptance
    value. The inputs are the temperature and the difference between the score of
    the neighbor solution (new solution) and the current solution. So first, calculate
    the difference, and if it’s below zero accept the current solution. If it’s above
    zero, calculate the acceptance value, or the probability of accepting this solution
    while it’s worse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c8de40e24b75a747d4a803693bbe5df1.png)'
  prefs: []
  type: TYPE_IMG
- en: As the algorithm progresses, the temperature is gradually decreased according
    to a cooling schedule. This reduces the likelihood of accepting worse solutions
    and allows the algorithm to converge towards the global minimum. There are different
    types of cooling schedules that can be used, such as linear, logarithmic, or geometric
    cooling.
  prefs: []
  type: TYPE_NORMAL
- en: SA terminates either when a satisfactory solution is found, or when a maximum
    number of iterations or a certain convergence criteria is met.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a7dab5df8bfff401eb24d203a6ef028.png)'
  prefs: []
  type: TYPE_IMG
- en: Pseudocode and functions used in a basic version of Simulated Annealing.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you want to get the best out of your simulated annealing implementation
    (and obviously you do), it’s important to pay attention to its parameters. Here
    are the most important ones:'
  prefs: []
  type: TYPE_NORMAL
- en: Temperature and Cooling Schedule
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The initial temperature is the starting temperature of the system and determines
    the probability of accepting a worse solution. If the initial temperature is too
    high, the algorithm may accept too many bad solutions, while if it is too low,
    the algorithm may get stuck in a local minimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'Directly related is the cooling schedule: it determines how fast the temperature
    decreases during the annealing process. If the cooling schedule is too slow, the
    algorithm may not converge to a good solution in a reasonable amount of time,
    while if it is too fast, the algorithm may converge to a suboptimal solution.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dba8609b3350c8bea415467e2448fcde.png)'
  prefs: []
  type: TYPE_IMG
- en: Different cooling schedules to determine the temperature for a given iteration.
    Starting temperature set to 100 and iteration limit of 100\. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Acceptance Criterion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The acceptance criterion determines the threshold for accepting or rejecting
    a new solution. The value of the acceptance criterion is the probability that
    a new solution is accepted while it scores worse than the current solution. It
    is based on the difference between the new solution score and the current solution
    score.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below the acceptance values for different deltas (1, 10 and 100) related to
    different cooling schedules:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8dd8328098dc9c3f93824d84041e1575.png)'
  prefs: []
  type: TYPE_IMG
- en: Acceptance values for different deltas and cooling schedules. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'So if we look at one example, iteration 75 and delta 10, the probabilities
    for accepting the solution are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e9214f081a01982f6586a2e9e1ada6e1.png)'
  prefs: []
  type: TYPE_IMG
- en: The differences are quite large! That’s why it’s important to test different
    cooling schedules with different parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Number of Iterations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The number of iterations should be set high enough to allow the algorithm to
    converge to a good solution, but not so high that the algorithm takes too long
    to run.
  prefs: []
  type: TYPE_NORMAL
- en: Neighborhood Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the set of moves that can be applied to the current solution to generate
    a new one. The neighborhood function should be chosen carefully to balance the
    exploration of the search space with the exploitation of good solutions. For every
    problem, you can construct it in different ways. Below are 4 different types of
    local moves for the traveling salesman problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fcda92795fa9d83acc1cbca2bdbd2af3.png)'
  prefs: []
  type: TYPE_IMG
- en: Common local moves for the traveling salesman problem. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Strengths & Weaknesses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SA is powerful and effective. Here are five strength points of the meta-heuristic:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Strength 1\. Effective in finding global optima**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of getting stuck in local optima (like local search), SA is designed
    to find the global minimum of a cost function. By allowing for random moves and
    probabilistic acceptance of worse solutions, it can effectively search large solution
    spaces and find global optima, even in the presence of local optima.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below you can see what happens when you apply local search without meta-heuristic
    like SA. You start somewhere and you only accept better solutions. Then you end
    up in a local minimum:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3c269143cb8a2e8e67fd81431da1683.png)'
  prefs: []
  type: TYPE_IMG
- en: Local search. The final solution depends on the starting solution. Gif by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Simulated annealing can escape local minima, because it accepts worse solutions.
    If we start with the same solution, this can happen with SA:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af2bb59146f79a4249504e32775a9810.png)'
  prefs: []
  type: TYPE_IMG
- en: Simulated annealing search for the optimal solution. Gif by author.
  prefs: []
  type: TYPE_NORMAL
- en: Besides SA, you can try other meta-heuristics to escape local minima, like iterated
    local search or tabu search.
  prefs: []
  type: TYPE_NORMAL
- en: '**Strength 2\. Robustness**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Simulated annealing can handle a wide range of optimization problems with different
    types of cost functions, constraints, and search spaces. It does not rely on assumptions
    about the nature of the cost function, and is robust to noise and uncertainty
    in the input.
  prefs: []
  type: TYPE_NORMAL
- en: '**Strength 3\. Flexibility**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another strength is that SA can be easily customized and adapted to different
    problem domains and search spaces. The cooling schedule, neighborhood structure,
    and acceptance criterion can be tuned to suit the specific problem being solved.
  prefs: []
  type: TYPE_NORMAL
- en: '**Strength 4\. Efficient**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generally, the algorithm is efficient in terms of computational resources required
    to find a solution. It can often find good solutions quickly, particularly in
    the early stages of the algorithm when large, random moves are allowed. If the
    problem is large, you can save a lot of time by using SA instead of exact algorithms
    like MIP, while still getting good quality solutions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Strength 5\. Parallelizable**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Parallelizing allows for faster optimization and the ability to search a wider
    range of solutions simultaneously. SA is easy to parallelize. Below a code snippet
    that sets up parallelization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Beware, the implementations of `simulated_annealing` and the `Problem` class
    are really simplistic and unreal. A random number is returned and the lowest one
    is accepted as the best solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with any optimization technique, there are some downsides too. Here are
    the most important ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Weakness 1\. Tuning**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tuning is required. E.g. the cooling schedule, neighborhood structure, and acceptance
    criterion require tuning in order to achieve good results. To finetune these parameters,
    it’s best to use a combination of domain knowledge and experimentation. It’s not
    only important to tune for different problems, but problems with the same structure
    that are different in size are probably also better off with different parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Weakness 2\. No guarantee of optimality**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Simulated annealing cannot guarantee finding the global optimum, as it relies
    on probabilistic moves and acceptance of worse solutions. It can only guarantee
    finding a locally optimal solution. By using SA, you don’t know the gap between
    the optimal solution and the current best solution, while this is possible if
    you would use an exact algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0a92ba1d2b3bb45d2ff31ec4c76ce9d3.png)'
  prefs: []
  type: TYPE_IMG
- en: Gurobi log with % gap between the current solution and the best possible solution.
    Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Weakness 3\. Slow convergence**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some cases, SA can converge slowly, particularly for complex problems with
    large search spaces. This can make it inefficient for problems with tight time
    constraints. One way to solve this is to specify a time limit.
  prefs: []
  type: TYPE_NORMAL
- en: When should you try Simulated Annealing?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several use cases that are a good fit for simulated annealing. [Combinatorial
    optimization problems](https://en.wikipedia.org/wiki/Combinatorial_optimization)
    with a large search space and a [non-convex](https://en.wikipedia.org/wiki/Convex_set)
    objective function, are really well suited for simulated annealing. Examples are
    the minimum spanning tree problem, traveling salesman, capacitated vehicle routing
    problem or the graph coloring problem.
  prefs: []
  type: TYPE_NORMAL
- en: Also for [nonlinear](https://en.wikipedia.org/wiki/Nonlinear_programming) optimization
    problems, simulated annealing can be effective. If these problems have multiple
    local optima, traditional gradient-based optimization techniques may fail. For
    example, simulated annealing has been used for [protein folding](https://pubmed.ncbi.nlm.nih.gov/16732545/)
    and [molecular conformation](https://www.cell.com/structure/pdf/S0969-2126(97)00190-1.pdf)
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, for certain types of problems you shouldn’t consider simulated
    annealing. It may not be the best choice for [convex](https://en.wikipedia.org/wiki/Convex_set)
    optimization problems, where traditional gradient-based optimization techniques
    are often more efficient and effective.
  prefs: []
  type: TYPE_NORMAL
- en: Simulated annealing can be computationally expensive and slow for large-scale
    optimization problems with high-dimensional search spaces. In these cases, other
    optimization techniques such as genetic algorithms or particle swarm optimization
    may be more effective, because they look at multiple solutions at once.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1e6ce417cd06f81f58a1fcfd7650a9e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Protein folding. Photo by [National Cancer Institute](https://unsplash.com/@nci?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simulated annealing is a stochastic optimization algorithm based on the physical
    process of annealing in metallurgy. It can be used to find the global minimum
    of a cost function by allowing for random moves and probabilistic acceptance of
    worse solutions, thus effectively searching large solution spaces and avoiding
    getting stuck in local minima. SA is powerful, robust, flexible, efficient, and
    parallelizable. However, it requires tuning, cannot guarantee finding the global
    optimum, and can converge slowly in some cases. To get the best results, it’s
    important to pay attention to its parameters, such as the initial temperature,
    cooling schedule, neighborhood function, acceptance criterion, and number of iterations.
    For some use cases, you should consider other techniques like genetic algorithms
    or particle swarm optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Related
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](/exact-algorithm-or-heuristic-20d59d7fb359?source=post_page-----87fd1e3676dd--------------------------------)
    [## Exact Algorithm or Heuristic?'
  prefs: []
  type: TYPE_NORMAL
- en: A step-by-step guide to make the right choice for your mathematical optimization
    problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/exact-algorithm-or-heuristic-20d59d7fb359?source=post_page-----87fd1e3676dd--------------------------------)
    [](/four-ways-to-combine-mathematical-optimization-and-machine-learning-8cb874276254?source=post_page-----87fd1e3676dd--------------------------------)
    [## Five ways to combine Mathematical Optimization and Machine Learning
  prefs: []
  type: TYPE_NORMAL
- en: Practical examples of combining two forces.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/four-ways-to-combine-mathematical-optimization-and-machine-learning-8cb874276254?source=post_page-----87fd1e3676dd--------------------------------)
    [](/mathematical-optimization-heuristics-every-data-scientist-should-know-b26de0bd43e6?source=post_page-----87fd1e3676dd--------------------------------)
    [## Mathematical Optimization Heuristics Every Data Scientist Should Know
  prefs: []
  type: TYPE_NORMAL
- en: Local search, genetic algorithms and more.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/mathematical-optimization-heuristics-every-data-scientist-should-know-b26de0bd43e6?source=post_page-----87fd1e3676dd--------------------------------)
  prefs: []
  type: TYPE_NORMAL
