["```py\ndate        dcoilwtico\n2013-01-01  NaN\n2013-01-02  93.14\n2013-01-03  92.97\n2013-01-04  93.12\n2013-01-07  93.20\n2013-01-08  93.21\n2013-01-09  93.08\n2013-01-10  93.81\n2013-01-11  93.60\n2013-01-14  94.27\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\n\n# Fit scalers\nscalers = {}\nfor x in df.columns:\n  scalers[x] = StandardScaler().fit(df[x].values.reshape(-1, 1))\n\n# Transform data via scalers\nnorm_df = df.copy()\nfor i, key in enumerate(scalers.keys()):\n  norm = scalers[key].transform(norm_df.iloc[:, i].values.reshape(-1, 1))\n  norm_df.iloc[:, i] = norm\n```", "```py\n# Defining a function that creates sequences and targets as shown above\ndef generate_sequences(df: pd.DataFrame, tw: int, pw: int, target_columns, drop_targets=False):\n  '''\n  df: Pandas DataFrame of the univariate time-series\n  tw: Training Window - Integer defining how many steps to look back\n  pw: Prediction Window - Integer defining how many steps forward to predict\n\n  returns: dictionary of sequences and targets for all sequences\n  '''\n  data = dict() # Store results into a dictionary\n  L = len(df)\n  for i in range(L-tw):\n    # Option to drop target from dataframe\n    if drop_targets:\n      df.drop(target_columns, axis=1, inplace=True)\n\n    # Get current sequence  \n    sequence = df[i:i+tw].values\n    # Get values right after the current sequence\n    target = df[i+tw:i+tw+pw][target_columns].values\n    data[i] = {'sequence': sequence, 'target': target}\n  return data\n```", "```py\nclass SequenceDataset(Dataset):\n\n  def __init__(self, df):\n    self.data = df\n\n  def __getitem__(self, idx):\n    sample = self.data[idx]\n    return torch.Tensor(sample['sequence']), torch.Tensor(sample['target'])\n\n  def __len__(self):\n    return len(self.data)\n```", "```py\n# Here we are defining properties for our model\n\nBATCH_SIZE = 16 # Training batch size\nsplit = 0.8 # Train/Test Split ratio\n\nsequences = generate_sequences(norm_df.dcoilwtico.to_frame(), sequence_len, nout, 'dcoilwtico')\ndataset = SequenceDataset(sequences)\n\n# Split the data according to our split ratio and load each subset into a\n# separate DataLoader object\ntrain_len = int(len(dataset)*split)\nlens = [train_len, len(dataset)-train_len]\ntrain_ds, test_ds = random_split(dataset, lens)\ntrainloader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\ntestloader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n```", "```py\nclass LSTMForecaster(nn.Module):\n\n  def __init__(self, n_features, n_hidden, n_outputs, sequence_len, n_lstm_layers=1, n_deep_layers=10, use_cuda=False, dropout=0.2):\n    '''\n    n_features: number of input features (1 for univariate forecasting)\n    n_hidden: number of neurons in each hidden layer\n    n_outputs: number of outputs to predict for each training example\n    n_deep_layers: number of hidden dense layers after the lstm layer\n    sequence_len: number of steps to look back at for prediction\n    dropout: float (0 < dropout < 1) dropout ratio between dense layers\n    '''\n    super().__init__()\n\n    self.n_lstm_layers = n_lstm_layers\n    self.nhid = n_hidden\n    self.use_cuda = use_cuda # set option for device selection\n\n    # LSTM Layer\n    self.lstm = nn.LSTM(n_features,\n                        n_hidden,\n                        num_layers=n_lstm_layers,\n                        batch_first=True) # As we have transformed our data in this way\n\n    # first dense after lstm\n    self.fc1 = nn.Linear(n_hidden * sequence_len, n_hidden) \n    # Dropout layer \n    self.dropout = nn.Dropout(p=dropout)\n\n    # Create fully connected layers (n_hidden x n_deep_layers)\n    dnn_layers = []\n    for i in range(n_deep_layers):\n      # Last layer (n_hidden x n_outputs)\n      if i == n_deep_layers - 1:\n        dnn_layers.append(nn.ReLU())\n        dnn_layers.append(nn.Linear(nhid, n_outputs))\n      # All other layers (n_hidden x n_hidden) with dropout option\n      else:\n        dnn_layers.append(nn.ReLU())\n        dnn_layers.append(nn.Linear(nhid, nhid))\n        if dropout:\n          dnn_layers.append(nn.Dropout(p=dropout))\n    # compile DNN layers\n    self.dnn = nn.Sequential(*dnn_layers)\n\n  def forward(self, x):\n\n    # Initialize hidden state\n    hidden_state = torch.zeros(self.n_lstm_layers, x.shape[0], self.nhid)\n    cell_state = torch.zeros(self.n_lstm_layers, x.shape[0], self.nhid)\n\n    # move hidden state to device\n    if self.use_cuda:\n      hidden_state = hidden_state.to(device)\n      cell_state = cell_state.to(device)\n\n    self.hidden = (hidden_state, cell_state)\n\n    # Forward Pass\n    x, h = self.lstm(x, self.hidden) # LSTM\n    x = self.dropout(x.contiguous().view(x.shape[0], -1)) # Flatten lstm out \n    x = self.fc1(x) # First Dense\n    return self.dnn(x) # Pass forward through fully connected DNN.\n```", "```py\nnhid = 50 # Number of nodes in the hidden layer\nn_dnn_layers = 5 # Number of hidden fully connected layers\nnout = 1 # Prediction Window\nsequence_len = 180 # Training Window\n\n# Number of features (since this is a univariate timeseries we'll set\n# this to 1 -- multivariate analysis is coming in the future)\nninp = 1\n\n# Device selection (CPU | GPU)\nUSE_CUDA = torch.cuda.is_available()\ndevice = 'cuda' if USE_CUDA else 'cpu'\n\n# Initialize the model\nmodel = LSTMForecaster(ninp, nhid, nout, sequence_len, n_deep_layers=n_dnn_layers, use_cuda=USE_CUDA).to(device)\n```", "```py\n# Set learning rate and number of epochs to train over\nlr = 4e-4\nn_epochs = 20\n\n# Initialize the loss function and optimizer\ncriterion = nn.MSELoss().to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n```", "```py\n# Lists to store training and validation losses\nt_losses, v_losses = [], []\n# Loop over epochs\nfor epoch in range(n_epochs):\n  train_loss, valid_loss = 0.0, 0.0\n\n  # train step\n  model.train()\n  # Loop over train dataset\n  for x, y in trainloader:\n    optimizer.zero_grad()\n    # move inputs to device\n    x = x.to(device)\n    y  = y.squeeze().to(device)\n    # Forward Pass\n    preds = model(x).squeeze()\n    loss = criterion(preds, y) # compute batch loss\n    train_loss += loss.item()\n    loss.backward()\n    optimizer.step()\n  epoch_loss = train_loss / len(trainloader)\n  t_losses.append(epoch_loss)\n\n  # validation step\n  model.eval()\n  # Loop over validation dataset\n  for x, y in testloader:\n    with torch.no_grad():\n      x, y = x.to(device), y.squeeze().to(device)\n      preds = model(x).squeeze()\n      error = criterion(preds, y)\n    valid_loss += error.item()\n  valid_loss = valid_loss / len(testloader)\n  v_losses.append(valid_loss)\n\n  print(f'{epoch} - train: {epoch_loss}, valid: {valid_loss}')\nplot_losses(t_losses, v_losses)\n```", "```py\ndef make_predictions_from_dataloader(model, unshuffled_dataloader):\n  model.eval()\n  predictions, actuals = [], []\n  for x, y in unshuffled_dataloader:\n    with torch.no_grad():\n      p = model(x)\n      predictions.append(p)\n      actuals.append(y.squeeze())\n  predictions = torch.cat(predictions).numpy()\n  actuals = torch.cat(actuals).numpy()\n  return predictions.squeeze(), actuals\n```", "```py\n def one_step_forecast(model, history):\n      '''\n      model: PyTorch model object\n      history: a sequence of values representing the latest values of the time \n      series, requirement -> len(history.shape) == 2\n\n      outputs a single value which is the prediction of the next value in the\n      sequence.\n      '''\n      model.cpu()\n      model.eval()\n      with torch.no_grad():\n        pre = torch.Tensor(history).unsqueeze(0)\n        pred = self.model(pre)\n      return pred.detach().numpy().reshape(-1)\n\n  def n_step_forecast(data: pd.DataFrame, target: str, tw: int, n: int, forecast_from: int=None, plot=False):\n      '''\n      n: integer defining how many steps to forecast\n      forecast_from: integer defining which index to forecast from. None if\n      you want to forecast from the end.\n      plot: True if you want to output a plot of the forecast, False if not.\n      '''\n      history = data[target].copy().to_frame()\n\n      # Create initial sequence input based on where in the series to forecast \n      # from.\n      if forecast_from:\n        pre = list(history[forecast_from - tw : forecast_from][target].values)\n      else:\n        pre = list(history[self.target])[-tw:]\n\n      # Call one_step_forecast n times and append prediction to history\n      for i, step in enumerate(range(n)):\n        pre_ = np.array(pre[-tw:]).reshape(-1, 1)\n        forecast = self.one_step_forecast(pre_).squeeze()\n        pre.append(forecast)\n\n      # The rest of this is just to add the forecast to the correct time of \n      # the history series\n      res = history.copy()\n      ls = [np.nan for i in range(len(history))]\n\n      # Note: I have not handled the edge case where the start index + n is \n      # before the end of the dataset and crosses past it.\n      if forecast_from:\n        ls[forecast_from : forecast_from + n] = list(np.array(pre[-n:]))\n        res['forecast'] = ls\n        res.columns = ['actual', 'forecast']\n      else:\n        fc = ls + list(np.array(pre[-n:]))\n        ls = ls + [np.nan for i in range(len(pre[-n:]))]\n        ls[:len(history)] = history[self.target].values\n        res = pd.DataFrame([ls, fc], index=['actual', 'forecast']).T\n      return res\n```"]