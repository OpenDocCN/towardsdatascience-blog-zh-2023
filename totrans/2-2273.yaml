- en: Using OpenCLIP for Image Search and Automatic Captioning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 OpenCLIP 进行图像搜索和自动字幕生成
- en: 原文：[https://towardsdatascience.com/using-openclip-for-image-search-and-automatic-captioning-fa1cbbd48ce4](https://towardsdatascience.com/using-openclip-for-image-search-and-automatic-captioning-fa1cbbd48ce4)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/using-openclip-for-image-search-and-automatic-captioning-fa1cbbd48ce4](https://towardsdatascience.com/using-openclip-for-image-search-and-automatic-captioning-fa1cbbd48ce4)
- en: How LAION used more data and new ML training techniques to improve image and
    text embeddings for various applications
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何利用更多数据和新的机器学习训练技术来改善图像和文本嵌入，以应用于各种场景
- en: '[](https://robgon.medium.com/?source=post_page-----fa1cbbd48ce4--------------------------------)[![Robert
    A. Gonsalves](../Images/96b4da0f602a1cd9d1e1d2917868cbee.png)](https://robgon.medium.com/?source=post_page-----fa1cbbd48ce4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fa1cbbd48ce4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fa1cbbd48ce4--------------------------------)
    [Robert A. Gonsalves](https://robgon.medium.com/?source=post_page-----fa1cbbd48ce4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://robgon.medium.com/?source=post_page-----fa1cbbd48ce4--------------------------------)[![Robert
    A. Gonsalves](../Images/96b4da0f602a1cd9d1e1d2917868cbee.png)](https://robgon.medium.com/?source=post_page-----fa1cbbd48ce4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fa1cbbd48ce4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fa1cbbd48ce4--------------------------------)
    [Robert A. Gonsalves](https://robgon.medium.com/?source=post_page-----fa1cbbd48ce4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fa1cbbd48ce4--------------------------------)
    ·12 min read·Mar 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fa1cbbd48ce4--------------------------------)
    ·阅读时间 12 分钟·2023 年 3 月 7 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/01279abccfb8391b5c22a466bcb77154.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01279abccfb8391b5c22a466bcb77154.png)'
- en: “**high-tech computer system for finding pictures in a large library,”** I*mage
    created using an AI image creation program,* Midjourney, and edited by the author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: “**用于在大型库中查找图片的高科技计算机系统**，”我*使用 AI 图像创建程序* Midjourney 创建，并由作者编辑
- en: I have been using and writing about OpenAI’s CLIP system since it came out in
    2021 [1]. It consists of image and text encoding models that can be used for various
    forms of cross-modal comparison, like using a text query to find the best matching
    image in a library quickly.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自 2021 年推出以来，我一直在使用和撰写关于 OpenAI 的 CLIP 系统的文章 [1]。它包含可以用于各种跨模态比较的图像和文本编码模型，例如使用文本查询快速找到库中最匹配的图像。
- en: In December 2022, an independent group of researchers known as LAION released
    a paper called “Reproducible scaling laws for contrastive language-image learning”
    [2] that describes how they first reimplemented and trained a model similar to
    CLIP and then experimented with improving the system by training with a larger
    dataset and using new ML techniques. They call their new model OpenCLIP.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 2022 年 12 月，一个名为 LAION 的独立研究小组发布了一篇名为《对比语言-图像学习的可重复扩展定律》的论文 [2]，描述了他们如何首先重新实现并训练了一个类似于
    CLIP 的模型，然后通过使用更大的数据集和新的机器学习技术来改进系统。他们称他们的新模型为 OpenCLIP。
- en: In this article, I will provide some background info on the original CLIP, describe
    how LAION improved the model, and show some results from my experiments with the
    two systems using images from the [Library of Congress’s Flickr photostream](https://www.flickr.com/photos/library_of_congress/).
    I also implemented a cool technique called “embedding arithmetic” from Meta AI
    to search for photos using both images and text prompts [3].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将提供有关原始 CLIP 的一些背景信息，描述 LAION 如何改进该模型，并展示我使用 [国会图书馆 Flickr 照片流](https://www.flickr.com/photos/library_of_congress/)
    的图像进行的两个系统实验的一些结果。我还实现了一种称为“嵌入算术”的酷技术，这项技术来自 Meta AI，用于通过图像和文本提示搜索照片 [3]。
- en: I’ll end the article with a demo of using a variant of LAION’s OpenCLIP model
    that automatically generates picture captions. For example, I created the title
    picture for this article using Midjourney with the text prompt, “high-tech computer
    system for finding pictures in a large library.” When I sent the image into OpenCLIP,
    it generated the caption, “a girl is standing in a library looking at a television.”
    Not bad!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在文章末尾展示一个使用 LAION 的 OpenCLIP 模型变体自动生成图片字幕的演示。例如，我使用 Midjourney 创建了这篇文章的标题图片，文本提示为“用于在大型库中查找图片的高科技计算机系统”。当我将该图像输入
    OpenCLIP 时，它生成了标题为“一个女孩站在图书馆里看电视”的字幕。还不错！
- en: '**OpenAI’s CLIP**'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**OpenAI 的 CLIP**'
- en: In 2021, OpenAI published a paper called “Learning Transferable Visual Models
    From Natural Language Supervision,” where they described their new system called
    Contrastive Language-Image Pre-training (CLIP.) It comprises two AI models, a
    Text Encoder and an Image Encoder, trained to create arrays of numbers called
    embeddings. They released the source code and pre-trained models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在2021年，OpenAI发布了一篇名为“从自然语言监督中学习可转移的视觉模型”的论文，描述了他们的新系统，称为对比语言-图像预训练（CLIP）。该系统包括两个AI模型，一个文本编码器和一个图像编码器，训练以生成称为嵌入的数字数组。他们发布了源代码和预训练模型。
- en: '![](../Images/7dbd95a11ad1fe78c8338489e0a2eb57.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7dbd95a11ad1fe78c8338489e0a2eb57.png)'
- en: '**System Components for CLIP,** Diagram by OpenAI'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**CLIP的系统组件，** OpenAI绘制的图示'
- en: OpenAI trained the encoders using 400 million image/caption pairs with the goal
    of having the encoders generate embeddings that are similar when the text and
    images are similar. The system can be used for various tasks involving text and
    images like retrieval, i.e., searching for images with text, and classification,
    i.e., automatically assigning images to categories. You can read more about CLIP
    in my [article on using the system](https://medium.com/towards-data-science/using-openais-clip-to-search-for-design-patents-7fcc63d91033)
    to search for design patents.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI使用4亿对图像/文本对训练编码器，目标是使编码器生成相似的嵌入，当文本和图像相似时。该系统可用于涉及文本和图像的各种任务，如检索，即用文本搜索图像，以及分类，即自动将图像分配到类别中。你可以在我的[使用该系统的文章](https://medium.com/towards-data-science/using-openais-clip-to-search-for-design-patents-7fcc63d91033)中了解更多关于CLIP的信息。
- en: LAION’s OpenCLIP
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LAION的OpenCLIP
- en: LAION is a group of independent researchers that provides datasets, tools, and
    AI models. They reimplemented OpenAI’s CLIP models and further trained them on
    their dataset of 2 billion image/caption pairs to improve performance. In their
    paper [2], they discussed how they overcame issues when training with more data
    by using a floating-point number format called bfloat16, invented by Google [4].
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: LAION是一个独立研究者小组，提供数据集、工具和AI模型。他们重新实现了OpenAI的CLIP模型，并在他们的20亿对图像/文本对的数据集上进一步训练以提高性能。在他们的论文[2]中，他们讨论了如何通过使用Google发明的浮点数格式bfloat16解决在更多数据训练时遇到的问题[4]。
- en: For larger model scales … we observed loss spikes during training which had
    an adverse effect on performance. We fixed the issue by switching from mixed precision
    with float16 to mixed precision with bfloat16\. We hypothesize that bfloat16 fixed
    the issue due to larger models typically showing larger activation values … making
    bfloat16 more suitable with its wider dynamic range. — Mehdi Cherti, et al., from
    LAION
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于更大规模的模型……我们观察到训练过程中损失值的激增，对性能产生了不利影响。我们通过从float16的混合精度切换到bfloat16的混合精度解决了这个问题。我们推测bfloat16解决了这个问题，因为较大的模型通常显示更大的激活值……使bfloat16在其更宽的动态范围下更为合适。——
    Mehdi Cherti等，来自LAION
- en: Their paper shows how their OpenCLIP system is better than OpenAI’s CLIP on
    tasks like searching for images. The graph below shows results from OpenCLIP in
    orange and CLIP in blue, where smaller is better.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的论文展示了他们的OpenCLIP系统在图像搜索等任务上优于OpenAI的CLIP。下图中，OpenCLIP用橙色表示，CLIP用蓝色表示，其中数值越小越好。
- en: '![](../Images/b4228581b4fdc02b324a04af68800b02.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4228581b4fdc02b324a04af68800b02.png)'
- en: '**Results of Using CLIP (blue) and Open (clip) for Image Retrieval, Smaller
    is Better,** Chart by LAION'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用CLIP（蓝色）和OpenCLIP进行图像检索的结果（数值越小越好），** LAION绘制的图表'
- en: 'There’s a lot of info in the graph. I’ll see if I can unpack it. The horizontal
    axis indicates how much computation was used for training in Giga multiply-accumulate
    operations (GMACs.) The vertical axis shows the accuracy of results, defined as
    100 - Recall @ 5 for the Flicker 30K dataset. For example, if the system searched
    for five pictures of cats, and only four contained a cat, it would get an 80%
    value for Recall@5\. But subtracted from 100 it would get a 20%, where lower is
    better. Are you still with me? The shapes in the graph are various datasets with
    different sizes, as indicated by the key on the right. The blue line shows how
    CLIP performed with CLIP-WIT, their dataset, trained with multiple configurations.
    The orange line shows how the best OpenCLIP models performed with the LAION datasets
    with various configurations. The bottom line: OpenCLIP is better than CLIP.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图表中有很多信息。我会看看能否将其拆解。横轴表示用于训练的计算量，以 Giga 乘加操作 (GMACs) 为单位。纵轴显示了结果的准确度，定义为 100
    - Recall @ 5 针对 Flicker 30K 数据集。例如，如果系统搜索了五张猫的照片，只有四张包含猫，则 Recall@5 值为 80%。但从
    100 中减去它将得到 20%，值越低越好。你跟上了吗？图表中的形状代表各种数据集，右侧的键显示了不同的大小。蓝线显示了 CLIP 在 CLIP-WIT 数据集上的表现，经过多种配置训练。橙线显示了最佳
    OpenCLIP 模型在 LAION 数据集上的表现，使用了各种配置。底线：OpenCLIP 比 CLIP 更好。
- en: As shown in the above graph, LAION calculated the equations for the relationships
    between training and performance with exponential components. They discuss using
    this “scaling law” to predict how much more training would be needed to improve
    their models further [2].
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，LAION 计算了训练和性能之间关系的方程，带有指数成分。他们讨论了使用这种“缩放定律”来预测需要多少额外的训练以进一步提高他们的模型[2]。
- en: In the next section, I will show how I built and ran some tests to show how
    the two systems perform with US Library of Congress images.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我将展示如何构建和运行一些测试，以展示这两个系统在美国国会图书馆图像上的表现。
- en: Comparing OpenCLIP with CLIP
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较 OpenCLIP 和 CLIP
- en: I used photos from the [Flickr photostream](https://www.flickr.com/people/library_of_congress/)
    of the Library of Congress (LOC) for my tests. The library posted over 40 thousand
    images with captions in its collection for people to browse and post comments.
    Note that all the photos in the collection are marked as “no known copyright restrictions,”
    so they can be used for any purpose.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了[国会图书馆 Flickr 照片流](https://www.flickr.com/people/library_of_congress/)中的照片进行测试。图书馆发布了超过四万张带有说明的图片供人们浏览和评论。请注意，所有图片均标记为“无已知版权限制”，因此可以用于任何目的。
- en: Here are some samples from the dataset, with captions above the images.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些数据集样本，图像上方有说明。
- en: '![](../Images/2c9c475033372fb2fecbbb3cedb166a5.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c9c475033372fb2fecbbb3cedb166a5.png)'
- en: '**Samples from the Library of Congress Flickr Photostream**, Images from the
    LOC'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**来自国会图书馆 Flickr 照片流的样本**，来自 LOC 的图像'
- en: With these samples, you can get a sense of the types of images in the dataset,
    paintings, old photos, new photos, etc.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些样本，你可以对数据集中图像的类型有个了解，包括绘画、旧照片、新照片等。
- en: To test the systems, I ran the six captions and images through CLIP and OpenCLIP
    and calculated the cosine similarity, a measure of closeness between the text
    and image embeddings. Note that the results range roughly from 0.0 to 0.4, where
    the lower numbers indicate a non-match, and the higher numbers indicate a match.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这些系统，我将六个说明和图像通过 CLIP 和 OpenCLIP，并计算了余弦相似度，这是一种衡量文本和图像嵌入之间接近度的指标。请注意，结果范围大致从
    0.0 到 0.4，较低的数字表示不匹配，较高的数字表示匹配。
- en: '![](../Images/f1b9d42a709ece24a7b8abd6c8388c8d.png)![](../Images/3c9f01943cee7a4fc551d2be6dc952d9.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1b9d42a709ece24a7b8abd6c8388c8d.png)![](../Images/3c9f01943cee7a4fc551d2be6dc952d9.png)'
- en: The images are shown across the top horizontally, and the corresponding captions
    are listed down the left vertically. You can see that the results from OpenCLIP
    on the right tend to have higher matching scores on the diagonal blocks (brighter
    yellows) and lower non-matching scores (darker blues) as compared to the results
    from CLIP on the left. This means that if you search for images with text using
    the systems, you will get better results using OpenCLIP than CLIP.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图像横向展示在顶部，相应的说明纵向列在左侧。你可以看到，右侧的 OpenCLIP 结果在对角线块（较亮的黄色）上的匹配分数更高，而在非匹配分数（较暗的蓝色）上则更低，相比之下，左侧的
    CLIP 结果较低。这意味着，如果你使用这些系统搜索带有文本的图像，使用 OpenCLIP 会得到比 CLIP 更好的结果。
- en: Exploring Images from the LOC using OpenCLIP
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 OpenCLIP 探索 LOC 图像
- en: To explore the Flickr photostream from the Library of Congress, I created a
    [Google Colab](https://colab.research.google.com/github/robgon-art/open-clip/blob/main/Index_LOC_Photos_with_OpenCLIP.ipynb)
    that downloaded all 40K images and ran them through the OpenCLIP image encoder
    to perform text searches.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索国会图书馆的Flickr照片流，我创建了一个[Google Colab](https://colab.research.google.com/github/robgon-art/open-clip/blob/main/Index_LOC_Photos_with_OpenCLIP.ipynb)，下载了所有4万张图像，并通过OpenCLIP图像编码器进行文本搜索。
- en: '![](../Images/272d98998a4b89f81ebc3ff243928d9d.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/272d98998a4b89f81ebc3ff243928d9d.png)'
- en: '**Components to use OpenCLIP for Searching for LOC Images**, Diagram by Author,
    Images from the LOC'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用OpenCLIP搜索LOC图像的组件**，图表由作者绘制，图像来自LOC'
- en: I started by using the [Flickr API](https://www.flickr.com/services/api/) to
    download all 40 thousand photos to a local folder. Next, I sent the pictures into
    the OpenCLIP image encoder to create image embeddings. The encoders were previously
    trained by LAION using 2 billion images with captions. I then entered a text query,
    like “gone fishing” and sent it through the text encoder to create an embedding.
    I calculated the cosine similarity between the text embedding and the 40K image
    embeddings to find the best matches. As the last step, I sorted the array and
    showed the top six images for the search.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我开始使用[Flickr API](https://www.flickr.com/services/api/)将所有4万张照片下载到本地文件夹。接下来，我将图片发送到OpenCLIP图像编码器以创建图像嵌入。编码器之前由LAION使用20亿张带有说明的图像进行训练。然后，我输入了文本查询，例如“gone
    fishing”，并通过文本编码器创建了一个嵌入。我计算了文本嵌入与40K图像嵌入之间的余弦相似度，以找到最佳匹配。最后，我对数组进行排序并显示了搜索的前六张图像。
- en: Here are some example searches using OpenCLIP with results from the dataset.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用OpenCLIP和数据集结果的一些示例搜索。
- en: “boat vacation”
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “boat vacation”
- en: I entered a search phrase and hit the “run” button to see the top six hits with
    scores.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我输入了一个搜索短语并点击了“运行”按钮，以查看得分前六的结果。
- en: '![](../Images/81040c84a8a7693dcd990231752a2eb8.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81040c84a8a7693dcd990231752a2eb8.png)'
- en: '**OpenCLIP Search Results for “boat vacation”,** Screenshot by Author, Images
    from the LOC'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**“boat vacation”的OpenCLIP搜索结果**，截图由作者提供，图像来自LOC'
- en: Sure enough, it found some boats with people on vacation, presumably. Notice
    the relatively low scores of the matches (0.259 to 0.281). The reason these scores
    are low is probably due to the use of the somewhat abstract word “vacation.” Next,
    I tried something more concrete.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 果然，它找到了几艘假期中的船只。注意这些匹配的得分相对较低（0.259到0.281）。这些得分较低的原因可能是由于使用了有些抽象的词语“vacation”。接下来，我尝试了一些更具体的内容。
- en: “building an airplane engine”
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “建造飞机发动机”
- en: Here I tried using a more specific search phrase.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我尝试使用了更具体的搜索短语。
- en: '![](../Images/60570c12ade28e70060d2c7b16912254.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60570c12ade28e70060d2c7b16912254.png)'
- en: '**OpenCLIP Search Results for “building an airplane engine”,** Screenshot by
    Author, Images from the LOC'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**“building an airplane engine”的OpenCLIP搜索结果**，截图由作者提供，图像来自LOC'
- en: OK, this search's scores were much higher (0.302 to 0.326). The top hit shows
    a nice picture of people building an airplane engine. Next up, I tried something
    fun.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这次搜索的得分要高得多（0.302到0.326）。最佳结果展示了一张人们建造飞机发动机的漂亮照片。接下来，我尝试了一些有趣的东西。
- en: “mini golf”
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “mini golf”
- en: There were a lot of images with an Americana vibe in the dataset, so I checked
    to see if there were any images of mini golf courses.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有很多具有美国风情的图像，所以我检查了一下是否有迷你高尔夫球场的图像。
- en: '![](../Images/0f5a0a5be120c70111402a93bd700198.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f5a0a5be120c70111402a93bd700198.png)'
- en: Sure enough, the answer was, “Yes!” Notice the higher scores for these images
    (0.378 to 0.395). The top hit is a classic windmill hole, with the words “MINI
    GOLF” written twice on the blades. I’ll revisit this search after I describe a
    cool new way to refine image searches.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 果然，答案是“是的！”注意这些图像的得分较高（0.378到0.395）。最佳结果是一个经典的风车洞，风车叶片上写着“MINI GOLF”两次。在描述一种酷炫的新方法来优化图像搜索之后，我会重新审视这个搜索。
- en: Embedding Arithmetic
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入算术
- en: In October 2022, Meta AI published a paper with an intriguing title, “Embedding
    Arithmetic of Multimodal Queries for Image Retrieval,” where multimodal means
    other forms of media, like text [8].
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年10月，Meta AI发布了一篇标题引人注目的论文，《多模态查询的嵌入算术用于图像检索》，其中“多模态”指的是其他形式的媒体，如文本[8]。
- en: 'Here’s the concept: if you find an image in a dataset and want to find another
    image that retains some of the qualities but changes others, you can build a query
    by combining image and text elements. Running the new query should find what you
    are looking for, if such an image exists in the dataset.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 概念是这样的：如果你在数据集中找到一张图像，并且想要找到另一张保留部分特质但改变其他特质的图像，你可以通过结合图像和文本元素来构建查询。运行新的查询应该能够找到你所寻找的内容，前提是数据集中存在这样的图像。
- en: Here's a visual example from the paper.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是论文中的一个视觉示例。
- en: '![](../Images/74301a78f5026155bf2062826f7b2fe8.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74301a78f5026155bf2062826f7b2fe8.png)'
- en: '**Using Embedding Arithmetic for Image Retrieval**, Image by Meta AI'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用嵌入算术进行图像检索**，图片由 Meta AI 提供'
- en: It starts with an image of a cat at the top, which is encoded into an embedding
    E(I). Then the words “cat” and “dog” are sent through the text encoder to get
    E(W1) and E(W2), respectively. The delta between E(W2) and E(W1) is added to the
    embedding of the cat image, to land where a similar image of a dog should be.
    Running a retrieval from the image database shows a close match, as seen at the
    bottom. The match is evaluated by swapping the word “dog” for “cat” in the original
    caption to get “A dog is sitting on the grass.” The text embedding from the transformed
    caption is compared to the embedding of the dog image to see if there is a match
    or not.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这开始于顶部的猫图像，该图像被编码成嵌入 E(I)。然后，“cat”和“dog”这两个词通过文本编码器分别得到 E(W1) 和 E(W2)。E(W2)
    和 E(W1) 之间的差值被加到猫图像的嵌入中，从而找到类似的狗图像所在的位置。从图像数据库中检索得到的结果显示了一个接近的匹配，如底部所示。匹配通过将原始标题中的“dog”替换为“cat”来评估，得到“狗坐在草地上。”
    将转换后的标题文本嵌入与狗图像的嵌入进行比较，以查看是否匹配。
- en: The paper discusses how a scaling factor, λ, can be used to adjust the amount
    of modification made from the text prompts. Here is the equation to produce the
    new embedding, *x*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 论文讨论了如何使用缩放因子 λ 来调整从文本提示中进行的修改量。这里是产生新嵌入 *x* 的方程。
- en: '![](../Images/472c153d6251751f0ddc870d30fc6985.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/472c153d6251751f0ddc870d30fc6985.png)'
- en: The paper discusses how a scaling factor between 1.0 and 1.5 works well for
    many searches.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 论文讨论了如何在 1.0 到 1.5 之间的缩放因子对于许多搜索效果良好。
- en: I implemented this form of embedding math in my Colab. Here are some results,
    starting with a modified search based on the mini golf windmill image.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我在我的 Colab 中实现了这种形式的嵌入数学。这里是一些结果，从基于迷你高尔夫风车图像的修改搜索开始。
- en: mini golf windmill image + 1.5 (“Yosemite Sam” - “windmill”)
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迷你高尔夫风车图像 + 1.5（“约塞米蒂·萨姆” - “风车”）
- en: For this search, I started with the portrait photo of the mini golf windmill
    and added in the phrase “Yosemite Sam” and dropped “windmill.” I used a scaling
    factor of 1.5.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这次搜索，我从迷你高尔夫风车的肖像照片开始，添加了“约塞米蒂·萨姆”这一短语，并删除了“风车”。我使用了 1.5 的缩放因子。
- en: '![](../Images/e6a9b10fd7e648a8570161f12962d58d.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6a9b10fd7e648a8570161f12962d58d.png)'
- en: '**mini golf windmill image + 1.5 (“Yosemite Sam” - “windmill”)**, Screenshot
    by Author, Images by LOC'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**迷你高尔夫风车图像 + 1.5（“约塞米蒂·萨姆” - “风车”）**，作者截图，图片来自 LOC'
- en: The original image is at the top left, and the best match is next to it, with
    a high score of 0.407\. The top hit is very similar to the starting image, except
    that it shows Yosemite Sam instead of the windmill. Up next are some images of
    roadside eateries.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像在左上角，最佳匹配在其旁边，得分很高，为 0.407。顶部的结果与起始图像非常相似，只是显示了约塞米蒂·萨姆而不是风车。接下来是一些路边餐馆的图像。
- en: donut shop image + 1.2 (“hamburger” - ”donut”)
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 甜甜圈店图像 + 1.2（“汉堡包” - “甜甜圈”）
- en: For the next test, I started by searching for “donut shop” and chose an image
    of an interesting place called the Donut Hole. Next, I used “hamburger” as the
    positive prompt and “donut” as the negative prompt. I used a scaling factor of
    1.2\. Here are the results.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下一个测试，我从搜索“甜甜圈店”开始，选择了一张名为甜甜圈洞的有趣地方的图像。接下来，我使用了“汉堡包”作为正向提示，“甜甜圈”作为负向提示。我使用了
    1.2 的缩放因子。以下是结果。
- en: '![](../Images/50697bf2f704c0a69d64243853591a9d.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50697bf2f704c0a69d64243853591a9d.png)'
- en: '**donut shop image + 1.2 (“hamburger” - ”donut”)**, Screenshot by Author, Images
    by LOC'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**甜甜圈店图像 + 1.2（“汉堡包” - “甜甜圈”）**，作者截图，图片来自 LOC'
- en: Wow, it found a classic Macdonalds restaurant where the golden arches line up
    nicely with the giant donuts in the starting image. Notice the very high score
    of 0.533 for the top match. My last search involves some old photos of famous
    people.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，它找到了一家经典的麦当劳餐厅，其中金色拱门与起始图像中的巨型甜甜圈完美对齐。注意到顶部匹配的非常高的分数 0.533。我的最后一次搜索涉及一些名人的旧照片。
- en: Abraham Lincoln image + 1.1 (“Oscar Wilde” - ”Abraham Lincoln”)
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚伯拉罕·林肯图像 + 1.1（“奥斯卡·王尔德” - “亚伯拉罕·林肯”）
- en: For my final test, I first searched for “Abraham Lincoln” and chose a well-known
    image of him sitting in a chair. I used the system to see if the dataset had a
    similar image of Oscar Wild. I used a scaling factor of 1.1 for this test.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我的最终测试，我首先搜索了“亚伯拉罕·林肯”，并选择了一张他坐在椅子上的著名图像。我使用系统检查数据集中是否有类似的奥斯卡·王尔德图像。我为这次测试使用了
    1.1 的缩放因子。
- en: '![](../Images/d2b2460f2686c2ae7b3b16bd2141d59a.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2b2460f2686c2ae7b3b16bd2141d59a.png)'
- en: '**Abraham Lincoln image + 1.1 (“Oscar Wilde” - ”Abraham Lincoln”)**, Screenshot
    by Author, Images by LOC'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**亚伯拉罕·林肯图像 + 1.1（“奥斯卡·王尔德” - “亚伯拉罕·林肯”）**，作者截图，LOC 提供的图像'
- en: Sure enough, it found a sepia-toned image of Oscar wild sitting in a wooden
    chair. The score for the top match is the highest I’ve seen at 0.675, even though
    the pose differs. The high score may be because the strong correlation between
    the name and face of a famous person overrides the other factors. Next, I’ll show
    how I used OpenCLIP to generate image captions.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 果然，它找到了一个色调为褐色的奥斯卡·王尔德坐在木椅上的图像。尽管姿势不同，但最高匹配的得分是我见过的最高，为 0.675。高分可能是因为著名人物的名字和面孔之间的强相关性超越了其他因素。接下来，我将展示我如何使用
    OpenCLIP 生成图像字幕。
- en: Using CoCa and OpenCLIP to Create Captions
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CoCa 和 OpenCLIP 创建字幕
- en: 'In their paper from 2022, “CoCa: Contrastive Captioners are Image-Text Foundation
    Models,” [5] the authors show how a model similar to OpenAI’s CLIP can be trained
    to generate captions from images automatically.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '在他们 2022 年的论文《CoCa: 对比字幕生成模型是图像-文本基础模型》中，[5] 作者展示了如何训练类似于 OpenAI 的 CLIP 的模型来自动从图像生成字幕。'
- en: In this work we present Contrastive Captioners (CoCa), a new image-text foundation
    model family that subsumes existing vision pretraining paradigms with natural
    language supervision. Pretrained on image-text pairs from various data sources
    in a single stage, CoCa efficiently combines contrastive and captioning objectives
    in an encoder-decoder model. — Jiahui Yu and Zirui Wang from Google
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这项工作中，我们展示了对比字幕生成模型（CoCa），这是一个新的图像-文本基础模型系列，它包含了现有的视觉预训练范式，并结合了自然语言监督。CoCa
    在来自各种数据源的图像-文本对上进行了单阶段预训练，能够高效地结合对比和字幕生成目标于一个编码器-解码器模型中。 — 来自 Google 的 Jiahui
    Yu 和 Zirui Wang
- en: The independent researcher Phil Wang, known as lucidrains, adapted the CoCa
    model to work with OpenCLIP. The results are excellent.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 独立研究员 Phil Wang，外号 lucidrains，将 CoCa 模型适配到了 OpenCLIP 上。结果非常出色。
- en: For example, here are six images with the original captions from the LOC.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这里有六张图像，附有来自 LOC 的原始字幕。
- en: '![](../Images/79e89654c0a1ff81f5603847e142651d.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79e89654c0a1ff81f5603847e142651d.png)'
- en: 'And here are the images with captions generated by CoCa/OpenCLIP:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是由 CoCa/OpenCLIP 生成的带字幕的图像：
- en: '![](../Images/36d12e92524b15fbbb879f644b9a634f.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36d12e92524b15fbbb879f644b9a634f.png)'
- en: Although the captions are missing certain details like naming specific people
    (who) and places (where), the system does a great job of describing the visual
    content of the images (what). You can check this out in my Colab [here](https://colab.research.google.com/github/robgon-art/open-clip/blob/main/Create_Captions_with_OpenCLIP.ipynb).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管字幕缺少一些细节，如具体的个人（谁）和地点（哪里），但系统在描述图像的视觉内容（什么）方面做得非常出色。你可以在我的 Colab [这里](https://colab.research.google.com/github/robgon-art/open-clip/blob/main/Create_Captions_with_OpenCLIP.ipynb)
    查看这一点。
- en: Societal Impact
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 社会影响
- en: Large AI models trained on data from the Internet can exhibit cultural biases
    and, left unmitigated, could cause societal harm. The authors of the OpenCLIP
    model express their concerns in their paper [2].
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在互联网上训练的大型 AI 模型可能会表现出文化偏见，如果不加以缓解，可能会对社会造成伤害。OpenCLIP 模型的作者在他们的论文 [2] 中表达了他们的担忧。
- en: Our work deals with studying function and properties of pre-trained models on
    large scales. Releasing these models to public can have both positive and negative
    implications, like with any research artefact that possesses generic functionality.
    … There is potential for abuse of technology based on large-scale pre-trained
    generalist models, and it is the task of democratic institutions to work out rules
    for sensitive applications that might involve those. Open release of models gives
    the broad research community also opportunity to study safety related aspects
    of such models, such to preventively design measures that make such abuse by malicious
    parties less probable, in a common transparent effort. — Mehdi Cherti, et al.,
    from LAION
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们的工作涉及研究大规模预训练模型的功能和属性。将这些模型公开可能有正面和负面的影响，就像任何具有通用功能的研究工具一样。…… 基于大规模预训练通用模型的技术可能被滥用，而民主机构的任务是制定涉及这些模型的敏感应用的规则。模型的开放发布也为广泛的研究社区提供了研究这些模型安全性方面的机会，以预防恶意方滥用技术，从而进行共同的透明努力。
    — Mehdi Cherti 等人，来自 LAION
- en: Their policy of transparency allows other researchers to assess and mitigate
    the use of their models.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的透明政策允许其他研究人员评估和缓解他们模型的使用。
- en: Discussion and Next Steps
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论与下一步
- en: The OpenCLIP system seems to work well for searching for images in a large dataset.
    And the new techniques for embedding math provide expert tools to help people
    find the perfect shot. The CoCa/OpenCLIP model does a fine job creating descriptive
    captions for images.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCLIP 系统在大规模数据集中搜索图像时表现良好。新的数学嵌入技术提供了专家工具，帮助人们找到完美的镜头。CoCa/OpenCLIP 模型在为图像创建描述性标题方面做得很好。
- en: An area for improvement would be to see if these systems could be fine-tuned
    to find or create captions for personal photographs. Unlike OpenAI, LAION released
    the training code for their models. Although their code is designed for large-scale
    training, it would be helpful if it could be adapted to fine-tune models with,
    say, only ten images of your uncle Bob.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一个改进的领域是看看这些系统是否可以微调以找到或创建个人照片的标题。与 OpenAI 不同，LAION 发布了他们模型的训练代码。尽管他们的代码是为大规模训练设计的，但如果能够调整为仅使用例如你叔叔
    Bob 的十张照片来微调模型，那将会很有帮助。
- en: Source Code
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 源代码
- en: The source code for this project is available on [GitHub](https://github.com/robgon-art/open-clip).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的源代码可在 [GitHub](https://github.com/robgon-art/open-clip) 上获得。
- en: '![](../Images/4e4cb7250c1bec4531e196f1ff265f49.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e4cb7250c1bec4531e196f1ff265f49.png)'
- en: '**Creative Commons Attribution Sharealike**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识共享 署名-相同方式共享**'
- en: Acknowledgments
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: I want to thank Jennifer Lim for her help with this project.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我想感谢 Jennifer Lim 在这个项目上的帮助。
- en: References
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] A. Radford et al., CLIP, [Learning Transferable Visual Models From Natural
    Language Supervision](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf)
    (2021)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] A. Radford 等人, CLIP, [从自然语言监督中学习可转移的视觉模型](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf)
    (2021)'
- en: '[2] M. Cherti et al., OpenCLIP, [Reproducible scaling laws for contrastive
    language-image learning](https://arxiv.org/pdf/2212.07143.pdf) (2022)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] M. Cherti 等人, OpenCLIP, [对比语言-图像学习的可重复性缩放法则](https://arxiv.org/pdf/2212.07143.pdf)
    (2022)'
- en: '[3] G. Couairon et al., [Embedding Arithmetic of Multimodal Queries for Image
    Retrieval](https://arxiv.org/pdf/2112.03162.pdf) (2022)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] G. Couairon 等人, [多模态查询的嵌入算术用于图像检索](https://arxiv.org/pdf/2112.03162.pdf)
    (2022)'
- en: '[4] S. Wang and P. Kanwar, [BFloat16: The secret to high performance on Cloud
    TPUs](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus)
    (2019)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] S. Wang 和 P. Kanwar, [BFloat16: 云 TPUs 高性能的秘密](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus)
    (2019)'
- en: '[5] J. Yu, [CoCa: Contrastive Captioners are Image-Text Foundation Models](https://arxiv.org/pdf/2205.01917.pdf)
    (2022)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] J. Yu, [CoCa: 对比式标题生成器是图像-文本基础模型](https://arxiv.org/pdf/2205.01917.pdf)
    (2022)'
