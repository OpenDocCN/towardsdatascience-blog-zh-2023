- en: 'Ensemble of Classifiers: Voting Classifier'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/ensemble-of-classifiers-voting-classifier-ef7f6a5b7795](https://towardsdatascience.com/ensemble-of-classifiers-voting-classifier-ef7f6a5b7795)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Combine many different models for better Prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://saptashwa.medium.com/?source=post_page-----ef7f6a5b7795--------------------------------)[![Saptashwa
    Bhattacharyya](../Images/b01238113a1f6b91cb6fb0fbfa50303a.png)](https://saptashwa.medium.com/?source=post_page-----ef7f6a5b7795--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ef7f6a5b7795--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ef7f6a5b7795--------------------------------)
    [Saptashwa Bhattacharyya](https://saptashwa.medium.com/?source=post_page-----ef7f6a5b7795--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ef7f6a5b7795--------------------------------)
    ·7 min read·Aug 17, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96aa1b7e6415940eeaa32ba8a06bad00.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Decision Boundaries with Voting Classifier (Image by Author: Codes in References)'
  prefs: []
  type: TYPE_NORMAL
- en: The word Ensemble in the context of ML refers to a collection of a finite number
    of ML models (may include ANN), trained for the same task. Usually, the models
    are trained independently and then their predictions are combined.
  prefs: []
  type: TYPE_NORMAL
- en: When the predictions from different models differ, it is sometimes more useful
    to use the ensemble for classification than any individual classifier. Here, we
    would like to combine different classifiers and create an ensemble and then use
    the ensemble for the prediction task. What will be discussed in this post?
  prefs: []
  type: TYPE_NORMAL
- en: Use Sklearn’s VotingClassifier to build an ensemble.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Hard and Soft Voting in VotingClassifier?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check individual model performance with VotingClassifier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, use GridSearchCV + VotingClassifier to find the best model parameters
    for individual models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin!
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Preparation:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To see an example of VotingClassifier in action, I’m using the [Heart Failure
    Prediction dataset](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)
    (available under open database licensing). Here the task is the binary classification
    for predicting whether a patient with specific attributes may have heart disease
    or not. The dataset has 10 attributes including their age, sex, resting blood
    pressure etc., for data collected over 900 patients. Let’s check some distributions
    for different parameters. We check the ‘ClassLabel’ counts (1 represents heart
    disease, 0 represents healthy), i.e. healthy and ill population as a function
    of Sex.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7185779fad5fb131d2b0f07734b630af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 1: ClassLabel distribution as a function of the sex of the participants.
    (Image by Author; Codes in References).'
  prefs: []
  type: TYPE_NORMAL
- en: In general, we see proportionately more Males are ill compared to Females. We
    can also check individual features such as Cholesterol and Resting BP distribution
    as below and we see that both the Cholesterol and Resting BP are higher for ill
    patients, especially for females.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/21752fae1a7b2642006128093b6e3b9d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 2: Cholesterol and Resting BP distributions for Males and Females for
    two different classes: Healthy and Ill (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: For numerical features, one can also use Boxplots to see distributions where
    the box represents the range of the central 50% of the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e99db4bff049650e0662767e7bcf0661.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 3: Some Numerical Parameter Distribution via Boxplot (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: We see some issues with the dataset here; There are quite a few data points
    with zero Cholesterols and RestingBP. Even though it is indeed possible to have
    [really low cholesterol](https://www.health.harvard.edu/blog/ldl-cholesterol-how-low-can-you-safely-go-2020012018638),
    to have zero for many data points (patients), is it a little too accidental? Anyway,
    as our main objective is to implement VotingClassifier so we leave those data
    points as they are and proceed to the next stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Train-Test and Classification:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After some preliminary analysis, we prepare the dataset for training and testing
    different classifiers and compare their performances. First, we split the data
    into train and test sets, separately standardize the numerical columns (to prevent
    data leakage) and initialize 3 different classifiers- [Support Vector Machines](/understanding-support-vector-machine-part-1-lagrange-multipliers-5c24a52ffc5e),
    [Logistic Regression](/logit-of-logistic-regression-understanding-the-fundamentals-f384152a33d1)
    and AdaBoost, which itself is an Ensemble learning method specifically falling
    under the Boosting method (the other one being known as ‘Bagging’). Next is to
    train these classifiers using the training set and check predictions for the test
    set. We check 3 different performance metrics-precision, recall and F1 score.
    Below is the code block:'
  prefs: []
  type: TYPE_NORMAL
- en: Check the performances of individual classifiers on the Heart Disease dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below are the scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that on average the F1 scores are around ~89%. One can also check the
    feature importance for the AdaBoost classifier and expectedly Cholesterol is one
    of the main important features. Below is the plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/993d1c247cfa2568eb54b3af171bdef7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 4: Feature Importance Plot: Considering only AdaBoost classifier, Cholesterol
    is of the highest importance (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The confusion matrix for the AdaBoost classifier separately looks as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62c3bc6b635dbfd4613f60ac34459387.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 5: Confusion matrix for AdaBoost classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: 'VotingClassifier to Form an Ensemble:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: VotingClassifier combines different machine learning classifiers and uses a
    voting rule (‘soft’ or ‘hard’) to predict the class labels.
  prefs: []
  type: TYPE_NORMAL
- en: It balances out the individual weaknesses of models when their performance is
    almost the same; here we will combine individual classifiers used before to build
    an ensemble. But what is soft and hard voting?
  prefs: []
  type: TYPE_NORMAL
- en: '***Majority (‘Hard’) Voting:*** The predicted class label for a particular
    sample is the class label that represents the majority (mode) of the class labels
    predicted by each individual classifier. Let’s look at the table below for an
    example of a classification task with 3 classes'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, SVC and AdaBoost both predict Class3 as the output label and in the Hard
    Voting scenario this will be selected.
  prefs: []
  type: TYPE_NORMAL
- en: '***Soft Voting:*** Soft voting returns the class label as argmax of the sum
    of predicted probabilities; It’s also possible to assign a weight array for the
    classifiers involved in creating the ensemble. Below is an example code block'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Once again take a look at the table below for an example classification task
    with weights [1, 1, 1] i.e. all the classifiers are equally weighted
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here Class3 will be chosen under the ‘Soft’ voting scheme. For more on VotingClassifier
    you can also check the [official Sklearn guide](https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier).
    Let’s now build a VotingClassifier and use this ensemble to train and perform
    predictions on the test set using both ‘Soft’ and ‘Hard’ schemes.
  prefs: []
  type: TYPE_NORMAL
- en: Using VotingClassifier indeed we could see improvement in the performance metrics
    compared to the individual estimators. Similarly, we can also plot the Confusion
    Matrix to further verify the findings.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/71bc3a0ddf31de760905ed952f35af96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 6: Confusion Matrix obtained using VotingClassifier with ‘Hard’ voting
    scheme. (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: 'VotingClassifier and GridSearchCV:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It’s also possible to tune the parameters (‘hyperparameters’) of individual
    estimators within VotingClassifier using GridSearchCV. GridSearch exhaustively
    considers all parameter combinations and uses Cross-Validation to find the best
    hyperparameters. We have gone through examples of building an analysis pipeline
    with an estimator like Support Vector Machine, Principal Component Analysis and
    GridSearchCV before [here](https://medium.com/towards-data-science/visualizing-support-vector-machine-decision-boundary-69e7591dacea).
    We show that it is fairly simple to obtain the best parameters of individual estimators
    within VotingClassifier and below is an example code block:'
  prefs: []
  type: TYPE_NORMAL
- en: For parameter space (in line 18), we scour over SVM ‘C’ and ‘gamma’ parameters
    (with a radial basis function kernel) which we have discussed in detail [before](/visualizing-support-vector-machine-decision-boundary-69e7591dacea).
    For the logistic regression, we check over several values of the inverse of the
    regularization strength parameter; finally, for the AdaBoost classifier which
    uses decision trees as base estimators, we check over a few possible values of
    the number of estimators where boosting is terminated.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusions:** In this post through the VotingClassifier, we have discussed
    one of the important concepts in machine learning, ensemble techniques. We build
    an ensemble using individual estimators whose predictions are then combined either
    via ‘Hard’ or ‘Soft’ voting scheme for an example classification task. It is generally
    argued that combining several different estimators may or may not beat the performance
    of the best estimator but the ensemble helps to reduce the risk of selection of
    a poorly performing classifier [1].'
  prefs: []
  type: TYPE_NORMAL
- en: Given the task at your hand, you can certainly consider the VotingClassifier
    ensemble technique than choosing the best classifier out of many.
  prefs: []
  type: TYPE_NORMAL
- en: '*Stay strong! Cheers!!*'
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] *“Ensemble based systems in decision making”*; R. Polikar, IEEExplore,
    DOI: [10.1109/MCAS.2006.1688199](https://doi.org/10.1109/MCAS.2006.1688199).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Sklearn VotingClassifier: [User Guide](https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] *“Ensemble-based Classifiers”*; L. Rokach, Artificial Intelligence Review
    33, 1–39 (2010). [Springer](https://link.springer.com/article/10.1007/s10462-009-9124-7).'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Codes and Notebooks: [GitHub](https://github.com/suvoooo/Machine_Learning/tree/master/VotingClassifier).'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Heart Failure Prediction Data; Available Under Open Database License (ODbl);
    [Link](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction).'
  prefs: []
  type: TYPE_NORMAL
