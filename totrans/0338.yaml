- en: AudioGPT — A Glimpse into the Future of Creating Music
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/audiogpt-a-glimpse-into-the-future-of-creating-music-9e8e0c65069e](https://towardsdatascience.com/audiogpt-a-glimpse-into-the-future-of-creating-music-9e8e0c65069e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Research paper analyzed and explained
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@maxhilsdorf?source=post_page-----9e8e0c65069e--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page-----9e8e0c65069e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9e8e0c65069e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9e8e0c65069e--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page-----9e8e0c65069e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9e8e0c65069e--------------------------------)
    ·10 min read·May 2, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c61267fd6c17c0ecfb42939253873df.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Pixabay](https://www.pexels.com/de-de/foto/low-angle-view-von-beleuchtungsgeraten-im-regal-257904/).
  prefs: []
  type: TYPE_NORMAL
- en: 'With [MusicLM](https://google-research.github.io/seanet/musiclm/examples/)
    released in January 2023, it became clear to musicians and data scientists alike:
    AI is here to disrupt the way we make music. Only a few days ago, the next big
    audio AI model was published. In this post, we will explore why I think **this
    model may be the foundation of a major technological disruption in music production**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This post addresses the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is AudioGPT?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are its capabilities & limitations?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What does this mean for the future of making music?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is AudioGPT?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AudioGPT is a research project by a group of Chinese and U.S. researchers published
    in April 2023 **[1]**. But what does it actually do and how does it relate to
    GPT models? Well, let’s ask it!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e7bbdf3f190c060c8b60a5e39a454a8a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'AudioGPT answers the question: “What is AudioGPT?”. Image by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: AudioGPT is a dialogue assistant
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you can see from the screenshot, AudioGPT can be used in a chatbot interface
    similar to something like ChatGPT. In fact, it works just like ChatGPT for most
    conversational applications. One unique feature of AudioGPT is that, aside from
    text, the chatbot can handle speech as an input by transcribing the audio to text,
    first. Hence, this is a real dialogue assistant that you can talk to or write
    to, depending on your needs.
  prefs: []
  type: TYPE_NORMAL
- en: AudioGPT can perform various audio tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AudioGPT’s dialogue capabilities are only a support function. Its true purpose
    is to provide a unified experience for solving a multitude of tasks in the realm
    of audio analysis and generation. Here’s a selection of some of the tasks it can
    handle:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Audio Captioning**: describe the content of an audio signal using text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Source Separation:** split an audio signal into different events (voices,
    noises, …)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image-to-audio:** generate audio that fits the content of an image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Score-to-audio:** generate a singing voice given text, notes, and note durations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Many more tasks** that we will cover later!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cool thing is that AudioGPT, in contrast to ChatGPT, is able to receive
    and send audio files. For example, when I asked AudioGPT to generate a specific
    sound for me, it created the sounds, exported it to a wav file, and sent me the
    location of the exported file.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62aaf175cdf853d3ea775325c0371fce.png)'
  prefs: []
  type: TYPE_IMG
- en: AudioGPT is asked to generate “the sound of a lion roaring with racing cars
    in the background”. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Before we can understand the significance of this technology for the future
    of music creation, let me first show you how AudioGPT actually works and what
    its strengths and limitations are.
  prefs: []
  type: TYPE_NORMAL
- en: How was AudioGPT implemented?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While AudioGPT might seem like a typical AI chatbot to the user, there is actually
    a lot more going on under the hood. In fact, the chatbot AI (ChatGPT) is only
    used as a translator between the user request and other AI models. Such approaches
    already exist for other domains like image ([TaskMatrix](https://github.com/microsoft/TaskMatrix))
    or text ([LangChain](https://github.com/hwchase17/langchain)). Let us look at
    the illustration of AudioGPTs workflow provided by the authors in their paper
    **[1]**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1e2355352532863074311bf69920068d.png)'
  prefs: []
  type: TYPE_IMG
- en: Internal workflow of AudioGPT. Image from Huang, Li, Yang, Shit et a. (2023)
    [1].
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the workflow is split into four different steps. Let us go through
    all of them, briefly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Modality transformation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AudioGPT is built to handle speech and text input. Therefore, the first step
    is to check if the user is texting or speaking to the system. If the input is
    speech, it is transcribed and transformed to text by a speech recognition system
    similar to Alexa or Siri. To the user, this conversion step should feel seamless.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Task analysis'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With this text input, ChatGPT takes over and tries to understand the user''s
    request. Whether you say “Generate a wav file of a thunder sound effect” or “Give
    me a thunder sound”: ChatGPT is great at understanding different formulations
    of the same problem and mapping the request to a specific task — In this case,
    text-to-audio sound generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Model assignment'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once ChatGPT has understood the request, it selects an appropriate AI model
    from a set of 17 models currently included in the system. Each of these 17 handles
    one specific task in a very specific way. It is therefore crucial that ChatGPT
    understands the request, finds the correct model, and presents the user request
    in a way that the model can process it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Response generation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once an appropriate model is found and run, it will generate an output. This
    output can have all sorts of different modalities (audio, text, image, video).
    That is where ChatGPT comes in, again. It collects the model output and presents
    it to the user in a way that they can understand and interpret it. For example,
    a text output may be directly passed on to the user, whereas an audio output will
    be exported and the user will receive a file path linking to the exported audio.
  prefs: []
  type: TYPE_NORMAL
- en: Memory & Chat History
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Solving one task is great. However, what really stands out about this chatbot
    approach, is that AudioGPT can look at the entire conversation history. This means
    that you can always reference requests, questions, or outputs from earlier in
    the conversation and ask AudioGPT to do something with them. In some sense, it
    feels just like ChatGPT, but with the capability to receive and send audio files.
  prefs: []
  type: TYPE_NORMAL
- en: What can AudioGPT do?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, I want to give you some examples from the paper of what AudioGPT
    can do. This is, of course, not a comprehensive list, but just some interesting
    highlights.
  prefs: []
  type: TYPE_NORMAL
- en: Image to audio generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/4272971c06721ab8d1fd67d09a2c5f73.png)'
  prefs: []
  type: TYPE_IMG
- en: Image to Audio Generation Example from Huang, Li, Yang, Shit et a. (2023) [1].
  prefs: []
  type: TYPE_NORMAL
- en: In this example, AudioGPT is asked to generate audio that fits the image of
    a cat. The system then responds with the location of an exported audio file and
    a visualization of the audio waveform. We cannot listen to this example from the
    paper, but the response is most likely a cat sound like a hiss or a purr. Under
    the hood, the image is first captioned and the image caption is then synthesized
    to an audio signal. This could be really helpful for musicians to create samples
    for their music by simply inputting an image of what they are looking for.
  prefs: []
  type: TYPE_NORMAL
- en: Singing voice generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/401d50f22b98b666f04bd771b834ca36.png)'
  prefs: []
  type: TYPE_IMG
- en: Singing Voice Generation Example from Huang, Li, Yang, Shit et a. (2023) [1].
  prefs: []
  type: TYPE_NORMAL
- en: Now, this one is relevant for musicians! If we give the model a text alongside
    information on notes and note durations, it synthesizes a singing voice and sends
    the audio back to you. Under the hood, state-of-the-art speech synthesis models
    (DiffSinger **[2]**, VISinger **[3])** are applied. It is easy to imagine how
    this kind of technology could be implemented directly in a DAW, for example, to
    create singing samples for hip-hop beats or even background vocals.
  prefs: []
  type: TYPE_NORMAL
- en: Sound extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/aa8d99f0e2ebf1bfffe5c27bbfb46275.png)'
  prefs: []
  type: TYPE_IMG
- en: Sound Extraction Example from Huang, Li, Yang, Shit et a. (2023) [1].
  prefs: []
  type: TYPE_NORMAL
- en: Based on a text prompt, AudioGPT identifies where a specific event occurs in
    an audio signal and cuts off the irrelevant part of the audio for the user. Cutting
    samples or sounds using nothing but verbal prompts could prove incredibly useful
    for musicians. We may not be far away from telling our DAW to “retrieve the most
    emotional part of this sample and cut it down to one bar” without doing any of
    the mechanical work ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: Source separation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/118bac72ba99073edaac2653d83bc4fd.png)'
  prefs: []
  type: TYPE_IMG
- en: Source Separation Example from Huang, Li, Yang, Shit et a. (2023) [1].
  prefs: []
  type: TYPE_NORMAL
- en: Here, AudioGPT is asked to separate two speakers in an audio signal and return
    both extracted speakers, separately. Currently, there is no music source separation
    tool included in this system. However, it is easy to imagine that we could soon
    extract specific instruments or instrument groups from an audio signal right inside
    our DAW through a chatbot interface.
  prefs: []
  type: TYPE_NORMAL
- en: What are AudioGPT’s limitations?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While these examples showcase how AudioGPT could build the basis for breakthrough
    technologies in the future, it still has lots of limitations.
  prefs: []
  type: TYPE_NORMAL
- en: It was not built for music
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the context of this post, it is important to note that AudioGPT is not a
    great tool for music analysis or generation, yet. The only real dedicated music
    model is the singing voice synthesis model. Some other models are able to produce
    musical sounds, but they are primarily built for speech and sounds, not for music.
  prefs: []
  type: TYPE_NORMAL
- en: However, this is not a limitation of the system per se. Instead, this is primarily
    because the developers did not decide to include more dedicated music AI models
    in this tool. With the current state of AudioGPT as a foundation, it is feasible
    to include more and more audio models in this system or to build a separate, music-specific
    system.
  prefs: []
  type: TYPE_NORMAL
- en: It is a work-in-progress
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From my limited experience using AudioGPT, I can already tell that the task
    assignment process does not work as well as I would like. Oftentimes, my request
    is misunderstood and the wrong model is called, which leads to a completely useless
    output. It seems that there still needs to be some optimization done to make this
    system more and more competent at understanding the user's needs.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the state of audio AI as a whole is still far behind the state
    of text AI, for example. Most of the 17 models included in AudioGPT work somewhat
    well but have obvious limitations. Hence, even if AudioGPT’s task assignment worked
    perfectly, the systems would still be limited by the capabilities of the underlying
    models.
  prefs: []
  type: TYPE_NORMAL
- en: What does this mean for music?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI composition/production assistants
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For now, AudioGPT does not really affect the work and lives of musicians in
    any way. However, in my estimation, this will change in the near future. Here
    are the two steps that would need to be taken to make these kinds of systems truly
    transformative in this field:'
  prefs: []
  type: TYPE_NORMAL
- en: Expand AudioGPT with music models (source separation, tagging, denoising, audio
    effects, etc) or build a separate MusicGPT model focused on these specific tasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Develop plugins that allow musicians to access AudioGPT (or MusicGPT) through
    a chat interface within their DAW.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clearly, this is no easy task. Especially the second step could be a huge hurdle
    to overcome. However, implementing such a chatbot in a DAW could be an immense
    competitive advantage for companies like Apple (Logic), Ableton, or Image-Line
    (FL Studio). A fleshed-out, domain-specific, and well-integrated version of AudiGPT
    could increase the efficiency, creative freedom, and fun of creating music dramatically.
  prefs: []
  type: TYPE_NORMAL
- en: If the system had all the audio and MIDI events in your current project stored
    in its memory, you could always move, edit, combine, delete, or enhance these
    events using simple speech or text commands. Further, some things in the creative
    process are easy to verbalize but hard to execute, manually. Say you want your
    song to sound more “laid back” and “chill”. Do you turn hundreds of knobs in your
    instrument selection, mixer, effect chain, and master settings? Or do you simply
    tell your AI assistant to turn the knobs for you? I am dreaming of a DAW plugin
    that allows us to generate sounds, apply effects, mix/master our tracks, separate
    instruments, analyze and tag our music, and much more, all within an intuitive
    chatbot interface. Such a system could undoubtedly make us all more productive
    and creative in our music-making workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancement, not replacement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over the last few months, generation models like MusicLM have provoked existential
    fear in many artists. While it is undisputed that AI will dramatically disrupt
    the music industry, I think AudioGPT is a great example of how these technologies
    can augment, not replace our work as musicians. A chatbot interface can be really
    helpful for creating, editing, and rearranging sounds, but it does not act as
    an autonomous agent. The creativity, intention, and emotion of the composer or
    producer are what give the final product its value and meaning.
  prefs: []
  type: TYPE_NORMAL
- en: In the long run, human-made music will always be of higher value to humans than
    AI-made music. This is simply because music is inherently social and a form of
    communication between people. To most of us, it does not matter if the music is
    “hand-made” in the sense of using only acoustical instruments. Instead, what really
    matters is that, whatever technologies were used in the creative process, it was
    a human who utilized these tools in a controlled manner and as a means of personal
    creative expression. In this light, expanding the functionalities of AudioGPT
    and integrating it into music-making tools allows us to profit from speed- or
    creativity-enhancing technology while maintaining our value and purpose as composers.
  prefs: []
  type: TYPE_NORMAL
- en: How can I use AudioGPT?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a programmer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a programmer, you can simply clone the [AudioGPT GitHub repository](https://github.com/AIGC-Audio/AudioGPT),
    install all of the models used, enter your OpenAI API key, and get started. This
    will allow to you use ALL of the features presented in the paper.
  prefs: []
  type: TYPE_NORMAL
- en: As a non-techie
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are not a programmer, you can still use AudioGPT, although to a limited
    extent, in this [HuggingFace web app](https://huggingface.co/spaces/AIGC-Audio/AudioGPT).
    To use the system, you will need an OpenAI API key. [Here](https://www.howtogeek.com/885918/how-to-get-an-openai-api-key/)
    is a tutorial on how to get it. Depending on OpenAI’s current terms of use, you
    may need to enter your credit card information to be able to use the token. This
    key is needed because AudioGPT uses ChatGPT in the background. Using ChatGPT is
    not very expensive (0.002$ cents per ~700 words as of April, 23\. See [documentation](https://openai.com/pricing)).
    Still, if you decide to use this key for AudioGPT, I recommend monitoring the
    costs invoked by the system in your OpenAI account.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, this HuggingFace web app has not been working all that well,
    for me. When I upload files, there is usually an error. The audio outputs are
    sometimes completely wrong, although my request seems to have been understood…
    If you already have an OpenAI API key, you should definitely try it out. If not,
    I am not sure if this web app is worth going through the effort of creating the
    account and key.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**[1] Huang, Li, Yang, Shi et al. (2023)**. AudioGPT: Understanding and Generating
    Speech, Music, Sound, and Talking Head. [arXiv:2304.12995v1](https://arxiv.org/abs/2304.12995)'
  prefs: []
  type: TYPE_NORMAL
- en: '**[2] Liu et al. (2021)**. DiffSinger: Singing Voice Synthesis via Shallow
    Diffusion Mechanism. [arXiv:2105.02446](https://arxiv.org/abs/2105.02446)'
  prefs: []
  type: TYPE_NORMAL
- en: '**[3] Zhang et al. (2021)**. VISinger: Variational Inference with Adversarial
    Learning for End-to-End Singing Voice Synthesis. [arXiv:2110.08813](https://arxiv.org/abs/2110.08813)'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you liked this post, check out my article on MusicLM, Google’s breakthrough
    music generation model: [**MusicLM: Has Google Solved AI Music Generation?**](https://medium.com/towards-data-science/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c)'
  prefs: []
  type: TYPE_NORMAL
