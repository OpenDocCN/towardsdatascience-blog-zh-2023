- en: Prompt Engineering Could Be the Hottest Programming Language of 2024 — Here’s
    Why
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/prompt-engineering-could-be-the-hottest-programming-language-of-2024-heres-why-a9ccf4ba8d49](https://towardsdatascience.com/prompt-engineering-could-be-the-hottest-programming-language-of-2024-heres-why-a9ccf4ba8d49)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Large Language Models are the next generation of Operating Systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://nabil-alouani.medium.com/?source=post_page-----a9ccf4ba8d49--------------------------------)[![Nabil
    Alouani](../Images/8ceea018e9b15413d318bfb710bb0011.png)](https://nabil-alouani.medium.com/?source=post_page-----a9ccf4ba8d49--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a9ccf4ba8d49--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a9ccf4ba8d49--------------------------------)
    [Nabil Alouani](https://nabil-alouani.medium.com/?source=post_page-----a9ccf4ba8d49--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a9ccf4ba8d49--------------------------------)
    ·16 min read·Dec 21, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8914fb851cbf6bd3ef466345eacf0eb1.png)'
  prefs: []
  type: TYPE_IMG
- en: Unless indicated otherwise, all the images are generated by the author using
    Midjourney, DALL-E, and Canva.
  prefs: []
  type: TYPE_NORMAL
- en: “I don’t think it’s accurate to think of Large Language Models as chatbots or
    some kind of word generators,” [Andrej Karpathy](https://karpathy.ai/), one of
    OpenAI’s founding members, said. “It’s a lot more correct to think about [them]
    as **the kernel process of an emerging Operating System**.”
  prefs: []
  type: TYPE_NORMAL
- en: Wait, but what the heck does that mean?
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) will gradually become the interface between you
    and computer systems.
  prefs: []
  type: TYPE_NORMAL
- en: Right now, you’re holding a device that has some computing power inside it,
    but you can’t directly access that power. Your interaction is mediated by an Operating
    System (such as Windows, Mac OS, and Android,), which transforms a collection
    of chips and circuits into a user-friendly interface.
  prefs: []
  type: TYPE_NORMAL
- en: Your Operating System (OS) allows you to perform a wide range of activities
    (like reading some bald dude’s article) through a variety of apps running on top
    of it. Each app has its own User Interface (UI) and its own set of tasks it can
    accomplish. You jump from one app to another, one UI to another, depending on
    what you need to do.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/702445b34dcdd1c17bfd8c0e23d264c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Tomorrow, you’ll have a single UI to do everything from writing an annual business
    report to building a new app from scratch. The said UI will be a chatbox or a
    “context window” inside which you can submit instructions in natural language
    — and that’s where Prompt Engineering comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Engineering is a fancy way to say “Write better and better instructions
    for your AI model until it does exactly what you want.” Except, it’s not merely
    wordplay; it’s the blueprint for the future of programming.
  prefs: []
  type: TYPE_NORMAL
- en: Programming as a (cheap) commodity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Programming refers to a technological process for telling a computer which
    tasks to perform in order to solve problems,” Coursera [wrote on their website](https://www.coursera.org/articles/what-is-programming).
    “You can think of programming as a collaboration between humans and computers,
    in which humans create instructions for a computer to follow (code) in a language
    computers can understand.”
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, programming turns computing power into a commodity: a resource
    you can use to accomplish your goals. Prompt Engineering is a tool that turns
    programming itself into a commodity. You submit an instruction to an LLM and it
    writes the code for you.'
  prefs: []
  type: TYPE_NORMAL
- en: Say you want to analyze a tiny data set for a work project. Normally, you’d
    start by gathering hundreds of CSV files scattered across your company’s cloud.
    You then double-click on Jupyter Notebook and type in a few lines of Python to
    compile your inputs into a single data frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'From there, you sprinkle some Data Science magic, run a dozen iterations, and
    congratulations: you got yourself a collection of elegant tables, fancy graphs,
    and data-driven predictions. Your last step is to compress your six weeks of work
    into 42 beautiful slides displayed on yet another app called Microsoft PowerPoint.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/234a6b82ed7ee30a81061e3d3691a201.png)'
  prefs: []
  type: TYPE_IMG
- en: Your typical Data Scientist on a random Tuesday.
  prefs: []
  type: TYPE_NORMAL
- en: You just combined off-the-shelf apps with code you wrote yourself to build a
    program that runs a specific data analysis. But what if instead, all you had to
    do was to write a few instructions in plain English?
  prefs: []
  type: TYPE_NORMAL
- en: “Hey AI buddy,” you’d say. “Here’s a messy dataset about the deliveries our
    company made across Paris in the last five years. Please clean up the mess and
    run a clustering algorithm. Display a heat map and zoom on high-density spots.
    Throw in a two-year projection and use the results to optimize the daily itinerary
    of our delivery fleet. When you’re done with math, generate a report with clear
    graphs and succinct comments. And take your time! I’ll be gone for at least six
    hours.”
  prefs: []
  type: TYPE_NORMAL
- en: Every time you write such a prompt, you’re effectively programming an app that
    solves a specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: It’s almost as Coursera described it — you collaborate with a computer to achieve
    a goal. The only difference is instead of code, you’ll use plain English. Okay,
    the prompt may be a few yards shy of the finish line, but in principle, that’s
    what your interactions with future LLMs will look like.
  prefs: []
  type: TYPE_NORMAL
- en: For a more concrete illustration, consider the famous demo below, where GPT-4
    turned a hand-drawn sketch into functional HTML code.
  prefs: []
  type: TYPE_NORMAL
- en: Prompts are all you need
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’ll be a while before we can replace data scientists, web developers, and
    software engineers with a bunch of clever prompts. In the meantime, we’ll augment
    them with AI assistants that make their work more efficient — and each one of
    these AI assistants will be programmed in plain English.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of using a bundle of complementary apps like Google Workspace, Jupyter
    Notebook, and Microsoft PowerPoint, you’ll put together an assistant called “StatSniffer:
    your personal Data Science expert.”'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/34aa9d60a397b4baeaa48b9d8c8ab62e.png)'
  prefs: []
  type: TYPE_IMG
- en: Much like the current ChatGPT PLUS, your personalized StatSniffer will be an
    LLM hooked to a series of tools that give it extra capabilities like browsing
    files, running code, and generating graphs. You can also infuse StatSniffer with
    top-performing methodologies by giving it access to research papers, case studies,
    and academic textbooks.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI is already experimenting with personalized AI assistants through the
    GPT store where you can build assistants called GPTs. The current GPTs are clunky,
    however. For instance, they’re vulnerable to simple jailbreaks that make them
    reveal their “core instructions.” GPTs also tend to revert to their default mode
    (GPT-4) after a few exchanges with the users.
  prefs: []
  type: TYPE_NORMAL
- en: This is not a surprise because the tech is still in its infancy. As AI research
    advances, and open-source models get better, the ecosystem of AI assistants will
    evolve to cover more capabilities with increased reliability. Speaking of which,
    there’s a long way to go.
  prefs: []
  type: TYPE_NORMAL
- en: Problems like planning and multi-step reasoning remain unsolved in part because
    [LLMs are still lagging behind humans](https://nabilalouani.substack.com/p/chatgpt-hype-is-proof-nobody-really)
    (and even cats) when it comes to understanding physical reality.
  prefs: []
  type: TYPE_NORMAL
- en: This doesn’t mean people will hold their breath for fully autonomous assistants
    versed in Quantum Gravity to leverage existing AI models. Even the so-called ‘dumb’
    LLMs can crank up your efficiency by a factor of two. Here’s an excerpt from a
    [McKinsey study on](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/unleashing-developer-productivity-with-generative-ai#/)
    how developers are using LLMs to accelerate their work.
  prefs: []
  type: TYPE_NORMAL
- en: “Our latest empirical research finds generative AI–based tools delivering impressive
    **speed gains** for many common developer tasks. **Documenting code** functionality
    for maintainability (which considers how easily code can be improved) can be **completed
    in** **half the time**, **writing new code in nearly half the time**, and **optimizing
    existing code** (called code refactoring) in **nearly two-thirds the time**.”
    [Emphasis by the author].
  prefs: []
  type: TYPE_NORMAL
- en: “For developers to effectively use the technology to augment their daily work,
    they will likely need a combination of training and coaching,” the McKinsey research
    team explained. “Initial training should include best practices and hands-on exercises
    for inputting natural-language prompts into the tools, often called prompt engineering.”
  prefs: []
  type: TYPE_NORMAL
- en: And this is not only true for software-related tasks. The same pattern extends
    to a wider range of corporate activities as well. For instance, researchers at
    [Harvard Business School](https://www.hbs.edu/faculty/Pages/item.aspx?num=64700)
    (HBS) conducted a [study](https://www.hbs.edu/faculty/Pages/item.aspx?num=64700)
    to assess the impact of equipping employees from Boston Consulting Group (BCG)
    with Generative AI tools.
  prefs: []
  type: TYPE_NORMAL
- en: “For each one of a set of 18 realistic consulting tasks within the frontier
    of AI capabilities, consultants using AI were significantly more productive (they
    completed 12.2% more tasks on average, and completed tasks 25.1% more quickly),
    and produced significantly higher quality results (more than 40% higher quality
    compared to a control group),” HBS researchers wrote.
  prefs: []
  type: TYPE_NORMAL
- en: This is a 2-minute summary of the study by [Rajiv Shah](https://www.linkedin.com/in/rajistics/overlay/about-this-profile/),
    a Machine Learning Engineer and YouTuber.
  prefs: []
  type: TYPE_NORMAL
- en: 'These studies feed into the new cliché of “AI won’t replace you, but someone
    who uses AI will.” Perhaps a fancier formulation could be: “AI won’t replace you,
    but a Prompt Engineer will.”'
  prefs: []
  type: TYPE_NORMAL
- en: The bigger the “frontier of AI” gets (meaning tasks that AI models can perform
    with high accuracy), the more problems we’ll be able to solve using prompts. This
    brings us to a widespread fallacy that suggests more capable AI models require
    less Prompt Engineering skills.
  prefs: []
  type: TYPE_NORMAL
- en: “Is Prompt Engineering dead?” No, it’s State of the Art
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way to see the relationship between a Large Language Model and Prompt Engineering
    is to picture the former as a multiverse and the latter as a pointer — yes, like
    a laser pointer.
  prefs: []
  type: TYPE_NORMAL
- en: When you ask an LLM a question, it considers a multiverse of relevant documents.
    Inside each document, there’s a cluster of possible answers, and each possible
    answer is a chain of probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Your prompt points toward the universe that’s most likely to contain the desired
    answer — and from there, your model tries to navigate its way to that desired
    answer, one word at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Each time the model predicts a token, it eliminates hundreds of alternative
    paths and continues to narrow down its options until all that’s left is a series
    of words that constitute “the destination.”
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/262ca56dd5ecd01082e543569bd55375.png)'
  prefs: []
  type: TYPE_IMG
- en: Your LLM navigating multiverses of possible answers (more like “navigating mushy
    planets” here, but you get the idea).
  prefs: []
  type: TYPE_NORMAL
- en: This destination is never the same, however. Even if you use the exact same
    prompt, you almost never reach the exact same address. Instead, you’ll land somewhere
    in the “neighborhood” of the most relevant answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a [more technical description](https://fchollet.substack.com/p/how-i-think-about-llm-prompt-engineering)
    by François Chollet, a software engineer and AI researcher who currently works
    at Google:'
  prefs: []
  type: TYPE_NORMAL
- en: If a LLM is like a database of millions of vector programs, then a prompt is
    like a search query in that database […] this “program database” is continuous
    and interpolative — it’s not a discrete set of programs.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This means that a slightly different prompt, like “Lyrically rephrase this text
    in the style of x” would still have pointed to a very similar location in program
    space, resulting in a program that would behave pretty closely but not quite identically.
    […]
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Prompt engineering is the process of searching through program space to find
    the program that empirically seems to perform best on your target task.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As Chollet pointed out, your prompt’s goal is to call the right program for
    the task you want to accomplish. The reasoning trap many people fall for is to
    believe future LLMs should be able to predict which program we want them to run,
    even when we give them vague assignments.
  prefs: []
  type: TYPE_NORMAL
- en: Except, as with humans, even if you hire the most technically capable engineer,
    she won’t be able to read your mind. You have to explain exactly what you want,
    otherwise, you’re wasting time and energy.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say you instructed your formidable engineer to build a product, but you
    didn’t like the result. You can either change the engineer or change your instructions.
    Since you know your engineer is highly skilled, common sense suggests you opt
    for the second option.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if your highly capable Language Model doesn’t produce the answer
    you want, you don’t throw it away. You don’t sit around hoping the next model
    will be able to read your mind. The most reasonable and cost-effective approach
    is to improve your prompt.
  prefs: []
  type: TYPE_NORMAL
- en: That’s what a research team at Microsoft did with GPT-4\. Instead of fine-tuning
    the model for a specific use case, [they used prompt engineering techniques to
    improve its performance](https://arxiv.org/abs/2311.16452).
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4’s score improved by up to 9% across nine different medical benchmarks.
    As a result, the model’s accuracy surpassed 90%, outperforming models that were
    specifically fine-tuned for medical applications.
  prefs: []
  type: TYPE_NORMAL
- en: Riley Goodside is arguably the first Prompt Engineer.
  prefs: []
  type: TYPE_NORMAL
- en: Note that fine-tuning requires extra resources like hiring experts to produce
    high-quality training data and some computing resources to retrain the model.
    Sure, fine-tuning requires a fraction of computing power compared to pre-training,
    but it’s still an extra cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plus, you need to invest the same resources every time you want to fine-tune
    your model for a new domain of expertise. In contrast, Microsoft produced a prompting
    technique that improves performance across different fields: electrical engineering,
    machine learning, philosophy, accounting, law, nursing, and clinical psychology.'
  prefs: []
  type: TYPE_NORMAL
- en: Another [example that highlights the power of prompt engineering comes from
    Anthropic](https://www.anthropic.com/index/claude-2-1-prompting). Their team enhanced
    the performance of their Claude 2.1 model by 98% in an information-retrieval evaluation,
    a feat they achieved by adding one sentence to their prompt. Just one sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating LLMs is like playing with an alien tool. The only way to figure
    out what it can do is to press its buttons in different ways. When a new version
    of this alien tool comes out, you’d expect it to have more capabilities, but also
    more buttons.
  prefs: []
  type: TYPE_NORMAL
- en: The naive approach is to think “more capable models require less prompting.”
    In reality, the more capable your model, the more features you can unlock using
    the right prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Time to shine, human agent!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the long term, we’ll use LLMs as Operating Systems to wield computing power
    and solve all kinds of problems. In the midterm, we’ll program AI agents to perform
    tasks we used to write code for. In both stages, we’ll use English as the main
    programming language.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, but what happens now?
  prefs: []
  type: TYPE_NORMAL
- en: Until AI agents catch up, it’s your time to shine. Think of yourself as a tech
    artisan who combines AI models, code, and traditional apps to tackle complex challenges.
    You’ll play Lego so to speak, and thanks to the open-source community in particular,
    you’ll have an endless supply of pieces you can combine to create innovative projects.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are special pieces of this hypothetical Lego because you’ll often end up
    with one at the center of your creations. This brings us to the two complementary
    flavors of Prompt Engineering.
  prefs: []
  type: TYPE_NORMAL
- en: 'See, [Prompt Engineering has two meanings](https://simonwillison.net/2023/Feb/21/in-defense-of-prompt-engineering/):
    (1) Write high-quality natural language instructions for LLMs and (2) Write code
    on top of LLMs to improve their output using conditional prompting and other techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: The second definition includes the first because, even if you wrap code around
    an LLM, you still use English to interact with it. Here’s how both of these definitions
    intertwine with LLM usage.
  prefs: []
  type: TYPE_NORMAL
- en: '**LLMs as stand-alone programs:** Hereyou write high-quality prompts in natural
    language to unlock the best possible outputs. Examples involve idea generation,
    document summary, and writing code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LLMs as part of a program you design:** Here you write software (in Python,
    Java, C++, or other programming languages) wrapped around LLMs to achieve specific
    tasks. Examples involve sentiment analysis of social media comments, specialized
    chatbots, and autonomous agents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let’s explore what Prompt Engineering looks like for each use-case.
  prefs: []
  type: TYPE_NORMAL
- en: 1️⃣ Prompt Engineering for “LLMs as stand-alone programs”
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most common use case for LLMs is to interact with them through web interfaces
    like ChatGPT’s, and Bard’s.
  prefs: []
  type: TYPE_NORMAL
- en: Based on your specific needs, you can build a personal library of prompts. You
    want your prompts to be templated and easy to update. This way you don’t have
    to rewrite your prompts from scratch or search for them in the chat history every
    time you want to run one of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below are three varied examples of flexible prompts that may inspire you:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 2️⃣ **Prompt Engineering for “LLMs as part of a program you design”**
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this case, you’ll use your LLMs as functions you can call to process, analyze,
    and generate natural language.
  prefs: []
  type: TYPE_NORMAL
- en: For example, your code can call an LLM to analyze sentiment in a series of comments
    related to a given product. After processing these comments, you can use another
    LLM-reliant function to generate a response based on the previous results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at how you can embed LLMs in your code. There are three primary
    ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Connect through an API provided by another company;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a local server within your company’s network;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install an open-source LLM directly on your computer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s a basic example of how you can use an LLM inside a program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How to improve your Prompt Engineering skills
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The short answer is an elegant quote I stole from Stephen King. “You must do
    two things above all others,” [he said](https://www.goodreads.com/quotes/312517-if-you-want-to-be-a-writer-you-must-do).
    “Read a lot, and write a lot.”
  prefs: []
  type: TYPE_NORMAL
- en: Much like writing, Prompt Engineering appears easy until you sit down and hit
    the keyboard. Since we use natural language to write prompts, we approach it with
    a false sense of simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: Pardon the repetition here, but AI models can’t read your mind (yet).
  prefs: []
  type: TYPE_NORMAL
- en: If you want high-quality responses, you have to learn to express your intentions
    as clearly as possible. Keep up with the literature to learn new techniques and
    put in as many reps as you can to integrate them.
  prefs: []
  type: TYPE_NORMAL
- en: You may get bored of typing random instructions into a flickering context window.
    The antidote is to find difficult problems to solve. How can you randomize parts
    of your output? How to dynamically change your prompt’s content? Can you program
    an assistant that resists jailbreaking attempts?
  prefs: []
  type: TYPE_NORMAL
- en: Find problems hard to solve and Prompt Engineering will go from “a skill you
    have to learn” to a daily dose of “blissful (but sometimes frustrating) intellectual
    stimulation.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Further, there are two other topics that deserve your attention: Machine Learning
    in general and Deep Learning in particular. You want to explore both their strengths
    and shortcomings, because once you understand how the tech behind generative AI
    works, you’ll develop intuitions on why your model behaves in certain ways.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few resources that can help you get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Making Friends of Machine Learning**](https://www.youtube.com/watch?v=lKXv19eRLZg&list=PLRKtJ4IpxJpDxl0NTvNYQWKCYzHNuy2xG&ab_channel=CassieKozyrkov)by
    Cassie Kozyrkov. (Series of YouTube videos)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Intro to Large Language Models**](https://www.youtube.com/watch?v=zjkBMFhNj_g&ab_channel=AndrejKarpathy)
    by Andrej Karpathy. (YouTube video)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Prompt Engineering for Developers**](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)
    by Isa Fulford and Andrew Ng (Free online course)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**How to Write Expert Prompts for LLMs**](/how-to-write-expert-prompts-for-chatgpt-gpt-4-and-other-language-models-23133dc85550?sk=49c4528973c462c1c6d3d28cc29855fe)
    by this bald dude (Full 16,000-word guide that includes 25+ prompting techniques,
    examples, and commentary).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](/how-to-write-expert-prompts-for-chatgpt-gpt-4-and-other-language-models-23133dc85550?source=post_page-----a9ccf4ba8d49--------------------------------)
    [## How to Write Expert Prompts for ChatGPT (GPT-4) and Other Language Models'
  prefs: []
  type: TYPE_NORMAL
- en: A beginner-friendly guide to prompt engineering with LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-write-expert-prompts-for-chatgpt-gpt-4-and-other-language-models-23133dc85550?source=post_page-----a9ccf4ba8d49--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The TL;DR version
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs are progressively becoming our interface with computing power. Prompt Engineering
    is the art of writing instructions that bring the best possible results from your
    LLMs (and other AI models).
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use natural language to interact with future Operating Systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before that, we’ll use natural language to program AI assistants.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt Engineering is mostly done in natural language but it doesn’t mean AI
    models can read your mind. You still have to express your instructions clearly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you can write high-quality prompts, you can write code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Those who write better instructions will build better programs and get better
    results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt Engineering is a key to unlocking the latent capabilities of Language
    Models (and AI models in general).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt Engineering is an empirical science. You can learn from other people’s
    experiences, but you learn the most from your own.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep in touch?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can [**subscribe to get email notifications**](https://nabil-alouani.medium.com/subscribe)when
    I publish new posts.
  prefs: []
  type: TYPE_NORMAL
- en: I’m also active on [**Linkedin**](https://www.linkedin.com/in/nabil-alouani/)
    and[**X**](https://twitter.com/Nabil_Alouani_)and reply to every single message.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Prompt Engineering inquiries, write me at: **nabil@nabilalouani.com.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'And in case you’re wondering: this article is 100% human-generated.'
  prefs: []
  type: TYPE_NORMAL
