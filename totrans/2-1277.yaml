- en: How we think about Data Pipelines is changing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯¹æ•°æ®ç®¡é“çš„æ€è€ƒæ­£åœ¨æ”¹å˜
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/how-we-think-about-data-pipelines-is-changing-51c3bf6f34dc](https://towardsdatascience.com/how-we-think-about-data-pipelines-is-changing-51c3bf6f34dc)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/how-we-think-about-data-pipelines-is-changing-51c3bf6f34dc](https://towardsdatascience.com/how-we-think-about-data-pipelines-is-changing-51c3bf6f34dc)
- en: '![](../Images/7d79fb389159e7b29999cbc1ea81691a.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d79fb389159e7b29999cbc1ea81691a.png)'
- en: Photo by [Ali Kazal](https://unsplash.com/@lureofadventure?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/a-mountain-range-with-trees-in-the-foreground-and-a-field-in-the-foreground-walahB6h_sU?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ç…§ç‰‡ç”±[Ali Kazal](https://unsplash.com/@lureofadventure?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)æä¾›ï¼Œæ¥è‡ª[Unsplash](https://unsplash.com/photos/a-mountain-range-with-trees-in-the-foreground-and-a-field-in-the-foreground-walahB6h_sU?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: The goal is to reliably and efficiently release data into production
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ˜¯å¯é ä¸”é«˜æ•ˆåœ°å°†æ•°æ®å‘å¸ƒåˆ°ç”Ÿäº§ç¯å¢ƒ
- en: '[](https://medium.com/@hugolu87?source=post_page-----51c3bf6f34dc--------------------------------)[![Hugo
    Lu](../Images/045de11463bb16ea70a816ba89118a9e.png)](https://medium.com/@hugolu87?source=post_page-----51c3bf6f34dc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----51c3bf6f34dc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----51c3bf6f34dc--------------------------------)
    [Hugo Lu](https://medium.com/@hugolu87?source=post_page-----51c3bf6f34dc--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@hugolu87?source=post_page-----51c3bf6f34dc--------------------------------)[![Hugo
    Lu](../Images/045de11463bb16ea70a816ba89118a9e.png)](https://medium.com/@hugolu87?source=post_page-----51c3bf6f34dc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----51c3bf6f34dc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----51c3bf6f34dc--------------------------------)
    [Hugo Lu](https://medium.com/@hugolu87?source=post_page-----51c3bf6f34dc--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----51c3bf6f34dc--------------------------------)
    Â·6 min readÂ·Nov 8, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨åœ¨[Towards Data Science](https://towardsdatascience.com/?source=post_page-----51c3bf6f34dc--------------------------------)
    Â·6åˆ†é’Ÿé˜…è¯»Â·2023å¹´11æœˆ8æ—¥
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Data Pipelines are series of tasks organised in a [directed acyclic graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph)
    or â€œDAGâ€. Historically, these are run on open-source workflow orchestration packages
    like [Airflow](https://airflow.apache.org/) or [Prefect](https://www.prefect.io/?gclid=Cj0KCQjwqP2pBhDMARIsAJQ0CzoV5DrzqjyDqDJonPcBPT5lE2ih47H2LMSKBst2jh6mR6h3azCcRnwaAhOJEALw_wcB),
    and require i[nfrastructure](https://www.bhavaniravi.com/apache-airflow/deploying-airflow-on-kubernetes)
    managed by data engineers or platform teams. These data pipelines typically run
    on a [schedule](https://airflow.apache.org/docs/apache-airflow/1.10.1/scheduler.html),
    and allow data engineers to update data in locations such as data warehouses or
    data lakes.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®ç®¡é“æ˜¯ä¸€ç³»åˆ—ç»„ç»‡æˆ[æœ‰å‘æ— ç¯å›¾](https://en.wikipedia.org/wiki/Directed_acyclic_graph)æˆ–â€œDAGâ€çš„ä»»åŠ¡ã€‚ä»å†å²ä¸Šçœ‹ï¼Œè¿™äº›ä»»åŠ¡æ˜¯åœ¨å¼€æºå·¥ä½œæµç¼–æ’è½¯ä»¶ï¼ˆå¦‚[Airflow](https://airflow.apache.org/)æˆ–[Prefect](https://www.prefect.io/?gclid=Cj0KCQjwqP2pBhDMARIsAJQ0CzoV5DrzqjyDqDJonPcBPT5lE2ih47H2LMSKBst2jh6mR6h3azCcRnwaAhOJEALw_wcB)ï¼‰ä¸Šè¿è¡Œçš„ï¼Œå¹¶ä¸”éœ€è¦ç”±æ•°æ®å·¥ç¨‹å¸ˆæˆ–å¹³å°å›¢é˜Ÿç®¡ç†çš„[åŸºç¡€è®¾æ–½](https://www.bhavaniravi.com/apache-airflow/deploying-airflow-on-kubernetes)ã€‚è¿™äº›æ•°æ®ç®¡é“é€šå¸¸æŒ‰[æ—¶é—´è¡¨](https://airflow.apache.org/docs/apache-airflow/1.10.1/scheduler.html)è¿è¡Œï¼Œå¹¶å…è®¸æ•°æ®å·¥ç¨‹å¸ˆæ›´æ–°æ•°æ®ä»“åº“æˆ–æ•°æ®æ¹–ç­‰ä½ç½®çš„æ•°æ®ã€‚
- en: This is now changing. There is a [great shift in mentality](/what-data-engineers-can-learn-from-software-engineers-and-vice-versa-643cade3ef23)
    happening. As the data engineering industry matures, mindsets are shifting from
    a â€œmove data to serve the business at all costsâ€ mindset to â€œreliability and efficiencyâ€
    / â€œsoftware engineeringâ€ mindset.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ­£åœ¨å‘ç”Ÿå˜åŒ–ã€‚[å¿ƒæ€å‘ç”Ÿäº†å·¨å¤§çš„è½¬å˜](/what-data-engineers-can-learn-from-software-engineers-and-vice-versa-643cade3ef23)ã€‚éšç€æ•°æ®å·¥ç¨‹è¡Œä¸šçš„æˆç†Ÿï¼Œå¿ƒæ€æ­£åœ¨ä»â€œä¸æƒœä¸€åˆ‡ä»£ä»·ç§»åŠ¨æ•°æ®ä»¥æœåŠ¡ä¸šåŠ¡â€çš„å¿ƒæ€è½¬å˜ä¸ºâ€œå¯é æ€§å’Œæ•ˆç‡â€/â€œè½¯ä»¶å·¥ç¨‹â€çš„å¿ƒæ€ã€‚
- en: Continuous Data Integration and Delivery
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æŒç»­æ•°æ®é›†æˆå’Œäº¤ä»˜
- en: Iâ€™ve written before about how [Data Teams ship *data*](https://medium.com/orchestras-data-release-pipeline-blog/a-new-paradigm-for-data-continuous-data-integration-and-delivery-miniseries-part-5-a3338b3ffd03)
    whereas software teams ship *code.*
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¹‹å‰å†™è¿‡å…³äº[æ•°æ®å›¢é˜Ÿå‘å¸ƒ*æ•°æ®*](https://medium.com/orchestras-data-release-pipeline-blog/a-new-paradigm-for-data-continuous-data-integration-and-delivery-miniseries-part-5-a3338b3ffd03)ï¼Œè€Œè½¯ä»¶å›¢é˜Ÿå‘å¸ƒ*ä»£ç *ã€‚
- en: This is a process called â€œContinuous Data Integration and Deliveryâ€, and is
    the process of reliably and efficiently releasing data into production. There
    are subtle differences with the definition of â€œ[CI/CD](https://aws.amazon.com/solutions/app-development/ci-cd/#:~:text=An%20integral%20part%20of%20development,with%20collaborative%20and%20automated%20processes.)â€
    as used in Software Engineer, illustrated below.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸ºâ€œæŒç»­æ•°æ®é›†æˆå’Œäº¤ä»˜â€ï¼Œæ˜¯å¯é é«˜æ•ˆåœ°å°†æ•°æ®å‘å¸ƒåˆ°ç”Ÿäº§ç¯å¢ƒçš„è¿‡ç¨‹ã€‚ä¸è½¯ä»¶å·¥ç¨‹ä¸­â€œ[CI/CD](https://aws.amazon.com/solutions/app-development/ci-cd/#:~:text=An%20integral%20part%20of%20development,with%20collaborative%20and%20automated%20processes.)â€çš„å®šä¹‰å­˜åœ¨ç»†å¾®å·®å¼‚ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/5a9b3efdc44da8184627cefa728133c0.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a9b3efdc44da8184627cefa728133c0.png)'
- en: Image the authorâ€™s
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…çš„å›¾ç‰‡
- en: In software engineering, Continuous Delivery is non-trivial because of the importance
    of having a [near exact replica](https://www.techtarget.com/searchsoftwarequality/definition/staging-environment#:~:text=A%20staging%20environment%20(stage)%20is,like%20environment%20before%20application%20deployment.)
    for code to operate in a staging environment.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è½¯ä»¶å·¥ç¨‹ä¸­ï¼ŒæŒç»­äº¤ä»˜å¹¶ä¸å®¹æ˜“ï¼Œå› ä¸ºåœ¨æš‚å­˜ç¯å¢ƒä¸­éœ€è¦æœ‰ä¸€ä¸ª[å‡ ä¹å®Œå…¨ç›¸åŒçš„å‰¯æœ¬](https://www.techtarget.com/searchsoftwarequality/definition/staging-environment#:~:text=A%20staging%20environment%20(stage)%20is,like%20environment%20before%20application%20deployment.)æ¥è¿è¡Œä»£ç ã€‚
- en: Within Data Engineering, this is not necessary because the good we ship is *data*.
    If there is a table of data, and we *know* that as long as a few conditions are
    satisfied, the data *is* of a sufficient quality to be used, then that is sufficient
    for it to be â€œreleasedâ€ into production, so to speak.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•°æ®å·¥ç¨‹ä¸­ï¼Œè¿™æ˜¯ä¸å¿…è¦çš„ï¼Œå› ä¸ºæˆ‘ä»¬å‘å¸ƒçš„æ˜¯*æ•°æ®*ã€‚å¦‚æœæœ‰ä¸€å¼ æ•°æ®è¡¨ï¼Œå¹¶ä¸”æˆ‘ä»¬*çŸ¥é“*åªè¦æ»¡è¶³ä¸€äº›æ¡ä»¶ï¼Œæ•°æ®å°±*è¶³å¤Ÿä¼˜è´¨*å¯ä»¥ä½¿ç”¨ï¼Œé‚£ä¹ˆè¿™å°±è¶³å¤Ÿè®©å®ƒè¢«â€œå‘å¸ƒâ€åˆ°ç”Ÿäº§ç¯å¢ƒä¸­ã€‚
- en: The process of releasing data into production â€” the analog for Continuous Delivery
    â€” is very simple, as it simply relates to copying or [cloning](https://docs.snowflake.com/en/sql-reference/sql/create-clone)
    a dataset.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ•°æ®å‘å¸ƒåˆ°ç”Ÿäº§ç¯å¢ƒçš„è¿‡ç¨‹â€”â€”æŒç»­äº¤ä»˜çš„ç±»æ¯”â€”â€”éå¸¸ç®€å•ï¼Œå› ä¸ºå®ƒåªæ¶‰åŠå¤åˆ¶æˆ–[å…‹éš†](https://docs.snowflake.com/en/sql-reference/sql/create-clone)æ•°æ®é›†ã€‚
- en: Furthermore, a *key pillar* of data engineering is *reacting* to new data *as
    it arrives* or checking to see if new data exists. There is no analog for this
    in software engineering â€” software applications do not need to poll APIs for the
    existence of new code, whereas data applications do.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæ•°æ®å·¥ç¨‹çš„ä¸€ä¸ª*å…³é”®æ”¯æŸ±*æ˜¯å¯¹æ–°æ•°æ®çš„*åŠæ—¶ååº”*ï¼Œæˆ–è€…æ£€æŸ¥æ–°æ•°æ®æ˜¯å¦å­˜åœ¨ã€‚åœ¨è½¯ä»¶å·¥ç¨‹ä¸­æ²¡æœ‰ç±»ä¼¼çš„æƒ…å†µâ€”â€”è½¯ä»¶åº”ç”¨ç¨‹åºä¸éœ€è¦è½®è¯¢APIä»¥æ£€æŸ¥æ–°ä»£ç çš„å­˜åœ¨ï¼Œè€Œæ•°æ®åº”ç”¨ç¨‹åºéœ€è¦ã€‚
- en: Given the analog of Continuous Delivery in data is so trivial, we can loosely
    define Continuous Data Integration as the process of reliably and efficiently
    releasing data into production in response to code changes. Code changes that
    govern the state of the data are â€œcontinuously integratedâ€ via a process of cloning,
    materialising views, and running tests.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: é‰´äºæ•°æ®ä¸­æŒç»­äº¤ä»˜çš„ç±»æ¯”å¦‚æ­¤å¾®ä¸è¶³é“ï¼Œæˆ‘ä»¬å¯ä»¥å®½æ³›åœ°å®šä¹‰æŒç»­æ•°æ®é›†æˆä¸ºå¯é é«˜æ•ˆåœ°å“åº”ä»£ç æ›´æ”¹å°†æ•°æ®å‘å¸ƒåˆ°ç”Ÿäº§ç¯å¢ƒçš„è¿‡ç¨‹ã€‚é€šè¿‡å…‹éš†ã€å®ä½“åŒ–è§†å›¾å’Œè¿è¡Œæµ‹è¯•æ¥â€œæŒç»­é›†æˆâ€æ§åˆ¶æ•°æ®çŠ¶æ€çš„ä»£ç æ›´æ”¹ã€‚
- en: We can also loosely define Continuous Data Delivery as the process of reliably
    and efficiently releasing *new data into production*. This covers the invocations
    of pipelines or operations in response to the existence of new data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹Ÿå¯ä»¥å®½æ³›åœ°å®šä¹‰æŒç»­æ•°æ®äº¤ä»˜ä¸ºå¯é é«˜æ•ˆåœ°å°†*æ–°æ•°æ®å‘å¸ƒåˆ°ç”Ÿäº§ç¯å¢ƒ*çš„è¿‡ç¨‹ã€‚è¿™åŒ…æ‹¬å¯¹æ–°æ•°æ®çš„å­˜åœ¨è¿›è¡Œç®¡é“è°ƒç”¨æˆ–æ“ä½œã€‚
- en: Thinking about these two processes as the same type of operation but in a different
    context is a fairly radical departure from how most data teams think about data
    pipelines, or *data release pipelines*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¿™ä¸¤ä¸ªè¿‡ç¨‹è§†ä¸ºç›¸åŒç±»å‹çš„æ“ä½œä½†åœ¨ä¸åŒçš„ä¸Šä¸‹æ–‡ä¸­ï¼Œè¿™ä¸å¤§å¤šæ•°æ•°æ®å›¢é˜Ÿå¯¹æ•°æ®ç®¡é“æˆ–*æ•°æ®å‘å¸ƒç®¡é“*çš„æ€è€ƒæ–¹å¼æœ‰äº†ç›¸å½“å¤§çš„æ”¹å˜ã€‚
- en: Additional considerations
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é¢å¤–çš„è€ƒè™‘å› ç´ 
- en: There are lots of additional considerations to think about here, and thatâ€™s
    because there is *so much to consider* beyond merely releasing data in production.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰å¾ˆå¤šé¢å¤–çš„è€ƒè™‘å› ç´ ï¼Œè¿™æ˜¯å› ä¸ºé™¤äº†ä»…ä»…åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å‘å¸ƒæ•°æ®ä¹‹å¤–ï¼Œè¿˜æœ‰*å¾ˆå¤šè¦è€ƒè™‘çš„ä¸œè¥¿*ã€‚
- en: Data isnâ€™t static. It doesnâ€™t simply exist in a place where it can be manipulated.
    It arrives in locations sparsely distributed across an organisation. It gets moved
    between tools. It arrives at different frequencies and only after the laborious
    process of â€œELTâ€ does it finally arrive in a data lake or data warehouse.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å¹¶éé™æ€çš„ã€‚å®ƒä¸ä»…ä»…å­˜åœ¨äºå¯ä»¥æ“ä½œçš„åœ°æ–¹ã€‚å®ƒåˆ†æ•£åœ¨ç»„ç»‡ä¸­çš„å„ä¸ªåœ°æ–¹ã€‚å®ƒåœ¨å·¥å…·ä¹‹é—´ç§»åŠ¨ã€‚å®ƒä»¥ä¸åŒçš„é¢‘ç‡åˆ°è¾¾ï¼Œåªæœ‰ç»è¿‡â€œELTâ€çš„ç¹çè¿‡ç¨‹åï¼Œå®ƒæ‰æœ€ç»ˆåˆ°è¾¾æ•°æ®æ¹–æˆ–æ•°æ®ä»“åº“ã€‚
- en: Furthermore, Github Actions isnâ€™t a sufficient infrastructure for doing all
    of this work. Perhaps as an orchestration layer, but certainly not for doing heavy-compute
    and doing data management.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼ŒGithubæ“ä½œå¹¶ä¸è¶³ä»¥æ”¯æŒæ‰€æœ‰è¿™äº›å·¥ä½œã€‚ä¹Ÿè®¸ä½œä¸ºç¼–æ’å±‚ï¼Œä½†è‚¯å®šä¸é€‚ç”¨äºè¿›è¡Œå¤§é‡è®¡ç®—å’Œæ•°æ®ç®¡ç†ã€‚
- en: These factors lead to many additional considerations for how to design a system
    thatâ€™s capable of delivering Continuous Data Integration and Delivery, which I
    discuss here
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å› ç´ å¯¼è‡´äº†è®¸å¤šé¢å¤–çš„è€ƒè™‘ï¼Œå…³äºå¦‚ä½•è®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿæä¾›æŒç»­æ•°æ®é›†æˆå’Œäº¤ä»˜çš„ç³»ç»Ÿï¼Œæˆ‘åœ¨è¿™é‡Œè®¨è®º
- en: User Interface
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”¨æˆ·ç•Œé¢
- en: Having a single User Interface to view Data deployments is key. Data teams who
    just use the UIs from multiple cloud data providers for DataOps will be at a loss
    when it comes to aggregating metadata to do effective DataOps, but also [BizFinOps](https://aws.amazon.com/blogs/enterprise-strategy/introducing-finops-excuse-me-devsecfinbizops/).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ‹¥æœ‰ä¸€ä¸ªå•ä¸€çš„ç”¨æˆ·ç•Œé¢æ¥æŸ¥çœ‹æ•°æ®éƒ¨ç½²æ˜¯å…³é”®çš„ã€‚åªä½¿ç”¨å¤šä¸ªäº‘æ•°æ®æä¾›å•†çš„ç”¨æˆ·ç•Œé¢è¿›è¡Œæ•°æ®è¿è¥çš„æ•°æ®å›¢é˜Ÿï¼Œåœ¨èšåˆå…ƒæ•°æ®ä»¥è¿›è¡Œæœ‰æ•ˆçš„æ•°æ®è¿è¥ï¼Œä»¥åŠ[BizFinOps](https://aws.amazon.com/blogs/enterprise-strategy/introducing-finops-excuse-me-devsecfinbizops/)æ–¹é¢å°†ä¼šæŸå¤±ã€‚
- en: 'There are also data deployments to aggregate, which typically arise due to:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰è¦èšåˆçš„æ•°æ®éƒ¨ç½²ï¼Œé€šå¸¸æ˜¯ç”±äºï¼š
- en: New data arriving
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ–°æ•°æ®åˆ°è¾¾
- en: A change in the logic for how to materialise the data
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ”¹å˜æ•°æ®å®ç°é€»è¾‘
- en: These are currently handled using a workflow orchestration tool and GitHub actions
    or something similar. This creates a disjoint â€” Data Teams need to inspect multiple
    tools to understand when data tables have been updated, what their definitions
    are, and so on. You could, of course, buy an observability tool. However this
    is *yet another UI, another tool, and another cost.*
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ä½¿ç”¨å·¥ä½œæµç¼–æ’å·¥å…·å’ŒGitHubæ“ä½œæˆ–ç±»ä¼¼å·¥å…·æ¥å¤„ç†è¿™äº›ã€‚è¿™ä¼šé€ æˆä¸è¿è´¯æ€§ â€” æ•°æ®å›¢é˜Ÿéœ€è¦æ£€æŸ¥å¤šä¸ªå·¥å…·ï¼Œä»¥äº†è§£æ•°æ®è¡¨ä½•æ—¶å·²æ›´æ–°ï¼Œå®ƒä»¬çš„å®šä¹‰æ˜¯ä»€ä¹ˆç­‰ç­‰ã€‚å½“ç„¶ï¼Œä½ å¯ä»¥è´­ä¹°ä¸€ä¸ªå¯è§‚å¯Ÿæ€§å·¥å…·ã€‚ç„¶è€Œï¼Œè¿™åˆæ˜¯*å¦ä¸€ä¸ªç”¨æˆ·ç•Œé¢ï¼Œå¦ä¸€ä¸ªå·¥å…·ï¼Œå¦ä¸€ä¸ªæˆæœ¬*ã€‚
- en: Having a genuine single pane of glass for Orchestration, Observability, and
    some kind of Ops would be a killer feature and one I would have loved to use at
    Codat, where weâ€™d stitched together a whole host of open-sourced and closed-source
    vendor SAAS tools.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ‹¥æœ‰ä¸€ä¸ªçœŸæ­£çš„å•ä¸€æ“ä½œé¢æ¿ï¼Œç”¨äºç¼–æ’ã€å¯è§‚å¯Ÿæ€§å’ŒæŸç§è¿ç»´åŠŸèƒ½ï¼Œå°†æ˜¯ä¸€ä¸ªæ€æ‰‹çº§åŠŸèƒ½ï¼Œæˆ‘å¾ˆæƒ³åœ¨Codatä½¿ç”¨ï¼Œæˆ‘ä»¬åœ¨é‚£é‡Œæ•´åˆäº†ä¸€æ•´å¥—å¼€æºå’Œé—­æºä¾›åº”å•†SAASå·¥å…·ã€‚
- en: Observability or Metadata gathering
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯è§‚å¯Ÿæ€§æˆ–å…ƒæ•°æ®æ”¶é›†
- en: I alluded to this in the previous section, but observability and metadata gathering
    is fundamental to a strong Data Pipeline.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨å‰ä¸€èŠ‚ä¸­æåˆ°äº†è¿™ä¸€ç‚¹ï¼Œä½†å¯è§‚å¯Ÿæ€§å’Œå…ƒæ•°æ®æ”¶é›†å¯¹äºå¼ºå¤§çš„æ•°æ®ç®¡é“è‡³å…³é‡è¦ã€‚
- en: The important thing here, which I believe Observability Platforms miss, is to
    place the Observation in the Pipeline itself. Otherwise, presenting data engineers
    with metadata, pieces of information like â€œthis failedâ€ or â€œthat table is staleâ€
    is undesirable, since itâ€™s A) ex-post (after itâ€™s too late) and B) unrelated to
    pipeline runs. Sure â€” a table is broken. But is it broken because of a change
    someone just pushed or because of some new data that arrived?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œé‡è¦çš„ä¸€ç‚¹ï¼Œæˆ‘è®¤ä¸ºå¯è§‚å¯Ÿæ€§å¹³å°å¿½ç•¥äº†çš„æ˜¯å°†è§‚å¯Ÿæ”¾åœ¨ç®¡é“æœ¬èº«ã€‚å¦åˆ™ï¼Œå‘æ•°æ®å·¥ç¨‹å¸ˆå‘ˆç°å…ƒæ•°æ®ã€è¯¸å¦‚â€œè¿™ä¸ªå¤±è´¥äº†â€æˆ–â€œé‚£ä¸ªè¡¨å·²è¿‡æ—¶â€çš„ä¿¡æ¯æ˜¯ä¸å¯å–çš„ï¼Œå› ä¸ºå®ƒä»¬æ˜¯Aï¼‰äº‹åï¼ˆä¸ºæ—¶å·²æ™šï¼‰å’ŒBï¼‰ä¸ç®¡é“è¿è¡Œæ— å…³ã€‚å½“ç„¶
    â€”â€” ä¸€ä¸ªè¡¨æ˜¯åçš„ã€‚ä½†å®ƒæ˜¯å› ä¸ºæŸäººåˆšåˆšæ¨é€äº†ä¸€ä¸ªæ›´æ”¹è¿˜æ˜¯å› ä¸ºæœ‰æ–°æ•°æ®åˆ°è¾¾ï¼Ÿ
- en: There was another good talk from [Andrew Jones](https://andrew-jones.com/categories/data-contracts/)
    I attended recently where he spoke about the 1, 10, 100 pyramid.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘æˆ‘å‚åŠ äº†[å®‰å¾·é²Â·ç¼æ–¯](https://andrew-jones.com/categories/data-contracts/)çš„å¦ä¸€ä¸ªç²¾å½©æ¼”è®²ï¼Œä»–è°ˆåˆ°äº†1ã€10ã€100é‡‘å­—å¡”ã€‚
- en: '![](../Images/c9e90125d2e5e686ec0628f83a9afc09.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9e90125d2e5e686ec0628f83a9afc09.png)'
- en: It costs $1 to prevent, $10 to mitigate and $100 once itâ€™s too late. Image credit
    to Andrew Jones, posted with permission.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„é˜²æˆæœ¬ä¸º1ç¾å…ƒï¼Œç¼“è§£æˆæœ¬ä¸º10ç¾å…ƒï¼Œä¸€æ—¦ä¸ºæ—¶å·²æ™šï¼Œæˆæœ¬ä¸º100ç¾å…ƒã€‚å›¾ç‰‡ç”±å®‰å¾·é²Â·ç¼æ–¯æä¾›ï¼Œè·å¾—è®¸å¯åå‘å¸ƒã€‚
- en: '**Prevention Cost** â€” Preventing an error in data at the point of extraction
    will cost you a $1.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é¢„é˜²æˆæœ¬** â€” åœ¨æ•°æ®æå–ç‚¹é¢„é˜²é”™è¯¯å°†èŠ±è´¹ä½ 1ç¾å…ƒã€‚'
- en: '**Correction Cost** â€” Having someone correct an error post extraction will
    cost you $10.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ›´æ­£æˆæœ¬** â€” åœ¨æå–åæœ‰äººçº æ­£é”™è¯¯å°†èŠ±è´¹ä½ 10ç¾å…ƒã€‚'
- en: '**Failure Cost** â€” Letting bad data run through a process to its end resting
    place will cost you $100.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤±è´¥æˆæœ¬** â€” è®©é”™è¯¯æ•°æ®é€šè¿‡æµç¨‹åˆ°è¾¾æœ€ç»ˆä½ç½®å°†èŠ±è´¹ä½ 100ç¾å…ƒã€‚'
- en: Observability tools are in the yellow to red section. If theyâ€™re setup on your
    prod databases, the likelihood is youâ€™re in a race against time to fix the issue
    before someone realises.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å¯è§‚å¯Ÿæ€§å·¥å…·å¤„äºé»„è‰²åˆ°çº¢è‰²åŒºåŸŸã€‚å¦‚æœå®ƒä»¬è®¾ç½®åœ¨ä½ çš„ç”Ÿäº§æ•°æ®åº“ä¸Šï¼Œå¾ˆå¯èƒ½ä½ æ­£åœ¨ä¸æ—¶é—´èµ›è·‘ï¼Œä»¥åœ¨æœ‰äººæ„è¯†åˆ°ä¹‹å‰è§£å†³é—®é¢˜ã€‚
- en: Having analysts patch bad data with hacky SQL sits in the yellow section.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†æå¸ˆä½¿ç”¨hacky SQLä¿®å¤é”™è¯¯æ•°æ®ä½äºé»„è‰²åŒºåŸŸã€‚
- en: Orchestration with observability combined is between the green and the yellow.
    If all your data pipelines have access to all your metadata, and can implement
    data quality tests as you materialise and update tables and views, then you can
    pause a pipeline *any time* these tests fail. This means [*no bad data ever gets
    into production*](https://medium.com/snowflake/avoid-bad-data-completely-continuous-delivery-architectures-in-the-modern-data-stack-part-1-22a0d48935f6)*.*
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰å¯è§‚å¯Ÿæ€§çš„ç¼–æ’ä½äºç»¿è‰²å’Œé»„è‰²ä¹‹é—´ã€‚å¦‚æœæ‚¨çš„æ‰€æœ‰æ•°æ®ç®¡é“éƒ½å¯ä»¥è®¿é—®æ‰€æœ‰å…ƒæ•°æ®ï¼Œå¹¶ä¸”å¯ä»¥åœ¨æ‚¨å®ç°å’Œæ›´æ–°è¡¨å’Œè§†å›¾æ—¶æ‰§è¡Œæ•°æ®è´¨é‡æµ‹è¯•ï¼Œé‚£ä¹ˆå½“è¿™äº›æµ‹è¯•å¤±è´¥æ—¶ï¼Œæ‚¨å¯ä»¥éšæ—¶æš‚åœç®¡é“ã€‚è¿™æ„å‘³ç€[*ä¸ä¼šæœ‰é”™è¯¯æ•°æ®è¿›å…¥ç”Ÿäº§ç¯å¢ƒ*](https://medium.com/snowflake/avoid-bad-data-completely-continuous-delivery-architectures-in-the-modern-data-stack-part-1-22a0d48935f6)*ã€‚*
- en: This is extremely powerful, which is why I believe having an Orchestration tool
    that executes data pipelines with access to granular metadata is the way forward
    (disclosure my start-up is doing just that).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™éå¸¸å¼ºå¤§ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ç›¸ä¿¡ï¼Œæ‹¥æœ‰ä¸€ä¸ªå…·æœ‰å¯¹ç»†ç²’åº¦å…ƒæ•°æ®è®¿é—®æƒé™çš„ç¼–æ’å·¥å…·æ˜¯æœªæ¥çš„å‘å±•æ–¹å‘ï¼ˆæˆ‘åˆ›åŠçš„åˆåˆ›å…¬å¸æ­£åœ¨åšè¿™ä¸ªï¼‰ã€‚
- en: Summary
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: We are moving away from data-agnostic workflow orchestration tools plus janky
    or non-existent Continuous Integration to unified Continuous *DATA* Integration
    and Delivery.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ­£åœ¨æ‘†è„±æ•°æ®ä¸å¯çŸ¥çš„å·¥ä½œæµç¼–æ’å·¥å…·åŠ ä¸Šä¸ç¨³å®šæˆ–ä¸å­˜åœ¨çš„æŒç»­é›†æˆï¼Œè½¬å‘ç»Ÿä¸€çš„æŒç»­*æ•°æ®*é›†æˆå’Œäº¤ä»˜ã€‚
- en: There will be platforms that enable data teams to get full, reliable, and efficient
    version-controlling of datasets and rock-solid data pipelines. These will have
    observability capabilities built-in, and while many are not end-to-end yet, it
    certainly feels like this is the way things are heading.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ä¼šæœ‰å¹³å°ä½¿æ•°æ®å›¢é˜Ÿèƒ½å¤Ÿå¯¹æ•°æ®é›†è¿›è¡Œå®Œæ•´ã€å¯é å’Œé«˜æ•ˆçš„ç‰ˆæœ¬æ§åˆ¶ï¼Œå¹¶æ‹¥æœ‰åšå›ºçš„æ•°æ®ç®¡é“ã€‚è¿™äº›å¹³å°å°†å†…ç½®å¯è§‚å¯Ÿæ€§åŠŸèƒ½ï¼Œè™½ç„¶è®¸å¤šå¹³å°å°šæœªå®ç°ç«¯åˆ°ç«¯ï¼Œä½†è‚¯å®šæ„Ÿè§‰äº‹æƒ…æ­£åœ¨æœç€è¿™ä¸ªæ–¹å‘å‘å±•ã€‚
- en: There are many mature data tools that are doing this. For example, for data
    warehousing CI and CD, Y42 have the [concept](https://www.y42.com/blog/virtual-data-builds-one-data-warehouse-environment-for-every-git-commit/)
    of â€œVirtual Data Buildsâ€ which is basically the same thing as part 1 of this article.
    For the Data Lake environment, Einat Orr over at Lake FS / Treeverse posted on
    this [recently](https://www.linkedin.com/feed/update/urn:li:activity:7126949272050106369/?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7126949272050106369%2C7126969193651924992%29&dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287126969193651924992%2Curn%3Ali%3Aactivity%3A7126949272050106369%29&dashReplyUrn=urn%3Ali%3Afsd_comment%3A%287126970713164439552%2Curn%3Ali%3Aactivity%3A7126949272050106369%29&replyUrn=urn%3Ali%3Acomment%3A%28activity%3A7126949272050106369%2C7126970713164439552%29)
    â€” what they do is functionally pretty similar and easily implementable in Snowflake
    ([I wrote an article about that here](https://medium.com/snowflake/why-snowflakes-clone-command-changes-the-game-for-ci-cd-in-data-ccb6fb9955ba)).
    SQLMeshâ€™ â€œenterpriseâ€ version (donâ€™t believe the open source [happy clappyness](https://medium.com/@hugolu87/y-combinator-had-282-companies-this-winter-and-c-50-were-open-source-sunday-scaries-128e53318454),
    this *is* a venture backed business. They *will* try to make money, like all of
    us) has observability and version control built into it, and itâ€™s pretty cool.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è®¸å¤šæˆç†Ÿçš„æ•°æ®å·¥å…·æ­£åœ¨åšè¿™ä¸ªã€‚ä¾‹å¦‚ï¼Œå¯¹äºæ•°æ®ä»“åº“çš„æŒç»­é›†æˆå’Œäº¤ä»˜ï¼ŒY42æ‹¥æœ‰â€œè™šæ‹Ÿæ•°æ®æ„å»ºâ€ï¼ˆVirtual Data Buildsï¼‰çš„[æ¦‚å¿µ](https://www.y42.com/blog/virtual-data-builds-one-data-warehouse-environment-for-every-git-commit/)ï¼ŒåŸºæœ¬ä¸Šä¸æœ¬æ–‡çš„ç¬¬ä¸€éƒ¨åˆ†ç›¸åŒã€‚å¯¹äºæ•°æ®æ¹–ç¯å¢ƒï¼ŒLake
    FS / Treeverseçš„Einat Orræœ€è¿‘åœ¨[LinkedInä¸Šå‘è¡¨äº†æ–‡ç« ](https://www.linkedin.com/feed/update/urn:li:activity:7126949272050106369/?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7126949272050106369%2C7126969193651924992%29&dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287126969193651924992%2Curn%3Ali%3Aactivity%3A7126949272050106369%29&dashReplyUrn=urn%3Ali%3Afsd_comment%3A%287126970713164439552%2Curn%3Ali%3Aactivity%3A7126949272050106369%29&replyUrn=urn%3Ali%3Acomment%3A%28activity%3A7126949272050106369%2C7126970713164439552%29ï¼‰-
    ä»–ä»¬æ‰€åšçš„åœ¨åŠŸèƒ½ä¸Šéå¸¸ç›¸ä¼¼ï¼Œå¹¶ä¸”åœ¨Snowflakeä¸­å¾ˆå®¹æ˜“å®ç°ï¼ˆ[æˆ‘åœ¨è¿™é‡Œå†™äº†ä¸€ç¯‡æ–‡ç« ](https://medium.com/snowflake/why-snowflakes-clone-command-changes-the-game-for-ci-cd-in-data-ccb6fb9955ba)ï¼‰ã€‚SQLMeshçš„â€œä¼ä¸šâ€ç‰ˆæœ¬ï¼ˆä¸è¦ç›¸ä¿¡å¼€æºçš„[å¿«ä¹](https://medium.com/@hugolu87/y-combinator-had-282-companies-this-winter-and-c-50-were-open-source-sunday-scaries-128e53318454)ï¼Œè¿™æ˜¯ä¸€å®¶é£é™©æŠ•èµ„æ”¯æŒçš„ä¼ä¸šã€‚ä»–ä»¬ä¼šè¯•å›¾èµšé’±ï¼Œå°±åƒæˆ‘ä»¬æ‰€æœ‰äººä¸€æ ·ï¼‰å…·æœ‰å†…ç½®çš„å¯è§‚å¯Ÿæ€§å’Œç‰ˆæœ¬æ§åˆ¶åŠŸèƒ½ï¼Œéå¸¸é…·ã€‚
- en: You can, of course, still do *all of this* using something like Airflow. Hell,
    you could brew your morning coffee with Airflow if you wanted to. I guess the
    question is â€” do you have the time, the patience, and the expertise to write all
    of that code? Or are you like me, and do you just want to get shit done? ğŸ 
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å½“ç„¶å¯ä»¥ä½¿ç”¨ç±»ä¼¼Airflowè¿™æ ·çš„å·¥å…·æ¥å®Œæˆ*æ‰€æœ‰è¿™äº›*ã€‚è¯¥æ­»çš„ï¼Œå¦‚æœä½ æ„¿æ„ï¼Œä½ ç”šè‡³å¯ä»¥ç”¨Airflowç…®æ—©æ™¨çš„å’–å•¡ã€‚æˆ‘æƒ³é—®é¢˜æ˜¯â€”â€”ä½ æœ‰æ—¶é—´ã€è€å¿ƒå’Œä¸“ä¸šçŸ¥è¯†æ¥ç¼–å†™æ‰€æœ‰è¿™äº›ä»£ç å—ï¼Ÿè¿˜æ˜¯åƒæˆ‘ä¸€æ ·ï¼Œåªæ˜¯æƒ³æŠŠäº‹æƒ…åšå¥½ï¼ŸğŸ 
