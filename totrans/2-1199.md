# 如何利用贝叶斯高斯混合模型提高聚类准确性

> 原文：[https://towardsdatascience.com/how-to-improve-clustering-accuracy-with-bayesian-gaussian-mixture-models-2ef8bb2d603f](https://towardsdatascience.com/how-to-improve-clustering-accuracy-with-bayesian-gaussian-mixture-models-2ef8bb2d603f)

## 聚类

## 一种用于现实世界数据的更高级的聚类技术

[](https://medium.com/@maclayton?source=post_page-----2ef8bb2d603f--------------------------------)[![Mike Clayton](../Images/2d37746b13b7d2ff1c6515893914da97.png)](https://medium.com/@maclayton?source=post_page-----2ef8bb2d603f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2ef8bb2d603f--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2ef8bb2d603f--------------------------------) [Mike Clayton](https://medium.com/@maclayton?source=post_page-----2ef8bb2d603f--------------------------------)

·发布在 [数据科学前沿](https://towardsdatascience.com/?source=post_page-----2ef8bb2d603f--------------------------------) ·阅读时长 27 分钟·2023年2月15日

--

![](../Images/4dfa3e01cd737d3436bde521a650137a.png)

图片由 [Tima Miroshnichenko](https://www.pexels.com/photo/mixture-of-paint-on-palette-5034000/) 提供，来自 [Pexels](https://www.pexels.com/)

**在现实世界中，你常常会发现数据遵循某种概率分布。无论是高斯（或正态）分布、韦布尔分布、泊松分布、指数分布等，取决于具体的数据。**

**了解哪个分布描述了你的数据，或者哪个分布可能最能描述你的数据，可以让你利用这一点，从而改善你的推断和/或预测。**

**本文将探讨如何利用数据集的潜在概率分布来改善标准 K-Means 聚类模型的拟合效果，甚至允许从数据中直接自动选择合适的簇数。**

# 介绍

许多引人注目的机器学习/深度学习技术往往涉及***有监督***的机器学习/深度学习，即数据已经被标记，模型从中学习正确的答案。然后，将训练好的模型应用于未来的数据以进行预测。

这些都非常有用，但现实是数据是由全球的企业和个人不断产生的，其中大多数数据是未标记的。在绝大多数情况下，标记数据实际上是非常昂贵和耗时的。这就是***无监督***学习派上用场的地方。

> …数据是由全球的企业和个人不断产生的，其中大多数数据是未标记的。

找到从未标记数据中推断意义的最佳方法是许多企业非常重要的追求。这可以发现潜在的未知或不明显的趋势或分组。然后可以分配资源、针对特定客户群体，或只是进行额外的研究和开发。

此外，大多数情况下，数据涉及某种形式的人或自然过程。自然过程和人类行为通常由高斯分布很好地捕捉和描述。

鉴于此，本文将探讨如何利用高斯分布，包括高斯混合模型（GMM）和贝叶斯高斯混合模型（BGMM），来提高表示现实世界数据集中的“自然过程”的数据集的聚类准确性。

作为比较和判断的基础，将使用普遍的K均值聚类算法。

# 计划

![](../Images/563f971fc8b592a6744b15fcb3a0e363.png)

照片由 [Christina Morillo](https://www.pexels.com/photo/man-standing-infront-of-white-board-1181345/) 提供，来源于 [Pexels](https://www.pexels.com/)

本文将涵盖相当广泛的内容，并包含来自综合Jupyter笔记本的示例。

本节应为您提供一些关于涵盖内容的指导，并指出如果需要访问特定信息应跳过的位置。

1.  最初，文章将涵盖关于一般使用高斯混合模型的“什么”和“为什么”问题。

1.  由于scikit-learn库中有两个现成的高斯混合模型实现，接下来将讨论普通 [高斯混合模型](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) 和 [贝叶斯高斯混合模型](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html#sklearn.mixture.BayesianGaussianMixture) 之间的主要区别。

1.  文章将深入探讨如何使用高斯混合模型对现实世界的多特征数据集进行聚类。所有示例将使用K均值算法、普通高斯混合模型和贝叶斯高斯混合模型来实现。

1.  随后将有两个额外的部分，主要集中于算法的更清晰可视化。数据的复杂性将通过a) 使用两个组件的主成分分析（PCA）和b) 仅分析数据集中的两个特征来减少。

***注意：*** *选择的数据集实际上是有标签的。这是故意为之，以便将聚类的性能与100%正确且已知的主要聚类进行比较。*

# 解释

![](../Images/9a2591d5a3de758c76a2b5a4140b25c8.png)

照片由 [Polina Zimmerman](https://www.pexels.com/photo/notes-on-board-3782142/) 提供，来源于 [Pexels](https://www.pexels.com/)

在进入数据并查看实际效果之前，我们先讨论“是什么”和“为什么”类型的问题。

在我们开始之前，值得注意的是，本文章不会讨论什么是高斯/正态分布，或者什么是K-Means聚类。市面上有很多很好的资源可以参考，本文章没有足够的空间来覆盖这些内容。因此，从这一点起，假设你对这些概念有基本了解。

***注意：*** *“高斯分布”和“正态分布”这两个短语指的是同一个东西，本文中将交替使用。*

## 什么是高斯混合模型？

简而言之，该算法假设你提供的数据可以用未指定的不同高斯分布的混合来近似。

算法会尝试提取和分离这些高斯分布的混合，并将它们作为单独的簇返回给你。

就是这样。

所以本质上，它就像K-Means聚类，但它有一个额外的优点，即能够对数据应用额外的统计约束。这使得它能更灵活地捕捉簇的形状。此外，它允许更精确地分离紧密或稍微连接的簇，因为算法可以访问由假设的高斯分布生成的统计概率。

从视觉上看，如果分析局限于二维或三维，K-Means算法会确定簇中心，并在这些中心周围应用“圆形”或“球形”分布。

然而，如果底层数据是高斯的，那么由于高斯分布的尾部，分布被拉伸到某种程度是完全可能的，甚至是预期的。这在形状上相当于一个“椭圆”或“椭球体”。这些拉伸的“椭圆”或“椭球体”形状是K-Means无法准确建模的，但高斯混合模型可以。

另一方面，如果你向高斯混合模型提供的数据明显远离高斯分布，算法仍会假设它是高斯的。因此，你可能最终得到的，充其量，也不比K-Means更好。

***附注：*** *高斯混合模型和贝叶斯高斯混合模型可以使用K-Means聚类算法来生成模型的一些初始参数（实际上，这是scikit-learn中的默认设置）。*

这就引出了一个问题……

## 高斯混合模型可以用于什么类型的数据？

自然过程通常是一个很好的起点。原因主要是由于中心极限定理（CLT）：

> 在概率论中，**中心极限定理**（**CLT**）确定了在许多情况下，当独立随机变量相加时，它们经过适当标准化的和趋向于正态分布，即使原始变量本身不是正态分布。
> 
> [-wikipedia.org](https://en.wikipedia.org/wiki/Central_limit_theorem)

简单来说，这本质上意味着，在自然过程中，虽然特定变量（例如人的身高）是由许多可能是或不是正态分布的因素（饮食、生活方式、环境、基因等）引起的，但这些部分的标准化总和（人的身高）将会是（大致上）正态分布的。这就是为什么我们经常看到自然过程呈现正态分布的原因。

![](../Images/d3e28ba209ef19938dd728233aaa7674.png)

图片由 [Ivan Samkov](https://www.pexels.com/photo/person-holding-black-pen-and-an-x-ray-4989192/) 提供，来自 [Pexels](https://www.pexels.com/)

因此，现实中有大量需要处理高斯分布的实例，这使得高斯混合模型在适当的场景下非常有用：

+   **客户行为** —— 这可以指购买行为、消费金额、注意力跨度、流失率等。

+   **人体特征** —— 身高、体重、鞋码、智商（或教育表现）等。

+   **自然现象** —— 在医学领域（癌症、疾病、基因等）或其他科学领域识别模式/群体

显然，还有许多其他例子，以及其他原因导致高斯分布的情况。

当然，也可能有些情况下你无法辨别数据的潜在结构，这自然需要进一步调查，但这也是领域专家如此重要的原因之一。

## 为什么使用高斯混合模型？

主要原因是，如果你比较确定你的数据是高斯分布（或更准确地说，是高斯数据的混合），那么你将能更准确地分离出真正的聚类，从而大大提高准确性。

算法不仅寻找基本聚类，还考虑每个聚类最合适的形状或分布。这使得紧密分布或稍微连在一起的聚类能够更准确和轻松地分离出来。

此外，在分离不清晰的情况下（或可能只是为了更深入的分析），可以生成并分析每个数据点属于每个聚类的概率。这让你更好地了解哪些是核心可靠的数据点，哪些可能是边缘的或不清楚的数据点。

使用贝叶斯高斯混合模型，算法也可以从数据中推断出最合适的聚类数量。这比依赖于 [肘部法则](https://en.wikipedia.org/wiki/Elbow_method_(clustering)) 或生成 [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion) / [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion) 曲线要好。

## 高斯混合模型是如何工作的？

基本上，这个方法使用迭代更新过程来逐渐优化多个高斯分布与数据的拟合。我想这与梯度下降有些类似。

> 检查拟合——调整——再次检查拟合——调整……并重复，直到收敛。

在这种情况下，算法被称为[期望最大化](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)（EM）算法。更具体地说，情况是这样的：

1.  模型的一个输入参数是簇的数量，所以这是一个已知的量。例如，如果设置了两个簇，那么初始的两个高斯分布将有其参数被分配。参数可以通过 K-Means 分析（scikit-learn 的默认设置）来分配，或者随机分配。如果有非常具体的情况，参数甚至可以为每个数据点指定。现在进入迭代部分……

1.  **期望**——现在有两个高斯分布定义了特定的参数。算法首先将每个数据点分配给两个高斯分布中的一个。它是根据数据点适合特定分布的**概率**来进行分配的，相对于另一个分布。

1.  **最大化**——一旦所有点都被分配，每个高斯分布的参数会稍作调整，以更好地拟合整个数据，基于从上一步生成的信息。

1.  重复步骤 2 和 3，直到收敛。

上述内容过于简化，我没有详细说明每个时刻优化的确切数学机制（或方程）。然而，它至少应能给你对算法操作的概念理解。

一如既往，网上有许多数学较重的文章更详细地解释了确切机制，如果你感兴趣的话。

## 普通高斯混合模型和贝叶斯高斯混合模型之间有什么区别？

我要说的是，解释贝叶斯高斯混合模型相较于标准高斯混合模型所使用的额外过程实际上相当复杂。它需要同时理解一些不同且复杂的数学概念，而本文的篇幅显然不足以充分说明。

我将旨在引导你找到正确的方向，并概述优缺点。你还将随着文章的深入，在数据集分析过程中收集更多信息。

***实际差异***

突出的区别在于，标准高斯混合模型使用[期望最大化](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)（EM）算法，而贝叶斯高斯混合模型使用变分推断（VI）。

不幸的是，变分推断在数学上并不直观，但如果你想深入了解，我建议阅读 [Jonathan Hui](https://medium.com/u/bd51f1a63813?source=post_page-----2ef8bb2d603f--------------------------------) 的 [这篇优秀文章](https://jonathan-hui.medium.com/machine-learning-variational-inference-273d8e6480bb)。

主要的结论有这些：

1.  变分推断是期望最大化算法的一个扩展。两者都旨在在数据中找到高斯分布（至少在这个实例中）。

1.  贝叶斯高斯混合模型需要提供更多的输入参数，这可能更加复杂/繁琐。

1.  变分推断本质上内置了一种正则化形式。

1.  变分推断不太可能产生“不稳定”或“边缘”解决方案。这使得算法更有可能趋向于一个有坚实支持的“真实”解决方案。或者正如 [scikit-learn 的文档](https://scikit-learn.org/stable/modules/mixture.html) 所述，"*由于纳入了先验信息，变分解决方案比期望最大化解决方案具有更少的病态特殊情况。*"

1.  贝叶斯高斯混合模型可以直接估计输入数据的最合适的聚类数量（无需肘部方法！）

…所以总结一下：

变分推断是期望最大化的更高级的**扩展**。理论上，它应该更准确，并且对杂乱数据或异常值更具抵抗力。

***资源与进一步阅读***

首先，我建议你查看 scikit-learn 文献中提供的优秀概述：

[](https://scikit-learn.org/stable/modules/mixture.html?source=post_page-----2ef8bb2d603f--------------------------------) [## 2.1. 高斯混合模型

### sklearn.mixture 是一个包，可以用来学习高斯混合模型（对角线、球形、绑定和完整…）

[scikit-learn.org](https://scikit-learn.org/stable/modules/mixture.html?source=post_page-----2ef8bb2d603f--------------------------------)

进一步阅读时，可以查阅的一些相关主题包括：

1.  期望最大化（EM）

1.  变分推断（VI）

1.  狄利克雷分布和狄利克雷过程

# 现在来看一些真实数据

![](../Images/f0fb3cecf09aa609211f0dbf9bd5ffc4.png)

图片由 [Couple of Cup](https://www.pexels.com/photo/a-glass-of-red-wine-on-a-wooden-bench-8473214/) 提供，来自 [Pexels](https://www.pexels.com/)

讨论和理论很好，但我经常发现探索真实实现可以澄清很多。因此，以下部分将比较以下聚类方法在真实世界数据集上的表现：

1.  K均值（基准）

1.  高斯混合模型

1.  贝叶斯高斯混合模型

# 数据

将要使用的[真实世界数据集](https://archive-beta.ics.uci.edu/dataset/109/wine)¹ 描述了来自意大利同一地区的三种不同葡萄酒的化学成分。

这个数据集已经标注，因此尽管以下所有的聚类分析不会在分析中使用这些标签，但它将允许与已知的正确答案进行比较。因此，这三种聚类方法可以公平且无偏差地进行比较。

此外，数据集符合“自然”数据集的标准，这应该适合于高斯方法，这是预期的测试目标。

为了方便加载数据，我已将原始数据以 CSV 格式在我的 GitHub 仓库中提供：

[](https://github.com/thetestspecimen/notebooks/tree/main/datasets/wine?source=post_page-----2ef8bb2d603f--------------------------------) [## notebooks/datasets/wine at main · thetestspecimen/notebooks

### 这些数据是对来自意大利同一地区的葡萄酒的化学分析结果，但来源于三...

github.com](https://github.com/thetestspecimen/notebooks/tree/main/datasets/wine?source=post_page-----2ef8bb2d603f--------------------------------)

## 参考笔记本

所有后续的分析都已在一个全面的 Jupyter notebook 中提供。

原始笔记本可以在这里找到，适用于你的本地环境：

[](https://github.com/thetestspecimen/notebooks/blob/main/bayesian_gaussian_mixture_model.ipynb?source=post_page-----2ef8bb2d603f--------------------------------) [## notebooks/bayesian_gaussian_mixture_model.ipynb at main · thetestspecimen/notebooks

### 目前无法执行该操作。你在另一个标签页或窗口中登录了。你在另一个标签页或窗口中已登出...

github.com](https://github.com/thetestspecimen/notebooks/blob/main/bayesian_gaussian_mixture_model.ipynb?source=post_page-----2ef8bb2d603f--------------------------------)

…或者如果你需要一个在线解决方案，可以在 Deepnote 或 Colab 中开始：

[![](../Images/23c9a383e533919051f0579081cff99b.png)](https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fthetestspecimen%2Fnotebooks%2Fblob%2Fmain%2Fbayesian_gaussian_mixture_model.ipynb)[![](../Images/f8d7c9abab0134f402cd7732c7eaff36.png)](https://colab.research.google.com/github/thetestspecimen/notebooks/blob/main/bayesian_gaussian_mixture_model.ipynb)

笔记本中使用的一些函数需要特定库的版本较新（特别是 scikit-learn 和 matplotlib），因此接下来的部分将描述所需的内容。

## 环境设置 — 本地或 Deepnote

无论是使用本地环境还是 Deepnote，只需确保可用的 scikit-learn 和 matplotlib 的版本正确即可。实现这一点最简单的方法是将其添加到你的“requirements.txt”文件中。

对于 Deepnote，你可以在右侧窗格的文件部分创建一个名为“requirements.txt”的文件，并添加以下内容：

[PRE0]

（更新版本也可以）。

## 环境设置 — Colab

由于在Colab中没有类似“requirements.txt”文件的访问权限，你需要显式安装正确版本的scikit-learn和matplotlib。为此，在空白单元格中运行以下代码以安装适当版本：

[PRE1]

（更新版本也可以）。

然后在尝试运行任何代码之前刷新网页，以确保库正确加载。

## 数据探索

在进行实际聚类之前，可能值得对数据进行粗略概览。

每个特征有13个特征和178个示例（没有缺失或空数据）：

…因此这是一个不错的干净数字数据集，可以开始使用。

唯一真正需要更改的是刻度。每个特征中的数字范围差异较大，因此将应用简单的[MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)将特征缩放到0和1之间。

## 数据分布

如前所述，理想情况下数据应该至少大致呈高斯分布，以便高斯混合模型有效工作。那么它的表现如何？

![](../Images/7327b5fe92d14d1d0251d64d5cc27bb6.png)

特征数据分布 — 图片由作者提供

现在其中一些看起来相当高斯（例如，灰分的碱度），但实际上大多数并不是。这是问题吗？不一定，因为在许多情况下，实际上存在***高斯分布混合***（或近似高斯分布）。

高斯混合模型的关键在于它可以从多个高斯分布的混合中找到并分离出各个高斯分布。

在真实的聚类问题中，你无法实现下一步，因为你不会知道数据的真实聚类。然而，仅为说明目的，实际上是可以（因为我们知道标签）绘制每个“真实”簇的分布：

![](../Images/2957b6337e73e1ea1f40b78374d2196b.png)

按标签划分的特征数据分布 — 图片由作者提供

如今你可以看到，真实的簇在大多数情况下大致呈高斯分布。虽然不是所有情况都是如此，但在真实数据中这种情况是不可避免的。由于数据基于“自然”数据，我们确实在处理大致高斯分布的数据。

对原始数据分布的这一基本调查说明了了解所处理数据类型的重要性，以及哪个工具最适合分析。这也是领域专家重要性的一个很好的案例。

## 特征关系

最后，展示了一些变量之间如何分布的例子（如果你想要更全面的图示，请查看Jupyter笔记本）：

![](../Images/e3285596aa5d60f468bf2d77eaf594e7.png)

组件之间的数据分布示例 — 图片由作者提供

从上面的散点图可以看出，一些特征显示出合理的分离。然而，也有许多特征之间存在混合，尤其是在每个簇的边缘。形状（圆形、长条形等）也变化很大。

# 分析

![](../Images/e1cc2074a9677a86fae0bae305924800.png)

图片由 [Artem Podrez](https://www.pexels.com/photo/a-chemist-analyzing-chemicals-on-test-tubes-in-a-laboratory-8532827/) 提供，来自 [Pexels](https://www.pexels.com/)

如前所述，将比较三种不同的算法：

1.  K-Means（基准）

1.  高斯混合模型

1.  贝叶斯高斯混合模型

探索过程将分为三个阶段：

1.  所有原始数据一起分析

1.  在通过主成分分析（PCA）降低复杂性/特征后分析所有数据

1.  使用仅两个特征进行的分析（主要为了比使用完整数据集更容易进行说明）

# 分析 1 — 完整数据集

为了保持比较的一致性，并减少本文的复杂性，我将跳过对正确组件数量的深入调查。

但是，为了完整性，你应该知道，进行任何聚类时的关键步骤是了解使用适当数量的簇。一些实现这一点的常见方法包括 [肘部法则](https://en.wikipedia.org/wiki/Elbow_method_(clustering))、[贝叶斯信息准则 (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion) 和 [赤池信息量准则](https://en.wikipedia.org/wiki/Akaike_information_criterion)。以下是整个原始数据集的 BIC 和 AIC 示例：

![](../Images/42cda83eb5b83991b996464814d58c50.png)

该数据集的 BIC 和 AIC 曲线 — 图片由作者提供

BIC 结果建议两个组件是合适的，但我现在不打算进一步讨论这一结果。然而，这将在本文后续讨论中出现。

从现在开始，将假设三个簇，以保持一致性和便于比较。

让我们开始吧！

## K-Means

![](../Images/1245574f17b17241104524497823a9f8.png)

K-Means 混淆矩阵 — 图片由作者提供

嗯，这确实是一个相当令人印象深刻的结果。

将所有数据一起考虑时，簇 1 和簇 3 之间显然有很好的分离。然而，由于簇 2 通常位于簇 1 和簇 3 之间（参考前一部分的四个示例散点图），因此簇 2 边缘的点可能被错误地分配到簇 1 和簇 3（即这些簇之间的边界可能定义不准确）。

让我们看看使用高斯混合模型是否有所改善。

## 高斯混合模型

![](../Images/fc2196ef094c37b1773bb0e7c45bbad8.png)

高斯混合模型混淆矩阵 — 图片由作者提供

有所改进。高斯混合模型成功地识别出三个额外的点，并将它们正确地分配到簇 2。

虽然这看起来不是什么大问题，但值得注意的是数据集相当小，使用更全面的数据集可能会得到更令人印象深刻的结果。

## 贝叶斯高斯混合模型

在产生结果之前，鉴于这是文章的主要焦点，我认为值得花一点时间来解释一些可以指定的相关参数：

1.  **n_components** — 这是你希望算法考虑的簇数。然而，算法可能返回或更倾向于少于这里设置的簇数，这也是该算法的主要优势之一（我们很快会看到这一点）。

1.  **covariance_type** — 这里有四个选项：*full*、*tied*、*diag* 和 *spherical*。最‘准确’且通常首选的是 *full*。这个参数本质上决定了分布拟合形状的限制，[这里](https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html#sphx-glr-auto-examples-mixture-plot-gmm-covariances-py)提供了一个很好的说明。

1.  **weight_concentration_prior_type** — 这可以是 *dirichlet_process*（无限混合模型）或 *dirichlet_distribution*（有限混合模型）。一般来说，选择Dirichlet过程更好，因为它对参数变化不那么敏感，并且不容易将自然簇划分为不必要的子组件，而Dirichlet分布有时可能会这样做。

1.  **weight_concentration_prior** — 指定一个低值（例如0.01）将使模型将更多的组件设置为零，只留下几个具有显著值的组件。高值（例如100000）则倾向于保留更多的活动组件并具有相关值，即较少的组件被设置为零。

由于额外参数较多，在某些情况下进行交叉验证分析可能是明智的。例如，在这次初步运行中，covariance_type 将设置为‘diag’而非‘full’，因为建议的簇数更具说服力。我怀疑在这种特定情况下，这可能是由于数据集较小以及特征数量较多，导致‘full’协方差类型出现过拟合。

现在可以审查算法在相关簇方面所做的决定。这可以通过从拟合模型中提取权重来实现。

更清晰地以图形形式展示：

![](../Images/17cad7c7d61780fedb0aac10ae7bda39.png)

贝叶斯高斯混合模型组权重 — 图片由作者提供

老实说，这并不是很有说服力。

很明显，前面有三个簇领先于其他簇，但这只是因为答案已知才直观。事实上，这并不是很清楚。那么为什么会这样呢？

主要原因可能是数据缺乏和特征数量过多（至少相比于数据量）的结合。

数据不足会导致像异常值这样的项对整体分布产生更大的影响，同时也减少了模型生成分布的能力。

在接下来的部分，我们将探讨可能的解决方案，并分析更小的特征选择，以查看在这些情况下的结果。

目前，既然我们知道正确的组件数量是三个，我们将继续前进。

![](../Images/498a7e4d4b5f3b24923485b83caa147a.png)

贝叶斯高斯混合模型混淆矩阵 — 作者提供的图片

几乎完美的聚类。仅有两个点仍被错误分配。

让我们快速查看一下总体比较。

![](../Images/753353ed02cca979e6d5d3d84d5e99e3.png)

不同聚类方法的准确性比较 — 作者提供的图片

## 讨论

从准确率结果可以看出，从一种方法到另一种方法有逐渐改进。这表明理解数据的基本结构很重要，因为它可以更准确地表示原始数据中的模式，在我们这里是高斯分布。

即使在高斯混合模型的范围内，也可以清楚地看到（至少在这种情况下），贝叶斯高斯混合模型中使用变分推断可以比期望最大化方法获得更准确的结果。

所有这些都是预期中的，但看到这些结果仍然很有趣。

## 可视化

为了概述高斯算法究竟在做什么，我将特征“酒精”与其他每个特征进行了对比，并在每个图上为每个三簇包含了贝叶斯高斯混合模型的置信椭圆。

***注意：*** *绘制置信椭圆所使用的算法改编自* [*这个算法*](https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm.html) *，该算法在 scikit-learn 文档中提供。*

![](../Images/0a66fef771997685b32ef8fdb07c49b9.png)

尽管这很有趣，且很好地展示了算法如何处理不同形状的簇（即圆形或更长的椭圆形），但它并没有提供任何关于可能影响结果的因素的理解。

这主要是因为在将输出表示为可解释的内容时，需要处理的维度（即特征）过多。

## 更好的可视化和进一步的调查

将各种方法生成的分布以清晰简洁的方式进行可视化和比较，将是很有趣的，这样可以准确地看到背后发生了什么。

然而，由于模型正在处理大量特征（因此维度也很多），无法用简单的2D图形来表示正在发生的情况，正如前一节所示。

鉴于此，本文接下来的两个部分的目标是通过减少维度来简化分析。因此，接下来的两个部分将关注：

1.  使用两个组件的主成分分析（PCA）。这将允许将所有十三个特征提炼为两个特征，同时保留数据中嵌入的***大部分***重要信息。

1.  专门仅使用两个特征进行分析。

这两项调查将允许对这些簇进行直接可视化，因为只有两个组件，因此可以将它们绘制在标准的2D图上。

此外，这些方法将可能使得贝叶斯高斯混合模型的自动簇选择更加有效，因为特征数量与示例数量的关系更加平衡。

# 分析2 — 主成分分析（PCA）

通过运行两个组件的主成分分析（PCA），13个特征可以压缩为2个组件。

本节的主要目标如下：

1.  从PCA降低复杂性所提供的数据分布中获取见解。

1.  审查由贝叶斯高斯混合模型从PCA数据集中生成的自动组件选择。

1.  在两个PCA组件上运行贝叶斯高斯混合模型，并以2D图形形式审查聚类结果。

## PCA的结果

![](../Images/51c1da959333213c5eaca7385b619c4e.png)

PCA对所有数据的两个组件及其分布（颜色为实际标签簇）— 图片由作者提供

PCA的结果因多种原因而有趣。

首先，主要PCA组件（以及在某种程度上次要PCA组件）的分布与高斯分布非常接近，所有三个组件均如此。这与我们在文章早期数据调查中发现的情况相吻合。

记住PCA是所有特征的提炼，看到三个真实簇之间有很好的分离是有趣的。这意味着有很大的可能性，一个针对数据良好的聚类算法（不管PCA如何）有潜力实现簇的良好和准确分离。

然而，这些簇距离足够近，如果聚类算法不能正确表示簇的分布或形状，则不能保证簇之间的边界会被准确找到。

例如，簇1的分布沿y轴明显拉长。任何算法都需要模仿这种拉长的形状，以正确表示该簇。

## 自动簇

在初步分析中，自动估计正确的簇数有些模糊。现在数据复杂性已降低，我们来看看是否有所改善。

再次，输入请求8个组件：

![](../Images/437868a7c7cd34a4e34812c293cae83d.png)

8 个请求簇的权重 — 作者提供的图像

…我们到此为止。非常清楚的表明，尽管模型设置为考虑多达 8 个簇，算法显然认为实际合适的簇数是 3。这一点我们当然知道是正确的。

这在一定程度上表明，为了让贝叶斯高斯混合模型有效工作，在自动簇选择方面，有必要考虑数据集是否足够大，以生成合理和明确的结果，如果簇的数量尚未确定。

## 结果

为了完整性，让我们看看 PCA 的贝叶斯高斯混合模型的聚类结果如何。

![](../Images/c9b0c1bff6bc78e738c7554e48f4efde.png)

贝叶斯高斯混合模型 (PCA) 混淆矩阵 — 作者提供的图像

与使用原始数据相比，准确度明显下降。这是可以预期的。通过运行 PCA，我们确实会丢失数据，这一点无法避免，在这种情况下，足以引入额外的 5 个错误分配的数据点。

让我们从视觉上更详细地看一下。

![](../Images/796ddef9ed81b7c82c0acd49d35e9b1a.png)

贝叶斯高斯混合的两个成分 PCA（错误匹配的点被红色圈出） — 原始分析中的错误匹配点被蓝色圈出 — 作者提供的图像

上面的图表信息量很大，所以让我们逐步解析：

1.  彩色点表示的是 PCA 数据被算法分配到的簇。

1.  大的阴影椭圆是置信椭球体，它们基本上表示了由算法生成的底层分布的形状（协方差）。

1.  红色圆圈表示的是在 PCA 分析中，由贝叶斯高斯混合模型错误分配的数据点。

1.  蓝色圆圈表示的是在原始分析中，由贝叶斯高斯混合模型错误分配的数据点，这些原始分析使用了所有的原始数据。

特别有趣的是，通过运行 PCA 分析，并且可能由于数据固有的损失，它迫使一些数据点被移动到生成的簇/置信椭球体内部（查看带有文本标注的橙色点和在绿色置信椭球体内被蓝色圆圈圈出的点）。

对于被蓝色圆圈标出的绿色点来说，它有所帮助，相比原始的“所有原始数据”分析有所改进。然而，橙色点明显被错误分配，它永远不会被正确分配，因为它过于嵌入在橙色簇中，而实际上它应该在绿色簇中。然而，原始分析正确地将此数据点分配到绿色簇中。

## PCA 摘要

在这种特定情况下，使用较小的数据集运行PCA有助于贝叶斯Gaussian混合模型确定适当的簇数量。即使作为帮助更好地可视化数据集的分布，这也是非常有用的。

然而，使用PCA生成的数据作为分析的最终输入并不是最佳解决方案。数据的偏移/丢失足以使某些数据点永久性地错误分配，并且整体准确性也降低。

# 分析3—较少的特征

为了更仔细地查看三种聚类算法之间的差异，将提取两个特定特征，并仅在这些特征上运行聚类算法。

从审查方法的角度来看，这种优势在于可以在二维平面上（即普通的二维图表）可视化发生了什么。

附加的好处是，现有的数据比整个数据集包含的信息要少得多。这不仅减少了样本数量少与特征数量之间的差异，还迫使聚类算法在减少信息的情况下更努力地工作以实现适当的拟合。

## 更详细地查看数据

![](../Images/5f8ed9534f61d43a3cbab46607faced8.png)

稀释酒的颜色强度与OD280/OD315（原始数据带标签）— 作者提供的图像

选择这一对（颜色强度和稀释酒的OD280/OD315）的原因是数据集对不同的聚类算法提出了挑战。

如你所见，有两个相当交织的簇（1和2）。此外，与更加圆形的簇1相比，簇2和簇3的分布较为拉长。

理论上，Gaussian混合方法应该比K均值表现得更好，因为它们能够准确地调整其分布特征以适应拉长的分布，而K均值则不能，因为它仅限于圆形表示。

此外，从图表两侧的KDE图中可以看到，数据分布是相当接近高斯分布的，正如我们在本文中之前确认过的那样。

## K均值—结果

![](../Images/9c36810830ac0f34a12dffa3c045ce4d.png)

K均值的混淆矩阵—减少特征—作者提供的图像

## Gaussian混合模型—结果

![](../Images/a03a45755258f45732976454846c20e8.png)

Gaussian混合模型的混淆矩阵—减少特征—作者提供的图像

## 贝叶斯Gaussian混合模型—组件选择

在直接进入结果之前，由于贝叶斯Gaussian混合模型有能力自动选择适当的组件数量，我们将再次‘请求’8个组件，并查看模型的建议。

![](../Images/a20ad9ed4481170344df5ff3cb58d8f8.png)

仅使用两个特征的贝叶斯Gaussian混合模型的组件选择—作者提供的图像

正如之前在降维PCA分析中看到的那样，模型发现区分已知存在的3个簇要容易得多。

这可以相当明确地确认：

1.  必须拥有足够的数据样本，以确保模型能够有一定的机会提供正确的簇数建议。这可能是由于需要足够的数据来正确表示底层分布。在这种情况下是高斯分布。

1.  对抗样本不足的一种潜在方法是以某种方式简化或概括数据，以尝试提取适当数量的簇，然后再回到完整数据集进行最终的全面聚类分析。

## 贝叶斯高斯混合模型 — 结果

![](../Images/1b7904fef6da253611770b634d724c61.png)

贝叶斯高斯混合模型的混淆矩阵 — 降维特征 — 图片由作者提供

## 最终结果比较

![](../Images/6de741d5d4023807dcda878d84a6807e.png)

每种聚类方法在特征颜色强度和OD280/OD315的稀释酒中的准确性 — 图片由作者提供

正如预期的那样，准确率远低于使用完整数据集时的准确率。然而，尽管如此，各种方法的准确率之间存在一些明显的差异。

让我们通过并排比较进一步探讨。

![](../Images/c5a4729c50f0f7fc725ad35a7efe9763.png)

对三种聚类方法中每种簇的分配进行比较 — 簇1（红色）/ 簇2（橙色/绿色）/ 簇3（灰色/蓝色） — 图片由作者提供

首先要注意的是，真实标签显示在某些情况下簇之间存在相当深的混合/交叉，因此100%的准确率是不可能的。

以下讨论的参考：

+   簇1 — 红色

+   簇2 — 橙色/绿色

+   簇3 — 灰色/蓝色

***K-means***

K-means在分离簇方面表现尚可，但由于分布必须最终限制为圆形，因此从未有准确捕捉簇2或簇3的希望。

然而，由于第3簇与其他两个簇有很好的分离，延长的形状成为的障碍较小。实际上，对下部簇的圆形表示就足够了，并且与高斯方法提供了类似的表示。

在考虑簇1和簇2时，K-Means方法未能充分表示数据。它固有地无法正确表示簇2的椭圆形状。这导致簇2在簇1和簇3之间被“压缩”，因为K-Means算法无法充分描述真实的向上扩展。

***高斯混合模型***

基本的高斯混合模型在这种情况下只是略有改进。

正如上一节对K均值算法的讨论，即使集群3的分布更适合椭圆形而非圆形分布，在这个特定场景中也没有优势。

然而，当查看集群1和集群2的表示时，由于集群2的底层椭圆分布（即更好地捕捉高斯分布的尾部）能够更准确地表示，从而导致准确性略微提高。

***贝叶斯高斯混合模型***

首先，贝叶斯高斯混合模型相比其他方法在准确性上的提高超过了10%，这作为一个单一统计数字确实令人印象深刻。

在审视置信椭球体时，变得清晰明了为何会如此。集群2在形状、倾斜度和大小方面几乎被完美表示。这使得实际集群的表示非常准确。

尽管确切原因总是略显模糊，但这无疑与标准高斯混合模型使用的期望最大化算法和贝叶斯高斯混合模型使用的变分推断之间的差异有关。

正如文章前面所讨论的，主要区别在于：

+   内置正则化

+   变分推断产生“边际正确”解决方案的倾向较小

# 结论

很明显，使用高斯混合模型可以帮助提升聚类的准确性，特别是对于那些可能以高斯分布为基础的数据。

这对于自然过程，包括人类过程尤其相关且有用，使得这种分析方法在大量行业中具有广泛的适用性。

此外，在贝叶斯高斯混合模型中引入变分推断，可以在几乎没有额外开销的情况下，进一步提高聚类的准确性。算法甚至有一个不容忽视的额外好处，即它能够建议底层数据的适当聚类数量。

我希望这篇文章能为你提供对高斯混合模型和贝叶斯高斯混合模型的良好理解，以及它们是否能帮助你处理的数据。

如果使用得当，它们确实是一种强大的工具。

如果你觉得这篇文章有趣或有用，记得关注我，或[订阅我的新闻通讯](https://medium.com/@maclayton/subscribe)，获取更多类似内容。

如果你还没有，你也可以考虑[订阅 Medium](https://medium.com/@maclayton/membership)。你的会员费用不仅直接支持我，还支持你阅读的其他作家。你还将获得对 Medium 上每个故事的全面无限制访问权限。

使用我的推荐链接注册将使我获得少量佣金，而不会对你的会员资格产生任何影响，所以如果你选择这样做，我会非常感谢。

[](https://medium.com/@maclayton/membership?source=post_page-----2ef8bb2d603f--------------------------------) [## 使用我的推荐链接加入Medium - Mike Clayton

### 阅读Mike Clayton的每一篇故事（以及Medium上成千上万其他作家的作品）。你的会员费用直接支持……

medium.com](https://medium.com/@maclayton/membership?source=post_page-----2ef8bb2d603f--------------------------------)

# 参考资料

[1] Riccardo Leardi, [Wine](https://archive-beta.ics.uci.edu/dataset/109/wine) (1991), UC Irvine机器学习库, 许可协议: [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/legalcode)
