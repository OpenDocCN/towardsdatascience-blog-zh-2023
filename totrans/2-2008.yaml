- en: The Curse of Dimensionality, Demystified
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-curse-of-dimensionality-demystified-2fc9b0bb1126](https://towardsdatascience.com/the-curse-of-dimensionality-demystified-2fc9b0bb1126)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understanding the mathematical intuition behind the curse of dimensionality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://reza-bagheri79.medium.com/?source=post_page-----2fc9b0bb1126--------------------------------)[![Reza
    Bagheri](../Images/7c5a7dc9e6e31048ce31c8d49055987c.png)](https://reza-bagheri79.medium.com/?source=post_page-----2fc9b0bb1126--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2fc9b0bb1126--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2fc9b0bb1126--------------------------------)
    [Reza Bagheri](https://reza-bagheri79.medium.com/?source=post_page-----2fc9b0bb1126--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2fc9b0bb1126--------------------------------)
    ·23 min read·Oct 6, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61dacdeae7a371398548e2bad19472fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [https://pixabay.com/illustrations/ancient-art-background-cosmos-dark-764930/](https://pixabay.com/illustrations/ancient-art-background-cosmos-dark-764930/)'
  prefs: []
  type: TYPE_NORMAL
- en: The *curse of dimensionality* refers to the problems that arise when analyzing
    high-dimensional data. The *dimensionality* or *dimension* of a dataset refers
    to the number of linearly independent features in that dataset, so a *high-dimensional*
    dataset is a dataset with a large number of features. This term was first coined
    by Bellman in 1961 when he observed that the number of samples required to estimate
    an arbitrary function with a certain accuracy grows exponentially with respect
    to the number of parameters that the function takes.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we take a detailed look at the mathematical problems that arise
    when analyzing a high-dimensional set. Though these problems may look counterintuitive,
    it is possible to erxpalin them intuitively. Instead of a purely theoretical discussion,
    we use Python to create and analyze high-dimensional datasets and see how the
    curse of dimensionality manifests itself in practice. *In this article, all images,
    unless otherwise noted, are by the author.*
  prefs: []
  type: TYPE_NORMAL
- en: '**Dimension of a dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, the dimension of a dataset is defined as the number of
    linearly independent features that it has. A linearly independent feature cannot
    be written as a linear combination of the features in that dataset. Hence, if
    a feature or column in a dataset is a linear combination of some other features,
    it won’t add to the dimension of that dataset. For example, Figure 1 shows two
    datasets. The first one has two linearly independent columns and its dimension
    is 2\. In the second dataset, one column is a multiple of another, hence we only
    have one independent feature. As the plot of this dataset shows, despite having
    two features, all the data points are along a 1-dimensional line. Hence the dimension
    of this dataset is one.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ee523475faa332e048de327c6fe310b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1
  prefs: []
  type: TYPE_NORMAL
- en: '**The effect of dimensionality on volume**'
  prefs: []
  type: TYPE_NORMAL
- en: The main reason for the curse of dimensionality is the effect of the dimension
    on volume. Here, we focus on the geometrical interpretation of a dataset. Generally,
    we can assume that a dataset is a random sample drawn from a population. For example,
    assume that our population is the set of points on a square shown in Figure 2\.
    This square is the set of all points (*x*₁, *x*₂) such that 0≤*x*₁≤1 and 0≤*x*₂≤1,
    and the side length of this square is one.
  prefs: []
  type: TYPE_NORMAL
- en: Each population has its own distribution, and here we assume that the points
    on this square are uniformly distributed. It means that if we try to randomly
    select a point from this population, all the points have an equal chance of being
    selected. Now, if we draw a random sample of size *n* from this population (which
    means randomly selecting *n* points from this square), these points form a dataset
    with *n* rows and two features. So, the dimension of this dataset is 2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05563aa0917fa4f78e1b857a610de8ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we can create a dataset with a dimension of 3 by drawing a random
    sample from a cube of edge length 1 (Figure 2). This cube is the set of all points
    (*x*₁, *x*₂, *x*₃) such that 0≤*x*₁≤1, 0≤*x*₂≤1, and 0≤*x*₃≤1, and the points
    on this cube are uniformly distributed.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can extend this idea and create a *d*-dimensional dataset by drawing
    a random sample of size *n* from a *d*-dimensional hypercube with an edge length
    of 1\. The hypercube is defined as the set of all points (*x*₁, *x*₂,…, *x*_*d*)
    such that 0≤*xᵢ*≤1 (for *i*=1…*d*). Again, we can assume that the points in this
    hypercube are uniformly distributed. The resulting dataset has *n* examples (observations)
    and *d* features.
  prefs: []
  type: TYPE_NORMAL
- en: The volume of a *d*-dimensional hypercube with an edge length of *L* is *L^d*.
    (please note that if *d*=2, this formula gives the area of a square with a side
    length of *L*. However, here we assume that area is a special case of volume for
    a 2-dimensional object). Hence, if *L*=1, the volume is 1 no matter what the value
    of *d* is. But what is important is not the total volume of the hypercube, but
    how volume is distributed inside the hypercube. Here we use an example to explain
    it.
  prefs: []
  type: TYPE_NORMAL
- en: We can divide a unit square into 10 shells as shown in Figure 3\. The first
    shell is indeed a small square at the center of the unit square and its edge length
    is 0.1\. The bottom-right corner of this square is located at *x*₁=0.55\. The
    bottom-right corner of the second shell is at *x*₁=0.6, so its thickness is 0.05\.
    All the remaining shells have the same thickness and the bottom-right corner of
    the outermost shell is at *x*₁=1\. Hence, it covers the sides of the unit square.
    Figure 3 shows one of these shells as an example whose bottom-right corner is
    at *x*₁=0.8.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf90bcb98ad1d97a3bb80728ec67ab50.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3
  prefs: []
  type: TYPE_NORMAL
- en: 'We can easily calculate the volume (area) of a shell whose bottom-right corner
    is located at *x*₁=*c*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e38b6a7fbd1dc6d9cc9aedbba675e2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we can extend this procedure to a *d*-dimensional hypercube. We divide the
    hypercube into 10 hypercubic shells with the same thickness.
  prefs: []
  type: TYPE_NORMAL
- en: 'The volume of a shell with one of its corners at *x*₁=*c* is determined by
    the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/85338605d33150377a6ddacee9ee810a.png)'
  prefs: []
  type: TYPE_IMG
- en: Listing 1 calculates the volume of all these shells for different values of
    *d* andcreates a bar plot of the volumes. The result is shown in Figure 4.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3fa0c41a2a21eca5abb97d9c10d914c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4
  prefs: []
  type: TYPE_NORMAL
- en: As you see as the number of dimensions increases, the share of the total volume
    for the inner shells decreases quickly. In fact, for *d≥*50, the outermost shell
    (whose corner is located at *x*₁=1) has more than 99% of the total volume of the
    unit hypercube. Hence, we conclude that in a high-dimensional hypercube, almost
    all of the volume is near the faces of the hypercube not inside it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s calculate the probability of finding a data point in each shell.
    Remember that we assumed that the points in this hypercube are uniformly distributed.
    The probability that a random data point is within a certain shell can be calculated
    by integrating the probability density function (PDF) of the uniform distribution
    over the volume of that shell. The PDF of a continuous uniform distribution over
    a unit hypercube is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed3843d964f94569d49e7110338837c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So the probability that a random data point lies within one of the shells (denoted
    by *S*) is simply equal to the volume of that shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a599abf83d92ad3e67cd565356e13064.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, based on the results of Figure 4, if we randomly sample a data point
    from the high-dimensional hypercube, there is a great chance that it will be in
    the outer shells. For *d*≥50, the probability of this random data point being
    in the inner shells is almost zero.
  prefs: []
  type: TYPE_NORMAL
- en: We can also confirm these results using Python. Listing 2 takes a sample of
    size 5000 from a uniform disitrubtion defined over a *d*-dimensional unit hypercube.
    Then it calculates the number of data points that belong to each shell and creates
    a bar plot of them. It uses the same values of *d* used in Listing 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/63ceec4bd4438b1b3e971e56f52afcb8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5
  prefs: []
  type: TYPE_NORMAL
- en: The plots are shown in Figure 5, and as you see the shape of the bar plots is
    similar to that of Figure 4\. The number of data points in each shell is proportional
    to the corresponding probability of that shell given in Equation 1, so they are
    proportional to the volume of that shell. In fact, when *d*=100, almost all of
    the data points are contained within the outermost shell, and this is a manifestation
    of the curse of dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: In a high-dimensional hypercube, almost all the observations of a random sample
    are near the faces (Figure 6). If the hypercube is *d*-dimensional, these observations
    can also represent a dataset with *d* features. Such a dataset is clearly not
    representative of the population that it was sampled from. Here the population
    is the set of all the data points in the hypercube, but our sample only contains
    the data points near the faces of the hypercube. In practice, we have no data
    points from the regions which are closer to the center of the hypercube. Suppose
    that we use such a sample as a training data set for a machine-learning model.
    The model never sees a data point that is near the center of the hypercube during
    training, so its prediction for such a data point won’t be reliable. In fact,
    the model cannot generalize its prediction to such data points.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c00cf2a85f58f40a1a3ab72d50dc1350.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also find a different interpretation for these results. Suppose that
    we have uniform disitrubtion defined on the interval [0,1]. This is like a 1-dimensional
    space. If we take a random sample of size 1 from it and denote it by *X*₁, then
    we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ef35f6c61c1832d7a8023c610c50195.png)![](../Images/f2ec1505d10c4ded47eb121f6c203a51.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, the probability that the *X*₁ is within 0.05 units of the interval’s edges
    is just 10%. Now suppose that we have a sample of size 1 from a 100-dimensional
    uniform distribution and denote it by *X*₁, *X*₂, … *X*₁₀₀. This is an IID (independent
    and identically distributed) sample which means that all *Xᵢ*s are independent
    and each *Xᵢ* has uniform disitrubtion on the interval [0,1]. So, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38c52f31c75b947cbb2327b24ba7cec6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e1a7aadbab1765c60d6168d70dd8a02f.png)'
  prefs: []
  type: TYPE_IMG
- en: As a result, the probability that at least one *Xᵢ* is within 0.05 units of
    one of the hypercube’s faces is almost 1 (and this means that the observation
    with that *Xᵢ* is within 0.05 units of that face). On average, in a sample of
    size 37594, we can find only one data point that is not in this region (1 / 2.66e-5
    ≈ 37594). This means that we need a very large sample to get just a few data points
    that are not near the faces of the hypercube.
  prefs: []
  type: TYPE_NORMAL
- en: As the number of dimensions (or features) in a dataset increases, the amount
    of observations needed to generalize the machine learning model accurately increases
    exponentially. The curse of dimensionality makes the training data set sparse,
    and generalizing the model’s predictions becomes more difficult. Hence, we need
    much more training data (more observations) to generalize the model. In reality,
    preparing such a big training data set may be impractical.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7 shows an analogy for the effect of the curse dimensionality on random
    sampling. Initially, we have a 2-dimensional shooting target and a novice shooter
    who randomly shoots at it. He has a chance of hitting the innermost ring. At the
    bottom, we have the same target which now has the curse of dimensionality. Almost
    all the targeting area belongs to the outermost ring, so the chance of hitting
    the innermost ring is almost zero.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df39c780e35649c85a39993feba96b6e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7
  prefs: []
  type: TYPE_NORMAL
- en: The effect of the curse of dimensionality on random sampling is not limited
    to a uniform distribution. In fact, it happens for any high-dimensional dataset
    regardless of its probability disitrubtion. Let’s see what happens if the features
    in a dataset have a normal disitrubtion. Let’s assume that all the features in
    our dataset are independent and have a standard normal disitrubtion. As a result,
    if we combine these features in a vector, that vector has a standard multivariate
    normal distribution. Figure 8 shows the PDF of a 2-dimensional standard multivariate
    normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/478ffc7733f0722984516eb7203529ff.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8
  prefs: []
  type: TYPE_NORMAL
- en: 'The PDF of the standard norma disitrubtion is the product of the PDF of its
    marginal distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d48abd9c07a60cae9708477face4697.png)'
  prefs: []
  type: TYPE_IMG
- en: where each *f*(*xᵢ*) is the PDF of a standard normal disitrubtion. Using this
    equation we can plot the PDF of the standard normal disitrubtion at higher dimensions.
    Listing 3 plots the PDF of a *d*-dimensional standard normal disitrubtion for
    3 values of *d* along one of the axes (*xᵢ*). Please note that the PDF has a symmetric
    shape, so the plot is the same for all *xᵢ* and for all lines that pass through
    the origin. Based on these plots, we conclude that the maximum value of the PDF
    is at the origin for any value of *d*, and it drops as we move away from the origin.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/880a2305cb0d4129894e07de0f5445a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4 takes a sample of size 5000 from a *d*-dimensional standard multivariate
    normal distribution. Then it calculates the Euclidian distance of each *d-*dimensional
    data point in this sample from the origin. Finally, it creates a histogram of
    these distances. It uses the same values of *d* used in Listing 1\. The result
    is shown in Figure 10.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ad5c3a2ff7fe4da7342959eb0b584888.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10
  prefs: []
  type: TYPE_NORMAL
- en: 'Here the number of data points in each bin of the histogram is proportional
    to the probability of finding a data point in that bin. As this figure shows,
    even when *d*=2, the peak of the histogram is not at zero. At the origin, the
    PDF has the maximum value (Figure 8), however, the probability also depends on
    the volume over which we take the integral:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/134ba93e7b7d834cb49e05816a5489f7.png)'
  prefs: []
  type: TYPE_IMG
- en: By increasing the dimensionality, the peak of the radial histogram moves further
    from the origin. So the majority of the data points lie in a thin ring, and the
    probability of finding a point anywhere else is almost zero. Please note that
    the maximum value of the PDF is still at the origin at any value of *d* (Figure
    9), however, the probability of obtaining a data point near the origin is almost
    zero. Again the dataset is not representative of the population that it was sampled
    from, and we have no data points from the regions that are very close to the origin
    and have a high PDF value.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the curse of dimensionality has nothing to do with
    the PDF of the probability disitrubtion of the dataset. For example, a uniform
    disitrubtion has the same value over its support no matter how many dimensions
    we have. However, since the probability depends on both the PDF and volume, it
    will be affected by the number of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '**The effect of dimensionality on distance functions**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dimensionality also has an important effect on distance. First, let’s see
    what we mean by distance. Let ***x*** be a *d-*dimensionalvector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7c0fb88ed925daec18f7013be5d1ac1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The *p*-norm of ***x*** is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa367bf0a342736b0ba642290ef02323.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can calculate the distance between the vectors ***x*** and ***y*** using
    *p*-norms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6bc5209a2f63ae3f1414f5ea102ee41a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Please note that there are other types of distance functions, but in this article,
    we only focus on this type. When *p*=2, we get the familiar Euclidian distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e1ac604e40df86e07c6c39042d5ddc9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the length of the vector ***x***-***y*** which is also denoted by ***||x-y||*
    (**we can use a *p*-norm without a subscript when *p*=2**)**. Now let’s see how
    dimensionality affects the distance function. Suppose that the dimensionality
    of our dataset is *d*. We pick one of the points in a data set as a query point
    and denote this point by the vector ***x***_*q*. We find the nearest and farthest
    data points to the query point and denote them by the vectors ***x***_*n* and
    ***x***_*f* respectively (Figure 11). Then we calculate the distance between the
    query point and the nearest and farthest data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5cef2cd988e2af670a18fce72617ef58.png)![](../Images/7b12743f6e4614180b2102cb2b70c900.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now it can be shown that under certain reasonable assumptions on the data distribution,
    we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df2941e4dc1b0e0787690736e52d940a.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, as dimensionality increases, the distance of the query point to the farthest
    data point approaches its distance to the nearest data point. Hence, differentiating
    the nearest data point from the other data points becomes impossible.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96da5563bc5d560c4db5aee2e3c1262c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 5 shows the effect of dimensionality on distance function. It draws
    a sample of size 500 from a uniform disitrubtion defined over a *d*-dimensional
    unit hypercube. Then it calculates the Euclidian distance between each pair of
    data points and plots a histogram of these distances. It uses the same values
    of *d* used in Listing 1\. Figure 12 shows these histograms. Please note that
    in a *d*-dimensional unit hypercube, the minimum possible Euclidian distance between
    two points is zero and the maximum possible distance is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/344e1ea40b8f87c08552ab2067c9932e.png)'
  prefs: []
  type: TYPE_IMG
- en: So, for each value of *d*, the limits of the *x*-axis of the histogram are 0
    and √*n*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2d1660125479e744b2eedce2a29c07a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12
  prefs: []
  type: TYPE_NORMAL
- en: As the dimensionality (*d*) increases, the histogram becomes sharper, so all
    pairwise distances lie within a narrow range. This indicates that for each data
    point in the dataset, DMAX (the distance to the farthest data point) approaches
    DMIN (the distance to the nearest data point).
  prefs: []
  type: TYPE_NORMAL
- en: Let me explain the intuition behind this effect. Figure 13 shows a query point
    (***x***_*q*) and its nearest (***x***_*n*) and farthest data points (***x***_*f).*
    In a 2-dimensional space, each of the vectors
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/608de6d4f3354b5df6375af64f4232ef.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f460e3bec9694dcd188cb4f8d0e1dde.png)'
  prefs: []
  type: TYPE_IMG
- en: have 2 components. The components of ***x***_*n-****x***_*q* are marked in blue
    and those of ***x***_*f-****x***_*q* are marked inred. Similarly, in a *d*-dimensional
    space, these vectors have *d* components.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a6efa42d16c9d2802e569fa6181ba6a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13
  prefs: []
  type: TYPE_NORMAL
- en: In a 2-dimensional space, to go from one point to another we should pass through
    2 dimensions. For example, in Figure 13, to go from ***x***_*q* to ***x***_*f*,
    we should move *l*₁ units to the right and *l*₂ units up. Please note that *l*₁
    and *l*₂ are the components of the vector ***x***_*f-****x***_*q*, and the distance
    between these points is the square root of *l*₁²+*l*₂². In a similar way, in a
    *d-*dimensional space, to go from ***x***_*q* to ***x***_*f*, we should pass through
    *d* dimensions. Now the vector ***x***_*f-****x***_*q* has *d* components. If
    the *i*-th component is denoted by *lᵢ*, then it means that we should move *lᵢ*
    units along the *i*-th dimension (Figure 13).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6 creates a sample of size 500 from a uniform disitrubtion defined over
    a *d*-dimensional unit hypercube. It then randomly picks a query data point (***x***_*q*)
    and finds its nearest (***x***_*n*) and farthest data point (***x***_*f*). Next,
    it calculates the components of the vectors ***x***_*n-****x***_*q* and***x***_*f-****x***_*q.*
    Finally, the absolute values of these components are plotted in two bar plots
    for *d*=2 and *d*=500\. The result is shown in Figure 14.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/84c3b318650e84b71e274ac18c15d587.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14
  prefs: []
  type: TYPE_NORMAL
- en: The length of the vector ***x***_*n-****x***_*q* gives the distance between
    ***x***_*q* and ***x***_*n*. Similarly, ||***x***_*f-****x***_*q||* givesthe distance
    between ***x***_*q* and ***x***_*f*. These distances are also shown in separate
    bar plots in Figure 14\. As this figure shows, in a 2-dimensional space, the absolute
    value of the components of ***x***_*n-****x***_*q* is much smaller than that of
    ***x***_*f-****x***_*q.* Hence the difference between ||***x***_*f-****x***_*q||*
    and ||***x***_*n-****x***_*q||* is noticable. In a 500-dimensional space, we have
    500 components. As you see, the absolute value of some of the components of ***x***_*n-****x***_*q*
    is even greater than those of***x***_*f-****x***_*q*. Remember that the length
    of a vector is obtained from the sum of the squares of these components. Hence,
    ||***x***_*f-****x***_*q||* and ||***x***_*n-****x***_*q||* are now much closer
    relatively.
  prefs: []
  type: TYPE_NORMAL
- en: We know that all the data points are chosen randomly, so the components of each
    data point are random numbers. When the dimensionality is high, to go from one
    point to another we should move through each dimension by a random distance (Figure
    13), and since we have many dimensions, the total distance between each pair of
    points is roughly the same. Hence the distance from the query point to any other
    points is roughly the same. In fact, as the dimensionality increases, the average
    distance between the points increases, but the relative difference between these
    distances decreases (Figure 14).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15 gives an analogy for this effect. Assume that the path between two
    places is made up of several segments. In a 1-dimensional space, we only have
    one segment. In a 2-dimensional space, we have 2 segments between each pair of
    points, and in a *d*-dimensional space, we have *d* segments between them. When
    we only have one segment, the path length gives the actual horizontal distance
    between two points. Hence we can easily distinguish the closer point (*n*) from
    a further point (*f*).
  prefs: []
  type: TYPE_NORMAL
- en: As the number of segments increases, the path length (which is the sum of the
    length of all segments) increases and diverges from the actual distance between
    the points. Now the horizontal distance between the points doesn’t affect the
    path that much. Instead, the vertical movements along the segments determine the
    path length. As the number of segments goes to infinity, the length of the path
    between *q* and *n* approaches the length of the path between *q* and *f*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/32d121eec4e5f41f2343308b03fb58f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15
  prefs: []
  type: TYPE_NORMAL
- en: A path with *d* segments is similar to the distance between two points in a
    *d*-dimensional space, and each segment can represent one of the components of
    the vector that connects those points in that space. As dimensionality increases,
    we have more segments (components), and the relative difference between the distances
    approaches zero.
  prefs: []
  type: TYPE_NORMAL
- en: '**The effect of dimensionality on learning models**'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in the previous section, in a high-dimensional space, the concept
    of proximity or distance is not meaningful anymore. Hence, any machine learning
    model that relies on the distance functions can break down in a high-dimensional
    space. In other words, when we have so many features in a training dataset, such
    models won’t give a reliable prediction anymore. One example is the *k-*NN (*k*
    Nearest Neighbors) algorithm. In this section, we will see how the curse of dimensionality
    affects the prediction error of a *k-*NN model (the examples are modified versions
    of those in [1]).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our dataset has *d* features and 1000 examples (observations). These examples
    are drawn from a uniform disitrubtion over a unit *d*-dimensional hypercube. Hence
    each example of this dataset can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3dafba6893dcc66f093c48a595868f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The target of this observation is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c13516a3158afc739cdba662a0984641.png)'
  prefs: []
  type: TYPE_IMG
- en: So, the target is simply the square of the first feature of each example. We
    use a *k*-NN model to predict the target for a test data point using this training
    dataset. To predict the target of a test data point, the *k*-NN model first finds
    the *k* nearest neighbors of that data point in the training data set (*k* nearest
    data point to the test point). It finds them using a distance function (Euclidian
    distance in this example). The predicted target of the test point is the average
    of the targets of these *k* nearest neighbors. In this example, we use the 3 nearest
    neighbors (*k*=3).
  prefs: []
  type: TYPE_NORMAL
- en: 'The test point is at the center of the hypercube:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e244802b77e6ca577383293ef85ca940.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We run 1000 simulations. In each simulation, we sample 1000 data points from
    a *d*-dimensional uniform distribution to create examples of the training dataset
    and calculate their target values. Then we train a *k-*NN model (with *k*=3) on
    this training dataset. Finally, we predict the target of the test data point (denoted
    by *y*^_*t*) using this model and compare it with the actual target which is 0.25\.
    Once we have the predicted target for all the simulations, we can calculate the
    bias, variance, and mean squared error (MSE) of these predictions. Bias is defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8df1889a84fb998f23ed36446e643686.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is the difference between the mean of the predicted target (*y*^_*t*) over
    these simulations and the actual target of the test point (*y*_*t*). The variance
    is the variance of the predicted targets over these simulations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77872b54f4e7b703d32cf9d23778c52a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The mean squared error (MSE) is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91d772e6230c670df79e940c7c1f900d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It can be shown that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/214a61453af31445fe84a52be054218b.png)'
  prefs: []
  type: TYPE_IMG
- en: Listing 7 calculates the bias, variance, and MSE for this example for different
    values of *d*. The results are plotted in Figure 16.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/93e6298bdbc2451a585fe973ea8ed8fa.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16
  prefs: []
  type: TYPE_NORMAL
- en: The *k*-NN model makes the assumption that close data points should also have
    close targets. However, in a high-dimensional dataset, all the data points are
    roughly at the same distance, so the *k* nearest neighbors can be very far from
    the test point. As a result, The average target of these neighbors is not an accurate
    prediction for the actual target of the test point anymore.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8 shows compares the *k*-NN prediction (*y*^_*t*) of all simulations
    for *d*=2 and *d*=10 (Figure 17). It also shows the actual target of the test
    point (*y_t*) with a black dashed line and the mean of all *k*-NN predictions
    with a blue line. Please note that the distance between the black dashed line
    and the blue line gives the bias.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3a309e8f2eacb4eb3fef91b6e3b90e69.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17
  prefs: []
  type: TYPE_NORMAL
- en: As dimensionality increases, the nearest neighbors will be near the faces of
    the hypercube, and very far from the test point at the center of the hypercube.
    The *x*₁ components of these points are not necessarily close to the *x*₁ of the
    test point and can vary a lot, hence the variance of the predictions quickly increases
    with *d.* However, on average they are close to *x*₁ of the test point, so the
    bias remains relatively small (Figure 16). The mean squared error (MSE) is proportional
    to both bias and variance, so it increases with dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 9 shows another example. Here the setup is similar to that of Listing
    7, however, the target of the dataset is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/405529ce5ed95c2bad3cb536eab29810.png)'
  prefs: []
  type: TYPE_IMG
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1f25252327599653e36fe3137615bd24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The test point is again at the center of the hypercube:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1e6a9a8ea917375d242269142c7c00f.png)'
  prefs: []
  type: TYPE_IMG
- en: The plots of bias, variance, and MSE for this example are shown in Figure 18\.
    Here the bias increases quickly with dimensionality and variance remains relatively
    small.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/19d32c989bdcc44647c446d0778e0e0a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18
  prefs: []
  type: TYPE_NORMAL
- en: Remember that as dimensionality increases the distance between each pair of
    points increases but the relative difference between these distances approaches
    zero. Hence, for all data points including the the nearest neighbors, ||***x***||
    increases and *y* goes to zero. So the average *y^* of the nearest neighbors diverges
    from the target of the test point. The result is an increase in bias and MSE,
    but the variance remains small.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, an increase in dimensionality increases MSE. However, its effect
    on bias or variance depends on how the target of the dataset is defined. These
    examples clearly show that *k*-NN is not a reliable model for high-dimensional
    data.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we discussed the curse of dimensionality and its effect on
    volume and distance functions. We saw that in a high-dimensional space, almost
    all of the data points will be far from the origin. Hence, if we use a high-dimensional
    dataset to train a model, the model’s prediction for a data point that is close
    to the origin won’t be reliable. In addition, the concept of distance is not meaningful
    anymore. In a high-dimensional dataset, the pairwise distance of all the data
    points is very close to each other. Hence the machine learning models that rely
    on distance functions won’t be able to give an accurate prediction for a high-dimensional
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Hastie, T., et al. The Elements of Statistical Learning. Springer, 2009.'
  prefs: []
  type: TYPE_NORMAL
