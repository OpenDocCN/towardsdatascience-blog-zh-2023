- en: Fine-Tuning Sentence Transformers with MNR Loss
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 MNR 损失微调句子变换器
- en: 原文：[https://towardsdatascience.com/fine-tuning-sentence-transformers-with-mnr-loss-cd6a26685b81](https://towardsdatascience.com/fine-tuning-sentence-transformers-with-mnr-loss-cd6a26685b81)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fine-tuning-sentence-transformers-with-mnr-loss-cd6a26685b81](https://towardsdatascience.com/fine-tuning-sentence-transformers-with-mnr-loss-cd6a26685b81)
- en: '[NLP For Semantic Search](https://jamescalam.medium.com/list/nlp-for-semantic-search-d3a4b96a52fe)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[语义搜索的 NLP](https://jamescalam.medium.com/list/nlp-for-semantic-search-d3a4b96a52fe)'
- en: Next-Gen Sentence Embeddings with Multiple Negatives Ranking Loss
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多负样本排名损失的下一代句子嵌入
- en: '[](https://jamescalam.medium.com/?source=post_page-----cd6a26685b81--------------------------------)[![James
    Briggs](../Images/cb34b7011748e4d8607b7ff4a8510a93.png)](https://jamescalam.medium.com/?source=post_page-----cd6a26685b81--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cd6a26685b81--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cd6a26685b81--------------------------------)
    [James Briggs](https://jamescalam.medium.com/?source=post_page-----cd6a26685b81--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jamescalam.medium.com/?source=post_page-----cd6a26685b81--------------------------------)[![James
    Briggs](../Images/cb34b7011748e4d8607b7ff4a8510a93.png)](https://jamescalam.medium.com/?source=post_page-----cd6a26685b81--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cd6a26685b81--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cd6a26685b81--------------------------------)
    [James Briggs](https://jamescalam.medium.com/?source=post_page-----cd6a26685b81--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cd6a26685b81--------------------------------)
    ·8 min read·Feb 3, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----cd6a26685b81--------------------------------)
    ·8分钟阅读·2023年2月3日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/1281f41ecb283e0989e5a4230b2c20f6.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1281f41ecb283e0989e5a4230b2c20f6.png)'
- en: Photo by [Lars Kienle](https://unsplash.com/@larskienle?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral). [Article
    originally published on pinecone.io](https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/)
    where the author is employed.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Lars Kienle](https://unsplash.com/@larskienle?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)。[文章最初发布于
    pinecone.io](https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/)，作者在该网站工作。
- en: Transformer-produced sentence embeddings have come a long way in a very short
    time. Starting with the slow but accurate similarity prediction of BERT cross-encoders,
    the world of [sentence embeddings](https://apps-showcase--optimistic-curran-b817a8.netlify.app/learn/sentence-embeddings/)
    was ignited with the introduction of SBERT in 2019 [1]. Since then, many more
    sentence transformers have been introduced. These models quickly made the original
    SBERT obsolete.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 生成的句子嵌入在很短的时间内取得了长足的进展。从 BERT 交叉编码器的缓慢但准确的相似度预测开始，句子嵌入的世界在 2019
    年随着 SBERT 的出现而被点燃[1]。自那时以来，许多新的句子变换器被引入，这些模型迅速使原始 SBERT 成为过时的技术。
- en: How did these newer sentence transformers manage to outperform SBERT so quickly?
    The answer is *multiple negatives ranking (MNR) loss*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这些新型句子变换器是如何如此迅速超越 SBERT 的？答案是*多负样本排名（MNR）损失*。
- en: This article will cover what MNR loss is, the data it requires, and how to implement
    it to fine-tune our own high-quality sentence transformers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将介绍 MNR 损失是什么，它所需的数据，以及如何实施以微调我们自己高质量的句子变换器。
- en: Implementation will cover two training approaches. The first is more involved
    and outlines the exact steps to fine-tune the model. The second approach makes
    use of the `sentence-transformers` library’s excellent utilities for fine-tuning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 实施将涵盖两种训练方法。第一种方法更为复杂，详细说明了微调模型的具体步骤。第二种方法则利用了`sentence-transformers`库出色的微调工具。
- en: NLI Training
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLI 训练
- en: Our article on [softmax loss](https://www.pinecone.io/learn/train-sentence-transformers-softmax/)
    explains that we can fine-tune sentence transformers using **N**atural **L**anguage
    **I**nference (NLI) datasets.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关于[softmax 损失](https://www.pinecone.io/learn/train-sentence-transformers-softmax/)的文章解释了如何使用**自然语言推理**（NLI）数据集来微调句子变换器。
- en: 'These datasets contain many sentence pairs, some that *imply* each other and
    others that *do not imply* each other. As with the softmax loss article, we will
    use two datasets: the Stanford Natural Language Inference (SNLI) and Multi-Genre
    NLI (MNLI) corpora.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据集包含许多句子对，有些句子对*暗示*彼此，而其他句子对*不暗示*彼此。与softmax损失文章一样，我们将使用两个数据集：斯坦福自然语言推理（SNLI）和多领域NLI（MNLI）语料库。
- en: 'These two corpora total 943K sentence pairs. Each pair consists of a `premise`
    and `hypothesis` sentence, which are assigned a `label`:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个语料库总共有943K句子对。每对包括一个`premise`和一个`hypothesis`句子，并且被分配一个`label`：
- en: '**0** — *entailment*, e.g., the `premise` suggests the `hypothesis`.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**0** — *蕴涵*，例如，`premise`暗示了`hypothesis`。'
- en: '**1** — *neutral*, the `premise` and `hypothesis` could both be true, but they
    are not necessarily related.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1** — *中立*，`premise`和`hypothesis`可能都是真的，但它们不一定相关。'
- en: '**2** — *contradiction*, the `premise` and `hypothesis` contradict each other.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2** — *矛盾*，`premise`和`hypothesis`相互矛盾。'
- en: When fine-tuning with MNR loss, we will drop all rows with *neutral* or *contradiction*
    labels — keeping only the positive *entailment* pairs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用MNR损失进行微调时，我们将丢弃所有标记为*中立*或*矛盾*的行——仅保留正样本的*蕴涵*对。
- en: We will feed sentence A (the `premise`, known as the *anchor*) followed by sentence
    B (the `hypothesis`, when the label is **0**, this is called the *positive*) into
    BERT on each step. Unlike softmax loss, we do not use the `label` feature.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每一步将句子A（`premise`，称为*锚点*）和句子B（`hypothesis`，当标签为**0**时，这称为*正样本*）输入BERT。与softmax损失不同，我们不使用`label`特征。
- en: These training steps are performed in batches. Meaning several anchor-positive
    pairs are processed at once.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些训练步骤是分批进行的。这意味着同时处理多个锚点-正样本对。
- en: The model is then optimized to produce similar embeddings between pairs while
    maintaining different embeddings for non-pairs. We will explain this in more depth
    soon.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后对模型进行优化，以在对之间生成相似的嵌入，同时对非对保持不同的嵌入。我们会很快更深入地解释这一点。
- en: Data Preparation
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: Let’s look at the data preparation process. We first need to download and merge
    the two NLI datasets. We will use the `datasets` library from Hugging Face.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看数据准备过程。我们首先需要下载并合并两个NLI数据集。我们将使用Hugging Face的`datasets`库。
- en: Because we are using MNR loss, we only want anchor-positive pairs. We can apply
    a filter to remove all other pairs (including erroneous `-1` labels).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是MNR损失，我们只需要锚点-正样本对。我们可以应用过滤器来移除所有其他对（包括错误的`-1`标签）。
- en: The dataset is now prepared differently depending on our training method. We
    will continue preparation for the more involved PyTorch approach. If you’d rather
    just train a model and care less about the steps involved, feel free to skip ahead
    to the next section.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的准备方式现在取决于我们的训练方法。我们将继续准备更复杂的PyTorch方法。如果你更愿意直接训练模型而不关心涉及的步骤，可以跳过到下一部分。
- en: For the PyTorch approach, we must tokenize our own data. To do that, we will
    be using a `BertTokenizer` from the `transformers` library and applying the `map`
    method on our `dataset`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于PyTorch方法，我们必须对自己的数据进行分词。为此，我们将使用`transformers`库中的`BertTokenizer`并在我们的`dataset`上应用`map`方法。
- en: After that, we’re ready to initialize our `DataLoader`, which will be used to
    load data batches into our model during training.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，我们就准备初始化我们的`DataLoader`，它将在训练期间用于将数据批次加载到模型中。
- en: And with that, our data is ready. Let’s move on to training.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们的数据就准备好了。让我们继续训练。
- en: PyTorch Fine-Tuning
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch微调
- en: When training SBERT models, we don’t start from scratch. Instead, we begin with
    an already pretrained BERT — all we need to do is *fine-tune* it for building
    sentence embeddings.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 训练SBERT模型时，我们不是从零开始，而是从已经预训练的BERT开始——我们只需*微调*它来构建句子嵌入。
- en: MNR and softmax loss training approaches use a * ‘siamese’*-BERT architecture
    during fine-tuning. Meaning that during each step, we process a *sentence A* (our
    *anchor*) into BERT, followed by *sentence B* (our *positive*).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: MNR和softmax损失训练方法在微调过程中使用*‘siamese’*–BERT架构。这意味着在每一步中，我们将*句子A*（我们的*锚点*）输入BERT，然后是*句子B*（我们的*正样本*）。
- en: '![](../Images/6a40444126cf3a5ef5105b792e645664.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a40444126cf3a5ef5105b792e645664.png)'
- en: Siamese-BERT network, the *anchor* and *positive* sentence pairs are processed
    separately. A mean pooling layer converts token embeddings into sentence embeddings.
    Sentence A is our *anchor* and sentence B the *positive*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Siamese-BERT网络中，*锚点*和*正样本*句子对被分别处理。一个均值池化层将token嵌入转换为句子嵌入。句子A是我们的*锚点*，句子B是*正样本*。
- en: Because these two sentences are processed *separately*, it creates a *siamese*-like
    network with two identical BERTs trained in parallel. In reality, there is only
    a single BERT being used twice in each step.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这两个句子是*分别*处理的，所以它创建了一个类似*孪生*的网络，两个相同的 BERT 并行训练。实际上，每一步只使用一个 BERT。
- en: We can extend this further with *triplet*-networks. In the case of triplet networks
    for MNR, we would pass three sentences, an *anchor*, it’s *positive*, and it’s
    *negative*. However, we are *not* using triplet-networks, so we have removed the
    negative rows from our dataset (rows where `label` is `2`).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步扩展到*三元组*网络。对于 MNR 的三元组网络，我们会传递三个句子，一个*锚点*，一个*正样本*和一个*负样本*。然而，我们*没有*使用三元组网络，因此我们已从数据集中移除了负样本行（`label`
    为 `2` 的行）。
- en: '![](../Images/da2d651eb4d13417cfdb094d6d663e13.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da2d651eb4d13417cfdb094d6d663e13.png)'
- en: Triplet networks use the same logic but with an added sentence. For MNR loss
    this other sentence is the *negative* pair of the *anchor*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Triplet 网络使用相同的逻辑，但增加了一个句子。对于 MNR 损失，这个句子是*负样本*对的*锚点*。
- en: BERT outputs 512 768-dimensional embeddings. We convert these into *averaged*
    sentence embeddings using *mean-pooling*. Using the siamese approach, we produce
    two of these per step — one for the *anchor* that we will call `a`, and another
    for the *positive* called `p`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 输出 512 个 768 维的嵌入。我们通过*均值池化*将这些嵌入转换为*平均*句子嵌入。使用孪生网络方法，每一步生成两个这样的嵌入——一个是我们称之为`a`的*锚点*，另一个是称之为`p`的*正样本*。
- en: In the `mean_pool` function, we’re taking these token-level embeddings (the
    512) and the sentence `attention_mask` tensor. We resize the `attention_mask`
    to match the higher `768`-dimensionality of the token embeddings.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在`mean_pool`函数中，我们取这些 token 级别的嵌入（512）和句子 `attention_mask` 张量。我们调整`attention_mask`以匹配
    token 嵌入的更高 `768` 维度。
- en: The resized mask `in_mask` is applied to the token embeddings to exclude padding
    tokens from the mean pooling operation. Mean-pooling takes the average activation
    of values across each dimension but *excluding* those padding values, which would
    reduce the average activation. This operation transformers our token-level embeddings
    (shape `512*768`) to sentence-level embeddings (shape `1*768`).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的掩码`in_mask`应用于 token 嵌入，以排除均值池化操作中的填充 token。均值池化计算每个维度的值的平均激活，但*排除*那些填充值，以免减少平均激活。此操作将我们的
    token 级别嵌入（形状为`512*768`）转换为句子级别嵌入（形状为`1*768`）。
- en: These steps are performed in *batches*, meaning we do this for many *(anchor,
    positive)* pairs in parallel. That is important in our next few steps.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤是在*批次*中执行的，即我们同时处理许多（*锚点，正样本*）对。这在接下来的步骤中很重要。
- en: First, we calculate the cosine similarity between each anchor embedding (`a`)
    and *all* of the positive embeddings in the same batch (`p`).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们计算每个锚点嵌入（`a`）与同一批次中*所有*正样本嵌入（`p`）之间的余弦相似度。
- en: From here, we produce a vector of cosine similarity scores (of size `batch_size`)
    for each anchor embedding `a_i` *(or size* `*2 * batch_size*` *for triplets)*.
    Each anchor should share the highest score with its positive pair, `p_i`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们为每个锚点嵌入`a_i`生成一个余弦相似度分数向量（大小为`batch_size`）（或大小为`*2 * batch_size*`的*三元组*）。每个锚点应与其正样本`p_i`共享最高分数。
- en: '![](../Images/5730faa4fa2216092158c79e73367056.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5730faa4fa2216092158c79e73367056.png)'
- en: Cosine similarity scores using five pairs/triples in a triplet network (with
    `(a, p, n)`). A siamese network is the same but excludes the dark blue `n` blocks
    (`n`).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用五对/三元组的三元组网络进行余弦相似度评分（使用 `(a, p, n)`）。孪生网络相同，但排除了深蓝色的 `n` 块（`n`）。
- en: To optimize for this, we use a set of increasing label values to mark where
    the highest score should be for each `a_i`, and categorical [cross-entropy loss](https://apps-showcase--optimistic-curran-b817a8.netlify.app/learn/cross-entropy-loss/).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化，我们使用一组递增的标签值来标记每个`a_i`的最高分数位置，并使用分类 [交叉熵损失](https://apps-showcase--optimistic-curran-b817a8.netlify.app/learn/cross-entropy-loss/)。
- en: And that’s every component we need for fine-tuning with MNR loss. Let’s put
    that all together and set up a training loop. First, we move our model and layers
    to a CUDA-enabled GPU *if available*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们用于 MNR 损失微调所需的所有组件。让我们把这些整合起来，设置一个训练循环。首先，我们将模型和层移动到支持 CUDA 的 GPU *（如果有的话）*。
- en: Then we set up the optimizer and schedule for training. We use an Adam optimizer
    with a linear warmup for 10% of the total number of steps.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们设置优化器和训练计划。我们使用 Adam 优化器，并对总步数的 10% 进行线性预热。
- en: And now, we define the training loop using the same training process that we
    worked through before.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用相同的训练过程定义训练循环。
- en: With that, we’ve fine-tuned our BERT model using MNR loss. Now we save it to
    file.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，我们使用MNR损失微调了我们的BERT模型。现在我们将其保存到文件中。
- en: And this can now be loaded using either the `SentenceTransformer` or HF `from_pretrained`
    methods. Before we move on to testing the model performance, let’s look at how
    we can replicate that fine-tuning logic using the *much simpler* `sentence-transformers`
    library.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以使用`SentenceTransformer`或HF `from_pretrained`方法加载它。在我们开始测试模型性能之前，让我们看看如何使用*更简单的*
    `sentence-transformers`库来复制微调逻辑。
- en: Fast Fine-Tuning
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速微调
- en: As we already mentioned, there is an easier way to fine-tune models using MNR
    loss. The `sentence-transformers` library allows us to use pretrained sentence
    transformers and comes with some handy training utilities.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，使用MNR损失微调模型有一个更简单的方法。`sentence-transformers`库允许我们使用预训练的句子变换器，并附带一些方便的训练工具。
- en: We will start by preprocessing our data. This is the same as we did before for
    the first few steps.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从预处理数据开始。这与我们之前进行的前几步相同。
- en: Before, we tokenized our data and then loaded it into a PyTorch `DataLoader`.
    This time we follow a *slightly different format*. We * don’t* tokenize; we reformat
    into a list of `sentence-transformers` `InputExample` objects and use a slightly
    different `DataLoader`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们将数据标记化，然后加载到PyTorch `DataLoader`中。这次我们遵循*稍微不同的格式*。我们*不*标记化；我们重新格式化为`sentence-transformers`
    `InputExample`对象的列表，并使用略有不同的`DataLoader`。
- en: Our `InputExample` contains just our `a` and `p` sentence pairs, which we then
    feed into the `NoDuplicatesDataLoader` object. This data loader ensures that each
    batch is duplicate-free — a helpful feature when ranking pair similarity across
    *randomly* sampled pairs with MNR loss.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`InputExample`仅包含我们的`a`和`p`句子对，然后将其输入到`NoDuplicatesDataLoader`对象中。该数据加载器确保每个批次没有重复——这是在使用MNR损失对*随机*抽样对进行排名时的一个有用功能。
- en: Now we define the model. The `sentence-transformers` library allows us to build
    models using *modules*. We need just a transformer model (we will use `bert-base-uncased`
    again) and a mean pooling module.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义模型。`sentence-transformers` 库允许我们使用*模块*来构建模型。我们只需要一个变换器模型（我们将再次使用`bert-base-uncased`）和一个平均池化模块。
- en: We now have an initialized model. Before training, all that’s left is the loss
    function — MNR loss.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了一个初始化的模型。在训练之前，剩下的就是损失函数——MNR损失。
- en: And with that, we have our data loader, model, and loss function ready. All
    that’s left is to fine-tune the model! As before, we will train for a single epoch
    and warmup for the first 10% of our training steps.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们的数据加载器、模型和损失函数就准备好了。剩下的就是微调模型！与之前一样，我们将训练一个周期，并在训练步骤的前10%进行热身。
- en: And a couple of hours later, we have a new sentence transformer model trained
    using MNR loss. It goes without saying that using the `sentence-transformers`
    training utilities makes life *much easier*. To finish off the article, let’s
    look at the performance of our MNR loss SBERT next to other sentence transformers.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 几个小时后，我们有了一个使用MNR损失训练的新句子变换器模型。毫无疑问，使用`sentence-transformers`训练工具使生活*轻松得多*。为了结束这篇文章，让我们看看我们的MNR损失SBERT与其他句子变换器的性能。
- en: Compare Sentence Transformers
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较句子变换器
- en: We’re going to use a semantic textual similarity (STS) dataset to test the performance
    of *four models*; our *MNR loss* SBERT (using PyTorch and `sentence-transformers`),
    the *original* SBERT, and an MPNet model trained with MNR loss on a [1B+ sample
    dataset](https://huggingface.co/spaces/flax-sentence-embeddings/sentence-embeddings).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用语义文本相似性（STS）数据集来测试*四个模型*的性能；我们的*MNR损失* SBERT（使用PyTorch和`sentence-transformers`）、*原始*
    SBERT，以及一个在[1B+样本数据集](https://huggingface.co/spaces/flax-sentence-embeddings/sentence-embeddings)上用MNR损失训练的MPNet模型。
- en: The first thing we need to do is download the STS dataset. Again we will use
    `datasets` from Hugging Face.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们需要下载STS数据集。我们将再次使用Hugging Face的`datasets`。
- en: STSb (or STS benchmark) contains sentence pairs in features `sentence1` and
    `sentence2` assigned a similarity score from *0 -> 5*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: STSb（或STS基准）包含特征`sentence1`和`sentence2`中的句子对，并分配了从*0 -> 5*的相似性分数。
- en: Three samples from the validation set of STSb.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: STSb验证集中的三个样本。
- en: Because the similarity scores range from 0 -> 5, we need to normalize them to
    a range of 0 -> 1\. We use `map` to do this.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于相似性分数范围为0 -> 5，我们需要将其归一化到0 -> 1的范围。我们使用`map`来完成这项工作。
- en: We’re going to be using `sentence-transformers` evaluation utilities. We first
    need to reformat the STSb data using the `InputExample` class — passing the sentence
    features as `texts` and similarity scores to the `label` argument.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`sentence-transformers`的评估工具。我们首先需要使用`InputExample`类重新格式化STSb数据——将句子特征作为`texts`，将相似度分数传递给`label`参数。
- en: To evaluate the models, we need to initialize the appropriate evaluator object.
    As we are evaluating continuous similarity scores, we use the `EmbeddingSimilarityEvaluator`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要评估模型，我们需要初始化适当的评估对象。由于我们在评估连续相似度分数，我们使用`EmbeddingSimilarityEvaluator`。
- en: And with that, we’re ready to begin evaluation. We load our model as a `SentenceTransformer`
    object and pass the model to our `evaluator`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就准备好开始评估了。我们将模型加载为`SentenceTransformer`对象，并将模型传递给我们的`evaluator`。
- en: The evaluator outputs the * Spearman’s rank correlation* between the cosine
    similarity scores calculated from the model’s output embeddings and the similarity
    scores provided in STSb. A high correlation between the two values outputs a value
    close to *+1*, and no correlation would output *0*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 评估器输出*斯皮尔曼等级相关系数*，用于计算模型输出嵌入与STSb提供的相似度分数之间的余弦相似度。两个值之间的高相关性输出接近*+1*，没有相关性则输出*0*。
- en: For the model fine-tuned with `sentence-transformers`, we output a correlation
    of *0.84*, meaning our model outputs good similarity scores according to the scores
    assigned to STSb. Let’s compare that with other models.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用`sentence-transformers`微调的模型，我们输出了*0.84*的相关性，这意味着我们的模型根据STSb分配的分数输出了良好的相似度分数。让我们将其与其他模型进行比较。
- en: The top two models are trained using MNR loss, followed by the original SBERT.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 排名前两的模型使用MNR损失训练，其次是原始SBERT。
- en: These results support the advice given by the authors of `sentence-transformers`,
    that models trained with MNR loss outperform those trained with softmax loss in
    building high-performing sentence embeddings [2].
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果支持了`sentence-transformers`作者给出的建议，即使用MNR损失训练的模型在构建高性能句子嵌入方面优于使用softmax损失训练的模型[2]。
- en: Another key takeaway here is that despite our best efforts and the complexity
    of building these models with PyTorch, *every* model trained using the easy-to-use
    `sentence-transformers` utilities far outperformed them.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键点是，尽管我们尽了最大努力，并且构建这些PyTorch模型的复杂性很高，但*每个*使用易用的`sentence-transformers`工具训练的模型都远远超越了它们。
- en: In short, fine-tune your models with MNR loss, and do it with the `sentence-transformers`
    library.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，用MNR损失微调你的模型，并使用`sentence-transformers`库进行微调。
- en: That’s it for this walkthrough and guide to fine-tuning sentence transformer
    models with multiple negatives ranking loss — the current best approach for building
    high-performance models.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是本次关于使用多负样本排名损失微调句子变换器模型的教程和指南——当前构建高性能模型的最佳方法。
- en: We covered preprocessing the two most popular NLI datasets — the Stanford NLI
    and multi-genre NLI corpora — for fine-tuning with MNR loss. Then we delved into
    the details of this fine-tuning approach using PyTorch before taking advantage
    of the excellent training utilities provided by the `sentence-transformers` library.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先处理了两个最受欢迎的NLI数据集——斯坦福NLI和多类别NLI语料库——以便使用MNR损失进行微调。随后，我们深入探讨了使用PyTorch进行这种微调方法的细节，然后利用了`sentence-transformers`库提供的出色训练工具。
- en: Finally, we learned how to evaluate our sentence transformer models with the
    semantic textual similarity benchmark (STSb) — identifying the highest-performing
    models.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们学习了如何使用语义文本相似性基准（STSb）评估我们的句子变换器模型——识别表现最佳的模型。
- en: References
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] N. Reimers, I. Gurevych, [Sentence-BERT: Sentence Embeddings using Siamese
    BERT-Networks](https://arxiv.org/abs/1908.10084) (2019), ACL'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] N. Reimers, I. Gurevych, [Sentence-BERT: Sentence Embeddings using Siamese
    BERT-Networks](https://arxiv.org/abs/1908.10084) (2019), ACL'
- en: '[2] N. Reimers, [Sentence Transformers NLI Training Readme](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/nli),
    GitHub'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] N. Reimers, [Sentence Transformers NLI Training Readme](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/nli),
    GitHub'
