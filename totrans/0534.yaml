- en: Common AB testing mistakes. Vol 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/common-ab-testing-mistakes-vol-2-3f3040a65e8b](https://towardsdatascience.com/common-ab-testing-mistakes-vol-2-3f3040a65e8b)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s learn from our mistakes!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://markeltsefon.medium.com/?source=post_page-----3f3040a65e8b--------------------------------)[![Mark
    Eltsefon](../Images/5ab4cccd496f73cd155bbb253f85ec4d.png)](https://markeltsefon.medium.com/?source=post_page-----3f3040a65e8b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3f3040a65e8b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3f3040a65e8b--------------------------------)
    [Mark Eltsefon](https://markeltsefon.medium.com/?source=post_page-----3f3040a65e8b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3f3040a65e8b--------------------------------)
    ·4 min read·Apr 13, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: A year ago, I published an [article](https://medium.com/towards-data-science/common-mistakes-during-a-b-testing-bdb9eefdc7f0)
    discussing common mistakes during AB testing. It appears that many people are
    keenly interested in experimentation challenges and overcoming them. As such,
    I've decided to publish an article on the next three common mistakes that people
    make.
  prefs: []
  type: TYPE_NORMAL
- en: By avoiding these common mistakes, we can ensure that our experiments are reliable,
    valid, and informative, ultimately leading to better decision-making and more
    successful outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5fcc613a06f202706f62e105f24a3f48.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Santa Barbara](https://unsplash.com/@barbaris778?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Multiple the required sample size by number of hypotheses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a well-known formula to calculate the sample size.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ba42fbf5eeb041899aaaf4c0c4d48d4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: It takes into account the variance of the metric, significance level, power
    of test, and MDE (minimum detectable effect) .
  prefs: []
  type: TYPE_NORMAL
- en: However, when conducting multiple hypotheses testing, people often make the
    mistake of simply replacing the number “2” with the number of groups
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d08c0aa9da7f69dcbe858f40174af26d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Is it the right approach? Not exactly. Increased number of hypotheses leads
    to an inflated type I error rate, therefore we need to control the type I error
    and therefore significance level. To control it, the Bonferroni correction is
    commonly used. The main idea is dividing the type I error by the number of hypotheses.
  prefs: []
  type: TYPE_NORMAL
- en: Every comparison between groups should be considered as a separate hypothesis,
    rather than just every group.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, when there are 4 groups, for instance, the number of hypotheses is
    6, which is the number of possible combinations of groups.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Our correct formula is :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3157bd60386fddb9a31ba5975e0fd944.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Let’s compare the wrong approach and the right one.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when MDE is 0.1, significance level is 0.05, power of test is 0.8,
    and variance is 1.5, the wrong approach would require 7064 samples, whereas the
    correct approach would require 10899 samples.
  prefs: []
  type: TYPE_NORMAL
- en: Concluding an AB test after 7064 samples can result in an erroneous decision.
  prefs: []
  type: TYPE_NORMAL
- en: Not running health checks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most people rush into conducting AB tests without first performing health checks.
    Health checks can ensure that the testing environment is stable and unbiased.
    If the testing environment is unstable or biased, the test results may be invalid
    and unreliable.
  prefs: []
  type: TYPE_NORMAL
- en: A/A testing on historical data is an example of such checks. When running an
    A/A test, it’s crucial to observe the distribution of p-values, rather than focusing
    on a single number, since finding a difference between control and treatment groups
    is always a possibility.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The approach is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Select a sample size. You should choose the same formula and similar values
    to those used in real A/B tests.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create control and treatment groups. It’s essential to use the same splitting
    algorithm used in the production system, just applied to historical data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Measure the results: Measure the results for both groups. Calculate the desired
    metric.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Analyze the results: Compare the results for both groups to ensure that they
    are statistically similar. This can be done by calculating the p-value.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the steps 1–5 at least a thousand times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upon repeating the A/A test multiple times, examine the distribution of obtained
    p-values. The distribution should be uniform. If not, it indicates that your health
    check is incomplete and further analysis is required.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/09e42b0fc560030f6dab35b9d952e610.png)'
  prefs: []
  type: TYPE_IMG
- en: Uniform P value distribution. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: To be indifferent about the negative results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In fact, ignoring negative results can have serious consequences for a business’s
    bottom line.
  prefs: []
  type: TYPE_NORMAL
- en: First and foremost, negative results can provide valuable information about
    what doesn’t work. It’s easy to get excited about the positive results of an AB
    test, but negative results are equally important. They can reveal flaws in a design
    or strategy, highlighting areas that need improvement or further exploration.
    If a business ignores negative results and simply goes with the option that performed
    better in the test, they may be missing out on opportunities to make meaningful
    improvements.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, negative results can be a warning sign that something isn’t working
    as intended. For example, if an AB test reveals that a new design change actually
    performs worse than the previous version, it could indicate a deeper issue with
    the design process or user experience. Ignoring negative results in this scenario
    could lead to a decline in user engagement, customer loyalty, and ultimately,
    revenue.
  prefs: []
  type: TYPE_NORMAL
- en: '**Thanks for reading and don’t be afraid to make mistakes and learn. It’s the
    only way to progress!**'
  prefs: []
  type: TYPE_NORMAL
