- en: 'PCA/LDA/ICA : a components analysis algorithms comparison'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/pca-lda-ica-a-components-analysis-algorithms-comparison-c5762c4148ff](https://towardsdatascience.com/pca-lda-ica-a-components-analysis-algorithms-comparison-c5762c4148ff)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Review the concepts and differences between these famous algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mocquin.medium.com/?source=post_page-----c5762c4148ff--------------------------------)[![Yoann
    Mocquin](../Images/b30a0f70c56972aabd2bc0a74baa90bb.png)](https://mocquin.medium.com/?source=post_page-----c5762c4148ff--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c5762c4148ff--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c5762c4148ff--------------------------------)
    [Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----c5762c4148ff--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c5762c4148ff--------------------------------)
    ·8 min read·Feb 19, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Before diving and comparing the algorithms, let’s review them independently.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note that is article does not aim at explaining each algorithm in depth, but
    rather compare their goals and results.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to know more about the difference between PCA and ZCA, check out
    my previous post based on numpy :'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/pca-whitening-vs-zca-whitening-a-numpy-2d-visual-518b32033edf?source=post_page-----c5762c4148ff--------------------------------)
    [## PCA-whitening vs ZCA-whitening : a numpy 2d visual'
  prefs: []
  type: TYPE_NORMAL
- en: The process of whitening data consists in a transformation such that the transformed
    data has identity matrix as…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/pca-whitening-vs-zca-whitening-a-numpy-2d-visual-518b32033edf?source=post_page-----c5762c4148ff--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA : Principal Component Analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PCA is an unsupervised linear dimensionality reduction technique that aims to
    find a new set of orthogonal variables that captures the most important sources
    of variability in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is widely used for feature extraction and data compression, and can be used
    for exploratory data analysis or as a preprocessing step for machine learning
    algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The resulting components are ranked by the amount of variance they explain,
    and can be used to visualize and interpret the data, as well as for clustering
    or classification tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LDA : Linear Discriminant Analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LDA is a supervised linear dimensionality reduction technique that aims to find
    a new set of variables that maximizes the separation between classes while minimizing
    the variation within each class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is widely used for feature extraction and classification, and can be used
    to reduce the dimensionality of the data while preserving the discriminative information
    between classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The resulting components are ranked by their discriminative power, and can be
    used to visualize and interpret the data, as well as for classification or regression
    tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ICA : Independent Component Analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ICA is an unsupervised linear dimensionality reduction technique that aims to
    find a new set of variables that are statistically independent and non-Gaussian.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is widely used for signal processing and source separation, and can be used
    to extract underlying sources of variability in the data that are not accessible
    through other techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The resulting components are ranked by their independence, and can be used to
    visualize and interpret the data, as well as for clustering or classification
    tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Results on the iris dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s compare their results on the famous iris dataset using sklearn. First
    let’s plot the iris dataset using a pairplot on each of the 4 numerical features,
    and color as the categorical feature :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e21a5a91b7240a59fc855aecd97a9f20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by author : the iris dataset pairplot'
  prefs: []
  type: TYPE_NORMAL
- en: We can now compute each transformation and plot the results. Notice we use only
    2 components, since LDA requires at most (N-1) components where N is the number
    of categories (here equal to 3 since there are 3 types of iris flowers).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This code loads the Iris dataset, applies LDA, PCA, and ICA with 2 components
    each, and then plots the results using different colors for each class.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that it is generally a good practice to standardize the data before applying
    PCA, ICA, or LDA. Standardization is important because these techniques are sensitive
    to the scale of the input features. Standardizing the data ensures that each feature
    has a mean of zero and a standard deviation of one, which puts all the features
    on the same scale and avoids one feature dominating over the others.
  prefs: []
  type: TYPE_NORMAL
- en: Since LDA is a supervised dimensionality reduction technique, it takes class
    labels as input. In contrast, PCA and ICA are unsupervised techniques, meaning
    that they only use the input data and do not take class labels into account.
  prefs: []
  type: TYPE_NORMAL
- en: The results of LDA can be interpreted as a projection of the data onto a space
    that maximizes class separation, whereas the results of PCA and ICA can be interpreted
    as a projection of the data onto a space that captures the most important sources
    of variability or independence, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c694162272d0a7c305fdff2a639d80b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by author : Comparison of LDA, PCA and ICA on iris dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that ICA still shows separation between the category, althought not
    its purpose : that’s because the categories are already quite sorted in the input
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s put the LDA aside and focus on the differences between PCA and ICA- since
    LDA is a supervised technique, focuses on separating categories and enforces a
    maximum of component, while PCA and ICA focus on creating a new matrix with the
    same shape as the input matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the ouputs for 4 components, both for PCA and ICA :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8ebe69b93f0e70c3343ba2e7e8c2e2b.png)![](../Images/662ce9b0c5d9e375135f26d2ce21867b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Left : pairplot of PCA / Right : pairplot of ICA (images by author)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also compare the correlation matrix for each transformed data : notice
    that both methods result in uncorrelated vectors (in other words, the transformed
    data features are orthogonal). That is because it’s a constraint in the PCA algorithm
    — each new vector must be orthogonal to the previous ones-, and a consequence
    of the ICA algorithm — which implies that the original dataset are independent
    signals that have been mixed together and must be reconstructed.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9fa1ad50c63f0f26fc02699da4374e2d.png)![](../Images/febab233a24837aac0ef85e4019273a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Left : correlation heatmap of ICA / Right : correlation heatmap of PCA (images
    by author)'
  prefs: []
  type: TYPE_NORMAL
- en: 'So PCA and ICA seem to give results with similar properties : that is because
    of the 2 following reasons :'
  prefs: []
  type: TYPE_NORMAL
- en: the independance is “encoded” in both algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the iris dataset exhibits well separeted classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That’s why we need another example, more fitted for the ICA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example :'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s see another example : we first generate a synthetic dataset with two
    independent sources, a sine wave and a square wave, which are mixed together as
    a linear combination to create a mixed signal.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The actual, true, independant signals are the following :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b946d7257636cf8e299e1725cf74bb3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'They are mixed together, as 2 linear combinations :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0dcdd153cb87196744e755c93a376e03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s see how PCA and ICA perform on this new dataset :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8210745ad64f961a94995c7923ebaf0d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice how PCA created a new component that exhibits a lot of variance, as
    a linear combination of the inputs, but that absolutely does not match the original
    data : that’s indeed not the purpose of PCA.'
  prefs: []
  type: TYPE_NORMAL
- en: On the opposite, ICA performed very well in recovering the original dataset,
    independantly of variance composition.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The PCA, LDA, and ICA algorithms might seem like a custom version of each other,
    but they really do not have the same purpose. To summup:'
  prefs: []
  type: TYPE_NORMAL
- en: PCA aims to create new components that hold the maximum variance of the input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LDA aims to create new components that separate clusters based on a categorical
    feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ICA aims to retrieve original features that are mixed together in a linear combination
    in the input dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hopefully, you understand better the differences between these algorithm and
    will be able to quickly identify the one you need in the future.
  prefs: []
  type: TYPE_NORMAL
- en: '**If you liked this story, make sure to follow me and help me reach my 100
    subscribers goal :)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out some of my other stories below :'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/300-times-faster-resolution-of-finite-difference-method-using-numpy-de28cdade4e1?source=post_page-----c5762c4148ff--------------------------------)
    [## 300-times faster resolution of Finite-Difference Method using numpy'
  prefs: []
  type: TYPE_NORMAL
- en: Finite-difference method is a powerfull technique to solve complex problems,
    and numpy makes it fast !
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/300-times-faster-resolution-of-finite-difference-method-using-numpy-de28cdade4e1?source=post_page-----c5762c4148ff--------------------------------)
    [](/wrapping-numpys-arrays-971e015e14bb?source=post_page-----c5762c4148ff--------------------------------)
    [## Wrapping numpy’s arrays
  prefs: []
  type: TYPE_NORMAL
- en: The container approach.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/wrapping-numpys-arrays-971e015e14bb?source=post_page-----c5762c4148ff--------------------------------)
    [](https://medium.com/analytics-vidhya/deep-dive-into-seaborn-palettes-7b5fae5a258e?source=post_page-----c5762c4148ff--------------------------------)
    [## Deep dive into seaborn palettes
  prefs: []
  type: TYPE_NORMAL
- en: Drowning in seaborn palettes ?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'medium.com](https://medium.com/analytics-vidhya/deep-dive-into-seaborn-palettes-7b5fae5a258e?source=post_page-----c5762c4148ff--------------------------------)
    [](/pca-whitening-vs-zca-whitening-a-numpy-2d-visual-518b32033edf?source=post_page-----c5762c4148ff--------------------------------)
    [## PCA-whitening vs ZCA-whitening : a numpy 2d visual'
  prefs: []
  type: TYPE_NORMAL
- en: The process of whitening data consists in a transformation such that the transformed
    data has identity matrix as…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/pca-whitening-vs-zca-whitening-a-numpy-2d-visual-518b32033edf?source=post_page-----c5762c4148ff--------------------------------)
  prefs: []
  type: TYPE_NORMAL
