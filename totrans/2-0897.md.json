["```py\n!pip install \"sagemaker==2.162.0\" s3path boto3 --quiet\n\nfrom sagemaker.huggingface import HuggingFace\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker import s3_utils\nimport sagemaker\nimport boto3\nimport json\n```", "```py\n# Define S3 paths\nbucket             = \"<YOUR-S3-BUCKET>\"\ntraining_data_path = f\"s3://{bucket}/toy_data/train/data.jsonl\"\ntest_data_path     = f\"s3://{bucket}/toy_data/test/data.jsonl\"\noutput_path        = f\"s3://{bucket}/outputs\"\ncode_location      = f\"s3://{bucket}/code\"\n\n# Create SageMaker session\nsagemaker_session  = sagemaker.Session()\nregion             = sagemaker_session.boto_region_name\nrole               = sagemaker.get_execution_role()\n```", "```py\n{\n    \"prompt\": \"What is a Pastel de Nata?\",\n    \"response\": \"A Pastel de Nata is a Portuguese egg custard tart pastry, optionally dusted with cinnamon.\"\n}\n```", "```py\nprompt_template = \"\"\"Write a response that appropriately answers the question below.\n### Question:\n{question}\n\n### Response:\n\"\"\"\n\ndataset = [\n    {\"prompt\": \"What is a Pastel de Nata?\",\n     \"response\": \"A Pastel de Nata is a Portuguese egg custard tart pastry, optionally dusted with cinnamon.\"},\n    {\"prompt\": \"Which museums are famous in Amsterdam?\",\n     \"response\": \"Amsterdam is home to various world-famous museums, and no trip to the city is complete without stopping by the Rijksmuseum, Van Gogh Museum, or Stedelijk Museum.\"},\n    {\"prompt\": \"Where is the European Parliament?\",\n     \"response\": \"Strasbourg is the official seat of the European Parliament.\"},\n    {\"prompt\": \"How is the weather in The Netherlands?\",\n     \"response\": \"The Netherlands is a country that boasts a typical maritime climate with mild summers and cold winters.\"},\n    {\"prompt\": \"What are Poffertjes?\",\n     \"response\": \"Poffertjes are a traditional Dutch batter treat. Resembling small, fluffy pancakes, they are made with yeast and buckwheat flour.\"},\n]\n\n# Format prompt based on template\nfor example in dataset:\n    example[\"prompt\"] = prompt_template.format(question=example[\"prompt\"])\n\ntraining_data, test_data = dataset[0:4], dataset[4:]\n\nprint(f\"Size of training data: {len(training_data)}\\nSize of test data: {len(test_data)}\")\n```", "```py\ndef write_jsonlines_to_s3(data, s3_path):\n    \"\"\"Writes list of dictionaries as a JSON lines file to S3\"\"\"\n\n    json_string = \"\"\n    for d in data:\n        json_string += json.dumps(d) + \"\\n\"\n\n    s3_client   = boto3.client(\"s3\")\n\n    bucket, key = s3_utils.parse_s3_url(s3_path)\n    s3_client.put_object(\n         Body   = json_string,\n         Bucket = bucket,\n         Key    = key,\n    )\n\nwrite_jsonlines_to_s3(training_data, training_data_path)\nwrite_jsonlines_to_s3(test_data, test_data_path)\n```", "```py\n└── **fine-tune-mpt-7b-sagemaker**/\n    ├── training_script_launcher.sh\n    ├── fine_tuning_config.yaml\n    ├── sagemaker_finetuning.ipynb\n```", "```py\nmax_seq_len: 512\nglobal_seed: 17\n\n...\n# Dataloaders\ntrain_loader:\n  name: finetuning\n  dataset:\n    hf_name: json\n    hf_kwargs:\n        data_dir: /opt/ml/input/data/train/\n...\n\neval_loader:\n  name: finetuning\n  dataset:\n    hf_name: json\n    hf_kwargs:\n        data_dir: /opt/ml/input/data/test/\n\n...\nmax_duration: 3ep\neval_interval: 1ep\n...\nglobal_train_batch_size: 128\n\n...\n# FSDP\nfsdp_config:\n  sharding_strategy: FULL_SHARD\n  mixed_precision: PURE\n  activation_checkpointing: true\n  activation_checkpointing_reentrant: false\n  activation_cpu_offload: false\n  limit_all_gathers: true\n  verbose: false\n\n# Checkpoint to local filesystem or remote object store\nsave_folder: /tmp/checkpoints\ndist_timeout: 2000\n```", "```py\n# Clone llm-foundry package from MosaicML\n# This is where the training script is hosted\ngit clone https://github.com/mosaicml/llm-foundry.git\ncd llm-foundry\n\n# Install required packages\npip install -e \".[gpu]\"\npip install git+https://github.com/mosaicml/composer.git@dev\n\n# Run training script with fine-tuning configuration\ncomposer scripts/train/train.py /opt/ml/code/finetuning_config.yaml\n\n# Convert Composer checkpoint to HuggingFace model format\npython scripts/inference/convert_composer_to_hf.py \\\n    --composer_path /tmp/checkpoints/latest-rank0.pt \\\n    --hf_output_path /opt/ml/model/hf_fine_tuned_model \\\n    --output_precision bf16\n\n# Print content of the model artifact directory\nls /opt/ml/model/\n```", "```py\n# Define container image for the training job\ntraining_image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-training:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04-v1.1\"\n\n# Define metrics to send to CloudWatch\nmetrics = [\n    # On training set\n    {\"Name\": \"train:LanguageCrossEntropy\",\n     \"Regex\": \"Train metrics\\/train\\/LanguageCrossEntropy: ([+-]?((\\d+\\.?\\d*)|(\\.\\d+)))\"},\n    {\"Name\": \"train:LanguagePerplexity\",\n     \"Regex\": \"Train metrics\\/train\\/LanguagePerplexity: ([+-]?((\\d+\\.?\\d*)|(\\.\\d+)))\"},\n    # On test set\n    {\"Name\": \"test:LanguageCrossEntropy\",\n     \"Regex\": \"Eval metrics\\/eval\\/LanguageCrossEntropy: ([+-]?((\\d+\\.?\\d*)|(\\.\\d+)))\"},\n    {\"Name\": \"test:LanguagePerplexity\",\n     \"Regex\": \"Eval metrics\\/eval\\/LanguagePerplexity: ([+-]?((\\d+\\.?\\d*)|(\\.\\d+)))\"},\n]\n\nestimator_args = {\n    \"image_uri\": training_image_uri,     # Training container image\n    \"entry_point\": \"launcher.sh\",        # Launcher bash script\n    \"source_dir\": \".\",                   # Directory with launcher script and configuration file\n    \"instance_type\": \"ml.g5.48xlarge\",   # Instance type\n    \"instance_count\": 1,                 # Number of training instances\n    \"base_job_name\": \"fine-tune-mpt-7b\", # Prefix of the training job name\n    \"role\": role,                        # IAM role\n    \"volume_size\": 300,                  # Size of the EBS volume attached to the instance (GB)\n    \"py_version\": \"py310\",               # Python version\n    \"metric_definitions\": metrics,       # Metrics to track\n    \"output_path\": output_path,          # S3 location where the model artifact will be uploaded\n    \"code_location\": code_location,      # S3 location where the source code will be saved\n    \"disable_profiler\": True,            # Do not create profiler instance\n    \"keep_alive_period_in_seconds\": 240, # Enable Warm Pools while experimenting\n}\n\nhuggingface_estimator = HuggingFace(**estimator_args)\n```", "```py\nhuggingface_estimator.fit({\n    \"train\": TrainingInput(\n        s3_data=training_data_path,\n        content_type=\"application/jsonlines\"),\n    \"test\": TrainingInput(\n        s3_data=test_data_path,\n        content_type=\"application/jsonlines\"),\n}, wait=True)\n```"]