- en: 'Courage to Learn ML: Demystifying L1 & L2 Regularization (part 2)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35](https://towardsdatascience.com/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unlocking the Intuition Behind L1 Sparsity with Lagrange multipliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://amyma101.medium.com/?source=post_page-----1bb171e43b35--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----1bb171e43b35--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1bb171e43b35--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1bb171e43b35--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page-----1bb171e43b35--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1bb171e43b35--------------------------------)
    ·6 min read·Nov 25, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'Welcome back to ‘[Courage to Learn ML](/towardsdatascience.com/tagged/courage-to-learn-ml):
    Demystifying L1 & L2 Regularization,’ Part Two. In [our previous discussion](https://medium.com/@yujing-ma45/understanding-l1-l2-regularization-part-1-9c7affe6f920),
    we explored the benefits of smaller coefficients and the means to attain them
    through weight penalization techniques. Now, in this follow-up, our mentor and
    learner will delve even deeper into the realm of L1 and L2 regularization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’ve been pondering questions like these, you’re in the right place:'
  prefs: []
  type: TYPE_NORMAL
- en: What’s the reason behind the names L1 and L2 regularization?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we interpret the classic L1 and L2 regularization graph?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are Lagrange multipliers, and how can we understand them intuitively?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying Lagrange multipliers to comprehend L1 sparsity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your engagement — likes, comments, and follows — does more than just boost morale;
    it powers our journey of discovery! So, let’s dive in.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb38cadf6bb97d614c0c0bc8ddd9755a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Aarón Blanco Tejedor](https://unsplash.com/@the_meaning_of_love?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Why they call L1, l2 regularization?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The name, L1 and L2 regularization, comes from the concept of Lp norms directly.
    Lp norms represent different ways to calculate distances from a point to the origin
    in a space. For instance, the L1 norm, also known as Manhattan distance, calculates
    the distance using the absolute values of coordinates, like ∣*x*∣+∣*y*∣. On the
    other hand, the L2 norm, or Euclidean distance, calculates it as the square root
    of the sum of the squared values, which is sqrt(x² + y²)
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of regularization in machine learning, these norms are used
    to create penalty terms that are added to the loss function. You can think of
    Lp regularization as measuring the total distance of the model’s weights from
    the origin in a high-dimensional space. The choice of norm affects the nature
    of this penalty: the L1 norm tends to make some coefficients zero, effectively
    selecting more important features, while the L2 norm shrinks the coefficients
    towards zero, ensuring no single feature disproportionately influences the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, L1 and L2 regularization are named after these mathematical norms
    — L1 norm and L2 norm — due to the way they apply their respective distance calculations
    as penalties to the model’s weights. This helps in controlling overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '**I often come across the graph below when studying L1 and L2 regularization,
    but I find it quite challenging to interpret. Could you help clarify what it represents?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/9c4081ca5e2c0ac4ec1bb37796236b09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The traditional yet perplexing L1 and L2 regularization graph found in textbooks..
    source: [https://commons.wikimedia.org/wiki/File:Regularization.jpg](https://commons.wikimedia.org/wiki/File:Regularization.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Alright, let’s unpack this graph step-by-step. To start, it’s essential to understand
    what its different elements signify. Imagine our loss function is defined by just
    two weights, w1 and w2 (in the graph we use beta instead of w, but they represent
    the same concept). The axes of the graph represent these weights we aim to optimize.
  prefs: []
  type: TYPE_NORMAL
- en: Without any weight penalties, our goal is to find w1 and w2 values that minimize
    our loss function. You can visualize this function’s landscape as a valley or
    basin, illustrated in the graph by the elliptical contours.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s delve into the penalties. The L1 norm, shown as a diamond shape,
    essentially measures the Manhattan distance of w1 and w2 from the origin. The
    L2 norm forms a circle, representing the sum of squared weights.
  prefs: []
  type: TYPE_NORMAL
- en: The center of the elliptical contour indicates the global minimum of the objective
    function, where we find our ideal weights. The centers of the L1 and L2 shapes
    (diamond and circle) at the origin, where all weights are zero, highlight the
    minimal weight penalty scenario. As we increasing the penalty term’s intensity,
    the model’s weights would gravitate closer to zero. This graph is a visual guide
    to understanding these dynamics and the impact of penalties on the weights.
  prefs: []
  type: TYPE_NORMAL
- en: Understood. So…. The graph shows a dimension created by the weights, and the
    two distinct shapes illustrate the objective function and the penalty, respectively.
    How should we interpret the intersection point labeled w*? What does this point
    signify?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand the above graph as a whole, it’s essential to grasp the concept
    of Lagrange multipliers, a key tool in optimization. Lagrange multipliers aid
    in finding the optimal points (maximum or minimum) of a function within certain
    constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you’re hiking up a mountain with the goal of reaching the peak. There
    are various paths, but due to safety, you’re required to stay on a designated
    safe path. Here, reaching the peak represents the optimization problem, and the
    safe path symbolizes the constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, suppose you have a function f(x, y) to optimize. This optimization
    must adhere to a constraint, represented by another function g(x, y) = 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/68a20f2eaf9a3abd420a45a322c23f22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Lagrange Multipliers 2D. Source: [https://upload.wikimedia.org/wikipedia/commons/b/bf/LagrangeMultipliers2D.svg](https://upload.wikimedia.org/wikipedia/commons/b/bf/LagrangeMultipliers2D.svg)'
  prefs: []
  type: TYPE_NORMAL
- en: In the ‘Lagrange Multipliers 2D’ graph from Wikipedia, the blue contours represent
    f(x, y) (the mountain’s landscape), and the red curves indicate the constraints.
    The point where these two intersect, although not the peak point on the f(x, y)
    contour, represents the optimal solution under the given constraint. Lagrange
    multipliers solve this by merging the objective function with its constraints.
    In the other word, Lagrange multipliers will help you find the point easier.
  prefs: []
  type: TYPE_NORMAL
- en: So, if we circle back to that L1 and L2 graph, are you suggesting that the diamond
    and circle shapes represent constraints? And does that mean the spot where they
    intersect, that tangent point, is essentially the sweet spot for hitting the max
    of f(x, y) within those constraints?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Correct! The L1 and L2 regularization techniques can indeed be visualized as
    imposing constraints in the form of a diamond and a circle, respectively. So the
    graph helps us understand how these regularization methods impact the optimization
    of a function, typically the loss function in machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf23589d50e185f421663dc8687a7a73.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A better illustration on L2 (left), L1 (right) regularizations. Source: [https://www.researchgate.net/figure/Parameter-norm-penalties-L2-norm-regularization-left-and-L1-norm-regularization_fig2_355020694](https://www.researchgate.net/figure/Parameter-norm-penalties-L2-norm-regularization-left-and-L1-norm-regularization_fig2_355020694)'
  prefs: []
  type: TYPE_NORMAL
- en: '**L1 Regularization (Diamond Shape)**: The L1 norm creates a diamond-shaped
    constraint. This shape is characterized by **its sharp corners along the axes**.
    When the optimization process (like gradient descent) seeks the point that minimizes
    the loss function while staying within this diamond, it’s more likely to hit these
    corners. At these corners, one of the weights (parameters of the model) becomes
    zero while others remain non-zero**. This property of the L1 norm leads to sparsity
    in the model parameters, meaning some weights are exactly zero. This sparsity
    is useful for feature selection, as it effectively removes some features from
    the model.**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**L2 Regularization (Circle Shape)**: On the other hand, the L2 norm creates
    a circular-shaped constraint. **The smooth, round nature of the circle means that
    the optimization process is less likely to find solutions at the axes where weights
    are zero.** Instead, the L2 norm tends to shrink the weights uniformly without
    necessarily driving any to zero. This controls the model complexity by preventing
    weights from becoming too large, thereby helping to avoid overfitting. However,
    unlike the L1 norm, it doesn’t lead to sparsity in the model parameters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep in mind, Lagrange multipliers aren’t the only method to grasp L1 and L2
    regularizations. Let’s take a break here, and I’ll address more of your questions
    in [our next installment](/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a).
    See you soon!
  prefs: []
  type: TYPE_NORMAL
- en: 'Other posts in this series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Courage to Learn ML: Demystifying L1 & L2 Regularization (part 1)](https://medium.com/@yujing-ma45/understanding-l1-l2-regularization-part-1-9c7affe6f920)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Courage to Learn ML:](/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a)
    [Demystifying](https://medium.com/@yujing-ma45/understanding-l1-l2-regularization-part-1-9c7affe6f920)
    [L1 & L2 Regularization (part 3)](/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***If you liked the article, you can find me on*** [***LinkedIn***](https://www.linkedin.com/in/amyma101/)***.***'
  prefs: []
  type: TYPE_NORMAL
