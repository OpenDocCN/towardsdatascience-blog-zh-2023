["```py\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data[:, :2] # we only take the first two features\ny = iris.target\n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n```", "```py\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nclf = GradientBoostingClassifier(random_state=42)\nclf.fit(X_train, y_train)\n```", "```py\nprint(f'Train accuracy: {clf.score(X_train, y_train):.4f}')\nprint(f'Test accuracy: {clf.score(X_test, y_test):.4f}')\n```", "```py\nTrain accuracy: 0.9554\nTest accuracy: 0.7895\n```", "```py\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparams = {\n    'n_estimators': [10, 50, 100, 200, 500],\n    'max_depth': np.arange(3, 11),\n    'subsample': np.arange(0.5, 1.0, 0.1),\n    'max_features': ['sqrt', 'log2', None]    \n}\n\nsearch = RandomizedSearchCV(GradientBoostingClassifier(random_state=42), params, n_iter=50, cv=3, n_jobs=-1)\nsearch.fit(X_train, y_train)\n\nprint(search.best_params_)\n```", "```py\n{'subsample': 0.6, 'n_estimators': 10, 'max_features': 'sqrt', 'max_depth': 3}\n```", "```py\nbest_clf = search.best_estimator_\nprint(f'Train accuracy: {best_clf.score(X_train, y_train):.4f}')\nprint(f'Test accuracy: {best_clf.score(X_test, y_test):.4f}')\n```", "```py\nTrain accuracy: 0.8125\nTest accuracy: 0.8684\n```", "```py\nfrom sklearn.datasets import fetch_california_housing\n\ndata = fetch_california_housing()\nX, y = data.data, data.target\nfeature_names = data.feature_names\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n```", "```py\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nreg = GradientBoostingRegressor(random_state=0)\nreg.fit(X_train, y_train)\n```", "```py\ntrain_score = reg.score(X_train, y_train)\nprint(f'R2 score (train): {train_score:.4f}')\n\ntest_score = reg.score(X_test, y_test)\nprint(f'R2 score (test): {test_score:.4f}')\n```", "```py\nR2 score (train): 0.8027\nR2 score (test): 0.7774\n```", "```py\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparams = {\n    'n_estimators': [10, 50, 100, 200, 500],\n    'max_depth': np.arange(3, 11),\n    'subsample': np.arange(0.5, 1.0, 0.1),\n    'max_features': ['sqrt', 'log2', None]    \n}\n\nsearch = RandomizedSearchCV(GradientBoostingRegressor(random_state=0), params, n_iter=50, cv=3, n_jobs=-1)\nsearch.fit(X_train, y_train)\n\nprint(search.best_params_)\n```", "```py\n{'subsample': 0.7999999999999999, 'n_estimators': 500, 'max_features': 'log2', 'max_depth': 7}\n```", "```py\nbest_reg = search.best_estimator_\nprint(f'R2 score (train): {best_reg.score(X_train, y_train):.4f}')\nprint(f'R2 score (test): {best_reg.score(X_test, y_test):.4f}')\n```", "```py\nR2 score (train): 0.9849\nR2 score (test): 0.8519\n```", "```py\nfrom sklearn.metrics import mean_squared_error as MSE\n\ntest_score = np.zeros(best_reg.n_estimators_)\nfor i, y_test_pred in enumerate(best_reg.staged_predict(X_test)):\n    test_score[i] = MSE(y_test, y_test_pred)\n\nplt.plot(np.arange(best_reg.n_estimators), best_reg.train_score_, label='Training loss')\nplt.plot(np.arange(best_reg.n_estimators), test_score, 'r', label='Test loss')\n\nplt.xlabel('Boosting iterations')\nplt.ylabel('MSE')\nplt.legend()\n```", "```py\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparams = {    \n    'max_depth': np.arange(3, 11),\n    'subsample': np.arange(0.5, 1.0, 0.1),\n    'max_features': ['sqrt', 'log2', None]    \n}\n\nsearch = RandomizedSearchCV(GradientBoostingRegressor(random_state=0, n_estimators=500, n_iter_no_change=5), \n                            params, n_iter=50, cv=3, n_jobs=-1)\nsearch.fit(X_train, y_train)\n\nprint(search.best_params_)\n\nreg = search.best_estimator_\nprint(f'R2 score (train): {reg.score(X_train, y_train):.4f}')\nprint(f'R2 score (test): {reg.score(X_test, y_test):.4f}')\n```", "```py\n{'subsample': 0.8999999999999999, 'max_features': 'log2', 'max_depth': 7}\nR2 score (train): 0.9227\nR2 score (test): 0.8402\n```", "```py\nreg.n_estimators_\n```", "```py\n118\n```", "```py\n# Sort the features by their importance\nfeature_importance = best_reg.feature_importances_\nsorted_idx = np.argsort(feature_importance)\n\n# Plot the feature importances\npos = np.arange(len(feature_importance))\nplt.barh(pos, feature_importance[sorted_idx])\nplt.yticks(pos, np.array(feature_names)[sorted_idx])\nplt.xlabel('Feature importance')\n```", "```py\nfrom sklearn.datasets import make_hastie_10_2\n\nX, y = make_hastie_10_2(n_samples=50000, random_state=0)\n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n```", "```py\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nclf = GradientBoostingClassifier(random_state=0)\n%timeit clf.fit(X_train, y_train)\n```", "```py\n12.6 s ± 358 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n```", "```py\nprint(f'Train accuracy: {clf.score(X_train, y_train):.4f}')\nprint(f'Test accuracy: {clf.score(X_test, y_test):.4f}')\n```", "```py\nTrain accuracy: 0.9392\nTest accuracy: 0.9231\n```", "```py\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nclf = HistGradientBoostingClassifier(random_state=0)\n%timeit clf.fit(X_train, y_train)\n```", "```py\n1.53 s ± 120 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n```", "```py\nprint(f'Train accuracy: {clf.score(X_train, y_train):.4f}')\nprint(f'Test accuracy: {clf.score(X_test, y_test):.4f}')\n```", "```py\nTrain accuracy: 0.9725\nTest accuracy: 0.9467\n```"]