- en: How GPT Models Work
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT模型的工作原理
- en: 原文：[https://towardsdatascience.com/how-gpt-models-work-b5f4517d5b5](https://towardsdatascience.com/how-gpt-models-work-b5f4517d5b5)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-gpt-models-work-b5f4517d5b5](https://towardsdatascience.com/how-gpt-models-work-b5f4517d5b5)
- en: Learn the core concepts behind OpenAI’s GPT models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解OpenAI的GPT模型背后的核心概念
- en: '[](https://medium.com/@bea_684?source=post_page-----b5f4517d5b5--------------------------------)[![Beatriz
    Stollnitz](../Images/63a2a7daeca6d93e26b3ac0556c42aa1.png)](https://medium.com/@bea_684?source=post_page-----b5f4517d5b5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b5f4517d5b5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b5f4517d5b5--------------------------------)
    [Beatriz Stollnitz](https://medium.com/@bea_684?source=post_page-----b5f4517d5b5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bea_684?source=post_page-----b5f4517d5b5--------------------------------)[![Beatriz
    Stollnitz](../Images/63a2a7daeca6d93e26b3ac0556c42aa1.png)](https://medium.com/@bea_684?source=post_page-----b5f4517d5b5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b5f4517d5b5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b5f4517d5b5--------------------------------)
    [Beatriz Stollnitz](https://medium.com/@bea_684?source=post_page-----b5f4517d5b5--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b5f4517d5b5--------------------------------)
    ·14 min read·May 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b5f4517d5b5--------------------------------)
    ·14分钟阅读·2023年5月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3aacc19848583f3211780a4e9883415e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3aacc19848583f3211780a4e9883415e.png)'
- en: Photo by [KOMMERS](https://unsplash.com/@kommers?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[KOMMERS](https://unsplash.com/@kommers?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '**Introduction**'
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: It was 2021 when I wrote my first few lines of code using a GPT model, and that
    was the moment I realized that text generation had reached an inflection point.
    Prior to that, I had written language models from scratch in grad school, and
    I had experience working with other text generation systems, so I knew just how
    difficult it was to get them to produce useful results. I was fortunate to get
    early access to GPT-3 as part of my work on the announcement of its release within
    the Azure OpenAI Service, and I tried it out in preparation for its launch. I
    asked GPT-3 to summarize a long document and experimented with few-shot prompts.
    I could see that the results were far more advanced than those of prior models,
    making me excited about the technology and eager to learn how it’s implemented.
    And now that the follow-on GPT-3.5, ChatGPT, and GPT-4 models are rapidly gaining
    wide adoption, more people in the field are also curious about how they work.
    While the details of their inner workings are proprietary and complex, all the
    GPT models share some fundamental ideas that aren’t too hard to understand. My
    goal for this post is to explain the core concepts of language models in general
    and GPT models in particular, with the explanations geared toward data scientists
    and machine learning engineers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我在2021年写了第一段使用GPT模型的代码，那时我意识到文本生成已经达到了一个拐点。在那之前，我在研究生阶段从零开始编写过语言模型，也有使用其他文本生成系统的经验，所以我知道让它们产生有用的结果有多么困难。我有幸在Azure
    OpenAI服务发布时获得了GPT-3的早期访问权限，并在其发布前进行了尝试。我要求GPT-3总结一份长文档，并尝试了一些少样本提示。我发现结果远远超出了之前模型的水平，这让我对这项技术感到兴奋，并渴望了解它是如何实现的。现在，后续的GPT-3.5、ChatGPT和GPT-4模型正迅速获得广泛应用，更多领域的人也对它们的工作原理充满好奇。尽管它们的内部工作细节是专有且复杂的，但所有GPT模型共享一些基本概念，这些概念并不难理解。我的目标是解释语言模型的一般核心概念，特别是GPT模型的核心概念，解释面向数据科学家和机器学习工程师。
- en: '**How generative language models work**'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**生成式语言模型的工作原理**'
- en: 'Let’s start by exploring how generative language models work. The very basic
    idea is the following: they take *n* tokens as input, and produce one token as
    output.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始探索生成式语言模型的工作原理。最基本的想法如下：它们将*n*个标记作为输入，生成一个标记作为输出。
- en: '![](../Images/17f45cd61a894d04e59bee04bfe1ca34.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17f45cd61a894d04e59bee04bfe1ca34.png)'
- en: This seems like a fairly straightforward concept, but in order to really understand
    it, we need to know what a token is.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这看似是一个相当简单的概念，但为了真正理解它，我们需要知道什么是标记。
- en: A token is a chunk of text. In the context of OpenAI GPT models, common and
    short words typically correspond to a single token, such as the word “We” in the
    image below. Long and less commonly used words are generally broken up into several
    tokens. For example the word “anthropomorphizing” in the image below is broken
    up into three tokens. Abbreviations like “ChatGPT” may be represented with a single
    token or broken up into multiple, depending on how common it is for the letters
    to appear together. You can go to OpenAI’s [Tokenizer page](https://platform.openai.com/tokenizer),
    enter your text, and see how it gets split up into tokens. You can choose between
    “GPT-3” tokenization, which is used for text, and “Codex” tokenization, which
    is used for code. We’ll keep the default “GPT-3” setting.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个标记是文本的一部分。在 OpenAI GPT 模型的上下文中，常见且简短的词通常对应一个标记，例如下图中的“我们”一词。较长且不常用的词通常会被拆分为多个标记。例如，下图中的“人性化”一词被拆分为三个标记。像“ChatGPT”这样的缩写可能被表示为一个标记或拆分为多个标记，这取决于这些字母出现在一起的频率。你可以访问
    OpenAI 的 [Tokenizer page](https://platform.openai.com/tokenizer)，输入你的文本，并查看它如何被拆分成标记。你可以在“GPT-3”分词（用于文本）和“Codex”分词（用于代码）之间进行选择。我们将保持默认的“GPT-3”设置。
- en: '![](../Images/26ba16e92da500d366df1ff0040280e1.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26ba16e92da500d366df1ff0040280e1.png)'
- en: You can also use OpenAI’s open-source [tiktoken](https://github.com/openai/tiktoken)
    library to tokenize using Python code. OpenAI offers a few different tokenizers
    that each have a slightly different behavior. In the code below we use the tokenizer
    for “davinci,” which is a GPT-3 model, to match the behavior you saw using the
    UI.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用 OpenAI 的开源 [tiktoken](https://github.com/openai/tiktoken) 库通过 Python
    代码进行分词。OpenAI 提供了几种不同的分词器，每种分词器的行为略有不同。在下面的代码中，我们使用了用于“davinci”（一个 GPT-3 模型）的分词器，以匹配你在
    UI 中看到的行为。
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can see in the output of the code that this tokenizer contains 50,257 different
    tokens, and that each token is internally mapped into an integer index. Given
    a string, we can split it into integer tokens, and we can convert those integers
    into the sequence of characters they correspond to. Encoding and decoding a string
    should always give us the original string back.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在代码的输出中看到，这个分词器包含 50,257 个不同的标记，每个标记在内部映射为一个整数索引。给定一个字符串，我们可以将其拆分为整数标记，并将这些整数转换为它们对应的字符序列。对字符串进行编码和解码应始终返回原始字符串。
- en: This gives you a good intuition for how OpenAI’s tokenizer works, but you may
    be wondering why they chose those token lengths. Let’s consider some other options
    for tokenization. Suppose we try the simplest possible implementation, where each
    letter is a token. That makes it easy to break up the text into tokens, and keeps
    the total number of different tokens small. However, we can’t encode nearly as
    much information as in OpenAI’s approach. If we used letter-based tokens in the
    example above, 11 tokens could only encode “We need to”, while 11 of OpenAI’s
    tokens can encode the entire sentence. It turns out that the current language
    models have a limit on the maximum number of tokens that they can receive. Therefore,
    we want to pack as much information as possible in each token.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这给你一个关于 OpenAI 分词器如何工作的良好直觉，但你可能会想知道他们为何选择这些标记长度。让我们考虑一些其他的分词选项。假设我们尝试最简单的实现，即每个字母都是一个标记。这使得将文本拆分为标记变得容易，并且保持不同标记的总数较小。然而，我们无法编码像
    OpenAI 方法那样多的信息。如果我们在上述示例中使用基于字母的标记，11 个标记只能编码“我们需要”，而 11 个 OpenAI 的标记可以编码整个句子。事实证明，当前的语言模型对它们可以接收的最大标记数量有限。因此，我们希望在每个标记中尽可能地打包更多信息。
- en: Now let’s consider the scenario where each word is a token. Compared to OpenAI’s
    approach, we would only need seven tokens to represent the same sentence, which
    seems more efficient. And splitting by word is also straighforward to implement.
    However, language models need to have a complete list of tokens that they might
    encounter, and that’s not feasible for whole words — not only because there are
    so many words in the dictionary, but also because it would be difficult to keep
    up with domain-specific terminology and any new words that are invented.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑一种情况，每个单词都是一个标记。与 OpenAI 的方法相比，我们只需七个标记即可表示相同的句子，这似乎更高效。而且按单词拆分也很容易实现。然而，语言模型需要一个完整的标记列表，以便处理它们可能遇到的标记，这对于整个单词来说是不可行的——不仅因为词典中的单词太多，还因为难以跟上特定领域的术语和任何新发明的词汇。
- en: So it’s not surprising that OpenAI settled for a solution somewhere in between
    those two extremes. Other companies have released tokenizers that follow a similar
    approach, for example [Sentence Piece](https://github.com/google/sentencepiece)
    by Google.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，OpenAI 选择了这两种极端之间的某种解决方案也就不足为奇了。其他公司也发布了采用类似方法的分词器，例如 Google 的 [Sentence
    Piece](https://github.com/google/sentencepiece)。
- en: Now that we have a better understanding of tokens, let’s go back to our original
    diagram and see if we can understand it a bit better. Generative models take *n*
    tokens in, which could be a few words, a few paragraphs, or a few pages. And they
    produce a single token out, which could be a short word or a part of a word.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对令牌有了更好的理解，让我们回到原始图示，看看是否可以更好地理解它。生成模型接受 *n* 个令牌，这些令牌可以是几个词、几个段落或几页内容。它们输出一个单一的令牌，这可能是一个短词或词的一部分。
- en: '![](../Images/17f45cd61a894d04e59bee04bfe1ca34.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17f45cd61a894d04e59bee04bfe1ca34.png)'
- en: That makes a bit more sense now.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这就更有意义了。
- en: But if you’ve played with [OpenAI’s ChatGPT](https://chat.openai.com/), you
    know that it produces many tokens, not just a single token. That’s because this
    basic idea is applied in an expanding-window pattern. You give it *n* tokens in,
    it produces one token out, then it incorporates that output token as part of the
    input of the next iteration, produces a new token out, and so on. This pattern
    keeps repeating until a stopping condition is reached, indicating that it finished
    generating all the text you need.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你使用过 [OpenAI 的 ChatGPT](https://chat.openai.com/)，你会知道它产生了许多令牌，而不仅仅是一个令牌。这是因为这个基本思想以扩展窗口模式应用。你给它
    *n* 个令牌，它输出一个令牌，然后将这个输出令牌作为下一次迭代的输入的一部分，产生一个新的令牌，依此类推。这个模式不断重复，直到达到停止条件，表明它已经生成了你需要的所有文本。
- en: 'For example, if I type “We need to” as input to my model, the algorithm may
    produce the results shown below:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我向我的模型输入“我们需要”，算法可能会产生如下结果：
- en: '![](../Images/38bc679fc69c93fe6594faf1a7b48612.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38bc679fc69c93fe6594faf1a7b48612.png)'
- en: 'While playing with ChatGPT, you may also have noticed that the model is not
    deterministic: if you ask it the exact same question twice, you’ll likely get
    two different answers. That’s because the model doesn’t actually produce a single
    predicted token; instead it returns a probability distribution over all the possible
    tokens. In other words, it returns a vector in which each entry expresses the
    probability of a particular token being chosen. The model then samples from that
    distribution to generate the output token.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 ChatGPT 的过程中，你可能也注意到模型不是确定性的：如果你问它完全相同的问题两次，你可能会得到两个不同的答案。这是因为模型实际上并不会产生一个单一的预测令牌；相反，它返回的是所有可能令牌的概率分布。换句话说，它返回一个向量，其中每个条目表示选择特定令牌的概率。然后，模型从该分布中采样以生成输出令牌。
- en: '![](../Images/f20149deffafd1bcff44bd7e0a942985.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f20149deffafd1bcff44bd7e0a942985.png)'
- en: How does the model come up with that probability distribution? That’s what the
    training phase is for. During training, the model is exposed to a lot of text,
    and its weights are tuned to predict good probability distributions, given a sequence
    of input tokens. GPT models are trained with a large portion of the internet,
    so their predictions reflect a mix of the information they’ve seen.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是如何产生概率分布的？这就是训练阶段的目的。在训练过程中，模型接触了大量文本，并且其权重被调整以预测好的概率分布，给定一系列输入令牌。GPT模型是用互联网上的大量数据进行训练的，因此它们的预测反映了它们见过的信息的混合。
- en: You now have a very good understanding of the idea behind generative models.
    Notice that I’ve only explained the idea though, I haven’t yet given you an algorithm.
    It turns out that this idea has been around for many decades, and it has been
    implemented using several different algorithms over the years. Next we’ll look
    at some of those algorithms.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在对生成模型背后的理念有了很好的理解。请注意，我只是解释了这个理念，尚未给出具体的算法。事实上，这个理念已经存在了几十年，并且多年来已经用几种不同的算法实现了。接下来我们将看看其中的一些算法。
- en: '**A brief history of generative language models**'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**生成语言模型的简史**'
- en: 'Hidden Markov Models (HMMs) became popular in the 1970s. Their internal representation
    encodes the grammatical structure of sentences (nouns, verbs, and so on), and
    they use that knowledge when predicting new words. However, because they are Markov
    processes, they only take into consideration the most recent token when generating
    a new token. So, they implement a very simple version of the “*n* tokens in, one
    token out” idea, where *n* = 1\. As a result, they don’t generate very sophisticated
    output. Let’s consider the following example:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 隐马尔可夫模型（HMMs）在 1970 年代变得流行。它们的内部表示编码了句子的语法结构（名词、动词等），并在预测新单词时使用这些知识。然而，由于它们是马尔可夫过程，它们在生成新标记时只考虑最新的标记。因此，它们实现了“*n*
    个标记输入，一个标记输出”想法的非常简单版本，其中 *n* = 1\. 结果是，它们不会生成非常复杂的输出。我们来看以下示例：
- en: '![](../Images/2a4a1dc365a978e00dc2e3c44090808c.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a4a1dc365a978e00dc2e3c44090808c.png)'
- en: If we input “The quick brown fox jumps over the” to a language model, we would
    expect it to return “lazy.” However, an HMM will only see the last token, “the,”
    and with such little information it’s unlikely that it will give us the prediction
    we expect. As people experimented with HMMs, it became clear that language models
    need to support more than one input token in order to generate good outputs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将“ The quick brown fox jumps over the”输入到语言模型中，我们会期待它返回“lazy”。然而，HMM 只会看到最后一个标记“the”，而且只有这么少的信息，它不太可能给出我们期望的预测。随着人们对
    HMMs 的实验，变得明显的是，语言模型需要支持多个输入标记以生成好的输出。
- en: N-grams became popular in the 1990s because they fixed the main limitation with
    HMMs by taking more than one token as input. An n-gram model would probably do
    pretty well at predicting the word “lazy” for the previous example.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: N-gram 在 1990 年代变得流行，因为它通过使用多个标记作为输入解决了隐马尔可夫模型（HMMs）的主要局限性。一个 n-gram 模型可能在预测前一个示例中的单词“lazy”时表现得非常好。
- en: The simplest implementation of an n-gram is a bi-gram with character-based tokens,
    which given a single character, is able to predict the next character in the sequence.
    You can create one of these in just a few lines of code, and I encourage you to
    give it a try. First, count the number of different characters in your training
    text (let’s call it *n*), and create an *n x n* 2D matrix initialized with zeros.
    Each pair of input characters can be used to locate a particular entry in this
    matrix, by choosing the row corresponding to the first character, and the column
    corresponding to the second character. As you parse your training data, for every
    pair of characters, you simply add one to the corresponding matrix cell. For example,
    if your training data contains the word “car,” you would add one to the cell in
    the “c” row and “a” column, and then add one to the cell in the “a” row and “r”
    column. Once you have accumulated the counts for all your training data, convert
    each row into a probability distribution by dividing each cell by the total across
    that row.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的 n-gram 实现是基于字符的二元模型（bi-gram），它能够根据一个字符预测序列中的下一个字符。你可以用几行代码创建一个这样的模型，我鼓励你试试。首先，计算训练文本中不同字符的数量（我们称之为
    *n*），并创建一个 *n x n* 的二维矩阵，初始值为零。每对输入字符可以用来在这个矩阵中定位一个特定的条目，通过选择对应第一个字符的行和第二个字符的列。当你解析训练数据时，对于每对字符，你只需将对应矩阵单元的值加一。例如，如果你的训练数据包含单词“car”，你会在“c”行和“a”列的单元格中加一，然后在“a”行和“r”列的单元格中加一。一旦你积累了所有训练数据的计数，将每一行转换为概率分布，方法是将每个单元格的值除以该行的总和。
- en: '![](../Images/7f59202ea9413d60863fd5b94cfd7e73.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f59202ea9413d60863fd5b94cfd7e73.png)'
- en: Then to make a prediction, you need to give it a single character to start,
    for example, “c”. You look up the probability distribution that corresponds to
    the “c” row, and sample that distribution to produce the next character. Then
    you take the character you produced, and repeat the process, until you reach a
    stopping condition. Higher-order n-grams follow the same basic idea, but they’re
    able to look at a longer sequence of input tokens by using n-dimensional tensors.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了做出预测，你需要提供一个起始字符，例如“c”。你查找与“c”行对应的概率分布，并从中采样以生成下一个字符。然后，你取出你生成的字符，重复这个过程，直到达到停止条件。高阶
    n-gram 遵循相同的基本思想，但它们能够通过使用 n 维张量查看更长的输入标记序列。
- en: N-grams are simple to implement. However, because the size of the matrix grows
    exponentialy as the number of input tokens increases, they don’t scale well to
    a larger number of tokens. And with just a few input tokens, they’re not able
    to produce good results. A new technique was needed to continue making progress
    in this field.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: N-gram 简单易实现。然而，由于矩阵的大小随着输入标记数量的增加而指数级增长，它们在处理大量标记时的扩展性较差。并且对于仅有少量输入标记的情况，它们无法生成良好的结果。因此，需要一种新技术来继续在这一领域取得进展。
- en: In the 2000s, Recurrent Neural Networks (RNNs) became quite popular because
    they’re able to accept a much larger number of input tokens than previous techniques.
    In particular, LSTMs and GRUs, which are types of RNNs, became widely used and
    proved capable of generating fairly good results.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在2000年代，循环神经网络（RNNs）变得相当流行，因为它们能够接受比以前技术更多的输入标记。特别是，LSTMs 和 GRUs，这些是 RNNs 的类型，变得广泛使用并证明能够生成相当不错的结果。
- en: 'RNNs are a type of neural network, but unlike traditional feed-forward neural
    networks, their architecture can adapt to accepting any number of inputs and produce
    any number of outputs. For example, if we give an RNN the input tokens “We,” “need,”
    and “to,” and want it to generate a few more tokens until a full point is reached,
    the RNN might have the following structure:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs 是一种神经网络，但与传统的前馈神经网络不同，它们的架构可以适应任何数量的输入并生成任何数量的输出。例如，如果我们给 RNN 输入标记“We”，“need”和“to”，并希望它生成更多标记直到达到完整的句点，RNN
    可能具有以下结构：
- en: '![](../Images/87a57ce60506341b949e6d71f44aac10.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/87a57ce60506341b949e6d71f44aac10.png)'
- en: Each of the nodes in the structure above has the same weights. You can think
    of it as a single node that connects to itself and executes repeatedly (hence
    the name “recurrent”), or you can think of it in the expanded form shown in the
    image above. One key capability added to LSTMs and GRUs over basic RNNs is the
    presence of an internal memory cell that gets passed from one node to the next.
    This enables later nodes to remember certain aspects of previous ones, which is
    essential to make good text predictions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 上述结构中的每个节点具有相同的权重。你可以把它看作是一个单一的节点，连接到自身并重复执行（因此叫“递归”），或者可以按照上图中的扩展形式来理解。LSTMs
    和 GRUs 相较于基础 RNNs 增加的一个关键能力是存在一个从一个节点传递到下一个节点的内部记忆单元。这使得后续节点能够记住前面节点的某些方面，这对于生成良好的文本预测是必不可少的。
- en: However, RNNs have instability issues with very long sequences of text. The
    gradients in the model tend to grow exponentially (called “exploding gradients”)
    or decrease to zero (called “vanishing gradients”), preventing the model from
    continuing to learn from training data. LSTMs and GRUs mitigate the vanishing
    gradients issue, but don’t prevent it completely. So, even though in theory their
    architecture allows for inputs of any length, in practice there are limitations
    to that length. Once again, the quality of the text generation was limited by
    the number of input tokens supported by the algorithm, and a new breakthrough
    was needed.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，RNNs 在处理非常长的文本序列时存在不稳定问题。模型中的梯度往往会呈指数级增长（称为“梯度爆炸”）或减小为零（称为“梯度消失”），这阻碍了模型继续从训练数据中学习。LSTMs
    和 GRUs 缓解了梯度消失问题，但并没有完全消除。因此，即使在理论上它们的架构允许任意长度的输入，但在实践中存在长度限制。文本生成的质量再次受到算法支持的输入标记数量的限制，需要新的突破。
- en: In 2017, the [paper](https://arxiv.org/abs/1706.03762) that introduced Transformers
    was released by Google, and we entered a new era in text generation. The architecture
    used in Transformers allows a huge increase in the number of input tokens, eliminates
    the gradient instability issues seen in RNNs, and is highly parallelizable, which
    means that it is able to take advantage of the power of GPUs. Transformers are
    widely used today, and they’re the technology chosen by OpenAI for their latest
    GPT text generation models.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年，[论文](https://arxiv.org/abs/1706.03762) 介绍了 Transformers，谷歌发布了这一技术，我们进入了文本生成的新纪元。Transformers
    的架构允许输入标记数量的大幅增加，消除了RNNs中出现的梯度不稳定问题，并且高度可并行化，这意味着它能够利用GPU的强大性能。如今，Transformers
    被广泛使用，它们是 OpenAI 为其最新的 GPT 文本生成模型所选择的技术。
- en: 'Transformers are based on the “attention mechanism,” which allows the model
    to pay more attention to some inputs than others, regardless of where they show
    up in the input sequence. For example, let’s consider the following sentence:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 基于“注意力机制”，它允许模型对某些输入给予比其他输入更多的关注，无论这些输入在输入序列中的位置如何。例如，我们来考虑以下句子：
- en: '![](../Images/1c625b09b52dfa8ee6159958d4b05fd5.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c625b09b52dfa8ee6159958d4b05fd5.png)'
- en: In this scenario, when the model is predicting the verb “bought,” it needs to
    match the past tense of the verb “went.” In order to do that, it has to pay a
    lot of attention to the token “went.” In fact, it may pay more attention to the
    token “went” than to the token “and,” despite the fact that “went” appears much
    earlier in the input sequence.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，当模型预测动词“bought”时，它需要与动词“went”的过去式进行匹配。为了实现这一点，它必须非常关注“went”这个词。实际上，它可能会比对“went”这个词的注意力更强，即使“went”在输入序列中出现得更早。
- en: 'This selective attention behavior in GPT models is enabled by a novel idea
    in the 2017 paper: the use of a “masked multi-head attention” layer. Let’s break
    down this term, and dive deeper into each of its sub-terms:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 模型中的这种选择性注意行为得益于 2017 年论文中的一个新颖概念：使用“掩码多头注意力”层。让我们来解析这个术语，并深入探讨它的每个子术语：
- en: '**Attention**: An “attention” layer contains a matrix of weights representing
    the strength of the relationship between all pairs of token positions in the input
    sentence. These weights are learned during training. If the weight that corresponds
    to a pair of positions is large, then the two tokens in those positions greatly
    influence each other. This is the mechanism that enables the Transfomer to pay
    more attention to some tokens than others, regardless of where they show up in
    the sentence.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意力**: “注意力”层包含一个权重矩阵，表示输入句子中所有标记位置对之间的关系强度。这些权重在训练过程中学习得到。如果对应一对位置的权重大，则这两个位置上的标记会相互影响很大。这是使得
    Transformer 能够对一些标记给予更多关注的机制，无论这些标记在句子中出现的位置如何。'
- en: '**Masked**: The attention layer is “masked” if the matrix is restricted to
    the relationship between each token position and earlier positions in the input.
    This is what GPT models use for text generation, because an output token can only
    depend on the tokens that come before it.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**掩码**: 如果注意力层的矩阵仅限于每个标记位置与输入中的早期位置之间的关系，则称为“掩码”。这就是 GPT 模型在文本生成中使用的方式，因为一个输出标记只能依赖于它之前的标记。'
- en: '**Multi-head**: The Transformer uses a masked “multi-head” attention layer
    because it contains several masked attention layers that operate in parallel.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**多头**: Transformer 使用了一个带有掩码的“多头”注意力层，因为它包含多个并行工作的掩码注意力层。'
- en: The memory cell of LSTMs and GRUs also enables later tokens to remember some
    aspects of earlier tokens. However, if two related tokens are very far apart,
    the gradient issues could get in the way. Transformers don’t have that problem
    because each token has a direct connection to all other tokens that precede it.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 和 GRU 的记忆单元也使得后续的标记能够记住一些早期标记的方面。然而，如果两个相关的标记距离非常远，梯度问题可能会成为障碍。Transformers
    没有这个问题，因为每个标记与所有在它之前的其他标记都有直接连接。
- en: Now that you understand the main ideas of the Transformer architecture used
    in GPT models, let’s take a look at the distinctions between the various GPT models
    that are currently available.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了 GPT 模型中使用的 Transformer 架构的主要思想，让我们来看一下目前可用的各种 GPT 模型之间的区别。
- en: '**How different GPT models are implemented**'
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**不同 GPT 模型的实现方式**'
- en: At the time of writing, the three latest text generation models released by
    OpenAI are GPT-3.5, ChatGPT, and GPT-4, and they are all based on the Transformer
    architecture. In fact, “GPT” stands for “Generative Pre-trained Transformer.”
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写时，OpenAI 发布的三款最新文本生成模型是 GPT-3.5、ChatGPT 和 GPT-4，它们都基于 Transformer 架构。实际上，“GPT”代表“生成预训练变换器”。
- en: GPT-3.5 is a transformer trained as a completion-style model, which means that
    if we give it a few words as input, it’s capable of generating a few more words
    that are likely to follow them in the training data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3.5 是一个训练为完成式模型的 Transformer，这意味着如果我们给它几个词作为输入，它能够生成几个可能跟随这些词的词。
- en: ChatGPT, on the other hand, is trained as a conversation-style model, which
    means that it performs best when we communicate with it as if we’re having a conversation.
    It’s based on the same transformer base model as GPT-3.5, but it’s fine-tuned
    with conversation data. Then it’s further fine-tuned using Reinforcement Learning
    with Human Feedback (RLHF), which is a technique that OpenAI introduced in their
    [2022 InstructGPT paper](https://arxiv.org/abs/2203.02155). In this technique,
    we give the model the same input twice, get back two different outputs, and ask
    a human ranker which output it prefers. That choice is then used to improve the
    model through fine-tuning. This technique brings alignment between the outputs
    of the model and human expectations, and it’s critical to the success of OpenAI’s
    latest models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，ChatGPT 被训练为对话风格的模型，这意味着当我们像进行对话一样与它交流时，它的表现最佳。它基于与 GPT-3.5 相同的 transformer
    基础模型，但经过对话数据的微调。然后，通过使用人类反馈的强化学习（RLHF）进一步微调，这是一种 OpenAI 在其 [2022 InstructGPT 论文](https://arxiv.org/abs/2203.02155)
    中引入的技术。在这一技术中，我们将相同的输入提供给模型两次，得到两个不同的输出，并询问人类评分者哪个输出更好。这个选择然后用于通过微调来改进模型。这项技术使模型的输出与人类期望保持一致，并且对
    OpenAI 最新模型的成功至关重要。
- en: GPT-4 on the other hand, can be used both for completion and conversation, and
    has its own entirely new base model. This base model is also fine-tuned with RLHF
    for better alignment with human expectations.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，GPT-4 可用于完成任务和对话，并且拥有全新的基础模型。该基础模型也经过 RLHF 微调，以更好地符合人类期望。
- en: '**Writing code that uses GPT models**'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**编写使用 GPT 模型的代码**'
- en: 'You have two options to write code that uses GPT models: you can use the [OpenAI
    API directly](https://platform.openai.com/docs/api-reference/introduction), or
    you can use the [OpenAI API on Azure](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/chatgpt-quickstart?pivots=programming-language-studio&tabs=command-line).
    Either way, you write code using the same API calls, which you can learn about
    in OpenAI’s [API reference](https://platform.openai.com/docs/api-reference/introduction)
    pages.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你有两个选项来编写使用 GPT 模型的代码：你可以直接使用 [OpenAI API](https://platform.openai.com/docs/api-reference/introduction)，或者使用
    [Azure 上的 OpenAI API](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/chatgpt-quickstart?pivots=programming-language-studio&tabs=command-line)。无论哪种方式，你都使用相同的
    API 调用，你可以在 OpenAI 的 [API 参考](https://platform.openai.com/docs/api-reference/introduction)
    页面中了解这些调用。
- en: 'The main difference between the two is that Azure provides the following additional
    features:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 两者之间的主要区别在于 Azure 提供了以下附加功能：
- en: Automated responsible AI filters that mitigate unethical uses of the API
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化的负责任 AI 过滤器，减轻 API 的不道德使用
- en: Azure’s security features, such as private networks
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure 的安全功能，如私人网络
- en: Regional availability, for the best performance when interacting with the API
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区域可用性，以在与 API 交互时获得最佳性能
- en: 'If you’re writing code that uses these models, you’ll need to pick the specific
    version you want to use. Here’s a quick cheat-sheet with the versions that are
    currently available in the Azure OpenAI Service:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在编写使用这些模型的代码，你需要选择你想使用的具体版本。以下是 Azure OpenAI 服务中当前可用版本的快速备忘单：
- en: 'GPT-3.5: text-davinci-002, text-davinci-003'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GPT-3.5: text-davinci-002, text-davinci-003'
- en: 'ChatGPT: gpt-35-turbo'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ChatGPT: gpt-35-turbo'
- en: 'GPT-4: gpt-4, gpt-4–32k'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GPT-4: gpt-4, gpt-4–32k'
- en: 'The two GPT-4 versions differ mainly in the number of tokens they support:
    gpt-4 supports 8,000 tokens, and gpt-4–32k supports 32,000\. In contrast, the
    GPT-3.5 models only support 4,000 tokens.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 两个 GPT-4 版本的主要区别在于它们支持的 token 数量：gpt-4 支持 8,000 个 token，而 gpt-4–32k 支持 32,000
    个 token。相比之下，GPT-3.5 模型仅支持 4,000 个 token。
- en: Since GPT-4 is currently the most expensive option, it’s a good idea to start
    with one of the other models, and upgrade only if needed. For more details about
    these models, check out [the documentation](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/models).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 GPT-4 目前是最昂贵的选项，因此最好从其他模型之一开始，只有在需要时才升级。有关这些模型的更多详细信息，请查看 [文档](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/models)。
- en: '**Conclusion**'
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**结论**'
- en: In this article, we’ve covered the fundamental principles common to all generative
    language models, and the distinctive aspects of the latest GPT models from OpenAI
    in particular.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们涵盖了所有生成性语言模型的基本原则，以及特别是 OpenAI 最新 GPT 模型的独特方面。
- en: 'Along the way, we emphasized the core idea of language models: “*n* tokens
    in, one token out.” We explored how tokens are broken up, and why they are broken
    up that way. And we traced the decades-long evolution of language models from
    the early days of Hidden Markov Models to the recent Transformer-based models.
    Finally, we described the three latest Transformer-based GPT models from OpenAI,
    how each of them is implemented, and how you can write code that makes use of
    them.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在过程中，我们强调了语言模型的核心理念：“*n* 个令牌输入，一个令牌输出。”我们探讨了令牌如何被拆分以及为何如此拆分。我们追溯了语言模型从早期的隐马尔可夫模型到近期的基于
    Transformer 的模型的数十年演变。最后，我们描述了 OpenAI 最新的三个基于 Transformer 的 GPT 模型，每个模型的实现方式，以及如何编写代码利用这些模型。
- en: By now, you should be well equipped to have informed conversations about GPT
    models, and to start using them in your own coding projects. I plan to write more
    of these explainers about language models, so please follow me and let me know
    which topics you’d like to see covered! Thank you for reading!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你应该已经能够对 GPT 模型进行有深度的讨论，并在自己的编码项目中开始使用它们。我计划撰写更多关于语言模型的解释文章，请关注我，并告诉我你希望看到哪些话题！感谢阅读！
- en: '**Note**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: All images are by the author unless otherwise noted. You can use any of the
    original images in this blog post for any purpose, with attribution (a link to
    this article).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图片均由作者提供，除非另有说明。你可以在本博客文章中出于任何目的使用原始图片，但需注明出处（链接到本文）。
