["```py\n$ docker --version\n# Output: $ Docker version 20.10.23\n```", "```py\n$ docker-compose --version\n# Output: Docker Compose version v2.15.1\n```", "```py\n$ sh start_docker_stack.sh\n```", "```py\n$ sh stop_docker_stack.sh\n```", "```py\nn = 1000\ntest_size = 0.25\ndata_seed = 73 \n\nX, y = datasets.make_moons(\n    n_samples = n, \n    noise = 0.25, \n    random_state = data_seed)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size = test_size,\n    random_state = 42)\n\nplt.scatter(\n    x = X[:,0], \n    y = X[:,1], \n    s = 40, \n    c = y, \n    cmap = plt.cm.Accent);\n```", "```py\nwith mlflow.start_run(run_name='random_forest_model') as run:\n    model_rf = RandomForestClassifier(random_state = 42)\n    model_rf.fit(X_train, y_train)\n    y_pred = model_rf.predict(X_test)\n\n    # metrics\n    precision_0 = classification_report(\n    y_true=y_test, \n    y_pred=y_pred, \n    target_names=target_names, \n    output_dict=True)[\"0\"][\"precision\"]\n\n    ...\n\n    f1_score_1 = classification_report(\n    y_true=y_test, \n    y_pred=y_pred, \n    target_names=target_names, \n    output_dict=True)[\"1\"][\"f1-score\"]\n\n    err, acc, tpr, tnr, fnr, fpr = get_confusion_matrix_metrics(y_test=y_test, y_pred=y_pred)\n\n    # log metrics\n    mlflow.log_metric(\"precision_0\", precision_0)\n\n    ...\n\n    mlflow.log_metric(\"f1_score_1\", f1_score_1)\n\n    mlflow.log_metric(\"err\", err)\n    mlflow.log_metric(\"acc\", acc)\n    mlflow.log_metric(\"tpr\", tpr)\n    mlflow.log_metric(\"tnr\", tnr)\n    mlflow.log_metric(\"fnr\", fnr)\n    mlflow.log_metric(\"fpr\", fpr)\n\n    ...\n\n    mlflow.log_artifact(\"logging_example.ipynb\")\n```"]