- en: Bring Correctness Back to Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/bring-correctness-back-to-machine-learning-a56a96262f17](https://towardsdatascience.com/bring-correctness-back-to-machine-learning-a56a96262f17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Are we building our field on wrong assumptions?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mattiadigangi?source=post_page-----a56a96262f17--------------------------------)[![Mattia
    Di Gangi](../Images/ccd89021df6724797d45cc3c655a38a5.png)](https://medium.com/@mattiadigangi?source=post_page-----a56a96262f17--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a56a96262f17--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a56a96262f17--------------------------------)
    [Mattia Di Gangi](https://medium.com/@mattiadigangi?source=post_page-----a56a96262f17--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a56a96262f17--------------------------------)
    ·9 min read·Oct 13, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4abe69e4718d076a4a8eb6168a88bd6.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Andrea De Santis](https://unsplash.com/@santesson89?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'Introduction: What Correctness?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Research papers are still the main way to communicate new findings in the machine
    learning space. Yet, it happens quite frequently that a paper’s results cannot
    be reproduced and it’s not clear why.
  prefs: []
  type: TYPE_NORMAL
- en: Here I want to bring my take on the pros and cons of research papers as a communication
    tool. I will present my take on the role of science and scientific production
    in the developing of collective human knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: A recent paper found a hole in the process of research publication itself. I
    find this paper very compelling and I will present the main claims to you with
    the goal of raising the awareness about the role of **code correctness** in the
    spreading of machine learning knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: The problem is not limited to machine learning, but it is a field where many
    researchers don’t have strong engineering skills — and often try to run away from
    engineering work — which causes problems in having reliable, correct, usable,
    working software.
  prefs: []
  type: TYPE_NORMAL
- en: Do you work with machine learning and read research papers to find new ideas?
    Then, this article will help you being more critical of what you read in a principled
    way.
  prefs: []
  type: TYPE_NORMAL
- en: Are you a researcher and an author of research papers? I hope you will find
    the topic interesting, read the cited paper, and contribute to the discussion.
  prefs: []
  type: TYPE_NORMAL
- en: Now, enough introduction, let’s get into jump into the discussion!
  prefs: []
  type: TYPE_NORMAL
- en: Science is Knowledge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Science as a profession is [more popular](https://www.theatlantic.com/science/archive/2018/11/diminishing-returns-science/575665/)
    than ever according to all metrics that allow us to measure it: number of scientists
    (and Ph.D.), available funds, grant applications, and so on. A growing portion
    of all scientists consists of researchers in machine learning, theoretical and
    even more applied.'
  prefs: []
  type: TYPE_NORMAL
- en: What is the job of a scientist? It is to discover new knowledge in a field of
    study, thus expanding the horizon of what is known by humankind, by growing or
    disproving existing knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases a scientist builds on top of existing knowledge to advance one
    step further. In some cases, some previous scientific evidence turns out to be
    wrong or misleading. For instance, the chosen sample could not be representative
    of the population at large and the results would then not generalize.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason can be that a study was performed under specific conditions and
    its results extrapolated to different conditions. An example in machine learning
    is when a method turns out to outperform the state of the art… but only when the
    training set is artificially small. In normal data conditions, it does not perform
    as well as the baseline.
  prefs: []
  type: TYPE_NORMAL
- en: The main artifact to disseminate scientific discoveries, although considered
    obsolete by [some](https://www.theatlantic.com/science/archive/2018/04/the-scientific-paper-is-obsolete/556676/),
    is without doubts the peer-reviewed research paper. With research papers, scientists
    describe their discoveries in a structured and organized way. They describe the
    field of the discovery, the problem they address, the holes in the field of knowledge,
    their hypotheses, and their experiments that aim to bring evidence in favor of
    the hypothesis or disprove it.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the description, other scientists decide if the paper is trustworthy
    and the discovery is worth to be published, in a process called peer review. Note
    that this is a tricky and imperfect system, and many times the peer review fails
    in publishing worthy research or, the other way round, allows for the publication
    of sub-par papers. No system is perfect and this is part of the game. A game that
    is the main process of scientific dissemination, and mostly decides the future
    of research funds and researchers’ careers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, a number of questions can be raised about the fundamentals of the process:
    “how can we trust claims in a paper?”, “is one paper sufficient to consider a
    piece of knowledge as acquired?”, “what if a new paper contradicts the results
    of previously published papers?”'
  prefs: []
  type: TYPE_NORMAL
- en: To answer such questions we need to introduce some concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Reproducibility, Soundness, Correctness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine learning conferences are placing much emphasis on the problem of reproducibility,
    and they are absolutely right to do so. Other scientific fields, most notably
    psychology and medicine, have suffered serious credibility issues after their
    famous “[replication crisis](https://en.wikipedia.org/wiki/Replication_crisis)”:
    paper results could not be replicated by different independent researchers, casting
    a lot of doubts on the entire work of the field. However, the problem goes well
    beyond the two mentioned fields.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd4fe1c8821c677ee944d85f1dbf1e02.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Julia Koblitz](https://unsplash.com/@jkoblitz?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Though utterly important, reproducibility is only a part of the process to produce
    new scientific knowledge. Machine learning conferences often ask reviewers to
    also evaluate the “soundness” of the experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us try to define them in more clear terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reproducibility** is the possibility of reproducing a scientific study by
    another group by following the protocol explained into the paper. This implies
    that all the relevant details are described in the paper, maybe the software used
    is released (we are talking about ML here), and is possible to acquire the same
    training data.'
  prefs: []
  type: TYPE_NORMAL
- en: A study is not reproducible if independent researchers follow the described
    methodology but their results differ significantly from those described in the
    paper.
  prefs: []
  type: TYPE_NORMAL
- en: '**Soundness** is a judgment about the correctness of the protocol. Are the
    experiments coherent with the hypotheses to prove? Do the results actually show
    what the authors claim? Are the experiments biased or incomplete?'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the soundness score reflects the technical correctness of the study
    from a scientific point of view.
  prefs: []
  type: TYPE_NORMAL
- en: These two aspects are widely discussed in ML conferences, but there is a third
    aspect that is less discussed yet relevant.
  prefs: []
  type: TYPE_NORMAL
- en: '**Code Correctness** is a yes/ no question: does the code actually implement
    the methodology described in the paper?'
  prefs: []
  type: TYPE_NORMAL
- en: Code correctness is usually given for granted, and as such is not checked nor
    enforced.
  prefs: []
  type: TYPE_NORMAL
- en: Building Correct Knowledge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Reproducibility is fundamental to build knowledge. Let me repeat it: results
    that cannot be independently replicated do not represent scientific knowledge.
    They do not necessarily come because of a fraud (though sometimes they are), but
    can be the fruit of some details considered “minor” by the authors and not worth
    describing, which are instead important with the proposed method, or even more
    important than the proposed method to obtain the claimed results.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, correctness is fundamental to build correct knowledge. If
    we read a paper and find interesting results, then we try to reproduce them and
    obtain exactly the same results, we can consider ourselves happy with our newly
    gained knowledge about the domain.
  prefs: []
  type: TYPE_NORMAL
- en: What if the reference implementation hides some “bugs” and it does not follow
    the described idea in some fundamental ways?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c3d1d4670393089d3812a727f31e779.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Dmitry Bukhantsov](https://unsplash.com/@bdv91?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In that case, we are building knowledge on top of wrong facts. Stack up many
    wrong facts and we don’t know anymore what is important and what not. Quite scary
    for people like us that want to be “experts” in a field, isn’t it?
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem is addressed in the paper [When Good and Reproducible Results are
    a Giant with Feet of Clay: The Importance of Software Quality in NLP](https://arxiv.org/abs/2303.16166)
    by Papi and colleagues [1], which takes as an example some widely-used implementations
    of Conformer [2]. Such implementations in common open source frameworks sped up
    research for many groups that did not need to implement conformer and training
    recipes for it by themselves. The problem is that all the implementations under
    exam contain three categories of bugs that show an impact on results when the
    inference batch size is larger than one. Note that, in a normal situation, inference
    batch size should affect the resource usage during inference, and thus the inference
    speed, but not the output.'
  prefs: []
  type: TYPE_NORMAL
- en: For detailed information about the bugs, I strongly recommend reading the paper.
    Here, I just want to highlight that the differences are usually small enough to
    not notice there is a problem in the implementation, and that’s probably the reason
    why they went unnoticed for so long. Yet, in one case the degradation is huge
    when the batch size is very large.
  prefs: []
  type: TYPE_NORMAL
- en: Some may wonder why we should care about a few decimals of BLEU or WER, but
    I would say this view misses the point. This paper only takes the example of **popular**
    implementations of conformer used by hundreds or thousands of researchers and
    practitioners around the world. Yet, it finds bugs in all of them, including one
    that is not easy to solve while keeping an efficient implementation.
  prefs: []
  type: TYPE_NORMAL
- en: And remember, “bug” is just the nice way that the software industry calls software
    errors. A lovely “bug” is always a discrepancy between software requirements and
    its actual behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Just imagine what happens with all the private implementation of the hundreds
    of deep learning networks proposed every week. In many cases they are developed
    in small groups, or by single developers, with nobody reviewing their code. The
    matching between implementation and description in the paper is left to the author,
    and is not even a point of discussion in scientific peer reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then the problem is: under these assumptions, how can we reliably trust the
    claims in the papers we read?'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there are no clear answers to this question. The authors of [1]
    propose a set of solutions inspired by how software development tackles the correctness
    problem. I think it goes towards the correct direction but unfortunately too many
    people weigh too much the perceived additional work (what about the cost of having
    wrong software?) and I don’t see it being widely adopted soon.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The topic of good science is definitely a harsh territory. So many different
    aspects are involved in research work and the last thing the researchers want
    is to have more work to do for the same number of publications.
  prefs: []
  type: TYPE_NORMAL
- en: However, when we rely on incorrect software, we are building wrong knowledge
    and maybe follow wrong directions based on that. Imagine working one year or two
    with a software tool that you consider reliable, then a bug is raised by someone
    and, when it is fixed, the performance of your new models suddenly drop. I think
    nobody would like to end up in this situation. And we know from the best engineering
    practices that errors should be found early in the process to minimize their cost.
  prefs: []
  type: TYPE_NORMAL
- en: A machine learning testing tool like the one proposed in the paper can help
    researchers to produce more correct software in an easy way, and I am really looking
    forward to seeing it released. If open source, the whole community can contribute
    and raise the coding standards overall.
  prefs: []
  type: TYPE_NORMAL
- en: What is your opinion? Do you have solutions to enforce correctness in ML research?
    Let me know in the comments!
  prefs: []
  type: TYPE_NORMAL
- en: '*And if you read till this point, thank you a lot for your time! I know your
    time is limited and precious and still you decided to devote some to read my thoughts,
    it is really appreciated!*'
  prefs: []
  type: TYPE_NORMAL
- en: More from me
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](/tips-for-reading-and-writing-an-ml-research-paper-a505863055cf?source=post_page-----a56a96262f17--------------------------------)
    [## Tips for Reading and Writing an ML Research Paper'
  prefs: []
  type: TYPE_NORMAL
- en: Lessons learned by dozens of peer reviews given and received
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/tips-for-reading-and-writing-an-ml-research-paper-a505863055cf?source=post_page-----a56a96262f17--------------------------------)
    [](/without-further-ado-automate-dev-environments-and-build-f2f9bcaaae1e?source=post_page-----a56a96262f17--------------------------------)
    [## Without Further Ado: Automate Dev Environments and Build'
  prefs: []
  type: TYPE_NORMAL
- en: Bring joy to your fellow developers by making your software easy to use through
    environment and build automation. With…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/without-further-ado-automate-dev-environments-and-build-f2f9bcaaae1e?source=post_page-----a56a96262f17--------------------------------)
    [](/3-common-bug-sources-and-how-to-avoid-them-182f9974d2ab?source=post_page-----a56a96262f17--------------------------------)
    [## 3 Common Bug Sources and How to Avoid Them
  prefs: []
  type: TYPE_NORMAL
- en: Some coding patterns are more prone to hide bugs. Writing high quality code
    and knowing how our brain works can help to…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/3-common-bug-sources-and-how-to-avoid-them-182f9974d2ab?source=post_page-----a56a96262f17--------------------------------)
    [](/introduction-to-speech-enhancement-part-1-df6098b47b91?source=post_page-----a56a96262f17--------------------------------)
    [## Introduction to Speech Enhancement: Part 1 — Concepts and Task Definition'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction into the concepts, methods, and algorithms that allow us to
    improve the quality of degraded speech or…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/introduction-to-speech-enhancement-part-1-df6098b47b91?source=post_page-----a56a96262f17--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Medium Membership
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Do you like my writing and are considering subscribing for a Medium Membership
    for having unlimited access to the articles?
  prefs: []
  type: TYPE_NORMAL
- en: If you subscribe through this link you will support me through your subscription
    with no additional cost for you [https://medium.com/@mattiadigangi/membership](https://medium.com/@mattiadigangi/membership)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Papi, S, et al. “When Good and Reproducible Results are a Giant with Feet
    of Clay: The Importance of Software Quality in NLP” arxiv.org/2303.16166'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Gulati, Anmol, et al. “[Conformer: Convolution-augmented Transformer for
    Speech Recognition.](https://arxiv.org/abs/2005.08100)” *Proc. Interspeech 2020*
    (2020): 5036–5040.'
  prefs: []
  type: TYPE_NORMAL
