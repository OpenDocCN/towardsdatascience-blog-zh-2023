- en: 'Jane the Discoverer: Enhancing Causal Discovery with Large Language Models
    (Causal Python)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《发现者简：利用大语言模型增强因果发现（因果Python）》
- en: 原文：[https://towardsdatascience.com/jane-the-discoverer-enhancing-causal-discovery-with-large-language-models-causal-python-564a63425c93](https://towardsdatascience.com/jane-the-discoverer-enhancing-causal-discovery-with-large-language-models-causal-python-564a63425c93)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/jane-the-discoverer-enhancing-causal-discovery-with-large-language-models-causal-python-564a63425c93](https://towardsdatascience.com/jane-the-discoverer-enhancing-causal-discovery-with-large-language-models-causal-python-564a63425c93)
- en: A practical guideline to LLM-enhanced causal discovery that minimizes the risks
    of hallucinations (with Python code)
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一份实用指南，介绍如何利用大语言模型增强因果发现，减少幻觉风险（附Python代码）
- en: '[](https://aleksander-molak.medium.com/?source=post_page-----564a63425c93--------------------------------)[![Aleksander
    Molak](../Images/7fca5018f6ce88297fae31cef1fe0e6c.png)](https://aleksander-molak.medium.com/?source=post_page-----564a63425c93--------------------------------)[](https://towardsdatascience.com/?source=post_page-----564a63425c93--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----564a63425c93--------------------------------)
    [Aleksander Molak](https://aleksander-molak.medium.com/?source=post_page-----564a63425c93--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://aleksander-molak.medium.com/?source=post_page-----564a63425c93--------------------------------)[![Aleksander
    Molak](../Images/7fca5018f6ce88297fae31cef1fe0e6c.png)](https://aleksander-molak.medium.com/?source=post_page-----564a63425c93--------------------------------)[](https://towardsdatascience.com/?source=post_page-----564a63425c93--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----564a63425c93--------------------------------)
    [Aleksander Molak](https://aleksander-molak.medium.com/?source=post_page-----564a63425c93--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----564a63425c93--------------------------------)
    ·18 min read·Oct 22, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----564a63425c93--------------------------------)
    ·18分钟阅读·2023年10月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/495b9dc6cb0d9d7c93e6b71f72139551.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/495b9dc6cb0d9d7c93e6b71f72139551.png)'
- en: Image by [Artem Podrez](https://www.pexels.com/@artempodrez/) at [Pexels.com](http://pexels.com)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Artem Podrez](https://www.pexels.com/@artempodrez/) 提供，来源于 [Pexels.com](http://pexels.com)
- en: “The world is everything that is the case.”
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “世界即一切存在的事物。”
- en: ''
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Ludwig Wittgenstein — Tractatus Logico-Philosophicus (1922)**'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**路德维希·维特根斯坦 — 《逻辑哲学论》（1922年）**'
- en: No baby understands the nature of motion at birth.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 婴儿在出生时无法理解运动的本质。
- en: We — humans — and many other non-human animals come to this world equipped with
    systems that help us learn about the environment, yet on the day of our birth
    we don’t know much about the environment itself¹.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们——人类——以及许多其他非人类动物，都天生拥有帮助我们了解环境的系统，但在我们出生时对环境本身知之甚少¹。
- en: We need to learn.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要学习。
- en: In this regard, we bear a similarity to machine learning systems.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，我们与机器学习系统有相似之处。
- en: Early discoveries in psychology and *(proto)-*neuroscience regarding learning
    in humans and other non-human animals became an inspiration to build artificial
    systems that can learn from experience.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 早期心理学和*(原始)*神经科学对人类和其他非人类动物学习的发现，激发了构建可以从经验中学习的人工系统的灵感。
- en: One of the most successful learning paradigms that led to the 2010s machine
    learning revolution was supervised learning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 20世纪10年代机器学习革命的最成功学习范式之一是监督学习。
- en: Neural networks trained in a supervised manner allowed us to move decades ahead
    with progress on long-standing problems like image classification or machine translation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 经过监督训练的神经网络使我们在图像分类或机器翻译等长期存在的问题上取得了几十年的进展。
- en: Humans, and other non-human animals are equipped in a learning apparatus that
    (conceptually², but not necessary in terms of implementation) resembles supervised
    learning³.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 人类和其他非人类动物具备一种学习机制（在概念上²，但不一定在实施上）类似于监督学习³。
- en: Yet, there’s also a fundamental difference between how babies and supervised
    algorithms learn.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，婴儿和监督算法的学习方式之间也存在根本性差异。
- en: '![](../Images/4140bfcabb62fa0605834fcb80a1973d.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4140bfcabb62fa0605834fcb80a1973d.png)'
- en: '**Fig. 1.** A drawing of a Purkinje neuron. These are one of the biggest neurons
    in human brain and they can be found in cerebellum. Drawing by Santiago Ramón
    y Cajal, around 1900\. Sou[rce: https://commons.wikimedia.org/wiki/Category:Santiago_Ram%C3%B3n_y_](https://commons.wikimedia.org/wiki/Category:Santiago_Ram%C3%B3n_y_Cajal)Cajal'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.** 一幅 Purkinje 神经元的图画。这些是人脑中最大的神经元之一，可以在小脑中找到。图画由 Santiago Ramón y Cajal
    绘制，约 1900 年。来源：[https://commons.wikimedia.org/wiki/Category:Santiago_Ram%C3%B3n_y_](https://commons.wikimedia.org/wiki/Category:Santiago_Ram%C3%B3n_y_Cajal)Cajal'
- en: Throwing Objects *(But Not* Against You)
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扔物体*（但不要*对着你）
- en: Have you ever seen a parent trying to convince their child to stop throwing
    around a toy? Some parents tend to interpret this type of behavior as rude, destructive,
    or aggressive, but babies often have a very different set of motivations ([Molak,
    2023](https://amzn.to/3M3R0YI)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你有没有见过父母试图说服孩子停止乱扔玩具？一些父母倾向于将这种行为解读为粗鲁、破坏性或攻击性，但婴儿往往有一套非常不同的动机（[Molak, 2023](https://amzn.to/3M3R0YI)）。
- en: They are running systematic experiments that allow them to learn the
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 他们正在进行系统实验，允许他们学习
- en: laws of physics and the rules of social interactions ([Gopnik, 2009](https://amzn.to/46z9Or1)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 物理定律和社会互动规则（[Gopnik, 2009](https://amzn.to/46z9Or1)）。
- en: Thanks to these actions babies can learn not only by observing static data (which
    they also can do), but also by interacting with the world.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 得益于这些行动，婴儿不仅可以通过观察静态数据（他们也可以这样做）来学习，还可以通过与世界互动来学习。
- en: Going Beyond Observations
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越观察
- en: Interacting with the world allows children to build world models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 与世界互动使孩子们能够建立世界模型。
- en: World models are hypotheses regarding data generating processes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 世界模型是关于数据生成过程的假设。
- en: A sufficiently rich world model allows us to answer interventional (*which decision
    will lead to the best outcome?*) and counterfactual (*what would have happened
    if I did X instead of Y?*) queries.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一个足够丰富的世界模型可以帮助我们回答干预性（*哪个决定将导致最佳结果？*）和反事实（*如果我做了 X 而不是 Y，会发生什么？*）查询。
- en: Supervised machine learning models (and other flavors of models based on associative
    principles) cannot answer these questions directly⁴.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 监督式机器学习模型（及其他基于关联原理的模型）不能直接回答这些问题⁴。
- en: Can We Teach Machines To Answer Causal Queries?
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们可以教机器回答因果查询吗？
- en: The short answer is “yes”.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 简短的回答是“可以”。
- en: There are a couple of ways to do this. In this post we’ll limit ourselves to
    cases where machines do not interact directly with the environment.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以做到这一点。在这篇文章中，我们将仅限于机器不直接与环境互动的情况。
- en: 'One popular causal query is: “*what would be the average effect of my treatment*
    ***T*** *if I administer it to a group of individuals with some characteristics*
    ***X***”.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一个流行的因果查询是：“*如果我对一组具有某些特征* ***X*** *的个体进行治疗* ***T*** *，那么治疗的平均效果会是什么*”。
- en: This is known as **heterogeneous treatment effect** or **conditional average
    treatment effect** (**CATE**).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为**异质处理效应**或**条件平均处理效应**（**CATE**）。
- en: In order to answer a query of this type, we’ll need to provide a causal model
    with the data and additional information about the structure of the data generating
    process.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这种类型的查询，我们需要提供因果模型以及关于数据生成过程结构的额外信息。
- en: The latter is usually encoded in a form of a **directed acyclic graph** (**DAG**)⁵.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 后者通常以**有向无环图**（**DAG**）⁵的形式编码。
- en: Traditionally, **DAG**s are (1) created manually, based on relevant expert knowledge,
    (2) using automated causal discovery⁶ or (3) a combination of both, in a (potentially
    iterative) process sometimes referred to as **human-in-the-loop causal discovery**.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，**DAG**（有向无环图）是（1）基于相关专家知识手动创建的，（2）使用自动化因果发现⁶，或（3）两者结合的过程，有时称为**人机交互因果发现**。
- en: 'There exist various families of causal discovery algorithms, each leveraging
    slightly different set of assumptions. See this post for an introduction:'
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 存在各种因果发现算法，每种算法利用的假设略有不同。请参见这篇文章以了解介绍：
- en: '[](/beyond-the-basics-level-up-your-causal-discovery-skills-in-python-now-2023-cabe0b938715?source=post_page-----564a63425c93--------------------------------)
    [## Causal Python — Level Up Your Causal Discovery Skills in Python (2023)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/beyond-the-basics-level-up-your-causal-discovery-skills-in-python-now-2023-cabe0b938715?source=post_page-----564a63425c93--------------------------------)
    [## Causal Python — 提升你的 Python 因果发现技能（2023）'
- en: …and unlock the potential of the best Causal Discovery package in Python!
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: …并解锁 Python 中最佳因果发现包的潜力！
- en: towardsdatascience.com](/beyond-the-basics-level-up-your-causal-discovery-skills-in-python-now-2023-cabe0b938715?source=post_page-----564a63425c93--------------------------------)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/beyond-the-basics-level-up-your-causal-discovery-skills-in-python-now-2023-cabe0b938715?source=post_page-----564a63425c93--------------------------------)'
- en: Dancing In The Rain
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 雨中舞蹈
- en: The task of building a DAG representing our problem of interest can sometimes
    be challenging, especially when we work with complex and open-ended systems⁷.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个表示我们感兴趣问题的有向无环图（DAG）的任务有时可能很具挑战性，尤其是当我们处理复杂且开放的系统时⁷。
- en: One of the challenges that businesses might face when building causal graphs
    is the challenge of collecting expert knowledge.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 企业在构建因果图时可能面临的一个挑战是收集专家知识的挑战。
- en: Collecting expert knowledge might be expensive, especially when we intend to
    do it in a systematic way. Experts might not be easily available and their time
    might be costly.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 收集专家知识可能代价昂贵，尤其是当我们打算以系统化的方式进行时。专家可能不容易获得，而且他们的时间可能很宝贵。
- en: I Want Large Language Models
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我想要大型语言模型
- en: Some authors (e.g., Kıcıman et al., 2023) proposed that **Large Language Models**
    (**LLM**s) could be potentially used for causal discovery.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一些作者（例如 Kıcıman et al., 2023）提出，**大型语言模型**（**LLM**s）可能有潜力用于因果发现。
- en: The promise of using general-purpose models to gain the understanding of causal
    structure is tempting — it opens the door for fast and cost-effective process.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用通用模型来获得因果结构的理解的前景令人心动——它为快速且具有成本效益的过程开辟了道路。
- en: Unfortunately, the usefulness of out-of-the-box LLMs for causal discovery seems
    to be limited as they exhibit unpredictable failure modes, hallucinations and
    fail to distinguish between causal and non-causal relationships between variables
    (Kıcıman et al., 2023; Zečević et al., 2023).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，现成的语言模型（LLMs）在因果发现中的有效性似乎有限，因为它们表现出不可预测的失败模式、幻觉，并且无法区分变量之间的因果关系和非因果关系（Kıcıman
    et al., 2023; Zečević et al., 2023）。
- en: In this episode of The **Causal Bandits Podcast**, we discuss the problems in
    LLMs and causal reasoning. **Audio-only** version is available [here](https://www.buzzsprout.com/2272512/13918140-causality-llms-abstractions-matej-zecevic-causal-bandits-ep-000).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在**因果强盗播客**这一集里，我们讨论了 LLMs 和因果推理中的问题。**仅音频**版本可在[这里](https://www.buzzsprout.com/2272512/13918140-causality-llms-abstractions-matej-zecevic-causal-bandits-ep-000)获取。
- en: One, recently growing in popularity way to counter hallucinations and failure
    modes in LLMs is **retrieval augmented generation** (**RAG**).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一种最近越来越受欢迎的应对 LLMs 幻觉和失败模式的方法是**检索增强生成**（**RAG**）。
- en: RAG is a set of techniques that provide the model with an external source of
    knowledge (a document or a corpus) and the model generates its response based
    on this source.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 是一组技术，它为模型提供外部知识来源（如文档或语料库），模型基于这些来源生成响应。
- en: This approach has been shown to [reduce hallucinations](https://www.pinecone.io/learn/retrieval-augmented-generation/)
    and help the model stay relevant.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法已被证明能[减少幻觉](https://www.pinecone.io/learn/retrieval-augmented-generation/)并帮助模型保持相关性。
- en: 'To learn more about RAG and why it is important, see this post by my colleague
    [Anthony Alcaraz](https://medium.com/u/30bc9ffd2f4b?source=post_page-----564a63425c93--------------------------------):'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 要了解更多关于 RAG 及其重要性的内容，请参阅我同事[Anthony Alcaraz](https://medium.com/u/30bc9ffd2f4b?source=post_page-----564a63425c93--------------------------------)的这篇文章：
- en: '[](https://ai.plainenglish.io/why-retrieval-augmentation-is-critical-for-enterprise-llms-and-which-current-model-to-choose-9ce5d9fa3817?source=post_page-----564a63425c93--------------------------------)
    [## Why Retrieval Augmentation is Critical for Enterprise LLMs and which current
    model to choose ?'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 为什么检索增强对企业 LLMs 至关重要，以及选择哪个当前模型？](https://ai.plainenglish.io/why-retrieval-augmentation-is-critical-for-enterprise-llms-and-which-current-model-to-choose-9ce5d9fa3817?source=post_page-----564a63425c93--------------------------------)'
- en: Large language models (LLMs) have demonstrated immense potential to revolutionize
    workflows in corporate settings.
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）展示了在企业环境中彻底改变工作流程的巨大潜力。
- en: ai.plainenglish.io](https://ai.plainenglish.io/why-retrieval-augmentation-is-critical-for-enterprise-llms-and-which-current-model-to-choose-9ce5d9fa3817?source=post_page-----564a63425c93--------------------------------)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[ai.plainenglish.io](https://ai.plainenglish.io/why-retrieval-augmentation-is-critical-for-enterprise-llms-and-which-current-model-to-choose-9ce5d9fa3817?source=post_page-----564a63425c93--------------------------------)'
- en: Two Perspectives
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 两种视角
- en: There’s a fundamental difference between how LLMs and traditional causal discovery
    algorithms approach the task of finding causal structure.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 和传统因果发现算法在寻找因果结构的任务中存在根本差异。
- en: Traditional causal discovery algorithms leverage a combination of observational,
    interventional or mixed data and inductive biases encoded in a form of assumptions
    in order to infer from the data information about the structure of the process
    that generated this data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的因果发现算法利用观测、干预或混合数据的组合以及以假设形式编码的归纳偏差，从数据中推断生成这些数据的过程的结构信息。
- en: LLMs — on the other hand — do not look at the (measurement) data at all. They
    leverage learned semantic representations of concepts and transform them in order
    to answer the queries about possible causal relationships between these concepts.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，LLM 完全不看（测量）数据。它们利用概念的学习语义表示并进行转换，以回答关于这些概念之间可能的因果关系的查询。
- en: One perspective on LLMs in this context is that they leverage **correlations
    of causal facts** (for more details see Zečević et al., 2023) in order to determine
    causal relationships between two or more entities.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，LLM 的一个视角是，它们利用**因果事实的相关性**（有关详细信息，请参见 Zečević et al., 2023），以确定两个或多个实体之间的因果关系。
- en: '![](../Images/a7495db6a7b49762ec9d37df882f3bca.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7495db6a7b49762ec9d37df882f3bca.png)'
- en: '**Fig 2**. Contextual equivalence of textual and measurement representations
    of causal facts. Source: [Zečević et al., 2023](https://arxiv.org/pdf/2308.13067.pdf).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2**。因果事实的文本和测量表示的上下文等效性。来源：[Zečević et al., 2023](https://arxiv.org/pdf/2308.13067.pdf)。'
- en: We talk about ***correlations*** of facts here, because LLMs are trained in
    an associative manner and as such they cannot be generally expected to operate
    on causally sound representations⁸.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里讨论***事实的相关性***，因为 LLM 是以联想的方式进行训练的，因此通常无法期望它们基于因果合理的表示进行操作⁸。
- en: Good news is that RAG can reduce the risk of failure modes and hallucinations
    by focusing the model response to the context of a provided corpus.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是 RAG 可以通过将模型响应集中于提供的语料库的上下文，减少失败模式和幻觉的风险。
- en: This opens up a path for a more meaningful role of LLMs in the causal discovery
    process.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这为 LLM 在因果发现过程中的更有意义的角色打开了道路。
- en: Documents and Measurements
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文档和测量
- en: Let’s imagine that we want to build a causal graph for a problem that interests
    us.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设我们想为一个感兴趣的问题建立一个因果图。
- en: We have measured a set of variables relevant to this problem over an extensive
    period of time. We also have expert knowledge regarding the problem stored in
    a number of documents.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一段时间内测量了一组与此问题相关的变量。我们还有存储在多个文档中的关于该问题的专家知识。
- en: We could simply run a causal discovery algorithm over the measurements, but
    that’s risky if we cannot guarantee that the algorithm’s assumptions are met.
    That would also mean throwing away valuable expert knowledge that we have at our
    disposal.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简单地在测量数据上运行因果发现算法，但如果无法保证算法的假设得到满足，这样做是有风险的。这也意味着抛弃我们手头上宝贵的专家知识。
- en: On the other hand, we could use a RAG-LLM to query the corpus and parse the
    model outputs to obtain a graph.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们可以使用 RAG-LLM 查询语料库并解析模型输出以获取图形。
- en: This might still be risky, because even with RAG, we have no guarantees that
    the model will correctly answer the queries. Moreover, the knowledge stored in
    the corpus might be incomplete, meaning that some causal relationships present
    in the data might not be reflected in the corpus.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能仍然有风险，因为即使有 RAG，我们也不能保证模型会正确回答查询。此外，语料库中的知识可能不完整，这意味着数据中存在的一些因果关系可能在语料库中没有反映出来。
- en: Wanna Be Friends?
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 想成为朋友吗？
- en: An alternative approach could be to combine both methods and triangulate semantic
    and measurement-based perspectives.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一种替代方法可能是结合两种方法，三角测量语义和基于测量的视角。
- en: One way to achieve this is by querying the RAG-LLM system to extract causal
    relationships from textual representations, and then use the obtained information
    as expert knowledge that we inject into a causal discovery algorithm, that solely
    operates on the measurement data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一种实现方法是通过查询 RAG-LLM 系统从文本表示中提取因果关系，然后使用获得的信息作为专家知识，注入到仅在测量数据上操作的因果发现算法中。
- en: Fancy giving it a try?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 想尝试一下吗？
- en: Can Mehendretex Help Mountain Climbers?
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mehendretex 能帮助登山者吗？
- en: Let’s put the idea we’ve just described to the test and see how it works.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将刚刚描述的想法付诸实践，看看它如何运作。
- en: I used a canonical example of the relationship between altitude and temperature
    as an inspiration for our exercise.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了一个关于高度和温度关系的经典例子作为我们练习的灵感。
- en: To make the task more interesting, we’re going to add more variables to the
    problem and make one of them completely new to the LLM (neither the model itself
    nor the corpus of documents contain any prior knowledge about it).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使任务更加有趣，我们将向问题中添加更多变量，并使其中一个完全陌生于 LLM（模型本身和文档语料库中都没有关于它的先验知识）。
- en: We’ll be interested in finding a causal structure between a set of variables
    that can impact a mountain climber’s risk of death.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将关注找到一组变量之间的因果结构，这些变量可能影响登山者的死亡风险。
- en: In order to find the structure, we’ll use a dataset of synthetic measurements⁹,
    and prior knowledge about some of the variables available in Wikipedia (our corpus).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到结构，我们将使用合成测量数据⁹，并利用维基百科中提供的一些变量的先验知识（我们的语料库）。
- en: 'The variables in the problem statement are:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 问题陈述中的变量有：
- en: altitude
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 海拔
- en: temperature
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 温度
- en: oxygen density
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 氧气密度
- en: risk of death
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 死亡风险
- en: usage of Mehendretex (our imaginary treatment)
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehendretex 的使用（我们虚构的处理方法）
- en: The function of the latter variable — Mehendretex (nothing like this exists
    in reality) — is to stress test the LLM part of our system.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 后者变量——Mehendretex（现实中不存在类似的东西）——的功能是对我们系统中的 LLM 部分进行压力测试。
- en: We expect that under a relevant prompt a RAG-LLM will tell us that it cannot
    say **anything** about causal relationship between Mehendretex and any other variable.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预计，在相关提示下，一个 RAG-LLM 会告诉我们，它不能对 Mehendretex 和任何其他变量之间的因果关系说**任何**话。
- en: '*Fig 3* presents the ground truth DAG for our problem represented by a graph
    (left) and an adjacency matrix (right).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3* 展示了我们问题的真实结构 DAG，以图形（左）和邻接矩阵（右）的形式表示。'
- en: '![](../Images/054c3d17f487d797a9f5b3ebb391660a.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/054c3d17f487d797a9f5b3ebb391660a.png)'
- en: '**Fig 3.** The true structure underlying our problem represented as a graph
    (left) and as an adjacency matrix (right). Black fields in the matrix denote an
    edge, white fields — a lack of an edge. For instance, a black square with coordinates
    (4, 3) indicates that there’s an edge from node 4 to node 3.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 3.** 我们问题的真实结构表示为图形（左）和邻接矩阵（右）。矩阵中的黑色区域表示一条边，白色区域表示没有边。例如，坐标为 (4, 3) 的黑色方块表示从节点
    4 到节点 3 有一条边。'
- en: Where’s My Python?
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我的 Python 在哪里？
- en: Let’s code the problem.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写代码解决这个问题。
- en: '**Link to GitHub repo available at the end of the article.'
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**GitHub 仓库的链接见文章末尾。**'
- en: We’ll use `numpy` and `scipy` to generate the data, [**gCastle**](https://github.com/huawei-noah/trustworthyAI/tree/master/gcastle)
    will serve as our causal discovery library, OpenAI’s **GPT-4** as an LLM, and
    [**LangChain**](https://www.langchain.com/) as a framework that will help us feed
    relevant Wikipedia articles to GPT-4.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`numpy`和`scipy`生成数据，[**gCastle**](https://github.com/huawei-noah/trustworthyAI/tree/master/gcastle)
    将作为我们的因果发现库，OpenAI 的**GPT-4** 作为 LLM，[**LangChain**](https://www.langchain.com/)
    作为框架，帮助我们将相关的维基百科文章提供给 GPT-4。
- en: On top of this we’ll import `os` for managing the OpenAI key and `itertools.combinations`
    to help us generate variable pairs for our RAG-LLM agent.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，我们将导入`os`来管理 OpenAI 密钥，`itertools.combinations`来帮助我们生成 RAG-LLM 代理的变量对。
- en: Setup
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置
- en: 'Let’s import the libraries:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们导入库：
- en: '[PRE0]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s set the OpenAI key:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置 OpenAI 密钥：
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Data generation
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据生成
- en: Now, let’s generate the data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们生成数据。
- en: Note that we only generate fictional **measurement data**. The textual data
    that we’ll use are actual Wikipedia articles.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们仅生成虚构的**测量数据**。我们将使用的文本数据是实际的维基百科文章。
- en: In this sense, we use **semi-synthetic** data in this example.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个意义上讲，我们在这个例子中使用**半合成**数据。
- en: 'Let’ s start by defining a mapping between variable names and indices. This
    will help us later to encode expert knowledge obtained from the LLM in a matrix
    format:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从定义变量名称和索引之间的映射开始。这将帮助我们在后续中以矩阵格式编码从 LLM 获得的专家知识：
- en: '[PRE2]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we set the sample size and generate synthetic measurement data for our
    variables according to the structure presented in *Fig 3*:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设置样本大小，并根据*图 3*中展示的结构生成变量的合成测量数据：
- en: '[PRE3]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice that in order to make the dataset more realistic we use a combination
    of Normal and Half-normal distributions and combine it with clipping.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了使数据集更加逼真，我们使用了正态分布和半正态分布的组合，并与截断相结合。
- en: This will also make the job harder for the causal discovery algorithm.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这也将使因果发现算法的工作更加困难。
- en: 'Let’s stack all variables in a single matrix:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将所有变量堆叠到一个矩阵中：
- en: '[PRE4]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '…and define the structure from *Fig 3* in a matrix form:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: …并以矩阵形式定义*图 3*中的结构：
- en: '[PRE5]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Algorithmic causal discovery
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法因果发现
- en: Let’s start our discovery process by applying a classic PC algorithm to our
    measurement data.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将经典的PC算法应用于我们的测量数据来开始发现过程。
- en: 'We’ll use the gCastle implementation of the stable variant of PC:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用gCastle实现的PC稳定变体：
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The result is presented in *Fig 4*:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 结果展示在*图4*中：
- en: '![](../Images/cd1f0d55f6b1bf339bc98d6d686bc0c5.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd1f0d55f6b1bf339bc98d6d686bc0c5.png)'
- en: '**Fig 4.** Results of applying the PC algorithm to our measurement data. On
    the right we see the ground truth adjacency matrix; on the left, the matrix produced
    by the PC algorithm. Black “fields” in the matrix denote an edge, white fields
    — a lack of an edge. For example, a black square with coordinates (2, 1) indicates
    that there’s an edge from node 2 to node 1.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**图4**：将PC算法应用于我们的测量数据的结果。右侧是实际邻接矩阵；左侧是PC算法生成的矩阵。矩阵中的黑色“区域”表示一条边，白色区域表示没有边。例如，坐标为(2,
    1)的黑色方块表示从节点2到节点1有一条边。'
- en: As we can see, the results are not great to say the least. The algorithm identified
    some edges correctly (e.g., `(4, 3)`, `(0, 1)` or `(0, 2)`), but many edges remain
    undirected.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，结果并不理想。算法正确识别了一些边（例如，`(4, 3)`、`(0, 1)`或`(0, 2)`），但许多边仍然未定向。
- en: For instance, notice that in the matrix on the left we have an edge `(4, 3)`,
    but also `(3, 4)` which indicates that the model was able to figure out that there’s
    a connection between nodes 3 and 4, but it was not able to decide on the direction
    of this connection.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，在左侧的矩阵中，我们有一个边`(4, 3)`，但也有`(3, 4)`，这表明模型能够识别节点3和4之间的连接，但无法确定这一连接的方向。
- en: Knowing the underlying structure of our dataset, this is not surprising, as
    PC algorithm depends on conditional independence testing and cannot orient edges
    under certain structural configurations.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 了解我们数据集的基础结构，这并不令人惊讶，因为PC算法依赖于条件独立性测试，并且在某些结构配置下无法确定边的方向。
- en: To learn more about PC and why this is the case, check [**this blog post**](/beyond-the-basics-level-up-your-causal-discovery-skills-in-python-now-2023-cabe0b938715)
    for an introduction or *Part 3* of my [**recent book on causal modeling**](https://amzn.to/46EW9yB)
    for a more detailed treatment.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于PC的信息及其原因，请查看[**这篇博客文章**](/beyond-the-basics-level-up-your-causal-discovery-skills-in-python-now-2023-cabe0b938715)进行介绍或我[**最近的因果建模书籍的第3部分**](https://amzn.to/46EW9yB)以获得更详细的处理。
- en: When we look at the metrics, F1 score for this graph is equal to 0.53¹⁰.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看指标时，这张图的F1分数等于0.53¹⁰。
- en: Defining an LLM agent
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义一个LLM代理
- en: Let’s see what will happen when we add an RAG-LLM agent to the picture.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当我们将RAG-LLM代理添加到这个场景中会发生什么。
- en: First, we’ll define a prior knowledge object, which we’ll then use to store
    the knowledge extracted by our agent.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将定义一个先验知识对象，然后用它来存储我们的代理提取的知识。
- en: '[PRE7]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Next, let’s instantiate the agent.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们实例化代理。
- en: '[PRE8]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We will use GPT-4 as our model of choice. We set temperature to `0` in order
    to make model outputs as conservative as possible.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将选择GPT-4作为我们的模型。我们将温度设置为`0`，以使模型输出尽可能保守。
- en: 'Let’s add LangChain’s tool that will allow the model to access Wikipedia:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们添加LangChain的工具，使模型能够访问维基百科：
- en: '[PRE9]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, let’s instantiate the agent:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们实例化代理：
- en: '[PRE10]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We’ll use the agent to look through Wikipedia pages in order to find information
    about causal connections between the variables pertinent to our problem.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用代理浏览维基百科页面，以寻找与我们问题相关的变量之间的因果关系信息。
- en: The findings from this agent will then be fed to another instance of GPT-4.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个代理获得的发现将被传递给另一个GPT-4实例。
- en: This second GPT-4 instance will be tasked with interpreting the initial output
    and translating it into a format compatible with our prior knowledge object.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这个第二个GPT-4实例将负责解释初始输出，并将其转换为与我们之前的知识对象兼容的格式。
- en: 'In order to make the code more reusable, we’ll encapsulate it in a function:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使代码更具可重用性，我们将其封装在一个函数中：
- en: '[PRE11]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Notice how we first call the agent, using one prompt and then pass the output
    of this agent to another (non-agent) instance of an LLM that is initialized with
    another prompt.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们如何首先调用代理，使用一个提示，然后将这个代理的输出传递给另一个（非代理）LLM实例，该实例使用另一个提示进行初始化。
- en: In my experiments, this hierarchical approach led to better and more consistent
    results than just using a single agent.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的实验中，这种分层方法比仅使用单一代理获得了更好、更一致的结果。
- en: I also noticed that repeating the definition of causality to both agents was
    helpful in improving the results.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我还注意到，重复向两个代理解释因果关系对改善结果有帮助。
- en: In both prompts we use Pearl’s interventional definition of causation. Additionally,
    the first prompt specifies that we’re interested in a broad spectrum of causal
    effects, as long as they follow the basic definition.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个提示中，我们使用 Pearl 的干预因果定义。此外，第一个提示指定我们对广泛的因果效应感兴趣，只要它们符合基本定义。
- en: In the experiments, the latter helped the model to be more decisive in some
    cases.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，后者在某些情况下帮助模型变得更加果断。
- en: 'The function returns:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 函数返回：
- en: '`(0,1)` if the model decides that there’s a causal edge from `var_1` to `var_2`'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型判断存在从`var_1`到`var_2`的因果边，则为`(0,1)`
- en: '`(1,0)` if the model decides that the causal direction is from `var_2` to `var_1`'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型判断因果方向是从`var_2`到`var_1`，则为`(1,0)`
- en: '`(0,0)` if the model decides that there is **no** causal relationship between
    `var_1` and `var_2`'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型判断`var_1`和`var_2`之间**没有**因果关系，则为`(0,0)`
- en: '`(-1,-1)` if the model doesn’t know the answer (note that we emphasized in
    the prompt that the model should not come up with an answer if it doesn’t know
    it).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型不知道答案（请注意我们在提示中强调了模型如果不知道答案就不应给出答案），则为`(-1,-1)`
- en: LLM-enhanced causal discovery
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM 增强的因果发现
- en: We’re now ready to collect expert knowledge from Wikipedia using our hierarchical
    two-prompt system.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备使用我们的分层双提示系统从维基百科收集专家知识。
- en: 'We’ll loop over all possible combinations of variables, record model answers
    for each of them and store them in the prior knowledge object:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遍历所有可能的变量组合，记录每个组合的模型回答，并将其存储在先验知识对象中：
- en: '[PRE12]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that we only pass the information from the model to the prior knowledge
    object when the model decides that **there is** a causal relationship between
    variables (output is either `(0,1)` or `(1,0)`), but we skip it when the model
    claims **there’s no** causal relationship between the variables.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们仅在模型判断变量之间**存在**因果关系（输出为`(0,1)`或`(1,0)`）时，将信息从模型传递到先验知识对象，但当模型声称变量之间**没有**因果关系时，我们会跳过这一步。
- en: This decision is based on the observation that model was often overly cautious
    when it comes to claiming the existence of causal influence.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这个决定基于观察到模型在声称存在因果影响时通常过于谨慎。
- en: By ignoring model’s null findings, we essentially allow the causal discovery
    algorithm to decide if the relationship exists in such a case.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 通过忽略模型的空结果，我们实质上允许因果发现算法在这种情况下决定是否存在关系。
- en: Let’s see what the model was able to learn.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看模型能够学到什么。
- en: '*Fig 5* shows the results of our LLM agent compared to the ground truth.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5* 显示了我们的 LLM 代理与真实情况的对比结果。'
- en: '![](../Images/410851b6e4ff7a5412026030b62aebd5.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/410851b6e4ff7a5412026030b62aebd5.png)'
- en: '**Fig 5.** Prior knowledge retrieved by our LLM agent (left) and the ground
    truth graph (right).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.** 我们的 LLM 代理检索的先验知识（左）和真实情况图（右）。'
- en: As we can see, the model was able to learn some useful knowledge, but was not
    able to recreate the entire graph. This is partially because Wikipedia does not
    contain any information about our imagined treatment — Mehendretex.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，模型能够学到一些有用的知识，但未能重建整个图。这部分是因为维基百科没有包含我们设想的处理—Mehendretex 的信息。
- en: 'Let’s take a quick look at one of the logs from our model that involves Mehendretex:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速查看一下我们模型中涉及 Mehendretex 的一个日志：
- en: '[PRE13]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: It seems that the model correctly responded to the query that contained a term
    not present in the corpus the model was using (see the **bold** text in the box
    above).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎模型正确回应了包含不在模型使用的语料库中的术语的查询（见上框中的**粗体**文本）。
- en: 'We’re now ready to pass the collected knowledge to the causal discovery algorithm:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备将收集到的知识传递给因果发现算法：
- en: '[PRE14]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that we’re initializing a new instance of the PC algorithm and pass the
    `priori_knowledge` object containing the knowledge gathered by the LLM to the
    constructor.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们正在初始化 PC 算法的新实例，并将包含 LLM 收集到的知识的`priori_knowledge`对象传递给构造函数。
- en: PC will now re-calculate the results of all conditional independence tests,
    but will also take prior knowledge into account when orienting the edges.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: PC 现在将重新计算所有条件独立性测试的结果，但在确定边的方向时还会考虑先验知识。
- en: Ready to see the results?
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好查看结果了吗？
- en: '*Fig 6* shows the results of our RAG-LLM-induced PC algorithm.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6* 显示了我们 RAG-LLM 诱导的 PC 算法的结果。'
- en: '![](../Images/affa755ee213d2590822d09c03bd8bb2.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/affa755ee213d2590822d09c03bd8bb2.png)'
- en: '**Fig 6.** The results of the PC algorithms with RAG-LLM-based prior knowledge
    (left) vs the ground truth (right).'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 6.** 使用基于 RAG-LLM 的先验知识的 PC 算法结果（左）与真实情况（右）。'
- en: This looks pretty good!
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来非常好！
- en: 'There’s only one edge that remained undirected: the edge between nodes with
    indices `0` and `2` (altitude and temperature), with all other edges being oriented
    correctly.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一条边保持无向：索引为`0`和`2`（海拔和温度）的节点之间的边，所有其他边都被正确定向。
- en: F1 score for this output is 0.93, which is an improvement of 40 percentage points
    over PC without LLM assistance.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 该输出的F1分数为0.93，比没有LLM帮助的PC提高了40个百分点。
- en: Summarizing the results
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结结果
- en: By combining traditional causal discovery with RAG-LLM, we were able to get
    the results better than any single method has offered.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将传统的因果发现方法与RAG-LLM结合，我们能够获得比任何单一方法更好的结果。
- en: '*Fig 7* summarizes the results at each step of the process.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7* 总结了每个步骤的结果。'
- en: '![](../Images/fa0f0d4224ab61ec1cae807ac17cd538.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa0f0d4224ab61ec1cae807ac17cd538.png)'
- en: '**Fig 7\.** Adjacency matrices of model outputs at each stage (left column)
    vs the ground truth (right column) and respective values of F1 scores.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**图7** 模型输出在每个阶段的邻接矩阵（左列）与真实值（右列）以及相应的F1分数值。'
- en: Summary
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The idea to use large language models as “knowledge parsers” that can translate
    textual information into the format compatible with traditional causal discovery
    algorithms and expand the capabilities of the latter is powerful.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大型语言模型作为“知识解析器”，将文本信息转换为与传统因果发现算法兼容的格式，并扩展后者的能力，这一想法非常强大。
- en: Limitations
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: One of the main limitations of the system that we discussed in this blog post
    is the instability of LLM outputs. I ran around 20 iterations of the process presented
    here and LLM’s decisions were not always consistent between the runs (even with
    temperature set to zero).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这篇博客文章中讨论的系统主要限制之一是LLM输出的不稳定性。我运行了大约20次这里呈现的过程，LLM的决策在不同运行间并不总是保持一致（即使温度设置为零）。
- en: The PC algorithm comes with a set of theoretical guarantees, but they only hold
    for infinite sample sizes, which is of course never the case in practice. Moreover,
    default conditional independence test that we used in this post is not very powerful
    and might not work well in more complex cases.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: PC算法有一套理论保证，但这些保证仅适用于无限样本量，这在实际中当然是不可能的。此外，我们在这篇文章中使用的默认条件独立性测试并不十分强大，可能在更复杂的情况下表现不佳。
- en: PC algorithm requires a lack of hidden confounding in the data — a condition
    that not always can be guaranteed, especially in open-ended complex systems.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: PC算法要求数据中没有隐藏的混淆——这是一个不总是可以保证的条件，特别是在开放复杂系统中。
- en: For larger datasets, running the RAG-LLM part can quickly becomes costly as
    the system might be performing multiple API calls for each pair of variables behind
    the scenes. If you decide to run the code for your own use case, **be aware of
    the potential high API costs**.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较大的数据集，运行RAG-LLM部分可能会迅速变得昂贵，因为系统可能在幕后对每对变量进行多个API调用。如果你决定为自己的用例运行代码，**要注意潜在的高API费用**。
- en: My recommendation is to **set a limit** in your OpenAI account, before starting
    the experiments.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我的建议是**在你的OpenAI账户中设定一个限制**，在开始实验之前。
- en: Further improvements
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步改进
- en: Naturally, systems like this do not have to be limited to Wikipedia as a source
    of external knowledge and any corpus of documents can be used.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，这样的系统不必局限于维基百科作为外部知识来源，任何文档语料库都可以使用。
- en: Some of the instabilities in LLM output could be potentially addressed by creating
    better prompts.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 一些大型语言模型（LLM）输出的不稳定性可以通过创建更好的提示来潜在解决。
- en: The PC algorithm’s performance could be improved by using a more general, kernel-based
    conditional independence test. Other causal discovery algorithms could also be
    used instead of PC if that suits the data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用更通用的基于核的方法进行条件独立性测试，PC算法的性能可以得到改善。如果数据适合，其他因果发现算法也可以替代PC。
- en: In particular, when hidden confounding cannot be excluded, algorithms like **fast
    causal discovery** (**FCI**) can be a better choice.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是当无法排除隐藏混淆时，像**快速因果发现**（**FCI**）这样的算法可能是更好的选择。
- en: Querying the LLM with each possible pair of variables is not scalable and can
    be very expensive for larger datasets. A potential pre-selection mechanism could
    be based on a regular LLM, assessing the likelihood that two variables are causally
    connected. Combining multiple variables in a single query could help decrease
    the costs.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对每对可能的变量进行LLM查询不可扩展，对于较大的数据集可能非常昂贵。一个潜在的预选择机制可以基于常规LLM，评估两个变量是否有因果连接的可能性。在单个查询中结合多个变量可能有助于降低成本。
- en: The downside of this solution is that it comes with a higher risk of hallucinations,
    but can be effective in case of variables and relationships that are commonly
    known.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案的缺点是它伴随着更高的幻觉风险，但在处理常见变量和关系时可能是有效的。
- en: Conclusions
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Jane the Discoverer is a proof-of-concept idea. I consider it a causal assistant
    that can be useful for small to mid-scale graphs.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Jane the Discoverer 是一个概念验证的想法。我认为它是一个因果助手，适用于小型到中型图。
- en: For the educational purposes, the code in the blog is written in a way that
    suggests autonomous usage, but I find a human-in-the-loop paradigm more effective.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 出于教育目的，博客中的代码是以建议自主使用的方式编写的，但我发现人机交互范式更为有效。
- en: Congrats on reaching the end!
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你阅读完毕！
- en: How could we further improve Jane?
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何进一步改进 Jane？
- en: Share your thoughts and suggestions in the comment section below and stay causal!
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的评论区分享你的想法和建议，保持因果联系！
- en: 'If you liked this post, you might also enjoy:'
  id: totrans-211
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章，你可能也会喜欢：
- en: ''
  id: totrans-212
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 🔸 [Causal Bandits Podcast](https://causalbanditspodcast.com)
  id: totrans-213
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 🔸 [因果 Bandits 播客](https://causalbanditspodcast.com)
- en: ''
  id: totrans-214
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 🔸[Causal Python Weekly](https://bit.ly/46FF2N4) newsletter
  id: totrans-215
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 🔸[因果 Python 每周](https://bit.ly/46FF2N4)通讯
- en: '**Notebook** with **full code** for this post is available on GitHub:'
  id: totrans-216
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**笔记本**与本文的**完整代码**可以在 GitHub 上找到：'
- en: '[](https://github.com/AlxndrMlk/blogs-code/tree/main/Jane%20the%20Discoverer?source=post_page-----564a63425c93--------------------------------)
    [## blogs-code/Jane the Discoverer at main · AlxndrMlk/blogs-code'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/AlxndrMlk/blogs-code/tree/main/Jane%20the%20Discoverer?source=post_page-----564a63425c93--------------------------------)
    [## blogs-code/Jane the Discoverer at main · AlxndrMlk/blogs-code'
- en: Code for blog posts. Contribute to AlxndrMlk/blogs-code development by creating
    an account on GitHub.
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 博客文章的代码。通过在 GitHub 上创建帐户来贡献 AlxndrMlk/blogs-code 的开发。
- en: github.com](https://github.com/AlxndrMlk/blogs-code/tree/main/Jane%20the%20Discoverer?source=post_page-----564a63425c93--------------------------------)
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/AlxndrMlk/blogs-code/tree/main/Jane%20the%20Discoverer?source=post_page-----564a63425c93--------------------------------)
- en: Footnotes
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 脚注
- en: ¹ We could argue that on the day of birth, we actually know something about
    the world as many studies suggest that fetuses can learn and habituate (e.g.,
    James et al., 2022)
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 我们可以争辩说，在出生的那一天，我们实际上对世界有所了解，因为许多研究表明胎儿可以学习和习惯（例如，James 等，2022）
- en: ² Associative learning is prevalent among species. It’s present even in simple
    organisms like [*C. elegans*](https://en.wikipedia.org/wiki/Caenorhabditis_elegans).
    Note that in this text we use the term *associative learning* to describe any
    learning based on associations that does not include systematic interventions.
    This definition may be inconsistent with some definitions used in psychology and/or
    biology.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ² 联想学习在各个物种中普遍存在。即使在像[*C. elegans*](https://en.wikipedia.org/wiki/Caenorhabditis_elegans)这样的简单生物中也有出现。注意，在本文中我们使用术语*联想学习*来描述任何基于关联的学习，这不包括系统干预。这一定义可能与心理学和/或生物学中使用的一些定义不一致。
- en: ³ We could argue here about the differences in implementation and potential
    differences in sample efficiency between animals and supervised machine learning
    algorithms in the context of associative learning. As a side note, I think the
    topic of sample efficiency is very interesting. At the same time, I believe that
    relevant answers are more complex than popular statements like “a child can learn
    what a dog is from two examples, while a CNN or Transformer architecture requires
    millions”.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ³ 我们可以在这里讨论在联想学习背景下动物与监督机器学习算法之间的实现差异及潜在的样本效率差异。附带说明，我认为样本效率的话题非常有趣。同时，我相信相关的答案比“一个孩子可以通过两个例子了解什么是狗，而CNN或Transformer架构需要数百万个例子”这样的流行说法要复杂得多。
- en: ⁴ This is also includes self-supervised and some reinforcement learning models.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴ 这也包括自监督学习和一些强化学习模型。
- en: ⁵ By using more advanced or more general identification methods we can extend
    this to partially directed or cyclic graphs.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵ 通过使用更先进或更通用的识别方法，我们可以将其扩展到部分有向或循环图。
- en: ⁶ Causal discovery is a set of methods aiming at recovering the structure of
    the data-generating process from observational, interventional or mixed data generated
    by this process.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ⁶ 因果发现是一系列方法，旨在从观察性、干预性或混合数据中恢复数据生成过程的结构。
- en: ⁷ Building a graph for a production line that is well documented and practically
    isolated from external influences is usually easier than building a graph for
    public policy optimization. There are a number of other cases that fall in between
    these two extremes.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 为生产线建立一个文档完善且实际隔离于外部影响的图形通常比为公共政策优化建立图形要容易一些。在这两种极端情况之间还有许多其他情况。
- en: ⁸ That said, LLMs can learn active (causal) strategies if we can intervene at
    test time. See [Lampinen et al. (2023)](https://arxiv.org/pdf/2305.16183.pdf)
    for details.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ⁸ 也就是说，如果我们可以在测试时进行干预，LLMs可以学习主动（因果）策略。有关详细信息，请参见 [Lampinen 等（2023）](https://arxiv.org/pdf/2305.16183.pdf)。
- en: ⁹ We use synthetic measurements, to be sure that we know the ground truth.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ⁹ 我们使用合成测量，以确保我们知道真实情况。
- en: ¹⁰ The F1 score is not necessarily the most informative metric when it comes
    to comparisons between graphical structures. However, I decided to use it here
    because of the fact that it is a relatively well understood and very popular metric
    within the broader data science community. I hope that using it (as opposed to
    more specialized metrics like SHD) will make this article more accessible to a
    broader audience.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ¹⁰ F1 分数不一定是比较图形结构时最具信息量的度量指标。然而，我决定在这里使用它，因为它是数据科学社区中相对被理解且非常流行的度量指标。我希望使用它（而不是像
    SHD 这样的更专业的度量指标）能使这篇文章对更广泛的受众更具可及性。
- en: References
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Gopnik, A. (2009). [*The philosophical baby: What children’s minds tell us
    about truth, love, and the meaning of life*](https://amzn.to/46z9Or1). New York:
    Farrar, Straus and Giroux.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Gopnik, A. (2009). [*哲学婴儿：儿童的思维告诉我们关于真理、爱和生活意义的东西*](https://amzn.to/46z9Or1).
    纽约：Farrar, Straus and Giroux.
- en: 'Kıcıman, E., Ness, R., Sharma, A., & Tan, C. (2023). [Causal Reasoning and
    Large Language Models: Opening a New Frontier for Causality.](https://arxiv.org/pdf/2304.05524.pdf)
    *ArXiv.*'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: Kıcıman, E., Ness, R., Sharma, A., & Tan, C. (2023). [因果推理与大型语言模型：为因果关系开辟新领域。](https://arxiv.org/pdf/2304.05524.pdf)
    *ArXiv.*
- en: Lampinen, A. K., Chan, S. C. Y., Dasgupta, I., Nam, A. J., & Wang, J. X. (2023).
    [Passive learning of active causal strategies in agents and language models](https://arxiv.org/pdf/2305.16183.pdf).
    *ArXiv.*
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Lampinen, A. K., Chan, S. C. Y., Dasgupta, I., Nam, A. J., & Wang, J. X. (2023).
    [代理和语言模型中的主动因果策略的被动学习](https://arxiv.org/pdf/2305.16183.pdf). *ArXiv.*
- en: 'Molak, A. (2023). [*Causal Inference and Discovery in Python: Unlock the secrets
    of modern causal machine learning with DoWhy, EconML, PyTorch and more*.](https://amzn.to/3M3R0YI)
    Packt Publishing.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Molak, A. (2023). [*Python中的因果推断与发现：利用DoWhy、EconML、PyTorch等解锁现代因果机器学习的秘密*](https://amzn.to/3M3R0YI).
    Packt Publishing.
- en: 'Stanovich, K., & West, R. (2000). Individual differences in reasoning: Implications
    for the rationality debate? *Behavioral and Brain Sciences,* *23*(5), 645–665\.
    doi:10.1017/S0140525X00003435'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Stanovich, K., & West, R. (2000). 个人推理差异：对理性辩论的影响？*行为与脑科学,* *23*(5), 645–665\.
    doi:10.1017/S0140525X00003435
- en: 'Zečević, M., Willig, M., Dhami, D. S., & Kersting, K. (2023). [Causal Parrots:
    Large Language Models May Talk Causality But Are Not Causal](https://arxiv.org/pdf/2308.13067.pdf).
    *ArXiv.*'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Zečević, M., Willig, M., Dhami, D. S., & Kersting, K. (2023). [因果鹦鹉：大型语言模型可能谈论因果关系但不具备因果能力](https://arxiv.org/pdf/2308.13067.pdf).
    *ArXiv.*
- en: '*This text might contain affiliate links for books. If you purchase a book
    using such a link, the author will earn a small commission.*'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '*这篇文章可能包含书籍的联盟链接。如果你通过这些链接购买书籍，作者将获得少量佣金。*'
