- en: 'Jane the Discoverer: Enhancing Causal Discovery with Large Language Models
    (Causal Python)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/jane-the-discoverer-enhancing-causal-discovery-with-large-language-models-causal-python-564a63425c93](https://towardsdatascience.com/jane-the-discoverer-enhancing-causal-discovery-with-large-language-models-causal-python-564a63425c93)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A practical guideline to LLM-enhanced causal discovery that minimizes the risks
    of hallucinations (with Python code)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://aleksander-molak.medium.com/?source=post_page-----564a63425c93--------------------------------)[![Aleksander
    Molak](../Images/7fca5018f6ce88297fae31cef1fe0e6c.png)](https://aleksander-molak.medium.com/?source=post_page-----564a63425c93--------------------------------)[](https://towardsdatascience.com/?source=post_page-----564a63425c93--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----564a63425c93--------------------------------)
    [Aleksander Molak](https://aleksander-molak.medium.com/?source=post_page-----564a63425c93--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----564a63425c93--------------------------------)
    ·18 min read·Oct 22, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/495b9dc6cb0d9d7c93e6b71f72139551.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Artem Podrez](https://www.pexels.com/@artempodrez/) at [Pexels.com](http://pexels.com)
  prefs: []
  type: TYPE_NORMAL
- en: “The world is everything that is the case.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Ludwig Wittgenstein — Tractatus Logico-Philosophicus (1922)**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: No baby understands the nature of motion at birth.
  prefs: []
  type: TYPE_NORMAL
- en: We — humans — and many other non-human animals come to this world equipped with
    systems that help us learn about the environment, yet on the day of our birth
    we don’t know much about the environment itself¹.
  prefs: []
  type: TYPE_NORMAL
- en: We need to learn.
  prefs: []
  type: TYPE_NORMAL
- en: In this regard, we bear a similarity to machine learning systems.
  prefs: []
  type: TYPE_NORMAL
- en: Early discoveries in psychology and *(proto)-*neuroscience regarding learning
    in humans and other non-human animals became an inspiration to build artificial
    systems that can learn from experience.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most successful learning paradigms that led to the 2010s machine
    learning revolution was supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks trained in a supervised manner allowed us to move decades ahead
    with progress on long-standing problems like image classification or machine translation.
  prefs: []
  type: TYPE_NORMAL
- en: Humans, and other non-human animals are equipped in a learning apparatus that
    (conceptually², but not necessary in terms of implementation) resembles supervised
    learning³.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, there’s also a fundamental difference between how babies and supervised
    algorithms learn.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4140bfcabb62fa0605834fcb80a1973d.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Fig. 1.** A drawing of a Purkinje neuron. These are one of the biggest neurons
    in human brain and they can be found in cerebellum. Drawing by Santiago Ramón
    y Cajal, around 1900\. Sou[rce: https://commons.wikimedia.org/wiki/Category:Santiago_Ram%C3%B3n_y_](https://commons.wikimedia.org/wiki/Category:Santiago_Ram%C3%B3n_y_Cajal)Cajal'
  prefs: []
  type: TYPE_NORMAL
- en: Throwing Objects *(But Not* Against You)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you ever seen a parent trying to convince their child to stop throwing
    around a toy? Some parents tend to interpret this type of behavior as rude, destructive,
    or aggressive, but babies often have a very different set of motivations ([Molak,
    2023](https://amzn.to/3M3R0YI)).
  prefs: []
  type: TYPE_NORMAL
- en: They are running systematic experiments that allow them to learn the
  prefs: []
  type: TYPE_NORMAL
- en: laws of physics and the rules of social interactions ([Gopnik, 2009](https://amzn.to/46z9Or1)).
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to these actions babies can learn not only by observing static data (which
    they also can do), but also by interacting with the world.
  prefs: []
  type: TYPE_NORMAL
- en: Going Beyond Observations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Interacting with the world allows children to build world models.
  prefs: []
  type: TYPE_NORMAL
- en: World models are hypotheses regarding data generating processes.
  prefs: []
  type: TYPE_NORMAL
- en: A sufficiently rich world model allows us to answer interventional (*which decision
    will lead to the best outcome?*) and counterfactual (*what would have happened
    if I did X instead of Y?*) queries.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised machine learning models (and other flavors of models based on associative
    principles) cannot answer these questions directly⁴.
  prefs: []
  type: TYPE_NORMAL
- en: Can We Teach Machines To Answer Causal Queries?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The short answer is “yes”.
  prefs: []
  type: TYPE_NORMAL
- en: There are a couple of ways to do this. In this post we’ll limit ourselves to
    cases where machines do not interact directly with the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'One popular causal query is: “*what would be the average effect of my treatment*
    ***T*** *if I administer it to a group of individuals with some characteristics*
    ***X***”.'
  prefs: []
  type: TYPE_NORMAL
- en: This is known as **heterogeneous treatment effect** or **conditional average
    treatment effect** (**CATE**).
  prefs: []
  type: TYPE_NORMAL
- en: In order to answer a query of this type, we’ll need to provide a causal model
    with the data and additional information about the structure of the data generating
    process.
  prefs: []
  type: TYPE_NORMAL
- en: The latter is usually encoded in a form of a **directed acyclic graph** (**DAG**)⁵.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, **DAG**s are (1) created manually, based on relevant expert knowledge,
    (2) using automated causal discovery⁶ or (3) a combination of both, in a (potentially
    iterative) process sometimes referred to as **human-in-the-loop causal discovery**.
  prefs: []
  type: TYPE_NORMAL
- en: 'There exist various families of causal discovery algorithms, each leveraging
    slightly different set of assumptions. See this post for an introduction:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](/beyond-the-basics-level-up-your-causal-discovery-skills-in-python-now-2023-cabe0b938715?source=post_page-----564a63425c93--------------------------------)
    [## Causal Python — Level Up Your Causal Discovery Skills in Python (2023)'
  prefs: []
  type: TYPE_NORMAL
- en: …and unlock the potential of the best Causal Discovery package in Python!
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/beyond-the-basics-level-up-your-causal-discovery-skills-in-python-now-2023-cabe0b938715?source=post_page-----564a63425c93--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Dancing In The Rain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The task of building a DAG representing our problem of interest can sometimes
    be challenging, especially when we work with complex and open-ended systems⁷.
  prefs: []
  type: TYPE_NORMAL
- en: One of the challenges that businesses might face when building causal graphs
    is the challenge of collecting expert knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting expert knowledge might be expensive, especially when we intend to
    do it in a systematic way. Experts might not be easily available and their time
    might be costly.
  prefs: []
  type: TYPE_NORMAL
- en: I Want Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some authors (e.g., Kıcıman et al., 2023) proposed that **Large Language Models**
    (**LLM**s) could be potentially used for causal discovery.
  prefs: []
  type: TYPE_NORMAL
- en: The promise of using general-purpose models to gain the understanding of causal
    structure is tempting — it opens the door for fast and cost-effective process.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the usefulness of out-of-the-box LLMs for causal discovery seems
    to be limited as they exhibit unpredictable failure modes, hallucinations and
    fail to distinguish between causal and non-causal relationships between variables
    (Kıcıman et al., 2023; Zečević et al., 2023).
  prefs: []
  type: TYPE_NORMAL
- en: In this episode of The **Causal Bandits Podcast**, we discuss the problems in
    LLMs and causal reasoning. **Audio-only** version is available [here](https://www.buzzsprout.com/2272512/13918140-causality-llms-abstractions-matej-zecevic-causal-bandits-ep-000).
  prefs: []
  type: TYPE_NORMAL
- en: One, recently growing in popularity way to counter hallucinations and failure
    modes in LLMs is **retrieval augmented generation** (**RAG**).
  prefs: []
  type: TYPE_NORMAL
- en: RAG is a set of techniques that provide the model with an external source of
    knowledge (a document or a corpus) and the model generates its response based
    on this source.
  prefs: []
  type: TYPE_NORMAL
- en: This approach has been shown to [reduce hallucinations](https://www.pinecone.io/learn/retrieval-augmented-generation/)
    and help the model stay relevant.
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about RAG and why it is important, see this post by my colleague
    [Anthony Alcaraz](https://medium.com/u/30bc9ffd2f4b?source=post_page-----564a63425c93--------------------------------):'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://ai.plainenglish.io/why-retrieval-augmentation-is-critical-for-enterprise-llms-and-which-current-model-to-choose-9ce5d9fa3817?source=post_page-----564a63425c93--------------------------------)
    [## Why Retrieval Augmentation is Critical for Enterprise LLMs and which current
    model to choose ?'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) have demonstrated immense potential to revolutionize
    workflows in corporate settings.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ai.plainenglish.io](https://ai.plainenglish.io/why-retrieval-augmentation-is-critical-for-enterprise-llms-and-which-current-model-to-choose-9ce5d9fa3817?source=post_page-----564a63425c93--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Two Perspectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There’s a fundamental difference between how LLMs and traditional causal discovery
    algorithms approach the task of finding causal structure.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional causal discovery algorithms leverage a combination of observational,
    interventional or mixed data and inductive biases encoded in a form of assumptions
    in order to infer from the data information about the structure of the process
    that generated this data.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs — on the other hand — do not look at the (measurement) data at all. They
    leverage learned semantic representations of concepts and transform them in order
    to answer the queries about possible causal relationships between these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: One perspective on LLMs in this context is that they leverage **correlations
    of causal facts** (for more details see Zečević et al., 2023) in order to determine
    causal relationships between two or more entities.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7495db6a7b49762ec9d37df882f3bca.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Fig 2**. Contextual equivalence of textual and measurement representations
    of causal facts. Source: [Zečević et al., 2023](https://arxiv.org/pdf/2308.13067.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: We talk about ***correlations*** of facts here, because LLMs are trained in
    an associative manner and as such they cannot be generally expected to operate
    on causally sound representations⁸.
  prefs: []
  type: TYPE_NORMAL
- en: Good news is that RAG can reduce the risk of failure modes and hallucinations
    by focusing the model response to the context of a provided corpus.
  prefs: []
  type: TYPE_NORMAL
- en: This opens up a path for a more meaningful role of LLMs in the causal discovery
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Documents and Measurements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s imagine that we want to build a causal graph for a problem that interests
    us.
  prefs: []
  type: TYPE_NORMAL
- en: We have measured a set of variables relevant to this problem over an extensive
    period of time. We also have expert knowledge regarding the problem stored in
    a number of documents.
  prefs: []
  type: TYPE_NORMAL
- en: We could simply run a causal discovery algorithm over the measurements, but
    that’s risky if we cannot guarantee that the algorithm’s assumptions are met.
    That would also mean throwing away valuable expert knowledge that we have at our
    disposal.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, we could use a RAG-LLM to query the corpus and parse the
    model outputs to obtain a graph.
  prefs: []
  type: TYPE_NORMAL
- en: This might still be risky, because even with RAG, we have no guarantees that
    the model will correctly answer the queries. Moreover, the knowledge stored in
    the corpus might be incomplete, meaning that some causal relationships present
    in the data might not be reflected in the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Wanna Be Friends?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An alternative approach could be to combine both methods and triangulate semantic
    and measurement-based perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: One way to achieve this is by querying the RAG-LLM system to extract causal
    relationships from textual representations, and then use the obtained information
    as expert knowledge that we inject into a causal discovery algorithm, that solely
    operates on the measurement data.
  prefs: []
  type: TYPE_NORMAL
- en: Fancy giving it a try?
  prefs: []
  type: TYPE_NORMAL
- en: Can Mehendretex Help Mountain Climbers?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s put the idea we’ve just described to the test and see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: I used a canonical example of the relationship between altitude and temperature
    as an inspiration for our exercise.
  prefs: []
  type: TYPE_NORMAL
- en: To make the task more interesting, we’re going to add more variables to the
    problem and make one of them completely new to the LLM (neither the model itself
    nor the corpus of documents contain any prior knowledge about it).
  prefs: []
  type: TYPE_NORMAL
- en: We’ll be interested in finding a causal structure between a set of variables
    that can impact a mountain climber’s risk of death.
  prefs: []
  type: TYPE_NORMAL
- en: In order to find the structure, we’ll use a dataset of synthetic measurements⁹,
    and prior knowledge about some of the variables available in Wikipedia (our corpus).
  prefs: []
  type: TYPE_NORMAL
- en: 'The variables in the problem statement are:'
  prefs: []
  type: TYPE_NORMAL
- en: altitude
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: temperature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: oxygen density
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: risk of death
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: usage of Mehendretex (our imaginary treatment)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function of the latter variable — Mehendretex (nothing like this exists
    in reality) — is to stress test the LLM part of our system.
  prefs: []
  type: TYPE_NORMAL
- en: We expect that under a relevant prompt a RAG-LLM will tell us that it cannot
    say **anything** about causal relationship between Mehendretex and any other variable.
  prefs: []
  type: TYPE_NORMAL
- en: '*Fig 3* presents the ground truth DAG for our problem represented by a graph
    (left) and an adjacency matrix (right).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/054c3d17f487d797a9f5b3ebb391660a.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Fig 3.** The true structure underlying our problem represented as a graph
    (left) and as an adjacency matrix (right). Black fields in the matrix denote an
    edge, white fields — a lack of an edge. For instance, a black square with coordinates
    (4, 3) indicates that there’s an edge from node 4 to node 3.'
  prefs: []
  type: TYPE_NORMAL
- en: Where’s My Python?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s code the problem.
  prefs: []
  type: TYPE_NORMAL
- en: '**Link to GitHub repo available at the end of the article.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We’ll use `numpy` and `scipy` to generate the data, [**gCastle**](https://github.com/huawei-noah/trustworthyAI/tree/master/gcastle)
    will serve as our causal discovery library, OpenAI’s **GPT-4** as an LLM, and
    [**LangChain**](https://www.langchain.com/) as a framework that will help us feed
    relevant Wikipedia articles to GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: On top of this we’ll import `os` for managing the OpenAI key and `itertools.combinations`
    to help us generate variable pairs for our RAG-LLM agent.
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s import the libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s set the OpenAI key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Data generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let’s generate the data.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we only generate fictional **measurement data**. The textual data
    that we’ll use are actual Wikipedia articles.
  prefs: []
  type: TYPE_NORMAL
- en: In this sense, we use **semi-synthetic** data in this example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’ s start by defining a mapping between variable names and indices. This
    will help us later to encode expert knowledge obtained from the LLM in a matrix
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we set the sample size and generate synthetic measurement data for our
    variables according to the structure presented in *Fig 3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Notice that in order to make the dataset more realistic we use a combination
    of Normal and Half-normal distributions and combine it with clipping.
  prefs: []
  type: TYPE_NORMAL
- en: This will also make the job harder for the causal discovery algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s stack all variables in a single matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '…and define the structure from *Fig 3* in a matrix form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Algorithmic causal discovery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start our discovery process by applying a classic PC algorithm to our
    measurement data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use the gCastle implementation of the stable variant of PC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is presented in *Fig 4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cd1f0d55f6b1bf339bc98d6d686bc0c5.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Fig 4.** Results of applying the PC algorithm to our measurement data. On
    the right we see the ground truth adjacency matrix; on the left, the matrix produced
    by the PC algorithm. Black “fields” in the matrix denote an edge, white fields
    — a lack of an edge. For example, a black square with coordinates (2, 1) indicates
    that there’s an edge from node 2 to node 1.'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the results are not great to say the least. The algorithm identified
    some edges correctly (e.g., `(4, 3)`, `(0, 1)` or `(0, 2)`), but many edges remain
    undirected.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, notice that in the matrix on the left we have an edge `(4, 3)`,
    but also `(3, 4)` which indicates that the model was able to figure out that there’s
    a connection between nodes 3 and 4, but it was not able to decide on the direction
    of this connection.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing the underlying structure of our dataset, this is not surprising, as
    PC algorithm depends on conditional independence testing and cannot orient edges
    under certain structural configurations.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about PC and why this is the case, check [**this blog post**](/beyond-the-basics-level-up-your-causal-discovery-skills-in-python-now-2023-cabe0b938715)
    for an introduction or *Part 3* of my [**recent book on causal modeling**](https://amzn.to/46EW9yB)
    for a more detailed treatment.
  prefs: []
  type: TYPE_NORMAL
- en: When we look at the metrics, F1 score for this graph is equal to 0.53¹⁰.
  prefs: []
  type: TYPE_NORMAL
- en: Defining an LLM agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s see what will happen when we add an RAG-LLM agent to the picture.
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll define a prior knowledge object, which we’ll then use to store
    the knowledge extracted by our agent.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s instantiate the agent.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We will use GPT-4 as our model of choice. We set temperature to `0` in order
    to make model outputs as conservative as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s add LangChain’s tool that will allow the model to access Wikipedia:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let’s instantiate the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We’ll use the agent to look through Wikipedia pages in order to find information
    about causal connections between the variables pertinent to our problem.
  prefs: []
  type: TYPE_NORMAL
- en: The findings from this agent will then be fed to another instance of GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: This second GPT-4 instance will be tasked with interpreting the initial output
    and translating it into a format compatible with our prior knowledge object.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to make the code more reusable, we’ll encapsulate it in a function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we first call the agent, using one prompt and then pass the output
    of this agent to another (non-agent) instance of an LLM that is initialized with
    another prompt.
  prefs: []
  type: TYPE_NORMAL
- en: In my experiments, this hierarchical approach led to better and more consistent
    results than just using a single agent.
  prefs: []
  type: TYPE_NORMAL
- en: I also noticed that repeating the definition of causality to both agents was
    helpful in improving the results.
  prefs: []
  type: TYPE_NORMAL
- en: In both prompts we use Pearl’s interventional definition of causation. Additionally,
    the first prompt specifies that we’re interested in a broad spectrum of causal
    effects, as long as they follow the basic definition.
  prefs: []
  type: TYPE_NORMAL
- en: In the experiments, the latter helped the model to be more decisive in some
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(0,1)` if the model decides that there’s a causal edge from `var_1` to `var_2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(1,0)` if the model decides that the causal direction is from `var_2` to `var_1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(0,0)` if the model decides that there is **no** causal relationship between
    `var_1` and `var_2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(-1,-1)` if the model doesn’t know the answer (note that we emphasized in
    the prompt that the model should not come up with an answer if it doesn’t know
    it).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLM-enhanced causal discovery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’re now ready to collect expert knowledge from Wikipedia using our hierarchical
    two-prompt system.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll loop over all possible combinations of variables, record model answers
    for each of them and store them in the prior knowledge object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note that we only pass the information from the model to the prior knowledge
    object when the model decides that **there is** a causal relationship between
    variables (output is either `(0,1)` or `(1,0)`), but we skip it when the model
    claims **there’s no** causal relationship between the variables.
  prefs: []
  type: TYPE_NORMAL
- en: This decision is based on the observation that model was often overly cautious
    when it comes to claiming the existence of causal influence.
  prefs: []
  type: TYPE_NORMAL
- en: By ignoring model’s null findings, we essentially allow the causal discovery
    algorithm to decide if the relationship exists in such a case.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see what the model was able to learn.
  prefs: []
  type: TYPE_NORMAL
- en: '*Fig 5* shows the results of our LLM agent compared to the ground truth.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/410851b6e4ff7a5412026030b62aebd5.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Fig 5.** Prior knowledge retrieved by our LLM agent (left) and the ground
    truth graph (right).'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the model was able to learn some useful knowledge, but was not
    able to recreate the entire graph. This is partially because Wikipedia does not
    contain any information about our imagined treatment — Mehendretex.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a quick look at one of the logs from our model that involves Mehendretex:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: It seems that the model correctly responded to the query that contained a term
    not present in the corpus the model was using (see the **bold** text in the box
    above).
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re now ready to pass the collected knowledge to the causal discovery algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note that we’re initializing a new instance of the PC algorithm and pass the
    `priori_knowledge` object containing the knowledge gathered by the LLM to the
    constructor.
  prefs: []
  type: TYPE_NORMAL
- en: PC will now re-calculate the results of all conditional independence tests,
    but will also take prior knowledge into account when orienting the edges.
  prefs: []
  type: TYPE_NORMAL
- en: Ready to see the results?
  prefs: []
  type: TYPE_NORMAL
- en: '*Fig 6* shows the results of our RAG-LLM-induced PC algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/affa755ee213d2590822d09c03bd8bb2.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Fig 6.** The results of the PC algorithms with RAG-LLM-based prior knowledge
    (left) vs the ground truth (right).'
  prefs: []
  type: TYPE_NORMAL
- en: This looks pretty good!
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s only one edge that remained undirected: the edge between nodes with
    indices `0` and `2` (altitude and temperature), with all other edges being oriented
    correctly.'
  prefs: []
  type: TYPE_NORMAL
- en: F1 score for this output is 0.93, which is an improvement of 40 percentage points
    over PC without LLM assistance.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing the results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By combining traditional causal discovery with RAG-LLM, we were able to get
    the results better than any single method has offered.
  prefs: []
  type: TYPE_NORMAL
- en: '*Fig 7* summarizes the results at each step of the process.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa0f0d4224ab61ec1cae807ac17cd538.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Fig 7\.** Adjacency matrices of model outputs at each stage (left column)
    vs the ground truth (right column) and respective values of F1 scores.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea to use large language models as “knowledge parsers” that can translate
    textual information into the format compatible with traditional causal discovery
    algorithms and expand the capabilities of the latter is powerful.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the main limitations of the system that we discussed in this blog post
    is the instability of LLM outputs. I ran around 20 iterations of the process presented
    here and LLM’s decisions were not always consistent between the runs (even with
    temperature set to zero).
  prefs: []
  type: TYPE_NORMAL
- en: The PC algorithm comes with a set of theoretical guarantees, but they only hold
    for infinite sample sizes, which is of course never the case in practice. Moreover,
    default conditional independence test that we used in this post is not very powerful
    and might not work well in more complex cases.
  prefs: []
  type: TYPE_NORMAL
- en: PC algorithm requires a lack of hidden confounding in the data — a condition
    that not always can be guaranteed, especially in open-ended complex systems.
  prefs: []
  type: TYPE_NORMAL
- en: For larger datasets, running the RAG-LLM part can quickly becomes costly as
    the system might be performing multiple API calls for each pair of variables behind
    the scenes. If you decide to run the code for your own use case, **be aware of
    the potential high API costs**.
  prefs: []
  type: TYPE_NORMAL
- en: My recommendation is to **set a limit** in your OpenAI account, before starting
    the experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Further improvements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Naturally, systems like this do not have to be limited to Wikipedia as a source
    of external knowledge and any corpus of documents can be used.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the instabilities in LLM output could be potentially addressed by creating
    better prompts.
  prefs: []
  type: TYPE_NORMAL
- en: The PC algorithm’s performance could be improved by using a more general, kernel-based
    conditional independence test. Other causal discovery algorithms could also be
    used instead of PC if that suits the data.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, when hidden confounding cannot be excluded, algorithms like **fast
    causal discovery** (**FCI**) can be a better choice.
  prefs: []
  type: TYPE_NORMAL
- en: Querying the LLM with each possible pair of variables is not scalable and can
    be very expensive for larger datasets. A potential pre-selection mechanism could
    be based on a regular LLM, assessing the likelihood that two variables are causally
    connected. Combining multiple variables in a single query could help decrease
    the costs.
  prefs: []
  type: TYPE_NORMAL
- en: The downside of this solution is that it comes with a higher risk of hallucinations,
    but can be effective in case of variables and relationships that are commonly
    known.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Jane the Discoverer is a proof-of-concept idea. I consider it a causal assistant
    that can be useful for small to mid-scale graphs.
  prefs: []
  type: TYPE_NORMAL
- en: For the educational purposes, the code in the blog is written in a way that
    suggests autonomous usage, but I find a human-in-the-loop paradigm more effective.
  prefs: []
  type: TYPE_NORMAL
- en: Congrats on reaching the end!
  prefs: []
  type: TYPE_NORMAL
- en: How could we further improve Jane?
  prefs: []
  type: TYPE_NORMAL
- en: Share your thoughts and suggestions in the comment section below and stay causal!
  prefs: []
  type: TYPE_NORMAL
- en: 'If you liked this post, you might also enjoy:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 🔸 [Causal Bandits Podcast](https://causalbanditspodcast.com)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 🔸[Causal Python Weekly](https://bit.ly/46FF2N4) newsletter
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Notebook** with **full code** for this post is available on GitHub:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://github.com/AlxndrMlk/blogs-code/tree/main/Jane%20the%20Discoverer?source=post_page-----564a63425c93--------------------------------)
    [## blogs-code/Jane the Discoverer at main · AlxndrMlk/blogs-code'
  prefs: []
  type: TYPE_NORMAL
- en: Code for blog posts. Contribute to AlxndrMlk/blogs-code development by creating
    an account on GitHub.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/AlxndrMlk/blogs-code/tree/main/Jane%20the%20Discoverer?source=post_page-----564a63425c93--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Footnotes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ¹ We could argue that on the day of birth, we actually know something about
    the world as many studies suggest that fetuses can learn and habituate (e.g.,
    James et al., 2022)
  prefs: []
  type: TYPE_NORMAL
- en: ² Associative learning is prevalent among species. It’s present even in simple
    organisms like [*C. elegans*](https://en.wikipedia.org/wiki/Caenorhabditis_elegans).
    Note that in this text we use the term *associative learning* to describe any
    learning based on associations that does not include systematic interventions.
    This definition may be inconsistent with some definitions used in psychology and/or
    biology.
  prefs: []
  type: TYPE_NORMAL
- en: ³ We could argue here about the differences in implementation and potential
    differences in sample efficiency between animals and supervised machine learning
    algorithms in the context of associative learning. As a side note, I think the
    topic of sample efficiency is very interesting. At the same time, I believe that
    relevant answers are more complex than popular statements like “a child can learn
    what a dog is from two examples, while a CNN or Transformer architecture requires
    millions”.
  prefs: []
  type: TYPE_NORMAL
- en: ⁴ This is also includes self-supervised and some reinforcement learning models.
  prefs: []
  type: TYPE_NORMAL
- en: ⁵ By using more advanced or more general identification methods we can extend
    this to partially directed or cyclic graphs.
  prefs: []
  type: TYPE_NORMAL
- en: ⁶ Causal discovery is a set of methods aiming at recovering the structure of
    the data-generating process from observational, interventional or mixed data generated
    by this process.
  prefs: []
  type: TYPE_NORMAL
- en: ⁷ Building a graph for a production line that is well documented and practically
    isolated from external influences is usually easier than building a graph for
    public policy optimization. There are a number of other cases that fall in between
    these two extremes.
  prefs: []
  type: TYPE_NORMAL
- en: ⁸ That said, LLMs can learn active (causal) strategies if we can intervene at
    test time. See [Lampinen et al. (2023)](https://arxiv.org/pdf/2305.16183.pdf)
    for details.
  prefs: []
  type: TYPE_NORMAL
- en: ⁹ We use synthetic measurements, to be sure that we know the ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: ¹⁰ The F1 score is not necessarily the most informative metric when it comes
    to comparisons between graphical structures. However, I decided to use it here
    because of the fact that it is a relatively well understood and very popular metric
    within the broader data science community. I hope that using it (as opposed to
    more specialized metrics like SHD) will make this article more accessible to a
    broader audience.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gopnik, A. (2009). [*The philosophical baby: What children’s minds tell us
    about truth, love, and the meaning of life*](https://amzn.to/46z9Or1). New York:
    Farrar, Straus and Giroux.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kıcıman, E., Ness, R., Sharma, A., & Tan, C. (2023). [Causal Reasoning and
    Large Language Models: Opening a New Frontier for Causality.](https://arxiv.org/pdf/2304.05524.pdf)
    *ArXiv.*'
  prefs: []
  type: TYPE_NORMAL
- en: Lampinen, A. K., Chan, S. C. Y., Dasgupta, I., Nam, A. J., & Wang, J. X. (2023).
    [Passive learning of active causal strategies in agents and language models](https://arxiv.org/pdf/2305.16183.pdf).
    *ArXiv.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Molak, A. (2023). [*Causal Inference and Discovery in Python: Unlock the secrets
    of modern causal machine learning with DoWhy, EconML, PyTorch and more*.](https://amzn.to/3M3R0YI)
    Packt Publishing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stanovich, K., & West, R. (2000). Individual differences in reasoning: Implications
    for the rationality debate? *Behavioral and Brain Sciences,* *23*(5), 645–665\.
    doi:10.1017/S0140525X00003435'
  prefs: []
  type: TYPE_NORMAL
- en: 'Zečević, M., Willig, M., Dhami, D. S., & Kersting, K. (2023). [Causal Parrots:
    Large Language Models May Talk Causality But Are Not Causal](https://arxiv.org/pdf/2308.13067.pdf).
    *ArXiv.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*This text might contain affiliate links for books. If you purchase a book
    using such a link, the author will earn a small commission.*'
  prefs: []
  type: TYPE_NORMAL
