- en: Testing the Massively Multilingual Speech (MMS) Model that Supports 1162 Languages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/testing-the-massively-multilingual-speech-mms-model-that-supports-1162-languages-5db957ee1602](https://towardsdatascience.com/testing-the-massively-multilingual-speech-mms-model-that-supports-1162-languages-5db957ee1602)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explore the cutting-edge multilingual features of Meta’s latest automatic speech
    recognition (ASR) model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@luisroque?source=post_page-----5db957ee1602--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----5db957ee1602--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5db957ee1602--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5db957ee1602--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----5db957ee1602--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5db957ee1602--------------------------------)
    ·8 min read·May 26, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Massively Multilingual Speech (MMS)¹ is the latest release by Meta AI (just
    a few days ago). It pushes the boundaries of speech technology by expanding its
    reach from about 100 languages to over 1,000\. This was achieved by building a
    single multilingual speech recognition model. The model can also identify over
    4,000 languages, representing a 40-fold increase over previous capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The MMS project aims to make it easier for people to access information and
    use devices in their preferred language. It expands text-to-speech and speech-to-text
    technology to underserved languages, continuing to reduce language barriers in
    our global world. Existing applications can now include a wider variety of languages,
    such as virtual assistants or voice-activated devices. At the same time, new use
    cases emerge in cross-cultural communication, for example, in messaging services
    or virtual and augmented reality.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will walk through the use of MMS for ASR in English and
    Portuguese and provide a step-by-step guide on setting up the environment to run
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a2b17ee13d06f5681126543d65c4f142.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Massively Multilingual Speech (MMS) is capable of identifying over
    4,000 languages and supports 1162 ([source](https://unsplash.com/photos/ZzWsHbu2y80))'
  prefs: []
  type: TYPE_NORMAL
- en: 'This article belongs to “Large Language Models Chronicles: Navigating the NLP
    Frontier”, a new weekly series of articles that will explore how to leverage the
    power of large models for various NLP tasks. By diving into these cutting-edge
    technologies, we aim to empower developers, researchers, and enthusiasts to harness
    the potential of NLP and unlock new possibilities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Articles published so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Summarizing the latest Spotify releases with ChatGPT](https://medium.com/towards-data-science/summarizing-the-latest-spotify-releases-with-chatgpt-553245a6df88)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Master Semantic Search at Scale: Index Millions of Documents with Lightning-Fast
    Inference Times using FAISS and Sentence Transformers](https://medium.com/towards-data-science/master-semantic-search-at-scale-index-millions-of-documents-with-lightning-fast-inference-times-fa395e4efd88)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Unlock the Power of Audio Data: Advanced Transcription and Diarization with
    Whisper, WhisperX, and PyAnnotate](https://medium.com/towards-data-science/unlock-the-power-of-audio-data-advanced-transcription-and-diarization-with-whisper-whisperx-and-ed9424307281)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Whisper JAX vs PyTorch: Uncovering the Truth about ASR Performance on GPUs](https://medium.com/towards-data-science/whisper-jax-vs-pytorch-uncovering-the-truth-about-asr-performance-on-gpus-8794ba7a42f5)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Vosk for Efficient Enterprise-Grade Speech Recognition: An Evaluation and
    Implementation Guide](https://medium.com/towards-data-science/vosk-for-efficient-enterprise-grade-speech-recognition-an-evaluation-and-implementation-guide-87a599217a6c)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As always, the code is available on my [Github](https://github.com/luisroque/large_laguage_models).
  prefs: []
  type: TYPE_NORMAL
- en: The Approach to build the Massively Multilingual Speech Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Meta used religious texts, such as the Bible, to build a model covering this
    wide range of languages. These texts have several interesting components: first,
    they are translated into many languages, and second, there are publicly available
    audio recordings of people reading these texts in different languages. Thus, the
    main dataset where this model was trained was the New Testament, which the research
    team was able to collect for over 1,100 languages and provided more than 32h of
    data per language. They went further to make it recognize 4,000 languages. This
    was done by using unlabeled recordings of various other Christian religious readings.
    From the experiments results, even though the data is from a specific domain,
    it can generalize well.'
  prefs: []
  type: TYPE_NORMAL
- en: These are not the only contributions of the work. They created a new preprocessing
    and alignment model that can handle long recordings. This was used to process
    the audio, and misaligned data was removed using a final cross-validation filtering
    step. Recall from one of our previous articles that we saw that one of the challenges
    of Whisper was the incapacity to align the transcription properly. Another important
    step of the approach was the usage of wav2vec 2.0, a self-supervised learning
    model, to train their system on a massive amount of speech data (about 500,000
    hours) in over 1,400 languages. The labeled dataset we discussed previously is
    not enough to train a model of the size of MMS, so wav2vec 2.0 was used to reduce
    the need for labeled data. Finally, the resulting models were then fine-tuned
    for a specific speech task, such as multilingual speech recognition or language
    identification.
  prefs: []
  type: TYPE_NORMAL
- en: The MMS models were open-sourced by Meta a few days ago and were made available
    in the Fairseq repository. In the next section, we cover what Fairseq is and how
    we can test these new models from Meta.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overview of the Fairseq Repository: A Powerful Toolkit for Sequence-to-Sequence
    Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fairseq is an open-source sequence-to-sequence toolkit developed by Facebook
    AI Research, also known as FAIR. It provides reference implementations of various
    sequence modeling algorithms, including convolutional and recurrent neural networks,
    transformers, and other architectures.
  prefs: []
  type: TYPE_NORMAL
- en: The Fairseq repository is based on PyTorch, another open-source project initially
    developed by the Meta and now under the umbrella of the Linux Foundation. It is
    a very powerful machine learning framework that offers high flexibility and speed,
    particularly when it comes to deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: The Fairseq implementations are designed for researchers and developers to train
    custom models and it supports tasks such as translation, summarization, language
    modeling, and other text generation tasks. One of the key features of Fairseq
    is that it supports distributed training, meaning it can efficiently utilize multiple
    GPUs either on a single machine or across multiple machines. This makes it well-suited
    for large-scale machine learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Massively Multilingual Speech Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Fairseq provides two pre-trained models for download: MMS-300M and MMS-1B.
    You also have access to fine-tuned models available for different languages and
    datasets. For our purpose, we test the MMS-1B model fine-tuned for 102 languages
    in the FLEURS dataset and also the MMS-1B-all, which is fine-tuned to handle 1162
    languages (!), fine-tuned using several different datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Automatic Speech Recognition with Massively Multilingual Speech
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Remember that these models are still in research phase, making testing a bit
    more challenging. There are additional steps that you would not find with production-ready
    software.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to set up a `.env` file in your project root to configure your
    environment variables. It should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, you need to configure the YAML file located at `fairseq/examples/mms/asr/config/infer_common.yaml`.
    This file contains important settings and parameters used by the script.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the YAML file, use a full path for the `checkpoint` field like this (unless
    you are using a containerized application to run the script):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This full path is necessary to avoid potential permission issues unless you
    are running the application in a container.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you plan on using a CPU for computation instead of a GPU, you will need
    to add the following directive to the top level of the YAML file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This setting directs the script to use the CPU for computations.
  prefs: []
  type: TYPE_NORMAL
- en: We use the `dotevn` python library to load these environment variables in our
    Python script. Since we are overwriting some system variables, we will need to
    use a trick to make sure that we get the right variables loaded. We use the`dotevn_values`method
    and store the output in a variable. This ensures that we get the variables stored
    in our `.env`file and not random system variables even if they have the same name.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Then, we can clone the fairseq GitHub repository and install it in our machine.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We already discussed the models that we use in this article, so let’s download
    them to our local environment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'There is one additional restriction related to the input of the MMS model,
    the sampling rate of the audio data needs to be 16000 Hz. In our case, we defined
    two ways to generate these files: one that converts video to audio and another
    that resamples audio files for the correct sampling rate.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready to run the inference process using our MMS-1B-all model, which
    supports 1162 languages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Automatic Speech Recognition Results with Fairseq
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we describe our experimentation setup and discuss the results.
    We performed ASR using two different models from Fairseq, MMS-1B-all and MMS-1B-FL102,
    in both English and Portuguese. You can find the audio files in my GitHub repo.
    These are files that I generated myself just for testing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the MMS-1B-all model. Here is the input and output for the
    English and Portuguese audio samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Eng**: just requiring a small clip to understand if the new facebook research
    model really performs on'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Por**: ora bem só agravar aqui um exemplo pa tentar perceber se de facto
    om novo modelo da facebook research realmente funciona ou não vamos estar'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'With the MMS-1B-FL102, the generated speech was significantly worse. Let’s
    see the same example for English:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Eng**: just recarding a small ho clip to understand if the new facebuok research
    model really performs on speed recognition tasks lets see'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: While the speech generated is not super impressive for the standard of models
    we have today, we need to address these results from the perspective that these
    models open up ASR to a much wider range of the global population.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Massively Multilingual Speech model, developed by Meta, represents one more
    step to foster global communication and broaden the reach of language technology
    using AI. Its ability to understand over 4,000 languages and function effectively
    across 1,162 of them increases accessibility for numerous languages that have
    been traditionally underserved.
  prefs: []
  type: TYPE_NORMAL
- en: Our testing of the MMS models showcased the possibilities and limitations of
    the technology at its current stage. Although the speech generated by the MMS-1B-FL102
    model was not as impressive as expected, the MMS-1B-all model provided promising
    results, demonstrating its capacity to transcribe speech in both English and Portuguese.
    Portuguese has been one of those underserved languages, specially when we consider
    Portuguese from Portugal.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to try it out in your preferred language and to share the transcription
    and feedback in the comment section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep in touch: [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] — [Pratap, V., Tjandra, A., Shi, B., Tomasello, P., Babu, A., Kundu, S.,
    Elkahky, A., Ni, Z., Vyas, A., Fazel-Zarandi, M., Baevski, A., Adi, Y., Zhang,
    X., Hsu, W.-N., Conneau, A., & Auli, M. (2023). Scaling Speech Technology to 1,000+
    Languages. arXiv.](https://scontent.fopo6-1.fna.fbcdn.net/v/t39.8562-6/348827959_6967534189927933_6819186233244071998_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=nLkMCxk5EFMAX_VcRa0&_nc_ht=scontent.fopo6-1.fna&oh=00_AfDVqBOzIgYWtX65XMtxu0wXy8DPWJWl-Z_xwk0eZxUDsw&oe=64764402)'
  prefs: []
  type: TYPE_NORMAL
