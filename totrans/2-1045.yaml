- en: Hierarchical Transformers — part 1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/hierarchical-transformers-54f6d59fa8fc](https://towardsdatascience.com/hierarchical-transformers-54f6d59fa8fc)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: More efficient language models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mina.ghashami?source=post_page-----54f6d59fa8fc--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----54f6d59fa8fc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----54f6d59fa8fc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----54f6d59fa8fc--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----54f6d59fa8fc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----54f6d59fa8fc--------------------------------)
    ·9 min read·Oct 4, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1b3d6e9ce3c3e6c0dec3c8326bd0faf9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [unsplash.com](https://unsplash.com/photos/x22UAIdif_k)
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we look at hierarchical transformers: what they are, how they
    work, how they differ from standard Transformers and what are their benefits.
    Let’s get started.'
  prefs: []
  type: TYPE_NORMAL
- en: What Are Hierarchical Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The “hierarchical Transformer” refers to a transformer architecture that operates
    on multiple scales or resolutions of the input sequence.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Why do we need hierarchical transformers?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Standard Transformers as amazing that they are are very time consuming. The
    attention mechanism inside Transformers takes O(n²) to run on an input sequence
    of n tokens. This means Transformers are not practical for long sequences. One
    way to address this inefficiencies, is to have hierarchical transformers. Is it
    the only way? no! another way is to have improve efficiency of attention mechanism.
    But this is a topic for another day.
  prefs: []
  type: TYPE_NORMAL
- en: How does hierarchy in Transformers help?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hierarchical Transformers enable the model to operate on different levels of
    the input e.g. words, sentences, paragraphs etc. This matches how humans process
    text too. This forces attention over different hierarchies to model relationships
    between entities at different granularities.
  prefs: []
  type: TYPE_NORMAL
- en: There are many methods for hierarchical transformers; in this article, we strive
    to intuitively explain one of these methods.
  prefs: []
  type: TYPE_NORMAL
- en: The Hourglass Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Hourglass* [1] network is a joint work by OpenAI, Google Research and University
    of Warsaw. It is a hierarchical autoregressive Transformer that takes an input
    sequence, and forms a hierarchy of sequences from full resolution to smaller and
    smaller scales; at each scale it processes the sequence within that resolution,
    and finally it expands the sequence back to the full size. This makes the model
    more efficient as the shorter sequences are cheaper to process.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in autoregressive transformers the first and last layer at the very
    least have to operate on full-scale of the input. This is because the first layer
    is processing the input so it has to operate at full-scale, and the last layer
    (since model is autoregressive) is producing an output so has to operate on full-scale
    again.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at this architecture. The image below, demonstrates *Hourglass:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c2850b54eb73ade9bd07ff1bfca40be.png)'
  prefs: []
  type: TYPE_IMG
- en: “Hourglass” architecture — image taken from [[1](https://arxiv.org/pdf/2110.13711.pdf)]
  prefs: []
  type: TYPE_NORMAL
- en: We describe it step by step. We start from the left of above image where the
    input tokens are depicted as a grey box.
  prefs: []
  type: TYPE_NORMAL
- en: The model first processes the full input sequence using standard Transformer
    layers. So if the input sequence (shown as grey box titled “input tokens”) has
    L tokens, then they are all passed through standard Transformer layers (depicted
    in blue and called as *pre-vanilla layers).* These layers output L embedding vectors
    one for each token.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The pre-vanilla layers are Transformer layers operating on the full token-level
    sequence before shortening.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So if the task is “language modeling on text”, the input to pre-vanilla layers
    is a sequence of subword tokens representing the text. If the task is “image generation”,
    the input would be pixel values flattened into a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/87f94c52be0937da9a9f537ff96842c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Step 1 — hourglass architecture — Image from [[1](https://arxiv.org/pdf/2110.13711.pdf)]
    modified by the author
  prefs: []
  type: TYPE_NORMAL
- en: 2\. In the second step, the model shortens the sequence from *L* tokens into
    fewer tokens. This step is shown as an orange trapezius and the “shortening factor”
    is stated as *sf = k₁*. Note *sf* stands for “shortening factor”, and if it is
    set to *k₁* it means every *k₁* tokens is merged into 1 token. The shortening
    happens via using some sort of pooling operations such as *average pooling*, *linear
    pooling,* or *attention pooling*. We will talk about them soon. The output of
    this step is *L/k₁* tokens. This step is also known as down *down-sampling step.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/831300273cf5532bcb62ddf33f13a74e.png)'
  prefs: []
  type: TYPE_IMG
- en: Step 2— hourglass architecture — Image from [[1](https://arxiv.org/pdf/2110.13711.pdf)]
    modified by the author
  prefs: []
  type: TYPE_NORMAL
- en: 3\. The shortened sequence is processed by more Transformer layers called *shortened
    layers*. In the image they are shown by yellow boxes. These layers output updated
    embeddings for the tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8e94cfdc3920c0eb7e86066cce58595e.png)'
  prefs: []
  type: TYPE_IMG
- en: Step 3— hourglass architecture — Image from [[1](https://arxiv.org/pdf/2110.13711.pdf)]
    modified by the author
  prefs: []
  type: TYPE_NORMAL
- en: 4\. If there is more shortening left to do, we simply repeat the process. In
    the image below, in the second orange trapezius, we are shortening the input sequence
    by a factor of *sf = k₂.* This will merge every *k₂* tokens into 1 token, and
    so will output *L/(k₁.k₂)* tokens. The outputted tokens get passed through more
    *shortened layers* shown in light yellow.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3bbd5aae06806bc0f99deb46551b8d7d.png)'
  prefs: []
  type: TYPE_IMG
- en: Step 4— hourglass architecture — Image from [[1](https://arxiv.org/pdf/2110.13711.pdf)]
    modified by the author
  prefs: []
  type: TYPE_NORMAL
- en: By now, we are in the middle of the architecture…
  prefs: []
  type: TYPE_NORMAL
- en: From here, the up-sampling starts!
  prefs: []
  type: TYPE_NORMAL
- en: '5\. Up-sampling layers are used to expand the shortest sequence back to the
    original full resolution. Since we had two down-samplings (one from L tokens to
    *L/k₁* and the second from *L/k₁*tokens to *L/(k₁.k₂)* tokens), we will perform
    two up-sampling to bring back the number of tokens to *L* tokens. The first up-sampling
    is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b74b538cdd7848eed4a1a56dd91e693.png)'
  prefs: []
  type: TYPE_IMG
- en: Step 5 (first upsampling) — hourglass architecture — Image from [[1](https://arxiv.org/pdf/2110.13711.pdf)]
    modified by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'And the second upsampling is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eec60403b7b7f0fd713258a57efe3248.png)'
  prefs: []
  type: TYPE_IMG
- en: Step 5 (second upsampling)— hourglass architecture — Image from [[1](https://arxiv.org/pdf/2110.13711.pdf)]
    modified by the author
  prefs: []
  type: TYPE_NORMAL
- en: After every up-sampling operation, we pass the token embeddings through Transformer
    layers. In the image they are called either *shortened layers* or *post-vanilla
    layers.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc461c041498f9df0afb5b8234fe83c6.png)'
  prefs: []
  type: TYPE_IMG
- en: step 5 — upsampling involves Transformer layers either as shortened layers or
    as post vanilla. — Image from [[1](https://arxiv.org/pdf/2110.13711.pdf)] modified
    by the author
  prefs: []
  type: TYPE_NORMAL
- en: The last upsampling passes embeddings through *post vanilla layers* and that
    outputs the embedding of the next predicted token.
  prefs: []
  type: TYPE_NORMAL
- en: Down-sampling Step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The downsampling step (also known as shortening step in the paper) shortens
    the input sequence to fewer tokens. This step is done by merging tokens into groups
    using various pooling operations such as: 1) average pooling, 2) linear pooling
    and 3) attention pooling.'
  prefs: []
  type: TYPE_NORMAL
- en: '**1)Average pooling:** On a high level, average pooling merges k adjacent token
    embeddings into a single embedding by taking the average. This method has two
    hyper- parameters: “pool size” and “stride”.'
  prefs: []
  type: TYPE_NORMAL
- en: '“Pool size” is the size of the window, and “stride” is how many steps the window
    size would move forward each time. For example in a sequence of “ABCDEF” and with
    pool size = stride = 2, first two tokens make up the first window and the window
    moves forward 2 tokens at a time. So the windows will be: [AB], [CD], [EF].'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The paper sets both “pool size” and “stride” to a same number and call it “shortening
    factor (sf)”. Let’s see this in an example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the input sequence is *[x1, x2, x3, x4, x5, x6, x7, x8, x9, x10]* and hyper-parameters
    *pool size=stride=3*, then **average pooling** divides the sequence into chunks
    of size 3 i.e. *[x1, x2, x3], [x4, x5, x6], [x7, x8, x9], [x10]* and averages
    token embeddings in each window to get a single embedding i.e.:'
  prefs: []
  type: TYPE_NORMAL
- en: '*e1 = mean(x1, x2, x3)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*e2 = mean(x4, x5, x6)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*e3 = mean(x7, x8, x9)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*e4 = x10*'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore the shortened sequence will be *[e1, e2, e3, e4]* . Note the length
    of the shortened sequence is input length/sf = 10/3 = 3.
  prefs: []
  type: TYPE_NORMAL
- en: '**2) Linear pooling:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This method sets a stride = k and divides the input sequence of length L tokens
    to L/k windows. Each window has k tokens where each token has an embedding vector
    of dimension let’s say d. The method then flattens each window into k*d dimensional
    vector, and forms the following matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95774df342131fe20dc9d812f3fa8429.png)'
  prefs: []
  type: TYPE_IMG
- en: linear pooling — part 1 — image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say the input sequence is *[x1, x2, x3, x4, x5, x6, x7, x8, x9, x10]*
    and stride=3, then we have the following windows: *[x1, x2, x3] , [x4, x5, x6],
    [x7, x8, x9], [x10],* and if every token has a 100-dimensional embedding vector,
    then the above matrix becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/53a1c77052346acea2c58fbdc166a2f9.png)'
  prefs: []
  type: TYPE_IMG
- en: linear pooling — part 2 — image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Note that we have now reduced length of sequence from 10 to 4, but now each
    new token has dimensionality of 300 instead of 100! To bring it back to the original
    dimensionality, linear pooling projects them to a 100-dim space using a learned
    linear transformation. A linear transformation is a 300*100 matrix that is learned
    from the data.
  prefs: []
  type: TYPE_NORMAL
- en: '**3) Attention pooling:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This method starts similar to above two methods: the input sequence is divided
    into windows of size k, then an attention is applied within each window, this
    allows tokens inside each window to attend to each other. At the end the embeddings
    produced by attention of tokens in each window are summed together. After this
    step, a feedforward layer is applied on the chunk embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c0865f8ea2e8dffe7d87b2115617ac5.png)'
  prefs: []
  type: TYPE_IMG
- en: Attention pooling — image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Up-sampling Step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The up-sampling step expands the shortened sequence back to the original full
    length. There are two simple ways to do upsampling:'
  prefs: []
  type: TYPE_NORMAL
- en: '*repeat expansion:* The *repeat expansion* just simply copies each embedding
    multiple times. This is computationally very efficient.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*linear expansion*. The *linear expansion* projects into the higher dimension
    then expands it. For example if the shortened sequence is [e1, e2, e3, e4] and
    the *sf=k=3*, then each embedding is linearly projected to a vector of size *k
    * d*, where *d* is the original embedding dimension. The projection weight matrix
    is learnable and is trained end-to-end along with the full model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To maintain fidelity to the original input sequence, a residual connection (shown
    as red dotted line) adds the input sequence from before shortening to the upsampled
    sequence. Think of this as a way to acoustic context through the multiple shorten-expand
    cycles.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af7582679c40f4b36b7f9f20603184f9.png)'
  prefs: []
  type: TYPE_IMG
- en: residual connections added for maintaining fidelity — image from [[1](https://arxiv.org/pdf/2110.13711.pdf)]
    modified by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a more advanced upsampling method too that is called *attention upsampling*
    and it works as following:'
  prefs: []
  type: TYPE_NORMAL
- en: If the shortened sequence is [e1, e2, e3, e4] and *sf=k=3,* then first a linear
    or repeat upsampling is applied to expand this to the original length. This gives
    [u1, u2, …, u12].
  prefs: []
  type: TYPE_NORMAL
- en: 'Let the embeddings before shortening be [x1, x2, …, x12]; these are added to
    the upsampled embeddings via residual connection (the red dotted lines) and form
    [u1+x1, …, u12+x12]. Now the self-attention mechanism is applied over this sequence,
    where:'
  prefs: []
  type: TYPE_NORMAL
- en: Queries (Q) come from the summed embeddings [u1+x1, …, u12+x12].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keys (K) and Values (V) come from the upsampled embeddings [u1, u2, …, u12].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This updates the summed embeddings and that would be the final output. The attention
    over the upsampled sequence helps amplify relevant parts and combine it with the
    pre-shortened context.
  prefs: []
  type: TYPE_NORMAL
- en: Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: They[1] evaluated their model on language modeling using enwik8, and image generation
    using ImageNet-32/64\. They showed perplexity was improved on Enwik8 dataset by
    10–15% over Transformer-XL baseline; and they achieved new State-of-the-art for
    autoregressive Transformer models on ImageNet-32 image generation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d80a600921bf4e70847f3cdad6668b2e.png)'
  prefs: []
  type: TYPE_IMG
- en: parameters of the experiment — image by the author
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the hourglass network. In next article, we will look into other
    hierarchical transformer models.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article we reviewed a hierarchical architecture for transformers which
    improves efficiency and reduces memory usage in processing long sequences. The
    architecture is called Hourglass [1] and it consists of two major components:
    1) shortening or downsampling, and 2) upsampling. The shortening is done by merging
    tokens into groups using pooling operations like average pooling or linear pooling.
    The sequence length is reduced by a shorten factor k in the middle layers of the
    network. The upsampling component uses methods like linear upsampling or attention
    upsampling to expand the shortened sequences back to the original length. Hourglass
    models improve perplexity compared to baseline Transformers like Transformer-XL
    [1]. In fact, it achieves new SOTA for Transformer models on ImageNet32 image
    generation task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have any questions or suggestions, feel free to reach out to me:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Email: mina.ghashami@gmail.com'
  prefs: []
  type: TYPE_NORMAL
- en: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Hierarchical Transformers Are More Efficient Language Models](https://arxiv.org/pdf/2110.13711.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[An Exploration of Hierarchical Attention Transformers for Efficient Long Document
    Classification](https://arxiv.org/abs/2210.05529)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
