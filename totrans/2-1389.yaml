- en: 'KServe: Highly scalable machine learning deployment with Kubernetes'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KServe：基于Kubernetes的高可扩展机器学习部署
- en: 原文：[https://towardsdatascience.com/kserve-highly-scalable-machine-learning-deployment-with-kubernetes-aa7af0b71202](https://towardsdatascience.com/kserve-highly-scalable-machine-learning-deployment-with-kubernetes-aa7af0b71202)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/kserve-highly-scalable-machine-learning-deployment-with-kubernetes-aa7af0b71202](https://towardsdatascience.com/kserve-highly-scalable-machine-learning-deployment-with-kubernetes-aa7af0b71202)
- en: Kubenetes model inferencing made easy.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes模型推理变得简单。
- en: '[](https://medium.com/@lloyd.hamilton?source=post_page-----aa7af0b71202--------------------------------)[![Lloyd
    Hamilton](../Images/0c516beb55ed3aac21978b3bc85ca9b1.png)](https://medium.com/@lloyd.hamilton?source=post_page-----aa7af0b71202--------------------------------)[](https://towardsdatascience.com/?source=post_page-----aa7af0b71202--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----aa7af0b71202--------------------------------)
    [Lloyd Hamilton](https://medium.com/@lloyd.hamilton?source=post_page-----aa7af0b71202--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@lloyd.hamilton?source=post_page-----aa7af0b71202--------------------------------)[![Lloyd
    Hamilton](../Images/0c516beb55ed3aac21978b3bc85ca9b1.png)](https://medium.com/@lloyd.hamilton?source=post_page-----aa7af0b71202--------------------------------)[](https://towardsdatascience.com/?source=post_page-----aa7af0b71202--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----aa7af0b71202--------------------------------)
    [Lloyd Hamilton](https://medium.com/@lloyd.hamilton?source=post_page-----aa7af0b71202--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----aa7af0b71202--------------------------------)
    ·10 min read·May 29, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----aa7af0b71202--------------------------------)
    ·阅读时间10分钟·2023年5月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/65c9c56369db8df360025d18d9c73a22.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65c9c56369db8df360025d18d9c73a22.png)'
- en: Image sourced from [KServe](https://kserve.github.io/website/0.10/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [KServe](https://kserve.github.io/website/0.10/)
- en: In the wake of the release of chatGPT, it is becoming increasingly difficult
    to avoid technologies that leverage machine learning. From text prediction on
    your messaging app to facial recognition on your smart door bell, machine learning
    (ML) can be found in almost every piece of tech we use today.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着chatGPT的发布，避免利用机器学习的技术变得越来越困难。从你消息应用中的文本预测到智能门铃上的面部识别，机器学习（ML）几乎存在于我们今天使用的每一项技术中。
- en: How machine learning technologies are delivered to consumers is one of the many
    challenges organisations will have to address during development. The deployment
    strategies of ML products have a significant impact on the end users of your product.
    This can mean the difference between Siri on your iPhone or chatGPT in your web
    browser.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习技术如何交付给消费者是组织在开发过程中必须面对的许多挑战之一。ML产品的部署策略对产品的最终用户有着显著影响。这可能意味着你iPhone上的Siri和你网页浏览器中的chatGPT之间的区别。
- en: Beyond the sleek user interface and overly assertive chat dialogues of ChatGPT,
    hides the complex mechanisms required to deploy the large language ML model. ChatGPT
    is built on a highly scalable framework that is designed to deliver and support
    the model during its exponential adoption. In reality, the actual ML model will
    only make up a small proportion of the whole project. Such projects are often
    cross disciplinary and require expertise in data engineering, data science and
    software development. Therefore, frameworks that simplifies the model deployment
    processes are becoming increasingly vital in delivering models to production,
    helping organisations save time and money.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在ChatGPT那光鲜的用户界面和过于自信的聊天对话背后，隐藏着部署大型语言ML模型所需的复杂机制。ChatGPT建立在一个高度可扩展的框架上，旨在在其指数级采用过程中提供和支持该模型。实际上，实际的ML模型仅占整个项目的一小部分。这些项目通常是跨学科的，要求在数据工程、数据科学和软件开发方面的专业知识。因此，简化模型部署过程的框架在将模型推向生产中变得越来越重要，帮助组织节省时间和金钱。
- en: Without the proper operational framework to support and manage ML models, organisations
    will often meet bottle necks when attempting to scale the number of ML model in
    production.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有合适的操作框架来支持和管理ML模型，组织在尝试扩大生产中的ML模型数量时通常会遇到瓶颈。
- en: While no single tool has emerged as a clear winner in the highly saturated market
    of MLOps toolkits, KServe is becoming an increasingly popular tool to help organisations
    meet scalability requirements of ML models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在竞争激烈的MLOps工具市场中没有出现明显的赢家，但KServe正成为帮助组织满足ML模型可扩展性要求的越来越受欢迎的工具。
- en: '**Note: I am not affiliated with KServe nor have been sponsored to write this
    article.**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：我与KServe没有任何关联，也没有获得赞助来撰写这篇文章。**'
- en: What is KServe?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是KServe？
- en: KServe is a highly scalable machine learning deployment toolkit for Kubernetes.
    It is an orchestration tool that is built on top of Kubernetes and leverages two
    other open sourced projects, Knative-Serving and Istio; more on this later.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: KServe是一个高度可扩展的机器学习部署工具包，适用于Kubernetes。它是一个建立在Kubernetes之上的编排工具，利用了另外两个开源项目，Knative-Serving和Istio；稍后会详细介绍。
- en: '![](../Images/65c9c56369db8df360025d18d9c73a22.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65c9c56369db8df360025d18d9c73a22.png)'
- en: Image sourced from [KServe](https://kserve.github.io/website/0.10/)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [KServe](https://kserve.github.io/website/0.10/)
- en: KServe significantly simplifies the deployment process of ML Models into a Kubernetes
    cluster by unifying the deployment into a single resource definition. It makes
    the machine learning deployment part of any ML project easy to learn and ultimately
    decreases the barrier to entry. Therefore, models deployed using KServe can be
    easier to maintain than models deployed using traditional Kubernetes deployment
    that require a Flask or FastAPI service.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: KServe通过将部署统一到一个资源定义中，显著简化了将ML模型部署到Kubernetes集群中的过程。它使得机器学习部署成为任何ML项目的一部分，易于学习，并最终降低了进入门槛。因此，使用KServe部署的模型比使用传统的Kubernetes部署（需要Flask或FastAPI服务）的模型更易于维护。
- en: With KServe, there is no requirement to wrap your model inside a FastAPI or
    Flask app before exposing it through the internet via HTTPS. KServe has built-in
    functionality that essentially replicates this process but without the overhead
    of having to maintain API endpoints, configure pod replicas, or configure internal
    routing networks in Kubernetes. All you have to do is point KServe to your model,
    and it will handle the rest.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用KServe，无需在通过HTTPS将模型暴露到互联网之前将其封装在FastAPI或Flask应用程序中。KServe内置了本质上复制这一过程的功能，但无需维护API端点、配置Pod副本或在Kubernetes中配置内部路由网络。你只需将KServe指向你的模型，它将处理其余的部分。
- en: Beyond the simplification of the deployment processes, KServe also offers many
    features, including canary deployments, inference autoscaling, and request batching.
    These features will not be discussed as they are out of scope. However, this guide
    will hopefully set the foundation of understanding required to explore further.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 除了简化部署过程之外，KServe还提供了许多功能，包括金丝雀部署、推理自动扩展和请求批处理。这些功能将不在本讨论范围之内。然而，本指南希望能够为进一步探索打下基础。
- en: First, let’s talk about the two key technologies, Istio and Knative, that accompany
    KServe.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们讨论KServe所依赖的两个关键技术，Istio和Knative。
- en: Istio
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Istio
- en: Much of the functionality that KServe brings to the table would be difficult
    without Istio. Istio is a service mesh that extends your applications deployed
    in Kubernetes. It is a dedicated infrastructure layer that adds capabilities such
    as observability, traffic management, and security. For those familiar with Kubernetes,
    Istio replaces the standard ingress definitions typically found in a Kubernetes
    cluster.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: KServe所带来的许多功能在没有Istio的情况下将很难实现。Istio是一个服务网格，扩展了部署在Kubernetes中的应用程序。它是一个专门的基础设施层，增加了可观测性、流量管理和安全性等功能。对于熟悉Kubernetes的人来说，Istio替代了通常在Kubernetes集群中找到的标准Ingress定义。
- en: The complexity of managing traffic and maintaining observability grows as a
    Kubernetes-based system scales. One of the best features of Istio is its ability
    to centralize the controls of service-level communications. This gives developers
    greater control and transparency over communications among services.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 随着基于Kubernetes的系统规模扩大，管理流量和维护可观测性的复杂性也在增加。Istio的一个最佳功能是能够集中控制服务级别的通信。这使得开发人员对服务之间的通信有更大的控制和透明度。
- en: With Istio, developers do not need to design applications that can handle traffic
    authentication or authorization. Ultimately, Istio helps reduce the complexity
    of deployed apps and allows developers to concentrate on the important components
    of the apps.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Istio，开发人员不需要设计能够处理流量认证或授权的应用程序。最终，Istio有助于减少部署应用程序的复杂性，使开发人员能够集中精力关注应用程序的关键组件。
- en: By leveraging the networking features of Istio, KServe can bring features that
    include canary deployment, inference graphs, and custom transformers.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用Istio的网络功能，KServe可以提供包括金丝雀部署、推理图和自定义变换器等功能。
- en: KNative
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KNative
- en: Knative, on the other hand, is an open-source enterprise-level solution to build
    serverless and event-driven applications. Knative is built on top of Istio to
    bring serverless code execution capabilities that are similarly offered by AWS
    Lambdas and Azure Functions. Knative is a platform-agnostic solution for running
    serverless deployments in Kubernetes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Knative 是一个开源企业级解决方案，用于构建无服务器和事件驱动的应用程序。Knative 基于 Istio 构建，提供类似于 AWS Lambdas
    和 Azure Functions 的无服务器代码执行能力。Knative 是一个与平台无关的解决方案，用于在 Kubernetes 中运行无服务器部署。
- en: One of the best features of Knative is the scale-to-zero feature. This is a
    critical component of KServe’s ability to scale up or down ML model deployment
    and one that maximizes resource utilization and saves on costs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Knative 的一个最佳功能是缩放到零的特性。这是 KServe 能够扩展或缩小 ML 模型部署的关键组件，并且能够最大化资源利用率和节省成本。
- en: Should I use KServe?
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我应该使用 KServe 吗？
- en: KServe, like many other tools, is not a one size fits all solution that will
    fit your organisation’s requirements. It has a high cost of entry due to the fact
    that some experience in working with Kubernetes is required. If you are just getting
    started with Kubernetes, there are many resources online and I highly recommend
    checking out resources such as the [DevOps](https://www.youtube.com/channel/UCFe9-V_rN9nLqVNiI8Yof3w)
    guy on Youtube. Nonetheless, even without a deep understanding of Kubernetes,
    it is possible to learn to use KServe.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: KServe 和许多其他工具一样，并不是一个适合所有组织需求的一刀切解决方案。由于需要一定的 Kubernetes 经验，其入门成本较高。如果你刚刚开始使用
    Kubernetes，有许多在线资源，我强烈建议查看 [DevOps](https://www.youtube.com/channel/UCFe9-V_rN9nLqVNiI8Yof3w)
    频道的资源。不过，即使没有深入了解 Kubernetes，也可以学习使用 KServe。
- en: KServe will be ideal in organisations already leveraging Kubernetes where there
    are existing knowledge in working with Kubernetes. It may also suit organisations
    looking to move away or complement managed services like SageMaker or Azure Machine
    Learning in order to have greater control over your model deployment process.
    The increase in ownership can result in significant cost reductions and increased
    configurability to meet project specific requirements.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: KServe 在已经使用 Kubernetes 的组织中将是理想选择，这些组织已经具备了与 Kubernetes 相关的知识。它也适合那些希望摆脱或补充像
    SageMaker 或 Azure Machine Learning 等托管服务的组织，以便对模型部署过程拥有更大的控制权。增加的所有权可以显著降低成本，并提高配置能力，以满足项目的具体要求。
- en: Nevertheless, the right cloud infrastructure decision will depend on a case
    by case basis as infrastructure requirements will differ across companies.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，正确的云基础设施决策将依赖于具体情况，因为基础设施需求在不同公司之间会有所不同。
- en: Pre-requisites
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 先决条件
- en: This guide will take you though the steps required to get you set up with KServe.
    You will be walked through the steps to install KServe and serve your first model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南将引导你完成设置 KServe 所需的步骤。你将了解如何安装 KServe 并服务你的第一个模型。
- en: 'There are several pre-requisites that will need to be met before proceeding.
    You will require the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，需要满足几个先决条件。你将需要以下内容：
- en: '[Kubectl](https://kubernetes.io/docs/tasks/tools/) installation'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kubectl](https://kubernetes.io/docs/tasks/tools/) 安装'
- en: '[Helm](https://helm.sh/docs/intro/install/) installation'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Helm](https://helm.sh/docs/intro/install/) 安装'
- en: '[Kubectx](https://github.com/ahmetb/kubectx) installation (optional)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kubectx](https://github.com/ahmetb/kubectx) 安装（可选）'
- en: Kubernetes Cluster
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 集群
- en: For this tutorial, I recommend experimenting with a Kubernetes cluster using
    [Kind](https://kind.sigs.k8s.io). It is a tool to run a local Kubernetes cluster
    without needing to spin up cloud resources. In addition, I highly recommend Kubectx
    as a tool to easily switch between Kubernetes context if you are working across
    multiple clusters.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本教程，我推荐使用 [Kind](https://kind.sigs.k8s.io) 来实验 Kubernetes 集群。它是一个运行本地 Kubernetes
    集群的工具，无需启动云资源。此外，如果你在多个集群之间工作，我强烈推荐使用 Kubectx 作为一个轻松切换 Kubernetes 上下文的工具。
- en: However, when running production workload, you will need access to a fully functioning
    Kubernetes cluster to configure DNS and HTTPS.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在运行生产工作负载时，你将需要访问一个完全功能的 Kubernetes 集群来配置 DNS 和 HTTPS。
- en: 'Deploy a Kubernetes cluster in Kind with:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kind 中部署 Kubernetes 集群，使用：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Switch to the correct Kubernetes context with:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 切换到正确的 Kubernetes 上下文，使用：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Installation
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装
- en: The following steps will install Istio v1.16, Knative Serving v1.7.2 and KServe
    v0.10.0\. These versions are best suited for this tutorial as Knative v1.8 onwards
    will require DNS configuration for ingress which adds a layer of complexity that
    is currently out of scope.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将安装Istio v1.16、Knative Serving v1.7.2和KServe v0.10.0。这些版本最适合本教程，因为Knative
    v1.8及以后版本将需要DNS配置进行入口，这增加了当前教程范围之外的复杂性。
- en: Istio Installation.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Istio安装。
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 2\. Install KNative Serving.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 安装KNative Serving。
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 3\. Install cert manager. Cert manager is required to manage valid certificate
    for HTTPs traffic.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 安装证书管理器。证书管理器用于管理HTTPs流量的有效证书。
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 4\. Create a namespace for model.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 为模型创建一个命名空间。
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 5\. Clone the [KServe](https://github.com/kserve/kserve) repository.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 克隆[KServe](https://github.com/kserve/kserve)仓库。
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 6\. Install KServe Cutom Resource Definitions and KServe Runtimes into the model
    namespace in your cluster.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. 将KServe自定义资源定义和KServe运行时安装到集群中的模型命名空间。
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Great! We now have KServe installed on the cluster. Let’s get deploying!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们现在已经在集群上安装了KServe。让我们开始部署吧！
- en: First Inference Service
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一个推理服务
- en: In order to ensure that the deployment went smoothly, let us deploy a demo inference
    service. The source code for the deployment can be found [here](https://kserve.github.io/website/0.10/get_started/first_isvc/#1-create-a-namespace).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保部署顺利，让我们部署一个演示推理服务。部署的源代码可以在[这里](https://kserve.github.io/website/0.10/get_started/first_isvc/#1-create-a-namespace)找到。
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The yaml resource definition above deploys a test inference service that sources
    a publicly available model trained using the SciKit-Learn library. KServe supports
    many different flavours of [machine learning libraries](https://kserve.github.io/website/0.10/modelserving/v1beta1/serving_runtime/).
    These include MLFlow, PyTorch or XGBoost models; more are added at each release.
    If none of these out-of-the-box libraries meet your requirements, KServe also
    support [custom predictors](https://kserve.github.io/website/0.10/modelserving/v1beta1/custom/custom_model/).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的yaml资源定义部署了一个测试推理服务，该服务使用SciKit-Learn库训练的公开可用模型。KServe支持许多不同的[机器学习库](https://kserve.github.io/website/0.10/modelserving/v1beta1/serving_runtime/)。这些包括MLFlow、PyTorch或XGBoost模型；每次发布时都会添加更多。如果这些现成的库都不符合你的要求，KServe还支持[自定义预测器](https://kserve.github.io/website/0.10/modelserving/v1beta1/custom/custom_model/)。
- en: It is possible to monitor the status of the current deployment by getting the
    available pods in the namespace.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过获取命名空间中的可用pod来监控当前部署的状态。
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/1c11648c1c5afabf69556e4d3a2ed99b.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c11648c1c5afabf69556e4d3a2ed99b.png)'
- en: Image by author
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'If you run into issues with the deployment use the following to debug:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在部署过程中遇到问题，请使用以下命令进行调试：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can also check the status of the inference service deployment with:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过以下命令检查推理服务部署的状态：
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/f4bac95397a4b4970d541b2482199c3b.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4bac95397a4b4970d541b2482199c3b.png)'
- en: Image by author
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: If the inference service is marked true, we are ready to perform our first prediction.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果推理服务标记为true，我们就可以进行第一次预测了。
- en: Performing a prediction
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行预测
- en: To perform a prediction, we will need to determine if our Kubernetes cluster
    is running in an environment that supports external load balancers.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行预测，我们需要确定我们的Kubernetes集群是否运行在支持外部负载均衡器的环境中。
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Kind Cluster**'
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Kind集群**'
- en: Clusters deployed using Kind does not support external load balancers therefore
    you will have an ingress gateway that looks similar to below.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Kind部署的集群不支持外部负载均衡器，因此你将会有一个类似下面的入口网关。
- en: '![](../Images/823bdfea40270bb0906d67d5c451f56b.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/823bdfea40270bb0906d67d5c451f56b.png)'
- en: Kind External Load Balancer (Image by author)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Kind外部负载均衡器（图片由作者提供）
- en: In this case we would have to port-forward the istio-ingressgateway which will
    allow us to access it via `localhost`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们需要将istio-ingressgateway的端口转发，这样可以通过`localhost`访问它。
- en: 'Port-forward the istio ingress gateway service to port `8080` on `localhost`
    with:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 将istio入口网关服务的端口转发到`localhost`上的端口`8080`，使用以下命令：
- en: '[PRE13]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then set the ingress host and port with:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后用以下命令设置入口主机和端口：
- en: '[PRE14]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Kubernetes Cluster**'
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Kubernetes集群**'
- en: If the external IP is valid and does not display `<pending>`, we are able to
    send an inference request through the internet at the IP address.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果外部IP有效且不显示`<pending>`，我们就能够通过该IP地址在互联网上发送推理请求。
- en: '![](../Images/073c75ea25c6b436a5b5063b31c9dc0b.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/073c75ea25c6b436a5b5063b31c9dc0b.png)'
- en: Ingress Gateway IP address (Image by author)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 入口网关IP地址（图片由作者提供）
- en: 'Set the ingress host and port with:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 用以下命令设置入口主机和端口：
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Perform Inference
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行推理
- en: Prepare an input request `json` file for the inference request.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 准备一个用于推断请求的输入 `json` 文件。
- en: '[PRE16]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then perform an inference with curl:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用 curl 执行推断：
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The request will be sent to the KServe deployment through the istio-ingress
    gateway. If everything is in order, we will get a `json` reply from the inference
    service with the prediction of `[1,1]` for each of the instances.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 请求将通过 istio-ingress 网关发送到 KServe 部署。如果一切正常，我们将从推断服务收到一个 `json` 回复，其中包含每个实例的预测值
    `[1,1]`。
- en: '![](../Images/db8f0720a7ddb9c4c3c0ddf4978a5393.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db8f0720a7ddb9c4c3c0ddf4978a5393.png)'
- en: Scaling to Zero
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缩放到零
- en: By leveraging the features of Knative, KServe supports scale-to-zero capabilities.
    This feature effectively manages limited resources across the cluster by scaling
    pods not in use to zero. Scaling to zero capabilities allow the creation of a
    reactive system that responds to requests, as opposed to a system that is always
    up. This will facilitate the deployment of a greater number of models in the cluster
    than traditional deployment configurations can.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用 Knative 的特性，KServe 支持缩放到零的能力。此功能通过将未使用的 Pods 缩放为零来有效管理集群中的有限资源。缩放到零的能力允许创建一个响应请求的反应系统，而不是一个始终运行的系统。这将使得在集群中部署更多的模型成为可能，相较于传统的部署配置。
- en: However, note that there is a cold start penalty for pods that have been scaled
    down. This will vary depending on the size of the image/model and the available
    cluster resources. A cold start can take 5 minutes if the cluster needs to scale
    an additional node or 10 seconds if the model is already cached on the node.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，对于已缩放下来的 Pods，会有冷启动的惩罚。这将根据镜像/模型的大小和可用的集群资源而有所不同。如果集群需要扩展额外的节点，冷启动可能需要
    5 分钟；如果模型已经缓存到节点上，冷启动可能需要 10 秒。
- en: 'Let us modify the existing scikit-learn inference service and enable scale
    to zero by defining `minReplicas: 0`.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们修改现有的 scikit-learn 推断服务，通过定义 `minReplicas: 0` 来启用缩放到零。'
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: By setting minReplicas to 0, this will command Knative to scale the inference
    service down to zero when there are no HTTP traffic. You will notice that after
    a period of 30 seconds, the pods for Sklearn-Iris model will be scaled down.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将 `minReplicas` 设置为 0，这将指示 Knative 在没有 HTTP 流量时将推断服务缩放为零。你会注意到，在 30 秒后，Sklearn-Iris
    模型的 Pods 将会被缩放到零。
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/9d24abadd3ddf3c8f69b5fa420059792.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d24abadd3ddf3c8f69b5fa420059792.png)'
- en: Sklearn-Iris predictors scales down to zero
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Sklearn-Iris 预测器缩放到零
- en: To reinitialise the inference service, send a prediction request to the same
    endpoint.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要重新初始化推断服务，请向相同的端点发送预测请求。
- en: '[PRE20]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/3399799fc2385ef8f6b425161a638977.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3399799fc2385ef8f6b425161a638977.png)'
- en: This will trigger pod initialisation from cold start and return a prediction.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这将触发 Pods 的冷启动初始化并返回一个预测结果。
- en: Conclusion
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: KServe simplifies the process of machine learning deployment and shortens the
    path to production. When combined with Knative and Istio, KServe has the added
    bonus of being highly customisable and bring many features that easily rivals
    those offered in managed cloud solutions.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: KServe 简化了机器学习部署的过程，并缩短了生产的路径。当与 Knative 和 Istio 结合使用时，KServe 具有高度的可定制性，并带来许多与托管云解决方案提供的功能相媲美的特性。
- en: Of course, migrating the model deployment process in-house has its own innate
    complexities. However, the increase in platform ownership will confer greater
    flexibility in meeting project specific requirements. With the right Kubernetes
    expertise, KServe can be a powerful tool that will allow organisations to easily
    scale their machine learning deployment across any cloud provider to meet increasing
    demand.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，将模型部署过程迁移到内部也有其固有的复杂性。然而，平台所有权的增加将提供更大的灵活性，以满足项目特定要求。凭借正确的 Kubernetes 专业知识，KServe
    可以成为一个强大的工具，使组织能够轻松地在任何云提供商上扩展其机器学习部署，以满足不断增长的需求。
