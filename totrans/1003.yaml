- en: 'GPT-4 vs. ChatGPT: An exploration of training, performance, capabilities, and
    limitations'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/gpt-4-vs-chatgpt-an-exploration-of-training-performance-capabilities-and-limitations-35c990c133c5](https://towardsdatascience.com/gpt-4-vs-chatgpt-an-exploration-of-training-performance-capabilities-and-limitations-35c990c133c5)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: GPT-4 is an improvement, but temper your expectations.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mary.newhauser?source=post_page-----35c990c133c5--------------------------------)[![Mary
    Newhauser](../Images/7f0d7287f7b735bb9391858f1fc641ee.png)](https://medium.com/@mary.newhauser?source=post_page-----35c990c133c5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----35c990c133c5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----35c990c133c5--------------------------------)
    [Mary Newhauser](https://medium.com/@mary.newhauser?source=post_page-----35c990c133c5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----35c990c133c5--------------------------------)
    ·7 min read·Mar 17, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ebc0c71e1eaa32026c30fe54eb8b0d9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI stunned the world when it dropped [ChatGPT](https://openai.com/blog/chatgpt)
    in late 2022\. The new generative language model is expected to totally transform
    entire industries, including media, education, law, and tech. In short, ChatGPT
    threatens to disrupt just about everything. And even before we had time to truly
    envision a post-ChatGPT world, OpenAI dropped [GPT-4](https://openai.com/research/gpt-4).
  prefs: []
  type: TYPE_NORMAL
- en: In recent months, the speed with which groundbreaking large language models
    have been released is astonishing. If you still don’t understand how ChatGPT differs
    from GPT-3, let alone GPT-4, I don’t blame you.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will cover the key similarities and differences between
    ChatGPT and GPT-4, including their training methods, performance and capabilities,
    and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 'ChatGPT vs. GPT-4: Similarities & differences in training methods'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ChatGPT and GPT-4 both stand on the shoulders of giants, building on previous
    versions of GPT models while adding improvements to model architecture, employing
    more sophisticated training methods, and increasing the number of training parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Both models are based on the transformer architecture. [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)
    and [GPT-3](https://arxiv.org/pdf/2005.14165.pdf) use multi-headed self-attention
    to decide which text inputs to pay the most attention to. The models also use
    a [decoder-only](https://jalammar.github.io/illustrated-gpt2/) architecture that
    generates output sequences one token at a time, iteratively predicting the next
    token in a sequence. Although the precise architectures for ChatGPT and GPT-4
    have not been released, we can assume they continue to be decoder-only models.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI’s [GPT-4 Technical Report](https://cdn.openai.com/papers/gpt-4.pdf) offers
    little information on GPT-4’s model architecture and training process, citing
    the “competitive landscape and the safety implications of large-scale models.”
    What we do know is that ChatGPT and GPT-4 are probably trained in a similar manner,
    which is a departure from training methods used for GPT-2 and GPT-3\. We know
    much more about the training methods for ChatGPT than GPT-4, so we’ll start there.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To start with, ChatGPT is trained on dialogue datasets, including demonstration
    data, in which human annotators provide demonstrations of the expected output
    of a chatbot assistant in response to specific prompts. This data is used to fine-tune
    GPT3.5 with supervised learning, producing a policy model, which is used to generate
    multiple responses when fed prompts. Human annotators then rank which of the responses
    for a given prompt produced the best results, which is used to train a reward
    model. The reward model is then used to iteratively fine-tune the policy model
    using reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c520850d7d2cfde00d40aafaf710a0b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: To sum it up in one sentence, ChatGPT is trained using [Reinforcement Learning
    from Human Feedback](https://openai.com/research/learning-from-human-preferences)
    (RLHF), a way of incorporating human feedback to improve a language model during
    training. *This allows the model’s output to align to the task requested by the
    user, rather than just predict the next word in a sentence based on a corpus of
    generic training data, like GPT-3.*
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenAI has yet to divulge details on how it trained GPT-4\. Their Technical
    Report doesn’t include “details about the architecture (including model size),
    hardware, training compute, dataset construction, training method, or similar.”
    *What we do know is that GPT-4 is a transformer-style generative multimodal model
    trained on both publicly available data and licensed third-party data and subsequently
    fine-tuned using RLHF.* Interestingly, OpenAI did share details regarding their
    upgraded RLHF techniques to make the model responses more accurate and less likely
    to veer outside safety guardrails.
  prefs: []
  type: TYPE_NORMAL
- en: After training a policy model (as with ChatGPT), RLHF is used in adversarial
    training, a process that trains a model on malicious examples intended to deceive
    the model in order to defend the model against such examples in the future. In
    the case of GPT-4, human domain experts across several fields rate the responses
    of the policy model to adversarial prompts. These responses are then used to train
    additional reward models that iteratively fine-tune the policy model, resulting
    in a model that’s less likely to give out dangerous, evasive, or inaccurate responses.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3f22d2d1c180b28800d23f55ed6a6a46.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'ChatGPT vs. GPT-4: Similarities & differences in performance and capabilities'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Capabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In terms of capabilities, ChatGPT and GPT-4 are more similar than they are different.
    Like its predecessor, GPT-4 also interacts in a conversational style that aims
    to align with the user. As you can see below, the responses between the two models
    for a broad question are very similar.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/678e8b3ff33d0bfa3463a090d0379362.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI agrees that the distinction between the models can be subtle and claims
    that “difference comes out when the complexity of the task reaches a sufficient
    threshold.” Given the six months of adversarial training the GPT-4 base model
    underwent in its post-training phase, this is probably an accurate characterization.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike ChatGPT, which accepts only text, GPT-4 accepts prompts composed of both
    images and text, returning textual responses. As of the publishing of this article,
    unfortunately, the capacity for using image inputs is not yet available to the
    public.
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As referenced earlier, OpenAI reports significant improvement in safety performance
    for GPT-4, compared to GPT-3.5 (from which ChatGPT was fine-tuned). However, whether
    the reduction in responses to requests for disallowed content, reduction in toxic
    content generation, and improved responses to sensitive topics are due to the
    GPT-4 model itself or the additional adversarial testing is unclear at this time.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, GPT-4 outperforms GPT-3.5 on most academic and professional exams
    taken by humans. Notably, GPT-4 scores in the 90th percentile on the Uniform Bar
    Exam compared to GPT-3.5, which scores in the 10th percentile. GPT-4 also significantly
    outperforms its predecessor on traditional language model benchmarks as well as
    other SOTA models (although sometimes just barely).
  prefs: []
  type: TYPE_NORMAL
- en: 'ChatGPT vs. GPT-4: Similarities & differences in limitations'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both ChatGPT and GPT-4 have significant limitations and risks. The GPT-4 System
    Card includes insights from a detailed exploration of such risks conducted by
    OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are just a few of the risks associated with both models:'
  prefs: []
  type: TYPE_NORMAL
- en: Hallucination (the tendency to produce nonsensical or factually inaccurate content)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Producing harmful content that violates OpenAI’s policies (e.g. hate speech,
    incitements to violence)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amplifying and perpetuating stereotypes of marginalized people
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating realistic disinformation intended to deceive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While ChatGPT and GPT-4 struggle with the same limitations and risks, OpenAI
    has made special efforts, including extensive adversarial testing, to mitigate
    them for GPT-4\. While this is encouraging, the GPT-4 System Card ultimately demonstrates
    how vulnerable ChatGPT was (and possibly still is). For a more detailed explanation
    of harmful unintended consequences, I recommend reading the GPT-4 System Card,
    which starts on page 38 of the [GPT-4 Technical Report](https://cdn.openai.com/papers/gpt-4.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we review the most important similarities and differences between
    ChatGPT and GPT-4, including their training methods, performance and capabilities,
    and limitations and risks.
  prefs: []
  type: TYPE_NORMAL
- en: While we know much less about the model architecture and training methods behind
    GPT-4, it appears to be a refined version of ChatGPT that now accepts image and
    text inputs and claims to be safer, more accurate, and more creative. Unfortunately,
    we will have to take OpenAI’s word for it, as GPT-4 is only available as part
    of the ChatGPT Plus subscription.
  prefs: []
  type: TYPE_NORMAL
- en: 'The table below illustrates the most important similarities and differences
    between ChatGPT and GPT-4:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38414f3c07eba431c9d5bd868106ee91.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The race for creating the most accurate and dynamic large language models has
    reached breakneck speed, with the release of ChatGPT and GPT-4 within mere months
    of each other. Staying informed on the advancements, risks, and limitations of
    these models is essential as we navigate this exciting but rapidly evolving landscape
    of large language models.
  prefs: []
  type: TYPE_NORMAL
- en: '*If you’d like to stay up-to-date on the latest data science trends, technologies,
    and packages, consider becoming a Medium member. You’ll get unlimited access to
    articles and blogs like Towards Data Science and you’ll be supporting my writing.
    (I earn a small commission for each membership).*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mary.newhauser/membership?source=post_page-----35c990c133c5--------------------------------)
    [## Join Medium with my referral link - Mary Newhauser'
  prefs: []
  type: TYPE_NORMAL
- en: Get access to unlimited Medium articles for $5 per month 🤗 Your membership fee
    directly supports Mary Newhauser and…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@mary.newhauser/membership?source=post_page-----35c990c133c5--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Want to connect?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 📖 Follow me on [Medium](https://medium.com/@mary.newhauser)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 💌 [Subscribe](https://medium.com/@mary.newhauser/subscribe) to get an email
    whenever I publish
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 🖌️ Check out my generative AI [blog](https://www.gptechblog.com/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 🔗 Take a look at my [portfolio](https://www.datascienceportfol.io/marynewhauser)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 👩‍🏫 I’m also a data science [coach](https://www.datajump.co/)!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I’ve also written:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](https://medium.com/nlplanet/fine-tuning-distilbert-on-senator-tweets-a6f2425ca50e?source=post_page-----35c990c133c5--------------------------------)
    [## Fine-tuning DistilBERT on senator tweets'
  prefs: []
  type: TYPE_NORMAL
- en: A guide to fine-tuning DistilBERT on the tweets of American Senators with snscrape,
    SQLite, and Transformers (PyTorch)…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/nlplanet/fine-tuning-distilbert-on-senator-tweets-a6f2425ca50e?source=post_page-----35c990c133c5--------------------------------)
    [](/making-the-jump-from-data-analyst-to-data-scientist-in-2023-74e2cf7fc139?source=post_page-----35c990c133c5--------------------------------)
    [## Making the Jump from Data Analyst to Data Scientist in 2023
  prefs: []
  type: TYPE_NORMAL
- en: The skills and resources you need to transition from a data analyst to data
    scientist position.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/making-the-jump-from-data-analyst-to-data-scientist-in-2023-74e2cf7fc139?source=post_page-----35c990c133c5--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: (1) OpenAI, [Introducing ChatGPT](https://openai.com/blog/chatgpt) (2022).
  prefs: []
  type: TYPE_NORMAL
- en: (2) OpenAI, [GPT-4](https://openai.com/research/gpt-4) (2023).
  prefs: []
  type: TYPE_NORMAL
- en: (3) A. Radford et al., [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)
    (2019).
  prefs: []
  type: TYPE_NORMAL
- en: (4) T. Brown et al., [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)
    (2020).
  prefs: []
  type: TYPE_NORMAL
- en: (5) J. Alammar, [The Illustrated GPT-2 (Visualizing Transformer Language Models)](https://jalammar.github.io/illustrated-gpt2/)
    (2019).
  prefs: []
  type: TYPE_NORMAL
- en: (6) OpenAI, [GPT-4 Technical Report](https://cdn.openai.com/papers/gpt-4.pdf)
    (2023).
  prefs: []
  type: TYPE_NORMAL
- en: (7) OpenAI, [Learning from human preferences](https://openai.com/research/learning-from-human-preferences)
    (2017).
  prefs: []
  type: TYPE_NORMAL
