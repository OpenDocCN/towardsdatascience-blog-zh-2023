# Kaiming He 初始化在神经网络中的数学证明

> 原文：[https://towardsdatascience.com/kaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4](https://towardsdatascience.com/kaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4)

## 推导具有 ReLU 激活函数的神经网络层中权重矩阵的最优初始方差

[](https://medium.com/@EsterHlav?source=post_page-----73b9a0d845c4--------------------------------)[![Ester Hlav](../Images/5fe679b93c42d568d0d5b331a8bf92b9.png)](https://medium.com/@EsterHlav?source=post_page-----73b9a0d845c4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----73b9a0d845c4--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----73b9a0d845c4--------------------------------) [Ester Hlav](https://medium.com/@EsterHlav?source=post_page-----73b9a0d845c4--------------------------------)

·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----73b9a0d845c4--------------------------------) ·阅读时间约 10 分钟·2023 年 2 月 15 日

--

![](../Images/0e072a98caece16e7471a537515610cc.png)

初始化技术是成功训练深度学习架构的前提条件之一。传统上，权重初始化方法需要与激活函数的选择兼容，因为不匹配可能会对训练产生负面影响。

ReLU 是深度学习中最常用的激活函数之一。它的特性使其成为扩展到大型神经网络的非常方便的选择。一方面，由于它是一个线性函数，具有步进函数导数，在反向传播过程中计算导数的开销较小。另一方面，ReLU 有助于减少特征相关性，因为它是一个非负激活函数，*即* 特征只能对后续层产生积极贡献。它在输入维度较大且神经网络往往非常深的卷积架构中是一种流行的选择。

在 He *et al.*（2015）的*“深入探讨整流器：超越人类水平的 ImageNet 分类”* ⁽*¹* ⁾ 中，作者提出了一种使用 ReLU 激活函数来最优初始化神经网络层的方法。这种技术使神经网络在前向和反向传播过程中在输入和输出之间保持常数方差，这在经验上显示出训练稳定性和速度的显著改善。在接下来的章节中，我们将提供 He 初始化技术背后的详细和完整的推导过程。

## 符号

+   神经网络中的一层，由权重矩阵 *Wₖ* 和偏置向量 *bₖ* 组成，经过两个连续的变换。第一个变换是 *yₖ = xₖ Wₖ + bₖ*，第二个变换是 *xₖ ₊ ₁ = f(yₖ)*

+   *xₖ* 是实际层，*yₖ* 是预激活层

+   一层有*nₖ*个单元，因此*xₖ ∈ ℝⁿ⁽* ᵏ*⁾, Wₖ ∈ ℝⁿ⁽* ᵏ*⁾*˙*ⁿ⁽* ᵏ ⁺ ¹*⁾, bₖ ∈ℝⁿ⁽* ᵏ ⁺ ¹*⁾*

+   *xₖWₖ + bₖ* 的维度是（*1 × nₖ*）×（*nₖ × nₖ ₊* ₁*）+ 1 × nₖ ₊* ₗ* = 1 × nₖ ₊* ₁

+   激活函数*f*按元素应用，不改变向量的形状。因此，*xₖ ₊* ₁*= f(xₖ Wₖ+ bₖ)∈ ℝⁿ⁽* ᵏ ⁺ ¹*⁾*

+   对于深度为*n*的神经网络，输入层由*x₀*表示，输出层由*xₙ*表示

+   网络的损失函数由*L*表示

+   *Δx = ∂L/∂x* 表示损失函数关于向量*x*的梯度

## 假设

+   *假设 1*：

    对于这个初始化设置，我们假设一个*非线性*激活函数ReLU定义为*f(x) =* *ReLU(x) = max(0, x).* 作为在两个区间上分别定义的函数，它的导数在*ℝ*的严格正半轴上值为1，在严格负半轴上值为0。技术上，由于两侧的极限不相等，ReLU在0处的导数未定义，即*f’(0⁻*⁻*) = 0 ≠ 1 = f’(0⁺).* 实际上，为了反向传播的目的，假设ReLU’(0)为0。

+   *假设 2*：

    假设神经网络中的所有输入、权重和层在初始化时都是独立同分布的（*iid*），梯度也是如此。

+   *假设 3*：

    假设输入已归一化为零均值，权重和偏置从以零为中心的对称分布中初始化，即*𝔼[x₀] = 𝔼[Wₖ] = 𝔼[bₖ] = 0*。这意味着*xₖ*和yₖ在初始化时都有零的期望，由于*f(0) = 0*，*yₖ*在初始化时具有对称分布。

## 动机

本证明的目的是通过在两个约束条件下找到*Var[W]*来确定权重矩阵的分布：

1.  ∀*k, Var[yₖ] = Var[yₖ ₋* ₁*]*，*即*前向信号中的方差是恒定的

1.  ∀*k, Var[Δxₖ] = Var[Δxₖ ₊* ₁*]，即*反向信号中的方差是恒定的

确保网络中所有层和梯度的方差在初始化时保持恒定，有助于防止神经网络中的梯度爆炸和梯度消失。如果增益*大于* 一，则会导致梯度爆炸和优化发散；如果增益*小于* 一，则会导致梯度消失并停止学习。上述两个方程确保信号增益恰好为一。

本文的动机及推导基于五年前发表的 Xavier Glorot 初始化⁽²⁾ 论文。虽然前述工作使用了后激活层来保持前向信号的恒定方差，但 He 初始化证明使用了前激活层。同样地，对于反向信号，He 的推导使用了后激活层，而非 Glorot 初始化中的前激活层。鉴于这两个证明有一些相似之处，比较这两者有助于理解为什么控制权重的方差在*任何*神经网络中如此重要。（更多细节见 “[Xavier Glorot Initialization in Neural Networks — Math Proof](https://medium.com/towards-data-science/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3)”）

[](/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3?source=post_page-----73b9a0d845c4--------------------------------) [## Xavier Glorot Initialization in Neural Networks — Math Proof

### 详细推导了在深度学习层中使用 tanh 激活函数时寻找权重矩阵的最优初始分布……

towardsdatascience.com](/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3?source=post_page-----73b9a0d845c4--------------------------------)

# 数学证明：Kaiming He 初始化

## I. 前向传播

我们正在寻找 *Wₖ* 使得每个后续前激活层 *y* 的方差相等，*即* *Var[yₖ] = Var[yₖ ₋* ₁*].*

我们知道 *yₖ = xₖ Wₖ+ bₖ*。

为了简化，我们查看前激活层的*i-th* 元素 *yₖ* 并对前面的方程两边应用方差算子。

![](../Images/16cb584b879d9662cbbfbfc8a3ac57bc.png)

+   在第一步中，我们完全去除 *bₖ*，因为根据*假设1*，它初始化为零值。此外，我们利用 *W* 和 *x* 的独立性，将和的方差转换为方差之和，依据*Var[X+Y] = Var[X] + Var[Y]，其中 X* ⟂ *Y*。

+   在第二步中，由于 *W* 和 *x* 是 *i.i.d.*，因此和中的每一项都是相等的，因此和仅仅是 *Var[xW]* 的 *nₖ* 次重复。

+   在第三步中，我们遵循 *X* ⟂ *Y* 的公式，这意味着 *Var[XY] = E[X²]E[Y²] - E[X]²E[Y]²*。这使我们能够将 *W* 和 *x* 对前激活层方差的贡献分开。

![](../Images/dd865a2fc54dec78238b3aa7a6ccab66.png)

+   在第四步中，我们利用了权重和层在初始化时的零期望的*假设3*。这使我们剩下一个涉及平方期望的单一项。

+   在第五步中，我们将平方期望转换为方差，因为*Var[X] = E[( X - E[X])²] = E[X²]*，如果*X*的均值为零。现在我们可以将前激活层的方差表示为层方差和权重方差的单独乘积。

最后，为了将 *Var[yₖ] 与 Var[yₖ ₋* ₁*]* 关联起来，我们使用 *无意识统计学家定律* 将平方期望 *E[xₖ²]* 表达为 *Var[yₖ ₋* ₁*]*。

![](../Images/438ceff83f96e7fda4ee45850bfbeb9e.png)

定理指出，我们可以将随机变量函数的任何期望表示为其函数和概率密度 *p* 的积分。由于我们知道 *xₖ = max(0, yₖ ₋* ₁*)*，我们可以将 *xₖ* 的平方期望重写为在 *ℝ* 上关于 *y* 的积分。

![](../Images/ff34a215683d295eb7d4e630f9029d68.png)

+   在第六步中，我们利用 *y* 在 ℝ⁻ 上为零来简化积分。

+   在第七步中，我们利用 *y* 作为对称随机变量的统计特性，因此它具有对称的密度函数 *p*，并注意到整个积分项是偶函数。偶函数相对于 ℝ 上的 0 是对称的，这意味着从 0 到 *a* 的积分与从 *-a* 到 0 的积分是相同的。我们使用这个技巧将积分重新表述为在 ℝ 上的积分。

![](../Images/f0613a7a42c13e34c3e7a65b014cffbb.png)

+   在第九和第十步中，我们将这个积分重写为随机变量的函数的积分。通过应用 LOTUS——这次是从右到左——我们可以将此积分转换为关于随机变量 *y* 的函数的期望。作为零均值变量的平方期望，这本质上是方差。

![](../Images/8a86c6fac095d23f1346be8be0d7bfa2.png)

最终，我们将步骤五和步骤十的结果综合起来——前置激活层的方差与其前置激活方差以及该层权重的方差直接相关。由于我们要求 *Var[yₖ] = Var[yₖ ₋* ₁*]*，这使我们可以确认该层权重的方差 *Var[Wₖ]* 应该是 2/*nₖ*。

总结一下，这里是本节回顾的前向传播的完整推导：

![](../Images/b3525ec2d5fdd6d851a28475d664d725.png)![](../Images/d09f218de73be6e53ae2a95eafb5b2b2.png)

## II. 反向传播

我们正在寻找 *Wₖ*，使得 *Var[Δxₖ] = Var[Δxₖ ₊* ₁*]*。

在这里*， xₖ ₊* ₁*= f (yₖ)* 和 *yₖ = xₖ Wₖ + bₖ*。

在应用方差运算符之前，让我们首先计算损失 *L* 相对于 *x* 和 *y* 的偏导数：*Δxₖ* 和 *Δyₖ*。

![](../Images/9c17c3bc893544ffa5202e2901d6cead.png)

+   首先，我们使用链式法则和线性乘积的导数是其线性系数的事实——在这种情况下是 *Wₖ*。

+   其次，我们利用 *假设 2* 说明梯度和权重彼此独立。利用独立性，积的方差等于方差的积，因为权重假设为零均值初始化。因此，*L w.r.t. x* 的梯度期望为零。

+   第三，我们使用链式法则将 *Δyₖ* 和 *Δxₖ ₊* ₁ 关联起来，因为 *x* 相对于 *y* 的偏导数是 ReLU 对 *y* 的导数。

![](../Images/9b88943311d1a917e2bc3bc18529d10c.png)

+   第四，回顾ReLU的导数，我们使用之前的方程计算*Δyₖ*的期望。由于*f’(x)*被分为两个等概率的部分*½*，我们可以将其写作两个项的和：分别对ℝ⁺和ℝ⁻的期望。从之前的计算中，我们知道*Δxₖ*的期望为零，因此可以确认两个梯度的均值均为0。

![](../Images/197febab4ff1f2ccdf31069750903908.png)

+   第五，我们使用之前相同的规则，将平方期望写作方差，这里是*Δyₖ*。

+   第六，我们利用*假设2*指出梯度在初始化时是独立的，以此分离两个梯度*Δxₖ ₊* ₁和*f’(yₖ)*的方差。进一步简化源自*假设3*，我们最终可以计算ReLU的平方期望，因为它在正负区间之间是均匀分布的。

![](../Images/313e8009108a1814e9b693539fcf9548.png)

最后，利用上述各节中收集到的结果，并重新应用*iid*假设，我们得出反向传播的结果与正向传播类似，即给定*Var[Δxₖ] = Var[Δxₖ ₊* ₁*]*，任何一层的权重的方差*Var[Wₖ]*等于2/*nₖ*。

总结一下，这里是反向传播部分包含的重要逐步计算的提醒：

![](../Images/be426dc6afa5c2a15f929bc66627b32c.png)

# III. 权重分布

在前两节中，我们对正向和反向设置得出了以下结论：

![](../Images/0f745c22499c3eca5a1fafb02cc475cb.png)

有趣的是，这个结果与Glorot初始化方法不同，其中作者实际上必须对正向和反向传播中得到的*两个*不同结果进行平均。此外，我们观察到He方法中的方差是翻倍的，这直观上是因为ReLU的零负区域将方差减少了一半。

随后，了解了分布的方差后，我们可以使用正态分布*N(0, 𝜎²)*或均匀分布*U(-a, a)*来初始化权重。根据经验，没有证据表明一种分布优于另一种分布，似乎性能的提升完全归结为所选分布的对称性和尺度属性。此外，我们确实需要记住*假设3*，限制分布选择为对称且以0为中心。

+   **对于正态分布 *N(0, 𝜎²)***

如果*X ~ N(0, 𝜎²)*，则*Var[X] = 𝜎²*，因此权重矩阵的方差和标准差可以写作：

![](../Images/e1085f0b569de97721c8dd5bb2bc15f7.png)

因此我们可以得出结论，*Wₖ*遵循以下系数的正态分布：

![](../Images/2ba793a077bfd84a7661bbde1ee61e1b.png)

作为提醒，*nₖ*是层*k*的输入数量。

+   **对于均匀分布 *U(-a, a)***

如果*X ~ U(-a, a)*，则使用以下均匀分布随机变量方差的公式，我们可以找到界限 *a*：

![](../Images/2adb9197e433a65dc09424df786bfce6.png)

最后，我们可以得出结论，*Wₖ* 遵循具有系数的均匀分布：

![](../Images/a689c2c60b45301651c2fd13a4d62a25.png)

# 结论

本文提供了为什么*He 初始化方法*对于使用 ReLU 激活函数的神经网络是最优的的逐步推导，前提是前向和反向传播的方差保持恒定。

该证明的方法论还扩展到线性激活函数的更广泛家族，如 PReLU（在 (1) 中由 He *等人* 讨论）或 Leaky ReLU（允许在负区间中有微小的梯度流动）。对于这些 ReLU 激活函数的变体，也可以推导出类似的最优方差公式。

**引用**

(1)[*深入探讨激活函数：超越人类水平的 ImageNet 分类表现*](https://arxiv.org/abs/1502.01852)，He *等人* (2015)

(2)[*理解训练深度前馈神经网络的难度*](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)，Glorot *等人* (2010)

*来源：以上所有方程和图像均为我个人制作。*
