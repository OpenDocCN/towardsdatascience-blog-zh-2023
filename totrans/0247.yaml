- en: The ABCs of Differential Privacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/abcs-of-differential-privacy-8dc709a3a6b3](https://towardsdatascience.com/abcs-of-differential-privacy-8dc709a3a6b3)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: MASTERING BASICS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Guide to Understanding Fundamental Definitions and Key Principles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.ealizadeh.com/?source=post_page-----8dc709a3a6b3--------------------------------)[![Essi
    Alizadeh](../Images/be2244231732f93bcadf09682ef8ca37.png)](https://medium.ealizadeh.com/?source=post_page-----8dc709a3a6b3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8dc709a3a6b3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8dc709a3a6b3--------------------------------)
    [Essi Alizadeh](https://medium.ealizadeh.com/?source=post_page-----8dc709a3a6b3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8dc709a3a6b3--------------------------------)
    ¬∑8 min read¬∑Apr 27, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a9c9c860a5cbeead7b567f365584a0b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy (DP) is a rigorous mathematical framework that permits
    the analysis and manipulation of sensitive data while providing robust privacy
    guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: DP is based on the premise that the inclusion or exclusion of a single individual
    should not significantly change the results of any analysis or query carried out
    on the dataset as a whole. In other words, the algorithm should come up with comparable
    findings when comparing these two sets of data, making it difficult to figure
    out anything distinctive about that individual. This safety keeps private information
    from getting out but still lets useful insights be drawn from the data.
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy initially appeared in the study ‚ÄúDifferential Privacy‚Äù
    by Cynthia Dwork [1] while she was working at Microsoft Research.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs take a look at an example to better understand how differential privacy
    helps to protect data.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of How Differential Privacy Safeguards Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Example 1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a study that looks at the link between social class and health results, researchers
    ask subjects for private information like where they live, how much money they
    have, and their medical background [2].
  prefs: []
  type: TYPE_NORMAL
- en: John, one of the participants, is worried that his personal information could
    get out and hurt his applications for life insurance or a mortgage. To make sure
    that John‚Äôs worries are taken care of, the researchers can use differential privacy.
    This makes sure that any data that is shared won‚Äôt reveal specific information
    about him. Different levels of privacy can be shown by John‚Äôs ‚Äúopt-out‚Äù situation,
    in which his data is left out of the study. This protects his anonymity because
    the analysis's results are not tied to any of his personal details.
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy seeks to protect privacy in the real world as if the data
    were being looked at in an opt-out situation. Since John‚Äôs data is not part of
    the computation, the results regarding him can only be as accurate as the data
    available to everyone else.
  prefs: []
  type: TYPE_NORMAL
- en: A precise description of differential privacy requires formal mathematical language
    and technical concepts, but the basic concept is to protect the privacy of individuals
    by limiting the information that can be obtained about them from the released
    data, thereby ensuring that their sensitive information remains private.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The U.S. Census Bureau used a differential privacy framework as a part of its
    disclosure avoidance strategy to strike a compromise between the data collection
    and reporting needs and the privacy concerns of the respondents. You can find
    more information about the confidentiality protection provided by the U.S. Census
    Bureau [here](https://www.census.gov/library/working-papers/2022/adrm/CED-WP-2022-003.html).
    Moreover, Garfinkel provides an explanation of how DP was utilized in the 2020
    US Census data [here](https://mit-serc.pubpub.org/pub/differential-privacy-2020-us-census).
  prefs: []
  type: TYPE_NORMAL
- en: Definition and key concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The meaning of ‚Äúdifferential‚Äù within the realm of DP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The term ‚Äúdifferential‚Äù privacy refers to its emphasis on the dissimilarity
    between the results produced by a privacy-preserving algorithm on two datasets
    that differ by just one individual‚Äôs data.
  prefs: []
  type: TYPE_NORMAL
- en: Mechanism M
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A *mechanism* *M* is a mathematical method or process that is used on the data
    to make sure privacy is maintained while still giving useful information.
  prefs: []
  type: TYPE_NORMAL
- en: Epsilon (Œµ)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Œµ is a privacy parameter that controls the level of privacy given by a differentially
    private mechanism. In other words, Œµ regulates how much the output of the mechanism
    can vary between two neighboring databases and measures how much privacy is lost
    when the mechanism is run on the database [3].
  prefs: []
  type: TYPE_NORMAL
- en: Stronger privacy guarantees are provided by a smaller Œµ, but the output may
    be less useful as a result [4]. Œµcontrols the amount of noise added to the data
    and shows how much the output probability distribution can change when the data
    of a single person is altered.
  prefs: []
  type: TYPE_NORMAL
- en: Delta (ùõø)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ùõø is an extra privacy option that lets you set how likely it is that your privacy
    will be compromised. Hence, ùõø controls the probability of an extreme privacy breach,
    where the added noise (controlled by Œµ) does not provide sufficient protection.
  prefs: []
  type: TYPE_NORMAL
- en: ùõø is a non-negative number that measures the chance of a data breach. It is
    usually very small and close to zero. This change makes it easier to do more complicated
    studies and machine learning models while still protecting privacy (see [4]).
  prefs: []
  type: TYPE_NORMAL
- en: If ùõø is low, there is less of a chance that someone‚Äôs privacy is going to get
    compromised. But this comes at a cost. If ùõø is too small, more noise might be
    introduced into the data, diminishing the quality of the end-result. ùõø is one
    parameter to consider, but it must be balanced with epsilon and the data‚Äôs practicality.
  prefs: []
  type: TYPE_NORMAL
- en: Unveiling the Mathematics Behind Differential Privacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consider two databases, D and D', that differ by only one record.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, a mechanism M is Œµ-differentially private if, for any two adjacent
    datasets D and D‚Äô, and for any possible output O, the following holds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2180335699dc30ba27c1a4213a95dfce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, we can reframe the above equation in terms of divergences, resulting
    in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5494a4e9e80abb08a64a578400c1e365.png)![](../Images/7c92b5ab70004d237615fc4579c6ba6c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Differential privacy in the context of divergences (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: Here **div[‚ãÖ‚à£‚à£‚ãÖ]** denotes the R√©nyi divergence. See the paper [Renyi Differential
    Privacy](https://arxiv.org/abs/1702.07476) by Ilya Mironov for more information.
  prefs: []
  type: TYPE_NORMAL
- en: (Œµ, ùõø)-DP Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A randomized *M* is considered (Œµ, ùõø)-differentially private if the probability
    of a significant privacy breach (i.e., a breach that would not occur under Œµ-differential
    privacy) is no more than ùõø. More formally, a mechanism M is (Œµ, ùõø)-differentially
    private if
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ecfb021e6c59b32d894209bac8c92e90.png)'
  prefs: []
  type: TYPE_IMG
- en: If ùõø = 0, then (Œµ, ùõø)-DP is reduced to a Œµ-DP.
  prefs: []
  type: TYPE_NORMAL
- en: (Œµ, ùõø)-DP mechanism may be thought of informally as Œµ-DP with a probability
    of 1 ‚Äî ùõø.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Properties of Differential Privacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 1\. Post-processing immunity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The differentially private output can be subjected to any function or analysis,
    and the outcome will continue to uphold the original privacy assurances. For instance,
    if you apply a differentially private mechanism to a dataset and then take the
    average age of the individuals in the dataset, the resulting average age will
    still be differentially private and will provide the same level of privacy assurances
    as the output it was originally designed to provide.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to the post-processing feature, we can use differentially private mechanisms
    in the same way as generic ones. Hence, it is possible to combine several differentially
    private mechanisms without sacrificing the integrity of differential privacy.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Composition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When multiple differentially private techniques are used on the same data or
    when queries are combined, composition is the property that ensures the privacy
    guarantees of differential privacy still apply. Composition can be either sequential
    or parallel. If you apply two mechanisms, *M1* with Œµ1-DP and *M2* with Œµ2-DP
    on a dataset, then the composition of *M1* and *M2* is at least (Œµ1 + Œµ2)-DP.
  prefs: []
  type: TYPE_NORMAL
- en: 'WARNING: Despite composition‚Äôs ability to protect privacy, the composition
    theorem makes clear that there is a ceiling; as the value of Œµ rises, so does
    the amount of privacy lost whenever a new mechanism is employed. If Œµ becomes
    too large, then differential privacy guarantees are mostly meaningless [3].'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '3\. Robustness to auxiliary information:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Differential privacy is resistant to auxiliary information attackers, which
    means that even if an attacker has access to other relevant data, they will not
    be able to learn anything about a person from a DP output. For instance, if a
    hospital were to share differentially private information regarding individuals‚Äô
    medical situations, an attacker with access to other medical records would not
    be able to greatly increase their knowledge of a given patient from the published
    numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Common Misunderstandings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The notion of differential privacy has been misunderstood in several publications,
    especially during its early days. Dwork *et al.* wrote a short paper [5] to correct
    some widespread misunderstandings. Here are a few examples of common misunderstandings:'
  prefs: []
  type: TYPE_NORMAL
- en: DP is not an algorithm but rather a definition. DP is a mathematical guarantee
    that an algorithm must meet in order to disclose statistics about a dataset. Several
    distinct algorithms meet the criteria.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Various algorithms can be differentialy private while still meeting various
    requirements. If someone claims that differential privacy, a specific requirement
    on ratios of probability distributions, is incompatible with any accuracy target,
    they must provide evidence for that claim. This means proving that there is no
    way a DP algorithm can perform to some specified standard. It‚Äôs challenging to
    come up with that proof, and our first guesses about what is and isn‚Äôt feasible
    are often off.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are no ‚Äúgood‚Äù or ‚Äúbad‚Äù results for any given database. Generating the
    outputs in a way that preserves privacy (perfect or differential) is the key.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DP has shown itself as a viable paradigm for the protection of data privacy,
    which is particularly important in this day and age, when machine learning and
    big data are becoming more widespread. Several key concepts were covered in this
    essay, including the various DP control settings like Œµ and *Œ¥*. In addition,
    we provided several mathematical definitions of the DP. We also explained key
    features of the DP and addressed some of the most common misconceptions.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Dwork, Cynthia (2006). ‚ÄúDifferential Privacy.‚Äù In *Proceedings of the 33rd
    International Colloquium on Automata, Languages and Programming*, 1‚Äì12\. Berlin,
    Heidelberg: Springer Berlin Heidelberg. [https://doi.org/10.1007/11787006_1](https://doi.org/10.1007/11787006_1).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Wood, Alexandra, Micah Altman, Aaron Bembenek, Mark Bun, Marco Gaboardi,
    James Honaker, Kobbi Nissim, David O‚ÄôBrien, Thomas Steinke, and Salil Vadhan (2018).
    ‚ÄúDifferential Privacy: A Primer for a Non-Technical Audience.‚Äù *Vand. J. Ent.
    & Tech. L.* 21 (1): 209‚Äì76\. [https://doi.org/10.2139/ssrn.3338027](https://doi.org/10.2139/ssrn.3338027).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Brubaker, M., and S. Prince (2021). ‚ÄúTutorial #12: Differential Privacy
    I: Introduction.‚Äù *Borealis AI*. [https://www.borealisai.com/research-blogs/tutorial-12-differential-privacy-i-introduction/](https://www.borealisai.com/research-blogs/tutorial-12-differential-privacy-i-introduction/).'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Dwork, Cynthia, Aaron Roth, et al. (2014). ‚ÄúThe Algorithmic Foundations
    of Differential Privacy.‚Äù *Foundations and Trends in Theoretical Computer Science*
    9 (3‚Äì4): 211‚Äì407.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Dwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2011\. ‚ÄúDifferential
    Privacy ‚Äî A Primer for the Perplexed.‚Äù *Joint UNECE/Eurostat Work Session on Statistical
    Data Confidentiality* 11.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Originally published at* [*https://ealizadeh.com*](https://ealizadeh.com/blog/abc-of-differential-privacy/)
    *on April 27, 2023.*'
  prefs: []
  type: TYPE_NORMAL
