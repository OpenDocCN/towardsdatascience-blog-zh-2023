- en: Google Generative AI Transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/google-generative-ai-transformations-edb4164935cb](https://towardsdatascience.com/google-generative-ai-transformations-edb4164935cb)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ETL is about to be transformed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://franklyai.medium.com/?source=post_page-----edb4164935cb--------------------------------)[![Frank
    Neugebauer](../Images/0da70d082d0f9c7ad8ccf574ed215df2.png)](https://franklyai.medium.com/?source=post_page-----edb4164935cb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----edb4164935cb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----edb4164935cb--------------------------------)
    [Frank Neugebauer](https://franklyai.medium.com/?source=post_page-----edb4164935cb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----edb4164935cb--------------------------------)
    ·8 min read·Jun 12, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc5dd2d38e77ff28d92f5c611af6d217.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Suzanne D. Williams](https://unsplash.com/fr/@scw1217?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: What’s This About?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large language models (LLMs) can extract information and generate information,
    but they can also **transform** it, making extract, transform, and load (ETL)
    a potentially different effort entirely. I’ll provide an example that illustrates
    these ideas, which should also show how LLMs can, and should, be used for many
    related tasks including transforming unstructured text to structured text.
  prefs: []
  type: TYPE_NORMAL
- en: Google recently made its large language model (LLM) suite of offerings publicly
    available in preview and have branded a part of the offering “Generative AI Studio.”
    In short, GenAI Studio within the Google Cloud Platform Console is a UI to Google’s
    LLMs. However, unlike Google Bard (which is a commercial application using an
    LLM), no data is kept by Google for any reason. Note that Google also released
    an API for many of the capabilities outlined here.
  prefs: []
  type: TYPE_NORMAL
- en: Using GenAI Studio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting into GenAI Studio is pretty straightforward — from the GCP Console,
    simply use the navigation bar on the left, hover over Vertex AI, and select **Overview**
    under **GENERATIVE AI STUDIO.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75986da065190c636c214115e263a5b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: As of late May 2023, there are two options — Language and Speech. (Before long,
    Google is also expected to release a Vision category here.) Each option contains
    some sample prompt styles, which can help you spawn ideas and focus your existing
    ideas into useful prompts. But more than that, this is a “safe” Bard-like experience
    in that your data is not kept by Google.
  prefs: []
  type: TYPE_NORMAL
- en: Language
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The landing page for **Language**, which is the only feature used for this example,
    has several different capabilities, while also containing an easy way to tune
    the foundation model (currently, tuning can only be done in certain regions).
  prefs: []
  type: TYPE_NORMAL
- en: Create Prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/f8ff280de1ff6ad0fda6e76833722abb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The **Get started** area is where un-guided interactions with Google’s models
    (one or more depending on the timing and interaction type) are quickly created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Selecting **TEXT PROMPT** invokes a Bard-like UI with some important differences
    (in addition to data privacy):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bfd382acbc8e24a8a7ecd3edb5310a84.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: The underlying LLM can be changed. Currently, the text-bison001 model is the
    only one available but others will appear over time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model parameters can be changed. Google provides explanations for each parameter
    using the question marks next to each.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The filter for blocking unsafe responses can be adjusted (options include “Block
    few”, “Block some”, and “Block most”.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inappropriate responses can be easily reported.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aside from the obvious differences with Bard, using the models this way also
    lacks some of the Bard “add-ons,” such as current events. For example, if a prompt
    asking about yesterday’s weather in Chicago is entered, this model will not give
    the correct answer, but Bard will.
  prefs: []
  type: TYPE_NORMAL
- en: The large text section is where a prompt is entered.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b48bfc42abde01430962705f91c98731.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: A prompt is created by entering the text within the **Prompt** section, (optionally)
    adjusting parameters, and then selecting the **SUBMIT** button. In this example,
    the prompt is “What is 1+1?” using the text-bison001 model and default parameter
    values. Notice the model simply returns the number 2, which is a good example
    of the effect Temperature has on replies. Repeating this prompt (by selecting
    **SUBMIT** repeatedly) yields “2” **most of the time**, but randomly a different
    reply is given. Changing the Temperature to 1.0 yields, “The answer is 2\. 1+1=2
    is one of the most basic mathematical equations that everyone learns in elementary
    school. It is the foundation for all other math that is learned later on.” This
    happens because Temperature adjusts the probabilistic selection for tokens, the
    lower the value the less variable (i.e., more deterministic) the replies are.
    If the value is set to 0 in this example, the model will always return “2.” Pretty
    cool, and very Bard-like but better. You can also save prompts and view code for
    the prompt. The following is the code for “What is 1+1?”
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The generated code contains the prompt, but it’s easy to see that the function,
    `predict_large_language_model_sample` is general-purpose and can be used for any
    text prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Get to the ETL Already!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In my day job, I spend lots of time figuring out how to extract information
    from text (including documents). LLMs can do this in surprisingly easy and accurate
    ways, and in doing so can also change the data. An example illustrates this potential.
  prefs: []
  type: TYPE_NORMAL
- en: 'Presume for the sake of this example, that the following email message is received
    by a fictitious ACME Incorporated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Also presume that the objectives for the system are to extract specific data
    from the email, apply prices (and subtotals) for each item entered, and also generate
    a grand total.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re thinking an LLM can’t do all that, think again!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'There’s a prompt style called **extractive Q&A** that fits the bill very nicely
    in some situations (maybe all situations if applied by tuning the model versus
    simply prompt engineering). The idea is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: Provide a **Background**, which is the original text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide a **Q** (for Question), which should be something extractive, such as
    “Extract all the information as JSON.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optionally provide an **A** (for Answer) that has the desired output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If no **A** is provided, then zero shot engineering is applied (and this works
    better than I expected). You can provide one-shot or multi-shot as well, up to
    a point. There’s a limit to the size of a prompt, which restricts how many samples
    you can provide.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, an extractive Q&A prompt has the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the example, the email is *the text*, and “Extract all information as JSON”
    is *the extractive question*. If nothing is provided as **A:** the LLM will attempt
    to do the extraction (zero shot). (JSON stands for JavaScript Object Notation.
    It is a lightweight data-interchange format.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the zero shot output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You don’t need to bold *Background:*, *Q:*, and *A:*, I just did so for clarity.
  prefs: []
  type: TYPE_NORMAL
- en: In the UI, I left the prompt as **FREEFORM** and I entered the prompt above
    in the Prompt area. Then, I set the *Temperature* to 0 (I want the same answer
    for the same input every time) and increased the *Token limit* to 512 to allow
    for a longer response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what the zero shot prompt and reply looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fed968781d22e04adfc0a407e84790e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The “E”xtract works and even does a nice job of putting the line items in a
    list within the JSON. But that’s really good enough. Assume my requirements are
    to have specific labels for the data, and also presume I want to capture the purchasing
    agent and their phone. Finally, assume I want line item subtotals and a grand
    total (this presumption requires that a line item price exists).
  prefs: []
  type: TYPE_NORMAL
- en: 'My ideal output, which is both an “E”xtract and “T”ransform, looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: For this prompt, I change the UI from **FREEFORM** to **STRUCTURED**, which
    makes laying out the data a bit easier. With this UI, I can set a **Context**
    for the LLM (which can have a surprising effect on model responses). Then, I provide
    one **Example**— both the input text and the output text — and then a **Test**
    input.
  prefs: []
  type: TYPE_NORMAL
- en: The parameters are the same for STRUCTURED and FREEFORM. Here is the *Context*,
    and *Example* (both *Input* and *Output*) for the invoice ETL example.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4de6f68ab9e03a592b4bec718f6ba130.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: I added a Test email, with entirely different data (same widgets though). Here’s
    everything, shown in the UI. I then selected **SUBMIT,** which filled in the Test
    JSON, which is in the bottom right pane in the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1edc292d0aa056c0cfc4b5c745029461.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: '***That right there is voodoo magic.*** Yes, the math is completely correct.'
  prefs: []
  type: TYPE_NORMAL
- en: What About the “L”?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, I’ve shown extract and transform — it’s time for the load bit.
    That part is actually very simple, with zero-shot (if this is done with the API,
    it’s two calls — one for E+T, one for L.
  prefs: []
  type: TYPE_NORMAL
- en: I provided the JSON from the last step as the **Background** and changed the
    **Q:** to “Convert the JSON to a SQL insert statement.” Here’s the result, which
    deduces an *invoices* table and an *invoice_items* table. (You can fine-tune that
    SQL either with the question and/or an example SQL.)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c29faaf95a6d2b3399e694dc814a7b3e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: What This Means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This example demonstrates a pretty amazing LLM capability, which may very well
    change the nature of ETL work. I have no doubt there are limits to what LLMs can
    do in this space, but I don’t know what those limits are yet. Working with the
    model on your problems is critical in understanding what can, cannot, and *should*
    be done with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: The future looks bright, and GenAI Studio can get you going very quickly. Remember,
    the UI gives you some simple copy/paste code so you can use the API rather than
    the UI, which is required for actual applications doing this type of work.
  prefs: []
  type: TYPE_NORMAL
- en: This also means that the hammer still doesn’t make houses. By this I mean that
    the model didn’t figure out this ETL example. The LLM is the very elaborate “hammer”
    — I was the carpenter, just like you.
  prefs: []
  type: TYPE_NORMAL
- en: Disclaimer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article is the author’s opinion and perspective and does not reflect those
    of his employer. (Just in case Google is watching.)
  prefs: []
  type: TYPE_NORMAL
