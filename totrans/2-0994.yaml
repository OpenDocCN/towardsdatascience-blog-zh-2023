- en: Google Generative AI Transformations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谷歌生成式 AI 转型
- en: 原文：[https://towardsdatascience.com/google-generative-ai-transformations-edb4164935cb](https://towardsdatascience.com/google-generative-ai-transformations-edb4164935cb)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/google-generative-ai-transformations-edb4164935cb](https://towardsdatascience.com/google-generative-ai-transformations-edb4164935cb)
- en: ETL is about to be transformed
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ETL 即将被改造
- en: '[](https://franklyai.medium.com/?source=post_page-----edb4164935cb--------------------------------)[![Frank
    Neugebauer](../Images/0da70d082d0f9c7ad8ccf574ed215df2.png)](https://franklyai.medium.com/?source=post_page-----edb4164935cb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----edb4164935cb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----edb4164935cb--------------------------------)
    [Frank Neugebauer](https://franklyai.medium.com/?source=post_page-----edb4164935cb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://franklyai.medium.com/?source=post_page-----edb4164935cb--------------------------------)[![Frank
    Neugebauer](../Images/0da70d082d0f9c7ad8ccf574ed215df2.png)](https://franklyai.medium.com/?source=post_page-----edb4164935cb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----edb4164935cb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----edb4164935cb--------------------------------)
    [Frank Neugebauer](https://franklyai.medium.com/?source=post_page-----edb4164935cb--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----edb4164935cb--------------------------------)
    ·8 min read·Jun 12, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----edb4164935cb--------------------------------)
    ·阅读时间 8 分钟·2023 年 6 月 12 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/dc5dd2d38e77ff28d92f5c611af6d217.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc5dd2d38e77ff28d92f5c611af6d217.png)'
- en: Photo by [Suzanne D. Williams](https://unsplash.com/fr/@scw1217?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Suzanne D. Williams](https://unsplash.com/fr/@scw1217?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: What’s This About?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这是什么？
- en: Large language models (LLMs) can extract information and generate information,
    but they can also **transform** it, making extract, transform, and load (ETL)
    a potentially different effort entirely. I’ll provide an example that illustrates
    these ideas, which should also show how LLMs can, and should, be used for many
    related tasks including transforming unstructured text to structured text.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）可以提取信息并生成信息，但它们也可以**转换**信息，这使得提取、转换和加载（ETL）可能完全是一项不同的工作。我将提供一个例子来说明这些想法，这也应当展示
    LLM 如何以及应该用于包括将非结构化文本转换为结构化文本在内的许多相关任务。
- en: Google recently made its large language model (LLM) suite of offerings publicly
    available in preview and have branded a part of the offering “Generative AI Studio.”
    In short, GenAI Studio within the Google Cloud Platform Console is a UI to Google’s
    LLMs. However, unlike Google Bard (which is a commercial application using an
    LLM), no data is kept by Google for any reason. Note that Google also released
    an API for many of the capabilities outlined here.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌最近公开了其大型语言模型（LLM）系列产品，并将其中一部分命名为“生成式 AI 工作室”。简而言之，Google Cloud Platform 控制台中的
    GenAI Studio 是谷歌 LLM 的用户界面。然而，与谷歌 Bard（一个使用 LLM 的商业应用）不同，谷歌不会出于任何原因保存你的数据。请注意，谷歌还发布了一个涵盖许多此处所述功能的
    API。
- en: Using GenAI Studio
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GenAI Studio
- en: Getting into GenAI Studio is pretty straightforward — from the GCP Console,
    simply use the navigation bar on the left, hover over Vertex AI, and select **Overview**
    under **GENERATIVE AI STUDIO.**
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 进入 GenAI Studio 非常简单——在 GCP 控制台中，只需使用左侧的导航栏，悬停在 Vertex AI 上，然后选择 **概览** 下的 **生成式
    AI 工作室**。
- en: '![](../Images/75986da065190c636c214115e263a5b7.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75986da065190c636c214115e263a5b7.png)'
- en: Image by Author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: As of late May 2023, there are two options — Language and Speech. (Before long,
    Google is also expected to release a Vision category here.) Each option contains
    some sample prompt styles, which can help you spawn ideas and focus your existing
    ideas into useful prompts. But more than that, this is a “safe” Bard-like experience
    in that your data is not kept by Google.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 截至 2023 年 5 月底，有两个选项——语言和语音。（不久后，谷歌预计还会在此处发布视觉类别。）每个选项包含一些示例提示样式，这些样式可以帮助你激发想法并将现有的想法集中为有用的提示。但更重要的是，这是一个“安全”的类似
    Bard 的体验，因为你的数据不会被谷歌保存。
- en: Language
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言
- en: The landing page for **Language**, which is the only feature used for this example,
    has several different capabilities, while also containing an easy way to tune
    the foundation model (currently, tuning can only be done in certain regions).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**语言**的登录页面是这个例子中唯一使用的功能，它有多个不同的功能，同时也包含了一个简单的方法来调整基础模型（目前调整只能在特定区域进行）。'
- en: Create Prompt
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建提示
- en: '![](../Images/f8ff280de1ff6ad0fda6e76833722abb.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8ff280de1ff6ad0fda6e76833722abb.png)'
- en: Image by Author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: The **Get started** area is where un-guided interactions with Google’s models
    (one or more depending on the timing and interaction type) are quickly created.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**开始使用**区域是快速创建与 Google 模型（取决于时间和交互类型，可能是一个或多个）的无指导交互的地方。'
- en: 'Selecting **TEXT PROMPT** invokes a Bard-like UI with some important differences
    (in addition to data privacy):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 选择**文本提示**会调用一个类似 Bard 的用户界面，但有一些重要的区别（除了数据隐私之外）：
- en: '![](../Images/bfd382acbc8e24a8a7ecd3edb5310a84.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfd382acbc8e24a8a7ecd3edb5310a84.png)'
- en: Image by Author.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者。
- en: The underlying LLM can be changed. Currently, the text-bison001 model is the
    only one available but others will appear over time.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 底层的 LLM 可以更改。目前，text-bison001 模型是唯一可用的，但其他模型会随着时间出现。
- en: Model parameters can be changed. Google provides explanations for each parameter
    using the question marks next to each.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型参数可以调整。Google 提供了每个参数的解释，旁边有问号。
- en: The filter for blocking unsafe responses can be adjusted (options include “Block
    few”, “Block some”, and “Block most”.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于阻止不安全回复的过滤器可以调整（选项包括“阻止少数”、“阻止一些”和“阻止大多数”）。
- en: Inappropriate responses can be easily reported.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不适当的回复可以轻松报告。
- en: Aside from the obvious differences with Bard, using the models this way also
    lacks some of the Bard “add-ons,” such as current events. For example, if a prompt
    asking about yesterday’s weather in Chicago is entered, this model will not give
    the correct answer, but Bard will.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 除了与 Bard 的明显区别外，这种方式使用模型还缺乏一些 Bard 的“附加功能”，如时事。例如，如果输入一个询问昨天芝加哥天气的提示，这个模型不会给出正确答案，但
    Bard 会。
- en: The large text section is where a prompt is entered.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 大文本区域是输入提示的地方。
- en: '![](../Images/b48bfc42abde01430962705f91c98731.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b48bfc42abde01430962705f91c98731.png)'
- en: Image by Author
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: A prompt is created by entering the text within the **Prompt** section, (optionally)
    adjusting parameters, and then selecting the **SUBMIT** button. In this example,
    the prompt is “What is 1+1?” using the text-bison001 model and default parameter
    values. Notice the model simply returns the number 2, which is a good example
    of the effect Temperature has on replies. Repeating this prompt (by selecting
    **SUBMIT** repeatedly) yields “2” **most of the time**, but randomly a different
    reply is given. Changing the Temperature to 1.0 yields, “The answer is 2\. 1+1=2
    is one of the most basic mathematical equations that everyone learns in elementary
    school. It is the foundation for all other math that is learned later on.” This
    happens because Temperature adjusts the probabilistic selection for tokens, the
    lower the value the less variable (i.e., more deterministic) the replies are.
    If the value is set to 0 in this example, the model will always return “2.” Pretty
    cool, and very Bard-like but better. You can also save prompts and view code for
    the prompt. The following is the code for “What is 1+1?”
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在**提示**部分输入文本、（可选地）调整参数，然后选择**提交**按钮来创建提示。在这个例子中，提示是“1+1 等于多少？”使用的是 text-bison001
    模型和默认参数值。注意，模型仅返回数字 2，这是温度对回复影响的一个好例子。重复这个提示（通过重复选择**提交**）**大多数时候**会得到“2”，但有时会随机给出不同的回复。将温度调整为
    1.0 会得到：“答案是 2。1+1=2 是每个人在小学时学习的最基本的数学方程之一。它是以后学习的所有其他数学的基础。”这是因为温度调整了对标记的概率选择，值越低，回复的变化越少（即越确定）。如果在这个例子中将值设置为
    0，模型将始终返回“2”。这很酷，非常类似 Bard，但更好。你还可以保存提示并查看提示的代码。以下是“1+1 等于多少？”的代码。
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The generated code contains the prompt, but it’s easy to see that the function,
    `predict_large_language_model_sample` is general-purpose and can be used for any
    text prompt.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的代码包含提示信息，但很明显，函数`predict_large_language_model_sample`是通用的，可用于任何文本提示。
- en: Get to the ETL Already!
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 赶紧进入 ETL 吧！
- en: In my day job, I spend lots of time figuring out how to extract information
    from text (including documents). LLMs can do this in surprisingly easy and accurate
    ways, and in doing so can also change the data. An example illustrates this potential.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的日常工作中，我花费大量时间来确定如何从文本（包括文档）中提取信息。LLM可以以令人惊讶的简单和准确的方式做到这一点，并且在这样做时还可以更改数据。一个例子说明了这种潜力。
- en: 'Presume for the sake of this example, that the following email message is received
    by a fictitious ACME Incorporated:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 假设为了这个例子，以下电子邮件消息由一个虚构的ACME Incorporated接收：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Also presume that the objectives for the system are to extract specific data
    from the email, apply prices (and subtotals) for each item entered, and also generate
    a grand total.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 还假设系统的目标是从电子邮件中提取特定数据，为每个输入项应用价格（和小计），并生成总计。
- en: If you’re thinking an LLM can’t do all that, think again!
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你认为LLM无法做到这些，重新考虑一下！
- en: 'There’s a prompt style called **extractive Q&A** that fits the bill very nicely
    in some situations (maybe all situations if applied by tuning the model versus
    simply prompt engineering). The idea is simple:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种提示风格叫做**extractive Q&A**，在某些情况下（如果通过调整模型而不是简单的提示工程应用，它可能在所有情况下都适用）非常合适。其思路很简单：
- en: Provide a **Background**, which is the original text.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供一个**Background**，即原始文本。
- en: Provide a **Q** (for Question), which should be something extractive, such as
    “Extract all the information as JSON.”
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供一个**Q**（即问题），应该是一些提取性的，例如“将所有信息提取为JSON。”
- en: Optionally provide an **A** (for Answer) that has the desired output.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可选择性地提供一个**A**（即答案），以获得所需的输出。
- en: If no **A** is provided, then zero shot engineering is applied (and this works
    better than I expected). You can provide one-shot or multi-shot as well, up to
    a point. There’s a limit to the size of a prompt, which restricts how many samples
    you can provide.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有提供**A**，则应用零样本工程（这效果比我预期的要好）。你也可以提供一次性或多次样本，但有一定限制。提示的大小有限制，这限制了你可以提供的样本数量。
- en: 'In summary, an extractive Q&A prompt has the following form:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，提取性Q&A提示的形式如下：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the example, the email is *the text*, and “Extract all information as JSON”
    is *the extractive question*. If nothing is provided as **A:** the LLM will attempt
    to do the extraction (zero shot). (JSON stands for JavaScript Object Notation.
    It is a lightweight data-interchange format.)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，电子邮件是*文本*，而“将所有信息提取为JSON”是*提取性问题*。如果没有提供**A:**，LLM将尝试进行提取（零样本）。(JSON代表JavaScript对象表示法。它是一种轻量级的数据交换格式。)
- en: 'Here is the zero shot output:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是零样本输出：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You don’t need to bold *Background:*, *Q:*, and *A:*, I just did so for clarity.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要加粗*Background:*, *Q:*, 和 *A:*，我这样做只是为了清晰。
- en: In the UI, I left the prompt as **FREEFORM** and I entered the prompt above
    in the Prompt area. Then, I set the *Temperature* to 0 (I want the same answer
    for the same input every time) and increased the *Token limit* to 512 to allow
    for a longer response.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在UI中，我将提示保持为**FREEFORM**，并在提示区域中输入了上述提示。然后，我将*Temperature*设置为0（我希望每次相同的输入得到相同的回答），并将*Token
    limit*增加到512，以允许更长的响应。
- en: 'Here is what the zero shot prompt and reply looks like:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是零样本提示和回复的样子：
- en: '![](../Images/fed968781d22e04adfc0a407e84790e2.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fed968781d22e04adfc0a407e84790e2.png)'
- en: Image by Author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: The “E”xtract works and even does a nice job of putting the line items in a
    list within the JSON. But that’s really good enough. Assume my requirements are
    to have specific labels for the data, and also presume I want to capture the purchasing
    agent and their phone. Finally, assume I want line item subtotals and a grand
    total (this presumption requires that a line item price exists).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: “E”xtract有效，甚至能够很好地将条目列表放入JSON中。但这已经足够好了。假设我的要求是为数据提供特定的标签，并且假设我还希望捕捉采购代理及其电话。最后，假设我希望有行项目小计和总计（这一假设要求存在行项目价格）。
- en: 'My ideal output, which is both an “E”xtract and “T”ransform, looks like this:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我理想的输出，即“E”xtract和“T”ransform，如下所示：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: For this prompt, I change the UI from **FREEFORM** to **STRUCTURED**, which
    makes laying out the data a bit easier. With this UI, I can set a **Context**
    for the LLM (which can have a surprising effect on model responses). Then, I provide
    one **Example**— both the input text and the output text — and then a **Test**
    input.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个提示，我将UI从**FREEFORM**更改为**STRUCTURED**，这使得数据布局稍微容易一些。使用这个UI，我可以为LLM设置一个**Context**（这可能对模型响应产生惊人的效果）。然后，我提供一个**Example**——包括输入文本和输出文本——以及一个**Test**输入。
- en: The parameters are the same for STRUCTURED and FREEFORM. Here is the *Context*,
    and *Example* (both *Input* and *Output*) for the invoice ETL example.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 参数对于 STRUCTURED 和 FREEFORM 是相同的。这里是*上下文*和*示例*（包括*输入*和*输出*）用于发票 ETL 示例。
- en: '![](../Images/4de6f68ab9e03a592b4bec718f6ba130.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4de6f68ab9e03a592b4bec718f6ba130.png)'
- en: Image by Author
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: I added a Test email, with entirely different data (same widgets though). Here’s
    everything, shown in the UI. I then selected **SUBMIT,** which filled in the Test
    JSON, which is in the bottom right pane in the image.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我添加了一个测试电子邮件，数据完全不同（尽管小部件相同）。以下是 UI 中显示的所有内容。我然后选择了**提交，**这填充了测试 JSON，在图片的右下角窗格中。
- en: '![](../Images/1edc292d0aa056c0cfc4b5c745029461.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1edc292d0aa056c0cfc4b5c745029461.png)'
- en: Image by Author.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: '***That right there is voodoo magic.*** Yes, the math is completely correct.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '***那就是巫术魔法。*** 是的，数学完全正确。'
- en: What About the “L”?
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那么 “L” 呢？
- en: At this point, I’ve shown extract and transform — it’s time for the load bit.
    That part is actually very simple, with zero-shot (if this is done with the API,
    it’s two calls — one for E+T, one for L.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我已经展示了提取和转换——现在是加载部分。那部分其实很简单，对于零-shot（如果通过 API 完成，则需要两个调用——一个用于 E+T，一个用于
    L）。
- en: I provided the JSON from the last step as the **Background** and changed the
    **Q:** to “Convert the JSON to a SQL insert statement.” Here’s the result, which
    deduces an *invoices* table and an *invoice_items* table. (You can fine-tune that
    SQL either with the question and/or an example SQL.)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我将上一步的 JSON 作为**背景**，并将**Q:** 更改为“将 JSON 转换为 SQL 插入语句。”以下是结果，它推断出一个*invoices*
    表和一个*invoice_items* 表。（你可以通过提问和/或示例 SQL 来微调 SQL。）
- en: '![](../Images/c29faaf95a6d2b3399e694dc814a7b3e.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c29faaf95a6d2b3399e694dc814a7b3e.png)'
- en: Image by Author
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: What This Means
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这意味着什么
- en: This example demonstrates a pretty amazing LLM capability, which may very well
    change the nature of ETL work. I have no doubt there are limits to what LLMs can
    do in this space, but I don’t know what those limits are yet. Working with the
    model on your problems is critical in understanding what can, cannot, and *should*
    be done with LLMs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例展示了一个相当惊人的 LLM 能力，这可能会改变 ETL 工作的性质。我毫不怀疑 LLM 在这个领域的能力有限，但我还不知道那些限制是什么。与模型一起解决你的问题对于理解
    LLM 能做什么、不能做什么，以及*应该*做什么至关重要。
- en: The future looks bright, and GenAI Studio can get you going very quickly. Remember,
    the UI gives you some simple copy/paste code so you can use the API rather than
    the UI, which is required for actual applications doing this type of work.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 未来看起来光明，GenAI Studio 可以让你迅速上手。记住，UI 提供了一些简单的复制/粘贴代码，以便你可以使用 API 而不是 UI，这对于实际进行此类工作的应用程序是必要的。
- en: This also means that the hammer still doesn’t make houses. By this I mean that
    the model didn’t figure out this ETL example. The LLM is the very elaborate “hammer”
    — I was the carpenter, just like you.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这也意味着锤子仍然不能建造房子。我的意思是模型没有搞定这个 ETL 示例。LLM 就是那个非常复杂的“锤子”——我就是木匠，就像你一样。
- en: Disclaimer
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 免责声明
- en: This article is the author’s opinion and perspective and does not reflect those
    of his employer. (Just in case Google is watching.)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是作者的观点和看法，并不反映其雇主的立场。（以防万一 Google 在观看。）
