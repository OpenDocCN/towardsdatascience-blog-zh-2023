- en: Your Own Personal ChatGPT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你自己的个人 ChatGPT
- en: 原文：[https://towardsdatascience.com/your-own-personal-chatgpt-cb0512091e3f](https://towardsdatascience.com/your-own-personal-chatgpt-cb0512091e3f)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/your-own-personal-chatgpt-cb0512091e3f](https://towardsdatascience.com/your-own-personal-chatgpt-cb0512091e3f)
- en: How you can fine-tune OpenAI’s GPT-3.5 Turbo model to perform new tasks using
    your custom data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用你的自定义数据对 OpenAI 的 GPT-3.5 Turbo 模型进行微调，以执行新的任务
- en: '[](https://robgon.medium.com/?source=post_page-----cb0512091e3f--------------------------------)[![Robert
    A. Gonsalves](../Images/96b4da0f602a1cd9d1e1d2917868cbee.png)](https://robgon.medium.com/?source=post_page-----cb0512091e3f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cb0512091e3f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cb0512091e3f--------------------------------)
    [Robert A. Gonsalves](https://robgon.medium.com/?source=post_page-----cb0512091e3f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://robgon.medium.com/?source=post_page-----cb0512091e3f--------------------------------)[![Robert
    A. Gonsalves](../Images/96b4da0f602a1cd9d1e1d2917868cbee.png)](https://robgon.medium.com/?source=post_page-----cb0512091e3f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cb0512091e3f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cb0512091e3f--------------------------------)
    [Robert A. Gonsalves](https://robgon.medium.com/?source=post_page-----cb0512091e3f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cb0512091e3f--------------------------------)
    ·15 min read·Sep 8, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cb0512091e3f--------------------------------)
    ·15分钟阅读·2023年9月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7464d9a6182286a2f69d41a6779a233c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7464d9a6182286a2f69d41a6779a233c.png)'
- en: '**“a minimalist painting of a cute robot in an art class,”** Image created
    using an AI image creation program, Midjourney, and edited by the author'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**“一个在艺术课堂上画着可爱机器人极简主义风格的画作，”** 图像使用 AI 图像创作程序 Midjourney 创建，并由作者编辑'
- en: I was excited when I got an email from OpenAI announcing the ability to fine-tune
    ChatGPT. The update came in response to the requests from developers and businesses
    looking to customize the model to better cater to their specific requirements.
    By leveraging this fine-tuning, it’s now possible to improve steerability, achieve
    more consistent output formatting, and establish a desired custom tone. Another
    noteworthy aspect is that users can send shorter prompts without any notable dip
    in performance.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当我收到 OpenAI 宣布可以微调 ChatGPT 的邮件时，我感到很兴奋。这个更新是响应开发者和企业的要求，旨在定制模型以更好地满足他们的具体需求。通过利用这种微调，现在可以改进引导性，实现更一致的输出格式，并建立所需的自定义语调。另一个值得注意的方面是，用户可以发送更短的提示而不会显著影响性能。
- en: Here’s what OpenAI says on their development blog [1].
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 OpenAI 在他们的开发博客上所说的[1]。
- en: This update gives developers the ability to customize models that perform better
    for their use cases and run these custom models at scale. Early tests have shown
    a fine-tuned version of GPT-3.5 Turbo can match, or even outperform, base GPT-4-level
    capabilities on certain narrow tasks. As with all our APIs, data sent in and out
    of the fine-tuning API is owned by the customer and is not used by OpenAI, or
    any other organization, to train other models. — Andrew Peng, et al., OpenAI
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此更新赋予开发者自定义模型的能力，使其在他们的使用场景中表现更佳，并能够大规模运行这些自定义模型。早期测试表明，经过微调的 GPT-3.5 Turbo
    版本在某些特定任务上可以匹配甚至超越基础 GPT-4 级别的能力。与我们所有的 API 一样，进出微调 API 的数据归客户所有，OpenAI 或任何其他组织不会用这些数据来训练其他模型。
    — Andrew Peng 等，OpenAI
- en: In this article, I’ll demonstrate how I used text from my Medium articles as
    training and test data to convert plain text into Markdown format automatically.
    Before I describe the experiment, I’ll give you a little background on ChatGPT.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将展示如何使用我的 Medium 文章中的文本作为训练和测试数据，将纯文本自动转换为 Markdown 格式。在我描述实验之前，我会给你介绍一下
    ChatGPT 的背景。
- en: Background
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: The AI model known as ChatGPT was introduced in November of 2022 [2]. It was
    the first public chatbot that OpenAI released, and I have written about it several
    times [here on Medium](https://robgon.medium.com/list/chatgpt-d71f5c6d0f10). The
    model works as well as a general chatbot but has a few limitations. For example,
    its training cutoff was September 2021, so it doesn’t directly know about any
    new information since then. Using browser plug-ins to fetch and augment the model’s
    data is possible, but this is currently slow and cumbersome.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 被称为 ChatGPT 的 AI 模型在 2022 年 11 月推出 [2]。这是 OpenAI 发布的第一个公共聊天机器人，我在 [Medium 上的这里](https://robgon.medium.com/list/chatgpt-d71f5c6d0f10)
    多次写到过这个模型。该模型作为通用聊天机器人运行良好，但也有一些局限性。例如，它的训练截止日期是 2021 年 9 月，因此不知道此后的任何新信息。可以使用浏览器插件来获取和增强模型的数据，但目前这一过程较慢且繁琐。
- en: A better way to infuse new information and skills into ChatGPT is using OpenAI’s
    [fine-tuning API](https://platform.openai.com/docs/guides/fine-tuning). Fine-tuning
    ChatGPT through the API offers improved results compared to regular prompting.
    It enables training on more examples than a prompt can handle, leads to shorter
    prompts that save tokens, and results in quicker response times. There is a cost
    to using this, however.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 将新信息和技能融入 ChatGPT 的一种更好方法是使用 OpenAI 的 [微调 API](https://platform.openai.com/docs/guides/fine-tuning)。通过
    API 微调 ChatGPT 能提供比常规提示更好的结果。它可以在比提示更多的示例上进行训练，导致更短的提示，从而节省 tokens，并且响应时间更快。不过，这需要费用。
- en: Pricing
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定价
- en: Here are prices for using the various models from [OpenAI’s pricing page](https://openai.com/pricing),
    in US dollars.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用不同模型的价格，来源于 [OpenAI 的定价页面](https://openai.com/pricing)，以美元为单位。
- en: '![](../Images/fc027f2b5e606c07a0492b7bd8aa6511.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc027f2b5e606c07a0492b7bd8aa6511.png)'
- en: '**Prices for Using OpenAI’s Models,** Source [OpenAI](https://openai.com/pricing)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 OpenAI 模型的价格**，来源 [OpenAI](https://openai.com/pricing)'
- en: The term “token” refers to the number of word parts used for prompts and results,
    where 750 words can be represented with about 1,000 tokens. The term “context”
    refers to the total number of tokens used for an interaction’s inputs and results.
    The chart shows that the tokens for the input words (i.e., the prompt) cost less
    than those for the output words (i.e., the results.) Using GPT-4 costs more than
    using GPT-3.5 Turbo. A fine-tuned version of GPT-3.5 Turbo is somewhere between
    the two other models. However, it would be worth the cost if a fine-tuned GPT-3.5
    Turbo can outperform GPT-4\. We’ll see if it’s true in my experiment.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: “token” 这个术语指的是用于提示和结果的词语部分，其中 750 个词可以用大约 1,000 个 tokens 表示。“context” 指的是用于交互输入和结果的
    tokens 总数。图表显示，输入词（即提示）的 tokens 成本低于输出词（即结果）的 tokens 成本。使用 GPT-4 的成本高于使用 GPT-3.5
    Turbo。微调版本的 GPT-3.5 Turbo 介于这两种模型之间。如果微调的 GPT-3.5 Turbo 能超过 GPT-4，那么它的成本是值得的。我们将在我的实验中看看是否真的如此。
- en: Note that these prices are subject to change over time and may be different
    across locales and currencies. Any changes will impact the limitations and trade-offs
    discussed above, so please check your local prices before making any decisions.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些价格可能会随时间变化，并且在不同的地区和货币之间可能有所不同。任何变化都将影响上述讨论的限制和权衡，因此在做出决定之前，请检查本地价格。
- en: Automatically Formatting Documents
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动格式化文档
- en: To kick things off, I ran an experiment to see if the GPT models could be trained
    to render text files to Markdown format automatically. This lightweight markup
    language specifies text formats like headings, blockquotes, code blocks, etc.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启动这一过程，我进行了一个实验，看看 GPT 模型是否能够自动将文本文件转换为 Markdown 格式。这种轻量级的标记语言指定了诸如标题、引用、代码块等文本格式。
- en: '![](../Images/9b0fb12ad254ed50ffa9bb899466c0f1.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b0fb12ad254ed50ffa9bb899466c0f1.png)'
- en: '**Components for Formatting Text Files**, Diagram by Author'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**格式化文本文件的组件**，图示由作者提供'
- en: To test the models, I downloaded 36 of my articles on Medium and converted the
    HTML files to Markdown and plain text format using the Beautiful Soup [library](https://code.launchpad.net/beautifulsoup/).
    The text files are used as inputs, and the Markdown files are used as outputs
    for the training and testing. I used 32 articles for training and three for testing.
    I tested three variants of OpenAI’s language models, GPT-4, GPT-3.5 Turbo, and
    the fine-tuned GPT-3.5 Turbo, and collected the results.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这些模型，我下载了我在 Medium 上的 36 篇文章，并使用 Beautiful Soup [库](https://code.launchpad.net/beautifulsoup/)
    将 HTML 文件转换为 Markdown 和纯文本格式。文本文件用作输入，而 Markdown 文件用作训练和测试的输出。我使用了 32 篇文章用于训练，3
    篇用于测试。我测试了三种 OpenAI 的语言模型变体：GPT-4、GPT-3.5 Turbo 和微调的 GPT-3.5 Turbo，并收集了结果。
- en: Here’s what one of the articles looks like in Markdown format, before and after
    rendering.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是其中一篇文章在Markdown格式中的样子，渲染前后对比。
- en: Markdown Format
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Markdown 格式
- en: '![](../Images/e31c8f15b46097924611c4fd6ce32f72.png)![](../Images/dfe4012bf674baa9c741555aea5ccb60.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e31c8f15b46097924611c4fd6ce32f72.png)![](../Images/dfe4012bf674baa9c741555aea5ccb60.png)'
- en: '**Example Article Showing Markdown Format** (left), **and Rendered** (right),
    Images by Author'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例文章展示Markdown格式**（左），**以及渲染效果**（右），图片由作者提供'
- en: 'You can see the Markdown format characters on the left in dark red, like #
    for Heading 1 and ## for Heading 2\. On the right, you can see how the file is
    rendered, showing formatting like the block quote at the end.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到左侧以深红色显示的Markdown格式字符，如#表示标题1，##表示标题2。在右侧，你可以看到文件的渲染效果，展示了如块引用这样的格式。
- en: Fine-tuning GPT-3.5 Turbo
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调 GPT-3.5 Turbo
- en: Fine-tuning the model was easy. I followed the instructions from OpenAI [here](https://platform.openai.com/docs/guides/fine-tuning).
    The first step was to gather my training and validation data into JSON files.
    I divided my articles into four sections to keep within the 4K token limit for
    GPT-3.5 Turbo. Each entry in the file had a message with an optional system prompt,
    a user message, and an expected response from the assistant. Here is an example
    entry from my training file.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 微调模型很简单。我遵循了 OpenAI [这里](https://platform.openai.com/docs/guides/fine-tuning)
    的说明。第一步是将我的训练和验证数据整理成JSON文件。我将文章分为四个部分，以保持在GPT-3.5 Turbo的4K token限制内。文件中的每个条目都有一个可选的系统提示、用户消息和助手的预期回应。以下是我训练文件中的一个示例条目。
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The system prompt is the same for every entry. The user message is plain text
    from one of my articles, and the assistant’s response is the same text in Markdown
    format.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 系统提示对于每个条目都是相同的。用户消息是我一篇文章中的纯文本，助手的回应是相同文本的Markdown格式。
- en: As mentioned, OpenAI does not use any data submitted to and generated by their
    API to train models or improve their service. However, this is different for the
    interactive version of their service. More info is available [here](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，OpenAI 不会使用任何提交到其 API 并由其生成的数据来训练模型或改进服务。然而，这对于其互动服务版本是不同的。更多信息请参见 [这里](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance)。
- en: Checking the Data
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查数据
- en: Next, I ran the check_file() function from OpenAI on the training and testing
    files. Here are the results from the check.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我在训练和测试文件上运行了 OpenAI 的 check_file() 函数。以下是检查结果。
- en: '![](../Images/b43683a9ea97e76646ec7187603cadff.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b43683a9ea97e76646ec7187603cadff.png)'
- en: '**Output from check_file**, Image by Author'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**check_file**的输出，图片由作者提供'
- en: You can see that I used 132 examples for training, which is 32 articles with
    4 sections each. The script showed me if there were any errors and calculated
    the total number of tokens, which I used to estimate the cost of training.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我使用了132个示例进行训练，即32篇文章，每篇4个部分。脚本展示了是否有错误，并计算了总 token 数量，我用来估算训练成本。
- en: Run the Training
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行训练
- en: Once I had my training and testing files set up and checked, I uploaded the
    files to my account.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我设置并检查了训练和测试文件，我就将文件上传到我的账户。
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: I then got the names of the files and ran this command to start the training.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我获取了文件的名称并运行了这个命令以开始训练。
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The suffix is a way to provide some metadata so I can identify the fine-tuned
    model later. By default, the system ran for three epochs, which took about a half
    hour to finish. When the training was done, OpenAI sent me an email notification.
    Here’s what the training and validation loss looked like throughout 384 training
    steps (32 articles * 4 sections * 3 epochs.)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 后缀是一种提供一些元数据的方法，以便我以后能识别微调后的模型。默认情况下，系统运行了三个周期，完成大约花费了半小时。当训练完成后，OpenAI 发送了电子邮件通知我。以下是384个训练步骤中的训练和验证损失情况（32篇文章
    * 4部分 * 3个周期）。
- en: '![](../Images/b3817fb0dd5ee3b8a413ed8865523ec5.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3817fb0dd5ee3b8a413ed8865523ec5.png)'
- en: '**Train and Validation Loss for Fine-tuning GPT-3.5 Turbo**, Chart by Author'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-3.5 Turbo 微调的训练和验证损失**，图表由作者提供'
- en: It did a good job reducing the loss throughout the training. The validation
    also seemed to follow a downward trend, so it didn’t overfit the training data.
    Let’s see the new model in action.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个训练过程中，它很好地减少了损失。验证也似乎呈下降趋势，因此它没有过拟合训练数据。让我们看看新模型的实际表现。
- en: Testing the Fine-tuned Model
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试微调模型
- en: I logged into my OpenAI account and navigated to the Playground to test the
    system. I chose my fine-tuned model, entered the system prompt, added a few lines
    from the head of this article, and hit the submit button.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我登录到我的 OpenAI 账户，进入 Playground 测试系统。我选择了我的微调模型，输入系统提示，添加了这篇文章开头的几行，并点击了提交按钮。
- en: '![](../Images/826a9d405fc1b93b107ea57e0e8de8d1.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/826a9d405fc1b93b107ea57e0e8de8d1.png)'
- en: '**OpenAI Playground Test of the Fine-tuned Model**, Screenshot by Author'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**OpenAI Playground 测试微调模型**，作者截屏'
- en: It worked well! With a minimal prompt, it matched the formatting that I expected
    from the training.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 它效果很好！使用最小提示，它的格式化与我期望的训练结果匹配。
- en: Comparative Evaluation
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较评估
- en: To compare the accuracy of the fine-tuned model with the standard models, I
    created a [longish prompt](https://gist.github.com/robgon-art/95ec5a7932d8276db9162737f363dcfc#file-prompt-py)
    to test GPT-4 and GPT-3.5 Turbo with one-shot inferencing. I gave one example
    of before and after text in the system prompt and the text to be formatted in
    the user prompt for these two models.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较微调模型与标准模型的准确性，我创建了一个 [较长的提示](https://gist.github.com/robgon-art/95ec5a7932d8276db9162737f363dcfc#file-prompt-py)
    来测试 GPT-4 和 GPT-3.5 Turbo 的单次推理。我在系统提示中给出了一例前后文本，并在用户提示中给出了需要格式化的文本，供这两个模型使用。
- en: I then ran the 12 examples (3 articles with 4 sections) through the three models
    to get the results. To get a quantitative measure of accuracy, I calculated the
    Character Error Rate (CER) between the predicted formatted text and the original
    formatted text as a reference. In addition to the CER results, I report the duration
    of running the inferences and the costs for comparison.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我将 12 个示例（3 篇文章，每篇 4 个部分）运行通过三个模型以获得结果。为了获得准确性的定量测量，我计算了预测格式化文本与原始格式化文本之间的字符错误率（CER）。除了
    CER 结果外，我还报告了运行推理的时间和成本，以供比较。
- en: '![](../Images/4e69f512f46fd1a00db45589c68311b8.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e69f512f46fd1a00db45589c68311b8.png)'
- en: '**Results of Formatting Text Documents with Various GPT Models**, Table by
    Author'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用各种 GPT 模型格式化文本文档的结果**，作者制表'
- en: You can see that the GPT-4 model had fewer errors than the stock GPT-3.5 Turbo
    model. However, formatting the three articles took a lot longer and had a much
    higher cost. The fine-tuned GPT-3.5 Turbo model had far fewer errors and ran much
    faster. However, the inferencing cost was in the middle and was burdened with
    the fine-tuning cost. I would have to run seven experiments (e.g., 21 formatted
    articles) to break even with the GPT-4 costs. But it may be worth it because of
    the low CER.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到 GPT-4 模型的错误比标准的 GPT-3.5 Turbo 模型少。然而，格式化这三篇文章花费了更长的时间和更高的成本。微调后的 GPT-3.5
    Turbo 模型错误少得多，运行速度更快。然而，推理成本居中，并且负担了微调成本。我需要进行七次实验（例如，21篇格式化的文章）才能平衡 GPT-4 的成本。但由于
    CER 较低，这可能是值得的。
- en: Discussion
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论
- en: Although the fine-tuned model did a great job of automatically formatting text,
    I was curious to see if it “learned” anything about the source material it saw
    while fine-tuning. So I asked it this question.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管微调后的模型在自动格式化文本方面表现出色，我还是好奇它在微调过程中是否“学到了”一些关于源材料的内容。所以我问了它这个问题。
- en: 'USER - Do you know about an article in Towards Data Science called Writing
    Songs with GPT-4: Part 1, Lyrics?'
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 用户 - 你知道在 Towards Data Science 上有一篇名为“用 GPT-4 写歌：第 1 部分，歌词”的文章吗？
- en: ''
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'ASSISTANT - Yes, I am familiar with the article you mentioned. It is titled
    “Writing Songs with GPT-4: Part 1, Lyrics” and it was published on Towards Data
    Science. This article explores the use of GPT-4, a language model developed by
    OpenAI, for generating song lyrics. It covers how to fine-tune the model and generate
    creative lyrics.'
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 助手 - 是的，我熟悉你提到的文章。它的标题是“用 GPT-4 写歌：第 1 部分，歌词”，发表在 Towards Data Science。这篇文章探讨了如何使用
    OpenAI 开发的 GPT-4 语言模型生成歌曲歌词。它讲述了如何微调模型并生成创意歌词。
- en: ''
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: USER - Who wrote the article?
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 用户 - 谁写了这篇文章？
- en: ''
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'ASSISTANT - The article “Writing Songs with GPT-4: Part 1, Lyrics” was written
    by Ellie Chen.'
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 助手 - 文章“用 GPT-4 写歌：第 1 部分，歌词”是 Ellie Chen 写的。
- en: OK, it is hallucinating here. First, I didn’t fine-tune GPT-4 for this article.
    Second, the model thinks it was written by someone named Ellie Chen. So, the training
    data didn’t sink in. As a next step, I could explore ways to inject new data into
    the GPT models during fine-tuning for later recall and use.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这里出现了幻觉。首先，我并没有为这篇文章微调 GPT-4。其次，模型认为是一个名叫 Ellie Chen 的人写的。所以，训练数据并没有被充分吸收。下一步，我可以探索在微调过程中向
    GPT 模型注入新数据以供后续回忆和使用的方法。
- en: Conclusion
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: OpenAI’s introduction of fine-tuning capabilities for ChatGPT presents a new
    avenue for developers to adapt the model to specific tasks. This article explored
    the process and outcomes of using this feature, particularly in automatically
    converting plain text into Markdown format. Although the model worked well with
    this straightforward task, it didn’t seem to comprehend and remember the new text
    it saw, so it can’t reliably be used for generative queries about the new material.
    As AI technology progresses, the exploration of creating specialized models becomes
    a topic of interest, highlighting the importance of continued research and assessment.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 推出的 ChatGPT 微调功能为开发者提供了将模型适应特定任务的新途径。本文探讨了使用此功能的过程和结果，特别是在将纯文本自动转换为 Markdown
    格式的应用。尽管模型在这个简单的任务上表现良好，但它似乎无法理解和记住所见的新文本，因此不能可靠地用于生成有关新材料的查询。随着 AI 技术的发展，创建专用模型的探索成为一个有趣的话题，突显了持续研究和评估的重要性。
- en: Source Code
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 源代码
- en: The source code for this project is available on GitHub.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目的源代码可在 GitHub 上获取。
- en: '![](../Images/71d27e760510e5537d17fec893c347ef.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71d27e760510e5537d17fec893c347ef.png)'
- en: '**Creative Commons Attribution Sharealike**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识共享署名-相同方式共享**'
- en: Acknowledgments
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: I thank Jennifer Lim for reviewing the article and providing feedback.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢 Jennifer Lim 审阅文章并提供反馈。
- en: References
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] A. Peng et al., [GPT-3.5 Turbo fine-tuning and API updates](https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates),
    2023'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] A. Peng 等人，[GPT-3.5 Turbo 微调和 API 更新](https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates),
    2023'
- en: '[2] OpenAI, [Introducing ChatGPT](https://openai.com/blog/chatgpt), 2022'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] OpenAI, [介绍 ChatGPT](https://openai.com/blog/chatgpt), 2022'
