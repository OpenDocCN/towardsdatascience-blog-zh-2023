# 超越NeRF（第一部分）

> 原文：[https://towardsdatascience.com/beyond-nerfs-part-one-7e84eae816d8](https://towardsdatascience.com/beyond-nerfs-part-one-7e84eae816d8)

## 提高NeRF训练速度100倍或更多……

[](https://wolfecameron.medium.com/?source=post_page-----7e84eae816d8--------------------------------)[![Cameron R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----7e84eae816d8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7e84eae816d8--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7e84eae816d8--------------------------------) [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----7e84eae816d8--------------------------------)

·发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7e84eae816d8--------------------------------) ·15分钟阅读·2023年6月7日

--

![](../Images/3185851e9a8756d697031a415a23b913.png)

（照片由 [Mathew Schwartz](https://unsplash.com/@cadop?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 提供，来源于 [Unsplash](https://unsplash.com/s/photos/speed?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)）

正如我们在之前的概述中所见，[神经辐射场（NeRFs）](https://cameronrwolfe.substack.com/p/understanding-nerfs) [4] 的提出在神经场景表示领域是一个突破。给定一些底层场景的图像，我们可以训练一个NeRF以高分辨率生成该场景的任意视角。简而言之，NeRF利用深度学习提供3D场景的摄影级渲染。

但，它们有一些显著的问题。在本概述中，我们将特别关注NeRF的两个局限性：

1.  训练一个可以准确渲染新视角的NeRF需要大量的场景图像。

1.  使用NeRF进行训练（和渲染）是很慢的。

作为解决这些问题的方案，我们将概述NeRF方法的两个显著扩展：PixelNeRF [1] 和 InstantNGP [2]。在学习这些方法的过程中，我们会看到，NeRF所面临的大部分问题可以通过制作更高质量的输入数据以及利用深度神经网络将已学模式推广到新场景的能力来解决。

![](../Images/ea4ac87de7567d65a45b1908a380b387.png)

（来自 [1, 2]）

# 背景

我们最近了解到许多不同的使用深度学习建模3D形状和场景的方法。这些概述包含了一些背景概念，这些概念也将有助于理解本概述中的概念：

+   前馈神经网络 [[链接](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)]

+   位置信息嵌入 [[链接](https://cameronrwolfe.substack.com/i/97915766/position-encodings)]

+   签名距离函数 [[链接](https://cameronrwolfe.substack.com/i/94634004/signed-distance-functions)]

+   3D数据的表示方式 [[链接](https://cameronrwolfe.substack.com/i/94634004/representing-d-shapes)]

除了这些概念外，在这个概述中，理解NeRFs [4]也会非常有用。为了建立这种理解，我建议阅读我对NeRFs的概述[这里](https://cameronrwolfe.substack.com/p/understanding-nerfs)。

## 特征金字塔

在这篇文章中，我们将看到多个实例，展示如何使用深度神经网络将图像转换为相应的（金字塔）特征表示。但有些人可能对这个概念不太熟悉。因此，我们需要快速了解特征表示，并概述我们在深度学习中可能遇到的一些不同变体。

**什么是特征？** 在了解特征金字塔之前，我们需要理解“特征”一词的含义。通常，神经网络的输出会是分类、边界框集合、分割掩码或其他类似的东西。例如，在图像分类中，我们将图像作为输入，通过神经网络传递，网络的最后一层是一个分类模块，将隐藏状态转换为类别概率向量。简单明了！

![](../Images/0c77c5951bc7bbada893433c2a3f1e26.png)

从深度神经网络中提取特征（由作者创建）

然而，有时我们不希望执行最后一步。相反，我们可以直接取网络的最终隐藏状态（在分类模块之前），并将这个向量作为数据的表示；见上文。这个向量，也称为特征（或特征表示），是数据中语义信息的压缩表示，我们可以用它来执行各种任务（例如，相似性搜索）。

**什么是特征金字塔？** 多尺度（或“金字塔”）策略是计算机视觉中的一个重要基本概念。基本思想很简单。在神经网络的层次中，我们偶尔*(i)* 降采样特征的空间分辨率，并*(ii)* 增加通道维度。例如，见[ResNet-18](https://pytorch.org/vision/0.8/_modules/torchvision/models/resnet.html) [6]的示意图。这个CNN包含四个“部分”，每个部分的通道维度逐渐增高，空间维度逐渐降低；见下文。

![](../Images/67ed7ce4de99f80edc9a8d338e8e35f0.png)

ResNet架构中“部分”的基本示意图（由作者创建）

从这个网络中提取特征的一种方法是仅使用最终的隐藏表示。但是，与网络早期层相比，这种表示不包含太多的空间信息（即，空间维度在每一层中逐渐降低！）。这对于依赖图像中空间信息的密集预测任务（例如目标检测）是一个问题！为了解决这个问题，我们需要构建一个*特征金字塔*[3]。

简而言之，特征金字塔从网络中的几个不同层中提取特征，而不是仅使用网络最终层的特征；见下文。

![](../Images/ba386f48c7c39c7f786be698e3fc52a3.png)

（来自[3]）

得到的特征集包含不同量的空间和语义信息，因为每一层的空间和通道维度都不同。因此，特征金字塔通常生成对各种不同任务有用的图像特征。在本概述中，我们将看到特征金字塔用于为NeRF的变体提供额外的输入信息！

## 输入编码

有时，我们有些数据不想直接输入到机器学习模型中，因此我们将这些数据的编码版本作为输入。这是机器学习中的一个基本概念。例如，[独热编码](https://www.educative.io/blog/one-hot-encoding)的分类变量。一个更复杂的例子是[核函数](https://www.geeksforgeeks.org/major-kernel-functions-in-support-vector-machine-svm/#:~:text=Kernel%20Function%20is%20a%20method,window%20to%20manipulate%20the%20data.)，或我们将数据通过的函数（即，可能使其[线性可分](https://en.wikipedia.org/wiki/Linear_separability)）然后再提供给模型。在这些情况下，我们都在编码/转换输入，以便它以更适合模型的格式呈现。

![](../Images/272dae94587286f4b5e18369396a06aa.png)

NeRF架构中的位置编码（由作者创建）

**位置编码。** 类似地，当我们将3D坐标作为输入传递给NeRF的前馈网络时，我们不想直接使用这些坐标作为输入。相反，我们使用位置编码方案将它们转换为更高维的向量；见上文。这种位置编码方案是用于在变换器[6]中为标记化输入添加位置数据的完全相同的技术。在NeRF中，位置编码已被证明能显著改善场景渲染效果。

**可学习的嵌入层。** 位置编码方案有一个问题——*它们是固定的*。如果我们想学习这些编码呢？一种方法是构造一个*嵌入矩阵*。给定一个将每个空间位置映射到矩阵中的索引的函数，我们可以检索每个空间位置对应的嵌入并将其作为输入。然后，这些嵌入可以像普通模型参数一样进行训练！

# 出版物

现在，我们将概述一些扩展和改进 NeRF 的出版物。特别是，这些出版物 *(i)* 通过较少的场景图像生成高质量的场景表示，并 *(ii)* 使 NeRF 的训练和渲染过程更快。

## [PixelNeRF：来自一张或几张图片的神经辐射场](https://arxiv.org/abs/2012.02190) [1]

![](../Images/9dec6ef785de3edadc878b37506e1e2d.png)

（来源于 [1]）

原始 NeRF 公式的主要缺点之一是它必须针对每个场景进行训练和使用。通过 NeRF 获取每个场景的表示在计算上是昂贵的，并且需要许多具有姿态的场景图像。PixelNeRF [1] 旨在通过将 NeRF 的输出条件化为由预训练的深度神经网络创建的图像特征来缓解这一问题。通过使用图像特征作为输入，*PixelNeRF 可以利用先前的信息，在仅有少数场景图像的情况下生成高质量的场景渲染*。因此，它在数据有限的情况下显著提高了场景表示的质量。

**方法。** PixelNeRF 与原始 NeRF 公式非常相似。它使用前馈神经网络通过预测给定空间位置和视角方向（已转换为 [位置嵌入](https://cameronrwolfe.substack.com/i/97915766/position-encodings)）的颜色和不透明度值来建模辐射场。体积渲染和训练过程没有改变。这些方法之间的主要区别在于 pixelNeRF 具有一个额外的输入组件：*从底层场景视图中衍生的图像特征*。

![](../Images/beb0ac1511966f3b6880d17347bc7f13.png)

（来源于 [1]）

PixelNeRF 能够将一个或多个场景图像作为输入的一部分。图像首先通过一个预训练的编码器——一个特征金字塔 ResNet 变体[6]——来生成特征金字塔。从这里，我们可以提取这些特征中对应于特定空间位置的区域（这可以通过相机位姿信息比较容易地完成；见 [1] 的第4.1节）。然后，我们将这些提取的特征与对应的空间位置和视角方向串联作为 PixelNeRF 前馈网络的输入。

让我们思考一下 PixelNeRF 的前馈神经网络的单次前向传播。我们在这一前向传播中考虑一个单一的空间位置和观察方向。如果我们可以访问到场景的单幅图像，我们可以通过以下方式包含这些信息：

1.  通过编码器传递图像以生成特征网格。

1.  通过提取与当前空间位置对应的特征金字塔区域来获取特征。

1.  连接空间、方向和特征输入。

然后，PixelNeRF 的其余组件与原始的 NeRF 公式相匹配；见下文。

![](../Images/c3d9cdf82abf657888eb905f4d15289d.png)

（来自 [1]）

如果有多个场景图像可用，我们只需将 PixelNeRF 的前馈网络分为两个组件。第一个组件使用上述过程单独处理每张图像。即，网络通过将每张图像的特征与相同的空间和方向输入信息连接起来来执行单独的前向传播。

![](../Images/133dac84694b854f15956b047e23fbf1.png)

PixelNeRF 具有多个输入视角的架构（作者创建）

每次前向传播都会产生一个输出向量。我们可以通过计算这些向量的平均值来聚合它们，然后将这个平均向量通过更多的前馈层以生成最终的 RGB 和不透明度输出；见上文。尽管有这种修改后的架构，PixelNeRF 的训练过程与 NeRF 相似，并且只需要场景图像的数据集。

**结果。** PixelNeRF 在 [ShapeNet](https://shapenet.org/) 上进行物体和场景视图合成等任务的评估，以及其表示真实世界场景的能力。首先，PixelNeRF 被训练以代表来自特定 ShapeNet 类（例如，椅子或汽车）的对象，给定一张或两张输入图像（即，一张或两张图像场景）。在这种情况下，我们发现 PixelNeRF 在从少量输入图像重建对象方面优于基线；见下文。

![](../Images/64e6be681dea36cdf7a5254befba17ca.png)

（来自 [1]）

此外，PixelNeRF 不进行任何测试时优化，而像 [SRNs](https://cameronrwolfe.substack.com/p/scene-representation-networks) [5] 这样的基线则不是这样。因此，尽管 PixelNeRF 更快且解决了比基线更困难的问题，但它的表现更为出色。当我们以类别无关的方式训练 PixelNeRF（即，在 ShapeNet 的 13 个对象类别上），我们看到其性能提升更为显著！PixelNeRF 在表示这更广泛的对象集方面超越了基线；见下文。

![](../Images/695f19b191e20c87d07770d7e569cf98.png)

（来自 [1]）

当 PixelNeRF 在更复杂的设置中进行评估时（例如，未见过的类别、多对象场景、真实图像等），我们继续看到性能的提升。最值得注意的是，PixelNeRF 在捕捉多对象场景和在测试时推断未见对象的能力上显著优于基线；见下文。

![](../Images/198773df4fc7a96b24930b348358e02d.png)

(来自[1])

将这一点推向极限，PixelNeRF可以仅凭三张真实场景的输入图像就重建出相当高保真的场景；见下文。这些结果强调了PixelNeRF在给定有限且噪声数据下建模场景的能力。在这种情况下，NeRF无法准确重建场景。

![](../Images/097f0d1408321d400285d687f1e0e513.png)

(来自[1])

## [多分辨率哈希编码的即时神经图形原语](https://arxiv.org/abs/2201.05989) [2]

![](../Images/cb5fbc37dc1e89ba445c76e73755bdf6.png)

(来自[2])

PixelNeRF [1] 允许我们从少量图像中恢复场景表示。但请记住，NeRF的训练过程也很慢（即单个GPU上需要[2天](https://cameronrwolfe.substack.com/i/97915766/takeaways)）！考虑到这一点，我们可能会问自己：*我们能多快训练一个NeRF？* 在[2]中提出的即时神经图形原语（InstantNGP）展示了我们可以*大大*加快训练NeRF的速度。

![](../Images/b71c2a3ae37298e253e3a394d8a00fcf.png)

使用哈希函数对简单的特征嵌入矩阵进行索引（作者创建）

InstantNGP的方法类似于NeRF [4]——唯一的区别在于我们如何构建前馈网络的输入。我们没有使用位置编码方案，而是构建了一个多分辨率哈希表，将每个输入坐标映射到一个可训练的特征向量；见上文。该方法*(i)* 向NeRF添加了更多可学习的参数，并*(ii)* 为每个输入坐标生成丰富的输入表示，从而使前馈网络变得更小。总体而言，这种方法可以显著加快NeRF的训练过程。

**方法。** 实际上构建和查询输入特征哈希表的方法（不幸的是）比上述简单示意图要复杂得多。让我们更深入地探讨InstantNGP [2]是如何处理输入特征的。

InstantNGP采用参数化的方法来编码输入。与使用位置嵌入函数将坐标映射到固定的高维输入的NeRF不同，InstantNGP在训练过程中*学习*输入特征。从高层次看，这是通过以下方式完成的：

1.  在嵌入矩阵中存储输入特征。

1.  基于输入坐标对嵌入矩阵进行索引。

1.  通过[随机梯度下降](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)正常更新特征。

让我们逐一处理这些组件。首先，我们需要创建一个可以索引的可学习输入特征表。在[2]中，输入特征存储在一个包含`L`级特征（即`L`个不同的嵌入矩阵）的多分辨率表中。每一层表都有`T`个维度为`F`的特征向量（即一个`T x F`大小的矩阵）。通常，这些参数遵循以下所示的设置。

![](../Images/8616752a4ef99a20f3dbd88a74a4e1d5.png)

(来源于 [2])

每个级别旨在以不同的分辨率表示3D空间，从`N-min`（最低分辨率）到`N-max`（最高分辨率）。我们可以将其视为将3D空间划分为不同粒度的[体素网格](https://cameronrwolfe.substack.com/i/94634004/representing-d-shapes)（例如，`N-min`将使用非常大/粗糙的体素）。通过这种方式划分3D空间，我们可以确定输入坐标所在的体素——这对于每个分辨率级别都是不同的。坐标所在的体素随后用于将该坐标映射到每个级别的嵌入矩阵中的一个条目。

![](../Images/dfa2b985f730a98b41b2ac9152d9fddd.png)

(来源于 [2])

更具体地说，[2]中的作者使用上面显示的哈希函数将体素位置（即，由体素边缘的坐标给出）映射到每个分辨率级别的嵌入矩阵中的条目索引。值得注意的是，分辨率较粗的级别（即，较大的体素）将具有较少的哈希冲突，这意味着完全不同位置的输入坐标被映射到相同特征向量的可能性较小。

![](../Images/0397cb039d736fdfdeb775bacbf70c12.png)

(来源于 [2])

在我们检索到每个分辨率级别的相应特征向量后，我们会得到多个特征向量对应于单个输入坐标。为了合并这些向量，我们进行线性插值，其中插值的权重是通过输入坐标在每个级别的体素中的相对位置得出的。从这里开始，我们将这些向量与其他输入信息（例如，位置编码的视角方向）拼接起来，形成最终输入！InstantNGP中的完整多分辨率方法如上图所示。

![](../Images/10ef757aa97375f97112aa07e141e0ac.png)

(来源于 [2])

由于使用了更高质量的可学习输入特征，InstantNGP能够相对于NeRF使用更小的前馈网络，同时在质量上取得类似的结果；见上文。当这些修改与更高效的实现（即，完全融合的cuda内核，最小化带宽和浪费操作）结合时，NeRF的训练时间可以显著缩短。事实上，*我们可以在几秒钟内使用InstantNGP获得高质量的场景表示*。

**结果。** InstantNGP使用与[4]中提出的几乎相同的设置来训练NeRF，除了修改的输入编码方案和更小的前馈神经网络。坐标输入使用多分辨率哈希表进行编码，而视角方向则使用普通的、位置编码的嵌入进行编码。

![](../Images/5908e130cb652443b322e0a97dadb114.png)

(来源于 [2])

使用提出的方法和更快的渲染程序，[2]中的作者发现 InstantNGP 可以在几秒钟内训练场景表示，甚至可以以 60 FPS 渲染场景！这相对于 NeRF 是一个巨大的效率提升；详见上文。值得注意的是，InstantNGP 在训练仅 15 秒后就能与 NeRF（需要数小时训练）竞争，*表现突出*！

为了确定这种加速是否来源于更高效的 cuda 实现或多分辨率哈希表，作者进行了一些分析。他们发现高效的实现确实提供了很大的加速，但仅使用哈希表和较小的前馈网络即可在训练 NeRFs 时获得 20 倍到 60 倍的加速。

> *“我们用频率编码替代了哈希编码，并扩大了 MLP 以大致匹配[NeRF]的架构……我们算法的这个版本在训练约 ∼5 分钟后接近 NeRF 的质量，但在训练更短时间（5 秒至 15 秒）后被我们的完整方法超越，这得益于哈希编码和较小的 MLP，使得效率提高了 20 倍到 60 倍。”* — 摘自[2]

在某些情况下，我们确实看到基准方法在包含复杂的视角依赖反射和[非朗伯](https://en.wikipedia.org/wiki/Lambertian_reflectance)效应的场景中优于 InstantNGP。作者声称这是由于[2]中使用了较小的前馈网络；详见下文。

![](../Images/2f03238dd843e73b9297cf1cb9963deb.png)

（摘自[2]）

**我们还可以用它做什么？** 尽管我们专注于改进 NeRF，但 InstantNGP 的方法相当通用——它可以提高各种计算机图形原语（即，描述外观的函数）的效率。例如，InstantNGP 在[2]中被证明在以下方面有效：

1.  生成[超分辨率](http://www.infognition.com/articles/what_is_super_resolution.html)图像

1.  建模[签名距离函数](https://cameronrwolfe.substack.com/i/94634004/signed-distance-functions)（SDFs）

1.  执行[神经辐射缓存](https://research.nvidia.com/publication/2021-06_real-time-neural-radiance-caching-path-tracing)

# 收获

尽管 NeRFs 革新了神经场景表示的质量，但在本综述中我们看到还有很大的改进空间！NeRFs 仍然需要很长时间来训练，并且需要大量的训练数据才能良好工作。下面概述了一些减轻这些问题的基本要点。

**提高样本复杂性。** 在其原始形式中，NeRF 需要大量的输入观察来进行视图合成。这主要是因为 NeRF 是逐场景训练的，无法利用任何先前的信息来生成新的视图。PixelNeRF [1] 通过将预训练的图像特征作为输入添加到 NeRF 的前馈网络中来缓解这个问题。这种方法允许利用来自其他训练数据的学习到的先验信息。因此，这种方法可以仅凭几张图像就生成场景表示！

**更高质量的输入非常重要！** 正如 InstantNGP [2] 所示，NeRF 使用的输入编码方案至关重要。使用更丰富、可学习的编码方案可以缩小前馈网络的大小，从而在训练和渲染效率上取得显著提升。在我看来，这种发现可以激发未来大量的工作。*我们能找到更好的编码方案吗？是否有其他类型的深度学习模型可以应用这个概念？*

**局限性。** 我们在此概述中看到的方法在解决 NeRF 已知的局限性方面做了很多努力，但它们并不完美。InstantNGP 在 NeRF 训练时间上提供了令人难以置信的加速，但结果场景表示的质量并不总是最佳的。与基线相比，InstantNGP 在捕捉复杂效果如反射方面表现不佳，这表明我们为更快的训练牺牲了表示质量。

> *“一方面，我们的方法在几何细节丰富的场景中表现最佳… 另一方面，mip-NeRF 和 NSVF 在具有复杂视角依赖反射的场景中超越了我们的方法… 我们将此归因于我们为了获得比这些竞争实现快几个数量级的速度提升而必然使用的更小的MLP。”* — 来自 [2]

此外，由于 PixelNeRF [1] 在其初始前馈组件中分别处理每个输入图像，其运行时间随输入视图数量线性增长。这种线性依赖性可能导致训练和渲染速度相当慢。因此，我们可以解决一些 NeRF 的主要问题，但可能会付出一些代价！

## 结语

非常感谢你阅读这篇文章。我是 [Cameron R. Wolfe](https://cameronrwolfe.me/)，[Rebuy](https://www.rebuyengine.com/) 的 AI 总监。我研究深度学习的经验和理论基础。你也可以查看我在 medium 上的 [其他著作](https://medium.com/@wolfecameron)！如果你喜欢这篇文章，请在 [twitter](https://twitter.com/cwolferesearch) 上关注我或订阅我的 [Deep (Learning) Focus 新闻通讯](https://cameronrwolfe.substack.com/)，我通过对流行论文的易懂概述来帮助读者更深入地理解深度学习研究中的主题。

## 参考文献

[1] Yu, Alex 等人。“pixelnerf: 从一张或几张图片中生成神经辐射场。” *IEEE/CVF计算机视觉与模式识别会议论文集*。2021年。

[2] Müller, Thomas 等人。“具有多分辨率哈希编码的即时神经图形原语。” *ACM图形学汇刊（ToG）* 41.4（2022年）：1–15。

[3] Lin, Tsung-Yi 等人。“用于目标检测的特征金字塔网络。” *IEEE计算机视觉与模式识别会议论文集*。2017年。

[4] Mildenhall, Ben 等人。“Nerf：将场景表示为神经辐射场以进行视图合成。” *ACM通讯* 65.1（2021年）：99–106。

[5] Sitzmann, Vincent, Michael Zollhöfer 和 Gordon Wetzstein。“场景表示网络：连续的3D结构感知神经场景表示。” *神经信息处理系统进展* 32（2019年）。

[6] Vaswani, Ashish 等人。“注意力机制是你所需的一切。” *神经信息处理系统进展* 30（2017年）。
