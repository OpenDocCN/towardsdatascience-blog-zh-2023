- en: A Comprehensive Comparison of ML Experiment Tracking Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-comprehensive-comparison-of-ml-experiment-tracking-tools-9f0192543feb](https://towardsdatascience.com/a-comprehensive-comparison-of-ml-experiment-tracking-tools-9f0192543feb)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/2899664dc480f7c5aeb24145ebb7f7e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [BoliviaInteligente](https://unsplash.com/@boliviainteligente?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/pwfFRdS-A-I?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: What are the pros and cons of 7 leading tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://eryk-lewinson.medium.com/?source=post_page-----9f0192543feb--------------------------------)[![Eryk
    Lewinson](../Images/56e09e19c0bbfecc582da58761d15078.png)](https://eryk-lewinson.medium.com/?source=post_page-----9f0192543feb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9f0192543feb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9f0192543feb--------------------------------)
    [Eryk Lewinson](https://eryk-lewinson.medium.com/?source=post_page-----9f0192543feb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9f0192543feb--------------------------------)
    ·12 min read·Apr 26, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Building machine learning models is a highly iterative process. After building
    a simple MVP for our project, we will most likely carry out a series of experiments
    in which we try out different models (along with their hyperparameters), create
    or add various features, or utilize data preprocessing techniques. All with the
    goal of achieving better performance.
  prefs: []
  type: TYPE_NORMAL
- en: As the number of experiments increases, it becomes challenging to keep track
    of them. At this point, a piece of paper or an Excel sheet might not be sufficient.
    Moreover, additional complexity arises when we potentially need to reproduce the
    best-performing experiment before putting it into production. This is where ML
    experiment tracking comes into play!
  prefs: []
  type: TYPE_NORMAL
- en: What is ML Experiment Tracking?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML experiment tracking is the process of recording, organizing, and analyzing
    the results of ML experiments. It helps data scientists keep track of their experiments,
    reproduce their results, and collaborate with others effectively. Experiment tracking
    tools enable us to log experiment metadata, such as hyperparameters, dataset/code
    versions, and model performance metrics. Furthermore, we can easily visualize
    experiment results and compare their performance. By doing so, we can identify
    the most effective combinations of hyperparameters and other experimental settings,
    leading to better-performing models.
  prefs: []
  type: TYPE_NORMAL
- en: How to Choose the ML Experiment Tracking Tool that Fits Your Needs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Running experiments without proper tools can easily result in a disorganized
    and unmanageable workflow, even for simple projects. As a data scientist, it is
    essential to choose the right experiment tracking tool that best fits your needs
    and workflow. With numerous options available, the task of selecting the ideal
    tool can be daunting.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will delve into some of the most popular experiment tracking
    tools available and compare their features to help you make an informed decision.
    By the end of this article, you will have a clear understanding of each tool’s
    strengths and limitations, allowing you to choose the best one for your specific
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of experiment tracking tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1\. MLflow](https://mlflow.org/)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MLflow is an open-source platform designed to manage the end-to-end machine
    learning lifecycle. It offers a suite of tools for experiment tracking, storing,
    and versioning ML models in a centralized registry, packaging code into reproducible
    runs, and deploying models to various serving environments and platforms.
  prefs: []
  type: TYPE_NORMAL
- en: '**Main characteristics**'
  prefs: []
  type: TYPE_NORMAL
- en: MLflow is a highly customizable open-source project.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLflow is language- and framework-agnostic, and it offers convenient integration
    with the most popular machine learning and deep learning frameworks. It also has
    APIs for R and Java, and it supports REST APIs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLflow offers automatic logging for the most popular machine learning and deep
    learning libraries. By using it, we do not have to use explicit log statements
    to keep track of metrics, parameters, and models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is effortless to integrate MLflow into an existing codebase with just a few
    lines of code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLflow has a very large and active community and is widely adopted in the industry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLflow can log results both locally and to a remote server, enabling a team
    of data scientists to share a single dashboard.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of storing large files, MLflow can be configured to store them on
    S3 or another cloud storage provider.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLflow’s web UI allows for the viewing and comparison of results from numerous
    experiments carried out by different users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional notes about experiments can be stored in MLflow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLflow offers not only experiment tracking but also end-to-end ML lifecycle
    management.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keep in mind**'
  prefs: []
  type: TYPE_NORMAL
- en: MLflow is only available as an open-source solution. As such, using MLflow in
    a company setting requires maintaining servers and infrastructure to support the
    tool, which might be challenging for smaller organizations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In terms of security, MLflow does not have robust security features out-of-the-box.
    Therefore, it might require additional configuration and setup to ensure the secure
    handling of sensitive data and managing access control. As such, it might not
    be that easy to share experiment results with others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While MLflow supports collaboration, it does not have the same level of collaboration
    features as some other platforms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2\. DVC](https://dvc.org/)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DVC (Data Version Control) is an open-source MLOps tool for data versioning
    and experiment tracking. It is often described as a Git-like system used for versioning
    your data and models. Essentially, DVC enables us to track data with Git without
    storing the data in the Git repository. Furthermore, it provides pipeline management
    functionalities, which help with experiment reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: '**Main characteristics**'
  prefs: []
  type: TYPE_NORMAL
- en: DVC is an open-source, language-agnostic tool that is free to use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DVC uses Git-like commands for version control, making it easy for developers
    who are already familiar with Git.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracked metrics are stored in plain text files and versioned with Git.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DVC handles everything for us in a clean way that doesn’t clutter the repository,
    so we don’t need to create dedicated Git branches for each experiment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DVC is platform-agnostic and can work with a wide range of storage providers,
    making it easy to manage data across different platforms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DVC can track changes to code, data, and artifacts, making it easy to reproduce
    any executed experiment, even if one of the components changes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DVC is easy to use and doesn’t require special infrastructure or external services.
    We can track experiments (and other components) locally or using a cloud remote.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With DVC, we don’t need to rebuild previous models or data modeling techniques
    to achieve the same past state of results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With DVC, we can also track output images of each experiment, such as confusion
    matrices or feature importance plots.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can work with DVC-tracked experiments from the command line or leverage Iterative
    Studio (previously called DVC Studio), a web application that we can access online
    or host on-prem. Alternatively, DVC offers a VS Code extension that facilitates
    experiment management, comparison, and evaluation, all within the IDE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can easily log machine learning parameters, metrics, and other metadata in
    simple file formats using a companion library called DVCLive. It also provides
    auto-logging for the most popular machine learning and deep learning libraries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keep in mind**'
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, you might encounter scalability issues when working with very
    large datasets or a large number of experiments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage is organized in a content-addressable, and all operations are executed
    using a Git repo. Without working with it before, it might come off as unexpected
    and while there are a lot of benefits for this, it might be that some teams cannot
    integrate that approach with their tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While DVCLive supports auto-logging for the majority of frameworks, it does
    not support `scikit-learn`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3\. ClearML](https://clear.ml/)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ClearML is an open-source platform for managing machine learning (ML) experiments.
    It allows users to easily track, monitor, and reproduce their ML experiments,
    visualize their results as well as collaborate with team members.
  prefs: []
  type: TYPE_NORMAL
- en: '**Main characteristics**'
  prefs: []
  type: TYPE_NORMAL
- en: ClearML provides easy experiment tracking and management, allowing users to
    keep track of experiments, metrics, hyperparameters, metadata, and more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ClearML supports automatic logging. It also logs any metrics reported to leading
    visualization libraries such as TensorBoard and Matplotlib. Additionally, ClearML
    captures and logs everything written to standard output, from debug messages to
    errors and library warning messages. Lastly, it automatically tracks information
    such as usage of GPU, CPU, Memory, and Network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ClearML is compatible with leading machine learning and deep learning libraries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ClearML can be deployed either on-premises or in the cloud. We can interact
    with the platform using either a web interface or a Python API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can work with ClearML experiments in offline mode, in which all information
    is saved in a local folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ClearML allows multiple users to collaborate on the same project, enabling easy
    sharing of experiments and data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ClearML provides users with various visualizations, making it easy to interpret
    and analyze experiment data. Additionally, its customizable UI enables users to
    sort models by different metrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ClearML is easy to integrate into existing workflows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ClearML has built-in hyperparameter optimization capabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keep in mind**'
  prefs: []
  type: TYPE_NORMAL
- en: While ClearML offers a free tier, more advanced features (for example, hyperparameter
    optimization or role-based access control) require a paid subscription.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ClearML has a smaller user base compared to other similar platforms, which can
    make finding support or resources more difficult.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the large number of modifications that need to be run for auto-logging
    (ClearML replaces some built-in functions of other frameworks), the system may
    be comparatively fragile.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up and configuring ClearML can be challenging, especially for users
    who are new to the platform. For example, installing the open-source version on
    your servers is relatively complicated compared to MLflow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4\. TensorBoard](https://www.tensorflow.org/tensorboard)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorBoard is an open-source, web-based visualization tool for machine learning
    experiments that makes it easier to understand, debug, and optimize TensorFlow
    models. It provides a suite of visualizations for monitoring training progress,
    evaluating model performance, and visualizing data flow graphs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Main characteristics**'
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard is often the first choice for TensorFlow users as it enables us
    to track our machine learning experiments and visualize various aspects such as
    metrics (e.g. loss and accuracy) or model graphs. Additionally, we can use it
    to compare experiments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorBoard is not limited to tracking experiments based on TensorFlow alone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorBoard includes the What-If Tool (WIT), which is an easy-to-use interface
    for explainability and understanding of black-box ML models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The strong and large community of users provides excellent support for TensorBoard.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to the open-source, locally-hosted version, TensorBoard.dev is available
    as a free service on a managed server, which allows us to host, track, and share
    our ML experiments with anyone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorBoard also provides well-developed features for working with images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keep in mind**'
  prefs: []
  type: TYPE_NORMAL
- en: Some users may find TensorBoard complex to use, with a steep learning curve.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorBoard may not scale well with a large number of experiments, causing slowdowns
    when viewing and tracking large-scale experimentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorBoard’s capability for experiment comparison is limited.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorBoard is primarily designed for single-user and local machine usage, rather
    than team usage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It lacks user management features. While sharing is available using TensorBoard.dev,
    there is no way to manage the privacy of data shared.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorBoard does not store data or code versions, making it unable to provide
    full reproducibility.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5\. Weights and Biases](https://wandb.ai/site)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Weights & Biases (W&B, or WandB) is an MLOps platform that enables experiment
    tracking, versioning of your data/models, as well as team collaboration on ML
    projects.
  prefs: []
  type: TYPE_NORMAL
- en: '**Main characteristics**'
  prefs: []
  type: TYPE_NORMAL
- en: W&B logs various experiment metadata (such as hyperparameters, metrics, artifacts,
    etc.) and allows users to compare experiments and analyze performance using interactive
    visualizations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: W&B makes it easy to reproduce experiments by tracking all dependencies and
    providing a consistent environment for each experiment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: W&B offers a highly customizable UI that allows teams to visualize and organize
    their workflows, including any custom metrics and visualizations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: W&B offers a selection of functionalities supporting collaborative work in teams.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: W&B supports all major ML/DL frameworks, cloud platforms, and workflow orchestration
    tools (such as Airflow).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: W&B supports deployment to a wide range of platforms, including cloud services
    and containerized environments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: W&B offers built-in hyperparameter optimization functionalities through integration
    with leading libraries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is easy to integrate W&B tracking into existing codebases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is possible to set up a self-hosted instance of W&B in case of working with
    sensitive data that cannot be shared externally.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It allows for easy debugging of audio, video, and image objects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keep in mind**'
  prefs: []
  type: TYPE_NORMAL
- en: W&B is a commercial platform and might require a paid subscription for certain
    features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limited integrations with some lesser-known ML frameworks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collaboration features require a paid tier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pricing plan is based on usage time (referred to as tracked hours), which
    may be counter-intuitive for users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6\. Comet](https://www.comet.com/site/)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Comet (formerly CometML) is a cloud-based platform for managing and tracking
    machine learning experiments. Additionally, it can be used to version training
    data, keep track of our models in a model registry, and monitor the performance
    of models in production.
  prefs: []
  type: TYPE_NORMAL
- en: '**Main characteristics**'
  prefs: []
  type: TYPE_NORMAL
- en: Comet allows us to log and track experiments, providing an easy way to visualize
    and compare results over time. It also offers real-time metrics and charts for
    the experiments being run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comet offers several features that foster collaboration with a team. For example,
    it allows us to share projects, comment and tag other team members. Additionally,
    it comes with user management features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comet has an integrated suite of tools for optimizing the hyperparameters of
    our models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It easily integrates with the most popular machine learning and deep learning
    frameworks. Additionally, it supports languages other than Python, such as Javascript,
    Java, R, or even REST APIs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comet supports autologging for quite a big selection of the most popular machine
    learning and deep learning libraries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comet’s platform can be used on their cloud environment, a virtual private cloud
    (VPC), or on-premises.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comet’s UI is highly customizable, allowing us to easily build reports and dashboards.
    We can create custom visualizations for our experiments or use some of the community-provided
    templates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comet supports not only scripts but also notebooks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With Comet, we can debug model errors, environment-specific errors, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Has dedicated modules for vision, audio, text, and tabular data that allow us
    to easily identify any issues with the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keep in mind**'
  prefs: []
  type: TYPE_NORMAL
- en: Comet is a commercial platform and might require a paid subscription for certain
    features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7\. DagsHub](https://dagshub.com/)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DagsHub is a web-based platform that provides a suite of tools for managing
    and collaborating on machine learning projects. It is designed to help data scientists
    and machine learning engineers track, version, and share their code, together
    with the corresponding data and experiments.
  prefs: []
  type: TYPE_NORMAL
- en: '**Main characteristics**'
  prefs: []
  type: TYPE_NORMAL
- en: DagsHub enables effortless tracking and management of machine learning experiments,
    including hyperparameters, metrics, and code versions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With collaborative coding tools, DagsHub provides a central location for data
    science teams to visualize, compare, and review their experiments, eliminating
    the need to set up any infrastructure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DagsHub offers two distinct ways to track experiments: via MLflow and Git.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the MLflow implementation, the remote setup is done for us, eliminating
    the need to store experiment data locally or host the server ourselves. Additionally,
    the implementation already comes with team-based access and security protocols.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Git implementation relies on simple, transparent, and open file formats.
    Thanks to the DagsHub Logger, it is incredibly easy to adapt to any language or
    framework and export the tracked metrics with a simple Git push.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Git integration means that experiments are automatically reproducible and
    linked to their code, data, pipelines, and models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Certain frameworks are supported by DagsHub Logger’s auto-logging feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DagsHub detects and supports DVC’s `metrics`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and `params` file formats, and it also sets up a DVC remote where we can version
    our data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It is possible to use DagsHub with your own data storage or use a virtual private
    cloud/on-prem solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Free to use for open-source and personal projects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keep in mind**'
  prefs: []
  type: TYPE_NORMAL
- en: Auto-logging capabilities of DagsHub Logger are currently limited to a few frameworks,
    including PyTorch Lightning, Keras, and [fast.ai v2](http://fast.ai/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is not possible to create advanced or custom visualizations at this time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For organizations, DagsHub is a commercial platform and certain features require
    a paid subscription.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Currently, there is no support for live logging using the logger approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though we have already covered 7 different experiment tracking tools/frameworks,
    that definitely does not exhaust all the available options. For the sake of brevity,
    we will only mention some other alternatives, which you can research on your own.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other experiment tracking (and not only) tools available:'
  prefs: []
  type: TYPE_NORMAL
- en: Neptune
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sacred
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guild.ai
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polyaxon
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Valohai
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubeflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verta AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Sagemaker Studio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pachyderm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wrapping up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Due to choice overload, choosing the right ML experiment tracking tool for
    your project or your entire team can be a daunting task. When making this decision,
    you have to consider many factors such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Open-source vs. commercial
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether the tool comes with a web UI or is console-based
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrations with ML/DL frameworks, cloud platforms, and programming languages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What exactly is tracked and how easy it is to add custom things to log
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage — cloud-based storage or local storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualization features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stability and scalability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether the tool facilitates collaboration between team members
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether the tool requires setting up a remote server by the user
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security and team-based access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether the tool offers additional features related to the ML lifecycle, for
    example, deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use the following table as a reference to quickly compare the tools
    we have covered today. Hopefully, it will help you make a decision for your next
    project!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ac72739982125d7da79903162092033.png)'
  prefs: []
  type: TYPE_IMG
- en: As always, any constructive feedback is more than welcome. You can reach out
    to me on [Twitter](https://twitter.com/erykml1) or in the comments.
  prefs: []
  type: TYPE_NORMAL
- en: '*Liked the article? Become a Medium member to continue learning by reading
    without limits. If you use* [*this link*](https://eryk-lewinson.medium.com/membership)
    *to become a member, you will support me at no extra cost to you. Thanks in advance
    and see you around!*'
  prefs: []
  type: TYPE_NORMAL
- en: 'You might also be interested in one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/geekculture/top-10-vs-code-extensions-for-data-science-ce3e24e24347?source=post_page-----9f0192543feb--------------------------------)
    [## Top 10 VS Code Extensions for Data Science'
  prefs: []
  type: TYPE_NORMAL
- en: Enhance Your Productivity with These Must-Have Tools!
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/geekculture/top-10-vs-code-extensions-for-data-science-ce3e24e24347?source=post_page-----9f0192543feb--------------------------------)
    [](/enhance-your-ml-experimentation-workflow-with-real-time-plots-434106b1a1c2?source=post_page-----9f0192543feb--------------------------------)
    [## Enhance Your ML Experimentation Workflow with Real-Time Plots
  prefs: []
  type: TYPE_NORMAL
- en: Part 2 of the tutorial on how to run and evaluate experiments without leaving
    your IDE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/enhance-your-ml-experimentation-workflow-with-real-time-plots-434106b1a1c2?source=post_page-----9f0192543feb--------------------------------)
    [](https://eryk-lewinson.medium.com/introducing-the-second-edition-of-python-for-finance-cookbook-f42f59c8acd0?source=post_page-----9f0192543feb--------------------------------)
    [## Introducing the second edition of Python for Finance Cookbook
  prefs: []
  type: TYPE_NORMAL
- en: What led me to write a second edition and what you can expect from reading it
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: eryk-lewinson.medium.com](https://eryk-lewinson.medium.com/introducing-the-second-edition-of-python-for-finance-cookbook-f42f59c8acd0?source=post_page-----9f0192543feb--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: All images, unless noted otherwise, are by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The article appeared originally on [DagsHub blog](https://dagshub.com/blog/best-8-experiment-tracking-tools-for-machine-learning-2023/).
  prefs: []
  type: TYPE_NORMAL
