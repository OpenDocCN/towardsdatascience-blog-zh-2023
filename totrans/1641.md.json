["```py\n# Load the Wine dataset\nfrom sklearn.datasets import load_wine\n\nwine = load_wine()\nX = wine.data\ny = wine.target\n\nprint(\"Wine dataset size:\", X.shape)\n```", "```py\n# Feature scaling\nfrom sklearn.preprocessing import StandardScaler\n\nX_scaled = StandardScaler().fit_transform(X)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Select the best number of components\n# by running PCA with all components\npca = PCA(n_components=None)\npca.fit(X_scaled)\n\n# Plot cumulative explained variances\nexp_var = pca.explained_variance_ratio_ * 100\ncum_exp_var = np.cumsum(exp_var)\n\nplt.bar(range(1, 14), exp_var, align='center',\n        label='Individual explained variance')\n\nplt.step(range(1, 14), cum_exp_var, where='mid',\n         label='Cumulative explained variance', color='red')\n\nplt.ylabel('Explained variance percentage')\nplt.xlabel('Principal component index')\nplt.xticks(ticks=list(range(1, 14)))\nplt.legend(loc='best')\nplt.tight_layout()\n\nplt.savefig(\"cumulative_explained_variance_plot.png\")\n```", "```py\n# Run PCA again with selected (7) components\npca = PCA(n_components=7)\nX_pca = pca.fit_transform(X_scaled)\nprint(\"PCA reduced wine dataset size:\", X_pca.shape)\n```", "```py\nimport numpy as np\nfrom tensorflow.keras import Model, Input\nfrom tensorflow.keras.layers import Dense\n\n# Build the autoencoder\ninput_dim = X.shape[1]\nlatent_vec_dim = 7\n\ninput_layer = Input(shape=(input_dim,))\n\n# Define encoder\nx = Dense(8, activation='relu')(input_layer)\nx = Dense(4, activation='relu')(x)\nencoder = Dense(latent_vec_dim, activation=\"tanh\")(x)\n\n# Define decoder\nx = Dense(4, activation='relu')(encoder)\nx = Dense(8, activation='relu')(x)\ndecoder = Dense(input_dim, activation=\"sigmoid\")(x)\n\nautoencoder = Model(inputs=input_layer, outputs=decoder)\n\n# Compile the model with optimizer and loss function\nautoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n\n# Train the model with 100 epochs\nautoencoder.fit(X_scaled, X_scaled, epochs=100, verbose=0,\n                batch_size=16, shuffle=True)\n\n# Use the encoder part to obtain the lower dimensional,\n# encoded representation of the data\nencoder_model = Model(inputs=input_layer, outputs=encoder)\nX_encoded = encoder_model.predict(X_scaled)\n\n# Print the shape of the encoded data\nprint(\"Autoencoder reduced wine dataset size:\", X_encoded.shape)\n```", "```py\nimport matplotlib.pyplot as plt\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n# Plot PCA output\nax1.scatter(X_pca[:, 0], X_pca[:, 1], c=y, s=25, cmap='plasma')\nax1.set_title('PCA')\nax1.set_xlabel('Component 1')\nax1.set_ylabel('Component 2')\n\n# Plot autoencoder output\nax2.scatter(X_encoded[:, 0], X_encoded[:, 1], c=y, s=25, cmap='plasma')\nax2.set_title('Autoencoder')\nax2.set_xlabel('Dimension 1')\nax2.set_ylabel('Dimension 2')\n\nplt.savefig(\"Output.png\")\n```"]