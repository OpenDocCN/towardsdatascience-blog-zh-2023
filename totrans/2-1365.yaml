- en: Is F1-Score Really Better than Accuracy?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: F1分数真的比准确率更好吗？
- en: 原文：[https://towardsdatascience.com/is-f1-score-really-better-than-accuracy-5f87be75ae01](https://towardsdatascience.com/is-f1-score-really-better-than-accuracy-5f87be75ae01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/is-f1-score-really-better-than-accuracy-5f87be75ae01](https://towardsdatascience.com/is-f1-score-really-better-than-accuracy-5f87be75ae01)
- en: What’s the cost of being wrong (and the gain of being right) according to different
    metrics
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 根据不同指标，错误（和正确）的成本是多少？
- en: '[](https://medium.com/@mazzanti.sam?source=post_page-----5f87be75ae01--------------------------------)[![Samuele
    Mazzanti](../Images/432477d6418a3f79bf25dec42755d364.png)](https://medium.com/@mazzanti.sam?source=post_page-----5f87be75ae01--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5f87be75ae01--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5f87be75ae01--------------------------------)
    [Samuele Mazzanti](https://medium.com/@mazzanti.sam?source=post_page-----5f87be75ae01--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mazzanti.sam?source=post_page-----5f87be75ae01--------------------------------)[![Samuele
    Mazzanti](../Images/432477d6418a3f79bf25dec42755d364.png)](https://medium.com/@mazzanti.sam?source=post_page-----5f87be75ae01--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5f87be75ae01--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5f87be75ae01--------------------------------)
    [Samuele Mazzanti](https://medium.com/@mazzanti.sam?source=post_page-----5f87be75ae01--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5f87be75ae01--------------------------------)
    ·10 min read·Apr 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5f87be75ae01--------------------------------)
    ·10分钟阅读·2023年4月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a09fe8380a1e7a9f6979a654ee98ace5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a09fe8380a1e7a9f6979a654ee98ace5.png)'
- en: '[Image by Author]'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[图片来源：作者]'
- en: 'If you google “What metric is better, accuracy or F1-score?” you will probably
    find an answer like:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你搜索“哪个指标更好，准确率还是F1分数？”，你可能会找到类似的答案：
- en: “If your dataset is unbalanced, forget accuracy and go for F1-score.”
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “如果你的数据集不平衡，忘记准确率，选择F1分数。”
- en: If you look for explanations, you will probably find a vague reference to the
    “different costs” associated with false negatives and false positives. But the
    sources do not state clearly which these costs are in the different metrics.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你寻找解释，可能会发现对“假阴性和假阳性相关的‘不同成本’”的模糊提及。但这些来源并没有明确说明不同指标中这些成本是什么。
- en: 'This is why I felt the need to write this article: to try and answer the following
    questions. What is the cost of false negatives and false positives in accuracy?
    And in F1-score? Is there a metric that allows us to assign custom values to such
    costs?'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我觉得需要写这篇文章的原因：尝试回答以下问题。误报和漏报的成本在准确率中是什么？在F1分数中又是什么？是否有某种指标可以让我们为这些成本分配自定义的值？
- en: Starting from confusion
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从混淆矩阵开始
- en: Both accuracy and F1-score can be calculated from the so-called **confusion
    matrix**. A confusion matrix is a square matrix comparing the labels predicted
    by our model with the true labels.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率和F1分数都可以从所谓的**混淆矩阵**中计算得出。混淆矩阵是一个方阵，用于比较我们模型预测的标签与真实标签。
- en: 'This is an example of a confusion matrix in the case of two labels: 0 (a.k.a.
    negatives) and 1 (a.k.a. positives). Note that, in this article, we will stick
    to binary classification, but everything we say can be generalized to the multiclass
    case.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个包含两个标签：0（即负类）和1（即正类）的混淆矩阵示例。请注意，在本文中，我们将专注于二分类，但我们所说的内容可以推广到多分类情况。
- en: '![](../Images/624a6f85dbff501b72b18ed7511fe32a.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/624a6f85dbff501b72b18ed7511fe32a.png)'
- en: Confusion matrix of a binary classifier. The elements of the matrix are true
    negatives (TN), false positives (FP), false negatives (FN), true positives (TP).
    [Image by Author]
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类器的混淆矩阵。矩阵的元素包括真阴性（TN）、假阳性（FP）、假阴性（FN）和真阳性（TP）。 [图片来源：作者]
- en: 'As a result of the comparison, our observations may fall into these 4 cases:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于比较，我们的观察结果可能会落入以下4种情况：
- en: 'True negatives (TN): our model correctly predicts 0.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真阴性（TN）：我们的模型正确预测了0。
- en: 'False positives (FP): our model predicts 1, but the true label is 0.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假阳性（FP）：我们的模型预测为1，但真实标签是0。
- en: 'False negatives (FN): our model predicts 0, but the true label is 1.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假阴性（FN）：我们的模型预测为0，但真实标签是1。
- en: 'True positives (TP): our model correctly predicts 1.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真阳性（TP）：我们的模型正确预测了1。
- en: 'In Python, the simplest way to obtain the confusion matrix of a classifier
    is through Scikit-learn’s `confusion_matrix`. This function has two mandatory
    arguments: `y_true` (array of true values) and `y_pred` (array of predictions).
    In this case, I will also set the optional argument `normalize="all"` to get the
    relative count of observations, rather than the absolute count.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，获取分类器的混淆矩阵最简单的方法是通过 Scikit-learn 的 `confusion_matrix`。这个函数有两个必需的参数：`y_true`（真实值数组）和
    `y_pred`（预测值数组）。在这种情况下，我还将设置可选参数 `normalize="all"` 以获取观察值的相对计数，而不是绝对计数。
- en: 'I have taken a dataset with 20% of positives and trained a model on it. This
    is the confusion matrix on the test set:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了一个包含 20% 正类的数据集，并在其上训练了一个模型。这是测试集上的混淆矩阵：
- en: '![](../Images/f50f34698c75e0b0e2b077a7be237294.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f50f34698c75e0b0e2b077a7be237294.png)'
- en: Computing confusion matrix with Scikit-learn. [Image by Author]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Scikit-learn 计算混淆矩阵。[图片由作者提供]
- en: 'Here is a more friendly visualization of this outcome:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个更友好的结果可视化：
- en: '![](../Images/82b65d83efd6013a52fb8f2983c21364.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82b65d83efd6013a52fb8f2983c21364.png)'
- en: A confusion matrix. [Image by Author]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵。[图片由作者提供]
- en: 'Let’s read it together:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一起来阅读一下：
- en: 'True negatives (TN): 78% of the test set has been correctly labeled as 0.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真负类（TN）：78% 的测试集被正确标记为 0。
- en: 'False positives (FP): 2% of the test set has been mistakenly labeled as 1.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假正类（FP）：2% 的测试集被错误标记为 1。
- en: 'False negatives (FN): 9% of the test set has been mistakenly labeled as 0.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假负类（FN）：9% 的测试集被错误标记为 0。
- en: 'True positives (TP): 11% of the test set has been correctly labeled as 1.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真正类（TP）：11% 的测试集被正确标记为 1。
- en: The confusion matrix gives us all the information we may need about our classifier.
    However, we usually want a single metric that is able to summarize the performance
    of the model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵为我们提供了关于分类器的所有信息。然而，我们通常需要一个能够总结模型性能的单一指标。
- en: 'Two of the most popular metrics are accuracy and F1-score. Let’s see how they
    can be computed starting from the confusion matrix:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 两个最受欢迎的指标是准确率和 F1-score。我们来看看如何从混淆矩阵中计算这两个指标：
- en: '**Accuracy: percentage of items classified correctly**. It can be computed
    as the percentage of observations that are either true negatives or true positives.
    Having the confusion matrix, this is simply the sum of the elements on the principal
    diagonal, in this case: 78% + 11% = 89%.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率: 正确分类的项目百分比**。它可以通过正确分类的观察值百分比来计算。拥有混淆矩阵后，这就是主对角线上的元素之和，在本例中为：78% + 11%
    = 89%。'
- en: '**F1-score: harmonic mean of precision and recall (of the positive class).**
    This metric is a bit less intuitive than accuracy. Since F1 is based on precision
    and recall, let’s see how to compute these two metrics first. Precision is the
    percentage of predicted positives that are correctly classified, so `precision
    = TP/(FP+TP) = 11%/(2%+11%) = 85%`. Recall is the percentage of actual positives
    that are correctly classified, so `recall = TP/(FN+TP) = 11%/(9%+11%) = 55%`.
    Once we have precision and recall, F1 is the harmonic mean of these two quantities
    so: `f1_score = (2*precision*recall)/(precision+recall) = (2*85%*55%)/(85%+55%)
    = 67%`.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F1-score: 精确度和召回率（正类）的调和均值**。这个指标相比准确率稍显不直观。由于 F1 是基于精确度和召回率的，我们先来看一下如何计算这两个指标。精确度是指被正确分类的预测正类的百分比，所以
    `precision = TP/(FP+TP) = 11%/(2%+11%) = 85%`。召回率是指被正确分类的实际正类的百分比，所以 `recall =
    TP/(FN+TP) = 11%/(9%+11%) = 55%`。得到精确度和召回率后，F1 是这两个数量的调和均值，所以：`f1_score = (2*precision*recall)/(precision+recall)
    = (2*85%*55%)/(85%+55%) = 67%`。'
- en: Now that we know what accuracy and F1-score are, let’s move on.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了准确率和 F1-score，我们继续前进。
- en: From one matrix to many matrices
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从一个矩阵到多个矩阵
- en: You may not have noticed it, but the confusion matrix we have seen above comes
    from an arbitrary choice.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能没有注意到，但我们上面看到的混淆矩阵来源于一个任意选择。
- en: Indeed, the output of a binary classifier — in its rawest form — is not an array
    of labels, but rather an array of probabilities. For each observation, the classifier
    outputs the probability that it belongs to the positive class. To move from the
    probability to the label, we need to set a probability threshold.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，二分类器的输出——最原始的形式——不是标签数组，而是概率数组。对于每个观察值，分类器输出它属于正类的概率。要从概率转到标签，我们需要设置一个概率阈值。
- en: 'Usually, the default threshold is set to 50%: any observation above 50% will
    be classified as 1, whereas any observation below that level will be classified
    as 0.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，默认的阈值设置为 50%：任何高于 50% 的观察值将被分类为 1，而低于该水平的观察值将被分类为 0。
- en: However, depending on the specific application, it may be preferable to use
    a different threshold. Let’s see how the confusion matrix would change based on
    the threshold that we set.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，取决于具体应用，可能更适合使用不同的阈值。让我们看看根据我们设置的阈值混淆矩阵会如何变化。
- en: '![](../Images/be51f125e6bbd480a9fe06e4a7bb4899.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be51f125e6bbd480a9fe06e4a7bb4899.png)'
- en: Confusion matrices of a binary classifier, depending on the probability threshold.
    [Image by Author]
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类器的混淆矩阵，取决于概率阈值。[图片来源：作者]
- en: 'Of course, as the threshold increases, we label fewer and fewer observations
    as positives: this is why the values under column “1” get smaller and smaller.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，随着阈值的增加，我们标记为正例的观察结果越来越少：这就是为什么“1”列下的值越来越小。
- en: Here, we have seen 6 different thresholds as an example. However, the possible
    thresholds are often thousands. In these cases, it is more convenient to draw
    a plot. However, since the probability threshold doesn’t say much, I find it more
    convenient to put on the *x*-axis the % of observations that are predicted as
    positives, rather than the threshold itself.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到了6个不同的阈值作为示例。然而，可能的阈值通常有成千上万。在这些情况下，绘制图表更为方便。然而，由于概率阈值不太直观，我觉得在*X*轴上显示被预测为正例的观察百分比，而不是阈值本身，更为方便。
- en: '![](../Images/c057e50e95ad606c3f8e35bc4e16cc0f.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c057e50e95ad606c3f8e35bc4e16cc0f.png)'
- en: The four elements of the confusion matrix depending on the probability threshold
    (and so on the percentage of predicted positives). [Image by Author]
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 根据概率阈值（即预测正例的百分比），混淆矩阵的四个元素。[图片来源：作者]
- en: Also, for our purposes, it is probably more interesting to look directly at
    accuracy and F1-score rather than at the raw values of the confusion matrices.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，就我们的目的而言，直接查看准确率和F1分数可能比查看混淆矩阵的原始值更有趣。
- en: '![](../Images/a263f09e2805ade404632ca38df05b9e.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a263f09e2805ade404632ca38df05b9e.png)'
- en: Accuracy and F1-score, depending on the probability threshold (and so on the
    percentage of predicted positives). [Image by Author]
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率和F1分数，取决于概率阈值（即预测正例的百分比）。[图片来源：作者]
- en: Since both accuracy and F1-score are the-greater-the-better metrics, we would
    **choose the threshold such that the corresponding metric is at its highest value**.
    Thus, in the above plot, I have drawn a dashed line in correspondence with the
    maximum value of the respective metric.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于准确率和F1分数都是越大越好的指标，我们会**选择使得相应指标达到最高值的阈值**。因此，在上述图表中，我画了一条虚线，标示出相应指标的最大值。
- en: 'As you can see, different metrics would lead us to choose different thresholds.
    In particular:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，不同的指标会导致我们选择不同的阈值。特别是：
- en: accuracy would lead us to choose a higher probability threshold, such that 14%
    of the observations are classified as positives.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确率会使我们选择一个较高的概率阈值，以使14%的观察结果被分类为正例。
- en: F1-score would lead us to choose a smaller probability threshold, with more
    observations (19%) classified as positives.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F1分数会使我们选择一个较小的概率阈值，使更多的观察结果（19%）被分类为正例。
- en: So, which metric should we prefer?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们应该选择哪个指标呢？
- en: The cost of wrong predictions
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 错误预测的成本
- en: The common wisdom among practitioners is that F1-score must be preferred over
    accuracy, especially in unbalanced problems.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 实践者的普遍智慧是，在不平衡问题中，F1分数必须优于准确率。
- en: However, if you look for the reason why F1-score is supposedly superior to accuracy,
    you will find vague explanations. Many of them revolve around the “different costs”
    associated with false negatives and false positives. But they never clearly quantify
    these costs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你去寻找为什么F1分数被认为优于准确率的原因，你会发现解释模糊。许多解释围绕“不同成本”展开，这些成本与假阴性和假阳性有关。但它们从未明确量化这些成本。
- en: Moreover, focusing only on errors would give us a limited view of the problem.
    Indeed, **if wrong predictions certainly carry a cost, then right predictions
    bring necessarily a profit**.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，仅仅关注错误会使我们对问题的理解变得有限。事实上，**如果错误预测确实会带来成本，那么正确预测必然会带来收益**。
- en: So, we should take into account not only the cost of false positives and false
    negatives but also the value of true positives and true negatives. From this idea,
    we can derive a new matrix — which I will call the “value matrix” — that assigns
    a (possibly economic) value to each element of the confusion matrix.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们应该考虑不仅是假阳性和假阴性的成本，还要考虑真正阳性和真正阴性的价值。从这个想法出发，我们可以得出一个新的矩阵——我称之为“价值矩阵”——它为混淆矩阵的每个元素分配一个（可能是经济的）价值。
- en: What’s the point of this matrix?
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵有什么意义？
- en: Well, since it assigns a value to each quantity of the confusion matrix, it
    is pretty natural to multiply these two matrices (element-wise) to obtain the
    expected value of any group (TN, FP, FN, TP).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，由于它为混淆矩阵中的每个量分配了一个值，因此将这两个矩阵（逐元素）相乘以获得任何组（TN、FP、FN、TP）的预期值是很自然的。
- en: This is an expected value because the confusion matrix contains the percentage
    of observations that belong to any group. Being a percentage, this can be interpreted
    as a probability. So, **multiplying this probability by a (possibly economic)
    value gives us an expected value**.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个预期值，因为混淆矩阵包含属于任何组的观察百分比。作为百分比，这可以被解释为概率。因此，**将这个概率乘以（可能是经济的）值给我们一个预期值**。
- en: Summing the expected values of true negatives, false positives, false negatives
    and true positives gives us the expected value of the whole model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 将真阴性、假阳性、假阴性和真正性预期值相加，给出整个模型的预期值。
- en: '![](../Images/b5dfbd3165f161ebb5d175bb71cb2c13.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5dfbd3165f161ebb5d175bb71cb2c13.png)'
- en: The expected value of a binary classifier. The confusion matrix contains the
    frequency of each element. The value matrix contains the value (profit or loss)
    of each element. The element product of the two matrices yields the expected value
    of each component. Finally, summing them we obtain the overall expected value
    of the model. [Image by Author]
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类器的预期值。混淆矩阵包含每个元素的频率。值矩阵包含每个元素的值（利润或损失）。两个矩阵的元素乘积得出每个组件的预期值。最后，我们将它们相加，得到模型的整体预期值。[图像来源：作者]
- en: The decomposition into confusion matrix and value matrix is very intuitive also
    for non-technical stakeholders. After all, everyone can understand percentages
    and dollars!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对混淆矩阵和值矩阵的分解对非技术利益相关者也非常直观。毕竟，每个人都能理解百分比和美元！
- en: 'At this point, we are ready to answer the initial question: **what are the
    value matrices of accuracy and F1-score?**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 到此，我们准备回答最初的问题：**准确率和F1-score的值矩阵是什么？**
- en: Finding the value matrix of accuracy
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查找准确率的值矩阵
- en: We have already seen that accuracy simply sums the values on the principal diagonal
    of the confusion matrix. Thus, it is straightforward to prove that **the value
    matrix of accuracy is the identity matrix**.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，准确率只是将混淆矩阵主对角线上的值相加。因此，直接证明**准确率的值矩阵是单位矩阵**是很简单的。
- en: '![](../Images/1b1e6fccbbfbe2409247e5ed5e730749.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b1e6fccbbfbe2409247e5ed5e730749.png)'
- en: Accuracy as the expected value of a classifier. [Image by Author]
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率作为分类器的预期值。[图像来源：作者]
- en: 'The shortcoming of accuracy should be immediately apparent: it is equivalent
    to assuming that we gain 1 in the case of right predictions (true negatives or
    true positives) and we don’t lose anything in case of wrong predictions. **This
    assumption is at the very least unlikely for most use cases**.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率的缺陷应该是显而易见的：它等同于假设在正确预测（真正性或假阴性）情况下我们获得1，而在错误预测情况下我们不损失任何东西。**这种假设对于大多数用例至少是不太可能的**。
- en: Finding the value matrix of F1-score
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查找F1-score的值矩阵
- en: So what are the values associated with right/wrong answers in F1-score?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，在F1-score中，与正确/错误答案相关的值是什么？
- en: 'In practice, we would like to find out which elements of the value matrix will
    give us the resulting F1-score. This corresponds to filling in the question marks
    in the following figure:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们希望找出值矩阵的哪些元素会给我们得到的F1-score。这对应于填补以下图中的问号：
- en: '![](../Images/ebfaefa4e1d661f6c7b52b7b7cf2bcfa.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ebfaefa4e1d661f6c7b52b7b7cf2bcfa.png)'
- en: F1-score as the expected value of a classifier. [Image by Author]
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: F1-score作为分类器的预期值。[图像来源：作者]
- en: 'Unfortunately for us, the answer is not as simple as it was for accuracy. In
    fact, in this case, there is no closed-form solution. However, we can exploit
    the information we have extracted from the different thresholds:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，对于我们来说，答案不像准确率那样简单。事实上，在这种情况下，没有闭式解。然而，我们可以利用从不同阈值中提取的信息：
- en: '![](../Images/85c2d844b3d426f568e926077a154127.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85c2d844b3d426f568e926077a154127.png)'
- en: F1-score for different confusion matrices (based on different probability thresholds).
    [Image by Author]
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 不同混淆矩阵的F1-score（基于不同的概率阈值）。[图像来源：作者]
- en: We can use true negatives, false positives, false negatives and true positives
    as independent variables and the resulting F1-score as the dependent variable.
    It’s enough to fit a linear regression to get an estimate of the resulting coefficients
    (i.e. the numbers to fill our value matrix).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用真负样本、假阳性、假负样本和真阳性作为自变量，将结果F1分数作为因变量。只需进行线性回归即可获得结果系数的估计值（即填充值矩阵的数字）。
- en: 'In this case, these are the values that we get:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们得到的值如下：
- en: '![](../Images/51092ab9668d4bebced1cd2c9dbecbbe.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51092ab9668d4bebced1cd2c9dbecbbe.png)'
- en: Value matrix (estimated through linear regression) for F1-score. [Image by Author]
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: F1分数的值矩阵（通过线性回归估计）。[作者图片]
- en: It is clear from here that **F1-score puts more stress on positive observations
    (whether they are classified as 0 or 1)**.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里可以看出，**F1分数更注重正样本（无论它们被分类为0还是1）**。
- en: Note that this is an approximate solution! Differently from accuracy, an exact
    solution for the value matrix does not exist here. If you are curious, this is
    our result (dashed line) compared to the actual curve of F1-score.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这是一个近似解！与准确率不同，这里不存在值矩阵的精确解。如果你感兴趣，这是我们的结果（虚线）与F1分数实际曲线的比较。
- en: '![](../Images/f5f6c77f8e3fc50a67a59575c53fa154.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5f6c77f8e3fc50a67a59575c53fa154.png)'
- en: Actual F1-score vs. our estimated curve. [Image by Author]
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 实际F1分数与我们估计的曲线。[作者图片]
- en: Beyond accuracy and F1-score
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越准确率和F1分数
- en: We have seen the value matrices of accuracy and F1-score. The former is arbitrary
    and the latter cannot be even calculated directly (it will be an approximate solution
    anyway).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过准确率和F1分数的值矩阵。前者是任意的，而后者甚至不能直接计算（无论如何都会是近似解）。
- en: So, why not just set our custom value matrix?
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 那么，为什么不直接设置我们的自定义值矩阵呢？
- en: In fact, depending on your use case, it should be relatively easy to figure
    out how much a wrong (right) prediction costs (is worth) to you.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，根据你的使用案例，应该相对容易计算错误（正确）预测对你来说的成本（价值）。
- en: Let’s make an example. You want to predict which customers are more likely to
    churn. You know that a customer is worth on average $10 per month. In order to
    prevent customers to leave the company, you want to propose to them a discount
    that cuts your profit by $2 per customer (let’s here assume that the discount
    will be accepted by everyone).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子。你想预测哪些客户更可能流失。你知道一个客户平均每月价值$10。为了防止客户离开公司，你想给他们提供一个折扣，每个客户减少$2的利润（这里假设折扣会被所有人接受）。
- en: 'So your value matrix will be the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你的值矩阵将是如下：
- en: '![](../Images/22fb800de9dc5b447c70644d4f8eecfc.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22fb800de9dc5b447c70644d4f8eecfc.png)'
- en: Value matrix for churn example. [Image by Author]
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 流失示例的值矩阵。[作者图片]
- en: Why is that?
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会这样？
- en: 'True Negatives: $0\. Your prediction task will not affect them, so the value
    is 0.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真负样本：$0。你的预测任务不会影响它们，所以值为0。
- en: 'False Positives: -$2\. You are giving them a discount, but they wouldn’t churn
    anyway, so you are cutting your profit by $2 for nothing.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假阳性：-$2。你给了他们折扣，但他们本来也不会流失，因此你无故减少了$2的利润。
- en: 'False Negatives: $0\. Your prediction task will not affect them, so the value
    is 0.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假负样本：$0。你的预测任务不会影响它们，所以值为0。
- en: 'True Positives: +$8\. You correctly predicted that they would churn, so you
    save $8 ($10 minus $2 due to your action).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真阳性：+$8。你正确预测了他们会流失，因此你节省了$8（$10减去由于你的行为产生的$2）。
- en: The point is that the characteristics of the value matrix depend entirely on
    your specific use case.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，值矩阵的特性完全取决于你的具体使用案例。
- en: For instance, let’s set some example value matrices, and see how the respective
    expected value would change.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们设置一些示例值矩阵，并观察相应的期望值如何变化。
- en: '![](../Images/bc78e8b0bb75c64d6bfcaf93c0d61f55.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc78e8b0bb75c64d6bfcaf93c0d61f55.png)'
- en: Some examples of how the expected value of a classifier changes according to
    the value matrix. [Image by Author]
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一些示例说明了分类器的期望值如何根据值矩阵变化。[作者图片]
- en: 'We can repeat this process for all the possible thresholds:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对所有可能的阈值重复这个过程：
- en: '![](../Images/5d4e729e327fa8aa4ace7679c6f7f6b9.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d4e729e327fa8aa4ace7679c6f7f6b9.png)'
- en: The expected value of a classifier, depending on the value associated with right/wrong
    predictions. [Image by Author]
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的期望值，取决于正确/错误预测相关的值。[作者图片]
- en: Differently from accuracy and F1-scores, these custom KPIs may assume negative
    values. This makes a lot of sense! In fact, in the real world, there are scenarios
    in which your classifier can make you lose money. For sure, you want to be aware
    of that possibility in advance, and this custom metric allows you to do that.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 与准确率和F1分数不同，这些自定义KPI可能会取负值。这是很有意义的！事实上，在现实世界中，有些情况可能导致你的分类器让你损失金钱。你肯定希望提前知道这种可能性，而这个自定义指标可以让你做到这一点。
- en: 'It’s interesting to observe the behavior of the three curves. Other things
    being equal, a higher cost of false negatives and a lower cost of false positives
    (i.e. moving from the black to the green line) incentivizes us to predict more
    observations as positives (i.e. from 16% to 39%). It’s intuitive: a high cost
    for false negatives is an incentive to classify fewer observations as negatives,
    and vice-versa.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 观察三条曲线的行为很有趣。其他条件相同时，假阴性的高成本和假阳性的低成本（即从黑线移动到绿线）会激励我们将更多观察值预测为正例（即从16%增加到39%）。这很直观：假阴性的高成本会激励我们将更少的观察值分类为负例，反之亦然。
- en: In general, using a custom value matrix allows you to place yourself in the
    perfect spot in the precision/recall trade-off.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，使用自定义价值矩阵可以让你在精准率/召回率的权衡中找到最佳位置。
- en: Conclusions
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we have compared accuracy and F1-score, based on the different
    costs that they attribute to errors.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们比较了准确率和F1分数，基于它们对错误的不同成本分配。
- en: 'Accuracy is a bit naive since it attributes a value of 1 to correct predictions
    and a null cost to errors. On the other hand, F1-score is more like a black box:
    you will always need to reverse-engineer it to get its value matrix (and, in any
    case, it will be an approximate solution).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率有点过于简单，因为它给正确预测赋值1，而对错误赋值为零。另一方面，F1分数更像是一个黑箱：你总是需要反向工程来获得其价值矩阵（无论如何，这将是一个近似解）。
- en: My advice is to use a custom value matrix, depending on your specific application.
    Once you have set the value matrix, you can multiply it by your confusion matrix
    and get the expected value of your classifier. This is the only way to get an
    idea of the actual economic impact of using your classifier out there in the real
    world.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我的建议是使用自定义价值矩阵，这取决于你的具体应用。一旦你设置了价值矩阵，你可以将其与混淆矩阵相乘，得到分类器的期望值。这是唯一能让你了解在现实世界中使用你的分类器的实际经济影响的方法。
- en: '*Thank you for reading!*'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢阅读！*'
- en: '*If you find my work useful, you can subscribe to* [***get an email every time
    that I publish a new article***](https://medium.com/@mazzanti.sam/subscribe) *(usually
    once a month).*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你觉得我的工作有用，你可以订阅* [***每次我发布新文章时获得电子邮件通知***](https://medium.com/@mazzanti.sam/subscribe)
    *(通常每月一次)。*'
- en: '*If you want to support my work, you can* [***buy me a coffee***](https://ko-fi.com/samuelemazzanti)*.*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你想支持我的工作，你可以* [***请我喝咖啡***](https://ko-fi.com/samuelemazzanti)*。*'
- en: '*If you’d like,* [***add me on Linkedin***](https://www.linkedin.com/in/samuelemazzanti/)*!*'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你愿意，* [***加我LinkedIn***](https://www.linkedin.com/in/samuelemazzanti/)*！*'
