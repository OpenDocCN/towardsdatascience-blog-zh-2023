# 自然语言处理初学者指南

> 原文：[https://towardsdatascience.com/natural-language-processing-for-absolute-beginners-a195549a3164](https://towardsdatascience.com/natural-language-processing-for-absolute-beginners-a195549a3164)

## 用10行Python代码解决复杂的NLP任务

[](https://dmitryelj.medium.com/?source=post_page-----a195549a3164--------------------------------)[![Dmitrii Eliuseev](../Images/7c48f0c016930ead59ddb785eaf3e0e6.png)](https://dmitryelj.medium.com/?source=post_page-----a195549a3164--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a195549a3164--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a195549a3164--------------------------------) [Dmitrii Eliuseev](https://dmitryelj.medium.com/?source=post_page-----a195549a3164--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a195549a3164--------------------------------) ·阅读时间9分钟·2023年9月2日

--

![](../Images/bc90b1586db7d9db3689cfb34615b58a.png)

作者提供的图片（使用Craiyon生成）

NLP（自然语言处理）通常被认为是计算机科学中的复杂领域。像SpaCy或NLTK这样的框架体积庞大，通常需要一些学习。但借助开源的大型语言模型（LLMs）和现代Python库，许多任务可以更容易地解决。而且，甚至在几年前只有科学论文中才能找到的结果，现在也可以通过仅仅10行Python代码实现。

话不多说，我们开始吧。

## 1. 语言翻译

你是否曾想过Google翻译是如何工作的？Google [使用](https://blog.research.google/2020/06/recent-advances-in-google-translate.html)一个在大量文本上训练的深度学习模型。现在，借助 [Transformers库](https://huggingface.co/docs/transformers/index)，不仅在Google实验室，还能在普通PC上完成这项工作。在此示例中，我将使用一个预训练的 [T5-base](https://huggingface.co/t5-base)（文本到文本转换器）模型。该模型最初在原始文本数据上训练，然后在诸如（“将英语翻译成德语：这所房子很棒”，“Das Haus ist Wunderbar”）的源-目标对上微调。在这里，“将英语翻译成德语”是一个前缀，告诉模型该做什么，而短语是模型应该学习的实际上下文。

> **重要警告**。大型语言模型的体积确实很大。此示例中使用的 *T5ForConditionalGeneration* 类将自动下载约900 MB大小的“t5-base”模型。在运行代码之前，请确保有足够的磁盘空间，并且流量没有限制。

可以在Python中使用预训练的T5模型：

[PRE0]

在这里，一个 *T5Tokenizer* 类将源字符串转换为数字形式；这个过程叫做 *分词*。在我们的例子中，一个“*将英语翻译成德语：天气很好*”的文本将被转换为 *[13959, 1566, 12, 2968, 10, 8, 1969, 19, 207, 1]* 数组。“*生成*”方法实际执行任务，最后，分词器进行反向转换。作为输出，我们将得到结果“Das Wetter ist gut”。

我们能否使这段代码更简洁？实际上，我们可以。借助 [Transformer 的 *pipeline*](https://huggingface.co/docs/transformers/main_classes/pipelines) 类，我们可以创建一个抽象管道，只需 2 行 Python 代码即可完成这个任务：

[PRE1]

出于自学的目的，我通常更喜欢第一种方法，因为它更容易理解“幕后”的情况。但对于“生产”目的，第二种方法更具灵活性，它还允许在不更改代码的情况下使用不同的模型。

## 2\. 摘要生成

文本摘要的目标是将文档转换为简短版本，这显然如果手动完成会花费时间。令人惊讶的是，T5 模型也可以做到这一点；我们唯一需要更改的是前缀：

[PRE2]

正如我们所见，结果非常准确。

与第一个例子一样，使用管道可以为相同的任务生成更简短的代码：

[PRE3]

读者可能对使用 “t5-base” 模型可以完成的其他任务感兴趣。我们可以轻松地将它们全部打印出来：

[PRE4]

## 3\. 问答

大型语言模型还可以提供另一种有趣的功能，即在给定的上下文中回答问题。我将使用与之前示例相同的文本：

[PRE5]

在这种情况下，我使用了一个 [distilbert-base-cased-distilled-squad](https://huggingface.co/distilbert-base-cased-distilled-squad) 模型，模型大小为 261 MB。正如我们所见，该模型不仅提供了答案，还从原始文本中检索了位置，这对验证结果很有帮助。

## 4\. 语言生成

另一个有趣的过程是语言生成。对于这个例子，我将使用一个 [GPT-2 模型](https://huggingface.co/gpt2)。这显然不是我们今天拥有的最新 GPT 模型，但 GPT-2 是免费提供的，并且其文件大小为 548 MB，足够在普通 PC 上运行。

让我们看看它是如何工作的：

[PRE6]

同样，可以使用更简短的代码通过管道完成：

[PRE7]

实际上，我在这些文本中看不到太多意义，但从语法角度来看，它们足够好，对于某种自动化或单元测试，它们可能是有用的。有趣的是，GPT-2 模型于 2019 年发布。为了好玩，我向 GPT-3.5（发布于 2022 年）提出了相同的问题，“请继续我将要说的短语”，并得到了以下答案：

[PRE8]

这个结果要好得多；这三年取得了巨大的进步。但显然，即使GPT-3.5模型在今天发布于公共领域，也不可能在普通电脑上运行。

## 5\. 情感分析

之前的示例主要是为了娱乐，但情感分析对于商业来说更为重要。情感分析是分析给定文本中的情感并找出主观意见的过程。对于网络商店、流媒体平台和其他许多用户可以发布评论的服务，这可能特别重要。

对于这个测试，我将使用一个[distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)模型：

[PRE9]

我故意尝试使用不易分析的短语，但模型给出了正确的答案。显然，自然语言是非常灵活的，仍然可能出现导致错误结果的文本。例如，这个模型对短语“我期待它很糟糕，但我错了”给出了错误的回答。同时，（体积大得多的）GPT-3.5模型能够正确解析。另一方面，考虑到DistilBERT模型的大小仅为268 MB且可以免费使用（模型拥有Apache 2.0许可证），结果还是相当不错的。读者也可以尝试[其他开源模型](https://huggingface.co/models?pipeline_tag=text-classification)，并选择最适合他们需求的模型。

## 6\. 命名实体识别（NER）

自然语言处理的另一个有趣部分是“命名实体识别”。这是从非结构化文本中提取实体，如名称、地点、日期等的过程。对于这个测试，我将使用一个[bert-base-NER模型](https://huggingface.co/dslim/bert-base-NER)（文件大小为433 MB）。

让我们考虑一个例子：

[PRE10]

正如我们所见，该模型能够正确确定文本中提到的主要实体，如名称、地点和公司。

## 7\. 关键词提取

在最后一个例子中，我们测试了NER，但不是所有的参数都从文本中提取出来了。一个单独的关键词提取算法对于同一任务也可能很有用。对于这个例子，我将使用[KeyBERT](https://maartengr.github.io/KeyBERT/index.html)：

[PRE11]

正如我们所见，关键词提取作为NER的补充也很有用，它可以从相同的短语中提取一些额外的数据。

## 结论

在这篇文章中，我测试了不同的自然语言处理（NLP）算法。正如我在文章开头承诺的那样，通过现代库的帮助，复杂的任务可以在10行Python代码中解决。还要提到的是，这些代码都可以在本地运行，无需任何API订阅或密钥。最后但同样重要的是，我希望读者能看到NLP也可以很有趣。

感谢阅读。如果你喜欢这个故事，可以随时[订阅](https://medium.com/@dmitryelj/membership)Medium，你将会在我的新文章发布时收到通知，并且可以全面访问其他作者的成千上万篇故事。
