["```py\ndef transcribe(audio_file: str, model_name: str, device: str = \"cpu\") -> Dict[str, Any]:\n    \"\"\"\n    Transcribe an audio file using a speech-to-text model.\n\n    Args:\n        audio_file: Path to the audio file to transcribe.\n        model_name: Name of the model to use for transcription.\n        device: The device to use for inference (e.g., \"cpu\" or \"cuda\").\n\n    Returns:\n        A dictionary representing the transcript, including the segments, the language code, and the duration of the audio file.\n    \"\"\"\n    model = whisper.load_model(model_name, device)\n    result = model.transcribe(audio_file)\n\n    language_code = result[\"language\"]\n    return {\n        \"segments\": result[\"segments\"],\n        \"language_code\": language_code,\n    }\n```", "```py\ndef align_segments(\n    segments: List[Dict[str, Any]],\n    language_code: str,\n    audio_file: str,\n    device: str = \"cpu\",\n) -> Dict[str, Any]:\n    \"\"\"\n    Align the transcript segments using a pretrained alignment model.\n\n    Args:\n        segments: List of transcript segments to align.\n        language_code: Language code of the audio file.\n        audio_file: Path to the audio file containing the audio data.\n        device: The device to use for inference (e.g., \"cpu\" or \"cuda\").\n\n    Returns:\n        A dictionary representing the aligned transcript segments.\n    \"\"\"\n    model_a, metadata = load_align_model(language_code=language_code, device=device)\n    result_aligned = align(segments, model_a, metadata, audio_file, device)\n    return result_aligned\n```", "```py\ndef diarize(audio_file: str, hf_token: str) -> Dict[str, Any]:\n    \"\"\"\n    Perform speaker diarization on an audio file.\n\n    Args:\n        audio_file: Path to the audio file to diarize.\n        hf_token: Authentication token for accessing the Hugging Face API.\n\n    Returns:\n        A dictionary representing the diarized audio file, including the speaker embeddings and the number of speakers.\n    \"\"\"\n    diarization_pipeline = DiarizationPipeline(use_auth_token=hf_token)\n    diarization_result = diarization_pipeline(audio_file)\n    return diarization_result\n```", "```py\ndef assign_speakers(\n    diarization_result: Dict[str, Any], aligned_segments: Dict[str, Any]\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Assign speakers to each transcript segment based on the speaker diarization result.\n\n    Args:\n        diarization_result: Dictionary representing the diarized audio file, including the speaker embeddings and the number of speakers.\n        aligned_segments: Dictionary representing the aligned transcript segments.\n\n    Returns:\n        A list of dictionaries representing each segment of the transcript, including the start and end times, the\n        spoken text, and the speaker ID.\n    \"\"\"\n    result_segments, word_seg = assign_word_speakers(\n        diarization_result, aligned_segments[\"segments\"]\n    )\n    results_segments_w_speakers: List[Dict[str, Any]] = []\n    for result_segment in result_segments:\n        results_segments_w_speakers.append(\n            {\n                \"start\": result_segment[\"start\"],\n                \"end\": result_segment[\"end\"],\n                \"text\": result_segment[\"text\"],\n                \"speaker\": result_segment[\"speaker\"],\n            }\n        )\n    return results_segments_w_speakers\n```", "```py\ndef transcribe_and_diarize(\n    audio_file: str,\n    hf_token: str,\n    model_name: str,\n    device: str = \"cpu\",\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Transcribe an audio file and perform speaker diarization to determine which words were spoken by each speaker.\n\n    Args:\n        audio_file: Path to the audio file to transcribe and diarize.\n        hf_token: Authentication token for accessing the Hugging Face API.\n        model_name: Name of the model to use for transcription.\n        device: The device to use for inference (e.g., \"cpu\" or \"cuda\").\n\n    Returns:\n        A list of dictionaries representing each segment of the transcript, including the start and end times, the\n        spoken text, and the speaker ID.\n    \"\"\"\n    transcript = transcribe(audio_file, model_name, device)\n    aligned_segments = align_segments(\n        transcript[\"segments\"], transcript[\"language_code\"], audio_file, device\n    )\n    diarization_result = diarize(audio_file, hf_token)\n    results_segments_w_speakers = assign_speakers(diarization_result, aligned_segments)\n\n    # Print the results in a user-friendly way\n    for i, segment in enumerate(results_segments_w_speakers):\n        print(f\"Segment {i + 1}:\")\n        print(f\"Start time: {segment['start']:.2f}\")\n        print(f\"End time: {segment['end']:.2f}\")\n        print(f\"Speaker: {segment['speaker']}\")\n        print(f\"Transcript: {segment['text']}\")\n        print(\"\")\n\n    return results_segments_w_speakers\n```"]