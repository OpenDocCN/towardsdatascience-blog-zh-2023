- en: Hierarchical Transformers — part 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分层变换器 — 第2部分
- en: 原文：[https://towardsdatascience.com/hierarchical-transformers-part-2-2616eecacb21](https://towardsdatascience.com/hierarchical-transformers-part-2-2616eecacb21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/hierarchical-transformers-part-2-2616eecacb21](https://towardsdatascience.com/hierarchical-transformers-part-2-2616eecacb21)
- en: Hierarchical attention is faster
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分层注意力更快
- en: '[](https://medium.com/@mina.ghashami?source=post_page-----2616eecacb21--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----2616eecacb21--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2616eecacb21--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2616eecacb21--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----2616eecacb21--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mina.ghashami?source=post_page-----2616eecacb21--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----2616eecacb21--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2616eecacb21--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2616eecacb21--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----2616eecacb21--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2616eecacb21--------------------------------)
    ·6 min read·Oct 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [数据科学前沿](https://towardsdatascience.com/?source=post_page-----2616eecacb21--------------------------------)
    ·阅读时间6分钟·2023年10月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: This article requires you to have knowledge of standard transformers and how
    they work. If you are a beginner and you’d like to know about transformers, please
    take a look at [Transformer for Beginners](https://medium.com/p/4deaf9b199f9/edit)
    article.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章要求你具备标准变换器及其工作原理的知识。如果你是初学者，想了解变换器，请查看 [变换器入门](https://medium.com/p/4deaf9b199f9/edit)
    文章。
- en: In [Hierarchical Transformer — part 1](https://medium.com/towards-data-science/hierarchical-transformers-54f6d59fa8fc)
    we defined, what we mean by “hierarchical transformers”, and we reviewed one of
    prominent work in this domain which was called *Hourglass*.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [分层变换器 — 第1部分](https://medium.com/towards-data-science/hierarchical-transformers-54f6d59fa8fc)
    中，我们定义了“分层变换器”的含义，并回顾了该领域的一项重要工作，即 *Hourglass*。
- en: In this article, we will continue the line of work by looking into another well-known
    work called *Hierarchical Attention Transformers (HAT).*
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将继续探讨另一项著名的工作，即 *分层注意力变换器（HAT）。*
- en: Let’s get started.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Hierarchical Attention Transformer (HAT)
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分层注意力变换器（HAT）
- en: This method was initially proposed for classifying long documents, typically
    in length of thousands of words. A usecase of this is classifying legal documents
    or biomedical documents which are typically very long.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法最初是为分类长文档而提出的，通常长达数千个单词。一个应用案例是分类法律文档或生物医学文档，这些文档通常非常长。
- en: Tokenization and Segmentation
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词和分段
- en: The **HAT** method works by taking an input document, and tokenizing it using
    Byte-Pair Encoding (BPE) tokenizer that breaks text into subwords/tokens. This
    tokenizer is used in many well-known large language models such as BERT, RoBERTA
    and GPT family.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**HAT** 方法通过获取输入文档，并使用字节对编码（BPE）分词器将其拆分成子词/标记来工作。这个分词器被许多著名的大型语言模型使用，如 BERT、RoBERTA
    和 GPT 家族。'
- en: Then it splits the tokenized document into *N* equally-sized chunks; i.e. if
    *S* denote the input document then *S = [C1, …., CN]* are *N* equally-sized chunks.
    (Through out this article, we sometimes refer to chunks as segments, but they
    are the same concept.) Each chunk is a sequence of *k* tokens *Ci = [Wi[cls],
    Wi1…, Wik-1]* that the first token, *Wi[cls]*, is the *CLS* token which represents
    the chunk.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将分词后的文档拆分为 *N* 个大小相等的块；即如果 *S* 代表输入文档，则 *S = [C1, …., CN]* 是 *N* 个大小相等的块。（在整篇文章中，我们有时将块称为段，但它们是相同的概念。）每个块是一个由
    *k* 个标记组成的序列 *Ci = [Wi[cls], Wi1…, Wik-1]*，其中第一个标记 *Wi[cls]* 是 *CLS* 标记，代表该块。
- en: '![](../Images/e8e9433b29ed4e664313d30c3955b06e.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8e9433b29ed4e664313d30c3955b06e.png)'
- en: Image by the author
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: As we see in image above, every chunk is a sequence of k tokens, where the first
    token is the *CLS* token.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，每个块是一个由 k 个标记组成的序列，其中第一个标记是 *CLS* 标记。
- en: '**Model Architecture**'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**模型架构**'
- en: 'After tokenizing and segmenting the input sequence, it feeds it to the **HAT**
    transformer model. The HAT model is an encoder-transformer and consists of two
    main components:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在对输入序列进行标记化和分段后，将其输入到**HAT**变换器模型中。HAT模型是一个编码器变换器，由两个主要组件组成：
- en: 'segment-wise encoder (SWE): this is a shared encoder block that takes in sequence
    of a segment (aka chunk) and processes the chunk.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分段编码器（SWE）：这是一个共享的编码器块，接收一个段（也称为块）的序列并处理该块。
- en: 'cross-segment encoder (CSE): this is another encoder block that takes is CLS
    tokens of all segments (aka chunks) and process cross-segment relations.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交段编码器（CSE）：这是另一个编码器块，它处理所有段（也称为块）的CLS标记，处理交段关系。
- en: '![](../Images/cfab73a0c1cf70f783fc88686522180a.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cfab73a0c1cf70f783fc88686522180a.png)'
- en: image from [[1](https://arxiv.org/abs/2210.05529)]
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源于[[1](https://arxiv.org/abs/2210.05529)]
- en: As we see in above image, the segment-wise encoder on the left, takes in all
    k tokens of a segment, process them and outputs updated representation of the
    tokens. On the left we see the cross-segment encoder takes CLS token embedding
    of all segments and output an updated representation for them.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 正如上图所示，左侧的分段编码器接收一个段的所有k个标记，处理它们并输出更新后的标记表示。在左侧，我们看到交段编码器接收所有段的CLS标记嵌入，并输出它们的更新表示。
- en: The two components can be used in several different layouts. For example, we
    can have them in “ad-hoc” layout where a stack L layers of SWE encoders are put
    first, then two layers of CSE encoders are on top of them. Note that the SWE in
    each layer share weight. The arrow between one layer of SWEs and the next layer
    indicate passing embedding between layers.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个组件可以用于几种不同的布局。例如，我们可以将它们放在“临时”布局中，其中一个堆叠的L层SWE编码器放在底部，然后在其上方放置两层CSE编码器。请注意，每层的SWE共享权重。层与层之间的箭头表示在层之间传递嵌入。
- en: '![](../Images/73c498ae9fb022b908d5b55984497acf.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73c498ae9fb022b908d5b55984497acf.png)'
- en: image from [[1](https://arxiv.org/abs/2210.05529)]
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源于[[1](https://arxiv.org/abs/2210.05529)]
- en: 'Another layout is the “interleaved” layer that is as following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种布局是“交错”层，如下所示：
- en: '![](../Images/7859404a727d2444761c021c46d2660a.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7859404a727d2444761c021c46d2660a.png)'
- en: image from [[1](https://arxiv.org/abs/2210.05529)]
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源于[[1](https://arxiv.org/abs/2210.05529)]
- en: The interleaved layout, as we see above, is a stack of paired segment-wise encoder
    and cross-segment encoders, where cross-segment attention is performed across
    several layers of the model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，交错布局是一个配对的分段编码器和交段编码器的堆叠，其中跨越模型的几层执行交段注意力。
- en: Authors in [1] explore few layout (see the image below) and experimentally they
    find out the “interleaved” layout outperforms other variants.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]中的作者探讨了几种布局（见下图），并通过实验发现“交错”布局优于其他变体。'
- en: '![](../Images/b850a831f29ba46cf4607eb6a5b19d88.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b850a831f29ba46cf4607eb6a5b19d88.png)'
- en: image from [[1](https://arxiv.org/abs/2210.05529)]
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源于[[1](https://arxiv.org/abs/2210.05529)]
- en: 'The full architecture of the model is as following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的完整架构如下：
- en: '![](../Images/3eb3d63c252fff650b8df34441cbf480.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3eb3d63c252fff650b8df34441cbf480.png)'
- en: image from [[1](https://arxiv.org/abs/2210.05529)]
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源于[[1](https://arxiv.org/abs/2210.05529)]
- en: The architecture consists of N layers, where each layer is as shown above. Note
    that stacking them together bring us the “interleaved” architecture. All segment-wise
    encoders in one layer share weights and they process input segments (chunks) independently
    and in parallel. Every segment has its own positional embeddings.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 架构由N层组成，每层如上所示。请注意，将它们堆叠在一起会带来“交错”架构。一个层中的所有分段编码器共享权重，它们独立且并行地处理输入段（块）。每个段都有自己的位置嵌入。
- en: The output of *segment-wise encoders* will be updated embeddings of segment
    tokens. The first token which is the CLS token embedding is added to the segment
    positional embedding and is passed to the *cross-segment encoder.*
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*分段编码器*的输出将是分段标记的更新嵌入。第一个标记，即CLS标记嵌入，被添加到分段位置嵌入中，并传递给*交段编码器*。'
- en: The cross segment encoder captures relationship between segments and update
    the CLS embedding of each segments and outputs it. The output of the layer would
    be the updated embeddings for all tokens.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 交段编码器捕捉段之间的关系，并更新每个段的CLS嵌入并输出。该层的输出将是所有标记的更新嵌入。
- en: '**Positional embeddings**. Note there are two positional embeddings in the
    architecture:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**位置嵌入**。请注意，架构中有两个位置嵌入：'
- en: 'positional embeddings for tokens of a segment: this is to indicate positions
    of tokens within a segment. This is only used in *segment-wise encoder*.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 段的标记位置嵌入：这是为了指示标记在段中的位置。这仅在*段级编码器*中使用。
- en: 'positional embeddings for segments: this is to indicate orders of segments
    and is only used in *cross-segment encoder*.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 段的标记位置嵌入：这是为了指示段的顺序，仅在*跨段编码器*中使用。
- en: Model Training
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练
- en: 'Authors[1] trained the model in two stage: pre-training and fine-tuning.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 作者[1]将模型训练分为两个阶段：预训练和微调。
- en: '**Pre-training**: since the network is an encoder transformer, it is trained
    using Masked Language Modeling (MLM) objective, where a fraction (15%) of tokens
    are masked and language model is supposed to predict them.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**预训练**：由于网络是一个编码器变换器，它使用掩码语言建模（MLM）目标进行训练，其中一部分（15%）的标记被掩码，语言模型应该预测这些标记。'
- en: '**Fine-Tuning**: They used document classification task on few labelled datasets
    to fine tune the model.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**微调**：他们在几个标注数据集上使用文档分类任务来微调模型。'
- en: Different Level of Embeddings
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同级别的嵌入
- en: 'Using this network, we can obtain embeddings at various scales: word, paragraph,
    document.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个网络，我们可以在不同的尺度上获得嵌入：词、段落、文档。
- en: Word embeddings or token embeddings are directly accessible via last layer of
    the model,
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词嵌入或标记嵌入可以通过模型的最后一层直接获取，
- en: Paragraph or segment embeddings are accessible via CLS token embedding of a
    segment.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 段落或段的嵌入可以通过段的CLS标记嵌入获得。
- en: Document embedding can be taken by max pooling (or average pooling) of all segment
    CLS token embeddings. In the paper [1], authors chose max pooling.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档嵌入可以通过对所有段CLS标记嵌入进行最大池化（或平均池化）来获得。在论文[1]中，作者选择了最大池化。
- en: Evaluation
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: 'What I really like about this paper is that they consider a thorough evaluation
    in three levels:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我真正喜欢这篇论文的地方在于，他们在三个层次上进行了全面的评估：
- en: '1)**upstream evaluation tasks**: these are tasks that aim to pre-train the
    encoder in generic way. For this task, they take the MLM (masked language modeling)
    task.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 1)**上游评估任务**：这些任务旨在以通用方式预训练编码器。对于这个任务，他们采用了MLM（掩码语言建模）任务。
- en: '2) **midstream evaluation tasks**: these are tasks that aim to assess quality
    of learned representations by the pre-trained model. For this evaluation, authors[1]
    consider few tasks such as the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 2) **中游评估任务**：这些任务旨在评估预训练模型学习到的表示的质量。为了这个评估，作者[1]考虑了几个任务，例如：
- en: 'segment order prediction: this is to predict the order of few segments. The
    input to the model in this task is a shuffled sequence of segments from a document[1],
    and the goal is to predict the right ordering among them. Since this is a regression
    problem they use the Mean Absolute Error (MAE) as loss function. Intuitively,
    this task uses assess the quality of CLS token embeddings.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 段顺序预测：这是为了预测几个段的顺序。模型在此任务中的输入是来自文档[1]的打乱的段序列，目标是预测它们之间的正确顺序。由于这是一个回归问题，他们使用平均绝对误差（MAE）作为损失函数。直观上，这个任务评估CLS标记嵌入的质量。
- en: '3) **downstream evaluation tasks**: where they estimate model’s performance
    in realistic applications. For this evaluation they considered document classification
    in the usecase of classifying discharge summaries from US hospitals. To obtain
    document embedding, they used max pooling on CLS token embeddings of all segments
    in the document and they used cross entropy as the loss function for classification.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 3) **下游评估任务**：他们在现实应用中评估模型的性能。为了这个评估，他们考虑了文档分类，用于对美国医院的出院总结进行分类。为了获得文档嵌入，他们对文档中所有段的CLS标记嵌入进行了最大池化，并使用交叉熵作为分类的损失函数。
- en: For more details, please refer to the paper at [1] .
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 更多细节请参考论文 [1]。
- en: Experiment Results
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验结果
- en: There are many experiment results in the paper, but one notable one is where
    they compare their model in the interleaved layout with Longformer [2] and BigBird
    [3] models. Longformer and BigBird are from sparse attention family of methods,
    they implement an efficient attention mechanism by forcing each token to attend
    to few tokens in its neighborhood and few global tokens. This is as opposed to
    standard attention where every token attends to every other token. In this experiment,
    they show their HAT method beats both Longformer and BigBird.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中有许多实验结果，但一个显著的结果是，他们将自己的模型与 Longformer [2] 和 BigBird [3] 模型进行比较。Longformer
    和 BigBird 属于稀疏注意力方法，它们通过强制每个标记仅关注其邻域中的少数标记和少量全局标记来实现高效的注意力机制。这与标准注意力方法相对，后者每个标记都关注每个其他标记。在这个实验中，他们展示了
    HAT 方法在性能上超越了 Longformer 和 BigBird。
- en: '![](../Images/122a9ae9347af5715b0d6dd2ab0cbc05.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/122a9ae9347af5715b0d6dd2ab0cbc05.png)'
- en: image from [[1](https://arxiv.org/abs/2210.05529)]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源 [[1](https://arxiv.org/abs/2210.05529)]
- en: Summary
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this post, we looked at another hierarchical transformer architecture called
    as Hierarchical Attention Transformer (HAT). This is an encoder based model that
    splits input into segments of equal length. Each segment starts with a CLS token
    that represents the segment. The model architecture consists of two main components:
    a segment-wise encoder, and a cross-segment encoder. The first encoder learns
    representations for individual segments, whereas the second one learns cross-relation
    between segments. Together they are able to learn representations for various
    level of hierarchy in the input such as word representation, sentence representation
    and document representation.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们探讨了另一种分层的变换器架构，称为分层注意力变换器（HAT）。这是一种基于编码器的模型，将输入分割成相等长度的片段。每个片段以一个表示该片段的CLS标记开始。该模型架构包括两个主要组件：片段级编码器和跨片段编码器。第一个编码器学习单个片段的表示，而第二个编码器学习片段之间的跨关系。它们一起能够学习输入中各种层次的表示，如词表示、句子表示和文档表示。
- en: 'If you have any questions or suggestions, feel free to reach out to me:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何问题或建议，随时与我联系：
- en: 'Email: mina.ghashami@gmail.com'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 邮箱：mina.ghashami@gmail.com
- en: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
- en: References
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[An Exploration of Hierarchical Attention Transformers for Efficient Long Document
    Classification](https://arxiv.org/abs/2210.05529)'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[对分层注意力变换器在长文档分类中的有效性的探索](https://arxiv.org/abs/2210.05529)'
- en: '[Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)'
- en: '[Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)'
