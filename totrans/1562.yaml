- en: Naive Bayes from scratch with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947](https://towardsdatascience.com/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Probabilistic Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@luisroque?source=post_page-----6e04c5a25947--------------------------------)[![Lu√≠s
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----6e04c5a25947--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6e04c5a25947--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6e04c5a25947--------------------------------)
    [Lu√≠s Roque](https://medium.com/@luisroque?source=post_page-----6e04c5a25947--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6e04c5a25947--------------------------------)
    ¬∑10 min read¬∑Jan 18, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article belongs to the series ‚ÄúProbabilistic Deep Learning‚Äù. This weekly
    series covers probabilistic approaches to deep learning. The main goal is to extend
    deep learning models to quantify uncertainty, i.e., know what they do not know.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we present an examination of the Naive Bayes algorithm for
    classification tasks using a dataset of wine samples. The Naive Bayes algorithm
    is a probabilistic machine learning technique based on Bayes‚Äô theorem, which makes
    assumptions about the independence of features given the target label. To facilitate
    visualization of the separation of classes, we limit the model to only two features.
  prefs: []
  type: TYPE_NORMAL
- en: Our objective is to classify wine samples based on selected characteristics.
    To accomplish this, we begin by exploring the data and selecting features that
    effectively separate the classes. We then proceed to construct class prior distributions
    and class-conditional densities, leading to the ability to predict the class with
    the highest probability. The dataset used in this study consists of various features
    of wines, such as hue, alcohol, flavonoids, and a target class, and is obtained
    from the scikit-learn library [1].
  prefs: []
  type: TYPE_NORMAL
- en: 'Articles published so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Gentle Introduction to TensorFlow Probability: Distribution Objects](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Gentle Introduction to TensorFlow Probability: Trainable Parameters](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Maximum Likelihood Estimation from scratch in TensorFlow Probability](/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Probabilistic Linear Regression from scratch in TensorFlow](/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Probabilistic vs. Deterministic Regression with Tensorflow](https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Frequentist vs. Bayesian Statistics with Tensorflow](https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Deterministic vs. Probabilistic Deep Learning](/deterministic-vs-probabilistic-deep-learning-5325769dc758)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Naive Bayes from scratch with TensorFlow
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/32c7b8ce4cb661b5ede5f8501c912b4e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The motto for today: be naive when it comes to wine classification?
    ([source](https://unsplash.com/photos/3uJt73tr4hI))'
  prefs: []
  type: TYPE_NORMAL
- en: We develop our models using TensorFlow and TensorFlow Probability. TensorFlow
    Probability is a Python library built on top of TensorFlow. We will start with
    the basic objects we can find in TensorFlow Probability and understand how we
    can manipulate them. We will increase complexity incrementally over the following
    weeks and combine our probabilistic models with deep learning on modern hardware
    (e.g. GPU).
  prefs: []
  type: TYPE_NORMAL
- en: As usual, the code is available on my [GitHub](https://github.com/luisroque/probabilistic_deep_learning_with_TFP).
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Our goal is to investigate the use of the Naive Bayes algorithm for classifying
    wine samples based on selected characteristics. To accomplish this, we begin by
    conducting an exploratory analysis of the data. Let‚Äôs start by identifying two
    features that effectively separate the target variable and utilize them to predict
    the class of wine.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ce21d60bcffb8b0fc6367373d093b396.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Analyzing pairs of features from the wine dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Alcohol and hue are features that separate the classes quite nicely. As such,
    these are the two features that we will be using to build our Naive Bayes model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f6e044ba304f9de780daf52a77059f79.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Distribution of the target samples by alcohol and hue.'
  prefs: []
  type: TYPE_NORMAL
- en: We can now split our data into a train and a test set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Naive Bayes Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Naive Bayes is a widely used probabilistic machine learning algorithm based
    on Bayes‚Äô theorem. It is particularly useful for classification tasks and is known
    for its simplicity and efficiency. Despite its name, the ‚Äúnaive‚Äù assumption of
    independence among features is not always a limitation and can often yield good
    results in practice. In this article, we comprehensively review the Naive Bayes
    algorithm, its variants, and its implementation from the first principles.
  prefs: []
  type: TYPE_NORMAL
- en: We begin by briefly introducing Bayes‚Äô theorem, which is the foundation of the
    Naive Bayes algorithm. Bayes‚Äô theorem states that the probability of a hypothesis
    (H) given some evidence (E) is proportional to the prior probability of the hypothesis
    multiplied by the likelihood of the evidence given the hypothesis. The Naive Bayes
    algorithm uses this theorem to classify new instances by computing the posterior
    probability for each class and then selecting the class with the highest probability.
  prefs: []
  type: TYPE_NORMAL
- en: The basic principle of the Naive Bayes algorithm is to assume that the features
    of a given instance are conditionally independent given the class label. This
    assumption, also known as the ‚Äúnaive‚Äù assumption, allows for a computationally
    efficient algorithm, as it reduces the number of parameters to be estimated. However,
    it can also lead to a decrease in accuracy when the features are not truly independent.
  prefs: []
  type: TYPE_NORMAL
- en: There are several variants of the Naive Bayes algorithm, each suited to different
    types of data. For example, the Gaussian Naive Bayes is used for continuous data,
    while the Multinomial Naive Bayes is used for discrete data. The Bernoulli Naive
    Bayes is used for binary data. In this case, we will be focusing on implementing
    Gaussian Naive Bayes.
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes algorithm has been applied to a wide range of domains, including
    natural language processing, computer vision, and bioinformatics. In natural language
    processing, it is commonly used for text classification, such as spam detection
    and sentiment analysis. In computer vision, it is used for image classification
    and object detection. In bioinformatics, it is used for protein classification
    and gene prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we stated above the Naive Bayes classifier is based on Bayes rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e4c9aa0f9a2d31017169c48f72d3a12.png)'
  prefs: []
  type: TYPE_IMG
- en: where *ùëã* are the input features, *ùëå* the output classes and *ùêæ* the number
    of classes. More specifically, *ùëÉ*(*ùëå*) represents the class prior distribution,
    *ùëÉ*(*ùëã*|*ùëå*) the class-conditional distribution over the inputs and *ùëÉ*(*ùëå*|*ùëã*)
    the probability of getting a class given the input features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The assumption of independence simplifies the algorithm substantially as we
    do not need to estimate the full joint distribution *ùëÉ*(*ùëã*|*ùëå*=*ùë¶ùëò*). Instead,
    the class-conditional distribution can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b26ab4071d2731831e6640f443935d02.png)'
  prefs: []
  type: TYPE_IMG
- en: where *ùëì* denotes the number of features.
  prefs: []
  type: TYPE_NORMAL
- en: Prior
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the Naive Bayes algorithm, the class prior distribution is a probability
    distribution that describes the probability of each class in the training data.
    It is a fundamental component of the algorithm, as it is used to compute the posterior
    probability of a class given some evidence.
  prefs: []
  type: TYPE_NORMAL
- en: The class prior distribution is defined as the probability of a class, given
    the total number of instances in the training data. It is usually denoted as *ùëÉ*(*ùëå*=*ùë¶ùëò*),
    where *ùëò* is the class label. The class prior distribution is estimated using
    the relative frequency of each class in the training data. For example, if there
    are 100 instances in the training data, and 60 of them belong to class A, then
    the class prior for class A is estimated as P(Y=A) = 0.6.
  prefs: []
  type: TYPE_NORMAL
- en: The class prior distribution plays a crucial role in the Naive Bayes algorithm,
    as it is used to compute the posterior probability of a class given some evidence.
    The posterior probability is computed as the product of the class prior and the
    likelihood of the evidence given the class, normalized by the marginal likelihood
    of the evidence. In other words, the class prior distribution acts as a weighting
    factor that adjusts the relative importance of the likelihood function.
  prefs: []
  type: TYPE_NORMAL
- en: However, if the class prior distribution is estimated from a biased training
    data, it may lead to poor performance of the algorithm, particularly if the test
    data is from a different distribution. This is known as the class imbalance problem,
    and it can be mitigated by using techniques such as oversampling, undersampling,
    or synthetic data generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The class prior distribution is the proportion of data examples belonging to
    the class *ùëò*. We can write it in the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/28dbed717467724e2cbd6b707474dcab.png)'
  prefs: []
  type: TYPE_IMG
- en: where *ùëõ* denotes the *ùëõ*-th dataset example, *ùëÅ* is the total number of examples
    in the dataset and *ùõø* is the Kronecker delta function (returns 1 when the classes
    match and 0 otherwise). It returns a categorical distribution with probabilities
    corresponding to *ùëÉ*(*ùëå*=*ùë¶ùëò*).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs plot the our prior distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/91631c4fe3d458c9e623ebd6cefb3003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Prior probability for each target class.'
  prefs: []
  type: TYPE_NORMAL
- en: Likelihood
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the Naive Bayes algorithm, the class conditional densities are probability
    distributions that describe the likelihood of each feature given the class label.
    They are used to compute the posterior probability of a class given some evidence,
    and are a fundamental component of the algorithm. The class conditional densities
    are defined as the probability density functions (pdf) of each feature given the
    class label. They are usually denoted as *ùëÉ*(*ùëãùëñ*|*ùëå*=*ùë¶ùëò*), where *ùëãùëñ* is a feature
    and *ùëò* is the class label. The class conditional densities are estimated from
    the training data using various techniques, depending on the type of data. For
    example, for continuous data, the class conditional density can be estimated using
    the Gaussian distribution, while for discrete data, it can be estimated using
    the multinomial or Bernoulli distributions. As we stated earlier, in our case,
    we have continuous features and so we will explore the Gaussian approach.
  prefs: []
  type: TYPE_NORMAL
- en: The class conditional densities play a crucial role in the Naive Bayes algorithm,
    as they are used to compute the likelihood of the evidence given the class label.
    This likelihood is computed by evaluating the class conditional densities for
    each feature of the evidence, and then multiplying them together. The class conditional
    densities act as a weighting factor that adjusts the relative importance of each
    feature for the classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is time to define *ùëÉ*(*ùëã*|*ùëå*) the class-conditional distribution over the
    inputs. In this case, we use univariate Gaussian distributions (remember the independence
    assumption):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8a42de9e92b88c95baceb8575bd21346.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *ùúáùëñùëò* and *ùúéùëñùëò* are the parameters to estimate. Using maximum likelihood,
    the estimates are just the mean and variance of the sample data points for each
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2cce666011e1a0307f40a86c6cf120e7.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The contour plot below shows the class-conditional densities. Notice how the
    contours of each distribution correspond to a Gaussian distribution with diagonal
    covariance matrix, since the model assumes that each feature is independent given
    the class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7b525c510c0f1d96f30e2b6f52f72add.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Training set with class-conditional density contours.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After performing the computations described above, the last step of the algorithm
    is to predict the class *ùëå*ÃÇ for a new data input *ùëã*ÃÉ :=(*ùëã*ÃÉ 1,‚Ä¶,*ùëã*ÃÉ *ùëì*).
    This can be done by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a121752ff358a9a4e89d09cbecb08f4.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article, we applied the Naive Bayes algorithm to classify wine samples
    based on selected characteristics. Specifically, we used two features: hue and
    alcohol, to predict the class of a wine. Our results indicate that the model achieved
    an accuracy of more than 91% for this task.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: To further analyze the performance of the model, we also plotted the decision
    regions of the model, i.e. the boundaries that separate the different classes.
    The decision regions help to visualize the separation of classes performed by
    the algorithm. As we can see, the model was able to separate the three classes
    in the dataset quite effectively.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that the Naive Bayes algorithm makes an assumption of independence
    among features, which might not be true in real-world scenarios. The correlation
    between features can help in improving the accuracy of the model. Therefore, incorporating
    a correlation between model features could help improve its performance. Furthermore,
    other algorithms that allow the correlation between features can be considered
    to improve the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/24462cec3281dc09ff436f5997e1665d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Training set decision regions.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we implemented the Naive Bayes algorithm from scratch using
    TensorFlow Probability. We applied it for a classification task using a dataset
    of wine samples. We selected two features, hue and alcohol, to predict the class
    of wine, and achieved an accuracy of more than 91%. We also visualized the decision
    regions of the model, which helped to understand the separation of classes performed
    by the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: This simple example shows the simplicity and effectiveness of the Naive Bayes
    algorithm for classification tasks. However, the Naive Bayes algorithm makes an
    assumption of independence among features, which may not be true in real-world
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: References and Materials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] ‚Äî [Wine Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] ‚Äî [Coursera: Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] ‚Äî [Coursera: TensorFlow 2 for Deep Learning](https://www.coursera.org/specializations/tensorflow2-deeplearning)
    Specialization'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] ‚Äî [TensorFlow Probability Guides and Tutorials](https://www.tensorflow.org/probability/overview)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] ‚Äî [TensorFlow Probability Posts in TensorFlow Blog](https://blog.tensorflow.org/search?label=TensorFlow+Probability&max-results=20)'
  prefs: []
  type: TYPE_NORMAL
