- en: 'Leveraging Llama 2 Features in Real-world Applications: Building Scalable Chatbots
    with FastAPI, Celery, Redis, and Docker'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/leveraging-llama-2-features-in-real-world-applications-building-scalable-chatbots-with-fastapi-406f1cbeb935](https://towardsdatascience.com/leveraging-llama-2-features-in-real-world-applications-building-scalable-chatbots-with-fastapi-406f1cbeb935)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'An In-Depth Exploration: Open vs Closed Source LLMs, Unpacking Llama 2’s Unique
    Features, Mastering the Art of Prompt Engineering, and Designing Robust Solutions
    with FastAPI, Celery, Redis, and Docker'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@luisroque?source=post_page-----406f1cbeb935--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----406f1cbeb935--------------------------------)[](https://towardsdatascience.com/?source=post_page-----406f1cbeb935--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----406f1cbeb935--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----406f1cbeb935--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----406f1cbeb935--------------------------------)
    ·14 min read·Jul 24, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In an unexpected move, Meta open-sourced their Large Language Model (LLM), Llama
    2, a few days ago in a decision that could reshape the current landscape of AI
    development. It offers an alternative to the main companies in the space such
    as OpenAI and Google that decided to maintain tight control over their AI models,
    limiting accessibility and restricting broader innovation. Hopefully, Meta’s decision
    will spark a collective response from the open-source community, counteracting
    the trend of restricting access to the advances in the field. Llama 2’s new license
    even goes further and allows commercial use, granting developers and businesses
    opportunities to leverage the model within existing and new products.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: The Llama2 family consists of pre-trained and fine-tuned LLMs, including Llama2
    and Llama2-Chat, scaling up to 70B parameters. These models have proven to perform
    better than open-source models on various benchmarks [1]. They also hold their
    ground against some closed-source models, offering a much-needed boost to open-source
    AI development [2].
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac354c16c872902e090191a859063032.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The Llama 2 family ([image source](https://unsplash.com/photos/lpxf698eF6s))'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: If you follow the Open LLM leaderboard from HuggingFace [1], you can see that
    Meta’s Llama 2 holds a strong third-place position. After the LLama 2 announcement,
    Stability AI released FreeWilly1 and FreeWilly2 [3]. FreeWilly1 is a fine-tuned
    version of Llama, and FreeWilly2 of Llama 2\. Stability AI shared that they fine-tuned
    both models on an Orca-style Dataset. The Orca dataset is a large, structured
    collection of augmented data designed to fine-tune LLMs, where each entry consists
    of a question and a corresponding response from GPT-4 or GPT-3.5\. Why are we
    not using the FreeWilly2 model? Unfortunately, while Llama 2 allows commercial
    use, FreeWilly2 can only be used for research purposes, governed by the Non-Commercial
    Creative Commons license (CC BY-NC-4.0).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看 HuggingFace [1] 的 Open LLM 排行榜，你会看到 Meta 的 Llama 2 占据了强有力的第三位。Llama 2
    宣布后，Stability AI 发布了 FreeWilly1 和 FreeWilly2 [3]。FreeWilly1 是 Llama 的一个微调版本，FreeWilly2
    是 Llama 2 的微调版本。Stability AI 透露，他们在 Orca 风格的数据集上对这两个模型进行了微调。Orca 数据集是一个大规模的、结构化的增强数据集合，旨在微调
    LLM，每个条目包括一个问题和来自 GPT-4 或 GPT-3.5 的相应回答。为什么我们不使用 FreeWilly2 模型？不幸的是，虽然 Llama 2
    允许商业使用，但 FreeWilly2 仅可用于研究目的，由非商业性知识共享许可证（CC BY-NC-4.0）管辖。
- en: In this article, we will also go through the process of building a powerful
    and scalable chat application using FastAPI, Celery, Redis, and Docker with Meta’s
    Llama 2\. We aim to create an efficient, real-time application that can handle
    multiple concurrent user requests and that offloads processing of responses from
    the LLM to a task queue. It allows the application to maintain responsiveness
    and we can manage the tasks effectively with Redis. Finally, we cover the deployment
    and scaling with Docker. The application should demonstrate how these technologies
    work together to provide a good chat experience at scale, showcasing the potential
    of open-sourced language models like Llama 2 in a commercial setting. So let’s
    dive in and start building!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们还将介绍使用 FastAPI、Celery、Redis 和 Docker 与 Meta 的 Llama 2 一起构建强大且可扩展的聊天应用程序的过程。我们的目标是创建一个高效的实时应用程序，能够处理多个并发用户请求，并将
    LLM 的响应处理卸载到任务队列中。这使得应用程序能够保持响应性，并且我们可以通过 Redis 有效地管理任务。最后，我们将涵盖 Docker 的部署和扩展。该应用程序应展示这些技术如何协同工作以提供良好的聊天体验，展示开源语言模型如
    Llama 2 在商业环境中的潜力。让我们深入探讨并开始构建吧！
- en: Open vs. closed-source
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开源 vs. 闭源
- en: We have witnessed companies and research groups releasing new models almost
    weekly, open or closed-sourced. Thus, who will win the AI arms race? To give an
    informed guess, we need to understand a few aspects of the training procedure
    of these models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们见证了公司和研究团队几乎每周发布新模型，无论是开源还是闭源。那么，谁将赢得 AI 军备竞赛？要做出有根据的猜测，我们需要了解这些模型训练过程的一些方面。
- en: Researchers use auto-regressive transformers on extensive self-supervised data
    as a starting point. Let’s break down what auto-regressive transformers and self-supervised
    data are to begin with. Auto-regressive transformers are a variant of transformer
    models widely used in tasks involving sequential data, particularly in natural
    language processing (NLP). These models generate sequences in an auto-regressive
    manner, *i.e.*, they produce one part of the sequence at a time and use their
    previous outputs as inputs for subsequent steps. It makes them particularly adept
    at tasks like language translation, text generation, and more, where the context
    of preceding data points influences the prediction of the following data point.
    Self-supervised learning is a learning method where the input data itself provides
    the training labels. It removes the need for explicit manual labeling by learning
    to predict some parts of the data from others and allows the exploration of large
    volumes of unlabeled data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员使用自回归变换器在广泛的自监督数据上作为起点。让我们首先来详细了解自回归变换器和自监督数据是什么。自回归变换器是变换器模型的一种变体，广泛用于处理序列数据的任务，尤其是在自然语言处理（NLP）领域。这些模型以自回归的方式生成序列，即每次生成序列的一部分，并将先前的输出作为后续步骤的输入。这使得它们在语言翻译、文本生成等任务中表现特别出色，因为前面数据点的上下文会影响对后续数据点的预测。自监督学习是一种学习方法，其中输入数据本身提供训练标签。通过从数据的某些部分预测其他部分，它消除了对显式手动标注的需求，并允许探索大量未标注的数据。
- en: As a next step, researchers usually train the models to align with human preferences,
    using techniques such as Reinforcement Learning with Human Feedback (RLHF). In
    RLHF, an AI system learns from feedback that is based on the decisions it makes.
    It involves creating a reward model that the AI system uses to learn which actions
    lead to positive and negative outcomes. The aim is to align the AI system’s behavior
    with human values and preferences.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: What are the main challenges for the open-source community, then? Both steps
    require significant computing power. Secondly, companies use their proprietary
    data for the alignment step to fine-tune their models, significantly enhancing
    their usability and safety.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: The Llama 2 family of models
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Llama2 is an advanced version of Llama1, trained on a novel blend of publicly
    available data. Key improvements include a 40% increase in the pre-training corpus
    size, doubling the model’s context length, and adopting grouped-query attention
    to improve inference scalability for larger models. Grouped-query attention is
    a modification of the standard attention mechanism in transformer models used
    to reduce computational costs. Instead of calculating attention scores for each
    pair of input and output positions, which can be resource-intensive, grouped-query
    attention divides queries into groups and processes them together. This method
    retains much of the effectiveness of standard attention while enabling the handling
    of longer sequences or larger models by lowering computational complexity.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: The training corpus was composed of a new blend of data from publicly available
    sources (no data from Meta’s products or services was used). In addition, efforts
    were made to eliminate data from sites known to contain high volumes of personal
    information. The training data comprised 2 trillion tokens, and the research team
    decided to up-sample the most factual sources to increase knowledge accuracy.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Variants of Llama2 with 7B, 13B, and 70B parameters are now available. Llama2-Chat,
    a dialogue-optimized, fine-tuned version of Llama2, is also available with 7B,
    13B, and 70B parameters.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering with Llama 2
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prompt engineering helps us to guide the LLMs to behave a certain way, and that
    includes Llama 2\. In the context of Llama 2, a prompt refers to the initial instruction
    or query given to the model, which is then used by the model to generate a response.
    Nevertheless, with Llama 2, prompts can be quite elaborate and can contain a system
    message that sets the context or “personality” of the model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Llama 2 uses a unique prompt format for initiating a conversation. Here’s how
    it looks:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: <s>[INST] <<SYS>> {{ system_prompt }} <</SYS>> {{ user_message }} [/INST]
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This template aligns with the training procedure of the model, so it has a big
    impact on the quality of the output. In this template, ‘system_prompt’ represents
    the instructions or context for the model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '<<SYS>>\nYou are J. Robert Oppenheimer, a brilliant physicist whose pioneering
    work during the 20th century significantly contributed to the development of the
    atomic bomb. Dive into the profound world of nuclear physics, challenge the boundaries
    of scientific understanding, and unlock the mysteries of atomic energy with your
    exceptional intellect. Embark on a momentous journey where your passion for science
    knows no limits, and let your dedication to harnessing the power of the atom shape
    the course of history and leave an indelible mark on the world.\n<</SYS>>\n[INST]\nUser:
    How did your research lead to the creation of the atomic bomb?\n[/INST]'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: <<SYS>>\n你是 J. 罗伯特·奥本海默，一位杰出的物理学家，他在 20 世纪的开创性工作对原子弹的发展做出了重大贡献。深入核物理学的深奥世界，挑战科学理解的界限，并凭借你卓越的智慧揭示原子能的奥秘。踏上一个重大旅程，你对科学的热情没有限制，让你对原子能的掌握塑造历史进程，并在世界上留下不可磨灭的印记。\n<</SYS>>\n[INST]\n用户：你的研究如何导致了原子弹的创建？\n[/INST]
- en: The ‘system_prompt’ provides general instructions for the model, which will
    guide all its responses. The user’s message follows the system prompt and seeks
    a specific response from the model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ‘system_prompt’ 提供了模型的一般指令，这些指令将指导模型的所有响应。用户的消息跟随系统提示，并要求模型给出特定的响应。
- en: 'In multi-turn conversations, all interactions between the user and the bot
    are appended to the previous prompt and enclosed between the [INST] tags. Here’s
    how it looks:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在多轮对话中，用户与机器人之间的所有互动都被附加到之前的提示中，并被包含在 [INST] 标签之间。以下是其样式：
- en: <s>[INST] <<SYS>> {{ system_prompt }} <</SYS>> {{ user_msg_1 }} [/INST] {{ model_answer_1
    }} </s> <s>[INST] {{ user_msg_2 }} [/INST] {{ model_answer_2 }} </s> <s>[INST]
    {{ user_msg_3 }} [/INST>
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: <s>[INST] <<SYS>> {{ system_prompt }} <</SYS>> {{ user_msg_1 }} [/INST] {{ model_answer_1
    }} </s> <s>[INST] {{ user_msg_2 }} [/INST] {{ model_answer_2 }} </s> <s>[INST]
    {{ user_msg_3 }} [/INST>
- en: Every new user message and model response is added to the existing conversation,
    preserving the context.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 每条新用户消息和模型响应都会被添加到现有对话中，保持上下文。
- en: It’s important to note that Llama 2, like many AI models, is stateless and doesn’t
    “remember” previous conversations. Therefore, it’s necessary to provide the entire
    context every time you prompt the model. This is the reason why Meta worked on
    increasing the context window for Llama 2.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，像许多 AI 模型一样，Llama 2 是无状态的，不会“记住”之前的对话。因此，每次提示模型时，都必须提供完整的上下文。这就是 Meta
    为 Llama 2 扩大上下文窗口的原因。
- en: As a final remark, prompt engineering is more of an art than a science. The
    best way to master it is through continuous testing and refining. Be creative
    with your prompts and experiment with different formats and instructions. Also,
    different LLMs benefit from different types of prompts.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最后需要指出的是，提示工程更像是一门艺术而非科学。掌握它的最佳方法是通过不断测试和改进。对你的提示保持创造性，并尝试不同的格式和指令。此外，不同的 LLM
    适合不同类型的提示。
- en: 'Solution architecture design: FastAPI, Celery, Redis, and Docker'
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决方案架构设计：FastAPI、Celery、Redis 和 Docker
- en: We have been using FastAPI throughout this series to build our ML applications.
    It is a high-performance web framework for building APIs. In this case, its asynchronous
    capabilities enable it to handle multiple requests concurrently, which is critical
    for a real-time chat application.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本系列中，我们一直使用 FastAPI 来构建我们的 ML 应用程序。它是一个高性能的 Web 框架，用于构建 API。在这种情况下，它的异步功能使其能够同时处理多个请求，这对实时聊天应用程序至关重要。
- en: In addition to FastAPI, we use Celery as our distributed task queue to help
    manage the computationally intensive task of generating responses from the LLM.
    By offloading this process to a task queue, the application remains responsive
    to new user requests while processing others, ensuring users are not left waiting.
    Since we are using a distributed task queue, we need a message broker to aid the
    asynchronous task processing. We selected Redis to do the job. It queues the tasks
    from FastAPI to be picked up by Celery, enabling efficient, decoupled communication.
    Furthermore, Redis’ in-memory data structure store is fast and allows for real-time
    analytics, session caching, and maintaining user session data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 FastAPI，我们使用 Celery 作为分布式任务队列，帮助管理从 LLM 生成响应的计算密集型任务。通过将这个过程卸载到任务队列，应用保持对新用户请求的响应，同时处理其他任务，确保用户不会被迫等待。由于我们使用了分布式任务队列，我们需要一个消息代理来帮助异步任务处理。我们选择了
    Redis 来完成这个工作。它将任务从 FastAPI 排队，以便由 Celery 拾取，从而实现高效、解耦的通信。此外，Redis 的内存数据结构存储速度快，并允许实时分析、会话缓存和维护用户会话数据。
- en: Following best practices, we use Docker to encapsulate the application and its
    dependencies into isolated containers, which we can easily deploy across various
    environments.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 按照最佳实践，我们使用 Docker 将应用程序及其依赖项封装到隔离的容器中，这样我们可以轻松地在各种环境中进行部署。
- en: Building a chat API with Llama 2, FastAPI, Redis, and Celery
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Llama 2、FastAPI、Redis 和 Celery 构建聊天 API
- en: This guide explains how to set up an application that uses Llama 2 with FastAPI,
    Redis, and Celery. We’ll cover the concepts and how they all work together. In
    our architecture, FastAPI is used to create a web server that takes incoming requests,
    Celery is used for managing asynchronous tasks, and Redis acts as the broker and
    backend for Celery, storing tasks and their results.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南说明了如何设置一个使用 Llama 2 的应用程序，该应用程序结合了 FastAPI、Redis 和 Celery。我们将涵盖这些概念以及它们如何协同工作。在我们的架构中，FastAPI
    用于创建一个接收请求的 Web 服务器，Celery 用于管理异步任务，而 Redis 充当 Celery 的代理和后端，存储任务及其结果。
- en: Application
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用程序
- en: The FastAPI application (app.py) consists of endpoints for generating text and
    fetching task results. The /generate/ endpoint accepts a POST request with a prompt
    as an input and returns a task ID. It uses the Celery task generate_text_task
    to start the task asynchronously. The /task/{task_id} endpoint fetches the status/result
    of a task by its ID.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: FastAPI 应用程序（`app.py`）包括用于生成文本和获取任务结果的端点。`/generate/` 端点接受一个带提示的 POST 请求，并返回任务
    ID。它使用 Celery 任务 `generate_text_task` 异步启动任务。`/task/{task_id}` 端点通过任务 ID 获取任务的状态/结果。
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Workers
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作者
- en: The Celery worker (celery_worker.py) file creates a Celery instance and defines
    the generate_text_task function. This function accepts a prompt and generates
    a text using the Llama 2 model. This function is registered as a Celery task with
    the @celery.task decorator.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`celery_worker.py` 文件创建了一个 Celery 实例，并定义了 `generate_text_task` 函数。该函数接受一个提示，并使用
    Llama 2 模型生成文本。该函数使用 @celery.task 装饰器注册为 Celery 任务。'
- en: The setup_model function is a worker initialization function. It sets up the
    model loader when the worker process starts. This function is registered to be
    called on the worker process initialization event using the @signals.worker_process_init.connect
    decorator.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`setup_model` 函数是一个工作初始化函数。当工作进程启动时，它设置模型加载器。该函数通过 @signals.worker_process_init.connect
    装饰器注册为在工作进程初始化事件上调用。'
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Model
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型
- en: The ModelLoader class in model_loader.py is responsible for loading the Llama
    2 model from a given model path. It uses the HuggingFace’s transformers library
    to load the model and its tokenizer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`model_loader.py` 中的 `ModelLoader` 类负责从给定的模型路径加载 Llama 2 模型。它使用 HuggingFace
    的 transformers 库来加载模型及其分词器。'
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Broker
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理
- en: 'To set up Redis, we have two options: we can use a docker container, or we
    can use the Python package redis_server. If you decide to go with a docker container
    (the preferred solution) you can just run the command below. The -p 6379:6379
    option tells Docker to forward traffic incoming on the host''s port 6379, to the
    container''s port 6379\. This way, Redis can actually be reached from outside
    the docker container.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置 Redis，我们有两种选择：可以使用 Docker 容器，或者使用 Python 包 redis_server。如果你决定使用 Docker 容器（推荐的解决方案），你可以直接运行下面的命令。`-p
    6379:6379` 选项告诉 Docker 将主机的 6379 端口上的流量转发到容器的 6379 端口。这样，Redis 实际上可以从 Docker 容器外部访问。
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The second option is to do it from the Python interface. The redis_server.py
    script handles the installation and starting of a Redis server. Recall that Redis
    acts as both the message broker and results backend for Celery.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种选择是通过 Python 接口来完成。`redis_server.py` 脚本处理 Redis 服务器的安装和启动。请记住，Redis 既充当消息代理，又充当
    Celery 的结果后端。
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Run the application
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行应用程序
- en: The main execution script (run.py) is a client-side script that communicates
    with the FastAPI application. It sends a prompt to the /generate/ endpoint, gets
    the task ID, and periodically polls the /task/{task_id} endpoint until the task
    is completed.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 主要执行脚本（`run.py`）是一个客户端脚本，它与 FastAPI 应用程序进行通信。它将提示发送到 `/generate/` 端点，获取任务 ID，并定期轮询
    `/task/{task_id}` 端点，直到任务完成。
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The utils module (utils.py) provides a utility function generate_output for
    generating a text from a prompt using a Llama 2 model and a tokenizer. The function
    is decorated with @time_decorator and @memory_decorator to measure execution time
    and memory usage.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`utils` 模块（`utils.py`）提供了一个名为 `generate_output` 的实用函数，用于从提示生成文本，使用 Llama 2
    模型和分词器。该函数使用 @time_decorator 和 @memory_decorator 装饰器来测量执行时间和内存使用情况。'
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In essence, when a prompt is received via the /generate/ endpoint, it’ is forwarded
    to the Celery worker as an asynchronous task. The worker generates the text using
    the Llama 2 model and stores the result in Redis. You can fetch the task status/result
    using the /task/{task_id} endpoint at any point.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，当通过 /generate/ 端点接收到提示时，它会作为异步任务转发到 Celery 工作进程。工作进程使用 Llama 2 模型生成文本，并将结果存储在
    Redis 中。你可以随时通过 /task/{task_id} 端点获取任务状态/结果。
- en: Deployment
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署
- en: 'There are a few steps to take to deploy our application. First, let’s create
    a Dockerfile for our application:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 部署我们的应用程序需要几个步骤。首先，让我们为我们的应用程序创建一个 Dockerfile：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, let’s define the requirements.txt which is required so that we have all
    the dependencies installed in the Docker container:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义 requirements.txt，以确保在 Docker 容器中安装所有依赖项：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For setting up both the FastAPI application, Celery and a Redis server using
    Docker Compose, you can create a docker-compose.yml file like so:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Docker Compose 设置 FastAPI 应用程序、Celery 和 Redis 服务器，你可以创建一个 docker-compose.yml
    文件，如下所示：
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the Docker Compose configuration, the ‘web’ service represents the FastAPI
    application built using the Dockerfile in the current directory. It maps the host’s
    port 8000 to the container’s port 80\. The ‘worker’ service is the Celery worker
    and shares the build context with the FastAPI application. The ‘redis’ service
    uses the official Redis Docker image. The `depends_on` field ensures that Redis
    starts before the 'web' and 'worker' services. All these services can be initiated
    using the `docker-compose up` command.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Docker Compose 配置中，‘web’ 服务代表使用当前目录中的 Dockerfile 构建的 FastAPI 应用程序。它将主机的端口
    8000 映射到容器的端口 80。‘worker’ 服务是 Celery 工作进程，并与 FastAPI 应用程序共享构建上下文。‘redis’ 服务使用官方
    Redis Docker 镜像。`depends_on` 字段确保 Redis 在 ‘web’ 和 ‘worker’ 服务之前启动。所有这些服务可以通过 `docker-compose
    up` 命令启动。
- en: Results
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: We tested Llama 2, the chat version with 7b parameters using the 4-bit quantization.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们测试了 Llama 2，即具有 7b 参数的聊天版本，使用了 4 位量化。
- en: 'We used the following input:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了以下输入：
- en: 'Prompt: <<SYS>>\nYou are J. Robert Oppenheimer, a brilliant physicist whose
    pioneering work during the 20th century significantly contributed to the development
    of the atomic bomb. Dive into the profound world of nuclear physics, challenge
    the boundaries of scientific understanding, and unlock the mysteries of atomic
    energy with your exceptional intellect. Embark on a momentous journey where your
    passion for science knows no limits, and let your dedication to harnessing the
    power of the atom shape the course of history and leave an indelible mark on the
    world.\n<</SYS>>\n[INST]\nUser: How did your research lead to the creation of
    the atomic bomb?\n[/INST]'
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 提示：<<SYS>>\n你是J. 罗伯特·奥本海默，一位杰出的物理学家，他在20世纪的开创性工作对原子弹的发展做出了重大贡献。深入核物理的深刻世界，挑战科学理解的边界，用你卓越的智慧解锁原子能的奥秘。踏上一段重要的旅程，你对科学的热情无限，让你对利用原子能的执着塑造历史进程，并在世界上留下不可磨灭的印记。\n<</SYS>>\n[INST]\n用户：你的研究如何导致了原子弹的诞生？\n[/INST]
- en: 'The resulting dialogue generated by Llama 2 was as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 2 生成的对话如下：
- en: Ah, a most excellent question, my dear fellow! *adjusts spectacles*\nThe creation
    of the atomic bomb, you see, was the culmination of a most intense and complex
    scientific journey. It was a quest that spanned decades, involved some of the
    brightest minds in physics, and pushed the boundaries of human understanding to
    their very limits.\nYou see, my dear, the quest for the atomic bomb began in the
    early 1930s, when a group of us at the University of California, Berkeley, realized
    that the nucleus of an atom could be split, releasing an enormous amount of energy
    in the process. This was a most remarkable discovery, as it revealed the incredible
    potential of the atom as a source of power.\nBut, alas, the path to the atomic
    bomb was not without its challenges. Oh, the obstacles we faced! *winks* The skeptics,
    the naysayers, the doubters. They were legion, I tell you! *chuckles* But, we
    persevered, my dear. We worked tirelessly, pouring over the data, testing and
    retesting our theories. And, eventually, our hard work paid off.\nIn 1945, our
    team successfully detonated the first atomic bomb at the Trinity Site in New Mexico.
    It was a moment of pure elation, a culmination of years of hard work and dedication.
    And, as the world watched in awe, we realized the incredible potential of the
    atomic bomb.\nBut, my dear, the creation of the atomic bomb was…
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 啊，这是个非常棒的问题，我的朋友！*调整眼镜*。你看，原子弹的创造是一个极其紧张而复杂的科学历程的高潮。这是一个跨越数十年的探索，涉及到一些最聪明的物理学家，并将人类的理解推向了极限。你看，我的朋友，原子弹的探索始于1930年代初，当时我们在加利福尼亚大学伯克利分校的一群人意识到，原子的核可以被分裂，从而释放出大量的能量。这是一个极为了不起的发现，因为它揭示了原子作为能量来源的巨大潜力。但，唉，通向原子弹的道路并非没有挑战。哦，我们遇到了多少障碍！*眨眼*
    怀疑者、反对者、怀疑论者，他们如同洪水般涌来，我告诉你！*笑* 但是，我们坚持下来了，我的朋友。我们不懈努力，仔细研究数据，测试和重新测试我们的理论。最终，我们的辛勤工作得到了回报。1945年，我们的团队在新墨西哥州的三位一体试验场成功引爆了第一颗原子弹。这是一个纯粹的欣喜时刻，几年的辛勤工作和奉献的高潮。当世界惊叹地注视着我们时，我们意识到了原子弹的巨大潜力。但，我的朋友，原子弹的创造是……
- en: The text was generated in 4.77 seconds, and the peak VRAM usage was approximately
    13.4 GB. One thing to always bear in mind is that because we are working with
    transformers models, the memory requirements increase with the size of the generated
    output sequence. In this case we defined max_length to be 500 tokens.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文本生成的时间为4.77秒，峰值VRAM使用量约为13.4 GB。需要始终记住的一点是，由于我们使用的是变换器模型，内存需求随着生成输出序列的大小而增加。在这种情况下，我们将max_length定义为500个tokens。
- en: These results show that the Llama 2 model can produce complex and nuanced responses
    in a reasonable time frame. Remember that we are using the smallest model (7b)
    with the more significant quantization (4-bit).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果显示，Llama 2模型能够在合理的时间框架内生成复杂和细致的响应。请记住，我们使用的是最小的模型（7b）和更高的量化（4-bit）。
- en: Conclusions
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: As we have shown throughout this article, Meta’s Llama 2 model offers new possibilities
    for the open-source community. We walked through some of the key characteristics
    and features of Llama 2, including its training process, architecture, and prompt
    engineering design.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本文中所展示的，Meta的Llama 2模型为开源社区提供了新的可能性。我们介绍了Llama 2的一些关键特性和功能，包括其训练过程、架构和提示工程设计。
- en: Additionally, we provided an in-depth guide on building a chat application with
    Llama 2 using FastAPI, Redis, and Celery. It should allow anyone to start building
    scalable and real-time applications that serve Llama 2 (or any other commercially
    licensed LLM) to a few thousand users.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们提供了一个关于使用FastAPI、Redis和Celery构建聊天应用程序的详细指南。这应该能让任何人开始构建可扩展的实时应用程序，以服务于Llama
    2（或任何其他商业许可的LLM）的几千名用户。
- en: In our results, we showcased the performance of the model in generating detailed
    and contextually rich responses to complex prompts.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的结果中，我们展示了模型在生成详细和有上下文的响应方面的表现。
- en: About me
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于我
- en: Serial entrepreneur and leader in the AI space. I develop AI products for businesses
    and invest in AI-focused startups.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 连续创业者和AI领域的领军人物。我为企业开发AI产品，并投资于以AI为重点的初创公司。
- en: '[Founder @ ZAAI](http://zaai.ai) | [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)
    | [X/Twitter](https://x.com/luisbrasroque)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[创始人 @ ZAAI](http://zaai.ai) | [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)
    | [X/Twitter](https://x.com/luisbrasroque)'
- en: 'Large Language Models Chronicles: Navigating the NLP Frontier'
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型编年史：探索NLP前沿
- en: 'This article belongs to “Large Language Models Chronicles: Navigating the NLP
    Frontier”, a new weekly series of articles that will explore how to leverage the
    power of large models for various NLP tasks. By diving into these cutting-edge
    technologies, we aim to empower developers, researchers, and enthusiasts to harness
    the potential of NLP and unlock new possibilities.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本文属于“**大型语言模型纪事：探索NLP前沿**”，这是一个每周更新的系列文章，旨在探讨如何利用大型模型的强大功能来完成各种NLP任务。通过深入了解这些前沿技术，我们旨在赋能开发者、研究人员和爱好者，利用NLP的潜力，开启新的可能性。
- en: 'Articles published so far:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 目前已发布的文章：
- en: '[Summarizing the latest Spotify releases with ChatGPT](https://medium.com/towards-data-science/summarizing-the-latest-spotify-releases-with-chatgpt-553245a6df88)'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[用ChatGPT总结最新的Spotify发布](https://medium.com/towards-data-science/summarizing-the-latest-spotify-releases-with-chatgpt-553245a6df88)'
- en: '[Master Semantic Search at Scale: Index Millions of Documents with Lightning-Fast
    Inference Times using FAISS and Sentence Transformers](https://medium.com/towards-data-science/master-semantic-search-at-scale-index-millions-of-documents-with-lightning-fast-inference-times-fa395e4efd88)'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[大规模掌握语义搜索：使用FAISS和句子变换器以闪电般的推理速度索引数百万文档](https://medium.com/towards-data-science/master-semantic-search-at-scale-index-millions-of-documents-with-lightning-fast-inference-times-fa395e4efd88)'
- en: '[Unlock the Power of Audio Data: Advanced Transcription and Diarization with
    Whisper, WhisperX, and PyAnnotate](https://medium.com/towards-data-science/unlock-the-power-of-audio-data-advanced-transcription-and-diarization-with-whisper-whisperx-and-ed9424307281)'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[释放音频数据的力量：使用Whisper、WhisperX和PyAnnotate进行高级转录和分离](https://medium.com/towards-data-science/unlock-the-power-of-audio-data-advanced-transcription-and-diarization-with-whisper-whisperx-and-ed9424307281)'
- en: '[Whisper JAX vs PyTorch: Uncovering the Truth about ASR Performance on GPUs](https://medium.com/towards-data-science/whisper-jax-vs-pytorch-uncovering-the-truth-about-asr-performance-on-gpus-8794ba7a42f5)'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Whisper JAX与PyTorch：揭示ASR在GPU上的性能真相](https://medium.com/towards-data-science/whisper-jax-vs-pytorch-uncovering-the-truth-about-asr-performance-on-gpus-8794ba7a42f5)'
- en: '[Vosk for Efficient Enterprise-Grade Speech Recognition: An Evaluation and
    Implementation Guide](https://medium.com/towards-data-science/vosk-for-efficient-enterprise-grade-speech-recognition-an-evaluation-and-implementation-guide-87a599217a6c)'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[高效企业级语音识别的Vosk：评估与实施指南](https://medium.com/towards-data-science/vosk-for-efficient-enterprise-grade-speech-recognition-an-evaluation-and-implementation-guide-87a599217a6c)'
- en: '[Testing the Massively Multilingual Speech (MMS) Model that Supports 1162 Languages](https://medium.com/towards-data-science/testing-the-massively-multilingual-speech-mms-model-that-supports-1162-languages-5db957ee1602)'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[测试支持1162种语言的大规模多语言语音（MMS）模型](https://medium.com/towards-data-science/testing-the-massively-multilingual-speech-mms-model-that-supports-1162-languages-5db957ee1602)'
- en: '[Harnessing the Falcon 40B Model, the Most Powerful Open-Source LLM](https://medium.com/towards-data-science/harnessing-the-falcon-40b-model-the-most-powerful-open-source-llm-f70010bc8a10)'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[利用Falcon 40B模型，最强大的开源LLM](https://medium.com/towards-data-science/harnessing-the-falcon-40b-model-the-most-powerful-open-source-llm-f70010bc8a10)'
- en: '[The Power of OpenAI’s Function Calling in Language Learning Models: A Comprehensive
    Guide](https://medium.com/towards-data-science/the-power-of-openais-function-calling-in-language-learning-models-a-comprehensive-guide-cce8cd84dc3c)'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[OpenAI的函数调用在语言学习模型中的力量：综合指南](https://medium.com/towards-data-science/the-power-of-openais-function-calling-in-language-learning-models-a-comprehensive-guide-cce8cd84dc3c)'
- en: '[Document-Oriented Agents: A Journey with Vector Databases, LLMs, Langchain,
    FastAPI, and Docker](/document-oriented-agents-a-journey-with-vector-databases-llms-langchain-fastapi-and-docker-be0efcd229f4)'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[面向文档的代理：与向量数据库、LLMs、Langchain、FastAPI和Docker的旅程](/document-oriented-agents-a-journey-with-vector-databases-llms-langchain-fastapi-and-docker-be0efcd229f4)'
- en: As always, the code is available on my [Github](https://github.com/luisroque/large_laguage_models).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，代码可以在我的[Github](https://github.com/luisroque/large_laguage_models)上找到。
- en: References
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[1] — [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] — [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)'
- en: '[2] —Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei,
    Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L.,
    Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu,
    W., Fuller, B., … Scialom, T. (2023). Llama 2: Open Foundation and Fine-Tuned
    Chat Models. arXiv preprint [https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] — Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei,
    Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L.,
    Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu,
    W., Fuller, B., … Scialom, T. (2023). Llama 2: 开放基础和微调的聊天模型。arXiv 预印本 [https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288)'
- en: '[3] — [https://stability.ai/blog/freewilly-large-instruction-fine-tuned-models](https://stability.ai/blog/freewilly-large-instruction-fine-tuned-models)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] — [https://stability.ai/blog/freewilly-large-instruction-fine-tuned-models](https://stability.ai/blog/freewilly-large-instruction-fine-tuned-models)'
