- en: From Encodings to Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/from-encodings-to-embeddings-5b59bceef094](https://towardsdatascience.com/from-encodings-to-embeddings-5b59bceef094)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**concepts and fundamentals: from SVD to neural networks**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mina.ghashami?source=post_page-----5b59bceef094--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----5b59bceef094--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5b59bceef094--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5b59bceef094--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----5b59bceef094--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5b59bceef094--------------------------------)
    ¬∑16 min read¬∑Sep 7, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea2a97bef9136382285c2f7b5a61786a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'credit: [https://unsplash.com/](https://unsplash.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we will talk about two fundamental concepts in the fields
    of data representation and machine learning: **Encoding** and **Embedding**. The
    content of this article is partly taken from one of my lectures in [CS246 Mining
    Massive DataSet (MMDS) course at Stanford University](https://web.stanford.edu/class/cs246/).
    I hope you find it useful.'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All Machine Learning (ML) methods work with input feature vectors and almost
    all of them require input features to be *numerical*. From a ML perspective, there
    are four types of features:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Numerical (continuous or discrete)*: numerical data can be characterized by
    continuous or discrete data. Continuous data can assume any value within a range
    whereas discrete data has distinct values. Example of continues numerical variable
    is *`height`*, and an example of discrete numerical variable is *`age`*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Categorical (ordinal or nominal)*: categorical data represents characteristics
    such as eye color, and hometown. Categorical data can be ordinal or nominal. In
    ordinal variable, the data falls into ordered categories that are ranked in some
    particular way. An example is *`skill level`* that takes values of *[`beginner`,
    `intermediate`, `advanced`]*. Nominal variable has no order among its values.
    An example is *`eye color`* that takes values of *[`black`, `brown‚Äô, `blue`, `green`]*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Time series*: Time series is a sequence of numbers collected at regular intervals
    over some period of time. This data is ordered in time unlike previous variables.
    An example of this is *`average of home sale price over years in USA`*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Text*: Any document is a text data, that we often represent them as a ‚Äòbag
    of words‚Äô.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To feed any variables to an ML model, we have to convert them into numerical.
    Both encoding and embedding techniques do this trick.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Encoding is the process of converting raw data, such as text, images, or audio,
    into a structured numerical format that can be easily processed by computers.
    There are two ways to encode a categorical variable:'
  prefs: []
  type: TYPE_NORMAL
- en: 1Ô∏è‚É£ Integer encoding
  prefs: []
  type: TYPE_NORMAL
- en: 2Ô∏è‚É£ One-hot encoding
  prefs: []
  type: TYPE_NORMAL
- en: 3Ô∏è‚É£ Multi-hot encoding (this is the extension of one-hot encoding)
  prefs: []
  type: TYPE_NORMAL
- en: 'To explain each method let‚Äôs work through the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: üé¨ Consider a tiny movie dataset containing only 4 movies and 5 features.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/81d64c7f84e6ea98a7e74640933791c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: movie dataset ‚Äî Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: As we see, two features (release year, IMDB rating) are numerical, one feature
    (title) is text, and the remaining two (provider, IMDB genres) are categorical.
    Let‚Äôs see how encoding methods apply to these.
  prefs: []
  type: TYPE_NORMAL
- en: 1Ô∏è‚É£ **Integer encoding:**
  prefs: []
  type: TYPE_NORMAL
- en: 'This method assigns an integer to each category value. For example if *provider*
    variable takes four distinct values *`[Netflix, Prime Video, HBO Max, Hulu]`*,
    we assign them integers 1, 2, 3 and 4 respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Netflix -> 1, Prime Video -> 2, HBO Max ->3 , Hulu -> 4*'
  prefs: []
  type: TYPE_NORMAL
- en: The pro of this approach is that it provides a dense representation. The con
    is that it implies ordering between different categories, i.e.
  prefs: []
  type: TYPE_NORMAL
- en: '*Netflix < Prime Video < HBO Max < Hulu.*'
  prefs: []
  type: TYPE_NORMAL
- en: So it might makes more sense to use integer encoding for ordinal variables,
    e.g. for *`education‚Äô* taking values of *‚Äò[Diploma, Undergrad, Masters, PhD ]`*.
    However it still implies values are equally spaced out üôä!! Obviously, this is
    undesirable, so let‚Äôs move to the next method.
  prefs: []
  type: TYPE_NORMAL
- en: 'üíª In Python, you can perform integer encoding using various libraries, such
    as scikit-learn or TensorFlow/Keras. Here, we use scikit-learn‚Äôs `LabelEncoder`
    for encoding categorical labels into integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 2Ô∏è‚É£ **One-hot encoding:**
  prefs: []
  type: TYPE_NORMAL
- en: 'This method first applies integer encoding, then creates a binary vector that
    represents the numerical values e.g. for *`provider`* variable, we assign integers
    first: *Netflix -> 1, Prime Video -> 2, HBO Max ->3 , Hulu -> 4*. Then we create
    a binary vector of length 4 for each value as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1da255730afaec50cfc5c1747bde6fad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: one-hot vectors of provider variable ‚Äî Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: As you see, this method explodes the dimension of the feature vector to number
    of values the categorical feature takes üôà!! That can quickly get out of hands.
  prefs: []
  type: TYPE_NORMAL
- en: 'üíª In Python, you can perform one-hot encoding using libraries like scikit-learn
    or pandas. Here‚Äôs how to do it using scikit-learn‚Äôs `OneHotEncoder`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, let‚Äôs look at the extension of this method too.
  prefs: []
  type: TYPE_NORMAL
- en: 3Ô∏è‚É£ **Multi-hot encoding:**
  prefs: []
  type: TYPE_NORMAL
- en: Multi-hot encoding is an extension of one-hot encoding when a categorical variable
    can take multiple values at the same time. For example, there are 28 distinct
    IMDB genres, and a movie can take multiple genres, e.g. the movie `*stranger things`*
    is *drama, fantasy, horror* at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0a1bfdeb0a244d9794a44801dcf62229.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: IMDB genres with their integer encoding ‚Äî Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying multi-hot encoding to movies‚Äô genres give us 28-dimensional encoding
    vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0926c9622ff5b2f628471caa7a464357.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: multi-hot encoding of genres for each movie ‚Äî Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: We see that obviously, multi-hot encoding suffers from same caveat as one-hot
    encoding which is dimensionality explosion.
  prefs: []
  type: TYPE_NORMAL
- en: 'üíª We can use scikit-learn or pandas to achieve multi-hot encoding in Python.
    Here‚Äôs how to do it using scikit-learn‚Äôs `MultiLabelBinarizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, applying all above encodings on the movie dataset results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5f32868ac44b63c1710dbd1d0532d864.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: movie dataset with all encodings ‚Äî Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: As we see, after applying all encodings, dimension of data increased from 5
    to 35 üôä!! In fact, It will blow up to thousands or a million if we multi-hot encode
    the *`title`* variable too!
  prefs: []
  type: TYPE_NORMAL
- en: 'üììThe takeaway is that one-hot and multi-hot encodings are not practical for
    features with large value sets. In a corpus of documents with one million distinct
    words, representing each document via multi-hot encoding creates vectors that
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'high dimensional: multi-hot encodings will create 1-million dimensional vectors!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'sparse: Since an average document contains 500 words, the multi-hot vectors
    will be 99.95% sparse'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'short of semantic: encoding of two words ‚Äògood‚Äô and ‚Äògreat‚Äô are as different
    as encoding of ‚Äògood‚Äô and ‚Äòbad‚Äô!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‚úèÔ∏è In a nutshell, use one-hot/multi-hot encoding when the number of categories
    is small; usually lesser than 15 or so. For text data which has millions of categories
    (every word as one category) we have to use more efficient methods.
  prefs: []
  type: TYPE_NORMAL
- en: In the rest of the article, we work with the problem of *`computing word embeddings`*.
    Through this example, we will study few fundamental embedding methods.
  prefs: []
  type: TYPE_NORMAL
- en: '**Embedding**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To address above shortcomings, we move from high dimensional sparse vector to
    short dense vectors; these vectors are called ***embeddings***. An embedding is
    a translation of a high-dimensional vector into a low-dimensional space and captures
    semantic similarity.
  prefs: []
  type: TYPE_NORMAL
- en: '*‚ÄúOne of the benefits of using dense and low-dimensional vectors is computational:
    The majority of neural network toolkits do not play well with very high-dimensional,
    sparse vectors.‚Äù [5]*'
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs first look at a very simple embedding method called SVD.
  prefs: []
  type: TYPE_NORMAL
- en: Singular Value Decomposition (SVD)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The simplest embedding method is perhaps *Singular Value Decomposition (SVD)*
    that takes an input matrix *A* and decompose it into three matrices as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9a9ee8fc2dece9e9a50e7c01b3a60009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: SVD decomposition ‚Äî Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: '***U, V*** are left and right singular vectors, respectively. They are column
    orthonormal meaning that each column in them has norm of one, and every two column
    in U (and in V, respectively) are orthogonal. In mathematic, we write this as'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0dffcbbba175de107ca99c532e87b0a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Both *U* and *V* define an r-dimensional subspace, hence projecting ***A***
    onto them produces r-dimensional embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7d21a53734ef4b03161ed22eda0321be.png)'
  prefs: []
  type: TYPE_IMG
- en: üóí **Let‚Äôs look at an example together** üóí**:**
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a corpus of documents, we can use SVD to compute document embeddings
    & word embeddings. Here are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Convert it to bag of words (BOW) vectors and get a term-document
    matrix. We can use term frequencies (tf), or normalize using tf-idf technique.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ddecdc930870ddb7d838bb92b70c37c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: term frequency matrix ‚Äî Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2**: apply SVD on the term-document matrix and pick a value *r < rank(A)*.
    This will create three matrices, each of rank r.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a3eb55940c85f8504f4a232336853145.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: SVD decomposition of matrix A ‚Äî Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3**: compute embedding of documents as'
  prefs: []
  type: TYPE_NORMAL
- en: '*emb = [<doc, v1> , <doc, v2> , <doc, v3>]*'
  prefs: []
  type: TYPE_NORMAL
- en: The first two dot-products are shown in the figure below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1f92b7d90b085ee574cde786fb5bfc3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: embedding of first document ‚Äî only two dot-product operation are
    shown ‚Äî Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we compute all three vector dot-products, the embedding of the document
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f2b73592e696a5c5c60006e9ccb5ee3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: embedding of first document ‚Äî Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly we can compute word (term) embeddings as *emb = [<term, u1> , <term,
    u2> , <term, u3>].*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c3809d1aa915f00bde28ec158c6be8a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: term embedding for first term ‚Äî Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: We can show that these representations group similar terms and documents together
    in the 3-dimensional space. Terms and documents that are related or have similar
    context tend to have similar representations in this reduced space.
  prefs: []
  type: TYPE_NORMAL
- en: 'üíª In python, we use scikit-learn to convert a corpus of document to tf-idf
    matrices and then apply SVD on them. Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Overall, SVD is a simple and powerful technique for maintaining semantic information
    however, it is impractical on real-world datasets. It‚Äôs computationally heavy
    and does not utilize sparsity of the input matrix at all. Let‚Äôs see how we can
    avoid these shortcomings in another method.
  prefs: []
  type: TYPE_NORMAL
- en: '**Neural Networks as Embedder**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'üåüState of the art embedders are among Neural Networks (NN). There are many
    NN techniques to compute word embeddings: Word2Vec, Glove, BERT, fastText, etc.
    In this article, we look at *Word2Vec* which was developed by [Tomas Mikolov](https://scholar.google.com/citations?user=oBu8kMMAAAAJ&hl=en)
    and his colleagues at Google in a series of papers published between 2013 and
    2014 ([paper](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Word2Vec* is a statistical, self-supervised, and task independent method.
    It comes in two flavors: Continuous bag of words (CBOW), and Skip Gram. The two
    flavors are very similar, both use a shallow neural network with only one hidden
    layer and no activation function to learn the words representations. In this article,
    we study the skip-gram model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'üîë The key idea of word2Vec is that words with similar context have similar
    meanings. The more often a word appears in the ***context*** of certain other
    words, the closer they are in meaning. *Context* of a word are few words to its
    left and few words to its right. Formally, given a document, we set a window size
    (e.g. window_size = 2). Then for any given word in the document (call it *`target`*
    word), the *`window_size`* words to its left and *`window_size`* words to its
    right are its *context* . For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b9f9598081ebed87d9e2cac044fe371.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: target and context words (window size = 2 )‚Äî Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a document, we can slide the window from left to right and find all pairs
    of *(target, context)* words. Consider the following document:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Document: *‚ÄúI read sci-fi books and drink orange juice‚Äù*,'
  prefs: []
  type: TYPE_NORMAL
- en: Let window size = 2\. Then the set of (target, context) words are depicted below.
    The highlighted word in the image is the `target` word, and other words in the
    box are `context` words.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79792923bb530410d7b8ecd8577fa349.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: sliding window on a document to extract (target, context) words
    ‚Äî Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, that we know the meaning of target and context words, let‚Äôs see how Word2vec
    uses them. Word2vec is a 2-layer neural network that takes a target word as input,
    and predicts all context words in the window. Here is a schematic view of its
    architecture for window size = 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3f133049b4f0df7d8f3ce51032f4b5ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: skip-gram architecture for window size = 2 ‚Äî Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can guess why this architecture is called skip-gramüôÇ‚Ä¶ It‚Äôs because it
    predicts all the grams in the context except the target word as it is the input
    to the model, hence the name is skip-gram.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let‚Äôs focus on predicting only one context word and dive into details.
    Let *V = size of the vocabulary* and *N = embedding dimension*, i.e. the size
    of the only hidden layer. Then the architecture for predicting one context word
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04747218c33cee937ef2ad6544131be0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: skip-gram architecture for predicting one context word ‚Äî Image by
    the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'üß¨The input layer is the one-hot encoding of the target word i.e. w(t); since
    we have V word in our vocabulary the input layer is V-dimensional. The hidden
    layer is N-dimensional and produces the embeddings. Often *N << V,* for example
    in a corpus of web documents, we have millions of tokens (V is of order of millions)
    while N is somewhere between 256 to 512\. The point of the hidden layer is to
    map the words into a lower dimensionality while maintaining separation between
    dissimilar words. This layer does not use any activation function. The reason
    is most activation functions involve some ‚Äúsquishing‚Äù of space in one region and
    ‚Äúexpanding‚Äù of space in the other e.g., sigmoid/tanh will ‚Äúsquish together‚Äù all
    values < -1, and same with all values >1\. RELU would ‚Äúsquish together‚Äù all values
    <0, eliminating half of the representative capacity. As you can see, this actually
    *hurts* the separation quality, reducing the amount of ‚Äúword mapping space‚Äù available.
    The output layer is V dimensional, a softmax function applies on this layer therefore
    each neuron in the output layer is a probability and all neurons sum up to 1\.
    The j-th neuron in output layer i.e. y_j, indicates the probability that the j-th
    word is the context word. As we see, there are two weight matrices involved:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e9756e6106ff636cf8b364a37c9067c.png)'
  prefs: []
  type: TYPE_IMG
- en: Given the topology of the network, if ***x*** is the input and ***y*** is the
    output, then
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/188405c242c36054b4f5597c4e1ef14f.png)'
  prefs: []
  type: TYPE_IMG
- en: After the network is trained, we obtain the embedding of a word by multiplying
    its one-hot vector *x* with the weight matrix *W*.
  prefs: []
  type: TYPE_NORMAL
- en: 'üîÅ**How is the network trained?** Since there are no labels, we will create
    a fake task üòÉ! The fake task is:'
  prefs: []
  type: TYPE_NORMAL
- en: 'üìë*The fake task: `given a target word, predict its context words`*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, there are two questions: 1) how to make the training data, 2) what loss
    function to choose to train the network?'
  prefs: []
  type: TYPE_NORMAL
- en: 'üóí**How to make the training data?** We take our corpus of *data = {d1, d2,
    ‚Ä¶.}*, we might collect millions of documents, wiki pages, blot posts, etc. Then
    we tokenize all documents and build a vocabulary. There are many tokenization
    methods available e.g. workpiece, BytePairEncoding (BPE), k-gram. The simplest
    method is k-gram. If you take *k=1* and do tokenization by words (instead of characters)
    then 1-gram is equivalent to split sentences by space! After tokenization, we
    have a list of tokenized documents. We then move the sliding window over tokenized
    documents and collect training data as pairs of (target, context). See the example
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29424af3ffd1c684d0908e195c8a4f3f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: collecting training data from a document ‚Äî Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'üìà**what loss function to choose?** We use cross-entropy loss function. The
    reason is we train against the target-context pairs (w_t, w_c) where w_t is the
    target word and w_c is the context word. The context word w_c represents the ideal
    prediction, given the target word w_t. Note that W_c is represented as one-hot,
    i.e. it has value 1 at some position j and other positions are 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c99d46e9872335b145a7d9a2b0863fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: one-hot encoding of context word ‚Äî Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: The loss function needs to evaluate the output layer at the same position j,
    i.e. y_j. (Remember y is a probability distribution; ideal value of y_j is being
    1). The cross-entropy loss function is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03abb4184f315c05058444fd1c8ecdf2.png)'
  prefs: []
  type: TYPE_IMG
- en: For the example above,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/185c44256b650df1b21faa401bcbfbae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: cross entropy loss in an example ‚Äî Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'üî•**Training the neural network**: Now that the loss function and training data
    are clear, we will train the network to learn the weights. We want to find the
    values of ***W*** and ***W‚Ä≤*** that minimize the loss function. We do so by running
    back-propagation and updating parameters based on derivative of the loss. This
    concludes the word2vec algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: üìä**Results on word embeddings:** Below, we see an example of vector representations
    of four words *‚Äúman,‚Äù ‚Äúwoman,‚Äù ‚Äúking‚Äù ‚Äúqueen‚Äù* that illustrate how Word2Vec captures
    semantic. In the context of Word2Vec, the relationships between words can be represented
    as vector arithmetic. The algorithm captures relationships like *‚Äúking ‚Äî man +
    woman = queen‚Äù.* This is possible because the vector representation of ‚Äúking‚Äù
    minus the vector representation of ‚Äúman‚Äù plus the vector representation of ‚Äúwoman‚Äù
    results in a vector that is very close to the vector representation of ‚Äúqueen.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf41ccda2f851553c0d66f601054506e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: word embeddings capture semantics ‚Äî Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: It‚Äôs important to note that Word2Vec is trained on large text corpora, and the
    quality of learned embeddings depends on the quantity and quality of the training
    data. Additionally, while Word2Vec can capture many semantic relationships, it
    might not capture all possible nuances of language and meaning. Other word embedding
    algorithms and models have since been developed that further enhance the understanding
    of word relationships.
  prefs: []
  type: TYPE_NORMAL
- en: '**Item-Item Neural Collaborative Filtering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will look into another example of learning embeddings through
    neural network. This network models item-item collaborative filtering algorithm.
    We will study this network through the use case of video recommendation.
  prefs: []
  type: TYPE_NORMAL
- en: 'üé•**Video recommendation usecase**: Consider the use case of video recommendation
    where we have 1 million videos, and 500,000 users who have watched some of these
    movies, and the task at hand is to recommend videos to users. We want to solve
    this problem using neural networks, so we formulate it as multi-class classification
    where each video is a class. We design a neural network to learn embeddings for
    videos such that similar videos have similar embeddings. Let‚Äôs build the training
    data first, and then design the network architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: üóí**Building Training Data:** Given users logs that contain videos users have
    watched, we sort out watched videos of each user in ascending order in time. Then
    split them in time on a fixed proportion. We take the first split as train data
    and the second split as test data. Note that if our split ratio is 70‚Äì30, the
    data of each user is split on this ratio. By this time-based split, our task has
    implicitly become *‚Äòpredicting what users watch next‚Äô*.
  prefs: []
  type: TYPE_NORMAL
- en: Another way of splitting is to avoid sorting and randomly hold out few videos
    of each user as test data, and use the rest to build train data. This approach
    however, runs the risk of leaking information from train to test.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/45466c3c9f7f17936df09c768aa2daa0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: video recommendation train-test split ‚Äî Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: Once data is split, we will build train data as pairs of (movie1, movie2) where
    both movies are watched by the same user.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6cd414b95017f6fef54ed2233320aeb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: building training data, and test data ‚Äî Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: We then build a neural network that performs item-item collaborative filtering
    while learning 3-dim embeddings!! (Three is too small but for illustrative purposes
    we continue with it.)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/051b00575dd48aa45092f806b2752891.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: item-item neural collaborative filtering ‚Äî Image from [1] modified
    by author'
  prefs: []
  type: TYPE_NORMAL
- en: For every pair (m1, m2) in the training data, we feed one-hot vector of m1 in
    the input layer in blue, and feed m1 feature vectors (containing metadata such
    as genre, cast, director, popularity, etc.) in the yellow input layer. The one-hot
    of m2 will be used to compute the loss function. We want the model to learn the
    association between m1 and m2\. The more often m1 and m2 happen together in the
    training data, the closer their embedding will become.
  prefs: []
  type: TYPE_NORMAL
- en: '**How to recommend a movie to a user?** Assume Alice has watched m1, m4, m5\.
    To recommend a movie to her, we can find movies that have similar embeddings to
    m1\. For any movie v we compute the similarity score as *score= <emb(m1), emb(v)>*.
    We find the top 5 movies with highest similarity score, and recommend them to
    Alice. An even better recommendation is to repeat above for m1, m4 and m5, and
    recommend movies in intersection of these sets.'
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our section on item-item collaborative filtering. If you want
    to read more on neural collaborative filtering, take a look at [this paper](https://arxiv.org/abs/1708.05031).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we discussed encoding and embedding methods. We learnt that
    encoding refers to the process of converting raw data, such as text, images, or
    audio, into a structured format that can be easily processed by computers. This
    transformation often involves reducing the dimensionality of the data while retaining
    its essential features. On the other hand, embedding involves mapping data points
    into a lower-dimensional space, where each point is represented by a vector of
    continuous values. Embeddings are designed to capture semantic relationships and
    similarities between data points, enabling algorithms to effectively learn patterns
    and make meaningful predictions. Both encoding and embedding play crucial roles
    in various applications, from natural language processing and computer vision
    to recommendation systems and anomaly detection, enhancing the efficiency and
    effectiveness of data analysis and machine learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have any questions or suggestions, feel free to reach out to me:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Email: mina.ghashami@gmail.com'
  prefs: []
  type: TYPE_NORMAL
- en: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Google blog post on embedding](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture#:~:text=An%20embedding%20is%20a%20relatively,like%20sparse%20vectors%20representing%20words.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Distributed Representations of Words and Phrases and their Compositionality](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Learning embeddings ‚Äî CS246, Stanford University](https://web.stanford.edu/class/cs246/slides/14-emb.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Neural collaborative filtering](https://arxiv.org/abs/1708.05031)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Neural Network Methods in Natural Language Processing](http://amzn.to/2wycQKA),
    2017'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
