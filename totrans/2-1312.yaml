- en: Implement and Train a CNN from Scratch with PyTorch Lightning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始用 PyTorch Lightning 实现和训练 CNN
- en: 原文：[https://towardsdatascience.com/implement-and-train-a-cnn-from-scratch-with-pytorch-lightning-ce22f7dfad83](https://towardsdatascience.com/implement-and-train-a-cnn-from-scratch-with-pytorch-lightning-ce22f7dfad83)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/implement-and-train-a-cnn-from-scratch-with-pytorch-lightning-ce22f7dfad83](https://towardsdatascience.com/implement-and-train-a-cnn-from-scratch-with-pytorch-lightning-ce22f7dfad83)
- en: If you are not using PyTorch Lightning, you should give it a try.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如果你还没有使用 PyTorch Lightning，你应该尝试一下。
- en: '[](https://medium.com/@betty.ld?source=post_page-----ce22f7dfad83--------------------------------)[![Betty
    LD](../Images/1a908f5bcdb6cbc3be5f889f52d743e6.png)](https://medium.com/@betty.ld?source=post_page-----ce22f7dfad83--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ce22f7dfad83--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ce22f7dfad83--------------------------------)
    [Betty LD](https://medium.com/@betty.ld?source=post_page-----ce22f7dfad83--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@betty.ld?source=post_page-----ce22f7dfad83--------------------------------)[![Betty
    LD](../Images/1a908f5bcdb6cbc3be5f889f52d743e6.png)](https://medium.com/@betty.ld?source=post_page-----ce22f7dfad83--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ce22f7dfad83--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ce22f7dfad83--------------------------------)
    [Betty LD](https://medium.com/@betty.ld?source=post_page-----ce22f7dfad83--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ce22f7dfad83--------------------------------)
    ·19 min read·Aug 8, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ce22f7dfad83--------------------------------)
    ·19 分钟阅读·2023年8月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3c624d26acc13295bc8fc840a43add42.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c624d26acc13295bc8fc840a43add42.png)'
- en: The abstract idea of PyTorch Lightning. From [Marc Sendra Martorell](https://unsplash.com/@marcsm).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Lightning 的抽象概念。来自 [Marc Sendra Martorell](https://unsplash.com/@marcsm)。
- en: This article is a gentle introduction to Convolution Neural Networks (CNNs).
    This article details why PyTorch Lightning is so great, then makes a brief theoretical
    walkthrough of CNN components, and then describes the implementation of a training
    loop for a simple CNN architecture coded from scratch using the PyTorch Lightning
    library.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是对卷积神经网络（CNN）的温和介绍。本文详细说明了为什么 PyTorch Lightning 非常出色，然后对 CNN 组件进行简要的理论讲解，接着描述了如何使用
    PyTorch Lightning 库从头开始实现一个简单 CNN 架构的训练循环。
- en: '**Why PyTorch Lightning?**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**为什么选择 PyTorch Lightning？**'
- en: 'PyTorch is a flexible and user-friendly library. If PyTorch is great for research,
    I found Lightning even greater for engineering. The main advantages are:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 是一个灵活且用户友好的库。如果说 PyTorch 对研究很棒，那么我发现 Lightning 对工程更为出色。主要优点包括：
- en: '**Less code.** When running an ML project, so many things can go wrong that
    it is helpful to delegate the boilerplate code and focus on what is relevant for
    solving my specific problem. Using the built-in functionalities is reducing the
    amount of written code, and therefore the probability of bugs. The development
    (and debugging) time is reduced.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更少的代码**。在运行机器学习项目时，许多事情可能会出错，因此将样板代码委托出去，专注于解决我具体问题的相关内容是很有帮助的。使用内置功能可以减少书写代码的数量，从而降低错误的概率。开发（和调试）时间也会减少。'
- en: '**Well-structured code**.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构良好的代码**。'
- en: '**Efficiency and fast training**. Lightning also allows the use of all the
    multiprocessing and parallel workers tricks (like [DDP](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html))
    from PyTorch without coding an extra line.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高效和快速的训练**。Lightning 还允许使用 PyTorch 的所有多进程和并行工作器技巧（如 [DDP](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)），无需额外编写代码。'
- en: Built-in **development tools** like sanity checks (for validation and training
    loops as well as for the model architecture), creation of overfit dataset on the
    fly, early stopping callbacks, best weights management, etc. For example [https://lightning.ai/docs/pytorch/stable/debug/debugging_basic.html](https://lightning.ai/docs/pytorch/stable/debug/debugging_basic.html)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内置的 **开发工具** 如一致性检查（用于验证和训练循环以及模型架构）、即时创建过拟合数据集、早期停止回调、最佳权重管理等。例如 [https://lightning.ai/docs/pytorch/stable/debug/debugging_basic.html](https://lightning.ai/docs/pytorch/stable/debug/debugging_basic.html)
- en: For more official reasons why it is great, check out [there](https://pytorch-lightning.readthedocs.io/en/0.10.0/introduction_guide.html#why-pytorch-lightning).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多官方的优秀理由，请查看 [这里](https://pytorch-lightning.readthedocs.io/en/0.10.0/introduction_guide.html#why-pytorch-lightning)。
- en: In a nutshell, when using PyTorch Lightning I found it easy to code, easy to
    read, and easy to debug. These activities consist of what takes most of my time
    as a Machine Learning Engineer. Moreover, the documentation is well-written and
    contains many tutorials so it is also easy to [learn](https://lightning.ai/docs/pytorch/stable/levels/core_skills.html).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，使用 PyTorch Lightning 时，我发现它易于编写、易于阅读和调试。这些活动占据了我作为机器学习工程师的大部分时间。此外，文档写得很好，包含了许多教程，因此也很容易[学习](https://lightning.ai/docs/pytorch/stable/levels/core_skills.html)。
- en: CNN models refresher
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN 模型回顾
- en: LeNet is a good starting point when learning or rehearsing deep learning architectures
    for computer vision. LeNet was the first successful Convolutional Neural Network
    (CNN) architecture designed by Yann LeCun et al. in 1998, designed for handwritten
    digit recognition.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet 是学习或复习计算机视觉深度学习架构的一个很好的起点。LeNet 是 Yann LeCun 等人在 1998 年设计的第一个成功的卷积神经网络（CNN）架构，旨在进行手写数字识别。
- en: We’ll explain first the main components of the standard CNN module through the
    components of LeNet architecture.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先通过 LeNet 架构的组件解释标准 CNN 模块的主要组成部分。
- en: 'LeNet is composed of three types of layers:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet 由三种类型的层组成：
- en: Convolutional layers
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层
- en: Pooling layers
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层
- en: Fully Connected layer
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接层
- en: 1\. Convolutional Layers
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 卷积层
- en: 'The convolutional layers are responsible for extracting features from an input
    layer. With CNN, this first “layer” is often an image. Each convolutional layer
    consists of a set of learnable **filters** (also called “kernels”) that slide
    over the input layer, applying an operation called “convolution”. The convolution
    performs an element-wise multiplication and summation between the filter and the
    local area in the image. Since a (nonlinear) **activation function** is applied
    on the output feature maps, this output is also called “activation”. In this article,
    we use the most popular activation function: ReLU, shown in blue below.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层负责从输入层中提取特征。使用 CNN 时，这第一层通常是图像。每个卷积层由一组可学习的**滤波器**（也称为“内核”）组成，这些滤波器滑动在输入层上，应用一种称为“卷积”的操作。卷积在滤波器和图像中的局部区域之间执行逐元素乘法和求和。由于对输出特征图应用了（非线性）**激活函数**，所以该输出也称为“激活”。在本文中，我们使用最流行的激活函数：ReLU，如下图所示。
- en: '![](../Images/b53e41645b4b10b07065d17a70413a68.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b53e41645b4b10b07065d17a70413a68.png)'
- en: Rectified Linear Unit (ReLU) f(x)=max(0,x) and its variant GeLU. [Image under
    Creative Common License](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#/media/File:ReLU_and_GELU.svg).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 线性整流单元（ReLU）f(x)=max(0,x)及其变体 GeLU。[图片使用 Creative Common 许可证](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#/media/File:ReLU_and_GELU.svg)。
- en: Each convolutional layer is followed by an activation function mainly in order
    to add non-linearity. Without such the model would behave like a regular linear
    model regardless of its depth.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 每个卷积层后面都跟着一个激活函数，主要是为了加入非线性。没有这样的激活函数，模型将表现得像一个普通的线性模型，无论其深度如何。
- en: More about convolutional layers
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多关于卷积层的内容
- en: The next section explains how the convolution layers are used in CNN and how
    they enable CNN to perform extremely well on a range of computer vision tasks.
    In a nutshell, the **hierarchical** nature of successive convolutional layers
    makes it so efficient for image recognition tasks. One convolutional layer is
    not efficient, but a stack of them is.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分解释了卷积层在 CNN 中如何使用，以及它们如何使 CNN 在各种计算机视觉任务中表现极其出色。简而言之，连续卷积层的**层次结构**使其在图像识别任务中非常高效。一层卷积层效率不高，但一堆卷积层就很有效。
- en: The first layers enable the model to simple and generic patterns in a local
    area of the input image and then, the deeper layers grasp more complex and abstract
    representations.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层使模型能够在输入图像的局部区域中识别简单和通用的模式，然后，深层会抓取更复杂和抽象的表示。
- en: '![](../Images/f08fc6fe0e3d4d48fb3e3d67813b752d.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f08fc6fe0e3d4d48fb3e3d67813b752d.png)'
- en: The convolutional [operation is illustrated](https://commons.wikimedia.org/wiki/File:Convolution_arithmetic_-_Full_padding_no_strides.gif).
    The filter size is 3x3, the stride is 1 and there is zero padding amount is 2\.
    Image under MIT license.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积[操作的示意图](https://commons.wikimedia.org/wiki/File:Convolution_arithmetic_-_Full_padding_no_strides.gif)。滤波器大小为
    3x3，步幅为 1，零填充量为 2。图片使用 MIT 许可证。
- en: The **first convolutional layers** of a CNN have typically a small **spatial
    extent** (for example 3x3 pixels for the first convolutional layer of VGG16, 5x5
    pixels for LeNet, 11x11 for AlexNet).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的**第一卷积层**通常具有较小的**空间范围**（例如，VGG16的第一卷积层为3x3像素，LeNet为5x5像素，AlexNet为11x11像素）。
- en: After training, these layers would detect simple patterns like edges and corners
    similar to [classical computer vision technique](https://www.cs.toronto.edu/~urtasun/courses/CV/lecture02.pdf)s.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，这些层将检测类似于[经典计算机视觉技术](https://www.cs.toronto.edu/~urtasun/courses/CV/lecture02.pdf)的简单模式，如边缘和角点。
- en: Then, the filters become more sophisticated in the **intermediate convolutional
    layers**. Their relative size to the input layer is bigger than earlier layers,
    therefore they have more context (as they see a broader part of the image than
    the previous convolutional layer) and they can start detecting higher-level features
    like complex textures, shapes, and object parts.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，**中间卷积层**中的滤波器变得更加复杂。相对于输入层，它们的相对尺寸比早期层大，因此它们拥有更多的上下文（因为它们能看到比前一卷积层更广泛的图像区域），并且可以开始检测更高层次的特征，如复杂的纹理、形状和物体部件。
- en: For example in an image of a [Japanese set meal](https://kome-academy.com/en/teishoku_library/),
    these layers may detect the chopsticks, the rice bowl, and the miso soup.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一张[日本套餐](https://kome-academy.com/en/teishoku_library/)的图像中，这些层可能会检测到筷子、米饭碗和味增汤。
- en: '![](../Images/daead1a1554228f49844911728e308ef.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/daead1a1554228f49844911728e308ef.png)'
- en: '[LeNet architecture](https://www.researchgate.net/figure/LeNet-is-a-classic-convolutional-neural-network-employing-the-use-of-convolutions_fig1_369803371).
    Image under Creative Common license.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[LeNet架构](https://www.researchgate.net/figure/LeNet-is-a-classic-convolutional-neural-network-employing-the-use-of-convolutions_fig1_369803371)。图像遵循Creative
    Common许可。'
- en: In the **last convolutional layers**, the filters have an even larger spatial
    extent (they can see more context at once in the input layer) and are more specialized
    and abstract than the generic earlier edge, color, and object parts’ detector
    layers. They represent high-level features and are essential to make an accurate
    decision in image recognition tasks. They are used to encode complex object representations
    and capture the distinctive characteristics of the different classes in the dataset.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在**最后卷积层**中，滤波器具有更大的空间范围（它们可以在输入层中一次性看到更多上下文），并且比早期的边缘、颜色和物体部件检测层更加专业和抽象。它们代表高层次特征，对于图像识别任务的准确决策至关重要。它们用于编码复杂的对象表示，并捕捉数据集中不同类别的独特特征。
- en: It can for example recognize a Japanese set meal from a French lunch.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以例如从法国午餐中识别出日本套餐。
- en: The following picture summarizes what each layer can “see”.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片总结了每一层可以“看到”的内容。
- en: '![](../Images/4df73b66459f78b7555a71345c3cac25.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4df73b66459f78b7555a71345c3cac25.png)'
- en: Illustration of the output feature maps at different convolution stages. By
    [Stanford CS231](https://cs231n.github.io/convolutional-networks/). Image from
    [https://github.com/cs231n/cs231n.github.io/blob/master/convolutional-networks.md](https://github.com/cs231n/cs231n.github.io/blob/master/convolutional-networks.md)
    under MIT license.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 不同卷积阶段的输出特征图的示例。由[斯坦福 CS231](https://cs231n.github.io/convolutional-networks/)提供。图像来自[https://github.com/cs231n/cs231n.github.io/blob/master/convolutional-networks.md](https://github.com/cs231n/cs231n.github.io/blob/master/convolutional-networks.md)，遵循MIT许可。
- en: Pooling Layers
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 池化层
- en: It is common to add a pooling layer between two successive convolutional layers.
    The pooling layer downsamples the feature map obtained from the previous convolutional
    layer reducing the spatial dimension of the data while retaining the import information.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个连续的卷积层之间添加池化层是很常见的做法。池化层对从前一个卷积层获得的特征图进行降采样，减少数据的空间维度，同时保留重要信息。
- en: '**Why are pooling layers effective?**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么池化层有效？**'
- en: They downsize the feature maps and therefore allow faster computation and lower
    memory requirements.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们缩小了特征图，因此允许更快的计算和更低的内存需求。
- en: This information aggregation step reduces the number of parameters in the model
    and also prevents overfitting.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个信息聚合步骤减少了模型中的参数数量，同时防止了过拟合。
- en: They introduce a degree of translation invariance such that the network can
    recognize certain features even though they are slightly shifted, rotated, or
    distorted in the image. Being robust to spatial variation helps the model to generalize
    better.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们引入了一定程度的平移不变性，使得网络即使在图像中略微偏移、旋转或扭曲的情况下也能识别某些特征。对空间变化的鲁棒性有助于模型更好地泛化。
- en: The most common pooling operations are MAX pooling (select the maximum value
    from the element covered by the window as the value for this window) and AVERAGE
    pooling (same but takes the average value instead).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的池化操作有最大池化（MAX pooling）（从窗口覆盖的元素中选择最大值作为该窗口的值）和平均池化（AVERAGE pooling）（相同，但取平均值）。
- en: '![](../Images/165816e82847faf5c3a030a9c351fdc1.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/165816e82847faf5c3a030a9c351fdc1.png)'
- en: Pooling operation. On the feature map on the left and on a feature slice on
    the right. Image from [Stanford CS231.](https://cs231n.github.io/convolutional-networks/)
    [MIT license](https://github.com/cs231n/cs231n.github.io/blob/master/convolutional-networks.md).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 池化操作。左侧为特征图，右侧为特征切片。图像来自 [Stanford CS231.](https://cs231n.github.io/convolutional-networks/)
    [MIT 许可证](https://github.com/cs231n/cs231n.github.io/blob/master/convolutional-networks.md)。
- en: Fully Connected Layers
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全连接层
- en: The Fully Connected (FC) layers are the last layers of the model. They are responsible
    for making high-level decisions based on the extracted features from earlier layers.
    Unlike the convolutional filters that have local visibility of the input layer,
    the FC layers connect all the activations at once from the previous output feature
    maps to the next output maps activation, as seen in regular Neural Networks. It
    basically consists of a matric multiplication followed by a bias offset.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接（FC）层是模型的最后几层。它们负责根据早期层提取的特征做出高层决策。与具有局部可见性的卷积滤波器不同，FC 层将来自先前输出特征图的所有激活一次性连接到下一个输出特征图的激活中，就像在常规神经网络中一样。它基本上由矩阵乘法和偏置偏移组成。
- en: In the LeNet model, there are three FC layers at the very end of the model.
    The last layer is an FC layer used for the classification task. Its dimension
    is (pevious_layer_output_dimension, number_of_classes).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LeNet 模型中，模型的最末端有三个全连接（FC）层。最后一层是用于分类任务的全连接层，其维度为（pevious_layer_output_dimension,
    number_of_classes）。
- en: Other classical architectures such as AlexNet, VGG, ResNet, and Inception include
    an FC layer at the end of the architecture. However, recent architectures dropped
    this layer like MobileNet, YOLO, EfficientNet, and Vision Transformers.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其他经典架构如 AlexNet、VGG、ResNet 和 Inception 在架构的末端都包括一个全连接层。然而，最近的架构如 MobileNet、YOLO、EfficientNet
    和 Vision Transformers 删除了这个层。
- en: 'The reasons to drop the FC from CNN architectures are:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从 CNN 架构中删除全连接层的原因有：
- en: Reducing the number of parameters (preventing overfitting, especially on smaller
    datasets).
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少参数数量（防止过拟合，特别是在较小的数据集上）。
- en: FC discards the spatial information present in the feature maps.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FC 丢弃了特征图中的空间信息。
- en: Flexibility. CNN architecture without FC can handle input of varying sizes,
    and avoid resizing the input image to a fixed size.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灵活性。没有全连接层的 CNN 架构可以处理不同尺寸的输入，避免将输入图像调整为固定大小。
- en: The generic CNN architecture
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通用的卷积神经网络（CNN）架构
- en: The most common form of a CNN architecture is a stack of {Convolutional layer+non
    linear activation function} layers followed by {Pooling layers} applied successively
    until the image has been spatially downsized to a smaller size with more channels.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的 CNN 架构形式是 {卷积层+非线性激活函数} 层的堆叠，后跟 {池化层} 逐次应用，直到图像在空间上被缩小到更小的尺寸并增加更多通道。
- en: Lastly, it is common to have the final FC layer outputting the class scores.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，最终的全连接层通常会输出类别分数。
- en: 'To summarize, a CNN is structured as below:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，CNN 的结构如下：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'With usually ([source](https://cs231n.github.io/convolutional-networks/)):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通常是（[source](https://cs231n.github.io/convolutional-networks/)）：
- en: 0 ≤ N ≤ 3 stacked convolutions.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 ≤ N ≤ 3 个堆叠的卷积层。
- en: M≥0 pooled blocks.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: M≥0 池化块。
- en: 0≤ K< 3 FC stacked layers.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0≤ K< 3 个堆叠的全连接层。
- en: Model constraints ([source](https://cs231n.github.io/convolutional-networks/))
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型约束（[source](https://cs231n.github.io/convolutional-networks/)）
- en: It is usually recommended to ML practitioners to use existing state-of-the-art
    architectures rather than creating their own. However, it is important to be aware
    of the spatial constraints when using convolutional nets. For example, after applying
    a convolution layer with W as the input layer size (width or height), F as the
    filter size, P as the padding size, and S as the stride size, the output feature
    maps have a size Output size = ((W -F + 2P) / S ) +1\. For each convolution, the
    parameters W, F, P and S need to be chosen such as the output size is an integer.
    Usually adding padding solves most issues.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通常建议机器学习从业者使用现有的最先进架构，而不是自行创建。然而，在使用卷积网络时，需要注意空间约束。例如，在应用卷积层时，设W为输入层大小（宽度或高度），F为滤波器大小，P为填充大小，S为步幅大小，则输出特征图的大小为
    Output size = ((W -F + 2P) / S ) +1\。对于每个卷积，参数W、F、P和S需要选择使输出大小为整数。通常添加填充可以解决大多数问题。
- en: 'Other common constraints are:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其他常见约束包括：
- en: The input image should be divisible by 2 many times, depending on the depth
    of the model. This requirement comes from the pooling layers.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入图像应能够被2多次整除，具体取决于模型的深度。这个要求来自池化层。
- en: The convolutional layers should use small filters and small strides.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层应使用小滤波器和小步幅。
- en: Using “same-size” padding is recommended. If we keep the same feature size before
    and after applying a convolution, we delegate all the downsizing operations to
    the pooling layers and the architecture becomes easier to understand.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐使用“相同大小”的填充。如果我们在应用卷积前后保持相同的特征大小，我们将所有缩小操作委托给池化层，这使得网络结构更容易理解。
- en: LeNet model implementation
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LeNet模型实现
- en: Enough theory, now we’ll implement LetNet CNN with PyTorch Lightning. LeNet
    has been chosen as an example due to its simplicity and its small size.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 够多理论了，现在我们将使用PyTorch Lightning实现LeNet CNN。由于其简单性和小巧的规模，LeNet被选择作为示例。
- en: Model implementation
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型实现
- en: In PyTorch, a new module inherits from a `[pytorch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#module)`.
    In PyTorch Lighthing, the model class inherits from `ligthning.pytorch.LightningModule`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，一个新的模块继承自`[pytorch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#module)`。在PyTorch
    Lightning中，模型类继承自`ligthning.pytorch.LightningModule`。
- en: You can use the `ligthning.pytorch.LightningModule` exactly as the `nn.Module`
    class, it just includes more functionalities.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像使用`nn.Module`类一样使用`ligthning.pytorch.LightningModule`，它仅包含更多功能。
- en: 'The model’s module takes two arguments:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 模块的两个参数是：
- en: The number of input channels (which is 1 for grayscale images).
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入通道的数量（对于灰度图像为1）。
- en: The number of classes in the classifier (which is 10 for the MNIST dataset).
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类器中的类别数（对于MNIST数据集为10）。
- en: In PyTorch, the model is described in two parts `__init__()` and `forward()`.
    `__init__()` declares every component having learnable parameters as the initialization
    method. It optionally also contains more declarations like activation functions.
    Then, the forward method `forward()` applies all the layers and functions successively
    on the input image.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，模型由`__init__()`和`forward()`两个部分描述。`__init__()`声明每个具有可学习参数的组件作为初始化方法。它还可以包含更多声明，如激活函数。然后，`forward()`方法对输入图像依次应用所有层和函数。
- en: LeNet architecture is composed of two stacked convolutional blocks, each followed
    by a Pooling layer. Then the result is given to successive FC layers which output
    a tensor of size (batch_size, out_channels) where `out_channels` represents the
    number of classes.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet架构由两个堆叠的卷积块组成，每个卷积块后面跟着一个池化层。然后将结果传递给连续的全连接层，输出一个大小为(batch_size, out_channels)的张量，其中`out_channels`表示类别数。
- en: 'In the implementation block below, some miscellaneous attributes are first
    initialized:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的实现块中，首先初始化一些杂项属性：
- en: The `example_input_array` tensor used for [displaying a simulation of the tensor
    size](https://lightning.ai/docs/pytorch/stable/debug/debugging_basic.html) between
    each layer when running `print(model)`.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`example_input_array`张量用于[显示张量大小的仿真](https://lightning.ai/docs/pytorch/stable/debug/debugging_basic.html)，以便在运行`print(model)`时查看每一层之间的张量大小。'
- en: '![](../Images/ce02dbe770a43ff01540d989c373967f.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce02dbe770a43ff01540d989c373967f.png)'
- en: Automatic model logging when running `print(model)`. Image from author.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`print(model)`时会自动记录模型日志。图片来自作者。
- en: From the table above, we can confirm that the output tensor has the size (batch_size=16,
    num_classes=10).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从上表可以确认，输出张量的大小为(batch_size=16, num_classes=10)。
- en: 2\. The`Accuracy()` metric will be used during training and validation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. `Accuracy()`度量将在训练和验证过程中使用。
- en: 3\. The layers with learnable parameters are initialized as well. First the
    two{Convolutional + Max Pool} blocks and then the Fully Connected layers.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 具有可学习参数的层也会被初始化。首先是两个{卷积 + 最大池化}块，然后是全连接层。
- en: '[PRE1]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Notes about the implementation above**'
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**关于上述实现的说明**'
- en: '**Notes about the convolution layers**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于卷积层的说明**'
- en: In order to simplify the forward call, it is common to represent the stacked
    layers inside a `nn.Sequential()` submodule.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了简化前向调用，通常将堆叠的层表示在`nn.Sequential()`子模块中。
- en: The first conv layer takes (32x32) sized images and outputs a (16x16) sized
    image after dividing the size by 2 in the pooling layer.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个卷积层处理（32x32）大小的图像，并在池化层中将大小除以2后输出（16x16）大小的图像。
- en: LeNet expects an input image size of (32x32) but the available MNIST dataset
    has images of size (28x28). You can either resize the image or increase the padding
    size of the first convolutional layer (as written in the comment). Otherwise,
    the convolutional layers can take variable-size images but after two downsampling
    there is a dimension mismatch between the output activation from the last conv
    layer and the matrix multiplication (which dimension is fixed as shown below)
    of the first FC layer.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeNet期望输入图像大小为（32x32），但可用的MNIST数据集图像大小为（28x28）。你可以选择调整图像大小或增加第一个卷积层的填充大小（如注释中所述）。否则，卷积层可以处理可变大小的图像，但经过两次下采样后，最后一个卷积层的输出激活与第一个全连接层的矩阵乘法（其维度如下面所示）之间会出现维度不匹配。
- en: '[PRE2]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The second convolutional layer takes as input the same number of channels as
    the first conv has output filters (6).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个卷积层输入的通道数与第一个卷积层输出的滤波器数相同（6）。
- en: '**The order ReLU and MaxPool does not matter here.**'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ReLU和MaxPool的顺序在这里并不重要。**'
- en: The ReLU activations are not called before pooling, unlike it was mentioned
    in the earlier section. In this implementation, the ReLU activations functions
    are called in the `forward()`call only.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU激活函数不会在池化之前调用，与之前提到的不同。在此实现中，ReLU激活函数仅在`forward()`调用中被调用。
- en: A convolutional layer should always be followed by an activation function in
    order to add non-linearity. But if the convolutional layer is also followed by
    a Pooling layer, the order does not matter. Both operations are commutative MaxPool(Relu(x))
    = Relu(MaxPool(x)). Indeed, we can take the maximum of a local patch and set 0
    for all the negative values or set 0 for all the negative values and take the
    maximum of each local patch.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层后面应该总是跟一个激活函数，以添加非线性。但是如果卷积层后面还跟一个池化层，那么顺序并不重要。这两个操作是可交换的 MaxPool(Relu(x))
    = Relu(MaxPool(x))。实际上，我们可以在局部区域内取最大值并将所有负值设置为0，或将所有负值设置为0并取每个局部区域的最大值。
- en: '**Note about the FC layers.**'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关于全连接层的说明。**'
- en: The first FC layer takes as a tensor of size (number_output_filter_from_conv2
    * previous_activation_width * previous_activation_height). The size of the output
    activation is gradually decreasing through the three FC layers.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个全连接层输入的张量大小为（number_output_filter_from_conv2 * previous_activation_width
    * previous_activation_height）。输出激活的大小在三个全连接层中逐渐减小。
- en: 'All these layers are called during the forward pass:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传递过程中会调用所有这些层：
- en: '[PRE3]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The backward function which computes the gradient is automatically defined when
    using `autograd` .
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 计算梯度的反向函数在使用`autograd`时会自动定义。
- en: In most PyTorch implementations, the last layer (also called the softmax layer
    sometimes) outputs the raw activation values where each number corresponds to
    a score. Here the softmax function is not called in the forward pass but is built
    in the [Cross-Entropy loss function](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数PyTorch实现中，最后一层（有时也称为softmax层）输出的是原始激活值，其中每个数字对应一个得分。在这里，softmax函数在前向传递中并没有被调用，而是内置于[交叉熵损失函数](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)中。
- en: Implementing the training, validation, and test steps
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现训练、验证和测试步骤
- en: In the same file as before, under the `class LeNet(pl.LightningModule)` we override
    all the core functions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的相同文件中，在`class LeNet(pl.LightningModule)`下，我们重写了所有核心函数。
- en: 'the optimizer and scheduler: `configure_optimizers()`'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器和调度器：`configure_optimizers()`
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'the training loop: `training_step()`'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练循环：`training_step()`
- en: 'the validation loop: `validation_step()`'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证循环：`validation_step()`
- en: '[PRE5]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you may notice, the functions above are pretty short. There is no need to
    move the variable to `to(device)` , to delete the gradients with `optimizer.zero_grad()`
    or compute the new gradients with `loss.backward()` . Switching the model mode
    is also handled by the PyTorch Lightning library itself `model.eval()` ,`model.train()`
    .
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，上述函数都比较简短。无需将变量移动到 `to(device)`，也无需使用 `optimizer.zero_grad()` 删除梯度或使用 `loss.backward()`
    计算新的梯度。模型模式的切换也由 PyTorch Lightning 库本身处理 `model.eval()`，`model.train()`。
- en: You can notice that the `log()` method is called here. This method saves and
    displays the results when appropriate.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以注意到这里调用了 `log()` 方法。此方法在适当的时候保存和显示结果。
- en: 'If you want to customize it, the [documentation](https://pytorch-lightning.readthedocs.io/en/0.10.0/introduction_guide.html#logging)
    explains well how to use the logging correctly:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想自定义它，[文档](https://pytorch-lightning.readthedocs.io/en/0.10.0/introduction_guide.html#logging)很好地解释了如何正确使用日志记录：
- en: 'The `log()` method has a few options:'
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`log()` 方法有一些选项：'
- en: ''
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- on_step (logs the metric at that step in training)'
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- on_step（在训练中的该步骤记录指标）'
- en: ''
  id: totrans-124
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- on_epoch (automatically accumulates and logs at the end of the epoch)'
  id: totrans-125
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- on_epoch（在每个 epoch 结束时自动累积并记录）'
- en: ''
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- prog_bar (logs to the progress bar)'
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- prog_bar（记录到进度条中）'
- en: ''
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- logger (logs to the logger like Tensorboard)'
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- logger（将日志记录到类似 Tensorboard 的日志记录器中）'
- en: ''
  id: totrans-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Depending on where log is called from, Lightning auto-determines the correct
    mode for you. But of course you can override the default behavior by manually
    setting the flags
  id: totrans-131
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 根据 `log` 被调用的位置，Lightning 会自动确定正确的模式。不过，你当然可以通过手动设置标志来覆盖默认行为。
- en: 'Another good feature of PyTorch Lightning is the Validation Sanity check:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Lightning 的另一个好功能是验证一致性检查：
- en: You may have noticed the words **Validation sanity check** logged. This is because
    Lightning runs 2 batches of validation before starting to train. This is a kind
    of unit test to make sure that if you have a bug in the validation loop, you won’t
    need to potentially wait a full epoch to find out.
  id: totrans-133
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可能注意到了 **验证一致性检查** 被记录了。这是因为 Lightning 在开始训练之前会运行 2 批验证。这是一种单元测试，确保如果你在验证循环中有
    bug，你不会需要等待整个 epoch 才能发现。
- en: 'Finally, the methods for testing and prediction are also implemented under
    the same class:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，测试和预测的方法也在同一个类中实现：
- en: 'the test loop: `test_step()`'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试循环：`test_step()`
- en: 'the prediction loop: `predict_step()`'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测循环：`predict_step()`
- en: The model can either load its weights from a checkpoint or if called after a
    training loop, the model fetchs automatically the weights from the last or best
    (if a callback has been implemented) epochs.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可以从检查点加载权重，或者在训练循环结束后自动从最后一个或最佳（如果实现了回调）epoch 中提取权重。
- en: '[PRE6]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Managing the MNIST dataset
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理 MNIST 数据集
- en: You can use either the regular PyTorch DataLoader class or a PyTorch Lightning
    DataModule. In this article, I implemented the dataset and dataloading with the
    PyTorch Lightning DataModule. It aims at centralizing all the information relative
    to one dataset within one single file. It includes data downloading, data splitting,
    data loading, etc.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用常规的 PyTorch DataLoader 类或 PyTorch Lightning DataModule。在这篇文章中，我使用了 PyTorch
    Lightning DataModule 实现了数据集和数据加载。它旨在将与一个数据集相关的所有信息集中在一个文件中，包括数据下载、数据分割、数据加载等。
- en: For this tutorial, we use MNIST made of images of size 28x28\. The MNIST dataset
    is made available under the terms of the [Creative Commons Attribution-Share Alike
    3.0 license](https://creativecommons.org/licenses/by-sa/3.0/) ([source](https://keras.io/api/datasets/mnist/)).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本教程，我们使用的是大小为 28x28 的 MNIST 图像。MNIST 数据集根据 [Creative Commons Attribution-Share
    Alike 3.0 许可](https://creativecommons.org/licenses/by-sa/3.0/)（[来源](https://keras.io/api/datasets/mnist/)）提供。
- en: '![](../Images/335b143ea684822c964c16b1e9b765c8.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/335b143ea684822c964c16b1e9b765c8.png)'
- en: MNIST dataset visualization. [Image under CC license](https://en.wikipedia.org/wiki/MNIST_database#/media/File:MnistExamplesModified.png).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据集的可视化。[图片使用 CC 许可](https://en.wikipedia.org/wiki/MNIST_database#/media/File:MnistExamplesModified.png)。
- en: 'Here is the implementation of the data module to manage the MNIST dataset.
    It includes setting the standard parameters:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这是管理 MNIST 数据集的数据模块的实现，包括设置标准参数：
- en: Path to the data directory
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据目录路径
- en: Batch size
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量大小
- en: Tensor transforms
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tensor 转换
- en: As well as downloading and processing functionality in `prepare_data()` `setup()`
    .
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 以及 `prepare_data()` 和 `setup()` 中的数据下载和处理功能。
- en: '[PRE7]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'When starting training, the following functions are called in this order:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练时，以下函数按此顺序调用：
- en: The DataModule `prepare_data()` and `setup()` methods. The method `prepare_data()`
    runs on one CPU and it is used for downloading the data locally. Whereas the method`setup()`
    is a parallel process that can run data processing jobs. These methods are called
    each time when calling a method from the trainer like `trainer.fit()`, `trainer.validate()`,
    etc.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataModule 的 `prepare_data()` 和 `setup()` 方法。`prepare_data()` 方法在一个 CPU 上运行，用于在本地下载数据。而
    `setup()` 方法是一个并行进程，可以运行数据处理作业。这些方法在每次调用训练器中的方法时都会被调用，如 `trainer.fit()`、`trainer.validate()`
    等。
- en: The pl.LightningModule `configure_optimizers()` initializes the optimizer.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pl.LightningModule 的 `configure_optimizers()` 初始化优化器。
- en: 'Then, under the same class `MNISTDataModule`we implement the different dataloaders:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在相同的类 `MNISTDataModule` 下，我们实现了不同的数据加载器：
- en: '[PRE8]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The DataModule `train_dataloader()` retrieves the training DataLoader.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataModule 的 `train_dataloader()` 检索训练 DataLoader。
- en: The pl.LightningModule `training_step()` runs a forward and backward pass on
    a mini-batch obtained from the training DataLoader. The method is repeatedly called
    until all samples from the training DataLoader are seen once.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pl.LightningModule 的 `training_step()` 对从训练 DataLoader 获得的小批量数据执行前向和后向传递。该方法会重复调用，直到所有来自训练
    DataLoader 的样本都被看到一次。
- en: The pl.LightningModule `validation_step()` computes the loss and the metrics
    on the validation dataset.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pl.LightningModule 的 `validation_step()` 计算验证数据集上的损失和指标。
- en: The training stops once the max number of epochs has been hit or if the validation
    loss stops decreasing (early stop).
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦达到最大训练周期数，或者验证损失不再下降（早期停止），训练就会停止。
- en: Implementing the training loop
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现训练循环
- en: Finally, the only missing piece is the training script itself.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，唯一缺少的部分是训练脚本本身。
- en: 'The training script includes:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 训练脚本包括：
- en: Parsing CLI arguments and calling the main function
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析 CLI 参数并调用主函数
- en: '[PRE9]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The main function includes the model selection, the creation of an early stopping
    callback, and the calls to the trainer for training `trainer.fit(model, datamodule=data_module)`,
    validating `trainer.validate(datamodule=data_module)`, testing `trainer.test(datamodule=data_module)`,
    and predicting `output_preds = trainer.predict(datamodule=data_module, ckpt_path=”best”)`.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主函数包括模型选择、创建早期停止回调、以及对训练器的调用以进行训练 `trainer.fit(model, datamodule=data_module)`、验证
    `trainer.validate(datamodule=data_module)`、测试 `trainer.test(datamodule=data_module)`
    和预测 `output_preds = trainer.predict(datamodule=data_module, ckpt_path=”best”)`。
- en: '[PRE10]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The function to save predicted images (mostly for debugging).
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存预测图像的函数（主要用于调试）。
- en: '[PRE11]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Altogether with the imports and constant declarations, the script looks like:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 加上导入和常量声明，脚本看起来如下：
- en: '[PRE12]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Results**'
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**结果**'
- en: Running `python -m train --early-stopping`for 10 epochs of training (batch size
    of 64) takes less than two minutes on my machine with a GPU NVIDIA GeForce RTX
    3070.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的机器上使用 GPU NVIDIA GeForce RTX 3070 运行 `python -m train --early-stopping` 进行
    10 个周期的训练（批量大小为 64）不到两分钟。
- en: 'When the training reaches the default max_epochs (10), PyTorch Lightning outputs
    the loss and accuracy results on the validation and test datasets respectively:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练达到默认的 max_epochs（10）时，PyTorch Lightning 会分别输出验证和测试数据集上的损失和准确率结果：
- en: '![](../Images/cb93f5e958f8ae8ea6e5cc233d669683.png)![](../Images/e575a76d7f7db272a93622747b292ad3.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb93f5e958f8ae8ea6e5cc233d669683.png)![](../Images/e575a76d7f7db272a93622747b292ad3.png)'
- en: Result of the model on unseen data after 10 epochs of training.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 经过 10 个训练周期后，模型在未见数据上的结果。
- en: The trained model obtains almost 99% of accuracy on the unseen data.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后的模型在未见数据上的准确率几乎达到 99%。
- en: 'The script saves 10 images from the test set as well as the predicted class:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本保存了 10 张来自测试集的图像以及预测的类别：
- en: '![](../Images/54dfd8fc9be7924ac5569c5a6b4a456d.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54dfd8fc9be7924ac5569c5a6b4a456d.png)'
- en: Predicted as a 6
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 预测为 6
- en: Conclusion
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we have discovered the magic of PyTorch Lightning, we have
    then rehearsed the key technical concepts of CNNs and we have walked through a
    complete implementation of a training loop for a simple CNN architecture from
    scratch.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们发现了 PyTorch Lightning 的魔力，然后复习了 CNN 的关键技术概念，并从头到尾演练了一个简单 CNN 架构的训练循环的完整实现。
- en: I hope this introduction-level article was helpful in your journey to implement
    a basic architecture fast and reliably and helped you build a stronger foundation
    in your learning journey. You can check my public deep-learning repository for
    more content [https://github.com/bledem/deep-learning](https://github.com/bledem/deep-learning).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇入门级文章对你在快速而可靠地实现基本架构的过程中有所帮助，并且帮助你在学习旅程中建立了更坚实的基础。你可以查看我的公开深度学习仓库，获取更多内容
    [https://github.com/bledem/deep-learning](https://github.com/bledem/deep-learning)。
- en: References
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: Stanford Computer Vision classes
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斯坦福计算机视觉课程
- en: Andrew NG [https://www.youtube.com/watch?v=c1RBQzKsDCk&list=PLpFsSf5Dm-pd5d3rjNtIXUHT-v7bdaEIe&index=115&ab_channel=DeepLearningAI](https://www.youtube.com/watch?v=c1RBQzKsDCk&list=PLpFsSf5Dm-pd5d3rjNtIXUHT-v7bdaEIe&index=115&ab_channel=DeepLearningAI)
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrew NG [https://www.youtube.com/watch?v=c1RBQzKsDCk&list=PLpFsSf5Dm-pd5d3rjNtIXUHT-v7bdaEIe&index=115&ab_channel=DeepLearningAI](https://www.youtube.com/watch?v=c1RBQzKsDCk&list=PLpFsSf5Dm-pd5d3rjNtIXUHT-v7bdaEIe&index=115&ab_channel=DeepLearningAI)
- en: PyTorch Lightning docs [https://www.pytorchlightning.ai/tutorials](https://www.pytorchlightning.ai/tutorials)
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch Lightning 文档 [https://www.pytorchlightning.ai/tutorials](https://www.pytorchlightning.ai/tutorials)
