- en: Deterministic vs. Probabilistic Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定性与概率性深度学习
- en: 原文：[https://towardsdatascience.com/deterministic-vs-probabilistic-deep-learning-5325769dc758](https://towardsdatascience.com/deterministic-vs-probabilistic-deep-learning-5325769dc758)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deterministic-vs-probabilistic-deep-learning-5325769dc758](https://towardsdatascience.com/deterministic-vs-probabilistic-deep-learning-5325769dc758)
- en: Probabilistic Deep Learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概率性深度学习
- en: '[](https://medium.com/@luisroque?source=post_page-----5325769dc758--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----5325769dc758--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5325769dc758--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5325769dc758--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----5325769dc758--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@luisroque?source=post_page-----5325769dc758--------------------------------)[![路易斯·罗克](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----5325769dc758--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5325769dc758--------------------------------)[![数据科学之路](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5325769dc758--------------------------------)
    [路易斯·罗克](https://medium.com/@luisroque?source=post_page-----5325769dc758--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5325769dc758--------------------------------)
    ·9 min read·Jan 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [数据科学之路](https://towardsdatascience.com/?source=post_page-----5325769dc758--------------------------------)
    ·9分钟阅读·2023年1月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: This article belongs to the series “Probabilistic Deep Learning”. This weekly
    series covers probabilistic approaches to deep learning. The main goal is to extend
    deep learning models to quantify uncertainty, i.e., know what they do not know.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本文属于“概率性深度学习”系列。该系列每周覆盖概率性深度学习的方法。主要目标是扩展深度学习模型以量化不确定性，即了解它们所不知道的内容。
- en: This article covers the main differences between Deterministic and Probabilistic
    deep learning. Deterministic deep learning models are trained to optimize a scalar-valued
    loss function, while probabilistic deep learning models are trained to optimize
    a probabilistic objective function. Deterministic models provide a single prediction
    for each input, while probabilistic models provide a probabilistic characterization
    of the uncertainty in their predictions, as well as the ability to generate new
    samples from the model.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文涵盖了确定性与概率性深度学习之间的主要区别。确定性深度学习模型旨在优化标量值损失函数，而概率性深度学习模型则旨在优化概率目标函数。确定性模型为每个输入提供一个单一预测，而概率性模型则提供对其预测不确定性的概率性描述，并具备从模型生成新样本的能力。
- en: 'Articles published so far:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 目前已发布的文章：
- en: '[Gentle Introduction to TensorFlow Probability: Distribution Objects](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow Probability 简介：分布对象](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1)'
- en: '[Gentle Introduction to TensorFlow Probability: Trainable Parameters](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow Probability 简介：可训练参数](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15)'
- en: '[Maximum Likelihood Estimation from scratch in TensorFlow Probability](/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[从头开始的最大似然估计（TensorFlow Probability）](/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2)'
- en: '[Probabilistic Linear Regression from scratch in TensorFlow](/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[从头开始的概率性线性回归（TensorFlow）](/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00)'
- en: '[Probabilistic vs. Deterministic Regression with Tensorflow](https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[概率性与确定性回归在 TensorFlow 中的比较](https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef)'
- en: '[Frequentist vs. Bayesian Statistics with Tensorflow](https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[频率派与贝叶斯统计（Tensorflow）](https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5)'
- en: Deterministic vs. Probabilistic Deep Learning
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定性与概率性深度学习
- en: '![](../Images/71c0028b4a604a1f23ee10072d79519e.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71c0028b4a604a1f23ee10072d79519e.png)'
- en: 'Figure 1: The motto for today: we are always uncertain about things; why should
    our models be any different? ([source](https://unsplash.com/photos/Vkp9wg-VAsQ))'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: We develop our models using TensorFlow and TensorFlow Probability. TensorFlow
    Probability is a Python library built on top of TensorFlow. We will start with
    the basic objects we can find in TensorFlow Probability and understand how we
    can manipulate them. We will increase complexity incrementally over the following
    weeks and combine our probabilistic models with deep learning on modern hardware
    (e.g. GPU).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: As usual, the code is available on my [GitHub](https://github.com/luisroque/probabilistic_deep_learning_with_TFP).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic vs. Probabilistic Deep Learning
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning has become the dominant approach for a wide range of machine learning
    tasks, such as image and speech recognition, natural language processing, and
    reinforcement learning. A key feature of deep learning is the ability to learn
    complex, non-linear functions from large amounts of data. However, traditional
    deep learning models are deterministic, meaning they make the same predictions
    given the same inputs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic deep learning models, such as feedforward neural networks, are
    trained to optimize a scalar-valued loss function, such as mean squared error
    or cross-entropy. The trained model produces a single prediction for each input,
    but does not provide any information about the uncertainty of the prediction.
    In contrast, probabilistic deep learning models, such as variational autoencoders
    (VAEs) and generative adversarial networks (GANs), are trained to optimize a probabilistic
    objective function, such as the negative log-likelihood of the data or an approximate
    posterior distribution. These models provide a probabilistic characterization
    of the uncertainty in their predictions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: One of the main advantages of probabilistic deep learning models is the ability
    to generate new samples from the model. For example, a VAE can be used to generate
    new images that are similar to the training data, while a GAN can be used to generate
    new images that are different from the training data. Additionally, probabilistic
    deep learning models are often used for unsupervised learning, where the goal
    is to learn a compact representation of the data without any labels.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Data and Approach
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we employ the MNIST dataset and its corrupted counterpart to
    evaluate our approach. The corrupted version of the MNIST dataset introduces an
    added level of complexity through grey spatters superimposed on the original images,
    making classifying the handwritten digits more challenging.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Our objective is to construct a convolutional neural network (CNN) that effectively
    classifies the images of handwritten digits into 10 distinct classes. To this
    end, we make use of the aforementioned datasets, in order to provide a comprehensive
    evaluation of the performance of both the deterministic and the probabilistic
    CNNs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/98fbc67bcdc61423c1180bbbb62c181e.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Random examples from the MNIST dataset.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/216a793a81c0b023de312311daadf6cb.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Random examples from the corrupted version of the MNIST dataset.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic Architecture
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We begin by formally introducing the deterministic model, which is a convolutional
    neural network (CNN) classifier comprised of several key architectural components.
    Specifically, this model consists of:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: A convolutional layer, in which the convolutional operation is performed by
    a set of 8 filters with a kernel size of 5x5 and ‘VALID’ padding, and the output
    is passed through a rectified linear unit (ReLU) activation function.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A max-pooling layer, in which the maximum value within non-overlapping windows
    of size 6x6 is taken, reducing the spatial dimensionality of the feature maps.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A flatten layer, in which the pooled feature maps are collapsed into a single
    vector, allowing for the final dense layer to have fully connected computations.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dense layer, which is also known as fully-connected layer, has 10 units and
    applies a softmax activation function to obtain the final probability distribution
    over the class labels.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This CNN classifier architecture is designed to efficiently extract discriminative
    features and perform classification on high-dimensional image data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We have implemented the architecture discussed above, so now we are ready to
    start the training procedure.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Finally, we can check the accuracy on both datasets.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Probabilistic Architecture
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In contrast to the deterministic model previously discussed, our probabilistic
    model introduces a new approach to outputting a distribution object, specifically
    a One-Hot Categorical distribution. This enables the modeling of aleatoric uncertainty
    on the image labels, allowing for a more comprehensive characterization of the
    model’s predictions.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: The One-Hot Categorical distribution is a discrete probability distribution
    over one-hot bit vectors, with the event dimension equal to K, the number of classes.
    It is mathematically equivalent to the Categorical distribution, which is a discrete
    probability distribution over positive integers, with the key distinction being
    that Categorical has an empty event dimension, whereas One-Hot Categorical has
    event dimension equal to K.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Our proposed probabilistic CNN architecture consists of:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: A convolutional layer, in which the convolutional operation is performed by
    a set of 8 filters with a kernel size of 5x5 and ‘VALID’ padding, and the output
    is passed through a rectified linear unit (ReLU) activation function.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A max-pooling layer, in which the maximum value within non-overlapping windows
    of size 6x6 is taken, reducing the spatial dimensionality of the feature maps.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A flatten layer, in which the pooled feature maps are collapsed into a single
    vector, allowing for the final dense layer to have fully connected computations.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dense layer, which is also known as fully-connected layer, with the number
    of units required to parameterize the probabilistic layer that follows.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个密集层，也称为全连接层，具有参数化随后的概率层所需的单元数量。
- en: A OneHotCategorical distribution layer, with an event shape equal to 10, corresponding
    to the 10 classes.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 OneHotCategorical 分布层，其事件形状为 10，对应于 10 个类别。
- en: This novel architecture, incorporating One-Hot Categorical output distribution,
    enables the modeling of aleatoric uncertainty on the image labels, and allows
    for a more comprehensive characterization of the model’s predictions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这一新颖的架构，结合了 One-Hot Categorical 输出分布，使得能够对图像标签上的随机不确定性进行建模，并允许对模型预测进行更全面的表征。
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Our implementation of the model is done. Take a moment to compare the implementation
    above with the one that we did earlier that implements the corresponding deterministic
    architecture. If you have any doubts about specific components that we defined
    for the probabilistic version of the model, e.g. the loss function, please refer
    to the earlier articles in this series. Once again, we can now start the training
    procedure.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型实现完成了。请花一点时间将上述实现与我们之前进行的实现（实现相应的确定性架构）进行比较。如果您对我们为概率模型版本定义的特定组件（例如损失函数）有任何疑问，请参考本系列早期的文章。我们现在可以开始训练过程。
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Finally, we can check the accuracy of this version of the model. Note that the
    test accuracy of the probabilistic model is identical to the deterministic one.
    This is because the model architectures for both are equivalent; the only difference
    being that the probabilistic model returns a distribution object.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以检查这个版本模型的准确性。请注意，概率模型的测试准确性与确定性模型相同。这是因为两者的模型架构是等效的，唯一的区别是概率模型返回一个分布对象。
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Results and Discussion
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果与讨论
- en: Accuracy is an important metric to assess the performance of the model. Nonetheless,
    it is sometimes shallow since it does not provide information about the uncertainty
    of the predictions.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是评估模型性能的重要指标。然而，它有时较为浅显，因为它未提供有关预测不确定性的信息。
- en: In this section, we go beyond the predicted label and provide a visual representation
    of the uncertainty in the predictions. To accomplish this, we sample the predictive
    distribution of the model and calculate the percentiles of the resulting samples.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们超越预测标签，提供了对预测不确定性的可视化表示。为此，我们对模型的预测分布进行采样，并计算结果样本的百分位数。
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The model is very confident that the first image is a 7, which is correct. For
    the second image, the model struggles, assigning nonzero probabilities to many
    different classes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 模型对第一张图像是 7 的预测非常有信心，这是正确的。对于第二张图像，模型遇到困难，为许多不同类别分配了非零概率。
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/b04c8d0904a01b294ab8603b1e14abcf.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b04c8d0904a01b294ab8603b1e14abcf.png)'
- en: 'Figure 4: Predictions from the probabilistic model for the MNIST dataset.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：概率模型对 MNIST 数据集的预测。
- en: Once again, the model is confident about its prediction of the first image.
    Despite the spatters, the number is still easy to identify. The second number
    is significantly harder to identify. The model still does a good job of predicting
    the right number and showing uncertainty about that choice.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，该模型对第一张图像的预测非常有信心。尽管有些污点，但这个数字仍然容易识别。第二个数字则显著更难识别。模型仍然能够很好地预测正确的数字，并展示了对这个选择的不确定性。
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/20f19ec19e798f6683ece0420dd5a761.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20f19ec19e798f6683ece0420dd5a761.png)'
- en: 'Figure 5: Predictions from the probabilistic model for the corrupted version
    of the MNIST dataset.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：概率模型对 MNIST 数据集损坏版本的预测。
- en: Conclusion
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we have discussed the key differences between deterministic
    and probabilistic deep learning models, focusing on using these models for image
    classification tasks. By analyzing the performance of a CNN on the MNIST dataset
    and on the corrupted version of the same dataset, we have shown that probabilistic
    deep learning models can achieve similar accuracy levels to deterministic models
    and provide a probabilistic characterization of the uncertainty in their predictions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们讨论了确定性和概率深度学习模型之间的关键差异，重点介绍了这些模型在图像分类任务中的应用。通过分析 CNN 在 MNIST 数据集及其损坏版本上的表现，我们展示了概率深度学习模型可以实现与确定性模型相似的准确度水平，并提供了对其预测不确定性的概率描述。
- en: One of the main advantages of probabilistic deep learning models is the ability
    to generate new samples from the model. This can be useful for tasks such as image
    synthesis or data augmentation, where the goal is to create new, realistic images
    from a given dataset. Additionally, probabilistic deep learning models can be
    used for unsupervised learning, where the goal is to learn a compact representation
    of the data without any labels.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 概率深度学习模型的主要优势之一是能够从模型中生成新样本。这在图像合成或数据增强等任务中非常有用，目标是从给定的数据集中创建新的、真实的图像。此外，概率深度学习模型还可以用于无监督学习，其目标是学习数据的紧凑表示而无需任何标签。
- en: 'Keep in touch: [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '保持联系: [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)'
- en: References and Materials
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料和材料
- en: '[1] — [Coursera: Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] — [Coursera: 深度学习专业化](https://www.coursera.org/specializations/deep-learning)'
- en: '[2] — [Coursera: TensorFlow 2 for Deep Learning](https://www.coursera.org/specializations/tensorflow2-deeplearning)
    Specialization'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] — [Coursera: TensorFlow 2 深度学习](https://www.coursera.org/specializations/tensorflow2-deeplearning)
    专业化'
- en: '[3] — [TensorFlow Probability Guides and Tutorials](https://www.tensorflow.org/probability/overview)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] — [TensorFlow 概率指南与教程](https://www.tensorflow.org/probability/overview)'
- en: '[4] — [TensorFlow Probability Posts in TensorFlow Blog](https://blog.tensorflow.org/search?label=TensorFlow+Probability&max-results=20)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] — [TensorFlow 博客中的 TensorFlow 概率文章](https://blog.tensorflow.org/search?label=TensorFlow+Probability&max-results=20)'
