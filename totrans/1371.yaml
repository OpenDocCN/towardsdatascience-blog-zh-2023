- en: Is There Always a Tradeoff Between Bias and Variance?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/is-there-always-a-tradeoff-between-bias-and-variance-5ca44398a552](https://towardsdatascience.com/is-there-always-a-tradeoff-between-bias-and-variance-5ca44398a552)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Irreverent Demystifiers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The bias-variance tradeoff, part 1 of 3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://kozyrkov.medium.com/?source=post_page-----5ca44398a552--------------------------------)[![Cassie
    Kozyrkov](../Images/ad18dd12979a4a3ec130bdf8b889af23.png)](https://kozyrkov.medium.com/?source=post_page-----5ca44398a552--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ca44398a552--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ca44398a552--------------------------------)
    [Cassie Kozyrkov](https://kozyrkov.medium.com/?source=post_page-----5ca44398a552--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ca44398a552--------------------------------)
    ·5 min read·Feb 15, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Should you read this article? If you understand all the words in the next section,
    then no. If you don’t care to understand them, then also no. If you want the bolded
    bits explained, then yes.
  prefs: []
  type: TYPE_NORMAL
- en: The bias-variance tradeoff
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*“The bias-variance tradeoff”* is a popular phrase you’ll hear in the context
    of [ML/AI](http://bit.ly/quaesita_emperor). If you’re a [statistician](http://bit.ly/quaesita_statistics),
    you might think it’s about summarizing this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '**MSE = Bias² + Variance**'
  prefs: []
  type: TYPE_NORMAL
- en: It isn’t.
  prefs: []
  type: TYPE_NORMAL
- en: Well, it’s loosely related, but the phrase actually refers to a *practical recipe*
    for how to pick a model’s **complexity sweet spot**. It’s most useful when you’re
    tuning a **regularization** **hyperparameter**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/45a49487f48458ec80ce35b0cf1a3fd5.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '***Note:*** *If you’ve never heard of the MSE, you might need a bit of help
    with some of the jargon. When you hit a new term you want explained in more detail,
    you can follow the links to my other articles where I introduce the words I’m
    using.*'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The mean squared error ([MSE](http://bit.ly/quaesita_babymse)) is the most
    popular (and vanilla) choice for a model’s [loss function](http://bit.ly/quaesita_emperorm)
    and it tends to be the first one you’re taught. You’ll likely take a whole bunch
    of [stats](http://bit.ly/quaesita_statistics) classes before it occurs to anyone
    to tell you that you’re welcome to minimize other loss functions if you like.
    (But let’s be real: [parabolae are super easy to optimize](http://bit.ly/quaesita_msefav).
    Remember *d/dx* *x²*? 2*x*. That convenience is enough to keep most of you loyal
    to the MSE.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you learn about the MSE, it’s usually mere [moments](http://bit.ly/quaesita_lemur)
    until someone mentions the bias and variance formula:'
  prefs: []
  type: TYPE_NORMAL
- en: MSE = Bias² + Variance
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[I did it too](http://bit.ly/quaesita_lemur) and, like a garden variety [data
    science](http://bit.ly/quaesita_datascim) jerk, left the proof as [homework for
    the interested reader](https://twitter.com/kareem_carr/status/1535402776276217859?t=R60xEOxxTYkBq7mkj8CvnQ&s=09).'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s make amends — if you’d like me to derive it for you while making snide
    comments in the margins, take a small detour to [here](http://bit.ly/quaesita_mseformula).
    If you choose to skip the mathy stuff, then you’ll have to put up with my jazz
    hands and just take my word for it.
  prefs: []
  type: TYPE_NORMAL
- en: Positive vibes only
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Want me to tell the key thing to you bluntly? Notice that the formula consists
    of two terms that **can’t be negative**.
  prefs: []
  type: TYPE_NORMAL
- en: The quantity (MSE) you’re trying to optimize when you fit your predictive [ML/AI
    models](http://bit.ly/quaesita_emperor) can be decomposed into **always-positive
    terms** that involve bias only and variance only.
  prefs: []
  type: TYPE_NORMAL
- en: MSE = Bias² + Variance = (Bias)² + (Standard Deviation)²
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Even more bluntly? Okay, sure.
  prefs: []
  type: TYPE_NORMAL
- en: 'A better model has a lower MSE. E stands for [error](http://bit.ly/quaesita_msefav)
    and fewer errors are better, so the *best* model has a zero MSE: it makes no mistakes.
    That means it also has *no bias* and *no variance*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/330c6cb9039af21d8e2b327f3fa087d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Laura Crowe](https://unsplash.com/de/@laurarain?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Instead of perfect models, let’s look at going from good to better. **If you’re
    truly able to improve your model (in terms of MSE), there’s no need for a tradeoff
    between bias and variance.** If you became a better archer, you became a better
    archer. No tradeoff. (You probably needed more practice — [data](http://bit.ly/quaesita_hist)!
    — to get there.)
  prefs: []
  type: TYPE_NORMAL
- en: As Tolstoy would say, all perfect models are alike, but each unhappy model can
    be unhappy in its own way.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All perfect models are alike
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As [Tolstoy](https://en.wikipedia.org/wiki/Anna_Karenina_principle) would say,
    all perfect models are alike, but each unhappy model (for a given MSE) can be
    unhappy in its own way. You can get two equally rubbish yet different models with
    the same MSE: one model can have really good (low) bias but high variance while
    the other can have really good (low) variance but high bias, and yet both can
    have the same MSE (overall score).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62ffc025acf5afebcf290b9c1d6c3652.png)'
  prefs: []
  type: TYPE_IMG
- en: If we measure the performance of an archer by MSE, we’re saying that decreasing
    an archer’s standard deviation is worth the same as an equivalent decrease in
    bias. We’re saying we’re indifferent between the two. (Wait, what if *you’re*
    not indifferent between them? Then the MSE might not be your best choice here.
    Don’t like the MSE’s way of scoring performance? Not a problem. Make your own
    [loss function](http://bit.ly/quaesita_emperor).)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve set the table, head over to [Part 2](http://bit.ly/quaesita_bivar2)
    where we dig into the heart of the matter: Is there an actual tradeoff? (Yes!
    But not where you might think.) And what does overfitting have to do with it?
    (Hint: everything!)'
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading! How about a course?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you had fun here and you’re looking for an unboring leadership-oriented
    course designed to delight AI beginners and experts alike, [here’s a little something](https://bit.ly/funaicourse)
    I made for you:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/300b5280620ea948fc3dbffb708084d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Course link: [https://bit.ly/funaicourse](https://bit.ly/funaicourse)'
  prefs: []
  type: TYPE_NORMAL
- en: Prefer to hone your decision skills instead of building your AI muscles? You
    can learn decision intelligence from me via this [link to my free course:](https://bit.ly/decisioncourse)
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://bit.ly/decisioncourse?source=post_page-----5ca44398a552--------------------------------)
    [## The steering wheel for your life — Decision Intelligence Video Tutorial |
    LinkedIn Learning…'
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision-making is the most valuable skill you can learn. Your life boils down
    to two things: the quality of your…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: bit.ly](https://bit.ly/decisioncourse?source=post_page-----5ca44398a552--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*P.S. Have you ever tried hitting the clap button here on Medium more than
    once to see what happens?* ❤️'
  prefs: []
  type: TYPE_NORMAL
- en: Liked the author? Connect with Cassie Kozyrkov
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s be friends! You can find me on [Twitter](https://twitter.com/quaesita),
    [YouTube](https://www.youtube.com/channel/UCbOX--VOebPe-MMRkatFRxw), [Substack](http://decision.substack.com),
    and [LinkedIn](https://www.linkedin.com/in/kozyrkov/). Interested in having me
    speak at your event? Use [this form](http://bit.ly/makecassietalk) to get in touch.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://kozyrkov.medium.com/membership?source=post_page-----5ca44398a552--------------------------------)
    [## Join Medium'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Cassie Kozyrkov (and thousands of other writers on Medium).
    Your membership fee directly supports…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: kozyrkov.medium.com](https://kozyrkov.medium.com/membership?source=post_page-----5ca44398a552--------------------------------)
  prefs: []
  type: TYPE_NORMAL
