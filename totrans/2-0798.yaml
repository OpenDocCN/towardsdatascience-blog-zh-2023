- en: 'Embeddings: ChatGPT’s Secret Weapon'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/embeddings-chatgpts-secret-weapon-1870e590f32c](https://towardsdatascience.com/embeddings-chatgpts-secret-weapon-1870e590f32c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Embeddings, and how they help ChatGPT predict the next word
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://emmaccode.medium.com/?source=post_page-----1870e590f32c--------------------------------)[![Emma
    Boudreau](../Images/f7201d012b733643d6e97957f73fd1fa.png)](https://emmaccode.medium.com/?source=post_page-----1870e590f32c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1870e590f32c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1870e590f32c--------------------------------)
    [Emma Boudreau](https://emmaccode.medium.com/?source=post_page-----1870e590f32c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1870e590f32c--------------------------------)
    ·5 min read·Mar 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b73140f2b26fd1fb712cffe14b2f354e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: transformers and attention
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have been browsing the web a lot recently, or reading technical news
    stories, it is likely you have heard or read something about ChatGPT at some point.
    ChatGPT is OpenAI’s new **language transformer** model, and as far as these models
    go this is quite an accurate one that has produced some very compelling — sometimes
    viral — results. A **transformer** in this context refers to a machine-learning
    model which adopts **self-attention**. **Self-attention** is a Data Science word
    that simply means this model attempts to mimic human cognitive function, or human
    cognitive attention. The ***language*** part of this model is also significant;
    as this describes what the transformer hopes to predict, human language. This
    is often referred to as **Natural Language Processing**, or **NLP**, though NLP
    typically refers to the processing that takes place on the language data in order
    to turn it into numerical weights that a computer or neural network can understand.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformers have a number of significant properties that make their definition
    important to remember whenever we are discussing language transformers like ChatGPT.
    There are a few things that transformers, and more broadly attention models in
    general, that posses features that are not necessarily endemic to every type of
    machine-learning model. Under normal circumstances, a machine-learning model fits
    to some data and produces weights; we can think of this a lot like how a programming
    language compiles an executable: once the executable is compiled it becomes **static**
    and **immutable — it cannot be changed**, we cannot adjust the code from the inside.
    Transformers on the other hand have **soft weights**, which is a lot more like
    using a dynamically-typed programming language’s REPL, where the model weights
    **are mutable** and can be changed at runtime. This is the basis for many different
    and useful model types such as L**ong-Short-Term** **Memory**, or **LSTM**, models,
    and what we are discussing today: The Transformer.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a98179807beafbc730ef9ae844f079e.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: This figure shows a transformer model’s architecture. ([image by Wikimedia commons](https://en.wikipedia.org/wiki/File:The-Transformer-model-architecture.png))
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: embeddings
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This brings us to embeddings, which give a lot more power and capability to
    these soft weights. Embeddings work by creating a new layer of dimensionality
    that is lower than the dimensionality of your actual encoded sparse vectors. This
    can be thought of as almost a grouping for this data that factors into the final
    calculation of the model. In essence, embeddings are a low-dimensional space that
    gives cadence to much larger high-dimensional vectors. We are almost engineering
    a new feature into our model, which our model may use as a classification to infer
    more about nuances in our data — something that is very important when we consider
    the complexity of something like interpreting human language. Dimensionality,
    in this context, refers to the dimensions, or shape, of the data. A great way
    to think about embeddings is to think about a dart board.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Inside of a room, we have a dartboard. We want to utilize this dartboard in
    order to predict the level of skill of a given occupant. We find that people who
    are incredibly bad at darts tend to hit the bottom of the dartboard, whereas people
    who are a little skillful tend to hit the top, and very skilled people tend to
    hit the middle. The actual positions of where they are hitting would be our original
    features, which we would build weights and probability for. However, if we were
    to label each of these regions and associate them with how skilled people are
    to make a generalization on the data that lies there, which might help us to make
    more nuanced predictions within that context, this would be the concept of embeddings.
    The embedding becomes a point, data in and of itself, on axes that determine its
    similarity to other embeddings in this low-dimensional space. We could use this
    to predict how many years a darts player has been playing, for example.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77ea2fdc13d9657eab3e9b3cec011be7.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: a simplified interpretation of how this looks in your model. (image by author)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: In essence, what embeddings tell us about a particular set of data is that the
    data within the embedding is similar to the other data within that same embedding.
    Embeddings are essentially just categorical data on top of other data. Another
    thing to note is that these categories can also be multi-dimensional, meaning
    there can be more than one embedding that is used with weights potentially binded
    to the same embeddings. Embeddings can also be learned from data, which means
    this can be a component of a nueral network without adding a lot of things, making
    them a no-brainer for many applications, like transformers such as ChatGPT.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI has their own embeddings endpoint, which makes it super easy to perform
    natural language tasks, topic modeling, classification, and even clustering. If
    you would like to read more about OpenAI embeddingz, here is a link to the paper
    which discusses this in more detail:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://arxiv.org/abs/2201.10005?source=post_page-----1870e590f32c--------------------------------)
    [## Text and Code Embeddings by Contrastive Pre-Training'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Text embeddings are useful features in many applications such as semantic search
    and computing text similarity…
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: arxiv.org](https://arxiv.org/abs/2201.10005?source=post_page-----1870e590f32c--------------------------------)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to imagine how these embeddings play a role in processing text and
    creating an accurate language model prediction. Words can certainly be categorized,
    and when you consider the grand scheme of words in the English language, it can
    be very hard to get a grasp on what words actually do. However, if we break these
    words down into categories; articles, nouns, verbs, it can be much easier to ascertain
    an idea of how grammar actually works in our language. When it comes to language
    models, the goal of word embedding is to capture the meaning of a word in a vector
    representation. We create a general categorization for the meaning of a word and
    then utilize what we learn about that category to make inferences on our output.
    Also, the granularity ChatGPT uses to represent text is sub-words, not whole words.
    So ChatGPT utilizes these sorts of embeddings in order to categorize and describe
    certain parts of words.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易想象这些嵌入在处理文本和创建准确的语言模型预测中所扮演的角色。词语确实可以被分类，当你考虑到英语词汇的宏观图景时，理解词语实际作用可能会非常困难。然而，如果我们将这些词语分解为类别：冠词、名词、动词，那么了解语法如何在我们的语言中实际运作就会容易得多。对于语言模型来说，词嵌入的目标是捕捉词语在向量表示中的意义。我们为词语的意义创建一个通用分类，然后利用我们对该类别的了解来对输出做出推断。此外，ChatGPT
    用于表示文本的粒度是子词，而不是整个词。因此，ChatGPT 利用这些类型的嵌入来对词语的某些部分进行分类和描述。
- en: closing
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结束
- en: 'Embeddings are an essential concept for many different types of machine-learning
    models: recommendation algorithms, language transformers, and even classification
    models are some examples of targets that benefit greatly from having an embedding
    layer. OpenAI’s embedding implementation helps the ChatGPT model to interpret
    words based on categories and their numerical relation to those categories, which
    is a lot easier than trying to find insights on each individual word. If you would
    like to learn more about embeddings, and how to implement them into your own Tensorflow
    networks, I highly suggest the embedding tutorial that Google put out a few years
    ago, here is a link:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是许多不同类型机器学习模型的核心概念：推荐算法、语言转换器，甚至分类模型都是从拥有嵌入层中受益的例子。OpenAI 的嵌入实现帮助 ChatGPT
    模型根据类别及其与这些类别的数值关系来解释词语，这比尝试从每个单独词语中找出见解要容易得多。如果你想了解更多关于嵌入的内容，并且如何将它们应用到自己的 Tensorflow
    网络中，我强烈建议你查看 Google 几年前发布的嵌入教程，这里有一个链接：
- en: '[](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture?source=post_page-----1870e590f32c--------------------------------)
    [## Embeddings | Machine Learning | Google Developers'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture?source=post_page-----1870e590f32c--------------------------------)
    [## 嵌入 | 机器学习 | Google 开发者'
- en: An embedding is a relatively low-dimensional space into which you can translate
    high-dimensional vectors. Embeddings…
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入是一个相对低维的空间，你可以将高维向量转换到其中。嵌入…
- en: developers.google.com](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture?source=post_page-----1870e590f32c--------------------------------)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: developers.google.com](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture?source=post_page-----1870e590f32c--------------------------------)
- en: Thanks for reading!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
