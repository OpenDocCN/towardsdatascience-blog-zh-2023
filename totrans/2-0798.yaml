- en: 'Embeddings: ChatGPT’s Secret Weapon'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/embeddings-chatgpts-secret-weapon-1870e590f32c](https://towardsdatascience.com/embeddings-chatgpts-secret-weapon-1870e590f32c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Embeddings, and how they help ChatGPT predict the next word
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://emmaccode.medium.com/?source=post_page-----1870e590f32c--------------------------------)[![Emma
    Boudreau](../Images/f7201d012b733643d6e97957f73fd1fa.png)](https://emmaccode.medium.com/?source=post_page-----1870e590f32c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1870e590f32c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1870e590f32c--------------------------------)
    [Emma Boudreau](https://emmaccode.medium.com/?source=post_page-----1870e590f32c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1870e590f32c--------------------------------)
    ·5 min read·Mar 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b73140f2b26fd1fb712cffe14b2f354e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: transformers and attention
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have been browsing the web a lot recently, or reading technical news
    stories, it is likely you have heard or read something about ChatGPT at some point.
    ChatGPT is OpenAI’s new **language transformer** model, and as far as these models
    go this is quite an accurate one that has produced some very compelling — sometimes
    viral — results. A **transformer** in this context refers to a machine-learning
    model which adopts **self-attention**. **Self-attention** is a Data Science word
    that simply means this model attempts to mimic human cognitive function, or human
    cognitive attention. The ***language*** part of this model is also significant;
    as this describes what the transformer hopes to predict, human language. This
    is often referred to as **Natural Language Processing**, or **NLP**, though NLP
    typically refers to the processing that takes place on the language data in order
    to turn it into numerical weights that a computer or neural network can understand.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformers have a number of significant properties that make their definition
    important to remember whenever we are discussing language transformers like ChatGPT.
    There are a few things that transformers, and more broadly attention models in
    general, that posses features that are not necessarily endemic to every type of
    machine-learning model. Under normal circumstances, a machine-learning model fits
    to some data and produces weights; we can think of this a lot like how a programming
    language compiles an executable: once the executable is compiled it becomes **static**
    and **immutable — it cannot be changed**, we cannot adjust the code from the inside.
    Transformers on the other hand have **soft weights**, which is a lot more like
    using a dynamically-typed programming language’s REPL, where the model weights
    **are mutable** and can be changed at runtime. This is the basis for many different
    and useful model types such as L**ong-Short-Term** **Memory**, or **LSTM**, models,
    and what we are discussing today: The Transformer.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器具有一些显著的特性，使得它们的定义在讨论语言变换器如 ChatGPT 时特别重要。变换器以及更广泛的注意力模型，拥有一些特征，这些特征并非每种机器学习模型所特有。在正常情况下，机器学习模型适配某些数据并生成权重；我们可以把这看作编程语言编译可执行文件的过程：一旦可执行文件编译完成，它变得**静态**和**不可变——无法更改**，我们无法从内部调整代码。另一方面，变换器具有**软权重**，这更像是使用动态类型编程语言的
    REPL，模型权重**是可变的**，并且可以在运行时进行更改。这是许多不同且有用的模型类型的基础，如**长短期记忆**（**LSTM**）模型，以及我们今天讨论的变换器。
- en: '![](../Images/1a98179807beafbc730ef9ae844f079e.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a98179807beafbc730ef9ae844f079e.png)'
- en: This figure shows a transformer model’s architecture. ([image by Wikimedia commons](https://en.wikipedia.org/wiki/File:The-Transformer-model-architecture.png))
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图展示了变换器模型的架构。 ([图片来自维基共享资源](https://en.wikipedia.org/wiki/File:The-Transformer-model-architecture.png))
- en: embeddings
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: embeddings
- en: This brings us to embeddings, which give a lot more power and capability to
    these soft weights. Embeddings work by creating a new layer of dimensionality
    that is lower than the dimensionality of your actual encoded sparse vectors. This
    can be thought of as almost a grouping for this data that factors into the final
    calculation of the model. In essence, embeddings are a low-dimensional space that
    gives cadence to much larger high-dimensional vectors. We are almost engineering
    a new feature into our model, which our model may use as a classification to infer
    more about nuances in our data — something that is very important when we consider
    the complexity of something like interpreting human language. Dimensionality,
    in this context, refers to the dimensions, or shape, of the data. A great way
    to think about embeddings is to think about a dart board.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了 embeddings，它们为这些软权重提供了更多的力量和能力。Embeddings 通过创建一个维度低于实际编码稀疏向量的新的维度层来工作。这可以被认为是对这些数据的一种分组，这在模型的最终计算中起到作用。实际上，embeddings
    是一个低维空间，为更大的高维向量提供了节奏。我们几乎是在模型中加入了一个新的特征，我们的模型可能将其作为分类来推断数据中的更多细节——这在解释诸如人类语言这样复杂的事物时尤为重要。在这种情况下，维度指的是数据的维度或形状。理解
    embeddings 的一种很好的方式是把它想象成一个飞镖靶。
- en: Inside of a room, we have a dartboard. We want to utilize this dartboard in
    order to predict the level of skill of a given occupant. We find that people who
    are incredibly bad at darts tend to hit the bottom of the dartboard, whereas people
    who are a little skillful tend to hit the top, and very skilled people tend to
    hit the middle. The actual positions of where they are hitting would be our original
    features, which we would build weights and probability for. However, if we were
    to label each of these regions and associate them with how skilled people are
    to make a generalization on the data that lies there, which might help us to make
    more nuanced predictions within that context, this would be the concept of embeddings.
    The embedding becomes a point, data in and of itself, on axes that determine its
    similarity to other embeddings in this low-dimensional space. We could use this
    to predict how many years a darts player has been playing, for example.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个房间里，我们有一个飞镖板。我们希望利用这个飞镖板来预测给定房间内住客的技能水平。我们发现，非常差的飞镖手往往击中飞镖板的底部，而技能稍好的人往往击中顶部，非常有技巧的人则击中中间。实际击中位置将是我们的原始特征，我们将为其构建权重和概率。然而，如果我们给这些区域标记，并将它们与人的技能水平关联，以对这些数据做出更普遍的推断，这可能有助于我们在该上下文中做出更细致的预测，这就是嵌入的概念。嵌入成为一个点，数据本身，在确定其在这个低维空间中与其他嵌入的相似度的轴上。我们可以使用这来预测例如一个飞镖手已经玩了多少年。
- en: '![](../Images/77ea2fdc13d9657eab3e9b3cec011be7.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77ea2fdc13d9657eab3e9b3cec011be7.png)'
- en: a simplified interpretation of how this looks in your model. (image by author)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简化的示意图，展示了这在你的模型中的样子。（图片由作者提供）
- en: In essence, what embeddings tell us about a particular set of data is that the
    data within the embedding is similar to the other data within that same embedding.
    Embeddings are essentially just categorical data on top of other data. Another
    thing to note is that these categories can also be multi-dimensional, meaning
    there can be more than one embedding that is used with weights potentially binded
    to the same embeddings. Embeddings can also be learned from data, which means
    this can be a component of a nueral network without adding a lot of things, making
    them a no-brainer for many applications, like transformers such as ChatGPT.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，嵌入告诉我们关于一组数据的信息是，这组数据中的数据与该嵌入中的其他数据相似。嵌入本质上只是其他数据上的分类数据。另一个需要注意的点是，这些分类也可以是多维的，意味着可以有多个嵌入，并且权重可能绑定到相同的嵌入上。嵌入也可以从数据中学习，这意味着这可以是神经网络的一部分，而不会添加很多内容，使它们成为许多应用中的简单选择，例如
    ChatGPT 等变压器。
- en: 'OpenAI has their own embeddings endpoint, which makes it super easy to perform
    natural language tasks, topic modeling, classification, and even clustering. If
    you would like to read more about OpenAI embeddingz, here is a link to the paper
    which discusses this in more detail:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 有他们自己的嵌入端点，这使得执行自然语言任务、主题建模、分类，甚至聚类变得非常简单。如果你想了解更多关于 OpenAI 嵌入的内容，下面是一个详细讨论此主题的论文链接：
- en: '[](https://arxiv.org/abs/2201.10005?source=post_page-----1870e590f32c--------------------------------)
    [## Text and Code Embeddings by Contrastive Pre-Training'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arxiv.org/abs/2201.10005?source=post_page-----1870e590f32c--------------------------------)
    [## 文本和代码嵌入的对比预训练'
- en: Text embeddings are useful features in many applications such as semantic search
    and computing text similarity…
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本嵌入在许多应用中都是有用的特征，例如语义搜索和计算文本相似性……
- en: arxiv.org](https://arxiv.org/abs/2201.10005?source=post_page-----1870e590f32c--------------------------------)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: arxiv.org](https://arxiv.org/abs/2201.10005?source=post_page-----1870e590f32c--------------------------------)
- en: It is easy to imagine how these embeddings play a role in processing text and
    creating an accurate language model prediction. Words can certainly be categorized,
    and when you consider the grand scheme of words in the English language, it can
    be very hard to get a grasp on what words actually do. However, if we break these
    words down into categories; articles, nouns, verbs, it can be much easier to ascertain
    an idea of how grammar actually works in our language. When it comes to language
    models, the goal of word embedding is to capture the meaning of a word in a vector
    representation. We create a general categorization for the meaning of a word and
    then utilize what we learn about that category to make inferences on our output.
    Also, the granularity ChatGPT uses to represent text is sub-words, not whole words.
    So ChatGPT utilizes these sorts of embeddings in order to categorize and describe
    certain parts of words.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易想象这些嵌入在处理文本和创建准确的语言模型预测中所扮演的角色。词语确实可以被分类，当你考虑到英语词汇的宏观图景时，理解词语实际作用可能会非常困难。然而，如果我们将这些词语分解为类别：冠词、名词、动词，那么了解语法如何在我们的语言中实际运作就会容易得多。对于语言模型来说，词嵌入的目标是捕捉词语在向量表示中的意义。我们为词语的意义创建一个通用分类，然后利用我们对该类别的了解来对输出做出推断。此外，ChatGPT
    用于表示文本的粒度是子词，而不是整个词。因此，ChatGPT 利用这些类型的嵌入来对词语的某些部分进行分类和描述。
- en: closing
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结束
- en: 'Embeddings are an essential concept for many different types of machine-learning
    models: recommendation algorithms, language transformers, and even classification
    models are some examples of targets that benefit greatly from having an embedding
    layer. OpenAI’s embedding implementation helps the ChatGPT model to interpret
    words based on categories and their numerical relation to those categories, which
    is a lot easier than trying to find insights on each individual word. If you would
    like to learn more about embeddings, and how to implement them into your own Tensorflow
    networks, I highly suggest the embedding tutorial that Google put out a few years
    ago, here is a link:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是许多不同类型机器学习模型的核心概念：推荐算法、语言转换器，甚至分类模型都是从拥有嵌入层中受益的例子。OpenAI 的嵌入实现帮助 ChatGPT
    模型根据类别及其与这些类别的数值关系来解释词语，这比尝试从每个单独词语中找出见解要容易得多。如果你想了解更多关于嵌入的内容，并且如何将它们应用到自己的 Tensorflow
    网络中，我强烈建议你查看 Google 几年前发布的嵌入教程，这里有一个链接：
- en: '[](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture?source=post_page-----1870e590f32c--------------------------------)
    [## Embeddings | Machine Learning | Google Developers'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture?source=post_page-----1870e590f32c--------------------------------)
    [## 嵌入 | 机器学习 | Google 开发者'
- en: An embedding is a relatively low-dimensional space into which you can translate
    high-dimensional vectors. Embeddings…
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入是一个相对低维的空间，你可以将高维向量转换到其中。嵌入…
- en: developers.google.com](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture?source=post_page-----1870e590f32c--------------------------------)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: developers.google.com](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture?source=post_page-----1870e590f32c--------------------------------)
- en: Thanks for reading!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
