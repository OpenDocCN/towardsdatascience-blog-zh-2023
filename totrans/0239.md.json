["```py\nimport sys\nimport os\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n%matplotlib inline\n\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\npy.init_notebook_mode(connected=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom pandas import set_option\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer, RobustScaler\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score, GridSearchCV\nfrom sklearn.feature_selection import RFECV, SelectFromModel, SelectKBest, f_classif\nfrom sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score, ConfusionMatrixDisplay, f1_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\nfrom scipy.stats import uniform\n\nfrom imblearn.over_sampling import ADASYN\n\nimport swifter\n\n# Always good to set a seed for reproducibility\nSEED = 8\nnp.random.seed(SEED)\n```", "```py\ncolumns = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area_0', 'Wilderness_Area_1', 'Wilderness_Area_2',\n 'Wilderness_Area_3', 'Soil_Type_0', 'Soil_Type_1', 'Soil_Type_2', 'Soil_Type_3', 'Soil_Type_4', 'Soil_Type_5', 'Soil_Type_6', 'Soil_Type_7', 'Soil_Type_8',\n 'Soil_Type_9', 'Soil_Type_10', 'Soil_Type_11', 'Soil_Type_12', 'Soil_Type_13', 'Soil_Type_14', 'Soil_Type_15', 'Soil_Type_16', 'Soil_Type_17', 'Soil_Type_18',\n 'Soil_Type_19', 'Soil_Type_20', 'Soil_Type_21', 'Soil_Type_22', 'Soil_Type_23', 'Soil_Type_24', 'Soil_Type_25', 'Soil_Type_26', 'Soil_Type_27', 'Soil_Type_28',\n 'Soil_Type_29', 'Soil_Type_30', 'Soil_Type_31', 'Soil_Type_32', 'Soil_Type_33', 'Soil_Type_34', 'Soil_Type_35', 'Soil_Type_36', 'Soil_Type_37', 'Soil_Type_38',\n 'Soil_Type_39']  \n\nfrom sklearn import datasets\ndef sklearn_to_df(sklearn_dataset):\n    df = pd.DataFrame(sklearn_dataset.data, columns=columns)\n    df['target'] = pd.Series(sklearn_dataset.target)\n    return df\n\ndf = sklearn_to_df(datasets.fetch_covtype())\ndf_name=df.columns\ndf.head(3)\n```", "```py\n# here we are first separating our df into features (X) and target (y)\nX =  df[df_name[0:54]]\nY = df[df_name[54]]\n\n# now we are separating into training (80%) and test (20%) sets. The test set won't be seen until we want to test our top model!\nX_train, X_test, y_train, y_test =train_test_split(X,Y,\n                                                   train_size = 40_000,\n                                                   test_size=10_000,\n                                                   random_state=SEED,\n                                                   stratify=df['target']) # we stratify to ensure similar distribution in train/test\n```", "```py\n# engineering new columns from our df\ndef FeatureEngineering(X):\n\n    X['Aspect'] = X['Aspect'] % 360\n    X['Aspect_120'] = (X['Aspect'] + 120) % 360\n\n    X['Hydro_Elevation_sum'] = X['Elevation'] + X['Vertical_Distance_To_Hydrology']\n\n    X['Hydro_Elevation_diff'] = abs(X['Elevation'] - X['Vertical_Distance_To_Hydrology'])\n\n    X['Hydro_Euclidean'] = np.sqrt(X['Horizontal_Distance_To_Hydrology']**2 +\n                                   X['Vertical_Distance_To_Hydrology']**2)\n\n    X['Hydro_Manhattan'] = abs(X['Horizontal_Distance_To_Hydrology'] +\n                            X['Vertical_Distance_To_Hydrology'])\n\n    X['Hydro_Distance_sum'] = X['Horizontal_Distance_To_Hydrology'] + X['Vertical_Distance_To_Hydrology']\n\n    X['Hydro_Distance_diff'] = abs(X['Horizontal_Distance_To_Hydrology'] - X['Vertical_Distance_To_Hydrology'])\n\n    X['Hydro_Fire_sum'] = X['Horizontal_Distance_To_Hydrology'] + X['Horizontal_Distance_To_Fire_Points']\n\n    X['Hydro_Fire_diff'] = abs(X['Horizontal_Distance_To_Hydrology'] + X['Horizontal_Distance_To_Fire_Points'])\n\n    X['Hydro_Fire_mean'] = (X['Horizontal_Distance_To_Hydrology'] + X['Horizontal_Distance_To_Fire_Points'])/2\n\n    X['Hydro_Road_sum'] = X['Horizontal_Distance_To_Hydrology'] + X['Horizontal_Distance_To_Roadways']\n\n    X['Hydro_Road_diff'] = abs(X['Horizontal_Distance_To_Hydrology'] + X['Horizontal_Distance_To_Roadways'])\n\n    X['Hydro_Road_mean'] = (X['Horizontal_Distance_To_Hydrology'] + X['Horizontal_Distance_To_Roadways'])/2\n\n    X['Road_Fire_sum'] = X['Horizontal_Distance_To_Roadways'] + X['Horizontal_Distance_To_Fire_Points']\n\n    X['Road_Fire_diff'] = abs(X['Horizontal_Distance_To_Roadways'] - X['Horizontal_Distance_To_Fire_Points'])\n\n    X['Road_Fire_mean'] = (X['Horizontal_Distance_To_Roadways'] + X['Horizontal_Distance_To_Fire_Points'])/2\n\n    X['Hydro_Road_Fire_mean'] = (X['Horizontal_Distance_To_Hydrology'] + X['Horizontal_Distance_To_Roadways'] + \n                                  X['Horizontal_Distance_To_Fire_Points'])/3\n\n    return X\n\nX_train = X_train.swifter.apply(FeatureEngineering, axis = 1) \nX_test = X_test.swifter.apply(FeatureEngineering, axis = 1) \n```", "```py\nselector = SelectKBest(f_classif, k=15)\nselector.fit(X_train, y_train)\nmask = selector.get_support()\nX_train_reduced_cols = X_train.columns[mask]\n\nX_train_reduced_cols\n\n>>> Index(['Elevation', 'Wilderness_Area_3', 'Soil_Type_2', 'Soil_Type_3',\n       'Soil_Type_9', 'Soil_Type_37', 'Soil_Type_38', 'Hydro_Elevation_sum',\n       'Hydro_Elevation_diff', 'Hydro_Road_sum', 'Hydro_Road_diff',\n       'Hydro_Road_mean', 'Road_Fire_sum', 'Road_Fire_mean',\n       'Hydro_Road_Fire_mean'],\n        dtype='object')\n```", "```py\n# baseline models\ndef GetBaseModels():\n    baseModels = []\n    baseModels.append(('KNN'  , KNeighborsClassifier()))\n    baseModels.append(('RF'   , RandomForestClassifier()))\n    baseModels.append(('ET'   , ExtraTreesClassifier()))\n\n    return baseModels\n```", "```py\ndef ModelEvaluation(X_train, y_train,models):\n    # define number of folds and evaluation metric\n    num_folds = 10\n    scoring = \"f1_weighted\" #This is suitable for imbalanced classes\n\n    results = []\n    names = []\n    for name, model in models:\n        kfold = StratifiedKFold(n_splits=num_folds, random_state=SEED, shuffle = True)\n        cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring, n_jobs = -1)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n\n    return names, results\n```", "```py\ndef GetScaledModel(nameOfScaler):\n\n    if nameOfScaler == 'standard':\n        scaler = StandardScaler()\n    elif nameOfScaler =='minmax':\n        scaler = MinMaxScaler()\n\n    pipelines = []\n    pipelines.append((nameOfScaler+'KNN' , Pipeline([('Scaler', scaler),('KNN' , KNeighborsClassifier())])))\n    pipelines.append((nameOfScaler+'RF'  , Pipeline([('Scaler', scaler),('RF'  , RandomForestClassifier())])))\n    pipelines.append((nameOfScaler+'ET'  , Pipeline([('Scaler', scaler),('ET'   , ExtraTreesClassifier())])))\n\n    return pipelines \n```", "```py\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_reduced), columns=X_train_reduced.columns)\nX_test_scaled = pd.DataFrame(scaler.transform(X_test_reduced), columns=X_test_reduced.columns)\n```", "```py\nclass GridSearch(object):\n\n    def __init__(self,X_train,y_train,model,hyperparameters):\n\n        self.X_train = X_train\n        self.y_train = y_train\n        self.model = model\n        self.hyperparameters = hyperparameters\n\n    def GridSearch(self):\n\n        cv = 10\n        clf = GridSearchCV(self.model,\n                                 self.hyperparameters,\n                                 cv=cv,\n                                 verbose=0,\n                                 n_jobs=-1,\n                                 )\n        # fit grid search\n        best_model = clf.fit(self.X_train, self.y_train)\n        message = (best_model.best_score_, best_model.best_params_)\n        print(\"Best: %f using %s\" % (message))\n\n        return best_model,best_model.best_params_\n\n    def BestModelPredict(self,X_train):\n\n        best_model,_ = self.GridSearch()\n        pred = best_model.predict(X_train)\n        return pred\n```", "```py\n# 1) KNN\nmodel_KNN = KNeighborsClassifier()\nneighbors = [1,3,5,7,9,11,13,15,17,19] # Number of neighbors to use by default for k_neighbors queries\nparam_grid = dict(n_neighbors=neighbors)\n\n# 2) RF\nmodel_RF = RandomForestClassifier()\nn_estimators_value = [50,100,150,200,250,300] # The number of trees \ncriterion = ['gini', 'entropy', 'log_loss'] # The function to measure the quality of a split\nparam_grid = dict(n_estimators=n_estimators_value, criterion=criterion)\n\n# 3) ET\nmodel_ET = ExtraTreesClassifier()\nn_estimators_value = [50,100,150,200,250,300] # The number of trees \ncriterion = ['gini', 'entropy', 'log_loss'] # The function to measure the quality of a split\nparam_grid = dict(n_estimators=n_estimators_value, criterion=criterion) \n```", "```py\nparam = {'n_neighbors': 1}\nmodel1 = KNeighborsClassifier(**param)\n\nparam = {'criterion': 'entropy', 'n_estimators': 300}\nmodel2 = RandomForestClassifier(**param)\n\nparam = {'criterion': 'gini', 'n_estimators': 300}\nmodel3 = ExtraTreesClassifier(**param)\n\n# create the models based on above parameters\nestimators = [('KNN',model1), ('RF',model2), ('ET',model3)]\n\n# create the ensemble model\nkfold = StratifiedKFold(n_splits=10, random_state=SEED, shuffle = True)\nensemble = VotingClassifier(estimators)\nresults = cross_val_score(ensemble, X_train_scaled, y_train, cv=kfold)\nprint('F1 weighted score on train: ',results.mean())\nensemble_model = ensemble.fit(X_train_scaled,y_train)\npred = ensemble_model.predict(X_test_scaled)\nprint('F1 weighted score on test:' , (y_test == pred).mean())\n\n>>> F1 weighted score on train:  0.8747\n>>> F1 weighted score on test: 0.8836\n```", "```py\nfrom sklearn.metrics import plot_confusion_matrix\ncfm_raw = plot_confusion_matrix(ensemble_model, X_test_scaled, y_test, values_format = '') # add normalize = 'true' for precision matrix or 'pred' for recall matrix\nplt.savefig(\"cfm_raw.png\")\n```", "```py\n# add predicted values test_df to compare with ground truth\ntest_df['predicted'] = pred\n\n# create class 0 = no error , 1 = error\ntest_df['error'] = (test_df['target']!=test_df['predicted']).astype(int)\n\n# create our error classification set\nX_error = test_df[['Elevation', 'Wilderness_Area_3', 'Soil_Type_2', 'Soil_Type_3', 'Soil_Type_9', 'Soil_Type_37', 'Soil_Type_38',\n                       'Hydro_Elevation_sum', 'Hydro_Elevation_diff', 'Hydro_Road_sum', 'Hydro_Road_diff', 'Hydro_Road_mean', 'Road_Fire_sum',\n                       'Road_Fire_mean', 'Hydro_Road_Fire_mean']]\n\nX_error_names = X_error.columns\ny_error = test_df['error']\n```", "```py\nimport shap\nkfold = StratifiedKFold(n_splits=10, random_state=SEED, shuffle = True)\n\nlist_shap_values = list()\nlist_test_sets = list()\nfor train_index, test_index in kfold.split(X_error, y_error):\n    X_error_train, X_error_test = X_error.iloc[train_index], X_error.iloc[test_index]\n    y_error_train, y_error_test = y_error.iloc[train_index], y_error.iloc[test_index]\n    X_error_train = pd.DataFrame(X_error_train,columns=X_error_names)\n    X_error_test = pd.DataFrame(X_error_test,columns=X_error_names)\n\n    #training model\n    clf = RandomForestClassifier(criterion = 'entropy', n_estimators = 300, random_state=SEED)\n    clf.fit(X_error_train, y_error_train)\n\n    #explaining model\n    explainer = shap.TreeExplainer(clf)\n    shap_values = explainer.shap_values(X_error_test)\n    #for each iteration we save the test_set index and the shap_values\n    list_shap_values.append(shap_values)\n\n# flatten list of lists, pick the sv for 1 class, stack the result (you only need to look at 1 class for binary classification since values will be opposite to one another)\nshap_values_av = np.vstack([sv[1] for sv in list_shap_values])\nsv = np.abs(shap_values_av).mean(0) \nsv_std = np.abs(shap_values_av).std(0) \nsv_max = np.abs(shap_values_av).max(0) \nimportance_df = pd.DataFrame({\n    \"column_name\": X_error_names,\n    \"shap_values_av\": sv,\n    \"shap_values_std\": sv_std,\n    \"shap_values_max\": sv_max\n})\n```"]