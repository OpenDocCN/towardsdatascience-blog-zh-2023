- en: How to Do Language Detection Using Python, NLTK, and Some Easy Statistics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用 Python、NLTK 和一些简单的统计进行语言检测
- en: 原文：[https://towardsdatascience.com/how-to-do-language-detection-using-python-nltk-and-some-easy-statistics-6cec9a02148](https://towardsdatascience.com/how-to-do-language-detection-using-python-nltk-and-some-easy-statistics-6cec9a02148)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-do-language-detection-using-python-nltk-and-some-easy-statistics-6cec9a02148](https://towardsdatascience.com/how-to-do-language-detection-using-python-nltk-and-some-easy-statistics-6cec9a02148)
- en: A practical introduction to a technology you use every day.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个关于你每天使用的技术的实用介绍。
- en: '[](https://katherineamunro.medium.com/?source=post_page-----6cec9a02148--------------------------------)[![Katherine
    Munro](../Images/8013140495c7b9bd25ef08d712f097bf.png)](https://katherineamunro.medium.com/?source=post_page-----6cec9a02148--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6cec9a02148--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6cec9a02148--------------------------------)
    [Katherine Munro](https://katherineamunro.medium.com/?source=post_page-----6cec9a02148--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://katherineamunro.medium.com/?source=post_page-----6cec9a02148--------------------------------)[![Katherine
    Munro](../Images/8013140495c7b9bd25ef08d712f097bf.png)](https://katherineamunro.medium.com/?source=post_page-----6cec9a02148--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6cec9a02148--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6cec9a02148--------------------------------)
    [Katherine Munro](https://katherineamunro.medium.com/?source=post_page-----6cec9a02148--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6cec9a02148--------------------------------)
    ·12 min read·Jan 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----6cec9a02148--------------------------------)
    ·12分钟阅读·2023年1月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/645352f0855da593110591ce2a9202a4.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/645352f0855da593110591ce2a9202a4.png)'
- en: Photo by [Etienne Girardet](https://unsplash.com/@etiennegirardet) on [Unsplash](https://unsplash.com)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Etienne Girardet](https://unsplash.com/@etiennegirardet)提供，来源于[Unsplash](https://unsplash.com)
- en: Ever wondered how Google Translate’s ‘detect language’ feature works? Of course
    you didn’t, you had better things to do. But I went looking, and couldn’t find
    the answer (even though [I’ve literally written a book](https://www.amazon.com/Handbook-Data-Science-AI-Analytics/dp/1569908869)
    on Natural Language Processing (NLP)). It’s Google’s secret sauce. So today, I’ll
    instead show you a super simple way to do language detection yourself, using one
    highly underrated NLP tool and some really easy maths. You’ll be adding it to
    your GitHub portfolio in no time.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有没有想过 Google Translate 的“检测语言”功能是如何工作的？当然你没想过，你有更重要的事情要做。但我去寻找答案时却没有找到（尽管[我确实写了一本关于自然语言处理（NLP）的书](https://www.amazon.com/Handbook-Data-Science-AI-Analytics/dp/1569908869)）。这是
    Google 的秘密调料。所以今天，我将向你展示一种超级简单的方法，使用一个被低估的 NLP 工具和一些非常简单的数学，你自己进行语言检测。你很快就能将它添加到你的
    GitHub 作品集中。
- en: '**What is Language Detection and why is it used?**'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**什么是语言检测，为什么要使用它？**'
- en: 'Language detection just means identifying the language of a piece of input
    text. It’s a first step for many tasks in Natural Language Processing, including
    many that you use every day:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 语言检测仅仅意味着识别一段输入文本的语言。这是自然语言处理中的许多任务的第一步，包括许多你每天使用的任务：
- en: Spelling and grammar correction (think MS Word, Google Docs, etc)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拼写和语法修正（例如 MS Word、Google Docs 等）
- en: Next word prediction (your phone does this all the time!)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一个词预测（你的手机一直在做这个！）
- en: Machine translation (e.g. in Google Translate’s ‘detect language’ option)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译（例如 Google Translate 的“检测语言”选项）
- en: '![](../Images/e7bd41ad34bcc968094da7111a953d1a.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7bd41ad34bcc968094da7111a953d1a.png)'
- en: 'Source: Author'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者
- en: '**How can we detect a language?**'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**我们如何检测语言？**'
- en: 'A simple way to do language identification would be this: build vocabularies
    (word lists) for different languages, then count how many times each language’s
    words occur in a text. So if the test text contained five Japanese words and two
    English ones, we might conclude that it’s Japanese. We could even focus on so-called
    ‘stop words’, which are words which occur very frequently in languages and often
    deliver little meaning but are important for the grammar, such as ‘the’, ‘a’ and
    ‘and’.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 进行语言识别的一种简单方法是：为不同的语言建立词汇表（词汇列表），然后计算每种语言的词汇在文本中出现的次数。因此，如果测试文本包含五个日语单词和两个英语单词，我们可能会得出它是日语的结论。我们甚至可以专注于所谓的“停用词”，这些词在语言中出现频率很高，虽然往往没有什么实际意义，但对语法非常重要，例如“the”，“a”和“and”。
- en: 'The problem is, many words occur in multiple languages, even if they have different
    meanings. For example, ‘gift’ means ‘present’ in English, but ‘poison’ in German.
    So the phrase ‘Das Gift’ could present a problem, especially if there are typos*.
    Imagine we wanted to say ‘the poison is strong’: Das Gift ist stark. If we forgot
    the ‘-t’ in ‘ist’, we would have ‘Das Gift is stark’. Since ‘stark’ occurs in
    both languages, we now really have a problem. And focusing on stopwords could
    make it even worse. For example, French and German both frequently use ‘des’ and
    ‘du’, so if we only look at those, we’re going to come unstuck.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是，许多词汇在多种语言中都有出现，即使它们有不同的含义。例如，‘gift’在英语中表示‘礼物’，但在德语中表示‘毒药’。所以短语‘Das Gift’可能会带来问题，特别是如果有拼写错误的话。假设我们想说‘毒药很强’：Das
    Gift ist stark。如果我们忘记了‘ist’中的‘t’，我们就会得到‘Das Gift is stark’。由于‘stark’在两种语言中都出现，我们现在确实遇到问题了。而且，集中关注停用词可能会使问题更严重。例如，法语和德语都经常使用‘des’和‘du’，所以如果我们只关注这些，我们就会陷入困境。
- en: '*Fun fact: in Natural Language Processing, there are *always* typos.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*有趣的事实：在自然语言处理领域，*总是*会有拼写错误。*'
- en: An alternative is to concentrate on the distribution of letters, instead of
    words. For example, compared to English, German uses umlauts (ä, ö, ü), and French
    uses lots of special characters (ç, â/ê/î/ô/û, à/è/ì/ò/ù, ë/ï/ü). Combinations
    of 2 and 3 letters, called bi- and trigrams, work even better. That’s because
    different languages have letter combinations that don’t occur in other languages.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是集中于字母的分布，而不是单词。例如，与英语相比，德语使用变音符号（ä、ö、ü），法语使用很多特殊字符（ç、â/ê/î/ô/û、à/è/ì/ò/ù、ë/ï/ü）。2个和3个字母的组合，称为bi-和trigrams，效果更好。这是因为不同的语言有一些在其他语言中不存在的字母组合。
- en: '**Building a Language Detection Model in Python**'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**在Python中构建语言检测模型**'
- en: 'Our language detection method will use uni-, bi- and tri-grams: that is, individual
    letters, and combinations of two and three letters. The generic term for such
    combinations is ‘n-grams’. We will create statistical models for different languages
    by counting their n-gram frequencies. Then we’ll compare these with the frequences
    of n-grams in a test text. The language whose n-gram frequencies best match the
    test sentence will be our winner.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的语言检测方法将使用uni-、bi-和tri-grams：即，单个字母以及两个和三个字母的组合。这些组合的通用术语是‘n-grams’。我们将通过计算它们的n-gram频率来创建不同语言的统计模型。然后我们将这些与测试文本中的n-gram频率进行比较。n-gram频率与测试句子最匹配的语言将是我们的赢家。
- en: This approach is based on [1].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法基于[1]。
- en: '**Visualise N-Grams**'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**可视化N-Grams**'
- en: 'Let’s start by visualising some n-grams of different lengths:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始可视化一些不同长度的n-gram：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Build an N-Gram Extractor**'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**构建一个N-Gram提取器**'
- en: 'Let’s define a function extract_xgrams(). It will take a text and a list of
    numbers, n_vals, and extract n-grams of those lengths from the text:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个函数`extract_xgrams()`。它将接收一个文本和一个数字列表`n_vals`，并从文本中提取这些长度的n-grams：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Note that we lower case our test text. This will reduce the number of n-grams
    we get back, without losing much information about the language itself (think
    about it: If I say ‘i went to new york’, you still understand me, even without
    the capitalisation).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将测试文本转为小写。这将减少返回的n-gram数量，同时不会丢失关于语言本身的信息（想想看：如果我说‘i went to new york’，即使没有大写，你仍然能理解我）。
- en: '**Define A Function for Building a Language Model**'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**定义一个构建语言模型的函数**'
- en: Our build_model() function makes use of collections.Counter. The Counter takes
    a list, counts all occurrences of each item in the list, and returns a dictionary
    with each item and its frequency.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`build_model()`函数使用了collections.Counter。Counter接收一个列表，计算列表中每个项目的出现次数，并返回一个包含每个项目及其频率的字典。
- en: 'So for any language, we can model it by creating a dictionary of n-grams and
    their probability of occurring in that language. What’s the probability of an
    n-gram? It’s simply its frequency, divided by the total number of extracted n-grams.
    Let’s run the code and print the language model, sorted so that the most frequent
    n-gram comes first:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于任何语言，我们可以通过创建一个n-gram字典及其在该语言中出现的概率来对其建模。n-gram的概率是多少？它仅仅是其频率，除以提取的n-gram总数。让我们运行代码并打印语言模型，排序使得最频繁的n-gram排在前面：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Install NLTK and Download Our Text Data**'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**安装NLTK并下载我们的文本数据**'
- en: The Natural Language ToolKit is a hidden gem for Natural Language Processing.
    It contains classes and methods for text processing, and a large selection of
    text corpora (collections of prepared text data) for you to practice with. If
    you don’t have NLTK installed, you can install it using your preferred method,
    e.g. pip install nltk ([see the guide](https://www.nltk.org/install.html))
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言工具包（Natural Language ToolKit）是自然语言处理的一个隐藏宝石。它包含用于文本处理的类和方法，以及大量的文本语料库（准备好的文本数据集合）供你练习。如果你还没有安装
    NLTK，可以使用你喜欢的方法安装，例如 `pip install nltk` ([查看指南](https://www.nltk.org/install.html))
- en: 'For testing our language identifier we will use the Universal Declaration of
    Human Rights (UDHR), which is [included in NLTK](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&cd=&cad=rja&uact=8&ved=2ahUKEwi5p7avpMf8AhWI3KQKHbn7DmcQFnoECAgQAw&url=https%3A%2F%2Fwww.nltk.org%2Fbook%2Fch02.html&usg=AOvVaw1czTHzjPxNOk1JLrT_zeOS)
    in 300 languages. In reality, such a dataset is too small and the text is too
    clean (the UN presumably proofread their work, and they probably don’t use hashtags
    and emojis. Those buzzkills). But this dataset is enough to demonstrate the concepts
    of what we’re trying to do here. Plus, it will introduce you to working with NLTK:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们的语言识别器，我们将使用《世界人权宣言》（UDHR），它在 [NLTK 中包含](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&cd=&cad=rja&uact=8&ved=2ahUKEwi5p7avpMf8AhWI3KQKHbn7DmcQFnoECAgQAw&url=https%3A%2F%2Fwww.nltk.org%2Fbook%2Fch02.html&usg=AOvVaw1czTHzjPxNOk1JLrT_zeOS)
    300 种语言。实际上，这样的数据集太小，文本也太干净（联合国大概进行了校对，而且他们可能不使用标签和表情符号，那些扫兴的东西）。但这个数据集足以演示我们在这里尝试做的概念。此外，它将使你了解如何使用
    NLTK：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For simplicity, I’ll choose just a handful of languages to work with. They
    all use similar characters, so it’ll be a tougher test for our detector. Feel
    free to add more languages: the code comments will show you how:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我将选择几个语言进行处理。它们都使用类似的字符，因此对我们的检测器来说会是一个更艰难的测试。可以随意添加更多语言：代码注释将告诉你如何操作：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The command udhr.raw(fileids) returns the complete text of the specified fileid(s).
    We’ll use it to build a dictionary with each language name and its text, and from
    this dictionary we’ll build a model of each language:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 命令 `udhr.raw(fileids)` 返回指定 `fileid` 的完整文本。我们将使用它来建立一个包含每种语言名称及其文本的字典，然后从这个字典中构建每种语言的模型：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Determine the language for a given piece of text**'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**确定给定文本的语言**'
- en: We can now take a test text and compare its n-gram frequencies to those of our
    various language models. The aim will be to see which language has the closest
    frequencies to our test text.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用测试文本，将其 n-gram 频率与各种语言模型的频率进行比较。目的是查看哪种语言的频率与我们的测试文本最接近。
- en: 'We do this by calculating the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity),
    as per the formula below:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过计算 [余弦相似度](https://en.wikipedia.org/wiki/Cosine_similarity)，如下面的公式所示：
- en: '![](../Images/be95b3e7d99c3eac83df4a11cfcd6645.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be95b3e7d99c3eac83df4a11cfcd6645.png)'
- en: The cosine similarity formula
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度公式
- en: 'It looks scary, but we won’t go into the math. Basically the cosine similarity
    is used to compare two numeric vectors. The result will be in the range of −1,
    meaning exactly opposite, to 1, meaning exactly the same. Our calculate_cosine()
    formula implements the math:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来很吓人，但我们不会深入数学。基本上，余弦相似度用于比较两个数值向量。结果范围从−1，表示完全相反，到1，表示完全相同。我们的 `calculate_cosine()`
    公式实现了这些数学计算：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: It’s time to build an identify_language() function. This will take a test text,
    build a model for it using n grams of different sizes (specified by n_vals), and
    compare it to a dictionary of language models. The output will be the name of
    the language most-similar to the test text.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候构建一个 `identify_language()` 函数了。这个函数将接受一个测试文本，使用不同大小的 n-gram（由 `n_vals`
    指定）为其构建一个模型，并将其与语言模型字典进行比较。输出将是与测试文本最相似的语言名称。
- en: For demonstration purposes, I added a print statement in the function to show
    the similarity of each language to the test text. You can delete this after you’ve
    gotten a feeling for what cosine values look like.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 出于演示目的，我在函数中添加了一条打印语句以显示每种语言与测试文本的相似度。你可以在你对余弦值有了感觉后删除它。
- en: 'Running this function for our original test text, the highest similarity correctly
    occurs for English:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对原始测试文本运行此函数时，最高的相似度正确地出现在英语中：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Before you delete that print line, looks what happens when we test the function
    with a radically different text; the similarity values generally decrease:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在你删除那一行打印语句之前，看看当我们用一个截然不同的文本测试这个函数时会发生什么；相似度值通常会降低：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Test our language detector on different languages**'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**测试我们的语言检测器在不同语言上的效果**'
- en: 'Let’s see how we go with texts in Dutch, French and Spanish:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看荷兰语、法语和西班牙语文本的效果：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The results are correct, except that Italian is output for the second example,
    instead of French. Merde!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是正确的，除了第二个例子输出的是意大利语而不是法语。天哪！
- en: '**Improve the model**'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**改进模型**'
- en: 'Clearly our models aren’t perfect, but there are lots of ways we could improve
    them:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们的模型并不完美，但有很多方法可以改进它们：
- en: '**Use bigger and more representative data:** I admitted earlier that our training
    texts are too short and clean to realistically reflect language identification
    in the wild. In fact, they are only a *sample* of the Declaration, with each text
    truncated to approximately 1000 characters. You can see this by exploring the
    number of words and characters in the text for each language:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用更大、更具代表性的数据：** 我之前承认我们的训练文本太短且过于干净，无法真实反映实际语言识别情况。事实上，它们只是《宣言》的*样本*，每个文本被截断到大约1000个字符。你可以通过探索每种语言中文本的单词和字符数量来查看这一点：'
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The training texts were good enough to introduce you to NLTK, and to this simple
    method of language detection, but in order to improve our generalisability, we
    need to build models using longer, more diverse, real-world text data: typos,
    hashtags, emojis and all.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 训练文本足够让你了解NLTK和这种简单的语言检测方法，但为了提高我们的泛化能力，我们需要使用更长、更具多样性的真实世界文本数据：错别字、标签、表情符号等等。
- en: '![](../Images/bacf8c31a69c0a5ea9d2aa476900758c.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bacf8c31a69c0a5ea9d2aa476900758c.png)'
- en: 'I asked ChatGPT for the UDHR in ‘Twitter speak’. Even this is fairly clean
    and clear; it can get a lot worse. Source: Author provided screenshot from a dialog
    ‘ChatGPT’, from OpenAI.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我向ChatGPT询问了“Twitter 语言风格”的《世界人权宣言》。即便这是相当干净和清晰的，它也可能变得更糟。来源：作者提供的“ChatGPT”对话截图，来自OpenAI。
- en: 'Why do we need longer, more diverse texts? Simple: it’s the only way to capture
    each language, and what makes it different from other languages.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要更长、更具多样性的文本？很简单：这是捕捉每种语言及其与其他语言不同之处的唯一方法。
- en: Take the word ‘gnome’, for example. Unless access to a garden gnome is considered
    a universal human right, the trigram ‘ gn’ (whitespace, g, n) probably doesn’t
    feature in our English sample data. You might think that’s ok, because there aren’t
    many English words beginning with ‘gn’. (There are a few, but they’re rarely used).
    The problem is, what if this *is* a common pattern in other languages? There are,
    in fact, [lots of common German words like this](https://www.wordmine.info/Search?slang=de&stype=words-that-starts-with&sword=GN),
    but not one of them occurs in the UDHR (I checked). So if we see a test text with
    the trigram ‘ gn’, it won’t contribute to the x-gram probabilities we’re summing
    up, for either language. And that means it won’t help us differentiate between
    them.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以“gnome”这个词为例。除非将接触园艺侏儒视为普世人权，否则三字母组‘ gn’（空格，g，n）可能不在我们的英语样本数据中。你可能会觉得没关系，因为没有很多英语单词以‘gn’开头。（虽然有一些，但很少用）。问题是，如果这是其他语言中的*常见模式*呢？实际上，确实有[许多这样的德语常见词](https://www.wordmine.info/Search?slang=de&stype=words-that-starts-with&sword=GN)，但其中没有一个出现在《世界人权宣言》中（我查过了）。因此，如果我们看到含有三字母组‘
    gn’的测试文本，它不会对我们总结的x-gram概率产生贡献，无论是哪种语言。这意味着它无法帮助我们区分它们。
- en: '**Add more features:** We could have added character x-grams of additional
    lengths, like quadgrams (four letters). Word-based x-grams might also help. The
    benefit in both cases is the same as with using longer texts: more features help
    capture differentiating factors between languages. For example, I’m unlikely to
    say ‘die Marmelade!’, even though I hate the stuff. But some Germans would say
    this every day at breakfast (‘die’ is just one version of ‘the’). So using word
    x-grams could capture this difference.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**添加更多特征：** 我们本可以添加额外长度的字符x-grams，例如四字母组（quadgrams）。基于单词的x-grams也可能有帮助。这两种情况的好处与使用更长文本相同：更多特征有助于捕捉语言之间的区别。例如，尽管我讨厌果酱，但我不太可能说“die
    Marmelade!”。但一些德国人每天早餐时都会这么说（‘die’只是‘the’的一种变体）。因此，使用单词x-grams可能捕捉到这种差异。'
- en: 'There are some problems with word x-grams though. Most languages have many
    more words than they have characters in their alphabet, so just adding word bi-grams
    will explode the number of items in our language models. Tri-grams and larger
    will only make the matter worse. Bigger models will slow the entire process down,
    and for very little gain: the majority of these x-gram word combinations will
    barely ever occur, so they won’t even contribute much to helping differentiate
    between languages at test time.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，单词 x-grams 有一些问题。大多数语言的单词数量远远超过其字母表中的字符数量，因此仅仅添加单词二元组将会爆炸性地增加我们语言模型中的项数。三元组及更大的项只会使问题变得更严重。更大的模型会使整个过程变慢，而且收效甚微：这些
    x-gram 单词组合中的大多数几乎不会出现，因此它们甚至不会在测试时对区分语言贡献多少。
- en: A better approach could be to use stopwords, as each language has their own
    set that occur very frequently and are thus good indicators. I said earlier that
    modelling languages using only stopwords is risky, as they can appear in multiple
    languages. But using them as additional features to our character x-grams, or
    using them as part of word x-grams, tackles this problem.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更好的方法是使用停用词，因为每种语言都有自己频繁出现的停用词，这些停用词是很好的指示词。我之前提到过，单独使用停用词来建模语言是有风险的，因为它们可能出现在多种语言中。但将它们作为我们字符
    x-grams 的附加特征，或作为单词 x-grams 的一部分来使用，可以解决这个问题。
- en: Similarly, we can add the top 1000 or 10,000 words in each language (or use
    them in word x-grams). The theory behind this is that words tend to follow ‘[Zip’s
    law](https://en.wikipedia.org/wiki/Zipf%27s_law)’, where the most common word
    tends to occur about twice as often as the next most common, three times as often
    as the third most common, and so on. So by taking just the top n words, you can
    capture probabilities for the majority of words in your input data and — crucially
    — your test data. And these probabilities are what our language detection decision
    will be based on.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以在每种语言中添加前 1000 或 10,000 个单词（或在单词 x-grams 中使用它们）。其背后的理论是，单词往往遵循‘[Zipf
    定律](https://en.wikipedia.org/wiki/Zipf%27s_law)’，即最常见的单词出现的频率大约是第二常见单词的两倍，第三常见单词的三倍，以此类推。因此，通过仅取前
    n 个单词，你可以捕捉到输入数据和 —— 关键是 —— 测试数据中大多数单词的概率。这些概率将是我们语言检测决策的依据。
- en: '**Use machine learning:** You can’t talk about ‘features’ without thinking
    of machine learning. There are many algorithms we could try, including some surprisingly
    simple but effective options like Naive Bayes.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用机器学习：** 你不能谈论‘特征’而不考虑机器学习。我们可以尝试许多算法，包括一些令人惊讶的简单但有效的选项，如朴素贝叶斯。'
- en: Understanding those algorithms will take an entire new blog post, but the curious
    can read about Naive Bayes classifiers for language detection [here](https://dbs.cs.uni-duesseldorf.de/lehre/bmarbeit/barbeiten/ba_panich.pdf).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些算法需要整篇新博客，但好奇的读者可以在[这里](https://dbs.cs.uni-duesseldorf.de/lehre/bmarbeit/barbeiten/ba_panich.pdf)阅读关于语言检测的朴素贝叶斯分类器。
- en: '**Add more languages:** I used just a few, but there are thousands of languages
    in the world, and they all deserve love. So here’s a challenge to you: add more
    languages, and see how you go (I even gave you the code already).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**添加更多语言：** 我只用了几种语言，但世界上有成千上万种语言，它们都值得关注。所以这是给你的挑战：添加更多语言，看看你能做到什么（我甚至已经给了你代码）。'
- en: '**Applying this concept to other NLP tasks**'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**将这一概念应用于其他 NLP 任务**'
- en: The concepts covered in this post can easily be applied to other challenges,
    and NLTK’s built in corpora can help you do it. So follow me for future posts,
    where we’ll cover document classification and speaker identification using — you
    guessed it — Python, NLTK, and those really simple statistics.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中涉及的概念可以很容易地应用于其他挑战，而 NLTK 内置的语料库可以帮助你实现。因此，请关注我的后续帖子，我们将使用 —— 你猜对了 —— Python、NLTK
    和那些非常简单的统计学来讨论文档分类和说话人识别。
- en: '**Thanks for reading!**'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**感谢阅读！**'
- en: Firstly, cheers to the creators of [the tutorial](https://textmining.wp.hs-hannover.de/Sprachbestimmung.html)
    which inspired this piece.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，感谢[教程](https://textmining.wp.hs-hannover.de/Sprachbestimmung.html)的创作者，它们启发了这篇文章。
- en: If this article helped you — great !— please subscribe for more content on Natural
    Language Processing and other data science fundamentals. You can also connect
    with me on [Twitter](https://twitter.com/KatherineAMunro) (where I post loads
    of interesting content on AI, tech, ethics, and more).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这篇文章对你有所帮助 —— 太棒了！—— 请订阅获取更多关于自然语言处理和其他数据科学基础的内容。你还可以在[Twitter](https://twitter.com/KatherineAMunro)上与我联系（我会发布大量有趣的内容，涉及
    AI、技术、伦理等）。
- en: '[1] Cavnar, W.B., Trenkle, J.M.: [N-gram-based text categorization](https://dsacl3-2019.github.io/materials/CavnarTrenkle.pdf)
    (1994), Proceedings of SDAIR-94, 3rd annual symposium on document analysis and
    information retrieval.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Cavnar, W.B., Trenkle, J.M.：[基于N-gram的文本分类](https://dsacl3-2019.github.io/materials/CavnarTrenkle.pdf)
    (1994)，SDAIR-94文档分析与信息检索第三届年会论文集。'
