- en: How to Do Language Detection Using Python, NLTK, and Some Easy Statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-do-language-detection-using-python-nltk-and-some-easy-statistics-6cec9a02148](https://towardsdatascience.com/how-to-do-language-detection-using-python-nltk-and-some-easy-statistics-6cec9a02148)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A practical introduction to a technology you use every day.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://katherineamunro.medium.com/?source=post_page-----6cec9a02148--------------------------------)[![Katherine
    Munro](../Images/8013140495c7b9bd25ef08d712f097bf.png)](https://katherineamunro.medium.com/?source=post_page-----6cec9a02148--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6cec9a02148--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6cec9a02148--------------------------------)
    [Katherine Munro](https://katherineamunro.medium.com/?source=post_page-----6cec9a02148--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6cec9a02148--------------------------------)
    ·12 min read·Jan 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/645352f0855da593110591ce2a9202a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Etienne Girardet](https://unsplash.com/@etiennegirardet) on [Unsplash](https://unsplash.com)
  prefs: []
  type: TYPE_NORMAL
- en: Ever wondered how Google Translate’s ‘detect language’ feature works? Of course
    you didn’t, you had better things to do. But I went looking, and couldn’t find
    the answer (even though [I’ve literally written a book](https://www.amazon.com/Handbook-Data-Science-AI-Analytics/dp/1569908869)
    on Natural Language Processing (NLP)). It’s Google’s secret sauce. So today, I’ll
    instead show you a super simple way to do language detection yourself, using one
    highly underrated NLP tool and some really easy maths. You’ll be adding it to
    your GitHub portfolio in no time.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is Language Detection and why is it used?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Language detection just means identifying the language of a piece of input
    text. It’s a first step for many tasks in Natural Language Processing, including
    many that you use every day:'
  prefs: []
  type: TYPE_NORMAL
- en: Spelling and grammar correction (think MS Word, Google Docs, etc)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next word prediction (your phone does this all the time!)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine translation (e.g. in Google Translate’s ‘detect language’ option)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/e7bd41ad34bcc968094da7111a953d1a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: '**How can we detect a language?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A simple way to do language identification would be this: build vocabularies
    (word lists) for different languages, then count how many times each language’s
    words occur in a text. So if the test text contained five Japanese words and two
    English ones, we might conclude that it’s Japanese. We could even focus on so-called
    ‘stop words’, which are words which occur very frequently in languages and often
    deliver little meaning but are important for the grammar, such as ‘the’, ‘a’ and
    ‘and’.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem is, many words occur in multiple languages, even if they have different
    meanings. For example, ‘gift’ means ‘present’ in English, but ‘poison’ in German.
    So the phrase ‘Das Gift’ could present a problem, especially if there are typos*.
    Imagine we wanted to say ‘the poison is strong’: Das Gift ist stark. If we forgot
    the ‘-t’ in ‘ist’, we would have ‘Das Gift is stark’. Since ‘stark’ occurs in
    both languages, we now really have a problem. And focusing on stopwords could
    make it even worse. For example, French and German both frequently use ‘des’ and
    ‘du’, so if we only look at those, we’re going to come unstuck.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Fun fact: in Natural Language Processing, there are *always* typos.'
  prefs: []
  type: TYPE_NORMAL
- en: An alternative is to concentrate on the distribution of letters, instead of
    words. For example, compared to English, German uses umlauts (ä, ö, ü), and French
    uses lots of special characters (ç, â/ê/î/ô/û, à/è/ì/ò/ù, ë/ï/ü). Combinations
    of 2 and 3 letters, called bi- and trigrams, work even better. That’s because
    different languages have letter combinations that don’t occur in other languages.
  prefs: []
  type: TYPE_NORMAL
- en: '**Building a Language Detection Model in Python**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our language detection method will use uni-, bi- and tri-grams: that is, individual
    letters, and combinations of two and three letters. The generic term for such
    combinations is ‘n-grams’. We will create statistical models for different languages
    by counting their n-gram frequencies. Then we’ll compare these with the frequences
    of n-grams in a test text. The language whose n-gram frequencies best match the
    test sentence will be our winner.'
  prefs: []
  type: TYPE_NORMAL
- en: This approach is based on [1].
  prefs: []
  type: TYPE_NORMAL
- en: '**Visualise N-Grams**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start by visualising some n-grams of different lengths:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Build an N-Gram Extractor**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s define a function extract_xgrams(). It will take a text and a list of
    numbers, n_vals, and extract n-grams of those lengths from the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we lower case our test text. This will reduce the number of n-grams
    we get back, without losing much information about the language itself (think
    about it: If I say ‘i went to new york’, you still understand me, even without
    the capitalisation).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Define A Function for Building a Language Model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our build_model() function makes use of collections.Counter. The Counter takes
    a list, counts all occurrences of each item in the list, and returns a dictionary
    with each item and its frequency.
  prefs: []
  type: TYPE_NORMAL
- en: 'So for any language, we can model it by creating a dictionary of n-grams and
    their probability of occurring in that language. What’s the probability of an
    n-gram? It’s simply its frequency, divided by the total number of extracted n-grams.
    Let’s run the code and print the language model, sorted so that the most frequent
    n-gram comes first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Install NLTK and Download Our Text Data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Natural Language ToolKit is a hidden gem for Natural Language Processing.
    It contains classes and methods for text processing, and a large selection of
    text corpora (collections of prepared text data) for you to practice with. If
    you don’t have NLTK installed, you can install it using your preferred method,
    e.g. pip install nltk ([see the guide](https://www.nltk.org/install.html))
  prefs: []
  type: TYPE_NORMAL
- en: 'For testing our language identifier we will use the Universal Declaration of
    Human Rights (UDHR), which is [included in NLTK](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&cd=&cad=rja&uact=8&ved=2ahUKEwi5p7avpMf8AhWI3KQKHbn7DmcQFnoECAgQAw&url=https%3A%2F%2Fwww.nltk.org%2Fbook%2Fch02.html&usg=AOvVaw1czTHzjPxNOk1JLrT_zeOS)
    in 300 languages. In reality, such a dataset is too small and the text is too
    clean (the UN presumably proofread their work, and they probably don’t use hashtags
    and emojis. Those buzzkills). But this dataset is enough to demonstrate the concepts
    of what we’re trying to do here. Plus, it will introduce you to working with NLTK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For simplicity, I’ll choose just a handful of languages to work with. They
    all use similar characters, so it’ll be a tougher test for our detector. Feel
    free to add more languages: the code comments will show you how:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The command udhr.raw(fileids) returns the complete text of the specified fileid(s).
    We’ll use it to build a dictionary with each language name and its text, and from
    this dictionary we’ll build a model of each language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Determine the language for a given piece of text**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can now take a test text and compare its n-gram frequencies to those of our
    various language models. The aim will be to see which language has the closest
    frequencies to our test text.
  prefs: []
  type: TYPE_NORMAL
- en: 'We do this by calculating the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity),
    as per the formula below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be95b3e7d99c3eac83df4a11cfcd6645.png)'
  prefs: []
  type: TYPE_IMG
- en: The cosine similarity formula
  prefs: []
  type: TYPE_NORMAL
- en: 'It looks scary, but we won’t go into the math. Basically the cosine similarity
    is used to compare two numeric vectors. The result will be in the range of −1,
    meaning exactly opposite, to 1, meaning exactly the same. Our calculate_cosine()
    formula implements the math:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: It’s time to build an identify_language() function. This will take a test text,
    build a model for it using n grams of different sizes (specified by n_vals), and
    compare it to a dictionary of language models. The output will be the name of
    the language most-similar to the test text.
  prefs: []
  type: TYPE_NORMAL
- en: For demonstration purposes, I added a print statement in the function to show
    the similarity of each language to the test text. You can delete this after you’ve
    gotten a feeling for what cosine values look like.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running this function for our original test text, the highest similarity correctly
    occurs for English:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Before you delete that print line, looks what happens when we test the function
    with a radically different text; the similarity values generally decrease:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Test our language detector on different languages**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s see how we go with texts in Dutch, French and Spanish:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The results are correct, except that Italian is output for the second example,
    instead of French. Merde!
  prefs: []
  type: TYPE_NORMAL
- en: '**Improve the model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Clearly our models aren’t perfect, but there are lots of ways we could improve
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use bigger and more representative data:** I admitted earlier that our training
    texts are too short and clean to realistically reflect language identification
    in the wild. In fact, they are only a *sample* of the Declaration, with each text
    truncated to approximately 1000 characters. You can see this by exploring the
    number of words and characters in the text for each language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The training texts were good enough to introduce you to NLTK, and to this simple
    method of language detection, but in order to improve our generalisability, we
    need to build models using longer, more diverse, real-world text data: typos,
    hashtags, emojis and all.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bacf8c31a69c0a5ea9d2aa476900758c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'I asked ChatGPT for the UDHR in ‘Twitter speak’. Even this is fairly clean
    and clear; it can get a lot worse. Source: Author provided screenshot from a dialog
    ‘ChatGPT’, from OpenAI.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Why do we need longer, more diverse texts? Simple: it’s the only way to capture
    each language, and what makes it different from other languages.'
  prefs: []
  type: TYPE_NORMAL
- en: Take the word ‘gnome’, for example. Unless access to a garden gnome is considered
    a universal human right, the trigram ‘ gn’ (whitespace, g, n) probably doesn’t
    feature in our English sample data. You might think that’s ok, because there aren’t
    many English words beginning with ‘gn’. (There are a few, but they’re rarely used).
    The problem is, what if this *is* a common pattern in other languages? There are,
    in fact, [lots of common German words like this](https://www.wordmine.info/Search?slang=de&stype=words-that-starts-with&sword=GN),
    but not one of them occurs in the UDHR (I checked). So if we see a test text with
    the trigram ‘ gn’, it won’t contribute to the x-gram probabilities we’re summing
    up, for either language. And that means it won’t help us differentiate between
    them.
  prefs: []
  type: TYPE_NORMAL
- en: '**Add more features:** We could have added character x-grams of additional
    lengths, like quadgrams (four letters). Word-based x-grams might also help. The
    benefit in both cases is the same as with using longer texts: more features help
    capture differentiating factors between languages. For example, I’m unlikely to
    say ‘die Marmelade!’, even though I hate the stuff. But some Germans would say
    this every day at breakfast (‘die’ is just one version of ‘the’). So using word
    x-grams could capture this difference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some problems with word x-grams though. Most languages have many
    more words than they have characters in their alphabet, so just adding word bi-grams
    will explode the number of items in our language models. Tri-grams and larger
    will only make the matter worse. Bigger models will slow the entire process down,
    and for very little gain: the majority of these x-gram word combinations will
    barely ever occur, so they won’t even contribute much to helping differentiate
    between languages at test time.'
  prefs: []
  type: TYPE_NORMAL
- en: A better approach could be to use stopwords, as each language has their own
    set that occur very frequently and are thus good indicators. I said earlier that
    modelling languages using only stopwords is risky, as they can appear in multiple
    languages. But using them as additional features to our character x-grams, or
    using them as part of word x-grams, tackles this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we can add the top 1000 or 10,000 words in each language (or use
    them in word x-grams). The theory behind this is that words tend to follow ‘[Zip’s
    law](https://en.wikipedia.org/wiki/Zipf%27s_law)’, where the most common word
    tends to occur about twice as often as the next most common, three times as often
    as the third most common, and so on. So by taking just the top n words, you can
    capture probabilities for the majority of words in your input data and — crucially
    — your test data. And these probabilities are what our language detection decision
    will be based on.
  prefs: []
  type: TYPE_NORMAL
- en: '**Use machine learning:** You can’t talk about ‘features’ without thinking
    of machine learning. There are many algorithms we could try, including some surprisingly
    simple but effective options like Naive Bayes.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding those algorithms will take an entire new blog post, but the curious
    can read about Naive Bayes classifiers for language detection [here](https://dbs.cs.uni-duesseldorf.de/lehre/bmarbeit/barbeiten/ba_panich.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: '**Add more languages:** I used just a few, but there are thousands of languages
    in the world, and they all deserve love. So here’s a challenge to you: add more
    languages, and see how you go (I even gave you the code already).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Applying this concept to other NLP tasks**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concepts covered in this post can easily be applied to other challenges,
    and NLTK’s built in corpora can help you do it. So follow me for future posts,
    where we’ll cover document classification and speaker identification using — you
    guessed it — Python, NLTK, and those really simple statistics.
  prefs: []
  type: TYPE_NORMAL
- en: '**Thanks for reading!**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Firstly, cheers to the creators of [the tutorial](https://textmining.wp.hs-hannover.de/Sprachbestimmung.html)
    which inspired this piece.
  prefs: []
  type: TYPE_NORMAL
- en: If this article helped you — great !— please subscribe for more content on Natural
    Language Processing and other data science fundamentals. You can also connect
    with me on [Twitter](https://twitter.com/KatherineAMunro) (where I post loads
    of interesting content on AI, tech, ethics, and more).
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Cavnar, W.B., Trenkle, J.M.: [N-gram-based text categorization](https://dsacl3-2019.github.io/materials/CavnarTrenkle.pdf)
    (1994), Proceedings of SDAIR-94, 3rd annual symposium on document analysis and
    information retrieval.'
  prefs: []
  type: TYPE_NORMAL
