# 分层变换器 — 第2部分

> 原文：[https://towardsdatascience.com/hierarchical-transformers-part-2-2616eecacb21](https://towardsdatascience.com/hierarchical-transformers-part-2-2616eecacb21)

## 分层注意力更快

[](https://medium.com/@mina.ghashami?source=post_page-----2616eecacb21--------------------------------)[![Mina Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----2616eecacb21--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2616eecacb21--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2616eecacb21--------------------------------) [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----2616eecacb21--------------------------------)

·发表于 [数据科学前沿](https://towardsdatascience.com/?source=post_page-----2616eecacb21--------------------------------) ·阅读时间6分钟·2023年10月7日

--

这篇文章要求你具备标准变换器及其工作原理的知识。如果你是初学者，想了解变换器，请查看 [变换器入门](https://medium.com/p/4deaf9b199f9/edit) 文章。

在 [分层变换器 — 第1部分](https://medium.com/towards-data-science/hierarchical-transformers-54f6d59fa8fc) 中，我们定义了“分层变换器”的含义，并回顾了该领域的一项重要工作，即 *Hourglass*。

在这篇文章中，我们将继续探讨另一项著名的工作，即 *分层注意力变换器（HAT）。*

让我们开始吧。

# 分层注意力变换器（HAT）

该方法最初是为分类长文档而提出的，通常长达数千个单词。一个应用案例是分类法律文档或生物医学文档，这些文档通常非常长。

## 分词和分段

**HAT** 方法通过获取输入文档，并使用字节对编码（BPE）分词器将其拆分成子词/标记来工作。这个分词器被许多著名的大型语言模型使用，如 BERT、RoBERTA 和 GPT 家族。

然后将分词后的文档拆分为 *N* 个大小相等的块；即如果 *S* 代表输入文档，则 *S = [C1, …., CN]* 是 *N* 个大小相等的块。（在整篇文章中，我们有时将块称为段，但它们是相同的概念。）每个块是一个由 *k* 个标记组成的序列 *Ci = [Wi[cls], Wi1…, Wik-1]*，其中第一个标记 *Wi[cls]* 是 *CLS* 标记，代表该块。

![](../Images/e8e9433b29ed4e664313d30c3955b06e.png)

图片由作者提供

如上图所示，每个块是一个由 k 个标记组成的序列，其中第一个标记是 *CLS* 标记。

## **模型架构**

在对输入序列进行标记化和分段后，将其输入到**HAT**变换器模型中。HAT模型是一个编码器变换器，由两个主要组件组成：

1.  分段编码器（SWE）：这是一个共享的编码器块，接收一个段（也称为块）的序列并处理该块。

1.  交段编码器（CSE）：这是另一个编码器块，它处理所有段（也称为块）的CLS标记，处理交段关系。

![](../Images/cfab73a0c1cf70f783fc88686522180a.png)

图像来源于[[1](https://arxiv.org/abs/2210.05529)]

正如上图所示，左侧的分段编码器接收一个段的所有k个标记，处理它们并输出更新后的标记表示。在左侧，我们看到交段编码器接收所有段的CLS标记嵌入，并输出它们的更新表示。

这两个组件可以用于几种不同的布局。例如，我们可以将它们放在“临时”布局中，其中一个堆叠的L层SWE编码器放在底部，然后在其上方放置两层CSE编码器。请注意，每层的SWE共享权重。层与层之间的箭头表示在层之间传递嵌入。

![](../Images/73c498ae9fb022b908d5b55984497acf.png)

图像来源于[[1](https://arxiv.org/abs/2210.05529)]

另一种布局是“交错”层，如下所示：

![](../Images/7859404a727d2444761c021c46d2660a.png)

图像来源于[[1](https://arxiv.org/abs/2210.05529)]

如上所示，交错布局是一个配对的分段编码器和交段编码器的堆叠，其中跨越模型的几层执行交段注意力。

[1]中的作者探讨了几种布局（见下图），并通过实验发现“交错”布局优于其他变体。

![](../Images/b850a831f29ba46cf4607eb6a5b19d88.png)

图像来源于[[1](https://arxiv.org/abs/2210.05529)]

模型的完整架构如下：

![](../Images/3eb3d63c252fff650b8df34441cbf480.png)

图像来源于[[1](https://arxiv.org/abs/2210.05529)]

架构由N层组成，每层如上所示。请注意，将它们堆叠在一起会带来“交错”架构。一个层中的所有分段编码器共享权重，它们独立且并行地处理输入段（块）。每个段都有自己的位置嵌入。

*分段编码器*的输出将是分段标记的更新嵌入。第一个标记，即CLS标记嵌入，被添加到分段位置嵌入中，并传递给*交段编码器*。

交段编码器捕捉段之间的关系，并更新每个段的CLS嵌入并输出。该层的输出将是所有标记的更新嵌入。

**位置嵌入**。请注意，架构中有两个位置嵌入：

1.  段的标记位置嵌入：这是为了指示标记在段中的位置。这仅在*段级编码器*中使用。

1.  段的标记位置嵌入：这是为了指示段的顺序，仅在*跨段编码器*中使用。

## 模型训练

作者[1]将模型训练分为两个阶段：预训练和微调。

**预训练**：由于网络是一个编码器变换器，它使用掩码语言建模（MLM）目标进行训练，其中一部分（15%）的标记被掩码，语言模型应该预测这些标记。

**微调**：他们在几个标注数据集上使用文档分类任务来微调模型。

## 不同级别的嵌入

使用这个网络，我们可以在不同的尺度上获得嵌入：词、段落、文档。

+   词嵌入或标记嵌入可以通过模型的最后一层直接获取，

+   段落或段的嵌入可以通过段的CLS标记嵌入获得。

+   文档嵌入可以通过对所有段CLS标记嵌入进行最大池化（或平均池化）来获得。在论文[1]中，作者选择了最大池化。

# 评估

我真正喜欢这篇论文的地方在于，他们在三个层次上进行了全面的评估：

1)**上游评估任务**：这些任务旨在以通用方式预训练编码器。对于这个任务，他们采用了MLM（掩码语言建模）任务。

2) **中游评估任务**：这些任务旨在评估预训练模型学习到的表示的质量。为了这个评估，作者[1]考虑了几个任务，例如：

+   段顺序预测：这是为了预测几个段的顺序。模型在此任务中的输入是来自文档[1]的打乱的段序列，目标是预测它们之间的正确顺序。由于这是一个回归问题，他们使用平均绝对误差（MAE）作为损失函数。直观上，这个任务评估CLS标记嵌入的质量。

3) **下游评估任务**：他们在现实应用中评估模型的性能。为了这个评估，他们考虑了文档分类，用于对美国医院的出院总结进行分类。为了获得文档嵌入，他们对文档中所有段的CLS标记嵌入进行了最大池化，并使用交叉熵作为分类的损失函数。

更多细节请参考论文 [1]。

# 实验结果

论文中有许多实验结果，但一个显著的结果是，他们将自己的模型与 Longformer [2] 和 BigBird [3] 模型进行比较。Longformer 和 BigBird 属于稀疏注意力方法，它们通过强制每个标记仅关注其邻域中的少数标记和少量全局标记来实现高效的注意力机制。这与标准注意力方法相对，后者每个标记都关注每个其他标记。在这个实验中，他们展示了 HAT 方法在性能上超越了 Longformer 和 BigBird。

![](../Images/122a9ae9347af5715b0d6dd2ab0cbc05.png)

图片来源 [[1](https://arxiv.org/abs/2210.05529)]

# 摘要

在这篇文章中，我们探讨了另一种分层的变换器架构，称为分层注意力变换器（HAT）。这是一种基于编码器的模型，将输入分割成相等长度的片段。每个片段以一个表示该片段的CLS标记开始。该模型架构包括两个主要组件：片段级编码器和跨片段编码器。第一个编码器学习单个片段的表示，而第二个编码器学习片段之间的跨关系。它们一起能够学习输入中各种层次的表示，如词表示、句子表示和文档表示。

如果你有任何问题或建议，随时与我联系：

邮箱：mina.ghashami@gmail.com

LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)

# 参考文献

1.  [对分层注意力变换器在长文档分类中的有效性的探索](https://arxiv.org/abs/2210.05529)

1.  [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)

1.  [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)
