- en: Fine-tune a Large Language Model with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/fine-tune-a-large-language-model-with-python-b1c09dbc58b2](https://towardsdatascience.com/fine-tune-a-large-language-model-with-python-b1c09dbc58b2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/1d8727d8fba25a5fd2ed429d267bf0c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Manouchehr Hejazi](https://unsplash.com/@patrol?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to fine-tune a BERT from scratch on a custom dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)
    ¬∑4 min read¬∑Apr 18, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will deal with the **fine-tuning of BERT for sentiment classification**
    using PyTorch. BERT is a large language model that offers a good balance between
    popularity and model size, which can be fine-tuned **using a simple GPU**. We
    can download a **pre-trained BERT from Hugging Face (HF)**, so there is no need
    to train it from scratch. In particular, we will use the distilled (smaller) version
    of BERT, called **Distil-BERT.**
  prefs: []
  type: TYPE_NORMAL
- en: '[Distil-BERT](https://huggingface.co/docs/transformers/model_doc/distilbert#:~:text=DistilBERT%20is%20a%20small%2C%20fast,the%20GLUE%20language%20understanding%20benchmark.)
    is widely used in production since it has **40% fewer parameters** than BERT uncased.
    It runs **60% faster and retains 95% performance** in the [GLUE](https://arxiv.org/abs/1804.07461)
    language comprehension benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: We start by installing all the necessary libraries. The first line is to capture
    the output of the installation and keep your notebook clean.
  prefs: []
  type: TYPE_NORMAL
- en: I will use Deepnote to run the code in this article but you also use Google
    Colab if you prefer.
  prefs: []
  type: TYPE_NORMAL
- en: You can also check the version of the libraries you are using with the following
    line of code.
  prefs: []
  type: TYPE_NORMAL
- en: Now you need to specify some general setups, including the number of epochs
    and device hardware. We set a fixed random seed which helps for the reproducibility
    of the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the IMDb movie review dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs see how to prepare and tokenize the IMDb movie review dataset and fine-tune
    Distilled BERT. Fetch the compressed data and unzip it.
  prefs: []
  type: TYPE_NORMAL
- en: As usual, we need to split the data into training, validation, and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenize the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let‚Äôs **tokenize the texts into individual word tokens** using the tokenizer
    implementation inherited from the pre-trained model class.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization is used in natural language processing to split paragraphs and
    sentences into smaller units that can be more easily assigned meaning.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**With Hugging Face you will always find a tokenizer associated with each model**.
    If you are not doing research or experiments on tokenizers it‚Äôs always preferable
    to use the standard tokenizers.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Padding** is a strategy for ensuring tensors are rectangular by adding a
    special padding token to shorter sentences. On the other, sometimes a sequence
    may be too long for a model to handle. In this case, you‚Äôll need to truncate the
    sequence to a shorter length.'
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs pack everything into a Python class that we are going to name IMDbDataset.
    We are also going to use this custom dataset to create the corresponding dataloaders.
  prefs: []
  type: TYPE_NORMAL
- en: 'The encodings variable stores a lot of information about the tokenized text.
    We can extract only the most relevant information via dictionary comprehension.
    The dictionary contains:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids**: are the indices corresponding to each token in the sentence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels**: classes labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_mask**: indicates whether a token should be attended to or not
    (for example padding tokens will not be attended).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs construct datasets and corresponding dataloaders.
  prefs: []
  type: TYPE_NORMAL
- en: Loading and fine-tuning BERT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we are done with the data preprocessing, and we can start fine-tuning
    our model. Let‚Äôs define a model and an optimization algorithm, Adam in this case.
  prefs: []
  type: TYPE_NORMAL
- en: '*DistilbertForSequenceCLassification* specifies the downstream task we want
    to fine-tune the model on, which is sequence classification in this case. Note
    that ‚Äúuncased‚Äù means that the model does not distinguish between upper and lower
    case letters.'
  prefs: []
  type: TYPE_NORMAL
- en: Before training the model, we need to define some metrics to compare the model
    improvements. In this simple case, we can use conventional accuracy for classification.
    Notice that this function is quite long because we are loading the dataset batch
    by batch to work around ram and GPU limitations. Usually, these resources are
    never enough when fine-tuning huge datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In the compute_accuracy function, we load a given batch and then take the predicted
    labels from the outputs. While doing this, we keep track of the total number of
    examples via the variable num_examples. In the same way, we keep track of the
    number of correct predictions via the correct_pred variable. After we have iterated
    over the complete dataloader, we can compute the accuracy by the last division.
  prefs: []
  type: TYPE_NORMAL
- en: You can also notice how to use the model in the compute_accuracy function. We
    feed the model with input_ids along with the attention_mask information that denotes
    whether a token is an actual text token or padding. The model returns a SequenceClassificatierOutput
    object from which we get the logits and convert them into a class using the argmax
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Training (fine-tuning) loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you know how to code a training loop in PyTorch you won't have any issues
    understanding this fine-tuning loop. As in any neural network, we give inputs
    to the network, calculate the output, compute the loss, and do parameter updates
    based on this loss.
  prefs: []
  type: TYPE_NORMAL
- en: Every few epochs we print the training progress to get feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we have seen how to perform fine-tuning of a Large Language
    Model such as BERT by using PyTorch exclusively. Actually, there is a much faster
    and even smarter way to do this using the Transformers library from Hugging Face.
    This library allows us to create a Trainer object for fine-tuning where we can
    specify parameters such as the number of epochs and more in just a few lines of
    code. Follow me if you are curious to see how to do it in the next article! [üòâ](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
  prefs: []
  type: TYPE_NORMAL
- en: The End
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Marcello Politi*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Linkedin](https://www.linkedin.com/in/marcello-politi/), [Twitter](https://twitter.com/_March08_),
    [Website](https://marcello-politi.super.site/)'
  prefs: []
  type: TYPE_NORMAL
