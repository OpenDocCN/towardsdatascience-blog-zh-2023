- en: Linguistic Fingerprinting with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/linguistic-fingerprinting-with-python-5b128ae7a9fc](https://towardsdatascience.com/linguistic-fingerprinting-with-python-5b128ae7a9fc)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Attributing authorship with punctuation heatmaps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@lee_vaughan?source=post_page-----5b128ae7a9fc--------------------------------)[![Lee
    Vaughan](../Images/9f6b90bb76102f438ab0b9a4a62ffa3f.png)](https://medium.com/@lee_vaughan?source=post_page-----5b128ae7a9fc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5b128ae7a9fc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5b128ae7a9fc--------------------------------)
    [Lee Vaughan](https://medium.com/@lee_vaughan?source=post_page-----5b128ae7a9fc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5b128ae7a9fc--------------------------------)
    ·9 min read·Aug 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/84b60ea1de5e65b1d5632ec1303169e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Single forensic fingerprint in yellow tones with blue semicolons (image by DALL-E2
    and author)
  prefs: []
  type: TYPE_NORMAL
- en: '*Stylometry* is the quantitative study of literary style through computational
    text analysis. It’s based on the idea that we all have a unique, consistent, and
    recognizable style in our writing. This includes our vocabulary, our use of punctuation,
    the average length of our words and sentences, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: A typical application of stylometry is *authorship attribution*. This is the
    process of identifying the author of a document, such as when investigating plagiarism
    or resolving disputes on the origin of a historical document.
  prefs: []
  type: TYPE_NORMAL
- en: In this *Quick Success Data Science* project, we’ll use Python, seaborn, and
    the Natural Language Toolkit (NLTK) to see if Sir Arthur Conan Doyle left behind
    a linguistic fingerprint in his novel, *The Lost World*. More specifically, we’ll
    use *semicolons* to determine whether Sir Arthur or his contemporary, H.G. Wells,
    is the likely author of the book.
  prefs: []
  type: TYPE_NORMAL
- en: The Hound, The War, and The Lost World
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sir Arthur Conan Doyle (1859–1930) is best known for the Sherlock Holmes stories.
    H. G. Wells (1866–1946) is famous for several groundbreaking science fiction novels,
    such as *The Invisible Man*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In 1912, Strand Magazine published *The Lost World*, a serialized version of
    a science fiction novel. Although its author is known, let’s pretend it’s in dispute
    and it’s our job to solve the mystery. Experts have narrowed the field down to
    two authors: Doyle and Wells. Wells is slightly favored because *The Lost World*
    is a work of science fiction and includes troglodytes similar to the *Morlocks*
    in his 1895 book, *The Time Machine*.'
  prefs: []
  type: TYPE_NORMAL
- en: To solve this problem, we’ll need representative works for each author. For
    Doyle, we’ll use *The Hound of the Baskervilles*, published in 1901\. For Wells,
    we’ll use *The War of the Worlds*, published in 1898.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately for us, all three novels are in the public domain and available
    through [*Project Gutenberg*](https://www.gutenberg.org/). For convenience, I’ve
    downloaded them to this [Gist](https://gist.github.com/rlvaugh/b5e6033024620e261b9829261b27b6e1)
    and stripped out the licensing information.
  prefs: []
  type: TYPE_NORMAL
- en: The Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Authorship attribution requires the application of *Natural Language Processing
    (NLP).* NLP is a branch of linguistics and artificial intelligence concerned with
    giving computers the ability to derive meaning from written and spoken words.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common NLP tests for authorship analyze the following features of
    a text:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Word/Sentence length:** A frequency distribution plot of the length of words
    or sentences in a document.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stop words:** A frequency distribution plot of stop words (short, noncontextual
    function words like *the*, *but*, and *if).*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parts of speech:** A frequency distribution plot of words based on their
    syntactic functions (such as nouns and verbs).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Most common words:** A comparison of the most commonly used words in a text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jaccard similarity:** A statistic used for gauging the similarity and diversity
    of a sample set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Punctuation:** A comparison of the use of commas, colons, semicolons, and
    so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For this toy project, we’ll focus on the *punctuation* test. Specifically, we’ll
    look at the use of *semicolons*. As “optional” grammatical elements, their use
    is potentially distinctive. They’re also agnostic to the type of book, unlike
    question marks, which may be more abundant in detective novels than in classic
    science fiction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The high-level process will be to:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the books as text strings,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use NLTK to tokenize (break out) each word and punctuation mark,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the punctuation marks,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign semicolons a value of 1 and all other marks a value of 0,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create 2D NumPy arrays of the numerical values,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use seaborn to plot the results as a [heat map](https://en.wikipedia.org/wiki/Heat_map),
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We’ll use each heat map as a digital “fingerprint.” Comparing the fingerprints
    for the *known* books with the *unknown* book (*The Lost World*) will hopefully
    suggest one author over the other.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/65382f1d0ad75a3cf108a4aee27703f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Heatmaps for question mark use (blue) in a detective novel (left) vs. a sci-fi
    novel (right) (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: The Natural Language Toolkit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multiple third-party libraries can help you perform NLP with Python. These include
    NLTK, spaCy, Gensim, Pattern, and TextBlob.
  prefs: []
  type: TYPE_NORMAL
- en: We’re going to use [NLTK](https://www.nltk.org/), which is one of the oldest,
    most powerful, and most popular. It’s open source and works on Windows, macOS,
    and Linux. It also comes with highly detailed [documentation](https://www.nltk.org/book/).
  prefs: []
  type: TYPE_NORMAL
- en: Installing Libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To install NLTK with Anaconda use:'
  prefs: []
  type: TYPE_NORMAL
- en: '`conda install -c anaconda nltk`'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install with pip use:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pip install nltk` (or see [https://www.nltk.org/install.html](https://www.nltk.org/install.html))'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll also need seaborn for plotting. Built on matplotlib, this open-source
    visualization library provides an easier-to-use interface for drawing attractive
    and informative statistical graphs such as bar charts, scatterplots, heat maps,
    and so on. Here are the installation commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '`conda install seaborn`'
  prefs: []
  type: TYPE_NORMAL
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: '`pip install seaborn`'
  prefs: []
  type: TYPE_NORMAL
- en: The Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following code was written in JupyterLab and is described *by cell*.
  prefs: []
  type: TYPE_NORMAL
- en: Importing Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Among the imports, we’ll need the list of punctuation marks from the `string`
    module. We'll turn this list into a `set` datatype for faster searches.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Defining a Function to Load Text Files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since we’ll be working with multiple files, we’ll start by defining some reusable
    functions. The first one uses Python’s `urllib` library to open a file stored
    at a URL. The first line opens the file, the second reads it as bytes data, and
    the third converts the bytes to string (`str`) format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Defining a Function to Tokenize Text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next function accepts a dictionary where the keys are the authors’ names,
    and the values are their novels in string format. It then uses NLTK’s `word_tokenize()`
    method to reorganize the strings into *tokens*.
  prefs: []
  type: TYPE_NORMAL
- en: Python sees strings as a collection of *characters*, like letters, spaces, and
    punctuation. The process of tokenization groups these characters into other elements
    like words (or sentences), and punctuation. Each element is referred to as a *token,*
    and these tokens permit the use of sophisticated analyses with NLP.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The function ends by returning a new dictionary with the authors’ names as keys
    and the punctuation tokens as values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: I should note here that it’s possible to use basic Python to search the text
    file for semicolons and complete this project. Tokenization, however, brings two
    things to the table.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, NLTK’s default tokenizer (`word_tokenize()`) does not count apostrophes
    used in contractions or possessives, but rather, treats them as *two words* with
    the apostrophe attached to the second word (such as *I* + *‘ve*). This is in accordance
    with [grammatical usage](https://www.niu.edu/writingtutorial/punctuation/apostrophe.shtml).
  prefs: []
  type: TYPE_NORMAL
- en: The tokenizer also treats special marks, such as a double hyphen ( — — ) or
    ellipses (…) as a *single* mark, as the author intended, rather than as separate
    marks. With basic Python, each duplicate mark is counted.
  prefs: []
  type: TYPE_NORMAL
- en: The impact of this can be significant. For example, the total punctuation count
    for *The Lost World* using NLTK is 10,035\. With basic Python, it’s 14,352!
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, all of the other NLP attribution tests require the use of tokenization,
    so using NLTK here gives you the ability to expand the analysis later.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a Function to Convert Punctuation Marks to Numbers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Seaborn’s `heatmap()` method requires *numerical* data as input. The following
    function accepts a dictionary of punctuation tokens by author, assigns semicolons
    a value of `1` and all else a value of `0`, and returns a `list` datatype.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Defining a Function to Find the Next Lowest Square Value
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The list of heat values is in *1D*, but we want to plot it in *2D*. To accomplish
    this, we’ll convert the list into a square NumPy array.
  prefs: []
  type: TYPE_NORMAL
- en: This will have to be a *true* square, and it’s unlikely that the number of samples
    in each list will have an integer as its square root. So, we have to take the
    square root of the length of each list, convert it to an integer, and square it.
    The resulting value will be the *maximum length* of the list that can be displayed
    as a true square.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Loading and Preparing the Data for Plotting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the functions defined, we’re ready to apply them. First, we load the text
    data for each book into a dictionary named `strings_by_author`. For *The Lost
    World,* we'll enter “unknown” for the author’s name, as this represents a document
    of unknown origin.
  prefs: []
  type: TYPE_NORMAL
- en: We then turn this into a punctuation dictionary named `punct_by_author`. From
    this dictionary, we find the next lowest squares for each list and choose the
    *minimum* value going forward. This ensures that *all* the datasets can be converted
    into a square array and also normalizes the number of samples per dataset (*The
    Lost World*, for example, has 10,035 punctuation tokens compared to only 6,704
    for *The Hound of the Baskervilles*).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1c513622c91d423e3d0f8a2b76e9813d.png)'
  prefs: []
  type: TYPE_IMG
- en: Using an array with 6,561 tokens going forward means that each punctuation list
    will be truncated to a length of 6,561 before plotting.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting the Heat Maps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll use a `for` loop to plot a heat map for each author's punctuation. The
    first step is to convert the punctuation tokens to numbers. Remember, semicolons
    will be represented by `1` and everything else by `0`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we use our `perfect_square` value and NumPy’s `array()` and `reshape()`methods
    to both truncate the `heat` list and turn it into a square NumPy array. At this
    point, we only need to set up a matplotlib figure and call seaborn's `heatmap()`
    method. The loop will plot a separate figure for each author.
  prefs: []
  type: TYPE_NORMAL
- en: Semicolons will be colored *blue*. You can reverse this by changing the order
    of the colors in the `cmap` argument. You can also enter new colors if you don’t
    like yellow and blue.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a40c96d35c1f9fa54ce0645137d1485f.png)![](../Images/e01f8c9596cafbbf962bac2f7e975ba2.png)![](../Images/b1148106f8f5f9545e5c1e5c87e6acbf.png)'
  prefs: []
  type: TYPE_IMG
- en: Outcome
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It should be clear from the previous plots that — based strictly on semicolon
    usage — Doyle is the most likely author of *The Lost World*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having a visual display of these results is important. For example, the total
    semicolon counts for each book look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Wells: 243`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Doyle: 45`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Unknown: 103`'
  prefs: []
  type: TYPE_NORMAL
- en: 'And as a fraction of the total punctuation, they look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Wells: 0.032`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Doyle: 0.007`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Unknown: 0.010`'
  prefs: []
  type: TYPE_NORMAL
- en: From this textual data, is it immediately clear to you that the Unknown author
    is most likely Doyle? Imagine presenting this to a jury. Would the jurors be swayed
    more by the plots or the text? You can’t beat visualizations for communicating
    information!
  prefs: []
  type: TYPE_NORMAL
- en: Of course, for a *robust* determination of authorship, you would want to run
    *all* the NLP attribution tests listed previously, and you would want to use *all*
    of Doyle’s and Well’s novels.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Thanks for reading and please follow me for more *Quick Success Data Science*
    projects in the future. And if you want to see a more complete application of
    NLP tests on this dataset, see Chapter 2 of my book, [*Real Word Python: A Hacker’s
    Guide to Solving Problems with Code*](https://a.co/d/gzXvvoB).'
  prefs: []
  type: TYPE_NORMAL
