- en: Linguistic Fingerprinting with Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/linguistic-fingerprinting-with-python-5b128ae7a9fc](https://towardsdatascience.com/linguistic-fingerprinting-with-python-5b128ae7a9fc)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Attributing authorship with punctuation heatmaps
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@lee_vaughan?source=post_page-----5b128ae7a9fc--------------------------------)[![Lee
    Vaughan](../Images/9f6b90bb76102f438ab0b9a4a62ffa3f.png)](https://medium.com/@lee_vaughan?source=post_page-----5b128ae7a9fc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5b128ae7a9fc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5b128ae7a9fc--------------------------------)
    [Lee Vaughan](https://medium.com/@lee_vaughan?source=post_page-----5b128ae7a9fc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5b128ae7a9fc--------------------------------)
    ·9 min read·Aug 1, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/84b60ea1de5e65b1d5632ec1303169e2.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Single forensic fingerprint in yellow tones with blue semicolons (image by DALL-E2
    and author)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '*Stylometry* is the quantitative study of literary style through computational
    text analysis. It’s based on the idea that we all have a unique, consistent, and
    recognizable style in our writing. This includes our vocabulary, our use of punctuation,
    the average length of our words and sentences, and so on.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: A typical application of stylometry is *authorship attribution*. This is the
    process of identifying the author of a document, such as when investigating plagiarism
    or resolving disputes on the origin of a historical document.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: In this *Quick Success Data Science* project, we’ll use Python, seaborn, and
    the Natural Language Toolkit (NLTK) to see if Sir Arthur Conan Doyle left behind
    a linguistic fingerprint in his novel, *The Lost World*. More specifically, we’ll
    use *semicolons* to determine whether Sir Arthur or his contemporary, H.G. Wells,
    is the likely author of the book.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: The Hound, The War, and The Lost World
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sir Arthur Conan Doyle (1859–1930) is best known for the Sherlock Holmes stories.
    H. G. Wells (1866–1946) is famous for several groundbreaking science fiction novels,
    such as *The Invisible Man*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'In 1912, Strand Magazine published *The Lost World*, a serialized version of
    a science fiction novel. Although its author is known, let’s pretend it’s in dispute
    and it’s our job to solve the mystery. Experts have narrowed the field down to
    two authors: Doyle and Wells. Wells is slightly favored because *The Lost World*
    is a work of science fiction and includes troglodytes similar to the *Morlocks*
    in his 1895 book, *The Time Machine*.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: To solve this problem, we’ll need representative works for each author. For
    Doyle, we’ll use *The Hound of the Baskervilles*, published in 1901\. For Wells,
    we’ll use *The War of the Worlds*, published in 1898.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately for us, all three novels are in the public domain and available
    through [*Project Gutenberg*](https://www.gutenberg.org/). For convenience, I’ve
    downloaded them to this [Gist](https://gist.github.com/rlvaugh/b5e6033024620e261b9829261b27b6e1)
    and stripped out the licensing information.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The Process
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Authorship attribution requires the application of *Natural Language Processing
    (NLP).* NLP is a branch of linguistics and artificial intelligence concerned with
    giving computers the ability to derive meaning from written and spoken words.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common NLP tests for authorship analyze the following features of
    a text:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '**Word/Sentence length:** A frequency distribution plot of the length of words
    or sentences in a document.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stop words:** A frequency distribution plot of stop words (short, noncontextual
    function words like *the*, *but*, and *if).*'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parts of speech:** A frequency distribution plot of words based on their
    syntactic functions (such as nouns and verbs).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Most common words:** A comparison of the most commonly used words in a text.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jaccard similarity:** A statistic used for gauging the similarity and diversity
    of a sample set.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Punctuation:** A comparison of the use of commas, colons, semicolons, and
    so on.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For this toy project, we’ll focus on the *punctuation* test. Specifically, we’ll
    look at the use of *semicolons*. As “optional” grammatical elements, their use
    is potentially distinctive. They’re also agnostic to the type of book, unlike
    question marks, which may be more abundant in detective novels than in classic
    science fiction.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'The high-level process will be to:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Load the books as text strings,
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use NLTK to tokenize (break out) each word and punctuation mark,
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the punctuation marks,
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign semicolons a value of 1 and all other marks a value of 0,
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create 2D NumPy arrays of the numerical values,
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use seaborn to plot the results as a [heat map](https://en.wikipedia.org/wiki/Heat_map),
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We’ll use each heat map as a digital “fingerprint.” Comparing the fingerprints
    for the *known* books with the *unknown* book (*The Lost World*) will hopefully
    suggest one author over the other.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/65382f1d0ad75a3cf108a4aee27703f2.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: Heatmaps for question mark use (blue) in a detective novel (left) vs. a sci-fi
    novel (right) (image by the author)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The Natural Language Toolkit
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multiple third-party libraries can help you perform NLP with Python. These include
    NLTK, spaCy, Gensim, Pattern, and TextBlob.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: We’re going to use [NLTK](https://www.nltk.org/), which is one of the oldest,
    most powerful, and most popular. It’s open source and works on Windows, macOS,
    and Linux. It also comes with highly detailed [documentation](https://www.nltk.org/book/).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Installing Libraries
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To install NLTK with Anaconda use:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '`conda install -c anaconda nltk`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'To install with pip use:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '`pip install nltk` (or see [https://www.nltk.org/install.html](https://www.nltk.org/install.html))'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll also need seaborn for plotting. Built on matplotlib, this open-source
    visualization library provides an easier-to-use interface for drawing attractive
    and informative statistical graphs such as bar charts, scatterplots, heat maps,
    and so on. Here are the installation commands:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要seaborn来进行绘图。基于matplotlib，这个开源可视化库提供了一个更易于使用的界面来绘制吸引人且信息丰富的统计图表，如条形图、散点图、热图等。以下是安装命令：
- en: '`conda install seaborn`'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`conda install seaborn`'
- en: or
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '`pip install seaborn`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`pip install seaborn`'
- en: The Code
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码
- en: The following code was written in JupyterLab and is described *by cell*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码在JupyterLab中编写，并且*由单元格*描述。
- en: Importing Libraries
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入库
- en: Among the imports, we’ll need the list of punctuation marks from the `string`
    module. We'll turn this list into a `set` datatype for faster searches.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入模块中，我们需要`string`模块中的标点符号列表。我们将把这个列表转化为`set`数据类型，以便更快地进行搜索。
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Defining a Function to Load Text Files
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义一个加载文本文件的函数
- en: Since we’ll be working with multiple files, we’ll start by defining some reusable
    functions. The first one uses Python’s `urllib` library to open a file stored
    at a URL. The first line opens the file, the second reads it as bytes data, and
    the third converts the bytes to string (`str`) format.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将处理多个文件，我们将首先定义一些可重用的函数。第一个函数使用Python的`urllib`库打开存储在URL处的文件。第一行打开文件，第二行将其读取为字节数据，第三行将字节转换为字符串（`str`）格式。
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Defining a Function to Tokenize Text
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义一个分词函数
- en: The next function accepts a dictionary where the keys are the authors’ names,
    and the values are their novels in string format. It then uses NLTK’s `word_tokenize()`
    method to reorganize the strings into *tokens*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个函数接受一个字典，其中键是作者的名字，值是他们以字符串格式呈现的小说。然后，它使用NLTK的`word_tokenize()`方法将字符串重新组织为*令牌*。
- en: Python sees strings as a collection of *characters*, like letters, spaces, and
    punctuation. The process of tokenization groups these characters into other elements
    like words (or sentences), and punctuation. Each element is referred to as a *token,*
    and these tokens permit the use of sophisticated analyses with NLP.
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Python将字符串视为*字符*的集合，如字母、空格和标点符号。分词过程将这些字符分组为其他元素，如单词（或句子）和标点符号。每个元素称为*令牌*，这些令牌允许使用复杂的NLP分析。
- en: The function ends by returning a new dictionary with the authors’ names as keys
    and the punctuation tokens as values.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数通过返回一个新的字典来结束，其中作者的名字作为键，标点符号令牌作为值。
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: I should note here that it’s possible to use basic Python to search the text
    file for semicolons and complete this project. Tokenization, however, brings two
    things to the table.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该在这里指出，可以使用基本的Python来搜索文本文件中的分号并完成这个项目。然而，分词带来了两个好处。
- en: Firstly, NLTK’s default tokenizer (`word_tokenize()`) does not count apostrophes
    used in contractions or possessives, but rather, treats them as *two words* with
    the apostrophe attached to the second word (such as *I* + *‘ve*). This is in accordance
    with [grammatical usage](https://www.niu.edu/writingtutorial/punctuation/apostrophe.shtml).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，NLTK的默认分词器（`word_tokenize()`）不计算用在缩写或所有格中的撇号，而是将其视为*两个单词*，撇号附在第二个单词上（如*I*
    + *‘ve*）。这符合[语法用法](https://www.niu.edu/writingtutorial/punctuation/apostrophe.shtml)。
- en: The tokenizer also treats special marks, such as a double hyphen ( — — ) or
    ellipses (…) as a *single* mark, as the author intended, rather than as separate
    marks. With basic Python, each duplicate mark is counted.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器还将特殊标记（如双破折号（— —）或省略号（…））视为*单一*标记，如作者所意图，而不是作为单独的标记。使用基本Python时，每个重复的标记都会被计数。
- en: The impact of this can be significant. For example, the total punctuation count
    for *The Lost World* using NLTK is 10,035\. With basic Python, it’s 14,352!
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会产生显著影响。例如，使用NLTK计算*失落的世界*的总标点符号数量为10,035，而使用基本Python则为14,352！
- en: Secondly, all of the other NLP attribution tests require the use of tokenization,
    so using NLTK here gives you the ability to expand the analysis later.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，所有其他NLP归因测试都需要使用分词，因此在这里使用NLTK让你有能力在以后扩展分析。
- en: Defining a Function to Convert Punctuation Marks to Numbers
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义一个将标点符号转换为数字的函数
- en: Seaborn’s `heatmap()` method requires *numerical* data as input. The following
    function accepts a dictionary of punctuation tokens by author, assigns semicolons
    a value of `1` and all else a value of `0`, and returns a `list` datatype.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Seaborn的`heatmap()`方法需要*数值*数据作为输入。以下函数接受按作者分类的标点符号令牌字典，将分号赋值为`1`，其他所有赋值为`0`，并返回`list`数据类型。
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Defining a Function to Find the Next Lowest Square Value
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The list of heat values is in *1D*, but we want to plot it in *2D*. To accomplish
    this, we’ll convert the list into a square NumPy array.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: This will have to be a *true* square, and it’s unlikely that the number of samples
    in each list will have an integer as its square root. So, we have to take the
    square root of the length of each list, convert it to an integer, and square it.
    The resulting value will be the *maximum length* of the list that can be displayed
    as a true square.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Loading and Preparing the Data for Plotting
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the functions defined, we’re ready to apply them. First, we load the text
    data for each book into a dictionary named `strings_by_author`. For *The Lost
    World,* we'll enter “unknown” for the author’s name, as this represents a document
    of unknown origin.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: We then turn this into a punctuation dictionary named `punct_by_author`. From
    this dictionary, we find the next lowest squares for each list and choose the
    *minimum* value going forward. This ensures that *all* the datasets can be converted
    into a square array and also normalizes the number of samples per dataset (*The
    Lost World*, for example, has 10,035 punctuation tokens compared to only 6,704
    for *The Hound of the Baskervilles*).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/1c513622c91d423e3d0f8a2b76e9813d.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: Using an array with 6,561 tokens going forward means that each punctuation list
    will be truncated to a length of 6,561 before plotting.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Plotting the Heat Maps
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll use a `for` loop to plot a heat map for each author's punctuation. The
    first step is to convert the punctuation tokens to numbers. Remember, semicolons
    will be represented by `1` and everything else by `0`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Next, we use our `perfect_square` value and NumPy’s `array()` and `reshape()`methods
    to both truncate the `heat` list and turn it into a square NumPy array. At this
    point, we only need to set up a matplotlib figure and call seaborn's `heatmap()`
    method. The loop will plot a separate figure for each author.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Semicolons will be colored *blue*. You can reverse this by changing the order
    of the colors in the `cmap` argument. You can also enter new colors if you don’t
    like yellow and blue.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/a40c96d35c1f9fa54ce0645137d1485f.png)![](../Images/e01f8c9596cafbbf962bac2f7e975ba2.png)![](../Images/b1148106f8f5f9545e5c1e5c87e6acbf.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: Outcome
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It should be clear from the previous plots that — based strictly on semicolon
    usage — Doyle is the most likely author of *The Lost World*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'Having a visual display of these results is important. For example, the total
    semicolon counts for each book look like this:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '`Wells: 243`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '`Doyle: 45`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '`Unknown: 103`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'And as a fraction of the total punctuation, they look like this:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '`Wells: 0.032`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '`Doyle: 0.007`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '`Unknown: 0.010`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: From this textual data, is it immediately clear to you that the Unknown author
    is most likely Doyle? Imagine presenting this to a jury. Would the jurors be swayed
    more by the plots or the text? You can’t beat visualizations for communicating
    information!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些文本数据来看，你是否立即能判断出“未知”作者很可能是**道尔**？试想一下如果把这些呈现给陪审团，陪审员会更被情节还是文本打动？没有什么能比可视化更有效地传达信息了！
- en: Of course, for a *robust* determination of authorship, you would want to run
    *all* the NLP attribution tests listed previously, and you would want to use *all*
    of Doyle’s and Well’s novels.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，要对作者身份进行*稳健*的判断，你需要运行之前列出的*所有*NLP归属测试，并使用*所有*的道尔和威尔斯的小说。
- en: Thanks
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谢谢
- en: 'Thanks for reading and please follow me for more *Quick Success Data Science*
    projects in the future. And if you want to see a more complete application of
    NLP tests on this dataset, see Chapter 2 of my book, [*Real Word Python: A Hacker’s
    Guide to Solving Problems with Code*](https://a.co/d/gzXvvoB).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读，未来请关注我的更多*快速成功数据科学*项目。如果你想看到对该数据集进行更全面的NLP测试，请参阅我书中的第二章，[*真实世界的Python：黑客解决代码问题的指南*](https://a.co/d/gzXvvoB)。
