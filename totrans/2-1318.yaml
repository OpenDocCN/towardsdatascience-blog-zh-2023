- en: Implementing the Steepest Descent Algorithm in Python from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/implementing-the-steepest-descent-algorithm-in-python-from-scratch-d32da2906fe2](https://towardsdatascience.com/implementing-the-steepest-descent-algorithm-in-python-from-scratch-d32da2906fe2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://nicolo-albanese.medium.com/?source=post_page-----d32da2906fe2--------------------------------)[![Nicolo
    Cosimo Albanese](../Images/9a2c26207146741b58c3742927d09450.png)](https://nicolo-albanese.medium.com/?source=post_page-----d32da2906fe2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d32da2906fe2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d32da2906fe2--------------------------------)
    [Nicolo Cosimo Albanese](https://nicolo-albanese.medium.com/?source=post_page-----d32da2906fe2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d32da2906fe2--------------------------------)
    ·11 min read·Feb 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0fbf614bb4efdb8d5bcf885b4aa8bb4b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Table of contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Introduction](#8cf1)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[The steepest descent algorithm](#22bb)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2.1 [The search direction](#2e50)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.2 [The step size](#e299)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.3 [The algorithm](#b179)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Implementation](#43e9)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3.1 [Constant step size](#e046)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2 [Line search with the Armijo condition](#db67)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Conclusions](#6c34)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Optimization is the process of finding the set of variables `x` that minimize
    or maximize an objective function `f(x)`. Since maximizing a function is equivalent
    to minimizing its negative, we may focus on minimization problems alone:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b67697b8b88e26d3229b118e3a5329a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For our example, let us define a quadratic, multivariable objective function
    `f(x)` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5c859c0e02132db35f45dc85c9173f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Its gradient `∇f(x)` is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d119ef11abcf12441f1941a8d8bc58fe.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'One may leverage the helpful `[scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)`
    function from the popular `[SciPy](https://scipy.org/)` library to rapidly find
    the optimum:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can plot the objective function and its minimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/fc108ae1c10bd83187c0e3ddd302a0e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Let us now introduce the **steepest descent algorithm** and implement it from
    scratch. Our goal is to solve the optimization problem and find the minimum `[4.5,
    2.3]`.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. The steepest descent algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To solve the optimization problem `minₓ f(x)`, we start by positioning ourselves
    in some point in the coordinate space. From there, we move iteratively towards
    a better approximation of the minimum of `f(x)` through a search direction `p`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41fa6cdc5495051c2cdea3f913f2160f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x` is the input variable;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`p` is the *search direction*;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`α > 0` is the *step size* or *step length*; it describes how much we should
    move along the direction `p` in each iteration `k`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach requires a proper selection of the step size `α` and the search
    direction `p`.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. The search direction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As search direction, the steepest descent algorithm uses the negative gradient
    `-∇f(xₖ)` evaluated in the current iterate `xₖ`. This is a logical choice, since
    the negative gradient of a function always points to the direction in which the
    function decreases the most.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we can rewrite our expression as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5dc508c0983cdcac4fd017f61700104c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since the minimum is a stationary point, it is reasonable to stop the algorithm
    when the norm of the gradient is smaller than a given tolerance: if the gradient
    reaches zero, we may have found a minimum.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. The step size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we are trying to minimize `f(x)`, the ideal step size `α` is the minimizer
    of the following objective function `φ(α)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3cf141dc2c095c9d7480a14e2cf13019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Unfortunately, this would require to solve the additional optimization task
    within the current optimization task: `minₓ f(x)`. Moreover, although `φ(α)` is
    univariate, finding its minimizer may require too many evaluations of `f(x)` and
    its gradient.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In brief, we are looking for a trade-off between the selection of `α` and the
    time we need to make the choice. To this aim, we could simply pick a value of
    the step size that ensures that the objective function will decrease at least
    of a certain amount. Indeed, a popular inexact line search condition states that
    `α` should lead to a *sufficient decrease* of `f(x)` through the following inequality:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26615237f66d404aa0cc12e898942907.png)'
  prefs: []
  type: TYPE_IMG
- en: In literature, this is known as **sufficient decrease** or [**Armijo condition**](https://en.wikipedia.org/wiki/Wolfe_conditions#Armijo_rule_and_curvature),
    and it belongs to the set of [**Wolfe conditions**](https://en.wikipedia.org/wiki/Wolfe_conditions).
  prefs: []
  type: TYPE_NORMAL
- en: The constant `c` is chosen to be small; a common value is 10^-4.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the steepest descent uses the negative gradient `-∇f(xₖ)` as search direction
    `pₖ`, the expression `+ ∇f(xₖ)^T * pₖ` is equal to the negative square norm of
    the gradient. In Python: `-np.linalg.norm(∇f(xₖ))**2`. Therefore, our condition
    of sufficient decrease becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4abf53a86e890f8567da4396e412673a.png)'
  prefs: []
  type: TYPE_IMG
- en: 2.3\. The algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can now write the necessary steps required in the steepest descent method:'
  prefs: []
  type: TYPE_NORMAL
- en: Select a starting point `x = x₀`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a maximum number of iterations `M`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a tolerance `tol` close to zero to evaluate the gradient
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the step counter `n`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeat in loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 5.1 Update `α` through the Armijo condition (line search)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5.2 Construct the next point `x = x - α ⋅ ∇f(x)`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5.3 Evaluate the new gradient `∇f(x)` 5.4 Update the step counter `n = n + 1`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5.5 If the norm of the current gradient is sufficiently small `||∇f(x)|| < tol`
    or the maximum number of iterations has been reached `n = M`, then exit the loop
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Return `x`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let us implement it in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we share an implementation of the steepest descent algorithm.
    In particular, we proceed by steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We start with a constant step size, and then
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: we add the line search with the Armijo condition.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3.1 Constant step size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us begin by implementing a simplified version of our iterative approach
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1796f79057c2d4dbb0ea8841eef2a92.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where we apply a **constant** value for the step size `α` through all the iterations.
    Our purpose is to practically verify that convergence is not guaranteed for just
    any constant `α`, thus requiring to implement the line search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us use this function to solve our optimization task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'With `α = 0.3`, the minimum is reached in 72 steps. We can plot the points
    `x` at each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/dc549e08add1e4a549507241ec007c65.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: We observe how the steepest descent is characterized by a distinguishing “zig-zagging”
    path towards the minimum. This is due to the choice of the negative gradient `-∇f(x)`
    as search direction `p`.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed, **convergence is not guaranteed for any value of the step size**.
    In the previous example, we applied a constant step size `α = 0.3`, but what would
    have happened with a different choice?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/fa7f953a2f65336e40f051e91e5934c6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the step size is too large (`α = 0.4`), the algorithm does not converge:
    `xₖ` keeps oscillating without reaching the minimum until the maximum number of
    iterations is hit.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can better appreciate this behavior by observing the points `x` at each
    iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/dd26779d73130835fe22a708cd9ccc1c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: To guarantee the convergence of the steepest descent, we need to update iteratively
    `α` through the line search.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Line search with the Armijo condition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us modify our previous method by adding the Armijo rule. In the steepest
    descent loop, **before** calculating the next point `x - α ⋅ ∇f(x)` we need to
    choose a suitable step size: we start from the initial guess `α = 1` and half
    its value until the Armijo condition is met. This procedure is called *backtracking
    line search*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us now optimize the objective function with the steepest descent with line
    search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3eae8394c4fa88fd16e953cb10f9e764.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: We notice that we reached the minimum with 55 steps, while the use of a constant
    step size `α` led to a larger number of iterations, or it did not lead to convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, we introduced and implemented the steepest descent method, and
    tested it on a quadratic function in two variables. In particular, we showed how
    to update iteratively the step size with the sufficient decrease condition (Armijo).
  prefs: []
  type: TYPE_NORMAL
- en: The steepest descent with the Armijo line search is guaranteed to converge,
    but it is, in general, quite slow, as it requires a large number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: In future posts, we will explore a modification of this basic algorithm by changing
    the line search strategy and providing a different initialization approach for
    the step size.
  prefs: []
  type: TYPE_NORMAL
