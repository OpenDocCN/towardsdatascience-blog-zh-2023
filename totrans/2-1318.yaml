- en: Implementing the Steepest Descent Algorithm in Python from Scratch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头实现最速下降算法
- en: 原文：[https://towardsdatascience.com/implementing-the-steepest-descent-algorithm-in-python-from-scratch-d32da2906fe2](https://towardsdatascience.com/implementing-the-steepest-descent-algorithm-in-python-from-scratch-d32da2906fe2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/implementing-the-steepest-descent-algorithm-in-python-from-scratch-d32da2906fe2](https://towardsdatascience.com/implementing-the-steepest-descent-algorithm-in-python-from-scratch-d32da2906fe2)
- en: '[](https://nicolo-albanese.medium.com/?source=post_page-----d32da2906fe2--------------------------------)[![Nicolo
    Cosimo Albanese](../Images/9a2c26207146741b58c3742927d09450.png)](https://nicolo-albanese.medium.com/?source=post_page-----d32da2906fe2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d32da2906fe2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d32da2906fe2--------------------------------)
    [Nicolo Cosimo Albanese](https://nicolo-albanese.medium.com/?source=post_page-----d32da2906fe2--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://nicolo-albanese.medium.com/?source=post_page-----d32da2906fe2--------------------------------)[![Nicolo
    Cosimo Albanese](../Images/9a2c26207146741b58c3742927d09450.png)](https://nicolo-albanese.medium.com/?source=post_page-----d32da2906fe2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d32da2906fe2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d32da2906fe2--------------------------------)
    [Nicolo Cosimo Albanese](https://nicolo-albanese.medium.com/?source=post_page-----d32da2906fe2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d32da2906fe2--------------------------------)
    ·11 min read·Feb 20, 2023
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d32da2906fe2--------------------------------)
    ·11分钟阅读·2023年2月20日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0fbf614bb4efdb8d5bcf885b4aa8bb4b.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0fbf614bb4efdb8d5bcf885b4aa8bb4b.png)'
- en: Image by author.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: Table of contents
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: '[Introduction](#8cf1)'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[介绍](#8cf1)'
- en: '[The steepest descent algorithm](#22bb)'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[最速下降算法](#22bb)'
- en: 2.1 [The search direction](#2e50)
  id: totrans-10
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2.1 [搜索方向](#2e50)
- en: 2.2 [The step size](#e299)
  id: totrans-11
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2.2 [步长](#e299)
- en: 2.3 [The algorithm](#b179)
  id: totrans-12
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2.3 [算法](#b179)
- en: '[Implementation](#43e9)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[实现](#43e9)'
- en: 3.1 [Constant step size](#e046)
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3.1 [常数步长](#e046)
- en: 3.2 [Line search with the Armijo condition](#db67)
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3.2 [带有Armijo条件的线搜索](#db67)
- en: '[Conclusions](#6c34)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[结论](#6c34)'
- en: 1\. Introduction
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: 'Optimization is the process of finding the set of variables `x` that minimize
    or maximize an objective function `f(x)`. Since maximizing a function is equivalent
    to minimizing its negative, we may focus on minimization problems alone:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 优化是寻找一组变量`x`，使得目标函数`f(x)`最小化或最大化的过程。由于最大化一个函数等同于最小化它的负数，我们可以专注于最小化问题：
- en: '![](../Images/b67697b8b88e26d3229b118e3a5329a3.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b67697b8b88e26d3229b118e3a5329a3.png)'
- en: 'For our example, let us define a quadratic, multivariable objective function
    `f(x)` as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的例子，我们将定义一个二次的多变量目标函数`f(x)`如下：
- en: '![](../Images/f5c859c0e02132db35f45dc85c9173f2.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5c859c0e02132db35f45dc85c9173f2.png)'
- en: Its gradient `∇f(x)` is
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 它的梯度 `∇f(x)` 为
- en: '![](../Images/d119ef11abcf12441f1941a8d8bc58fe.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d119ef11abcf12441f1941a8d8bc58fe.png)'
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'One may leverage the helpful `[scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)`
    function from the popular `[SciPy](https://scipy.org/)` library to rapidly find
    the optimum:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 可以利用流行的`[scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)`函数来快速找到最优值，该函数来自于流行的`[SciPy](https://scipy.org/)`库：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can plot the objective function and its minimizer:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以绘制目标函数及其最小值：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/fc108ae1c10bd83187c0e3ddd302a0e6.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc108ae1c10bd83187c0e3ddd302a0e6.png)'
- en: Image by author.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: Let us now introduce the **steepest descent algorithm** and implement it from
    scratch. Our goal is to solve the optimization problem and find the minimum `[4.5,
    2.3]`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们介绍**最速下降算法**并从头实现它。我们的目标是解决优化问题并找到最小值`[4.5, 2.3]`。
- en: 2\. The steepest descent algorithm
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 最速下降算法
- en: 'To solve the optimization problem `minₓ f(x)`, we start by positioning ourselves
    in some point in the coordinate space. From there, we move iteratively towards
    a better approximation of the minimum of `f(x)` through a search direction `p`:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决优化问题`minₓ f(x)`，我们首先在坐标空间中的某一点开始。然后，我们通过搜索方向`p`迭代地移动，朝着`f(x)`的最小值更好的近似值前进：
- en: '![](../Images/41fa6cdc5495051c2cdea3f913f2160f.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41fa6cdc5495051c2cdea3f913f2160f.png)'
- en: 'In this expression:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个表达式中：
- en: '`x` is the input variable;'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x` 是输入变量；'
- en: '`p` is the *search direction*;'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p` 是*搜索方向*；'
- en: '`α > 0` is the *step size* or *step length*; it describes how much we should
    move along the direction `p` in each iteration `k`.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`α > 0` 是*步长*或*步幅*；它描述了我们在每次迭代 `k` 中应该沿着方向 `p` 移动多少。'
- en: This approach requires a proper selection of the step size `α` and the search
    direction `p`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法需要适当选择步长 `α` 和搜索方向 `p`。
- en: 2.1\. The search direction
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1\. 搜索方向
- en: As search direction, the steepest descent algorithm uses the negative gradient
    `-∇f(xₖ)` evaluated in the current iterate `xₖ`. This is a logical choice, since
    the negative gradient of a function always points to the direction in which the
    function decreases the most.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作为搜索方向，最陡下降算法使用在当前迭代 `xₖ` 中评估的负梯度 `-∇f(xₖ)`。这是一个合理的选择，因为函数的负梯度总是指向函数下降最快的方向。
- en: 'Therefore, we can rewrite our expression as:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将表达式重写为：
- en: '![](../Images/5dc508c0983cdcac4fd017f61700104c.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5dc508c0983cdcac4fd017f61700104c.png)'
- en: 'Since the minimum is a stationary point, it is reasonable to stop the algorithm
    when the norm of the gradient is smaller than a given tolerance: if the gradient
    reaches zero, we may have found a minimum.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于最小值是一个驻点，当梯度的范数小于给定的容忍度时，停止算法是合理的：如果梯度达到零，我们可能找到了最小值。
- en: 2.2\. The step size
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2\. 步长
- en: 'As we are trying to minimize `f(x)`, the ideal step size `α` is the minimizer
    of the following objective function `φ(α)`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们试图最小化 `f(x)`，理想的步长 `α` 是以下目标函数 `φ(α)` 的最小值：
- en: '![](../Images/3cf141dc2c095c9d7480a14e2cf13019.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cf141dc2c095c9d7480a14e2cf13019.png)'
- en: 'Unfortunately, this would require to solve the additional optimization task
    within the current optimization task: `minₓ f(x)`. Moreover, although `φ(α)` is
    univariate, finding its minimizer may require too many evaluations of `f(x)` and
    its gradient.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这需要在当前优化任务中解决额外的优化任务：`minₓ f(x)`。此外，虽然 `φ(α)` 是一元的，但找到它的最小值可能需要对 `f(x)`
    及其梯度进行过多的评估。
- en: 'In brief, we are looking for a trade-off between the selection of `α` and the
    time we need to make the choice. To this aim, we could simply pick a value of
    the step size that ensures that the objective function will decrease at least
    of a certain amount. Indeed, a popular inexact line search condition states that
    `α` should lead to a *sufficient decrease* of `f(x)` through the following inequality:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们在寻找`α`的选择与做出选择所需时间之间的权衡。为此，我们可以简单地选择一个步长值，确保目标函数至少减少一定量。实际上，一个流行的不精确线搜索条件指出，`α`应该通过以下不等式导致`f(x)`的*充分减少*：
- en: '![](../Images/26615237f66d404aa0cc12e898942907.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26615237f66d404aa0cc12e898942907.png)'
- en: In literature, this is known as **sufficient decrease** or [**Armijo condition**](https://en.wikipedia.org/wiki/Wolfe_conditions#Armijo_rule_and_curvature),
    and it belongs to the set of [**Wolfe conditions**](https://en.wikipedia.org/wiki/Wolfe_conditions).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献中，这被称为**充分减少**或 [**Armijo 条件**](https://en.wikipedia.org/wiki/Wolfe_conditions#Armijo_rule_and_curvature)，并且它属于
    [**Wolfe 条件**](https://en.wikipedia.org/wiki/Wolfe_conditions) 的集合。
- en: The constant `c` is chosen to be small; a common value is 10^-4.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 常数 `c` 被选择为较小的值；一个常见的值是 10^-4。
- en: 'Since the steepest descent uses the negative gradient `-∇f(xₖ)` as search direction
    `pₖ`, the expression `+ ∇f(xₖ)^T * pₖ` is equal to the negative square norm of
    the gradient. In Python: `-np.linalg.norm(∇f(xₖ))**2`. Therefore, our condition
    of sufficient decrease becomes:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于最陡下降法使用负梯度 `-∇f(xₖ)` 作为搜索方向 `pₖ`，表达式 `+ ∇f(xₖ)^T * pₖ` 等于梯度的负平方范数。在 Python
    中：`-np.linalg.norm(∇f(xₖ))**2`。因此，我们的充分减少条件变为：
- en: '![](../Images/4abf53a86e890f8567da4396e412673a.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4abf53a86e890f8567da4396e412673a.png)'
- en: 2.3\. The algorithm
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3\. 算法
- en: 'We can now write the necessary steps required in the steepest descent method:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以写出所需的最陡下降法步骤：
- en: Select a starting point `x = x₀`
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个起始点 `x = x₀`
- en: Select a maximum number of iterations `M`
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个最大迭代次数 `M`
- en: Select a tolerance `tol` close to zero to evaluate the gradient
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个接近零的容忍度 `tol` 来评估梯度
- en: Set the step counter `n`
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置步数计数器 `n`
- en: 'Repeat in loop:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在循环中重复：
- en: 5.1 Update `α` through the Armijo condition (line search)
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 5.1 通过 Armijo 条件（线搜索）更新 `α`
- en: 5.2 Construct the next point `x = x - α ⋅ ∇f(x)`
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 5.2 构造下一个点 `x = x - α ⋅ ∇f(x)`
- en: 5.3 Evaluate the new gradient `∇f(x)` 5.4 Update the step counter `n = n + 1`
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 5.3 评估新的梯度 `∇f(x)` 5.4 更新步数计数器 `n = n + 1`
- en: 5.5 If the norm of the current gradient is sufficiently small `||∇f(x)|| < tol`
    or the maximum number of iterations has been reached `n = M`, then exit the loop
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 5.5 如果当前梯度的范数足够小 `||∇f(x)|| < tol` 或达到最大迭代次数 `n = M`，则退出循环
- en: Return `x`
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回 `x`
- en: Let us implement it in Python.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 Python 来实现它。
- en: 3\. Implementation
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 实现
- en: 'In this section, we share an implementation of the steepest descent algorithm.
    In particular, we proceed by steps:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们分享了最陡下降算法的实现。特别是，我们按步骤进行：
- en: We start with a constant step size, and then
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从常数步长开始，然后
- en: we add the line search with the Armijo condition.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们添加了带有 Armijo 条件的线搜索。
- en: 3.1 Constant step size
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 常数步长
- en: Let us begin by implementing a simplified version of our iterative approach
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从实现我们迭代方法的简化版本开始
- en: '![](../Images/d1796f79057c2d4dbb0ea8841eef2a92.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1796f79057c2d4dbb0ea8841eef2a92.png)'
- en: 'where we apply a **constant** value for the step size `α` through all the iterations.
    Our purpose is to practically verify that convergence is not guaranteed for just
    any constant `α`, thus requiring to implement the line search:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有迭代中应用 **常数** 步长值 `α`。我们的目的是实际验证对于任何常数 `α` 收敛性并不保证，因此需要实现线搜索：
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let us use this function to solve our optimization task:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个函数来解决我们的优化任务：
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'With `α = 0.3`, the minimum is reached in 72 steps. We can plot the points
    `x` at each iteration:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `α = 0.3` 时，最小值在 72 步中达到了。我们可以绘制每次迭代中的点 `x`：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/dc549e08add1e4a549507241ec007c65.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc549e08add1e4a549507241ec007c65.png)'
- en: Image by author.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于作者。
- en: We observe how the steepest descent is characterized by a distinguishing “zig-zagging”
    path towards the minimum. This is due to the choice of the negative gradient `-∇f(x)`
    as search direction `p`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到最陡下降法的特征是通过一个显著的“之”字形路径向最小值前进。这是由于选择了负梯度 `-∇f(x)` 作为搜索方向 `p`。
- en: As we discussed, **convergence is not guaranteed for any value of the step size**.
    In the previous example, we applied a constant step size `α = 0.3`, but what would
    have happened with a different choice?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们讨论的，**对于任何步长值收敛性并不保证**。在之前的示例中，我们使用了常数步长 `α = 0.3`，但如果选择不同的步长会发生什么呢？
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/fa7f953a2f65336e40f051e91e5934c6.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa7f953a2f65336e40f051e91e5934c6.png)'
- en: Image by author.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于作者。
- en: 'When the step size is too large (`α = 0.4`), the algorithm does not converge:
    `xₖ` keeps oscillating without reaching the minimum until the maximum number of
    iterations is hit.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当步长过大（`α = 0.4`）时，算法不收敛：`xₖ` 一直振荡而没有达到最小值，直到达到最大迭代次数。
- en: 'We can better appreciate this behavior by observing the points `x` at each
    iteration:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过观察每次迭代中的点 `x` 来更好地理解这种行为：
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/dd26779d73130835fe22a708cd9ccc1c.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd26779d73130835fe22a708cd9ccc1c.png)'
- en: Image by author.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于作者。
- en: To guarantee the convergence of the steepest descent, we need to update iteratively
    `α` through the line search.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保证最陡下降法的收敛性，我们需要通过线搜索迭代更新 `α`。
- en: 3.3\. Line search with the Armijo condition
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3\. 使用 Armijo 条件的线搜索
- en: 'Let us modify our previous method by adding the Armijo rule. In the steepest
    descent loop, **before** calculating the next point `x - α ⋅ ∇f(x)` we need to
    choose a suitable step size: we start from the initial guess `α = 1` and half
    its value until the Armijo condition is met. This procedure is called *backtracking
    line search*.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过添加 Armijo 规则来修改我们之前的方法。在最陡下降循环中，**在**计算下一个点 `x - α ⋅ ∇f(x)` 之前，我们需要选择一个合适的步长：我们从初始猜测
    `α = 1` 开始，逐步将其值减半，直到满足 Armijo 条件。这个过程称为 *回溯线搜索*。
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let us now optimize the objective function with the steepest descent with line
    search:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用线搜索优化目标函数：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/3eae8394c4fa88fd16e953cb10f9e764.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3eae8394c4fa88fd16e953cb10f9e764.png)'
- en: Image by author.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于作者。
- en: We notice that we reached the minimum with 55 steps, while the use of a constant
    step size `α` led to a larger number of iterations, or it did not lead to convergence.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到在 55 步中达到了最小值，而使用常数步长 `α` 则导致了更多的迭代，或者没有收敛。
- en: 4\. Conclusions
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 结论
- en: In this post, we introduced and implemented the steepest descent method, and
    tested it on a quadratic function in two variables. In particular, we showed how
    to update iteratively the step size with the sufficient decrease condition (Armijo).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们介绍并实现了最陡下降法，并在两个变量的二次函数上进行了测试。特别是，我们展示了如何使用足够减少条件（Armijo）迭代更新步长。
- en: The steepest descent with the Armijo line search is guaranteed to converge,
    but it is, in general, quite slow, as it requires a large number of iterations.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 带有 Armijo 线搜索的最陡下降法保证了收敛，但通常较慢，因为它需要大量的迭代。
- en: In future posts, we will explore a modification of this basic algorithm by changing
    the line search strategy and providing a different initialization approach for
    the step size.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的帖子中，我们将探讨通过改变线性搜索策略并提供不同的步长初始化方法来修改这个基本算法。
