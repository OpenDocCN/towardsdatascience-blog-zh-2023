- en: 'TensorFlow-GNN: An End-To-End Guide For Graph Neural Networks'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow-GNN：图神经网络的端到端指南
- en: 原文：[https://towardsdatascience.com/tensorflow-gnn-an-end-to-end-guide-for-graph-neural-networks-a66bfd237c8c](https://towardsdatascience.com/tensorflow-gnn-an-end-to-end-guide-for-graph-neural-networks-a66bfd237c8c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/tensorflow-gnn-an-end-to-end-guide-for-graph-neural-networks-a66bfd237c8c](https://towardsdatascience.com/tensorflow-gnn-an-end-to-end-guide-for-graph-neural-networks-a66bfd237c8c)
- en: '![](../Images/f6cf81efc4d2972bc977bddcaf88dd1e.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6cf81efc4d2972bc977bddcaf88dd1e.png)'
- en: “Mapsterpiece” by Heidi Malin, used with permission
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: “Mapsterpiece”由Heidi Malin创作，已获许可使用
- en: Tutorial
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 教程
- en: How to do graph, node, and edge predictions using your own Pandas/NetworkX datasets
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用自己的Pandas/NetworkX数据集进行图、节点和边预测
- en: '[](https://michael-malin.medium.com/?source=post_page-----a66bfd237c8c--------------------------------)[![Michael
    Malin](../Images/070604c68a50e8f2996f2c8837df3ec9.png)](https://michael-malin.medium.com/?source=post_page-----a66bfd237c8c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a66bfd237c8c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a66bfd237c8c--------------------------------)
    [Michael Malin](https://michael-malin.medium.com/?source=post_page-----a66bfd237c8c--------------------------------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://michael-malin.medium.com/?source=post_page-----a66bfd237c8c--------------------------------)[![Michael
    Malin](../Images/070604c68a50e8f2996f2c8837df3ec9.png)](https://michael-malin.medium.com/?source=post_page-----a66bfd237c8c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a66bfd237c8c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a66bfd237c8c--------------------------------)
    [Michael Malin](https://michael-malin.medium.com/?source=post_page-----a66bfd237c8c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a66bfd237c8c--------------------------------)
    ·20 min read·Jan 16, 2023
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----a66bfd237c8c--------------------------------)
    ·20分钟阅读·2023年1月16日
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*A special thanks to Alvaro Sanchez Gonzalez from DeepMind and Bryan Perozzi
    and Sami Abu-el-haija from Google who assisted me with this tutorial*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*特别感谢DeepMind的Alvaro Sanchez Gonzalez和Google的Bryan Perozzi及Sami Abu-el-haija，他们在本教程中给予了帮助*'
- en: Updated 04/22/2023 for minor fixes and adding the Graph Nets approach
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 更新于 2023年04月22日，修复了小问题并添加了Graph Nets方法
- en: Graph data is everywhere. Graph research is in its infancy and tools for modeling
    graph data are only starting to emerge. **This makes it the perfect time to jump
    in if you are a data scientist looking to distinguish yourself**. Unfortunately,
    it can be difficult being on the cutting edgedue to a lack of tutorials and support.
    This guide hopes to significantly reduce that pain point.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图数据无处不在。图研究仍处于起步阶段，图数据建模工具刚刚开始出现。**这使得如果你是希望脱颖而出的数据科学家，现在是最佳时机**。不幸的是，由于缺乏教程和支持，处于前沿可能很困难。本指南希望显著减轻这个痛点。
- en: Why TensorFlow-GNN?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择TensorFlow-GNN？
- en: TF-GNN was recently released by Google for graph neural networks using TensorFlow.
    While there are other GNN libraries out there, TF-GNN’s modeling flexibility,
    performance on large-scale graphs due to distributed learning, and Google backing
    means it will likely emerge as an industry standard. This guide assumes you already
    understand the merits of this library, but please see [this paper](https://arxiv.org/abs/2207.03522)
    for more information and performance comparisons. Also, check out the [documentation](https://github.com/tensorflow/gnn/blob/e096b647831b5472b9ad0deb006df22f422bdbec/tensorflow_gnn/docs/guide/overview.md)
    for TF-GNN. If you are new to GNN altogether, check out [this guide](https://distill.pub/2021/gnn-intro/)
    for a conceptual understanding.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: TF-GNN是Google最近发布的用于图神经网络的TensorFlow库。虽然市场上还有其他GNN库，但由于TF-GNN在大规模图上的建模灵活性、分布式学习带来的性能优势和Google的支持，它很可能会成为行业标准。本指南假设你已经了解了这个库的优点，但请参阅[这篇论文](https://arxiv.org/abs/2207.03522)以获取更多信息和性能比较。此外，查看TF-GNN的[文档](https://github.com/tensorflow/gnn/blob/e096b647831b5472b9ad0deb006df22f422bdbec/tensorflow_gnn/docs/guide/overview.md)。如果你对GNN完全陌生，请查看[这本指南](https://distill.pub/2021/gnn-intro/)以获得概念理解。
- en: What is the downside?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺点是什么？
- en: 'With this library currently in an alpha stage, the code is very exact on the
    structures, input shapes, and formats required to model successfully. This makes
    it very difficult to navigate without a guide. Unfortunately, there is not much
    information out there for using TF-GNN. The guides I *could* find focus on the
    same context-level prediction use case using a pre-built TensorFlow dataset. As
    of writing this, there is not a single walk-through for:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于该库当前处于 alpha 阶段，代码对建模所需的结构、输入形状和格式非常严格。这使得没有指南的情况下很难进行导航。不幸的是，目前没有关于使用 TF-GNN
    的大量信息。我*能*找到的指南都集中在使用预构建 TensorFlow 数据集的相同上下文级预测用例上。截至写作时，没有一个完整的操作示例：
- en: Making edge or node predictions
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行边或节点预测
- en: Starting with your own Pandas or NetworkX datasets
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从您自己的 Pandas 或 NetworkX 数据集开始
- en: Creating holdout datasets
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建保留数据集
- en: Model tuning
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型调优
- en: Troubleshooting errors you may run into
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决您可能遇到的故障
- en: After a good month of rereading documentation, trial-and-error coding, and some
    direct help from the TensorFlow developers at Google/DeepMind, I decided to put
    this guide together.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过一个月的文档重读、反复试错编程和来自 Google/DeepMind 的 TensorFlow 开发人员的直接帮助后，我决定编写这个指南。
- en: “Many [hours] died to bring us this information.”
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “许多[小时]为我们带来了这些信息。”
- en: 'What this guide will cover:'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本指南将涵盖：
- en: 'First, we will start very simply to get the building blocks down. Then we will
    move to a more advanced example — college football conference predictions. Here
    is the outline of what will be covered:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将非常简单地开始，以掌握构建模块。然后我们将转向一个更高级的示例——大学橄榄球会议预测。以下是将要涵盖的内容概要：
- en: '**TF-GNN elements**'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TF-GNN 元素**'
- en: '- Building Blocks'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 构建模块'
- en: '- Graph tensor from Pandas'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 从 Pandas 生成的图形张量'
- en: '**Data setup**'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据设置**'
- en: '- Graph tensor from NetworkX'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 从 NetworkX 生成的图形张量'
- en: '- Feature engineering'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 特性工程'
- en: '- Creating test splits'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 创建测试拆分'
- en: '- Creating a graph TensorFlow dataset'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 创建图形 TensorFlow 数据集'
- en: '**Building the model**'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建模型**'
- en: '- Node model'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 节点模型'
- en: '- Edge model'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 边模型'
- en: '- Context model'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 上下文模型'
- en: '**Troubleshooting errors**'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**故障排除**'
- en: '**Parameter tuning**'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数调优**'
- en: TF-GNN elements
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TF-GNN 元素
- en: 'A graph consists of nodes and edges. Here is an example of a simple graph showing
    people (nodes) who recently had contact each other (edges):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一个图形由节点和边组成。以下是一个简单图形的示例，显示了最近互相接触的人（节点）：
- en: '![](../Images/132c610deee63e541103190bda44322b.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/132c610deee63e541103190bda44322b.png)'
- en: Example graph by author
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的示例图形
- en: This same graph could also be represented as node and edge tables. We can also
    add features to these nodes and edges. For example, we can add ‘age’ as a node
    feature and an ‘is-friend’ indicator as an edge feature.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的图形也可以表示为节点和边表。我们还可以为这些节点和边添加特性。例如，我们可以添加“年龄”作为节点特性，并将“是否为朋友”作为边特性。
- en: '![](../Images/09389ab67da2d4cd6def793b618471af.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09389ab67da2d4cd6def793b618471af.png)'
- en: Example node and edge data by author
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的示例节点和边数据
- en: 'When we add edges to TF-GNN, we need to index by number rather than name. We
    can do that like so:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们向 TF-GNN 添加边时，我们需要按数字而非名称进行索引。我们可以这样做：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/762698e968c380016ec11778531577ff.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/762698e968c380016ec11778531577ff.png)'
- en: Node and edge data with numeric index by author
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的带有数字索引的节点和边数据
- en: Finally, we might have a context value for the graph. For example, maybe this
    friend group scored an average of 84% on a certain test. That will not mean much
    for this single-graph example. If we had other friend graphs, we could perhaps
    predict scores for new friend groups based on learned group dynamics.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可能会得到图形的上下文值。例如，也许这个朋友小组在某次测试中的平均分为84%。这对于这个单一图形示例意义不大。如果我们有其他朋友图形，我们或许可以基于学到的群体动态预测新朋友小组的分数。
- en: Graph tensor from pandas
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 pandas 生成的图形张量
- en: 'With these elements, we can now build the foundation for our GNN: a graph tensor.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些元素，我们现在可以为我们的 GNN 构建基础：一个图形张量。
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Notice how the features we created fit into the nodes and edges. The indented
    structure makes it simple to add additional nodes, edges, and features. For example,
    we could easily add nodes and edges for the movies each friend has watched and
    include a graph context value this time.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们创建的特性如何适配到节点和边中。缩进结构使得添加额外的节点、边和特性变得简单。例如，我们可以轻松地为每个朋友观看的电影添加节点和边，并这次包含一个图形上下文值。
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note: Be very careful with your dtypes and shapes. Any deviations will cause
    errors or training issues. The only supported dtypes are ‘int32’, ‘float32’, and
    ‘string’. If you are having issues, please see the troubleshooting section towards
    the end of this article.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：请非常小心你的数据类型和形状。任何偏差都会导致错误或训练问题。唯一支持的数据类型是‘int32’，‘float32’，和‘string’。如果遇到问题，请参阅本文末尾的故障排除部分。
- en: You may have noticed that the graph tensor is directional with a source and
    target. This might be fine for Sam watching a movie, but communication is bidirectional.
    When Sam talks to Amy, Amy is also talking to Sam. For bidirectional data, you
    will want to duplicate those edges (with source and target reversed) to indicate
    both directions of data flow.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，图张量是有方向的，具有源节点和目标节点。这对于萨姆看电影可能没问题，但通信是双向的。当萨姆与艾米交谈时，艾米也在与萨姆交谈。对于双向数据，你需要复制那些边（将源和目标反转），以指示数据流的两个方向。
- en: '![](../Images/840945e3aebb0eb3adcab49fc15e2918.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/840945e3aebb0eb3adcab49fc15e2918.png)'
- en: Example bidirectional data by author
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的示例双向数据
- en: With this foundation, we are now ready to transition to making predictions on
    a real dataset.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个基础，我们现在准备过渡到在真实数据集上进行预测。
- en: Data setup
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据设置
- en: The training data is a network of American football games between Division IA
    colleges during regular season Fall 2000, as compiled
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据是2000年秋季期间IA分区大学之间的美式足球比赛网络，如下所示
- en: 'by M. Girvan and M. Newman. Node data includes college names and an index of
    the conference they belong to (e.g. conference 8 = Pac 10). Edges include the
    two college names, indicating a game was played between them. The data can be
    pulled as follows (see [Google Colab](https://colab.research.google.com/drive/1gNhC0YLc0aZeWwWh6QuSt2McbZMToY2S?usp=sharing)
    to follow along):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 作者：M. Girvan 和 M. Newman。节点数据包括大学名称和他们所属的会议索引（例如，会议8 = Pac 10）。边数据包括两个大学名称，表示它们之间进行了一场比赛。数据可以如下提取（参见
    [Google Colab](https://colab.research.google.com/drive/1gNhC0YLc0aZeWwWh6QuSt2McbZMToY2S?usp=sharing)
    以便跟进）：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Graph tensor from NetworkX
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从NetworkX导入图张量
- en: Our data is now in a NetworkX graph. Let’s see how it looks with nodes colored
    by which conference they belong to.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据现在在NetworkX图中。让我们看看用节点按其所属会议着色的效果如何。
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/5f1a9b7277bf3fefb4611fb8c8ac8413.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f1a9b7277bf3fefb4611fb8c8ac8413.png)'
- en: College American football network by author
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的大学美式足球网络
- en: '**For our node model, we will attempt to predict the conference a school belongs
    to. For our edge model, we will attempt to predict if a game was an in-conference
    game.** Both predictions will be evaluated on a holdout dataset.How do we do this
    from NetworkX? It is possible to build a graph tensor directly from a graph using
    these functions to extract the data:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**对于我们的节点模型，我们将尝试预测一个学校所属的会议。对于我们的边模型，我们将尝试预测一场比赛是否是会议内的比赛。** 这两个预测将基于持出数据集进行评估。我们如何从NetworkX做到这一点？可以直接从图中构建图张量，使用这些函数来提取数据：'
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The problem is, we still want to do some feature engineering and we do not yet
    have our holdout dataset. For these reasons, I highly recommend taking the approach
    of converting your graph data to Pandas. Later, we can plug our data into a graph
    tensor using the method shown in our first example.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是，我们仍然想做一些特征工程，但我们还没有持出数据集。基于这些原因，我强烈建议将你的图数据转换为Pandas。之后，我们可以使用在第一个示例中展示的方法将数据插入图张量。
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/7975807fc612d9189471b35cc64cd0e8.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7975807fc612d9189471b35cc64cd0e8.png)'
- en: College American football node and edge data by author
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的大学美式足球节点和边数据
- en: Feature engineering
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程
- en: Using the base graph, a model might be able to determine if two colleges are
    in the same conference based on the network. But how would it know which conference
    specifically? How could it learn the differences between conferences without any
    node or edge data? For this task, we will need to add more features.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基础图，一个模型可能能够基于网络确定两所大学是否在同一个会议中。但它如何知道具体是哪个会议呢？在没有任何节点或边数据的情况下，它如何学习会议之间的差异？为此任务，我们需要添加更多特征。
- en: What kind of features should we gather? I am no expert in college football,
    but I would imagine conferences are put together based on proximity and rank.
    This guide is focused on TF-GNN so I will add these new features using magic,
    but you can find the specific code in the linked [Google Colab](https://colab.research.google.com/drive/1gNhC0YLc0aZeWwWh6QuSt2McbZMToY2S?usp=sharing).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该收集什么样的特征？我不是大学橄榄球方面的专家，但我想会议的组成是基于邻近性和排名的。本指南侧重于 TF-GNN，因此我将使用魔法添加这些新特征，但你可以在链接的
    [Google Colab](https://colab.research.google.com/drive/1gNhC0YLc0aZeWwWh6QuSt2McbZMToY2S?usp=sharing)
    中找到具体的代码。
- en: For nodes, we will add latitude/longitude and the previous year’s (1999) rank,
    wins, and conference wins. We will also convert the conference column into 12
    dummy variable columns for a softmax prediction.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于节点，我们将添加纬度/经度以及前一年的（1999年）排名、胜场和会议胜场。我们还将把会议列转换为12个虚拟变量列，以进行 softmax 预测。
- en: '![](../Images/f322fc88b624005af21da86dbd54fd71.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f322fc88b624005af21da86dbd54fd71.png)'
- en: Final node dataset by author
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的最终节点数据集
- en: For edges, we will calculate the distance between schools, add a name similarity
    score (maybe schools with the same state in their name are more likely to be in
    the same conference), and a target value for games being a within-conference game.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于边，我们将计算学校之间的距离，添加名称相似度评分（也许名称中包含相同州的学校更有可能在同一会议中），以及比赛是否为会议内比赛的目标值。
- en: '![](../Images/5c2fc3b980805c0bd2f933d22e2d9c3d.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c2fc3b980805c0bd2f933d22e2d9c3d.png)'
- en: Final edge dataset by author
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的最终边数据集
- en: Let’s visualize our data with our new information (orange edges indicate a conference
    game). It definitely appears that geography at least plays a role in conference
    selection.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用新的信息可视化我们的数据（橙色边表示会议比赛）。地理位置显然在会议选择中至少发挥了作用。
- en: '![](../Images/335efbc0766b99f81ec0a82645c83718.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/335efbc0766b99f81ec0a82645c83718.png)'
- en: College data on US map by author
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的美国地图上的大学数据
- en: Creating test splits
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建测试拆分
- en: 'Creating a training split is straightforward; exclude the holdout nodes and
    edges the same way you normally would. Holdout data, however, is a little different
    from your typical machine learning application. Because the overall connections
    are important for an accurate prediction, the final prediction will need to be
    on the entire graph. Once a prediction is made, the results can be filtered down
    to the holdout data for the final evaluation. I will show this process in more
    detail at the prediction stage; here is how I create the splits for now:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 创建训练集是直接的；排除保留的节点和边的方式与你通常的做法相同。然而，保留数据与典型的机器学习应用有所不同。由于整体连接对于准确预测很重要，最终的预测需要基于整个图。一旦做出预测，结果可以过滤到保留数据中进行最终评估。我将在预测阶段更详细地展示这个过程；目前我创建拆分的方式如下：
- en: '[PRE7]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With our new splits, we can now make our bidirectional adjustments and add the
    edge index columns.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们新的拆分，现在我们可以进行双向调整并添加边索引列。
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Creating a TensorFlow dataset
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 TensorFlow 数据集
- en: We are now ready to create our graph tensors which we will transform into TensorFlow
    datasets.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备创建我们的图张量，我们将把它们转换成 TensorFlow 数据集。
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Before creating the dataset, we need a function that will split our graph into
    our training data and the target we will be predicting (shown as label below).
    For our node prediction problem, we will make ‘conference’ our label. We also
    need to drop the ‘conference_game’ feature from the dataset since it would create
    a data leakage issue (i.e. cheating).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建数据集之前，我们需要一个函数将图拆分为训练数据和我们将要预测的目标（如下所示的标签）。对于我们的节点预测问题，我们将“conference”作为我们的标签。我们还需要从数据集中删除“conference_game”特征，因为它会造成数据泄露问题（即作弊）。
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will do the reverse for our edge model: drop the ‘conference’ feature and
    split off ‘conference_game’ as our target (label).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对边模型进行反向操作：删除“conference”特征并将“conference_game”拆分为目标（标签）。
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can now create our dataset and map it through the function above.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以创建数据集，并通过上述函数进行映射。
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The order of these procedures is extremely important:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些程序的顺序非常重要：
- en: 1\. We create our dataset from the graph tensor.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 我们从图张量创建数据集。
- en: 2\. We split our dataset in batches (read up on batch sizes).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 我们将数据集按批次拆分（了解一下批次大小）。
- en: 3\. In the map function, we merge those batches back into one graph.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 在映射函数中，我们将这些批次合并回一个图中。
- en: 4\. We split/drop the features as needed.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 根据需要拆分/删除特征。
- en: The model will not train (or not correctly) if you do not follow this order
    precisely.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不严格按照此顺序操作，模型将无法训练（或无法正确训练）。
- en: Building the model
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: We have our datasets, now the fun part! First, we define the inputs using our
    dataset spec.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了数据集，现在是有趣的部分！首先，我们使用数据集规格定义输入。
- en: '[PRE13]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now we need to initialize our features. We will create functions for initializing
    the nodes and edges. Then we map our features through these functions. To keep
    things simple, I will create a dense layer for each feature.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要初始化特征。我们将创建初始化节点和边的函数。然后，我们通过这些函数映射我们的特征。为了简化，我将为每个特征创建一个密集层。
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: There is a lot of customization that can happen with this previous step. For
    example, we could create word embeddings for string features. We could probably
    gain some accuracy by hashing a latitude / longitude grid rather than just using
    a dense layer. TensorFlow has many options available to us.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的步骤中可以进行很多自定义。例如，我们可以为字符串特征创建词嵌入。我们可以通过对纬度/经度网格进行哈希处理而不是仅仅使用密集层来获得一些准确性。TensorFlow
    为我们提供了许多选项。
- en: 'A few things to note:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 几点说明：
- en: If you have multiple nodes or edges, you will need to add ‘if statements’ to
    apply features to the correct node/edge.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你有多个节点或边，你需要添加 ‘if 语句’ 以将特征应用到正确的节点/边。
- en: Nodes or edges without features can also be initialized with the ‘MakeEmptyFeature’
    function.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有特征的节点或边也可以使用 ‘MakeEmptyFeature’ 函数进行初始化。
- en: For a node-centric problem, initializing edges is optional (read more on node
    vs edge centric).
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于以节点为中心的问题，初始化边是可选的（阅读更多关于节点与边中心的内容）。
- en: The first node must have at least one feature. You may have to create an embedding
    on an index if you have no features (results will likely not be very good).
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个节点必须至少有一个特征。如果没有特征，你可能需要在一个索引上创建一个嵌入（结果可能不会很好）。
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Before we develop our update loop, we need one more helper function. As we add
    dense layers, we will want to make sure we are utilizing L2 regulation and/or
    dropout (L1 would work as well).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开发更新循环之前，我们需要一个额外的辅助函数。随着我们添加密集层，我们希望确保我们在使用 L2 正则化和/或 dropout（L1 也可以使用）。
- en: '[PRE16]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Node Model
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 节点模型
- en: There are several model architectures out there, but graph convolutional networks
    are by far the most common (see other approaches described [here](https://huggingface.co/blog/intro-graphml)).
    Graph convolutions are similar to convolutions commonly used in computer vision
    problems. The main difference is that graph convolutions work on the irregular
    data you find with graph structures. Let’s jump into the actual code.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多模型架构，但图卷积网络迄今为止是最常见的（见其他方法 [这里](https://huggingface.co/blog/intro-graphml)
    ）。图卷积类似于计算机视觉问题中常用的卷积。主要区别在于图卷积处理的是你在图结构中找到的不规则数据。让我们跳入实际代码中。
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The code above can seem a little confusing because of how TensorFlow stacking
    works. Remember that the (graph) labeled ‘#start here’ at the end of the ‘GraphUpdate’
    function is really the input for the code that comes before it. At first, this
    (graph) equals the initialized features we mapped previously. The input gets fed
    into the ‘GraphUpdate’ function becoming the new (graph). With each ‘graph_updates’
    loop, the previous ‘GraphUpdate’ becomes the input for the new ‘GraphUpdate’ along
    with a dense layer specified with the ‘NextStateFromConcat’ function. This diagram
    should help explain:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码可能有些令人困惑，因为 TensorFlow 堆叠的工作原理。请记住，‘#start here’ 标记的（图）实际上是前面代码的输入。在开始时，这个（图）等于我们之前映射的初始化特征。输入被送入
    ‘GraphUpdate’ 函数，成为新的（图）。每次‘graph_updates’循环中，之前的 ‘GraphUpdate’ 成为新的 ‘GraphUpdate’
    的输入，同时还指定了一个通过 ‘NextStateFromConcat’ 函数的密集层。这个图示应该有助于解释：
- en: '![](../Images/3dd765df1bb6d59a58a1ef924d8326c9.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3dd765df1bb6d59a58a1ef924d8326c9.png)'
- en: Graph convolutional network diagram showing two graph updates by author
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图卷积网络图，显示了作者进行的两次图更新
- en: The ‘GraphUpdate’ function simply updates the specified states (node, edge,
    or context) and adds a next state layer. In this case, we are only updating the
    node states with ‘NodeSetUpdate’ but we will explore an edge-centric approach
    when we work on our edge model. With this node update, we are applying a convolutional
    layer along the edges, allowing for information to feed to the node from neighboring
    nodes and edges. The number of graph updates is a tunable parameter, with each
    update allowing for information to travel from further nodes. For example, the
    three updates specified in our case allow for information to travel from up to
    three nodes away. After our graph updates, the final node state becomes the input
    for our prediction head labeled ‘logits’. Because we are predicting 12 different
    conferences, we have a dense layer of 12 units with a softmax activation. Now
    we can compile the model.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ‘GraphUpdate’函数简单地更新指定的状态（节点、边或上下文），并添加一个下一个状态层。在这种情况下，我们只用‘NodeSetUpdate’更新节点状态，但当我们处理边模型时，我们将探索以边为中心的方法。通过这个节点更新，我们在边缘上应用了一个卷积层，使信息能够从邻近的节点和边缘传递到节点。图形更新的数量是一个可调节的参数，每次更新允许信息从更远的节点传播。例如，我们在案例中指定的三次更新允许信息从最多三节点远的地方传播。在图形更新后，最终的节点状态成为我们标记为‘logits’的预测头的输入。由于我们预测12个不同的会议，我们有一个包含12个单元的密集层，并使用softmax激活函数。现在我们可以编译模型。
- en: '[PRE18]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: And finally, we train the model. I am using a callback to stop training when
    the validation dataset stops improving accuracy. It isn’t perfect since we have
    to use the full dataset (explained above). This will cause our accuracy number
    to include data leakage. A perfect solution would be to write a custom evaluation
    function that only returns accuracy for the validation nodes on the validation
    data, and training nodes for the training data. That is a lot of work (would take
    a tutorial in itself) to be a couple epochs closer to the most accurate stopping
    point. I choose to keep it simple and live with a marginally less accurate model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们训练模型。我使用了一个回调函数来在验证数据集停止提高准确率时停止训练。这并不完美，因为我们必须使用完整数据集（如上所述）。这将导致我们的准确率包括数据泄漏。一个完美的解决方案是编写一个自定义评估函数，仅返回验证数据上的验证节点的准确率，以及训练数据上的训练节点的准确率。这需要大量工作（要讲解的话会成为一个教程），以便在最准确的停止点上更近一步。我选择保持简单，并接受一个略微不那么准确的模型。
- en: '[PRE19]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Time to see how we did using node_model.predict(full_node_dataset) and printing
    the results on a map using magic (see [Google Colab](https://colab.research.google.com/drive/1gNhC0YLc0aZeWwWh6QuSt2McbZMToY2S?usp=sharing)).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来看一下我们使用 `node_model.predict(full_node_dataset)` 的效果，并通过魔法将结果打印在地图上（见[Google
    Colab](https://colab.research.google.com/drive/1gNhC0YLc0aZeWwWh6QuSt2McbZMToY2S?usp=sharing)）。
- en: '![](../Images/e526705ac81014ab669eddc20e737372.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e526705ac81014ab669eddc20e737372.png)'
- en: Node prediction accuracy comparison by author
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 按作者的节点预测准确性比较
- en: Overall, we had a respectable 88% accuracy (see [Google Colab](https://colab.research.google.com/drive/1gNhC0YLc0aZeWwWh6QuSt2McbZMToY2S?usp=sharing)
    for model parameters). The model seems to have a harder time for the mountain
    states. Diving in deeper yields some interesting insights. For example, the model
    falsely predicted Utah to be in the Pac 10 conference. The following year, however,
    Utah did in fact join Pac 10\. It is entirely possible that the model is correctly
    identifying how things should be and the ~12% error is really a measurement of
    human inconsistency when creating conferences. Another way to think about it is
    with a social network of friends. If the network predicts two people are friends
    when they have never met, is the model wrong or are they a good match to be friends?
    For many (or a majority) of graph problems, these “errors” are what you are really
    trying to find. They can then be used to recommend products to buy, movies to
    watch, people you should connect with, etc.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们的准确率达到了令人尊敬的88%（见[Google Colab](https://colab.research.google.com/drive/1gNhC0YLc0aZeWwWh6QuSt2McbZMToY2S?usp=sharing)获取模型参数）。模型在山区州的表现似乎较差。深入分析后，我们发现了一些有趣的见解。例如，模型错误地预测犹他州属于Pac
    10会议。然而，事实上，犹他州在第二年确实加入了Pac 10。完全有可能模型准确地识别了应该如何进行，而约12%的误差实际上是衡量在创建会议时的人为不一致性。另一种考虑方式是用社交网络中的朋友。如果网络预测两个人是朋友，而他们从未见过面，这个模型是错的还是他们实际上是好朋友？对于许多（或大多数）图形问题，这些“错误”实际上是你真正想要找到的。这些“错误”可以用于推荐购买的产品、观看的电影、应该联系的人等。
- en: For this case, let’s assume the data is perfect and we are interested in classification
    accuracy. To really know how well we did, we will need to test the accuracy on
    our holdout data. To do this, we will make a prediction on the full dataset and
    filter down to the holdout nodes.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: For this model, accuracy drops to ~72% (Don’t Panic, a drop is expected on a
    holdout dataset). Given the limited feature engineering, only one year of data,
    and 12 output predictions — those results are reasonable. Upon visual inspection
    of the maps below (and comparing to the full map above), most of the errors seem
    like decent guesses.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67636c0dd27a6de4f2ee8244698d61bb.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: Node holdout prediction accuracy comparison by author
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Edge Model
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we will try to predict if a specific game is an in-conference game. We
    already defined our edge datasets above and most of the steps can be reused with
    only one change:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We do need to make a few changes to the graph updates though. First, we need
    to add an ‘edge_sets’ update to our ‘GraphUpdate’ function. Leaving in the ‘node_sets’
    update is optional but the model does seem to do better for me when I keep it
    in. Next, we will switch from a GCN to a Graph Nets approach. This method treats
    edges as first-class citizens (i.e. a fancy way of saying they will learn their
    own weights which is what we are after). Finally, we need to update ‘logits’ to
    be a one unit sigmoid activation dense layer since we are predicting a dummy variable.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We compile the model using ‘binary_crossentropy’ this time.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: And we fit the model using the same callback defined in our node problem.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: When evaluated on the holdout dataset, we get 85% accuracy compared to a 56%
    mean. The model did its job and I am satisfied with those results.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Context Model
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This particular problem does not have a context value. Let’s imagine that we
    sliced the graph above so we had a separate graph for each conference. These new
    graphs would show every game played for teams in the conference and ignore all
    other games. We could then have values for each graph for how the conference was
    ranked. Now we can train a model to make context-level predictions.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to add our context values to the graph.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Next we need to create a new dataset with the context mapped to the label.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We have the ability to set our initial context state. In this case, we are
    predicting this feature so it will be absent from from our training data. For
    other models, context may be a trainable feature and can be set like so:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Again, we can optionally add a context update to the ‘GraphUpdate’ (see below).
    I have not tested this method so feel free to experiment.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Finally, we update our ‘logits’ for a context prediction
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Troubleshooting errors
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I ran into many errors and poorly trained models trying to figure out the code
    above. While I tried to keep things generic enough to apply to many different
    problems, you will no doubt run into errors as you make adjustments for your data.
    The trick is to identify the source of your error. The best way I found to diagnose
    errors was to create a graph schema.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试理解上述代码时，我遇到了许多错误和训练效果不佳的模型。虽然我尽量保持内容的通用性以适用于多种不同的问题，但你在为数据进行调整时无疑会遇到错误。诀窍是识别错误的来源。我发现诊断错误的最佳方法是创建一个图形模式。
- en: 'In our code above, we pulled the graph schema from our dataset. You can, however,
    build a graph schema directly. For our football example, the graph schema would
    look like this:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的代码中，我们从数据集中提取了图形模式。然而，你也可以直接构建一个图形模式。对于我们的足球示例，图形模式如下所示：
- en: '[PRE30]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We can test if our ‘graph_spec’ is at least valid by attempting to build and
    compile the model. If you get an error, there is likely an issue with your feature
    shapes or your ‘set_initial_…’ functions. If it works, you can verify that the
    schema you created is compatible with your ‘graph_tensor’.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过尝试构建和编译模型来测试‘graph_spec’是否至少有效。如果出现错误，可能是你的特征形状或‘set_initial_…’函数存在问题。如果成功，你可以验证你创建的模式是否与‘graph_tensor’兼容。
- en: '[PRE31]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: If false, you can print out ‘full_tensor.spec’ and ‘graph_spec’ to compare each
    piece to ensure the shapes and dtypes are exactly the same. You can also create
    a randomly generated graph tensor directly from the ‘graph_spec’.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果为假，你可以打印‘full_tensor.spec’和‘graph_spec’来比较每一部分，以确保形状和数据类型完全相同。你还可以直接从‘graph_spec’创建一个随机生成的图张量。
- en: '[PRE32]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: With this ‘random_graph’ you can attempt to train a model. This should help
    you determine if your error is with the spec or the model code. If you do not
    get any errors, you can print the values of the ‘random_graph’ to see how the
    outputs compare to your ‘graph_tensor’.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个‘random_graph’你可以尝试训练一个模型。这应该有助于你确定错误是出在规范还是模型代码上。如果没有错误，你可以打印‘random_graph’的值，看看输出与‘graph_tensor’的比较情况。
- en: '[PRE33]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: These steps should allow you to track down the majority of issues you run into.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤应该可以帮助你追踪大部分遇到的问题。
- en: Parameter tuning
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数调优
- en: We have successfully fixed any errors we were having and trained a model. Now
    we want to tune our hyperparameters for an accurate model. My tuner of choice
    is the Hyperopt library because of its ease of use and integrated Bayesian optimization.
    But first we want to convert our modeling code above to a class with variables.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功修复了所有错误并训练了一个模型。现在我们希望调整超参数以获得一个准确的模型。我选择的调优工具是 Hyperopt 库，因为它易于使用且集成了贝叶斯优化。但首先，我们需要将上述建模代码转换为具有变量的类。
- en: '[PRE34]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now we define our parameters. For our tuning parameters, we can either expressly
    define the value (e.g. ‘dropout’: 0.1) or define the space for Hyperopt to experiment
    with as I did below. ‘hp.choice’ will choose between options you specify while
    ‘hp.uniform’ will pick options between two values. There are many other options
    available in the H[yperopt documentation](http://hyperopt.github.io/hyperopt/getting-started/search_spaces/).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '现在我们定义我们的参数。对于调优参数，我们可以明确地定义值（例如，‘dropout’: 0.1），或像下面我所做的那样定义 Hyperopt 可以实验的空间。‘hp.choice’会在你指定的选项之间进行选择，而‘hp.uniform’会在两个值之间选择。H[yperopt
    文档](http://hyperopt.github.io/hyperopt/getting-started/search_spaces/)中还有许多其他可用的选项。'
- en: '[PRE35]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Next, we define a helper function and plug it into ‘fmin’ along with our parameters.
    Each evaluation is a trained model so this can take a while depending on your
    hardware. Consider doing fewer ‘max_evals’ if it is too slow for you. My personal
    rule of thumb is ~15 evaluations per tuned parameter so I would expressly define
    some of the parameters in relation to the drop in the number of evaluations.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个辅助函数，并将其与我们的参数一起插入到‘fmin’中。每次评估都是一个训练好的模型，因此根据你的硬件，这可能需要一段时间。如果速度对你来说太慢，可以考虑减少‘max_evals’的次数。我个人的经验法则是每个调优参数大约进行15次评估，因此我会明确地定义一些参数，以便根据评估次数的减少进行调整。
- en: '[PRE36]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now that we have our best hyperparameters, we can train our final model (NOTE:
    your accuracy will be slightly different due to how TensorFlow randomly initializes
    its weights).'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了最佳的超参数，可以训练最终模型（注意：由于 TensorFlow 随机初始化其权重，你的准确度可能会略有不同）。
- en: '[PRE37]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can tune and train our edge model with a few slight adjustments:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一些微小的调整来调优和训练我们的边缘模型：
- en: '[PRE38]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Final thoughts
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后的思考
- en: GNN research is still in its infancy. New modeling methods are likely to be
    discovered. With TF-GNN still in an alpha state, there is a good chance there
    may be some code changes over the years. Please comment below if you find changes
    or errors that I have not fixed yet and I will update this guide the best I can.
    If you did not like this article, feel free to draw an analogy between me and
    your favorite historical dictator in the comments. Otherwise, a clap or nice comment
    would be appreciated.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: GNN 研究仍处于初期阶段。新的建模方法很可能会被发现。由于 TF-GNN 仍处于 alpha 状态，未来可能会有一些代码变更。如果您发现我尚未修复的更改或错误，请在下方留言，我会尽力更新此指南。如果您不喜欢这篇文章，可以在评论中将我与您最喜欢的历史独裁者作类比。否则，鼓掌或友好的评论将不胜感激。
- en: My hope is that this guide can be a starting place for more people to enter
    this field and experiment. Consider this your opportunity to be at the beginning
    of the next AI wave!
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这份指南能成为更多人进入这一领域并进行实验的起点。请考虑这是您参与下一波 AI 浪潮的机会！
- en: About me
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于我
- en: 'I am a senior data scientist and part-time freelancer with over 12 years of
    experience. I am always looking to connect so please feel free to:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我是一名资深数据科学家和兼职自由职业者，拥有超过 12 年的经验。我始终希望能与他人建立联系，请随时：
- en: '[Connect with me on LinkedIn](https://www.linkedin.com/in/michael-a-malin/)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 LinkedIn 上与我连接](https://www.linkedin.com/in/michael-a-malin/)'
- en: '[Follow me on Twitter](https://twitter.com/alaska_malin)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Twitter 上关注我](https://twitter.com/alaska_malin)'
- en: '[Visit my website: www.modelforge.ai](http://www.modelforge.ai)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[访问我的网站：www.modelforge.ai](http://www.modelforge.ai)'
- en: '[See my other articles](https://michael-malin.medium.com/)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[查看我的其他文章](https://michael-malin.medium.com/)'
- en: '***Please feel free to comment below if you have any questions.***'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '***如果您有任何问题，请随时在下面留言。***'
