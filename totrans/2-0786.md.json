["```py\n# Installation\npip install hgboost\n```", "```py\n# Import the library\nfrom hgboost import hgboost\n\n# Initialize library.\nhgb = hgboost(\n    max_eval=1000,      # Search space is based  on the number of evaluations.\n    threshold=0.5,     # Classification threshold. In case of two-class model this is 0.5.\n    cv=5,              # k-folds cross-validation.\n    test_size=0.2,     # Percentage split for the testset.\n    val_size=0.2,      # Percentage split for the validationset.\n    top_cv_evals=10,   # Number of top best performing models that is evaluated.\n    is_unbalance=True, # Control the balance of positive and negative weights, useful for unbalanced classes.\n    random_state=None, # Fix the random state to create reproducible results.\n    n_jobs=-1,         # The number of CPU jobs to run in parallel. -1 means using all processors.\n    gpu=False,         # Compute using GPU in case of True.\n    verbose=3,         # Print progress to screen.\n)\n```", "```py\n#########################################\n# You do not need to execute these lines!\n#########################################\n\n# XGBoost\nxgb_reg_params = {\n    'learning_rate': hp.quniform('learning_rate', 0.05, 0.31, 0.05),\n    'max_depth': hp.choice('max_depth', np.arange(5, 30, 1, dtype=int)),\n    'min_child_weight': hp.choice('min_child_weight', np.arange(1, 10, 1, dtype=int)),\n    'gamma': hp.choice('gamma', [0, 0.25, 0.5, 1.0]),\n    'reg_lambda': hp.choice('reg_lambda', [0.1, 1.0, 5.0, 10.0, 50.0, 100.0]),\n    'subsample': hp.uniform('subsample', 0.5, 1),\n    'n_estimators': hp.choice('n_estimators', range(20, 205, 5)),\n}\n\n# LightBoost\nlgb_reg_params = {\n    'learning_rate': hp.quniform('learning_rate', 0.05, 0.31, 0.05),\n    'max_depth': hp.choice('max_depth', np.arange(5, 30, 1, dtype=int)),\n    'min_child_weight': hp.choice('min_child_weight', np.arange(1, 8, 1, dtype=int)),\n    'subsample': hp.uniform('subsample', 0.8, 1),\n    'n_estimators': hp.choice('n_estimators', range(20, 205, 5)),\n}\n\n# CatBoost\nctb_reg_params = {\n    'learning_rate': hp.quniform('learning_rate', 0.05, 0.31, 0.05),\n    'max_depth': hp.choice('max_depth', np.arange(2, 16, 1, dtype=int)),\n    'colsample_bylevel': hp.choice('colsample_bylevel', np.arange(0.3, 0.8, 0.1)),\n    'n_estimators': hp.choice('n_estimators', range(20, 205, 5)),\n}\n```", "```py\n##########################################\n# Train model\n##########################################\n\nresults  = hgb.xgboost_reg(X, y, eval_metric='mae')      # XGBoost\n# results = hgb.lightboost_reg(X, y, eval_metric='mae')  # LightBoost\n# results = hgb.catboost_reg(X, y, eval_metric='mae')    # CatBoost\n\n# [hgboost] >Start hgboost regression.\n# [hgboost] >Collecting xgb_reg parameters.\n# [hgboost] >method: xgb_reg\n# [hgboost] >eval_metric: mae\n# [hgboost] >greater_is_better: False\n# [hgboost] >*********************************************************************************\n# [hgboost] >Total dataset: (4134, 198) \n# [hgboost] >Validation set: (827, 198) \n# [hgboost] >Test-set: (827, 198) \n# [hgboost] >Train-set: (2480, 198) \n# [hgboost] >*********************************************************************************\n# [hgboost] >Searching across hyperparameter space for best performing parameters using maximum nr. evaluations: 1000\n# 100%|██████████| 1000/1000 [04:14<00:00,  1.02s/trial, best loss: 35223.92493340009]\n# [hgboost]> Collecting the hyperparameters from the [1000] trials.\n# [hgboost] >[mae]: 35220 Best performing model across 1000 iterations using Bayesian Optimization with Hyperopt.\n# [hgboost] >*********************************************************************************\n# [hgboost] >5-fold cross validation for the top 10 scoring models, Total nr. tests: 50\n# [hgboost] >[mae] (average): 35860 Best 5-fold CV model using optimized hyperparameters.\n# [hgboost] >*********************************************************************************\n# [hgboost] >Evaluate best [xgb_reg] model on validation dataset (827 samples, 20%)\n# [hgboost] >[mae]: 35270 using optimized hyperparameters on validation set.\n# [hgboost] >[mae]: 35540 using default (not optimized) parameters on validation set.\n# [hgboost] >*********************************************************************************\n# [hgboost] >Retrain [xgb_reg] on the entire dataset with the optimal hyperparameters.\n# [hgboost] >Fin!\n```", "```py\n# Results are stored in the object and can be found in:\nprint(hgb.results)\n\n# Results are also returned by the model:\nprint(results.keys())\n\n# ['params', 'summary', 'trials', 'model', 'val_results', 'comparison_results']\n\n###########################################################################################\n# The params contains the parameters to create the best performing model.\nprint(results['params'])\n\n# {'gamma': 1.0,\n# 'learning_rate': 0.1,\n# 'max_depth': 25,\n# 'min_child_weight': 1,\n# 'n_estimators': 55,\n# 'reg_lambda': 1.0,\n# 'subsample': 0.5522011244420446}\n\n###########################################################################################\n# The summary contains the model evaluations.\nprint(results['summary'])\n\n#     gamma gpu_id learning_rate  ... best_cv loss_validation default_params\n# 0     0.5      0           0.1  ...     0.0             NaN          False\n# 1     1.0      0           0.3  ...     0.0             NaN          False\n# 2       0      0           0.1  ...     0.0             NaN          False\n# 3     1.0      0          0.05  ...     0.0             NaN          False\n# 4    0.25      0           0.3  ...     0.0             NaN          False\n# ..    ...    ...           ...  ...     ...             ...            ...\n# 246   1.0      0          0.15  ...     0.0             NaN          False\n# 247  0.25      0           0.1  ...     0.0             NaN          False\n# 248   1.0      0          0.05  ...     0.0             NaN          False\n# 249   0.5      0           0.2  ...     0.0             NaN          False\n# 250  None    NaN          None  ...     NaN    35539.674538           True\n\n# [251 rows x 20 columns]\n###########################################################################################\n\n# The trials contains the object from hyperopt in case you want to further investigate.\nprint(results['trials'])\n# <hyperopt.base.Trials object at 0x000002B5A42E70A0>\n```", "```py\n# Plot the hyperparameter tuning.\nhgb.plot_params()\n\n# Plot the summary of all evaluted models.\nhgb.plot()\n\n# Plot results on the validation set.\nhgb.plot_validation()\n\n# Plot results on the k-fold cross-validation.\nhgb.plot_cv()\n\n# Plot the best performing tree.\nhgb.treeplot()\n```", "```py\n# Make new predictions using the model. Suppose the X is new and unseen data.\ny_pred, y_proba = hgb.predict(X)\n```", "```py\n# Save model\nstatus = hgb.save(filepath='hgboost_model.pkl', overwrite=True)\n# [pypickle] Pickle file saved: [hgboost_model.pkl]\n# [hgboost] >Saving.. True\n\n# Load model\nfrom hgboost import hgboost                      # Import library when using a fresh start\nhgb = hgboost()                                  # Initialize hgboost\nresults = hgb.load(filepath='hgboost_model.pkl') # Load the pickle file with model parameters and trained model.\n# [pypickle] Pickle file loaded: [hgboost_model.pkl]\n# [hgboost] >Loading succesful!\n\n# Make predictions again\ny_pred, y_proba = hgb.predict(X)\n```"]