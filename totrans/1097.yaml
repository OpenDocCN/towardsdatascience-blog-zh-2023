- en: How Simpson’s Paradox Can Mislead Statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-simpsons-paradox-can-mislead-statistics-f63d1c6a8e15](https://towardsdatascience.com/how-simpsons-paradox-can-mislead-statistics-f63d1c6a8e15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And why ML interpretations are not always reliable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jacky.kaub?source=post_page-----f63d1c6a8e15--------------------------------)[![Jacky
    Kaub](../Images/e66c699ee5a9d5bbd58a1a72d688234a.png)](https://medium.com/@jacky.kaub?source=post_page-----f63d1c6a8e15--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f63d1c6a8e15--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f63d1c6a8e15--------------------------------)
    [Jacky Kaub](https://medium.com/@jacky.kaub?source=post_page-----f63d1c6a8e15--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f63d1c6a8e15--------------------------------)
    ·12 min read·Jan 18, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf125e6aafe97bee3a80c16986995bba.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jason Leung](https://unsplash.com/es/@ninjason?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: If there is something I really hate hearing is the classical authority argument
    “Statistics shows that *insert a fact here*”.
  prefs: []
  type: TYPE_NORMAL
- en: With the democratization of statistical tools and machine learning, it is easier
    than ever to crunch some numbers into insights. Still, everyone should pay attention
    about not fall into some very counterintuitive traps that can lead to bias studies
    made a bit too fast or without the view of an expert of the subject.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will take a deep dive into one classic statistical trap,
    namely **Simpson’s paradox** that can lead to misinterpretation of a feature’s
    interpretation in a model due to the presence of hidden correlated factors.
  prefs: []
  type: TYPE_NORMAL
- en: I designed this post for people with basic knowledge in statistics and machine
    learning, and particularly for data scientists and analysts that are not aware
    of the paradox. Nevertheless, the cases exposed in [part II](#71b8) might be of
    interest to anyone willing to discover concrete cases in which data can be manipulated
    to say something and its opposite.
  prefs: []
  type: TYPE_NORMAL
- en: What is this paradox about ?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You are running an analysis on a dataset. You have a bunch of features X1, …
    Xn that you are using in order to predict a target variable “y”.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Simpson’s paradox can appear when two conditions are met:'
  prefs: []
  type: TYPE_NORMAL
- en: You are missing at least one variable (let’s call Xs) that explains a part of
    the variance of your target “y”.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a direct link of causality between this hidden variable Xs and one
    of your features Xb.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In that case, omitting Xs might directly lead to a misinterpretation of the
    impact of Xb on the target variable y.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7eb27dd841065593e9933d9a6eeddd80.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of the paradox
  prefs: []
  type: TYPE_NORMAL
- en: The case of the linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get an intuition behind the paradox, let’s study the simple case of a linear
    regression with a target “y” connected to two variables “X1” and “X2”. We also
    add an error term, independent and identically distributed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8fa46fc20d35c40d7b6b24dc89e494b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In normal conditions, when the variables X1 and X2 are not correlated, omitting
    X2 and regressing y on X1 would simply give:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ef23cf70eb1fe834b8b4187c2f4fed2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cd5f8c848f88e5bac01f16c4938ad888.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s verify this with a bunch of simulated data, taking a = 1 and b = 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6be5ed00675d4f083dd29ec883f96a63.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We confirm that when there is no correlation between X2 and X1:'
  prefs: []
  type: TYPE_NORMAL
- en: The slope of the linear regression of y on X1 is equal to “a” (2nd figure)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The error of the regression epsilon1 is equal to the term bX2 + epsilon (3rd
    figure).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When X1 and X2 are not independent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are now going to assume that there is a direct correlation between X1 and
    X2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ab59c1568ed6ff03093082f565fdd4d.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, we cannot re-inject directly X2 into the error term, as the error
    would not be independent anymore (due to the correlation between X1 and X2)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3bcf5a3df5191871290aa6c0ca43c954.png)'
  prefs: []
  type: TYPE_IMG
- en: Due to the term in X1, the error is not independent anymore
  prefs: []
  type: TYPE_NORMAL
- en: 'To retrieve an error independent, we need to rearrange our equation on X1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52dc18f2ca75a8b56145d2abf741af25.png)'
  prefs: []
  type: TYPE_IMG
- en: And this time, unlike before, the coefficient will not be anymore equal to “a”,
    but includes a term accounting for the correlation between X1 and X2 (a + bc).
  prefs: []
  type: TYPE_NORMAL
- en: 'And this is where lies the Simpson Paradox: depending on the values of b (explained
    variance of X2 on y) and c (explained variance of X2 on X1) we can now have a
    correlation between y and X1 that can be much higher, much lower or even… opposite!'
  prefs: []
  type: TYPE_NORMAL
- en: '*Note also that the paradox vanishes if b = 0 or c = 0, confirming the two
    conditions exposed earlier in this simplified case.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'To visualize this effect, let’s modify our simulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: With b = -1 and c = 2, we should find a negative coefficient when regressing
    y on X1 instead of a positive one…
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/400e347852e6756c74f28174eae77ef1.png)'
  prefs: []
  type: TYPE_IMG
- en: … And this is exactly what we find out (2nd figure). Note that we retrieve as
    well the linear regression error being epsilon + b epsilon_2 (3rd figure), as
    expected by our little calculation.
  prefs: []
  type: TYPE_NORMAL
- en: And this is it, we have exposed Simpson’s paradox for our linear system. By
    omitting the impact of the variable X2, correlated both to X1 and y, we exposed
    ourselves to a wrong interpretation of the coefficient of the regression of y
    on X1.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note that according to our simple example, the paradox can occur for a wide
    range of values of b and c. For example, b = 2 and c = -1 would create also a
    Simpson paradox in which X1 and X2 are negatively correlated while X2 has a strong
    influence on y.*'
  prefs: []
  type: TYPE_NORMAL
- en: This is of course a very simple example, but it is useful to develop the intuition
    around this particular phenomenon.
  prefs: []
  type: TYPE_NORMAL
- en: Some famous Simpson’s Paradox
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This looks like a fun theoretical problem, but the paradox can be found in many
    ways in real situations. Let’s dig into 2 real-life case studies to see where
    statistics could actually fool someone not totally aware of those subtleties,
    and see how it connects to what we highlighted in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: I took the examples from [the excellent post](https://scienceetonnante.com/2013/04/29/le-paradoxe-de-simpson/)
    (in french) of **David Louapre**, which inspired me in going further in the paradox
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Smoking and life expectancy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the first part of “Ignoring a covariate: An example of Simpson’s paradox”[1],
    Appleton, David R., Joyce M. French, and Mark PJ Vanderpump study the survival
    rate (y) of smoking and nonsmoking women (X0).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to visualize easily the paradox in such a case, we are going to simulate
    a similar dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/670c22ad2abeafa80bda2e38aa949657.png)'
  prefs: []
  type: TYPE_IMG
- en: Samples of the dataset (synthetic)
  prefs: []
  type: TYPE_NORMAL
- en: 'The figure below shows the life expectancy in our simulated dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29e4edb3e1d5d5998ced57ebad9437ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Life expectancy when splitting the dataset by our feature of interest (smoking),
    on the simulated dataset
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, a preliminary result when making a simple analysis in the global
    population seems to show that smokers tend to have a higher life expectancy.
  prefs: []
  type: TYPE_NORMAL
- en: This very strange result comes from the fact that a confounding variable, the
    age of the population studied (our variable X2 following the example in part I)
    is omitted in the chart.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now see how the results look like when adding the age variable in the
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/73c6ff116269c25c2ed9edf474133682.png)'
  prefs: []
  type: TYPE_IMG
- en: Life expectancy when splitting the dataset by our feature of interest (smoking)
    and by age, on the simulated dataset
  prefs: []
  type: TYPE_NORMAL
- en: This time we can see that for every group, the life expectancy in 20 years’
    time is better for the non-smokers in each age group.
  prefs: []
  type: TYPE_NORMAL
- en: When we look at the data from a different angle, we can spot the correlation
    between the age (X2) and the Smoker/Non-Smoker category (X1).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97f9b9ce56cbbc302426ad1dad7be1f5.png)'
  prefs: []
  type: TYPE_IMG
- en: Repartitions of smoker in the population by age group, in our simulated dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'And we fall back on our set of condition exposed in the first part:'
  prefs: []
  type: TYPE_NORMAL
- en: Our variable age “X2" is very negatively correlated to our target (y) “20 year’s
    time life expectancy” (giving b << 0), as elders are unlikely to live 20 years
    more compared to younger people.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also the age (X2) is negatively correlated to being a smoker (X1) (c < 0)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This result, if omitting the age X2, of having the impression that smoking improve
    life expectancy ( a + bc ) > 0 while it actually reduce it (a < 0 )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gender and school admissions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another very famous example of Simpson’s Paradox comes has been highlighted
    in « [Sex bias in graduate admissions: Data from Berkeley.](http://www.unc.edu/~nielsen/soci708/cdocs/Berkeley_admissions_bias.pdf)
    »[2]'
  prefs: []
  type: TYPE_NORMAL
- en: The original datasets expose the admission of people for Berkeley in 1973 by
    department and gender and we are also going to prepare a similar synthetic variation
    for illustrations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e0391639aed1772bbacf2dcb364f6dd1.png)'
  prefs: []
  type: TYPE_IMG
- en: Synthetic data to illustrate the sex bias in graduate admissions paradox
  prefs: []
  type: TYPE_NORMAL
- en: When we simply look at the statistics by gender (X1), we discover a bias in
    the % of Admission (y) toward male gender statistically significant.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af76c1442906f88d3e5bac7411c5b88d.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple split of admission rate, by gender, on synthetic data
  prefs: []
  type: TYPE_NORMAL
- en: On the other end, adding the Major (X2) in the analysis reverse in most of the
    department the conclusion on the gender bias, revealing the paradox.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2615c6fa39aaf4af5897c189d527e70f.png)'
  prefs: []
  type: TYPE_IMG
- en: Adding the major expose the paradox, example on synthetics data
  prefs: []
  type: TYPE_NORMAL
- en: As in the previous examples, the paradox can be explained by a correlation of
    the Major selected (X2) with both our target y (the Selectivity) and our original
    feature X1 (The Gender).
  prefs: []
  type: TYPE_NORMAL
- en: The graph below shows that in the dataset, females tend to apply more for very
    selective majors while males for the less selective ones.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8544f6f884c774c683d5ed9a00327e4e.png)'
  prefs: []
  type: TYPE_IMG
- en: The hidden correlations between the Major (X2) and our target and original feature
  prefs: []
  type: TYPE_NORMAL
- en: The paradox with advanced ML tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now we have discussed Simpson’s Paradox in relatively simple datasets.
    But it is also possible to generate much more complicated example when even advance
    ML and feature explanation algorithms could be misleading if not including all
    relevant data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this part, I simulate a “house price dataset” made of 5 features:'
  prefs: []
  type: TYPE_NORMAL
- en: Asset surface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garage surface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garden surface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: House condition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Area Fanciness (optional parameter)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And we are going to predict the asset price based on those 5 features.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a sneaky person, I designed the Area Fanciness feature a bit differently:'
  prefs: []
  type: TYPE_NORMAL
- en: 'I built this feature so that the fanciness of the area has a non-negligible
    impact on our target (SalePrice), but also on the Garage Surface which will take
    the role of the feature victim of the paradox (the “X1” from the previous examples):
    we can for example imagine that in fancy areas there is less place for big garages,
    explaining the negative correlation.'
  prefs: []
  type: TYPE_NORMAL
- en: This is completely made-up of course, so don’t take it too seriously, but it
    will allow me to make my point.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: I will skip the feature engineering and cleaning and jump directly to the conclusion.
    I used a fine-tuned **xgboost** to predict y using all the features of the dataset
    (with and without our “Area Fanciness” feature) and check the results in term
    of feature interpretation using the **shap** library, the same way as I would
    do for a classical ML project.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: For those who don’t know the library,* [*shap*](https://github.com/slundberg/shap)
    *is an amazing tool to perform features explanations, particularly efficient for
    tree-based algorithm.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Some keys to interpret the graphs, for those not familiar with shap:*'
  prefs: []
  type: TYPE_NORMAL
- en: '*The features are sorted top to bottom by importance, a feature placed in the
    top of the list have a big impact on the target while features in the bottom of
    the list have a much lower impact.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Each point represent a single sample. Red color indicate that the feature
    has a high value of the given sample while blue indicate a low value.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The more a point is located toward the left side, the more the feature had
    a negative impact for that sample. The more to the right, the more the positive
    impact was*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/002535319ca5739dcc1e7b72121b4ea8.png)'
  prefs: []
  type: TYPE_IMG
- en: Shap values give a different interpretation on the Garage Surface (X1) depending
    on the presence of the Area Fanciness feature (X2)
  prefs: []
  type: TYPE_NORMAL
- en: 'In our scenario, we can observe clearly the paradox: if we forget to include
    the “fancy area” feature, it will bias our interpretation of the results even
    if the scores are not so different. It will have two big consequences:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overestimating the impact of the Garage Surface: much higher in the list without
    Area Fanciness'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Changing our interpretation of the Garage Surface feature: if we don’t consider
    Area Fanciness, the shap model tells us that high garage surface tend to have
    a negative action on the selling price, while the explanation is that a high garage
    surface boost the selling price if you include that Area Fanciness'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case we can understand simply that there is a problem and that a big
    garage should theoretically increase the value of the asset.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, in other situations, with large amount of features harder to interpret,
    you might fall into the paradox without even realizing it, and provide wrong interpretations
    about your results.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article I tried to give you the intuition behind the famous Simpson
    Paradox and highlight some of the dangers that can hide behind some statistical
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: We saw in particular that having the best ML models and the state of the art
    explanation algorithms is not sufficient to avoid it if you don’t have a comprehensive
    view of the data you are studying.
  prefs: []
  type: TYPE_NORMAL
- en: I personally don’t have the solution to avoid this kind of paradox, but as data
    scientists and data analysts, we need to remember that “there is no free lunch”,
    request the help of field experts when possible and always questions our results.
  prefs: []
  type: TYPE_NORMAL
- en: This post is particularly important to me has I really become eager on learning
    more about statistics when I learnt for the first time about this paradox years
    ago, which lead me where I am today.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you enjoyed reading it as much as I enjoyed writing it.
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Appleton, David R., Joyce M. French, and Mark PJ Vanderpump. « Ignoring
    a covariate: An example of Simpson’s paradox. » The American Statistician 50.4
    (1996): 340–341.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[2] Bickel, Peter J., Eugene A. Hammel, and J. William O’Connell. « [Sex bias
    in graduate admissions: Data from Berkeley.](http://www.unc.edu/~nielsen/soci708/cdocs/Berkeley_admissions_bias.pdf)
    » *Science* 187.4175 (1975): 398–404.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
