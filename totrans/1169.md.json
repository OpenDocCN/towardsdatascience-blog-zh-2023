["```py\ntext = \"This is a test text\"\nn = 3 # 1 = Unigram, 2 = bigram, 3 = trigram\ntext_len = len(text)\nnum_ngrams = text_len - n + 1 # How many ngrams of length n will fit in this text\nprint(f\"The text is {text_len} characters long and will fit {num_ngrams} n-grams of length {n}.\")\n\nfor p in range(num_ngrams) :\n            print(f\"{p}: {text[p:p+n]}\")\n```", "```py\nimport typing\n\ndef extract_xgrams(text: str, n_vals: typing.List[int]) -> typing.List[str]:\n    \"\"\"\n    Extract a list of n-grams of different sizes from a text.\n    Params:\n        text: the test from which to extract ngrams\n        n_vals: the sizes of n-grams to extract\n        (e.g. [1, 2, 3] will produce uni-, bi- and tri-grams)\n    \"\"\"\n    xgrams = []\n\n    for n in n_vals:\n        # if n > len(text) then no ngrams will fit, and we would return an empty list\n        if n < len(text):\n            for i in range(len(text) - n + 1) :\n                ng = text[i:i+n]\n                xgrams.append(ng)\n\n    return xgrams\n\ntext = \"I was taught that the way of progress was neither swift nor easy.\".lower()\n# Quote from Marie Curie, the first woman to win a Nobel Prize, the only woman to win it twice, and the only human to win it in two different sciences.\n\n# Extract all ngrams of size 1 to 3.\nxgrams = extract_xgrams(text, n_vals=range(1,4))\n\nprint(xgrams)\n```", "```py\ndef build_model(text: str, n_vals: typing.List[int]) -> typing.Dict[str, int]:\n    \"\"\"\n    Build a simple model of probabilities of xgrams of various lengths in a text\n    Parms:\n        text: the text from which to extract the n_grams\n        n_vals: a list of n_gram sizes to extract\n    Returns:\n        A dictionary of ngrams and their probabilities given the input text\n    \"\"\"\n    model = collections.Counter(extract_xgrams(text, n_vals))  \n    num_ngrams = sum(model.values())\n\n    for ng in model:\n        model[ng] = model[ng] / num_ngrams\n\n    return model\n\ntest_model = build_model(text, n_vals=range(1,4))\nprint({k: v for k, v in sorted(test_model.items(), key=lambda item: item[1], reverse=True)})\n```", "```py\nimport nltk\nnltk.download('udhr') # udhr = Universal Declaration of Human Rights\n\n# Now import corpus and print number of files and the fileids (as these reveal the languages)\nfrom nltk.corpus import udhr \nprint(f\"There are {len(udhr.fileids())} files with the following ids: {udhr.fileids()}\")\n```", "```py\nlanguages = ['english', 'german', 'dutch', 'french', 'italian', 'spanish']\nlanguage_ids = ['English-Latin1', 'German_Deutsch-Latin1', 'Dutch_Nederlands-Latin1', 'French_Francais-Latin1', 'Italian_Italiano-Latin1', 'Spanish_Espanol-Latin1']# I chose the above sample of languages as they all use similar characters. \n\n### Optional: If you want to add more languages:\n\n# First use this function to find the language file id\ndef retrieve_fileid_by_first_letter(fileids, letter):\n    return [id for id in fileids if id.lower().startswith(letter.lower())]\n\n# Example usage\nprint(f\"Fileids beginning with 'R': {retrieve_fileid_by_first_letter(udhr.fileids(), letter='R')}\")\n\n# Then copy-paste the language name and language id into the relevant list:\nlanguages += []\nlanguage_ids += []\n```", "```py\nraw_texts = {language: udhr.raw(language_id) for language, language_id in zip(languages, language_ids)}\nprint(raw_texts['english'][:1000]) # Just print the first 1000 characters\n\n# Build a model of each language\nmodels = {language: build_model(text=raw_texts[language], n_vals=range(1,4)) for language in languages}\nprint(models['german'])\n```", "```py\nimport math\n\ndef calculate_cosine(a: typing.Dict[str, float], b: typing.Dict[str, float]) -> float:\n    \"\"\"\n    Calculate the cosine between two numeric vectors\n    Params:\n        a, b: two dictionaries containing items and their corresponding numeric values\n        (e.g. ngrams and their corresponding probabilities)\n    \"\"\"\n    numerator = sum([a[k]*b[k] for k in a if k in b])\n    denominator = (math.sqrt(sum([a[k]**2 for k in a])) * math.sqrt(sum([b[k]**2 for k in b])))\n    return numerator / denominator\n```", "```py\ndef identify_language(\n    text: str,\n    language_models: typing.Dict[str, typing.Dict[str, float]],\n    n_vals: typing.List[int]\n    ) -> str:\n    \"\"\"\n    Given a text and a dictionary of language models, return the language model \n    whose ngram probabilities best match those of the test text\n    Params:\n        text: the text whose language we want to identify\n        language_models: a Dict of Dicts, where each key is a language name and \n        each value is a dictionary of ngram: probability pairs\n        n_vals: a list of n_gram sizes to extract to build a model of the test \n        text; ideally reflect the n_gram sizes used in 'language_models'\n    \"\"\"\n    text_model = build_model(text, n_vals)\n    language = \"\"\n    max_c = 0\n    for m in language_models:\n        c = calculate_cosine(language_models[m], text_model)\n        # The following line is just for demonstration, and can be deleted\n        print(f'Language: {m}; similarity with test text: {c}')\n        if c > max_c:\n            max_c = c\n            language = m\n    return language\n\nprint(f\"Test text: {text}\")\nprint(f\"Identified language: {identify_language(text, models, n_vals=range(1,4))}\")\n\n# Prints\n# Test text: i was taught that the way of progress was neither swift nor easy.\n# Language: english; similarity with test text: 0.7812347488239613\n# Language: german; similarity with test text: 0.6638235631734796\n# Language: dutch; similarity with test text: 0.6495872103674768\n# Language: french; similarity with test text: 0.7073331083503462\n# Language: italian; similarity with test text: 0.6635204671187273\n# Language: spanish; similarity with test text: 0.6811923819801172\n# Identified language: english\n```", "```py\n# An example text in Slovenian\ntricky_text = \"učili so me, da pot napredka ni ne hitra ne lahka.\"\nprint(f\"Identified language: {identify_language(tricky_text, models, n_vals=range(1,4))}\")\n\n# Prints\n# Language: english; similarity with test text: 0.7287873650203188\n# Language: german; similarity with test text: 0.6721847143945305\n# Language: dutch; similarity with test text: 0.6794130641102911\n# Language: french; similarity with test text: 0.7395592659566902\n# Language: italian; similarity with test text: 0.7673665450525412\n# Language: spanish; similarity with test text: 0.7588017776235897\n# Identified language: italian\n```", "```py\nt = \"mij werd geleerd dat de weg van vooruitgang noch snel noch gemakkelijk is.\"  \nprint(identify_language(t, models, n_vals=range(1,4)))\n\nt = \"on m'a appris que la voie du progrès n'était ni rapide ni facile.\"  \nprint(identify_language(t, models, n_vals=range(1,4)))\n\nt = \"me enseñaron que el camino hacia el progreso no es ni rápido ni fácil.\"\nprint(identify_language(t, models, n_vals=range(1,4)))\n```", "```py\n from nltk.tokenize import word_tokenize  # A function from nltk for splitting strings into individual words\nnltk.download('punkt') # Required for word_tokenize to function\n\nprint(\"Number of characters and words per text per language:\")\nfor language in raw_texts.keys():\n    print(f\"\\n{language}: {len(raw_texts[language])} characters, {len(nltk.word_tokenize(raw_texts[language]))} words\")\n```"]