- en: Byte-Pair Encoding For Beginners
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/byte-pair-encoding-for-beginners-708d4472c0c7](https://towardsdatascience.com/byte-pair-encoding-for-beginners-708d4472c0c7)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An illustrative guide to BPE tokenizer in plain simple language
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mina.ghashami?source=post_page-----708d4472c0c7--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----708d4472c0c7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----708d4472c0c7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----708d4472c0c7--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----708d4472c0c7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----708d4472c0c7--------------------------------)
    ·6 min read·Oct 10, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/82f1a7e623d9991a0f1419ff63b1a6c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we look at one of most well-known tokenization algorithms called
    as Byte-Pair Encoding (BPE). It is used in many state of the art large language
    models such as BERT family, BART, and GPT family.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: Byte-Pair Encoding (BPE)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Byte-Pair Encoding (BPE) is a **corpus-based subword tokenization** algorithm**.**
    It is **corpus based** because it uses the training corpus to learn frequent characters
    (or symbols) and merge them together into one symbol. And it is a **subword tokenizer**
    because it breaks text into units smaller than (or equal to) words.
  prefs: []
  type: TYPE_NORMAL
- en: The image below, shows subword tokenization on the sentence “ it is raining”.
    Note while “it” and “is” are full word tokens; “rain” and “ing” are subwords from
    “raining”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b0b1349cb9ad950a80f5154d761c786f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'BPE algorithm has two main parts: token learner, token segmenter.'
  prefs: []
  type: TYPE_NORMAL
- en: '1- **Token learner**: this takes a corpus of text and creates a vocabulary
    containing tokens. This corpus acts as the training corpus.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3723bdade2749ac3cd543e34ba112533.png)'
  prefs: []
  type: TYPE_IMG
- en: token learner takes a corpus of text and build a vocabulary — image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '2- **Token segmenter**: this takes a piece of text such as sentence and segment
    it into tokens. This text is the test data. We use the learning we gained from
    previous step to tokenize the test data in this step.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/489e8da9aee801fd298bbb188293b477.png)'
  prefs: []
  type: TYPE_IMG
- en: token segmenter converts a sentence to its tokens — image by the author
  prefs: []
  type: TYPE_NORMAL
- en: It is worth to mention that,
  prefs: []
  type: TYPE_NORMAL
- en: “Byte Pair Encoding (BPE) (Gage, 1994) is an old data compression technique
    that iteratively replaces the most frequent pair of bytes in a sequence with a
    single unused byte.”[1]
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The current BPE algorithm that we know for tokenization, adapts this algorithm
    but instead of merging frequent pairs of bytes, it merges frequent characters
    (or character sequences).
  prefs: []
  type: TYPE_NORMAL
- en: The complete pseudocode of the algorithm is below. Let’s learn the algorithm
    step by step by following the pseudo code.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d2f448bd73522609ebb8c179a685cd4.png)'
  prefs: []
  type: TYPE_IMG
- en: BPE pseudocode — image from [[1](https://arxiv.org/pdf/1508.07909.pdf)]
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is highlighted below in a red box: we create a vocab by looking
    at the corpus of documents we have, split them by character and add every character
    with its frequency of occurrence to the vocab.'
  prefs: []
  type: TYPE_NORMAL
- en: We see that our corpus of documents contains *[low, lower, newest, widest]*
    each with different frequency. We use the *</w>* to indicate end of the word.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97914fa271d0cb192451e703f4fe7308.png)'
  prefs: []
  type: TYPE_IMG
- en: Step one — image from [[1](https://arxiv.org/pdf/1508.07909.pdf)] modified by
    author
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second step, for *num_merge=10* times, we call three functions consecutively:'
  prefs: []
  type: TYPE_NORMAL
- en: 'get_stats(): This function returns a dictionary (called pairs) of every character
    pair and their frequency of occurrence in the vocabulary.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'max(): This gets the pairs returned by get_stats() and return the pair with
    maximum occurrence.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'merge_vocab(): treats character pairs as one unit and replace every occurrence
    of character pair as one individual unit. It returns the updated vocabulary.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These three functions together get the most frequent character pairs in the
    corpus, merge them together into one symbol and update the vocabulary. We repeat
    this process for num_merges=10 times.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1387a6909484a49f7719af64a11af8ef.png)'
  prefs: []
  type: TYPE_IMG
- en: Step two— image from [[1](https://arxiv.org/pdf/1508.07909.pdf)] modified by
    author
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s work through the example above. Our corpus is as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4760f88d9013a3118c11e3909f64aa6.png)'
  prefs: []
  type: TYPE_IMG
- en: corpus example — image by author
  prefs: []
  type: TYPE_NORMAL
- en: We build our vocabulary by splitting all words into characters. We add _ at
    the end of the words to indicate *end of the word* before splitting them into
    characters. In the pseudocode, they used </w> symbol to denote end of the word,
    but here we use — .
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06e11fa6bf80d9166ef81360391d7dd7.png)'
  prefs: []
  type: TYPE_IMG
- en: vocabulary from corpus — image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to count frequency of each character in the corpus. To better
    represent the corpus with frequency of characters, we represent it as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6460a209518826129d21ec98c18bfe1e.png)'
  prefs: []
  type: TYPE_IMG
- en: corpus representation — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Note “l o w — 5” mean each character of *l, o, w* and *—* are repeated 5 times
    in the context of the word “low-”.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we find out **which two characters are next to each other most often?**
    We see “e” and “r” are next to each other 8 times and that’s the most. We merge
    them into one symbol “er”, add it to vocabulary and update the corpus representation
    with the new symbol.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6df93d118636e4a285ca3fee763b103f.png)'
  prefs: []
  type: TYPE_IMG
- en: merging “e” and “r” into “er”— Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'We repeat the process: **which two symbols are next to each other most often?**
    we see “w” and “er” occur next to each other 8 times. so we merge them into one
    symbol “wer”*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d38a5e955d772dd6f82edfc10a19113f.png)'
  prefs: []
  type: TYPE_IMG
- en: merging “w” and “er” into “wer” — image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'We repeat the process: the next two symbols that occur the most together are
    “wer” and “—” . we merge them into “wer-” and update the corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/013a6c126c6d0abb66490a2cbcf82863.png)'
  prefs: []
  type: TYPE_IMG
- en: merging “wer” and “—” into “wer-” — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The next two symbols that occur most frequently are “l” and “o” that occur 7
    times. We merge them together into “lo” and update the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/54be5dce5941273762b583706ea8c9c4.png)'
  prefs: []
  type: TYPE_IMG
- en: merging “l” and “o” into “lo” — image by author
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have done 4 merges. We will continue merging symbols together till
    we reach 10 merges. After that, our vocabulary is ready and we can use the segmenter
    algorithm of BPE to tokenize test data. **This is how the segmenter algorithm
    works:**
  prefs: []
  type: TYPE_NORMAL
- en: '**BPE segmenter algorithm:** Run each merged learned from the training data,
    on the test data. These merged learned are symbols we earlier added to the vocabulary.
    Run them one by one on the test data in the order we learned them from the training
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: Note, frequency of character pairs do not matter here, and no additional learning
    is needed. Just scan the test data for the learned vocabulary entries and split
    the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Properties of BPE Tokens
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The way BPE tokenizer works lead to having tokens that are often *frequent words*
    and *frequent subwords.* Frequent subwords are often morphemeslike *est* and *er.*
  prefs: []
  type: TYPE_NORMAL
- en: Morphemes are the smallest unit of language that has meaning. They give clues
    about meaning and grammar of the language.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is important to note that the vocabulary size is configurable by controlling
    how many merge operations are performed. A smaller vocabulary results in more
    frequent subword units.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, the Byte Pair Encoding (BPE) algorithm plays a pivotal role in
    modern NLP by efficiently breaking down text into subword units. Its token learner
    builds a subword vocabulary, while the token segmenter uses this vocabulary for
    text tokenization. Despite some drawbacks, BPE has proven to be highly effective
    in various NLP tasks, and it continues to be a fundamental component in many current
    Large Language Models like GPT-3, BERT, and RoBERTa.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have any questions or suggestions, feel free to reach out to me:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Email: mina.ghashami@gmail.com'
  prefs: []
  type: TYPE_NORMAL
- en: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Japanese and Korean Voice Search](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Subword Regularization: Improving Neural Network Translation Models with Multiple
    Subword Candidates](https://arxiv.org/pdf/1804.10959.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://smltar.com/tokenization.html](https://smltar.com/tokenization.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[SentencePiece: A simple and language independent subword tokenizer and detokenizer
    for Neural Text Processing](https://aclanthology.org/D18-2012.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
