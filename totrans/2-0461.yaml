- en: Byte-Pair Encoding For Beginners
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字节对编码初学者指南
- en: 原文：[https://towardsdatascience.com/byte-pair-encoding-for-beginners-708d4472c0c7](https://towardsdatascience.com/byte-pair-encoding-for-beginners-708d4472c0c7)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/byte-pair-encoding-for-beginners-708d4472c0c7](https://towardsdatascience.com/byte-pair-encoding-for-beginners-708d4472c0c7)
- en: An illustrative guide to BPE tokenizer in plain simple language
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一份通俗易懂的BPE标记器指南
- en: '[](https://medium.com/@mina.ghashami?source=post_page-----708d4472c0c7--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----708d4472c0c7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----708d4472c0c7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----708d4472c0c7--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----708d4472c0c7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mina.ghashami?source=post_page-----708d4472c0c7--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----708d4472c0c7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----708d4472c0c7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----708d4472c0c7--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----708d4472c0c7--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----708d4472c0c7--------------------------------)
    ·6 min read·Oct 10, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----708d4472c0c7--------------------------------)
    ·6分钟阅读·2023年10月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/82f1a7e623d9991a0f1419ff63b1a6c4.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82f1a7e623d9991a0f1419ff63b1a6c4.png)'
- en: Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: In this article, we look at one of most well-known tokenization algorithms called
    as Byte-Pair Encoding (BPE). It is used in many state of the art large language
    models such as BERT family, BART, and GPT family.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将深入了解一种最著名的标记化算法，即字节对编码（BPE）。它被广泛应用于许多最先进的大型语言模型中，如BERT家族、BART和GPT家族。
- en: Let’s get started.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Byte-Pair Encoding (BPE)
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字节对编码（BPE）
- en: Byte-Pair Encoding (BPE) is a **corpus-based subword tokenization** algorithm**.**
    It is **corpus based** because it uses the training corpus to learn frequent characters
    (or symbols) and merge them together into one symbol. And it is a **subword tokenizer**
    because it breaks text into units smaller than (or equal to) words.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 字节对编码（BPE）是一种**基于语料库的子词标记化**算法。**它是**基于语料库的，因为它使用训练语料库来学习频繁的字符（或符号）并将其合并成一个符号。它也是一种**子词标记器**，因为它将文本分解为比（或等于）单词更小的单元。
- en: The image below, shows subword tokenization on the sentence “ it is raining”.
    Note while “it” and “is” are full word tokens; “rain” and “ing” are subwords from
    “raining”.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图像展示了句子“ it is raining”的子词标记化。请注意，虽然“it”和“is”是完整的单词标记；“rain”和“ing”是“raining”的子词。
- en: '![](../Images/b0b1349cb9ad950a80f5154d761c786f.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0b1349cb9ad950a80f5154d761c786f.png)'
- en: 'BPE algorithm has two main parts: token learner, token segmenter.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: BPE算法有两个主要部分：标记学习器、标记分割器。
- en: '1- **Token learner**: this takes a corpus of text and creates a vocabulary
    containing tokens. This corpus acts as the training corpus.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 1- **标记学习器**：它处理一组文本并创建一个包含标记的词汇表。这些语料库作为训练语料库。
- en: '![](../Images/3723bdade2749ac3cd543e34ba112533.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3723bdade2749ac3cd543e34ba112533.png)'
- en: token learner takes a corpus of text and build a vocabulary — image by the author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 标记学习器处理一组文本并建立一个词汇表 —— 图片由作者提供
- en: '2- **Token segmenter**: this takes a piece of text such as sentence and segment
    it into tokens. This text is the test data. We use the learning we gained from
    previous step to tokenize the test data in this step.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 2- **标记分割器**：它将一段文本（如句子）分割成标记。该文本是测试数据。我们利用从前一步中获得的学习，在这一步中对测试数据进行标记化。
- en: '![](../Images/489e8da9aee801fd298bbb188293b477.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/489e8da9aee801fd298bbb188293b477.png)'
- en: token segmenter converts a sentence to its tokens — image by the author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 标记分割器将句子转换为其标记 —— 图片由作者提供
- en: It is worth to mention that,
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，
- en: “Byte Pair Encoding (BPE) (Gage, 1994) is an old data compression technique
    that iteratively replaces the most frequent pair of bytes in a sequence with a
    single unused byte.”[1]
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “字节对编码（BPE）（Gage，1994）是一种旧的数据压缩技术，它通过迭代替换序列中最频繁的字节对，用一个未使用的字节。”[1]
- en: The current BPE algorithm that we know for tokenization, adapts this algorithm
    but instead of merging frequent pairs of bytes, it merges frequent characters
    (or character sequences).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道的当前BPE算法用于分词，适应了这个算法，但不是合并频繁的字节对，而是合并频繁的字符（或字符序列）。
- en: The complete pseudocode of the algorithm is below. Let’s learn the algorithm
    step by step by following the pseudo code.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的完整伪代码如下。让我们按照伪代码一步一步地学习这个算法。
- en: '![](../Images/0d2f448bd73522609ebb8c179a685cd4.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d2f448bd73522609ebb8c179a685cd4.png)'
- en: BPE pseudocode — image from [[1](https://arxiv.org/pdf/1508.07909.pdf)]
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: BPE伪代码 — 图像来源 [[1](https://arxiv.org/pdf/1508.07909.pdf)]
- en: 'The first step is highlighted below in a red box: we create a vocab by looking
    at the corpus of documents we have, split them by character and add every character
    with its frequency of occurrence to the vocab.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第一阶段如下图红框中突出显示：我们通过查看我们拥有的文档语料库来创建一个词汇表，按字符拆分它们，并将每个字符及其出现频率添加到词汇表中。
- en: We see that our corpus of documents contains *[low, lower, newest, widest]*
    each with different frequency. We use the *</w>* to indicate end of the word.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到我们的文档语料库包含* [low, lower, newest, widest]*，每个的频率不同。我们使用*</w>*来表示单词的结束。
- en: '![](../Images/97914fa271d0cb192451e703f4fe7308.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97914fa271d0cb192451e703f4fe7308.png)'
- en: Step one — image from [[1](https://arxiv.org/pdf/1508.07909.pdf)] modified by
    author
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步 — 图像来源 [[1](https://arxiv.org/pdf/1508.07909.pdf)] 由作者修改
- en: 'In the second step, for *num_merge=10* times, we call three functions consecutively:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二步中，对于*num_merge=10*次，我们依次调用三个函数：
- en: 'get_stats(): This function returns a dictionary (called pairs) of every character
    pair and their frequency of occurrence in the vocabulary.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: get_stats()：此函数返回一个字典（称为pairs），其中包含词汇表中每对字符及其出现频率。
- en: 'max(): This gets the pairs returned by get_stats() and return the pair with
    maximum occurrence.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: max()：此函数获取get_stats()返回的字符对，并返回出现频率最高的对。
- en: 'merge_vocab(): treats character pairs as one unit and replace every occurrence
    of character pair as one individual unit. It returns the updated vocabulary.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: merge_vocab()：将字符对视为一个单位，将每个字符对的所有出现替换为一个单独的单位。它返回更新后的词汇表。
- en: These three functions together get the most frequent character pairs in the
    corpus, merge them together into one symbol and update the vocabulary. We repeat
    this process for num_merges=10 times.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个函数一起获取语料库中最常见的字符对，将它们合并为一个符号并更新词汇表。我们将此过程重复num_merges=10次。
- en: '![](../Images/1387a6909484a49f7719af64a11af8ef.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1387a6909484a49f7719af64a11af8ef.png)'
- en: Step two— image from [[1](https://arxiv.org/pdf/1508.07909.pdf)] modified by
    author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步 — 图像来源 [[1](https://arxiv.org/pdf/1508.07909.pdf)] 由作者修改
- en: 'Let’s work through the example above. Our corpus is as following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过上述示例进行演练。我们的语料库如下：
- en: '![](../Images/a4760f88d9013a3118c11e3909f64aa6.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4760f88d9013a3118c11e3909f64aa6.png)'
- en: corpus example — image by author
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库示例 — 图像由作者提供
- en: We build our vocabulary by splitting all words into characters. We add _ at
    the end of the words to indicate *end of the word* before splitting them into
    characters. In the pseudocode, they used </w> symbol to denote end of the word,
    but here we use — .
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将所有单词拆分为字符来构建词汇表。在单词末尾添加_以表示*单词的结束*，然后将其拆分为字符。在伪代码中，他们使用</w>符号表示单词的结束，但在这里我们使用—。
- en: '![](../Images/06e11fa6bf80d9166ef81360391d7dd7.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06e11fa6bf80d9166ef81360391d7dd7.png)'
- en: vocabulary from corpus — image by author
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库中的词汇表 — 图像由作者提供
- en: 'Next, we need to count frequency of each character in the corpus. To better
    represent the corpus with frequency of characters, we represent it as following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要计算语料库中每个字符的频率。为了更好地表示字符频率的语料库，我们将其表示如下：
- en: '![](../Images/6460a209518826129d21ec98c18bfe1e.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6460a209518826129d21ec98c18bfe1e.png)'
- en: corpus representation — Image by author
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库表示 — 图像由作者提供
- en: Note “l o w — 5” mean each character of *l, o, w* and *—* are repeated 5 times
    in the context of the word “low-”.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 注意“l o w — 5”表示在“low-”这个词的上下文中，*l, o, w* 和 *—* 每个字符都重复了5次。
- en: Next, we find out **which two characters are next to each other most often?**
    We see “e” and “r” are next to each other 8 times and that’s the most. We merge
    them into one symbol “er”, add it to vocabulary and update the corpus representation
    with the new symbol.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们找出**哪个字符对最常相邻？** 我们看到“e”和“r”最常相邻，共出现8次。因此，我们将它们合并为一个符号“er”，将其添加到词汇表中，并用新符号更新语料库表示。
- en: '![](../Images/6df93d118636e4a285ca3fee763b103f.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6df93d118636e4a285ca3fee763b103f.png)'
- en: merging “e” and “r” into “er”— Image by author
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 合并“e”和“r”为“er” — 图像由作者提供
- en: 'We repeat the process: **which two symbols are next to each other most often?**
    we see “w” and “er” occur next to each other 8 times. so we merge them into one
    symbol “wer”*.*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复这个过程：**接下来哪两个符号最常紧挨在一起？** 我们发现“w”和“er”最常紧挨在一起出现 8 次。因此，我们将它们合并为一个符号“wer”*。
- en: '![](../Images/d38a5e955d772dd6f82edfc10a19113f.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d38a5e955d772dd6f82edfc10a19113f.png)'
- en: merging “w” and “er” into “wer” — image by author
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 合并“w”和“er”为“wer” — 图片来自作者
- en: 'We repeat the process: the next two symbols that occur the most together are
    “wer” and “—” . we merge them into “wer-” and update the corpus:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复这个过程：接下来最常一起出现的两个符号是“wer”和“—”。我们将它们合并为“wer-”并更新语料库：
- en: '![](../Images/013a6c126c6d0abb66490a2cbcf82863.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/013a6c126c6d0abb66490a2cbcf82863.png)'
- en: merging “wer” and “—” into “wer-” — Image by author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 合并“wer”和“—”为“wer-” — 图片来自作者
- en: The next two symbols that occur most frequently are “l” and “o” that occur 7
    times. We merge them together into “lo” and update the vocabulary.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 频率最高的两个符号是“l”和“o”，它们出现了 7 次。我们将它们合并为“lo”并更新词汇表。
- en: '![](../Images/54be5dce5941273762b583706ea8c9c4.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54be5dce5941273762b583706ea8c9c4.png)'
- en: merging “l” and “o” into “lo” — image by author
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 合并“l”和“o”为“lo” — 图片来自作者
- en: So far, we have done 4 merges. We will continue merging symbols together till
    we reach 10 merges. After that, our vocabulary is ready and we can use the segmenter
    algorithm of BPE to tokenize test data. **This is how the segmenter algorithm
    works:**
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经进行了 4 次合并。我们将继续将符号合并，直到达到 10 次合并。之后，我们的词汇表就准备好了，可以使用 BPE 的分词算法对测试数据进行分词。**这就是分词算法的工作原理：**
- en: '**BPE segmenter algorithm:** Run each merged learned from the training data,
    on the test data. These merged learned are symbols we earlier added to the vocabulary.
    Run them one by one on the test data in the order we learned them from the training
    data.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**BPE 分词器算法：** 在测试数据上运行每个从训练数据中学习到的合并。这些已学习的合并是我们之前添加到词汇表中的符号。按我们从训练数据中学到的顺序逐一在测试数据上运行它们。'
- en: Note, frequency of character pairs do not matter here, and no additional learning
    is needed. Just scan the test data for the learned vocabulary entries and split
    the sequence.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，字符对的频率在这里并不重要，也不需要额外的学习。只需扫描测试数据以查找已学得的词汇条目，并拆分序列。
- en: Properties of BPE Tokens
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BPE Tokens 的属性
- en: The way BPE tokenizer works lead to having tokens that are often *frequent words*
    and *frequent subwords.* Frequent subwords are often morphemeslike *est* and *er.*
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: BPE 分词器的工作方式会导致生成的 tokens 常常是*频繁出现的词汇*和*频繁出现的子词*。频繁出现的子词通常是像*est*和*er*这样的词素。
- en: Morphemes are the smallest unit of language that has meaning. They give clues
    about meaning and grammar of the language.
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 词素是具有意义的最小语言单位。它们提供关于语言的意义和语法的线索。
- en: It is important to note that the vocabulary size is configurable by controlling
    how many merge operations are performed. A smaller vocabulary results in more
    frequent subword units.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，词汇表的大小可以通过控制合并操作的数量来配置。较小的词汇表会导致更频繁的子词单元。
- en: Summary
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In conclusion, the Byte Pair Encoding (BPE) algorithm plays a pivotal role in
    modern NLP by efficiently breaking down text into subword units. Its token learner
    builds a subword vocabulary, while the token segmenter uses this vocabulary for
    text tokenization. Despite some drawbacks, BPE has proven to be highly effective
    in various NLP tasks, and it continues to be a fundamental component in many current
    Large Language Models like GPT-3, BERT, and RoBERTa.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，字节对编码（BPE）算法在现代自然语言处理（NLP）中发挥了关键作用，通过高效地将文本拆分为子词单元。其 token 学习器建立了一个子词词汇表，而
    token 分词器使用这个词汇表进行文本分词。尽管存在一些缺点，BPE 在各种 NLP 任务中已经证明了其高效性，并且仍然是许多当前大型语言模型（如 GPT-3、BERT
    和 RoBERTa）的基础组件。
- en: 'If you have any questions or suggestions, feel free to reach out to me:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何问题或建议，请随时与我联系：
- en: 'Email: mina.ghashami@gmail.com'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件：mina.ghashami@gmail.com
- en: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
- en: References
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf)'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[稀有词汇的神经机器翻译与子词单元](https://arxiv.org/pdf/1508.07909.pdf)'
- en: '[Japanese and Korean Voice Search](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[日语和韩语语音搜索](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)'
- en: '[Subword Regularization: Improving Neural Network Translation Models with Multiple
    Subword Candidates](https://arxiv.org/pdf/1804.10959.pdf)'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[子词正则化：通过多子词候选项提升神经网络翻译模型](https://arxiv.org/pdf/1804.10959.pdf)'
- en: '[https://smltar.com/tokenization.html](https://smltar.com/tokenization.html)'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://smltar.com/tokenization.html](https://smltar.com/tokenization.html)'
- en: '[SentencePiece: A simple and language independent subword tokenizer and detokenizer
    for Neural Text Processing](https://aclanthology.org/D18-2012.pdf)'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[SentencePiece：一个简单且语言无关的子词分词器和去分词器，用于神经文本处理](https://aclanthology.org/D18-2012.pdf)'
