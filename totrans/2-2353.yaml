- en: Where Are All the Women?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 所有的女性都在哪里？
- en: 原文：[https://towardsdatascience.com/where-are-all-the-women-3c79dabfdfc2](https://towardsdatascience.com/where-are-all-the-women-3c79dabfdfc2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/where-are-all-the-women-3c79dabfdfc2](https://towardsdatascience.com/where-are-all-the-women-3c79dabfdfc2)
- en: Exploring large language models’ biases in historical knowledge
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索大型语言模型在历史知识中的偏见
- en: '[](https://medium.com/@artfish?source=post_page-----3c79dabfdfc2--------------------------------)[![Yennie
    Jun](../Images/b635e965f21c3d55833269e12e861322.png)](https://medium.com/@artfish?source=post_page-----3c79dabfdfc2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3c79dabfdfc2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3c79dabfdfc2--------------------------------)
    [Yennie Jun](https://medium.com/@artfish?source=post_page-----3c79dabfdfc2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@artfish?source=post_page-----3c79dabfdfc2--------------------------------)[![Yennie
    Jun](../Images/b635e965f21c3d55833269e12e861322.png)](https://medium.com/@artfish?source=post_page-----3c79dabfdfc2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3c79dabfdfc2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3c79dabfdfc2--------------------------------)
    [Yennie Jun](https://medium.com/@artfish?source=post_page-----3c79dabfdfc2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3c79dabfdfc2--------------------------------)
    ·10 min read·Jul 26, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3c79dabfdfc2--------------------------------)
    ·10分钟阅读·2023年7月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/84e3040a1892146c1f28f18f09536c4d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84e3040a1892146c1f28f18f09536c4d.png)'
- en: A few of the top historical figures mentioned the most often by the GPT-4 and
    Claude. Individual images sourced from Wikipedia. Collage created by the author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4和Claude最常提到的一些顶级历史人物。单个图像来源于维基百科。拼贴画由作者创建。
- en: '*(This article was originally posted on* [*my personal blog*](https://www.artfish.ai/p/where-are-all-the-women)*)*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*(这篇文章最初发布在* [*我的个人博客*](https://www.artfish.ai/p/where-are-all-the-women)*)*'
- en: Large language models (LLMs) such as ChatGPT are being increasingly used in
    educational and professional settings. It is important to understand and study
    the many biases present in such models before integrating them into existing applications
    and our daily lives.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）如ChatGPT在教育和专业环境中被越来越多地使用。在将这些模型整合到现有应用程序和日常生活中之前，了解和研究其中存在的多种偏见非常重要。
- en: One of the biases I studied in my [previous article](https://blog.yenniejun.com/p/world-history-through-ai)
    was regarding historical events. I probed LLMs to understand what historical knowledge
    they encoded in the form of major historical events. I found that they encoded
    a serious Western bias towards understanding major historical events.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我[之前的文章](https://blog.yenniejun.com/p/world-history-through-ai)中研究的一个偏见是关于历史事件的。我探讨了LLMs了解历史事件的方式。我发现它们在编码主要历史事件时存在严重的西方偏见。
- en: On a similar vein, in this article, I probe language models regarding their
    understanding of important historical figures. I asked two LLMs who the most important
    historical people in history were. I repeated this process 10 times for 10 different
    languages. Some names, like Gandhi and Jesus, appeared extremely frequently. Other
    names, like Marie Curie or Cleopatra, appeared less frequently. Compared to the
    number of male names generated by the models, there were extremely few female
    names.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在这篇文章中，我探讨了语言模型对重要历史人物的理解。我询问了两个LLM历史上最重要的人物是谁。我对10种不同语言进行了10次重复的过程。一些名字，比如甘地和耶稣，出现得非常频繁。其他名字，如居里夫人或克利奥帕特拉，出现得较少。与模型生成的男性名字相比，女性名字的出现频率极低。
- en: 'The biggest question I had was: Where were all the women?'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾经最大的问题是：所有的女性都在哪里？
- en: 'Continuing the theme of evaluating historical biases encoded by language models,
    I probed [OpenAI’s GPT-4](https://openai.com/gpt-4) and [Anthropic’s Claude](https://www.anthropic.com/index/introducing-claude)
    regarding major historical figures. In this article, I show how both models contain:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 继续评估语言模型编码的历史偏见这一主题，我探讨了[OpenAI的GPT-4](https://openai.com/gpt-4)和[Anthropic的Claude](https://www.anthropic.com/index/introducing-claude)对主要历史人物的理解。在这篇文章中，我展示了这两个模型都包含：
- en: 'Gender bias: Both models disproportionately predict male historical figures.
    **GPT-4 generated the names of female historical figures 5.4% of the time and
    Claude did so 1.8% of the time**. This pattern held across all 10 languages.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性别偏见：这两个模型都不成比例地预测男性历史人物。**GPT-4 生成女性历史人物的频率为5.4%，而Claude 的频率为1.8%**。这一模式在所有10种语言中均存在。
- en: 'Geographic bias: Regardless of the language the model was prompted in, there
    was a bias towards predicting Western historical figures. **GPT-4 generated historical
    figures from Europe 60% of the time and Claude did so 52% of the time.**'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 地理偏见：无论模型是在什么语言下进行提示，都存在偏向于预测西方历史人物的现象。**GPT-4 在60%的时间里生成了来自欧洲的历史人物，而Claude
    在52%的时间里也这样做了。**
- en: 'Language bias: Certain languages suffered from gender or geographic biases
    more. For example, **when prompted in Russian, both GPT-4 and Claude generated
    zero women across all of my experiments.** Additionally, language quality was
    lower for some languages. For example, when prompted in Arabic, the models were
    more likely to respond incorrectly by generating famous *locations* instead of
    people.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言偏见：某些语言在性别或地理偏见上受到的影响更大。例如，**当使用俄语进行提示时，无论是GPT-4 还是Claude 在我的所有实验中都没有生成女性。**
    另外，一些语言的语言质量较低。例如，当使用阿拉伯语进行提示时，模型更容易错误地生成著名的*地点*而不是人物。
- en: Experiments
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验
- en: I prompted OpenAI’s GPT-4 and Anthropic’s Claude in 10 different languages (English,
    Korean, Chinese, Japanese, Spanish, French, Italian, German, Russian, and Arabic)
    to list out the top 10 important historical figures. The original prompt and translations
    can be found at the end of the article.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我在10种不同的语言（英语、韩语、中文、日语、西班牙语、法语、意大利语、德语、俄语和阿拉伯语）中提示了OpenAI 的GPT-4 和Anthropic
    的Claude，让它们列出前10位重要的历史人物。原始提示和翻译可以在文章末尾找到。
- en: I took all of the generated names, translated them into English, and standardized
    them into the same version. I found each name on Wikipedia to obtain metadata
    about that person, such as their country of origin, gender, and profession. I
    used that information to conduct the analyses in this article. A more detailed
    technical explanation of this process can be found at the end of the article.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我将所有生成的名字翻译成英语，并将其标准化为相同版本。我在维基百科上查找每个名字，以获取有关该人物的元数据，如他们的出生国家、性别和职业。我使用这些信息来进行本文中的分析。有关此过程的更详细的技术说明可以在文章末尾找到。
- en: Who were the most popular men?
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谁是最受欢迎的男性？
- en: '![](../Images/f66073f90b37ba7545a8588991492cf2.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f66073f90b37ba7545a8588991492cf2.png)'
- en: 'Word cloud of top historical figures generated by two popular Large Language
    Models: GPT-4 and Claude. Image generated by author.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 两种流行的大型语言模型（GPT-4 和Claude）生成的主要历史人物的词云。图像由作者生成。
- en: For each of the two models, I picked the historical figures who were generated
    at least 8 out of the 10 times for at least one of the prompted languages.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两个模型，我选择了在至少一种提示语言下生成了至少8次的历史人物。
- en: The top historical figures are almost all entirely men. Can you spot the one
    woman?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最主要的历史人物几乎全部是男性。你能找出唯一的女性吗？
- en: GPT-4 consistently generated figures such as Gandhi, Martin Luther King Jr.,
    and Einstein across most languages. (Note that the reason some of the scores are
    11 is because, once in a while, a model would generate the same figure twice in
    its list of top 10 historical figures)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 在大多数语言中一致地生成了如甘地、马丁·路德·金和爱因斯坦等人物。（注意，有些分数为11的原因是因为有时模型会在其前10个历史人物列表中生成相同的人物两次）
- en: '![](../Images/574af148923434536e2cd2bbbbc9f3fc.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/574af148923434536e2cd2bbbbc9f3fc.png)'
- en: '*Top historical figures generated by GPT-4, and the number of times generated
    per language. Heatmap generated by author.*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*GPT-4 生成的主要历史人物，以及每种语言生成的次数。热图由作者生成。*'
- en: '**Claude generated more religious and philosophical figures** such as Jesus,
    the Buddha, Muhammad, and Confucius. Note some interesting patterns: when prompted
    in English, German, and Spanish, Claude generated Muhammad 90–100% of the time.
    When prompted in Arabic, Claude generated Muhammad 0% of the time. Also, note
    the appearance of Mao Zedong almost only when prompted in Chinese.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**Claude 生成了更多宗教和哲学人物**，如耶稣、佛陀、穆罕默德和孔子。注意一些有趣的模式：当使用英语、德语和西班牙语进行提示时，Claude
    90-100%的时间生成了穆罕默德。而在使用阿拉伯语时，Claude 0%的时间生成穆罕默德。另外，注意毛泽东几乎只在使用中文进行提示时出现。'
- en: '![](../Images/3b623ad97b9a1bdbd61e72242d2b4790.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b623ad97b9a1bdbd61e72242d2b4790.png)'
- en: '*Top historical figures generated by Claude, and the number of times generated
    per language. Heatmap generated by author.*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*Claude 生成的顶级历史人物及每种语言的生成次数。热力图由作者生成。*'
- en: Diversity of figures varies for language
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 历史人物的多样性因语言而异
- en: I prompted both Claude and GPT-4 10 times for each of the 10 languages, which
    resulted in many historical figures being repeated across languages.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我分别用 10 种语言提示了 Claude 和 GPT-4 各 10 次，这导致许多历史人物在语言间重复出现。
- en: But, looking at just the unique historical figures predicted per language, how
    *diverse* are the predictions? That is, does each language model generate the
    same few historical figures, or do they generate across a more diverse spectrum?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 但仅看每种语言预测的独特历史人物，其*多样性*如何？即，每个语言模型是否生成相同的少数几个历史人物，还是生成了更广泛的多样性？
- en: This depends on the language and the model. **Languages such as French, Spanish,
    and German generated less diverse historical figures.** Languages such as Korean,
    Chinese, and Arabic generated more variety. Interestingly, for some languages,
    GPT-4 was more diverse, and in others, Claude was more diverse. There was no clear
    pattern on one model generating more diverse historical figures overall.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这取决于语言和模型。**法语、西班牙语和德语生成的历史人物多样性较低。** 而韩语、中文和阿拉伯语生成了更多的多样性。有趣的是，对于某些语言，GPT-4
    的多样性更高，而在其他语言中，Claude 更具多样性。没有明显的模式表明某一模型整体上生成了更多多样化的历史人物。
- en: '![](../Images/37518bc0d8ce715d26ce4a47e904daee.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37518bc0d8ce715d26ce4a47e904daee.png)'
- en: '*The number of unique historical people generated by each model, broken down
    by the language the model was prompted in. For Arabic, the dots overlap, indicating
    that both models generated the same number of unique people. Plot generated by
    author.*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*按语言模型提示的每种模型生成的独特历史人物数量。对于阿拉伯语，点重叠，表示两个模型生成的独特人物数量相同。图表由作者生成。*'
- en: Gender Bias
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性别偏见
- en: Looking at overall figures predicted, GPT-4 generated female historical figures
    5.4% of the time and Claude did so 1.8% of the time.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 从总体预测数据来看，GPT-4 生成女性历史人物的频率为 5.4%，而 Claude 为 1.8%。
- en: Since the same figures were predicted multiple times, I looked at the narrower
    set of *unique* historical figures. **For unique historical figures, GPT-4 generated
    female figures 14.0% of the time and Claude did so 4.9% of the time.**
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于相同的历史人物被多次预测，我查看了较窄范围的*独特*历史人物。**对于独特的历史人物，GPT-4 生成女性人物的比例为 14.0%，而 Claude
    为 4.9%。**
- en: '![](../Images/b346c526b254e0b848b060565002e390.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b346c526b254e0b848b060565002e390.png)'
- en: '*Gender breakdown of unique historical figures generated by GPT-4 and Claude.
    Plot generated by author.*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*GPT-4 和 Claude 生成的独特历史人物的性别分布。图表由作者生成。*'
- en: '**Breakdown by language**'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**按语言分解**'
- en: I find the breakdown by language and model insightful. When prompted in certain
    languages (e.g. Russian), the language model generated **zero important historical
    figures who were female** (not even Catherine the Great!). This propensity to
    generate more male or female historical figures varied greatly per language.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现按语言和模型进行的分解很有启发性。当提示使用某些语言时（例如俄语），语言模型生成了**零个重要的女性历史人物**（甚至连叶卡捷琳娜大帝都没有！）。生成更多男性或女性历史人物的倾向在不同语言间差异很大。
- en: '**GPT-4**'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**GPT-4**'
- en: '**For GPT-4, the proportion of female historical figures varied by language:
    20% for English and 0% for Russian.**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**对于 GPT-4，女性历史人物的比例因语言而异：英语为 20%，俄语为 0%。**'
- en: The female historical figures (ordered by number of times generated):Cleopatra
    (16), Marie Curie (14), Victoria of the United Kingdom (5), Elizabeth I of England
    (4), Rosa Parks (3), Joan of Arc (3), Virginia Woolf (1), Virgin Mary (1), Mother
    Teresa (1), Diana, Princess of Wales (1), Isabella I of Castile (1), Benazir Bhutto
    (1), Frida Kahlo (1), Elizabeth II (1)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 女性历史人物（按生成次数排序）：克利奥帕特拉（16 次）、玛丽·居里（14 次）、维多利亚女王（5 次）、英格兰的伊丽莎白一世（4 次）、罗莎·帕克斯（3
    次）、贞德（3 次）、弗吉尼亚·伍尔夫（1 次）、圣母玛利亚（1 次）、特蕾莎修女（1 次）、威尔士的戴安娜王妃（1 次）、卡斯蒂利亚的伊莎贝拉一世（1 次）、贝娜齐尔·布托（1
    次）、弗里达·卡罗（1 次）、伊丽莎白二世（1 次）
- en: '![](../Images/06320a446b51d53e37cf77bde85b344e.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06320a446b51d53e37cf77bde85b344e.png)'
- en: '*Gender breakdown of unique historical figures generated by GPT-4, separated
    by language model was prompted in. Plot generated by author.*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*GPT-4 生成的独特历史人物按语言模型分开的性别分布。图表由作者生成。*'
- en: '**Claude**'
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Claude**'
- en: '**Claude, trained for safety, generated fewer women than GPT-4\.** In fact,
    **zero female historical figures were generated when prompted in English.**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**Claude，经过安全训练，生成的女性人数少于 GPT-4。** 实际上，**在英语提示下生成了零个女性历史人物。**'
- en: 'The female historical figures (ordered by number of times generated): Cleopatra
    (8), Marie Curie (3), Mother Teresa (2), Eleanor Roosevelt (1), Margaret Thatcher
    (1), Hippolyta (1), Yu Gwansun (1)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 女性历史人物（按生成次数排序）：克利奥帕特拉（8），玛丽·居里（3），特蕾莎修女（2），埃莉诺·罗斯福（1），玛格丽特·撒切尔（1），希波吕忒（1），柳寬順（1）
- en: '* One of the female historical figures is [Hippolyta](https://en.wikipedia.org/wiki/Hippolyta),
    who is a mythological figure'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*其中一位女性历史人物是[希波吕忒](https://en.wikipedia.org/wiki/Hippolyta)，她是一个神话人物'
- en: '![](../Images/7388d7eba35490bcdcffcaeef330bf84.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7388d7eba35490bcdcffcaeef330bf84.png)'
- en: '*Gender breakdown of unique historical figures generated by Claude, separated
    by language model was prompted in. Plot generated by author.*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*Claude生成的独特历史人物按语言模型分类的性别分布。图表由作者生成。*'
- en: '**The models exhibited gender bias, but not any more than what is already on
    the Internet.** Yes, both of the LLMs disproportionately generated male historical
    figures. But should this even be surprising, considering what we find on the Internet,
    and the fact that language models are mostly trained on text from the Internet?'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**这些模型表现出性别偏见，但不比互联网上已有的偏见多。** 是的，这两个LLM确实不成比例地生成了男性历史人物。但考虑到我们在互联网上发现的内容，以及语言模型主要训练于互联网上的文本，这是否值得惊讶呢？'
- en: 'I found three different “top 100 historical figures” lists on the Internet:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我在互联网上找到三个不同的“前100位历史人物”列表：
- en: A [1978 book](https://history.fandom.com/wiki/The_100:_A_Ranking_of_the_Most_Influential_Persons_in_History)
    titled “100 Most Influential People in the World” by Michael H. Hart contained
    2 women (Queen Elizabeth I, Queen Isabella I)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1978年书籍](https://history.fandom.com/wiki/The_100:_A_Ranking_of_the_Most_Influential_Persons_in_History)《世界上最有影响力的100人》由迈克尔·H·哈特撰写，其中包含2位女性（伊丽莎白一世，伊莎贝拉一世）'
- en: A [2013 Times list](https://ideas.time.com/2013/12/10/whos-biggest-the-100-most-significant-figures-in-history/)
    of “The 100 Most Significant Figures in History” contained 3 women (Queen Elizabeth
    I, Queen Victoria, and Joan of Arc)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2013年《时代》名单](https://ideas.time.com/2013/12/10/whos-biggest-the-100-most-significant-figures-in-history/)的“历史上最重要的100人”包含3位女性（伊丽莎白一世，维多利亚女王和贞德）'
- en: A [2019 Biography Online](https://www.biographyonline.net/people/famous-100.html)
    “List of Top 100 Famous People” contained 26 women
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2019年传记在线](https://www.biographyonline.net/people/famous-100.html)的“100位名人榜单”包含26位女性'
- en: Geographic bias
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 地理偏见
- en: Looking at the unique people predicted, what proportion of historical figures
    predicted by each LLM came from different global subregions? ([Subregions are
    based on Wikipedia’s categorization](https://en.wikipedia.org/wiki/Subregion#:~:text=A%20subregion%20is%20a%20part%20of%20a%20larger%20region%20or%20continent%20and%20is%20usually%20based%20on%20location)).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 看看预测出的独特人物，各个LLM预测的历史人物中有多少比例来自不同的全球子区域？（[子区域基于维基百科的分类](https://en.wikipedia.org/wiki/Subregion#:~:text=A%20subregion%20is%20a%20part%20of%20a%20larger%20region%20or%20continent%20and%20is%20usually%20based%20on%20location)）
- en: I expected a bit of a Western bias ([considering the Western bias in LLMs’ understanding
    of historical events](https://blog.yenniejun.com/p/world-history-through-ai)).
    And indeed, a third of the unique people generated by GPT-4 were from Western
    Europe or North America.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我预期有一点西方偏见（[考虑到LLM对历史事件的西方偏见](https://blog.yenniejun.com/p/world-history-through-ai)）。确实，GPT-4生成的独特人物中有三分之一来自西欧或北美。
- en: What surprised me was that 28% of unique people generated by Claude were from
    Eastern Asia!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我惊讶的是，Claude生成的独特人物中有28%来自东亚！
- en: '![](../Images/ee5255ebdc48bf90e0b5d27c01b8ea91.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee5255ebdc48bf90e0b5d27c01b8ea91.png)'
- en: '*Subregion breakdown of unique historical figures generated by GPT-4 and Claude.
    Plot generated by author.*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*GPT-4和Claude生成的独特历史人物的子区域分类。图表由作者生成。*'
- en: Of the many Eastern Asian figures generated by Claude, the majority were Chinese
    (25 from China, 3 from Korea, 2 from Japan, 1 each from Mongolia, Taiwan, and
    Tibet).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Claude生成的众多东亚人物中，大多数是中国人（来自中国25人，韩国3人，日本2人，蒙古、台湾和西藏各1人）。
- en: While Claude generated many *unique* Eastern Asian figures, these figures are
    actually one-offs that the model only generated infrequently. This becomes clear
    upon looking at the overall number of people predicted by each model. The predicted
    figures are more likely to be Western and Southern Europe.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Claude生成了许多*独特*的东亚人物，但这些人物实际上是模型仅偶尔生成的。这一点在查看每个模型预测的总人数时变得明显。预测的历史人物更可能是西欧和南欧的。
- en: '![](../Images/d80156f7420faa20812d0a9c00573ee3.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d80156f7420faa20812d0a9c00573ee3.png)'
- en: '*Subregion breakdown of total historical figures generated by GPT-4 and Claude.
    Plot generated by author.*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*由GPT-4和Claude生成的总历史人物的次区域分类。图表由作者生成。*'
- en: 'So, yes, there is a Western bias. In terms of *unique* people, there was more
    diversity of Eastern Asian historical figures. However, in terms of *total* historical
    figures predicted, **there was a bias for generating historical figures from Western
    and Southern Europe: Europeans constituted 60% of the people generated by GPT-4
    and 52% of those generated by Claude.** There were extremely few historical figures
    from Central America (0.12% for GPT-4, and 0% for Claude) or from the entire continent
    of Africa (5.9% for GPT-5 and 4.0% for Claude).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，确实存在西方偏见。就*独特*的人物而言，东方历史人物的多样性更多。然而，就*总*历史人物的预测而言，**生成来自西欧和南欧的历史人物存在偏见：欧洲人在GPT-4生成的人物中占60%，在Claude生成的人物中占52%。**
    中美洲的历史人物极少（GPT-4为0.12%，Claude为0%），整个非洲大陆的历史人物更少（GPT-5为5.9%，Claude为4.0%）。
- en: This underscores these models’ implicit or explicit understanding of a very
    Western-focused history, where the idea of important figures in history are European
    (even when prompted in non-European languages!).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这突显了这些模型对非常西方化历史的隐含或显性理解，即历史中重要人物的观念主要是欧洲人（即使在非欧洲语言中提示！）。
- en: Professions
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 职业
- en: Looking at the distribution of *unique* historical figures, both GPT-4 and Claude
    were more likely to generate politicians and philosophers. This is an interesting
    skew towards more political and philosophical history.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 看看*独特*历史人物的分布，GPT-4和Claude更倾向于生成政治家和哲学家。这是一个有趣的偏向，更侧重于政治和哲学历史。
- en: This also is a very reductionist view, because many people cannot be described
    with a singular profession. For example, what was Leonardo Da Vinci’s profession?
    He was an “Italian polymath of the High Renaissance who was active as a painter,
    draughtsman, engineer, scientist, theorist, sculptor, and architect” ([source](https://en.wikipedia.org/wiki/Leonardo_da_Vinci)).
    However, his “official” profession through the Wikipedia API was “painter”.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是一种非常还原主义的观点，因为许多人无法用单一职业来描述。例如，莱昂纳多·达·芬奇的职业是什么？他是“意大利文艺复兴时期的博学者，曾从事画家、绘图员、工程师、科学家、理论家、雕塑家和建筑师”等多种活动”（[来源](https://en.wikipedia.org/wiki/Leonardo_da_Vinci)）。然而，通过维基百科API他的“官方”职业是“画家”。
- en: '![](../Images/a0889fcb096de6ffd96843c388a0d15e.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0889fcb096de6ffd96843c388a0d15e.png)'
- en: '*Profession breakdown of unique historical figures generated by GPT-4 and Claude.
    Plot generated by author.*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*由GPT-4和Claude生成的独特历史人物职业分类。图表由作者生成。*'
- en: Discussion & Conclusion
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论与结论
- en: In this article, I probed two closed-source large language models regarding
    top historical figures. I showed that there was a gender bias towards generating
    male historical figures and a geographic bias towards generating people from Europe.
    I also showed that there was a language bias as well, in which prompting in certain
    languages (such as Russian) suffered from gender bias more severely.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我探讨了两个封闭源的大型语言模型关于顶级历史人物的问题。我展示了在生成男性历史人物方面存在性别偏见，并且在生成来自欧洲的人物方面存在地理偏见。我还展示了存在语言偏见的情况，其中某些语言（如俄语）的提示在性别偏见方面更加严重。
- en: It would be interesting to extend this analysis to open source models. I did
    a preliminary analysis on the newest model, [Llama 2 (70B)](https://ai.meta.com/llama/)
    and found that the results in non-English languages failed to properly answer
    the prompt or produced gibberish generations (suggesting that it had not been
    trained as much in most non-English languages). Because of this, I did not include
    the analysis in here, but I encourage the reader to try and share any insights
    they may find.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展这个分析到开源模型会很有趣。我对最新的模型进行了初步分析，[Llama 2 (70B)](https://ai.meta.com/llama/) 的结果显示，在非英语语言中未能准确回答提示或生成了无意义的内容（这表明它在大多数非英语语言中的训练不够）。因此，我没有在这里包含这项分析，但我鼓励读者尝试并分享他们可能发现的任何见解。
- en: There is not (and likely never will be) a commonly accepted list of “most important
    historical figures” — this is an intentionally subjective question. How any one
    person answers this question (whether or not they be a historian) depends on cultural
    context (George Washington, for example, is very important in American history,
    but arguably insignificant in Korean history), the discipline (Isaac Newton may
    be more important in scientific history but less so in political history), and
    personal understanding of the world and society.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 没有（也可能永远不会有）一个普遍接受的“最重要历史人物”名单——这是一个故意主观的问题。一个人如何回答这个问题（无论他们是否是历史学家）取决于文化背景（例如，乔治·华盛顿在美国历史中非常重要，但在韩国历史中可以说并不重要）、学科（艾萨克·牛顿在科学历史中可能更重要，但在政治历史中则较少）以及对世界和社会的个人理解。
- en: With this article, I hoped to call attention to the lack of women in so many
    obvious and non-obvious spheres of our lives. If you opened up any high school
    history textbook, I’m sure the gender bias of historical figures in those books
    would be as biased as the results from the large language models. But that’s exactly
    the point. These results are *normal* in society (at least, in the Western society
    I currently live in).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这篇文章，我希望引起对我们生活中许多明显和不明显领域中女性缺乏关注。如果你翻开任何一本高中历史教科书，我相信那些书中的历史人物性别偏见将与大型语言模型的结果一样偏见。但这正是关键所在。这些结果在社会中是*正常*的（至少在我目前生活的西方社会中是如此）。
- en: Having taken several women’s history courses, I *know* there were many important
    women in history — queens and warriors, pirates and poets, activists and scientists.
    Historians of the past, who were mostly men, tended to write women out of these
    narratives. The podcast [History Chicks](https://thehistorychicks.com/) talks
    about the many women in history who have contributed to the world but were forgotten
    or erased. Such is the story of many women, such as [Rosalind Franklin](https://en.wikipedia.org/wiki/Rosalind_Franklin),
    who, despite her contributions to the discovery of the structure of DNA, was largely
    unrecognized during her life and is still not given the same recognition that
    her colleagues, Watson and Crick, are given.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在参加了几个女性历史课程后，我*知道*历史上有许多重要的女性——包括女王和战士、海盗和诗人、活动家和科学家。过去的历史学家，大多是男性，往往将女性排除在这些叙述之外。播客[历史女孩](https://thehistorychicks.com/)讲述了许多为世界做出贡献但被遗忘或抹去的女性的故事。这就是许多女性的故事，例如[罗莎琳德·弗兰克林](https://en.wikipedia.org/wiki/Rosalind_Franklin)，尽管她对DNA结构的发现作出了贡献，但在她生前几乎没有得到认可，至今也没有获得像她的同事沃森和克里克那样的认可。
- en: The language models *reflect* biases that already exist in society and on the
    texts that they were trained on — they perpetuate and remix these biases and,
    perhaps, exacerbate them in new ways. It is important for both users and developers
    of these large language models to be cognizant of these biases (and the many,
    many more they encode!) as they continue to use them in a variety of educational
    and professional settings.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型*反映*了社会和它们训练的文本中已经存在的偏见——它们延续并重新混合这些偏见，并且可能以新的方式加剧这些偏见。对这些大型语言模型的用户和开发者来说，了解这些偏见（以及它们编码的许多更多偏见！）在各种教育和专业环境中继续使用时是非常重要的。
- en: '*Thank you for reading and supporting my work!*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢你阅读和支持我的工作！*'
- en: '*This article was originally posted on* [*my personal blog*](https://blog.yenniejun.com/p/where-are-all-the-women),
    *where I post my exploration of data and AI more frequently :)*'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*这篇文章最初发布在* [*我的个人博客*](https://blog.yenniejun.com/p/where-are-all-the-women)，*我在这里更频繁地发布关于数据和人工智能的探索
    :)*'
- en: Appendix
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录
- en: Prompts
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示
- en: '[PRE0]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Entity Normalization
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实体标准化
- en: When the model generated a historical figure, how did I get from that to its
    wikipedia metadata?
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型生成一个历史人物时，我是如何从那个信息到达其维基百科元数据的？
- en: Data cleaning — removing extraneous punctuation marks (like periods or quotation
    marks)
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据清理——去除多余的标点符号（如句号或引号）
- en: Find that person on Wikipedia using [pywikibot](https://github.com/wikimedia/pywikibot),
    a Python library that interfaces with [MediaWiki](https://www.mediawiki.org/wiki/MediaWiki).
    This library allowed me to connect with Wikibase, the knowledge base driving Wikidata,
    and obtain structured metadata about each entity, such as gender, country of origin,
    and profession.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用[pywikibot](https://github.com/wikimedia/pywikibot)在维基百科上查找那个人，这是一个与[MediaWiki](https://www.mediawiki.org/wiki/MediaWiki)接口的Python库。这个库让我能够连接到驱动Wikidata的知识库Wikibase，并获得关于每个实体的结构化元数据，如性别、原产国和职业。
- en: Often, the text generated by the models need to be normalized into the proper
    form. For example, the person Mahatma Gandhi can be referred to by “Gandhi” or
    “Mohandas Gandhi” or some other variation, but only “Mahatma Gandhi” will normalize
    to the correct Wiki page. In order to do this, I utilized existing SERP knowledge
    encoded by search engines such as Bing. Using a Bing scraper, I was able to extract
    the normalized Wikipedia name for a given entity
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通常，由模型生成的文本需要被规范化成适当的形式。例如，人物甘地可以被称作“甘地”或“莫汉达斯·甘地”或其他变体，但只有“圣雄甘地”会规范到正确的维基百科页面。为了做到这一点，我利用了由搜索引擎如必应编码的现有SERP知识。通过使用必应抓取工具，我能够提取给定实体的规范化维基百科名称。
