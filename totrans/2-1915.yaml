- en: 'Stepping Stones to Understanding: Knowledge Graphs as Scaffolds for Interpretable
    Chain-of-Thought Reasoning with LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/stepping-stones-to-understanding-knowledge-graphs-as-scaffolds-for-interpretable-chain-of-thought-2b9139c28c60](https://towardsdatascience.com/stepping-stones-to-understanding-knowledge-graphs-as-scaffolds-for-interpretable-chain-of-thought-2b9139c28c60)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@alcarazanthony1?source=post_page-----2b9139c28c60--------------------------------)[![Anthony
    Alcaraz](../Images/6a71a1752677bd07c384246fb0c7f7e8.png)](https://medium.com/@alcarazanthony1?source=post_page-----2b9139c28c60--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2b9139c28c60--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2b9139c28c60--------------------------------)
    [Anthony Alcaraz](https://medium.com/@alcarazanthony1?source=post_page-----2b9139c28c60--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2b9139c28c60--------------------------------)
    ·7 min read·Nov 21, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*Artificial intelligence software was used to enhance the grammar, flow, and
    readability of this article’s text.*'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs), trained on vast volumes of text data, has sparked
    a revolution in AI. Their ability to generate remarkably eloquent, coherent language
    simply from a short text prompt has opened up new horizons across domains from
    creative writing to conversational assistants.
  prefs: []
  type: TYPE_NORMAL
- en: However, mastery of linguistic expression alone does not equate true intelligence.
    LLMs still lack semantic understanding of concepts and logical reasoning abilities
    required for situational comprehension and complex problem solving. Their knowledge
    remains confined to superficial patterns discerned from training corpora rather
    than grounded facts about the real world.
  prefs: []
  type: TYPE_NORMAL
- en: As we pose more open-ended, multifaceted questions to these models, their limitations
    become increasingly apparent. They cannot logically synthesize details from different
    documents or make inferences spanning multiple steps to derive answers.
  prefs: []
  type: TYPE_NORMAL
- en: Once queries start departing from the distribution of training data, hallucinated
    or contradictory responses start emerging.
  prefs: []
  type: TYPE_NORMAL
- en: To address these pitfalls, the AI community has pivoted focus toward retrieval
    augmented generative (RAG) frameworks. These systems aim to synergize the linguistic
    prowess of language models with fast, targeted access to external knowledge sources
    that can ground them in factual context instead of hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: Most existing architectures retrieve supplementary information using semantic
    similarity over vector representations of passages from text corpora. However,
    this struggles with fuzzy relevance between retrieved passages and actual query
    context. Key details get lost when condensing passages to singular opaque vectors
    devoid of contextual links. Deriving coherent narratives tying disparate facts
    through logical reasoning remains arduous.
  prefs: []
  type: TYPE_NORMAL
- en: This underscores the need for incorporating structured knowledge sources that
    encapsulate real-world entities and relationships between them. Knowledge graphs
    meet this need — encoding facts as interconnected nodes and edges that can be
    traversed along explanatory pathways. However, effectively grounding free-form
    reasoning of language models upon structured graphs presents interfacing challenges.
    Creatively bridging neural approaches with symbolic representations remains an
    open problem.
  prefs: []
  type: TYPE_NORMAL
- en: An emerging technique that offers promise in this direction is chain-of-thought
    (CoT) prompting. CoT nudges language models to reveal their reasoning in step-by-step
    inferential chains. Each connection becomes plainly visible, enhancing transparency.
    However, coherence rapidly falls apart over long histories in purely free-form
    linguistic spaces. Knowledge graphs could provide the missing scaffolding to give
    structure to these unraveling reasoning trajectories.
  prefs: []
  type: TYPE_NORMAL
- en: Explicitly tracing CoT steps along knowledge graph pathways may enable logically
    sound reasoning firmly grounded in chains of facts. However, finding the right
    alignments between unstructured neural outputs and structured symbolic knowledge
    remains an open challenge. Innovations in this direction offer hope for blending
    strengths of both approaches — symbolic representations with sound deductive chains
    anchored to real-world entities connected fluidly through vector spaces allowing
    efficient statistical inference.
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the article will explore this promising intersection of knowledge
    graphs and CoT reasoning within LLMs for more robust situational intelligence.
    We delve into techniques leveraging each approach’s complementary strengths while
    mitigating their weaknesses in isolation.
  prefs: []
  type: TYPE_NORMAL
- en: I. Knowledge Graphs for Robust Few-Shot Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](https://ai.plainenglish.io/new-research-proves-knowledge-graphs-drastically-improve-accuracy-of-large-language-models-on-0f1dbcc08d61?source=post_page-----2b9139c28c60--------------------------------)
    [## New Research Proves Knowledge Graphs Drastically Improve Accuracy of Large
    Language Models on…'
  prefs: []
  type: TYPE_NORMAL
- en: Natural language interfaces to databases have long been a holy grail of both
    industry and academia. Recently, advances…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ai.plainenglish.io](https://ai.plainenglish.io/new-research-proves-knowledge-graphs-drastically-improve-accuracy-of-large-language-models-on-0f1dbcc08d61?source=post_page-----2b9139c28c60--------------------------------)
    [](/vector-search-is-not-all-you-need-ecd0f16ad65e?source=post_page-----2b9139c28c60--------------------------------)
    [## Vector Search Is Not All You Need
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/vector-search-is-not-all-you-need-ecd0f16ad65e?source=post_page-----2b9139c28c60--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Most existing RAG systems rely solely on passage embeddings for semantic similarity
    matching. However, these struggle with fuzzy relevance and inability to jointly
    analyze connected facts scattered across passages. Knowledge graphs address this
    by retaining symbolic facts and relationships enabling explainable multi-hop reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Diverse Graph Algorithms for Versatile Reasoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Knowledge graphs equip us with an entire new repertoire of algorithms optimized
    for different reasoning modalities:'
  prefs: []
  type: TYPE_NORMAL
- en: Graph traversal algorithms like Personalized PageRank allow flexible associative
    reasoning by analyzing indirect connections between entities. This supports deducing
    new relations from inference chains spanning multiple edges.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms tuned for search (e.g. Approximate Nearest Neighbors) allow efficiently
    querying facts related to specific entities. This aids precise factual retrieval.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph summarization algorithms can concisely distill subgraphs with the most
    pertinent information to simplify reasoning. This reduces noise and improves focus.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimized Knowledge Graph Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition, knowledge graph elements like entities, relations and text can
    be encoded into vector spaces as well, enabling mathematical operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Transitive embeddings improve deductive reasoning across multi-hop inference
    chains by maintaining equivalence across composition of relations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical embeddings encode taxonomy hierarchies between entities, allowing
    inheritance-based reasoning. This inherits facts from ancestral classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contextual embeddings from large transformer language models capture semantic
    nuances in textual attributes of entities and relations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The partnerships between rich symbolic representations and flexible vector spaces
    provide the best foundations for few-shot learning — neatly structured knowledge
    to scaffold explicit logical deductions explicated through dynamically fluid vectors
    spaces that distill salient patterns.
  prefs: []
  type: TYPE_NORMAL
- en: II. Augmenting Chain-of-Thought with Structured Knowledge Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](/achieving-structured-reasoning-with-llms-in-chaotic-contexts-with-thread-of-thought-prompting-and-a4b8018b619a?source=post_page-----2b9139c28c60--------------------------------)
    [## Achieving Structured Reasoning with LLMs in Chaotic Contexts with Thread of
    Thought Prompting and…'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) demonstrated impressive few-shot learning capabilities,
    rapidly adapting to new tasks with…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/achieving-structured-reasoning-with-llms-in-chaotic-contexts-with-thread-of-thought-prompting-and-a4b8018b619a?source=post_page-----2b9139c28c60--------------------------------)
    [](https://arxiv.org/abs/2308.09687?source=post_page-----2b9139c28c60--------------------------------)
    [## Graph of Thoughts: Solving Elaborate Problems with Large Language Models'
  prefs: []
  type: TYPE_NORMAL
- en: 'We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities
    in large language models (LLMs)…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'arxiv.org](https://arxiv.org/abs/2308.09687?source=post_page-----2b9139c28c60--------------------------------)
    [](https://arxiv.org/abs/2310.01061?source=post_page-----2b9139c28c60--------------------------------)
    [## Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) have demonstrated impressive reasoning abilities
    in complex tasks. However, they lack…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: arxiv.org](https://arxiv.org/abs/2310.01061?source=post_page-----2b9139c28c60--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Chain-of-thought (CoT) prompting guides language models to reveal their reasoning
    in explanatory chains of inferential steps. However, as inferences span longerhistories,
    coherence often unravels in free-form linguistic space. Knowledge graphs can provide
    missing structure.
  prefs: []
  type: TYPE_NORMAL
- en: Explicitly Encoding Concepts and Relations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Knowledge graphs encode concepts as interlinked symbolic nodes, capturing relations
    between them. Traversing explanatory pathways over this explicit network can scaffold
    CoT reasoning. Recent approaches like Graph-of-Thoughts (GoT) explore assembling
    situational graphs to model evolving CoT steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond just modeling, the structured representations can participate in steering
    the reasoning:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Querying Ontologies: Initial ontology queries establishing definitions and
    high-level context can frame the reasoning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Traversing Relations: Graph algorithms can gather connected facts relevant
    for each CoT step.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Updating Representations: Embeddings encode extracted details, focussing attention.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Coordinating Hybrid Reasoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Integrating neural CoT prompting with structured knowledge graphs requires
    coordinating distributed reasoning modules:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Manager: Sequences module execution, balancing language model requests with
    retrieval.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Prompter: Elicits free-form rationales from language models through CoT prompting.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Retriever: Gathers pertinent graph details using algorithms like Personalized
    PageRank.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Parser: Transforms retrieved facts to natural language or vectors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scorer: Assesses relevance of retrieved facts for current context.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fuser: Combines salient knowledge with updated prompt for next round.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The modular architecture allows combining strengths of neural and symbolic approaches.
    The language model thinks fluidly while the graph preserves logic — each compensating
    for the other’s limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Choreographing Hybrid Reasoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Orchestrating the staged interplay between structured knowledge and fluid vector
    inferences is key to unlocking new reasoning capabilities combining:'
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability of explicit symbolic modeling with scalable pattern recognition
    using embeddings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logical soundness of axiomatic knowledge with adaptive improvisation of neural
    approaches.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explainability afforded by graph traversals with efficient computation via vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Innovations on this synthesis promise more reliable, versatile and transparent
    reasoning than possible with either approach in isolation. The partnerships open
    new frontiers for situated intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: III. Current Gaps and Future Directions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](/knowledge-graph-transformers-architecting-dynamic-reasoning-for-evolving-knowledge-712e056725e0?source=post_page-----2b9139c28c60--------------------------------)
    [## Knowledge Graph Transformers: Architecting Dynamic Reasoning for Evolving
    Knowledge'
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge graphs, which represent facts as interconnected entities, have emerged
    as a pivotal technique for enhancing…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/knowledge-graph-transformers-architecting-dynamic-reasoning-for-evolving-knowledge-712e056725e0?source=post_page-----2b9139c28c60--------------------------------)
    [](https://ai.plainenglish.io/unlocking-the-power-of-graphs-for-ai-reasoning-378c7e9dac02?source=post_page-----2b9139c28c60--------------------------------)
    [## Unlocking the Power of Graphs for AI Reasoning
  prefs: []
  type: TYPE_NORMAL
- en: Graphs are ubiquitous in the modern data-driven business world.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ai.plainenglish.io](https://ai.plainenglish.io/unlocking-the-power-of-graphs-for-ai-reasoning-378c7e9dac02?source=post_page-----2b9139c28c60--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'While knowledge graphs offer promise for grounding reasoning, realizing their
    full potential has obstacles like sub-optimal construction, alignment, personalization
    and handling evolution:'
  prefs: []
  type: TYPE_NORMAL
- en: Comprehensive, High-fidelity Knowledge Graphs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Manually curating extensive high-quality knowledge graphs spanning diverse nuanced
    domains poses scaling bottlenecks. Meanwhile open-source graphs suffer from sparsity,
    inconsistency and noise issues ill-suited for supporting sound deductive chains.
    Clean ontological knowledge combined with noisy web extractions brings integration
    difficulties.
  prefs: []
  type: TYPE_NORMAL
- en: Smoothly Integrating Vector and Symbolic Spaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bridging the symbolic structure of knowledge graphs with latent vector spaces
    of language models to enable seamless exchange of information is non-trivial.
    Simple approaches like direct vector lookups struggle to adequately capture symbolic
    semantics. More advanced techniques like graph neural networks show promise for
    elegantly grounding graphs within language model vector spaces to enable tightly
    coupled reasoning. But research remains nascent.
  prefs: []
  type: TYPE_NORMAL
- en: Personalized and Current Temporal Graphs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Static knowledge graphs poorly reflect individual users’ unique contexts, hindering
    personalized reasoning aligned with personal world knowledge. Constructing customizable
    user-specific knowledge graphs remains prohibitively expensive. Meanwhile, static
    graphs also grow obsolete, failing to track constantly shifting real-world states
    and events critical for contemporary reasoning. Dynamic graphs accurately mirroring
    our ephemeral environments are pivotal.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Innovations to Overcome Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Nevertheless innovations hold promise in mitigating these limitations through
    fusion techniques combining curated and extracted knowledge, improved grounding
    algorithms deeply intertwining symbolic and neural reasoning, customizable dynamic
    graph construction aided by smart assistants, and stream learning continuously
    updating representations — all coming together to realize the symbiotic potential
    of integrated reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sources :'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](https://arxiv.org/abs/2311.11797?source=post_page-----2b9139c28c60--------------------------------)
    [## Igniting Language Intelligence: The Hitchhiker''s Guide From Chain-of-Thought
    Reasoning to Language…'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) have dramatically enhanced the field of language
    intelligence, as demonstrably evidenced…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: arxiv.org](https://arxiv.org/abs/2311.11797?source=post_page-----2b9139c28c60--------------------------------)
    ![](../Images/183c5b218b071fc0f3327c8bfceeec2b.png)
  prefs: []
  type: TYPE_NORMAL
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
