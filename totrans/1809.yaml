- en: Sb3, the Swiss Army Knife of Applied RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/sb3-the-swiss-army-knife-of-applied-rl-5548535d09cd](https://towardsdatascience.com/sb3-the-swiss-army-knife-of-applied-rl-5548535d09cd)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Your choice of model, with any environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@byjameskoh?source=post_page-----5548535d09cd--------------------------------)[![James
    Koh, PhD](../Images/8e7af8b567cdcf24805754801683b426.png)](https://medium.com/@byjameskoh?source=post_page-----5548535d09cd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5548535d09cd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5548535d09cd--------------------------------)
    [James Koh, PhD](https://medium.com/@byjameskoh?source=post_page-----5548535d09cd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5548535d09cd--------------------------------)
    ·8 min read·Oct 26, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3359413fa03ba1b4537646411e4dea66.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by DALL·E 3 based on the prompt “Create a realistic looking image
    of an opened swiss army knife.”
  prefs: []
  type: TYPE_NORMAL
- en: Stablebaseline3 (sb3) is like a Swiss Army knife. It is a multi-function utility
    tool, that can be used for many purpose. And, just like a Swiss Army knife can
    save your life if you are stranded in a jungle, sb3 can save your life in the
    office, when you have seemingly impossible deadlines to meet.
  prefs: []
  type: TYPE_NORMAL
- en: This guide uses gymnasium=0.28.1 and stable-baselines=2.1.0\. If you use different
    versions, or perhaps even refer to other old guides, you may not get the results
    below. But fret not, an installation guide is given here as well. I guarantee
    you can get the results if you follow my instructions.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[1] What You Will Get Here'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stablebaseline3 is easy to use. It is also well documented, and you can follow
    the tutorials on your own. But…
  prefs: []
  type: TYPE_NORMAL
- en: Have you referred to older guides (perhaps those using `gym`), only to find
    errors on your machine?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are you able to always ensure compatibility?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What if you want to use `gymnasium`'s environment and modify perhaps the rewards?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you know how to wrap your own tasks, such that SOTA models can be applied
    in a few lines?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That’s the objective of this article! After reading this guided demonstration,
    you will…
  prefs: []
  type: TYPE_NORMAL
- en: Solve classic environments with sb3 models, visualize the results, as well as
    save (or load) the trained model in a few lines of code. **[Section 3.1]**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understand how to check the action space and observation space for compatibility.
    **[Section 3.2]**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learn how to wrap `gymnasium`environments so that any sb3 models can be used,
    without any restrictions on `box` or `discrete`. **[Section 4.1]**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learn how to wrap `gymnasium`environments for reward shaping. **[Section 4.2]**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learn how to wrap your own custom environments to be compatible with sb3, with
    minimal changes to your original code which may follow a different structure.
    **[Section 5]**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2] Installation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a virtual environment and set up the relevant dependencies. I cater
    to the majority — here the guide is created using Windows and has Anaconda installed.
    Open your Anaconda prompt and do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, we will be using jupyter notebook, as it is a more user-friendly tool
    for teaching.
  prefs: []
  type: TYPE_NORMAL
- en: '[3] A Taste of Success — See Your Trained RL Agents'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first thing is to import the required libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[3.1] DQN on Cartpole'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start small, on the Cartpole task where the objective is to push the cart
    (left or right) to keep the pole upright.
  prefs: []
  type: TYPE_NORMAL
- en: What’s the absolute minimum you need? Just this, to train.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: And this, to evaluate.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Finally, this, to visualize.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In just 10+ lines of code, and a couple of seconds, we solved a classic RL problem.
    This is a good example of the extent to which AI has been democratized!
  prefs: []
  type: TYPE_NORMAL
- en: Agent trained and visualized with the exact code above. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: To save your sb3 model, just add a callback during the training execution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Your model can subsequently be loaded in just two lines.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[3.2] Check action/observation space'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose we try a different model, say, using `model=SAC("MlpPolicy", env)`.
    An error would result.
  prefs: []
  type: TYPE_NORMAL
- en: This is because SAC (Soft Actor Critic) only works with continuous action space,
    as stated on the official Stable Baselines3 [documentations](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html),
    while the cartpole environment has discrete action space.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ve compiled the action space constraints into a simple function below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: With this, `is_compatible(env,'DQN')` returns `True`, while `is_compatible(env,'SAC')`
    returns `False`.
  prefs: []
  type: TYPE_NORMAL
- en: There are no constraints on observation spaces, for any of the models in sb3.
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Wrap `gymnasium` Env'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What if we want to modify the `gymnasium` environment according to our own specifications?
    Should we write the code from scratch? Or perhaps look at the source code and
    make modifications over there?
  prefs: []
  type: TYPE_NORMAL
- en: The answer to both is, **no**.
  prefs: []
  type: TYPE_NORMAL
- en: It is better to simply wrap the `gymnasium` objects. Not only is this fast and
    easy, doing so makes your code readable and trustworthy.
  prefs: []
  type: TYPE_NORMAL
- en: People do not need to scrutinize every line of your code. Instead, they only
    need to look at the modifications within your wrapper (assuming they are convinced
    with the correctness of `gymnasium`).
  prefs: []
  type: TYPE_NORMAL
- en: '[4.1] Agnostic to `box` or `discrete`'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In section 3.2, we see that SAC is not compatible with Cartpole.
  prefs: []
  type: TYPE_NORMAL
- en: This is a workaround for this. In fact, any sb3 model can be used on any environment;
    we just need a simple wrapper.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: With this, you can solve an environment with discrete action space using a model
    like SAC which deals with continuous action space.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Any sb3 model can be made compatible with any classic gymnasium environment.
    Don’t just take my word for it. Try out the following.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that the purpose here is just to *show* that the environments can be wrapped
    to be made compatible. The performance might not be ideal, but that is not the
    point here.
  prefs: []
  type: TYPE_NORMAL
- en: The point is to show you that if you understand how sb3 works with gymnasium,
    you are able to wrap anything for universal compatibility.
  prefs: []
  type: TYPE_NORMAL
- en: '[4.2] Reward shaping'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose we want to modify a gymnasium environment, to try out reward shaping.
    For example, you may have played with [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/)
    and observed that an agent trained with default hyperparameters may hover at the
    top, in order not to risk a crash.
  prefs: []
  type: TYPE_NORMAL
- en: Lunar Lander hovering at the top. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we can impose a penalty when the agent persistently stays at the
    top.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Keep in mind that after training with the pseudo-rewards, the agent should be
    fine-tuned using the actual environment with the original rewards.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Lunar Lander solved by agent trained with reward shaping. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: This looks much better!
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Wrapper Over Custom Tasks'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final section, I will deliver my 5ᵗʰ promise — *Learn how to wrap your
    own custom environments to be compatible with sb3, with minimal changes to your
    original code which may follow a different structure.*
  prefs: []
  type: TYPE_NORMAL
- en: As a learner, we train RL agents to solve well known benchmark problems. However,
    the industry pays you to solve real problems, and not toy problems. If you are
    employed for your RL expertise, chances are that you have to solve problems that
    are unique to your company.
  prefs: []
  type: TYPE_NORMAL
- en: However, sb3 and gymnasium still remain your good friends!
  prefs: []
  type: TYPE_NORMAL
- en: For the purpose of illustration, let’s consider the following simple GridWorld.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Notice here that the `transition` method returns `reward`, `next_state`, and
    `done`. Stable baselines3 will not accept this style.
  prefs: []
  type: TYPE_NORMAL
- en: Do you have to re-code your environment? No!
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we build a simple wrapper around it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the above, we define a method `step`, which wraps around the original environment’s
    `transition`, and returns what sb3 expects.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, I’ve used this opportunity to demonstrate that we can perform
    modifications without dissecting the original environment. Here, `CustomEnv` terminates
    the episode (with a large penalty) if the goal is not reached in 50 steps.
  prefs: []
  type: TYPE_NORMAL
- en: How do we know if the environment is wrapped properly? First, it has to pass
    the following basic check.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Next, we can use an sb3 model to train on the wrapped environment. You could
    also play around with the hyperparameters here, as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, you have learnt to set up your own environment to run sb3 and
    gymnasium. You now have ability to implement state-of-the-art RL algorithms on
    ***any*** environment of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: Enjoy!
  prefs: []
  type: TYPE_NORMAL
