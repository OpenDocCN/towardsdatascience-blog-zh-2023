- en: 7 Signs You’ve Become an Advanced Sklearn User Without Even Realizing It
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/7-signs-youve-become-an-advanced-sklearn-user-without-even-realizing-it-3b7085c600f1](https://towardsdatascience.com/7-signs-youve-become-an-advanced-sklearn-user-without-even-realizing-it-3b7085c600f1)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: and a pro ML engineer with that…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ibexorigin.medium.com/?source=post_page-----3b7085c600f1--------------------------------)[![Bex
    T.](../Images/516496f32596e8ad56bf07f178a643c6.png)](https://ibexorigin.medium.com/?source=post_page-----3b7085c600f1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3b7085c600f1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3b7085c600f1--------------------------------)
    [Bex T.](https://ibexorigin.medium.com/?source=post_page-----3b7085c600f1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3b7085c600f1--------------------------------)
    ·10 min read·Jun 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/850fb929f22706fc26aa71c6693186d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by me with Midjourney
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Get ready to be pleasantly amazed! I am about to drop **seven** undeniable signs
    you’ve become an advanced Sklearn user without a foggiest clue of it happening.
    And since Sklearn is the most widely used machine learning library on planet Earth,
    you might as well take these signs as indicators that you are already a very able
    machine learning practitioner.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: 0\. Three partitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Almost everything we do in machine learning is in service of avoiding overfitting.
    And one of the greatest tools in your arsenal to fight it is splitting your data
    into not two but three sets!
  prefs: []
  type: TYPE_NORMAL
- en: Cassie Kozyrkov, the head of Decision Intelligence at Google, says that data
    splitting is the most powerful idea in machine learning and you agree with her.
  prefs: []
  type: TYPE_NORMAL
- en: You are aware that overfitting can occur not only on the train set but also
    on the validation set. You’ve observed that using the same set for both testing
    and hyperparameter tuning often introduces subtle data leakage.
  prefs: []
  type: TYPE_NORMAL
- en: By continuously tweaking the hyperparameters based on the performance of the
    model on that specific test set, there is a risk of overfitting the model to that
    particular set.
  prefs: []
  type: TYPE_NORMAL
- en: So, you train your selected model using 50% of the available data. Then, you
    fine-tune and evaluate the model using a separate validation set containing 25%
    of the data. Finally, just when your baby model is ready to be deployed into the
    wild, you test it one final time using a completely untouched and pristine (I
    mean you haven’t even looked at the first five rows) test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this rule in mind, you’ve saved this code snippet on your desktop to copy/paste
    any time you want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'SYDD! If an unexamined life is not worth living, then here are the four words
    to live by: Split Your Damned Data. — Cassie Kozyrkov'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](/the-most-powerful-idea-in-data-science-78b9cd451e72?source=post_page-----3b7085c600f1--------------------------------)
    [## The most powerful idea in data science'
  prefs: []
  type: TYPE_NORMAL
- en: Edit description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/the-most-powerful-idea-in-data-science-78b9cd451e72?source=post_page-----3b7085c600f1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Setting a common sense baseline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How does a person know if anything they are doing in an ML project is contributing
    to the end product — the perfect model?
  prefs: []
  type: TYPE_NORMAL
- en: Here, the “I know when I see it” approach is never going to cut it. That person
    needs a North Star, something they can always refer to in order to know if a model
    they just trained can pass even the simplest of tests.
  prefs: []
  type: TYPE_NORMAL
- en: You’ve seen time and time again in your projects that this crucial North Star
    is a **common sense baseline** performance.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have identified the machine learning task, selected the features, and
    defined the target variable, you fit either a `DummyRegressor`or `DummyClassifier`
    to evaluate the performance of a random guessing model on your specific problem.
    This score serves as a baseline, and all your subsequent experiments aim to improve
    this initial result.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You are also never tired of recommending this practice to others, including
    your good friend Bex.
  prefs: []
  type: TYPE_NORMAL
- en: He once foolishly tried all suitable Sklearn models and increasingly complex
    deep learning architectures for an image classification task to figure out why
    all his experiments were failing to improve beyond a certain threshold.
  prefs: []
  type: TYPE_NORMAL
- en: You indicated that Bex was missing a baseline, and when he tried `DummyClassifier`,
    he realized that the entire task was unsolvable and all his efforts were a total
    waste of time because none of his experiments managed to beat a random guessing
    model. He had to look for a new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Nothing beats the horror of **over-engineering**, of being utterly convinced
    that your model is exceptional, only to realize it is just as effective as a monkey
    throwing darts blindfolded.
  prefs: []
  type: TYPE_NORMAL
- en: You taught your friend a valuable lesson.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Feature selection vs. engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When faced with a dataset with dozens or even hundreds of features, you are
    mindful of your precious time and hardware resources. Instead of blindly busting
    models on all existing features, you take a step back and try to isolate the most
    promising ones.
  prefs: []
  type: TYPE_NORMAL
- en: First, you might look at features one at a time and use `VarianceThreshold`
    to drop the ones with insignificant variances. Then, you look at the bigger picture
    by analyzing the relationships between features and decide which ones are worthy
    from there.
  prefs: []
  type: TYPE_NORMAL
- en: To help you decide, you might use metrics such as pairwise correlation coefficients
    or, even better, employ model-based approaches using classes like `RFECV` (recursive
    feature elimination with CV) or `SelectFromModel`.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/3-step-feature-selection-guide-in-sklearn-to-superchage-your-models-e994aa50c6d2?source=post_page-----3b7085c600f1--------------------------------)
    [## 3-Step Feature Selection Guide in Sklearn to Superchage Your Models'
  prefs: []
  type: TYPE_NORMAL
- en: Edit description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/3-step-feature-selection-guide-in-sklearn-to-superchage-your-models-e994aa50c6d2?source=post_page-----3b7085c600f1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Afterwards, you apply your feature engineering magic to get the most out of
    the best features. Here, your knowledge of the intricate differences between numeric
    transformations such as normalization, standardization, and log transforms will
    be key.
  prefs: []
  type: TYPE_NORMAL
- en: You understand the trade-offs among `StandardScaler`, `MinMaxScaler`, `QuantileTransformer`,
    `PowerTransformer`, and more like the back of your hand.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-to-differentiate-between-scaling-normalization-and-log-transformations-69873d365a94?source=post_page-----3b7085c600f1--------------------------------)
    [## How to Differentiate Between Scaling, Normalization, and Log Transformations'
  prefs: []
  type: TYPE_NORMAL
- en: Edit description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-differentiate-between-scaling-normalization-and-log-transformations-69873d365a94?source=post_page-----3b7085c600f1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to properly imputing missing values, you go beyond simple strategies
    like mean/mode imputation with `SimpleImputer`. You have a firm grasp on advanced
    model-based imputation techniques such as `KNNImputer` or `IterativeImputer` and
    can choose the most appropriate one based on the missingness type.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/advanced-missing-data-imputation-methods-with-sklearn-d9875cbcc6eb?source=post_page-----3b7085c600f1--------------------------------)
    [## In-depth Tutorial to Advanced Missing Data Imputation Methods with Sklearn'
  prefs: []
  type: TYPE_NORMAL
- en: Edit description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/advanced-missing-data-imputation-methods-with-sklearn-d9875cbcc6eb?source=post_page-----3b7085c600f1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: As for categorical features, you use a variety of techniques to properly encode
    them, such as `OrdinalEncoder` or `OneHotEncoder`. When using the latter, you
    are very careful of the dummy-variable trap, which can leak multicollinearity
    into the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Even though I just listed feature selection and feature engineering one after
    the other, you, the seasoned pro, know that you shouldn’t lock the two into a
    rigid order. You iterate between the two, experimenting and refining your approach
    iteratively.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Model selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is where you showcase the many online courses you’ve taken or your fancy
    education didn’t go down the drain.
  prefs: []
  type: TYPE_NORMAL
- en: With an air of *total* coolness and confidence, you intuitively select the most
    suitable algorithms and model architectures for the given task. Here, your aim
    is not always to choose the best-performing model, but rather the model that is
    most suitable for the problem within the existing constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'These constraints act as five giant barriers in your brain:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Computation resources**: You opt for simpler and smaller models if hardware
    is limited, even at the cost of lower performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Time constraints**: You lean towards old linear models or shallow trees when
    forests and deep learning models put too much pressure on your time and budget.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model complexity vs. performance trade-offs**: You strike a balance between
    powerful models with massive predictive power but with a tendency to overfit,
    and simpler models with lower complexity that may underfit.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Interpretability**: You prioritize linear models or trees when model explainability
    and stakeholder confidence are essential.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Dataset size**: You guide model selection based on the size of the data as
    well. Some models excel on large datasets, such as deep learning models, while
    models like SVMs are more suitable for small datasets.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model selection is an art, and you treat it like so.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Strategic cross-validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If there is one thing you enjoy more than splitting your data into three, it’s
    cross-validation. Whenever your time and resource constraints allow it, you try
    to use appropriate CV strategies all the time.
  prefs: []
  type: TYPE_NORMAL
- en: You are well aware that CV is the method that provides the most reliable estimation
    of model performance, making it your most valuable tool in combating overfitting
    or underfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the simple `KFold` CV, you leverage other powerful strategies,
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '`StratifiedKFold` - for classification tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`RepeatedKFold` - when you are particularly vigilant about accurate estimation
    of model performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`ShuffleSplit` - for more control over train/test splits.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`GroupKFold` - for non-IID data (independent and identically distributed).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`TimeSeriesSplit` - for time series data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[](/one-stop-tutorial-on-all-cross-validation-techniques-you-can-should-use-7e1645fb703c?source=post_page-----3b7085c600f1--------------------------------)
    [## One-Stop Tutorial On ALL Cross-Validation Techniques You Can (Should) Use'
  prefs: []
  type: TYPE_NORMAL
- en: Edit description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/one-stop-tutorial-on-all-cross-validation-techniques-you-can-should-use-7e1645fb703c?source=post_page-----3b7085c600f1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: In your cross-validation endeavors, Sklearn pipelines hold a special place.
    Instead of performing preprocessing outside and modeling inside the CV splitter,
    you combine them all into a single pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines are your extra safeguard against data leakage and allow you to write
    readable, modular code that is a delight for others to read.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/sklearn-pipelines-for-the-modern-ml-engineer-9-techniques-you-cant-ignore-637788f05df5?source=post_page-----3b7085c600f1--------------------------------)
    [## Sklearn Pipelines for the Modern ML Engineer: 9 Techniques You Can’t Ignore'
  prefs: []
  type: TYPE_NORMAL
- en: Edit description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/sklearn-pipelines-for-the-modern-ml-engineer-9-techniques-you-cant-ignore-637788f05df5?source=post_page-----3b7085c600f1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Total control over randomness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You don’t joke around with randomness. You have learned from experience that
    keeping a tight leash on the pseudo-random generator can save you hours in terms
    of:'
  prefs: []
  type: TYPE_NORMAL
- en: Reproducibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debugging and testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model performance comparison
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You are also justifiedly proud in knowing the nuances of various seeding methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`random_state=None` - Sklearn uses the global NumPy seed set with `np.random.seed(seed_number)`
    as the default behavior. In this case, each call to functions that involve randomness,
    such as `fit`, `train_test_split`, `split`, CV classes, etc., produces different
    results.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`random_state=integer` - calling functions (as mentioned above) that rely on
    randomness always produces the same results for the given integer seed. You have
    your favorite seed (mine is 1121218).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`random_state=np.random.RandomState` (an RNG instance) - this seeding method
    provides the most robust CV results, although passing integers is considered safer
    and preferable. It returns different results in each run.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And you have a weekly reminder on your iPhone to read the [Controlling randomness](https://scikit-learn.org/stable/common_pitfalls.html#controlling-randomness)
    section of the Sklearn user guide so that these details don’t leak out from memory.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Not using Sklearn for hyperparameter tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You respect for Sklearn for introducing `HalvingGridSearch` and `HalvingRandomSearch`
    to fix the slower-than-a-turtle `GridSearch` but at this point of your workflow,
    you kindly switch to other tools.
  prefs: []
  type: TYPE_NORMAL
- en: Like the cool kids of today, your go-to choice is Optuna. It’s more Pythonic,
    intelligent, and offers an awful lot of trinkets that Sklearn won’t even consider
    adding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your most favorite features of Optuna are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Seamless integration with various frameworks: Jupyter, Sklearn, XGBoost, CatBoost,
    TensorFlow, PyTorch, you name it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State-of-the-art tuning algorithms with names that you can’t even pronounce.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic elimination of unpromising hyperparameters before the tuning process
    even begins.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More Pythonic and readable parameter grids.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualization and analysis capabilities, including parameter importance plots,
    parallel coordinate plots, and optimization history plots.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed computing to leverage multiple cores.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And quite possibly, your favorite Optuna tutorial is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/why-is-everyone-at-kaggle-obsessed-with-optuna-for-hyperparameter-tuning-7608fdca337c?source=post_page-----3b7085c600f1--------------------------------)
    [## Why Is Everyone at Kaggle Obsessed with Optuna For Hyperparameter Tuning?'
  prefs: []
  type: TYPE_NORMAL
- en: Edit description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/why-is-everyone-at-kaggle-obsessed-with-optuna-for-hyperparameter-tuning-7608fdca337c?source=post_page-----3b7085c600f1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Model evaluation on another level
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just like you are a master at cooking the main course, you excel at preparing
    the dessert — model evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Countless times in your life, you’ve called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: to look at all the options you have for measuring your model’s performance.
    Rather than settling for a single metric from this list, you prefer to choose
    multiple metrics to assess and optimize your model from various angles.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd?source=post_page-----3b7085c600f1--------------------------------)
    [## Comprehensive Guide on Multiclass Classification Metrics'
  prefs: []
  type: TYPE_NORMAL
- en: Edit description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd?source=post_page-----3b7085c600f1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Your biggest strength in this regard is differentiating between all the crazy
    classification metrics and the subtle ways they change based on whether your chosen
    classifier is **OVO** or **OVR**.
  prefs: []
  type: TYPE_NORMAL
- en: You’ve also been called a geek a couple times for setting these images as auto-changing
    wallpapers on your phone (but you aren’t bothered in the slightest :).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1217baa6511f0acdda37c2ad02222ee8.png)![](../Images/0eb0fc3695b08fcda902effca46357fc.png)![](../Images/eee767ffe9c80acbef0b4632863a35c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Images by me
  prefs: []
  type: TYPE_NORMAL
- en: 'And trying hard not to show off, you use some of the following visualization
    tools for a more accurate depiction of your model’s performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eeca4994d950b5ed1a49b4b1845e0f34.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Screenshot of Sklearn user guide. BSD-3 clause license.](https://scikit-learn.org/stable/visualizations.html#available-plotting-utilities)'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though the title and article had a playful tone in acknowledging the old
    Sklearn pro, my underlying intention was to offer guidance to beginners. I wanted
    to bridge the gap between the worn-out, copy/paste knowledge rampant in cheap
    online courses and the practical knowledge and expertise that can only be gained
    through months of real-world practice and experience.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this article has provided both inspiration and a little entertainment
    for you to explore the depths of Sklearn and become a master at it!
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs: []
  type: TYPE_NORMAL
- en: Loved this article and, let’s face it, its bizarre writing style? Imagine having
    access to dozens more just like it, all written by a brilliant, charming, witty
    author (that’s me, by the way :).
  prefs: []
  type: TYPE_NORMAL
- en: For only 4.99$ membership, you will get access to not just my stories, but a
    treasure trove of knowledge from the best and brightest minds on Medium. And if
    you use [my referral link](https://ibexorigin.medium.com/membership), you will
    earn my supernova of gratitude and a virtual high-five for supporting my work.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://ibexorigin.medium.com/membership?source=post_page-----3b7085c600f1--------------------------------)
    [## Join Medium with my referral link - Bex T.'
  prefs: []
  type: TYPE_NORMAL
- en: Get exclusive access to all my ⚡premium⚡ content and all over Medium without
    limits. Support my work by buying me a…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ibexorigin.medium.com](https://ibexorigin.medium.com/membership?source=post_page-----3b7085c600f1--------------------------------)
    ![](../Images/ecb20212397d966ec82b2333cf9ec8f5.png)
  prefs: []
  type: TYPE_NORMAL
- en: Image by me with Midjourney
  prefs: []
  type: TYPE_NORMAL
