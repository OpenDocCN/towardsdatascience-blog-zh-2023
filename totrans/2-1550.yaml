- en: MusicLM — Has Google Solved AI Music Generation?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MusicLM — 谷歌是否解决了AI音乐生成问题？
- en: 原文：[https://towardsdatascience.com/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c](https://towardsdatascience.com/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c](https://towardsdatascience.com/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c)
- en: Paper and significance, explained
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 论文及其重要性解释
- en: '[](https://medium.com/@maxhilsdorf?source=post_page-----c6859e76bc3c--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page-----c6859e76bc3c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c6859e76bc3c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c6859e76bc3c--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page-----c6859e76bc3c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maxhilsdorf?source=post_page-----c6859e76bc3c--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page-----c6859e76bc3c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c6859e76bc3c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c6859e76bc3c--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page-----c6859e76bc3c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c6859e76bc3c--------------------------------)
    ·11 min read·Feb 2, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c6859e76bc3c--------------------------------)
    ·阅读时间11分钟·2023年2月2日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d9c2dc9f2b4448695343134c707b1f1d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d9c2dc9f2b4448695343134c707b1f1d.png)'
- en: Image by [Placidplace](https://pixabay.com/de/illustrations/sprecher-lautsprecher-7459276/).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Placidplace](https://pixabay.com/de/illustrations/sprecher-lautsprecher-7459276/)。
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: On January 25th, 2023, in a PowerPoint presentation, I described the generation
    of long sequences of high-quality music as one of the major challenges in the
    field of audio AI to be solved in the near future. One day later, my slides were
    outdated.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年1月25日的一个PowerPoint演示中，我描述了生成高质量长序列音乐作为音频AI领域在不久的将来需要解决的主要挑战之一。一天之后，我的幻灯片就过时了。
- en: MusicLM, developed by Google Research, generates minute-long high-quality music
    in all styles and genres based on a simple text query in natural human language.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: MusicLM由Google Research开发，可以根据自然语言中的简单文本查询生成风格和流派各异的高质量音乐，时长为一分钟。
- en: It is best you get your own impression and check out the [demo page](https://google-research.github.io/seanet/musiclm/examples/)
    with lots of musical examples. If you are interested in the details, feel free
    to check out the research paper as well, although this article will cover all
    the relevant topics as well.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最好还是亲自体验一下，查看充满音乐示例的[演示页面](https://google-research.github.io/seanet/musiclm/examples/)。如果你对细节感兴趣，也可以查阅研究论文，尽管本文也会涵盖所有相关主题。
- en: '![](../Images/4f5ba45fc72cd2e2716f904790cd3c8e.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f5ba45fc72cd2e2716f904790cd3c8e.png)'
- en: An excerpt from the MusicLM demo page. Image by the author.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 来自MusicLM演示页面的摘录。图片由作者提供。
- en: Now, what makes MusicLM such a technological leap? Which problems does it solve
    that have been troubling AI researchers over the last decade? And why do I still
    consider MusicLM a transitional technology — a bridge into a different world of
    producing music? These questions and more will be answered here without boring
    you with maths or too much tech jargon.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，是什么让MusicLM成为如此巨大的技术飞跃？它解决了过去十年困扰AI研究人员的哪些问题？为什么我仍然认为MusicLM是一种过渡技术——通向音乐制作不同世界的桥梁？这些问题以及更多内容将在这里解答，而不会让你感到枯燥的数学或过多的技术术语。
- en: 'Challenge 1: Translating Text into Music'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战1：将文本转换为音乐
- en: '![](../Images/9568e0eccfb60062ebbadd913b2b94a8.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9568e0eccfb60062ebbadd913b2b94a8.png)'
- en: Photo by [Debby Hudson](https://unsplash.com/@hudsoncrafted?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Debby Hudson](https://unsplash.com/@hudsoncrafted?utm_source=medium&utm_medium=referral)
    供图于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: MusicLM makes use of a recently released model that puts both, music and text,
    onto the same “map”. Like computing the distance from London to Stockholm, MusicLM
    can compute the “similarity” between audio-text pairs.
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: MusicLM利用了最近发布的一种模型，将音乐和文本都映射到同一个“地图”上。就像计算从伦敦到斯德哥尔摩的距离一样，MusicLM可以计算音频-文本对之间的“相似性”。
- en: Music is Difficult to Describe
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 音乐难以描述
- en: Translating text into music is a complex task because music is a multi-dimensional
    art form that involves not just the melody and harmony of the music, but also
    rhythm, tempo, timbre, and much more. In order to translate text into music, a
    machine learning model needs to be able to understand and interpret the meaning
    of the text, and then use that understanding to create a musical composition that
    accurately represents the text.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本转化为音乐是一项复杂的任务，因为音乐是一种多维度的艺术形式，不仅涉及音乐的旋律和和声，还包括节奏、速度、音色等等。为了将文本转化为音乐，机器学习模型需要能够理解和解释文本的含义，然后利用这种理解来创作出准确表现文本的音乐作品。
- en: Another problem with translating text into music is that music is a highly subjective
    art form. What one person considers to be “happy” music may sound “bittersweet”
    or “peaceful” to another. This makes it difficult for a machine learning model
    to create a composition that will be universally considered “happy”. Although
    music is often (falsely, in my estimation) described as a universal language,
    no objective translation from spoken language into music seems possible.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本转化为音乐的另一个问题是，音乐是一种高度主观的艺术形式。一个人认为是“快乐”的音乐可能会被另一个人听成是“苦涩”或“平静”。这使得机器学习模型很难创作出被普遍认为是“快乐”的作品。尽管音乐常被（在我看来是错误地）描述为一种普遍语言，但从口语到音乐的客观翻译似乎是不可能的。
- en: MusicLM’s approach
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MusicLM的方法
- en: With this in mind, it may surprise you that translating text into music is not
    the major contribution of MusicLM. Machine learning models that relate text to
    audio, images to text, or audio to images (we call them “cross-modal models”)
    have become rather established in academia & industry in the last 2–3 years. Certainly,
    the most famous example of a cross-modal model is [DALL-E 2](https://openai.com/dall-e-2/)
    which generates high-resolution images based on an input text.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，你可能会惊讶地发现，将文本转化为音乐并不是MusicLM的主要贡献。将文本与音频、图像与文本或音频与图像关联起来的机器学习模型（我们称之为“跨模态模型”）在过去2-3年里在学术界和工业界变得相当成熟。当然，最著名的跨模态模型之一是[DALL-E
    2](https://openai.com/dall-e-2/)，它根据输入文本生成高分辨率图像。
- en: In MusicLM, the researchers did not train the cross-modal part themselves. Instead,
    they make use of a pre-trained model called “MuLan” which was released in 2022
    (see the paper [here](https://arxiv.org/pdf/2208.12415.pdf)). MuLan was trained
    to relate music to text through a method called “contrastive learning”. Here,
    the training data typically consists of thousands of pairs of music with an associated
    text describing the music. The learning goal is that, when presented with any
    pair of music and text (not necessarily associated), the model can tell if the
    text belongs to the music or not. Once this is achieved, the model is able to
    compute the degree of similarity between audio-audio, text-audio, or text-text
    pairs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在MusicLM中，研究人员并没有自己训练跨模态部分。相反，他们利用了一个名为“MuLan”的预训练模型，该模型于2022年发布（详见论文[这里](https://arxiv.org/pdf/2208.12415.pdf)）。MuLan通过一种称为“对比学习”的方法训练，以将音乐与文本关联起来。在这里，训练数据通常包括成千上万对音乐及其描述文本的配对。学习目标是，当呈现任何一对音乐和文本（不一定是关联的）时，模型能够判断文本是否属于音乐。一旦实现了这一点，模型就能够计算音频-音频、文本-音频或文本-文本对之间的相似度。
- en: 'Challenge 2: Reducing Time & Resources to Generate Music'
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战2：减少生成音乐的时间和资源
- en: '![](../Images/9ec957f37e0d23c04520b84abb9bdfdc.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ec957f37e0d23c04520b84abb9bdfdc.png)'
- en: Photo by [Agê Barros](https://unsplash.com/@agebarros?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Agê Barros](https://unsplash.com/@agebarros?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: MusicLM makes use of a state-of-the-art audio compression tool to drastically
    reduce the amount of information needed to produce high-quality audio signals.
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: MusicLM利用最先进的音频压缩工具，显著减少了生成高质量音频信号所需的信息量。
- en: At this point, the model is able to tell whether the music it generates fits
    the given text input. However, there are some challenges associated with the audio
    generation process itself, a major one being the time and resources needed to
    create a piece of music.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，该模型能够判断它生成的音乐是否符合给定的文本输入。然而，音频生成过程本身存在一些挑战，主要是创建一段音乐所需的时间和资源。
- en: The Dimensionality Issue
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维度问题
- en: Although music is easy to process for our human ears, it is quite a complex
    type of data to deal with for data scientists. A regular pop song (3:30 min) at
    CD quality is stored in a computer as a vector of almost 10 million numbers. By
    comparison, a picture in HD quality (1280 x 720 pixels) does not even reach 1
    million values to store and process. Over the last couple of years, many methods
    have been developed to compress music into a less computationally expensive format
    while maintaining high-quality sound.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管音乐对我们人耳来说很容易处理，但对于数据科学家来说，它是一种相当复杂的数据类型。普通的流行歌曲（3:30 min）在CD质量下以几乎1000万个数字存储在计算机中。相比之下，高清质量的图片（1280
    x 720像素）存储和处理的值甚至不到100万。近年来，已经开发了许多方法，将音乐压缩成计算上更便宜的格式，同时保持高质量的声音。
- en: 'With traditional approaches, generating 1 minute of music at CD quality (44100
    Hz) would require a machine learning model to generate around 2.6 million numbers
    — one after the other. If generating one number takes only 0.01 seconds, this
    process would still take over 7 hours to complete. It is not hard to imagine that
    if you had asked a professional musician to compose and record the music, they
    would solve the task faster. The key point is: so far, there has been a huge trade-off
    between fast audio generation and the quality of the output.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用传统的方法，生成1分钟的CD质量音乐（44100 Hz）需要机器学习模型生成大约260万个数字——一个接一个。如果生成一个数字仅需0.01秒，那么这个过程仍然需要超过7小时来完成。不难想象，如果你请一位专业音乐家作曲和录制音乐，他们会更快地完成任务。关键点是：到目前为止，在快速音频生成和输出质量之间存在巨大的权衡。
- en: Previous Approaches
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 以往的方法
- en: Many attempts have been made to tackle this issue. One rather new approach is
    to generate audio indirectly, by first generating an image representation of the
    audio signal (for example, a “spectrogram”) and then converting this image to
    “real” audio (as done in [“Riffusion”](https://www.riffusion.com/)). Another approach
    is to avoid generating audio directly by creating a symbolic representation instead.
    The most widely known symbolic representation of music is sheet music. As you
    will know, a music sheet is not a real audio event, but a musician is able to
    translate it into one. In the past, we have seen quite a bit of success from machine
    learning models generating music in the symbolic MIDI format (see Magenta’s [Chamber
    Ensemble Generator](https://magenta.tensorflow.org/ceg-and-cocochorales), for
    instance). Both of these methods have their weaknesses, however, and exist mostly
    because generating “the real thing” is so difficult.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为解决这个问题，已经尝试了许多方法。一种较新的方法是间接生成音频，即首先生成音频信号的图像表示（例如，“频谱图”），然后将此图像转换为“真实”音频（如在
    [“Riffusion”](https://www.riffusion.com/) 中所做）。另一种方法是通过创建符号表示来避免直接生成音频。音乐的最广泛知晓的符号表示是乐谱。如你所知，乐谱并不是一个真实的音频事件，但音乐家能够将其转换为真实的音频。过去，我们已经看到机器学习模型在符号MIDI格式下生成音乐的相当成功（例如，参见
    Magenta 的 [Chamber Ensemble Generator](https://magenta.tensorflow.org/ceg-and-cocochorales)）。然而，这两种方法都有其弱点，主要是因为生成“真实的东西”非常困难。
- en: MusicLM’s Approach
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MusicLM的 Approach
- en: Finally, let’s discuss the approach that MusicLM takes. Instead of generating
    a proxy (like image or MIDI) for audio, MusicLM applies a state-of-the-art audio
    compression algorithm called [“SoundStream”](https://ai.googleblog.com/2021/08/soundstream-end-to-end-neural-audio.html),
    published in 2021\. With SoundStream, the model is able to generate audio at 24
    kHz (24,000 numbers per second of audio) while actually computing only 600 numbers
    itself. The mapping from 600 values per second to 24,000 values per second is
    handled by SoundStream. In other words, the model needs to generate 97.5% less
    information while achieving approximately the same result. While there have been
    other great compression algorithms in the past, SoundStream beats all of them
    considerably.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们讨论 MusicLM 采用的方法。MusicLM 并不是生成音频的代理（如图像或MIDI），而是应用了一种最先进的音频压缩算法 [“SoundStream”](https://ai.googleblog.com/2021/08/soundstream-end-to-end-neural-audio.html)，该算法于2021年发布。使用
    SoundStream，模型能够以24 kHz（每秒24000个数字）的速度生成音频，同时实际计算仅600个数字。每秒从600个值映射到24000个值的工作由
    SoundStream 处理。换句话说，模型需要生成97.5%更少的信息，同时实现大致相同的结果。虽然过去也有其他优秀的压缩算法，但 SoundStream
    明显优于所有这些算法。
- en: 'Challenge 3: Generating Coherent and Authentic Music'
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战3：生成连贯且真实的音乐
- en: '![](../Images/37e744a72a75e434f814ba5a9f720f31.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37e744a72a75e434f814ba5a9f720f31.png)'
- en: Photo by [Brantley Neal](https://unsplash.com/@brantley_neal?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Brantley Neal](https://unsplash.com/@brantley_neal?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: By separating the task of relating text to music from the actual audio generation
    part, MusicLM could be trained on hundreds of thousands of hours on unlabelled
    audio data. This contributed to the richness of its generated music.
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通过将文本与音乐的关联任务与实际音频生成部分分开，MusicLM可以在数十万小时的未标注音频数据上进行训练。这有助于其生成音乐的丰富性。
- en: Terminology
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 术语
- en: It is certainly up for debate what “coherent” and “authentic” music actually
    is. In the context of AI-generated music, however, it could be argued that the
    fact that we would even consider calling MusicLM’s compositions “coherent” and
    “authentic” says enough already. For a loose working definition, let us say that
    coherent music has an underlying structure that is acted out through different
    sections and/or through repetition, alteration, or citation of musical ideas.
    By “authentic”, I mean that an AI-generated piece of music presents itself in
    a way that could convince us that a human being could have purposefully crafted
    it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“连贯”和“真实”音乐究竟是什么，确实值得讨论。然而，在AI生成音乐的背景下，可以说，我们甚至会考虑将MusicLM的作品称为“连贯”和“真实”，这一点已经足够说明问题。对于一个宽松的工作定义，假设连贯的音乐具有一个通过不同部分和/或通过音乐创意的重复、变化或引用来表现的基础结构。这里的“真实”指的是，AI生成的音乐呈现出一种能够让我们相信是人类有意创作的方式。
- en: Musical “Memory”
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 音乐的“记忆”
- en: Generating coherent music is not a breakthrough of MusicLM. As early as 2018,
    [“Music Transformer”](https://magenta.tensorflow.org/music-transformer) by Google
    Magenta could compose MIDI music with clear melodic and harmonic sequences where
    musical ideas repeated themselves or were altered. Music Transformer is able to
    keep track of musical events that are more than 45 seconds in the past. However,
    since raw audio is so much more complex than a symbolic MIDI representation, such
    a large “memory” has long been unachievable for models generating raw audio. MusicLM
    has a “memory” of 30 seconds, which is more than that of any comparable model
    I know of (although I may be wrong here — there have been so many models released…).
    While this does not allow MusicLM to compose epic 15-minute-long masterpieces,
    that is still enough to maintain basic musical structures like tempo, rhythm,
    harmony, and timbre for a long period of time.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 生成连贯的音乐并不是MusicLM的突破。早在2018年，Google Magenta的 [“音乐变换器”](https://magenta.tensorflow.org/music-transformer)
    就能够创作具有清晰旋律和和声序列的MIDI音乐，其中音乐创意会重复或被改变。音乐变换器能够跟踪过去超过45秒的音乐事件。然而，由于原始音频远比符号MIDI表示复杂，这样的大“记忆”对于生成原始音频的模型来说长期以来都是难以实现的。MusicLM拥有30秒的“记忆”，这比我所知道的任何类似模型都要多（尽管我可能错了——发布的模型实在是太多了……）。虽然这并不允许MusicLM创作出长达15分钟的史诗作品，但仍足以维持基本的音乐结构，如节奏、韵律、和声和音色，较长时间内不会丧失。
- en: Authentic Outputs
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 真实输出
- en: What is even more significant, in my estimation, is that the music composed
    by MusicLM sounds surprisingly authentic. A technical explanation for this could
    be that MusicLM found a clever way to train a text-to-music model on thousands
    of hours of unlabelled music, that is music without text descriptions. By using
    the pre-trained “MuLan” model for relating text to music, they designed their
    model architecture so that it can learn the audio generation part separately from
    unlabelled audio data. The underlying assumption is that relating music to text
    is not as difficult as creating authentic-sounding music. This “trick” of re-framing
    the problem and adapting the architecture to that may be a key factor in MusicLM’s
    success.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，在我看来，MusicLM创作的音乐听起来出奇地真实。对此的技术解释可能是，MusicLM找到了一种巧妙的方法，利用数千小时的未标注音乐（即没有文本描述的音乐）来训练文本到音乐的模型。通过使用预训练的“MuLan”模型来关联文本和音乐，他们设计了模型架构，使其可以将音频生成部分与未标注音频数据分开学习。其基本假设是，将音乐与文本关联并不像创作真实音乐那么困难。将问题重新定义并调整架构的这一“技巧”可能是MusicLM成功的关键因素。
- en: In some sense, the results speak for themselves. For the first time, an AI model
    does not create something that is either an intermediary product somewhere between
    a composition and a piece of music, or something that could be distinguished from
    human-made music by any 4-year-old. It truly feels like something different this
    time. It feels similar to the first time I read a text written by GPT-3\. Similar
    to the first time I saw an image generated by DALL-E-2\. MusicLM may just be THAT
    breakthrough AI-music model that will go down in history.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种意义上说，结果不言而喻。首次出现的AI模型不仅仅是介于作曲和音乐之间的中间产品，或者是任何4岁儿童都能区分开的人造音乐。它这次确实感觉有所不同。感觉类似于我第一次阅读GPT-3写的文本时。类似于我第一次看到DALL-E-2生成的图像时。MusicLM可能正是那个将在历史上留下印记的突破性AI音乐模型。
- en: Shortcomings of MusicLM & Future Perspectives
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MusicLM的不足与未来展望
- en: '![](../Images/6abdcd10427ee0b626f57077a868d7c8.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6abdcd10427ee0b626f57077a868d7c8.png)'
- en: Photo by [Kenny Eliason](https://unsplash.com/@neonbrand?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kenny Eliason](https://unsplash.com/@neonbrand?utm_source=medium&utm_medium=referral)拍摄于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
- en: Quantitative Shortcomings
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定量不足
- en: Despite all these amazing qualities of MusicLM, the model is by no means perfect.
    I would even say that, compared to models like GPT-3 for text or DALL-E-2 for
    images, MusicLM seems much more limited. One reason is that the music generated
    is considered high-quality only by the machine learning community. Without an
    effective way to upsample the 24 kHz music to 44.1 kHz, the generated pieces can
    never be used in the real world, because when you listen carefully, the quality
    difference between CD recordings and MusicLM’s output is noticeable, even for
    non-experts. While a 1024 x 1024 image (as generated by DALL-E-2) can already
    be used for websites, blog posts, etc., a piece of music at 24kHz will always
    be considered substandard.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管MusicLM具备所有这些令人惊叹的特点，但该模型绝非完美。我甚至会说，与文本的GPT-3或图像的DALL-E-2等模型相比，MusicLM似乎更加有限。一个原因是，生成的音乐只有机器学习社区认为是高质量的。如果没有有效的方法将24
    kHz的音乐上采样到44.1 kHz，生成的作品将永远无法在现实世界中使用，因为仔细聆听时，即使是非专家也能注意到CD录音和MusicLM输出之间的质量差异。而一个1024
    x 1024的图像（如DALL-E-2生成的）已经可以用于网站、博客等，而24kHz的音乐始终会被视为不合格。
- en: Similarly, while a “memory” of 30 seconds is impressive for an audio machine
    learning model, a trained composer can write hours of coherent music and a trained
    musician can easily perform it. There is still a long way to go for machine learning
    models to catch up to humans in this regard. However, both, the sampling rate,
    and the “memory” of the model will undoubtedly increase as more computing resources
    are available. Additionally, improvements in audio compression and machine learning
    methods could accelerate this process further. Seeing how fast generative AI models
    have been improving over the last 2–3 years, I am positive that these issues will
    be more or less mitigated by the end of this year.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，虽然30秒的“记忆”对于一个音频机器学习模型来说令人印象深刻，但一个训练有素的作曲家可以创作数小时的连贯音乐，训练有素的音乐家也可以轻松演奏这些音乐。机器学习模型在这方面要赶上人类还有很长的路要走。然而，随着计算资源的增加，模型的采样率和“记忆”无疑会增加。此外，音频压缩和机器学习方法的改进可能会进一步加速这一过程。看到生成式AI模型在过去2-3年里提高得如此迅速，我相信这些问题在今年年底之前或多或少会得到缓解。
- en: Qualitative / Ethical Shortcomings
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 质量/伦理不足
- en: 'However, there is also something that cannot be solved by scale alone: the
    issue of intellectual property. In the recent past, many large generative models
    have been subject to copyright lawsuits ([GitHub Copilot](https://www.theverge.com/2022/11/8/23446821/microsoft-openai-github-copilot-class-action-lawsuit-ai-copyright-violation-training-data)
    & [StableDiffusion](https://indianexpress.com/article/technology/ai-image-tools-stable-diffusion-midjourney-sued-by-artists-8389495/#:~:text=Stable%20Diffusion%20is%20currently%20facing,that%20were%20protected%20by%20copyright.),
    just to name two). In most cases, the models were trained on data that was not
    intended for commercial use. And although the model’s creations are “new”, you
    can make the argument that it still “uses” the training data commercially. The
    same applies to MusicLM. What’s more, there is always a real possibility of getting
    unlucky and generating something that “steals” entire melodies or chord sequences
    from a copyright-protected piece.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也有一些问题无法仅通过规模解决：知识产权问题。在最近的过去，许多大型生成模型已遭遇版权诉讼（[GitHub Copilot](https://www.theverge.com/2022/11/8/23446821/microsoft-openai-github-copilot-class-action-lawsuit-ai-copyright-violation-training-data)
    & [StableDiffusion](https://indianexpress.com/article/technology/ai-image-tools-stable-diffusion-midjourney-sued-by-artists-8389495/#:~:text=Stable%20Diffusion%20is%20currently%20facing,that%20were%20protected%20by%20copyright.)，仅举其中两个）。在大多数情况下，这些模型是在未经商业用途许可的数据上进行训练的。虽然模型的创作是“新的”，但你可以认为它仍然“商业性地使用”了训练数据。MusicLM也是如此。此外，总是有可能生成“窃取”了整个旋律或和弦序列的版权保护作品的内容。
- en: 'In the MusicLM paper, there’s a probability of less than 0.2% to generate an
    “exact match” with a piece of music from the training data. While that does sound
    low, keep in mind that — assuming a rate of 0.2% — 1 in 500 generated tracks would
    be a safe suspect for copyright claims. It is almost certain that bigger datasets
    with more variety as well as improved model architectures or training algorithms
    can help bring this rate down, but the core issue remains, as it does in other
    domains like image or text: If we plan to use a generative AI model trained on
    copyright-protected data, we cannot generate outputs on a massive scale without
    risking major legal consequences. However, this is not only a financial risk but
    also a major ethical concern.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在MusicLM论文中，生成与训练数据中的一段音乐“完全匹配”的概率不到0.2%。虽然这个概率听起来很低，但请记住——假设0.2%的生成率——每500个生成的曲目中可能有1个是版权声明的安全嫌疑对象。更大的数据集、更多的多样性以及改进的模型架构或训练算法几乎肯定能帮助降低这一比率，但核心问题依然存在，就像图像或文本等其他领域一样：如果我们计划使用在版权保护数据上训练的生成型AI模型，我们不能在大规模上生成输出而不冒重大法律风险。然而，这不仅仅是财务风险，也是一个重大的伦理问题。
- en: Additionally, neither MusicLM nor the training data were publicly released.
    This raises ethical concerns about the transparency and accountability of AI systems.
    As AI models like MusicLM have the potential to disrupt an entire industry, it
    is important that the development process and methodology are open to scrutiny.
    This would enable researchers to understand how the model was trained, evaluate
    its biases, and identify any limitations that may affect its outputs. Without
    access to the model, it becomes difficult to assess its impact on society and
    the potential risks that it poses.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，MusicLM及其训练数据都没有公开发布。这引发了关于AI系统透明度和问责制的伦理问题。由于像MusicLM这样的AI模型有可能颠覆整个行业，因此开发过程和方法论应当接受审查。这将使研究人员能够了解模型的训练方式，评估其偏见，并识别可能影响其输出的任何局限性。如果没有模型的访问权限，很难评估其对社会的影响以及可能带来的风险。
- en: Finally, it is unclear what the business use cases for MusicLM or future models
    are. There are already millions of people in the world producing great music effectively
    for free. Bringing the cost of musical composition down by replacing humans with
    machines is therefore not even economically effective, not to mention ethically
    undesirable. While there will certainly be ways to make money with MusicLM as
    it is, I see even more potential and value in generative AI as assistants for
    human composers, allowing them to quickly prototype musical ideas and focus on
    creating artistic value for the world.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，目前尚不清楚MusicLM或未来模型的商业应用场景。世界上已经有数以百万计的人在有效地免费制作优秀音乐。因此，通过用机器替代人类来降低音乐创作成本既在经济上没有效益，更不用说在伦理上是不可取的。尽管MusicLM仍有一定的盈利潜力，我认为生成型AI作为人类作曲家的助手，帮助他们快速原型化音乐创意并专注于为世界创造艺术价值，具有更大的潜力和价值。
- en: Future Perspectives
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 未来展望
- en: 'It is hard to say where the future will lead us in terms of generative AI for
    music. One thing is certain: MusicLM will be replaced and improved upon by even
    larger models utilizing an even larger dataset and even smarter algorithms. These
    models will undoubtedly be able to overcome many of MusicLM’s shortcomings. It
    seems inevitable that technologies like this will drastically disrupt the music
    market — and more likely sooner than later. However, I believe that focusing all
    of our attention on black-box models would be an error. The world, by and large,
    does not need machines for end-to-end music production. We have humans for that.
    What really matters is that we use AI technologies to bring more artistic value
    into this world by enabling new ways of inventing, creating, and enjoying music.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 很难预测未来在音乐生成AI方面会带我们走向何方。可以确定的一点是：MusicLM 将被更大规模的模型取代，这些模型将使用更大的数据集和更智能的算法。这些模型无疑能够克服许多
    MusicLM 的不足之处。似乎像这样的技术会不可避免地彻底改变音乐市场——而且这种改变可能会比我们预期的更早。然而，我认为将全部注意力集中在黑箱模型上是一个错误。总体而言，世界并不需要完全依赖机器进行音乐制作。我们有的是人类来完成这一任务。真正重要的是我们应当利用
    AI 技术，通过开辟新的发明、创造和享受音乐的方式，将更多的艺术价值带入这个世界。
- en: '**Thanks for reading this article!** I write mostly about the intersection
    of AI and music, so if that interests you, you may also like my other work.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**感谢阅读本文！** 我主要撰写有关 AI 和音乐交集的内容，如果你对此感兴趣，你可能还会喜欢我的其他作品。'
