- en: MusicLM — Has Google Solved AI Music Generation?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c](https://towardsdatascience.com/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Paper and significance, explained
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@maxhilsdorf?source=post_page-----c6859e76bc3c--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page-----c6859e76bc3c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c6859e76bc3c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c6859e76bc3c--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page-----c6859e76bc3c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c6859e76bc3c--------------------------------)
    ·11 min read·Feb 2, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d9c2dc9f2b4448695343134c707b1f1d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Placidplace](https://pixabay.com/de/illustrations/sprecher-lautsprecher-7459276/).
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On January 25th, 2023, in a PowerPoint presentation, I described the generation
    of long sequences of high-quality music as one of the major challenges in the
    field of audio AI to be solved in the near future. One day later, my slides were
    outdated.
  prefs: []
  type: TYPE_NORMAL
- en: MusicLM, developed by Google Research, generates minute-long high-quality music
    in all styles and genres based on a simple text query in natural human language.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is best you get your own impression and check out the [demo page](https://google-research.github.io/seanet/musiclm/examples/)
    with lots of musical examples. If you are interested in the details, feel free
    to check out the research paper as well, although this article will cover all
    the relevant topics as well.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f5ba45fc72cd2e2716f904790cd3c8e.png)'
  prefs: []
  type: TYPE_IMG
- en: An excerpt from the MusicLM demo page. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Now, what makes MusicLM such a technological leap? Which problems does it solve
    that have been troubling AI researchers over the last decade? And why do I still
    consider MusicLM a transitional technology — a bridge into a different world of
    producing music? These questions and more will be answered here without boring
    you with maths or too much tech jargon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Challenge 1: Translating Text into Music'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/9568e0eccfb60062ebbadd913b2b94a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Debby Hudson](https://unsplash.com/@hudsoncrafted?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: MusicLM makes use of a recently released model that puts both, music and text,
    onto the same “map”. Like computing the distance from London to Stockholm, MusicLM
    can compute the “similarity” between audio-text pairs.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Music is Difficult to Describe
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Translating text into music is a complex task because music is a multi-dimensional
    art form that involves not just the melody and harmony of the music, but also
    rhythm, tempo, timbre, and much more. In order to translate text into music, a
    machine learning model needs to be able to understand and interpret the meaning
    of the text, and then use that understanding to create a musical composition that
    accurately represents the text.
  prefs: []
  type: TYPE_NORMAL
- en: Another problem with translating text into music is that music is a highly subjective
    art form. What one person considers to be “happy” music may sound “bittersweet”
    or “peaceful” to another. This makes it difficult for a machine learning model
    to create a composition that will be universally considered “happy”. Although
    music is often (falsely, in my estimation) described as a universal language,
    no objective translation from spoken language into music seems possible.
  prefs: []
  type: TYPE_NORMAL
- en: MusicLM’s approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With this in mind, it may surprise you that translating text into music is not
    the major contribution of MusicLM. Machine learning models that relate text to
    audio, images to text, or audio to images (we call them “cross-modal models”)
    have become rather established in academia & industry in the last 2–3 years. Certainly,
    the most famous example of a cross-modal model is [DALL-E 2](https://openai.com/dall-e-2/)
    which generates high-resolution images based on an input text.
  prefs: []
  type: TYPE_NORMAL
- en: In MusicLM, the researchers did not train the cross-modal part themselves. Instead,
    they make use of a pre-trained model called “MuLan” which was released in 2022
    (see the paper [here](https://arxiv.org/pdf/2208.12415.pdf)). MuLan was trained
    to relate music to text through a method called “contrastive learning”. Here,
    the training data typically consists of thousands of pairs of music with an associated
    text describing the music. The learning goal is that, when presented with any
    pair of music and text (not necessarily associated), the model can tell if the
    text belongs to the music or not. Once this is achieved, the model is able to
    compute the degree of similarity between audio-audio, text-audio, or text-text
    pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Challenge 2: Reducing Time & Resources to Generate Music'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/9ec957f37e0d23c04520b84abb9bdfdc.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Agê Barros](https://unsplash.com/@agebarros?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: MusicLM makes use of a state-of-the-art audio compression tool to drastically
    reduce the amount of information needed to produce high-quality audio signals.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: At this point, the model is able to tell whether the music it generates fits
    the given text input. However, there are some challenges associated with the audio
    generation process itself, a major one being the time and resources needed to
    create a piece of music.
  prefs: []
  type: TYPE_NORMAL
- en: The Dimensionality Issue
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although music is easy to process for our human ears, it is quite a complex
    type of data to deal with for data scientists. A regular pop song (3:30 min) at
    CD quality is stored in a computer as a vector of almost 10 million numbers. By
    comparison, a picture in HD quality (1280 x 720 pixels) does not even reach 1
    million values to store and process. Over the last couple of years, many methods
    have been developed to compress music into a less computationally expensive format
    while maintaining high-quality sound.
  prefs: []
  type: TYPE_NORMAL
- en: 'With traditional approaches, generating 1 minute of music at CD quality (44100
    Hz) would require a machine learning model to generate around 2.6 million numbers
    — one after the other. If generating one number takes only 0.01 seconds, this
    process would still take over 7 hours to complete. It is not hard to imagine that
    if you had asked a professional musician to compose and record the music, they
    would solve the task faster. The key point is: so far, there has been a huge trade-off
    between fast audio generation and the quality of the output.'
  prefs: []
  type: TYPE_NORMAL
- en: Previous Approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many attempts have been made to tackle this issue. One rather new approach is
    to generate audio indirectly, by first generating an image representation of the
    audio signal (for example, a “spectrogram”) and then converting this image to
    “real” audio (as done in [“Riffusion”](https://www.riffusion.com/)). Another approach
    is to avoid generating audio directly by creating a symbolic representation instead.
    The most widely known symbolic representation of music is sheet music. As you
    will know, a music sheet is not a real audio event, but a musician is able to
    translate it into one. In the past, we have seen quite a bit of success from machine
    learning models generating music in the symbolic MIDI format (see Magenta’s [Chamber
    Ensemble Generator](https://magenta.tensorflow.org/ceg-and-cocochorales), for
    instance). Both of these methods have their weaknesses, however, and exist mostly
    because generating “the real thing” is so difficult.
  prefs: []
  type: TYPE_NORMAL
- en: MusicLM’s Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, let’s discuss the approach that MusicLM takes. Instead of generating
    a proxy (like image or MIDI) for audio, MusicLM applies a state-of-the-art audio
    compression algorithm called [“SoundStream”](https://ai.googleblog.com/2021/08/soundstream-end-to-end-neural-audio.html),
    published in 2021\. With SoundStream, the model is able to generate audio at 24
    kHz (24,000 numbers per second of audio) while actually computing only 600 numbers
    itself. The mapping from 600 values per second to 24,000 values per second is
    handled by SoundStream. In other words, the model needs to generate 97.5% less
    information while achieving approximately the same result. While there have been
    other great compression algorithms in the past, SoundStream beats all of them
    considerably.
  prefs: []
  type: TYPE_NORMAL
- en: 'Challenge 3: Generating Coherent and Authentic Music'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/37e744a72a75e434f814ba5a9f720f31.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Brantley Neal](https://unsplash.com/@brantley_neal?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: By separating the task of relating text to music from the actual audio generation
    part, MusicLM could be trained on hundreds of thousands of hours on unlabelled
    audio data. This contributed to the richness of its generated music.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Terminology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is certainly up for debate what “coherent” and “authentic” music actually
    is. In the context of AI-generated music, however, it could be argued that the
    fact that we would even consider calling MusicLM’s compositions “coherent” and
    “authentic” says enough already. For a loose working definition, let us say that
    coherent music has an underlying structure that is acted out through different
    sections and/or through repetition, alteration, or citation of musical ideas.
    By “authentic”, I mean that an AI-generated piece of music presents itself in
    a way that could convince us that a human being could have purposefully crafted
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Musical “Memory”
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generating coherent music is not a breakthrough of MusicLM. As early as 2018,
    [“Music Transformer”](https://magenta.tensorflow.org/music-transformer) by Google
    Magenta could compose MIDI music with clear melodic and harmonic sequences where
    musical ideas repeated themselves or were altered. Music Transformer is able to
    keep track of musical events that are more than 45 seconds in the past. However,
    since raw audio is so much more complex than a symbolic MIDI representation, such
    a large “memory” has long been unachievable for models generating raw audio. MusicLM
    has a “memory” of 30 seconds, which is more than that of any comparable model
    I know of (although I may be wrong here — there have been so many models released…).
    While this does not allow MusicLM to compose epic 15-minute-long masterpieces,
    that is still enough to maintain basic musical structures like tempo, rhythm,
    harmony, and timbre for a long period of time.
  prefs: []
  type: TYPE_NORMAL
- en: Authentic Outputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is even more significant, in my estimation, is that the music composed
    by MusicLM sounds surprisingly authentic. A technical explanation for this could
    be that MusicLM found a clever way to train a text-to-music model on thousands
    of hours of unlabelled music, that is music without text descriptions. By using
    the pre-trained “MuLan” model for relating text to music, they designed their
    model architecture so that it can learn the audio generation part separately from
    unlabelled audio data. The underlying assumption is that relating music to text
    is not as difficult as creating authentic-sounding music. This “trick” of re-framing
    the problem and adapting the architecture to that may be a key factor in MusicLM’s
    success.
  prefs: []
  type: TYPE_NORMAL
- en: In some sense, the results speak for themselves. For the first time, an AI model
    does not create something that is either an intermediary product somewhere between
    a composition and a piece of music, or something that could be distinguished from
    human-made music by any 4-year-old. It truly feels like something different this
    time. It feels similar to the first time I read a text written by GPT-3\. Similar
    to the first time I saw an image generated by DALL-E-2\. MusicLM may just be THAT
    breakthrough AI-music model that will go down in history.
  prefs: []
  type: TYPE_NORMAL
- en: Shortcomings of MusicLM & Future Perspectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/6abdcd10427ee0b626f57077a868d7c8.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Kenny Eliason](https://unsplash.com/@neonbrand?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Quantitative Shortcomings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite all these amazing qualities of MusicLM, the model is by no means perfect.
    I would even say that, compared to models like GPT-3 for text or DALL-E-2 for
    images, MusicLM seems much more limited. One reason is that the music generated
    is considered high-quality only by the machine learning community. Without an
    effective way to upsample the 24 kHz music to 44.1 kHz, the generated pieces can
    never be used in the real world, because when you listen carefully, the quality
    difference between CD recordings and MusicLM’s output is noticeable, even for
    non-experts. While a 1024 x 1024 image (as generated by DALL-E-2) can already
    be used for websites, blog posts, etc., a piece of music at 24kHz will always
    be considered substandard.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, while a “memory” of 30 seconds is impressive for an audio machine
    learning model, a trained composer can write hours of coherent music and a trained
    musician can easily perform it. There is still a long way to go for machine learning
    models to catch up to humans in this regard. However, both, the sampling rate,
    and the “memory” of the model will undoubtedly increase as more computing resources
    are available. Additionally, improvements in audio compression and machine learning
    methods could accelerate this process further. Seeing how fast generative AI models
    have been improving over the last 2–3 years, I am positive that these issues will
    be more or less mitigated by the end of this year.
  prefs: []
  type: TYPE_NORMAL
- en: Qualitative / Ethical Shortcomings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'However, there is also something that cannot be solved by scale alone: the
    issue of intellectual property. In the recent past, many large generative models
    have been subject to copyright lawsuits ([GitHub Copilot](https://www.theverge.com/2022/11/8/23446821/microsoft-openai-github-copilot-class-action-lawsuit-ai-copyright-violation-training-data)
    & [StableDiffusion](https://indianexpress.com/article/technology/ai-image-tools-stable-diffusion-midjourney-sued-by-artists-8389495/#:~:text=Stable%20Diffusion%20is%20currently%20facing,that%20were%20protected%20by%20copyright.),
    just to name two). In most cases, the models were trained on data that was not
    intended for commercial use. And although the model’s creations are “new”, you
    can make the argument that it still “uses” the training data commercially. The
    same applies to MusicLM. What’s more, there is always a real possibility of getting
    unlucky and generating something that “steals” entire melodies or chord sequences
    from a copyright-protected piece.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the MusicLM paper, there’s a probability of less than 0.2% to generate an
    “exact match” with a piece of music from the training data. While that does sound
    low, keep in mind that — assuming a rate of 0.2% — 1 in 500 generated tracks would
    be a safe suspect for copyright claims. It is almost certain that bigger datasets
    with more variety as well as improved model architectures or training algorithms
    can help bring this rate down, but the core issue remains, as it does in other
    domains like image or text: If we plan to use a generative AI model trained on
    copyright-protected data, we cannot generate outputs on a massive scale without
    risking major legal consequences. However, this is not only a financial risk but
    also a major ethical concern.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, neither MusicLM nor the training data were publicly released.
    This raises ethical concerns about the transparency and accountability of AI systems.
    As AI models like MusicLM have the potential to disrupt an entire industry, it
    is important that the development process and methodology are open to scrutiny.
    This would enable researchers to understand how the model was trained, evaluate
    its biases, and identify any limitations that may affect its outputs. Without
    access to the model, it becomes difficult to assess its impact on society and
    the potential risks that it poses.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it is unclear what the business use cases for MusicLM or future models
    are. There are already millions of people in the world producing great music effectively
    for free. Bringing the cost of musical composition down by replacing humans with
    machines is therefore not even economically effective, not to mention ethically
    undesirable. While there will certainly be ways to make money with MusicLM as
    it is, I see even more potential and value in generative AI as assistants for
    human composers, allowing them to quickly prototype musical ideas and focus on
    creating artistic value for the world.
  prefs: []
  type: TYPE_NORMAL
- en: Future Perspectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is hard to say where the future will lead us in terms of generative AI for
    music. One thing is certain: MusicLM will be replaced and improved upon by even
    larger models utilizing an even larger dataset and even smarter algorithms. These
    models will undoubtedly be able to overcome many of MusicLM’s shortcomings. It
    seems inevitable that technologies like this will drastically disrupt the music
    market — and more likely sooner than later. However, I believe that focusing all
    of our attention on black-box models would be an error. The world, by and large,
    does not need machines for end-to-end music production. We have humans for that.
    What really matters is that we use AI technologies to bring more artistic value
    into this world by enabling new ways of inventing, creating, and enjoying music.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Thanks for reading this article!** I write mostly about the intersection
    of AI and music, so if that interests you, you may also like my other work.'
  prefs: []
  type: TYPE_NORMAL
