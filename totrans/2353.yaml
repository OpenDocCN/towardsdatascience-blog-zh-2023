- en: Where Are All the Women?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/where-are-all-the-women-3c79dabfdfc2](https://towardsdatascience.com/where-are-all-the-women-3c79dabfdfc2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exploring large language models’ biases in historical knowledge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@artfish?source=post_page-----3c79dabfdfc2--------------------------------)[![Yennie
    Jun](../Images/b635e965f21c3d55833269e12e861322.png)](https://medium.com/@artfish?source=post_page-----3c79dabfdfc2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3c79dabfdfc2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3c79dabfdfc2--------------------------------)
    [Yennie Jun](https://medium.com/@artfish?source=post_page-----3c79dabfdfc2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3c79dabfdfc2--------------------------------)
    ·10 min read·Jul 26, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/84e3040a1892146c1f28f18f09536c4d.png)'
  prefs: []
  type: TYPE_IMG
- en: A few of the top historical figures mentioned the most often by the GPT-4 and
    Claude. Individual images sourced from Wikipedia. Collage created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '*(This article was originally posted on* [*my personal blog*](https://www.artfish.ai/p/where-are-all-the-women)*)*'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) such as ChatGPT are being increasingly used in
    educational and professional settings. It is important to understand and study
    the many biases present in such models before integrating them into existing applications
    and our daily lives.
  prefs: []
  type: TYPE_NORMAL
- en: One of the biases I studied in my [previous article](https://blog.yenniejun.com/p/world-history-through-ai)
    was regarding historical events. I probed LLMs to understand what historical knowledge
    they encoded in the form of major historical events. I found that they encoded
    a serious Western bias towards understanding major historical events.
  prefs: []
  type: TYPE_NORMAL
- en: On a similar vein, in this article, I probe language models regarding their
    understanding of important historical figures. I asked two LLMs who the most important
    historical people in history were. I repeated this process 10 times for 10 different
    languages. Some names, like Gandhi and Jesus, appeared extremely frequently. Other
    names, like Marie Curie or Cleopatra, appeared less frequently. Compared to the
    number of male names generated by the models, there were extremely few female
    names.
  prefs: []
  type: TYPE_NORMAL
- en: 'The biggest question I had was: Where were all the women?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing the theme of evaluating historical biases encoded by language models,
    I probed [OpenAI’s GPT-4](https://openai.com/gpt-4) and [Anthropic’s Claude](https://www.anthropic.com/index/introducing-claude)
    regarding major historical figures. In this article, I show how both models contain:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gender bias: Both models disproportionately predict male historical figures.
    **GPT-4 generated the names of female historical figures 5.4% of the time and
    Claude did so 1.8% of the time**. This pattern held across all 10 languages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geographic bias: Regardless of the language the model was prompted in, there
    was a bias towards predicting Western historical figures. **GPT-4 generated historical
    figures from Europe 60% of the time and Claude did so 52% of the time.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Language bias: Certain languages suffered from gender or geographic biases
    more. For example, **when prompted in Russian, both GPT-4 and Claude generated
    zero women across all of my experiments.** Additionally, language quality was
    lower for some languages. For example, when prompted in Arabic, the models were
    more likely to respond incorrectly by generating famous *locations* instead of
    people.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I prompted OpenAI’s GPT-4 and Anthropic’s Claude in 10 different languages (English,
    Korean, Chinese, Japanese, Spanish, French, Italian, German, Russian, and Arabic)
    to list out the top 10 important historical figures. The original prompt and translations
    can be found at the end of the article.
  prefs: []
  type: TYPE_NORMAL
- en: I took all of the generated names, translated them into English, and standardized
    them into the same version. I found each name on Wikipedia to obtain metadata
    about that person, such as their country of origin, gender, and profession. I
    used that information to conduct the analyses in this article. A more detailed
    technical explanation of this process can be found at the end of the article.
  prefs: []
  type: TYPE_NORMAL
- en: Who were the most popular men?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/f66073f90b37ba7545a8588991492cf2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Word cloud of top historical figures generated by two popular Large Language
    Models: GPT-4 and Claude. Image generated by author.'
  prefs: []
  type: TYPE_NORMAL
- en: For each of the two models, I picked the historical figures who were generated
    at least 8 out of the 10 times for at least one of the prompted languages.
  prefs: []
  type: TYPE_NORMAL
- en: The top historical figures are almost all entirely men. Can you spot the one
    woman?
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 consistently generated figures such as Gandhi, Martin Luther King Jr.,
    and Einstein across most languages. (Note that the reason some of the scores are
    11 is because, once in a while, a model would generate the same figure twice in
    its list of top 10 historical figures)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/574af148923434536e2cd2bbbbc9f3fc.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Top historical figures generated by GPT-4, and the number of times generated
    per language. Heatmap generated by author.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Claude generated more religious and philosophical figures** such as Jesus,
    the Buddha, Muhammad, and Confucius. Note some interesting patterns: when prompted
    in English, German, and Spanish, Claude generated Muhammad 90–100% of the time.
    When prompted in Arabic, Claude generated Muhammad 0% of the time. Also, note
    the appearance of Mao Zedong almost only when prompted in Chinese.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3b623ad97b9a1bdbd61e72242d2b4790.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Top historical figures generated by Claude, and the number of times generated
    per language. Heatmap generated by author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Diversity of figures varies for language
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I prompted both Claude and GPT-4 10 times for each of the 10 languages, which
    resulted in many historical figures being repeated across languages.
  prefs: []
  type: TYPE_NORMAL
- en: But, looking at just the unique historical figures predicted per language, how
    *diverse* are the predictions? That is, does each language model generate the
    same few historical figures, or do they generate across a more diverse spectrum?
  prefs: []
  type: TYPE_NORMAL
- en: This depends on the language and the model. **Languages such as French, Spanish,
    and German generated less diverse historical figures.** Languages such as Korean,
    Chinese, and Arabic generated more variety. Interestingly, for some languages,
    GPT-4 was more diverse, and in others, Claude was more diverse. There was no clear
    pattern on one model generating more diverse historical figures overall.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37518bc0d8ce715d26ce4a47e904daee.png)'
  prefs: []
  type: TYPE_IMG
- en: '*The number of unique historical people generated by each model, broken down
    by the language the model was prompted in. For Arabic, the dots overlap, indicating
    that both models generated the same number of unique people. Plot generated by
    author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Gender Bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Looking at overall figures predicted, GPT-4 generated female historical figures
    5.4% of the time and Claude did so 1.8% of the time.
  prefs: []
  type: TYPE_NORMAL
- en: Since the same figures were predicted multiple times, I looked at the narrower
    set of *unique* historical figures. **For unique historical figures, GPT-4 generated
    female figures 14.0% of the time and Claude did so 4.9% of the time.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b346c526b254e0b848b060565002e390.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Gender breakdown of unique historical figures generated by GPT-4 and Claude.
    Plot generated by author.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Breakdown by language**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I find the breakdown by language and model insightful. When prompted in certain
    languages (e.g. Russian), the language model generated **zero important historical
    figures who were female** (not even Catherine the Great!). This propensity to
    generate more male or female historical figures varied greatly per language.
  prefs: []
  type: TYPE_NORMAL
- en: '**GPT-4**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**For GPT-4, the proportion of female historical figures varied by language:
    20% for English and 0% for Russian.**'
  prefs: []
  type: TYPE_NORMAL
- en: The female historical figures (ordered by number of times generated):Cleopatra
    (16), Marie Curie (14), Victoria of the United Kingdom (5), Elizabeth I of England
    (4), Rosa Parks (3), Joan of Arc (3), Virginia Woolf (1), Virgin Mary (1), Mother
    Teresa (1), Diana, Princess of Wales (1), Isabella I of Castile (1), Benazir Bhutto
    (1), Frida Kahlo (1), Elizabeth II (1)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06320a446b51d53e37cf77bde85b344e.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Gender breakdown of unique historical figures generated by GPT-4, separated
    by language model was prompted in. Plot generated by author.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Claude**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Claude, trained for safety, generated fewer women than GPT-4\.** In fact,
    **zero female historical figures were generated when prompted in English.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The female historical figures (ordered by number of times generated): Cleopatra
    (8), Marie Curie (3), Mother Teresa (2), Eleanor Roosevelt (1), Margaret Thatcher
    (1), Hippolyta (1), Yu Gwansun (1)'
  prefs: []
  type: TYPE_NORMAL
- en: '* One of the female historical figures is [Hippolyta](https://en.wikipedia.org/wiki/Hippolyta),
    who is a mythological figure'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7388d7eba35490bcdcffcaeef330bf84.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Gender breakdown of unique historical figures generated by Claude, separated
    by language model was prompted in. Plot generated by author.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**The models exhibited gender bias, but not any more than what is already on
    the Internet.** Yes, both of the LLMs disproportionately generated male historical
    figures. But should this even be surprising, considering what we find on the Internet,
    and the fact that language models are mostly trained on text from the Internet?'
  prefs: []
  type: TYPE_NORMAL
- en: 'I found three different “top 100 historical figures” lists on the Internet:'
  prefs: []
  type: TYPE_NORMAL
- en: A [1978 book](https://history.fandom.com/wiki/The_100:_A_Ranking_of_the_Most_Influential_Persons_in_History)
    titled “100 Most Influential People in the World” by Michael H. Hart contained
    2 women (Queen Elizabeth I, Queen Isabella I)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A [2013 Times list](https://ideas.time.com/2013/12/10/whos-biggest-the-100-most-significant-figures-in-history/)
    of “The 100 Most Significant Figures in History” contained 3 women (Queen Elizabeth
    I, Queen Victoria, and Joan of Arc)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A [2019 Biography Online](https://www.biographyonline.net/people/famous-100.html)
    “List of Top 100 Famous People” contained 26 women
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geographic bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Looking at the unique people predicted, what proportion of historical figures
    predicted by each LLM came from different global subregions? ([Subregions are
    based on Wikipedia’s categorization](https://en.wikipedia.org/wiki/Subregion#:~:text=A%20subregion%20is%20a%20part%20of%20a%20larger%20region%20or%20continent%20and%20is%20usually%20based%20on%20location)).
  prefs: []
  type: TYPE_NORMAL
- en: I expected a bit of a Western bias ([considering the Western bias in LLMs’ understanding
    of historical events](https://blog.yenniejun.com/p/world-history-through-ai)).
    And indeed, a third of the unique people generated by GPT-4 were from Western
    Europe or North America.
  prefs: []
  type: TYPE_NORMAL
- en: What surprised me was that 28% of unique people generated by Claude were from
    Eastern Asia!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee5255ebdc48bf90e0b5d27c01b8ea91.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Subregion breakdown of unique historical figures generated by GPT-4 and Claude.
    Plot generated by author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Of the many Eastern Asian figures generated by Claude, the majority were Chinese
    (25 from China, 3 from Korea, 2 from Japan, 1 each from Mongolia, Taiwan, and
    Tibet).
  prefs: []
  type: TYPE_NORMAL
- en: While Claude generated many *unique* Eastern Asian figures, these figures are
    actually one-offs that the model only generated infrequently. This becomes clear
    upon looking at the overall number of people predicted by each model. The predicted
    figures are more likely to be Western and Southern Europe.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d80156f7420faa20812d0a9c00573ee3.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Subregion breakdown of total historical figures generated by GPT-4 and Claude.
    Plot generated by author.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, yes, there is a Western bias. In terms of *unique* people, there was more
    diversity of Eastern Asian historical figures. However, in terms of *total* historical
    figures predicted, **there was a bias for generating historical figures from Western
    and Southern Europe: Europeans constituted 60% of the people generated by GPT-4
    and 52% of those generated by Claude.** There were extremely few historical figures
    from Central America (0.12% for GPT-4, and 0% for Claude) or from the entire continent
    of Africa (5.9% for GPT-5 and 4.0% for Claude).'
  prefs: []
  type: TYPE_NORMAL
- en: This underscores these models’ implicit or explicit understanding of a very
    Western-focused history, where the idea of important figures in history are European
    (even when prompted in non-European languages!).
  prefs: []
  type: TYPE_NORMAL
- en: Professions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Looking at the distribution of *unique* historical figures, both GPT-4 and Claude
    were more likely to generate politicians and philosophers. This is an interesting
    skew towards more political and philosophical history.
  prefs: []
  type: TYPE_NORMAL
- en: This also is a very reductionist view, because many people cannot be described
    with a singular profession. For example, what was Leonardo Da Vinci’s profession?
    He was an “Italian polymath of the High Renaissance who was active as a painter,
    draughtsman, engineer, scientist, theorist, sculptor, and architect” ([source](https://en.wikipedia.org/wiki/Leonardo_da_Vinci)).
    However, his “official” profession through the Wikipedia API was “painter”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a0889fcb096de6ffd96843c388a0d15e.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Profession breakdown of unique historical figures generated by GPT-4 and Claude.
    Plot generated by author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Discussion & Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, I probed two closed-source large language models regarding
    top historical figures. I showed that there was a gender bias towards generating
    male historical figures and a geographic bias towards generating people from Europe.
    I also showed that there was a language bias as well, in which prompting in certain
    languages (such as Russian) suffered from gender bias more severely.
  prefs: []
  type: TYPE_NORMAL
- en: It would be interesting to extend this analysis to open source models. I did
    a preliminary analysis on the newest model, [Llama 2 (70B)](https://ai.meta.com/llama/)
    and found that the results in non-English languages failed to properly answer
    the prompt or produced gibberish generations (suggesting that it had not been
    trained as much in most non-English languages). Because of this, I did not include
    the analysis in here, but I encourage the reader to try and share any insights
    they may find.
  prefs: []
  type: TYPE_NORMAL
- en: There is not (and likely never will be) a commonly accepted list of “most important
    historical figures” — this is an intentionally subjective question. How any one
    person answers this question (whether or not they be a historian) depends on cultural
    context (George Washington, for example, is very important in American history,
    but arguably insignificant in Korean history), the discipline (Isaac Newton may
    be more important in scientific history but less so in political history), and
    personal understanding of the world and society.
  prefs: []
  type: TYPE_NORMAL
- en: With this article, I hoped to call attention to the lack of women in so many
    obvious and non-obvious spheres of our lives. If you opened up any high school
    history textbook, I’m sure the gender bias of historical figures in those books
    would be as biased as the results from the large language models. But that’s exactly
    the point. These results are *normal* in society (at least, in the Western society
    I currently live in).
  prefs: []
  type: TYPE_NORMAL
- en: Having taken several women’s history courses, I *know* there were many important
    women in history — queens and warriors, pirates and poets, activists and scientists.
    Historians of the past, who were mostly men, tended to write women out of these
    narratives. The podcast [History Chicks](https://thehistorychicks.com/) talks
    about the many women in history who have contributed to the world but were forgotten
    or erased. Such is the story of many women, such as [Rosalind Franklin](https://en.wikipedia.org/wiki/Rosalind_Franklin),
    who, despite her contributions to the discovery of the structure of DNA, was largely
    unrecognized during her life and is still not given the same recognition that
    her colleagues, Watson and Crick, are given.
  prefs: []
  type: TYPE_NORMAL
- en: The language models *reflect* biases that already exist in society and on the
    texts that they were trained on — they perpetuate and remix these biases and,
    perhaps, exacerbate them in new ways. It is important for both users and developers
    of these large language models to be cognizant of these biases (and the many,
    many more they encode!) as they continue to use them in a variety of educational
    and professional settings.
  prefs: []
  type: TYPE_NORMAL
- en: '*Thank you for reading and supporting my work!*'
  prefs: []
  type: TYPE_NORMAL
- en: '*This article was originally posted on* [*my personal blog*](https://blog.yenniejun.com/p/where-are-all-the-women),
    *where I post my exploration of data and AI more frequently :)*'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Entity Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When the model generated a historical figure, how did I get from that to its
    wikipedia metadata?
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning — removing extraneous punctuation marks (like periods or quotation
    marks)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find that person on Wikipedia using [pywikibot](https://github.com/wikimedia/pywikibot),
    a Python library that interfaces with [MediaWiki](https://www.mediawiki.org/wiki/MediaWiki).
    This library allowed me to connect with Wikibase, the knowledge base driving Wikidata,
    and obtain structured metadata about each entity, such as gender, country of origin,
    and profession.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Often, the text generated by the models need to be normalized into the proper
    form. For example, the person Mahatma Gandhi can be referred to by “Gandhi” or
    “Mohandas Gandhi” or some other variation, but only “Mahatma Gandhi” will normalize
    to the correct Wiki page. In order to do this, I utilized existing SERP knowledge
    encoded by search engines such as Bing. Using a Bing scraper, I was able to extract
    the normalized Wikipedia name for a given entity
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
