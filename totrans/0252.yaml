- en: Achieving Structured Reasoning with LLMs in Chaotic Contexts with Thread of
    Thought Prompting and Parallel Knowledge Graph Retrieval
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/achieving-structured-reasoning-with-llms-in-chaotic-contexts-with-thread-of-thought-prompting-and-a4b8018b619a](https://towardsdatascience.com/achieving-structured-reasoning-with-llms-in-chaotic-contexts-with-thread-of-thought-prompting-and-a4b8018b619a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@alcarazanthony1?source=post_page-----a4b8018b619a--------------------------------)[![Anthony
    Alcaraz](../Images/6a71a1752677bd07c384246fb0c7f7e8.png)](https://medium.com/@alcarazanthony1?source=post_page-----a4b8018b619a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a4b8018b619a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a4b8018b619a--------------------------------)
    [Anthony Alcaraz](https://medium.com/@alcarazanthony1?source=post_page-----a4b8018b619a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a4b8018b619a--------------------------------)
    ·8 min read·Nov 18, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*Artificial intelligence software was used to enhance the grammar, flow, and
    readability of this article’s text.*'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) demonstrated impressive few-shot learning capabilities,
    rapidly adapting to new tasks with just a handful of examples.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://ai.plainenglish.io/why-rag-retrieval-augmented-generation-will-become-a-cornerstone-of-system-design-using-llm-719657fbfebe?source=post_page-----a4b8018b619a--------------------------------)
    [## Why RAG (Retrieval Augmented Generation) will become a cornerstone of system
    design using LLM…'
  prefs: []
  type: TYPE_NORMAL
- en: Recent advances in large language models (LLMs) like GPT-3 [1] have demonstrated
    powerful few-shot learning abilities —…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ai.plainenglish.io](https://ai.plainenglish.io/why-rag-retrieval-augmented-generation-will-become-a-cornerstone-of-system-design-using-llm-719657fbfebe?source=post_page-----a4b8018b619a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: However, despite their advances, LLMs still face limitations in complex reasoning
    involving chaotic contexts overloaded with disjoint facts. To address this challenge,
    researchers have explored techniques like chain-of-thought prompting that guide
    models to incrementally analyze information. Yet on their own, these methods struggle
    to fully capture all critical details across vast contexts.
  prefs: []
  type: TYPE_NORMAL
- en: This article proposes a technique combining Thread-of-Thought (ToT) prompting
    with a Retrieval Augmented Generation (RAG) framework accessing multiple knowledge
    graphs in parallel. While ToT acts as the reasoning “backbone” that structures
    thinking, the RAG system broadens available knowledge to fill gaps. Parallel querying
    of diverse information sources improves efficiency and coverage compared to sequential
    retrieval. Together, this framework aims to enhance LLMs’ understanding and problem-solving
    abilities in chaotic contexts, moving closer to human cognition.
  prefs: []
  type: TYPE_NORMAL
- en: We begin by outlining the need for structured reasoning in chaotic environments
    where both relevant and irrelevant facts intermix. Next, we introduce the RAG
    system design and how it expands an LLM’s accessible knowledge. We then explain
    integrating ToT prompting to methodically guide the LLM through step-wise analysis.
    Finally, we discuss optimization strategies like parallel retrieval to efficiently
    query multiple knowledge sources concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: Through both conceptual explanation and Python code samples, this article illuminates
    a novel technique to orchestrate an LLM’s strengths with complementary external
    knowledge. Creative integrations such as this highlight promising directions for
    overcoming inherent model limitations and advancing AI reasoning abilities. The
    proposed approach aims to provide a generalizable framework amenable to further
    enhancement as LLMs and knowledge bases evolve.
  prefs: []
  type: TYPE_NORMAL
- en: The Need for Structured Reasoning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While LLMs have made strides in language understanding, their reasoning abilities
    remain limited. When faced with complex contexts containing multiple disjoint
    facts, models often get lost or omit critical information.
  prefs: []
  type: TYPE_NORMAL
- en: Chaotic contexts include both relevant facts needed to address the question/task,
    as well as irrelevant information unnecessary for the reasoning process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The relevant and irrelevant pieces are mixed together within the context rather
    than separated into clear sections.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is no clear logical progression or grouping of the facts. The order may
    be completely random rather than building in a structured way.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The relevant and irrelevant details are deeply interwoven. For example, a key
    relevant sentence may be sandwiched between two irrelevant ones.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The lack of structure means models cannot rely on the position or sequence of
    information to determine relevance. The pertinent facts can appear anywhere.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is a high density and volume of information packed into the context. Lots
    of facts jump out but few explicitly stand out as relevant.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The chaotic mixing of overload information obscures which details connect to
    each other in meaningful ways. It fragments the logical thread.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Without a coherent thread to follow, models can easily lose track of key points
    that get drowned out by the chaos.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The distractions disrupt the reasoning process, causing models to miss insights
    that require connecting multiple scattered details.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This makes it challenging to methodically build an understanding grounded in
    the relevant information. Reasoning gives way to randomness.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In essence, chaotic contexts overload models with complex intersections of signal
    and noise. This entanglement strains even structured reasoning techniques, showing
    gaps that demand additional solutions.
  prefs: []
  type: TYPE_NORMAL
- en: To address this, the Thread of Thought (ToT) prompting strategy was introduced.
    Inspired by human cognition, ToT guides models to methodically analyze contexts
    step-by-step. This technique improves comprehension and reasoning without retraining
    models.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://paperswithcode.com/paper/thread-of-thought-unraveling-chaotic-contexts?source=post_page-----a4b8018b619a--------------------------------)
    [## Papers with Code - Thread of Thought Unraveling Chaotic Contexts'
  prefs: []
  type: TYPE_NORMAL
- en: No code available yet.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: paperswithcode.com](https://paperswithcode.com/paper/thread-of-thought-unraveling-chaotic-contexts?source=post_page-----a4b8018b619a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: However, ToT prompting alone has limitations in extremely chaotic contexts.
    This article proposes complementing it with a RAG system accessing diverse knowledge
    sources in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the RAG System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Retrieval augmented generation (RAG) combines the capabilities of LLMs with
    scalable retrieval. Indexed knowledge sources allow models to adapt rapidly using
    few-shot learning instead of encoding all information statically.
  prefs: []
  type: TYPE_NORMAL
- en: 'We design a RAG system with multiple knowledge graphs (KGs) indexed using LLama
    index. For each KG, we:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a query engine to retrieve relevant passages as contexts
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a tool to encapsulate the query engine
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct an agent for the tool to interact with the LLM
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine all agents into a super agent orchestrating the KG tools
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Querying diverse KGs in parallel expands available knowledge, while few-shot
    learning enables fast adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Integrating ToT Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To enable structured reasoning, we integrate ToT prompting into the RAG system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The super agent prompts the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“Walk me through this context in manageable parts step-by-step, summarizing
    and analyzing as we go”*'
  prefs: []
  type: TYPE_NORMAL
- en: This initiates incremental analysis of retrieved passages.
  prefs: []
  type: TYPE_NORMAL
- en: 'The agent then extracts the conclusion with:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“Therefore, the answer is:”*'
  prefs: []
  type: TYPE_NORMAL
- en: Guiding the model through ToT reasoning with diverse knowledge enhances understanding
    and avoids overlooking critical facts.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Parallel Retrieval
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To efficiently query multiple KGs, we leverage asynchronous parallel retrieval:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Querying KGs concurrently accelerates retrieval compared to sequential queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Not only the knowledge scope but also the underlying graph algorithms and embeddings
    used can provide complementary reasoning strengths. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Graph algorithms optimized for traversal like Personalized PageRank can infer
    indirect connections between concepts. This supports more flexible associative
    reasoning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms tuned for search (e.g. Approximate Nearest Neighbors) allow efficiently
    finding related entities. This aids precise factual lookups.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge graph embeddings translate symbols into vector spaces amenable for
    mathematical operations. This enables analogical reasoning and vector arithmetic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multimodal embeddings combine text, images, audio and more. This supports visual-semantic
    reasoning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Similarly, constraints and training objectives for embeddings also influence
    reasoning:'
  prefs: []
  type: TYPE_NORMAL
- en: Transitive embeddings improve logical deduction along chains of facts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Equivalent class embeddings group synonymous terms. This aids generalization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical embeddings capture taxonomic relationships between classes. This
    allows inheritance-based reasoning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By orchestrating diverse algorithms and embedding strategies, we engender different
    reasoning modalities within the same overarching framework. The language model
    can adaptively employ the appropriate reasoning style for each query. This augmented
    reasoning repertoire approaches the multifaceted inference capabilities of human
    cognition.
  prefs: []
  type: TYPE_NORMAL
- en: The RAG system’s flexible architecture enables interweaving knowledge graphs
    and reasoning engines tailored to different uses. This cohesive framework integrates
    hybrid reasoning under one roof, while keeping knowledge sources modular and expandable
    as needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example with a medical knowledge graph, we can leverage:'
  prefs: []
  type: TYPE_NORMAL
- en: Transitive embeddings to improve clinical decision making through logical deduction.
    This allows deducing new drug interactions from chains of known interactions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Equivalent class embeddings to group synonymous medical terms. This aids generalization
    across different terminologies like layperson and expert terms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical embeddings to capture taxonomy of diseases, symptoms, and treatments.
    This enables inheritance-based reasoning, like inferring properties of a specific
    cancer from its parent cancer class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query Ontology First
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This allows first tapping into the ontology to get definitions and high-level
    knowledge about the query. The ontology results are appended to the original question
    to provide context.
  prefs: []
  type: TYPE_NORMAL
- en: The enhanced question containing ontology information is then fed to the super
    agent. This primes the RAG system with initial background before retrieving from
    the knowledge graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Querying the ontology first provides a “warm start” for the RAG system by establishing
    basic concepts and context. The knowledge graphs can then fill in more specific
    relational facts tailored to the grounded query.
  prefs: []
  type: TYPE_NORMAL
- en: This technique combines top-down hierarchical reasoning from the ontology with
    bottom-up retrieval of granular knowledge. The ontology clarifies ambiguous or
    broad terminology, while the knowledge graphs provide in-depth information on
    refined query aspects.
  prefs: []
  type: TYPE_NORMAL
- en: Choreographing different knowledge sources in this staged manner allows smoothly
    flowing between levels of abstraction. This progresses from high-level concepts
    down to contextual details relevant for the question.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article proposed an approach to enhance large language models’ reasoning
    abilities in complex contexts by integrating Thread-of-Thought prompting with
    a Retrieval Augmented Generation system.
  prefs: []
  type: TYPE_NORMAL
- en: The Thread-of-Thought technique provides structured sequencing of thought to
    methodically analyze chaotic information. Meanwhile, the RAG framework expands
    accessible knowledge through parallel querying of diverse knowledge graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Together, this hybrid technique aims to orchestrate the strengths of LLMs with
    complementary external knowledge sources. Guiding the model to incrementally reason
    while drawing from rich contextual data augments its understanding.
  prefs: []
  type: TYPE_NORMAL
- en: The proposed approach offers a generalizable framework to address inherent model
    limitations. As LLMs continue evolving along with knowledge bases, this technique
    provides a pathway to incrementally add structured reasoning capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: While challenges remain in replicating multifaceted human reasoning, innovations
    in prompting strategies, knowledge grounding, and parallel architectures illuminate
    promising directions.
  prefs: []
  type: TYPE_NORMAL
- en: This article highlighted the value of creatively combining strengths of existing
    methods to achieve emergent capabilities greater than the sum of parts. As we
    learn more about choreographing LLMs’ aptitudes, meta-architectures akin to this
    proposal may become prevailing paradigms.
  prefs: []
  type: TYPE_NORMAL
- en: Moving forward, adaptive utilization of LLMs along with structured knowledge
    and reasoning techniques remains a key pillar in advancing artificial intelligence.
    With sustained research exploring these intersections, the dream of human-like
    language understanding may gradually approach reality.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4db009311e011b5157b3d3d1eb5225f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
