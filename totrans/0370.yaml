- en: Best Data Wrangling Functions in PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/best-data-wrangling-functions-in-pyspark-3e903727319e](https://towardsdatascience.com/best-data-wrangling-functions-in-pyspark-3e903727319e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn the most helpful functions when wrangling Big Data with PySpark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://gustavorsantos.medium.com/?source=post_page-----3e903727319e--------------------------------)[![Gustavo
    Santos](../Images/a19a9f4525cdeb6e7a76cd05246aa622.png)](https://gustavorsantos.medium.com/?source=post_page-----3e903727319e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3e903727319e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3e903727319e--------------------------------)
    [Gustavo Santos](https://gustavorsantos.medium.com/?source=post_page-----3e903727319e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3e903727319e--------------------------------)
    ·7 min read·Dec 12, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa66dae99d44fd5c8cadf67a519d043e.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Oskar Yildiz](https://unsplash.com/@oskaryil?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/turned-on-gray-laptop-computer-cOkpTiJMGzA?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I work with PySpark in Databricks on a daily basis. My work as a Data Scientist
    requires me to deal with large amounts of data in many different tables. It is
    a challenging job, many times.
  prefs: []
  type: TYPE_NORMAL
- en: 'As much as the **Extract, Transform and Load (ETL)** process sounds like something
    simple, I can tell that it is not always like that. When we work with Big Data,
    a lot of our thinking has to change for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The amounts of data are way bigger than regular datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When working with parallel computing in clusters, we must take into account
    that the data will be split among many worker nodes to perform part of the job
    and then be brought together as a whole. And this process, many times, can become
    very time consuming if the query is too complex.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Knowing that, we must learn how to be write smart queries for Big Data. In this
    post, I will show a few of my favorite functions from the module `pyspark.sql.functions`,
    aiming to help you during your Data Wrangling in PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: Best Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let’s move on to the content in this post.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like many other languages, PySpark has the benefit of the modules, where
    you can find many ready-to-use functions for the most different purposes. Here’s
    the one we will load to our session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you want to see how extended is the list of functions inside `pyspark.sql.functions`,
    [go to this web site](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html),
    where the API Reference is. Have in mind that this is for the version 3.5.0\.
    Some older versions may not carry all the functions I will show in this post.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dataset to be used as our example is the *Diamonds*, from ggplot2, shared
    under [MIT License](https://tidyverse.tidyverse.org/LICENSE.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Creating an Index Column
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For those who work with Pandas in Python, it feels weird at first to deal with
    a data frame without index. So, if we want to add an index column, the function
    is `monotonically_increasing_id()`. The counting starts with 0\. Ergo, if we want
    to start in one, just add a `+1` after the function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Sum, Mean, Max, Min
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The classic mathematical functions are surely making this list. Useful in every
    case.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Count and Count Distinct
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is important to count values and know how many distinct values are in the
    data as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Literal Value
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The function `lit()` lets you write a literal value for every row in the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/59621d7d64a0addc9fa86688499e98b2.png)'
  prefs: []
  type: TYPE_IMG
- en: lit(‘my text or number’). Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Floor, Ceiling and Percentile
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some great math functions that help a lot while wrangling data are `floor` —
    the closest integer down — and `ceiling` — the closest integer up.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/84e1f5f531b5d738cd8fbec50bdebcac.png)'
  prefs: []
  type: TYPE_IMG
- en: Ceiling and Floor. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Now percentile is useful especially to calculate the median value. Until not
    so long ago, I remember I was looking to calculate the median and kept getting
    an error. Now I saw that it is there in version 3.5.0\. The best workaround was
    to calculate it using `percentile()` at 50%.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/392403329d6eb2cc7ed52df62d06c602.png)'
  prefs: []
  type: TYPE_IMG
- en: Descriptive Stats
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite not being within the `sql.functions` module, `describe()` is also for
    those familiar with Pandas. It brings the descriptive statistics of the dataset.
    The numbers provided here are: count, mean, standard deviation, min and max.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Resulting in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/44e3d001ba714793e35540781be19018.png)'
  prefs: []
  type: TYPE_IMG
- en: Result of the describe( ) function. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Logarithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Working as Data Scientists, we keep the log functions close. Especially for
    linear regression, it is a helper for normalization of variables.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/88c24d78bf7326eb3d97f274045bb9f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Log variables calculated. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Array Aggregate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`array_agg` is a good function to take the values of a group and list them
    in a new column. Let’s say we want to group the diamonds by the quality of the
    cut and have a look at the prices listed. The next snippet performs that.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f56e02ac600c6ac37357af261b7a5407.png)'
  prefs: []
  type: TYPE_IMG
- en: List of values by group. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Count IF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I bet it sounds familiar if you ever used MS Excel, right? And the idea is the
    same. If we want to count only the diamonds that cost more than $18,000 after
    grouping, we can use this function. Check this out.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We have more *Ideal* and *Premium* cuts with those expensive price, and almost
    no Fair.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b893c60a11440e84580a76de386be102.png)'
  prefs: []
  type: TYPE_IMG
- en: Count if in action. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Mode
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The mode of a variable is the most common values. Now we want to know what is
    the most common carat for each quality of cut.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Regression Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These are really interesting. We can quickly calculate linear regression metrics
    like R-Squared, intercept, slope, using these functions: `regr_r2`, `regr_intercept`,
    `regr_slope`, `regr_avgx`.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next snippet, we will calculate the regression R-Squared and the formula
    for each group.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This is very cool! The *Ideal* cut (best quality) has the highest R², while
    the *Fair* cut (lowest quality) has the lowest. It makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e7c0df9b9c7faa70259c62370a2028ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Regression by group. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Regular Expressions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Yes, the *regexps* are everywhere. I see a lot of people rolling their eyes
    to those beautiful expressions, but it is an awesome tool Imagine that you can
    extract mostly anything from a text using these expressions. It might be difficult
    and bumpy at first, but once you learn more about it, you start to love them.
    And PySpark has them in the functions too.
  prefs: []
  type: TYPE_NORMAL
- en: Here, I am using the `regexp()` combined with a literal text `lit` to check
    if the variable `clarity` has digits. The next line is the function `locate` ,
    to locate the position of the first occurrence of the letter ‘S’ in the same variable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5cb5d4978a55f79d4b0851b93b131907.png)'
  prefs: []
  type: TYPE_IMG
- en: Text parsing functions. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Text Parsing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: More about text parsing, we can use `split()` to split a text in parts. In the
    next snippet I am converting the column `carat` to text and splitting it by `.`.
    So, a number like 0.23 becomes [“0” , “23”]. Then I just use a slicing notation
    to place the results in separate columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5c9d494b18adb387563220f1f000ceaa.png)'
  prefs: []
  type: TYPE_IMG
- en: Split function result. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Another parsing possibility is the function `left`, similar to MS Excel. You
    have a given column with text where you want to get only N characters from it.
    Just use `left` combined with `lit`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2aca4893cbca553d19afb73af79f2d15.png)'
  prefs: []
  type: TYPE_IMG
- en: Left function result. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Before You Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Wrangling data is an art. Being it in PySpark, R or Python, you will always
    need the best functions to make your transformations happen. And here, I listed
    just a few from the module `pyspark.sql.functions`. I suggest you to visit the
    documentation page and create your own best list.
  prefs: []
  type: TYPE_NORMAL
- en: 'My best tip to transform data is:'
  prefs: []
  type: TYPE_NORMAL
- en: Know what your end result should look like.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From there, you can break the process down to smaller steps that will make it
    happen.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I learned that when I started my career and used to work a lot with Excel sheets.
    When I didn’t know how to write a big fancy formula, I used to write smaller pieces
    in different cells until I got to the desired result. Then, it was just a matter
    of gathering the pieces and making it work as a single formula.
  prefs: []
  type: TYPE_NORMAL
- en: The same applies to programming. Create your strategy with the end point in
    mind and go step by step, if needed. That’s it.
  prefs: []
  type: TYPE_NORMAL
- en: If you liked this content, follow my blog for more and subscribe to my newsletter.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@gustavorsantos?source=post_page-----3e903727319e--------------------------------)
    [## Gustavo Santos - Medium'
  prefs: []
  type: TYPE_NORMAL
- en: Read writing from Gustavo Santos on Medium. Data Scientist. I extract insights
    from data to help people and companies…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@gustavorsantos?source=post_page-----3e903727319e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Also find me on [LinkedIn](https://www.linkedin.com/in/gurezende/).
  prefs: []
  type: TYPE_NORMAL
- en: Interested in Learning More About PySpark?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I am just releasing my new online course [**Mastering Data Processing With PySpark
    in Databrick**](https://www.udemy.com/course/master-data-processing-pyspark/?referralCode=44AA924FEB5260F5EB25).
    This is a good opportunity for you that want to upskill yourself and learn more
    about Wrangling Big Data!
  prefs: []
  type: TYPE_NORMAL
- en: I have made available [**this free Webinar with my top tips to write faster
    queries in PySpark**](https://www.gustavorsantos.com/webinar-funnel-3-cf-2-0--71744).
    Check it out, and you will find a coupon with the most special launching price
    inside!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[## Functions - PySpark 3.5.0 documentation'
  prefs: []
  type: TYPE_NORMAL
- en: pyspark.sql.SparkSession.builder.getOrCreate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: spark.apache.org](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html?source=post_page-----3e903727319e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
