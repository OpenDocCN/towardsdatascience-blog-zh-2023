- en: 6 Types of Clustering Methods — An Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/6-types-of-clustering-methods-an-overview-7522dba026ca](https://towardsdatascience.com/6-types-of-clustering-methods-an-overview-7522dba026ca)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Types of clustering methods and algorithms and when to use them
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://kayjanwong.medium.com/?source=post_page-----7522dba026ca--------------------------------)[![Kay
    Jan Wong](../Images/28e803eca6327d97b6aa97ee4095d7bd.png)](https://kayjanwong.medium.com/?source=post_page-----7522dba026ca--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7522dba026ca--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7522dba026ca--------------------------------)
    [Kay Jan Wong](https://kayjanwong.medium.com/?source=post_page-----7522dba026ca--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7522dba026ca--------------------------------)
    ·8 min read·Mar 24, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/78a95243080c867f084198a587c15efc.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Kier in Sight](https://unsplash.com/@kierinsight?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Clustering is one of the branches of Unsupervised Learning where **unlabelled
    data** is divided into groups with similar data instances assigned to the same
    cluster while dissimilar data instances are assigned to different clusters. Clustering
    has various uses in market segmentation, outlier detection, and network analysis,
    to name a few.
  prefs: []
  type: TYPE_NORMAL
- en: There are different types of clustering methods, each with its advantages and
    disadvantages. This article introduces the different types of clustering methods
    with algorithm examples, and when to use each algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Centroid-based / Partitioning (*K-means*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connectivity-based (*Hierarchical Clustering*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density-based (*DBSCAN*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph-based (*Affinity Propagation*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distribution-based (*Gaussian Mixture Model*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compression-based (*Spectral Clustering, BIRCH*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centroid-based / Partitioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Centroid-based methods group data points together based on the proximity of
    data points to the centroid (cluster center)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/c8c964bd30a5eddbc2e8dd1378b8b3dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 1: Example of clustering output for centroid-based method (K-means) — Image
    from [sklearn](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html)'
  prefs: []
  type: TYPE_NORMAL
- en: The proximity between data points to the centroid is measured by
  prefs: []
  type: TYPE_NORMAL
- en: '**Euclidean distance**: Shortest (straight-line) distance, useful for numerical
    features, this is the default distance measure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manhattan distance**: Sum of absolute differences, useful for categorical
    features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hamming distance**: Percentage of bits that differ, useful for binary features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mahalanobis distance**: Standardised form of Euclidean distance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Minkowski distance**: Generalized form of Euclidean and Manhattan distance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chebyshev distance**: Maximum absolute difference'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regardless of which distance measure, numerical features should always be standardized
    when performing clustering!
  prefs: []
  type: TYPE_NORMAL
- en: K-means
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: K-means involves initialization and performing iterative Expectation Maximization
    (EM) steps until convergence or when maximum iteration is reached.
  prefs: []
  type: TYPE_NORMAL
- en: During initialization, `k` number of centroids are assigned based on the `k-means++`
    optimization algorithm. In the Expectation (E) step, data points are assigned
    to clusters following their closest (by Euclidean distance) centroid. In the Maximization
    (M) step, centroids are updated to minimize the inertia or within-cluster sum-of-squares
    (WCSS). EM steps are repeated until convergence to local minima where the cluster
    assignment and centroids do not change.
  prefs: []
  type: TYPE_NORMAL
- en: When to use K-means
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You want **interpretability**: K-means is easy to understand and interpret.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clusters are **even-sized and globular-shaped**: K-means work well when clusters
    are well-separated globular shapes but do not perform well if clusters are long
    and irregularly shaped.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When to NOT use K-means
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You are **unsure about the number of clusters**: K-means require the number
    of clusters to be predefined. Usually, the Elbow method, which plots WCSS against
    the number of clusters, is used to determine the optimal number of clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You have **outliers in the data**: All data points are assigned to a cluster,
    hence the presence of outliers can skew the results and have to be removed or
    transformed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You want **computation efficiency**: The computation cost of K-means increases
    with the size of data as it runs in `O(tkn)` time where `t` is the number of iterations,
    `k` is the number of clusters, and `n` is the number of data points. Using dimensionality
    reduction algorithms such as PCA can speed up computation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Other centroid-based algorithms**: K-Medoids, Mean Shift, Fuzzy Clustering
    (soft K-means; data points can be part of multiple clusters)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Edit**: There are also Variance-Ratio Criterion (VRC) and Bayesian Information
    Criterion (BIC) as more robust alternatives to the Elbow method (Credits to Victor
    for this information — link to [paper](https://ar5iv.labs.arxiv.org/html/2212.12189)).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Connectivity-based
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Connectivity-based methods group data points together based on the proximity
    between the clusters
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/8b314755a33b723683abaf21331467f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 2: Example of clustering output for connectivity-based method (Hierarchical
    Clustering) — Image from [sklearn](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html)'
  prefs: []
  type: TYPE_NORMAL
- en: The linkage criterion calculates the proximity between clusters,
  prefs: []
  type: TYPE_NORMAL
- en: '**Single linkage**: Distance between closest points of clusters; minimum distance
    between clusters. Good for detecting arbitrarily shaped clusters but cannot detect
    overlapping clusters. It is efficient to compute but is not robust to noisy data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complete / Maximum linkage**: Distance between furthest points of clusters;
    maximum distance between clusters. Good for detecting overlapping clusters but
    cannot detect arbitrarily shaped clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Average linkage**: Average of all distances across two clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Centroid linkage**: Distance between centers of two clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ward linkage**: Sum of squared distance from each data point to the centroid
    of the cluster they are assigned to. This results in cluster merging that gives
    the smallest increase in total variance within all clusters. Ward linkage is the
    default linkage criterion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Agglomerative hierarchical clustering works by doing an iterative **bottom-up
    approach** where each data point is considered as an individual cluster and the
    two closest (by linkage criteria) clusters get iteratively merged until one large
    cluster is left.
  prefs: []
  type: TYPE_NORMAL
- en: Divisive hierarchical clustering does the opposite and performs an iterative
    **top-down approach** where it starts from one large cluster and continuously
    breaks down into two smaller clusters until every data point is an individual
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: When to use Hierarchical Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You are **unsure about the number of clusters**: Hierarchical clustering requires
    the number of clusters to be predefined, but the clusters can be viewed on a dendrogram
    and the number of clusters can be tweaked without recomputation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You want **computation efficiency**: Cluster labels at any intermediate stage
    can be recovered, therefore it is not necessary to recompute the proximity matrix
    if the number of clusters changes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You have **high dimensional data**: Output can be visualized using a dendrogram
    which can be used with higher dimensional data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density-based
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Density-based methods group data points together based on density instead of
    distance
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/eb1555c77c6d282903a4bb6da175c1fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 3: Example of clustering output for density-based method (DBSCAN) — Image
    from [sklearn](https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Density-Based Spatial Clustering of Applications with Noise (DBSCAN)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DBSCAN algorithm clusters data points that are in dense regions together, separated
    by areas of low density. Samples in denser areas within `eps` units apart with
    more than `min_samples` number of data points are called **core samples**.
  prefs: []
  type: TYPE_NORMAL
- en: When to use DBSCAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You are **unsure about the number of clusters**: DBSCAN does not require the
    number of clusters to be predefined.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You want **arbitrarily-shaped clusters**: Clusters are determined by dense
    regions, hence cluster shapes can be odd-shaped or complex.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You want to **detect outliers**: Data points that are not assigned to any cluster
    as they do not fall in any dense region will be considered outliers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When to NOT use DBSCAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you want **stable performance**: DBSCAN output is highly influenced and
    sensitive to the `eps` parameter, which must be chosen appropriately for the data
    set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Other density-based algorithms**: Ordering Points To Identify the Clustering
    Structure (OPTICS), Hierarchical DBSCAN (HDBSCAN)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Graph-based
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Graph-based methods group data points together based on graph distance
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/b08720086f04f4f4676ce89fc418e3c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 4: Example of clustering output for graph-based method (Affinity Propagation)
    — Image from [sklearn](https://scikit-learn.org/stable/auto_examples/cluster/plot_affinity_propagation.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Affinity Propagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Affinity propagation works by pair-wise sending of messages between data points
    until convergence. Exemplars, which are points that best represent the surrounding
    data points, are chosen and each point is assigned a cluster of its nearest exemplar.
  prefs: []
  type: TYPE_NORMAL
- en: When to use Affinity Propagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You are **unsure about the number of clusters**: Affinity Propagation does
    not require the number of clusters to be predefined.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**When to NOT use Affinity Propagation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You want **computation efficiency**: Affinity propagation runs in `O(tn²)`
    time complexity where `t` is the number of iterations and `n` is the number of
    data points, as messages are sent pair-wise between every data point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You want **space efficiency**: If a dense similarity matrix is used, affinity
    propagation takes up `O(n²)` memory as messages sent between every data point
    are considered a ‘vote’ cast to elect the exemplar.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distribution-based
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Distribution-based methods group data points together based on their likelihood
    of belonging to the same probability distribution
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/f2001805fbeef5503b21d7baae18ca9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 5: Example of clustering output for distribution-based method (GMM) — Image
    from [sklearn](https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_selection.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Distribution-based methods use statistical inference to cluster data such that
    the closer the data point is to a central point, the higher the probability to
    be assigned to that cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to clustering methods that consider distance measures, distribution-based
    methods are more flexible in determining the shape of clusters, provided that
    the clusters follow a predefined distribution such as with synthetic or simulated
    data. If clusters are noisy, the results will overfit.
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian Mixture Model (GMM)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GMM assumes that every data point is generated from multiple Gaussian distributions
    with unknown parameters and performs iterative Expectation Maximization (EM) steps
    to fit the data points.
  prefs: []
  type: TYPE_NORMAL
- en: In the Expectation (E) step, data points are assigned to clusters that assume
    randomly selected Gaussian parameters. In the Maximization (M) step, the parameters
    of the Gaussian distribution are updated to best fit the data points in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: When to use Gaussian Mixture Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You want **computation efficiency**: GMM is the fastest distribution-based
    algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Your clusters follow a **Gaussian distribution**: Distribution can be tweaked
    to fit spherical, diagonal, tied, or full covariance matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When to NOT use Gaussian Mixture Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You have **insufficient data in each cluster**: It is hard to compute the covariance
    matrices when there are insufficient data points in a Gaussian distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compression-based
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compression-based methods transform data points to an embedding and subsequently
    perform clustering on the lower-dimension data
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Spectral Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spectral clustering transforms the affinity matrix between data points to a
    low-dimension embedding before performing clustering (such as K-means).
  prefs: []
  type: TYPE_NORMAL
- en: When to use Spectral Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You want **computation efficiency**: Spectral clustering is fast if the affinity
    matrix is sparse and if the `pyamg` solver is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When to NOT use Spectral Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You are **unsure about the number of clusters**: Spectral Clustering requires
    the number of clusters to be predefined and the number of clusters should be ideally
    small.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BIRCH
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BIRCH performs lossy compression of data points to a set of Clustering Features
    nodes (CF Nodes) that forms the Clustering Feature Tree (CFT). New data points
    are ‘shuffled’ into the tree until it reaches a leaf and store information regarding
    the subcluster within the node.
  prefs: []
  type: TYPE_NORMAL
- en: Information regarding the final cluster centroids is read off the leaf and other
    clustering algorithms (such as hierarchical clustering) can be subsequently performed.
  prefs: []
  type: TYPE_NORMAL
- en: When to NOT use BIRCH
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You have **high dimensional data**: BIRCH does not scale well to high dimensional
    data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is interesting how the end goal of obtaining clusters can be obtained with
    vastly different approaches and algorithms. This article does not aim to cover
    all the possible clustering algorithms or go in-depth into the mathematical formulas
    involved in each algorithm, but I hope it does provide some high-level detail
    on the types of clustering methods and when to use different algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Related Stories
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another aspect of clustering is to evaluate the clustering algorithms to determine
    which performs the *best* using statistical measures.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2?source=post_page-----7522dba026ca--------------------------------)
    [## 7 Evaluation Metrics for Clustering Algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: In-depth explanation with Python examples of unsupervised learning evaluation
    metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2?source=post_page-----7522dba026ca--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Related Links
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`sklearn` Clustering Documentation: [https://scikit-learn.org/stable/modules/clustering.html](https://scikit-learn.org/stable/modules/clustering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
