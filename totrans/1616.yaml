- en: 'Outlier Detection with Scikit-Learn and Matplotlib: a Practical Guide'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/outlier-detection-with-scikit-learn-and-matplotlib-a-practical-guide-382d1411b8ec](https://towardsdatascience.com/outlier-detection-with-scikit-learn-and-matplotlib-a-practical-guide-382d1411b8ec)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn how visualizations, algorithms, and statistics help you to identify anomalies
    for your machine learning tasks.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@riccardo.andreoni?source=post_page-----382d1411b8ec--------------------------------)[![Riccardo
    Andreoni](../Images/5e22581e419639b373019a809d6e65c1.png)](https://medium.com/@riccardo.andreoni?source=post_page-----382d1411b8ec--------------------------------)[](https://towardsdatascience.com/?source=post_page-----382d1411b8ec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----382d1411b8ec--------------------------------)
    [Riccardo Andreoni](https://medium.com/@riccardo.andreoni?source=post_page-----382d1411b8ec--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----382d1411b8ec--------------------------------)
    ·10 min read·Oct 27, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b52c5234e67397f7a6f6010da838a994.png)'
  prefs: []
  type: TYPE_IMG
- en: 'What do balloons have to do with outliers? Find the answer in the introduction.
    Image source: [pixabay.com](https://pixabay.com/illustrations/balloons-spring-nature-watercolor-1615032/).'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a room filled with **colorful balloons**, each symbolizing a data point
    in a dataset. Due to their different features, the balloons float at different
    heights. Now, picture some **helium-filled balloons** that unexpectedly soar far
    above the rest. Just as these exceptional balloons disrupt the uniformity of the
    room, outliers disrupt the pattern in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Returning from this colorful analogy to pure statistic, **outliers** are defined
    as anomalies, or better, data points that deviate significantly from the rest
    of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a **Machine Learning algorithm** developed to diagnose diseases based
    on patient data. In this real-world example, outliers could be extremely high
    values in laboratory results or physiological parameters. While their origin may
    consist in various reasons like **data collection errors**, **measurement inaccuracies**,
    or genuine **rare events**, their presence can lead the algorithm to make incorrect
    diagnoses.
  prefs: []
  type: TYPE_NORMAL
- en: This is the reason why we, Machine Learning or Data Science practitioners, must
    always **treat outliers with care**.
  prefs: []
  type: TYPE_NORMAL
- en: In this short post, I will discuss several methods to efficiently identify and
    remove outliers from your data.
  prefs: []
  type: TYPE_NORMAL
- en: One of them is [**SVM**](https://en.wikipedia.org/wiki/Support_vector_machine),
    which I explored in this post.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/support-vector-machine-with-scikit-learn-a-friendly-introduction-a2969f2ff00d?source=post_page-----382d1411b8ec--------------------------------)
    [## Support Vector Machine with Scikit-Learn: A Friendly Introduction'
  prefs: []
  type: TYPE_NORMAL
- en: Every data scientist should have SVM in their toolbox. Learn how to master this
    versatile model with a hands-on…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/support-vector-machine-with-scikit-learn-a-friendly-introduction-a2969f2ff00d?source=post_page-----382d1411b8ec--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: What Are Outliers?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Outliers are **nonrepresentative data points** in a dataset, or better, data
    points that deviate significantly from the rest. Despite their simple definition,
    detecting these anomalies is not always straightforward but first, let’s answer
    the following basic question.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we want to detect outliers in a dataset?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There exist two answers to this question. The **first reason** for detecting
    outliers is that these anomalies can hide meaningful patterns in data and distort
    the learning process of a Machine Learning algorithm. For instance, in a dataset
    representing [house prices](https://www.kaggle.com/datasets/yasserh/housing-prices-dataset)
    based on their features, an unusually high price for a small, poorly located apartment,
    could be an outlier and lead to biased predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Secondly**, a broad variety of Data Science applications have the only goal
    of detecting outliers. In these cases, outlier detection is not merely a data
    preparation task, but it represents the whole scope of the application. Think,
    as an instance, of [fraud detection in finance](https://complyadvantage.com/insights/what-is-fraud-detection/#:~:text=Fraud%20detection%20refers%20to%20the,laundering%20(AML)%20compliance%20processes.):
    the algorithms’ objective is to identify unusual transaction patterns, indicating
    fraudulent activities.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/99f230d3264b77bf8a15c7ab46929fa2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this introductive guide, I will present several outlier detection methods,
    falling into these three main categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Graphical Approach**: detect outliers through **data visualizations**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statistical Approach**: detect outliers through **statistical analysis**
    and probability distributions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithmic Approach**: detect outliers through **Machine Learning models**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/abe4ed6799313c67c954005df0d804c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Graphical Approach**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Graphical Approach for outlier detection leverages the **human brain’s**
    remarkable **ability to discern patterns**. It makes use of visualization tools
    like Scatter Plots, Box Plots, and Heatmaps to provide a narrative of the data
    and allow Data Scientists to spot **irregularities in the pattern**.
  prefs: []
  type: TYPE_NORMAL
- en: Scatter Plot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a [Scatter Plot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html),
    outliers will appear as points significantly **deviating from the main cluster**.
  prefs: []
  type: TYPE_NORMAL
- en: After generating synthetic data, I will use the [Matplotlib Pyplot](https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html)
    library to create a Scatter Plot in [Python](https://www.python.org/).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot will highlight a cluster of data having a significantly
    different value from the others:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1d9d85d7ecffc96e10a635fb3b100664.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Box Plot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the same data, we can display a [Box Plot](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.boxplot.html),
    in which outliers appear as points **outside the “whiskers”** of the Box Plot.
  prefs: []
  type: TYPE_NORMAL
- en: The [Python](https://www.python.org/) code is the following.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/83f22edd9fd52523f395e0719e593cdb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Violin Plot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to Box Plots, [Violin Plots](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.violinplot.html)
    don’t show only the distribution of the data but also display its **Probability
    Density**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c663f6960ced0a7486794e0be04e1f32.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, outliers are visible as a thin section extending beyond the bulk
    of the data. Generally speaking, if I notice portions of the Violin Plot extending
    far beyond the rest, those are likely outliers.
  prefs: []
  type: TYPE_NORMAL
- en: You can generate the same plot, or personalize it, with the following lines
    of code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Statistical Approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the graphical approach is indeed of more immediate understanding, it comes
    with limitations. The main issue is they provide just **qualitative but not quantitative**
    results. For this reason, Scatter Plots, Box Plots, and Distribution Plots aid
    **effective communication**, but for consistent analysis, we must rely on **mathematical
    rigor** and **statistical metrics**.
  prefs: []
  type: TYPE_NORMAL
- en: Statistic tools such as [Z-scores](https://www.khanacademy.org/math/statistics-probability/modeling-distributions-of-data/z-scores/a/z-scores-review#:~:text=A%20z%2Dscore%20measures%20exactly,z%20%3D%20x%20%E2%88%92%20%CE%BC%20%CF%83%20%E2%80%8D)
    and [Interquartile Range](https://en.wikipedia.org/wiki/Interquartile_range) (IQR)
    assess data points using statistical parameters. They enable Data Scientists to
    systematically identify outliers by measuring how data points deviate from the
    expected statistical distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the **Z-score**, it measures how many standard deviations a data point
    lies away from the mean. Data points with Z-scores beyond a certain threshold
    can be flagged as outliers. Commonly a Z-score higher than 2 or 3 denotes an outlier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In a similar fashion, **IQR** relies on the range between the first and third
    quartiles of the data distribution. Any data points lying significantly beyond
    this range, are identified as outliers. Since sometimes the range between the
    first and third quartiles can be too strict, we can parametrically adjust it and
    consider, for example, the range between the first and ninth deciles.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, data points outside the range *Q1–1.5*IQR* and *Q3+1.5*IQR*
    are commonly classified as outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, the [**Tukey’s Fence**](https://community.ibm.com/community/user/ai-datascience/blogs/moloy-de1/2021/03/23/points-to-ponder)
    method is a parametrical approach based on the IQR range. It considers outliers
    all the data points that fall outside the range *Q1-k*IQR* and *Q3+k*IQR,* where
    *k* is a constant. Usually, *k* takes a value between 1.5 and 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: It is interesting to graphically visualize the difference between these three
    methods of detecting outliers inside the same dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8a3e6003d7a957317df378e86c02fc3b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: You can see how, in this case, the vanilla IQR method is too strict.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithmic Approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, the Algorithmic Approach leverages the power of **Machine Learning
    algorithms**, overcoming the limitations of the simpler Statistical Approaches.
  prefs: []
  type: TYPE_NORMAL
- en: There exists a broad variety of Outlier Detection models, including the [**Isolation
    Forest**](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html),
    [**Local Outlier Factor**](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html),
    and the [**One-Class Support Vector Machine**](http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html).
    They all provide reliable techniques to discern anomalies inside complex and multidimensional
    datasets. Unlike conventional statistical methods, these algorithms are better
    at understanding the intricate pattern of the data and defining more complex decision
    boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: Isolation Forest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first Outlier Detection algorithm I present is the [Isolation Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html)
    as it is probably the single Machine Learning model I use **more frequently**
    in my daily tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Isolation Forest relies on the principle of the more famous Random Forest, and,
    overall, of the **Ensemble Learning Techniques**. If you are not familiar with
    Random Forest or Ensemble Learning I suggest catching up with this simple guide.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/ensemble-learning-with-scikit-learn-a-friendly-introduction-5dd64650de6c?source=post_page-----382d1411b8ec--------------------------------)
    [## Ensemble Learning with Scikit-Learn: A Friendly Introduction'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning algorithms like XGBoost or Random Forests are among the top-performing
    models in Kaggle competitions…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/ensemble-learning-with-scikit-learn-a-friendly-introduction-5dd64650de6c?source=post_page-----382d1411b8ec--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Isolation Forest’s core idea relies on the observation that anomalies are, by
    definition, rare and often require **fewer steps to isolate** within a tree structure,
    with respect to normal data. Hence, Isolation Forest builds an **ensemble of decision
    trees**, which isolate outliers more rapidly due to their sparse nature. By measuring
    the average path length of data points in these trees, Isolation Forest effectively
    quantify an anomaly score for each data point.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Isolation Forest in Python is very simple, thanks to the [Scikit-Learn](https://scikit-learn.org/)
    (sklearn) library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Local Outlier Factor (LOF)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on the idea that an anomaly is often more isolated, in the feature space,
    than its *k* nearest neighbors, [**Local Outlier Factor**](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html)
    (LOF) assesses the local neighborhood of each data point, calculating its density
    concerning its neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: Outliers often show significantly lower local densities compared to their neighbors,
    making them detectable through LOF algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '[Scikit-Learn](https://scikit-learn.org/) (sklearn) provides an easy tool to
    implement LOF with Python.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The parameters to tune, when applying the LOF algorithm, are the number of neighbors
    to consider for density estimation, and the contamination coefficient. The last
    one represents the expected proportion of outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I presented three distinct approaches for Outlier Detection, each of them having
    **advantages and limitations**.
  prefs: []
  type: TYPE_NORMAL
- en: The **Graphical Approach**, with its visualization techniques like Scatter and
    Box Plots, is indubitably the most intuitive one and it is well-suited for initial
    data explorations. However, it may struggle with high-dimensional data and it
    lacks of numerical precision, making it only a qualitative tool.
  prefs: []
  type: TYPE_NORMAL
- en: This last downside of the Graphical Approach is compensated by the numerical
    robustness of the **Statistical** one. Methods like the Z-score offer a precise
    metric for the data anomaly score and explore more complex data relationships.
    The Statistical Approach is limited by the common assumption of normality distribution
    of the data, leading to some struggles with skewed data.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, **Machine Learning** algorithms like the Isolation Forest are the cutting-edge
    approach as they are, in theory, more powerful than the Graphical and Statistical
    ones. They are state-of-the-art at understanding complex data spaces, where the
    pattern is difficult to find. These big qualities come with the limitation of
    parameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: This introduction is a good point to start from, however, it only scratches
    the surface of the Outlier Detection field. For those of you interested to deep
    dive into this field, I leave a list of interesting and insightful resources.
  prefs: []
  type: TYPE_NORMAL
- en: If you liked this story, consider following me to be notified of my upcoming
    projects and articles!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of my past projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/social-network-analysis-with-networkx-a-gentle-introduction-6123eddced3?source=post_page-----382d1411b8ec--------------------------------)
    [## Social Network Analysis with NetworkX: A Gentle Introduction'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how companies like Facebook and LinkedIn extract insights from networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/social-network-analysis-with-networkx-a-gentle-introduction-6123eddced3?source=post_page-----382d1411b8ec--------------------------------)
    [](/advanced-dimensionality-reduction-models-made-simple-639fca351528?source=post_page-----382d1411b8ec--------------------------------)
    [## Advanced Dimensionality Reduction Models Made Simple
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to efficiently apply state-of-the-art Dimensionality Reduction methods
    and boost your Machine Learning…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/advanced-dimensionality-reduction-models-made-simple-639fca351528?source=post_page-----382d1411b8ec--------------------------------)
    [](/ensemble-learning-with-scikit-learn-a-friendly-introduction-5dd64650de6c?source=post_page-----382d1411b8ec--------------------------------)
    [## Ensemble Learning with Scikit-Learn: A Friendly Introduction'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning algorithms like XGBoost or Random Forests are among the top-performing
    models in Kaggle competitions…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/ensemble-learning-with-scikit-learn-a-friendly-introduction-5dd64650de6c?source=post_page-----382d1411b8ec--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*References*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Hawkins, D. (1980). Identification of Outliers. Chapman and Hall.](https://link.springer.com/book/10.1007/978-94-015-3994-4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly detection: A survey.
    ACM Computing Surveys (CSUR), 41(3), 1–58.](https://dl.acm.org/doi/10.1145/1541880.1541882)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Tukey, J. W. (1977). Exploratory Data Analysis. Addison-Wesley.](https://onlinelibrary.wiley.com/doi/10.1002/bimj.4710230408)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Rousseeuw, P. J., & Leroy, A. M. (1987). Robust regression and outlier detection.
    Wiley.](https://onlinelibrary.wiley.com/doi/book/10.1002/0471725382)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hoaglin, D. C., Mosteller, F., & Tukey, J. W. (1983). Understanding robust
    and exploratory data analysis. Wiley.](https://www.wiley.com/en-us/Understanding+Robust+and+Exploratory+Data+Analysis-p-9780471384915)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Liu, F. T., Ting, K. M., & Zhou, Z. H. (2008). Isolation Forest. In Proceedings
    of the 2008 Eighth IEEE International Conference on Data Mining (pp. 413–422).](https://ieeexplore.ieee.org/document/4781136)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000). LOF: Identifying
    Density-Based Local Outliers. In Proceedings of the ACM SIGMOD International Conference
    on Management of Data (pp. 93–104).](https://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition
    — Aurélien Géron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)ù'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python Data Science Handbook Essential Tools for Working with Data](https://jakevdp.github.io/PythonDataScienceHandbook/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
