- en: 'Mastering ChatGPT: Effective Summarization with LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce](https://towardsdatascience.com/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to Prompt ChatGPT to get High-Quality Summaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@andvalenzuela?source=post_page-----16cf0e3625ce--------------------------------)[![Andrea
    Valenzuela](../Images/ddfc1534af92413fd91076f826cc49b6.png)](https://medium.com/@andvalenzuela?source=post_page-----16cf0e3625ce--------------------------------)[](https://towardsdatascience.com/?source=post_page-----16cf0e3625ce--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----16cf0e3625ce--------------------------------)
    [Andrea Valenzuela](https://medium.com/@andvalenzuela?source=post_page-----16cf0e3625ce--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----16cf0e3625ce--------------------------------)
    ·10 min read·May 22, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c875157182b9485516e809510ba0514.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-made gif.
  prefs: []
  type: TYPE_NORMAL
- en: Are you part of the population that leaves reviews in Google maps everytime
    you visit to a new restaurant?
  prefs: []
  type: TYPE_NORMAL
- en: Or perhaps you are the type to share your opinion on Amazon purchases, especially
    when you get triggered by a low-quality product?
  prefs: []
  type: TYPE_NORMAL
- en: '*Don’t worry, I won’t blame you — we all have our moments!*'
  prefs: []
  type: TYPE_NORMAL
- en: In today’s data world, we all contribute to the data deluge in multiple ways.
    One data type that I find particularly interesting due to its diversity and difficulty
    of interpretation is textual data, such as the countless reviews that are posted
    over the Internet every day. *Have you ever stopped to consider the importance
    of standardizing and condensing textual data?* **Welcome to the world of summarization
    agents!**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1300eb548d9ed867e48cea796690892.png)'
  prefs: []
  type: TYPE_IMG
- en: Summarization agents imagined by the AI image generation tool Dall-E.
  prefs: []
  type: TYPE_NORMAL
- en: Summarization agents have seamlessly integrated into our daily lives condensing
    information and providing quick access to relevant content across a multitude
    of applications and platforms.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will explore the utilization of ChatGPT as a powerful summarization
    agent for our custom applications. Thanks to the ability of Large Language Models
    (LLM) to process and understand texts, **they can assist in reading texts and
    generating accurate summaries or standarizing information**. However, it is important
    to know how to extract their potential in doing such a task, as well as to acknowledge
    their limitations.
  prefs: []
  type: TYPE_NORMAL
- en: '*The biggest limitation for summarization?*'
  prefs: []
  type: TYPE_NORMAL
- en: '**LLMs often fall short when it comes to adhering to specific character or
    word limitations** in their summaries.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Let’s explore the best practices for generating summaries with ChatGPT**
    for our custom application, as well as the reasons behind its limitations and
    how to overcome them!'
  prefs: []
  type: TYPE_NORMAL
- en: Effective Summarization with ChatGPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Summarization agents are used all over the Internet. For instance, websites
    use summarization agents to offer concise summaries of articles, enabling users
    to gain a quick overview of the news without diving into the entire content. Social
    media platforms and seach engines do this too.
  prefs: []
  type: TYPE_NORMAL
- en: '**From news aggregators and social media platforms to e-commerce websites,
    summarization agents have become an integral part of our digital landscape**.
    And with the raise of LLMs, some of these agents are now using AI for more effective
    summarization results.'
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT can be a good ally when building an application using summarization
    agents for speeding up reading tasks and classifying texts. For example, imagine
    we have an e-commerce and we are interested in processing all our costumer reviews.
    **ChatGPT could help us in summarizing any given review in a few sentences, standarizing
    it to a generic format, determine the sentiment of the review and classify it
    accordingly**.
  prefs: []
  type: TYPE_NORMAL
- en: While it is true that we could simply feed the review to ChatGPT, there are
    a list of best practices *— and things to avoid —* to leverage the power of ChatGPT
    in this concrete task.
  prefs: []
  type: TYPE_NORMAL
- en: '**Let’s explore the options by bring this example to life!**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: E-commerce Reviews'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/086702ae2103f86e931e36ff8ec4a39f.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-made gif.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the example above in which we are interested in processing all the
    reviews for a given product on our e-commerce website. We would be interested
    in processing reviews such as the following one about our star product: **a first
    computer for children!**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we would like that ChatGPT to:'
  prefs: []
  type: TYPE_NORMAL
- en: Classify the review into positive or negative.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide a summary of the review of 20 words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output the response with a concrete structure to standardize all the reviews
    into one single format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation Notes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here is the basic code structure we colud use to prompt ChatGPT from our custom
    application. I also provide a link to a [Jupyter Notebook](https://github.com/for-code-sake/chatgpt/blob/main/summarization-with-llms/summarization-examples.ipynb)
    with all the examples used in this article.
  prefs: []
  type: TYPE_NORMAL
- en: The function `get_completion()` calls the ChatGPT API with a given *prompt*.
    If the prompt contains additional *user text*, such as the review itself in our
    case, it is separated from the rest of the code by triple quotes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Let’s use the** `**get_completion()**` **function to prompt ChatGPT !**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a prompt fulfilling the requirements described above:'
  prefs: []
  type: TYPE_NORMAL
- en: ⚠️ The prompting guidelines used in this example such as using delimiters to
    separate the input text from the rest of the prompt and asking for a structured
    output are fully explained at [**What I Learned from OpenAI’s Course on Prompt
    Engineering — Prompting Guidelines**](https://medium.com/geekculture/prompt-engineering-prompting-guidelines-chatgpt-chatgpt3-chatgpt4-artificial-intelligence-6b74f35d2695)**.**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Here is ChatGPT’s answer:'
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe from the output, the review is accurate and well structured,
    although **it misses some information we could be interested on as the owners
    of the e-commerce**, such as information about the delivery of the product.
  prefs: []
  type: TYPE_NORMAL
- en: Summarize with a Focus on <Shipping and Delivery>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**We can iteratively improve our prompt asking to ChatGPT some focus to include
    in the summary**. In this case, we are interested in any details given about the
    shipping and delivery:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, ChatGPT’s answer is the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: Now the review is much more complete. **Giving details on the important focus
    of the original review is crucial to avoid ChatGPT skipping some information that
    might be valuable for our use case**.
  prefs: []
  type: TYPE_NORMAL
- en: '*Have you noticed that although this second trial includes information on the
    delivery, it skipped the only negative aspect of the original review?*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Let’s fix that!**'
  prefs: []
  type: TYPE_NORMAL
- en: “Extract” instead of “Summarize”
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By investigating summarization tasks, I found out that that **summarization
    can be a tricky task for LLMs if the user prompt is not accurate enough**.
  prefs: []
  type: TYPE_NORMAL
- en: When asking ChatGPT to provide a summary of a given text, it can skip information
    that might be relevant for us *— as we have recently experienced —* , or it will
    give the same importance to all the topics on the text, only providing an overview
    of the main points.
  prefs: []
  type: TYPE_NORMAL
- en: Experts in LLMs use the term *extract* and additional information on their focuses
    instead of *summarize* when doing such tasks assisted by these type of models.
  prefs: []
  type: TYPE_NORMAL
- en: '**While summarization aims to provide a concise overview of the text’s main
    points including topics non-related to the topic of focus, information extraction
    focuses on retrieving specific details** and can give us what we are exactly looking
    for. Let’s try then with extraction!'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, by using extraction, we only get information about our topic
    of focus: `Shipping: Arrived a day earlier than expected.`'
  prefs: []
  type: TYPE_NORMAL
- en: Automatization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This system works for one single review. Neverthtless, when designing a prompt
    for a concrete application, **it is important to test it in a batch of examples
    so that we can catch any outliers or misbehavior in the model**.
  prefs: []
  type: TYPE_NORMAL
- en: In case of processing multiple reviews, here is a sample Python code structure
    that can help.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the summaries of our batch of reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: ⚠️ Note that although the word restriction of our summaries was clear enough
    in our prompts, we can easily see that this word limitation is not accomplished
    in any of the iterations.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This mismatch in the word counting happens because **LLMs do not have a precise
    understanding of word or character count**. The reason behind relies on one of
    the main important components of their architecture: **the tokenizer**.'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs like ChatGPT are designed to generate text based on statistical patterns
    learned from vast amounts of language data. **While they are highly effective
    at generating fluent and coherent text, they lack precise control over the word
    count**.
  prefs: []
  type: TYPE_NORMAL
- en: In the examples above, when we have given instructions about a very precise
    word count, **ChatGPT has struggle to meet that requirements**. Instead, it has
    generated text that is actually shorter than the specified word count.
  prefs: []
  type: TYPE_NORMAL
- en: In other cases, it may generate longer texts or simply text that is overly verbose
    or lacking in detail. Additionally, **ChatGPT may prioritize other factors such
    as coherence and relevance, over strict adherence to the word count**. This can
    result in text that is high-quality in terms of its content and coherence, but
    which does not precisely match the word count requirement.
  prefs: []
  type: TYPE_NORMAL
- en: '**The tokenizer is the key element in the architecture of ChatGPT that clearly
    influences the number of words of the generated output**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eab3dee577752c4e18c4dc06019c4b5c.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-made gif.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizer Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The tokenizer is the first step in the process of text generation. **It is responsible
    for breaking down the piece of text that we input to ChatGPT into individual elements
    *— tokens —*** , which are then processed by the language model to generate new
    text.
  prefs: []
  type: TYPE_NORMAL
- en: When the tokenizer breaks down a piece of text into tokens, it does so based
    on a set of rules that are designed to identify the meaningful units of the target
    language. However, these rules are not always perfect, and **there can be cases
    where the tokenizer splits or merges tokens in a way that affects the overall
    word count of the text**.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the following sentence: *“I want to eat a peanut butter
    sandwich”.* If the tokenizer is configured to split tokens based on spaces and
    punctuation, it may break this sentence down into the following tokens with a
    total word count of 8, equal to the token count.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3c808ab0be23847a489e30ef868ef03.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-made image.
  prefs: []
  type: TYPE_NORMAL
- en: However, if the tokenizer is configured to treat *“peanut butter”* as a compound
    word, it may break the sentence down into the following tokens, **with a total
    word count of 8, but a token count of 7**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10d9eac60a0ece12a78885329a5adc1a.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Thus, the way the tokenizer is configured can affect the overall word count
    of the text**, and this can impact the ability of the LLM to follow instructions
    about precise word counts. While some tokenizers offer options to customize how
    text is tokenized, this is not always sufficient to ensure precise adherence to
    word count requirements. **For ChatGPT in this case, we cannot control this part
    of its architecture**.'
  prefs: []
  type: TYPE_NORMAL
- en: This makes ChatGPT not good accomplishing character or word limitations, **but
    one can try with sentences instead since the tokenizer do not affect on the number
    of sentences, but their length**.
  prefs: []
  type: TYPE_NORMAL
- en: Being aware of this restriction can help you to build the best suitable prompt
    for your application in mind. *Having this knowledge about how word count works
    on ChatGPT, let’s do a final iteration with our prompt for the e-commerce application!*
  prefs: []
  type: TYPE_NORMAL
- en: 'Wrapping up: E-commerce Reviews'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s combine our learnings from this article into a final prompt! In this
    case, we will be asking for the results in `HTML` format for a nicer output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'And here is the final output from ChatGPT:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6f0192580ead353510a25e251dc3cdfe.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-made screenshot from the [Jupyter Notebook](https://github.com/for-code-sake/chatgpt/blob/main/summarization-with-llms/summarization-examples.ipynb)
    with the examples used in this article.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, **we have discussed the best practices for using ChatGPT as
    a summarization agent for our custom application**.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen that when building an application, it is extremely difficult to
    come up with the perfect prompt that matches your application requirements in
    the first trial. I think a nice take-home message is to **think about prompting
    as an iterative process** where you refine and model your prompt until you get
    exactly the desired output.
  prefs: []
  type: TYPE_NORMAL
- en: By iteratively refining your prompt and applying it to a batch of examples before
    deploying it into production, you can make sure **the output is consistent across
    multiple examples and cover outlier responses**. In our example, it could happen
    that someone provides a random text instead of a review. **We can instruct ChatGPT
    to also have an standardize output to exclude this outlier responses**.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, when using ChatGPT for a specific task, it is also a good practice
    to learn about the pros and cons of using LLMs for our target task. That is how
    we came across with the fact that ***extraction* tasks are more effective than
    *summarization* when we want a common human-like summary** of an input text. We
    have also learn that **providing the focus of the summary can be game-changer**
    regarding the generated content.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, while LLMs can be highly effective at generating text, **they are not
    ideal for following precise instructions about word count or other specific formatting
    requirements**. To achieve these goals, it may be necessary to stick to sentence
    counting or use other tools or methods, such as manual editing or more specialized
    software.
  prefs: []
  type: TYPE_NORMAL
- en: And that is all! Many thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: I hope this article helps **when building custom applications with ChatGPT!**
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also subscribe to my [**Newsletter**](https://medium.com/@andvalenzuela/subscribe)
    to stay tuned for new content. **Especially**, **if you are interested in articles
    about ChatGPT**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/chatgpt-text-to-speech-artificial-intelligence-python-data-science-52456f51fad6?source=post_page-----16cf0e3625ce--------------------------------)
    [## Unlocking a New Dimension of ChatGPT: Text-to-Speech Integration'
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing User Experience in ChatGPT Interactions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/chatgpt-text-to-speech-artificial-intelligence-python-data-science-52456f51fad6?source=post_page-----16cf0e3625ce--------------------------------)
    [](https://medium.com/geekculture/prompt-engineering-prompting-guidelines-chatgpt-chatgpt3-chatgpt4-artificial-intelligence-6b74f35d2695?source=post_page-----16cf0e3625ce--------------------------------)
    [## What I Learned from OpenAI’s Course on Prompt Engineering — Prompting Guidelines
  prefs: []
  type: TYPE_NORMAL
- en: Get to know OpenAI’s Guidelines for Better Prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'medium.com](https://medium.com/geekculture/prompt-engineering-prompting-guidelines-chatgpt-chatgpt3-chatgpt4-artificial-intelligence-6b74f35d2695?source=post_page-----16cf0e3625ce--------------------------------)
    [](/what-chatgpt-knows-about-you-openai-towards-data-privacy-science-ai-b0fa2376a5f6?source=post_page-----16cf0e3625ce--------------------------------)
    [## What ChatGPT Knows about You: OpenAI’s Journey Towards Data Privacy'
  prefs: []
  type: TYPE_NORMAL
- en: New ways to manage personal data in ChatGPT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/what-chatgpt-knows-about-you-openai-towards-data-privacy-science-ai-b0fa2376a5f6?source=post_page-----16cf0e3625ce--------------------------------)
    [](https://levelup.gitconnected.com/improve-chatgpt-performance-prompt-engineering-data-science-artificial-intelligence-6fa3953bc5b6?source=post_page-----16cf0e3625ce--------------------------------)
    [## Improve ChatGPT Performance with Prompt Engineering
  prefs: []
  type: TYPE_NORMAL
- en: How to Ask Questions to ChatGPT to Maximize the Chances of a Successful Answer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/improve-chatgpt-performance-prompt-engineering-data-science-artificial-intelligence-6fa3953bc5b6?source=post_page-----16cf0e3625ce--------------------------------)
    [](https://medium.com/geekculture/openai-fine-tuning-custom-chatgpt-transfer-learning-prompt-data-science-machine-learning-chatgpt3-chatgpt4-2aad7148438a?source=post_page-----16cf0e3625ce--------------------------------)
    [## Create Your Custom ChatGPT with Transfer Learning
  prefs: []
  type: TYPE_NORMAL
- en: Enhance ChatGPT Capabilities Fine-tuning Your Own Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/geekculture/openai-fine-tuning-custom-chatgpt-transfer-learning-prompt-data-science-machine-learning-chatgpt3-chatgpt4-2aad7148438a?source=post_page-----16cf0e3625ce--------------------------------)
    [](https://pub.towardsai.net/chatgpt-vs-artificial-intelligence-detectors-chatgpt4-chatgpt3-openai-technology-2c46e6acf6b?source=post_page-----16cf0e3625ce--------------------------------)
    [## ChatGPT vs AI Detectors — Place your Bets!
  prefs: []
  type: TYPE_NORMAL
- en: Testing the Most Popular AI Detectors on the Internet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pub.towardsai.net](https://pub.towardsai.net/chatgpt-vs-artificial-intelligence-detectors-chatgpt4-chatgpt3-openai-technology-2c46e6acf6b?source=post_page-----16cf0e3625ce--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Feel free to forward any questions** you may have to *forcodesake.hello@gmail.com*
    :)'
  prefs: []
  type: TYPE_NORMAL
