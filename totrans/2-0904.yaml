- en: First Steps in the World Of Reinforcement Learning using Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3](https://towardsdatascience.com/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Original Python implementation of how to find the best places to be in one of
    the fundamental worlds of reinforcement learning — the grid world
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://eligijus-bujokas.medium.com/?source=post_page-----b843b76538e3--------------------------------)[![Eligijus
    Bujokas](../Images/061fd30136caea2ba927140e8b3fae3c.png)](https://eligijus-bujokas.medium.com/?source=post_page-----b843b76538e3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b843b76538e3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b843b76538e3--------------------------------)
    [Eligijus Bujokas](https://eligijus-bujokas.medium.com/?source=post_page-----b843b76538e3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b843b76538e3--------------------------------)
    ·15 min read·Jan 13, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b476cbb9f0b2f5a0f39114a7b0ebca24.png)'
  prefs: []
  type: TYPE_IMG
- en: Gridworld matrices; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of this article is to present fundamental concepts and definitions
    in Reinforcement Learning (from here on — **RL**) using Python code and comments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The article was heavily inspired by the great RL course: [https://www.coursera.org/learn/fundamentals-of-reinforcement-learning](https://www.coursera.org/learn/fundamentals-of-reinforcement-learning)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The theory is laid out in the book¹: [http://www.incompleteideas.net/book/RLbook2020.pdf](http://www.incompleteideas.net/book/RLbook2020.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for all my RL experiments can be seen in my Gitlab repo: [https://github.com/Eligijus112/rl-snake-game](https://github.com/Eligijus112/rl-snake-game)'
  prefs: []
  type: TYPE_NORMAL
- en: The grid world problem is a classic problem in RL where we want to create an
    optimal strategy for an agent to traverse a grid.
  prefs: []
  type: TYPE_NORMAL
- en: A grid is a square matrix of cells, and the agent can move in any of the four
    directions (up, down, left, right) in each cell. The agent receives a reward of
    -1 for each step it takes, and a reward of +10 if it reaches the goal cell. The
    numbers for the rewards are arbitrary and can be defined by the user.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, an agent in RL frameworks is defined as **the component that makes
    the decision of what action to take.** The agent takes action in concrete time
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the grid world setting, the whole action set is defined by the following
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b336858f0f5eab063a10922cc7926120.png)'
  prefs: []
  type: TYPE_IMG
- en: Action set
  prefs: []
  type: TYPE_NORMAL
- en: Or
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f396e4084c317ccd1fa466164aecb21.png)'
  prefs: []
  type: TYPE_IMG
- en: Action set
  prefs: []
  type: TYPE_NORMAL
- en: 'No matter where our agent is, it can only move either left, right up or down.
    Now let us define and visualize our grid world:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ddb3ae05c5ca79f13def722c6674a83b.png)'
  prefs: []
  type: TYPE_IMG
- en: Gridworld; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: In the above example, we have defined our first needed matrix — the **R** matrix
    or the **reward matrix**. The goals are at the centre and at the corners of the
    grid world. When the agent goes into one of the cells, it receives a reward of
    the value of that cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us define another key matrix - the state matrix **S**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/51f7e9d43aef3204ba3682c32376d982.png)'
  prefs: []
  type: TYPE_IMG
- en: The state space; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: In the grid world that we defined, there are in total **49 states** that an
    agent can be in. Each state can be identified by the integer from the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us assume that our agent is in state 17 and moves down. That action value**,
    denoted as q,** is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4e9be0dceaf7766eb7616a668122640c.png)'
  prefs: []
  type: TYPE_IMG
- en: The action value for action ‘down’ in state 17
  prefs: []
  type: TYPE_NORMAL
- en: 'The action value is 10 because the reward at grid 24 is equal to 10\. Thus,
    when we are using action values, we need to keep note of the grid **S** index
    and the reward matrix **G**. One can easily guess, that moving from the same state
    to the right will have a reward of -1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0938faa5a0b3ab7968d7ffc4391a783f.png)'
  prefs: []
  type: TYPE_IMG
- en: The action value for action ‘right’ in state 17
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, the function q, called action—value, maps a number to a state—action
    pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e7166d1a7357025897ef15cf4d3a83a.png)'
  prefs: []
  type: TYPE_IMG
- en: Action value function
  prefs: []
  type: TYPE_NORMAL
- en: The higher the number, the bigger the “reward” for the agent and thus, the agent
    always wants to make an action which **maximizes** **q from the current state**.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have defined the matrices **R** **(rewards)**, and **S (states)**.
    Another key matrix is the state value matrix **V**. The dimensions of the **V**
    matrix are the same as the S and G matrices and each element in the **V** matrix
    evaluates the “goodness” of the given state. The “goodness” here refers to the
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4c220ff25b15ac721c1892e297056cbf.png)'
  prefs: []
  type: TYPE_IMG
- en: Value of state s [1]
  prefs: []
  type: TYPE_NORMAL
- en: 'We read the above equation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The value of state s given policy pi is equal to the expected returns at time
    step t given that the state at time t was s.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We calculate the above value for all the states and store it in the matrix **V**.
    We have introduced new variables here so let us define them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f046ada703904fdc3e0bf22f7cadd61a.png)'
  prefs: []
  type: TYPE_IMG
- en: The total return at time step t [1]
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a87ffee384e210ada1ee944a256fde51.png)'
  prefs: []
  type: TYPE_IMG
- en: Discount factor [1]
  prefs: []
  type: TYPE_NORMAL
- en: The index **K** is called the terminal state where the agent reaches any of
    the goals in our grid world. In other words, **the value of G in each state shows
    the discounted reward sum when starting the agent path towards the goal from the
    given state. The bigger the value, the more desirable the state is.**
  prefs: []
  type: TYPE_NORMAL
- en: 'The pi term in the value of state equation is called **policy** and is a probability
    of taking an action in state s:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16502cff059ff88102a32e427a8ce0a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Policy
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us initiate the initial value matrix **V**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/faa47e2889ea96314d3c7fe5782f0445.png)'
  prefs: []
  type: TYPE_IMG
- en: Initial state value matrix; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: As we have not yet explored our created world of grids, all the returns of the
    states are 0.
  prefs: []
  type: TYPE_NORMAL
- en: The last matrix we will need is the policy matrix **P**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ca4fcca11a20fa8491331e3f6beaa831.png)'
  prefs: []
  type: TYPE_IMG
- en: Initial policy matrix; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: Each arrow in a grid represents the available actions that an agent can take.
    The probabilities in the initial matrix are uniform and there are no available
    moves in the goal states.
  prefs: []
  type: TYPE_NORMAL
- en: Having the **R, P, S** and **V** matrices, we can finally start calculating
    the answer for our RL problem. But we have yet to define, what is the RL objective.
  prefs: []
  type: TYPE_NORMAL
- en: '**The objective of an RL algorithm is for the agent to find the optimal policy
    P that maximizes the returns in each state.**'
  prefs: []
  type: TYPE_NORMAL
- en: Another formulation is that **the objective is to calculate the optimal state
    values in the matrix V.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us say we have a 5 by 5 grid with the goal at the centre:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3276db0f4e58cee158234d9b46fcec5c.png)'
  prefs: []
  type: TYPE_IMG
- en: Example grid world; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: 'To build intuition, I have calculated the optimal values and the policies.
    We will find out how to do it in the next section of this article but for now
    let us interpret the following **V** and **P** matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/522e430faa9e355649fec4444715db3f.png)'
  prefs: []
  type: TYPE_IMG
- en: Solved Value and Policy matrices; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that every value in the V matrix is the total accumulated discounted
    reward. Thus, our agent would want to always go to the state which has the highest
    value. To put it mathematically, at every state the agent is in, he will choose
    the next state by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a1d5be7b12aebedcd66dfb4efcf11f0f.png)'
  prefs: []
  type: TYPE_IMG
- en: The optimal choice at every state [1]
  prefs: []
  type: TYPE_NORMAL
- en: At every state, we will choose an action whichleads us to the state **s prime**
    wherethe **r + gamma * (new state value)** is the highest.
  prefs: []
  type: TYPE_NORMAL
- en: To put an even simpler intuition, we can just list out all the available actions
    from the current state, check which available state has the highest **V(s)** value
    and go there. Looking at the matrices above, we can see that, for example, state
    8 has two optimal choices — **down** and **left**. This is because these actions
    will lead the agent into equally good states. Thus having the **V** matrix, we
    will always infer the P matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, how to go from the V matrix with all zeroes to a matrix with values? We
    need to define the Bellman equation for each state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad0993137593b7dfc5db6303504664b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Bellman equation for state s with policy pi [1]
  prefs: []
  type: TYPE_NORMAL
- en: The above equation is intimidating and has a recursive property. For the grid
    world example, we can simplify the equation and write it without the conditional
    probabilities in the middle.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/66808347cf33d040418b7e026674b640.png)'
  prefs: []
  type: TYPE_IMG
- en: Simplified equation [1]
  prefs: []
  type: TYPE_NORMAL
- en: We can do this because when we make an action in a state s, we are guaranteed
    to go to just one next state.
  prefs: []
  type: TYPE_NORMAL
- en: In the solved matrix example, recall that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06df8d53e68b9b0497e8073ac5968a3f.png)'
  prefs: []
  type: TYPE_IMG
- en: Value of state 0
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that in the long run our agent if starting from position 0, will
    accumulate a -6.07 total reward. In order to estimate this and go from a recursive
    formula to a formula we can evaluate with a simple for loop, we will use the following
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/036a51aa40f92331d08f1e0c65fddf33.png)'
  prefs: []
  type: TYPE_IMG
- en: Value iteration algorithm [1]
  prefs: []
  type: TYPE_NORMAL
- en: 'We will simplify the middle part of the algorithm for the grid world problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f0ab254caf5769c503bc05223296f649.png)'
  prefs: []
  type: TYPE_IMG
- en: Value iteration algorithm simplified; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: Now let us transfer everything to Python code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The above function finds the Belman equation value for the state **s**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The above code chunk implements the value iteration algorithm for finding the
    optimal (or close to optimal) **V** matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have all the theory and code for us to start evaluating all the states
    in our grid world. Recall, that we have the initial grid world as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07a9e06cbad806a751627013cd488f21.png)'
  prefs: []
  type: TYPE_IMG
- en: State space; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ddb3ae05c5ca79f13def722c6674a83b.png)'
  prefs: []
  type: TYPE_IMG
- en: Gridworld; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a8da08bce9e812777606c180c4a48507.png)'
  prefs: []
  type: TYPE_IMG
- en: Initial value and policy matrices; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: Now let us update one state — the first or **s = 1**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The value matrix and the policy matrix now look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/555818070687f34a555a1516f51a5011.png)'
  prefs: []
  type: TYPE_IMG
- en: One value iteration for one state; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: Being in states 2 or 8 the optimal policy would be to move to state 1 because
    0 < 2.66 and thus state 1 is more valuable than its neighbours.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us now update state 3 and see what is happening:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d7e22b1776ea0246a92ec6717ab79788.png)'
  prefs: []
  type: TYPE_IMG
- en: Updating the third state; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: The value for being in state 3 is -1 and thus, the agent, as of right now in
    our grid world, would want to avoid this state when compared to its neighbours.
  prefs: []
  type: TYPE_NORMAL
- en: 'The value iteration algorithm works in the exact same fashion as above just
    for all the states (in our case — from states 0 to 48). To implement it use the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/fb7237cf07e6f7665cd3700e36645e2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Solved grid world; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: The agent could start in any non-terminal state and move along the arrows in
    the policy matrix. If there are two or more arrows in the same state, we can move
    with the same probability into each of the states that the arrows are pointing
    to.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, in a simple RL problem, we have 4 main matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: Reward matrix **R**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State value function **V**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy matrix **P**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State matrix **S**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, we need a finite set of actions **A**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate each state theoretically, we use the Bellman equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad0993137593b7dfc5db6303504664b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Bellman equation for state s with policy pi
  prefs: []
  type: TYPE_NORMAL
- en: 'The evaluate the state value practically, we use the value iteration algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f0ab254caf5769c503bc05223296f649.png)'
  prefs: []
  type: TYPE_IMG
- en: Value iteration algorithm simplified; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: The goal of an RL assignment is to find the optimal policy that our agent can
    follow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feel free to use the code and make adjustments to it here: [https://github.com/Eligijus112/rl-snake-game](https://github.com/Eligijus112/rl-snake-game).'
  prefs: []
  type: TYPE_NORMAL
- en: Happy learning!
  prefs: []
  type: TYPE_NORMAL
- en: '[1]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Author: **Richard S. Sutton, Andrew G. Barto**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Year: 2018'
  prefs: []
  type: TYPE_NORMAL
- en: 'Title: **Reinforcement Learning: An Introduction**'
  prefs: []
  type: TYPE_NORMAL
- en: URL:[**http://archive.ics.uci.edu/ml**](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)
  prefs: []
  type: TYPE_NORMAL
