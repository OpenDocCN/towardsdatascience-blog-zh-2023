- en: Multiclass Logistic Regression using Gradient Descent
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用梯度下降的多类逻辑回归
- en: 原文：[https://towardsdatascience.com/logistic-regression-a3a23e169eec](https://towardsdatascience.com/logistic-regression-a3a23e169eec)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/logistic-regression-a3a23e169eec](https://towardsdatascience.com/logistic-regression-a3a23e169eec)
- en: An introduction to multiclass logistic regression with theory and Python implementation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多类逻辑回归的介绍，包括理论和 Python 实现
- en: '[](https://cookieblues.medium.com/?source=post_page-----a3a23e169eec--------------------------------)[![Stefan
    Hrouda-Rasmussen](../Images/701ac16a5753432d493002208151d89f.png)](https://cookieblues.medium.com/?source=post_page-----a3a23e169eec--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a3a23e169eec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a3a23e169eec--------------------------------)
    [Stefan Hrouda-Rasmussen](https://cookieblues.medium.com/?source=post_page-----a3a23e169eec--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://cookieblues.medium.com/?source=post_page-----a3a23e169eec--------------------------------)[![Stefan
    Hrouda-Rasmussen](../Images/701ac16a5753432d493002208151d89f.png)](https://cookieblues.medium.com/?source=post_page-----a3a23e169eec--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a3a23e169eec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a3a23e169eec--------------------------------)
    [Stefan Hrouda-Rasmussen](https://cookieblues.medium.com/?source=post_page-----a3a23e169eec--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a3a23e169eec--------------------------------)
    ·6 min read·Mar 27, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----a3a23e169eec--------------------------------)
    ·阅读时间6分钟·2023年3月27日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/119f2a6d5cc440dcfe1bdf8df9510593.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/119f2a6d5cc440dcfe1bdf8df9510593.png)'
- en: Decision boundary of Logistic Regression. Image by author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的决策边界。图片由作者提供。
- en: Contents
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内容
- en: This post is a part of a series of posts that I will be making. Underneath you
    can see an overview of the series.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是我将要发布的一系列文章的一部分。下面你可以看到该系列的概览。
- en: 1\. Introduction to machine learning
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 机器学习简介
- en: '[(a) What is machine learning?](/what-is-machine-learning-91040db474f9)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[(a) 什么是机器学习？](/what-is-machine-learning-91040db474f9)'
- en: '[(b) Model selection in machine learning](/model-selection-in-machine-learning-813fe2e63ec6)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[(b) 机器学习中的模型选择](/model-selection-in-machine-learning-813fe2e63ec6)'
- en: '[(c) The curse of dimensionality](/the-curse-of-dimensionality-5673118fe6d2)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[(c) 维度的诅咒](/the-curse-of-dimensionality-5673118fe6d2)'
- en: '[(d) What is Bayesian inference?](/what-is-bayesian-inference-4eda9f9e20a6)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[(d) 什么是贝叶斯推断？](/what-is-bayesian-inference-4eda9f9e20a6)'
- en: 2\. Regression
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 回归
- en: '[(a) How linear regression actually works](/how-linear-regression-actually-works-theory-and-implementation-8d8dcae3222c)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[(a) 线性回归实际上是如何工作的](/how-linear-regression-actually-works-theory-and-implementation-8d8dcae3222c)'
- en: '[(b) How to improve your linear regression with basis functions and regularization](/how-to-improve-your-linear-regression-with-basis-functions-and-regularization-8a6fcebdc11c)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[(b) 如何通过基函数和正则化改善线性回归](/how-to-improve-your-linear-regression-with-basis-functions-and-regularization-8a6fcebdc11c)'
- en: 3\. Classification
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 分类
- en: '[(a) Overview of Classifiers](/overview-of-classifiers-d0a0d3eecfd1)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[(a) 分类器概述](/overview-of-classifiers-d0a0d3eecfd1)'
- en: '[(b) Quadratic Discriminant Analysis (QDA)](/quadratic-discriminant-analysis-ae55d8a8148a)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[(b) 二次判别分析 (QDA)](/quadratic-discriminant-analysis-ae55d8a8148a)'
- en: '[(c) Linear Discriminant Analysis (LDA)](/linear-discriminant-analysis-1894bbf04359)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[(c) 线性判别分析 (LDA)](/linear-discriminant-analysis-1894bbf04359)'
- en: '[(d) (Gaussian) Naive Bayes](/gaussian-naive-bayes-4d2895d139a)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[(d) (高斯) 朴素贝叶斯](/gaussian-naive-bayes-4d2895d139a)'
- en: '**(e) Multiclass Logistic Regression using Gradient Descent**'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**(e) 使用梯度下降的多类逻辑回归**'
- en: Setup and objective
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置和目标
- en: 'So far we’ve gone over generative classifiers (QDA, LDA, and Naive Bayes),
    but now we’ll turn our eyes to a discriminative classifier: logistic regression.
    As mentioned in [3(a)](/overview-of-classifiers-d0a0d3eecfd1), the overview of
    classifiers, **logistic regression is a discriminative classifier** meaning that
    **it models the conditional probability distribution** of the target *P*(*t*|**x**)
    directly.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '到目前为止，我们已经讨论了生成型分类器（QDA、LDA 和朴素贝叶斯），但现在我们将目光转向一种判别型分类器：逻辑回归。如在[3(a)](/overview-of-classifiers-d0a0d3eecfd1)中所提到的分类器概述，**逻辑回归是一种判别型分类器**，这意味着**它直接建模目标的条件概率分布**
    *P*(*t*|**x**)。 '
- en: But why is it called logistic *regression* and not logistic *classification*
    if it’s a classification model? Well, the answer is simply that we’re regressing
    the conditional probability — I know, it’s confusing.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么它叫做逻辑 *回归* 而不是逻辑 *分类*，如果它是一个分类模型呢？嗯，答案很简单，因为我们在回归条件概率——我知道，这很令人困惑。
- en: Before we move on, **I recommend that you have a good grasp on linear regression**
    before moving onto logistic regression, **as they’re very similar**. If you read
    [my post about linear regression](/how-linear-regression-actually-works-theory-and-implementation-8d8dcae3222c),
    you’ll have an easy time, as I’m using the same terminology and notation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，**我建议你在学习逻辑回归之前对线性回归有一个好的掌握**，**因为它们非常相似**。如果你阅读了 [我关于线性回归的文章](/how-linear-regression-actually-works-theory-and-implementation-8d8dcae3222c)，你会很轻松，因为我使用了相同的术语和符号。
- en: Given a training dataset of *N* input variables **x** with corresponding target
    variables *t*, logistic regression assumes that
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含 *N* 个输入变量 **x** 和相应目标变量 *t* 的训练数据集，逻辑回归假设
- en: '![](../Images/da98de87d2d4103e528ae5a0af6aa47d.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da98de87d2d4103e528ae5a0af6aa47d.png)'
- en: where *c* is any integer from 1 to *C* denoting the number of classes.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *c* 是从 1 到 *C* 的任意整数，表示类别的数量。
- en: The right hand side of (1) is the [softmax function](https://en.wikipedia.org/wiki/Softmax_function).
    Basically, **this is the function that we use to transform our linear combination
    into probabilities** between 0 and 1.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: （1）的右侧是 [softmax 函数](https://en.wikipedia.org/wiki/Softmax_function)。基本上，**这是我们用来将线性组合转换为
    0 到 1 之间概率的函数**。
- en: To understand what (1) really means, let’s look at the special case where *C=2*,
    i.e., we have 2 classes. We would now typically rewrite (1) as
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解（1）实际意义是什么，让我们看看特殊情况，即 *C=2*，也就是说，我们有 2 个类别。我们现在通常将（1）重写为
- en: '![](../Images/c6052154396bbad63ad877aad6ba8f1e.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6052154396bbad63ad877aad6ba8f1e.png)'
- en: where *σ* refers to the [logistic sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function),
    which is where the name logistic regression comes from.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *σ* 指的是 [逻辑 sigmoid 函数](https://en.wikipedia.org/wiki/Sigmoid_function)，这就是逻辑回归名称的由来。
- en: 'Now, the logistic sigmoid function looks like this:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，逻辑 sigmoid 函数看起来是这样的：
- en: insert image of sigmoid function*
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 插入 sigmoid 函数的图像*
- en: As you can see, the y-values lie between 0 and 1, and the higher the value,
    the closer to 1\. **This means that the larger wᵀx is, the higher the probability
    that point (x,** *t***) is of class 1** — and likewise for more than 2 classes,
    whichever value of *c* is higher in (1) will be the class we predict for *t*,
    or
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，y 值在 0 和 1 之间，值越高，越接近 1。**这意味着 wᵀx 越大，点 (x,** *t***) 属于类别 1 的概率越高**
    —— 对于超过 2 个类别的情况也是如此，公式（1）中 *c* 的值越高，我们将预测 *t* 属于哪个类别，或者
- en: '![](../Images/684b87e501bc7dca8b3c3c5bb63503ca.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/684b87e501bc7dca8b3c3c5bb63503ca.png)'
- en: Derivation and training
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推导和训练
- en: So, how do we find the values of **W**? Well, just like with linear regression
    **we’ll use maximum likelihood estimation to find the values of our parameters**.
    This works by writing up the likelihood function, taking its derivative, and finding
    for which values of the parameters the derivative is equal to 0, as this will
    be the maximum of the likelihood.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何找到 **W** 的值呢？嗯，就像线性回归一样，**我们将使用最大似然估计来找出参数的值**。这个过程是通过写出似然函数，求其导数，并找到参数的哪些值使导数等于
    0，因为这将是似然的最大值。
- en: To make notation easier, let **t**ₙ denote a *C*-dimensional vector, where *t*ₙ꜀
    is 1 if the observation belongs to class *c*, and all other components are 0\.
    We can now write the likelihood as
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化符号表示，让 **t**ₙ 表示一个 *C* 维向量，其中 *t*ₙ꜀ 为 1 如果观测值属于类别 *c*，而其他所有组件为 0。我们现在可以将似然写成
- en: '![](../Images/4c7ebfa35328336d1db3a8fe11636362.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c7ebfa35328336d1db3a8fe11636362.png)'
- en: Just like with linear regression, we’ll now take the negative logarithm of the
    likelihood, as we know **minimizing the negative log-likelihood is equivalent
    to maximizing the likelihood**, to help our derivation
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 就像线性回归一样，我们现在将取负对数似然，因为我们知道 **最小化负对数似然等同于最大化似然**，以帮助我们的推导
- en: '![](../Images/d5f33b8e3dc8d24ab5d2ecb7da99cbe7.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5f33b8e3dc8d24ab5d2ecb7da99cbe7.png)'
- en: Now that we have the negative log-likelihood, **we’re going to find the gradient**
    (the derivative) of it and find for which values of **w** it is equal to 0\. The
    function in (3) is also called the **cross-entropy loss function**.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了负对数似然，**我们将找到它的梯度**（导数），并找出 **w** 的哪些值使其等于 0。公式（3）也被称为 **交叉熵损失函数**。
- en: Before we begin the derivation, let’s define some terms to make our notation
    simpler. Firstly, let us denote the softmax function as
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始推导之前，先定义一些术语以简化我们的符号。首先，让我们将 softmax 函数表示为
- en: '![](../Images/d203f47e0bf01e1a6f8649145e054ab1.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d203f47e0bf01e1a6f8649145e054ab1.png)'
- en: Secondly, let *E* denote the function from (3)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，让 *E* 表示（3）中的函数
- en: '![](../Images/67557b76fab6c47d3c26cf33984e2aa2.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67557b76fab6c47d3c26cf33984e2aa2.png)'
- en: Now, using the chain rule, we can determine the gradient of *E*
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用链式法则，我们可以确定 *E* 的梯度
- en: '![](../Images/3b0206027bc938e36e6b2b440fd93b13.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b0206027bc938e36e6b2b440fd93b13.png)'
- en: Starting from the right, we have
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从右侧开始，我们有
- en: '![](../Images/355c41d7c16bc84ee514e0d0cde02e21.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/355c41d7c16bc84ee514e0d0cde02e21.png)'
- en: Next up, we have the derivative of the softmax function
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有 softmax 函数的导数
- en: '![](../Images/f1f0cb0906eee4e4f4bf3ac06d8c3baf.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1f0cb0906eee4e4f4bf3ac06d8c3baf.png)'
- en: but this is only when *i=c*. In the case where they are not equal, we get
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 但这仅在 *i=c* 时成立。在它们不相等的情况下，我们得到
- en: '![](../Images/d8c7a9f17bfe7153cd691e4b50b933da.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8c7a9f17bfe7153cd691e4b50b933da.png)'
- en: We can combine (6) and (7) to
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将（6）和（7）结合起来
- en: '![](../Images/b178cdfb1f4c43567a29453203ceb515.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b178cdfb1f4c43567a29453203ceb515.png)'
- en: Lastly, we have
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有
- en: '![](../Images/c8ad36d87098e4f5fa8c5fd91bb67a5c.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8ad36d87098e4f5fa8c5fd91bb67a5c.png)'
- en: and now we finally have all the pieces of (4) to put together. Putting (5),
    (7), and (9) into (4) gives us the following
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们终于将（4）的所有部分拼凑在一起。将（5）、（7）和（9）代入（4）得到以下结果
- en: '![](../Images/fb779b8891711b32a19429d0673d9ccc.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb779b8891711b32a19429d0673d9ccc.png)'
- en: After all that work we’ve finally found the gradient of the likelihood function.
    What we need to do now is figure out for which values of **w** it is equal to
    0\. The reason for this is that the **maxima and minima of the function will be
    where the gradient is 0**.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 经过这些工作，我们终于找到了似然函数的梯度。我们现在需要做的是确定 **w** 为 0 的值。原因是 **函数的极大值和极小值将出现在梯度为 0 的地方**。
- en: 'The problem is that we cannot find a closed-form solution for this, so we’ll
    need an algorithm that figures out where the function is equal to 0 for us. The
    algorithm we’ll use is called **gradient descent**. It works by using the following
    equation:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是我们无法找到这个问题的闭式解，因此我们需要一个算法来为我们找出函数何时等于 0。我们将使用的算法称为 **梯度下降**。它通过使用以下方程来工作：
- en: '![](../Images/52c8ff62af06c96c9cd545d793e51c80.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52c8ff62af06c96c9cd545d793e51c80.png)'
- en: where *η* is called the learning rate and is a hyperparameter. I’ll write a
    post detailing gradient descent in the future, but briefly **if the learning rate
    is too high you’ll miss the minimum of the gradient function**, and **if it’s
    too low it will take too long to reach the minimum**.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *η* 称为学习率，是一个超参数。我会在未来写一篇详细介绍梯度下降的文章，但简而言之，**如果学习率过高，你会错过梯度函数的最小值**，**如果学习率过低，则需要太长时间才能达到最小值**。
- en: Python implementation
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python 实现
- en: The code underneath is a simple implementation of (Gaussian) Naive Bayes that
    we just went over.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码是我们刚刚讨论过的（高斯）朴素贝叶斯的简单实现。
- en: Underneath is a chart with the data points (color coded to match their respective
    classes) and the decision boundaries generated by the logistic regression.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个图表，包含数据点（按颜色编码以匹配它们各自的类别）和由逻辑回归生成的决策边界。
- en: '![](../Images/36656acf4f0a70eb2da052e9a63a314d.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36656acf4f0a70eb2da052e9a63a314d.png)'
- en: Decision boundary produced by Logistic Regression with Gradient Descent optimisation.
    Image by author.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由梯度下降优化生成的逻辑回归决策边界。作者提供的图像。
- en: Summary
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Logistic **regression** is a **classification model**.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑 **回归** 是一个 **分类模型**。
- en: Logistic regression is a **discriminative classifier**.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归是一个 **判别分类器**。
- en: If we have 2 classes, we use the **logistic sigmoid function to transform our
    linear function into probabilities**.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们有 2 个类别，我们使用 **逻辑 sigmoid 函数将线性函数转换为概率**。
- en: The **softmax function is the generalisation** of the logistic sigmoid function
    **to multiple classes**.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**softmax 函数是逻辑 sigmoid 函数的多类别推广**。'
- en: The **negative log-likelihood in logistic regression can also be referred to
    as the cross-entropy loss function**.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逻辑回归中的负对数似然也被称为交叉熵损失函数**。'
- en: There is **no closed-form solution to logistic regression**, hence we use gradient
    descent.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逻辑回归没有闭式解**，因此我们使用梯度下降。'
- en: The **learning rate and number of iterations are hyperparameters** that you
    will have to tweak.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率和迭代次数是超参数**，你需要调整这些参数。'
