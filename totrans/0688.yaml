- en: 'The Concept of Tasks in Generative AI: The Building Blocks of Intelligent Systems'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/decoding-tasks-in-generative-ai-the-building-blocks-of-intelligent-systems-f677e8e2ee22](https://towardsdatascience.com/decoding-tasks-in-generative-ai-the-building-blocks-of-intelligent-systems-f677e8e2ee22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Generative AI for Large Enterprises: From Governance to Aggregating APIs, part
    1'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://eavanvalkenburg.medium.com/?source=post_page-----f677e8e2ee22--------------------------------)[![Eduard
    van Valkenburg](../Images/c0ab8a94cecc4ce247e345a60e9314f1.png)](https://eavanvalkenburg.medium.com/?source=post_page-----f677e8e2ee22--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f677e8e2ee22--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f677e8e2ee22--------------------------------)
    [Eduard van Valkenburg](https://eavanvalkenburg.medium.com/?source=post_page-----f677e8e2ee22--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f677e8e2ee22--------------------------------)
    ·8 min read·Sep 7, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Welcome, fellow tech enthusiasts and business leaders! I’m thrilled to kick
    off a series of blog posts dedicated to a topic that’s been creating waves in
    the world — Generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: As I delve deep into the fascinating world of enterprise generative AI, I’ll
    be exploring not only high-level concepts like governance, security, and audibility,
    but also offering practical guidance on topics such as aggregating APIs and understanding
    generative AI architectures.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b41403c9cc164c92a040855db7d2004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image created by OpenAI’s DALL-E model with prompt: a writing robot in the
    style of Vermeer'
  prefs: []
  type: TYPE_NORMAL
- en: Whether you’re a seasoned veteran in the AI space or a curious newcomer, this
    series aims to shed light on how large enterprises can harness the power of generative
    AI to drive innovation, efficiency, and value. I’ll be tackling the complex issues,
    breaking down jargon, and providing actionable insights to help you navigate the
    AI landscape with confidence. So, buckle up and join me on this exciting journey
    into the future of business technology. Let’s demystify AI together!
  prefs: []
  type: TYPE_NORMAL
- en: '***Disclaimer****: this article provides an overview of architectural concepts
    that are not specific to Azure, but are at times illustrated using Azure services
    since I am a Solution Architect at Microsoft.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tasks**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our first pit-stop is ‘Tasks’. In the grand scheme of AI, tasks are the small,
    yet mighty, cogs that keep the wheel turning. They are well-defined units of work
    that a Language Model (LLM) can perform for you, acting as the building blocks
    to your broader (AI) system.
  prefs: []
  type: TYPE_NORMAL
- en: When I talk about tasks, I’m referring to actions with specified inputs and
    outputs. Each task execution is independent, meaning that it doesn’t rely on past
    or future executions. It’s a standalone operation that works in its own bubble.
  prefs: []
  type: TYPE_NORMAL
- en: A prompt is built into these tasks to guide the LLM. This is a bit like giving
    a set of instructions to a friend — you need to be clear and concise, so they
    know exactly what to do. Sometimes this might involve adding few-shot examples
    (few-shots are show cases of the input and expected outputs), but there are many
    ways for prompts to be constructed, but that is outside the scope of this post.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the other things to consider are how to check outputs before sending
    it back to the originating user or application and this is where logging, monitoring
    and other regular DevOps processes come into play.
  prefs: []
  type: TYPE_NORMAL
- en: But for Generative AI there are also other techniques to consider. Often this
    comes in the form of a wrapper around the service you’re building that catches
    different content depending on specific filters, for instance using [Azure AI
    Content Safety](https://azure.microsoft.com/en-us/products/ai-services/ai-content-safety),
    but this could also include wrapping the execution with a human-in-the-loop setup
    that ensures the output is within the bounds, because people check some or all
    generated outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Finally another approach to consider is to use a tool like [Prompt Flow](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/overview-what-is-prompt-flow?view=azureml-api-2)
    to script different variants of input and outputs and continuously test whether
    those still give the expected outcome. This is useful both for evaluating a upgraded
    model (like when GPT-35-Turbo moved from version 0301 to 0613), but it can also
    be used to validate and choose the right model for a particular task, testing
    a task with smaller (and therefore cheaper and faster) models, like Meta’s Llama
    models, and larger models like GPT-35-Turbo and GPT4 or even fine-tuned models,
    and deciding automatically based on the metrics which models this task should
    be using to optimise costs, latency and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Integration & Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The beauty of this all is that it’s exposed through a well-documented API. This
    means that keys, billing, and logging are managed through some kind of API Management
    system, making the process seamless and user-friendly, while providing the power
    of LLM’s to many applications in different ways.
  prefs: []
  type: TYPE_NORMAL
- en: To bring this concept to life, let’s consider a few examples. Imagine you need
    to export a sales order from a text, or summarise a text with domain-specific
    highlights. These are tasks that a LLM can do for you, effectively and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 1: writing a job posting based on a list of requirements and existing
    job postings'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This task is a solution created by a HR department but potentially used by managers
    throughout a company, preferably embedded within the application that powers the
    job posting or career website (and from there integrates towards job boards like
    LinkedIn).
  prefs: []
  type: TYPE_NORMAL
- en: The user input for a task like this is the job title, responsibilities and qualifications,
    it also includes things like the department and levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this input the task loads relevant comparable postings to serve as examples.
    It might have language in the prompt like: “always end with this text: …”, which
    serves as a way for HR to enforce mandated language across all job postings.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally it does the actual call to a LLM to generate the new texts and returns
    that to the user. At this stage, the output might be checked for certain terms
    or texts to make sure that hallucinations of the model do not get in the way.
    There might also be some post-processing happening, for instance by default translating
    the text in multiple languages.
  prefs: []
  type: TYPE_NORMAL
- en: The returned result is then given to the requestor to validate and approve after
    making any changes or tweaks to it, the before mentioned checks and post-processing
    might also trigger after this, so that the user does not delete the mandatory
    wording and translation is done on the finished product.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72483e18cf9bde626b86d75cd6c05f22.png)'
  prefs: []
  type: TYPE_IMG
- en: Schematic of the flows for Example 1 — image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 2: parsing incoming orders from email or phone to JSON'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This second example is focused on automating tasks that are difficult to program.
    In this case sales orders for, let’s say a retailer, which come in by phone or
    by email.
  prefs: []
  type: TYPE_NORMAL
- en: In order to get the “technical” description of the sales order from those free-text
    formats, in most cases, manual labor is the only way to do it, even though you
    might be able to cobble together something with complex regexes and lots of code,
    but this kind of work is where LLM’s really excel in dealing with the human-made
    “mess”.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of a phone message, the first step is to transcribe that and there
    are many API’s and models that can do this, so I won’t go into detail here. The
    output is treated the same as the email.
  prefs: []
  type: TYPE_NORMAL
- en: 'The text of the email or transcription is sent to a API endpoint for this task,
    that task has a prompt which has instructions in it, like “from the text below
    generate a json-object, summarising the order with the following fields: customer_name,
    product, SKU, amount, etc…”, for some fields that might include allowed values,
    like brand or product names, and for other fields default values might be specified,
    all this is part of the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, the prompt could look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output when using a LLM for the input: *Good morning, I work for AW Bar
    on 1st Avenue South, Seattle, WA, I would like to order 5 crates of Contoso sparkling,
    2 cartons of AW soda, and 4 bottles of AdventureWorks.* … could look something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, post-processing is done on that JSON object to validate those values,
    checking the brand or product names, the SKU’s, the amounts ordered, etc. If there
    are mistakes a second prompt is used asking the LLM to correct the response for
    that specific field, and if that still fails validation the whole task is put
    on some kind of queue for someone in the sales order processing department to
    manually fix what was missing or to reach out to the customer to validate the
    order, this could then also be used to provide feedback to the task developer
    so that they can catch new misses over time.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the order, if correct, is entered into the order processing system,
    and depending on the situation, either get’s processed or the customer is asked
    to sign off on it before moving forward (and this can also be order dependent,
    for instance if the total order value is over a certain threshold that the customer
    is asked to confirm, and that threshold can be made dynamic by comparing to previous
    orders of this customer).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff7ef8a052bdf94523a9b16142e03594.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image created by OpenAI’s DALL-E model with prompt: a writing robot creating
    a sales order in the style of Vermeer'
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, wrapping prompts + LLM’s in a RESTful API to power specific tasks
    is a very useful pattern to make sure that different teams and applications can
    reuse smart thinking by other people and teams of the organisation without having
    to redevelop all the logic, it also allows an enterprise to make sure that certain
    standards are met, whether that is in the output, like in the first example, or
    in the structure and field names, like in the second example. By making these
    API’s available and adding things like a API aggregation layer over this, this
    can power enterprises to use and leverage LLM’s in scalable, predictable and valuable
    ways.
  prefs: []
  type: TYPE_NORMAL
- en: So, that wraps up the first blog post in this series about the different ways
    generative AI can be used across an enterprise. Some of the other topics that
    I plan to visit are chatbots, governance, and finally I will propose a model for
    how to combine all these things and make it work across larger enterprises.
  prefs: []
  type: TYPE_NORMAL
- en: Stay tuned for my next post, where I’ll dive deeper into the world of enterprise
    generative AI. Until then, happy tasking! As the next posts are created and published
    I will update this one with the links.
  prefs: []
  type: TYPE_NORMAL
