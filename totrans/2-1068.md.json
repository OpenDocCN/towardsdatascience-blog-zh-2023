["```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\ntorch.manual_seed(0)\n\nclass PolicyNetwork(nn.Module):\n    def __init__(self, n_in=1, n_out=3):\n        super(PolicyNetwork, self).__init__()\n        self.fc = nn.Linear(n_in, n_out)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.fc(x)\n        return self.softmax(x)\n```", "```py\npolicy = PolicyNetwork()\noptimizer = optim.Adam(policy.parameters(), lr=0.01)\n\nstate = torch.tensor([[1.]])\n\npi = policy(state)\nprint(\"Before: \", pi)\n\nadvantages = torch.tensor([[-2., 0., 0.]], requires_grad=True)\nloss = -torch.sum(pi * advantages)\n\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n\npi = policy(state)\nprint(\"After: \", pi)\n```", "```py\nepsilon = 0.2\n\nold_policy = PolicyNetwork()\nold_policy.load_state_dict(policy.state_dict())\n\npi_old = old_policy(state)\nr = pi / pi_old\n\nadvantages = torch.tensor([[0., 1.8, 0.]], requires_grad=True)\n\nclipped_ratio = torch.clamp(r, 1 - epsilon, 1 + epsilon)\nppo_objective = torch.min(r * advantages, clipped_ratio * advantages)\nloss = -torch.sum(ppo_objective)\n\nprint(\"Probabilities before update:\", policy(state))\n\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n\nprint(\"Probabilities after update:\", policy(state))\n```", "```py\nnum_iterations = 500\nadvatange_dict = {\n    0: [-2., 0., 0.],\n    1: [0., 1.8, 0.],\n    2: [0., 0., 0.1],\n}\nnp.random.seed(2023)\n\nfor i in range(num_iterations):\n    pi = policy(state)\n    pi_old = old_policy(state)\n    r = pi / pi_old\n    old_policy.load_state_dict(policy.state_dict())\n\n    noisy_advantange = [\n        x+np.random.randn() for x in advatange_dict[i%3] if x != 0\n    ]\n    advantage = torch.tensor(noisy_advantange, requires_grad=True)\n    ppo_objective = torch.min(\n        r * advantage, \n        torch.clamp(r, 1 - epsilon, 1 + epsilon) * advantage\n    )\n    loss = -torch.sum(ppo_objective)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint(\"After: \", pi)\n```"]