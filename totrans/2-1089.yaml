- en: How Meta’s AI Generates Music Based on a Reference Melody
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Meta的人工智能如何基于参考旋律生成音乐
- en: 原文：[https://towardsdatascience.com/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783](https://towardsdatascience.com/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783](https://towardsdatascience.com/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783)
- en: MusicGen, analyzed
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MusicGen分析
- en: '[](https://medium.com/@maxhilsdorf?source=post_page-----de34acd783--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page-----de34acd783--------------------------------)[](https://towardsdatascience.com/?source=post_page-----de34acd783--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----de34acd783--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page-----de34acd783--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maxhilsdorf?source=post_page-----de34acd783--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page-----de34acd783--------------------------------)[](https://towardsdatascience.com/?source=post_page-----de34acd783--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----de34acd783--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page-----de34acd783--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----de34acd783--------------------------------)
    ·10 min read·Jun 23, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----de34acd783--------------------------------)
    ·10分钟阅读·2023年6月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c46f4b570f4f5b87a647a152468cb6be.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c46f4b570f4f5b87a647a152468cb6be.png)'
- en: Image by author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者。
- en: MusicGen by Meta
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Meta的MusicGen
- en: On June 13th, 2023, Meta (formerly Facebook) made waves in the music and AI
    communities with the release of their generative music model, MusicGen. This model
    not only surpasses Google’s MusicLM, which was launched earlier this year, in
    terms of capabilities but is also trained on licensed music data and open-sourced
    for non-commercial use.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年6月13日，Meta（前身为Facebook）在音乐和人工智能界掀起了波澜，发布了他们的生成音乐模型MusicGen。这个模型不仅在能力上超越了今年早些时候发布的Google
    MusicLM，还使用了授权音乐数据进行训练，并且对非商业用途开源。
- en: This means that you can not only read the [research paper](https://arxiv.org/abs/2306.05284)
    or listen to [demos](https://ai.honu.io/papers/musicgen/) but also copy their
    code from [GitHub](https://github.com/facebookresearch/audiocraft) or experiment
    with the model in a web app on [HuggingFace](https://huggingface.co/spaces/facebook/MusicGen).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着你不仅可以阅读[研究论文](https://arxiv.org/abs/2306.05284)或听取[演示](https://ai.honu.io/papers/musicgen/)，还可以从[GitHub](https://github.com/facebookresearch/audiocraft)复制他们的代码，或者在[HuggingFace](https://huggingface.co/spaces/facebook/MusicGen)上的网页应用中试验模型。
- en: In addition to generating audio from a text prompt, MusicGen can also generate
    music based on a given reference melody, a feature known as melody conditioning.
    In this blog post, I will demonstrate how Meta implemented this useful and fascinating
    functionality into their model. But before we delve into that, let’s first understand
    how melody conditioning works in practice.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 除了从文本提示生成音频外，MusicGen还可以基于给定的参考旋律生成音乐，这一功能被称为旋律条件化。在这篇博客文章中，我将演示Meta如何将这个有用且迷人的功能实现到他们的模型中。但在我们深入之前，让我们先了解旋律条件化在实际中是如何工作的。
- en: Showcase
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 展示
- en: Base Track
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础曲目
- en: The following is a short electronic music snippet that I produced for this article.
    It features electronic drums, a dominant 808 bass and two syncopated synths. When
    listening to it, try to identify the “main melody” of the track.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我为这篇文章制作的一段短电子音乐片段。它包含电子鼓、突出的808低音和两个切分合成器。在听的时候，试着识别出这段音乐的“主要旋律”。
- en: Using MusicGen, I can now generate music in other genres that stick to the same
    main melody. All I need for that is my base track and a text prompt describing
    how the new piece should sound.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MusicGen，我现在可以在其他风格中生成与相同主要旋律相符的音乐。我所需的只是我的基础曲目和描述新作品声音的文本提示。
- en: Orchestral Variant
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管弦乐变奏
- en: A grand orchestral arrangement with thunderous percussion, epic brass fanfares,
    and soaring strings, creating a cinematic atmosphere fit for a heroic battle.
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一段宏伟的管弦乐编曲，配有震撼的打击乐、史诗般的铜管号角和高亢的弦乐，营造出适合英雄战斗的电影氛围。
- en: Reggae Variant
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 雷鬼变奏
- en: classic reggae track with an electronic guitar solo
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 经典雷鬼曲目加电子吉他独奏
- en: Jazz Variant
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 爵士变奏
- en: smooth jazz, with a saxophone solo, piano chords, and snare full drums
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 流行爵士乐，包含萨克斯风独奏、钢琴和弦，以及完整的军鼓
- en: How Good are the Results?
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果有多好？
- en: Although MusicGen doesn’t adhere closely to my text prompts and creates music
    that is slightly different from what I asked for, the generated pieces still accurately
    reflect the requested genre and, more importantly, each piece showcases its own
    interpretation of the main melody from the base track.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 MusicGen 并没有严格遵循我的文本提示，并创作出略微不同于我要求的音乐，但生成的作品仍然准确反映了所请求的音乐类型，更重要的是，每一首作品都展示了其对基础曲目主旋律的独特解读。
- en: While the results are not perfect, I find the capabilities of this model to
    be quite impressive. The fact that MusicGen has been one of the most popular models
    on HuggingFace ever since its release further emphasizes its significance. With
    that said, let’s delve deeper into the technical aspects of how melody conditioning
    works.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然结果并不完美，但我发现这个模型的能力相当令人印象深刻。自发布以来，MusicGen 一直是 HuggingFace 上最受欢迎的模型之一，这进一步强调了它的重要性。话虽如此，让我们更深入地探讨旋律条件化工作的技术方面。
- en: How Text-to-Music Models Are Trained
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本到音乐模型的训练方式
- en: '![](../Images/aaef8f27c95e8c387eb3636fcd546e9b.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aaef8f27c95e8c387eb3636fcd546e9b.png)'
- en: Three text-music pairs as they are used for training models like MusicLM or
    MusicGen. Image by author.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 三个文本-音乐对，作为训练模型如 MusicLM 或 MusicGen 的用途。图片由作者提供。
- en: Almost all current generative music models follow the same procedure during
    training. They are provided with a large database of music tracks accompanied
    by corresponding text descriptions. The model learns the relationship between
    words and sounds, as well as how to convert a given text prompt into a coherent
    and enjoyable piece of music. During the training process, the model optimizes
    its own compositions by comparing them to the real music tracks in the dataset.
    This enables the model to identify its strengths and areas that require improvement.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有当前的生成音乐模型在训练期间都遵循相同的程序。它们配备了一个包含大量音乐曲目及其对应文本描述的数据库。模型学习单词和声音之间的关系，以及如何将给定的文本提示转换为连贯且愉悦的音乐作品。在训练过程中，模型通过将其自身创作与数据集中的真实音乐曲目进行比较来优化自身创作。这使得模型能够识别自身的优点和需要改进的地方。
- en: The issue lies in the fact that once a machine learning model is trained for
    a specific task, such as text-to-music generation, it is limited to that particular
    task. While it is possible to make MusicGen perform certain tasks that it was
    not explicitly trained for, like continuing a given piece of music, it cannot
    be expected to tackle every music generation request. For instance, it cannot
    simply take a melody and transform it into a different genre. This would be like
    throwing potatoes into a toaster and expecting fries to come out. Instead, a separate
    model must be trained to implement this functionality.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，一旦机器学习模型针对特定任务进行训练，比如文本到音乐生成，它就局限于那个特定任务。虽然可以让 MusicGen 执行一些它没有明确训练过的任务，比如延续给定的音乐片段，但不能期望它能处理所有的音乐生成请求。例如，它不能简单地将一段旋律转变为不同的音乐风格。这就像把土豆扔进烤面包机期待出来的是薯条一样。相反，必须训练一个单独的模型来实现这种功能。
- en: A Simple Tweak to the Training Recipe
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对训练配方的简单调整
- en: 'Let’s explore how Meta adapted the model training procedure to enable MusicGen
    to generate variations of a given melody based on a text prompt. However, there
    are several challenges associated with this approach. One of the primary obstacles
    is the ambiguity in identifying “the melody” of a song and representing it in
    a computationally meaningful way. Nonetheless, for the purpose of understanding
    the new training procedure at a broader level, let’s assume a consensus on what
    constitutes “the melody” and how it can be easily extracted and fed into the model.
    In this scenario, the adjusted training method can be outlined as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨一下 Meta 如何调整模型训练过程，以使 MusicGen 能够根据文本提示生成给定旋律的变体。然而，这种方法存在几个挑战。主要障碍之一是识别歌曲的“旋律”并以计算上有意义的方式表示它的模糊性。尽管如此，为了从更广泛的层面理解新的训练过程，假设我们对什么构成“旋律”以及如何轻松提取并输入到模型中的达成共识。在这种情况下，调整后的训练方法可以概述如下：
- en: '![](../Images/c2ff451f59be81accfe76a431e7f968f.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2ff451f59be81accfe76a431e7f968f.png)'
- en: Three text-music-melody pairs as they were used for teaching MusicGen melody-conditioned
    generation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 三个文本-音乐-旋律对，作为教学 MusicGen 旋律条件生成的用途。
- en: For each track in the database, the first step is to extract its melody. Subsequently,
    the model is fed with both the track’s text description and its corresponding
    melody, prompting the model to recreate the original track. Essentially, this
    approach simplifies the original training objective, where the model was solely
    tasked with recreating the track based on text.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据库中的每个曲目，第一步是提取其旋律。随后，模型会接收曲目的文本描述和相应的旋律，促使模型重建原始曲目。本质上，这种方法简化了原始的训练目标，即模型仅仅被要求根据文本重建曲目。
- en: To understand why we do this, let’s ask ourselves what the AI model learns in
    this training procedure. In essence, it learns how a melody can be turned into
    a full piece of music based on a text description. This means that after the training,
    we can provide the model with a melody and request it to compose a piece of music
    with any genre, mood, or instrumentation. To the model, this is the same “semi-blind”
    generation task it has successfully accomplished countless times during training.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解我们这样做的原因，让我们问问自己AI模型在这个训练过程中学到了什么。从本质上讲，它学会了如何将旋律转化为基于文本描述的完整音乐作品。这意味着在训练结束后，我们可以提供旋律给模型，并要求它创作任何风格、情绪或乐器的音乐作品。对模型来说，这与它在训练过程中成功完成过无数次的“半盲”生成任务是一样的。
- en: Having grasped the technique employed by Meta to teach the model melody-conditioned
    music generation, we still need to tackle the challenge of precisely defining
    what constitutes “the melody.”
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 了解了Meta用来教模型旋律条件音乐生成的技术后，我们仍然需要解决如何精确定义“旋律”的挑战。
- en: What is “The Melody”?
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是“旋律”？
- en: The truth is, there is no objective method to determine or extract “the melody”
    of a polyphonic musical piece, except when all instruments are playing in unison.
    While there is often a prominent instrument such as a voice, guitar, or violin,
    it does not necessarily imply that the other instruments are not part of “the
    melody.” Take Queen’s “Bohemian Rhapsody” as an example. When you think of the
    song, you might first recall Freddie Mercury’s main vocal melodies. However, does
    that mean the piano in the intro, the background singers in the middle section,
    and the electric guitar before “So you think you can stone me […]” are not part
    of the melody?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，除非所有乐器同时演奏，否则没有客观的方法来确定或提取多声部音乐作品中的“旋律”。虽然通常会有一个突出的乐器，如声乐、吉他或小提琴，但这并不一定意味着其他乐器不属于“旋律”的一部分。以皇后乐队的《波希米亚狂想曲》为例。当你想到这首歌时，你可能首先会回忆起弗雷迪·摩克瑞的主要声乐旋律。然而，这是否意味着前奏中的钢琴、中段的背景歌手和“你认为你能石化我……”之前的电吉他并不属于旋律的一部分？
- en: One method for extracting “the melody” of a song is to consider the most prominent
    melody as the most dominant one, typically identified as the loudest melody in
    the mix. The chromagram is a widely utilized representation that visually displays
    the most dominant musical notes throughout a track. Below, you can find the chromagram
    of the reference track, initially with the complete instrumentation and then excluding
    drums and bass. On the left side, the most relevant notes for the melody (B, F#,
    G) are highlighted in blue.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 提取歌曲的“旋律”的一种方法是将最突出、通常是混音中最响亮的旋律视为最主导的旋律。色谱图是一种广泛使用的表示方法，直观地展示了整个曲目中最主导的音乐音符。下面，你可以找到参考曲目的色谱图，最初包括完整的乐器配置，然后去除鼓和贝斯。在左侧，旋律中最相关的音符（B，F#，G）用蓝色突出显示。
- en: '![](../Images/61f9dfe653cc45855c69396818f30a2c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61f9dfe653cc45855c69396818f30a2c.png)'
- en: Both chromagrams accurately depict the primary melody notes, with the version
    of the track without drums and bass providing a clearer visualization of the melody.
    Meta’s study also revealed the same observation, which led them to utilize their
    source separation tool ([DEMUCS](https://github.com/facebookresearch/demucs))
    to remove any disturbing rhythmic elements from the track. This process results
    in a sufficiently representative rendition of “the melody,” which can then be
    fed to the model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 两个色谱图都准确描绘了主要的旋律音符，其中不含鼓和贝斯的曲目版本提供了更清晰的旋律可视化。Meta的研究也揭示了相同的观察结果，这促使他们使用其源分离工具（[DEMUCS](https://github.com/facebookresearch/demucs)）去除曲目中的干扰节奏元素。这一过程得出的“旋律”表现足够代表，然后可以输入到模型中。
- en: 'In summary, we can now connect the pieces to understand the underlying process
    when requesting MusicGen to perform melody-conditioned generation. Here is a visual
    representation of the workflow:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们现在可以将这些信息连接起来，以理解请求 MusicGen 执行旋律条件生成时的基本过程。以下是工作流程的可视化表示：
- en: '![](../Images/d60310a09049230a2fcef54f043799b4.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d60310a09049230a2fcef54f043799b4.png)'
- en: How MusicGen produces a melody-conditioned music output. Image by author.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: MusicGen 如何生成旋律条件音乐输出。图片由作者提供。
- en: Limitations
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 局限性
- en: '![](../Images/668f4fbadf28ea7be86fa0e016e86bb4.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/668f4fbadf28ea7be86fa0e016e86bb4.png)'
- en: Photo by [Xavier von Erlach](https://unsplash.com/@xavier_von_erlach?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[Xavier von Erlach](https://unsplash.com/@xavier_von_erlach?utm_source=medium&utm_medium=referral)
    拍摄的照片，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
- en: While MusicGen shows promising advancements in melody-conditioning, it is important
    to acknowledge that the technology is still a work-in-progress. Chromagrams, even
    when drums and bass are removed, offer an imperfect representation of a track’s
    melody. One limitation is that chromagrams categorize all notes into the 12 western
    pitch classes, meaning they capture the transition between two pitch classes but
    not the direction (up or down) of the melody.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 MusicGen 在旋律条件生成方面展现了有希望的进展，但仍需认识到技术仍在不断完善中。即使去掉鼓和贝斯，色度图谱（chromagrams）也无法完美地表示乐曲的旋律。一个局限性是色度图谱将所有音符分类到
    12 个西方音高类别中，这意味着它们能捕捉到两个音高类别之间的过渡，但不能显示旋律的方向（上升或下降）。
- en: For instance, the melodic interval between moving from C4 to G4 (a perfect fifth)
    differs significantly from moving from C4 to G3 (a perfect fourth). However, in
    a chromagram, both intervals would appear the same. The issue worsens with octave
    jumps, as the chromagram would indicate the melody stayed on the same note. Consider
    how a chromagram would misinterpret the emotional octave jump performed by Céline
    Dion in “My Heart Will Go On” during the line “wher-e-ver you are” as a stable
    melodic movement. To demonstrate this, just look at the chromagram for the chorus
    in A-ha’s “Take on Me”, below. Does this reflect your idea of the song’s melody?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，从 C4 移动到 G4（一个纯五度）之间的旋律间隔与从 C4 移动到 G3（一个纯四度）之间的差异显著。然而，在色度图谱中，这两个间隔会显得相同。问题在于八度跳跃会加剧，因为色度图谱会显示旋律停留在相同音符上。考虑一下色度图谱如何误解
    Céline Dion 在“我心永恒”（“My Heart Will Go On”）中的“wher-e-ver you are”这一情感丰富的八度跳跃为稳定的旋律移动。为此，只需查看下面
    A-ha 的“Take on Me”合唱部分的色度图谱。这是否反映了你对歌曲旋律的想法？
- en: '![](../Images/0d0811f82da144bb314f95daf6ccc889.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d0811f82da144bb314f95daf6ccc889.png)'
- en: A chromagram of the chorus in “Take on Me” (A-ha), bass and drums removed. Image
    by author.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: “Take on Me”（A-ha）的合唱部分的色度图谱，去掉了贝斯和鼓。图片由作者提供。
- en: Another challenge is the inherent bias of the chromagram. It performs well in
    capturing the melody of some songs while completely missing the mark in others.
    This bias is systematic rather than random. Songs with dominant melodies, minimal
    interval jumps, and unison playing are better represented by the chromagram compared
    to songs with complex melodies spread across multiple instruments and featuring
    large interval jumps.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个挑战是色度图谱的固有偏差。它在捕捉某些歌曲的旋律时表现良好，而在其他歌曲中则完全失误。这种偏差是系统性的，而非随机的。色度图谱更能准确地表示具有主导旋律、最小音程跳跃和统一演奏的歌曲，相比之下，对于复杂旋律分布在多个乐器上并具有大音程跳跃的歌曲则表现不佳。
- en: Furthermore, the limitations of the generative AI model itself are worth noting.
    The output audio still exhibits noticeable differences from human-made music,
    and maintaining a consistent style over a six-second interval remains a struggle.
    Moreover, MusicGen falls short in faithfully capturing the more intricate aspects
    of the text prompt, as evidenced by the examples provided earlier. It will require
    further technological advancements for melody-conditioned generation to reach
    a level where it can be used not only for amusement and inspiration but also for
    generating end-user-friendly music.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，生成 AI 模型本身的局限性也值得注意。输出的音频与人工制作的音乐仍有明显差异，并且在六秒的时间间隔内保持一致风格仍然是一个挑战。此外，MusicGen
    在忠实捕捉文本提示的复杂方面上存在不足，之前提供的示例便证明了这一点。旋律条件生成要达到既能用于娱乐和灵感激发，也能生成适合最终用户的音乐的水平，还需要进一步的技术进步。
- en: Future Perspectives
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未来展望
- en: '![](../Images/a340ae4f2737c60c6d2ecd4b7095b9b5.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a340ae4f2737c60c6d2ecd4b7095b9b5.png)'
- en: Photo by [Marc Sendra Martorell](https://unsplash.com/pt-br/@marcsm?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Marc Sendra Martorell](https://unsplash.com/pt-br/@marcsm?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: How can we improve the AI?
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们如何改进AI？
- en: From my perspective, one of the primary concerns that future research should
    address regarding melody-conditioned music generation is the extraction and representation
    of “the melody” from a track. While the chromagram is a well-established and straightforward
    signal processing method, there are numerous newer and experimental approaches
    that utilize deep learning for this purpose. It would be exciting to witness companies
    like Meta drawing inspiration from these advancements, many of which are covered
    in a comprehensive 72-page review by [Reddy et al. (2022)](https://arxiv.org/pdf/2202.01078.pdf).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从我的角度来看，未来研究应主要关注的一个问题是如何从曲目中提取和表征“旋律”。虽然色谱图是一种成熟且直接的信号处理方法，但也有许多较新的实验性方法使用深度学习来实现这一目标。看到像Meta这样的公司从这些进展中获得灵感将是令人兴奋的，这些进展中的许多内容都在[Reddy
    et al. (2022)](https://arxiv.org/pdf/2202.01078.pdf)的一份全面的72页综述中进行了介绍。
- en: Regarding the quality of the model itself, both the audio quality and the comprehension
    of text inputs can be enhanced through scaling up the size of the model and training
    data, as well as the development of more efficient algorithms for this specific
    task. In my opinion, the release of MusicLM in January 2023 resembles a “GPT-2
    moment.” We are beginning to witness the capabilities of these models, but significant
    improvements are still needed across various aspects. If this analogy holds true,
    we can anticipate the release of a music generation model akin to GPT-3 sooner
    than we might expect.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 关于模型本身的质量，无论是音频质量还是对文本输入的理解，都可以通过扩大模型和训练数据的规模以及开发更高效的算法来提高。依我看来，2023年1月发布的MusicLM类似于“GPT-2时刻”。我们开始见证这些模型的能力，但在各个方面仍需显著改进。如果这个类比成立，我们可以预期，类似GPT-3的音乐生成模型的发布可能会比我们预期的要早。
- en: How does this impact musicians?
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这对音乐家有何影响？
- en: As is often the case with generative music AI, concerns arise regarding the
    potential negative impact on the work and livelihoods of music creators. I expect
    that in the future, it will become increasingly challenging to earn a living by
    creating variations of existing melodies. This is particularly evident in scenarios
    such as jingle production, where companies can effortlessly generate numerous
    variations of a characteristic jingle melody at minimal cost for new ad campaigns
    or personalized advertisements. Undoubtedly, this poses a threat to musicians
    who rely on such activities as a significant source of income. I reiterate my
    plea for creatives involved in producing music valued for its objective musical
    qualities rather than subjective, human qualities (such as stock music or jingles)
    to explore alternative income sources to prepare for the future.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 与生成音乐AI经常出现的情况一样，关于其对音乐创作者工作的潜在负面影响也引发了关注。我预计，未来通过创造现有旋律的变体来谋生将变得越来越困难。这在铃声制作等场景中特别明显，企业可以轻松地以最低成本为新的广告活动或个性化广告生成大量变化的铃声旋律。这无疑对依靠这些活动作为主要收入来源的音乐家构成了威胁。我重申我的呼吁，希望参与创作那些因其客观音乐特质而非主观人类特质（如库存音乐或铃声）的音乐创作者探索其他收入来源，为未来做准备。
- en: On the positive side, melody-conditioned music generation presents an incredible
    tool for enhancing human creativity. If someone develops a captivating and memorable
    melody, they can quickly generate examples of how it might sound in various genres.
    This process can help identify the ideal genre and style to bring the music to
    life. Moreover, it offers an opportunity to revisit past projects within one’s
    music catalogue, exploring their potential when translated into different genres
    or styles. Finally, this technology lowers the entry barrier for creatively inclined
    individuals without formal musical training to enter the field. Anyone can now
    come up with a melody, hum it into a smartphone microphone, and share remarkable
    arrangements of their ideas with friends, family, or even attempt to reach a wider
    audience.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从积极的一面来看，旋律条件的音乐生成提供了一种令人难以置信的工具，能够提升人类的创造力。如果有人开发出引人入胜且令人难忘的旋律，他们可以快速生成其在各种风格中的可能效果。这一过程有助于确定理想的音乐风格和类型，以使音乐栩栩如生。此外，它提供了一个机会，可以重新审视个人音乐目录中的过去项目，探索将其转化为不同风格或类型时的潜力。最后，这项技术降低了没有正式音乐培训的创造性人士进入这一领域的门槛。现在，任何人都可以想出一个旋律，将其哼唱到智能手机麦克风中，并与朋友、家人分享其杰出的编排，甚至尝试触及更广泛的受众。
- en: The question of whether AI music generation is beneficial to our societies remains
    open for debate. However, I firmly believe that melody-conditioned music generation
    is one of the use cases of this technology that genuinely enhances the work of
    both professional and aspiring creatives. It adds value by offering new avenues
    for exploration. I am eagerly looking forward to witnessing further advancements
    in this field in the near future.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: AI音乐生成是否对我们的社会有益的问题仍然有待讨论。然而，我坚信旋律条件的音乐生成是这一技术真正提升专业人士和有志创作者工作的一种应用场景。它通过提供新的探索途径来增加价值。我热切期待在不久的将来见证这一领域的进一步进展。
- en: 'If you are fascinated by the intersection of music and AI, you will also like
    some of my other articles about this topic:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对音乐与AI的交汇点感兴趣，你可能也会喜欢我关于这一主题的其他一些文章：
- en: '[How Google Used Fake Datasets to Train Generative Music AI](/how-google-used-fake-datasets-to-train-generative-music-ai-def6f3f71f19)'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[谷歌如何使用虚假数据集来训练生成音乐AI](/how-google-used-fake-datasets-to-train-generative-music-ai-def6f3f71f19)'
- en: '[Chatbots are About to Disrupt Music Search](/chatbots-are-about-to-disrupt-music-search-1e4a4cd7ba01)'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[聊天机器人即将颠覆音乐搜索](/chatbots-are-about-to-disrupt-music-search-1e4a4cd7ba01)'
- en: '[MusicLM — Has Google Solved AI Music Generation?](https://medium.com/towards-data-science/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c)'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[MusicLM — 谷歌是否解决了AI音乐生成问题？](https://medium.com/towards-data-science/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c)'
