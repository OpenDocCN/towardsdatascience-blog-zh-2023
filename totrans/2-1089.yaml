- en: How Meta’s AI Generates Music Based on a Reference Melody
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783](https://towardsdatascience.com/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: MusicGen, analyzed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@maxhilsdorf?source=post_page-----de34acd783--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page-----de34acd783--------------------------------)[](https://towardsdatascience.com/?source=post_page-----de34acd783--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----de34acd783--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page-----de34acd783--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----de34acd783--------------------------------)
    ·10 min read·Jun 23, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c46f4b570f4f5b87a647a152468cb6be.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: MusicGen by Meta
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On June 13th, 2023, Meta (formerly Facebook) made waves in the music and AI
    communities with the release of their generative music model, MusicGen. This model
    not only surpasses Google’s MusicLM, which was launched earlier this year, in
    terms of capabilities but is also trained on licensed music data and open-sourced
    for non-commercial use.
  prefs: []
  type: TYPE_NORMAL
- en: This means that you can not only read the [research paper](https://arxiv.org/abs/2306.05284)
    or listen to [demos](https://ai.honu.io/papers/musicgen/) but also copy their
    code from [GitHub](https://github.com/facebookresearch/audiocraft) or experiment
    with the model in a web app on [HuggingFace](https://huggingface.co/spaces/facebook/MusicGen).
  prefs: []
  type: TYPE_NORMAL
- en: In addition to generating audio from a text prompt, MusicGen can also generate
    music based on a given reference melody, a feature known as melody conditioning.
    In this blog post, I will demonstrate how Meta implemented this useful and fascinating
    functionality into their model. But before we delve into that, let’s first understand
    how melody conditioning works in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Showcase
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Base Track
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following is a short electronic music snippet that I produced for this article.
    It features electronic drums, a dominant 808 bass and two syncopated synths. When
    listening to it, try to identify the “main melody” of the track.
  prefs: []
  type: TYPE_NORMAL
- en: Using MusicGen, I can now generate music in other genres that stick to the same
    main melody. All I need for that is my base track and a text prompt describing
    how the new piece should sound.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestral Variant
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A grand orchestral arrangement with thunderous percussion, epic brass fanfares,
    and soaring strings, creating a cinematic atmosphere fit for a heroic battle.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Reggae Variant
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: classic reggae track with an electronic guitar solo
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jazz Variant
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: smooth jazz, with a saxophone solo, piano chords, and snare full drums
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How Good are the Results?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although MusicGen doesn’t adhere closely to my text prompts and creates music
    that is slightly different from what I asked for, the generated pieces still accurately
    reflect the requested genre and, more importantly, each piece showcases its own
    interpretation of the main melody from the base track.
  prefs: []
  type: TYPE_NORMAL
- en: While the results are not perfect, I find the capabilities of this model to
    be quite impressive. The fact that MusicGen has been one of the most popular models
    on HuggingFace ever since its release further emphasizes its significance. With
    that said, let’s delve deeper into the technical aspects of how melody conditioning
    works.
  prefs: []
  type: TYPE_NORMAL
- en: How Text-to-Music Models Are Trained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/aaef8f27c95e8c387eb3636fcd546e9b.png)'
  prefs: []
  type: TYPE_IMG
- en: Three text-music pairs as they are used for training models like MusicLM or
    MusicGen. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Almost all current generative music models follow the same procedure during
    training. They are provided with a large database of music tracks accompanied
    by corresponding text descriptions. The model learns the relationship between
    words and sounds, as well as how to convert a given text prompt into a coherent
    and enjoyable piece of music. During the training process, the model optimizes
    its own compositions by comparing them to the real music tracks in the dataset.
    This enables the model to identify its strengths and areas that require improvement.
  prefs: []
  type: TYPE_NORMAL
- en: The issue lies in the fact that once a machine learning model is trained for
    a specific task, such as text-to-music generation, it is limited to that particular
    task. While it is possible to make MusicGen perform certain tasks that it was
    not explicitly trained for, like continuing a given piece of music, it cannot
    be expected to tackle every music generation request. For instance, it cannot
    simply take a melody and transform it into a different genre. This would be like
    throwing potatoes into a toaster and expecting fries to come out. Instead, a separate
    model must be trained to implement this functionality.
  prefs: []
  type: TYPE_NORMAL
- en: A Simple Tweak to the Training Recipe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s explore how Meta adapted the model training procedure to enable MusicGen
    to generate variations of a given melody based on a text prompt. However, there
    are several challenges associated with this approach. One of the primary obstacles
    is the ambiguity in identifying “the melody” of a song and representing it in
    a computationally meaningful way. Nonetheless, for the purpose of understanding
    the new training procedure at a broader level, let’s assume a consensus on what
    constitutes “the melody” and how it can be easily extracted and fed into the model.
    In this scenario, the adjusted training method can be outlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c2ff451f59be81accfe76a431e7f968f.png)'
  prefs: []
  type: TYPE_IMG
- en: Three text-music-melody pairs as they were used for teaching MusicGen melody-conditioned
    generation.
  prefs: []
  type: TYPE_NORMAL
- en: For each track in the database, the first step is to extract its melody. Subsequently,
    the model is fed with both the track’s text description and its corresponding
    melody, prompting the model to recreate the original track. Essentially, this
    approach simplifies the original training objective, where the model was solely
    tasked with recreating the track based on text.
  prefs: []
  type: TYPE_NORMAL
- en: To understand why we do this, let’s ask ourselves what the AI model learns in
    this training procedure. In essence, it learns how a melody can be turned into
    a full piece of music based on a text description. This means that after the training,
    we can provide the model with a melody and request it to compose a piece of music
    with any genre, mood, or instrumentation. To the model, this is the same “semi-blind”
    generation task it has successfully accomplished countless times during training.
  prefs: []
  type: TYPE_NORMAL
- en: Having grasped the technique employed by Meta to teach the model melody-conditioned
    music generation, we still need to tackle the challenge of precisely defining
    what constitutes “the melody.”
  prefs: []
  type: TYPE_NORMAL
- en: What is “The Melody”?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The truth is, there is no objective method to determine or extract “the melody”
    of a polyphonic musical piece, except when all instruments are playing in unison.
    While there is often a prominent instrument such as a voice, guitar, or violin,
    it does not necessarily imply that the other instruments are not part of “the
    melody.” Take Queen’s “Bohemian Rhapsody” as an example. When you think of the
    song, you might first recall Freddie Mercury’s main vocal melodies. However, does
    that mean the piano in the intro, the background singers in the middle section,
    and the electric guitar before “So you think you can stone me […]” are not part
    of the melody?
  prefs: []
  type: TYPE_NORMAL
- en: One method for extracting “the melody” of a song is to consider the most prominent
    melody as the most dominant one, typically identified as the loudest melody in
    the mix. The chromagram is a widely utilized representation that visually displays
    the most dominant musical notes throughout a track. Below, you can find the chromagram
    of the reference track, initially with the complete instrumentation and then excluding
    drums and bass. On the left side, the most relevant notes for the melody (B, F#,
    G) are highlighted in blue.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61f9dfe653cc45855c69396818f30a2c.png)'
  prefs: []
  type: TYPE_IMG
- en: Both chromagrams accurately depict the primary melody notes, with the version
    of the track without drums and bass providing a clearer visualization of the melody.
    Meta’s study also revealed the same observation, which led them to utilize their
    source separation tool ([DEMUCS](https://github.com/facebookresearch/demucs))
    to remove any disturbing rhythmic elements from the track. This process results
    in a sufficiently representative rendition of “the melody,” which can then be
    fed to the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, we can now connect the pieces to understand the underlying process
    when requesting MusicGen to perform melody-conditioned generation. Here is a visual
    representation of the workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d60310a09049230a2fcef54f043799b4.png)'
  prefs: []
  type: TYPE_IMG
- en: How MusicGen produces a melody-conditioned music output. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/668f4fbadf28ea7be86fa0e016e86bb4.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Xavier von Erlach](https://unsplash.com/@xavier_von_erlach?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: While MusicGen shows promising advancements in melody-conditioning, it is important
    to acknowledge that the technology is still a work-in-progress. Chromagrams, even
    when drums and bass are removed, offer an imperfect representation of a track’s
    melody. One limitation is that chromagrams categorize all notes into the 12 western
    pitch classes, meaning they capture the transition between two pitch classes but
    not the direction (up or down) of the melody.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the melodic interval between moving from C4 to G4 (a perfect fifth)
    differs significantly from moving from C4 to G3 (a perfect fourth). However, in
    a chromagram, both intervals would appear the same. The issue worsens with octave
    jumps, as the chromagram would indicate the melody stayed on the same note. Consider
    how a chromagram would misinterpret the emotional octave jump performed by Céline
    Dion in “My Heart Will Go On” during the line “wher-e-ver you are” as a stable
    melodic movement. To demonstrate this, just look at the chromagram for the chorus
    in A-ha’s “Take on Me”, below. Does this reflect your idea of the song’s melody?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d0811f82da144bb314f95daf6ccc889.png)'
  prefs: []
  type: TYPE_IMG
- en: A chromagram of the chorus in “Take on Me” (A-ha), bass and drums removed. Image
    by author.
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge is the inherent bias of the chromagram. It performs well in
    capturing the melody of some songs while completely missing the mark in others.
    This bias is systematic rather than random. Songs with dominant melodies, minimal
    interval jumps, and unison playing are better represented by the chromagram compared
    to songs with complex melodies spread across multiple instruments and featuring
    large interval jumps.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the limitations of the generative AI model itself are worth noting.
    The output audio still exhibits noticeable differences from human-made music,
    and maintaining a consistent style over a six-second interval remains a struggle.
    Moreover, MusicGen falls short in faithfully capturing the more intricate aspects
    of the text prompt, as evidenced by the examples provided earlier. It will require
    further technological advancements for melody-conditioned generation to reach
    a level where it can be used not only for amusement and inspiration but also for
    generating end-user-friendly music.
  prefs: []
  type: TYPE_NORMAL
- en: Future Perspectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/a340ae4f2737c60c6d2ecd4b7095b9b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Marc Sendra Martorell](https://unsplash.com/pt-br/@marcsm?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: How can we improve the AI?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From my perspective, one of the primary concerns that future research should
    address regarding melody-conditioned music generation is the extraction and representation
    of “the melody” from a track. While the chromagram is a well-established and straightforward
    signal processing method, there are numerous newer and experimental approaches
    that utilize deep learning for this purpose. It would be exciting to witness companies
    like Meta drawing inspiration from these advancements, many of which are covered
    in a comprehensive 72-page review by [Reddy et al. (2022)](https://arxiv.org/pdf/2202.01078.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the quality of the model itself, both the audio quality and the comprehension
    of text inputs can be enhanced through scaling up the size of the model and training
    data, as well as the development of more efficient algorithms for this specific
    task. In my opinion, the release of MusicLM in January 2023 resembles a “GPT-2
    moment.” We are beginning to witness the capabilities of these models, but significant
    improvements are still needed across various aspects. If this analogy holds true,
    we can anticipate the release of a music generation model akin to GPT-3 sooner
    than we might expect.
  prefs: []
  type: TYPE_NORMAL
- en: How does this impact musicians?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As is often the case with generative music AI, concerns arise regarding the
    potential negative impact on the work and livelihoods of music creators. I expect
    that in the future, it will become increasingly challenging to earn a living by
    creating variations of existing melodies. This is particularly evident in scenarios
    such as jingle production, where companies can effortlessly generate numerous
    variations of a characteristic jingle melody at minimal cost for new ad campaigns
    or personalized advertisements. Undoubtedly, this poses a threat to musicians
    who rely on such activities as a significant source of income. I reiterate my
    plea for creatives involved in producing music valued for its objective musical
    qualities rather than subjective, human qualities (such as stock music or jingles)
    to explore alternative income sources to prepare for the future.
  prefs: []
  type: TYPE_NORMAL
- en: On the positive side, melody-conditioned music generation presents an incredible
    tool for enhancing human creativity. If someone develops a captivating and memorable
    melody, they can quickly generate examples of how it might sound in various genres.
    This process can help identify the ideal genre and style to bring the music to
    life. Moreover, it offers an opportunity to revisit past projects within one’s
    music catalogue, exploring their potential when translated into different genres
    or styles. Finally, this technology lowers the entry barrier for creatively inclined
    individuals without formal musical training to enter the field. Anyone can now
    come up with a melody, hum it into a smartphone microphone, and share remarkable
    arrangements of their ideas with friends, family, or even attempt to reach a wider
    audience.
  prefs: []
  type: TYPE_NORMAL
- en: The question of whether AI music generation is beneficial to our societies remains
    open for debate. However, I firmly believe that melody-conditioned music generation
    is one of the use cases of this technology that genuinely enhances the work of
    both professional and aspiring creatives. It adds value by offering new avenues
    for exploration. I am eagerly looking forward to witnessing further advancements
    in this field in the near future.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are fascinated by the intersection of music and AI, you will also like
    some of my other articles about this topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[How Google Used Fake Datasets to Train Generative Music AI](/how-google-used-fake-datasets-to-train-generative-music-ai-def6f3f71f19)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Chatbots are About to Disrupt Music Search](/chatbots-are-about-to-disrupt-music-search-1e4a4cd7ba01)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[MusicLM — Has Google Solved AI Music Generation?](https://medium.com/towards-data-science/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
