["```py\n# Import libraries\nfrom transformers import pipeline\n\n# Specify the model\nmodel = \"gpt2\"\n\n# Specify the task\ntask = \"text-generation\"\n\n# Instantiate pipeline\ngenerator = pipeline(model = model, task = task, max_new_tokens = 30)\n\n# Specify input text\ninput_text = \"If you are interested in learing more about data science, I can teach you how to\"\n\n# Perform text generation and store the results\noutput = generator(input_text)\n\n# Return the results\noutput\n```", "```py\n# Specify model\nmodel = 'distilbert-base-cased-distilled-squad'\n\n# Instantiate pipeline\nanswerer = pipeline(model = model, task=\"question-answering\")\n\n# Specify question and context\nquestion = \"What does NLP stand for?\"\ncontext = \"Today we are talking about machine learning and specifically the natural language processing, which enables computers to understand, process and generate languages\"\n\n# Generate predictions\npreds = answerer(\n    question = question,\n    context = context,\n)\n\n# Return results\nprint(\n    f\"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n)\n```", "```py\n# Specify model\nmodel = \"deepset/roberta-base-squad2\"\n\n# Specify task\ntask = \"question-answering\"\n\n# Instantiate pipeline\nanswerer = pipeline(task = task, model = model, tokenizer = model)\n\n# Specify input\nqa_input = {\n    'question': 'What does NLP stand for?',\n    'context': 'Today we are talking about machine learning and specifically the natural language processing, which enables computers to understand, process and generate languages'\n}\n\n# Generate predictions\noutput = answerer(qa_input)\n\n# Return results\noutput\n```", "```py\n# Specify pre-trained model to use\nmodel = 'distilbert-base-uncased-finetuned-sst-2-english'\n\n# Specify task\ntask = 'sentiment-analysis'\n\n# Text to be analyzed\ninput_text = 'Performing NLP tasks using HuggingFace pipeline is super easy!'\n\n# Instantiate pipeline\nanalyzer = pipeline(task, model = model)\n\n# Store the output of the analysis\noutput = analyzer(input_text)\n\n# Return output\noutput\n```", "```py\n# Specify model\nmodel = 'facebook/bart-large-mnli'\n\n# Specify Task\ntask = 'zero-shot-classification'\n\n# Specify input text\ninput_text = 'This is a tutorial about using pre-trained models through HuggingFace'\n\n# Identify the classes/categories/labels\nlabels = ['business', 'sports', 'education', 'politics', 'music']\n\n# Instantiate pipeline\nclassifier = pipeline(task, model = model)\n\n# Store the output of the analysis\noutput = classifier(input_text, candidate_labels = labels)\n\n# Return output\noutput\n```", "```py\n# Specify model and tokenizer\nmodel = \"t5-base\"\ntokenizer = \"t5-base\"\n\n# Specify task\ntask = \"summarization\"\n\n# Specify input text\ninput_text = \"Text summarization is the task of automatically summarizing textual input, while still conveying the main points and gist of the incoming text. One example of the business intuition behind the need for such summarization models is the situations where humans read incoming text communications (e.g. customer emails) and using a summarization model can save human time. \"\n\n# Instantiate pipeline\nsummarizer = pipeline(task = task, model = model, tokenizer = tokenizer, framework = \"tf\")\n\n# Summarize and store results\noutput = summarizer(input_text)\n\n# Return output\noutput\n```", "```py\n# Specify model\nmodel = 'google/pegasus-cnn_dailymail'\n\n# Specify task\ntask = 'summarization'\n\n# Specify inpute text\ninput_text = \"Text summarization is the task of automatically summarizing textual input, while still conveying the main points and gist of the incoming text. One example of the business intuition behind the need for such summarization models is the situations where humans read incoming text communications (e.g. customer emails) and using a summarization model can save human time. \"\n\n# Instantiate pipeline\nsummarizer = pipeline(task = task, model = model)\n\n# Summarize and store results\noutput = summarizer(input_text, max_length = 75, min_length = 25)\n\n# Return output\noutput\n```", "```py\n# Specify prefix\noriginal_language = 'English'\ntarget_language = 'French'\nprefix = f\"translate {original_language} to {target_language}: \"\n\n# Specify input text\ninput_text = f\"{prefix}This is a post on Medium about various NLP tasks using Hugging Face.\"\n\n# Specify model\nmodel = \"t5-base\"\n\n# Specify task\ntask = \"translation\"\n\n# Instantiate pipeline\ntranslator = pipeline(task = task, model = model)\n\n# Perform translation and store the output\noutput = translator(input_text)\n\n# Return output\noutput\n```", "```py\n# Import packages\nfrom transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n\n# Specify input text\ninput_text = \"This is a post on Medium about various NLP tasks using Hugging Face.\"\n\n# Specify model\nmodel_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n\n# Instantiate model and tokenizer\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\ntokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n\n# Specify source language\ntokenizer.src_lang = \"en_XX\"\n\n# Encode input text\nencoded_en = tokenizer(input_text, return_tensors=\"pt\")\n\n# Perform translation to the target language\ngenerated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id[\"fr_XX\"])\n\n# Decode the translation and store the output\noutput = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n\n# Return output\noutput\n```"]