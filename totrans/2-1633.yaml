- en: 'Parameter-Efficient Fine-Tuning (PEFT) for LLMs: A Comprehensive Introduction'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs 的参数高效微调 (PEFT)：全面介绍
- en: 原文：[https://towardsdatascience.com/parameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95](https://towardsdatascience.com/parameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/parameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95](https://towardsdatascience.com/parameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95)
- en: A conceptual survey of PEFT methods used by Hugging Face, Google’s Vertex AI,
    and eventually OpenAI
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对 Hugging Face、Google 的 Vertex AI 和最终 OpenAI 使用的 PEFT 方法进行概念性调查
- en: '[](https://medium.com/@smsmith714?source=post_page-----e52d03117f95--------------------------------)[![Sean
    Smith](../Images/611395d113b10ec4bbfaf781301139c7.png)](https://medium.com/@smsmith714?source=post_page-----e52d03117f95--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e52d03117f95--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e52d03117f95--------------------------------)
    [Sean Smith](https://medium.com/@smsmith714?source=post_page-----e52d03117f95--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@smsmith714?source=post_page-----e52d03117f95--------------------------------)[![肖恩·史密斯](../Images/611395d113b10ec4bbfaf781301139c7.png)](https://medium.com/@smsmith714?source=post_page-----e52d03117f95--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e52d03117f95--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e52d03117f95--------------------------------)
    [肖恩·史密斯](https://medium.com/@smsmith714?source=post_page-----e52d03117f95--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e52d03117f95--------------------------------)
    ·19 min read·Aug 22, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e52d03117f95--------------------------------)
    ·19 分钟阅读·2023 年 8 月 22 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/425a0569c2f36a07f176e6742754e25f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/425a0569c2f36a07f176e6742754e25f.png)'
- en: Image created by DALL-E. A Sunday Afternoon on the Island of La Grande Jatte
    but everyone is a humanoid.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 DALL-E 创建。《大碗岛上的一个星期天下午》，但每个人都是类人。
- en: Large Language Models (LLMs) are quite large by name. These models usually have
    anywhere from 7 to 70 billion parameters. To load a 70 billion parameter model
    in full precision would require 280 GB of GPU memory! To train that model you
    would update billions of tokens over millions or billions of documents. The computation
    required is substantial for updating those parameters. The self-supervised training
    of these models is expensive, [costing companies up to $100 million](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的名字就意味着它们很大。这些模型通常具有从 70 亿到 700 亿个参数。以全精度加载一个 700 亿参数的模型需要 280 GB
    的 GPU 内存！要训练这个模型，你需要在数百万或数十亿个文档上更新数十亿个标记。更新这些参数所需的计算量很大。这些模型的自监督训练成本很高，[公司最高花费可达
    1 亿美元](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/)。
- en: For the rest of us, there is significant interest in adapting our data to these
    models. With our limited datasets (in comparison) and lacking computing power,
    how do we create models that can improve on the major players at a fraction of
    the cost?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们来说，适应我们的数据以适配这些模型引起了显著的兴趣。面对有限的数据集（相比之下）和缺乏计算能力，我们如何在成本的一小部分下创建可以超越主要竞争者的模型？
- en: This is where the research field of Parameter-Efficient Fine-Tuning (PEFT) comes
    into play. Through various techniques, which we will soon explore in detail, we
    can augment small sections of these models so they are better suited to the tasks
    we aim to complete.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是参数高效微调（PEFT）研究领域的作用所在。通过各种技术，我们可以增强这些模型的小部分，使它们更适合我们要完成的任务。
- en: After reading this article, you will conceptually grasp each PEFT technique
    applied in Hugging Face and be able to distinguish the differences between them.
    One of the most helpful overviews I found before this article was from a [Reddit
    comment](https://www.reddit.com/r/MachineLearning/comments/14pkibg/d_is_there_a_difference_between_ptuning_and/jqkdam8/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button).
    There’s also another [exceptional article](https://lightning.ai/pages/community/article/understanding-llama-adapters/)
    available from lightning.ai (the creators of pytorch lightning.) Additionally,
    there’s a comprehensive survey that much of this piece is based on, authored by
    Liali et al [2]. In my article, I aim to address the gaps I identified while reviewing
    this material. At the time of writing, this article serves as a conceptual guide
    to all the PEFT methods present in the Hugging Face library. The goal for readers
    is to approach the research literature for other PEFT techniques with a fundamental
    understanding of the field.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本文后，你将概念性地掌握 Hugging Face 中应用的每种 PEFT 技术，并能够区分它们之间的差异。在这篇文章之前，我发现的一个最有帮助的概述来自
    [Reddit 评论](https://www.reddit.com/r/MachineLearning/comments/14pkibg/d_is_there_a_difference_between_ptuning_and/jqkdam8/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)。此外，lightning.ai（pytorch
    lightning 的创作者）还有另一篇 [杰出文章](https://lightning.ai/pages/community/article/understanding-llama-adapters/)。另外，还有一篇基于大量此文的全面调查，由
    Liali 等人撰写 [2]。在我的文章中，我旨在解决在审阅这些材料时识别出的空白点。在撰写本文时，本文作为 Hugging Face 库中所有 PEFT
    方法的概念指南。读者的目标是以对该领域的基本理解来接触其他 PEFT 技术的研究文献。
- en: 'A Moment for Self-Reflection: Is it time to fine-tune?'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自我反思的时刻：是时候进行微调了吗？
- en: I wrote a previous article about [considerations regarding fine-tuning LLMs](/thinking-about-fine-tuning-an-llm-heres-3-considerations-before-you-get-started-c1f483f293)
    and how similar performance could be achieved through In-Context Learning. Since
    then Llama 2 has been released and there has been great improvements in the open
    source LLM world. Here are some expanded thoughts I can share that extend beyond
    that article.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我写了一篇关于 [微调 LLMs 的考虑因素](/thinking-about-fine-tuning-an-llm-heres-3-considerations-before-you-get-started-c1f483f293)
    的文章，以及通过 In-Context Learning 实现类似性能的方法。自那时起，Llama 2 已经发布，开源 LLM 世界有了很大进步。这里是一些我可以分享的扩展思考，超出了那篇文章的范围。
- en: Fine-tuning is inherently dangerous for your organization. In a recent paper
    it was shown that LLMs can remember at least 1% of their training data [1]. If
    you have potential data duplication, that floor of 1% goes up even higher. If
    your fine-tuned LLMs will be used by non-internal users, ask yourself if it’s
    okay to give them the data you are going to train on. Users can do malicious things
    to your model such as a [prompt injection attack.](https://blog.mithrilsecurity.io/attacks-on-ai-models-prompt-injection-vs-supply-chain-poisoning/)
    I made a [LinkedIn post](https://www.linkedin.com/posts/sms714_fine-tuning-llms-a-high-stakes-game-activity-7095426067393384448-xR30?utm_source=share&utm_medium=member_desktop)
    about these security risks that serves as a quick overview. In the event that
    you can’t give away your data, dynamic observation selection with ICL is one of
    your best options (see my other [article](/thinking-about-fine-tuning-an-llm-heres-3-considerations-before-you-get-started-c1f483f293)
    for details.)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 微调本质上对你的组织是危险的。最近的一篇论文显示，LLMs 至少可以记住 1% 的训练数据 [1]。如果你有潜在的数据重复，这个 1% 的比例会更高。如果你的微调
    LLMs 将被非内部用户使用，问问自己是否可以将你要训练的数据提供给他们。用户可能对你的模型进行恶意操作，比如 [提示注入攻击](https://blog.mithrilsecurity.io/attacks-on-ai-models-prompt-injection-vs-supply-chain-poisoning/)。我在
    [LinkedIn](https://www.linkedin.com/posts/sms714_fine-tuning-llms-a-high-stakes-game-activity-7095426067393384448-xR30?utm_source=share&utm_medium=member_desktop)
    上发布了关于这些安全风险的帖子，作为一个快速概述。如果你不能提供你的数据，使用 ICL 的动态观察选择是你的最佳选择之一（详情请参见我的另一篇 [文章](/thinking-about-fine-tuning-an-llm-heres-3-considerations-before-you-get-started-c1f483f293)）。
- en: You must also prioritize the creation of high-quality data labels for your learning
    task. If the organization’s commitment to top-notch data is lacking, particularly
    in support of your project’s fine-tuning, I recommend considering an alternative
    approach. Models thrive on high-quality labeled inputs. If the commitment from
    your stakeholders is not there for human labelers, you risk disappointing all
    parties involved.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你还必须优先创建高质量的数据标签用于你的学习任务。如果组织对高质量数据的承诺不足，特别是对于你项目的微调支持，我建议考虑其他方法。模型依赖于高质量的标注输入。如果你的利益相关者对人工标注者的承诺不足，你可能会让所有相关方感到失望。
- en: Who Even Uses PEFT?
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谁在使用 PEFT？
- en: PEFT is used by most providers that offer the ability to fine-tune language
    models. If the provider doesn’t already make use of these techniques, I guarantee
    they have plans to. This article covers all of the techniques from [Hugging Face
    PEFT](https://huggingface.co/docs/peft/index) that are available at time of writing.
    The survey from Lialin et al. [2] is referenced by Google in their introductory
    video about [tuning foundation models on Vertex AI](https://www.youtube.com/watch?v=4A4W03qUTsw).
    While Vertex AI is more of a black box, I have heard use of [adapters](https://services.google.com/fh/files/misc/adaptation_of_foundation_models_whitepaper_google_cloud.pdf),
    prompt-tuning, and recently LoRa from sales pitches. It’s unclear exactly what
    they are use, but at it’s core the techniques we discuss here are what’s powering
    things.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: PEFT 被大多数提供语言模型微调能力的供应商使用。如果供应商尚未使用这些技术，我保证他们有计划使用这些技术。本文涵盖了在撰写时可用的[Hugging
    Face PEFT](https://huggingface.co/docs/peft/index)中的所有技术。Lialin 等人的调查[2]在 Google
    关于[在 Vertex AI 上调整基础模型](https://www.youtube.com/watch?v=4A4W03qUTsw)的介绍视频中被引用。虽然
    Vertex AI 更像是一个黑箱，但我听说过销售推介中提到的[适配器](https://services.google.com/fh/files/misc/adaptation_of_foundation_models_whitepaper_google_cloud.pdf)、提示微调，以及最近的
    LoRa。虽然不清楚他们具体使用了什么，但我们在这里讨论的技术就是支撑这些功能的核心。
- en: OpenAI does offer fine-tuning, but however famously [has not implemented any
    PEFT methods](https://web.archive.org/web/20230531203946/https://humanloop.com/blog/openai-plans)
    yet. This is based on a blog post that was requested to be deleted by OpenAI a
    few months ago. The article details OpenAI does not use Adapters or LoRa to make
    fine-tuning more compute friendly. There has been no announcement from OpenAI
    that this has been implemented, so the safe assumption is that these features
    are not available to users yet. It is included on the roadmap for OpenAI and since
    [fine-tuning is much more lucrative](https://openai.com/pricing) than normal model
    use I suspect it will be available in the near future.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 确实提供了微调服务，但[至今尚未实现任何 PEFT 方法](https://web.archive.org/web/20230531203946/https://humanloop.com/blog/openai-plans)。这一点基于
    OpenAI 几个月前要求删除的一篇博客文章。该文章详细说明了 OpenAI 不使用 Adapters 或 LoRa 来使微调更加计算友好。OpenAI 尚未宣布这些功能的实施，因此可以安全地假设这些功能目前还不可用。它已被纳入
    OpenAI 的路线图中，并且由于[微调比正常模型使用更具盈利性](https://openai.com/pricing)，我怀疑它将在不久的将来推出。
- en: Quick Transformer Review
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速 Transformer 复习
- en: I assume that readers of this article are familiar with the Transformer architecture.
    You don’t need to be intimately invested in the details of self-attention or any
    components, but you should have glanced at Vaswani et al. [3] and maybe had a
    walkthrough of [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
    (in my opinion that’s the best resource to learn the Transformer.)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设这篇文章的读者对 Transformer 架构已经有所了解。你不需要对自注意力或任何组件的细节有深入的了解，但你应该至少浏览过 Vaswani 等人的[论文](https://wiki.example.org/feynmans_learning_method)，并且可能对[注释版
    Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)有过一些了解（在我看来，这是学习
    Transformer 的最佳资源）。
- en: 'I am going to include this pseudo code for the transformer block. If you don’t
    know much about transformers, just know that at it’s core they do this:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我将包括 Transformer 块的伪代码。如果你对 Transformer 了解不多，只需知道它的核心功能如下：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: All functions from that pseudo code are as described in Vaswani et al. The FFN
    is a Feed Forward Network, which is 2 layers for our purposes. Many PEFT techniques
    that follow make changes to the transformer block or to self-attention, so I’ll
    reference and change this pseudo code as we move through the guide.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 那些伪代码中的所有功能都如 Vaswani 等人所述。FFN 是前馈网络，对于我们的目的来说是 2 层。许多随后的 PEFT 技术对 Transformer
    块或自注意力进行修改，因此在我们进行指南时，我会参考并修改这段伪代码。
- en: A Tour through PEFT Methods
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PEFT 方法概览
- en: '![](../Images/b9e462418b0698dca2f8e214b7774e8e.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9e462418b0698dca2f8e214b7774e8e.png)'
- en: Overview of methods and classes from [2].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [2] 的方法和类别概述。
- en: We’ll go through each technique by looking at the broader classes in the diagram
    above. The classes we will cover are Additive, Adapters, Soft-Prompts, Reparameterization,
    and one Hybrid method that is a combination of Reparameterization and Selective
    (that isn’t Sparse LoRa).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过查看上图中的更广泛类别来逐一介绍每种技术。我们将涵盖的类别包括加性方法、适配器、软提示、重新参数化，以及一种混合方法，它是重新参数化和选择性（而不是稀疏
    LoRa）的组合。
- en: Additive Methods
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加性方法
- en: Additive methods are probably the easiest to grasp. The goal of additive methods
    is to add an additional set of parameters or network layers to augment the model.
    When fine-tuning the data you update the weights only of these newly added parameters.
    This makes training computationally easier and also adapts to smaller datasets
    (think in the ballpark of 100–500 for starters, with a ceiling near 100,000.)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 加性方法可能是最容易理解的。加性方法的目标是添加一组额外的参数或网络层以增强模型。在微调数据时，您只更新这些新添加参数的权重。这使得训练计算上更容易，并且适应较小的数据集（起始时大约
    100-500，最高接近 100,000）。
- en: 'Method: Adapters'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法：适配器
- en: Adapters are simultaneously a method and a class. This technique was introduced
    in Houlsby et al [4]. The goal of adapters is to add small fully connected networks
    after Transformer sub-layers and learn those parameters. I follow the definitions
    from [2] and keep a strict definition of adapters as only adding fully connected
    layers to the network.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 适配器既是一种方法也是一种类别。这项技术在 Houlsby 等人 [4] 中被介绍。适配器的目标是在 Transformer 子层之后添加小的全连接网络，并学习这些参数。我遵循
    [2] 中的定义，并将适配器严格定义为仅向网络添加全连接层。
- en: Houlsby et al. propose a simple update to the transformer block. They add fully
    connected layers in two places as shown in this pseudo code.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Houlsby 等人提出了一种对 transformer 块进行简单更新的方法。他们在两个地方添加了全连接层，如下所示的伪代码。
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Method: (IA)³'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法：（IA）³
- en: Infused Adapter by Inhibiting and Amplifying Inner Activations, or (IA)³ is
    a very interesting additive method (adding parameters) that augments the transformer
    block with some new parameters. It was proposed by Liu et al. [5] in 2022\. Despite
    the name, this is not an adapter method since it does not strictly add fully connected
    layers after the sub-layers of the transformer block .
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过抑制和放大内部激活的注入适配器，或 (IA)³ 是一种非常有趣的加性方法（添加参数），它通过一些新参数扩展了 transformer 块。它由 Liu
    等人 [5] 于 2022 年提出。尽管名字如此，但这不是适配器方法，因为它并没有严格地在 transformer 块的子层之后添加全连接层。
- en: 'Let’s consider the scaled dot-product attention found in a normal transformer:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑正常 transformer 中的缩放点积注意力：
- en: Scaled dot-product attention from Vaswani et al. [3]
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 Vaswani 等人 [3] 的缩放点积注意力
- en: 'Since we are working with an additive method, we are seeking to add parameters
    to this network. We want the dimensionality to be quite small. (IA)³ proposes
    the following new vectors to be added to the attention mechanism:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在使用加性方法，我们寻求向此网络添加参数。我们希望维度尽可能小。（IA）³ 提出了以下新向量以添加到注意力机制中：
- en: Scaled dot-product attention in (IA)³ from [5]. Here we added two column vectors
    to the normal equation, l_k, l_v, which take the Hadamard product by the key and
    value terms, respectively.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在 (IA)³ 中的缩放点积注意力来自 [5]。这里我们在普通方程中添加了两个列向量 l_k 和 l_v，它们分别与键和值项进行 Hadamard 乘积。
- en: We just added column vectors l_k and l_v and take the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))
    between the column vector and the matrix (multiple the column vector against all
    columns of the matrix).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是添加了列向量 l_k 和 l_v，并进行 [Hadamard 乘积](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))，即列向量与矩阵之间的乘法（将列向量与矩阵的所有列相乘）。
- en: 'We also introduce one other learnable column vector l_{ff} that is added to
    the feed forward layers as follow:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还引入了另一个可学习的列向量 l_{ff}，它被添加到前馈层中，如下所示：
- en: Feed Forward update in (IA)³ adapted from [5]. We can see that we add the column
    vector l_{ff} to the network and take Hadamard product with the output of the
    first layer of the transformer blocks FFN. The function gamma is the activation
    function [GELU](https://paperswithcode.com/method/gelu#:~:text=The%20GELU%20activation%20function%20is%20x%20%CE%A6%20(%20x%20)%20%2C%20where,of%20as%20a%20smoother%20ReLU.).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: (IA)³ 中的前馈更新改编自 [5]。我们可以看到，我们将列向量 l_{ff} 添加到网络中，并与 transformer 块 FFN 的第一层输出进行
    Hadamard 乘积。函数 gamma 是激活函数 [GELU](https://paperswithcode.com/method/gelu#:~:text=The%20GELU%20activation%20function%20is%20x%20%CE%A6%20(%20x%20)%20%2C%20where,of%20as%20a%20smoother%20ReLU.)。
- en: 'In this example, gamma is the activation function applied to the product between
    the weights and input. Here is some pseudo code for (IA)³:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，gamma 是应用于权重和输入之间乘积的激活函数。这是 (IA)³ 的一些伪代码：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Soft-Prompts
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软提示
- en: To understand soft-prompts, let’s first discuss hard-prompts, a concept that
    most readers might be familiar with, even if not by name. In hard prompting, we
    put together a dataset of prompts that represent the task at hand. When someone
    interacts with the network by posing a question, they could phrase it in different
    ways. With hard prompting, the process involves curating a dataset that covers
    the various ways a particular task could be framed for the language model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解软提示，我们首先讨论硬提示，一个大多数读者可能都熟悉的概念，即使不是通过名字。在硬提示中，我们会将代表当前任务的数据集整理在一起。当有人通过提出问题与网络互动时，他们可能会用不同的方式表达问题。使用硬提示，这个过程涉及策划一个数据集，涵盖语言模型可以框定特定任务的各种方式。
- en: Soft-prompting is a technique that tries to avoid this dataset creation. In
    hard prompting, we are creating data in a discrete representation (picking words.)
    In soft-prompting, we seek a continuous representation of the text we will input
    to the model. This does imply you need one static prompt for the examples you
    are training on.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 软提示（Soft-prompting）是一种试图避免创建数据集的技术。在硬提示中，我们是在离散表示中创建数据（选择单词）。在软提示中，我们寻求对输入到模型中的文本进行连续表示。这意味着你需要为正在训练的示例提供一个静态提示。
- en: Depending on the technique, there are different methods for how the information
    is added to the network. The core idea is that the base model does not optimize
    the text itself but rather the continuous representation (i.e. some type of learnable
    tensor) of the prompt text. This can be some form of embedding or some transformation
    applied to that embedding. These techniques will be explored in more detail as
    we move on.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 根据技术的不同，信息添加到网络中的方法也各不相同。核心思想是基础模型并不优化文本本身，而是优化提示文本的连续表示（即某种类型的可学习张量）。这可以是某种形式的嵌入或对该嵌入应用的某种变换。这些技术将在我们继续深入探讨时详细说明。
- en: 'Method: Prompt-Tuning'
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法：Prompt-Tuning
- en: '![](../Images/08f053c16dae10c6ecf38a250d65c7dd.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08f053c16dae10c6ecf38a250d65c7dd.png)'
- en: Image from prompt-tuning from Lester et [11]. This shows that in prompt-tuning
    we concatenate the soft-prompt and the input text’s representation (embedding)
    to the pre-trained model. Doing this allows us to optimize our representation
    of the soft-prompt through a learnable tensor.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来自 Lester 等人 [11] 的 prompt-tuning。这表明，在 prompt-tuning 中，我们将软提示和输入文本的表示（嵌入）连接到预训练模型中。这样做使我们能够通过一个可学习的张量来优化软提示的表示。
- en: Prompt tuning is a technique from Lester et al. [11] that falls into the category
    of soft-prompts. With soft-prompts our goal is to add information to the base
    model that is more specific to our current task. With prompt tuning we accomplish
    this by creating a set of parameters for the prompt tokens and injecting this
    at the beginning of the network.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Prompt tuning 是 Lester 等人 [11] 提出的技术，属于软提示的范畴。使用软提示时，我们的目标是向基础模型添加更具体于当前任务的信息。通过
    prompt tuning，我们通过创建一组提示令牌的参数并将其注入到网络的开头来实现这一点。
- en: To find a representation of the soft prompt, we create a separate set of embeddings
    for the static prompt used during training. We concatenate the output embeddings
    with the sequence embeddings. We use this new information to pass into the language
    model. Creating this dual information allows us to learn a parameterization of
    the soft prompt without needing to create many prompts for the same task.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到软提示的表示，我们为训练过程中使用的静态提示创建了一组单独的嵌入。我们将输出嵌入与序列嵌入进行连接。我们使用这些新信息传递到语言模型中。创建这种双重信息使我们能够学习软提示的参数化，而无需为同一任务创建多个提示。
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: There are a number of rich benefits to fine-tuning through this approach. This
    new set of parameters can be very small, around 0.01% the size of the tunable
    parameters of the base model. This creates opportunities to have an ensemble of
    task specific models that all run off the same base model, which drastically decreases
    memory requirements for the model. For more on this, check out this post I shared
    on [LinkedIn](https://www.linkedin.com/posts/sms714_ensembling-has-long-been-a-favorite-technique-activity-7097213152114708480-BHxn?utm_source=share&utm_medium=member_desktop)
    and also the section on ensembling in [3].
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方法进行微调有许多丰富的好处。这组新参数可以非常小，大约是基础模型可调参数的 0.01%。这创造了一个使用相同基础模型的任务特定模型集合的机会，这大大减少了模型的内存需求。有关更多信息，请查看我在[LinkedIn](https://www.linkedin.com/posts/sms714_ensembling-has-long-been-a-favorite-technique-activity-7097213152114708480-BHxn?utm_source=share&utm_medium=member_desktop)上分享的帖子以及[3]中的集成部分。
- en: 'Method: Prefix Tuning'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法：前缀调整
- en: Prefix tuning is another soft prompting technique that is very similar to prompt
    tuning. In prompt tuning we created a separate set of parameters that we fed our
    input into and appended the outputs to the continuous representation of the text
    input in the model. In Prefix tuning we also find a continuous representation
    from a separate set of prompt tokens that are input into the base model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀调整是另一种与提示调整非常相似的软提示技术。在提示调整中，我们创建了一组单独的参数，将输入传递给这些参数，并将输出附加到模型中输入文本的连续表示上。在前缀调整中，我们还从一组单独的提示标记中找到一个连续的表示，这些标记被输入到基础模型中。
- en: The difference between prefix tuning and prompt tuning is that the representation
    from prefix tuning is fed to all layers of the transformer whereas prompt tuning
    was only concatenated with the embeddings. Additionally, we also learn additional
    parameters for the soft prompt for prefix tuning in the form of a fully connected
    network. After training the FFN is discarded and we only use the soft-prompt as
    input.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀调整和提示调整的区别在于，前缀调整的表示会传递到变换器的所有层，而提示调整仅与嵌入层级联。此外，我们还为前缀调整学习了额外的参数，形式为一个全连接网络。训练后，FFN
    被丢弃，我们只使用软提示作为输入。
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Method: P-Tuning'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法：P-Tuning
- en: '![](../Images/48f16389543c55a9a51eaeb37d614d7e.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48f16389543c55a9a51eaeb37d614d7e.png)'
- en: Image for P-Tuning from Liu et al [6]. This shows the creation of the prompt
    embedding throughout the prompt encoding being concatenated with the input embedding.
    The prompt encoder consists of and embedding, LSTM, and then some fully connected
    layers.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 刘等人[6]提供的 P-Tuning 图像。该图展示了提示嵌入的创建过程，整个提示编码与输入嵌入进行级联。提示编码器包括一个嵌入层、LSTM，然后是一些全连接层。
- en: P-Tuning is another soft-prompting method introduced by Liu et al. [6] that
    differs from prompt and prefix tuning. Colloquially we can think of P-Tuning as
    prompt-tuning but encoding the prompt using an LSTM.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: P-Tuning 是刘等人[6]提出的另一种软提示方法，与提示调整（prompt tuning）和前缀调整（prefix tuning）不同。通俗地说，我们可以将
    P-Tuning 视为提示调整，但使用 LSTM 对提示进行编码。
- en: P-Tuning sets out to solve two problems the authors noticed. The first is the
    discreteness of the word embeddings passed to the model. The authors argue that
    if the embeddings are randomly initialized and then optimized through Stochastic
    Gradient Descent, the model is likely to fall into a local minima. They second
    is association of the word embeddings. With the parameterization in prompt-tuning
    and prefix-tuning, the soft prompts are technically independent of each other.
    The authors wanted an approach that made the prompt tokens dependent on each other.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: P-Tuning 旨在解决作者注意到的两个问题。第一个是传递给模型的词嵌入的离散性。作者认为，如果嵌入是随机初始化的，然后通过随机梯度下降优化，模型可能会陷入局部最小值。第二个是词嵌入的关联性。在提示调整和前缀调整的参数化中，软提示在技术上是相互独立的。作者希望找到一种方法，使提示标记相互依赖。
- en: The authors propose that a prompt is a function that takes a context x and a
    target y and organizes itself into a template T. The authors provide the example
    sequence “The capital of Britain is [MASK]”. Here the prompt is “The capital of
    … is …”, the context is “Britain” and the target is [MASK]. We can use this formulation
    to create two sequences of tokens, everything before the context and everything
    after the context before the target. We can learn a representation of this additional
    information that can be reduced to a continuous output and fed into the language
    model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们提出，提示是一个函数，它接受一个上下文x和一个目标y，并将其组织成一个模板T。作者提供了示例序列“The capital of Britain is
    [MASK]”。这里的提示是“The capital of … is …”，上下文是“Britain”，目标是[MASK]。我们可以使用这种表述创建两个标记序列：上下文之前的所有内容以及上下文之后和目标之前的所有内容。我们可以学习这些附加信息的表示，并将其缩减为连续输出，输入到语言模型中。
- en: To embed the prompt in this way, we use a small network of an LSTM fed into
    a two layer FFN. We pass the prompt tokens, those before the context and those
    after and before the target.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以这种方式嵌入提示，我们使用一个由LSTM组成的小型网络，输入到一个两层的FFN中。我们传递提示标记，包括上下文之前的和目标之前的标记。
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Method: LLaMA-Adapater'
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法：LLaMA-Adapter
- en: '![](../Images/8b51188fac198efd68e593fdf1f575db.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b51188fac198efd68e593fdf1f575db.png)'
- en: Image for LLaMA-Adapter from Zhang et al. [7] We can see the addition of zero-initialized
    attention being used on the adaption prompts and that those are the only thing
    fine-tuned.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从Zhang等人[7]处获得的LLaMA-Adapter图像。我们可以看到零初始化的注意力被用于适配提示，并且这些是唯一被微调的内容。
- en: LLaMA adapter is a soft-prompting technique introduced by Zhang et al. [7] that
    applies a more efficient version of prefix learning to the Llama model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA adapter是Zhang等人[7]引入的一种软提示技术，它将更高效的前缀学习版本应用于Llama模型。
- en: LLaMA-Adapter has a few key differences from Prefix Tuning. They introduce Adaptation
    Prompts, which are soft-prompts appended with the input to the transformer layer.
    These adaption prompts are inserted in the L topmost of the N transformer layers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA-Adapter与Prefix Tuning有一些关键差异。它们引入了适配提示，这些是附加到变换器层输入的软提示。这些适配提示被插入到N个变换器层的最上层L处。
- en: The authors also introduce zero initialized attention. With additive methods
    we introduce a new set of parameters that have some random initialization over
    the weights. Because of this random noise added to the LM, we can potentially
    experience unstable fine-tuning which can cause a problem with large loss values
    at the early stages. To solve this problem, the authors introduce a gating factor,
    initialized to 0, that is multiplied by the self attention mechanism. The product
    of the gating factor and self-attention is referred to as zero-init attention.
    The gating value is adaptively tuned over the training steps to create a smoother
    update of the network parameters.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们还引入了零初始化注意力。通过加法方法，我们引入了一组在权重上有一些随机初始化的新参数。由于这种随机噪声的加入，可能会经历不稳定的微调，这可能在早期阶段导致较大的损失值。为了解决这个问题，作者们引入了一个初始化为0的门控因子，该因子与自注意力机制相乘。门控因子和自注意力的乘积称为零初始化注意力。门控值在训练步骤中自适应调整，以创建更平滑的网络参数更新。
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Reparameterization-Based Methods
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于重参数化的方法
- en: Reparameterization-Based methods focus on finding low dimensional representations
    of the same weight matrices found in the base model. The first connection between
    fine-tuning and a low dimensional representation was shown in Hu et al [8]. The
    authors make a connection between the full parameters of the model and a lower
    dimensional representation. Depending on the task, the authors are able to achieve
    90% of the results of the fully fine-tuned model with approximately 0.0002% of
    the trainable parameters.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 基于重参数化的方法专注于找到与基础模型中相同的权重矩阵的低维表示。Hu等人[8]首次展示了微调与低维表示之间的联系。作者在模型的完整参数和较低维度表示之间建立了联系。根据任务，作者能够用大约0.0002%的可训练参数实现完全微调模型的90%的结果。
- en: '**Method: LoRa**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法：LoRa**'
- en: '![](../Images/c7f6a7616ba55d625afb1f93ba61941d.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7f6a7616ba55d625afb1f93ba61941d.png)'
- en: Image taken from Hu & Shen et al [9]. Here we can see the pre-trained weights
    and the additional matrices A and B. A is initialized normally and B is initialized
    to 0\. We train only A and B.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从Hu & Shen等人[9]处获取的图像。我们可以看到预训练权重以及额外的矩阵A和B。A被正常初始化，而B被初始化为0。我们仅训练A和B。
- en: One of the most popular techniques in fine-tuning is a reparameterization-based
    method called Low-Rank Adaptation (LoRa) [9]. LoRa updates a weight matrix by
    learning a separate matrix which represents the updates from optimization. They
    go a step further to create two smaller dimension weight matrices to represent
    this difference. By creating smaller dimension weight matrices we have less parameters
    to learn.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 微调中最流行的技术之一是基于重参数化的方法，称为低秩适应（LoRa）[9]。LoRa通过学习一个单独的矩阵来更新权重矩阵，该矩阵表示来自优化的更新。它们进一步创建两个较小维度的权重矩阵来表示这种差异。通过创建较小维度的权重矩阵，我们需要学习的参数更少。
- en: To train LoRa we use the fundamental ideas of gradient descent, where we make
    incremental adjustments to a set of parameters that move us closer to our goal
    (loss function). In LoRa we choose to isolate all of our updates to a separate
    matrix. This matrix, we denote Delta W, represents all of the parameter updates
    we learn in this during the fine-tuning process.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练LoRa，我们使用梯度下降的基本思想，在这一过程中，我们对一组参数进行渐进调整，使我们更接近目标（损失函数）。在LoRa中，我们选择将所有更新隔离到一个单独的矩阵中。这个矩阵，我们称之为Delta
    W，表示我们在微调过程中学到的所有参数更新。
- en: Let’s assign W_0 to have dimensions dxk (d rows and k columns). We want to update
    it’s parameters so that it is aligned with our new goal. You can represent this
    update to the parameters by ΔW, which also the dimension dxk. We can model our
    update rule using the equation below.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将W_0的维度设为dxk（d行和k列）。我们希望更新它的参数，以使其与我们的新目标对齐。你可以通过ΔW来表示这个参数的更新，它的维度也是dxk。我们可以使用下面的方程来建模我们的更新规则。
- en: Update rule for W_0 from [9]. We isolate the changes to W_0 in DeltaW. We then
    represent DeltaW as the product of A and B, two smaller dimension matrices. This
    way we learn less parameters but still update W, which makes fine-tuning computational
    easier.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[9]的W_0的更新规则。我们将W_0的变化隔离到DeltaW中。然后，我们将DeltaW表示为A和B两个较小维度矩阵的乘积。这样，我们学习的参数更少，但仍然更新W，这使得微调的计算更加简单。
- en: 'Now let’s change our update rule such that ΔW is modeled by a matrix multiplication
    AB. We assign matrix A to have dimensions dxr and B to have dimensions rxk. If
    you’re up to speed on your matrix multiplications you will see that AB has the
    same dimensions as W_0, so the addition of these matrices is valid. Here’s why
    AB is a better choice than DeltaW: Matrix A only has dxr and matrix B has rxk.
    If we make r a very small number (r=8 is a typical value), then the number of
    parameters in A and B is drastically smaller than ΔW. If we only learn the parameters
    of A and B, then we learn `d*k-d*r-r*k` less parameters. In practice this allows
    us to learn only 0.1–0.5% of the parameters of the original network.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们改变更新规则，使得ΔW由矩阵乘法AB建模。我们将矩阵A指定为dxr的维度，矩阵B指定为rxk的维度。如果你对矩阵乘法有所了解，你会发现AB与W_0的维度相同，因此这些矩阵的加法是有效的。这里是AB比DeltaW更好的原因：矩阵A仅有dxr的维度，而矩阵B有rxk的维度。如果我们将r设为一个非常小的数值（r=8是一个典型值），那么A和B中的参数数量会比ΔW小得多。如果我们只学习A和B的参数，我们将学习`d*k-d*r-r*k`更少的参数。实际上，这使我们只需学习原始网络参数的0.1–0.5%。
- en: The walk through I just illustrated is the essence of how LoRa works. Instead
    of optimizing a matrix W through additional training steps, we alter a matrix
    ΔW through two new matrices A and B that have far fewer parameters. This result
    helps us optimize many fewer parameters, which makes our training much more efficient.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我刚刚描述的过程就是LoRa工作原理的精髓。我们不是通过额外的训练步骤来优化矩阵W，而是通过两个参数远少的新矩阵A和B来改变矩阵ΔW。这种结果帮助我们优化了更少的参数，使得训练更为高效。
- en: Typically we use this update rule for the key and value matrices of the self
    attention in the transformer block. We also add a scaling factor, set to 1/r,
    to the amount of information given to the update. See the pseudo code below.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我们将这个更新规则应用于变换器块中的自注意力的关键和数值矩阵。我们还添加了一个缩放因子，设为1/r，以调整更新所提供的信息量。请参见下面的伪代码。
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Selective Methods
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择性方法
- en: With selective methods we choose some of the parameters to update and do not
    update others. The problem with these approaches is that we have created a sparse
    matrix of parameters. Sparse matrix operations are not well supported on present
    day GPUs and provide computation challenges. For more information on why sparse
    matrices produce computational challenges, check out [10].
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用选择性方法，我们选择一些参数进行更新，而不更新其他参数。这些方法的问题在于我们创建了一个稀疏的参数矩阵。稀疏矩阵操作在现代GPU上不太被支持，并带来计算挑战。有关稀疏矩阵为何会产生计算挑战的更多信息，请查看[10]。
- en: There are also techniques in Selective methods that focus on pruning unsuccessful
    vectors or manipulating model biases. These also create additional complexity
    when attempting to train the model. In general these methods have more challenging
    implementations since their compute operations are more expensive than other operations.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择性方法中，还有一些技术专注于修剪不成功的向量或操控模型偏差。这些方法在训练模型时也会增加额外的复杂性。一般来说，这些方法的实现更具挑战性，因为它们的计算操作比其他操作更为昂贵。
- en: '**Method: AdaLoRa**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法：AdaLoRa**'
- en: This is a hybrid approach that combines ideas from reparameterized and selective
    methods. Zhang et al [12] developed AdaLoRa by studying LoRa and formed the question
    “How can we allocate the parameter budget adaptively according to [the] importance
    of modules to improve the performance of parameter-efficient fine-tuning?” What
    this translates to is “How can we give preference to the parameters that lead
    to better performance rather than treating all parameters equally?”
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种混合方法，结合了重新参数化和选择性方法的思想。Zhang 等人 [12] 通过研究 LoRa 开发了 AdaLoRa，并提出了这样一个问题：“我们如何根据模块的重要性自适应地分配参数预算，以提高参数高效微调的性能？”这意味着“我们如何优先考虑那些能带来更好性能的参数，而不是平等对待所有参数？”
- en: 'Instead of using two matrices A and B as we did in LoRa, AdaLoRa uses an approximation
    of [Singular Value Decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition)
    to reduce the dimensionality of a vector space into three matrices: P (left singular
    vectors), Lambda (singular values), and Q (right singular vectors). Using these
    three matrices we can reconstruct an approximation of our vector space Delta as
    P * Lambda * Q. The benefit of using SVD is the singular values represent the
    importance of the vector in this lower dimensional space. The contribution of
    this paper is applying some efficiency implementations to use an SVD related approach
    to consider importance around which weights should be optimized.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在 LoRa 中使用的两个矩阵 A 和 B 不同，AdaLoRa 使用[奇异值分解 (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition)的近似来将向量空间的维度降低到三个矩阵：P（左奇异向量）、Lambda（奇异值）和
    Q（右奇异向量）。使用这三个矩阵，我们可以重建向量空间 Delta 的近似值，即 P * Lambda * Q。使用 SVD 的好处在于奇异值表示了在这个低维空间中向量的重要性。本文的贡献在于应用一些高效实现，以使用与
    SVD 相关的方法来考虑哪些权重应当被优化。
- en: In LoRa, we saw that we can approximate delta W with two matrices A and B. Here
    we can replace A and B with our new approximation P * Lambda * Q. Since lambda
    only has values along the diagonal (singular values) we store it as a column vector.
    We pick the dimensions of our matrices P (d x r), Lambda (r x r), and Q (r x k)
    to match the dimensions of a weight matrix W (d x k).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LoRa 中，我们看到可以用两个矩阵 A 和 B 来近似 delta W。在这里，我们可以用新的近似值 P * Lambda * Q 来替代 A 和
    B。由于 Lambda 仅在对角线（奇异值）上有值，我们将其存储为列向量。我们选择矩阵 P（d x r）、Lambda（r x r）和 Q（r x k）的维度，以匹配权重矩阵
    W（d x k）的维度。
- en: The other novel result is the idea of using a special importance sampling technique
    to determine which elements of the SVD can be pruned out. Essentially the technique
    is to consider a group of triplets (each entry of the SVD) and determine how important
    they are to the lower dimensional representation. They accomplish this by using
    a function that relates the singular values and left/right singular vectors. These
    are then run through a sensitivity function that combines an exponential moving
    average between steps of the gradient weight product (pseudo importance) and another
    function called the uncertainty quantification, that is also exponentially averaged
    over the current step and previous step.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个新颖的结果是使用一种特殊的重标定技术来确定可以被修剪掉的 SVD 元素。基本上，该技术考虑一组三元组（SVD 的每个条目），并确定它们对低维表示的重要性。他们通过使用一个将奇异值与左/右奇异向量相关联的函数来实现这一点。这些函数随后经过一个灵敏度函数，该函数结合了梯度权重乘积的指数移动平均（伪重要性）以及另一个称为不确定性量化的函数，后者也在当前步骤和前一步骤之间进行指数平均。
- en: While pruning elements of the SVD, the rank of the lower dimension (the r term
    of the matrices) is iteratively changed as they delete the least important triplets.
    They accomplish this through a global budget scheduler that decays the rank r
    over the training steps. The budget is initialized as 1.5 times the magnitude
    of the target budget and follows a cubic decrease to the target budget after t
    warm up steps.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在修剪SVD元素时，通过删除最不重要的三元组，低维度的秩（矩阵的r项）会被迭代地改变。他们通过一个全局预算调度器来实现这一点，该调度器在训练步骤中逐渐降低秩r。预算初始化为目标预算的1.5倍，并在经过t次预热步骤后按立方方式减少到目标预算。
- en: This is conceptually a dense method to understand. If you’re technically inclined
    I encourage reading through the paper to understand the inner workings of the
    method. If you remember this as an efficient SVD implementation applied to LoRa
    that combines pruning less important singular vectors, that is probably safe at
    a conceptual level.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，这是一个难以理解的方法。如果您有技术背景，我鼓励您阅读论文以了解该方法的内部工作原理。如果您记住这是一个高效的SVD实现应用于LoRa，并结合了修剪不重要的奇异向量，那么在概念层面上，这可能是安全的。
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Comparison of Methods
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法比较
- en: To compare all of the methods in one place I created the table below to show
    their number of trainable parameters (which are all additional parameters to the
    network), the method type, and an informal summary of the method. The informal
    summary is how I would describe the method in one sentence to a college student
    who had never heard of it before.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在一个地方比较所有的方法，我创建了下面的表格来展示它们的可训练参数数量（这些都是网络的附加参数）、方法类型以及对方法的非正式总结。非正式总结是我如何用一句话向一个从未听说过该方法的大学生描述该方法。
- en: '![](../Images/92175ae12769830938d7d7d9d3d5c308.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92175ae12769830938d7d7d9d3d5c308.png)'
- en: Table adapted from Lialin et al [2]. Author contributed P-Tuning, LLaMA-Adapter,
    and AdaLoRa and informal summary column. Informal summary is how I would describe
    the paper in one sentence to a college student.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表格改编自Lialin等人[2]。作者贡献了P-Tuning、LLaMA-Adapter和AdaLoRa，并在非正式总结栏中进行总结。非正式总结是我如何用一句话向大学生描述这篇论文的方式。
- en: Is this the only conceptual guide you need?
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这是您需要的唯一概念性指南吗？
- en: I argue this is the only conceptual guide you need because after reading you
    understand the basics of PEFT techniques. If you noticed, all of the techniques
    expanded on the ideas from another. After this introduction you understand enough
    of the basics that you could explore the research papers yourself. However, if
    you end up needing another conceptual guide to understand the concepts, please
    share it in the comments of the article so other readers can find these resources!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这是您需要的唯一概念性指南，因为阅读后您将理解PEFT技术的基础知识。如果您注意到，所有技术都扩展了其他技术的思想。在这次介绍之后，您已经理解了足够的基础知识，您可以自己探索研究论文。然而，如果您最终需要另一个概念性指南来理解这些概念，请在文章的评论中分享，以便其他读者可以找到这些资源！
- en: Time to get started!
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现在是开始的时候了！
- en: After this conceptual review you are at a great point to start experimenting
    with these methods to train your own models. There are plenty of great implementation
    guides available from [Hugging Face](https://huggingface.co/docs/peft/index).
    If you want a less hands on approach you can work with Google’s Vertex AI models
    or work with OpenAI fine tuning.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次概念性回顾之后，您处于一个很好的起点，可以开始实验这些方法来训练自己的模型。来自[Hugging Face](https://huggingface.co/docs/peft/index)有很多很棒的实现指南。如果您希望采取更少的动手方法，您可以使用Google的Vertex
    AI模型或使用OpenAI的微调服务。
- en: '*Thank you for reading the article! If you have additional questions or something
    was unclear, leave a comment and I will get back to you. If you want to see more
    articles like this one, please follow me on* [*Medium*](https://medium.com/@smsmith714)
    *and on* [*LinkedIn*](https://www.linkedin.com/in/sms714/)*.*'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢您阅读这篇文章！如果您有其他问题或有不清楚的地方，请留言，我会回复您。如果您想看到更多类似的文章，请在* [*Medium*](https://medium.com/@smsmith714)
    *和* [*LinkedIn*](https://www.linkedin.com/in/sms714/)*上关注我。*'
- en: '*If you found a technical error in this article, please let me know ASAP! I
    strive to make sure the information I publish is as correct as possible, but no
    one is perfect.*'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果您发现本文中的技术错误，请尽快告知我！我努力确保我发布的信息尽可能准确，但没有人是完美的。*'
- en: '**References:**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: '[1] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian
    Tramer, & Chiyuan Zhang. (2023). Quantifying Memorization Across Neural Language
    Models.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian
    Tramer, & Chiyuan Zhang. (2023). 量化神经语言模型的记忆。'
- en: '[2] Vladislav Lialin, Vĳeta Deshpande, & Anna Rumshisky. (2023). Scaling Down
    to Scale Up: A Guide to Parameter-Efficient Fine-Tuning.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Vladislav Lialin, Vĳeta Deshpande, & Anna Rumshisky. (2023). 缩小规模以扩大规模：参数高效微调指南。'
- en: '[3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N. Gomez, Lukasz Kaiser, & Illia Polosukhin (2017). Attention Is All You
    Need*. CoRR, abs/1706.03762.*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N. Gomez, Lukasz Kaiser, & Illia Polosukhin (2017). 关注即是你所需要的*。CoRR, abs/1706.03762*。'
- en: '[4] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
    de Laroussilhe, Andrea Gesmundo, Mona Attariyan, & Sylvain Gelly (2019). Parameter-Efficient
    Transfer Learning for NLP*. CoRR, abs/1902.00751.*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
    de Laroussilhe, Andrea Gesmundo, Mona Attariyan, & Sylvain Gelly (2019). NLP 的参数高效迁移学习*。CoRR,
    abs/1902.00751*。'
- en: '[5] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit
    Bansal, & Colin Raffel. (2022). Few-Shot Parameter-Efficient Fine-Tuning is Better
    and Cheaper than In-Context Learning.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit
    Bansal, & Colin Raffel. (2022). 少样本参数高效微调优于上下文学习且更经济。'
- en: '[6] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang,
    & Jie Tang (2021). GPT Understands, Too*. CoRR, abs/2103.10385.*'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang,
    & Jie Tang (2021). GPT 也能理解*。CoRR, abs/2103.10385*。'
- en: '[7] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu,
    Shilin Yan, Pan Lu, Hongsheng Li, & Yu Qiao. (2023). LLaMA-Adapter: Efficient
    Fine-tuning of Language Models with Zero-init Attention.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu,
    Shilin Yan, Pan Lu, Hongsheng Li, & Yu Qiao. (2023). LLaMA-Adapter：零初始化注意力的语言模型高效微调。'
- en: '[8] Armen Aghajanyan, Luke Zettlemoyer, & Sonal Gupta (2020). Intrinsic Dimensionality
    Explains the Effectiveness of Language Model Fine-Tuning*. CoRR, abs/2012.13255.*'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Armen Aghajanyan, Luke Zettlemoyer, & Sonal Gupta (2020). 内在维度解释了语言模型微调的有效性*。CoRR,
    abs/2012.13255*。'
- en: '[9] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, & Weizhu Chen (2021). LoRA: Low-Rank Adaptation of Large Language
    Models*. CoRR, abs/2106.09685.*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, & Weizhu Chen (2021). LoRA：大语言模型的低秩适应*。CoRR, abs/2106.09685*。'
- en: '[10] Trevor Gale, Matei Zaharia, Cliff Young, & Erich Elsen. (2020). Sparse
    GPU Kernels for Deep Learning.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Trevor Gale, Matei Zaharia, Cliff Young, & Erich Elsen. (2020). 深度学习的稀疏
    GPU 内核。'
- en: '[11] Brian Lester, Rami Al-Rfou, & Noah Constant. (2021). The Power of Scale
    for Parameter-Efficient Prompt Tuning.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Brian Lester, Rami Al-Rfou, & Noah Constant. (2021). 参数高效提示调优的规模效应。'
- en: '[12] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng,
    Weizhu Chen, & Tuo Zhao. (2023). Adaptive Budget Allocation for Parameter-Efficient
    Fine-Tuning.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng,
    Weizhu Chen, & Tuo Zhao. (2023). 参数高效微调的自适应预算分配。'
