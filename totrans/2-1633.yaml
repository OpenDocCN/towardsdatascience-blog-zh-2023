- en: 'Parameter-Efficient Fine-Tuning (PEFT) for LLMs: A Comprehensive Introduction'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs 的参数高效微调 (PEFT)：全面介绍
- en: 原文：[https://towardsdatascience.com/parameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95](https://towardsdatascience.com/parameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/parameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95](https://towardsdatascience.com/parameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95)
- en: A conceptual survey of PEFT methods used by Hugging Face, Google’s Vertex AI,
    and eventually OpenAI
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对 Hugging Face、Google 的 Vertex AI 和最终 OpenAI 使用的 PEFT 方法进行概念性调查
- en: '[](https://medium.com/@smsmith714?source=post_page-----e52d03117f95--------------------------------)[![Sean
    Smith](../Images/611395d113b10ec4bbfaf781301139c7.png)](https://medium.com/@smsmith714?source=post_page-----e52d03117f95--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e52d03117f95--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e52d03117f95--------------------------------)
    [Sean Smith](https://medium.com/@smsmith714?source=post_page-----e52d03117f95--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@smsmith714?source=post_page-----e52d03117f95--------------------------------)[![肖恩·史密斯](../Images/611395d113b10ec4bbfaf781301139c7.png)](https://medium.com/@smsmith714?source=post_page-----e52d03117f95--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e52d03117f95--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e52d03117f95--------------------------------)
    [肖恩·史密斯](https://medium.com/@smsmith714?source=post_page-----e52d03117f95--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e52d03117f95--------------------------------)
    ·19 min read·Aug 22, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e52d03117f95--------------------------------)
    ·19 分钟阅读·2023 年 8 月 22 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/425a0569c2f36a07f176e6742754e25f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/425a0569c2f36a07f176e6742754e25f.png)'
- en: Image created by DALL-E. A Sunday Afternoon on the Island of La Grande Jatte
    but everyone is a humanoid.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 DALL-E 创建。《大碗岛上的一个星期天下午》，但每个人都是类人。
- en: Large Language Models (LLMs) are quite large by name. These models usually have
    anywhere from 7 to 70 billion parameters. To load a 70 billion parameter model
    in full precision would require 280 GB of GPU memory! To train that model you
    would update billions of tokens over millions or billions of documents. The computation
    required is substantial for updating those parameters. The self-supervised training
    of these models is expensive, [costing companies up to $100 million](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的名字就意味着它们很大。这些模型通常具有从 70 亿到 700 亿个参数。以全精度加载一个 700 亿参数的模型需要 280 GB
    的 GPU 内存！要训练这个模型，你需要在数百万或数十亿个文档上更新数十亿个标记。更新这些参数所需的计算量很大。这些模型的自监督训练成本很高，[公司最高花费可达
    1 亿美元](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/)。
- en: For the rest of us, there is significant interest in adapting our data to these
    models. With our limited datasets (in comparison) and lacking computing power,
    how do we create models that can improve on the major players at a fraction of
    the cost?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们来说，适应我们的数据以适配这些模型引起了显著的兴趣。面对有限的数据集（相比之下）和缺乏计算能力，我们如何在成本的一小部分下创建可以超越主要竞争者的模型？
- en: This is where the research field of Parameter-Efficient Fine-Tuning (PEFT) comes
    into play. Through various techniques, which we will soon explore in detail, we
    can augment small sections of these models so they are better suited to the tasks
    we aim to complete.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是参数高效微调（PEFT）研究领域的作用所在。通过各种技术，我们可以增强这些模型的小部分，使它们更适合我们要完成的任务。
- en: After reading this article, you will conceptually grasp each PEFT technique
    applied in Hugging Face and be able to distinguish the differences between them.
    One of the most helpful overviews I found before this article was from a [Reddit
    comment](https://www.reddit.com/r/MachineLearning/comments/14pkibg/d_is_there_a_difference_between_ptuning_and/jqkdam8/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button).
    There’s also another [exceptional article](https://lightning.ai/pages/community/article/understanding-llama-adapters/)
    available from lightning.ai (the creators of pytorch lightning.) Additionally,
    there’s a comprehensive survey that much of this piece is based on, authored by
    Liali et al [2]. In my article, I aim to address the gaps I identified while reviewing
    this material. At the time of writing, this article serves as a conceptual guide
    to all the PEFT methods present in the Hugging Face library. The goal for readers
    is to approach the research literature for other PEFT techniques with a fundamental
    understanding of the field.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本文后，你将概念性地掌握 Hugging Face 中应用的每种 PEFT 技术，并能够区分它们之间的差异。在这篇文章之前，我发现的一个最有帮助的概述来自
    [Reddit 评论](https://www.reddit.com/r/MachineLearning/comments/14pkibg/d_is_there_a_difference_between_ptuning_and/jqkdam8/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)。此外，lightning.ai（pytorch
    lightning 的创作者）还有另一篇 [杰出文章](https://lightning.ai/pages/community/article/understanding-llama-adapters/)。另外，还有一篇基于大量此文的全面调查，由
    Liali 等人撰写 [2]。在我的文章中，我旨在解决在审阅这些材料时识别出的空白点。在撰写本文时，本文作为 Hugging Face 库中所有 PEFT
    方法的概念指南。读者的目标是以对该领域的基本理解来接触其他 PEFT 技术的研究文献。
- en: 'A Moment for Self-Reflection: Is it time to fine-tune?'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自我反思的时刻：是时候进行微调了吗？
- en: I wrote a previous article about [considerations regarding fine-tuning LLMs](/thinking-about-fine-tuning-an-llm-heres-3-considerations-before-you-get-started-c1f483f293)
    and how similar performance could be achieved through In-Context Learning. Since
    then Llama 2 has been released and there has been great improvements in the open
    source LLM world. Here are some expanded thoughts I can share that extend beyond
    that article.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我写了一篇关于 [微调 LLMs 的考虑因素](/thinking-about-fine-tuning-an-llm-heres-3-considerations-before-you-get-started-c1f483f293)
    的文章，以及通过 In-Context Learning 实现类似性能的方法。自那时起，Llama 2 已经发布，开源 LLM 世界有了很大进步。这里是一些我可以分享的扩展思考，超出了那篇文章的范围。
- en: Fine-tuning is inherently dangerous for your organization. In a recent paper
    it was shown that LLMs can remember at least 1% of their training data [1]. If
    you have potential data duplication, that floor of 1% goes up even higher. If
    your fine-tuned LLMs will be used by non-internal users, ask yourself if it’s
    okay to give them the data you are going to train on. Users can do malicious things
    to your model such as a [prompt injection attack.](https://blog.mithrilsecurity.io/attacks-on-ai-models-prompt-injection-vs-supply-chain-poisoning/)
    I made a [LinkedIn post](https://www.linkedin.com/posts/sms714_fine-tuning-llms-a-high-stakes-game-activity-7095426067393384448-xR30?utm_source=share&utm_medium=member_desktop)
    about these security risks that serves as a quick overview. In the event that
    you can’t give away your data, dynamic observation selection with ICL is one of
    your best options (see my other [article](/thinking-about-fine-tuning-an-llm-heres-3-considerations-before-you-get-started-c1f483f293)
    for details.)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 微调本质上对你的组织是危险的。最近的一篇论文显示，LLMs 至少可以记住 1% 的训练数据 [1]。如果你有潜在的数据重复，这个 1% 的比例会更高。如果你的微调
    LLMs 将被非内部用户使用，问问自己是否可以将你要训练的数据提供给他们。用户可能对你的模型进行恶意操作，比如 [提示注入攻击](https://blog.mithrilsecurity.io/attacks-on-ai-models-prompt-injection-vs-supply-chain-poisoning/)。我在
    [LinkedIn](https://www.linkedin.com/posts/sms714_fine-tuning-llms-a-high-stakes-game-activity-7095426067393384448-xR30?utm_source=share&utm_medium=member_desktop)
    上发布了关于这些安全风险的帖子，作为一个快速概述。如果你不能提供你的数据，使用 ICL 的动态观察选择是你的最佳选择之一（详情请参见我的另一篇 [文章](/thinking-about-fine-tuning-an-llm-heres-3-considerations-before-you-get-started-c1f483f293)）。
- en: You must also prioritize the creation of high-quality data labels for your learning
    task. If the organization’s commitment to top-notch data is lacking, particularly
    in support of your project’s fine-tuning, I recommend considering an alternative
    approach. Models thrive on high-quality labeled inputs. If the commitment from
    your stakeholders is not there for human labelers, you risk disappointing all
    parties involved.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Who Even Uses PEFT?
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PEFT is used by most providers that offer the ability to fine-tune language
    models. If the provider doesn’t already make use of these techniques, I guarantee
    they have plans to. This article covers all of the techniques from [Hugging Face
    PEFT](https://huggingface.co/docs/peft/index) that are available at time of writing.
    The survey from Lialin et al. [2] is referenced by Google in their introductory
    video about [tuning foundation models on Vertex AI](https://www.youtube.com/watch?v=4A4W03qUTsw).
    While Vertex AI is more of a black box, I have heard use of [adapters](https://services.google.com/fh/files/misc/adaptation_of_foundation_models_whitepaper_google_cloud.pdf),
    prompt-tuning, and recently LoRa from sales pitches. It’s unclear exactly what
    they are use, but at it’s core the techniques we discuss here are what’s powering
    things.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI does offer fine-tuning, but however famously [has not implemented any
    PEFT methods](https://web.archive.org/web/20230531203946/https://humanloop.com/blog/openai-plans)
    yet. This is based on a blog post that was requested to be deleted by OpenAI a
    few months ago. The article details OpenAI does not use Adapters or LoRa to make
    fine-tuning more compute friendly. There has been no announcement from OpenAI
    that this has been implemented, so the safe assumption is that these features
    are not available to users yet. It is included on the roadmap for OpenAI and since
    [fine-tuning is much more lucrative](https://openai.com/pricing) than normal model
    use I suspect it will be available in the near future.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Quick Transformer Review
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I assume that readers of this article are familiar with the Transformer architecture.
    You don’t need to be intimately invested in the details of self-attention or any
    components, but you should have glanced at Vaswani et al. [3] and maybe had a
    walkthrough of [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
    (in my opinion that’s the best resource to learn the Transformer.)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'I am going to include this pseudo code for the transformer block. If you don’t
    know much about transformers, just know that at it’s core they do this:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: All functions from that pseudo code are as described in Vaswani et al. The FFN
    is a Feed Forward Network, which is 2 layers for our purposes. Many PEFT techniques
    that follow make changes to the transformer block or to self-attention, so I’ll
    reference and change this pseudo code as we move through the guide.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: A Tour through PEFT Methods
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/b9e462418b0698dca2f8e214b7774e8e.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: Overview of methods and classes from [2].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: We’ll go through each technique by looking at the broader classes in the diagram
    above. The classes we will cover are Additive, Adapters, Soft-Prompts, Reparameterization,
    and one Hybrid method that is a combination of Reparameterization and Selective
    (that isn’t Sparse LoRa).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Additive Methods
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Additive methods are probably the easiest to grasp. The goal of additive methods
    is to add an additional set of parameters or network layers to augment the model.
    When fine-tuning the data you update the weights only of these newly added parameters.
    This makes training computationally easier and also adapts to smaller datasets
    (think in the ballpark of 100–500 for starters, with a ceiling near 100,000.)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'Method: Adapters'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adapters are simultaneously a method and a class. This technique was introduced
    in Houlsby et al [4]. The goal of adapters is to add small fully connected networks
    after Transformer sub-layers and learn those parameters. I follow the definitions
    from [2] and keep a strict definition of adapters as only adding fully connected
    layers to the network.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Houlsby et al. propose a simple update to the transformer block. They add fully
    connected layers in two places as shown in this pseudo code.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Method: (IA)³'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Infused Adapter by Inhibiting and Amplifying Inner Activations, or (IA)³ is
    a very interesting additive method (adding parameters) that augments the transformer
    block with some new parameters. It was proposed by Liu et al. [5] in 2022\. Despite
    the name, this is not an adapter method since it does not strictly add fully connected
    layers after the sub-layers of the transformer block .
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider the scaled dot-product attention found in a normal transformer:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Scaled dot-product attention from Vaswani et al. [3]
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are working with an additive method, we are seeking to add parameters
    to this network. We want the dimensionality to be quite small. (IA)³ proposes
    the following new vectors to be added to the attention mechanism:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Scaled dot-product attention in (IA)³ from [5]. Here we added two column vectors
    to the normal equation, l_k, l_v, which take the Hadamard product by the key and
    value terms, respectively.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: We just added column vectors l_k and l_v and take the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))
    between the column vector and the matrix (multiple the column vector against all
    columns of the matrix).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'We also introduce one other learnable column vector l_{ff} that is added to
    the feed forward layers as follow:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Feed Forward update in (IA)³ adapted from [5]. We can see that we add the column
    vector l_{ff} to the network and take Hadamard product with the output of the
    first layer of the transformer blocks FFN. The function gamma is the activation
    function [GELU](https://paperswithcode.com/method/gelu#:~:text=The%20GELU%20activation%20function%20is%20x%20%CE%A6%20(%20x%20)%20%2C%20where,of%20as%20a%20smoother%20ReLU.).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, gamma is the activation function applied to the product between
    the weights and input. Here is some pseudo code for (IA)³:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，gamma 是应用于权重和输入之间乘积的激活函数。这是 (IA)³ 的一些伪代码：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Soft-Prompts
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软提示
- en: To understand soft-prompts, let’s first discuss hard-prompts, a concept that
    most readers might be familiar with, even if not by name. In hard prompting, we
    put together a dataset of prompts that represent the task at hand. When someone
    interacts with the network by posing a question, they could phrase it in different
    ways. With hard prompting, the process involves curating a dataset that covers
    the various ways a particular task could be framed for the language model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解软提示，我们首先讨论硬提示，一个大多数读者可能都熟悉的概念，即使不是通过名字。在硬提示中，我们会将代表当前任务的数据集整理在一起。当有人通过提出问题与网络互动时，他们可能会用不同的方式表达问题。使用硬提示，这个过程涉及策划一个数据集，涵盖语言模型可以框定特定任务的各种方式。
- en: Soft-prompting is a technique that tries to avoid this dataset creation. In
    hard prompting, we are creating data in a discrete representation (picking words.)
    In soft-prompting, we seek a continuous representation of the text we will input
    to the model. This does imply you need one static prompt for the examples you
    are training on.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 软提示（Soft-prompting）是一种试图避免创建数据集的技术。在硬提示中，我们是在离散表示中创建数据（选择单词）。在软提示中，我们寻求对输入到模型中的文本进行连续表示。这意味着你需要为正在训练的示例提供一个静态提示。
- en: Depending on the technique, there are different methods for how the information
    is added to the network. The core idea is that the base model does not optimize
    the text itself but rather the continuous representation (i.e. some type of learnable
    tensor) of the prompt text. This can be some form of embedding or some transformation
    applied to that embedding. These techniques will be explored in more detail as
    we move on.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 根据技术的不同，信息添加到网络中的方法也各不相同。核心思想是基础模型并不优化文本本身，而是优化提示文本的连续表示（即某种类型的可学习张量）。这可以是某种形式的嵌入或对该嵌入应用的某种变换。这些技术将在我们继续深入探讨时详细说明。
- en: 'Method: Prompt-Tuning'
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法：Prompt-Tuning
- en: '![](../Images/08f053c16dae10c6ecf38a250d65c7dd.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08f053c16dae10c6ecf38a250d65c7dd.png)'
- en: Image from prompt-tuning from Lester et [11]. This shows that in prompt-tuning
    we concatenate the soft-prompt and the input text’s representation (embedding)
    to the pre-trained model. Doing this allows us to optimize our representation
    of the soft-prompt through a learnable tensor.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来自 Lester 等人 [11] 的 prompt-tuning。这表明，在 prompt-tuning 中，我们将软提示和输入文本的表示（嵌入）连接到预训练模型中。这样做使我们能够通过一个可学习的张量来优化软提示的表示。
- en: Prompt tuning is a technique from Lester et al. [11] that falls into the category
    of soft-prompts. With soft-prompts our goal is to add information to the base
    model that is more specific to our current task. With prompt tuning we accomplish
    this by creating a set of parameters for the prompt tokens and injecting this
    at the beginning of the network.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Prompt tuning 是 Lester 等人 [11] 提出的技术，属于软提示的范畴。使用软提示时，我们的目标是向基础模型添加更具体于当前任务的信息。通过
    prompt tuning，我们通过创建一组提示令牌的参数并将其注入到网络的开头来实现这一点。
- en: To find a representation of the soft prompt, we create a separate set of embeddings
    for the static prompt used during training. We concatenate the output embeddings
    with the sequence embeddings. We use this new information to pass into the language
    model. Creating this dual information allows us to learn a parameterization of
    the soft prompt without needing to create many prompts for the same task.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到软提示的表示，我们为训练过程中使用的静态提示创建了一组单独的嵌入。我们将输出嵌入与序列嵌入进行连接。我们使用这些新信息传递到语言模型中。创建这种双重信息使我们能够学习软提示的参数化，而无需为同一任务创建多个提示。
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: There are a number of rich benefits to fine-tuning through this approach. This
    new set of parameters can be very small, around 0.01% the size of the tunable
    parameters of the base model. This creates opportunities to have an ensemble of
    task specific models that all run off the same base model, which drastically decreases
    memory requirements for the model. For more on this, check out this post I shared
    on [LinkedIn](https://www.linkedin.com/posts/sms714_ensembling-has-long-been-a-favorite-technique-activity-7097213152114708480-BHxn?utm_source=share&utm_medium=member_desktop)
    and also the section on ensembling in [3].
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方法进行微调有许多丰富的好处。这组新参数可以非常小，大约是基础模型可调参数的 0.01%。这创造了一个使用相同基础模型的任务特定模型集合的机会，这大大减少了模型的内存需求。有关更多信息，请查看我在[LinkedIn](https://www.linkedin.com/posts/sms714_ensembling-has-long-been-a-favorite-technique-activity-7097213152114708480-BHxn?utm_source=share&utm_medium=member_desktop)上分享的帖子以及[3]中的集成部分。
- en: 'Method: Prefix Tuning'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法：前缀调整
- en: Prefix tuning is another soft prompting technique that is very similar to prompt
    tuning. In prompt tuning we created a separate set of parameters that we fed our
    input into and appended the outputs to the continuous representation of the text
    input in the model. In Prefix tuning we also find a continuous representation
    from a separate set of prompt tokens that are input into the base model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀调整是另一种与提示调整非常相似的软提示技术。在提示调整中，我们创建了一组单独的参数，将输入传递给这些参数，并将输出附加到模型中输入文本的连续表示上。在前缀调整中，我们还从一组单独的提示标记中找到一个连续的表示，这些标记被输入到基础模型中。
- en: The difference between prefix tuning and prompt tuning is that the representation
    from prefix tuning is fed to all layers of the transformer whereas prompt tuning
    was only concatenated with the embeddings. Additionally, we also learn additional
    parameters for the soft prompt for prefix tuning in the form of a fully connected
    network. After training the FFN is discarded and we only use the soft-prompt as
    input.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀调整和提示调整的区别在于，前缀调整的表示会传递到变换器的所有层，而提示调整仅与嵌入层级联。此外，我们还为前缀调整学习了额外的参数，形式为一个全连接网络。训练后，FFN
    被丢弃，我们只使用软提示作为输入。
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Method: P-Tuning'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法：P-Tuning
- en: '![](../Images/48f16389543c55a9a51eaeb37d614d7e.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48f16389543c55a9a51eaeb37d614d7e.png)'
- en: Image for P-Tuning from Liu et al [6]. This shows the creation of the prompt
    embedding throughout the prompt encoding being concatenated with the input embedding.
    The prompt encoder consists of and embedding, LSTM, and then some fully connected
    layers.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 刘等人[6]提供的 P-Tuning 图像。该图展示了提示嵌入的创建过程，整个提示编码与输入嵌入进行级联。提示编码器包括一个嵌入层、LSTM，然后是一些全连接层。
- en: P-Tuning is another soft-prompting method introduced by Liu et al. [6] that
    differs from prompt and prefix tuning. Colloquially we can think of P-Tuning as
    prompt-tuning but encoding the prompt using an LSTM.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: P-Tuning 是刘等人[6]提出的另一种软提示方法，与提示调整（prompt tuning）和前缀调整（prefix tuning）不同。通俗地说，我们可以将
    P-Tuning 视为提示调整，但使用 LSTM 对提示进行编码。
- en: P-Tuning sets out to solve two problems the authors noticed. The first is the
    discreteness of the word embeddings passed to the model. The authors argue that
    if the embeddings are randomly initialized and then optimized through Stochastic
    Gradient Descent, the model is likely to fall into a local minima. They second
    is association of the word embeddings. With the parameterization in prompt-tuning
    and prefix-tuning, the soft prompts are technically independent of each other.
    The authors wanted an approach that made the prompt tokens dependent on each other.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: P-Tuning 旨在解决作者注意到的两个问题。第一个是传递给模型的词嵌入的离散性。作者认为，如果嵌入是随机初始化的，然后通过随机梯度下降优化，模型可能会陷入局部最小值。第二个是词嵌入的关联性。在提示调整和前缀调整的参数化中，软提示在技术上是相互独立的。作者希望找到一种方法，使提示标记相互依赖。
- en: The authors propose that a prompt is a function that takes a context x and a
    target y and organizes itself into a template T. The authors provide the example
    sequence “The capital of Britain is [MASK]”. Here the prompt is “The capital of
    … is …”, the context is “Britain” and the target is [MASK]. We can use this formulation
    to create two sequences of tokens, everything before the context and everything
    after the context before the target. We can learn a representation of this additional
    information that can be reduced to a continuous output and fed into the language
    model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们提出，提示是一个函数，它接受一个上下文x和一个目标y，并将其组织成一个模板T。作者提供了示例序列“The capital of Britain is
    [MASK]”。这里的提示是“The capital of … is …”，上下文是“Britain”，目标是[MASK]。我们可以使用这种表述创建两个标记序列：上下文之前的所有内容以及上下文之后和目标之前的所有内容。我们可以学习这些附加信息的表示，并将其缩减为连续输出，输入到语言模型中。
- en: To embed the prompt in this way, we use a small network of an LSTM fed into
    a two layer FFN. We pass the prompt tokens, those before the context and those
    after and before the target.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以这种方式嵌入提示，我们使用一个由LSTM组成的小型网络，输入到一个两层的FFN中。我们传递提示标记，包括上下文之前的和目标之前的标记。
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Method: LLaMA-Adapater'
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法：LLaMA-Adapter
- en: '![](../Images/8b51188fac198efd68e593fdf1f575db.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b51188fac198efd68e593fdf1f575db.png)'
- en: Image for LLaMA-Adapter from Zhang et al. [7] We can see the addition of zero-initialized
    attention being used on the adaption prompts and that those are the only thing
    fine-tuned.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从Zhang等人[7]处获得的LLaMA-Adapter图像。我们可以看到零初始化的注意力被用于适配提示，并且这些是唯一被微调的内容。
- en: LLaMA adapter is a soft-prompting technique introduced by Zhang et al. [7] that
    applies a more efficient version of prefix learning to the Llama model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA adapter是Zhang等人[7]引入的一种软提示技术，它将更高效的前缀学习版本应用于Llama模型。
- en: LLaMA-Adapter has a few key differences from Prefix Tuning. They introduce Adaptation
    Prompts, which are soft-prompts appended with the input to the transformer layer.
    These adaption prompts are inserted in the L topmost of the N transformer layers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA-Adapter与Prefix Tuning有一些关键差异。它们引入了适配提示，这些是附加到变换器层输入的软提示。这些适配提示被插入到N个变换器层的最上层L处。
- en: The authors also introduce zero initialized attention. With additive methods
    we introduce a new set of parameters that have some random initialization over
    the weights. Because of this random noise added to the LM, we can potentially
    experience unstable fine-tuning which can cause a problem with large loss values
    at the early stages. To solve this problem, the authors introduce a gating factor,
    initialized to 0, that is multiplied by the self attention mechanism. The product
    of the gating factor and self-attention is referred to as zero-init attention.
    The gating value is adaptively tuned over the training steps to create a smoother
    update of the network parameters.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们还引入了零初始化注意力。通过加法方法，我们引入了一组在权重上有一些随机初始化的新参数。由于这种随机噪声的加入，可能会经历不稳定的微调，这可能在早期阶段导致较大的损失值。为了解决这个问题，作者们引入了一个初始化为0的门控因子，该因子与自注意力机制相乘。门控因子和自注意力的乘积称为零初始化注意力。门控值在训练步骤中自适应调整，以创建更平滑的网络参数更新。
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Reparameterization-Based Methods
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于重参数化的方法
- en: Reparameterization-Based methods focus on finding low dimensional representations
    of the same weight matrices found in the base model. The first connection between
    fine-tuning and a low dimensional representation was shown in Hu et al [8]. The
    authors make a connection between the full parameters of the model and a lower
    dimensional representation. Depending on the task, the authors are able to achieve
    90% of the results of the fully fine-tuned model with approximately 0.0002% of
    the trainable parameters.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 基于重参数化的方法专注于找到与基础模型中相同的权重矩阵的低维表示。Hu等人[8]首次展示了微调与低维表示之间的联系。作者在模型的完整参数和较低维度表示之间建立了联系。根据任务，作者能够用大约0.0002%的可训练参数实现完全微调模型的90%的结果。
- en: '**Method: LoRa**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法：LoRa**'
- en: '![](../Images/c7f6a7616ba55d625afb1f93ba61941d.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7f6a7616ba55d625afb1f93ba61941d.png)'
- en: Image taken from Hu & Shen et al [9]. Here we can see the pre-trained weights
    and the additional matrices A and B. A is initialized normally and B is initialized
    to 0\. We train only A and B.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从Hu & Shen等人[9]处获取的图像。我们可以看到预训练权重以及额外的矩阵A和B。A被正常初始化，而B被初始化为0。我们仅训练A和B。
- en: One of the most popular techniques in fine-tuning is a reparameterization-based
    method called Low-Rank Adaptation (LoRa) [9]. LoRa updates a weight matrix by
    learning a separate matrix which represents the updates from optimization. They
    go a step further to create two smaller dimension weight matrices to represent
    this difference. By creating smaller dimension weight matrices we have less parameters
    to learn.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: To train LoRa we use the fundamental ideas of gradient descent, where we make
    incremental adjustments to a set of parameters that move us closer to our goal
    (loss function). In LoRa we choose to isolate all of our updates to a separate
    matrix. This matrix, we denote Delta W, represents all of the parameter updates
    we learn in this during the fine-tuning process.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assign W_0 to have dimensions dxk (d rows and k columns). We want to update
    it’s parameters so that it is aligned with our new goal. You can represent this
    update to the parameters by ΔW, which also the dimension dxk. We can model our
    update rule using the equation below.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Update rule for W_0 from [9]. We isolate the changes to W_0 in DeltaW. We then
    represent DeltaW as the product of A and B, two smaller dimension matrices. This
    way we learn less parameters but still update W, which makes fine-tuning computational
    easier.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s change our update rule such that ΔW is modeled by a matrix multiplication
    AB. We assign matrix A to have dimensions dxr and B to have dimensions rxk. If
    you’re up to speed on your matrix multiplications you will see that AB has the
    same dimensions as W_0, so the addition of these matrices is valid. Here’s why
    AB is a better choice than DeltaW: Matrix A only has dxr and matrix B has rxk.
    If we make r a very small number (r=8 is a typical value), then the number of
    parameters in A and B is drastically smaller than ΔW. If we only learn the parameters
    of A and B, then we learn `d*k-d*r-r*k` less parameters. In practice this allows
    us to learn only 0.1–0.5% of the parameters of the original network.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: The walk through I just illustrated is the essence of how LoRa works. Instead
    of optimizing a matrix W through additional training steps, we alter a matrix
    ΔW through two new matrices A and B that have far fewer parameters. This result
    helps us optimize many fewer parameters, which makes our training much more efficient.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Typically we use this update rule for the key and value matrices of the self
    attention in the transformer block. We also add a scaling factor, set to 1/r,
    to the amount of information given to the update. See the pseudo code below.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Selective Methods
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With selective methods we choose some of the parameters to update and do not
    update others. The problem with these approaches is that we have created a sparse
    matrix of parameters. Sparse matrix operations are not well supported on present
    day GPUs and provide computation challenges. For more information on why sparse
    matrices produce computational challenges, check out [10].
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: There are also techniques in Selective methods that focus on pruning unsuccessful
    vectors or manipulating model biases. These also create additional complexity
    when attempting to train the model. In general these methods have more challenging
    implementations since their compute operations are more expensive than other operations.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择性方法中，还有一些技术专注于修剪不成功的向量或操控模型偏差。这些方法在训练模型时也会增加额外的复杂性。一般来说，这些方法的实现更具挑战性，因为它们的计算操作比其他操作更为昂贵。
- en: '**Method: AdaLoRa**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法：AdaLoRa**'
- en: This is a hybrid approach that combines ideas from reparameterized and selective
    methods. Zhang et al [12] developed AdaLoRa by studying LoRa and formed the question
    “How can we allocate the parameter budget adaptively according to [the] importance
    of modules to improve the performance of parameter-efficient fine-tuning?” What
    this translates to is “How can we give preference to the parameters that lead
    to better performance rather than treating all parameters equally?”
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种混合方法，结合了重新参数化和选择性方法的思想。Zhang 等人 [12] 通过研究 LoRa 开发了 AdaLoRa，并提出了这样一个问题：“我们如何根据模块的重要性自适应地分配参数预算，以提高参数高效微调的性能？”这意味着“我们如何优先考虑那些能带来更好性能的参数，而不是平等对待所有参数？”
- en: 'Instead of using two matrices A and B as we did in LoRa, AdaLoRa uses an approximation
    of [Singular Value Decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition)
    to reduce the dimensionality of a vector space into three matrices: P (left singular
    vectors), Lambda (singular values), and Q (right singular vectors). Using these
    three matrices we can reconstruct an approximation of our vector space Delta as
    P * Lambda * Q. The benefit of using SVD is the singular values represent the
    importance of the vector in this lower dimensional space. The contribution of
    this paper is applying some efficiency implementations to use an SVD related approach
    to consider importance around which weights should be optimized.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在 LoRa 中使用的两个矩阵 A 和 B 不同，AdaLoRa 使用[奇异值分解 (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition)的近似来将向量空间的维度降低到三个矩阵：P（左奇异向量）、Lambda（奇异值）和
    Q（右奇异向量）。使用这三个矩阵，我们可以重建向量空间 Delta 的近似值，即 P * Lambda * Q。使用 SVD 的好处在于奇异值表示了在这个低维空间中向量的重要性。本文的贡献在于应用一些高效实现，以使用与
    SVD 相关的方法来考虑哪些权重应当被优化。
- en: In LoRa, we saw that we can approximate delta W with two matrices A and B. Here
    we can replace A and B with our new approximation P * Lambda * Q. Since lambda
    only has values along the diagonal (singular values) we store it as a column vector.
    We pick the dimensions of our matrices P (d x r), Lambda (r x r), and Q (r x k)
    to match the dimensions of a weight matrix W (d x k).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LoRa 中，我们看到可以用两个矩阵 A 和 B 来近似 delta W。在这里，我们可以用新的近似值 P * Lambda * Q 来替代 A 和
    B。由于 Lambda 仅在对角线（奇异值）上有值，我们将其存储为列向量。我们选择矩阵 P（d x r）、Lambda（r x r）和 Q（r x k）的维度，以匹配权重矩阵
    W（d x k）的维度。
- en: The other novel result is the idea of using a special importance sampling technique
    to determine which elements of the SVD can be pruned out. Essentially the technique
    is to consider a group of triplets (each entry of the SVD) and determine how important
    they are to the lower dimensional representation. They accomplish this by using
    a function that relates the singular values and left/right singular vectors. These
    are then run through a sensitivity function that combines an exponential moving
    average between steps of the gradient weight product (pseudo importance) and another
    function called the uncertainty quantification, that is also exponentially averaged
    over the current step and previous step.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个新颖的结果是使用一种特殊的重标定技术来确定可以被修剪掉的 SVD 元素。基本上，该技术考虑一组三元组（SVD 的每个条目），并确定它们对低维表示的重要性。他们通过使用一个将奇异值与左/右奇异向量相关联的函数来实现这一点。这些函数随后经过一个灵敏度函数，该函数结合了梯度权重乘积的指数移动平均（伪重要性）以及另一个称为不确定性量化的函数，后者也在当前步骤和前一步骤之间进行指数平均。
- en: While pruning elements of the SVD, the rank of the lower dimension (the r term
    of the matrices) is iteratively changed as they delete the least important triplets.
    They accomplish this through a global budget scheduler that decays the rank r
    over the training steps. The budget is initialized as 1.5 times the magnitude
    of the target budget and follows a cubic decrease to the target budget after t
    warm up steps.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: This is conceptually a dense method to understand. If you’re technically inclined
    I encourage reading through the paper to understand the inner workings of the
    method. If you remember this as an efficient SVD implementation applied to LoRa
    that combines pruning less important singular vectors, that is probably safe at
    a conceptual level.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Comparison of Methods
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To compare all of the methods in one place I created the table below to show
    their number of trainable parameters (which are all additional parameters to the
    network), the method type, and an informal summary of the method. The informal
    summary is how I would describe the method in one sentence to a college student
    who had never heard of it before.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/92175ae12769830938d7d7d9d3d5c308.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: Table adapted from Lialin et al [2]. Author contributed P-Tuning, LLaMA-Adapter,
    and AdaLoRa and informal summary column. Informal summary is how I would describe
    the paper in one sentence to a college student.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Is this the only conceptual guide you need?
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I argue this is the only conceptual guide you need because after reading you
    understand the basics of PEFT techniques. If you noticed, all of the techniques
    expanded on the ideas from another. After this introduction you understand enough
    of the basics that you could explore the research papers yourself. However, if
    you end up needing another conceptual guide to understand the concepts, please
    share it in the comments of the article so other readers can find these resources!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Time to get started!
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After this conceptual review you are at a great point to start experimenting
    with these methods to train your own models. There are plenty of great implementation
    guides available from [Hugging Face](https://huggingface.co/docs/peft/index).
    If you want a less hands on approach you can work with Google’s Vertex AI models
    or work with OpenAI fine tuning.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '*Thank you for reading the article! If you have additional questions or something
    was unclear, leave a comment and I will get back to you. If you want to see more
    articles like this one, please follow me on* [*Medium*](https://medium.com/@smsmith714)
    *and on* [*LinkedIn*](https://www.linkedin.com/in/sms714/)*.*'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '*If you found a technical error in this article, please let me know ASAP! I
    strive to make sure the information I publish is as correct as possible, but no
    one is perfect.*'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '**References:**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian
    Tramer, & Chiyuan Zhang. (2023). Quantifying Memorization Across Neural Language
    Models.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Vladislav Lialin, Vĳeta Deshpande, & Anna Rumshisky. (2023). Scaling Down
    to Scale Up: A Guide to Parameter-Efficient Fine-Tuning.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N. Gomez, Lukasz Kaiser, & Illia Polosukhin (2017). Attention Is All You
    Need*. CoRR, abs/1706.03762.*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
    de Laroussilhe, Andrea Gesmundo, Mona Attariyan, & Sylvain Gelly (2019). Parameter-Efficient
    Transfer Learning for NLP*. CoRR, abs/1902.00751.*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit
    Bansal, & Colin Raffel. (2022). Few-Shot Parameter-Efficient Fine-Tuning is Better
    and Cheaper than In-Context Learning.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang,
    & Jie Tang (2021). GPT Understands, Too*. CoRR, abs/2103.10385.*'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu,
    Shilin Yan, Pan Lu, Hongsheng Li, & Yu Qiao. (2023). LLaMA-Adapter: Efficient
    Fine-tuning of Language Models with Zero-init Attention.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Armen Aghajanyan, Luke Zettlemoyer, & Sonal Gupta (2020). Intrinsic Dimensionality
    Explains the Effectiveness of Language Model Fine-Tuning*. CoRR, abs/2012.13255.*'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, & Weizhu Chen (2021). LoRA: Low-Rank Adaptation of Large Language
    Models*. CoRR, abs/2106.09685.*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Trevor Gale, Matei Zaharia, Cliff Young, & Erich Elsen. (2020). Sparse
    GPU Kernels for Deep Learning.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[11] Brian Lester, Rami Al-Rfou, & Noah Constant. (2021). The Power of Scale
    for Parameter-Efficient Prompt Tuning.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[12] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng,
    Weizhu Chen, & Tuo Zhao. (2023). Adaptive Budget Allocation for Parameter-Efficient
    Fine-Tuning.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
