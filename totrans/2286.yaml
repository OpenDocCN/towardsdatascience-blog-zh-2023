- en: Variance Reduction with Importance Sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/variance-reduction-with-importance-sampling-4e5ca4b1c5a7](https://towardsdatascience.com/variance-reduction-with-importance-sampling-4e5ca4b1c5a7)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mathematical explanation and Python implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@hrmnmichaels?source=post_page-----4e5ca4b1c5a7--------------------------------)[![Oliver
    S](../Images/b5ee0fa2d5fb115f62e2e9dfcb92afdd.png)](https://medium.com/@hrmnmichaels?source=post_page-----4e5ca4b1c5a7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4e5ca4b1c5a7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4e5ca4b1c5a7--------------------------------)
    [Oliver S](https://medium.com/@hrmnmichaels?source=post_page-----4e5ca4b1c5a7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4e5ca4b1c5a7--------------------------------)
    ·6 min read·Jan 23, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08c4a7efaed06bcd1a42a2a74b87d3d5.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Edge2Edge Media](https://unsplash.com/@edge2edgemedia?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/uKlneQRwaxY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: 'In a previous post I introduced [different numerical sampling techniques](https://medium.com/towards-data-science/introduction-to-sampling-methods-c934b64b6b08),
    one of them being importance sampling. In that post we used this technique to
    allow sampling from complex distributions, sampling from which would otherwise
    be infeasible. However, importance sampling is frequently used for another reason,
    namely variance reduction: that is, by choosing a suited proposal distribution
    we can reduce the variance of our estimator — which we will cover here.'
  prefs: []
  type: TYPE_NORMAL
- en: Recap of Importance Sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Assume we don’t just want to calculate the expectation `E[X]` of a random variable
    `X`, but instead the expectation of a function of that variable, `f[X]`. In a
    continuous setting this is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a2277abaf1b807134e3d468a6746f55c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can approximate this expectation using numerical approximation, also known
    as Monte Carlo methods, by sampling `n` random values from the distribution `p`
    and then calculate the sample mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d71a510166f12a531ba446bebc91553.png)'
  prefs: []
  type: TYPE_IMG
- en: The idea behind importance sampling now is to use a simple re-formulation trick
    and write the expectation as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e934ad8311d1eebee46f1ea8f8b3475.png)'
  prefs: []
  type: TYPE_IMG
- en: '— giving the expectation of `f(x)p(x)/q(x)` over the distribution `q`! And
    with that, allowing us to calculate the sample mean by sampling from q:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bba9896d6d48eea3800081219a66f457.png)'
  prefs: []
  type: TYPE_IMG
- en: Variance Reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The variance of the standard Monte Carlo estimator is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b0a2d4c62b7d885309f51612c9ad053.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The variance for the reformulated importance sampling estimator is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96c5b4150dc04f404bc82ce766aaa920.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So as a first step we definitely observe a difference in variance, meaning
    with high probability we can also find a way to reduce this. And indeed it is
    relatively easy to see that this variance can be reduced to 0 by choosing `q`
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a1335f282bee4556dbe3531421a9cfb0.png)'
  prefs: []
  type: TYPE_IMG
- en: (Insert this term in the equation above, and picture `f(x)p(x)` cancelling each
    other out — leaving `Var[E[f(X)]]=0`.)
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, we don’t know `E[f(X)]`, as the reason we are doing this sampling
    after all is to find the expectation of `f`.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we can think of `E[f(X)]` as some normalisation constant, and at least
    take one essential insight away from this: we should construct `q` s.t. it has
    high density wherever `f(x)p(x)` is high. And with this, let’s dive into a practical
    example and apply this learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Practical Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the sake of demonstration, we want a pointy function `f`, and a probability
    distribution `p` which do not overlap too well. Thus for simplicity let us set
    both to be normal distributions, e.g. `f = N(5, 1)` and `p = N(9, 2)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5d099c84af14d9dc5aeed23aae07bff.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'I hope choosing a normal distribution for both does not confuse the reader,
    so let’s re-iterate what we’re trying to do here: we want to compute `E[f(X)]`,
    where `X` is a random variable which follows the distribution `p` — i.e. we want
    to compute the mean of `f` under `p`. Note this mean is not the mean usually associated
    to a normal distribution (which is a value on the x-axis, namely the mode of the
    distribution), but now we are after the mean of the y-values under `p`: in this
    example it is ~0.36 — a much lesser known and used value.'
  prefs: []
  type: TYPE_NORMAL
- en: To approximate this numerically, as stated above we would now sample values
    `x` from the distribution `p`, and compute the empirical mean of `f(x)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Intuitively one can see why sampling from this distribution is a bad idea,
    hopefully amplified by the previous section: for most values sampled from `p`,
    `f` will be close to 0 — but for a few sampled values `f` will be very large —
    thus we obtain a large variance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, following above introduced ideas we now propose a new distribution
    `q = N(5.8, 1)`, which satisfies the derived criterion that its density is high
    in regions where `f(x)p(x)` is high:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/80427c90aea2528b35de4eb2edd03e05.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Note it’s not trivial to find this function, and certainly there are much more
    difficult real-word scenarios. We have to try and satisfy the criterion as well
    as possible, but also take care of satisfying the importance sampling requirement
    of `p` covering `q`, etc. For this example I actually plotted `p(x)f(x)` and then
    picked a `q` which resembled it best.
  prefs: []
  type: TYPE_NORMAL
- en: Python Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s code this in Python. First, we introduce the necessary functions and
    distributions, and for convenience use `functools.partials` to obtain a function
    representing a normal distribution with fixed mean / standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we generate the plot from above for orientation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we come to the (importance) sampling part. First, we compute the direct
    Monte Carlo Estimator for `E[f(X)]`. We generate random samples `x` from `p`,
    and calculate the mean of `f(x)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we apply importance sampling, i.e. sample from `q`and correct via the importance
    weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Putting it all together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Original mean / variance: 0.036139 / 0.007696 Importance sampling mean / variance:
    0.036015 / 0.000027`'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we still obtain the correct mean, but have reduced variance by ~100x!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Importance sampling is a clever reformulation trick, allowing us to compute
    expectations and other moments by sampling from a different proposal distribution.
    This not only allows sampling from complex, otherwise hard-to-sample distributions,
    but also changes the variance of the resulting estimator. In this post we showed
    how to make use of this to reduce the variance. In particular, we proved and showed
    that selecting a proposal distribution with high probability in regions where
    `p(x)f(x)` (product of original distribution and function in question) is high,
    yields best results.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: 'This post is Part 2 of a series about sampling. You can find the others here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 1: [Introduction to Sampling Methods](https://medium.com/towards-data-science/introduction-to-sampling-methods-c934b64b6b08)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 3: [Introduction to Markov chain Monte Carlo (MCMC) Methods](https://medium.com/towards-data-science/introduction-to-markov-chain-monte-carlo-mcmc-methods-b5bad18bc243)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
